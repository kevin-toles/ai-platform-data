{
  "metadata": {
    "title": "PostgreSQL 14 Internals",
    "author": "Egor Rogov",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 629,
    "conversion_date": "2025-12-25T18:15:30.920577",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "PostgreSQL 14 Internals.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Introduction",
      "start_page": 25,
      "end_page": 46,
      "detection_method": "regex_chapter",
      "content": "Chapter 1 Introduction\n\nTablespaces\n\nUnlike databases and schemas, which determine logical distribution of objects, tablespaces define physical data layout. A tablespace is virtually a directory in a file system. You can distribute your data between tablespaces in such a way that archive data is stored on slow disks, while the data that is being actively updated goes to fast disks.\n\nOneandthesametablespacecanbeusedbydifferentdatabases,andeachdatabase can store data in several tablespaces. It means that logical structure and physical data layout do not depend on each other.\n\nEach database has the so-called default tablespace. All database objects are cre- ated in this tablespace unless another location is specified. System catalog objects related to this database are also stored there.\n\npg_global\n\ncommon cluster objects\n\npg_default\n\nxyzzy\n\npg_catalog\n\npublic\n\nplugh\n\npg_catalog\n\npublic\n\npostgres\n\ntemplate1\n\n26\n\n1.1 Data Organization\n\nDuring cluster initialization, two tablespaces are created:\n\npg_default is located in the ������/base directory; it is used as the default ta- blespace unless another tablespace is explicitly selected for this purpose.\n\npg_global is located in the ������/global directory; it stores system catalog objects\n\nthat are common to the whole cluster.\n\nWhen creating a custom tablespace,you can specify any directory; Postgre��� will create a symbolic link to this location in the ������/pg_tblspc directory. In fact, all paths used by Postgre��� are relative to the ������ directory, which allows you to move it to a different location (provided that you have stopped the server, of course).\n\nThe illustration on the previous page puts together databases, schemas, and ta- blespaces. Here the postgres database uses tablespace xyzzy as the default one, whereas the template1 database uses pg_default. Various database objects are shown at the intersections of tablespaces and schemas.\n\nRelations\n\nFor all of their differences, tables and indexes—the most important database objects—have one thing in common: they consist of rows. This point is quite self-evident when we think of tables, but it is equally true for �-tree nodes, which contain indexed values and references to other nodes or table rows.\n\nSome other objects also have the same structure; for example, sequences (virtual- ly one-row tables) and materialized views (which can be thought of as tables that “keep” the corresponding queries). Besides, there are regular views, which do not store any data but otherwise are very similar to tables.\n\nIn Postgre���, all these objects are referred to by the generic term relation.\n\nIn my opinion, it is not a happy term because it confuses database tables with “genuine” relations defined in the relational theory. Here we can feel the academic legacy of the project and the inclination of its founder,Michael Stonebraker,to see everything as a rela- tion. In one of his works,he even introduced the concept of an“ordered relation”to denote a table in which the order of rows is defined by an index.\n\n27\n\nChapter 1 Introduction\n\nThe system catalog table for relations was originally called pg_relation,but following the object orientation trend, it was soon renamed to pg_class, which we are now used to. Its columns still have the ��� prefix though.\n\nFiles and Forks\n\nAll information associated with a relation is stored in several different forks,1 each containing data of a particular type.\n\nAt first, a fork is represented by a single file. Its filename consists of a numeric �� (oid), which can be extended by a suffix that corresponds to the fork’s type.\n\nThe file grows over time, and when its size reaches � ��, another file of this fork is created (such files are sometimes called segments). The sequence number of the segment is added to the end of its filename.\n\nThefilesizelimitof���washistoricallyestablishedtosupportvariousfilesystems that could not handle large files. You can change this limit when building Post- gre��� (./configure --with-segsize).\n\nthe main fork\n\n12345.2\n\n12345.1\n\n12345\n\nfree space map\n\n12345_fsm.1\n\n12345_fsm\n\nvisibilitymap\n\n12345_vm\n\n1 postgresql.org/docs/14/storage-file-layout.html\n\n28\n\n1.1 Data Organization\n\nThus, a single relation is represented on disk by several files. Even a small table without indexes will have at least three files, by the number of mandatory forks.\n\nEach tablespace directory (except for pg_global) contains separate subdirectories for particular databases. All files of the objects belonging to the same tablespace and database are located in the same subdirectory. You must take it into account because too many files in a single directory may not be handled well by file systems.\n\nThere are several standard types of forks.\n\nThe main fork represents actual data: table rows or index rows. This fork is avail-\n\nable for any relations (except for views, which contain no data).\n\nFiles of the main fork are named by their numeric ��s, which are stored as relfilenode values in the pg_class table.\n\nLet’s take a look at the path to a file that belongs to a table created in the pg_default tablespace:\n\n=> CREATE UNLOGGED TABLE t(\n\na integer, b numeric, c text, d json\n\n);\n\n=> INSERT INTO t VALUES (1, 2.0, 'foo', '{}');\n\n=> SELECT pg_relation_filepath('t');\n\npg_relation_filepath −−−−−−−−−−−−−−−−−−−−−−\n\nbase/16384/16385\n\n(1 row)\n\nThe base directory corresponds to the pg_default tablespace, the next sub- directory is used for the database, and it is here that we find the file we are looking for:\n\n=> SELECT oid FROM pg_database WHERE datname = 'internals';\n\noid −−−−−−− 16384 (1 row)\n\n29\n\np. ���\n\nChapter 1 Introduction\n\n=> SELECT relfilenode FROM pg_class WHERE relname = 't';\n\nrelfilenode −−−−−−−−−−−−−\n\n16385\n\n(1 row)\n\nHere is the corresponding file in the file system:\n\n=> SELECT size FROM pg_stat_file('/usr/local/pgsql/data/base/16384/16385');\n\nsize −−−−−− 8192 (1 row)\n\nThe initialization fork1 is available only for unlogged tables (created with the ��- ������ clause) and their indexes. Such objects are the same as regular ones, except that any actions performed on them are not written into the write- ahead log. It makes these operations considerably faster, but you will not be able to restore consistent data in case of a failure. Therefore, Postgre��� sim- ply deletes all forks of such objects during recovery and overwrites the main fork with the initialization fork, thus creating a dummy file.\n\nThe t table is created as unlogged, so the initialization fork is present. It has the same name as the main fork, but with the _init suffix:\n\n=> SELECT size FROM pg_stat_file('/usr/local/pgsql/data/base/16384/16385_init');\n\nsize −−−−−−\n\n0\n\n(1 row)\n\nThe free space map2\n\nIts volume changes all the time, growing after vacuuming and getting smaller when new row versions appear. The free space map is used to quickly find a page that can accommodate new data being inserted.\n\nkeeps track of available space within pages.\n\n1 postgresql.org/docs/14/storage-init.html 2 postgresql.org/docs/14/storage-fsm.html backend/storage/freespace/README\n\n30\n\n1.1 Data Organization\n\nAll files related to the free space map have the _fsm suffix. Initially, no such files are created; they appear only when necessary. The easiest way to get them is to vacuum a table\n\n:\n\n=> VACUUM t;\n\n=> SELECT size FROM pg_stat_file('/usr/local/pgsql/data/base/16384/16385_fsm');\n\nsize −−−−−−− 24576 (1 row)\n\nTo speed up search, the free space map is organized as a tree; it takes at least three pages (hence its file size for an almost empty table).\n\nThe free space map is provided for both tables and indexes. But since an index row cannot be added into an arbitrary page (for example, �-trees define the place of insertion by the sort order), Postgre��� tracks only those pages that have been fully emptied and can be reused in the index structure.\n\nThe visibility map1 can quickly show whether a page needs to be vacuumed or\n\nfrozen. For this purpose, it provides two bits for each table page.\n\nThe first bit is set for pages that contain only up-to-date row versions. Vac- uum skips such pages because there is nothing to clean up. Besides, when a transaction tries to read a row from such a page, there is no point in checking its visibility, so an index-only scan can be used.\n\nThe second bit the term freeze\n\nis set for pages that contain only frozen row versions. I will use map to refer to this part of the fork.\n\nVisibility map files have the _vm suffix. They are usually the smallest ones:\n\n=> SELECT size FROM pg_stat_file('/usr/local/pgsql/data/base/16384/16385_vm');\n\nsize −−−−−− 8192 (1 row)\n\nThe visibility map is provided for tables, but not for indexes.\n\n1 postgresql.org/docs/14/storage-vm.html\n\n31\n\np. ���\n\np. ���\n\np. ���\n\nv. �.� p. ���\n\np. ��\n\np. ��\n\np. ���\n\np. ���\n\nChapter 1 Introduction\n\nPages\n\n(or blocks), which represent To facilitate �/�, all files are logically split into pages the minimum amount of data that can be read or written. Consequently, many internal Postgre��� algorithms are tuned for page processing.\n\nThe page size is usually � k�. It can be configured to some extent (up to �� k�),but only at build time (./configure --with-blocksize), and nobody usually does it. Once built and launched, the instance can work only with pages of the same size; it is impossible to create tablespaces that support different page sizes.\n\nRegardless of the fork they belong to, all the files are handled by the server in roughly the same way. Pages are first moved to the buffer cache (where they can be read and updated by processes) and then flushed back to disk as required.\n\nTOAST\n\nEach row must fit a single page: there is no way to continue a row on the next page. To store long rows,Postgre��� uses a special mechanism called �����1 (The Oversized Attributes Storage Technique).\n\nT���� implies several strategies. You can move long attribute values into a sep- arate service table, having sliced them into smaller “toasts.” Another option is to compress a long value in such a way that the row fits the page. Or you can do both: first compress the value, and then slice and move it.\n\nIf the main table contains potentially long attributes, a separate ����� table is created for it right away, one for all the attributes. For example, if a table has a column of the numeric or text type,a ����� table will be created even if this column will never store any long values.\n\nFor indexes, the ����� mechanism can offer only compression; moving long at- tributes into a separate table is not supported. It limits the size of the keys that can be indexed (the actual implementation depends on a particular operator class\n\n1 postgresql.org/docs/14/storage-toast.html\n\ninclude/access/heaptoast.h\n\n32\n\n).\n\n1.1 Data Organization\n\nBy default, the ����� strategy is selected based on the data type of a column. The easiest way to review the used strategies is to run the \\d+ command in psql, but I will query the system catalog to get an uncluttered output:\n\n=> SELECT attname, atttypid::regtype,\n\nCASE attstorage\n\nWHEN 'p' THEN 'plain' WHEN 'e' THEN 'external' WHEN 'm' THEN 'main' WHEN 'x' THEN 'extended'\n\nEND AS storage FROM pg_attribute WHERE attrelid = 't'::regclass AND attnum > 0;\n\nattname | atttypid | storage −−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−\n\na b c d\n\n| integer | numeric | text | json\n\n| plain | main | extended | extended\n\n(4 rows)\n\nPostgre��� supports the following strategies:\n\nplain means that ����� is not used (this strategy is applied to data types that are\n\nknown to be “short,” such as the integer type).\n\nextended allowsbothcompressingattributesandstoringtheminaseparate�����\n\ntable.\n\nexternal implies that long attributes are stored in the ����� table in an uncom-\n\npressed state.\n\nmain requires long attributes to be compressed first; they will be moved to the\n\n����� table only if compression did not help.\n\nIn general terms, the algorithm looks as follows.1 Postgre��� aims at having at least four rows in a page. So if the size of the row exceeds one fourth of the page, excluding the header (for a standard-size page it is about ���� bytes), we must ap- ply the ����� mechanism to some of the values. Following the workflow described below, we stop as soon as the row length does not exceed the threshold anymore:\n\n1 backend/access/heap/heaptoast.c\n\n33\n\nv. ��\n\nChapter 1 Introduction\n\n1. First of all, we go through attributes with external and extended strategies, starting from the longest ones. Extended attributes get compressed,and if the resulting value (on its own, without taking other attributes into account) ex- ceedsonefourthofthepage,itismovedtothe�����tablerightaway. External attributes are handled in the same way, except that the compression stage is skipped.\n\n2. If the row still does not fit the page after the first pass,we move the remaining attributes that use external or extended strategies into the ����� table,one by one.\n\n3. If it did not help either, we try to compress the attributes that use the main\n\nstrategy, keeping them in the table page.\n\n4. If the row is still not short enough, the main attributes are moved into the\n\n����� table.\n\nThe threshold value the toast_tuple_target storage parameter.\n\nis ���� bytes, but it can be redefined at the table level using\n\nIt may sometimes be useful to change the default strategy for some of the col- umns. If it is known in advance that the data in a particular column cannot be compressed (for example, the column stores ���� images), you can set the external strategy for this column; it allows you to avoid futile attempts to compress the data. The strategy can be changed as follows:\n\n=> ALTER TABLE t ALTER COLUMN d SET STORAGE external;\n\nIf we repeat the query, we will get the following result:\n\nattname | atttypid | storage −−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−\n\na b c d\n\n| integer | numeric | text | json\n\n| plain | main | extended | external\n\n(4 rows)\n\nT���� tables reside in a separate schema called pg_toast; it is not included into the search path, so ����� tables are usually hidden. For temporary tables, pg_toast_temp_N schemas are used, by analogy with pg_temp_N.\n\n34\n\n1.1 Data Organization\n\nLet’s take a look at the inner mechanics of the process. Suppose table t contains three potentially long attributes; it means that there must be a corresponding ����� table. Here it is:\n\n=> SELECT relnamespace::regnamespace, relname FROM pg_class WHERE oid = (\n\nSELECT reltoastrelid FROM pg_class WHERE relname = 't'\n\n);\n\nrelnamespace |\n\nrelname\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\npg_toast\n\n| pg_toast_16385\n\n(1 row)\n\n=> \\d+ pg_toast.pg_toast_16385\n\nTOAST table \"pg_toast.pg_toast_16385\"\n\nColumn\n\n|\n\nType\n\n| Storage\n\n−−−−−−−−−−−−+−−−−−−−−−+−−−−−−−−−\n\nchunk_id chunk_seq chunk_data | bytea\n\n| oid | plain | integer | plain | plain\n\nOwning table: \"public.t\" Indexes:\n\n\"pg_toast_16385_index\" PRIMARY KEY, btree (chunk_id, chunk_seq)\n\nAccess method: heap\n\nIt is only logical that the resulting chunks of the toasted row use the plain strategy: there is no second-level �����.\n\nApart from the ����� table itself, Postgre��� creates the corresponding index in the same schema. This index is always used to access ����� chunks. The name of the index is displayed in the output, but you can also view it by running the following query:\n\n=> SELECT indexrelid::regclass FROM pg_index WHERE indrelid = (\n\nSELECT oid FROM pg_class WHERE relname = 'pg_toast_16385'\n\n);\n\nindexrelid −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− pg_toast.pg_toast_16385_index\n\n(1 row)\n\n35\n\nChapter 1 Introduction\n\n=> \\d pg_toast.pg_toast_16385_index\n\nUnlogged index \"pg_toast.pg_toast_16385_index\"\n\nColumn\n\n|\n\nType\n\n| Key? | Definition\n\n−−−−−−−−−−−+−−−−−−−−−+−−−−−−+−−−−−−−−−−−−\n\nchunk_id | yes | oid chunk_seq | integer | yes\n\n| chunk_id | chunk_seq\n\nprimary key, btree, for table \"pg_toast.pg_toast_16385\"\n\nThus, a ����� table increases the minimum number of fork files used by the table up to eight: three for the main table, three for the ����� table, and two for the ����� index.\n\nColumn c uses the extended strategy, so its values will be compressed:\n\n=> UPDATE t SET c = repeat('A',5000);\n\n=> SELECT * FROM pg_toast.pg_toast_16385;\n\nchunk_id | chunk_seq | chunk_data −−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−− (0 rows)\n\nThe ����� table is empty: repeated symbols have been compressed by the �� al- gorithm, so the value fits the table page.\n\nAnd now let’s construct this value of random symbols:\n\n=> UPDATE t SET c = (\n\nSELECT string_agg( chr(trunc(65+random()*26)::integer), '') FROM generate_series(1,5000)\n\n) RETURNING left(c,10) || '...' || right(c,10);\n\n?column? −−−−−−−−−−−−−−−−−−−−−−−−− YEYNNDTSZR...JPKYUGMLDX\n\n(1 row) UPDATE 1\n\nThis sequence cannot be compressed, so it gets into the ����� table:\n\n=> SELECT chunk_id,\n\nchunk_seq, length(chunk_data), left(encode(chunk_data,'escape')::text, 10) || '...' || right(encode(chunk_data,'escape')::text, 10)\n\nFROM pg_toast.pg_toast_16385;\n\n36\n\n1.2 Processes and Memory\n\nchunk_id | chunk_seq | length |\n\n?column?\n\n−−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−\n\n16390 | 16390 | 16390 |\n\n0 | 1 | 2 |\n\n1996 | YEYNNDTSZR...TXLNDZOXMY 1996 | EWEACUJGZD...GDBWMUWTJY 1008 | GSGDYSWTKF...JPKYUGMLDX\n\n(3 rows)\n\nWe can see that the characters are sliced into chunks. The chunk size is selected in suchawaythatthepageofthe�����tablecanaccommodatefourrows. Thisvalue varies a little from version to version depending on the size of the page header.\n\nWhen a long attribute is accessed, Postgre��� automatically restores the original value and returns it to the client; it all happens seamlessly for the application. If long attributes do not participate in the query, the ����� table will not be read at all. It is one of the reasons why you should avoid using the asterisk in production solutions.\n\nIf required chunks only, even if the value has been compressed.\n\nthe client queries one of the first chunks of a long value,Postgre��� will read the\n\nNevertheless, data compression and slicing require a lot of resources; the same goes for restoring the original values. That’s why it is not a good idea to keep bulky data in Postgre���, especially if this data is being actively used and does not require transactional logic (like scanned accounting documents). Apotentially better alternative is to store such data in the file system, keeping in the database only the names of the corresponding files. But then the database system cannot guarantee data consistency.\n\n1.2 Processes and Memory\n\nA Postgre��� server instance consists of several interacting processes.\n\nThe first process launched at the server start is postgres, which is traditionally called postmaster. It spawns all the other processes (Unix-like systems use the fork system call for this purpose) and supervises them: if any process fails, postmas- ter restarts it (or the whole server if there is a risk that the shared data has been damaged).\n\n37\n\nv. ��\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\nChapter 1 Introduction\n\nBecause of its simplicity, the process model has been used in Postgre��� from the very beginning, and ever since there have been unending discussions about switching over to threads.\n\nThe current model has several drawbacks: static shared memory allocation does not allow resizing structures like buffer cache on the fly; parallel algorithms are hard to imple- ment and less efficient than they could be; sessions are tightly bound to processes. Using threads sounds promising, even though it involves some challenges related to isolation, OS compatibility, and resource management. However, their implementation would re- quire a radical code overhaul and years of work,so conservative views prevail for now: no such changes are expected in the near future.\n\nServer operation is maintained by background processes. Here are the main ones:\n\nstartup restores the system after a failure.\n\nautovacuum removes\n\nstale data from tables and indexes.\n\nwal writer writes ��� entries to disk .\n\ncheckpointer executes checkpoints\n\n.\n\nwriter flushes dirty pages to disk\n\n.\n\nstats collector collects usage statistics for the instance.\n\nwal sender sends ��� entries to a replica.\n\nwal receiver gets ��� entries on a replica.\n\nSome of these processes are terminated once the task is complete, others run in the background all the time, and some can be switched off.\n\nEach process is managed by configuration parameters, sometimes by dozens of them. To set up the server in a comprehensive manner, you have to be aware of its inner workings. But general considerations will only help you select more or less adequate initial values; later on, these settings have to be fine-tuned based on monitoring data.\n\nTo enable process interaction, postmaster allocates shared memory, which is avail- able to all the processes.\n\nSince disks (especially ���, but ��� too) are much slower than ���, Postgre��� some part of the shared ��� is reserved for recently read pages, in uses caching: hope that they will be needed more than once and the overhead of repeated disk\n\n38\n\n1.3 Clients and the Client-Server Protocol\n\naccess will be reduced. Modified data is also flushed to disk after some delay, not immediately.\n\nBuffercachetakesthegreaterpartofthesharedmemory,whichalsocontainsother buffers used by the server to speed up disk access.\n\nThe operating system has its own cache too. Postgre��� (almost) never bypasses the operating system mechanisms to use direct �/�, so it results in double caching.\n\npostmaster\n\nPostgreSQL instance\n\nclient client client application application application\n\nbackend backend backend\n\nbackground processes\n\nshared memory\n\nbuffer cache\n\ncache\n\noperating system\n\nIn case of a failure (such as a power outage or an operating system crash), the data kept in ��� is lost, including that of the buffer cache. The files that remain on disk have their pages written at different points in time. To be able to restore data consistency, Postgre��� maintains the write-ahead log (���) during its operation, which makes it possible to repeat lost operations when necessary.\n\n1.3 Clients and the Client-Server Protocol\n\nAnother task of the postmaster process is to listen for incoming connections. Once a new client appears, postmaster spawns a separate backend process.1 The client\n\n1 backend/tcop/postgres.c, PostgresMain function\n\n39\n\np. ���\n\np. ��� p. ���\n\np. ��\n\nChapter 1 Introduction\n\nestablishes a connection and starts a session with this backend. The session con- tinues until the client disconnects or the connection is lost.\n\nThe server has to spawn a separate backend for each client. If many clients are trying to connect, it can turn out to be a problem.\n\nEach process needs ��� to cache catalog tables, prepared statements , inter- , and other data. The more connections are open, the\n\nmediate query results more memory is required.\n\nIf connections are short and frequent (a client performs a small query and disconnects), the cost of establishing a connection, spawning a new process, and performing pointless local caching is unreasonably high.\n\nThemoreprocessesarestarted,themoretimeisrequiredtoscantheirlist,and As a result, performance may decline this operation is performed very often. as the number of clients grows.\n\nThis problem can be resolved by connection pooling, which limits the number of spawned backends. Postgre��� has no such built-in functionality, so we have to rely on third-party solutions: pooling managers integrated into the application server or external tools (such as PgBouncer1 or Odyssey2). This approach usually means that each server backend can execute transactions of different clients, one after another. It imposes some restrictions on application development since it is only allowed to use resources that are local to a transaction, not to the whole session.\n\nTo understand each other, a client and a server must use one and the same inter- facing protocol.3 It is usually based on the standard libpq library,but there are also other custom implementations.\n\nSpeaking in the most general terms, the protocol allows clients to connect to the server and execute ��� queries.\n\nA connection is always established to a particular database on behalf of a particu- lar role, or user. Although the server supports a database cluster, it is required to establish a separate connection to each database that you would like to use in your\n\n1 pgbouncer.org 2 github.com/yandex/odyssey 3 postgresql.org/docs/14/protocol.html\n\n40\n\n1.3 Clients and the Client-Server Protocol\n\napplication. Atthispoint,authenticationisperformed: thebackendprocessverifies the user’s identity (for example, by asking for the password) and checks whether this user has the right to connect to the server and to the specified database.\n\nS�� queries are passed to the backend process as text strings. The process parses the text, optimizes the query, executes it, and returns the result to the client.\n\n41\n\nPart I\n\nIsolation and MVCC\n\n2\n\nIsolation\n\n2.1 Consistency\n\nThe key feature of relational databases is their ability to ensure data consistency, that is, data correctness.\n\nIt is a known fact that at the database level it is possible to create integrity con- straints, such as ��� ���� or ������. The database system ensures that these con- straints are never broken, so data integrity is never compromised.\n\nIf all the required constraints could be formulated at the database level, consis- tency would be guaranteed. But some conditions are too complex for that, for example, they touch upon several tables at once. And even if a constraint can be defined in the database, but for some reason it is not, it does not mean that this constraint may be violated.\n\nThus, data consistency is stricter than integrity, but the database system has no idea what “consistency” actually means. If an application breaks it without break- ing the integrity,there is no way for the database system to find out. Consequently, it is the application that must lay down the criteria for data consistency, and we have to believe that it is written correctly and will never have any errors.\n\nBut if the application always executes only correct sequences of operators, where does the database system come into play?\n\nFirst of all,a correct sequence of operators can temporarily break data consistency, and—strange as it may seem—it is perfectly normal.\n\nAhackneyedbutclearexampleisatransferoffundsfromoneaccounttoanother. A consistency rule may sound as follows: a money transfer must never change the total\n\n45\n\nChapter 2 Isolation\n\nbalance of the affected accounts. It is quite difficult (although possible) to formulate this rule as an integrity constraint in ���, so let’s assume that it is defined at the applicationlevelandremainsopaquetothedatabasesystem. Atransferconsistsof two operations: the first one draws some money from one of the accounts,whereas the second one adds this sum to another account. The first operation breaks data consistency, whereas the second one restores it.\n\nIf the first operation succeeds, but the second one does not (because of some fail- ure),data consistency will be broken. Such situations are unacceptable,but it takes a great deal of effort to detect and address them at the application level. Luckily it is not required—the problem can be completely solved by the database system itself if it knows that these two operations constitute an indivisible whole, that is, a transaction.\n\nBut there is also a more subtle aspect here. Being absolutely correct on their own, transactions can start operating incorrectly when run in parallel. That’s because operations belonging to different transactions often get intermixed. There would be no such issues if the database system first completed all operations of one trans- actionandthenmovedontothenextone,butperformanceofsequentialexecution would be implausibly low.\n\nA truly simultaneous execution of transactions can only be achieved on systems with suit- able hardware: a multi-core processor, a disk array, and so on. But the same reasoning is also true for a server that executes commands sequentially in the time-sharing mode. For generalization purposes,both these situations are sometimes referred to as concurrent execution.\n\nCorrect transactions that behave incorrectly when run together result in concur- rency anomalies, or phenomena.\n\nHere is a simple example. To get consistent data from the database, the applica- tion must not see any changes made by other uncommitted transactions, at the very minimum. Otherwise (if some transactions are rolled back), it would see the database state that has never existed. Such an anomaly is called a dirty read. There are also many other anomalies, which are more complex.\n\nWhen running transactions concurrently, the database must guarantee that the result of such execution will be the same as the outcome of one of the possible se-\n\n46\n\n2.2 Isolation Levels and Anomalies in SQL Standard\n\nquential executions. In other words, it must isolate transactions from one another, thus taking care of any possible anomalies.\n\nTo sum it up, a transaction is a set of operations that takes the database from one correct state to another correct state (consistency), provided that it is executed in full (atomicity) and without being affected by other transactions (isolation). This definition combines the requirements implied by the first three letters of the ���� acronym. They are so intertwined that it makes sense to discuss them together. In fact, the durability requirement is hardly possible to split off either: after a crash, the system may still contain some changes made by uncommitted transactions, and you have to do something about it to restore data consistency.\n\nThus, the database system helps the application maintain data consistency by tak- ing transaction boundaries into account, even though it has no idea about the im- plied consistency rules.\n\nUnfortunately, full isolation is hard to implement and can negatively affect per- formance. Most real-life systems use weaker isolation levels, which prevent some anomalies, but not all of them. It means that the job of maintaining data consis- tency partially falls on the application. And that’s exactly why it is very important to understand which isolation level is used in the system, what is guaranteed at this level and what is not, and how to ensure that your code will be correct in such conditions.\n\n2.2 Isolation Levels and Anomalies in SQL Standard\n\nThe���standardspecifiesfourisolationlevels.1 Theselevelsaredefinedbythelist of anomalies that may or may not occur during concurrent transaction execution. So when talking about isolation levels, we have to start with anomalies.\n\nWe should bear in mind that the standard is a theoretical construct: it affects the practice, but the practice still diverges from it in lots of ways. That’s why all ex-\n\n1 postgresql.org/docs/14/transaction-iso.html\n\n47\n\np. ���\n\nChapter 2 Isolation\n\namples here are rather hypothetical. Dealing with transactions on bank accounts, these examples are quite self-explanatory,but I have to admit that they have noth- ing to do with real banking operations.\n\nIt is interesting that the actual database theory also diverges from the standard: it was developed after the standard had been adopted, and the practice was already well ahead.\n\nLost Update\n\nThe lost update anomaly occurs when two transactions read one and the same table row,thenoneofthetransactionsupdatesthisrow,andfinallytheothertransaction updates the same row without taking into account any changes made by the first transaction.\n\nSuppose that two transactions are going to increase the balance of one and the same account by $���. The first transaction reads the current value ($�,���), then the second transaction reads the same value. The first transaction increases the balance (making it $�,���) and writes the new value into the database. The second transaction does the same: it gets $�,��� after increasing the balance and writes this value. As a result, the customer loses $���.\n\nLost updates are forbidden by the standard at all isolation levels.\n\nDirty Reads and Read Uncommitted\n\nThe dirty read anomaly occurs when a transaction reads uncommitted changes made by another transaction.\n\nFor example, the first transaction transfers $��� to an empty account but does not commit this change. Another transaction reads the account state (which has been updated but not committed) and allows the customer to withdraw the money— even though the first transaction gets interrupted and its changes are rolled back, so the account is empty.\n\nThe standard allows dirty reads at the Read Uncommitted level.\n\n48\n\n2.2 Isolation Levels and Anomalies in SQL Standard\n\nNon-Repeatable Reads and Read Committed\n\nThe non-repeatable read anomaly occurs when a transaction reads one and the samerowtwice,whereasanothertransactionupdates(ordeletes)thisrowbetween these reads and commitsthe change. As a result,the first transaction gets different results.\n\nFor example, suppose there is a consistency rule that forbids having a negative bal- ance in bank accounts. The first transaction is going to reduce the account balance by $���. It checks the current value, gets $�,���, and decides that this operation is possible. At the same time, another transaction withdraws all the money from this account and commits the changes. If the first transaction checked the bal- ance again at this point, it would get $� (but the decision to withdraw the money is already taken, and this operation causes an overdraft).\n\nThe standard allows non-repeatable reads at the Read Uncommitted and Read Com- mitted levels.\n\nPhantom Reads and Repeatable Read\n\nThephantomread anomalyoccurswhenoneandthesametransactionexecutestwo identical queries returning a set of rows that satisfy a particular condition, while another transaction adds some other rows satisfying this condition and commits the changes in the time interval between these queries. As a result, the first trans- action gets two different sets of rows.\n\nFor example, suppose there is a consistency rule that forbids a customer to have more than three accounts. The first transaction is going to open a new account, so it checks how many accounts are currently available (let’s say there are two of them) and decides that this operation is possible. At this very moment,the second transaction also opens a new account for this client and commits the changes. If the first transaction double-checked the number of open accounts, it would get three (but it is already opening another account, and the client ends up having four of them).\n\nThe standard allows phantom reads at the Read Uncommitted,Read Committed,and Repeatable Read isolation levels.\n\n49",
      "page_number": 25
    },
    {
      "number": 2,
      "title": "Isolation",
      "start_page": 47,
      "end_page": 74,
      "detection_method": "regex_chapter",
      "content": "Chapter 2 Isolation\n\nNo Anomalies and Serializable\n\nThe standard also defines the Serializable level, which does not allow any anoma- lies. It is not the same as the ban on lost updates and dirty, non-repeatable, and phantom reads. In fact, there is a much higher number of known anomalies than the standard specifies, and an unknown number of still unknown ones.\n\nThe Serializable level must prevent any anomalies. It means that the application developer does not have to take isolation into account. If transactions execute correct operator sequences when run on their own, concurrent execution cannot break data consistency either.\n\nTo illustrate this idea, I will use a well-known table provided in the standard; the last column is added here for clarity:\n\nlost update\n\ndirty read\n\nnon-repeatable read\n\nphantom read\n\nother anomalies\n\nRead Uncommitted Read Committed Repeatable Read Serializable\n\n— — — —\n\nyes — — —\n\nyes yes — —\n\nyes yes yes —\n\nyes yes yes —\n\nWhy These Anomalies?\n\nOf all the possible anomalies,why does the standard mentions only some,and why exactly these ones?\n\nNo one seems to know it for sure. But it is not unlikely that other anomalies were simply not considered when the first versions of the standard were adopted, as theory was far behind practice at that time.\n\nBesides, it was assumed that isolation had to be based on locks. The widely used two-phase locking protocol (���) requires transactions to lock the affected rows dur- ing execution and release the locks upon completion. In simplistic terms,the more locks a transaction acquires, the better it is isolated from other transactions. And consequently, the worse is the system performance, as transactions start queuing to get access to the same rows instead of running concurrently.\n\n50\n\n2.3 Isolation Levels in PostgreSQL\n\nI believe that to a great extent the difference between the standard isolation levels is defined by the number of locks required for their implementation.\n\nIf the rows to be updated are locked for writes but not for reads, we get the Read Uncommitted isolation level, which allows reading data before it is committed.\n\nIf the rows to be updated are locked for both reads and writes, we get the Read Committed level: it is forbidden to read uncommitted data, but a query can return different values if it is run more than once (non-repeatable reads).\n\nLocking the rows to be read and to be updated for all operations gives us the Re- peatable Read level: a repeated query will return the same result.\n\nHowever, the Serializable level poses a problem: it is impossible to lock a row that does not exist yet. It leaves an opportunity for phantom reads to occur: a transac- tion can add a row that satisfies the condition of the previous query, and this row will appear in the next query result.\n\nThus,regular locks cannot provide full isolation: to achieve it,we have to lock con- ditions (predicates) rather than rows. Such predicate locks were introduced as early as ���� when System R was being developed; however,their practical applicability islimitedtosimpleconditionsforwhichitisclearwhethertwodifferentpredicates have never may conflict. As far as I know, predicate locks in their intended form been implemented in any system.\n\n2.3 Isolation Levels in PostgreSQL\n\nOver time,lock-based protocols for transaction management got replaced with the Snapshot Isolation (��) protocol. The idea behind this approach is that each trans- action accesses a consistent snapshot of data as it appeared at a particular point in time. The snapshot includes all the current changes committed before the snap- shot was taken.\n\nSnapshot isolation minimizes the number of required locks. In fact, a row will be locked only by concurrent update attempts. In all other cases, operations can be executed concurrently: writes never lock reads, and reads never lock anything.\n\n51\n\np. ���\n\np. ���\n\np. ���\n\np. ��\n\nChapter 2 Isolation\n\nPostgre��� uses a multiversion flavor of the �� protocol. Multiversion concurrency control implies that at any moment the database system can contain several ver- sions of one and the same row, so Postgre��� can include an appropriate version into the snapshot rather than abort transactions that attempt to read stale data.\n\nBased on snapshots, Postgre��� isolation differs from the requirements specified in the standard—in fact, it is even stricter. Dirty reads are forbidden by design. Technically,youcanspecifytheRead Uncommittedlevel,butitsbehaviorwillbethe same as that of Read Committed, so I am not going to mention this level anymore. Repeatable Read allows neither non-repeatable nor phantom reads (even though it does not guarantee full isolation). But in some cases, there is a risk of losing changes at the Read Committed level.\n\nlost updates\n\ndirty reads\n\nnon-repeatable reads\n\nphantom reads\n\nother anomalies\n\nRead Committed Repeatable Read Serializable\n\nyes — —\n\n— — —\n\nyes — —\n\nyes — —\n\nyes yes —\n\nBefore exploring the internal mechanisms of isolation, three isolation levels from the user’s perspective.\n\nlet’s discuss each of the\n\nFor this purpose,we are going to create the accounts table; Alice and Bob will have $�,��� each, but Bob will have two accounts:\n\n=> CREATE TABLE accounts(\n\nid integer PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY, client text, amount numeric\n\n);\n\n=> INSERT INTO accounts VALUES\n\n(1, 'alice', 1000.00), (2, 'bob', 100.00), (3, 'bob', 900.00);\n\nRead Committed\n\nNo dirtyreads. a transaction. By default, it uses the Read Committed1 isolation level:\n\nIt is easy to check that reading dirty data is not allowed. Let’s start\n\n1 postgresql.org/docs/14/transaction-iso.html#XACT-READ-COMMITTED\n\n52\n\n2.3 Isolation Levels in PostgreSQL\n\n=> BEGIN;\n\n=> SHOW transaction_isolation;\n\ntransaction_isolation −−−−−−−−−−−−−−−−−−−−−−−\n\nread committed\n\n(1 row)\n\nTo be more exact, the default level is set by the following parameter, which can be changed as required:\n\n=> SHOW default_transaction_isolation;\n\ndefault_transaction_isolation −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nread committed\n\n(1 row)\n\nTheopenedtransactionwithdrawssomefundsfromthecustomeraccountbutdoes not commit these changes yet. It will see its own changes though, as it is always allowed:\n\n=> UPDATE accounts SET amount = amount - 200 WHERE id = 1;\n\n=> SELECT * FROM accounts WHERE client = 'alice';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n1 | alice\n\n| 800.00\n\n(1 row)\n\nIn the second session, we start another transaction that will also run at the Read Committed level:\n\n=> BEGIN;\n\n=> SELECT * FROM accounts WHERE client = 'alice';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−−\n\n1 | alice\n\n| 1000.00\n\n(1 row)\n\nPredictably, the second transaction does not see any uncommitted changes—dirty reads are forbidden.\n\n53\n\nChapter 2 Isolation\n\nNon-repeatablereads. Nowletthefirsttransactioncommitthechanges. Thenthe second transaction will repeat the same query:\n\n=> COMMIT;\n\n=> SELECT * FROM accounts WHERE client = 'alice';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n1 | alice\n\n| 800.00\n\n(1 row)\n\n=> COMMIT;\n\nThe query receives an updated version of the data—and it is exactly what is under- stood by the non-repeatable read anomaly, which is allowed at the Read Committed level.\n\nA practical insight: in a transaction, you must not take any decisions based on the data read by the previous operator, as everything can change in between. Here is an example whose variations appear in the application code so often that it can be considered a classic anti-pattern:\n\nIF (SELECT amount FROM accounts WHERE id = 1) >= 1000 THEN\n\nUPDATE accounts SET amount = amount - 1000 WHERE id = 1;\n\nEND IF;\n\nDuring the time that passes between the check and the update, other transactions can freely change the state of the account, so such a “check” is absolutely useless. For better understanding,you can imagine that random operators of other transac- tions are“wedged”between the operators of the current transaction. For example, like this:\n\nIF (SELECT amount FROM accounts WHERE id = 1) >= 1000 THEN\n\nUPDATE accounts SET amount = amount - 200 WHERE id = 1;\n\nCOMMIT;\n\nUPDATE accounts SET amount = amount - 1000 WHERE id = 1;\n\nEND IF;\n\n54\n\n2.3 Isolation Levels in PostgreSQL\n\nIf everything goes wrong as soon as the operators are rearranged, then the code is incorrect. Do not delude yourself that you will never get into this trouble: any- thing that can go wrong will go wrong. Such errors are very hard to reproduce,and consequently, fixing them is a real challenge.\n\nHow can you correct this code? There are several options:\n\nReplace procedural code with declarative one.\n\nFor example, in this particular case it is easy to turn an �� statement into a ����� constraint:\n\nALTER TABLE accounts\n\nADD CHECK amount >= 0;\n\nNow you do not need any checks in the code: it is enough to simply run the command and handle the exception that will be raised if an integrity con- straint violation is attempted.\n\nUse a single ��� operator.\n\nData consistency can be compromised if a transaction gets committed within the time gap between operators of another transaction, thus changing data visibility. If there is only one operator, there are no such gaps.\n\nPostgre��� has enough capabilities to solve complex tasks with a single ��� statement. In particular, it offers common table expressions (���) that can contain operators like ������, ������, ������, as well as the ������ �� �������� operator that implements the following logic: insert the row if it does not exist, otherwise perform an update.\n\nApply explicit locks.\n\nThe last resort is to manually set an exclusive lock on all the required rows (������ ��� ������) or even on the whole table (���� �����) . This approach always works, but it nullifies all the advantages of ����: some operations that could be executed concurrently will run sequentially.\n\n55\n\np. ��� p. ���\n\nChapter 2 Isolation\n\nRead skew. However, it is not all that simple. The Postgre��� implementation allows other, less known anomalies, which are not regulated by the standard.\n\nSuppose the first transaction has started a money transfer between Bob’s accounts:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount - 100 WHERE id = 2;\n\nMeanwhile, the other transaction starts looping through all Bob’s accounts to cal- culate their total balance. It begins with the first account (seeing its previous state, of course):\n\n=> BEGIN;\n\n=> SELECT amount FROM accounts WHERE id = 2;\n\namount −−−−−−−− 100.00 (1 row)\n\nAt this moment, the first transaction completes successfully:\n\n=> UPDATE accounts SET amount = amount + 100 WHERE id = 3;\n\n=> COMMIT;\n\nThe second transaction reads the state of the second account (and sees the already updated value):\n\n=> SELECT amount FROM accounts WHERE id = 3;\n\namount −−−−−−−−− 1000.00\n\n(1 row)\n\n=> COMMIT;\n\nAs a result, the second transaction gets $�,��� because it has read incorrect data. Such an anomaly is called read skew.\n\nHow can you avoid this anomaly at the Read Committed level? The answer is obvi- ous: use a single operator. For example, like this:\n\nSELECT sum(amount) FROM accounts WHERE client = 'bob';\n\n56\n\n2.3 Isolation Levels in PostgreSQL\n\nI have been stating so far that data visibility can change only between operators, but is it really so? What if the query is running for a long time? Can it see different parts of data in different states in this case?\n\nLet’s check it out. A convenient way to do it is to add a delay to an operator by callingthepg_sleepfunction. Thenthefirstrowwillbereadatonce,butthesecond row will have to wait for two seconds:\n\n=> SELECT amount, pg_sleep(2) -- two seconds FROM accounts WHERE client = 'bob';\n\nWhile this statement is being executed, let’s start another transaction to transfer the money back:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount + 100 WHERE id = 2;\n\n=> UPDATE accounts SET amount = amount - 100 WHERE id = 3;\n\n=> COMMIT;\n\nThe result shows that the operator has seen all the data in the state that corre- sponds to the beginning of its execution, which is certainly correct:\n\namount\n\n| pg_sleep\n\n−−−−−−−−−+−−−−−−−−−−\n\n0.00 | 1000.00 |\n\n(2 rows)\n\nBut it is not all that simple either. If the query contains a function that is de- clared ��������, and this function executes another query, then the data seen by this nested query will not be consistent with the result of the main query.\n\nLet’s check the balance in Bob’s accounts using the following function:\n\n=> CREATE FUNCTION get_amount(id integer) RETURNS numeric AS $$\n\nSELECT amount FROM accounts a WHERE a.id = get_amount.id;\n\n$$ VOLATILE LANGUAGE sql;\n\n=> SELECT get_amount(id), pg_sleep(2) FROM accounts WHERE client = 'bob';\n\nWe will transfer the money between the accounts once again while our delayed query is being executed:\n\n57\n\nChapter 2 Isolation\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount + 100 WHERE id = 2;\n\n=> UPDATE accounts SET amount = amount - 100 WHERE id = 3;\n\n=> COMMIT;\n\nIn this case, we are going to get inconsistent data—$��� has been lost:\n\nget_amount | pg_sleep −−−−−−−−−−−−+−−−−−−−−−−\n\n100.00 | 800.00 |\n\n(2 rows)\n\nI would like to emphasize that this effect is possible only at the Read Committed isolation level, and only if the function is ��������. The trouble is that Postgre��� uses exactly this isolation level and this volatility category by default. So we have to admit that the trap is set in a very cunning way.\n\nRead skew instead of lost updates. The read skew anomaly can also occur within a single operator during an update—even though in a somewhat unexpected way.\n\nLet’s see what happens if two transactions try to modify one and the same row. Bob currently has a total of $�,��� in two accounts:\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n2 | bob 3 | bob\n\n| 200.00 | 800.00\n\n(2 rows)\n\nStart a transaction that will reduce Bob’s balance:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount - 100 WHERE id = 3;\n\nAt the same time, the other transaction will be calculating the interest for all cus- tomer accounts with the total balance of $�,��� or more:\n\n58\n\n2.3 Isolation Levels in PostgreSQL\n\n=> UPDATE accounts SET amount = amount * 1.01 WHERE client IN ( SELECT client FROM accounts GROUP BY client HAVING sum(amount) >= 1000\n\n);\n\nThe ������ operator execution virtually consists of two stages. First,the rows to be updated are selected based on the provided condition. Since the first transaction is not committed yet, the second transaction cannot see its result, so the selection of rows picked for interest accrual is not affected. Thus, Bob’s accounts satisfy the condition, and his balance must be increased by $�� once the ������ operation completes.\n\nAt the second stage, the selected rows are updated one by one. The second trans- action has to wait because the row with id = 3 is locked: it is being updated by the first transaction.\n\nMeanwhile, the first transaction commits its changes:\n\n=> COMMIT;\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client |\n\namount\n\n−−−−+−−−−−−−−+−−−−−−−−−−\n\n2 | bob 3 | bob\n\n| 202.0000 | 707.0000\n\n(2 rows)\n\nOn the one hand,the ������ command must not see any changes made by the first transaction. But on the other hand, it must not lose any committed changes.\n\nOnce the lock is released, the ������ operator re-reads the row to be updated (but only this row!). As a result, Bob gets $� of interest, based on the total of $���. But if he had $���, his accounts should not have been included into the query results in the first place.\n\nThus, our transaction has returned incorrect data: different rows have been read from different snapshots. Instead of a lost update, we observe the read skew anomaly again.\n\n59\n\np. ���\n\np. ��\n\nChapter 2 Isolation\n\nLost updates. However,the trick of re-reading the locked row will not help against lost updates if the data is modified by different ��� operators.\n\nHere is an example that we have already seen. (outside of the database) the current balance of Alice’s account:\n\nThe application reads and registers\n\n=> BEGIN;\n\n=> SELECT amount FROM accounts WHERE id = 1;\n\namount −−−−−−−− 800.00 (1 row)\n\nMeanwhile, the other transaction does the same:\n\n=> BEGIN;\n\n=> SELECT amount FROM accounts WHERE id = 1;\n\namount −−−−−−−− 800.00 (1 row)\n\nThefirsttransactionincreasesthepreviouslyregisteredvalueby$���andcommits this change:\n\n=> UPDATE accounts SET amount = 800.00 + 100 WHERE id = 1 RETURNING amount;\n\namount −−−−−−−− 900.00 (1 row) UPDATE 1\n\n=> COMMIT;\n\nThe second transaction does the same:\n\n=> UPDATE accounts SET amount = 800.00 + 100 WHERE id = 1 RETURNING amount;\n\namount −−−−−−−− 900.00 (1 row) UPDATE 1\n\n60\n\n2.3 Isolation Levels in PostgreSQL\n\n=> COMMIT;\n\nUnfortunately, Alice has lost $���. The database system does not know that the registered value of $��� is somehow related to accounts.amount, so it cannot pre- vent the lost update anomaly. At the Read Committed isolation level, this code is incorrect.\n\nRepeatable Read\n\nNo non-repeatable and phantom reads. As its name suggests,the Repeatable Read1 isolation level must guarantee repeatable reading. Let’s check it and make sure that phantom reads cannot occur either. For this purpose, we are going to start a transaction that will revert Bob’s accounts to their previous state and create a new account for Charlie:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = 200.00 WHERE id = 2;\n\n=> UPDATE accounts SET amount = 800.00 WHERE id = 3;\n\n=> INSERT INTO accounts VALUES\n\n(4, 'charlie', 100.00);\n\n=> SELECT * FROM accounts ORDER BY id;\n\nid | client\n\n| amount\n\n−−−−+−−−−−−−−−+−−−−−−−−\n\n| 900.00 1 | alice | 200.00 2 | bob 3 | bob | 800.00 4 | charlie | 100.00\n\n(4 rows)\n\nInthesecondsession,let’sstartanothertransaction,withtheRepeatableReadlevel explicitly specified in the ����� command (the level of the first transaction is not important):\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT * FROM accounts ORDER BY id;\n\n1 postgresql.org/docs/14/transaction-iso.html#XACT-REPEATABLE-READ\n\n61\n\np. ��\n\nChapter 2 Isolation\n\nid | client |\n\namount\n\n−−−−+−−−−−−−−+−−−−−−−−−−\n\n1 | alice 2 | bob 3 | bob\n\n| 900.00 | 202.0000 | 707.0000\n\n(3 rows)\n\nNow the first transaction commits its changes, and the second transaction repeats the same query:\n\n=> COMMIT;\n\n=> SELECT * FROM accounts ORDER BY id;\n\nid | client |\n\namount\n\n−−−−+−−−−−−−−+−−−−−−−−−−\n\n1 | alice 2 | bob 3 | bob\n\n| 900.00 | 202.0000 | 707.0000\n\n(3 rows)\n\n=> COMMIT;\n\nThesecondtransactionstillseesthesamedataasbefore: neithernewrowsnorrow updates are visible. At this isolation level,you do not have to worry that something will change between operators.\n\nSerialization failures instead of lost updates. As we have already seen ,if two trans- actions update one and the same row at the Read Committed level, it can cause the read skew anomaly: the waiting transaction has to re-read the locked row, so it sees the state of this row at a different point in time as compared to other rows.\n\nSuch an anomaly is not allowed at the Repeatable Read isolation level,and if it does happen,thetransactioncanonlybeabortedwithaserializationfailure. Let’scheck it out by repeating the scenario with interest accrual:\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n2 | bob 3 | bob\n\n| 200.00 | 800.00\n\n(2 rows)\n\n=> BEGIN;\n\n62\n\n2.3 Isolation Levels in PostgreSQL\n\n=> UPDATE accounts SET amount = amount - 100.00 WHERE id = 3;\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> UPDATE accounts SET amount = amount * 1.01 WHERE client IN ( SELECT client FROM accounts GROUP BY client HAVING sum(amount) >= 1000\n\n);\n\n=> COMMIT;\n\nERROR:\n\ncould not serialize access due to concurrent update\n\n=> ROLLBACK;\n\nThe data remains consistent:\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n2 | bob 3 | bob\n\n| 200.00 | 700.00\n\n(2 rows)\n\nThe same error will be raised by any concurrent row updates, even if they affect different columns.\n\nWe will also get this error if we try to update the balance based on the previously stored value:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT amount FROM accounts WHERE id = 1;\n\namount −−−−−−−− 900.00 (1 row)\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n63\n\nChapter 2 Isolation\n\n=> SELECT amount FROM accounts WHERE id = 1;\n\namount −−−−−−−− 900.00 (1 row)\n\n=> UPDATE accounts SET amount = 900.00 + 100.00 WHERE id = 1 RETURNING amount;\n\namount −−−−−−−−− 1000.00\n\n(1 row) UPDATE 1\n\n=> COMMIT;\n\n=> UPDATE accounts SET amount = 900.00 + 100.00 WHERE id = 1 RETURNING amount;\n\nERROR:\n\ncould not serialize access due to concurrent update\n\n=> ROLLBACK;\n\nA practical insight: if your application is using the Repeatable Read isolation level for write transactions, it must be ready to retry transactions that have been com- pleted with a serialization failure. For read-only transactions, such an outcome is impossible.\n\nWrite skew. As we have seen, the Postgre��� implementation of the Repeatable Read isolation level prevents all the anomalies described in the standard. But not all possible ones: no one knows how many of them exist. However, one important fact is proved for sure: snapshot isolation does not prevent only two anomalies, no matter how many other anomalies are out there.\n\nThe first one is write skew.\n\nLet’s define the following consistency rule: it is allowed to have a negative balance in some of the customer’s accounts as long as the total balance is non-negative.\n\nThe first transaction gets the total balance of Bob’s accounts:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n64\n\n2.3 Isolation Levels in PostgreSQL\n\n=> SELECT sum(amount) FROM accounts WHERE client = 'bob';\n\nsum −−−−−−−− 900.00 (1 row)\n\nThe second transaction gets the same sum:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT sum(amount) FROM accounts WHERE client = 'bob';\n\nsum −−−−−−−− 900.00 (1 row)\n\nThe first transaction fairly assumes that it can debit one of the accounts by $���:\n\n=> UPDATE accounts SET amount = amount - 600.00 WHERE id = 2;\n\nThe second transaction comes to the same conclusion, but debits the other ac- count:\n\n=> UPDATE accounts SET amount = amount - 600.00 WHERE id = 3;\n\n=> COMMIT;\n\n=> COMMIT;\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−−\n\n2 | bob 3 | bob\n\n| −400.00 100.00 |\n\n(2 rows)\n\nBob’s total balance is now negative, although both transactions would have been correct if run separately.\n\nRead-only transaction anomaly. The read-only transaction anomaly is the second and the last one allowed at the Repeatable Read isolation level. To observe this anomaly, we have to run three transactions: two of them are going to update the data, while the third one will be read-only.\n\n65\n\nChapter 2 Isolation\n\nBut first let’s restore Bob’s balance:\n\n=> UPDATE accounts SET amount = 900.00 WHERE id = 2;\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n3 | bob 2 | bob\n\n| 100.00 | 900.00\n\n(2 rows)\n\nThe first transaction calculates the interest to be accrued on Bob’s total balance and adds this sum to one of his accounts:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ; -- 1\n\n=> UPDATE accounts SET amount = amount + (\n\nSELECT sum(amount) FROM accounts WHERE client = 'bob'\n\n) * 0.01 WHERE id = 2;\n\nThen the second transaction withdraws some money from Bob’s other account and commits this change:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ; -- 2\n\n=> UPDATE accounts SET amount = amount - 100.00 WHERE id = 3;\n\n=> COMMIT;\n\nIf the first transaction gets committed at this point,there will be no anomalies: we couldassumethatthefirsttransactioniscommittedbeforethesecondone(butnot vice versa—the first transaction had seen the state of account with id = 3 before any updates were made by the second transaction).\n\nBut let’s imagine that at this very moment we start a ready-only transaction to query an account that is not affected by the first two transactions:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ; -- 3\n\n=> SELECT * FROM accounts WHERE client = 'alice';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−−\n\n1 | alice\n\n| 1000.00\n\n(1 row)\n\nAnd only now will the first transaction get committed:\n\n66\n\n2.3 Isolation Levels in PostgreSQL\n\n=> COMMIT;\n\nWhich state should the third transaction see at this point? Having started,it could see the changes made by the second transaction (which had already been commit- ted), but not by the first one (which had not been committed yet). But as we have already established, the second transaction should be treated as if it were started afterthefirstone. Anystateseenbythethirdtransactionwillbeinconsistent—this is exactly what is meant by the read-only transaction anomaly:\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n2 | bob 3 | bob\n\n| 900.00 0.00 |\n\n(2 rows)\n\n=> COMMIT;\n\nSerializable\n\nThe Serializable1 isolation level prevents all possible anomalies. This level is vir- tually built on top of snapshot isolation. Those anomalies that do not occur at the Repeatable Read isolation level (such as dirty, non-repeatable, or phantom reads) cannot occur at the Serializable level either. And those two anomalies that do occur (write skew and read-only transaction anomalies) get detected in a special way to abort the transaction, causing an already familiar serialization failure.\n\nNo anomalies. Let’s make sure that our write skew scenario with a serialization failure:\n\nwill eventually end\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE;\n\n=> SELECT sum(amount) FROM accounts WHERE client = 'bob';\n\nsum −−−−−−−−−− 910.0000\n\n(1 row)\n\n1 postgresql.org/docs/14/transaction-iso.html#XACT-SERIALIZABLE\n\n67\n\np. ��\n\nChapter 2 Isolation\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE;\n\n=> SELECT sum(amount) FROM accounts WHERE client = 'bob';\n\nsum −−−−−−−−−− 910.0000\n\n(1 row)\n\n=> UPDATE accounts SET amount = amount - 600.00 WHERE id = 2;\n\n=> UPDATE accounts SET amount = amount - 600.00 WHERE id = 3;\n\n=> COMMIT;\n\nCOMMIT\n\n=> COMMIT;\n\nERROR: among transactions DETAIL: commit attempt. HINT:\n\ncould not serialize access due to read/write dependencies\n\nReason code: Canceled on identification as a pivot, during\n\nThe transaction might succeed if retried.\n\nThe scenario with the read-only transaction anomaly will lead to the same error.\n\nDeferring a read-only transaction. To avoid situations when a read-only transac- tion can cause an anomaly that compromises data consistency, Postgre��� offers an interesting solution: this transaction can be deferred until its execution be- comes safe. It is the only case when a ������ statement can be blocked by row updates.\n\nWe are going to check it out by repeating the scenario that demonstrated the read- only transaction anomaly:\n\n=> UPDATE accounts SET amount = 900.00 WHERE id = 2;\n\n=> UPDATE accounts SET amount = 100.00 WHERE id = 3;\n\n=> SELECT * FROM accounts WHERE client = 'bob' ORDER BY id;\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n2 | bob 3 | bob\n\n| 900.00 | 100.00\n\n(2 rows)\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE; -- 1\n\n68\n\n2.3 Isolation Levels in PostgreSQL\n\n=> UPDATE accounts SET amount = amount + (\n\nSELECT sum(amount) FROM accounts WHERE client = 'bob'\n\n) * 0.01 WHERE id = 2;\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE; -- 2\n\n=> UPDATE accounts SET amount = amount - 100.00 WHERE id = 3;\n\n=> COMMIT;\n\nLet’s explicitly declare the third transaction as ���� ���� and ����������:\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE READ ONLY DEFERRABLE; -- 3\n\n=> SELECT * FROM accounts WHERE client = 'alice';\n\nAn attempt to run the query blocks the transaction—otherwise, it would have caused an anomaly.\n\nAnd only when the first transaction is committed, the third one can continue its execution:\n\n=> COMMIT;\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−−\n\n1 | alice\n\n| 1000.00\n\n(1 row)\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client |\n\namount\n\n−−−−+−−−−−−−−+−−−−−−−−−−\n\n2 | bob 3 | bob\n\n| 910.0000 0.00 |\n\n(2 rows)\n\n=> COMMIT;\n\nThus,if an application uses the Serializable isolation level,it must be ready to retry transactions that have ended with a serialization failure. (The Repeatable Read level requires the same approach unless the application is limited to read-only transactions.)\n\nThe Serializable isolation level brings ease of programming, but the price you pay is the overhead incurred by anomaly detection and forced termination of a certain\n\n69\n\np. ���\n\nread committed\n\nv. ��\n\nChapter 2 Isolation\n\nfraction of transactions. You can lower this impact by explicitly using the ���� ���� clause when declaring read-only transactions. But the main questions is, of course, how big the fraction of aborted transactions is—since these transactions will have to be retried. It would have been not so bad if Postgre��� aborted only those transactions that result in data conflicts and are really incompatible. But such an approach would inevitably be too resource-intensive, as it would involve tracking operations on each row.\n\nThe current implementation allows false positives: Postgre��� can abort some ab- solutely safe transactions that are simply out of luck. Their “luck” depends on many factors, such as the presence of appropriate indexes or the amount of ��� available, so the actual behavior is hard to predict in advance.\n\nIf you use the Serializable level, it must be observed by all transactions of the ap- plication. When combined with other levels, Serializable behaves as Repeatable Read without any notice. So if you decide to use the Serializable level, it makes default_transaction_isolation parameter value accordingly— sense to modify the even though someone can still overwrite it by explicitly setting a different level.\n\nfor example, queries run at the Serializable level There are also other restrictions; cannot be executed on replicas. And although the functionality of this level is constantly being improved, the current limitations and overhead make it less at- tractive.\n\n2.4 Which Isolation Level to Use?\n\nRead CommittedisthedefaultisolationlevelinPostgre���,andapparentlyitisthis level that is used in the vast majority of applications. This level can be convenient because it allows aborting transactions only in case of a failure; it does not abort anytransactionstopreservedataconsistency. Inotherwords,serializationfailures cannot occur, so you do not have to take care of transaction retries.\n\nThe downside of this level is a large number of possible anomalies, which have been discussed in detail above. A developer has to keep them in mind all the time and write the code in a way that prevents their occurrence. If it is impossible to define all the needed actions in a single ��� statement, then you have to resort to explicit locking. The toughest part is that the code is hard to test for errors\n\n70\n\n2.4 Which Isolation Level to Use?\n\nrelated to data inconsistency; such errors can appear in unpredictable and barely reproducible ways, so they are very hard to fix too.\n\nThe Repeatable Read isolation level eliminates some of the inconsistency prob- lems, but alas, not all of them. Therefore, you must not only remember about the remaining anomalies,but also modify the application to correctly handle serializa- tion failures, which is certainly inconvenient. However, for read-only transactions this level is a perfect complement to the Read Committed level; it can be very useful for cases like building reports that involve multiple ��� queries.\n\nAnd finally, the Serializable isolation level allows you not to worry about data con- sistency at all, which simplifies writing the code to a great extent. The only thing required from the application is the ability to retry any transaction that is aborted with a serialization failure. However, the number of aborted transactions and as- sociated overhead can significantly reduce system throughput. You should also keep in mind that the Serializable level is not supported on replicas and cannot be combined with other isolation levels.\n\n71\n\np. ���\n\n3\n\nPages and Tuples\n\n3.1 Page Structure\n\nEach page has a certain inner layout that usually consists of the following parts:1\n\npage header\n\nan array of item pointers\n\nfree space\n\nitems (row versions)\n\nspecial space\n\nPage Header\n\nThe page header is located in the lowest addresses and has a fixed size. It stores various information about the page , such as its checksum and the sizes of all the other parts of the page.\n\nThese sizes can be easily displayed using the pageinspect extension.2 Let’s take a look at the first page of the table (page numbering is zero-based):\n\n1 postgresql.org/docs/14/storage-page-layout.html\n\ninclude/storage/bufpage.h\n\n2 postgresql.org/docs/14/pageinspect.html\n\n72\n\n3.1 Page Structure\n\n=> CREATE EXTENSION pageinspect;\n\n=> SELECT lower, upper, special, pagesize FROM page_header(get_raw_page('accounts',0));\n\nlower | upper | special | pagesize −−−−−−−+−−−−−−−+−−−−−−−−−+−−−−−−−−−−\n\n152 |\n\n6904 |\n\n8192 |\n\n8192\n\n(1 row)\n\n0\n\nheader\n\n24\n\nan array of item pointers\n\nlower\n\nfree space\n\nupper\n\nitems\n\nspecial\n\nspecial space\n\npagesize\n\nSpecial Space\n\nThe special space is located in the opposite part of the page, taking its highest ad- dresses. It is used by some indexes to store auxiliary information; in other indexes and table pages this space is zero-sized.\n\nIn general,the layout of index pages is quite diverse; their content largely depends on a particular index type. Even one and the same index can have different kinds of pages: for example, �-trees have a metadata page of a special structure (page zero) and regular pages that are very similar to table pages.\n\nTuples\n\nRows contain the actual data stored in the database,together with some additional information. They are located just before the special space.\n\n73\n\np. ��\n\nChapter 3 Pages and Tuples\n\nIn the case of tables, we have to deal with row versions rather than rows because multiversion concurrency control implies having several versions of one and the same row. Indexes do not use this ���� mechanism; instead, they have to ref- erence all the available row versions, falling back on visibility rules to select the appropriate ones.\n\nBoth table row versions and index entries are often referred to as tuples. This term is borrowed from the relational theory—it is yet another legacy of Postgre���’s academic past.\n\nItem Pointers\n\nThe array of pointers to tuples serves as the page’s table of contents. It is located right after the header.\n\nIndex entries have to refer to particular heap tuples somehow. Postgre��� em- ploys six-byte tuple identifiers (���s) for this purpose. Each ��� consists of the page and a reference to a particular row version located in this number of the main fork page.\n\nIn theory, tuples could be referred to by their offset from the start of the page. But then it would be impossible to move tuples within pages without breaking these references, which in turn would lead to page fragmentation and other unpleasant consequences.\n\nFor this reason,Postgre��� uses indirect addressing: a tuple identifier refers to the corresponding pointer number, and this pointer specifies the current offset of the tuple. If the tuple is moved within the page, its ��� still remains the same; it is enough to modify the pointer, which is also located in this page.\n\nEach pointer takes exactly four bytes and contains the following data:\n\ntuple offset from the start of the page\n\ntuple length\n\nseveral bits defining the tuple status\n\n74\n\n3.2 Row Version Layout\n\nFree Space\n\nPages can have some free space left between pointers and tuples (which is reflected in the free space map ). There is no page fragmentation: all the free space available is always aggregated into one chunk.1\n\n3.2 Row Version Layout\n\nEach row version contains a header followed by actual data. The header consists of multiple fields, including the following:\n\nxmin,xmax represent transaction ��s; they are used to differentiate between this\n\nand other versions of one and the same row.\n\ninfomask provides a set of information bits that define version properties.\n\nctid is a pointer to the next updated version of the same row.\n\nnull bitmap is an array of bits marking the columns that can contain ���� values.\n\nAs a result, the header turns out quite big: it requires at least �� bytes for each tu- ple,and this value is often exceeded because of the null bitmap and the mandatory padding used for data alignment. In a “narrow” table, the size of various metadata can easily beat the size of the actual data stored.\n\nDatalayoutondiskfullycoincideswithdatarepresentationin���. Thepagealong with its tuples is read into the buffer cache as is, without any transformations. That’s why data files are incompatible between different platforms.2\n\nOne of the sources of incompatibility is the byte order. For example, the x�� ar- chitecture is little-endian, z/�rchitecture is big-endian, and ��� has configurable byte order.\n\nAnother reason is data alignment by machine word boundaries, which is required by many architectures. For example, in a ��-bit x�� system, integer numbers (the integer type, takes four bytes) are aligned by the boundary of four-byte words,\n\n1 backend/storage/page/bufpage.c, PageRepairFragmentation function 2 include/access/htup_details.h\n\n75\n\np. ��\n\nChapter 3 Pages and Tuples\n\njust like double-precision floating-point numbers (the double precision type, eight bytes). But in a ��-bit system, double values are aligned by the boundary of eight- byte words.\n\nData alignment makes the size of a tuple dependent on the order of fields in the table. This effect is usually negligible,but in some cases it can lead to a significant size increase. Here is an example:\n\n=> CREATE TABLE padding(\n\nb1 boolean, i1 integer, b2 boolean, i2 integer\n\n);\n\n=> INSERT INTO padding VALUES (true,1,false,2);\n\n=> SELECT lp_len FROM heap_page_items(get_raw_page('padding', 0));\n\nlp_len −−−−−−−−\n\n40 (1 row)\n\nI have used the heap_page_items function of the pageinspect extension to display some details about pointers and tuples.\n\nIn Postgre���, tables are often referred to as heap. This is yet another obscure term that hints at the similarity between space allocation for tuples and dynamic memory alloca- tion. Some analogy can certainly be seen,but tables are managed by completely different algorithms. We can interpret this term in the sense that“everything is piled up into a heap,” by contrast with ordered indexes.\n\nThe size of the row is �� bytes. Its header takes �� bytes, a column of the integer type takes � bytes, and boolean columns take � byte each. It makes �� bytes, and � bytes are wasted on four-byte alignment of integer columns.\n\nIf we rebuild the table, the space will be used more efficiently:\n\n=> DROP TABLE padding;\n\n=> CREATE TABLE padding(\n\ni1 integer, i2 integer, b1 boolean, b2 boolean\n\n);\n\n76\n\n3.3 Operations on Tuples\n\n=> INSERT INTO padding VALUES (1,2,true,false);\n\n=> SELECT lp_len FROM heap_page_items(get_raw_page('padding', 0));\n\nlp_len −−−−−−−−\n\n34 (1 row)\n\nAnother possible micro-optimization is to start the table with the fixed-length columns that cannot contain ���� values. Access to such columns will be more efficient because it is possible to cache their offset within the tuple.1\n\n3.3 Operations on Tuples\n\nTo identify different versions of one and the same row, Postgre��� marks each of them with two values: xmin and xmax. These values define “validity time” of each rowversion,butinsteadoftheactualtime,theyrelyonever-increasingtransaction ��s.\n\nWhen a row is created, its xmin value is set to the transaction �� of the ������ com- mand.\n\nWhenarowisdeleted,thexmaxvalueofitscurrentversionissettothetransaction �� of the ������ command.\n\nWith a certain degree of abstraction, the ������ command can be regarded as two separate operations: ������ and ������. First, the xmax value of the current row version is set to the transaction �� of the ������ command. Then a new ver- sion of this row is created; its xmin value will be the same as the xmax value of the previous version.\n\nNow let’s get down to some low-level details of different operations on tuples.2\n\nFor these experiments, we will need a two-column table with an index created on one of the columns:\n\n1 backend/access/common/heaptuple.c, heap_deform_tuple function 2 backend/access/transam/README\n\n77\n\np. ���",
      "page_number": 47
    },
    {
      "number": 3,
      "title": "Pages and Tuples",
      "start_page": 75,
      "end_page": 96,
      "detection_method": "regex_chapter",
      "content": "Chapter 3 Pages and Tuples\n\n=> CREATE TABLE t(\n\nid integer GENERATED ALWAYS AS IDENTITY, s text\n\n);\n\n=> CREATE INDEX ON t(s);\n\nInsert\n\nStart a transaction and insert one row:\n\n=> BEGIN;\n\n=> INSERT INTO t(s) VALUES ('FOO');\n\nHere is the current transaction ��:\n\n=> -- txid_current() before v.13 SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n776\n\n(1 row)\n\nTo denote the concept of a transaction,Postgre��� uses the term xact,which can be found both in ��� function names and in the source code. Consequently, a transaction �� can be called xact ��, ����, or simply ���. We are going to come across these abbreviations over and over again.\n\nLet’s take a look at the page contents. The heap_page_items function can give us all the required information, but it shows the data “as is,” so the output format is a bit hard to comprehend:\n\n=> SELECT * FROM heap_page_items(get_raw_page('t',0)) \\gx\n\n−[ RECORD 1 ]−−−−−−−−−−−−−−−−−−− lp lp_off lp_flags lp_len t_xmin t_xmax t_field3 t_ctid\n\n| 1 | 8160 | 1 | 32 | 776 | 0 | 0 | (0,1)\n\n78\n\n3.3 Operations on Tuples\n\nt_infomask2 | 2 t_infomask t_hoff t_bits t_oid t_data\n\n| 2050 | 24 | | | \\x0100000009464f4f\n\nTo make it more readable, we can leave out some information and expand a few columns:\n\n=> SELECT '(0,'||lp||')' AS ctid,\n\nCASE lp_flags\n\nWHEN 0 THEN 'unused' WHEN 1 THEN 'normal' WHEN 2 THEN 'redirect to '||lp_off WHEN 3 THEN 'dead'\n\nEND AS state, t_xmin as xmin, t_xmax as xmax, (t_infomask & 256) > 0 (t_infomask & 512) > 0 (t_infomask & 1024) > 0 AS xmax_committed, (t_infomask & 2048) > 0 AS xmax_aborted\n\nAS xmin_committed, AS xmin_aborted,\n\nFROM heap_page_items(get_raw_page('t',0)) \\gx\n\n−[ RECORD 1 ]−−+−−−−−−− | (0,1) ctid | normal state | 776 xmin xmax | 0 xmin_committed | f xmin_aborted | f xmax_committed | f | t xmax_aborted\n\nThis is what has been done here:\n\nThe lp pointer is converted to the standard format of a tuple ��: (page number,\n\npointer number).\n\nThelp_flagsstateisspelledout. Hereitissettothenormalvalue,whichmeans\n\nthat it really points to a tuple.\n\nOf all the information bits, we have singled out just two pairs so far. The xmin_committed and xmin_aborted bits show whether the xmin transaction is\n\n79\n\nv. ��\n\nChapter 3 Pages and Tuples\n\ncommitted or aborted. The xmax_committed and xmax_aborted bits give simi- lar information about the xmax transaction.\n\nextension provides the heap_tuple_infomask_flags function that explains The pageinspect all the information bits, but I am going to retrieve only those that are required at the moment, showing them in a more concise form.\n\nLet’s get back to our experiment. The ������ command has added pointer � to the heap page; it refers to the first tuple, which is currently the only one.\n\nThe xmin field of the tuple is set to the current transaction ��. This transaction is still active, so the xmin_committed and xmin_aborted bits are not set yet.\n\nThe xmax field contains �, which is a dummy number showing that this tuple has not been deleted and represents the current version of the row. Transactions will ignore this number because the xmax_aborted bit is set.\n\nIt may seem strange that the bit corresponding to an aborted transaction is set for the transaction that has not happened yet. But there is no difference between such transac- tions from the isolation standpoint: an aborted transaction leaves no trace, hence it has never existed.\n\nWe will use this query more than once,so I am going to wrap it into a function. And\n\nwhilebeingatit,Iwillalsomaketheoutputmoreconcisebyhidingtheinformation bit columns and displaying the status of transactions together with their ��s.\n\n=> CREATE FUNCTION heap_page(relname text, pageno integer) RETURNS TABLE(ctid tid, state text, xmin text, xmax text) AS $$ SELECT (pageno,lp)::text::tid AS ctid,\n\nCASE lp_flags\n\nWHEN 0 THEN 'unused' WHEN 1 THEN 'normal' WHEN 2 THEN 'redirect to '||lp_off WHEN 3 THEN 'dead'\n\nEND AS state, t_xmin || CASE\n\nWHEN (t_infomask & 256) > 0 THEN ' c' WHEN (t_infomask & 512) > 0 THEN ' a' ELSE '' END AS xmin,\n\n80\n\n3.3 Operations on Tuples\n\nt_xmax || CASE\n\nWHEN (t_infomask & 1024) > 0 THEN ' c' WHEN (t_infomask & 2048) > 0 THEN ' a' ELSE '' END AS xmax\n\nFROM heap_page_items(get_raw_page(relname,pageno)) ORDER BY lp; $$ LANGUAGE sql;\n\nNow it is much clearer what is happening in the tuple header:\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin | xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−+−−−−−−\n\n(0,1) | normal | 776\n\n| 0 a\n\n(1 row)\n\nYou can get similar but less detailed information from the table itself by querying the xmin and xmax pseudocolumns:\n\n=> SELECT xmin, xmax, * FROM t;\n\nxmin | xmax | id |\n\ns\n\n−−−−−−+−−−−−−+−−−−+−−−−− 0 |\n\n776 | (1 row)\n\n1 | FOO\n\nCommit\n\nOnce a transaction has been completed successfully, its status has to be stored somehow—it must be registered that the transaction is committed. For this pur- pose, Postgre��� employs a special ���� (commit log) structure.1 It is stored as files in the ������/pg_xact directory rather than as a system catalog table.\n\nPreviously, these files were located in ������/pg_clog, but in version �� this directory got renamed:2 it was not uncommon for database administrators unfamiliar with Postgre��� to delete it in search of free disk space, thinking that a “log” is something unnecessary.\n\n1 include/access/clog.h\n\nbackend/access/transam/clog.c 2 commitfest.postgresql.org/13/750\n\n81\n\np. ���\n\nChapter 3 Pages and Tuples\n\nC��� is split into several files solely for convenience. by page via buffers in the server’s shared memory.1\n\nThese files are accessed page\n\nJust like a tuple header, ���� contains two bits for each transaction: committed and aborted.\n\nOnce committed, a transaction is marked in ���� with the committed bit. When any other transaction accesses a heap page, it has to answer the question: has the xmin transaction already finished?\n\nIf not, then the created tuple must not be visible.\n\nTo check whether the transaction is still active, Postgre��� uses yet another structure located in the shared memory of the instance; it is called ProcArray. This structure contains the list of all the active processes,with the correspond- ing current (active) transaction specified for each process.\n\nIf yes,was it committed or aborted? In the latter case,the corresponding tuple\n\ncannot be visible either.\n\nIt is this check that requires ����. But even though the most recent ���� pages are stored in memory buffers, it is still expensive to perform this check every time. Once determined, the transaction status is written into the tuple header—morespecifically,intoxmin_committedandxmin_abortedinformation bits, which are also called hint bits. If one of these bits is set, then the xmin transaction status is considered to be already known,and the next transaction will have to access neither ���� nor ProcArray.\n\nWhy aren’t these bits set by the transaction that performs row insertion? The prob- lem is that it is not known yet at that time whether this transaction will complete successfully. Andwhenitiscommitted,itisalreadyunclearwhichtuplesandpages have been changed. If a transaction affects many pages,it may be too expensive to trackthem. Besides,someofthesepagesmaybenotinthecacheanymore; reading them again to simply update the hint bits would seriously slow down the commit.\n\n1 backend/access/transam/clog.c\n\n82\n\n3.3 Operations on Tuples\n\nThe flip side of this cost reduction is that any transaction (even a read-only ������ command) can start setting hint bits, thus leaving a trail of dirtied pages in the buffer cache.\n\nFinally, let’s commit the transaction started with the ������ statement:\n\n=> COMMIT;\n\nNothing has changed in the page (but we know that the transaction status has al- ready been written into ����):\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin | xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−+−−−−−−\n\n(0,1) | normal | 776\n\n| 0 a\n\n(1 row)\n\nNowthefirsttransactionthataccessesthepage(ina“standard”way,withoutusing pageinspect) has to determine the status of the xmin transaction and update the hint bits:\n\n=> SELECT * FROM t;\n\nid |\n\ns\n\n−−−−+−−−−− 1 | FOO\n\n(1 row)\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−− (0,1) | normal | 776 c | 0 a\n\n(1 row)\n\nDelete\n\nWhen a row is deleted,the xmax field of its current version is set to the transaction �� that performs the deletion, and the xmax_aborted bit is unset.\n\n83\n\np. ���\n\nChapter 3 Pages and Tuples\n\nWhile this transaction is active,the xmax value serves as a row lock. If another transaction is going to update or delete this row, it will have to wait until the xmax transaction is complete.\n\nLet’s delete a row:\n\n=> BEGIN;\n\n=> DELETE FROM t;\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n777\n\n(1 row)\n\nThe transaction �� has already been written into the xmax field, but the informa- tion bits have not been set yet:\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−− (0,1) | normal | 776 c | 777\n\n(1 row)\n\nAbort\n\nThe mechanism of aborting a transaction is similar to that of commit and happens just as fast, but instead of committed it sets the aborted bit in ����. Although the corresponding command is called ��������, no actual data rollback is happening: all the changes made by the aborted transaction in data pages remain in place.\n\n=> ROLLBACK;\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−− (0,1) | normal | 776 c | 777\n\n(1 row)\n\n84\n\n3.3 Operations on Tuples\n\nWhenthepageisaccessed,thetransactionstatusischecked,andthetuplereceives the xmax_aborted hint bit. The xmax number itself still remains in the page, but no one is going to pay attention to it anymore:\n\n=> SELECT * FROM t;\n\nid |\n\ns\n\n−−−−+−−−−− 1 | FOO\n\n(1 row)\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−− (0,1) | normal | 776 c | 777 a\n\n(1 row)\n\nUpdate\n\nAn update is performed in such a way as if the current tuple is deleted, and then a new one is inserted:\n\n=> BEGIN;\n\n=> UPDATE t SET s = 'BAR';\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n778\n\n(1 row)\n\nThe query returns a single row (its new version):\n\n=> SELECT * FROM t;\n\nid |\n\ns\n\n−−−−+−−−−− 1 | BAR\n\n(1 row)\n\n85\n\np. ���\n\nChapter 3 Pages and Tuples\n\nBut the page keeps both versions:\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−− (0,1) | normal | 776 c | 778 | 0 a (0,2) | normal | 778\n\n(2 rows)\n\nThe xmax field of the previously deleted version contains the current transaction ��. This value is written on top of the old one because the previous transaction was aborted. The xmax_aborted bit is unset since the status of the current transaction is still unknown.\n\nTo complete this experiment, let’s commit the transaction.\n\n=> COMMIT;\n\n3.4 Indexes\n\nRegardlessoftheirtype,indexesdonotuserowversioning; eachrowisrepresented by exactly one tuple. In other words, index row headers do not contain xmin and xmax fields. Index entries point to all the versions of the corresponding table row . To figure out which row version is visible, transactions have to access the table (unless the required page appears in the visibility map).\n\nFor convenience, let’s create a simple function that will use pageinspect to display all the index entries in the page (�-tree index pages store them as a flat list):\n\n=> CREATE FUNCTION index_page(relname text, pageno integer) RETURNS TABLE(itemoffset smallint, htid tid) AS $$ SELECT itemoffset,\n\nhtid -- ctid before v.13\n\nFROM bt_page_items(relname,pageno); $$ LANGUAGE sql;\n\nThe page references both heap tuples, the current and the previous one:\n\n86\n\n3.5 TOAST\n\n=> SELECT * FROM index_page('t_s_idx',1);\n\nitemoffset | htid −−−−−−−−−−−−+−−−−−−−\n\n1 | (0,2) 2 | (0,1)\n\n(2 rows)\n\nSince ��� < ���, the pointer to the second tuple comes first in the index.\n\n3.5 TOAST\n\nA ����� table is virtually a regular table, and it has its own versioning that does not depend on row versions of the main table. However, rows of ����� tables are handled in such a way that they are never updated; they can be either added or deleted, so their versioning is somewhat artificial.\n\nEachdatamodificationresultsincreationofanewtupleinthemaintable. Butifan updatedoesnotaffectanylongvaluesstoredin�����,thenewtuplewillreference an existing toasted value. Only when a long value gets updated will Postgre��� create both a new tuple in the main table and new “toasts.”\n\n3.6 Virtual Transactions\n\nTo consume transaction ��s sparingly, Postgre��� offers a special optimization.\n\nIf a transaction is read-only,it does not affect row visibility in any way. That’s why such a transaction is given a virtual ���1 at first, which consists of the backend process �� and a sequential number. Assigning a virtual ��� does not require any synchronization between different processes,so it happens very fast. At this point, the transaction has no real �� yet:\n\n=> BEGIN;\n\n1 backend/access/transam/xact.c\n\n87\n\np. ��\n\np. ���\n\nChapter 3 Pages and Tuples\n\n=> -- txid_current_if_assigned() before v.13 SELECT pg_current_xact_id_if_assigned();\n\npg_current_xact_id_if_assigned −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n(1 row)\n\nAt different points in time, the system can contain some virtual ���s that have already been used. And it is perfectly normal: virtual ���s exist only in ���, and only while the corresponding transactions are active; they are never written into data pages and never get to disk.\n\nOnce the transaction starts modifying data, it receives an actual unique ��:\n\n=> UPDATE accounts SET amount = amount - 1.00;\n\n=> SELECT pg_current_xact_id_if_assigned();\n\npg_current_xact_id_if_assigned −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n780\n\n(1 row)\n\n=> COMMIT;\n\n3.7 Subtransactions\n\nSavepoints\n\nS�� supports savepoints, which enable canceling some of the operations within a transaction without aborting this transaction as a whole. But such a scenario does not fit the course of action described above: the status of a transaction applies to all its operations, and no physical data rollback is performed.\n\nTo implement this functionality, a transaction containing a savepoint is split into several subtransactions,1 so their status can be managed separately.\n\n1 backend/access/transam/subtrans.c\n\n88\n\n3.7 Subtransactions\n\nSubtransactions have their own ��s (which are bigger than the �� of the main trans- action). The status of a subtransaction is written into ���� in the usual manner; however, committed subtransactions receive both the committed and the aborted bits at once. The final decision depends on the status of the main transaction: if it is aborted, all its subtransactions will be considered aborted too.\n\nThe information about subtransactions is stored under the ������/pg_subtrans di- rectory. File access is arranged via buffers that are located in the instance’s shared memory and have the same structure as ���� buffers.1\n\nDo not confuse subtransactions with autonomous ones. Unlike subtransactions, the latter do not depend on each other in any way. Vanilla Postgre��� does not support autonomous transactions, and it is probably for the best: they are required in very rare cases, but their availability in other database systems often provokes misuse, which can cause a lot of trouble.\n\nLet’s truncate the table, start a new transaction, and insert a row:\n\n=> TRUNCATE TABLE t;\n\n=> BEGIN;\n\n=> INSERT INTO t(s) VALUES ('FOO');\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n782\n\n(1 row)\n\nNow create a savepoint and insert another row:\n\n=> SAVEPOINT sp;\n\n=> INSERT INTO t(s) VALUES ('XYZ');\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n782\n\n(1 row)\n\nNote that the pg_current_xact_id function returns the �� of the main transaction, not that of a subtransaction.\n\n1 backend/access/transam/slru.c\n\n89\n\nChapter 3 Pages and Tuples\n\n=> SELECT * FROM heap_page('t',0) p\n\nLEFT JOIN t ON p.ctid = t.ctid;\n\nctid\n\n| state\n\n| xmin | xmax | id |\n\ns\n\n−−−−−−−+−−−−−−−−+−−−−−−+−−−−−−+−−−−+−−−−−\n\n(0,1) | normal | 782 (0,2) | normal | 783\n\n| 0 a | 0 a\n\n| |\n\n2 | FOO 3 | XYZ\n\n(2 rows)\n\nLet’s roll back to the savepoint and insert the third row:\n\n=> ROLLBACK TO sp;\n\n=> INSERT INTO t(s) VALUES ('BAR');\n\n=> SELECT * FROM heap_page('t',0) p\n\nLEFT JOIN t ON p.ctid = t.ctid;\n\nctid\n\n| state\n\n| xmin | xmax | id |\n\ns\n\n−−−−−−−+−−−−−−−−+−−−−−−+−−−−−−+−−−−+−−−−−\n\n(0,1) | normal | 782 (0,2) | normal | 783 (0,3) | normal | 784\n\n| 0 a | 0 a | 0 a\n\n| | |\n\n2 | FOO\n\n|\n\n4 | BAR\n\n(3 rows)\n\nThe page still contains the row added by the aborted subtransaction.\n\nCommit the changes:\n\n=> COMMIT;\n\n=> SELECT * FROM t;\n\nid |\n\ns\n\n−−−−+−−−−− 2 | FOO 4 | BAR\n\n(2 rows)\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−− (0,1) | normal | 782 c | 0 a (0,2) | normal | 783 a | 0 a (0,3) | normal | 784 c | 0 a\n\n(3 rows)\n\n90\n\n3.7 Subtransactions\n\nNow we can clearly see that each subtransaction has its own status.\n\nS�� does not allow using subtransactions directly, that is, you cannot start a new transaction before completing the current one:\n\n=> BEGIN;\n\nBEGIN\n\n=> BEGIN;\n\nWARNING: BEGIN\n\nthere is already a transaction in progress\n\n=> COMMIT;\n\nCOMMIT\n\n=> COMMIT;\n\nWARNING: COMMIT\n\nthere is no transaction in progress\n\nSubtransactions are employed implicitly: to implement savepoints, handle excep- tions in ��/pg���, and in some other, more exotic cases.\n\nErrors and Atomicity\n\nWhat happens if an error occurs during execution of a statement?\n\n=> BEGIN;\n\n=> SELECT * FROM t;\n\nid |\n\ns\n\n−−−−+−−−−− 2 | FOO 4 | BAR\n\n(2 rows)\n\n=> UPDATE t SET s = repeat('X', 1/(id-4));\n\nERROR:\n\ndivision by zero\n\nAfter a failure, the whole transaction is considered aborted and cannot perform any further operations:\n\n=> SELECT * FROM t;\n\nERROR: of transaction block\n\ncurrent transaction is aborted, commands ignored until end\n\n91\n\nChapter 3 Pages and Tuples\n\nAnd even if you try to commit the changes, Postgre��� will report that the trans- action is rolled back:\n\n=> COMMIT;\n\nROLLBACK\n\nWhy is it forbidden to continue transaction execution after a failure? Since the already executed operations are never rolled back, we would get access to some changes made before the error—it would break the atomicity of the statement,and hence that of the transaction itself.\n\nFor example,in our experiment the operator has managed to update one of the two rows before the failure:\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−− (0,1) | normal | 782 c | 785 (0,2) | normal | 783 a | 0 a (0,3) | normal | 784 c | 0 a | 0 a (0,4) | normal | 785\n\n(4 rows)\n\nOn a side note, psql provides a special mode that allows you to continue a transac- tion after a failure as if the erroneous statement were rolled back:\n\n=> \\set ON_ERROR_ROLLBACK on\n\n=> BEGIN;\n\n=> UPDATE t SET s = repeat('X', 1/(id-4));\n\nERROR:\n\ndivision by zero\n\n=> SELECT * FROM t;\n\nid |\n\ns\n\n−−−−+−−−−− 2 | FOO 4 | BAR\n\n(2 rows)\n\n=> COMMIT;\n\nCOMMIT\n\n92\n\n3.7 Subtransactions\n\nAs you can guess, psql simply adds an implicit savepoint before each command when run in this mode; in case of a failure, a rollback is initiated. This mode is not used by default because issuing savepoints (even if they are not rolled back to) incurs significant overhead.\n\n93\n\np. ��\n\n4\n\nSnapshots\n\n4.1 What is a Snapshot?\n\nA data page can contain several versions of one and the same row, although each transaction must see only one of them at the most. Together, visible versions of all the different rows constitute a snapshot . A snapshot includes only the current data committed by the time it was taken, thus providing a consistent (in the ���� sense) view of the data for this particular moment.\n\nToensureisolation,eachtransactionusesitsownsnapshot. Itmeansthatdifferent transactionscanseedifferentsnapshotstakenatdifferentpointsintime,whichare nevertheless consistent.\n\nAt the Read Committed isolation level, a snapshot is taken at the beginning of each statement, and it remains active only for the duration of this statement.\n\nAt the Repeatable Read and Serializable levels, a snapshot is taken at the begin- ning of the first statement of a transaction, and it remains active until the whole transaction is complete.\n\nsnapshot1\n\nsnapshot2\n\nsnapshot\n\nstatement1\n\nstatement2\n\nstatement1\n\nstatement2\n\nxid\n\nRead Committed\n\nRepeatable Read, Serializable\n\n94\n\nxid\n\n4.2 Row Version Visibility\n\n4.2 Row Version Visibility\n\nA snapshot is not a physical copy of all the required tuples. Instead, it is defined by several numbers, while tuple visibility is determined by certain rules.\n\nTuple visibility is defined by xmin and xmax fields of the tuple header (that is, ��s of transactions that perform insertion and deletion) and the corresponding hint bits. Since xmin–xmax intervals do not intersect, each row is represented in any snapshot by only one of its versions.\n\nThe exact visibility rules are quite complex,1 as they take into account a variety of differentscenariosandcornercases. Veryroughly,wecandescribethemasfollows: a tuple is visible in a snapshot that includes xmin transaction changes but excludes xmax transaction changes (in other words, the tuple has already appeared and has not been deleted yet).\n\nIn their turn, transaction changes are visible in a snapshot if this transaction was committed before the snapshot creation. As an exception, transactions can see their own uncommitted changes. If a transaction is aborted, its changes will not be visible in any snapshot.\n\nLet’s take a look at a simple example. In this illustration line segments represent transactions (from their start time till commit time):\n\nsnapshot\n\nxid\n\n1\n\n2\n\n3\n\nHere visibility rules are applied to transactions as follows:\n\nTransaction � was committed before the snapshot creation, so its changes are\n\nvisible.\n\n1 backend/access/heap/heapam_visibility.c\n\n95\n\noff\n\np. ���\n\nChapter 4 Snapshots\n\nTransaction � was active at the time of the snapshot creation, so its changes\n\nare not visible.\n\nTransaction � was started after the snapshot creation, so its changes are not visible either (it makes no difference whether this transaction is completed or not).\n\n4.3 Snapshot Structure\n\nUnfortunately,the previous illustration has nothing to do with the way Postgre��� actually sees this picture.1 The problem is that the system does not know when transactions got committed. It is only known when they were started (this moment isdefinedbythetransaction ��),whiletheircompletionisnotregisteredanywhere.\n\nCommit times can be tracked2 if you enable the track_commit_timestamp parameter, but they do not participate in visibility checks in any way (although it can still be useful to track them for other purposes, for example, to apply in external replication solutions).\n\nBesides, Postgre��� always logs commit and rollback times in the corresponding ��� en- tries\n\n, but this information is used only for point-in-time recovery.\n\nIt is only the current status of a transaction that we can learn. This information is available in the server’s shared memory: the ProcArray structure contains the list of all the active sessions and their transactions. Once a transaction is complete, it is impossible to find out whether it was active at the time of the snapshot creation.\n\nSo to create a snapshot,it is not enough to register the moment when it was taken: it is also necessary to collect the status of all the transactions at that moment. Otherwise, later it will be impossible to understand which tuples must be visible in the snapshot, and which must be excluded.\n\nTakealookattheinformationavailabletothesystemwhenthesnapshotwastaken and some time afterwards (the white circle denotes an active transaction, whereas the black circles stand for completed ones):\n\n1 include/utils/snapshot.h\n\nbackend/utils/time/snapmgr.c\n\n2 backend/access/transam/commit_ts.c\n\n96\n\n4.3 Snapshot Structure\n\nxid\n\nxid\n\n1\n\n2\n\n3\n\n1\n\n2\n\n3\n\nat snapshot creation…\n\n…and some time later\n\nSuppose we did not know that at the time the snapshot was taken the first transac- tion was still being executed and the third transaction had not started yet. Then it would seem that they were just like the second transaction (which was committed at that time), and it would be impossible to filter them out.\n\nFor this reason,Postgre��� cannot create a snapshot that shows a consistent state ofdataatsomearbitrarypointinthepast,evenifalltherequiredtuplesarepresent in heap pages. Consequently, it is impossible to implement retrospective queries (which are sometimes also called temporal or flashback queries).\n\nIntriguingly, such functionality was declared as one of the objectives of Postgres and was implemented at the very start, but it was removed from the database system when the project support was passed on to the community.1\n\nThus, a snapshot consists of several values saved at the time of its creation:2\n\nxmin is the snapshot’s lower boundary,which is represented by the �� of the oldest\n\nactive transaction.\n\nAll the transactions with smaller ��s are included into the snapshot) or aborted (so their changes are ignored).\n\nare either committed (so their changes\n\nxmax is the snapshot’s upper boundary, which is represented by the value that exceedsthe��ofthelatestcommittedtransactionbyone. Theupperboundary defines the moment when the snapshot was taken.\n\nAll the transactions whose ��s are equal to or greater than xmax are either still running or do not exist, so their changes cannot be visible.\n\nxip_list\n\nis the list of ��s of all the active transactions except for virtual ones,which\n\ndo not affect visibility in any way.\n\n1 Joseph M. Hellerstein, Looking Back at Postgres. https://arxiv.org/pdf/1901.01973.pdf 2 backend/storage/ipc/procarray.c, GetSnapshotData function\n\n97\n\np. ���\n\np. ��\n\nChapter 4 Snapshots\n\nSnapshots also include several other parameters, but we will ignore them for now.\n\nIn a graphical form, a snapshot can be represented as a rectangle that comprises transactions from xmin to xmax:\n\nxip_list\n\nxmin\n\nxmax\n\nxid\n\n1\n\n2\n\n3\n\nTo understand how visibility rules are defined by the snapshot, we are going to reproduce the above scenario on the accounts table.\n\n=> TRUNCATE TABLE accounts;\n\nThe first transaction inserts the first row into the table and remains open:\n\n=> BEGIN;\n\n=> INSERT INTO accounts VALUES (1, 'alice', 1000.00);\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n790\n\n(1 row)\n\nThe second transaction inserts the second row and commits this change immedi- ately:\n\n=> BEGIN;\n\n=> INSERT INTO accounts VALUES (2, 'bob', 100.00);\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n791\n\n(1 row)\n\n=> COMMIT;\n\n98\n\n4.3 Snapshot Structure\n\nAt this point, let’s create a new snapshot in another session. We could simply run any query for this purpose, but we will use a special function to take a look at this snapshot right away:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> -- txid_current_snapshot() before v.13 SELECT pg_current_snapshot();\n\npg_current_snapshot −−−−−−−−−−−−−−−−−−−−−\n\n790:792:790\n\n(1 row)\n\nThis function displays the following snapshot components, separated by colons: xmin, xmax, and xip_list (the list of active transactions; in this particular case it consists of a single item).\n\nOnce the snapshot is taken, commit the first transaction:\n\n=> COMMIT;\n\nThe third transaction is started after the snapshot creation. It modifies the second row, so a new tuple appears:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount + 100 WHERE id = 2;\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n792\n\n(1 row)\n\n=> COMMIT;\n\nOur snapshot sees only one tuple:\n\n=> SELECT ctid, * FROM accounts;\n\nctid\n\n| id | client | amount\n\n−−−−−−−+−−−−+−−−−−−−−+−−−−−−−−\n\n(0,2) |\n\n2 | bob\n\n| 100.00\n\n(1 row)\n\n99",
      "page_number": 75
    },
    {
      "number": 4,
      "title": "Snapshots",
      "start_page": 97,
      "end_page": 110,
      "detection_method": "regex_chapter",
      "content": "Chapter 4 Snapshots\n\nBut the table contains three of them:\n\n=> SELECT * FROM heap_page('accounts',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−\n\n(0,1) | normal | 790 c | 0 a (0,2) | normal | 791 c | 792 c (0,3) | normal | 792 c | 0 a\n\n(3 rows)\n\nSo how does Postgre��� choose which versions to show? By the above rules, changes are included into a snapshot only if they are made by committed trans- actions that satisfy the following criteria:\n\nIf xid < xmin,changes are shown unconditionally (like in the case of the trans-\n\naction that created the accounts table).\n\nIfxmin ⩽ xid < xmax,changesareshownonlyifthecorrespondingtransaction\n\n��s are not in xip_list.\n\nThe first row (�,�) is invisible because it is inserted by a transaction that appears in xip_list (even though this transaction falls into the snapshot range).\n\nThe latest version of the second row (�,�) is invisible because the corresponding transaction �� is above the upper boundary of the snapshot.\n\nBut the first version of the second row (�,�) is visible: row insertion was performed by a transaction that falls into the snapshot range and does not appear in xip_list (the insertion is visible),while row deletion was performed by a transaction whose �� is above the upper boundary of the snapshot (the deletion is invisible).\n\n=> COMMIT;\n\n4.4 Visibility of Transactions’Own Changes\n\nThings get a bit more complicated when it comes to defining visibility rules for transactions’ own changes: in some cases, only part of such changes must be vis- ible. For example, a cursor that was opened at a particular point in time must not see any changes that happened later, regardless of the isolation level.\n\n100\n\n4.4 Visibility of Transactions’ Own Changes\n\nTo address such situations, tuple headers provide a special field (displayed as cmin and cmax pseudocolumns) that shows the sequence number of the operation within the transaction. The cmin column identifies insertion, while cmax is used for deletion operations. To save space, these values are stored in a single field of the tuple header rather than in two different ones. It is assumed that one and the same row almost never gets both inserted and deleted within a single transaction. (If it does happen, Postgre��� writes a special combo identifier into this field, and the actual cmin and cmax values are stored by the backend in this case.1)\n\nAs an illustration, let’s start a transaction and insert a row into the table:\n\n=> BEGIN;\n\n=> INSERT INTO accounts VALUES (3, 'charlie', 100.00);\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n793\n\n(1 row)\n\nOpen a cursor to run the query that returns the number of rows in this table:\n\n=> DECLARE c CURSOR FOR SELECT count(*) FROM accounts;\n\nInsert one more row:\n\n=> INSERT INTO accounts VALUES (4, 'charlie', 200.00);\n\nNow extend the output by another column to display the cmin value for the rows inserted by our transaction (it makes no sense for other rows):\n\n=> SELECT xmin, CASE WHEN xmin = 793 THEN cmin END cmin, * FROM accounts;\n\nxmin | cmin | id | client\n\n| amount\n\n−−−−−−+−−−−−−+−−−−+−−−−−−−−−+−−−−−−−−−\n\n790 | 792 | 793 | 793 | (4 rows)\n\n| | 0 | 1 |\n\n| 1000.00 1 | alice 200.00 | 2 | bob 100.00 3 | charlie | 200.00 4 | charlie |\n\n1 backend/utils/time/combocid.c\n\n101\n\nChapter 4 Snapshots\n\nThe cursor query gets only three rows; the row inserted when the cursor was al- ready open does not make it into the snapshot because the cmin < 1 condition is not satisfied:\n\n=> FETCH c;\n\ncount −−−−−−−\n\n3\n\n(1 row)\n\nNaturally, this cmin number is also stored in the snapshot, but it is impossible to display it using any ��� means.\n\n4.5 Transaction Horizon\n\nAs mentioned earlier, the lower boundary of the snapshot is represented by xmin, which is the �� of the oldest transaction that was active at the moment of the snap- shot creation. This value is very important because it defines the horizon of the transaction that uses this snapshot.\n\nIf a transaction has no active snapshot (for example, at the Read Committed isola- tion level between statement execution), its horizon is defined by its own �� if it is assigned.\n\nAll the transactions that are beyond the horizon (those with xid < xmin) are gu- ranteed to be committed. It means that a transaction can see only the current row versions beyond its horizon.\n\nAs you can guess, this term is inspired by the concept of event horizon in physics.\n\nPostgre��� tracks the current horizons of all its processes; transactions can see their own horizons in the pg_stat_activity table:\n\n=> BEGIN;\n\n=> SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid();\n\nbackend_xmin −−−−−−−−−−−−−−\n\n793\n\n(1 row)\n\n102\n\n4.5 Transaction Horizon\n\nVirtual transactions have no real ��s, but they still use snapshots just like regular transactions, so they have their own horizons. The only exception is virtual trans- actions without an active snapshot: the concept of the horizon makes no sense for them, and they are fully “transparent” to the system when it comes to snapshots and visibility (even though pg_stat_activity.backend_xmin may still contain an xmin of an old snapshot).\n\nWe can also define the database horizon in a similar manner. For this purpose, we should take the horizons of all the transactions in this database and select the most remote one, which has the oldest xmin.1 Beyond this horizon, outdated heap tuples will never be visible to any transaction in this database. Such tuples can be safely cleaned up by vacuum—this is exactly why the concept of the horizon is so important from a practical standpoint.\n\ndatabase horizon\n\noutdated tuples that can be vacuumed\n\nxid\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\nLet’s draw some conclusions:\n\nIf a transaction (no matter whether it is real or virtual) at the Repeatable Read or Serializable isolation level is running for a long time, it thereby holds the database horizon and defers vacuuming.\n\n1 backend/storage/ipc/procarray.c, ComputeXidHorizons function\n\n103\n\nChapter 4 Snapshots\n\nA real transaction at the Read Committed isolation level holds the database horizon in the same way,even if it is not executing any operators (being in the “idle in transaction” state).\n\nA virtual transaction at the Read Committed isolation level holds the horizon\n\nonly while executing operators.\n\nThere is only one horizon for the whole database, so if it is being held by a trans- action, it is impossible to vacuum any data within this horizon—even if this data has not been accessed by this transaction.\n\nCluster-wide tables of the system catalog have a separate horizon that takes into account all transactions in all databases. Temporary tables, on the contrary, do not have to pay attention to any transactions except those that are being executed by the current process.\n\nLet’s get back to our current experiment. The active transaction of the first session still holds the database horizon; we can see it by incrementing the transaction counter:\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n794\n\n(1 row)\n\n=> SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid();\n\nbackend_xmin −−−−−−−−−−−−−−\n\n793\n\n(1 row)\n\nAnd only when this transaction is complete, the horizon moves forward, and out- dated tuples can be vacuumed:\n\n=> COMMIT;\n\n=> SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid();\n\nbackend_xmin −−−−−−−−−−−−−−\n\n795\n\n(1 row)\n\n104\n\n4.6 System Catalog Snapshots\n\nIn a perfect world, you should avoid combining long transactions with frequent updates (that spawn new row versions), as it will lead to table and index bloating.\n\n4.6 System Catalog Snapshots\n\nAlthough the system catalog consists of regular tables, they cannot be accessed via a snapshot used by a transaction or an operator. The snapshot must be “fresh” enoughtoincludeallthelatestchanges,otherwisetransactionscouldseeoutdated definitions of table columns or miss newly added integrity constraints.\n\nHere is a simple example:\n\n=> BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT 1; -- a snapshot for the transaction is taken\n\n=> ALTER TABLE accounts\n\nALTER amount SET NOT NULL;\n\n=> INSERT INTO accounts(client, amount)\n\nVALUES ('alice', NULL);\n\nERROR: violates not−null constraint DETAIL:\n\nnull value in column \"amount\" of relation \"accounts\"\n\nFailing row contains (1, alice, null).\n\n=> ROLLBACK;\n\nThe integrity constraint that appeared after the snapshot creation was visible to the ������ command. It may seem that such behavior breaks isolation, but if the inserting transaction had managed to reach the accounts table before the ����� ��- ��� command,the latter would have been blocked until this transaction completed.\n\nIn general, the server behaves as if a separate snapshot is created for each system catalog query. But the implementation is, of course, much more complex1 since frequent snapshot creation would negatively affect performance; besides, many system catalog objects get cached, and it must also be taken into account.\n\n1 backend/utils/time/snapmgr.c, GetCatalogSnapshot function\n\n105\n\np. ���\n\np. ���\n\nChapter 4 Snapshots\n\n4.7 Exporting Snapshots\n\nIn some situations, concurrent transactions must see one and the same snapshot by all means. For example, if the pg_dump utility is run in the parallel mode, all its processes must see the same database state to produce a consistent backup.\n\nWe cannot assume that snapshots will be identical simply because transactions were started “simultaneously.” To ensure that all the transactions see the same data, we must employ the snapshot export mechanism.\n\nThe pg_export_snapshot function returns a snapshot ��, which can be passed to another transaction (outside of the database system):\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT count(*) FROM accounts;\n\ncount −−−−−−−\n\n4\n\n(1 row)\n\n=> SELECT pg_export_snapshot();\n\npg_export_snapshot −−−−−−−−−−−−−−−−−−−−− 00000004−0000006E−1\n\n(1 row)\n\nBeforeexecutingthefirststatement,theothertransactioncanimportthesnapshot by running the �������������� �������� command. The isolation level must be set to Repeatable Read or Serializable because operators use their own snapshots at the Read Committed level:\n\n=> DELETE FROM accounts;\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SET TRANSACTION SNAPSHOT '00000004-0000006E-1';\n\nNow the second transaction is going to use the snapshot of the first transaction, and consequently, it will see four rows (instead of zero):\n\n106\n\n4.7 Exporting Snapshots\n\n=> SELECT count(*) FROM accounts;\n\ncount −−−−−−−\n\n4\n\n(1 row)\n\nClearly, the second transaction will not see any changes made by the first transac- tion after the snapshot export (and vice versa): regular visibility rules still apply.\n\nThe exported snapshot’s lifetime is the same as that of the exporting transaction.\n\n=> COMMIT;\n\n=> COMMIT;\n\n107\n\n100\n\np. ���\n\np. ��\n\n5\n\nPage Pruning and HOT Updates\n\n5.1 Page Pruning\n\nWhile a heap page is being read or updated, Postgre��� can perform some quick page cleanup, or pruning.1 It happens in the following cases:\n\nThe previous ������ operation did not find enough space to place a new tuple\n\ninto the same page. This event is reflected in the page header.\n\nThe heap page contains more data than allowed by the\n\nfillfactor storage pa-\n\nrameter.\n\nAn ������ operation can add a new row into the page only if this page is filled for less than fillfactor percent. The rest of the space is kept for ������ opera- tions (no such space is reserved by default).\n\nPage pruning removes the tuples that cannot be visible in any snapshot anymore (that is, that are beyond the database horizon ). It never goes beyond a single heap page, but in return it is performed very fast. Pointers to pruned tuples remain in place since they may be referenced from an index—which is already a different page.\n\nFor the same reason, neither the visibility map nor the free space map is refreshed (so the recovered space is set aside for updates, not for insertions).\n\nSince a page can be pruned during reads, any ������ statement can cause page modifications. This is yet another such case in addition to deferred setting of in- formation bits.\n\n1 backend/access/heap/pruneheap.c, heap_page_prune_opt function\n\n108\n\n5.1 Page Pruning\n\nLet’s take a look at how page pruning actually works. We are going to create a two-column table and build an index on each of the columns:\n\n=> CREATE TABLE hot(id integer, s char(2000)) WITH (fillfactor = 75);\n\n=> CREATE INDEX hot_id ON hot(id);\n\n=> CREATE INDEX hot_s ON hot(s);\n\nIf the s column contains only Latin letters, each heap tuple will have a fixed size of ���� bytes, plus �� bytes of the header. The fillfactor storage parameter is set to ��%. It means that the page has enough free space for four tuples, but we can insert only three.\n\nLet’s insert a new row and update it several times:\n\n=> INSERT INTO hot VALUES (1, 'A');\n\n=> UPDATE hot SET s = 'B';\n\n=> UPDATE hot SET s = 'C';\n\n=> UPDATE hot SET s = 'D';\n\nNow the page contains four tuples:\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−− (0,1) | normal | 801 c | 802 c (0,2) | normal | 802 c | 803 c (0,3) | normal | 803 c | 804 | 0 a (0,4) | normal | 804\n\n(4 rows)\n\nExpectedly, we have just exceeded the fillfactor threshold. You can tell it by the difference between the pagesize and upper values—it is bigger than ��% of the page size, which is ���� bytes:\n\n=> SELECT upper, pagesize FROM page_header(get_raw_page('hot',0));\n\nupper | pagesize −−−−−−−+−−−−−−−−−−\n\n64 |\n\n8192\n\n(1 row)\n\nThe next page access triggers page pruning that removes all the outdated tuples. Then a new tuple (�,�) is added into the freed space:\n\n109\n\np. ��\n\nChapter 5 Page Pruning and HOT Updates\n\n=> UPDATE hot SET s = 'E';\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−\n\n(0,1) | dead (0,2) | dead (0,3) | dead (0,4) | normal | 804 c | 805 | 0 a (0,5) | normal | 805\n\n| | |\n\n| | |\n\n(5 rows)\n\nThe remaining heap tuples are physically moved towards the highest addresses so that all the free space is aggregated into a single continuous chunk. The tuple pointers are also modified accordingly. As a result, there is no free space fragmen- tation in the page.\n\nThe pointers to the pruned tuples cannot be removed yet because they are still ref- erenced from the indexes; Postgre��� changes their status from normal to dead. Let’s take a look at the first page of the hot_s index (the zero page is used for meta- data):\n\n=> SELECT * FROM index_page('hot_s',1);\n\nitemoffset | htid −−−−−−−−−−−−+−−−−−−−\n\n1 | (0,1) 2 | (0,2) 3 | (0,3) 4 | (0,4) 5 | (0,5)\n\n(5 rows)\n\nWe can see the same picture in the other index too:\n\n=> SELECT * FROM index_page('hot_id',1);\n\nitemoffset | htid −−−−−−−−−−−−+−−−−−−−\n\n1 | (0,1) 2 | (0,2) 3 | (0,3) 4 | (0,4) 5 | (0,5)\n\n(5 rows)\n\n110\n\n5.1 Page Pruning\n\nAn index scan can return (�,�), (�,�), and (�,�) as tuple identifiers. The server tries to read the corresponding heap tuple but sees that the pointer has the dead status; it means that this tuple does not exist anymore and should be ignored. And while being at it, the server also changes the pointer status in the index page to avoid repeated heap page access.1\n\nLet’s extend the function pointer is dead:\n\ndisplaying index pages so that it also shows whether the\n\n=> DROP FUNCTION index_page(text, integer);\n\n=> CREATE FUNCTION index_page(relname text, pageno integer) RETURNS TABLE(itemoffset smallint, htid tid, dead boolean) AS $$ SELECT itemoffset, htid, dead -- starting from v.13\n\nFROM bt_page_items(relname,pageno); $$ LANGUAGE sql;\n\n=> SELECT * FROM index_page('hot_id',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,1) | f 2 | (0,2) | f 3 | (0,3) | f 4 | (0,4) | f 5 | (0,5) | f\n\n(5 rows)\n\nAll the pointers in the index page are active so far. But as soon as the first index scan occurs, their status changes:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM hot WHERE id = 1;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using hot_id on hot (actual rows=1 loops=1)\n\nIndex Cond: (id = 1)\n\n(2 rows)\n\n1 backend/access/index/indexam.c, index_fetch_heap function\n\n111\n\nv. ��\n\np. ���\n\nChapter 5 Page Pruning and HOT Updates\n\n=> SELECT * FROM index_page('hot_id',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,1) | t 2 | (0,2) | t 3 | (0,3) | t 4 | (0,4) | t 5 | (0,5) | f\n\n(5 rows)\n\nAlthough the heap tuple referenced by the fourth pointer is still unpruned and has the normal status, it is already beyond the database horizon. That’s why this pointer is also marked as dead in the index.\n\n5.2 HOT Updates\n\nIt would be very inefficient to keep references to all heap tuples in an index.\n\nTo begin with,each row modification triggers updates of all the indexes created on the table: once a new heap tuple appears, each index must include a reference to this tuple, even if the modified fields are not indexed.\n\nFurthermore, indexes accumulate references to historic heap tuples, so they have to be pruned together with these tuples.\n\nThings get worse as you create more indexes on a table.\n\nBut if the updated column is not a part of any index, there is no point in creating another index entry that contains the same key value. To avoid such redundancies, Postgre��� provides an optimization called Heap-Only Tuple updates.1\n\nIf such an update is performed,an index page contains only one entry for each row. This entry points to the very first row version; all the subsequent versions located in the same page are bound into a chain by ctid pointers in the tuple headers.\n\nRow versions that are not referenced from any index are tagged with the Heap-Only Tuple bit. If a version is included into the ��� chain, it is tagged with the Heap Hot Updated bit.\n\n1 backend/access/heap/README.HOT\n\n112\n\n5.2 HOT Updates\n\nIf an index scan accesses a heap page and finds a row version marked as Heap Hot Updated,it means that the scan should continue,so it goes further along the chain of ��� updates. Obviously, all the fetched row versions are checked for visibility before the result is returned to the client.\n\nTo take a look at how ��� updates are performed, let’s delete one of the indexes and truncate the table.\n\n=> DROP INDEX hot_s;\n\n=> TRUNCATE TABLE hot;\n\nFor convenience, we will redefine the heap_page function so that its output in- cludes three more fields: ctid and the two bits related to ��� updates:\n\n=> DROP FUNCTION heap_page(text,integer);\n\n=> CREATE FUNCTION heap_page(relname text, pageno integer) RETURNS TABLE(\n\nctid tid, state text, xmin text, xmax text, hhu text, hot text, t_ctid tid\n\n) AS $$ SELECT (pageno,lp)::text::tid AS ctid,\n\nCASE lp_flags\n\nWHEN 0 THEN 'unused' WHEN 1 THEN 'normal' WHEN 2 THEN 'redirect to '||lp_off WHEN 3 THEN 'dead'\n\nEND AS state, t_xmin || CASE\n\nWHEN (t_infomask & 256) > 0 THEN ' c' WHEN (t_infomask & 512) > 0 THEN ' a' ELSE '' END AS xmin, t_xmax || CASE\n\nWHEN (t_infomask & 1024) > 0 THEN ' c' WHEN (t_infomask & 2048) > 0 THEN ' a' ELSE '' END AS xmax, CASE WHEN (t_infomask2 & 16384) > 0 THEN 't' END AS hhu, CASE WHEN (t_infomask2 & 32768) > 0 THEN 't' END AS hot, t_ctid\n\nFROM heap_page_items(get_raw_page(relname,pageno)) ORDER BY lp; $$ LANGUAGE sql;\n\n113",
      "page_number": 97
    },
    {
      "number": 5,
      "title": "Page Pruning and HOT Updates",
      "start_page": 111,
      "end_page": 122,
      "detection_method": "regex_chapter",
      "content": "Chapter 5 Page Pruning and HOT Updates\n\nLet’s repeat the insert and update operations:\n\n=> INSERT INTO hot VALUES (1, 'A');\n\n=> UPDATE hot SET s = 'B';\n\nThe page now contains a chain of ��� updates:\n\nThe Heap Hot Updated bit shows that the executor should follow the ����\n\nchain.\n\nThe Heap Only Tuple bit indicates that this tuple is not referenced from any\n\nindexes.\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax | hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | normal | 812 c | 813 | 0 a (0,2) | normal | 813\n\n| t |\n\n| | t\n\n| (0,2) | (0,2)\n\n(2 rows)\n\nAs we make further updates, the chain will grow—but only within the page limits:\n\n=> UPDATE hot SET s = 'C';\n\n=> UPDATE hot SET s = 'D';\n\n=> SELECT * FROM heap_page('hot',0);\n\n| xmax −−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\nctid\n\n| state\n\n| xmin\n\n| hhu | hot | t_ctid\n\n(0,1) | normal | 812 c | 813 c | t (0,2) | normal | 813 c | 814 c | t | t (0,3) | normal | 814 c | 815 | | 0 a (0,4) | normal | 815\n\n| | t | t | t\n\n| (0,2) | (0,3) | (0,4) | (0,4)\n\n(4 rows)\n\nThe index still contains only one reference, which points to the head of this chain:\n\n=> SELECT * FROM index_page('hot_id',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,1) | f\n\n(1 row)\n\n114\n\n5.3 Page Pruning for HOT Updates\n\nA ��� update is possible if the modified fields are not a part of any index. Other- wise, some of the indexes would contain a reference to a heap tuple that appears in the middle of the chain, which contradicts the idea of this optimization. Since a ��� chain can grow only within a single page, traversing the whole chain never requires access to other pages and thus does not hamper performance.\n\n5.3 Page Pruning for HOT Updates\n\nA special case of page pruning—which is nevertheless important—is pruning of ��� update chains.\n\nIn the example above, the fillfactor threshold is already exceeded, so the next up- date should trigger page pruning. But this time the page contains a chain of ��� updates. The head of this chain must always remain in its place since it is refer- enced from the index, but other pointers can be released because they are sure to have no external references.\n\nToavoidmovingthehead,Postgre���usesdualaddressing: thepointerreferenced from the index (which is (�,�) in this case) receives the redirect status since it points to the tuple that currently starts the chain:\n\n=> UPDATE hot SET s = 'E';\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmax | hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | redirect to 4 | (0,2) | normal (0,3) | unused (0,4) | normal\n\n| | 0 a |\n\n| 816 | | 815 c | 816\n\n| | | | t\n\n| | t | | t\n\n| | (0,2) | | (0,2)\n\n(4 rows)\n\nThe tuples (�,�), (�,�), and (�,�) have been pruned; the head pointer � remains for redirection purposes, while pointers � and � have been deallocated (received the unused status) since they are guaranteed to have no references from indexes. The new tuple is written into the freed space as tuple (�,�).\n\n115\n\nChapter 5 Page Pruning and HOT Updates\n\nLet’s perform some more updates:\n\n=> UPDATE hot SET s = 'F';\n\n=> UPDATE hot SET s = 'G';\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmax\n\n| hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | redirect to 4 | (0,2) | normal (0,3) | normal (0,4) | normal (0,5) | normal\n\n|\n\n|\n\n| 816 c | 817 c | t | t | 817 c | 818 | 815 c | 816 c | t | 818\n\n| 0 a\n\n|\n\n| | t | t | t | t\n\n| | (0,3) | (0,5) | (0,2) | (0,5)\n\n(5 rows)\n\nThe next update is going to trigger page pruning:\n\n=> UPDATE hot SET s = 'H';\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmax | hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | redirect to 5 | (0,2) | normal (0,3) | unused (0,4) | unused (0,5) | normal\n\n| | 0 a | |\n\n| 819 | | | 818 c | 819\n\n| | | | | t\n\n| | t | | | t\n\n| | (0,2) | | | (0,2)\n\n(5 rows)\n\nAgain, some of the tuples are pruned, and the pointer to the head of the chain is shifted accordingly.\n\nIf unindexed columns are modified frequently, it makes sense to reduce the fillfac- tor value, thus reserving some space in the page for updates. Obviously, you have to keep in mind that the lower the fillfactor value is, the more free space is left in the page, so the physical size of the table grows.\n\n116\n\n5.4 HOT Chain Splits\n\n5.4 HOT Chain Splits\n\nIf the page has no more space to accommodate a new tuple, the chain will be cut off. Postgre��� will have to add a separate index entry to refer to the tuple located in another page.\n\nTo observe this situation, let’s start a concurrent transaction with a snapshot that blocks page pruning:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT 1;\n\nNow we are going to perform some updates in the first session:\n\n=> UPDATE hot SET s = 'I';\n\n=> UPDATE hot SET s = 'J';\n\n=> UPDATE hot SET s = 'K';\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmax\n\n| hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | redirect to 2 | (0,2) | normal (0,3) | normal (0,4) | normal (0,5) | normal\n\n|\n\n|\n\n| 819 c | 820 c | t | 820 c | 821 c | t | t | 821 c | 822 | | 0 a | 822\n\n| | t | t | t | t\n\n| | (0,3) | (0,4) | (0,5) | (0,5)\n\n(5 rows)\n\nWhen the next update happens,this page will not be able to accommodate another tuple, and page pruning will not manage to free any space:\n\n=> UPDATE hot SET s = 'L';\n\n=> COMMIT; -- the snapshot is not required anymore\n\n117\n\nChapter 5 Page Pruning and HOT Updates\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmax\n\n| hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | redirect to 2 | (0,2) | normal (0,3) | normal (0,4) | normal (0,5) | normal\n\n|\n\n|\n\n| 819 c | 820 c | t | 820 c | 821 c | t | 821 c | 822 c | t | 822 c | 823\n\n|\n\n| | t | t | t | t\n\n| | (0,3) | (0,4) | (0,5) | (1,1)\n\n(5 rows)\n\nTuple (�,�) contains the (�,�) reference that goes to page �:\n\n=> SELECT * FROM heap_page('hot',1);\n\nctid\n\n| state\n\n| xmin | xmax | hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−+−−−−−−+−−−−−−+−−−−−+−−−−−+−−−−−−−− | 0 a\n\n(1,1) | normal | 823\n\n|\n\n|\n\n| (1,1)\n\n(1 row)\n\nHowever, this reference is not used: the Heap Hot Updated bit is not set for tuple (�,�). As for tuple (�,�), it can be accessed from the index that now has two entries. Each of them points to the head of their own ��� chain:\n\n=> SELECT * FROM index_page('hot_id',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,1) | f 2 | (1,1) | f\n\n(2 rows)\n\n5.5 Page Pruning for Indexes\n\nI have declared that page pruning is confined to a single heap page and does not affect indexes. However, indexes have their own pruning,1 which also cleans up a single page—an index one in this case.\n\nIndex pruning happens when an insertion into a �-tree is about to split the page into two,as the original page does nothaveenough space anymore. The problem is that even if some index entries are deleted later, two separate index pages will not\n\n1 postgresql.org/docs/14/btree-implementation.html#BTREE-DELETION\n\n118\n\n5.5 Page Pruning for Indexes\n\nbe merged into one. It leads to index bloating, and once bloated, the index cannot shrink even if a large part of the data is deleted. But if pruning can remove some of the tuples, a page split may be deferred.\n\nThere are two types of tuples that can be pruned from an index.\n\nFirst of all, Postgre��� prunes those tuples that have been tagged as dead.1 As I have already said, Postgre��� sets such a tag during an index scan if it detects an index entry pointing to a tuple that is not visible in any snapshot anymore or simply does not exist.\n\n, Postgre��� checks those index entries that ref- If no tuples are known to be dead erence different versions of one and the same table row.2 Because of ����, update operations may generate a large number of row versions, and many of them are soon likely to disappear behind the database horizon. H�� updates cushion this effect, but they are not always applicable: if the column to update is a part of an index,the corresponding references are propagated to all the indexes. Before split- ting the page, it makes sense to search for the rows that are not tagged as dead yet but can already be pruned. To achieve this, Postgre��� has to check visibility of heap tuples. Such checks require table access, so they are performed only for “promising” index tuples, which have been created as copies of the existing ones for ���� purposes. It is cheaper to perform such a check than to allow an extra page split.\n\n1 backend/access/nbtree/README, Simple deletion section 2 backend/access/nbtree/README, Bottom-Up deletion section\n\ninclude/access/tableam.h\n\n119\n\nv. ��\n\np. ���\n\np. ��\n\n6\n\nVacuum and Autovacuum\n\n6.1 Vacuum\n\nPage pruning happens very fast, but it frees only part of the space that can be po- tentially reclaimed. Operating within a single heap page, it does not touch upon indexes (or vice versa, it cleans up an index page without affecting the table).\n\nRoutine vacuuming,1 which is the main vacuuming procedure, is performed by the ������command.2 Itprocessesthewholetableandeliminatesbothoutdatedheap tuples and all the corresponding index entries.\n\nVacuuming is performed in parallel with other processes in the database system. While being vacuumed, tables and indexes can be used in the usual manner, both for read and write operations (but concurrent execution of such commands as ���- ��� �����, ����� �����, and some others is not allowed\n\n).\n\n. Pages tracked To avoid scanning extra pages, Postgre��� uses a visibility map in this map are skipped since they are sure to contain only the current tuples, so a page will only be vacuumed if it does not appear in this map. If all the tuples remaininginapageaftervacuumingarebeyondthedatabasehorizon,thevisibility map is refreshed to include this page.\n\nThe free space map also gets updated to reflect the space that has been cleared.\n\nLet’s create a table with an index on it:\n\n1 postgresql.org/docs/14/routine-vacuuming.html 2 postgresql.org/docs/14/sql-vacuum.html\n\nbackend/commands/vacuum.c\n\n120\n\n6.1 Vacuum\n\n=> CREATE TABLE vac(\n\nid integer, s char(100)\n\n) WITH (autovacuum_enabled = off);\n\n=> CREATE INDEX vac_s ON vac(s);\n\nThe autovacuum_enabled storage parameter turns off autovacuum; we are doing it here solely for the purpose of experimentation to precisely control vacuuming start time.\n\nLet’s insert a row and make a couple of updates:\n\n=> INSERT INTO vac(id,s) VALUES (1,'A');\n\n=> UPDATE vac SET s = 'B';\n\n=> UPDATE vac SET s = 'C';\n\nNow the table contains three tuples:\n\n=> SELECT * FROM heap_page('vac',0);\n\n| xmax −−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\nctid\n\n| state\n\n| xmin\n\n| hhu | hot | t_ctid\n\n(0,1) | normal | 826 c | 827 c | | (0,2) | normal | 827 c | 828 | | 0 a (0,3) | normal | 828\n\n| | |\n\n| (0,2) | (0,3) | (0,3)\n\n(3 rows)\n\nEach tuple is referenced from the index:\n\n=> SELECT * FROM index_page('vac_s',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,1) | f 2 | (0,2) | f 3 | (0,3) | f\n\n(3 rows)\n\nVacuuming has removed all the dead tuples, leaving only the current one:\n\n=> VACUUM vac;\n\n121\n\nChapter 6 Vacuum and Autovacuum\n\n=> SELECT * FROM heap_page('vac',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax | hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | unused | (0,2) | unused | (0,3) | normal | 828 c | 0 a\n\n| |\n\n| | |\n\n| | |\n\n| | | (0,3)\n\n(3 rows)\n\nIn the case of page pruning, the first two pointers would be considered dead, but here they have the unused status since no index entries are referring to them now:\n\n=> SELECT * FROM index_page('vac_s',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,3) | f\n\n(1 row)\n\nPointers with the unused status are treated as free and can be reused by new row versions.\n\nNow the heap page appears in the visibility map; we can check it using the pg_vis- ibility extension:\n\n=> CREATE EXTENSION pg_visibility;\n\n=> SELECT all_visible FROM pg_visibility_map('vac',0);\n\nall_visible −−−−−−−−−−−−−\n\nt\n\n(1 row)\n\nThepageheaderhasalsoreceivedanattributeshowingthatallitstuplesarevisible in all snapshots:\n\n=> SELECT flags & 4 > 0 AS all_visible FROM page_header(get_raw_page('vac',0));\n\nall_visible −−−−−−−−−−−−−\n\nt\n\n(1 row)\n\n122\n\n6.2 Database Horizon Revisited\n\n6.2 Database Horizon Revisited\n\nVacuuming detects dead tuples based on the database horizon. This concept is so fundamental that it makes sense to get back to it once again.\n\nLet’s restart our experiment from the very beginning:\n\n=> TRUNCATE vac;\n\n=> INSERT INTO vac(id,s) VALUES (1,'A');\n\n=> UPDATE vac SET s = 'B';\n\nBut this time, before updating the row, we are going to open another transaction that will hold the database horizon (it can be almost any transaction , except for a virtual one executed at the Read Committed isolation level). For example, this transaction can modify some rows in another table.\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = 0;\n\n=> UPDATE vac SET s = 'C';\n\nNow our table contains three tuples,and the index contains three references. Let’s vacuum the table and see what changes:\n\n=> VACUUM vac;\n\n=> SELECT * FROM heap_page('vac',0);\n\n| xmax −−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\nctid\n\n| state\n\n| xmin\n\n| hhu | hot | t_ctid\n\n(0,1) | unused | | (0,2) | normal | 833 c | 835 c | | (0,3) | normal | 835 c | 0 a\n\n|\n\n| | |\n\n| | (0,3) | (0,3)\n\n(3 rows)\n\n=> SELECT * FROM index_page('vac_s',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,2) | f 2 | (0,3) | f\n\n(2 rows)\n\n123\n\np. ���\n\nChapter 6 Vacuum and Autovacuum\n\nWhile the previous run left only one tuple in the page, now we have two of them: ������ has decided that version (�,�) cannot be removed yet. The reason is the database horizon, which is defined by an unfinished transaction in this case:\n\n=> SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid();\n\nbackend_xmin −−−−−−−−−−−−−−\n\n834\n\n(1 row)\n\nWe can use the ������� clause when calling ������ to observe what is going on:\n\n=> VACUUM VERBOSE vac;\n\nINFO: INFO: in 1 out of 1 pages DETAIL: Skipped 0 pages due to buffer pins, 0 frozen pages. CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s. VACUUM\n\nvacuuming \"public.vac\" table \"vac\": found 0 removable, 2 nonremovable row versions\n\n1 dead row versions cannot be removed yet, oldest xmin: 834\n\nThe output shows the following information:\n\n������ has detected no tuples that can be removed (0 ���������).\n\nTwo tuples must not be removed (2 ������������).\n\nOne of the nonremovable tuples is dead (1 ����), the other is in use.\n\nThe current horizon respected by ������ (������ ����) is the horizon of the\n\nactive transaction.\n\nOnce the active transaction completes, the database horizon moves forward, and vacuuming can continue:\n\n=> COMMIT;\n\n124\n\n6.2 Database Horizon Revisited\n\n=> VACUUM VERBOSE vac;\n\nINFO: INFO: DETAIL: INFO: DETAIL: INFO: DETAIL: 0 index pages were newly deleted. 0 index pages are currently deleted, of which 0 are currently reusable. CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s. INFO: in 1 out of 1 pages DETAIL: Skipped 0 pages due to buffer pins, 0 frozen pages. CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s. VACUUM\n\nvacuuming \"public.vac\" scanned index \"vac_s\" to remove 1 row versions\n\nCPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s table \"vac\": removed 1 dead item identifiers in 1 pages CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s index \"vac_s\" now contains 1 row versions in 2 pages\n\n1 index row versions were removed.\n\ntable \"vac\": found 1 removable, 1 nonremovable row versions\n\n0 dead row versions cannot be removed yet, oldest xmin: 836\n\n������ has detected and removed a dead tuple beyond the new database horizon.\n\nNow the page contains no outdated row versions; the only version remaining is the current one:\n\n=> SELECT * FROM heap_page('vac',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax | hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | unused | (0,2) | unused | (0,3) | normal | 835 c | 0 a\n\n| |\n\n| | |\n\n| | |\n\n| | | (0,3)\n\n(3 rows)\n\nThe index also contains only one entry:\n\n=> SELECT * FROM index_page('vac_s',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,3) | f\n\n(1 row)\n\n125",
      "page_number": 111
    },
    {
      "number": 6,
      "title": "Vacuum and Autovacuum",
      "start_page": 123,
      "end_page": 146,
      "detection_method": "regex_chapter",
      "content": "64MB\n\nChapter 6 Vacuum and Autovacuum\n\n6.3 Vacuum Phases\n\nThe mechanism of vacuuming seems quite simple, but this impression is mislead- ing. After all, both tables and indexes have to be processed concurrently, without blocking other processes. To enable such operation, vacuuming of each table is carried out in several phases.1\n\nIt all starts with scanning a table in search of dead tuples; if found, they are first removed from indexes and then from the table itself. If too many dead tuples have to be vacuumed in one go, this process is repeated. Eventually, heap truncation may be performed.\n\nHeap Scan\n\nIn the first phase, a heap scan is performed.2 The scanning process takes the vis- ibility map into account: all pages tracked in this map are skipped because they are sure to contain no outdated tuples. If a tuple is beyond the horizon and is not required anymore, its �� is added to a special tid array. Such tuples cannot be re- moved yet because they may still be referenced from indexes.\n\nThe tid array resides in the local memory of the ������ process; the size of the maintenance_work_mem parameter. The allocated memory chunk is defined by the whole chunk is allocated at once rather than on demand. However, the allocated memory never exceeds the volume required in the worst-case scenario, so if the table is small, vacuuming may use less memory than specified in this parameter.\n\nIndex Vacuuming\n\nThe first phase can have two outcomes: either the table is scanned in full, or the memory allocated for the tid array is filled up before this operation completes. In any case, index vacuuming begins.3 In this phase, each of the indexes created on\n\n1 backend/access/heap/vacuumlazy.c, heap_vacuum_rel function 2 backend/access/heap/vacuumlazy.c, lazy_scan_heap function 3 backend/access/heap/vacuumlazy.c, lazy_vacuum_all_indexes function\n\n126\n\n6.3 Vacuum Phases\n\nthe table is fully scanned to find all the entries that refer to the tuples registered in the tid array. These entries are removed from index pages.\n\nAn index can help you quickly get to a heap tuple by its index key, but there is no way to quickly find an index entry by the corresponding tuple ��. This functionality is currently being implemented for �-trees,1 but this work is not completed yet.\n\nmin_parallel_index_scan_size value,they If there are several indexes bigger than the can be vacuumed by background workers running in parallel. Unless the level of parallelism is explicitly defined by the parallel N clause, ������ launches one worker per suitable index (within the general limits imposed on the number of background workers).2 One index cannot be processed by several workers.\n\nDuring the index vacuuming phase, Postgre��� updates the free space map and calculates statistics on vacuuming. However, this phase is skipped if rows are only inserted (and are neither deleted nor updated) because the table contains no dead tuples in this case. Then an index scan will be forced only once at the very end, as part of a separate phase of index cleanup.3\n\nThe index vacuuming phase leaves no references to outdated heap tuples in in- dexes,butthetuplesthemselvesarestillpresentinthetable. Itisperfectlynormal: index scans cannot find any dead tuples, while sequential scans of the table rely on visibility rules to filter them out.\n\nHeap Vacuuming\n\nThen the heap vacuuming phase begins.4 The table is scanned again to remove the tuples registered in the tid array and free the corresponding pointers. Now that all the related index references have been removed, it can be done safely.\n\nThe space recovered by ������ is reflected in the free space map, while the pages that now contain only the current tuples visible in all snapshots are tagged in the visibility map.\n\n1 commitfest.postgresql.org/21/1802 2 postgresql.org/docs/14/bgworker.html 3 backend/access/heap/vacuumlazy.c, lazy_cleanup_all_indexes function\n\nbackend/access/nbtree/nbtree.c, btvacuumcleanup function 4 backend/access/heap/vacuumlazy.c, lazy_vacuum_heap function\n\n127\n\n512kB v. ��\n\np. ���\n\nv. ��\n\np. ���\n\nChapter 6 Vacuum and Autovacuum\n\nIf the table was not read in full during the heap scan phase, the tid array is cleared, and the heap scan is resumed from where it left off last time.\n\nHeap Truncation\n\nVacuumed heap pages contain some free space; occasionally, you may be lucky to clear the whole page. If you get several empty pages at the end of the file, vacuum- ing can “bite off” this tail and return the reclaimed space to the operating system. It happens during heap truncation,1 which is the final vacuum phase.\n\nHeaptruncationrequiresashortexclusive processes for too long, attempts to acquire a lock do not exceed five seconds.\n\nlockonthetable. Toavoidholdingother\n\nSincethetablehastobelocked,truncationisonlyperformediftheemptytailtakes at least 1 of the table or has reached the length of �,��� pages. These thresholds 16 are hardcoded and cannot be configured.\n\nIf,despite all these precautions,table locks still cause any issues ,truncation can be disabled altogether using the vacuum_truncate and toast.vacuum_truncate storage parameters.\n\n6.4 Analysis\n\nWhen talking about vacuuming, we have to mention yet another task that is closely related to it, even though there is no formal connection between them. It is analysis,2 or gathering statistical information for the query planner. The collected statistics include the number of rows (pg_class.reltuples) and pages (pg_class.relpages) in relations, data distribution within columns, and some other information.\n\nYou can run the analysis manually using the������� command,3 or combine it with vacuuming by calling ������ �������. However, these two tasks are still performed sequentially, so there is no difference in terms of performance.\n\n1 backend/access/heap/vacuumlazy.c, lazy_truncate_heap function 2 postgresql.org/docs/14/routine-vacuuming.html#VACUUM-FOR-STATISTICS 3 backend/commands/analyze.c\n\n128\n\n6.5 Automatic Vacuum and Analysis\n\nHistorically, ������ ������� appeared first, in version �.�, while a separate ������� com- mand was not implemented until version �.�. In earlier versions, statistics were collected by a ��� script.\n\nAutomatic vacuum and analysis are set up in a similar way, so it makes sense to discuss them together.\n\n6.5 Automatic Vacuum and Analysis\n\nUnless the database horizon is held up for a long time, routine vacuuming should cope with its work. But how often do we need to call the ������ command?\n\nIf a frequently updated table is vacuumed too seldom, it will grow bigger than de- sired. Besides,it may accumulate too many changes,and then the next������ run will have to make several passes over the indexes.\n\nIfthetableisvacuumedtoooften,theserverwillbebusywithmaintenanceinstead of useful work.\n\nFurthermore,typicalworkloadsmaychangeovertime,sohavingafixedvacuuming schedule will not help anyway: the more often the table is updated,the more often it has to be vacuumed.\n\nThis problem is solved by autovacuum,1 which launches vacuum and analysis pro- cesses based on the intensity of table updates.\n\nAbout the Autovacuum Mechanism\n\nautovacuum configuration parameter is on), the au- When autovacuum is enabled ( tovacuum launcher process is always running in the system. This process defines the autovacuum schedule and maintains the list of“active”databases based on us- track_counts parameter is enabled. age statistics. Such statistics are collected if the Do not switch off these parameters, otherwise autovacuum will not work.\n\n1 postgresql.org/docs/14/routine-vacuuming.html#AUTOVACUUM\n\n129\n\non\n\non\n\n1min\n\n3\n\nChapter 6 Vacuum and Autovacuum\n\nautovacuum_naptime, the autovacuum launcher starts an autovacuum Once in worker1 for each active database in the list (these workers are spawned by post- master, as usual). Consequently, if there are N active databases in the cluster, N workers are spawned within the autovacuum_naptime interval. But the total num- berofautovacuumworkersrunninginparallelcannotexceedthethresholddefined by the\n\nautovacuum_max_workers parameter.\n\nAutovacuum workers are very similar to regular background workers, but they appeared much earlier than this general mechanism of task management. It was decided to leave the autovacuum implementation unchanged, so autovacuum workers do not use max_worker_processes slots.\n\nOnce started,the background worker connects to the specified database and builds two lists:\n\nthe list of all tables, materialized views, and ����� tables to be vacuumed\n\nthe list of all tables and materialized views to be analyzed (����� tables are\n\nnot analyzed because they are always accessed via an index)\n\nThen the selected objects are vacuumed or analyzed one by one (or undergo both operations), and once the job is complete, the worker is terminated.\n\nAutomatic vacuuming works similar to the manual one initiated by the ������ command, but there are some nuances:\n\nManual vacuuming accumulates tuple ��s in a memory chunk of the mainte- nance_work_mem size. However, using the same limit for autovacuum is un- desirable, as it can result in excessive memory consumption: there may be several autovacuum workers running in parallel, and each of them will get maintenance_work_mem of memory at once. Instead, Postgre��� provides a separate memory limit for autovacuum processes, which is defined by the au- tovacuum_work_mem parameter. autovacuum_work_mem parameter falls back on the regular By default, the maintenance_work_mem limit, so if the autovacuum_max_workers value is high, you may have to adjust the autovacuum_work_mem value accordingly.\n\nManual vacuuming accumulates tuple ��s in a memory chunk of the mainte- nance_work_mem size. However, using the same limit for autovacuum is un- desirable, as it can result in excessive memory consumption: there may be several autovacuum workers running in parallel, and each of them will get maintenance_work_mem of memory at once. Instead, Postgre��� provides a separate memory limit for autovacuum processes, which is defined by the au- tovacuum_work_mem parameter. autovacuum_work_mem parameter falls back on the regular By default, the maintenance_work_mem limit, so if the autovacuum_max_workers value is high, you may have to adjust the autovacuum_work_mem value accordingly.\n\n1 backend/postmaster/autovacuum.c\n\n130\n\n6.5 Automatic Vacuum and Analysis\n\nConcurrent processing of several indexes created on one table can be per- formed only by manual vacuuming; using autovacuum for this purpose would result in a large number of parallel processes, so it is not allowed.\n\nIfaworkerfailstocompleteallthescheduledtaskswithintheautovacuum_naptime interval, the autovacuum launcher spawns another worker to be run in parallel in that database. The second worker will build its own lists of objects to be vacuumed and analyzed and will start processing them. There is no parallelism at the table level; only different tables can be processed concurrently.\n\nWhich Tables Need to be Vacuumed?\n\nYou can disable autovacuum at the table level—although it is hard to imagine why it could be necessary. There are two storage parameters provided for this purpose, one for regular tables and the other for ����� tables:\n\nautovacuum_enabled\n\ntoast.autovacuum_enabled\n\nIn usual circumstances, autovacuum is triggered either by tuples or by insertion of new rows.\n\naccumulation of dead\n\nDead tuple accumulation. Dead tuples are constantly being counted by the statis- tics collector; their current number is shown in the system catalog table called pg_stat_all_tables.\n\nIt is assumed that dead tuples have to be vacuumed if they exceed the threshold defined by the following two parameters:\n\n\n\nautovacuum_vacuum_threshold, which specifies the number of dead tuples (an absolute value)\n\n\n\nautovacuum_vacuum_scale_factor, which sets the fraction of dead tuples in a table\n\n131\n\np. ���\n\n50\n\n0.2\n\nv. ��\n\np. ��� p. ���\n\n1000\n\n0.2\n\nChapter 6 Vacuum and Autovacuum\n\nVacuuming is required if the following condition is satisfied:\n\npg_stat_all_tables.n_dead_tup >\n\nautovacuum_vacuum_threshold +\n\nautovacuum_vacuum_scale_factor × pg_class.reltuples\n\nThe main parameter here is of course autovacuum_vacuum_scale_factor: its value is important for large tables (and it is large tables that are likely to cause the majority of issues). The default value of ��% seems too big and may have to be significantly reduced.\n\nFor different tables, optimal parameter values may vary: they largely depend on the table size and workload type. It makes sense to set more or less adequate initial values and then override them for particular tables using storage parameters:\n\nautovacuum_vacuum_threshold and toast.autovacuum_vacuum_threshold\n\nautovacuum_vacuum_scale_factor and toast.autovacuum_vacuum_scale_factor\n\nRow insertions. If rows are only inserted and are neither deleted nor updated, the table contains no dead tuples. But such tables should also be vacuumed to freeze and update the visibility map (thus enabling index-only heap tuples in advance scans\n\n).\n\nA table will be vacuumed if the number of rows inserted since the previous vacu- uming exceeds the threshold defined by another similar pair of parameters:\n\n\n\nautovacuum_vacuum_insert_threshold\n\n\n\nautovacuum_vacuum_insert_scale_factor\n\nThe formula is as follows:\n\npg_stat_all_tables.n_ins_since_vacuum >\n\nautovacuum_vacuum_insert_threshold +\n\nautovacuum_vacuum_insert_scale_factor × pg_class.reltuples\n\n132\n\n6.5 Automatic Vacuum and Analysis\n\nLike in the previous example,you can override these values at the table level using storage parameters:\n\nautovacuum_vacuum_insert_threshold and its ����� counterpart\n\nautovacuum_vacuum_insert_scale_factor and its ����� counterpart\n\nWhich Tables Need to Be Analyzed?\n\nAutomatic analysis needs to process only modified rows, so the calculations are a bit simpler than those for autovacuum.\n\nIt is assumed that a table has to be analyzed if the number of rows modified since the previous analysis exceeds the threshold defined by the following two configu- ration parameters:\n\n\n\nautovacuum_analyze_threshold\n\n\n\nautovacuum_analyze_scale_factor\n\nAutoanalysis is triggered if the following condition is met:\n\npg_stat_all_tables.n_mod_since_analyze >\n\nautovacuum_analyze_threshold +\n\nautovacuum_analyze_scale_factor × pg_class.reltuples\n\nTo override autoanalysis settings for particular tables,you can use the same-name storage parameters:\n\nautovacuum_analyze_threshold\n\nautovacuum_analyze_scale_factor\n\nSince ����� tables are not analyzed, they have no corresponding parameters.\n\n133\n\n50\n\n0.1\n\nChapter 6 Vacuum and Autovacuum\n\nAutovacuum in Action\n\nTo formalize everything said in this section,let’s create two views that show which tables currently need to be vacuumed and analyzed.1 The function used in these views returns the current value of the passed parameter, taking into account that this value can be redefined at the table level:\n\n=> CREATE FUNCTION p(param text, c pg_class) RETURNS float AS $$\n\nSELECT coalesce(\n\n-- use storage parameter if set (SELECT option_value\n\nFROM WHERE\n\npg_options_to_table(c.reloptions) option_name = CASE\n\n-- for TOAST tables the parameter name is different WHEN c.relkind = 't' THEN 'toast.' ELSE ''\n\nEND || param\n\n), -- else take the configuration parameter value current_setting(param)\n\n)::float;\n\n$$ LANGUAGE sql;\n\nThis is how a vacuum-related view can look like:\n\n=> CREATE VIEW need_vacuum AS WITH c AS (\n\nSELECT c.oid,\n\ngreatest(c.reltuples, 0) reltuples, p('autovacuum_vacuum_threshold', c) threshold, p('autovacuum_vacuum_scale_factor', c) scale_factor, p('autovacuum_vacuum_insert_threshold', c) ins_threshold, p('autovacuum_vacuum_insert_scale_factor', c) ins_scale_factor\n\nFROM pg_class c WHERE c.relkind IN ('r','m','t')\n\n) SELECT st.schemaname || '.' || st.relname AS tablename,\n\nst.n_dead_tup AS dead_tup, c.threshold + c.scale_factor * c.reltuples AS max_dead_tup, st.n_ins_since_vacuum AS ins_tup, c.ins_threshold + c.ins_scale_factor * c.reltuples AS max_ins_tup, st.last_autovacuum\n\nFROM pg_stat_all_tables st\n\nJOIN c ON c.oid = st.relid;\n\n1 backend/postmaster/autovacuum.c, relation_needs_vacanalyze function\n\n134\n\n6.5 Automatic Vacuum and Analysis\n\nThe max_dead_tup column shows the number of dead tuples that will trigger au- tovacuum, whereas the max_ins_tup column shows the threshold value related to insertion.\n\nHere is a similar view for analysis:\n\n=> CREATE VIEW need_analyze AS WITH c AS (\n\nSELECT c.oid,\n\ngreatest(c.reltuples, 0) reltuples, p('autovacuum_analyze_threshold', c) threshold, p('autovacuum_analyze_scale_factor', c) scale_factor\n\nFROM pg_class c WHERE c.relkind IN ('r','m')\n\n) SELECT st.schemaname || '.' || st.relname AS tablename,\n\nst.n_mod_since_analyze AS mod_tup, c.threshold + c.scale_factor * c.reltuples AS max_mod_tup, st.last_autoanalyze\n\nFROM pg_stat_all_tables st\n\nJOIN c ON c.oid = st.relid;\n\nThe max_mod_tup column shows the threshold value for autoanalysis.\n\nTo speed up the experiment, we will be starting autovacuum every second:\n\n=> ALTER SYSTEM SET autovacuum_naptime = '1s';\n\n=> SELECT pg_reload_conf();\n\nLet’s truncate the vac table and then insert �,��� rows. Note that autovacuum is turned off at the table level.\n\n=> TRUNCATE TABLE vac;\n\n=> INSERT INTO vac(id,s)\n\nSELECT id, 'A' FROM generate_series(1,1000) id;\n\nHere is what our vacuum-related view will show:\n\n=> SELECT * FROM need_vacuum WHERE tablename = 'public.vac' \\gx −[ RECORD 1 ]−−−+−−−−−−−−−−− | public.vac tablename | 0 dead_tup | 50 max_dead_tup | 1000 ins_tup | 1000 max_ins_tup last_autovacuum |\n\n135\n\nv. ��\n\nChapter 6 Vacuum and Autovacuum\n\nThe actual threshold value is max_dead_tup = 50, although the formula listed above suggests that it should be 50 + 0.2 × 1000 = 250. The thing is that statistics on this table are not available yet since the ������ command does not update it:\n\n=> SELECT reltuples FROM pg_class WHERE relname = 'vac';\n\nreltuples −−−−−−−−−−−\n\n−1\n\n(1 row)\n\nThe pg_class.reltuples value is set to −1; this special constant is used instead of zero to differentiate between a table without any statistics and a really empty table that has already been analyzed. For the purpose of calculation, the negative value is taken as zero, which gives us 50 + 0.2 × 0 = 50.\n\nThe value of max_ins_tup = 1000 differs from the projected value of �,��� for the same reason.\n\nLet’s have a look at the analysis view:\n\n=> SELECT * FROM need_analyze WHERE tablename = 'public.vac' \\gx\n\n−[ RECORD 1 ]−−−−+−−−−−−−−−−− | public.vac tablename | 1006 mod_tup max_mod_tup | 50 last_autoanalyze |\n\nWe have updated (inserted in this case) �,��� rows; as a result, the threshold is exceeded: since the table size is unknown, it is currently set to ��. It means that autoanalysis will be triggered immediately when we turn it on:\n\n=> ALTER TABLE vac SET (autovacuum_enabled = on);\n\nOnce the table analysis completes, the threshold is reset to an adequate value of ��� rows.\n\n=> SELECT reltuples FROM pg_class WHERE relname = 'vac';\n\nreltuples −−−−−−−−−−−\n\n1000\n\n(1 row)\n\n136\n\n6.5 Automatic Vacuum and Analysis\n\n=> SELECT * FROM need_analyze WHERE tablename = 'public.vac' \\gx\n\n−[ RECORD 1 ]−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− tablename mod_tup max_mod_tup last_autoanalyze | 2023−03−06 14:00:45.533464+03\n\n| public.vac | 0 | 150\n\nLet’s get back to autovacuum:\n\n=> SELECT * FROM need_vacuum WHERE tablename = 'public.vac' \\gx\n\n−[ RECORD 1 ]−−−+−−−−−−−−−−− | public.vac tablename | 0 dead_tup | 250 max_dead_tup | 1000 ins_tup max_ins_tup | 1200 last_autovacuum |\n\nThe max_dead_tup and max_ins_tup values have also been updated based on the actual table size discovered by the analysis.\n\nVacuuming will be started if at least one of the following conditions is met:\n\nMore than ��� dead tuples are accumulated.\n\nMore than ��� rows are inserted into the table.\n\nLet’s turn off autovacuum again and update ��� rows so that the threshold value is exceeded by one:\n\n=> ALTER TABLE vac SET (autovacuum_enabled = off);\n\n=> UPDATE vac SET s = 'B' WHERE id <= 251;\n\n=> SELECT * FROM need_vacuum WHERE tablename = 'public.vac' \\gx\n\n−[ RECORD 1 ]−−−+−−−−−−−−−−− | public.vac tablename | 251 dead_tup | 250 max_dead_tup | 1000 ins_tup max_ins_tup | 1200 last_autovacuum |\n\nNow the trigger condition is satisfied. Let’s enable autovacuum; after a while, we will see that the table has been processed, and its usage statistics has been reset:\n\n137\n\nv. ��\n\n200 0\n\np. ��� 1 2 20\n\nChapter 6 Vacuum and Autovacuum\n\n=> ALTER TABLE vac SET (autovacuum_enabled = on);\n\n=> SELECT * FROM need_vacuum WHERE tablename = 'public.vac' \\gx\n\n−[ RECORD 1 ]−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− tablename dead_tup max_dead_tup ins_tup max_ins_tup last_autovacuum | 2023−03−06 14:00:51.736815+03\n\n| public.vac | 0 | 250 | 0 | 1200\n\n6.6 Managing the Load\n\nOperating at the page level, vacuuming does not block other processes; but never- theless, it increases the system load and can have a noticeable impact on perfor- mance.\n\nVacuum Throttling\n\nTo control vacuuming intensity, Postgre��� makes regular pauses in table pro- vacuum_cost_limit units of work, the process falls cessing. After completing about asleep and remains idle for the\n\nvacuum_cost_delay time interval.\n\nThe default zero value of vacuum_cost_delay means that routine vacuuming actu- ally never sleeps, so the exact vacuum_cost_limit value makes no difference. It is assumed that if administrators have to resort to manual vacuuming,they are likely to expect its completion as soon as possible.\n\nIf the sleep time is set, then the process will pause each time it has spent vac- uum_cost_limit units of work on page processing in the buffer cache . The cost of vacuum_cost_page_hit units if the page is found in each page read is estimated at vacuum_cost_page_miss units otherwise.1 If a clean page is dirt- the buffer cache,or ied by vacuum, it adds another\n\nvacuum_cost_page_dirty units.2\n\n1 backend/storage/buffer/bufmgr.c, ReadBuffer_common function 2 backend/storage/buffer/bufmgr.c, MarkBufferDirty function\n\n138\n\n6.6 Managing the Load\n\nIf you keep the default value of the vacuum_cost_limit parameter, ������ can pro- cess up to ��� pages per cycle in the best-case scenario (if all the pages are cached, and no pages are dirtied by������) and only nine pages in the worst case (if all the pages are read from disk and become dirty).\n\nAutovacuum Throttling\n\nThrottling for autovacuum1 is quite similar to ������ throttling. However, auto- vacuum can be run with a different intensity as it has its own set of parameters:\n\n\n\nautovacuum_vacuum_cost_limit\n\n\n\nautovacuum_vacuum_cost_delay\n\nIf any of these parameters is set to −1,it falls back on the corresponding parameter for regular ������. Thus, the autovacuum_vacuum_cost_limit parameter relies on the vacuum_cost_limit value by default.\n\nPrior to version ��, the default value of autovacuum_vacuum_cost_delay was �� ms, and it led to very poor performance on modern hardware.\n\nAutovacuumworkunitsarelimitedtoautovacuum_vacuum_cost_limit percycle,and since they are shared between all the workers, the overall impact on the system re- mains roughly the same,regardless of their number. So if you need to speed up au- tovacuum, both the autovacuum_max_workers and autovacuum_vacuum_cost_limit values should be increased proportionally.\n\nIf required, you can override these settings for particular tables by setting the fol- lowing storage parameters:\n\nautovacuum_vacuum_cost_delay and toast.autovacuum_vacuum_cost_delay\n\nautovacuum_vacuum_cost_limit and toast.autovacuum_vacuum_cost_limit\n\n1 backend/postmaster/autovacuum.c, autovac_balance_cost function\n\n139\n\n−1\n\n2ms\n\nv. �.�\n\nv. ��\n\nChapter 6 Vacuum and Autovacuum\n\n6.7 Monitoring\n\nIf vacuuming is monitored, you can detect situations when dead tuples cannot be removed in one go, as references to them do not fit the maintenance_work_mem memory chunk. In this case, all the indexes will have to be fully scanned several times. It can take a substantial amount of time for large tables, thus creating a significant load on the system. Even though queries will not be blocked, extra �/� operations can seriously limit system throughput.\n\nSuch issues can be corrected either by vacuuming the table more often (so that each run cleans up fewer tuples) or by allocating more memory.\n\nMonitoring Vacuum\n\nWhen run with the������� clause,the������ command performs the cleanup and displays the status report, whereas the pg_stat_progress_vacuum view shows the current state of the started process.\n\nThere is also a similar view for analysis is usually performed very fast and is unlikely to cause any issues.\n\n(pg_stat_progress_analyze), even though it\n\nLet’s insert more rows into the table and update them all so that ������ has to run for a noticeable period of time:\n\n=> TRUNCATE vac;\n\n=> INSERT INTO vac(id,s)\n\nSELECT id, 'A' FROM generate_series(1,500000) id;\n\n=> UPDATE vac SET s\n\n= 'B';\n\nFor the purpose of this demonstration, we will limit the amount of memory allo- cated for the tid array by � ��:\n\n=> ALTER SYSTEM SET maintenance_work_mem = '1MB';\n\n=> SELECT pg_reload_conf();\n\nLaunch the ������ command and query the pg_stat_progress_vacuum view several times while it is running:\n\n140\n\n6.7 Monitoring\n\n=> VACUUM VERBOSE vac;\n\n=> SELECT * FROM pg_stat_progress_vacuum \\gx\n\n−[ RECORD 1 ]−−−−−−+−−−−−−−−−−−−−−−−−− pid datid datname relid phase heap_blks_total heap_blks_scanned heap_blks_vacuumed | 0 index_vacuum_count | 0 max_dead_tuples num_dead_tuples\n\n| 14531 | 16391 | internals | 16479 | vacuuming indexes | 17242 | 3009\n\n| 174761 | 174522\n\n=> SELECT * FROM pg_stat_progress_vacuum \\gx\n\n−[ RECORD 1 ]−−−−−−+−−−−−−−−−−−−−−−−−− pid datid datname relid phase heap_blks_total heap_blks_scanned heap_blks_vacuumed | 6017 index_vacuum_count | 2 max_dead_tuples num_dead_tuples\n\n| 14531 | 16391 | internals | 16479 | vacuuming indexes | 17242 | 17242\n\n| 174761 | 150956\n\nIn particular, this view shows:\n\nphase—thenameofthecurrentvacuumphase(Ihavedescribedthemainones, but there are actually more of them1)\n\nheap_blks_total—the total number of pages in a table\n\nheap_blks_scanned—the number of scanned pages\n\nheap_blks_vacuumed—the number of vacuumed pages\n\nindex_vacuum_count—the number of index scans\n\n1 postgresql.org/docs/14/progress-reporting.html#VACUUM-PHASES\n\n141\n\nChapter 6 Vacuum and Autovacuum\n\nThe overall vacuuming progress is defined by the ratio of heap_blks_vacuumed to heap_blks_total, but you have to keep in mind that it changes in spurts because of index scans. In fact,it is more important to pay attention to the number of vacuum cycles: if this value is greater than one, it means that the allocated memory was not enough to complete vacuuming in one go.\n\nYoucanseethewholepictureintheoutputofthe�������������command,which has already finished by this time:\n\nINFO: INFO: DETAIL: INFO: 3009 pages DETAIL: INFO: DETAIL: INFO: 3009 pages DETAIL: INFO: DETAIL: INFO: 2603 pages DETAIL: INFO: 932 pages DETAIL: 433 index pages were newly deleted. 433 index pages are currently deleted, of which 0 are currently reusable. CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s. INFO: nonremovable row versions in 17242 out of 17242 pages DETAIL: xmin: 851 Skipped 0 pages due to buffer pins, 0 frozen pages. CPU: user: 0.20 s, system: 0.03 s, elapsed: 0.53 s. VACUUM\n\nvacuuming \"public.vac\" scanned index \"vac_s\" to remove 174522 row versions\n\nCPU: user: 0.02 s, system: 0.00 s, elapsed: 0.05 s table \"vac\": removed 174522 dead item identifiers in\n\nCPU: user: 0.00 s, system: 0.01 s, elapsed: 0.07 s\n\nscanned index \"vac_s\" to remove 174522 row versions\n\nCPU: user: 0.02 s, system: 0.00 s, elapsed: 0.05 s table \"vac\": removed 174522 dead item identifiers in\n\nCPU: user: 0.00 s, system: 0.00 s, elapsed: 0.01 s\n\nscanned index \"vac_s\" to remove 150956 row versions\n\nCPU: user: 0.02 s, system: 0.00 s, elapsed: 0.04 s table \"vac\": removed 150956 dead item identifiers in\n\nCPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s\n\nindex \"vac_s\" now contains 500000 row versions in\n\n500000 index row versions were removed.\n\ntable \"vac\": found 500000 removable, 500000\n\n0 dead row versions cannot be removed yet, oldest\n\nAll in all, there have been three index scans; each scan has removed ���,��� pointers to dead tuples at the most. This value is defined by the number of tid pointers (each of them takes � bytes) that can fit into an array of the main-\n\n142\n\nindex vacuum\n\ntable vacuum\n\nindex vacuum\n\ntable vacuum\n\nindex vacuum\n\ntable vacuum\n\n6.7 Monitoring\n\ntenance_work_mem size. The maximum size possible is shown by pg_stat_prog- ress_vacuum.max_dead_tuples,but the actually used space is always a bit smaller. It guarantees that when the next page is read, all its pointers to dead tuples, no mat- ter how many of them are located in this page, will fit into the remaining memory.\n\nMonitoring Autovacuum\n\nThe main approach to monitoring autovacuum is to print its status information (which is similar to the output of the ������ ������� command) into the server log_autovacuum_min_duration parameter is set to log for further analysis. If the zero, all autovacuum runs are logged:\n\n=> ALTER SYSTEM SET log_autovacuum_min_duration = 0;\n\n=> SELECT pg_reload_conf();\n\n=> UPDATE vac SET s = 'C';\n\nUPDATE 500000\n\npostgres$ tail -n 13 /home/postgres/logfile\n\n2023−03−06 14:01:13.727 MSK [17351] LOG: \"internals.public.vac\": index scans: 3 pages: 0 removed, 17242 remain, 0 skipped due to pins, 0 skipped frozen tuples: 500000 removed, 500000 remain, 0 are dead but not yet removable, oldest xmin: 853 index scan needed: 8622 pages from table (50.01% of total) had 500000 dead item identifiers removed index \"vac_s\": pages: 1428 in total, 496 newly deleted, 929 currently deleted, 433 reusable avg read rate: 12.404 MB/s, avg write rate: 14.810 MB/s buffer usage: 46038 hits, 5670 misses, 6770 dirtied WAL usage: 40390 records, 15062 full page images, 89188595 bytes system usage: CPU: user: 0.31 s, system: 0.33 s, elapsed: 3.57 s 2023−03−06 14:01:14.117 MSK [17351] LOG: \"internals.public.vac\" avg read rate: 41.081 MB/s, avg write rate: 0.020 MB/s buffer usage: 15355 hits, 2035 misses, 1 dirtied system usage: CPU: user: 0.14 s, system: 0.00 s, elapsed: 0.38 s\n\nautomatic vacuum of table\n\nautomatic analyze of table\n\n143\n\n−1\n\nChapter 6 Vacuum and Autovacuum\n\nTo track the list of tables that have to be vacuumed and analyzed, you can use the need_vacuum and need_analyze views, which we have already reviewed. If this list grows, it means that autovacuum does not cope with the load and has to be sped up by either reducing the gap (autovacuum_vacuum_cost_delay) or increasing the amount of work done between the gaps (autovacuum_vacuum_cost_limit). It is not unlikely that the degree of parallelism will also have to be increased (autovac- uum_max_workers).\n\n144\n\n7\n\nFreezing\n\n7.1 Transaction ID Wraparound\n\nIn Postgre���, a transaction �� takes �� bits. Four billions seems to be quite a big number, but it can be exhausted very fast if the system is being actively used. For example, for an average load of �,��� transactions per second (excluding virtual ones), it will happen in about six weeks of continuous operation.\n\nOnce all the numbers are used up, the counter has to be reset to start the next round (this situation is called a “wraparound”). But a transaction with a smaller �� can only be considered older than another transaction with a bigger �� if the assigned numbers are always increasing. So the counter cannot simply start using the same numbers anew after being reset.\n\nAllocating �� bits for transaction ��s would have eliminated this problem alto- gether, so why doesn’t Postgre��� take advantage of it? The thing is that each tuple header has to store ��s for two transactions: xmin and xmax. The header is quite big already (at least �� bytes if data alignment is taken into account), and adding more bits would have given another � bytes.\n\nPostgre��� does implement ��-bit transaction ��s1 that extend a regular �� by a ��-bit epoch, but they are used only internally and never get into data pages.\n\nTocorrectlyhandlewraparound,Postgre���hastocomparetheageoftransactions (defined as the number of subsequent transactions that have appeared since the startofthistransaction)ratherthantransaction��s. Thus,insteadofthetermsless than and greater than we should use the concepts of older (precedes) and younger (follows).\n\n1 include/access/transam.h, FullTransactionId type\n\n145\n\np. ��\n\np. ���\n\nChapter 7 Freezing\n\nIn the code,this comparison is implemented by simply using the ��-bit arithmetic: first the difference between ��-bit transaction ��s is found, and then this result is compared to zero.1\n\nTo better visualize this idea, you can imagine a sequence of transaction ��s as a clock face. For each transaction,half of the circle in the clockwise direction will be in the future, while the other half will be in the past.\n\nT1\n\nT1\n\nT1T1\n\nT2\n\np a s t\n\nf u t u r e\n\nHowever, this visualization has an unpleasant catch. An old transaction (��) is in the remote past as compared to more recent transactions. But sooner or later a new transaction will see it in the half of the circle related to the future. If it were really so,it would have a catastrophic impact: from now on,all newer transactions would not see the changes made by transaction ��.\n\n7.2 Tuple Freezing and Visibility Rules\n\nTo prevent such “time travel,” vacuuming performs one more task (in addition to pagecleanup):2 itsearchesfortuplesthatarebeyondthedatabasehorizon(sothey are visible in all snapshots) and tags them in a special way, that is, freezes\n\nFor frozen tuples, visibility rules do not have to take xmin into account since such tuples are known to be visible in all snapshots, so this transaction �� can be safely reused.\n\n1 backend/access/transam/transam.c, TransactionIdPrecedes function 2 postgresql.org/docs/14/routine-vacuuming.html#VACUUM-FOR-WRAPAROUND\n\n146\n\nT2\n\nT3\n\nthem.\n\n7.2 Tuple Freezing and Visibility Rules\n\nYou can imagine that the xmin transaction �� is replaced in frozen tuples by a hy- pothetical “minus infinity” (pictured as a snowflake below); it is a sign that this tuple is created by a transaction that is so far in the past that its actual �� does not matter anymore. Yet in reality xmin remains unchanged, whereas the freezing attribute is defined by a combination of two hint bits: committed and aborted.\n\nT1(cid:94)\n\n(cid:94)\n\nT1(cid:94)\n\nT2\n\n(cid:94)\n\n(cid:94)\n\nT3\n\nT3\n\nT2\n\nT4\n\nT1\n\nT4\n\nT1\n\nT4\n\nMany sources (including the documentation) mention FrozenTransactionId = 2. It is the “minus infinity”that I have already referred to—this value used to replace xmin in versions prior to �.�, but now hint bits are employed instead. As a result, the original transaction �� remains in the tuple,which is convenient for both debugging and support. Old systems can still contain the obsolete FrozenTransactionId, even if they have been upgraded to higher versions.\n\nThe xmax transaction �� does not participate in freezing in any way. It is only present in outdated tuples, and once such tuples stop being visible in all snap- shots (which means that the xmax �� is beyond the database horizon), they will be vacuumed away.\n\nLet’s create a new table for our experiments. The fillfactor parameter should be set to the lowest value so that each page can accommodate only two tuples—it will be easier to track the progress this way. We will also disable autovacuum to make sure that the table is only cleaned up on demand.\n\n=> CREATE TABLE tfreeze(\n\nid integer, s char(300)\n\n) WITH (fillfactor = 10, autovacuum_enabled = off);\n\n(cid:94)\n\n147\n\nChapter 7 Freezing\n\nWe are going to create yet another flavor of the function that displays heap pages using pageinspect. Dealing with a range of pages, it will show the values of the freezing attribute (f) and the xmin transaction age for each tuple (it will have to call the age system function—the age itself is not stored in heap pages, of course):\n\n=> CREATE FUNCTION heap_page(\n\nrelname text, pageno_from integer, pageno_to integer\n\n) RETURNS TABLE(\n\nctid tid, state text, xmin text, xmin_age integer, xmax text\n\n) AS $$ SELECT (pageno,lp)::text::tid AS ctid,\n\nCASE lp_flags\n\nWHEN 0 THEN 'unused' WHEN 1 THEN 'normal' WHEN 2 THEN 'redirect to '||lp_off WHEN 3 THEN 'dead'\n\nEND AS state, t_xmin || CASE\n\nWHEN (t_infomask & 256+512) = 256+512 THEN ' f' WHEN (t_infomask & 256) > 0 THEN ' c' WHEN (t_infomask & 512) > 0 THEN ' a' ELSE '' END AS xmin, age(t_xmin) AS xmin_age, t_xmax || CASE\n\nWHEN (t_infomask & 1024) > 0 THEN ' c' WHEN (t_infomask & 2048) > 0 THEN ' a' ELSE '' END AS xmax\n\nFROM generate_series(pageno_from, pageno_to) p(pageno), heap_page_items(get_raw_page(relname, pageno))\n\nORDER BY pageno, lp; $$ LANGUAGE sql;\n\nNow let’s insert some rows into the table and run the ������ command that will immediately create the visibility map.\n\n=> CREATE EXTENSION IF NOT EXISTS pg_visibility;\n\n=> INSERT INTO tfreeze(id, s)\n\nSELECT id, 'FOO'||id FROM generate_series(1,100) id;\n\nINSERT 0 100\n\n148\n\n7.3 Managing Freezing\n\nWe are going to observe the first two heap pages using the pg_visibility exten- sion. When vacuuming completes, both pages get tagged in the visibility map (all_visible) but not in the freeze map (all_frozen ), as they still contain some un- frozen tuples:\n\n=> VACUUM tfreeze;\n\n=> SELECT * FROM generate_series(0,1) g(blkno),\n\npg_visibility_map('tfreeze',g.blkno)\n\nORDER BY g.blkno;\n\nblkno | all_visible | all_frozen −−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0 | t 1 | t\n\n| f | f\n\n(2 rows)\n\nThe xmin_age of the transaction that has created the rows equals 1 because it is the latest transaction performed in the system:\n\n=> SELECT * FROM heap_page('tfreeze',0,1);\n\nctid\n\n| state\n\n| xmin\n\n| xmin_age | xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−−−−+−−−−−−\n\n(0,1) | normal | 856 c | (0,2) | normal | 856 c | (1,1) | normal | 856 c | (1,2) | normal | 856 c |\n\n1 | 0 a 1 | 0 a 1 | 0 a 1 | 0 a\n\n(4 rows)\n\n7.3 Managing Freezing\n\nThere are four main parameters that control freezing. All of them represent trans- action age and define when the following events happen:\n\nFreezing starts (vacuum_freeze_min_age).\n\nAggressive freezing is performed (vacuum_freeze_table_age).\n\nFreezing is forced (autovacuum_freeze_max_age).\n\nFreezing receives priority\n\n(vacuum_failsafe_age).\n\n149\n\nv. �.�\n\nv. ��",
      "page_number": 123
    },
    {
      "number": 7,
      "title": "Freezing",
      "start_page": 147,
      "end_page": 162,
      "detection_method": "regex_chapter",
      "content": "50 million\n\np. ���\n\nChapter 7 Freezing\n\nMinimal Freezing Age\n\nvacuum_freeze_min_age parameter defines the minimal freezing age of xmin The transactions. The lower its value, the higher the overhead: if a row is “hot” and is actively being changed,then freezing all its newly created versions will be a wasted effort. Setting this parameter to a relatively high value allows you to wait for a while.\n\nTo observe the freezing process, let’s reduce this parameter value to one:\n\n=> ALTER SYSTEM SET vacuum_freeze_min_age = 1;\n\n=> SELECT pg_reload_conf();\n\nNow update one row in the zero page. The new row version will get into the same page because the fillfactor value is quite small:\n\n=> UPDATE tfreeze SET s = 'BAR' WHERE id = 1;\n\nThe age of all transactions has been increased by one,and the heap pages now look as follows:\n\n=> SELECT * FROM heap_page('tfreeze',0,1);\n\nctid\n\n| state\n\n| xmin\n\n| xmin_age | xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−−−−+−−−−−−\n\n(0,1) | normal | 856 c | (0,2) | normal | 856 c | (0,3) | normal | 857 | (1,1) | normal | 856 c | (1,2) | normal | 856 c |\n\n2 | 857 2 | 0 a 1 | 0 a 2 | 0 a 2 | 0 a\n\n(5 rows)\n\nAt this point, the tuples that are older than vacuum_freeze_min_age = 1 are subject : to freezing. But vacuum will not process any pages tagged in the visibility map\n\n=> SELECT * FROM generate_series(0,1) g(blkno), pg_visibility_map('tfreeze',g.blkno)\n\nORDER BY g.blkno;\n\nblkno | all_visible | all_frozen −−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0 | f 1 | t\n\n| f | f\n\n(2 rows)\n\n150\n\n7.3 Managing Freezing\n\nThe previous ������ command has removed the visibility bit of the zero page, so the tuple that has an appropriate xmin age in this page will be frozen. But the first page will be skipped altogether:\n\n=> VACUUM tfreeze;\n\n=> SELECT * FROM heap_page('tfreeze',0,1);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmin_age | xmax\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−−−−−+−−−−−−\n\n(0,1) | redirect to 3 | (0,2) | normal (0,3) | normal (1,1) | normal (1,2) | normal\n\n| | 856 f | | 857 c | | 856 c | | 856 c |\n\n|\n\n2 | 0 a 1 | 0 a 2 | 0 a 2 | 0 a\n\n(5 rows)\n\nNow the zero page appears in the visibility map again, and if nothing changes in it, vacuuming will not return to this page anymore:\n\n=> SELECT * FROM generate_series(0,1) g(blkno), pg_visibility_map('tfreeze',g.blkno)\n\nORDER BY g.blkno;\n\nblkno | all_visible | all_frozen −−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0 | t 1 | t\n\n| f | f\n\n(2 rows)\n\nAge for Aggressive Freezing\n\nAs we have just seen, if a page contains only the current tuples that are visible in all snapshots, vacuuming will not freeze them. To overcome this constraint, Post- vacuum_freeze_table_age parameter. It defines the transaction gre��� provides the age that allows vacuuming to ignore the visibility map, so any heap page can be frozen.\n\nFor each table,the system catalog keeps a transaction �� for which it is known that all the older transactions are sure to be frozen. It is stored as relfrozenxid:\n\n151\n\n150 million\n\nv. �.�\n\nChapter 7 Freezing\n\n=> SELECT relfrozenxid, age(relfrozenxid) FROM pg_class WHERE relname = 'tfreeze';\n\nrelfrozenxid | age −−−−−−−−−−−−−−+−−−−−\n\n854 |\n\n4\n\n(1 row)\n\nIt is the age of this transaction that is compared to the vacuum_freeze_table_age value to decide whether the time has come for aggressive freezing.\n\n, there is no need to perform a full table scan during vac- Thanks to the freeze map uuming: it is enough to check only those pages that do not appear in the map. Apart from this important optimization,the freeze map also brings fault tolerance: if vacuuming is interrupted,its next run will not have to get back to the pages that have already been processed and are tagged in the map.\n\nPostgre��� performs aggressive freezing of all pages in a table each time when the number of transactions in the system reaches the vacuum_freeze_table_age − vacuum_freeze_min_age limit (if the default values are used, it happens after each ��� million transactions). Thus, if the vacuum_freeze_min_age value is too big, it can lead to excessive freezing and increased overhead.\n\nTo freeze the whole table, let’s reduce the vacuum_freeze_table_age value to four; then the condition for aggressive freezing will be satisfied:\n\n=> ALTER SYSTEM SET vacuum_freeze_table_age = 4;\n\n=> SELECT pg_reload_conf();\n\nRun the ������ command:\n\n=> VACUUM VERBOSE tfreeze;\n\nINFO: INFO: versions in 50 out of 50 pages DETAIL: Skipped 0 pages due to buffer pins, 0 frozen pages. CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s. VACUUM\n\naggressively vacuuming \"public.tfreeze\" table \"tfreeze\": found 0 removable, 100 nonremovable row\n\n0 dead row versions cannot be removed yet, oldest xmin: 858\n\n152\n\n7.3 Managing Freezing\n\nNow that the whole table has been analyzed, the relfrozenxid value can be ad- vanced—heap pages are guaranteed to have no older unfrozen xmin transactions:\n\n=> SELECT relfrozenxid, age(relfrozenxid) FROM pg_class WHERE relname = 'tfreeze';\n\nrelfrozenxid | age −−−−−−−−−−−−−−+−−−−−\n\n857 |\n\n1\n\n(1 row)\n\nThe first page now contains only frozen tuples:\n\n=> SELECT * FROM heap_page('tfreeze',0,1);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmin_age | xmax\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−−−−−+−−−−−−\n\n(0,1) | redirect to 3 | (0,2) | normal (0,3) | normal (1,1) | normal (1,2) | normal\n\n| | 856 f | | 857 c | | 856 f | | 856 f |\n\n|\n\n2 | 0 a 1 | 0 a 2 | 0 a 2 | 0 a\n\n(5 rows)\n\nBesides, this page is tagged in the freeze map:\n\n=> SELECT * FROM generate_series(0,1) g(blkno),\n\npg_visibility_map('tfreeze',g.blkno)\n\nORDER BY g.blkno;\n\nblkno | all_visible | all_frozen −−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0 | t 1 | t\n\n| f | t\n\n(2 rows)\n\nAge for Forced Autovacuum\n\nSometimes it is not enough to configure the two parameters discussed above to timely freeze tuples. Autovacuum might be switched off, while regular ������ is not being called at all (it is a very bad idea, but technically it is possible). Besides,\n\n153\n\np. ���\n\n200 million\n\nChapter 7 Freezing\n\nsome inactive databases (like template0) may not be vacuumed handle such situations by forcing autovacuum in the aggressive mode.\n\n. Postgre��� can\n\nAutovacuum is forced1 (even if it is switched off) when there is a risk that the autovacu- age of some unfrozen transaction ��s in the database will exceed the um_freeze_max_age value. The decision is taken based on the age of the oldest pg_class.relfrozenxid transaction in all the tables, as all the older transactions are guaranteed to be frozen. The �� of this transaction is stored in the system catalog:\n\n=> SELECT datname, datfrozenxid, age(datfrozenxid) FROM pg_database;\n\ndatname\n\n| datfrozenxid | age\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−+−−−−−\n\npostgres | template1 | template0 | internals |\n\n726 | 132 726 | 132 726 | 132 726 | 132\n\n(4 rows)\n\ndatfrozenxid\n\nall row versions in the database are guaranteed to be frozen\n\nxid\n\nrelfrozenxid of table 1\n\nrelfrozenxid of table 3\n\nrelfrozenxid of table 2\n\nThe autovacuum_freeze_max_age limit is set to � billion transactions (a bit less than half of the circle), while the default value is �� times smaller. It is done for good reason: a big value increases the risk of transaction �� wraparound, as Postgre��� may fail to timely freeze all the required tuples. In this case, the server must stop immediately to prevent possible issues and will have to be restarted by an admin- istrator.\n\n1 backend/access/transam/varsup.c, SetTransactionIdLimit function\n\n154\n\n7.4 Manual Freezing\n\nThe autovacuum_freeze_max_age value also affects the size of ����. There is no needtokeepthestatusoffrozentransactions,andallthetransactionsthatprecede theonewiththeoldestdatfrozenxidintheclusteraresuretobefrozen. Those���� files that are not required anymore are removed by autovacuum.1\n\nChanging the autovacuum_freeze_max_age parameter requires a server restart. However,all the freezing settings discussed above can also be adjusted at the table level via the corresponding storage parameters. Note that the names of all these parameters start with “auto”:\n\nautovacuum_freeze_min_age and toast.autovacuum_freeze_min_age\n\nautovacuum_freeze_table_age and toast.autovacuum_freeze_table_age\n\nautovacuum_freeze_max_age and toast.autovacuum_freeze_max_age\n\nAge for Failsafe Freezing\n\nIf autovacuum is already struggling to prevent transaction �� wraparound and it is clearly a race against time, a safety switch is pulled: autovacuum will ignore the autovacuum_vacuum_cost_delay (vacuum_cost_delay) value and will stop vacuuming indexes to freeze heap tuples as soon as possible.\n\nA failsafe freezing mode2 is enabled if there is a risk that the age of an unfrozen vacuum_failsafe_agevalue. Itisassumed transactioninthedatabasewillexceedthe that this value must be higher than autovacuum_freeze_max_age.\n\n7.4 Manual Freezing\n\nIt is sometimes more convenient to manage freezing manually rather than rely on autovacuum.\n\n1 backend/commands/vacuum.c, vac_truncate_clog function 2 backend/access/heap/vacuumlazy.c, lazy_check_wraparound_failsafe function\n\n155\n\np. ��\n\nv. ��\n\n1.6 billion\n\nv. ��\n\np. ���\n\nChapter 7 Freezing\n\nFreezing by Vacuum\n\nYou can initiate freezing by calling the ������ command with the ������ op- tion. It will freeze all the heap tuples regardless of their transaction age, as if vacuum_freeze_min_age = 0.\n\nof such a call is to freeze heap tuples as soon as possible, it makes If the purpose sense to disable index vacuuming,like it is done in the failsafe mode. You can do it either explicitly, by running the ������ (freeze, index_cleanup false) command, or via the vacuum_index_cleanup storage parameter. It is rather obvious that it should not be done on a regular basis since in this case������ will not be coping well with its main task of page cleanup.\n\nFreezing Data at the Initial Loading\n\nThe data that is not expected to change can be frozen at once, while it is being loaded into the database. It is done by running the ���� command with the ������ option.\n\nTuples can be frozen during the initial loading only if the resulting table has been created or truncated within the same transaction,as both these operations acquire an exclusive lock on the table. This restriction is necessary because frozen tuples are expected to be visible in all snapshots, regardless of the isolation level; other- wise,transactions would suddenly see freshly-frozen tuples right as they are being uploaded. But if the lock is acquired, other transactions will not be able to get access to this table.\n\nNevertheless, it is still technically possible to break isolation. Let’s start a new transaction at the Repeatable Read isolation level in a separate session:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT 1; -- the snapshot is built\n\nTruncatethetfreezetableandinsertnewrowsintothistablewithinthesametrans- action. (If the read-only transaction had already accessed the tfreeze table, the �������� command will be blocked.)\n\n156\n\n7.4 Manual Freezing\n\n=> BEGIN;\n\n=> TRUNCATE tfreeze;\n\n=> COPY tfreeze FROM stdin WITH FREEZE; 1 FOO 2 BAR 3 BAZ \\.\n\n=> COMMIT;\n\nNow the reading transaction sees the new data as well:\n\n=> SELECT count(*) FROM tfreeze;\n\ncount −−−−−−−\n\n3\n\n(1 row)\n\n=> COMMIT;\n\nIt does break isolation, but since data loading is unlikely to happen regularly, in most cases it will not cause any issues.\n\nIf you load data with freezing ers receive the visibility attribute:\n\n, the visibility map is created at once, and page head-\n\n=> SELECT * FROM pg_visibility_map('tfreeze',0);\n\nall_visible | all_frozen −−−−−−−−−−−−−+−−−−−−−−−−−−\n\nt\n\n| t\n\n(1 row)\n\n=> SELECT flags & 4 > 0 AS all_visible FROM page_header(get_raw_page('tfreeze',0));\n\nall_visible −−−−−−−−−−−−−\n\nt\n\n(1 row)\n\nif the data has been loaded with freezing, the table will not be processed by Thus, vacuum (as long as the data remains unchanged). Unfortunately,this functionality is not supported for ����� tables yet: if an oversized value is loaded, vacuum will have to rewrite the whole ����� table to set visibility attributes in all page headers.\n\n157\n\nv. �� p. ���\n\nv. ��\n\np. ���\n\n8\n\nRebuilding Tables and Indexes\n\n8.1 Full Vacuuming\n\nWhy is Routine Vacuuming not Enough?\n\nRoutine vacuuming can free more space than page pruning, but sometimes it may still be not enough.\n\nIf table or index files have grown in size, ������ can clean up some space within pages, but it can rarely reduce the number of pages. The reclaimed space can only be returned to the operating system if several empty pages appear at the very end of the file, which does not happen too often.\n\nAn excessive size can lead to unpleasant consequences:\n\nFull table (or index) scan will take longer.\n\nA bigger buffer cache may be required (pages are cached as a whole, so data\n\ndensity decreases).\n\nB-trees can get an extra level, which slows down index access.\n\nFiles take up extra space on disk and in backups.\n\nIf the fraction of useful data in a file has dropped below some reasonable level, an administrator can perform full vacuuming by running the ������ ���� command.1 In this case, the table and all its indexes are rebuilt from scratch, and the data is packed as densely as possible (taking the fillfactor\n\nparameter into account).\n\n1 postgresql.org/docs/14/routine-vacuuming.html#VACUUM-FOR-SPACE-RECOVERY\n\n158\n\n8.1 Full Vacuuming\n\nWhen full vacuuming is performed, Postgre��� first fully rebuilds the table and then each of its indexes. While an object is being rebuilt, both old and new files have to be stored on disk,1 so this process may require a lot of free space.\n\nYou should also keep in mind that this operation fully blocks access to the table, both for reads and writes.\n\nEstimating Data Density\n\nFor the purpose of illustration, let’s insert some rows into the table:\n\n=> TRUNCATE vac;\n\n=> INSERT INTO vac(id,s)\n\nSELECT id, id::text FROM generate_series(1,500000) id;\n\nStorage density can be estimated using the pgstattuple extension:\n\n=> CREATE EXTENSION pgstattuple;\n\n=> SELECT * FROM pgstattuple('vac') \\gx\n\n−[ RECORD 1 ]−−−−−−+−−−−−−−−− | 70623232 table_len | 500000 tuple_count | 64500000 tuple_len | 91.33 tuple_percent | 0 dead_tuple_count | 0 dead_tuple_len dead_tuple_percent | 0 free_space free_percent\n\n| 381844 | 0.54\n\nThe function reads the whole table and displays statistics on space distribution in its files. The tuple_percent field shows the percentage of space taken up by use- ful data (heap tuples). This value is inevitably less than ���% because of various metadata within pages, but in this example it is still quite high.\n\nFor indexes, the displayed information differs a bit, but the avg_leaf_density field has the same meaning: it shows the percentage of useful data (in �-tree leaf pages).\n\n1 backend/commands/cluster.c\n\n159\n\nChapter 8 Rebuilding Tables and Indexes\n\n=> SELECT * FROM pgstatindex('vac_s') \\gx\n\n−[ RECORD 1 ]−−−−−−+−−−−−−−−−− | 4 version | 3 tree_level | 114302976 index_size | 2825 root_block_no | 376 internal_pages | 13576 leaf_pages | 0 empty_pages | 0 deleted_pages avg_leaf_density | 53.88 leaf_fragmentation | 10.59\n\nThe previously used pgstattuple functions read the table or index in full to get the precise statistics. For large objects, it can turn out to be too expensive, so the extension also provides another function called pgstattuple_approx, which skips the pages tracked in the visibility map to show approximate figures.\n\nAfaster but even less accurate method is to roughly estimate the ratio between the data volume and the file size using the system catalog.1\n\nHere are the current sizes of the table and its index:\n\n=> SELECT pg_size_pretty(pg_table_size('vac')) AS table_size, pg_size_pretty(pg_indexes_size('vac')) AS index_size;\n\ntable_size | index_size −−−−−−−−−−−−+−−−−−−−−−−−−\n\n67 MB (1 row)\n\n| 109 MB\n\nNow let’s delete ��% of all the rows:\n\n=> DELETE FROM vac WHERE id % 10 != 0;\n\nDELETE 450000\n\nRoutine vacuuming does not affect the file size because there are no empty pages at the end of the file:\n\n=> VACUUM vac;\n\n1 wiki.postgresql.org/wiki/Show_database_bloat\n\n160\n\n8.1 Full Vacuuming\n\n=> SELECT pg_size_pretty(pg_table_size('vac')) AS table_size, pg_size_pretty(pg_indexes_size('vac')) AS index_size;\n\ntable_size | index_size −−−−−−−−−−−−+−−−−−−−−−−−−\n\n67 MB (1 row)\n\n| 109 MB\n\nHowever, data density has dropped about �� times:\n\n=> SELECT vac.tuple_percent, vac_s.avg_leaf_density FROM pgstattuple('vac') vac, pgstatindex('vac_s') vac_s;\n\ntuple_percent | avg_leaf_density −−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−\n\n9.13 |\n\n6.71\n\n(1 row)\n\nThe table and the index are currently located in the following files:\n\n=> SELECT pg_relation_filepath('vac') AS vac_filepath,\n\npg_relation_filepath('vac_s') AS vac_s_filepath \\gx\n\n−[ RECORD 1 ]−−+−−−−−−−−−−−−−−−−− vac_filepath | base/16391/16514 vac_s_filepath | base/16391/16515\n\nLet’s check what we will get after ������ ����. While the command is running, its progress can be tracked in the pg_stat_progress_cluster view (which is similar to the pg_stat_progress_vacuum view provided for ������):\n\n=> VACUUM FULL vac;\n\n=> SELECT * FROM pg_stat_progress_cluster \\gx\n\n−[ RECORD 1 ]−−−−−−−+−−−−−−−−−−−−−−−−− pid datid datname relid command phase cluster_index_relid | 0 heap_tuples_scanned | 50000 heap_tuples_written | 50000 heap_blks_total heap_blks_scanned index_rebuild_count | 0\n\n| 19488 | 16391 | internals | 16479 | VACUUM FULL | rebuilding index\n\n| 8621 | 8621\n\n161\n\nv. ��\n\nChapter 8 Rebuilding Tables and Indexes\n\nExpectedly, ������ ���� phases1 differ from those of routine vacuuming.\n\nFull vacuuming has replaced old files with new ones:\n\n=> SELECT pg_relation_filepath('vac') AS vac_filepath,\n\npg_relation_filepath('vac_s') AS vac_s_filepath \\gx\n\n−[ RECORD 1 ]−−+−−−−−−−−−−−−−−−−− | base/16391/16526 vac_filepath vac_s_filepath | base/16391/16529\n\nBoth index and table sizes are much smaller now:\n\n=> SELECT pg_size_pretty(pg_table_size('vac')) AS table_size, pg_size_pretty(pg_indexes_size('vac')) AS index_size;\n\ntable_size | index_size −−−−−−−−−−−−+−−−−−−−−−−−−\n\n6904 kB\n\n| 6504 kB\n\n(1 row)\n\nAs a result, data density has increased. For the index, it is even higher than the original one: it is more efficient to create a �-tree from scratch based on the avail- able data than to insert entries row by row into an already existing index:\n\n=> SELECT vac.tuple_percent,\n\nvac_s.avg_leaf_density\n\nFROM pgstattuple('vac') vac,\n\npgstatindex('vac_s') vac_s;\n\ntuple_percent | avg_leaf_density −−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−\n\n91.23 |\n\n91.08\n\n(1 row)\n\nFreezing\n\nWhen the table is being rebuilt, Postgre��� freezes its tuples because this opera- tion costs almost nothing compared to the rest of the work:\n\n1 postgresql.org/docs/14/progress-reporting.html#CLUSTER-PHASES\n\n162\n\n8.1 Full Vacuuming\n\n=> SELECT * FROM heap_page('vac',0,0) LIMIT 5;\n\nctid\n\n| state\n\n| xmin\n\n| xmin_age | xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−−−−+−−−−−−\n\n(0,1) | normal | 861 f | (0,2) | normal | 861 f | (0,3) | normal | 861 f | (0,4) | normal | 861 f | (0,5) | normal | 861 f |\n\n5 | 0 a 5 | 0 a 5 | 0 a 5 | 0 a 5 | 0 a\n\n(5 rows)\n\nBut pages are registered neither in the visibility map nor in the freeze map,and the page header does not receive the visibility attribute (as it happens when the ���� command is executed with the ������ option\n\n):\n\n=> SELECT * FROM pg_visibility_map('vac',0);\n\nall_visible | all_frozen −−−−−−−−−−−−−+−−−−−−−−−−−−\n\nf\n\n| f\n\n(1 row)\n\n=> SELECT flags & 4 > 0 all_visible FROM page_header(get_raw_page('vac',0));\n\nall_visible −−−−−−−−−−−−−\n\nf\n\n(1 row)\n\nThe situation improves only after ������ is called (or autovacuum is triggered):\n\n=> VACUUM vac;\n\n=> SELECT * FROM pg_visibility_map('vac',0);\n\nall_visible | all_frozen −−−−−−−−−−−−−+−−−−−−−−−−−−\n\nt\n\n| t\n\n(1 row)\n\n=> SELECT flags & 4 > 0 AS all_visible FROM page_header(get_raw_page('vac',0));\n\nall_visible −−−−−−−−−−−−−\n\nt\n\n(1 row)\n\n163\n\np. ���\n\np. ���\n\np. ���\n\np. ��\n\np. ���\n\nChapter 8 Rebuilding Tables and Indexes\n\nIt essentially means that even if all tuples in a page are beyond the database hori- zon, such a page will still have to be rewritten.\n\n8.2 Other Rebuilding Methods\n\nAlternatives to Full Vacuuming\n\nIn addition to������ ����,there are several other commands that can fully rebuild tablesandindexes. Allofthemexclusivelylockthetable,allofthemdeleteolddata files and recreate them anew.\n\nThe ������� command is fully analogous to ������ ����,but it also reorders tuples in files based on one of the available indexes. In some cases,it can help the planner use index scans more efficiently . But you should bear in mind that clusterization is not supported: all further table updates will be breaking the physical order of tuples.\n\nProgrammatically, ������ ���� is simply a special instance of the ������� com- mand that does not require tuple reordering.1\n\nThe ������� command rebuilds one or more indexes.2 In fact, ������ ���� and ������� use this command under the hood when rebuilding indexes.\n\nThe �������� command3 deletes all table rows; it is a logical equivalent of ������ runwithoutthe�����clause. Butwhile������ simplymarksheaptuplesasdeleted (so they still have to be vacuumed), �������� creates a new empty file, which is usually faster.\n\nReducing Downtime During Rebuilding\n\n������ ���� is not meant to be run regularly, as it exclusively locks the table (even for queries) for the whole duration of its operation. This is usually not an option for highly available systems.\n\n1 backend/commands/cluster.c 2 backend/commands/indexcmds.c 3 backend/commands/tablecmds.c, ExecuteTruncate function\n\n164\n\n8.3 Precautions\n\nThere are several extensions (such as pg_repack1) that can rebuild tables and in- dexeswithalmostzerodowntime. Anexclusivelockisstillrequired,butonlyatthe beginning and at the end of this process, and only for a short time. It is achieved by a more complex implementation: all the changes made on the original table while it is being rebuilt are saved by a trigger and then applied to the new table. To complete the operation, the utility replaces one table with the other in the system catalog.\n\nAn unconventional solution is offered by the pgcompacttable utility.2 It performs multiple fake row updates (that do not change any data) so that current row ver- sions gradually move towards the start of the file.\n\nBetween these update series, vacuuming removes outdated tuples and truncates the file little by little. This approach takes much more time and resources, but it requires no extra space for rebuilding the table and does not lead to load spikes. Short-time exclusive locks are still acquired while the table is being truncated, but vacuuming handles them rather smoothly.\n\n8.3 Precautions\n\nRead-Only Queries\n\nOne of the reasons for file bloating is executing long-running transactions that hold the database horizon\n\nalongside intensive data updates.\n\nAs such, long-running (read-only) transactions do not cause any issues. So a com- monapproachistosplittheloadbetweendifferentsystems: keepfast����queries on the primary server and direct all ���� transactions to a replica. Although it makes the solution more expensive and complicated, such measures may turn out to be indispensable.\n\nIn some cases, long transactions are the result of application or driver bugs rather than a necessity. If an issue cannot be resolved in a civilized way,the administrator can resort to the following two parameters:\n\n1 github.com/reorg/pg_repack 2 github.com/dataegret/pgcompacttable\n\n165\n\np. ���\n\np. ���",
      "page_number": 147
    },
    {
      "number": 8,
      "title": "Rebuilding Tables and Indexes",
      "start_page": 163,
      "end_page": 172,
      "detection_method": "regex_chapter",
      "content": "v. �.�\n\nv. �.�\n\nChapter 8 Rebuilding Tables and Indexes\n\nThe old_snapshot_threshold\n\nparameter defines the maximum lifetime of a snapshot. Once this time is up, the server has the right to remove outdated tuples; if a long-running transaction still requires them, it will get an error (“snapshot too old”).\n\nTheidle_in_transaction_session_timeout\n\nparameterlimitsthelifetimeofanidle\n\ntransaction. The transaction is aborted upon reaching this threshold.\n\nData Updates\n\nAnother reason for bloating is simultaneous modification of a large number of tu- ples. If alltable rowsget updated,thenumber oftuples candouble,and vacuuming will not have enough time to interfere. Page pruning can reduce this problem, but not resolve it entirely.\n\nLet’s extend the output with another column to keep track of the processed rows:\n\n=> ALTER TABLE vac ADD processed boolean DEFAULT false;\n\n=> SELECT pg_size_pretty(pg_table_size('vac'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n6936 kB\n\n(1 row)\n\nOnce all the rows are updated, the table gets almost two times bigger:\n\n=> UPDATE vac SET processed = true;\n\nUPDATE 50000\n\n=> SELECT pg_size_pretty(pg_table_size('vac'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n14 MB (1 row)\n\nTo address this situation, you can reduce the number of changes performed by a single transaction, spreading them out over time; then vacuuming can delete out- dated tuples and free some space for new ones within the already existing pages. Assuming that each row update can be committed separately, we can use the fol- lowing query that selects a batch of rows of the specified size as a template:\n\n166\n\n8.3 Precautions\n\nSELECT ID FROM table WHERE filtering the already processed rows LIMIT batch size FOR UPDATE SKIP LOCKED\n\nThis code snippet selects and immediately locks a set of rows that does not ex- ceed the specified size. The rows that are already locked by other transactions are skipped : they will get into another batch next time. It is a rather flexible and con- venient solution that allows you to easily change the batch size and restart the operation in case of a failure. Let’s unset the processed attribute and perform full vacuuming to restore the original size of the table:\n\n=> UPDATE vac SET processed = false;\n\n=> VACUUM FULL vac;\n\nOnce the first batch is updated, the table size grows a bit:\n\n=> WITH batch AS (\n\nSELECT id FROM vac WHERE NOT processed LIMIT 1000 FOR UPDATE SKIP LOCKED\n\n) UPDATE vac SET processed = true WHERE id IN (SELECT id FROM batch);\n\nUPDATE 1000\n\n=> SELECT pg_size_pretty(pg_table_size('vac'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n7064 kB\n\n(1 row)\n\nBut from now on,the size remains almost the same because new tuples replace the removed ones:\n\n=> VACUUM vac;\n\n=> WITH batch AS (\n\nSELECT id FROM vac WHERE NOT processed LIMIT 1000 FOR UPDATE SKIP LOCKED\n\n) UPDATE vac SET processed = true WHERE id IN (SELECT id FROM batch);\n\nUPDATE 1000\n\n167\n\np. ���\n\nChapter 8 Rebuilding Tables and Indexes\n\n=> SELECT pg_size_pretty(pg_table_size('vac'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n7072 kB\n\n(1 row)\n\n168\n\nPart II\n\nBuffer Cache and WAL\n\n9\n\nBuffer Cache\n\n9.1 Caching\n\nIn modern computing systems, caching is omnipresent—both at the hardware and at the software level. The processor alone can have up to three or four levels of cache. R��� controllers and disks add their own cache too.\n\nCaching is used to even out performance difference between fast and slow types of memory. Fast memory is expensive and has smaller volume, while slow memory is bigger and cheaper. Therefore, fast memory cannot accommodate all the data stored in slow memory. But in most cases only a small portion of data is being actively used at each particular moment,so allocating some fast memory for cache to keep hot data can significantly reduce the overhead incurred by slow memory access.\n\nIn Postgre���, buffer cache1 holds relation pages, thus balancing access times to disks (milliseconds) and ��� (nanoseconds).\n\nThe operating system has its own cache that serves the same purpose. For this reason, database systems are usually designed to avoid double caching: the data stored on disk is usually queried directly, bypassing the �� cache. But Postgre��� uses a different approach: it reads and writes all data via buffered file operations.\n\nIt will reduce the overhead, as Double caching can be avoided if you apply direct �/�. Postgre��� will use direct memory access (���) instead of copying buffered pages into the �� address space; besides, you will gain immediate control over physical writes on disk. However, direct �/� does not support data prefetching enabled by bufferization, so , which requires massive code you have to implement it separately via asynchronous �/�\n\n1 backend/storage/buffer/README\n\n171\n\np. ���\n\nChapter 9 Buffer Cache\n\nmodifications in Postgre��� core, as well as handling �� incompatibilities when it comes to direct and asynchronous �/� support. But once the asynchronous communication is set up,you can enjoy additional benefits of no-wait disk access.\n\nThe Postgre��� community has already started this major effort,1 but it will take a long time for the actual results to appear.\n\n9.2 Buffer Cache Design\n\nBuffer cache is located in the server’s shared memory and is accessible to all the processes. It takes the major part of the shared memory and is surely one of the most important and complex data structures in Postgre���. Understanding how cache works is important in its own right, but even more so as many other struc- tures (such as subtransactions, ���� transaction status, and ��� entries) use a similar caching mechanism, albeit a simpler one.\n\nThe name of this cache is inspired by its inner structure,as it consists of an array of buffers. Each buffer reserves a memory chuck that can accommodate a single data page together with its header.2\n\nbuffer cache\n\nheader\n\npage\n\nA header contains some information about the buffer and the page in it, such as:\n\nphysical location of the page (file ��, fork, and block number in the fork)\n\nthe attribute showing that the data in the page has been modified and sooner\n\nor later has to be written back to disk (such a page is called dirty)\n\nbuffer usage count\n\npin count (or reference count)\n\n1 www.postgresql.org/message-id/flat/20210223100344.llw5an2aklengrmn%40alap3.anarazel.de 2 include/storage/buf_internals.h\n\n172\n\n9.2 Buffer Cache Design\n\nTo get access to a relation’s data page, a process requests it from the buffer man- ager1 and receives the �� of the buffer that contains this page. Then it reads the cached data and modifies it right in the cache if needed. While the page is in use, its buffer is pinned. Pins forbid eviction of the cached page and can be applied together with other locks\n\n. Each pin increments the usage count as well.\n\nAs long as the page is cached, its usage does not incur any file operations.\n\nWe can explore the buffer cache using the pg_buffercache extension:\n\n=> CREATE EXTENSION pg_buffercache;\n\nLet’s create a table and insert a row:\n\n=> CREATE TABLE cacheme(\n\nid integer\n\n) WITH (autovacuum_enabled = off);\n\n=> INSERT INTO cacheme VALUES (1);\n\nNow the buffer cache contains a heap page with the newly inserted row. You can see it for yourself by selecting all the buffers related to a particular table. We will need such a query again, so let’s wrap it into a function:\n\n=> CREATE FUNCTION buffercache(rel regclass) RETURNS TABLE(\n\nbufferid integer, relfork text, relblk bigint, isdirty boolean, usagecount smallint, pins integer\n\n) AS $$ SELECT bufferid,\n\nCASE relforknumber\n\nWHEN 0 THEN 'main' WHEN 1 THEN 'fsm' WHEN 2 THEN 'vm'\n\nEND, relblocknumber, isdirty, usagecount, pinning_backends FROM pg_buffercache WHERE relfilenode = pg_relation_filenode(rel) ORDER BY relforknumber, relblocknumber; $$ LANGUAGE sql;\n\n1 backend/storage/buffer/bufmgr.c\n\n173\n\np. ���\n\nChapter 9 Buffer Cache\n\n=> SELECT * FROM buffercache('cacheme');\n\nbufferid | relfork | relblk | isdirty | usagecount | pins −−−−−−−−−−+−−−−−−−−−+−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−−+−−−−−− 0 | t\n\n268 | main\n\n|\n\n|\n\n1 |\n\n0\n\n(1 row)\n\nThe page is dirty: it has been modified, but is not written to disk yet. Its usage count is set to one.\n\n9.3 Cache Hits\n\nWhen the buffer manager has to read a page,1 it first checks the buffer cache.\n\nAll buffer ��s are stored in a hash table,2 which is used to speed up their search.\n\nMany modern programming languages include hash tables as one of the basic data types. Hash tables are often called associative arrays, and indeed, from the user’s perspective they do look like an array; however, their index (a hash key) can be of any data type, for example, a text string rather than an integer.\n\nWhile the range of possible key values can be quite large, hash tables never contain that many different values at a time. The idea of hashing is to convert a key value into an integer number using a hash function. This number (or some of its bits) is used as an index of a regular array. The elements of this array are called hash table buckets.\n\nA good hash function distributes hash keys between buckets more or less uniformly,but it can still assign the same number to different keys,thus placing them into the same bucket; it is called a collision. For this reason,values are stored in buckets together with hash keys; to access a hashed value by its key, Postgre��� has to check all the keys in the bucket.\n\nThere are multiple implementations of hash tables; of all the possible options,the buffer cache uses the extendible table that resolves hash collisions by chaining.3\n\nA hash key consists of the �� of the relation file, the type of the fork, and the �� of the page within this fork’s file. Thus, knowing the page, Postgre��� can quickly find the buffer containing this page or make sure that the page is not currently cached.\n\n1 backend/storage/buffer/bufmgr.c, ReadBuffer_common function 2 backend/storage/buffer/buf_table.c 3 backend/utils/hash/dynahash.c\n\n174\n\n9.3 Cache Hits\n\n3501,0,3\n\n2610,0,7\n\nhash table\n\nThe buffer cache implementation has long been criticized for relying on a hash table: this structure is of no use when it comes to finding all the buffers taken bypages of a particular relation,which is required to remove pages from cache when running ���� and �������� commands or truncating a table duringvacuuming.1 Yet no one has suggested an adequate alternative so far.\n\nIfthehashtablecontainstherequiredbuffer��,thebuffermanagerpinsthisbuffer and returns its �� to the process. Then this process can start using the cached page without incurring any �/� traffic.\n\nTo pin a buffer, Postgre��� has to increment the pin counter in its header; a buffer can be pinned by several processes at a time. While its pin counter is greater than zero, the buffer is assumed to be in use, and no radical changes in its contents are allowed. For example, a new tuple can appear (it will be invisible following the visibility rules), but the page itself cannot be replaced.\n\nWhen run with the analyze and buffers options, the ������� command executes the displayed query plan and shows the number of used buffers:\n\n1 backend/storage/buffer/bufmgr.c, DropRelFileNodeBuffers function\n\n175\n\nChapter 9 Buffer Cache\n\n=> EXPLAIN (analyze, buffers, costs off, timing off, summary off)\n\nSELECT * FROM cacheme;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Seq Scan on cacheme (actual rows=1 loops=1)\n\nBuffers: shared hit=1\n\nPlanning:\n\nBuffers: shared hit=12 read=7\n\n(4 rows)\n\nHere hit=1 means that the only page that had to be read was found in the cache.\n\nBuffer pinning increases the usage count by one:\n\n=> SELECT * FROM buffercache('cacheme');\n\nbufferid | relfork | relblk | isdirty | usagecount | pins −−−−−−−−−−+−−−−−−−−−+−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−−+−−−−−− 0 | t\n\n268 | main\n\n|\n\n|\n\n2 |\n\n0\n\n(1 row)\n\nTo observe pinning in action during query execution, let’s open a cursor—it will hold the buffer pin, as it has to provide quick access to the next row in the result set:\n\n=> BEGIN;\n\n=> DECLARE c CURSOR FOR SELECT * FROM cacheme;\n\n=> FETCH c;\n\nid −−−− 1\n\n(1 row)\n\n=> SELECT * FROM buffercache('cacheme');\n\nbufferid | relfork | relblk | isdirty | usagecount | pins −−−−−−−−−−+−−−−−−−−−+−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−−+−−−−−− 0 | t\n\n268 | main\n\n|\n\n|\n\n3 |\n\n1\n\n(1 row)\n\nIf a process cannot use a pinned buffer, it usually skips it and simply chooses an- other one. We can see it during table vacuuming:\n\n176",
      "page_number": 163
    },
    {
      "number": 9,
      "title": "Buffer Cache",
      "start_page": 173,
      "end_page": 192,
      "detection_method": "regex_chapter",
      "content": "9.3 Cache Hits\n\n=> VACUUM VERBOSE cacheme;\n\nINFO: INFO: versions in 1 out of 1 pages DETAIL: 877 Skipped 1 page due to buffer pins, 0 frozen pages. CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s. VACUUM\n\nvacuuming \"public.cacheme\" table \"cacheme\": found 0 removable, 0 nonremovable row\n\n0 dead row versions cannot be removed yet, oldest xmin:\n\nThe page was skipped because its tuples could not be physically removed from the pinned buffer.\n\nBut if it is exactly this buffer that is required,the process joins the queue and waits for exclusive access to this buffer. An example of such an operation is vacuuming with freezing.1\n\nOnce the cursor closes or moves on to another page, the buffer gets unpinned. In this example, it happens at the end of the transaction:\n\n=> COMMIT;\n\n=> SELECT * FROM buffercache('cacheme');\n\nbufferid | relfork | relblk | isdirty | usagecount | pins −−−−−−−−−−+−−−−−−−−−+−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−−+−−−−−− 0 | t 0 | f\n\n268 | main 310 | vm\n\n| |\n\n| |\n\n3 | 2 |\n\n0 0\n\n(2 rows)\n\nPage modifications are protected by the same pinning mechanism. For example, let’s insert another row into the table (it will get into the same page):\n\n=> INSERT INTO cacheme VALUES (2);\n\n=> SELECT * FROM buffercache('cacheme');\n\nbufferid | relfork | relblk | isdirty | usagecount | pins −−−−−−−−−−+−−−−−−−−−+−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−−+−−−−−− 0 | t 0 | f\n\n268 | main 310 | vm\n\n| |\n\n| |\n\n4 | 2 |\n\n0 0\n\n(2 rows)\n\n1 backend/storage/buffer/bufmgr.c, LockBufferForCleanup function\n\n177\n\np. ���\n\np. ��\n\nChapter 9 Buffer Cache\n\nPostgre��� does not perform any immediate writes to disk: a page remains dirty in the buffer cache for a while, providing some performance gains for both reads and writes.\n\n9.4 Cache Misses\n\nIf the hash table has no entry related to the queried page, it means that this page is not cached. In this case, a new buffer is assigned (and immediately pinned), the page is read into this buffer,and the hash table references are modified accordingly.\n\nLet’s restart the instance to clear its buffer cache:\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\nAn attempt to read a page will result in a cache miss, and the page will be loaded into a new buffer:\n\n=> EXPLAIN (analyze, buffers, costs off, timing off, summary off)\n\nSELECT * FROM cacheme;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Seq Scan on cacheme (actual rows=2 loops=1)\n\nBuffers: shared read=1 dirtied=1\n\nPlanning:\n\nBuffers: shared hit=15 read=7\n\n(4 rows)\n\nInstead of hit, the plan now shows the read status, which denotes a cache miss. . Besides, this page has become dirty, as the query has modified some hint bits\n\nA buffer cache query shows that the usage count for the newly added page is set to one:\n\n=> SELECT * FROM buffercache('cacheme');\n\nbufferid | relfork | relblk | isdirty | usagecount | pins −−−−−−−−−−+−−−−−−−−−+−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−−+−−−−−− 0 | t\n\n98 | main\n\n|\n\n|\n\n1 |\n\n0\n\n(1 row)\n\n178\n\n9.4 Cache Misses\n\nThepg_statio_all_tablesviewcontainsthecompletestatisticsonbuffercacheusage by tables:\n\n=> SELECT heap_blks_read, heap_blks_hit FROM pg_statio_all_tables WHERE relname = 'cacheme';\n\nheap_blks_read | heap_blks_hit −−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−− 2 |\n\n5\n\n(1 row)\n\nPostgre���providessimilarviewsforindexesandsequences. Theycanalsodisplay statistics on �/� operations, but only if\n\ntrack_io_timing is enabled.\n\nBuffer Search and Eviction\n\nChoosing a buffer for a page is not so trivial.1 There are two possible scenarios:\n\n1. Right after the server start all the buffers are empty and are bound into a list.\n\nWhile some buffers are still free, the next page read from disk will occupy the first buffer, and it will be removed from the list.\n\nA buffer can return to the list2 only if its page disappears, without being re- placed by another page. It can happen if you call ���� or �������� commands, or if the table is truncated during vacuuming.\n\n2. Sooner or later no free buffers will be left (since the size of the database is usually bigger than the memory chunk allocated for cache). Then the buffer manager will have to select one of the buffers that is already in use and evict the cached page from this buffer. It is performed using the clock sweep algo- rithm, which is well illustrated by the clock metaphor. Pointing to one of the buffers, the clock hand starts going around the buffer cache and reduces the usage countforeach cached page by one as it passes. The first unpinned buffer with the zero count found by the clock hand will be cleared.\n\n1 backend/storage/buffer/freelist.c, StrategyGetBuffer function 2 backend/storage/buffer/freelist.c, StrategyFreeBuffer function\n\n179\n\noff\n\np. ���\n\nChapter 9 Buffer Cache\n\nThus, the usage count is incremented each time the buffer is accessed (that is, pinned), and reduced when the buffer manager is searching for pages to evict. Asaresult,theleastrecentlyusedpagesareevictedfirst,whilethosethathave been accessed more often will remain in the cache longer.\n\nAs you can guess,if all the buffers have a non-zero usage count,the clock hand has to complete more than one full circle before any of them finally reaches the zero value. To avoid running multiple laps, Postgre��� limits the usage count by �.\n\nOnce the buffer to evict is found, the reference to the page that is still in this buffer must be removed from the hash table.\n\nBut if this buffer is dirty, that is, it contains some modified data, the old page cannot be simply thrown away—the buffer manager has to write it to disk first.\n\nfree buffers\n\nclock hand\n\nThen the buffer manager reads a new page into the found buffer—no matter if it had to be cleared or was still free. It uses buffered �/� for this purpose, so the page will be read from disk only if the operating system cannot find it in its own cache.\n\n180\n\n9.5 Bulk Eviction\n\nThose database systems that use direct �/� and do not depend on the �� cache differentiate between logical reads (from ���, that is, from the buffer cache) and physical reads (from disk). From the standpoint of Postgre���, a page can be either read from the buffer cache or requested from the operating system, but there is no way to tell whether it was found in ��� or read from disk in the latter case.\n\nThe hash table is updated to refer to the new page, and the buffer gets pinned. Its usage count is incremented and is now set to one, which gives this buffer some time to increase this value while the clock hand is traversing the buffer cache.\n\n9.5 Bulk Eviction\n\nIf bulk reads or writes are performed, there is a risk that one-time data can quickly oust useful pages from the buffer cache.\n\nAs a precaution, bulk operations use rather small buffer rings, and eviction is per- formed within their boundaries, without affecting other buffers.\n\nAlongside the“buffer ring,”the code also uses the term“ring buffer”. However,this synonym is rather ambiguous because the ring buffer itself consists of several buffers (that belong to the buffer cache). The term “buffer ring”is more accurate in this respect.\n\nA buffer ring of a particular size consists of an array of buffers that are used one after another. At first,the buffer ring is empty,and individual buffers join it one by one, after being selected from the buffer cache in the usual manner. Then eviction comes into play, but only within the ring limits.1\n\nBuffers added into a ring are not excluded from the buffer cache and can still be used by other operations. So if the buffer to be reused turns out to be pinned, or its usage count is higher than one, it will be simply detached from the ring and replaced by another buffer.\n\nPostgre��� supports three eviction strategies.\n\nBulk reads strategy is used for sequential scans\n\nof large tables if their size exceeds\n\n1 4\n\nof the buffer cache. The ring buffer takes ��� k� (�� standard pages).\n\n1 backend/storage/buffer/freelist.c, GetBufferFromRing function\n\n181\n\np. ���\n\np. ��\n\nChapter 9 Buffer Cache\n\nThisstrategydoesnotallowwritingdirtypagestodisktofreeabuffer; instead, the buffer is excluded from the ring and replaced by another one. As a result, reading does not have to wait for writing to complete,so it is performed faster.\n\nIf it turns out that the table is already being scanned, the process that starts another scan joins the existing buffer ring and gets access to the currently available data,without incurring extra �/� operations.1 When the first process completes the scan, the second one gets back to the skipped part of the table.\n\nBulk writes strategy is applied by ���� ����, ������ ����� �� ������, and ������ ��- ���������� ���� commands, as well as by those ����� ����� flavors that cause table rewrites. The allocated ring is quite big, its default size being �� �� (���� standard pages), but it never exceeds 1 of the total size of the buffer 8 cache.\n\nVacuuming strategy is used by the process of vacuuming when it performs a full table scan without taking the visibility map into account. The ring buffer is assigned ��� k� of ��� (�� standard pages).\n\nBuffer rings do not always prevent undesired eviction. If ������ or ������ com- mands affect a lot of rows,the performed table scan applies the bulk reads strategy, but since the pages are constantly being modified, buffer rings virtually become useless.\n\nAnother example worth mentioning is storing oversized data in ����� tables. In spite of a potentially large volume of data that has to be read, toasted values are always accessed via an index, so they bypass buffer rings.\n\nLet’s take a closer look at the bulk reads strategy. For simplicity, we will create a tableinsuchawaythataninsertedrowtakesthewholepage. Bydefault,thebuffer cache size is ��,��� pages,� k� each. So the table must take more than ���� pages for the scan to use a buffer ring.\n\n=> CREATE TABLE big(\n\nid integer PRIMARY KEY GENERATED ALWAYS AS IDENTITY, s char(1000)\n\n) WITH (fillfactor = 10);\n\n1 backend/access/common/syncscan.c\n\n182\n\n9.5 Bulk Eviction\n\n=> INSERT INTO big(s)\n\nSELECT 'FOO' FROM generate_series(1,4096+1);\n\nLet’s analyze the table:\n\n=> ANALYZE big;\n\n=> SELECT relname, relfilenode, relpages FROM pg_class WHERE relname IN ('big', 'big_pkey');\n\nrelname\n\n| relfilenode | relpages\n\n−−−−−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−\n\n| big big_pkey |\n\n16546 | 16551 |\n\n4097 14\n\n(2 rows)\n\nRestart the server to clear the cache,as now it contains some heap pages that have been read during analysis.\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\nOnce the server is restarted, let’s read the whole table:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT id FROM big;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Seq Scan on big (actual rows=4097 loops=1)\n\n(1 row)\n\nHeap pages take only �� buffers, which make up the buffer ring for this operation:\n\n=> SELECT count(*) FROM pg_buffercache WHERE relfilenode = pg_relation_filenode('big'::regclass);\n\ncount −−−−−−−\n\n32\n\n(1 row)\n\nBut in the case of an index scan the buffer ring is not used:\n\n183\n\n128MB\n\nChapter 9 Buffer Cache\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM big ORDER BY id;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using big_pkey on big (actual rows=4097 loops=1)\n\n(1 row)\n\nAs a result, the buffer cache ends up containing the whole table and the whole index:\n\n=> SELECT relfilenode, count(*) FROM pg_buffercache WHERE relfilenode IN (\n\npg_relation_filenode('big'), pg_relation_filenode('big_pkey')\n\n) GROUP BY relfilenode;\n\nrelfilenode | count −−−−−−−−−−−−−+−−−−−−− 16546 | 16551 |\n\n4097 14\n\n(2 rows)\n\n9.6 Choosing the Buffer Cache Size\n\nshared_buffers parameter. Its default The size of the buffer cache is defined by the value is known to be low,so it makes sense to increase it right after the Postgre��� installation. You will have to reload the server in this case because shared memory is allocated for cache at the server start.\n\nBut how can we determine an appropriate value?\n\nEven a very large database has a limited set of hot data that is being used simulta- neously. In the perfect world,it is this set that must fit the buffer cache (with some space being reserved for one-time data). If the cache size is smaller, the actively used pages will be evicting each other all the time,thus leading to excessive �/� op- erations. But thoughtless increase of the cache size is not a good idea either: ��� is a scarce resource, and besides, larger cache incurs higher maintenance costs.\n\n184\n\n9.6 Choosing the Buffer Cache Size\n\nThe optimal buffer cache size differs from system to system: it depends on things like the total size of the available memory, data profiles, and workload types. Un- fortunately, there is no magic value or formula to suit everyone equally well.\n\nYou should also keep in mind that a cache miss in Postgre��� does not necessarily trigger a physical �/� operation. If the buffer cache is quite small, the �� cache uses the remaining free memory and can smooth things out to some extent. But unlike the database, the operating system knows nothing about the read data, so it applies a different eviction strategy.\n\nA typical recommendation is to start with 1 4 required.\n\nof ��� and then adjust this setting as\n\nThe best approach is experimentation: you can increase or decrease the cache size and compare the system performance. Naturally, it requires having a test system that is fully analogous to the production one, and you must be able to reproduce typical workloads.\n\nYou can also run some analysis using the pg_buffercache extension. For example, explore buffer distribution depending on their usage:\n\n=> SELECT usagecount, count(*) FROM pg_buffercache GROUP BY usagecount ORDER BY usagecount;\n\nusagecount | count −−−−−−−−−−−−+−−−−−−−\n\n1 | 2 | 3 | 4 | 5 |\n\n4128 50 4 4 73 | 12125\n\n(6 rows)\n\nN��� usage count values correspond to free buffers. They are quite expected in this case because the server was restarted and remained idle most of the time. The majority of the used buffers contain pages of the system catalog tables read by the backend to fill its system catalog cache and to perform queries.\n\nWe can check what fraction of each relation is cached,and whether this data is hot (a page is considered hot here if its usage count is bigger than one):\n\n185\n\nChapter 9 Buffer Cache\n\n=> SELECT c.relname, count(*) blocks, round( 100.0 * 8192 * count(*) /\n\npg_table_size(c.oid) ) AS \"% of rel\",\n\nround( 100.0 * 8192 * count(*) FILTER (WHERE b.usagecount > 1) /\n\npg_table_size(c.oid) ) AS \"% hot\"\n\nFROM pg_buffercache b\n\nJOIN pg_class c ON pg_relation_filenode(c.oid) = b.relfilenode\n\nWHERE b.reldatabase IN (\n\n0, -- cluster-wide objects (SELECT oid FROM pg_database WHERE datname = current_database())\n\n) AND b.usagecount IS NOT NULL GROUP BY c.relname, c.oid ORDER BY 2 DESC LIMIT 10;\n\nrelname\n\n| blocks | % of rel | % hot\n\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−+−−−−−−−−−−+−−−−−−−\n\n| big | pg_attribute | big_pkey | pg_proc | pg_operator | pg_class pg_proc_oid_index | pg_attribute_relid_attnum_index | | pg_proc_proname_args_nsp_index | pg_amproc\n\n4097 | 30 | 14 | 13 | 11 | 10 | 9 | 8 | 6 | 5 |\n\n100 | 48 | 100 | 12 | 61 | 59 | 82 | 73 | 18 | 56 |\n\n1 47 0 6 50 59 45 64 6 56\n\n(10 rows)\n\nThisexampleshowsthatthe bigtableanditsindexarefullycached,buttheirpages are not being actively used.\n\nAnalyzing data from different angles, you can gain some useful insights. However, make sure to follow these simple rules when running pg_buffercache queries:\n\nRepeat such queries several times since the returned figures will vary to some\n\nextent.\n\nDo not run such queries non-stop because the pg_buffercache extension locks\n\nthe viewed buffers, even if only briefly.\n\n186\n\n9.7 Cache Warming\n\n9.7 Cache Warming\n\nAfter a server restart, the cache requires some time to warm up, that is, to accu- mulate the actively used data. It may be helpful to cache certain tables right away, and the pg_prewarm extension serves exactly this purpose:\n\n=> CREATE EXTENSION pg_prewarm;\n\nApart from loading tables into the buffer cache (or into the �� cache only), this extension can write the current cache state to disk and then restore it after the server restart. To enable this functionality,you have to add this extension’s library to shared_preload_libraries and restart the server:\n\n=> ALTER SYSTEM SET shared_preload_libraries = 'pg_prewarm';\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\npg_prewarm.autoprewarm setting has not changed, a process called auto- If the prewarm leader will be started automatically after the server is reloaded; once in pg_prewarm.autoprewarm_interval seconds,this process will flush the list of cached pages to disk (using one of the max_parallel_processes slots).\n\npostgres$ ps -o pid,command \\ --ppid `head -n 1 /usr/local/pgsql/data/postmaster.pid` | \\ grep prewarm\n\n23124 postgres: autoprewarm leader\n\nNow that the server has been restarted, the big table is not cached anymore:\n\n=> SELECT count(*) FROM pg_buffercache WHERE relfilenode = pg_relation_filenode('big'::regclass);\n\ncount −−−−−−−\n\n0\n\n(1 row)\n\n187\n\nv. ��\n\non\n\n300s\n\nChapter 9 Buffer Cache\n\nIf you have well-grounded assumptions that the whole table is going to be actively used and disk access will make response times unacceptably high,you can load this table into the buffer cache in advance:\n\n=> SELECT pg_prewarm('big');\n\npg_prewarm −−−−−−−−−−−−\n\n4097\n\n(1 row)\n\n=> SELECT count(*) FROM pg_buffercache WHERE relfilenode = pg_relation_filenode('big'::regclass);\n\ncount −−−−−−− 4097 (1 row)\n\nThe list of pages is dumped into the ������/autoprewarm.blocks file. You can wait until the autoprewarm leader completes for the first time, but we will initiate the dump manually:\n\n=> SELECT autoprewarm_dump_now();\n\nautoprewarm_dump_now −−−−−−−−−−−−−−−−−−−−−−\n\n4224\n\n(1 row)\n\nThe number of flushed pages is bigger than ���� because all the used buffers are taken into account. The file is written in a text format; it contains the ��s of the database, tablespace, and file, as well as the fork and segment numbers:\n\npostgres$ head -n 10 /usr/local/pgsql/data/autoprewarm.blocks\n\n<<4224>> 0,1664,1262,0,0 0,1664,1260,0,0 16391,1663,1259,0,0 16391,1663,1259,0,1 16391,1663,1259,0,2 16391,1663,1259,0,3 16391,1663,1249,0,0 16391,1663,1249,0,1 16391,1663,1249,0,2\n\n188\n\n9.8 Local Cache\n\nLet’s restart the server again.\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\nThe table appears in the cache right away:\n\n=> SELECT count(*) FROM pg_buffercache WHERE relfilenode = pg_relation_filenode('big'::regclass);\n\ncount −−−−−−− 4097 (1 row)\n\nIt is again the autoprewarm leader that does all the preliminary work: it reads the file,sorts the pages by databases,reorders them (so that disk reads happen sequen- tially if possible), and then passes them to the autoprewarm worker for processing.\n\n9.8 Local Cache\n\nTemporary tables do not follow the workflow described above. Since temporary data is visible to a single process only,there is no point in loading it into the shared buffer cache. Therefore, temporary data uses the local cache of the process that owns the table.1\n\nIn general, local buffer cache works similar to the shared one:\n\nPage search is performed via a hash table.\n\nEviction follows the standard algorithm (except that buffer rings are not used).\n\nPages can be pinned to avoid eviction.\n\nHowever, local cache implementation is much simpler because it has to handle neither locks on memory structures (buffers can be accessed by a single process only) nor fault tolerance (temporary data exists till the end of the session at the most).\n\n1 backend/storage/buffer/localbuf.c\n\n189\n\np. ��� p. ���\n\n8MB\n\nChapter 9 Buffer Cache\n\nSince only few sessions typically use temporary tables, local cache memory is as- signed on demand. The maximum size of the local cache available to a session is limited by the\n\ntemp_buffers parameter.\n\nDespite a similar name, the temp_file_limit parameter has nothing to do with temporary tables; it is related to files that maybe created during queryexecution to temporarilystore intermediate data.\n\nIn the ������� command output, all calls to the local buffer cache are tagged as local instead of shared:\n\n=> CREATE TEMPORARY TABLE tmp AS SELECT 1;\n\n=> EXPLAIN (analyze, buffers, costs off, timing off, summary off)\n\nSELECT * FROM tmp;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Seq Scan on tmp (actual rows=1 loops=1)\n\nBuffers: local hit=1\n\nPlanning:\n\nBuffers: shared hit=12 read=7\n\n(4 rows)\n\n190\n\n10\n\nWrite-Ahead Log\n\n10.1 Logging\n\nIn case of a failure, such as a power outage, an �� error, or a database server crash, all the contents of ��� will be lost; only the data written to disk will persist. To starttheserverafterafailure,youhavetorestoredataconsistency. Ifthediskitself has been damaged, the same issue has to be resolved by backup recovery.\n\nIn theory, you could maintain data consistency on disk at all times. But in prac- tice it means that the server has to constantly write random pages to disk (even though sequential writing is cheaper), and the order of such writes must guaran- tee that consistency is not compromised at any particular moment (which is hard to achieve, especially if you deal with complex index structures).\n\nJust like the majority of database systems, Postgre��� uses a different approach.\n\nWhile the server is running, some of the current data is available only in ���, its writing to permanent storage being deferred. Therefore, the data stored on disk is always inconsistent during server operation, as pages are never flushed all at once. But each change that happens in ��� (such as a page update performed in the buffer cache) is logged: Postgre��� creates a log entry that contains all the essential information required to repeat this operation if the need arises.1\n\nA log entry related to a page modification must be written to disk ahead of the modified page itself. Hence the name of the log: write-ahead log, or ���. This requirement guarantees that in case of a failure Postgre��� can read ��� entries from disk and replay them to repeat the already completed operations whose re- sults were still in ��� and did not make it to disk before the crash.\n\n1 postgresql.org/docs/14/wal-intro.html\n\n191\n\nChapter 10 Write-Ahead Log\n\nKeeping a write-ahead log is usually more efficient than writing random pages to disk. W�� entries constitute a continuous stream of data, which can be handled even by ���s. Besides, ��� entries are often smaller than the page size.\n\nIt is required to log all operations that can potentially break data consistency in case of a failure. In particular, the following actions are recorded in ���:\n\npage modifications performed in the buffer cache—since writes are deferred\n\ntransaction commits and rollbacks—since the status change happens in ����\n\nbuffers and does not make it to disk right away\n\nfile operations (like creation and deletion of files and directories when ta- bles get added or removed)—since such operations must be in sync with data changes\n\nThe following actions are not logged:\n\noperations on �������� tables\n\noperations on temporary tables—since their lifetime is anyway limited by the\n\nsession that spawns them\n\nPrior to Postgre��� ��, hash indexes were not logged either. Their only purpose was to match hash functions to different data types.\n\nApart from crash recovery, ��� can also be used for point-in-time recovery from a backup and replication.\n\n10.2 WAL Structure\n\nLogical Structure\n\nSpeaking about its logical structure, we can describe ���1 as a stream of log en- triesofvariablelength. Eachentrycontainssomedataaboutaparticularoperation\n\n1 postgresql.org/docs/14/wal-internals.html\n\nbackend/access/transam/README\n\n192\n\n10.2 WAL Structure\n\npreceded by a standard header.1 Among other things, the header provides the fol- lowing information:\n\ntransaction �� related to the entry\n\nthe resource manager that interprets the entry2\n\nthe checksum to detect data corruption\n\nentry length\n\na reference to the previous ��� entry\n\nW�� is usually read in the forward direction, but some utilities like pg_rewind may scan it backwards.\n\nW�� data itself can have different formats and meaning. For example, it can be a page fragment that has to replace some part of the page at the specified offset. The corresponding resource manager must know how to interpret and replay a particu- lar entry. There are separate managers for tables, various index types, transaction status, and other entities.\n\nW�� files take up special buffers in the server’s shared memory. The size of the wal_buffers parameter. By default, this size is cache used by ��� is defined by the chosen automatically as 1 32\n\nof the total buffer cache size.\n\nW�� cache is quite similar to buffer cache,but it usually operates in the ring buffer mode: new entries are added to its head,while older entries are saved to disk start- ing at the tail. If ��� cache is too small, disk synchronization will be performed more often than necessary.\n\nUnder low load,the insert position (the buffer’s head) is almost always the same as the position of the entries that have already been saved to disk (the buffer’s tail):\n\n=> SELECT pg_current_wal_lsn(), pg_current_wal_insert_lsn();\n\npg_current_wal_lsn | pg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3DF56000\n\n| 0/3DF57968\n\n(1 row)\n\n1 include/access/xlogrecord.h 2 include/access/rmgrlist.h\n\n193\n\n−1\n\nChapter 10 Write-Ahead Log\n\nPrior to Postgre��� ��, all function names contained the ���� acronym instead of ���.\n\nTo refer to a particular entry, Postgre��� uses a special data type: pg_lsn (log se- quence number, ���). It represents a ��-bit offset in bytes from the start of the ��� to an entry. An ��� is displayed as two ��-bit numbers in the hexadecimal notation separated by a slash.\n\nLet’s create a table:\n\n=> CREATE TABLE wal(id integer);\n\n=> INSERT INTO wal VALUES (1);\n\nStart a transaction and note the ��� of the ��� insert position:\n\n=> BEGIN;\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3DF708D8\n\n(1 row)\n\nNow run some arbitrary command, for example, update a row:\n\n=> UPDATE wal SET id = id + 1;\n\nThe page modification is performed in the buffer cache in ���. This change is logged in a ��� page, also in ���. As a result, the insert ��� is advanced:\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3DF70920\n\n(1 row)\n\nTo ensure that the modified data page is flushed to disk strictly after the corre- sponding ��� entry, the page header stores the ��� of the latest ��� entry related to this page. You can view this ��� using pageinspect:\n\n=> SELECT lsn FROM page_header(get_raw_page('wal',0));\n\nlsn −−−−−−−−−−−− 0/3DF70920\n\n(1 row)\n\n194\n\n10.2 WAL Structure\n\nThere is only one ��� for the whole database cluster, and new entries constantly get appended to it. For this reason, the ��� stored in the page may turn out to be smaller than the one returned by the pg_current_wal_insert_lsn function some time ago. But if nothing has happened in the system, these numbers will be the same.\n\nNow let’s commit the transaction:\n\n=> COMMIT;\n\nThe commit operation is also logged, and the insert ��� changes again:\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3DF70948\n\n(1 row)\n\nA commit updates transaction status in ���� pages , which are kept in their own cache.1 The ���� cache usually takes ��� pages in the shared memory.2 To make sure that a ���� page is not flushed to disk before the corresponding ��� entry, the ��� of the latest ��� entry has to be tracked for ���� pages too. But this in- formation is stored in ���, not in the page itself.\n\n��� entries will make it to disk; then it will be possible to evict ���� At some point and data pages from the cache. If they had to be evicted earlier,it would have been discovered, and ��� entries would have been forced to disk first.3\n\nIf you know two ��� positions, you can calculate the size of ��� entries between them (in bytes) by simply subtracting one position from the other. You just have to cast them to the pg_lsn type:\n\n=> SELECT '0/3DF70948'::pg_lsn - '0/3DF708D8'::pg_lsn;\n\n?column? −−−−−−−−−−\n\n112\n\n(1 row)\n\n1 backend/access/transam/slru.c 2 backend/access/transam/clog.c, CLOGShmemBuffers function 3 backend/storage/buffer/bufmgr.c, FlushBuffer function\n\n195\n\np. ��\n\np. ���\n\n16MB\n\nv. ��\n\nv. ��\n\nChapter 10 Write-Ahead Log\n\nIn this particular case, ��� entries related to ������ and ������ operations took about a hundred of bytes.\n\nYou can use the same approach to estimate the volume of ��� entries generated by a particular workload per unit of time. This information will be required for the checkpoint setup.\n\nPhysical Structure\n\nOn disk, the ��� is stored in the ������/pg_wal directory as separate files, or seg- ments. Their size is shown by the read-only\n\nwal_segment_size parameter.\n\nFor high-load systems, it makes sense to increase the segment size since it may reduce the overhead, but this setting can be modified only during cluster initial- ization (initdb --wal-segsize).\n\nW�� entries get into the current file until it runs out of space; then Postgre��� starts a new file.\n\nWe can learn in which file a particular entry is located, and at what offset from the start of the file:\n\n=> SELECT file_name, upper(to_hex(file_offset)) file_offset FROM pg_walfile_name_offset('0/3DF708D8');\n\nfile_name\n\n| file_offset\n\n−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−\n\n00000001000000000000003D | F708D8\n\n(1 row) timeline\n\nlog sequence number\n\nThe name of the file consists of two parts. The highest eight hexadecimal digits define the timeline used for recovery from a backup, while the rest represent the highest ��� bits (the lowest ��� bits are shown in the file_offset field).\n\nTo view the current ��� files\n\n, you can call the following function:\n\n=> SELECT * FROM pg_ls_waldir() WHERE name = '00000001000000000000003D';\n\n196",
      "page_number": 173
    },
    {
      "number": 10,
      "title": "Write-Ahead Log",
      "start_page": 193,
      "end_page": 212,
      "detection_method": "regex_chapter",
      "content": "10.3 Checkpoint\n\nname\n\n|\n\nsize\n\n|\n\nmodification\n\n−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−− 00000001000000000000003D | 16777216 | 2023−03−06 14:01:48+03\n\n(1 row)\n\nNow let’s take a look at the headers of the newly created ��� entries using the pg_waldump utility, which can filter ��� entries both by the ��� range (like in this example) and by a particular transaction ��.\n\nThe pg_waldump utility should be started on behalf of the postgres �� user, as it needs access to ��� files on disk.\n\npostgres$ /usr/local/pgsql/bin/pg_waldump \\ -p /usr/local/pgsql/data/pg_wal -s 0/3DF708D8 -e 0/3DF70948#\n\nrmgr: Heap 0/3DF708D8, prev 0/3DF708B0, desc: HOT_UPDATE off 1 xmax 886 flags 0x40 ; new off 2 xmax 0, blkref #0: rel 1663/16391/16562 blk 0 rmgr: Transaction len (rec/tot): 886, lsn: 0/3DF70920, prev 0/3DF708D8, desc: COMMIT 2023−03−06 14:01:48.875861 MSK\n\nlen (rec/tot):\n\n69/\n\n69, tx:\n\n886, lsn:\n\n34/\n\n34, tx:\n\nHere we can see the headers of two entries.\n\nThe first one is the ���_������ The blkref field shows the filename and the page �� of the updated heap page:\n\noperation handled by the Heap resource manager.\n\n=> SELECT pg_relation_filepath('wal');\n\npg_relation_filepath −−−−−−−−−−−−−−−−−−−−−−\n\nbase/16391/16562\n\n(1 row)\n\nThe second entry is the ������ operation supervised by the Transaction resource manager.\n\n10.3 Checkpoint\n\nTo restore data consistency after a failure (that is,to perform recovery),Postgre��� has to replay the ��� in the forward direction and apply the entries that represent lost changes to the corresponding pages. To find out what has been lost, the ���\n\n197\n\np. ���\n\nv. �.�\n\nChapter 10 Write-Ahead Log\n\nof the page stored on disk is compared to the ��� of the ��� entry. But at which point should we start the recovery? If we start too late, the pages written to disk before this point will fail to receive all the changes, which will lead to irreversible data corruption. Starting from the very beginning is unrealistic: it is impossible to store such a potentially huge volume of data,and neither is it possible to accept such a long recovery time. We need a checkpoint that is gradually moving forward, thusmakingitsafetostarttherecoveryfromthispointandremovealltheprevious ��� entries.\n\nThe most straightforward way to create a checkpoint is to periodically suspend all system operations and force all dirty pages to disk. This approach is of course unacceptable, as the system will hang for an indefinite but quite significant time.\n\nFor this reason, the checkpoint is spread out over time, virtually constituting an interval. Checkpointexecutionisperformedbyaspecialbackgroundprocesscalled checkpointer.1\n\nCheckpoint start. The checkpointer process flushes to disk everything that can be written instantaneously: ���� transaction status, subtransactions’ metadata, and a few other structures.\n\nCheckpoint execution. Most of the checkpoint execution time is spent on flushing\n\ndirty pages to disk.2\n\nFirst, a special tag is set in the headers of all the buffers that were dirty at the checkpoint start. It happens very fast since no �/� operations are involved.\n\nThen checkpointer traverses all the buffers and writes the tagged ones to disk. Their pages are not evicted from the cache: they are simply written down, so usage and pin counts can be ignored.\n\nPages are processed in the order of their ��s to avoid random writing if pos- sible. For better load balancing, Postgre��� alternates between different ta- blespaces (as they may be located on different physical devices).\n\n1 backend/postmaster/checkpointer.c\n\nbackend/access/transam/xlog.c, CreateCheckPoint function\n\n2 backend/storage/buffer/bufmgr.c, BufferSync function\n\n198\n\n10.3 Checkpoint\n\nBackends can also write tagged buffers to disk—if they get to them first. In any case,buffertagsareremovedatthisstage,soforthepurposeofthecheckpoint each buffer will be written only once.\n\nNaturally,pages can still be modified in the buffer cache while the checkpoint is in progress. But since new dirty buffers are not tagged, checkpointer will ignore them.\n\nCheckpoint completion. When all the buffers that were dirty at the start of the checkpoint are written to disk, the checkpoint is considered complete. From now on (but not earlier!), the start of the checkpoint will be used as a new starting point of recovery. All the ��� entries written before this point are not required anymore.\n\nstart of recovery\n\nrequired WALfiles\n\nfailure\n\ntime\n\ncheckpoint\n\nstart of recovery\n\nrequired WALfiles\n\nfailure\n\ntime\n\ncheckpoint\n\ncheckpoint\n\nFinally, checkpointer creates a ��� entry that corresponds to the checkpoint completion, specifying the checkpoint’s start ���. Since the checkpoint logs nothing when it starts, this ��� can belong to a ��� entry of any type.\n\nThe ������/global/pg_control file also gets updated to refer to the latest com- pleted checkpoint. (Until this process is over, pg_control keeps the previous checkpoint.)\n\n199\n\nChapter 10 Write-Ahead Log\n\nPGDATA/global/pg_control\n\nLatest checkpoint location:\n\n0/3E7EF818\n\nLatest checkpoint's REDO location:\n\n0/3E7EF7E0\n\ncheckpoint start\n\ncheckpoint finish\n\nCHECKPOINT\n\nTo figure out once and for all what points where, let’s take a look at a simple ex- ample. We will make several cached pages dirty:\n\n=> UPDATE big SET s = 'FOO';\n\n=> SELECT count(*) FROM pg_buffercache WHERE isdirty;\n\ncount −−−−−−− 4119 (1 row)\n\nNote the current ��� position:\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3E7EF7E0\n\n(1 row)\n\nNow let’s complete the checkpoint manually. All the dirty pages will be flushed to disk; since nothing happens in the system, new dirty pages will not appear:\n\n=> CHECKPOINT;\n\n=> SELECT count(*) FROM pg_buffercache WHERE isdirty;\n\ncount −−−−−−−\n\n0\n\n(1 row)\n\nLet’s see how the checkpoint is reflected in the ���:\n\n200\n\n10.4 Recovery\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3E7EF890\n\n(1 row)\n\npostgres$ /usr/local/pgsql/bin/pg_waldump \\ -p /usr/local/pgsql/data/pg_wal -s 0/3E7EF7E0 -e 0/3E7EF890\n\nrmgr: Standby 0/3E7EF7E0, prev 0/3E7EF7B8, desc: RUNNING_XACTS nextXid 888 latestCompletedXid 887 oldestRunningXid 888 rmgr: XLOG 0/3E7EF818, prev 0/3E7EF7E0, desc: CHECKPOINT_ONLINE redo 0/3E7EF7E0; tli 1; prev tli 1; fpw true; xid 0:888; oid 24754; multi 1; offset 0; oldest xid 726 in DB 1; oldest multi 1 in DB 1; oldest/newest commit timestamp xid: 0/0; oldest running xid 888; online\n\nlen (rec/tot):\n\n50/\n\n50, tx:\n\n0, lsn:\n\nlen (rec/tot):\n\n114/\n\n114, tx:\n\n0, lsn:\n\nThe latest ��� entry is related to the checkpoint completion (����������_������). The start ��� of this checkpoint is specified after the word redo; this position cor- responds to the latest inserted ��� entry at the time of the checkpoint start.\n\nThe same information can also be found in the pg_control file:\n\npostgres$ /usr/local/pgsql/bin/pg_controldata \\ -D /usr/local/pgsql/data | egrep 'Latest.*location'\n\nLatest checkpoint location: Latest checkpoint's REDO location:\n\n0/3E7EF818 0/3E7EF7E0\n\n10.4 Recovery\n\nThe first process launched at the server start is postmaster. In its turn, postmaster spawns the startup process,1 which takes care of data recovery in case of a failure.\n\nTo determine whether recovery is needed, the startup process reads the pg_control file and checks the cluster status. The pg_controldata utility enables us to view the content of this file:\n\n1 backend/postmaster/startup.c\n\nbackend/access/transam/xlog.c, StartupXLOG function\n\n201\n\np. ��\n\nChapter 10 Write-Ahead Log\n\npostgres$ /usr/local/pgsql/bin/pg_controldata \\ -D /usr/local/pgsql/data | grep state\n\nDatabase cluster state:\n\nin production\n\nA properly stopped server has the “shut down” status; the “in production” status of a non-running server indicates a failure. In this case,the startup process will au- tomatically initiate recovery from the start ��� of the latest completed checkpoint found in the same pg_control file.\n\nIf the ������ directory contains a backup_label file related to a backup, the start ��� posi- tion is taken from that file.\n\nThe startup process reads ��� entries one by one, starting from the defined posi- tion, and applies them to data pages if the ��� of the page is smaller than the ��� of the ��� entry. If the page contains a bigger ���, ��� should not be applied; in fact, it must not be applied because its entries are designed to be replayed strictly sequentially.\n\nHowever, some ��� entries constitute a full page image, or ���. Entries of this type can be applied to any state of the page since all the page contents will be erased anyway. Such modifications are called idempotent. Another example of an idempo- tent operation is registering transaction status changes: each transaction status is defined in ���� by certain bits that are set regardless of their previous values, so there is no need to keep the ��� of the latest change in ���� pages.\n\nW�� entries are applied to pages in the buffer cache, just like regular page updates during normal operation.\n\nFiles get restored from ��� in a similar manner: for example, if a ��� entry shows that the file must exit, but it is missing for some reason, it will be created anew.\n\nOnce the recovery is over,all unlogged relations are overwritten by the correspond- ing initialization forks.\n\nFinally, the checkpoint is executed to secure the recovered state on disk.\n\nThe job of the startup process is now complete.\n\nIn its classic form, the recovery process consists of two phases. In the roll-forward phase, ��� entries are replayed, repeating the lost operations. In the roll-back phase, the server aborts the transactions that were not yet committed at the time of the failure.\n\n202\n\n10.4 Recovery\n\nIn Postgre���, the second phase is not required. After the recovery, the ���� will contain neither commit nor abort bits for an unfinished transaction (which technically denotes an active transaction), but since it is known for sure that the transaction is not running anymore, it will be considered aborted.1\n\nWe can simulate a failure by forcing the server to stop in the immediate mode:\n\npostgres$ pg_ctl stop -m immediate\n\nHere is the new cluster state:\n\npostgres$ /usr/local/pgsql/bin/pg_controldata \\ -D /usr/local/pgsql/data | grep 'state'\n\nDatabase cluster state:\n\nin production\n\nWhen we launch the server,the startup process sees that a failure has occurred and enters the recovery mode:\n\npostgres$ pg_ctl start -l /home/postgres/logfile\n\npostgres$ tail -n 6 /home/postgres/logfile\n\nLOG: 14:01:49 MSK LOG: in progress LOG: LOG: LOG: system: 0.00 s, elapsed: 0.00 s LOG:\n\ndatabase system was interrupted; last known up at 2023−03−06\n\ndatabase system was not properly shut down; automatic recovery\n\nredo starts at 0/3E7EF7E0 invalid record length at 0/3E7EF890: wanted 24, got 0 redo done at 0/3E7EF818 system usage: CPU: user: 0.00 s,\n\ndatabase system is ready to accept connections\n\nIf the server is being stopped normally,postmaster disconnects all clients and then executes the final checkpoint to flush all dirty pages to disk.\n\nNote the current ��� position:\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3E7EF908\n\n(1 row)\n\n1 backend/access/heap/heapam_visibility.c, HeapTupleSatisfiesMVCC function\n\n203\n\nChapter 10 Write-Ahead Log\n\nNow let’s stop the server properly:\n\npostgres$ pg_ctl stop\n\nHere is the new cluster state:\n\npostgres$ /usr/local/pgsql/bin/pg_controldata \\ -D /usr/local/pgsql/data | grep state\n\nDatabase cluster state:\n\nshut down\n\nAt the end of the ���, we can see the ����������_�������� entry, which denotes the final checkpoint:\n\npostgres$ /usr/local/pgsql/bin/pg_waldump \\ -p /usr/local/pgsql/data/pg_wal -s 0/3E7EF908\n\nrmgr: XLOG 0/3E7EF908, prev 0/3E7EF890, desc: CHECKPOINT_SHUTDOWN redo 0/3E7EF908; tli 1; prev tli 1; fpw true; xid 0:888; oid 24754; multi 1; offset 0; oldest xid 726 in DB 1; oldest multi 1 in DB 1; oldest/newest commit timestamp xid: 0/0; oldest running xid 0; shutdown pg_waldump: fatal: error in WAL record at 0/3E7EF908: invalid record length at 0/3E7EF980: wanted 24, got 0\n\nlen (rec/tot):\n\n114/\n\n114, tx:\n\n0, lsn:\n\nThe latest pg_waldump message shows that the utility has read the ��� to the end.\n\nLet’s start the instance again:\n\npostgres$ pg_ctl start -l /home/postgres/logfile\n\n10.5 Background Writing\n\nIf the backend needs to evict a dirty page from a buffer, it has to write this page to disk. Such a situation is undesired because it leads to waits—it is much better to perform writing asynchronously in the background.\n\nThis job is partially handled by checkpointer, but it is still not enough.\n\n204\n\n10.6 WAL Setup\n\nTherefore, Postgre��� provides another process called bgwriter,1 specifically for backgroundwriting. Itreliesonthesamebuffersearchalgorithmaseviction,except for the two main differences:\n\nThe bgwriter process uses its own clock hand that never lags behind that of\n\neviction and typically overtakes it.\n\nAs the buffers are being traversed, the usage count is not reduced.\n\nA dirty page is flushed to disk if the buffer is not pinned and has zero usage count. Thus, bgwriter runs before eviction and proactively writes to disk those pages that are highly likely to be evicted soon.\n\nIt raises the odds of the buffers selected for eviction being clean.\n\n10.6 WAL Setup\n\nConfiguring Checkpoints\n\nThe checkpoint duration (to be more exact, the duration of writing dirty buffers to checkpoint_completion_target parameter. Its value specifies disk) is defined by the the fraction of time between the starts of two neighboring checkpoints that is allot- ted to writing. Avoid setting this parameter to one: as a result,the next checkpoint may be due before the previous one is complete. No disaster will happen, as it is impossible to execute more than one checkpoint at a time, but normal operation may still be disrupted.\n\nWhen configuring other parameters, we can use the following approach. First, we define an appropriate volume of ��� files to be stored between two neighboring checkpoints. The bigger the volume, the smaller the overhead, but this value will anyway be limited by the available free space and the acceptable recovery time.\n\nTo estimate the time required to generate this volume by normal load, you need to note the initial insert ��� and check the difference between this and the current insert positions from time to time.\n\n1 backend/postmaster/bgwriter.c\n\n205\n\n0.9 v. ��\n\n5min p. ���\n\n1GB\n\nv. ��\n\nChapter 10 Write-Ahead Log\n\nThe received figure is assumed to be a typical interval between checkpoints, so we checkpoint_timeout parameter value. The default setting is likely will use it as the it is usually increased, for example, to �� minutes. to be too small;\n\nHowever, it is quite possible (and even probable) that the load will sometimes be higher, so the size of ��� files generated during this interval will be too big. In this case,the checkpoint must be executed more often. To set up such a trigger,we max_wal_size parameter. will limit the size of ��� files required for recovery by the When this threshold is exceeded, the server invokes an extra checkpoint.1\n\nrequired for recovery contain all the entries both for the latest completed W�� files checkpointandforthecurrentone,whichisnotcompletedyet. Sotoestimatetheir total volume you should multiply the calculated ��� size between checkpoints by 1 + checkpoint_completion_target.\n\nPrior to version ��, Postgre��� kept ��� files for two completed checkpoints, so the mul- tiplier was 2 + checkpoint_completion_target.\n\nFollowing this approach,most checkpoints are executed on schedule,once per the checkpoint_timeout interval; but should the load increase, the checkpoint is trig- gered when ��� size exceeds the max_wal_size value.\n\nThe actual progress is periodically checked against the expected figures:2\n\nThe actual progress is defined by the fraction of cached pages that have already\n\nbeen processed.\n\nThe expected progress (by time)\n\nis defined by the fraction of time that has al- ready elapsed, assuming that the checkpoint must be completed within the checkpoint_timeout × checkpoint_completion_target interval.\n\nThe expected progress (by size) is defined by the fraction of the already filled ��� files, their expected number being estimated based on the max_wal_size × checkpoint_completion_target value.\n\nIf dirty pages get written to disk ahead of schedule, checkpointer is paused for a while; if there is any delay by either of the parameters, it catches up as soon as\n\n1 backend/access/transam/xlog.c, LogCheckpointNeeded & CalculateCheckpointSegments functions 2 backend/postmaster/checkpointer.c, IsCheckpointOnSchedule function\n\n206\n\n10.6 WAL Setup\n\npossible.1 Since both time and data size are taken into account, Postgre��� can manage scheduled and on-demand checkpoints using the same approach.\n\nOnce the checkpoint has been completed, ��� files that are not required for recov- ery anymore are deleted;2 however, several files (up to min_wal_size in total) are kept for reuse and are simply renamed.\n\nSuch renaming tion,but you can turn off this feature using the need it.\n\nreduces the overhead incurred by constant file creation and dele- wal_recycle parameter if you do not\n\nThe following figure shows how the size of ��� files stored on disk changes under normal conditions.\n\nWALsize\n\ne z i s _ l a w _ x a m\n\ntime\n\ncheckpoint_timeout\n\nthe size of WALgenerated between the starts of two checkpoints\n\nIt is important to keep in mind that the actual size of ��� files on disk may exceed the max_wal_size value:\n\nThe max_wal_size parameter specifies the desired target value rather than a\n\nhard limit. If the load spikes, writing may lag behind the schedule.\n\nThe server has no right to delete ��� files that are yet to be replicated or han- dledbycontinuousarchiving. Ifenabled,thisfunctionalitymustbeconstantly monitored, as it can easily cause a disk overflow.\n\n1 backend/postmaster/checkpointer.c, CheckpointWriteDelay function 2 backend/access/transam/xlog.c, RemoveOldXlogFiles function\n\n207\n\n80MB\n\nv. �� on\n\nv. �� 0MB\n\n200ms\n\n2\n\n100\n\n30s\n\noff\n\nChapter 10 Write-Ahead Log\n\nYou can reserve a certain amount of space\n\nfor ��� files by configuring the\n\nwal_keep_size parameter.\n\nConfiguring Background Writing\n\nOnce checkpointer is configured, you should also set up bgwriter. Together, these processes must be able to cope with writing dirty buffers to disk before backends need to reuse them.\n\nDuring its operation, bgwriter makes periodic pauses, sleeping for units of time.\n\nbgwriter_delay\n\nThe number of pages written between two pauses depends on the average number of buffers accessed by backends since the previous run (Postgre��� uses a moving average to level out possible spikes and avoid depending on very old data at the bgwriter_lru_multiplier. same time). The calculated number is then multiplied by But in any case, the number of pages written in a single run cannot exceed the bgwriter_lru_maxpages value.\n\nIf no dirty buffers are detected (that is, nothing happens in the system), bgwriter sleeps until one of the backends accesses a buffer. Then it wakes up and continues its regular operation.\n\nMonitoring\n\nCheckpoint settings can and should be tuned based on monitoring data.\n\nIf size-triggered checkpoints have to be performed more often than defined by the checkpoint_warning parameter, Postgre��� issues a warning. This setting should be brought in line with the expected peak load.\n\nThe into the server log. Let’s turn it on:\n\nlog_checkpoints parameter enables printing checkpoint-related information\n\n=> ALTER SYSTEM SET log_checkpoints = on;\n\n=> SELECT pg_reload_conf();\n\nNow we will modify some data and execute a checkpoint:\n\n208\n\n10.6 WAL Setup\n\n=> UPDATE big SET s = 'BAR';\n\n=> CHECKPOINT;\n\nThe server log shows the number of written buffers, some statistics on ��� file changes after the checkpoint, the duration of the checkpoint, and the distance (in bytes) between the starts of two neighboring checkpoints:\n\npostgres$ tail -n 2 /home/postgres/logfile\n\nLOG: LOG: added, 1 removed, 0 recycled; write=0.076 s, sync=0.009 s, total=0.099 s; sync files=3, longest=0.007 s, average=0.003 s; distance=9213 kB, estimate=9213 kB\n\ncheckpoint starting: immediate force wait checkpoint complete: wrote 4100 buffers (25.0%); 0 WAL file(s)\n\nThe most useful data that can affect your configuration decisions is statistics on background writing and checkpoint execution provided in the pg_stat_bgwriter view.\n\nPrior to version 9.2, both tasks were performed by bgwriter; then a separate checkpointer process was introduced, but the common view remained unchanged.\n\n=> SELECT * FROM pg_stat_bgwriter \\gx\n\n−[ RECORD 1 ]−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− checkpoints_timed checkpoints_req checkpoint_write_time | 33111 checkpoint_sync_time buffers_checkpoint buffers_clean maxwritten_clean buffers_backend buffers_backend_fsync | 0 buffers_alloc stats_reset\n\n| 0 | 14\n\n| 221 | 14253 | 13066 | 122 | 84226\n\n| 86700 | 2023−03−06 14:00:07.369124+03\n\nAmong other things, this view displays the number of completed checkpoints:\n\nThe checkpoints_timed field shows scheduled checkpoints (which are triggered\n\nwhen the checkpoint_timeout interval is reached).\n\nThe checkpoints_req field shows on-demand checkpoints (including those trig-\n\ngered when the max_wal_size size is reached).\n\n209\n\nChapter 10 Write-Ahead Log\n\nA large checkpoint_req value (as compared to checkpoints_timed) indicates that checkpoints are performed more often than expected.\n\nThe following statistics on the number of written pages are also very important:\n\nbuffers_checkpoint pages written by checkpointer\n\nbuffers_backend pages written by backends\n\nbuffers_clean pages written by bgwriter\n\nIn a well-configured system, the buffers_backend value must be considerably lower than the sum of buffers_checkpoint and buffers_clean.\n\nWhen setting up background writing, pay attention to the maxwritten_clean value: it shows how many times bgwriter had to stop because of exceeding the threshold defined by bgwriter_lru_maxpages.\n\nThe following call will drop the collected statistics:\n\n=> SELECT pg_stat_reset_shared('bgwriter');\n\n210\n\n11\n\nWAL Modes\n\n11.1 Performance\n\nWhile the server is running normally, ��� files are being constantly written to disk. However, these writes are sequential: there is almost no random access, so even ���s can cope with this task. Since this type of load is very different from a typical data file access, it may be worth setting up a separate physical storage for ��� files and replacing the ������/pg_wal catalog by a symbolic link to a directory in a mounted file system.\n\nThere are a couple of situations when ��� files have to be both written and read. The first one is the obvious case of crash recovery; the second one is stream replication. The walsender1 process reads ��� entries directly from files.2 So if a replica does not receive ��� entries while the required pages are still in the �� buffers of the primary server, the data has to be read from disk. But the access will still be sequential rather than random.\n\n��� entries can be written in one of the following modes:\n\nThe synchronous mode forbids any further operations until a transaction com-\n\nmit saves all the related ��� entries to disk.\n\nThe asynchronous mode implies instant transaction commits, with ��� en-\n\ntries being written to disk later in the background.\n\nThe current mode is defined by the\n\nsynchronous_commit parameter.\n\n1 backend/replication/walsender.c 2 backend/access/transam/xlogreader.c\n\n211\n\non\n\n0s 5\n\n200ms\n\nChapter 11 WAL Modes\n\nSynchronous mode. To reliably register the fact of a commit, it is not enough to simply pass ��� entries to the operating system; you have to make sure that disk synchronization has completed successfully. Since synchronization im- plies actual �/� operations (which are quite slow), it is beneficial to perform it as seldom as possible.\n\nFor this purpose, the backend that completes the transaction and writes ��� commit_delay param- entries to disk can make a small pause as defined by the commit_siblings active eter. However, it will only happen if there are at least transactions in the system:1 during this pause, some of them may finish, and the server will manage to synchronize all the ��� entries in one go. It is a lot like holding doors of an elevator for someone to rush in.\n\nBy default, there is no pause. It makes sense to modify the commit_delay pa- rameter only for systems that perform a lot of short ���� transactions.\n\nAfter a potential pause, the process that completes the transaction flushes all the accumulated ��� entries to disk and performs synchronization (it is important to save the commit entry and all the previous entries related to this transaction; the rest is written just because it does not increase the cost).\n\nFrom this time on,the����’s durability requirement is guaranteed—the trans- action is considered to be reliably committed.2 That’s why the synchronous mode is the default one.\n\nThe downside of the synchronous commit is longer latencies (the ������ com- mand does not return control until the end of synchronization) and lower sys- tem throughput, especially for ���� loads.\n\nAsynchronous mode. To enable asynchronous commits,3 you have to turn off the\n\nsynchronous_commit parameter.\n\nIn the asynchronous mode, ��� entries are written to disk by the walwriter4 process, which alternates between work and sleep. The duration of pauses is defined by the\n\nwal_writer_delay value.\n\n1 backend/access/transam/xlog.c, XLogFlush function 2 backend/access/transam/xlog.c, RecordTransactionCommit function 3 postgresql.org/docs/14/wal-async-commit.html 4 backend/postmaster/walwriter.c\n\n212\n\n11.1 Performance\n\nWaking up from a pause,the process checks the cache for new completely filled ��� pages. If any such pages have appeared, the process writes them to disk, skipping the current page. Otherwise, it writes the current half-empty page since it has woken up anyway.1\n\nThe purpose of this algorithm is to avoid flushing one and the same page several times, which brings noticeable performance gains for workloads with intensive data changes.\n\nAlthough ��� cache is used as a ring buffer, walwriter stops when it reaches the last page of the cache; after a pause, the next writing cycle starts from the first page. So in the worst case walwriter needs three runs to get to a particular ��� entry: first,it will write all full pages located at the end of the cache,then it will get back to the beginning,and finally,it will handle the underfilled page containing the entry. But in most cases it takes one or two cycles.\n\nSynchronization is performed each time the data is written, and once again at the end of the writing cycle.\n\nwal_writer_flush_after amount of\n\nAsynchronous commits are faster than synchronous ones since they do not have to wait for physical writes to disk. But reliability suffers: you can lose the data committed within the 3 × wal_writer_delay timeframe before a failure (which is 0.6 seconds by default).\n\nIn the real world, these two modes complement each other. In the synchronous mode, ��� entries related to a long transaction can still be written asynchronously to free ��� buffers. And vice versa, a ��� entry related to a page that is about to be evicted from the buffer cache will be immediately flushed to disk even in the asynchronous mode—otherwise, it is impossible to continue operation.\n\nIn most cases, a hard choice between performance and durability has to be made by the system designer.\n\nThe synchronous_commit parameter can also be set for particular transactions. If it is possible to classify all transactions at the application level as either absolutely critical (such as handling financial data) or less important, you can boost perfor- mance while risking to lose only non-critical transactions.\n\n1 backend/access/transam/xlog.c, XLogBackgroundFlush function\n\n213\n\n1MB\n\nChapter 11 WAL Modes\n\nTo get some idea of potential performance gains of the asynchronous commit,let’s compare latency and throughput in the two modes using a pgbench test.1\n\nFirst, initialize the required tables:\n\npostgres$ /usr/local/pgsql/bin/pgbench -i internals\n\nStart a ��-second test in the synchronous mode:\n\npostgres$ /usr/local/pgsql/bin/pgbench -T 30 internals\n\npgbench (14.7) starting vacuum...end. transaction type: <builtin: TPC−B (sort of)> scaling factor: 1 query mode: simple number of clients: 1 number of threads: 1 duration: 30 s number of transactions actually processed: 20123 latency average = 1.491 ms initial connection time = 2.507 ms tps = 670.809688 (without initial connection time)\n\nAnd now run the same test in the asynchronous mode:\n\n=> ALTER SYSTEM SET synchronous_commit = off;\n\n=> SELECT pg_reload_conf();\n\npostgres$ /usr/local/pgsql/bin/pgbench -T 30 internals\n\npgbench (14.7) starting vacuum...end. transaction type: <builtin: TPC−B (sort of)> scaling factor: 1 query mode: simple number of clients: 1 number of threads: 1 duration: 30 s number of transactions actually processed: 61809 latency average = 0.485 ms initial connection time = 1.915 ms tps = 2060.399861 (without initial connection time)\n\n1 postgresql.org/docs/14/pgbench.html\n\n214\n\n11.2 Fault Tolerance\n\nIn the asynchronous mode, this simple benchmark shows a significantly lower la- tency and higher throughput (���). Naturally, each particular system will have its own figures depending on the current load, but it is clear that the impact on short ���� transactions can be quite tangible.\n\nLet’s restore the default settings:\n\n=> ALTER SYSTEM RESET synchronous_commit;\n\n=> SELECT pg_reload_conf();\n\n11.2 Fault Tolerance\n\nIt is self-evident that write-ahead logging must guarantee crash recovery under any circumstances (unless the persistent storage itself is broken). There are many factors that can affect data consistency, but I will cover only the most important ones: caching, data corruption, and non-atomic writes.1\n\nCaching\n\nBefore reaching a non-volatile storage (such as a hard disk),data can pass through various caches.\n\nA disk write simply instructs the operating system to place the data into its cache (which is also prone to crashes, just like any other part of ���). The actual writing is performed asynchronously, as defined by the settings of the �/� scheduler of the operating system.\n\nOnce the scheduler decides to flush the accumulated data, this data is moved to the cache of a storage device (like an ���). Storage devices can also defer writing, for example, to group of adjacent pages together. A ���� controller adds one more caching level between the disk and the operating system.\n\nUnless special measures are taken, the moment when the data is reliably stored on disk remains unknown. It is usually not so important because we have the ���,\n\n1 postgresql.org/docs/14/wal-reliability.html\n\n215\n\non\n\nChapter 11 WAL Modes\n\nbut ��� entries themselves must be reliably saved on disk right away.1 It is equally true for the asynchronous mode—otherwise,it is impossible to guarantee that ��� entries get do disk ahead of the modified data.\n\nThe checkpointer process must also save the data in a reliable way, ensuring that dirty pages make it to disk from the �� cache. Besides,it has to synchronize all the file operations that have been performed by other processes (such as page writes or file deletions): when the checkpoint completes, the results of all these actions must be already saved on disk.2\n\nThere are also some other situations that demand fail-safe writing,such as execut- ing unlogged operations at the minimal ��� level.\n\nOperating systems provide various means to guarantee immediate writing of data into a non-volatile storage. All of them boil down to the following two main ap- proaches: either a separate synchronization command is called after writing (such as fsync or fdatasync), or the requirement to perform synchronization (or even di- rect writing that bypasses �� cache) is specified when the file is being opened or written into.\n\nThe pg_test_fsync utility can help you determine the best way to synchronize the ��� depending on your �� and file system; the preferred method can be specified in the wal_sync_method parameter. For other operations, an appropriate synchro- nization method is selected automatically and cannot be configured.3\n\nA subtle aspect here is that in each particular case the most suitable method de- pends on the hardware. For example, if you use a controller with a backup battery, you can take advantage of its cache, as the battery will protect the data in case of a power outage.\n\nYou should keep in mind that the asynchronous commit and lack of synchroniza- fsync tion are two totally different stories. Turning off synchronization (by the parameter) boosts system performance, yet any failure will lead to fatal data loss. The asynchronous mode guarantees crash recovery up to a consistent state, but some of the latest data updates may be missing.\n\n1 backend/access/transam/xlog.c, issue_xlog_fsync function 2 backend/storage/sync/sync.c 3 backend/storage/file/fd.c, pg_fsync function\n\n216",
      "page_number": 193
    },
    {
      "number": 11,
      "title": "WAL Modes",
      "start_page": 213,
      "end_page": 230,
      "detection_method": "regex_chapter",
      "content": "11.2 Fault Tolerance\n\nData Corruption\n\nTechnical equipment is imperfect, and data can get damaged both in memory and on disk,or while it is being transferred via interface cables. Such errors are usually handled at the hardware level, yet some can escape.\n\nTocatchissuesingoodtime,Postgre���alwaysprotects���entriesbychecksums.\n\nChecksumscanbecalculatedfordatapagesaswell.1 Itisdoneeitherduringcluster pg_checksums2 utility when the server is stopped.3 initialization or by running the\n\nIn production systems, checksums must always be enabled, despite some (minor) calculation and verification overhead. It raises the chance of timely corruption discovery, even though some corner cases still remain:\n\nChecksum verification is performed only when the page is accessed, so data corruption can go unnoticed for a long time, up to the point when it gets into all backups and leaves no source of correct data.\n\nA zeroed page is considered correct, so if the file system zeroes out a page by\n\nmistake, this issue will not be discovered.\n\nChecksums are calculated only for the main fork of relations; other forks and\n\nfiles (such as transaction status in ����) remain unprotected.\n\nLet’s take a look at the read-only data_checksums parameter to make sure that checksums are enabled:\n\n=> SHOW data_checksums;\n\ndata_checksums −−−−−−−−−−−−−−−−\n\non\n\n(1 row)\n\nNow stop the server and zero out several bytes in the zero page of the main fork of the table:\n\n1 backend/storage/page/README 2 postgresql.org/docs/14/app-pgchecksums.html 3 commitfest.postgresql.org/27/2260\n\n217\n\nv. ��\n\noff\n\nChapter 11 WAL Modes\n\n=> SELECT pg_relation_filepath('wal');\n\npg_relation_filepath −−−−−−−−−−−−−−−−−−−−−−\n\nbase/16391/16562\n\n(1 row)\n\npostgres$ pg_ctl stop\n\npostgres$ dd if=/dev/zero of=/usr/local/pgsql/data/base/16391/16562 \\ oflag=dsync conv=notrunc bs=1 count=8\n\n8+0 records in 8+0 records out 8 bytes copied, 0,00776573 s, 1,0 kB/s\n\nStart the server again:\n\npostgres$ pg_ctl start -l /home/postgres/logfile\n\nIn fact, we could have left the server running—it is enough to write the page to disk and evict it from cache (otherwise, the server will continue using its cached version). But such a workflow is harder to reproduce.\n\nNow let’s attempt to read the table:\n\n=> SELECT * FROM wal LIMIT 1;\n\nWARNING: expected 28733 ERROR:\n\npage verification failed, calculated checksum 20397 but\n\ninvalid page in block 0 of relation base/16391/16562\n\nIf the data cannot be restored from a backup, it makes sense to at least try to read the damaged page (risking to get garbled output). For this purpose, you have to enable the\n\nignore_checksum_failure parameter:\n\n=> SET ignore_checksum_failure = on;\n\n=> SELECT * FROM wal LIMIT 1;\n\nWARNING: expected 28733\n\npage verification failed, calculated checksum 20397 but\n\nid −−−− 2\n\n(1 row)\n\nEverything went fine in this case because we have damaged a non-critical part of the page header (the ��� of the latest ��� entry), not the data itself.\n\n218\n\n11.2 Fault Tolerance\n\nNon-Atomic Writes\n\nA database page usually takes � k�, but at the low level writing is performed by blocks, which are often smaller (typically ��� bytes or � k�). Thus, if a failure oc- curs, a page may be written only partially. It makes no sense to apply regular ��� entries to such a page during recovery.\n\nTo avoid partial writes, Postgre��� saves a full page image (���) in the ��� when this page is modified for the first time after the checkpoint start. This behavior is full_page_writes parameter,but turning it off can lead to fatal data controlled by the corruption.\n\nIf the recovery process comes across an ��� in the ���,it will unconditionally write it to disk (without checking its ���); just like any ��� entry, ���s are protected by checksums,so their damage cannot go unnoticed. Regular ��� entries will then be applied to this state, which is guaranteed to be correct.\n\nThere is no separate ��� entry type for setting hint bits : this operation is consid- ered non-critical because any query that accesses a page will set the required bits anew. However, any hint bit change will affect the page’s checksum. So if check- wal_log_hints parameter is on), hint bit modifications sums are enabled (or if the are logged as ���s.1\n\nEven though the logging mechanism excludes empty space from an ���,2 the size of the generated ��� files still significantly increases. The situation can be greatly improved if you enable ��� compression via the\n\nwal_compression parameter.\n\nLet’s run a simple experiment using the pgbench utility. We will perform a check- point and immediately start a benchmark test with a hard-set number of transac- tions:\n\n=> CHECKPOINT;\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/42CE5DA8\n\n(1 row)\n\n1 backend/storage/buffer/bufmgr.c, MarkBufferDirtyHint function 2 backend/access/transam/xloginsert.c, XLogRecordAssemble function\n\n219\n\np. ���\n\non\n\np. ��\n\noff\n\noff\n\nChapter 11 WAL Modes\n\npostgres$ /usr/local/pgsql/bin/pgbench -t 20000 internals\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/449113E0\n\n(1 row)\n\nHere is the size of the generated ��� entries:\n\n=> SELECT pg_size_pretty('0/449755C0'::pg_lsn - '0/42CE5DA8'::pg_lsn);\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n29 MB (1 row)\n\nIn this example, ���s take more than half of the total ��� size. You can see it for yourselfinthecollectedstatisticsthatshowthenumberof ��� entries(N),thesize of regular entries (Record size), and the ��� size for each resource type (Type):\n\npostgres$ /usr/local/pgsql/bin/pg_waldump --stats \\ -p /usr/local/pgsql/data/pg_wal -s 0/42CE5DA8 -e 0/449755C0\n\n(%) Type −−− −−−− XLOG 3,31) Transaction 20004 ( 15,41) 0,00) Storage 0,00) CLOG Standby 0,00) 24774 ( 19,09) Heap2 80234 ( 61,81) Heap 0,38) Btree\n\nN − 4294 (\n\n1 ( 1 ( 6 (\n\nRecord size −−−−−−−−−−−\n\n(%) −−− 2,50) 8,10) 0,00) 0,00) 0,00) 1536253 ( 18,27) 5946242 ( 70,73) 0,39)\n\n210406 ( 680536 ( 42 ( 30 ( 416 (\n\n(%) FPI size −−−−−−−− −−− 19820068 ( 93,78) 0,00) 0 ( 0,00) 0 ( 0,00) 0 ( 0,00) 0 ( 0,12) 24576 ( 1,40) 295664 ( 4,70) 993860 (\n\n494 (\n\n32747 (\n\nTotal\n\n−−−−−− 129808\n\n−−−−−−−−\n\n8406672 [28,46%]\n\n−−−−−−−− 21134168 [71,54%]\n\nThis ratio will be smaller if data pages get modified between checkpoints several times. It is yet another reason to perform checkpoints less often.\n\nWe will repeat the same experiment to see if compression can help.\n\n=> ALTER SYSTEM SET wal_compression = on;\n\n=> SELECT pg_reload_conf();\n\n=> CHECKPOINT;\n\n220\n\n11.3 WAL Levels\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/44D4C228\n\n(1 row)\n\npostgres$ /usr/local/pgsql/bin/pgbench -t 20000 internals\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/457653B0\n\n(1 row)\n\nHere is the ��� size with compression enabled:\n\n=> SELECT pg_size_pretty('0/457653B0'::pg_lsn - '0/44D4C228'::pg_lsn);\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n10 MB (1 row)\n\npostgres$ /usr/local/pgsql/bin/pg_waldump --stats \\ -p /usr/local/pgsql/data/pg_wal -s 0/44D4C228 -e 0/457653B0\n\n(%) Type −−− −−−− XLOG 0,29) Transaction 20001 ( 16,73) 0,00) Storage Standby 0,00) 18946 ( 15,84) Heap2 80141 ( 67,02) Heap 0,12) Btree\n\nN − 344 (\n\n1 ( 5 (\n\nRecord size −−−−−−−−−−−\n\n(%) −−− 0,22) 8,68) 0,00) 0,00) 1207425 ( 15,42) 5918020 ( 75,56) 0,11)\n\n17530 ( 680114 ( 42 ( 330 (\n\nFPI size −−−−−−−−\n\n(%) −−− 435492 ( 17,75) 0,00) 0,00) 0,00) 4,14) 1627008 ( 66,31) 289654 ( 11,80)\n\n0 ( 0 ( 0 ( 101601 (\n\n143 (\n\n8443 (\n\nTotal\n\n−−−−−− 119581\n\n−−−−−−−−\n\n7831904 [76,14%]\n\n−−−−−−−−\n\n2453755 [23,86%]\n\nTo sum it up,when there is a large number of ���s caused by enabled checksums or full_page_writes (that is,almost always),it makes sense to use compression despite some additional ��� overhead.\n\n11.3 WAL Levels\n\nThe main objective of write-ahead logging is to enable crash recovery. But if you extend the scope of logged information, a ��� can be used for other purposes too.\n\n221\n\nreplica\n\nv. �� 2MB\n\nv. ��\n\n10\n\nChapter 11 WAL Modes\n\nPostgre���providesminimal,replica,andlogicallogginglevels. Eachlevelincludes everything that is logged on the previous one and adds some more information.\n\nThe level in use is defined by the server restart.\n\nwal_level parameter; its modification requires a\n\nMinimal\n\nThe minimal level guarantees only crash recovery. To save space, the operations on relations that have been created or truncated within the current transaction are not logged if they incur insertion of large volumes of data (like in the case of ������ ����� �� ������ and ������ ����� commands).1 Instead of being logged, all the required data is immediately flushed to disk, and system catalog changes become visible right after the transaction commit.\n\nIf such an operation is interrupted by a failure, the data that has already made it to disk remains invisible and does not affect consistency. If a failure occurs when the operation is complete, all the data required for applying the subsequent ��� entries is already saved to disk.\n\nThe volume of data optimization to take effect is defined by the\n\nthat has to be written into a newly created relation for this\n\nwal_skip_threshold parameter.\n\nLet’s see what gets logged at the minimal level.\n\na higher replica level is used, which supports data replication. If you By default, choose the minimal level, you also have to set the allowed number of walsender processes to zero in the\n\nmax_wal_senders parameter:\n\n=> ALTER SYSTEM SET wal_level = minimal;\n\n=> ALTER SYSTEM SET max_wal_senders = 0;\n\nThe server has to be restarted for these changes to take effect:\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\nNote the current ��� position:\n\n=> SELECT pg_current_wal_insert_lsn();\n\n1 include/utils/rel.h, RelationNeedsWAL macro\n\n222\n\n11.3 WAL Levels\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/45767698\n\n(1 row)\n\nTruncate the table and keep inserting new rows within the same transaction until the wal_skip_threshold is exceeded:\n\n=> BEGIN;\n\n=> TRUNCATE TABLE wal;\n\n=> INSERT INTO wal\n\nSELECT id FROM generate_series(1,100000) id;\n\n=> COMMIT;\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/45767840\n\n(1 row)\n\nInstead of creating a new table, I run the �������� command as it generates fewer ��� entries.\n\nLet’s examine the generated ��� using the already familiar pg_waldump utility.\n\npostgres$ /usr/local/pgsql/bin/pg_waldump \\ -p /usr/local/pgsql/data/pg_wal -s 0/45767698 -e 0/45767840#\n\nrmgr: Storage 0/45767698, prev 0/45767660, desc: CREATE base/16391/24784 rmgr: Heap 0/457676C8, prev 0/45767698, desc: UPDATE off 45 xmax 122844 flags 0x60 ; new off 48 xmax 0, blkref #0: rel 1663/16391/1259 blk 0 rmgr: Btree 0/45767748, prev 0/457676C8, desc: INSERT_LEAF off 176, blkref #0: rel 1663/16391/2662 blk 2 rmgr: Btree 0/45767788, prev 0/45767748, desc: INSERT_LEAF off 147, blkref #0: rel 1663/16391/2663 blk 2 rmgr: Btree 0/457677C8, prev 0/45767788, desc: INSERT_LEAF off 254, blkref #0: rel 1663/16391/3455 blk 4 rmgr: Transaction len (rec/tot): 122844, lsn: 0/45767808, prev 0/457677C8, desc: COMMIT 2023−03−06 14:03:58.395214 MSK; rels: base/16391/24783\n\n42, tx:\n\n42/\n\n0, lsn:\n\nlen (rec/tot):\n\n122844, lsn:\n\n123, tx:\n\nlen (rec/tot):\n\n123/\n\n64/\n\nlen (rec/tot):\n\n64, tx:\n\n122844, lsn:\n\n64/\n\n64, tx:\n\n122844, lsn:\n\nlen (rec/tot):\n\n64/\n\n122844, lsn:\n\nlen (rec/tot):\n\n64, tx:\n\n54/\n\n54, tx:\n\n223\n\np. ���\n\np. ���\n\np. ��\n\nChapter 11 WAL Modes\n\nThe first entry logs creation of a new file for the relation (since �������� rewrites the table).\n\nvirtually\n\nThe next four entries are associated with system catalog operations. They reflect the changes in the pg_class table and its three indexes.\n\nFinally, there is a commit-related entry. Data insertion is not logged.\n\nReplica\n\nDuring crash recovery, ��� entries are replayed to restore the data on disk up to a consistent state. Backup recovery works in a similar way, but it can also restore the database state up to the specified recovery target point using a ��� archive. The number of archived ��� entries can be quite high (for example, they can span several days), so the recovery period will include multiple checkpoints. Therefore, the minimal ��� level is not enough: it is impossible to repeat an operation if it is unlogged. For backup recovery, ��� files must include all the operations.\n\nThe same is true for replication: unlogged commands will not be sent to a replica and will not be replayed on it.\n\nThings get even more complicated if a replica is used for executing queries. First of all, it needs to have the information on exclusive locks acquired on the primary server since they may conflict with queries on the replica. Second, it must be able tocapturesnapshots ,whichrequirestheinformationonactivetransactions. When we deal with a replica, both local transactions and those running on the primary server have to be taken into account.\n\nThe only way to send this data to a replica is to periodically write it into ��� files.1 It is done by the bgwriter2 process, once in �� seconds (the interval is hard-coded).\n\nThe ability to perform data recovery from a backup and use physical replication is guaranteed at the replica level.\n\n1 backend/storage/ipc/standby, LogStandbySnapshot function 2 backend/postmaster/bgwriter.c\n\n224\n\n11.3 WAL Levels\n\nThe replica level ured above and restart the server:\n\nis used by default, so we can simply reset the parameters config-\n\n=> ALTER SYSTEM RESET wal_level;\n\n=> ALTER SYSTEM RESET max_wal_senders;\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\nLet’s repeat the same workflow as before (but now we will insert only one row to get a neater output):\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/45D88E48\n\n(1 row)\n\n=> BEGIN;\n\n=> TRUNCATE TABLE wal;\n\n=> INSERT INTO wal VALUES (42);\n\n=> COMMIT;\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/45D89108\n\n(1 row)\n\nCheck out the generated ��� entries.\n\nApart from what we have seen at the minimal level, we have also got the following entries:\n\nreplication-related entries of the Standby resource manager: �������_�����\n\n(active transactions) and ����\n\nthe entry that logs the ������+���� operation,which initializes a new page and\n\ninserts a new row into this page\n\n225\n\nv. ��\n\nChapter 11 WAL Modes\n\npostgres$ /usr/local/pgsql/bin/pg_waldump \\ -p /usr/local/pgsql/data/pg_wal -s 0/45D88E48 -e 0/45D89108\n\nrmgr: Standby 0/45D88E48, prev 0/45D88DD0, desc: LOCK xid 122846 db 16391 rel 16562 rmgr: Storage 0/45D88E78, prev 0/45D88E48, desc: CREATE base/16391/24786 rmgr: Heap 0/45D88EA8, prev 0/45D88E78, desc: UPDATE off 49 xmax 122846 flags 0x60 ; new off 50 xmax 0, blkref #0: rel 1663/16391/1259 blk 0 rmgr: Btree 0/45D88F28, prev 0/45D88EA8, desc: INSERT_LEAF off 178, blkref #0: rel 1663/16391/2662 blk 2 rmgr: Btree 0/45D88F68, prev 0/45D88F28, desc: INSERT_LEAF off 149, blkref #0: rel 1663/16391/2663 blk 2 rmgr: Btree 0/45D88FA8, prev 0/45D88F68, desc: INSERT_LEAF off 256, blkref #0: rel 1663/16391/3455 blk 4 rmgr: Heap 0/45D88FE8, prev 0/45D88FA8, desc: INSERT+INIT off 1 flags 0x00, blkref #0: rel 1663/16391/24786 blk 0 rmgr: Standby 0/45D89028, prev 0/45D88FE8, desc: LOCK xid 122846 db 16391 rel 16562 rmgr: Standby 0/45D89058, prev 0/45D89028, desc: RUNNING_XACTS nextXid 122847 latestCompletedXid 122845 oldestRunningXid 122846; 1 xacts: 122846 122846, lsn: rmgr: Transaction len (rec/tot): 0/45D89090, prev 0/45D89058, desc: COMMIT 2023−03−06 14:04:14.538399 MSK; rels: base/16391/24785; inval msgs: catcache 51 catcache 50 relcache 16562\n\n122846, lsn:\n\nlen (rec/tot):\n\n42, tx:\n\n42/\n\n122846, lsn:\n\n42, tx:\n\n42/\n\nlen (rec/tot):\n\n123, tx:\n\n122846, lsn:\n\nlen (rec/tot):\n\n123/\n\n122846, lsn:\n\n64, tx:\n\n64/\n\nlen (rec/tot):\n\n64, tx:\n\n122846, lsn:\n\n64/\n\nlen (rec/tot):\n\n122846, lsn:\n\n64/\n\nlen (rec/tot):\n\n64, tx:\n\n122846, lsn:\n\nlen (rec/tot):\n\n59, tx:\n\n59/\n\n0, lsn:\n\nlen (rec/tot):\n\n42, tx:\n\n42/\n\n54, tx:\n\n54/\n\nlen (rec/tot):\n\n0, lsn:\n\n114, tx:\n\n114/\n\nLogical\n\nLast but not least,the logical level enables logical decoding and logical replication. It has to be activated on the publishing server.\n\nIf we take a look at ��� entries, we will see that this level is almost the same as replica: it adds the entries related to replication sources and some arbitrary logical entries that may be generated by applications. For the most part, logical decoding depends on the information about active transactions (�������_�����) because it requires capturing a snapshot to track system catalog changes.\n\n226\n\nPart III\n\nLocks\n\n12\n\nRelation-Level Locks\n\n12.1 About Locks\n\nLocks control concurrent access to shared resources.\n\nConcurrent access implies that several processes try to get one and the same re- source at the same time. It makes no difference whether these processes are ex- ecuted in parallel (if the hardware permits) or sequentially in the time-sharing mode. If there is no concurrent access, there is no need to acquire locks (for exam- ple, shared buffer cache requires locking, while local cache can do without it).\n\nBefore accessing a resource, the process must acquire a lock on it; when the oper- ation is complete, this lock must be released for the resource to become available to other processes. If locks are managed by the database system, the established order of operations is maintained automatically; if locks are controlled by the ap- plication, the protocol must be enforced by the application itself.\n\nAtalowlevel,alockissimplyachunkofsharedmemorythatdefinesthelockstatus (whether it is acquired or not); it can also provide some additional information, such as the process number or acquisition time.\n\nAs you can guess, a shared memory segment is a resource in its own right. Concurrent access to such resources is regulated bysynchronization primitives (such as semaphores or mutexes) provided by the operating system. They guarantee strictly consecutive execution of the code that accesses a shared resource. At the lowest level,these primitives are based on atomic ��� instructions (such as test-and-set or compare-and-swap).\n\nIn general, we can use locks to protect any resource as long as it can be unambigu- ously identified and assigned a particular lock address.\n\n229\n\nChapter 12 Relation-Level Locks\n\nFor example, we can lock a database object, such as a table (identified by oid in the system catalog), a data page (identified by a filename and a position within this file),a row version (identified by a page and an offset within this page). We can also lock a memory structure,such as a hash table or a buffer (identified by an assigned ��). We can even lock an abstract resource that has no physical representation.\n\nBut it is not always possible to acquire a lock at once: a resource can be already locked by someone else. Then the process either joins the queue (if it is allowed for this particular lock type) or tries again some time later. Either way, it has to wait for the lock to be released.\n\nI would like to single out two factors that can greatly affect locking efficiency.\n\nGranularity, or the“grain size”of a lock. Granularity is important if resources form\n\na hierarchy.\n\nFor example, a table consists of pages, which, in their turn, consist of tu- ples. All these objects can be protected by locks. Table-level locks are coarse- grained; they forbid concurrent access even if the processes need to get to different pages or rows.\n\nRow-level locks are fine-grained, so they do not have this drawback; however, the number of locks grows. To avoid using too much memory for lock-related metadata, Postgre��� can apply various methods, one of them being lock es- calation: if the number of fine-grained locks exceeds a certain threshold, they are replaced by a single lock of coarser granularity.\n\nA set of modes in which a lock can be acquired.\n\nAs a rule, only two modes are applied. The exclusive mode is incompatible with all the other modes, including itself. The shared mode allows a resource to be locked by several processes at a time. The shared mode can be used for reading, while the exclusive mode is applied for writing.\n\nIn general, there may be other modes too. Names of modes are unimportant, it is their compatibility matrix that matters.\n\nFiner granularity and support for multiple compatible modes give more opportu- nities for concurrent execution.\n\n230\n\n12.2 Heavyweight Locks\n\nAll locks can be classified by their duration.\n\nLong-term locks are acquired for a potentially long time (in most cases,till the end of the transaction); they typically protect such resources as relations and rows. These locks are usually managed by Postgre��� automatically, but a user still has some control over this process.\n\nLong-term locks offer multiple modes that enable various concurrent oper- ations on data. They usually have extensive infrastructure (including such features as wait queues, deadlock detection, and instrumentation) since its maintenance is anyway much cheaper than operations on protected data.\n\nShort-term locks are acquired for fractions of a second and rarely last longer than several ��� instructions; they usually protect data structures in the shared memory. Postgre��� manages such locks in a fully automated way.\n\nShort-term locks typically offer very few modes and only basic infrastructure, which may have no instrumentation at all.\n\nPostgre��� supports various types of locks.1 Heavyweight locks (which are acquired locks are considered long-term. Short- on relations and other objects) and row-level term locks comprise various locks on memory structures . Besides, there is also a distinct group of predicate locks\n\n, which, despite their name, are not locks at all.\n\n12.2 Heavyweight Locks\n\nHeavyweight locks are long-term ones. Acquired at the object level,they are mainly used for relations, but can also be applied to some other types of objects. Heavy- weightlockstypicallyprotectobjectsfromconcurrentupdatesorforbidtheirusage during restructuring,but they can address other needs too. Such a vague definition is deliberate: locks of this type are used for all kinds of purposes. The only thing they have in common is their internal structure.\n\nUnless explicitly specified otherwise, the term lock usually implies a heavyweight lock.\n\n1 backend/storage/lmgr/README\n\n231\n\np. ��� p. ��� p. ���\n\n64 100\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\nChapter 12 Relation-Level Locks\n\nHeavyweightlocksarelocatedintheserver’ssharedmemory1 andcanbedisplayed inthepg_locksview. Theirtotalnumberislimitedbythe max_locks_per_transaction max_connections. value multiplied by\n\nAll transactions use a common pool of locks, so one transaction can acquire more than max_locks_per_transaction locks. What really matters is that the total number of locks in the system does not exceed the defined limit. Since the pool is initial- ized when the server is launched,changing any of these two parameters requires a server restart.\n\nIf a resource is already locked in an incompatible mode, the process trying to ac- quire another lock joins the queue. Waiting processes do not waste ��� time: they fall asleep until the lock is released and the operating system wakes them up.\n\nTwo transactions can find themselves in a deadlock if the first transaction is unable to continue its operation until it gets a resource locked by the other transaction, which, in its turn, needs a resource locked by the first transaction. This case is rather simple; a deadlock can also involve more than two transactions. Since dead- locks cause infinite waits, Postgre��� detects them automatically and aborts one of the affected transactions to ensure that normal operation can continue.\n\nDifferent types of heavyweight locks serve different purposes, protect different re- sources, and support different modes, so we will consider them separately.\n\nThe following list provides the names of lock types as they appear in the locktype column of the pg_locks view:\n\ntransactionid and virtualxid — a lock\n\non a transaction ��\n\nrelation — a relation-level lock\n\ntuple — a lock acquired on a tuple\n\nobject — a lock on an object\n\nthat is not a relation\n\nextend — a relation extension lock\n\npage — a page-level lock\n\nused by some index types\n\nadvisory — an advisory lock\n\n1 backend/storage/lmgr/lock.c\n\n232\n\n12.3 Locks on Transaction IDs\n\nAlmost all heavyweight locks are acquired automatically as needed and are re- leased automatically when the corresponding transaction completes. There are some exceptions though: for example, a relation-level lock can be set explicitly, while advisory locks are always managed by users.\n\n12.3 Locks on Transaction IDs\n\nEach transaction always holds an exclusive lock on its own �� (both virtual if available).\n\nand real,\n\nPostgre��� offers two locking modes for this purpose, exclusive and shared. Their compatibility matrix is very simple: the shared mode is compatible with itself, while the exclusive mode cannot be combined with any mode.\n\nShared Exclusive\n\nShared\n\n×\n\nExclusive\n\n×\n\n×\n\nTo track completion of a particular transaction, a process can request a lock on this transaction’s ��, in any mode. Since the transaction itself is already holding an exclusive lock on its own ��, another lock is impossible to acquire. The process requesting this lock joins the queue and falls asleep. Once the transaction com- pletes, the lock is released, and the queued process wakes up. Clearly, it will not manage to acquire the lock because the corresponding resource has already disap- peared, but this lock is not what is actually needed anyway.\n\nLet’s start a transaction in a separate session and get the process �� (���) of the backend:\n\n=> BEGIN;\n\n=> SELECT pg_backend_pid();\n\npg_backend_pid −−−−−−−−−−−−−−−−\n\n28980\n\n(1 row)\n\nThe started transaction holds an exclusive lock on its own virtual ��:\n\n233\n\np. ��\n\nChapter 12 Relation-Level Locks\n\n=> SELECT locktype, virtualxid, mode, granted FROM pg_locks WHERE pid = 28980;\n\nlocktype\n\n| virtualxid |\n\nmode\n\n| granted\n\n−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−\n\nvirtualxid | 5/2\n\n| ExclusiveLock | t\n\n(1 row)\n\nHere locktype is the type of the lock, virtualxid is the virtual transaction �� (which identifies the locked resource), and mode is the locking mode (exclusive in this case). The granted flag shows whether the requested lock has been acquired.\n\nOnce the transaction gets a real ��, the corresponding lock is added to this list:\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n122849\n\n(1 row)\n\n=> SELECT locktype, virtualxid, transactionid AS xid, mode, granted FROM pg_locks WHERE pid = 28980;\n\nlocktype\n\n| virtualxid |\n\nxid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−\n\nvirtualxid transactionid |\n\n| 5/2\n\n| ExclusiveLock | t | | 122849 | ExclusiveLock | t\n\n(2 rows)\n\nNow this transaction holds exclusive locks on both its ��s.\n\n12.4 Relation-Level Locks\n\nPostgre��� provides as many as eight modes in which a relation (a table, an index, or any other object) can be locked.1 Such a variety allows you to maximize the number of concurrent commands that can be run on a relation.\n\nThe next page shows the compatibility matrix extended with examples of com- mands that require the corresponding locking modes. There is no point in mem- orizing all these modes or trying to find the logic behind their naming, but it is\n\n1 postgresql.org/docs/14/explicit-locking#LOCKING-TABLES.html\n\n234\n\n12.4 Relation-Level Locks\n\ndefinitely useful to look through this data, draw some general conclusions, and refer to this table as required.\n\nAS RS RE SUE S SRE E AE\n\nAccess Share\n\n× SELECT\n\nRow Share\n\n× × SELECTFOR UPDATE/SHARE\n\nRow Exclusive\n\n× × × × INSERT,UPDATE,DELETE\n\nShare Update Exclusive\n\n× × × × × VACUUM,CREATE INDEX CONCURRENTLY\n\nShare\n\n× ×\n\n× × × CREATE INDEX\n\nShare Row Exclusive\n\n× × × × × × CREATE TRIGGER\n\nExclusive\n\n× × × × × × × REFRESH MAT.VIEW CONCURRENTLY\n\nAccess Exclusive\n\n× × × × × × × × DROP,TRUNCATE,VACUUM FULL,\n\nLOCKTABLE,REFRESH MAT.VIEW\n\nThe Access Share mode is the weakest one; it can be used with any other mode except Access Exclusive, which is incompatible with all the modes. Thus, a ������ command can be run in parallel with almost any operation, but it does not let you drop a table that is being queried.\n\nThe first four modes allow concurrent heap modifications, while the other four do not. For example, the ������ ����� command uses the Share mode, which is com- patible with itself (so you can create several indexes on a table concurrently) and with the modes used by read-only operations. As a result, ������ commands can run in parallel with index creation,while ������, ������,and ������ commands will be blocked.\n\nConversely, unfinished transactions that modify heap data block the ������ ����� command. Instead, you can call ������ ����� ������������, which uses a weaker Share Update Exclusive mode: it takes longer to create an index (and this operation can even fail), but in return, concurrent data updates are allowed.\n\nThe ����� ����� command has multiple flavors that use different locking modes (Share Update Exclusive, Share Row Exclusive, Access Exclusive). All of them are described in the documentation.1\n\n1 postgresql.org/docs/14/sql-altertable.html\n\n235",
      "page_number": 213
    },
    {
      "number": 12,
      "title": "Relation-Level Locks",
      "start_page": 231,
      "end_page": 242,
      "detection_method": "regex_chapter",
      "content": "Chapter 12 Relation-Level Locks\n\nExamples in this part of the book rely on the accounts table again:\n\n=> TRUNCATE accounts;\n\n=> INSERT INTO accounts(id, client, amount) VALUES\n\n100.00), (1, 'alice', (2, 'bob', 200.00), (3, 'charlie', 300.00);\n\nWe will have to access the pg_locks table more than once,so let’s create a view that shows all ��s in a single column, thus making the output more concise:\n\n=> CREATE VIEW locks AS SELECT pid, locktype, CASE locktype\n\nWHEN 'relation' THEN relation::regclass::text WHEN 'transactionid' THEN transactionid::text WHEN 'virtualxid' THEN virtualxid\n\nEND AS lockid, mode, granted\n\nFROM pg_locks ORDER BY 1, 2, 3;\n\nThe transaction that is still running in the first session updates a row. This opera- tion locks the accounts table and all its indexes, which results in two new locks of the relation type acquired in the Row Exclusive mode:\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\n=> SELECT locktype, lockid, mode, granted FROM locks WHERE pid = 28980;\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\nrelation relation transactionid | 122849 virtualxid\n\n| accounts | RowExclusiveLock | t | accounts_pkey | RowExclusiveLock | t | t | t\n\n| ExclusiveLock | ExclusiveLock\n\n| 5/2\n\n(4 rows)\n\n236\n\n12.5 Wait Queue\n\n12.5 Wait Queue\n\nHeavyweight locks form a fair wait queue.1 A process joins the queue if it attempts to acquire a lock that is incompatible either with the current lock or with the locks requested by other processes already in the queue.\n\nWhile the first session is working on an update, let’s try to create an index on this table in another session:\n\n=> SELECT pg_backend_pid();\n\npg_backend_pid −−−−−−−−−−−−−−−−\n\n29459\n\n(1 row)\n\n=> CREATE INDEX ON accounts(client);\n\nThe command hangs,waiting for the resource to be released. The transaction tries to lock the table in the Share mode but cannot do it:\n\n=> SELECT locktype, lockid, mode, granted FROM locks WHERE pid = 29459;\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−\n\nrelation virtualxid | 6/3\n\n| accounts | ShareLock\n\n| f | ExclusiveLock | t\n\n(2 rows)\n\nNowletthethirdsessionstartthe������ ����command. Itwillalsojointhequeue because it requires the Access Exclusive mode, which conflicts with all the other modes:\n\n=> SELECT pg_backend_pid();\n\npg_backend_pid −−−−−−−−−−−−−−−−\n\n29662\n\n(1 row)\n\n=> VACUUM FULL accounts;\n\n1 backend/storage/lmgr/lock.c, LockAcquire function\n\n237\n\nChapter 12 Relation-Level Locks\n\n=> SELECT locktype, lockid, mode, granted FROM locks WHERE pid = 29662;\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\nrelation transactionid | 122853 virtualxid\n\n| accounts | AccessExclusiveLock | f | t | ExclusiveLock | t | ExclusiveLock\n\n| 7/4\n\n(3 rows)\n\nAll the subsequent contenders will now have to join the queue, regardless of their locking mode. Even simple ������ queries will honestly follow ������ ����, al- though they are compatible with the Row Exclusive lock held by the first session performing the update.\n\n=> SELECT pg_backend_pid();\n\npg_backend_pid −−−−−−−−−−−−−−−−\n\n29872\n\n(1 row)\n\n=> SELECT * FROM accounts;\n\n=> SELECT locktype, lockid, mode, granted FROM locks WHERE pid = 29872;\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\nrelation virtualxid | 8/3\n\n| accounts | AccessShareLock | f | t\n\n| ExclusiveLock\n\n(2 rows)\n\nT1\n\nUPDATE\n\nT2\n\nrelation\n\nT3\n\nCREATE INDEX\n\nT4\n\nVACUUM FULL\n\nSELECT\n\n238\n\n12.5 Wait Queue\n\npg_blocking_pids function gives a high-level overview of all waits. It shows The the ��s of all processes queued before the specified one that are already holding or would like to acquire an incompatible lock:\n\n=> SELECT pid,\n\npg_blocking_pids(pid), wait_event_type, state, left(query,50) AS query\n\nFROM pg_stat_activity WHERE pid IN (28980,29459,29662,29872) \\gx\n\n−[ RECORD 1 ]−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− pid pg_blocking_pids | {} wait_event_type | Client state | idle in transaction | UPDATE accounts SET amount = amount + 100.00 WHERE query −[ RECORD 2 ]−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− pid pg_blocking_pids | {28980} | Lock wait_event_type | active state query | CREATE INDEX ON accounts(client); −[ RECORD 3 ]−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− pid pg_blocking_pids | {28980,29459} wait_event_type state query −[ RECORD 4 ]−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− pid pg_blocking_pids | {29662} wait_event_type state query\n\n| 28980\n\n| 29459\n\n| 29662\n\n| Lock | active | VACUUM FULL accounts;\n\n| 29872\n\n| Lock | active | SELECT * FROM accounts;\n\nTogetmoredetails,youcanreviewtheinformationprovidedinthepg_lockstable.1\n\nOnce the transaction is completed (either committed or aborted), all its locks are released.2 The first process in the queue gets the requested lock and wakes up.\n\n1 wiki.postgresql.org/wiki/Lock_dependency_information 2 backend/storage/lmgr/lock.c, LockReleaseAll & LockRelease functions\n\n239\n\nv. �.�\n\nChapter 12 Relation-Level Locks\n\nHere the transaction commit in the first session leads to sequential execution of all the queued processes:\n\n=> ROLLBACK;\n\nROLLBACK\n\nCREATE INDEX\n\nVACUUM\n\nid | client\n\n| amount\n\n−−−−+−−−−−−−−−+−−−−−−−−\n\n| 100.00 1 | alice 2 | bob | 200.00 3 | charlie | 300.00\n\n(3 rows)\n\n240\n\n13\n\nRow-Level Locks\n\n13.1 Lock Design\n\nThanks to snapshot isolation, heap tuples do not have to be locked for reading. However, two write transactions must not be allowed to modify one and the same row at the same time. Rows must be locked in this case, but heavyweight locks are not a very good choice for this purpose: each of them takes space in the server’s shared memory (hundreds of bytes, not to mention all the supporting infrastruc- ture),andPostgre���internalmechanismsarenotdesignedtohandleahugenum- ber of concurrent heavyweight locks.\n\nSome database systems solvethis problem bylock escalation: if row-levellocks are too many, they are replaced by a single lock of finer granularity (for example, by a page-level or table-level lock). It simplifies the implementation, but can greatly limit system throughput.\n\nIn Postgre���, the information on whether a particular row is locked is kept only in the header of its current heap tuple. Row-level locks are virtually attributes in heap pages rather than actual locks, and they are not reflected in ��� in any way.\n\nA row is typically locked when it is being updated or deleted. In both cases, the current version of the row is marked as deleted. The attribute used for this pur- pose is the current transaction’s �� specified in the xmax field, and it is the same �� (combined with additional hint bits) that indicates that the row is locked. If a transaction wants to modify a row but sees an active transaction �� in the xmax field of its current version, it has to wait for this transaction to complete. Once it is over, all the locks are released, and the waiting transaction can proceed.\n\nThis mechanism allows locking as many rows as required at no extra cost.\n\n241\n\np. ��\n\nChapter 13 Row-Level Locks\n\nThe downside of this solution is that other processes cannot form a queue, as ��� contains no information about such locks. Therefore, heavyweight locks are still required: a process waiting for a row to be released requests a lock on the �� of the transaction currently busy with this row. Once the transaction completes, the row becomes available again. Thus, the number of heavyweight locks is proportional to the number of concurrent processes rather than rows being modified.\n\n13.2 Row-Level Locking Modes\n\nRow-level locks support four modes.1 Two of them implement exclusive locks that can be acquired by only one transaction at a time, while the other two provide shared locks that can be held by several transactions simultaneously.\n\nHere is the compatibility matrix of these modes:\n\nKeyShare\n\nShare\n\nNo Key Update\n\nUpdate\n\nKeyShare\n\n×\n\nShare\n\n×\n\n×\n\nNo KeyUpdate\n\n×\n\n×\n\n×\n\nUpdate\n\n×\n\n×\n\n×\n\n×\n\nExclusive Modes\n\nThe Update mode allows modifying any tuple fields and even deleting the whole tuple, while the No Key Update mode permits only those changes that do not in- volve any fields related to unique indexes (in other words, foreign keys must not be affected).\n\nThe ������ command automatically chooses the weakest locking mode possible; keys usually remain unchanged, so rows are typically locked in the No Key Update mode.\n\n1 postgresql.org/docs/14/explicit-locking#LOCKING-ROWS.html\n\n242\n\n13.2 Row-Level Locking Modes\n\nLet’s create a function that uses pageinspect to display some tuple metadata that we are interested in, namely the xmax field and several hint bits:\n\n=> CREATE FUNCTION row_locks(relname text, pageno integer) RETURNS TABLE(\n\nctid tid, xmax text, lock_only text, is_multi text, keys_upd text, keyshr text, shr text\n\n) AS $$ SELECT (pageno,lp)::text::tid,\n\nt_xmax, & 128 CASE WHEN t_infomask CASE WHEN t_infomask & 4096 CASE WHEN t_infomask2 & 8192 CASE WHEN t_infomask CASE WHEN t_infomask\n\n= 128 = 4096 = 8192 = 16\n\nTHEN 't' END, THEN 't' END, THEN 't' END, THEN 't' END,\n\n& 16 & 16+64 = 16+64 THEN 't' END\n\nFROM heap_page_items(get_raw_page(relname,pageno)) ORDER BY lp; $$ LANGUAGE sql;\n\nNow start a transaction on the accounts table to update the balance of the first account (the key remains the same) and the �� of the second account (the key gets updated):\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\n=> UPDATE accounts SET id = 20 WHERE id = 2;\n\nThe page now contains the following metadata:\n\n=> SELECT * FROM row_locks('accounts',0) LIMIT 2;\n\nctid\n\n|\n\nxmax\n\n| lock_only | is_multi | keys_upd | keyshr | shr\n\n−−−−−−−+−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−+−−−−−\n\n(0,1) | 122858 | (0,2) | 122858 |\n\n| |\n\n| | t\n\n| |\n\n| |\n\n(2 rows)\n\nThe locking mode is defined by the keys_updated hint bit.\n\n=> ROLLBACK;\n\n243\n\nChapter 13 Row-Level Locks\n\nThe ������ ��� command uses the same xmax field as a locking attribute, but in this case the xmax_lock_only hint bit must also be set. This bit indicates that the tuple is locked but not deleted, which means that it is still current:\n\n=> BEGIN;\n\n=> SELECT * FROM accounts WHERE id = 1 FOR NO KEY UPDATE;\n\n=> SELECT * FROM accounts WHERE id = 2 FOR UPDATE;\n\n=> SELECT * FROM row_locks('accounts',0) LIMIT 2;\n\nctid\n\n|\n\nxmax\n\n| lock_only | is_multi | keys_upd | keyshr | shr\n\n−−−−−−−+−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−+−−−−−\n\n(0,1) | 122859 | t (0,2) | 122859 | t\n\n| |\n\n| | t\n\n| |\n\n| |\n\n(2 rows)\n\n=> ROLLBACK;\n\nShared Modes\n\nThe Share mode can be applied when a row needs to be read, but its modification by another transaction must be forbidden. The Key Share mode allows updating any tuple fields except key attributes.\n\nOf all the shared modes, the Postgre��� core uses only Key Share, which is applied when foreign keys are being checked. Since it is compatible with the No Key Update exclusive mode, foreign key checks do not interfere with concurrent updates of non-key attributes. As for applications, they can use any shared modes they like.\n\nLet me stress once again that simple ������ commands never use row-level locks.\n\n=> BEGIN;\n\n=> SELECT * FROM accounts WHERE id = 1 FOR KEY SHARE;\n\n=> SELECT * FROM accounts WHERE id = 2 FOR SHARE;\n\nHere is what we see in the heap tuples:\n\n=> SELECT * FROM row_locks('accounts',0) LIMIT 2;\n\nctid\n\n|\n\nxmax\n\n| lock_only | is_multi | keys_upd | keyshr | shr\n\n−−−−−−−+−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−+−−−−−\n\n(0,1) | 122860 | t (0,2) | 122860 | t\n\n| |\n\n| |\n\n| t | t\n\n| | t\n\n(2 rows)\n\n244\n\n13.3 Multitransactions\n\nThexmax_keyshr_lockbitissetforbothoperations,butyoucanrecognizetheShare mode by other hint bits.1\n\n13.3 Multitransactions\n\nAs we have seen,the locking attribute is represented by the xmax field,which is set to the �� of the transaction that has acquired the lock. So how is this attribute set for a shared lock held by several transactions at a time?\n\nWhen dealing with shared locks, Postgre��� applies so-called multitransactions (multixacts).2 Amultitransaction is a group of transactions that is assigned a sepa- rate ��. Detailed information on group members and their locking modes is stored in files under the ������/pg_multixact directory. For faster access,locked pages are cached in the shared memory of the server;3 all changes are logged to ensure fault tolerance.\n\nMultixact ��s have the same ��-bit length as regular transaction ��s, but they are issued independently. It means that transactions and multitransactions can po- tentially have the same ��s. To differentiate between the two, Postgre��� uses an additional hint bit: xmax_is_multi.\n\nLet’s add one more exclusive lock acquired by another transaction (Key Share and No Key Update modes are compatible):\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\n=> SELECT * FROM row_locks('accounts',0) LIMIT 2;\n\nctid\n\n|\n\nxmax\n\n| lock_only | is_multi | keys_upd | keyshr | shr\n\n−−−−−−−+−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−+−−−−−\n\n(0,1) | 1 (0,2) | 122860 | t\n\n|\n\n| t |\n\n| |\n\n| | t\n\n| | t\n\n(2 rows)\n\n1 include/access/htup_details.h 2 backend/access/transam/multixact.c 3 backend/access/transam/slru.c\n\n245\n\np. ���\n\nChapter 13 Row-Level Locks\n\nThe xmax_is_multi bit shows that the first row uses a multitransaction �� instead of a regular one.\n\nWithout going into further implementation details, let’s display the information on all the possible row-level locks using the pgrowlocks extension:\n\n=> CREATE EXTENSION pgrowlocks;\n\n=> SELECT * FROM pgrowlocks('accounts') \\gx\n\n−[ RECORD 1 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−− locked_row | (0,1) locker multi xids modes pids −[ RECORD 2 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−− locked_row | (0,2) locker multi xids modes pids\n\n| 1 | t | {122860,122861} | {\"Key Share\",\"No Key Update\"} | {30423,30723}\n\n| 122860 | f | {122860} | {\"For Share\"} | {30423}\n\nIt looks a lot like querying the pg_locks view, but the pgrowlocks function has to access heap pages, as ��� contains no information on row-level locks.\n\n=> COMMIT;\n\n=> ROLLBACK;\n\nSince multixact ��s are ��-bit, they are subject to wraparound because of counter limits, just like regular transaction ��s. Therefore, Postgre��� has to process mul- tixact ��s in a way similar to freezing: old multixact ��s are replaced with new ones (or with a regular transaction �� if only one transaction is holding the lock by that time).1\n\nBut while regular transaction ��s are frozen only in the xmin field (as a non-empty xmax indicates that the tuple is outdated and will soon be removed), it is the xmax field that has to be frozen for multitransactions: the current row version may be repeatedly locked by new transactions in a shared mode.\n\n1 backend/access/heap/heapam.c, FreezeMultiXactId function\n\n246\n\n13.4 Wait Queue\n\nFreezing of multitransactions can be managed by server parameters, which are similar to those provided for regular freezing: vacuum_multixact_freeze_min_age, vacuum_multixact_freeze_table_age, autovacuum_multixact_freeze_max_age, as well as vacuum_multixact_failsafe_age\n\n.\n\n13.4 Wait Queue\n\nExclusive Modes\n\nSince a row-level lock is just an attribute, the queue is arranged in a not-so-trivial way. When a transaction is about to modify a row, it has to follow these steps:1\n\n� If the xmax field and the hint bits indicate that the row is locked in an incom- patible mode,acquire an exclusive heavyweight lock on the tuple that is being modified.\n\n� If necessary, wait until all the incompatible locks are released by requesting a lockonthe��ofthexmaxtransaction(orseveraltransactionsifxmaxcontains a mutixact ��).\n\n� Write its own �� into xmax in the tuple header and set the required hint bits.\n\n� Release the tuple lock if it was acquired in the first step.\n\nA tuple lock is yet another kind of heavyweight locks,which has the tuple type (not to be confused with a regular row-level lock).\n\nIt may seem that steps � and � are redundant and it is enough to simply wait until all the locking transactions are over. However, if several transactions are trying to update one and the same row, all of them will be waiting on the transaction currentlyprocessingthisrow. Onceitcompletes,theywillfindthemselvesinarace condition for the right to lock the row, and some “unlucky” transactions may have to wait for an indefinitely long time. Such a situation is called resource starvation.\n\nA tuple lock identifies the first transaction in the queue and guarantees that it will be the next one to get the lock.\n\n1 backend/access/heap/README.tuplock\n\n247\n\nv. ��",
      "page_number": 231
    },
    {
      "number": 13,
      "title": "Row-Level Locks",
      "start_page": 243,
      "end_page": 266,
      "detection_method": "regex_chapter",
      "content": "Chapter 13 Row-Level Locks\n\nBut you can see it for yourself. Since Postgre��� acquires many different locks during its operation,and each of them is reflected in a separate row in the pg_locks table, I am going to create yet another view on top of pg_locks. It will show this information in a more concise form,keeping only those locks that we are currently interested in (the ones related to the accounts table and to the transaction itself, except for any locks on virtual ��s):\n\n=> CREATE VIEW locks_accounts AS SELECT pid, locktype, CASE locktype\n\nWHEN 'relation' THEN relation::regclass::text WHEN 'transactionid' THEN transactionid::text WHEN 'tuple' THEN relation::regclass||'('||page||','||tuple||')'\n\nEND AS lockid, mode, granted\n\nFROM pg_locks WHERE locktype in ('relation','transactionid','tuple')\n\nAND (locktype != 'relation' OR relation = 'accounts'::regclass)\n\nORDER BY 1, 2, 3;\n\nLet’s start the first transaction and update a row:\n\n=> BEGIN;\n\n=> SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122863 |\n\n30723\n\n(1 row)\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\nThetransactionhascompletedallthefourstepsoftheworkflowandisnowholding a lock on the table:\n\n=> SELECT * FROM locks_accounts WHERE pid = 30723;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30723 | relation 30723 | transactionid | 122863\n\n| accounts | RowExclusiveLock | t | t\n\n| ExclusiveLock\n\n(2 rows)\n\nStart the second transaction and try to update the same row. The transaction will hang, waiting on a lock:\n\n248\n\n13.4 Wait Queue\n\n=> BEGIN;\n\n=> SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122864 |\n\n30794\n\n(1 row)\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\nT1 No KeyUpdate\n\nT2\n\nctid xmin xmax (0,1) T1\n\ndata\n\ntuple (0,1)\n\nThe second transaction only gets as far as the second step. For this reason, apart from locking the table and its own ��, it adds two more locks, which are also re- flected in the pg_locks view: the tuple lock acquired at the first step and the lock of the �� of the second transaction requested at the second step:\n\n=> SELECT * FROM locks_accounts WHERE pid = 30794;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30794 | relation 30794 | transactionid | 122863 30794 | transactionid | 122864 30794 | tuple\n\n| RowExclusiveLock | t | f | ShareLock | t | ExclusiveLock | t | accounts(0,1) | ExclusiveLock\n\n| accounts\n\n(4 rows)\n\nThe third transaction will get stuck on the first step. It will try to acquire a lock on the tuple and will stop at this point:\n\n=> BEGIN; => SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122865 |\n\n30865\n\n(1 row) => UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\n249\n\nChapter 13 Row-Level Locks\n\n=> SELECT * FROM locks_accounts WHERE pid = 30865;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30865 | relation 30865 | transactionid | 122865 30865 | tuple\n\n| accounts\n\n| RowExclusiveLock | t | t | ExclusiveLock | f | accounts(0,1) | ExclusiveLock\n\n(3 rows)\n\nThe fourth and all the subsequent transactions trying to update this row will not differ from the third transaction in this respect: all of them will be waiting on the same tuple lock.\n\n=> BEGIN;\n\n=> SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122866 |\n\n30936\n\n(1 row)\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\n=> SELECT * FROM locks_accounts WHERE pid = 30865;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30865 | relation 30865 | transactionid | 122865 30865 | tuple\n\n| accounts\n\n| RowExclusiveLock | t | t | ExclusiveLock | f | accounts(0,1) | ExclusiveLock\n\n(3 rows)\n\nT1 No KeyUpdate\n\nT2\n\nctid xmin xmax (0,1) T1\n\ndata\n\nT3\n\ntuple (0,1)\n\nT4\n\nTo get the full picture of the current waits,you can extend the pg_stat_activity view with the information on locking processes:\n\n250\n\n13.4 Wait Queue\n\n=> SELECT pid,\n\nwait_event_type, wait_event, pg_blocking_pids(pid)\n\nFROM pg_stat_activity WHERE pid IN (30723,30794,30865,30936);\n\npid\n\n| wait_event_type |\n\nwait_event\n\n| pg_blocking_pids\n\n−−−−−−−+−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−\n\n30723 | Client 30794 | Lock 30865 | Lock 30936 | Lock\n\n| ClientRead | transactionid | {30723} | {30794} | tuple | {30794,30865} | tuple\n\n| {}\n\n(4 rows)\n\nIf the first transaction is aborted, everything will work as expected: all the subse- quent transactions will move one step further without jumping the queue.\n\nAnd yet it is more likely that the first transaction will be committed. At the Repeat- able Read or Serializable isolation levels, it would result in a serialization failure, so the second transaction would have to be aborted1 (and all the subsequent trans- actions in the queue would get aborted too). But at the Read Committed isolation level the modified row will be re-read, and its update will be retried.\n\nSo, the first transaction is committed:\n\n=> COMMIT;\n\nThe second transaction wakes up and successfully completes the third and the fourth steps of the workflow:\n\nUPDATE 1\n\n=> SELECT * FROM locks_accounts WHERE pid = 30794;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30794 | relation 30794 | transactionid | 122864\n\n| accounts | RowExclusiveLock | t | t\n\n| ExclusiveLock\n\n(2 rows)\n\nAs soon as the second transaction releases the tuple lock, the third one also wakes up, but it sees that the xmax field of the new tuple contains a different �� already.\n\n1 backend/executor/nodeModifyTable.c, ExecUpdate function\n\n251\n\nChapter 13 Row-Level Locks\n\nAt this point, the above workflow is over. At the Read Committed isolation level, one more attempt to lock the row is performed,1 but it does not follow the outlined steps. The third transaction is now waiting for the second one to complete without trying to acquire a tuple lock:\n\n=> SELECT * FROM locks_accounts WHERE pid = 30865;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30865 | relation 30865 | transactionid | 122864 30865 | transactionid | 122865\n\n| accounts | RowExclusiveLock | t | f | ShareLock | t | ExclusiveLock\n\n(3 rows)\n\nThe fourth transaction does the same:\n\n=> SELECT * FROM locks_accounts WHERE pid = 30936;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30936 | relation 30936 | transactionid | 122864 30936 | transactionid | 122866\n\n| accounts | RowExclusiveLock | t | f | ShareLock | t | ExclusiveLock\n\n(3 rows)\n\nNow both the third and the fourth transactions are waiting for the second one to complete, risking to get into a race condition. The queue has virtually fallen apart.\n\nT3\n\nT4\n\nT2 No KeyUpdate\n\nctid xmin xmax (0,1) T1 (0,2) T2\n\nT1\n\ndata\n\nIf other transactions had joined the queue while it still existed, all of them would have been dragged into this race.\n\n1 backend/access/heap/heapam_handler.c, heapam_tuple_lock function\n\n252\n\n13.4 Wait Queue\n\nConclusion: it is not a good idea to update one and the same table row in mul- tiple concurrent processes. Under high load, this hotspot can quickly turn into a bottleneck that causes performance issues.\n\nLet’s commit all the started transactions.\n\n=> COMMIT;\n\nUPDATE 1\n\n=> COMMIT;\n\nUPDATE 1\n\n=> COMMIT;\n\nShared Modes\n\nPostgre��� acquires shared locks only for referential integrity checks. Using them in a high-load application can lead to resource starvation, and a two-level locking model cannot prevent such an outcome.\n\nLet’s recall the steps a transaction should take to lock a row:\n\n� If the xmax field and hint bits indicate that the row is locked in the exclusive\n\nmode, acquire an exclusive heavyweight tuple lock.\n\n� If required, wait for all the incompatible locks to be released by requesting a lockonthe��ofthexmaxtransaction(orseveraltransactionsifxmaxcontains a multixact ��).\n\n� Write its own �� into xmax in the tuple header and set the required hint bits.\n\n� Release the tuple lock if it was acquired in the first step.\n\nThe first two steps imply that if the locking modes are compatible, the transaction will jump the queue.\n\nLet’s repeat our experiment from the very beginning.\n\n=> TRUNCATE accounts;\n\n253\n\nChapter 13 Row-Level Locks\n\n=> INSERT INTO accounts(id, client, amount) VALUES\n\n(1,'alice',100.00), (2,'bob',200.00), (3,'charlie',300.00);\n\nStart the first transaction:\n\n=> BEGIN;\n\n=> SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122869 |\n\n30723\n\n(1 row)\n\nThe row is now locked in a shared mode:\n\n=> SELECT * FROM accounts WHERE id = 1 FOR SHARE;\n\nThe second transaction tries to update the same row, but it is not allowed: Share and No Key Update modes are incompatible:\n\n=> BEGIN;\n\n=> SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122870 |\n\n30794\n\n(1 row)\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\nWaiting for the first transaction to complete,the second transaction is holding the tuple lock, just like in the previous example:\n\n=> SELECT * FROM locks_accounts WHERE pid = 30794;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30794 | relation 30794 | transactionid | 122869 30794 | transactionid | 122870 30794 | tuple\n\n| accounts\n\n| RowExclusiveLock | t | f | ShareLock | t | ExclusiveLock | t | accounts(0,1) | ExclusiveLock\n\n(4 rows)\n\n254\n\n13.4 Wait Queue\n\nT1\n\nT2\n\nShare ctid xmin xmax (0,1) T1\n\ndata\n\ntuple (0,1)\n\nNow let the third transaction lock the row in a shared mode. Such a lock is com- patible with the already acquired lock, so this transaction jumps the queue:\n\n=> BEGIN;\n\n=> SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122871 |\n\n30865\n\n(1 row)\n\n=> SELECT * FROM accounts WHERE id = 1 FOR SHARE;\n\nWe have got two transactions locking the same row:\n\n=> SELECT * FROM pgrowlocks('accounts') \\gx\n\n−[ RECORD 1 ]−−−−−−−−−−−−−−− locked_row | (0,1) locker multi xids modes pids\n\n| 2 | t | {122869,122871} | {Share,Share} | {30723,30865}\n\nT1\n\nT3\n\nT2\n\nShare ctid xmin xmax (0,1) multi\n\ndata\n\ntuple (0,1)\n\n255\n\nChapter 13 Row-Level Locks\n\nIf the first transaction completes at this point, the second one will wake up to see that the row is still locked and will get back to the queue—but this time it will find itself behind the third transaction:\n\n=> COMMIT;\n\n=> SELECT * FROM locks_accounts WHERE pid = 30794;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30794 | relation 30794 | transactionid | 122870 30794 | transactionid | 122871 30794 | tuple\n\n| accounts\n\n| RowExclusiveLock | t | t | ExclusiveLock | f | ShareLock | t\n\n| accounts(0,1) | ExclusiveLock\n\n(4 rows)\n\nAnd only when the third transaction completes will the second one be able to per- form an update (unless other shared locks appear within this time interval).\n\n=> COMMIT;\n\nUPDATE 1\n\n=> COMMIT;\n\nForeignkeychecksareunlikelytocauseanyissues,askeyattributesusuallyremain unchanged and Key Share can be used together with No Key Update. But in most cases, you should avoid shared row-level locks in applications.\n\n13.5 No-Wait Locks\n\nS�� commands usually wait forthe requested resources to be freed. But sometimes it makes sense to cancel the operation if the lock cannot be acquired immediately. For this purpose, commands like ������, ����, and ����� offer the ������ clause.\n\nLet’s lock a row:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\n256\n\n13.5 No-Wait Locks\n\nThe command with the ������ clause immediately completes with an error if the requested resource is locked:\n\n=> SELECT * FROM accounts FOR UPDATE NOWAIT;\n\nERROR:\n\ncould not obtain lock on row in relation \"accounts\"\n\nSuch an error can be captured and handled by the application code.\n\nThe ������ and ������ commands do not have the ������ clause. Instead, you can try to lock the row using the ������ ��� ������ ������ command and then update or delete it if the attempt is successful.\n\nIn some rare cases, it may be convenient to skip the already locked rows and start processing the available ones right away. This is exactlywhat ��������� does when run with the ���� ������ clause:\n\n=> SELECT * FROM accounts ORDER BY id FOR UPDATE SKIP LOCKED LIMIT 1;\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n2 | bob\n\n| 200.00\n\n(1 row)\n\nIn this example, the first (already locked) row was skipped, and the query locked and returned the second row.\n\nThis approach enables us to process rows in batches or set up parallel processing of event queues. However, avoid inventing other use cases for this command—most tasks can be addressed using much simpler methods.\n\nLast but not least, you can avoid long waits by setting a timeout:\n\n=> SET lock_timeout = '1s';\n\n=> ALTER TABLE accounts DROP COLUMN amount;\n\nERROR:\n\ncanceling statement due to lock timeout\n\nThecommand completeswith anerror because ithas failedto acquirea lock within one second. A timeout can be set not only at the session level, but also at lower levels, for example, for a particular transaction.\n\n257\n\np. ���\n\nChapter 13 Row-Level Locks\n\nThis method prevents long waits during table processing when the command re- quiring an exclusive lock is executed under load. If an error occurs, this command can be retried after a while.\n\nWhile statement_timeout limits the total time of operator execution, the lock_timeout pa- rameter defines the maximum time that can be spent waiting on a lock.\n\n=> ROLLBACK;\n\n13.6 Deadlocks\n\nA transaction may sometimes require a resource that is currently being used by another transaction, which, in its turn, may be waiting on a resource locked by the third transaction, and so on. Such transactions get queued using heavyweight locks.\n\nBut occasionally a transaction already in the queue may need yet another resource, so it has to join the same queue again and wait for this resource to be released. A deadlock1 occurs: the queue now has a circular dependency that cannot resolve on its own.\n\nFor better visualization,let’s draw a wait-for graph. Its nodes represent active pro- cesses, while the edges shown as arrows point from the processes waiting on locks to the processes holding these locks. If the graph has a cycle, that is, a node can reach itself following the arrows, it means that a deadlock has occurred.\n\nThe illustrations here show transactions rather than processes. This substitution is usually acceptable because one transaction is executed by one process, and locks can only be acquired within a transaction. But in general,it is more correct to talk about processes,as some locks may not be released right away when the transaction is complete.\n\nIf a deadlock has occurred, and none of its participants has set a timeout, transac- tions will be waiting on each other forever. That’s why the lock manager2 performs automatic deadlock detection.\n\nHowever, this check requires some effort, which should not be wasted each time a lock is requested (after all, deadlocks do not happen too often). So if the process\n\n1 postgresql.org/docs/14/explicit-locking#LOCKING-DEADLOCKS.html 2 backend/storage/lmgr/README\n\n258\n\n13.6 Deadlocks\n\nT3\n\nT2\n\nresource 3\n\nT1\n\nresource 2\n\nresource 1\n\nmakes an unsuccessful attempt to acquire a lock and falls asleep after joining the deadlock_timeout queue,Postgre��� automatically sets a timeout as defined by the parameter.1 If the resource becomes available earlier—great, then the extra cost of the check will be avoided. But if the wait continues after the deadlock_timeout units of time, the waiting process wakes up and initiates the check.2\n\nThis check effectively consists in building a wait-for graph and searching it for cy- cles.3 To “freeze” the current state of the graph, Postgre��� stops any processing of heavyweight locks for the whole duration of the check.\n\nIf no deadlocks are detected, the process falls asleep again; sooner or later its turn will come.\n\nIf a deadlock is detected, one of the transactions will be forced to terminate, thus releasing its locks and enabling other transactions to continue their execution. In most cases, it is the transaction initiating the check that gets interrupted, but if the cycle includes an autovacuum process that is not currently freezing tuples to prevent wraparound, the server terminates autovacuum as having lower priority.\n\nDeadlocksusuallyindicatebadapplicationdesign. Todiscoversuchsituations,you have two things to watch out for: the corresponding messages in the server log and an increasing deadlocks value in the pg_stat_database table.\n\n1 backend/storage/lmgr/proc.c, ProcSleep function 2 backend/storage/lmgr/proc.c, CheckDeadLock function 3 backend/storage/lmgr/deadlock.c\n\n259\n\n1s\n\nChapter 13 Row-Level Locks\n\nDeadlocks by Row Updates\n\nAlthough deadlocks are ultimately caused by heavyweight locks, it is mostly row- level locks acquired in different order that lead to them.\n\nSuppose a transaction is going to transfer $��� between two accounts. It starts by drawing this sum from the first account:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount - 100.00 WHERE id = 1;\n\nUPDATE 1\n\nAt the same time, another transaction is going to transfer $�� from the second account to the first one. It begins by drawing this sum from the second account:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount - 10.00 WHERE id = 2;\n\nUPDATE 1\n\nNow the first transaction attempts to increase the amount in the second account but sees that the corresponding row is locked:\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 2;\n\nThen the second transaction tries to update the first account but also gets locked:\n\n=> UPDATE accounts SET amount = amount + 10.00 WHERE id = 1;\n\nThis circular wait will never resolve on its own. Unable to obtain the resource within one second,the first transaction initiates a deadlock check and gets aborted by the server:\n\nERROR: DETAIL: blocked by process 30723. Process 30723 waits for ShareLock on transaction 122876; blocked by process 30423. HINT: CONTEXT:\n\ndeadlock detected\n\nProcess 30423 waits for ShareLock on transaction 122877;\n\nSee server log for query details.\n\nwhile updating tuple (0,2) in relation \"accounts\"\n\nNow the second transaction can continue. It wakes up and performs an update:\n\n260\n\n13.6 Deadlocks\n\nUPDATE 1\n\nLet’s complete the transactions.\n\n=> ROLLBACK;\n\n=> ROLLBACK;\n\nTherightwaytoperformsuchoperationsistolockresourcesinthesameorder. For example, in this particular case the accounts could have been locked in ascending order based on their numbers.\n\nDeadlocks Between Two UPDATE Statements\n\nIn some cases deadlocks seem impossible, and yet they do occur.\n\nWe usually assume that ��� commands are atomic, but are they really? Let’s take a closer look at ������: this command locks rows as they are being updated rather than all at once,and it does not happen simultaneously. So if one ������ command modifies several rows in one order while the other is doing the same in a different order, a deadlock can occur.\n\nLet’s reproduce this scenario. First, we are going to build an index on the amount column, in descending order:\n\n=> CREATE INDEX ON accounts(amount DESC);\n\nTo be able to observe the process, we can write a function that slows things down:\n\n=> CREATE FUNCTION inc_slow(n numeric) RETURNS numeric AS $$\n\nSELECT pg_sleep(1); SELECT n + 100.00;\n\n$$ LANGUAGE sql;\n\nThe first ������ command is going to update all the tuples. The execution plan relies on a sequential scan of the whole table.\n\n261\n\np. ���\n\nChapter 13 Row-Level Locks\n\n=> EXPLAIN (costs off) UPDATE accounts SET amount = inc_slow(amount);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nUpdate on accounts\n\n−> Seq Scan on accounts\n\n(2 rows)\n\nTo make sure that the heap page stores the rows in ascending order based on the amount column, we have to truncate the table and insert the rows anew:\n\n=> TRUNCATE accounts;\n\n=> INSERT INTO accounts(id, client, amount) VALUES\n\n(1,'alice',100.00), (2,'bob',200.00), (3,'charlie',300.00);\n\n=> ANALYZE accounts;\n\n=> SELECT ctid, * FROM accounts;\n\nctid\n\n| id | client\n\n| amount\n\n−−−−−−−+−−−−+−−−−−−−−−+−−−−−−−−\n\n(0,1) | (0,2) | (0,3) | (3 rows)\n\n| 100.00 1 | alice 2 | bob | 200.00 3 | charlie | 300.00\n\nThe sequential scan will update the rows in the same order (it is not always true for large tables\n\nthough).\n\nLet’s start the update:\n\n=> UPDATE accounts SET amount = inc_slow(amount);\n\nMeanwhile, we are going to forbid sequential scans in another session:\n\n=> SET enable_seqscan = off;\n\nAs a result, the planner chooses an index scan for the next ������ command.\n\n=> EXPLAIN (costs off) UPDATE accounts SET amount = inc_slow(amount) WHERE amount > 100.00;\n\n262\n\n13.6 Deadlocks\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nUpdate on accounts\n\n−> Index Scan using accounts_amount_idx on accounts\n\nIndex Cond: (amount > 100.00)\n\n(3 rows)\n\nThe second and third rows satisfy the condition; since the index is descending,the rows will get updated in the reverse order.\n\nLet’s start the next update:\n\n=> UPDATE accounts SET amount = inc_slow(amount) WHERE amount > 100.00;\n\nThe pgrowlocks extension shows that the first operator has already updated the first row (�,�), while the second one has managed to update the last row (�,�):\n\n=> SELECT locked_row, locker, modes FROM pgrowlocks('accounts');\n\nlocked_row | locker |\n\nmodes\n\n−−−−−−−−−−−−+−−−−−−−−+−−−−−−−−−−−−−−−−−−−\n\n(0,1) (0,3) (2 rows)\n\n| 122883 | {\"No Key Update\"} | 122884 | {\"No Key Update\"}\n\nfirst second\n\nAnother second passes. The first operator has updated the second row, and the other one would like to do it too, but it is not allowed.\n\n=> SELECT locked_row, locker, modes FROM pgrowlocks('accounts');\n\nlocked_row | locker |\n\nmodes\n\n−−−−−−−−−−−−+−−−−−−−−+−−−−−−−−−−−−−−−−−−−\n\n(0,1) (0,2) (0,3) (3 rows)\n\n| 122883 | {\"No Key Update\"} | 122883 | {\"No Key Update\"} | 122884 | {\"No Key Update\"}\n\nthe first one wins\n\nNow the first operator would like to update the last table row, but it is already locked by the second operator. A deadlock has occurred.\n\nOne of the transactions is aborted:\n\n263\n\nChapter 13 Row-Level Locks\n\nERROR: DETAIL: blocked by process 30723. Process 30723 waits for ShareLock on transaction 122884; blocked by process 30794. HINT: CONTEXT:\n\ndeadlock detected\n\nProcess 30794 waits for ShareLock on transaction 122883;\n\nSee server log for query details.\n\nwhile updating tuple (0,2) in relation \"accounts\"\n\nAnd the other completes its execution:\n\nUPDATE 3\n\nAlthough such situations seem impossible, they do occur in high-load systems when batch row updates are performed.\n\n264\n\n14\n\nMiscellaneous Locks\n\n14.1 Non-Object Locks\n\nTo lock a resource that is not considered a relation, Postgre��� uses heavyweight locks of the object type.1 You can lock almost anything that is stored in the sys- tem catalog: tablespaces, subscriptions, schemas, roles, policies, enumerated data types, and so on.\n\nLet’s start a transaction that creates a table:\n\n=> BEGIN;\n\n=> CREATE TABLE example(n integer);\n\nNow take a look at non-relation locks in the pg_locks table:\n\n=> SELECT database,\n\n(\n\nSELECT datname FROM pg_database WHERE oid = database\n\n) AS dbname, classid, (\n\nSELECT relname FROM pg_class WHERE oid = classid\n\n) AS classname, objid, mode, granted\n\nFROM pg_locks WHERE locktype = 'object'\n\nAND pid = pg_backend_pid() \\gx\n\n1 backend/storage/lmgr/lmgr.c, LockDatabaseObject & LockSharedObject functions\n\n265\n\nChapter 14 Miscellaneous Locks\n\n−[ RECORD 1 ]−−−−−−−−−−−−−− | 16391 database | internals dbname | 2615 classid classname | pg_namespace objid mode granted\n\n| 2200 | AccessShareLock | t\n\nThe locked resource is defined here by three values:\n\ndatabase — the oid of the database that contains the object being locked (or zero\n\nif this object is common to the whole cluster)\n\nclassid — the oid listed in pg_class that corresponds to the name of the system\n\ncatalog table defining the type of the resource\n\nobjid — the oid listed in the system catalog table referenced by classid\n\nThe database value points to the internals database; it is the database to which the current session is connected. The classid column points to the pg_namespace table, which lists schemas.\n\nNow we can decipher the objid:\n\n=> SELECT nspname FROM pg_namespace WHERE oid = 2200;\n\nnspname −−−−−−−−− public (1 row)\n\nThus,Postgre��� has locked the public schema to make sure that no one can delete it while the transaction is still running.\n\nSimilarly, object deletion requires exclusive locks on both the object itself and all the resources it depends on.1\n\n=> ROLLBACK;\n\n1 backend/catalog/dependency.c, performDeletion function\n\n266\n\n14.2 Relation Extension Locks\n\n14.2 Relation Extension Locks\n\nAs the number of tuples in a relation grows,Postgre��� inserts newtuples into free space in the already available pages whenever possible. But it is clear that at some point it will have to add new pages, that is, to extend the relation. In terms of the physical layout, new pages get added to the end of the corresponding file (which, in turn, can lead to creation of a new file).\n\nFor new pages to be added by only one process at a time,this operation is protected by a special heavyweight lock of the extend type.1 Such a lock is also used by index vacuuming to forbid adding new pages during an index scan.\n\nRelation extension locks behave a bit differently from what we have seen so far:\n\nThey are released as soon as the extension is created, without waiting for the\n\ntransaction to complete.\n\nThey cannot cause a deadlock,so they are not included into the wait-for graph.\n\nHowever,a deadlock check will still be performed if the procedure of extending a relation is taking longer than deadlock_timeout. It is not a typical situation, but it can happen if a large number of processes perform multiple insertions concurrently. In this case,the check can be called multiple times,virtually paralyzing normal system operation.\n\nTo minimize this risk,heap files are extended byseveral pages at once (in proportion to the number of processes awaiting the lock, but by not more than ��� pages per operation).2 An exception to this rule is �-tree index files, which are extended by one page at a time.3\n\n14.3 Page Locks\n\nApage-level heavyweight lock of the page type4 is applied only by ��� indexes,and only in the following case.\n\n1 backend/storage/lmgr/lmgr.c, LockRelationForExtension function 2 backend/access/heap/hio.c, RelationAddExtraBlocks function 3 backend/access/nbtree/nbtpage.c, _bt_getbuf function 4 backend/storage/lmgr/lmgr.c, LockPage function\n\n267\n\nv. �.�\n\non\n\nChapter 14 Miscellaneous Locks\n\nG�� indexes can speed up search of elements in compound values,such as words in textdocuments. Theycanberoughlydescribedas�-treesthatstoreseparatewords rather than the whole documents. When a new document is added, the index has to be thoroughly updated to include each word that appears in this document.\n\nTo improve performance, ��� indexes allow deferred insertion, which is controlled fastupdate storage parameter. New words are first quickly added into an by the unordered pendinglist,and aftera while allthe accumulatedentries aremovedinto the main index structure. Since different documents are likely to contain duplicate words, this approach proves to be quite cost-effective.\n\nTo avoid concurrent transfer of words by several processes, the index metapage is locked in the exclusive mode until all the words are moved from the pending list to the main index. This lock does not interfere with regular index usage.\n\nJust like relation extension locks, page locks are released immediately when the taskiscomplete,withoutwaitingfortheendofthetransaction,sotheynevercause deadlocks.\n\n14.4 Advisory Locks\n\nUnlike other heavyweight locks (such as relation locks), advisory locks1 are never acquired automatically: they are controlled by the application developer. These locks are convenient to use if the application requires dedicated locking logic for some particular purpose.\n\nSupposeweneedtolockaresourcethatdoesnotcorrespondtoanydatabaseobject (which we could lock using ������ ��� or ���� ����� commands). In this case, the resource needs to be assigned a numeric ��. If the resource has a unique name,the easiest way to do it is to generate a hash code for this name:\n\n=> SELECT hashtext('resource1');\n\nhashtext −−−−−−−−−−− 991601810\n\n(1 row)\n\n1 postgresql.org/docs/14/explicit-locking#ADVISORY-LOCKS.html\n\n268\n\n14.4 Advisory Locks\n\nPostgre��� providesawholeclassoffunctionsformanagingadvisorylocks.1 Their names begin with the pg_advisory prefix and can contain the following words that hint at the function purpose:\n\nlock — acquire a lock\n\ntry — acquire a lock if it can be done without waits\n\nunlock — release the lock\n\nshare — use a shared locking mode (by default, the exclusive mode is used)\n\nxact — acquire and hold a lock till the end of the transaction (by default, the lock\n\nis held till the end of the session)\n\nLet’s acquire an exclusive lock until the end of the session:\n\n=> BEGIN;\n\n=> SELECT pg_advisory_lock(hashtext('resource1'));\n\n=> SELECT locktype, objid, mode, granted FROM pg_locks WHERE locktype = 'advisory' AND pid = pg_backend_pid();\n\nlocktype |\n\nobjid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−\n\nadvisory | 991601810 | ExclusiveLock | t\n\n(1 row)\n\nFor advisory locks to actually work, other processes must also observe the estab- lished order when accessing the resource; it must be guaranteed by the application.\n\nThe acquired lock will be held even after the transaction is complete:\n\n=> COMMIT;\n\n=> SELECT locktype, objid, mode, granted FROM pg_locks WHERE locktype = 'advisory' AND pid = pg_backend_pid();\n\nlocktype |\n\nobjid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−\n\nadvisory | 991601810 | ExclusiveLock | t\n\n(1 row)\n\nOnce the operation on the resource is over, the lock has to be explicitly released:\n\n=> SELECT pg_advisory_unlock(hashtext('resource1'));\n\n1 postgresql.org/docs/14/functions-admin#FUNCTIONS-ADVISORY-LOCKS.html\n\n269\n\np. ��\n\np. ��\n\nChapter 14 Miscellaneous Locks\n\n14.5 Predicate Locks\n\nThe term predicate lock appeared as early as the first attempts to implement full isolation based on locks.1 The problem confronted at that time was that locking all the rows to be read and updated still could not guarantee full isolation. Indeed, if new rows that satisfy the filter condition get inserted into the table, they will become phantoms.\n\nFor this reason, it was suggested to lock conditions (predicates) rather than rows. If you run a query with the a > 10 predicate, locking this predicate will not allow adding new rows into the table if they satisfy this condition, so phantoms will be avoided. The trouble is that if a query with a different predicate appears, such as a < 20, you have to find out whether these predicates overlap. In theory, this problem is algorithmically unsolvable; in practice, it can be solved only for a very simple class of predicates (like in this example).\n\nIn Postgre���, the Serializable isolation level is implemented in a different way: it uses the Serializable Snapshot Isolation (���) protocol.2 The term predicate lock still remains, but its sense has radically changed. In fact, such “locks” do not lock anything: they are used to track data dependencies between different transactions.\n\nsnapshot isolation at the Repeatable Read level allows no anoma- It is proved that lies except for the write skew and the read-only transaction anomaly. These two anomalies result in certain patterns in the data dependence graph that can be dis- covered at a relatively low cost.\n\nThe problem is that we must differentiate between two types of dependencies:\n\nThefirsttransactionreadsarowthatislaterupdatedbythesecondtransaction\n\n(�� dependency).\n\nThefirsttransactionmodifiesarowthatislaterreadbythesecondtransaction\n\n(�� dependency).\n\n1 K. P. Eswaran, J. N. Gray, R. A. Lorie, I. L. Traiger. The notions of consistency and predicate locks in a\n\ndatabase system\n\n2 backend/storage/lmgr/README-SSI backend/storage/lmgr/predicate.c\n\n270\n\n14.5 Predicate Locks\n\nW� dependencies can be detected using regular locks, but �� dependencies have to be tracked via predicate locks. Such tracking is turned on automatically at the Serializable isolation level, and that’s exactly why it is important to use this level for all transactions (or at least all the interconnected ones). If any transaction is running at a different level, it will not set (or check) predicate locks, so the Serial- izable level will be downgraded to Repeatable Read.\n\nIwouldliketostressonceagainthatdespitetheirname,predicatelocksdonotlock anything. Instead, a transaction is checked for “dangerous” dependencies when it is about to be committed, and if Postgre��� suspects an anomaly, this transaction will be aborted.\n\nLet’s create a table with an index that will span several pages (it can be achieved by using a low fillfactor value):\n\n=> CREATE TABLE pred(n numeric, s text);\n\n=> INSERT INTO pred(n) SELECT n FROM generate_series(1,10000) n;\n\n=> CREATE INDEX ON pred(n) WITH (fillfactor = 10);\n\n=> ANALYZE pred;\n\nIf the query performs a sequential scan, a predicate lock is acquired on the whole table (even if some of the rows do not satisfy the provided filter conditions).\n\n=> SELECT pg_backend_pid();\n\npg_backend_pid −−−−−−−−−−−−−−−−\n\n34753\n\n(1 row)\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE;\n\n=> EXPLAIN (analyze, costs off, timing off, summary off)\n\nSELECT * FROM pred WHERE n > 100;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Seq Scan on pred (actual rows=9900 loops=1)\n\nFilter: (n > '100'::numeric) Rows Removed by Filter: 100\n\n(3 rows)\n\n271",
      "page_number": 243
    },
    {
      "number": 14,
      "title": "Miscellaneous Locks",
      "start_page": 267,
      "end_page": 278,
      "detection_method": "regex_chapter",
      "content": "Chapter 14 Miscellaneous Locks\n\nAlthough predicate locks have their own infrastructure, the pg_locks view displays them together with heavyweight locks. All predicate locks are always acquired in the SIRead mode, which stands for Serializable Isolation Read:\n\n=> SELECT relation::regclass, locktype, page, tuple FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 34753 ORDER BY 1, 2, 3, 4;\n\nrelation | locktype | page | tuple −−−−−−−−−−+−−−−−−−−−−+−−−−−−+−−−−−−−\n\npred (1 row)\n\n| relation |\n\n|\n\n=> ROLLBACK;\n\nNote that predicate locks may be held longer than the transaction duration, as they are used to track dependencies between transactions. But anyway, they are managed automatically.\n\nIf the query performs an index scan, the situation improves. For a �-tree index, it is enough to set a predicate lock on the read heap tuples and on the scanned leaf pages of the index. It will “lock” the whole range that has been read, not only the exact values.\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE;\n\n=> EXPLAIN (analyze, costs off, timing off, summary off)\n\nSELECT * FROM pred WHERE n BETWEEN 1000 AND 1001;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using pred_n_idx on pred (actual rows=2 loops=1)\n\nIndex Cond: ((n >= '1000'::numeric) AND (n <= '1001'::numeric))\n\n(2 rows)\n\n=> SELECT relation::regclass, locktype, page, tuple FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 34753 ORDER BY 1, 2, 3, 4;\n\nrelation\n\n| locktype | page | tuple\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−+−−−−−−−\n\npred pred pred_n_idx | page\n\n| tuple | tuple\n\n| | |\n\n4 | 4 | 28 |\n\n96 97\n\n(3 rows)\n\n272\n\n14.5 Predicate Locks\n\nThe number of leaf pages corresponding to the already scanned tuples can change: for example, an index page can be split when new rows get inserted into the table. However, Postgre��� takes it into account and locks newly appeared pages too:\n\n=> INSERT INTO pred\n\nSELECT 1000+(n/1000.0) FROM generate_series(1,999) n;\n\n=> SELECT relation::regclass, locktype, page, tuple FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 34753 ORDER BY 1, 2, 3, 4;\n\nrelation\n\n| locktype | page | tuple\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−+−−−−−−−\n\npred pred pred_n_idx | page pred_n_idx | page pred_n_idx | page pred_n_idx | page pred_n_idx | page\n\n| tuple | tuple\n\n| | | | | | |\n\n4 | 4 | 28 | 266 | 267 | 268 | 269 |\n\n96 97\n\n(7 rows)\n\nEach read tuple is locked separately, and there may be quite a few of such tuples. Predicate locks use their own pool allocated at the server start. The total number max_pred_locks_per_transactionvaluemultiplied ofpredicatelocksislimitedbythe max_connections (despite the parameter names, predicate locks are not being by counted per separate transactions).\n\nHere we get the same problem as with row-level locks,but it is solved in a different way: lock escalation is applied.1\n\nAs soon as the number of tuple locks related to one page exceeds the value of the max_pred_locks_per_page parameter, they are replaced by a single page-level lock.\n\n=> EXPLAIN (analyze, costs off, timing off, summary off)\n\nSELECT * FROM pred WHERE n BETWEEN 1000 AND 1002;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using pred_n_idx on pred (actual rows=3 loops=1)\n\nIndex Cond: ((n >= '1000'::numeric) AND (n <= '1002'::numeric))\n\n(2 rows)\n\n1 backend/storage/lmgr/predicate.c, PredicateLockAcquire function\n\n273\n\n64 100\n\nv. �� 2\n\nv. �� −2\n\n64\n\nv. ��\n\nChapter 14 Miscellaneous Locks\n\nInstead of three locks of the tuple type we now have one lock of the page type:\n\n=> SELECT relation::regclass, locktype, page, tuple FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 34753 ORDER BY 1, 2, 3, 4;\n\nrelation\n\n| locktype | page | tuple\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−+−−−−−−−\n\npred | page pred_n_idx | page pred_n_idx | page pred_n_idx | page pred_n_idx | page pred_n_idx | page\n\n| | | | | |\n\n4 | 28 | 266 | 267 | 268 | 269 |\n\n(6 rows)\n\n=> ROLLBACK;\n\n. If the number of such Escalation of page-level locks follows the same principle max_pred_locks_per_relation value, they locks for a particular relation exceeds the get replaced by a single relation-level lock. (If this parameter is set to a negative max_pred_locks_per_transaction divided by the value, the threshold is calculated as absolute value of max_pred_locks_per_relation; thus, the default threshold is ��).\n\nLock escalation is sure to lead to multiple false-positive serialization errors,which negatively affects system throughput. So you have to find an appropriate balance between performance and spending the available ��� on locks.\n\nPredicate locks support the following index types:\n\n�-trees\n\nhash indexes, �i��, and ���\n\nIf an index scan is performed, but the index does not support predicate locks, the wholeindexwillbelocked. Itisonlytobeexpectedthatthenumberoftransactions aborted for no good reason will also increase in this case.\n\nFor more efficient operation at the Serializable level, it makes sense to explicitly declare read-only transactions as such using the ���� ���� clause. If the lock man- ager sees that a read-only transaction will not conflict with other transactions,1 it\n\n1 backend/storage/lmgr/predicate.c, SxactIsROSafe macro\n\n274\n\n14.5 Predicate Locks\n\ncan release the already set predicate locks and refrain from acquiring new ones. And if such a transaction is also declared ����������, the read-only transaction p. �� anomaly will be avoided too.\n\n275\n\n15\n\nLocks on Memory Structures\n\n15.1 Spinlocks\n\nTo protect data structures in shared memory, Postgre��� uses several types of lighter and less expensive locks rather than regular heavyweight ones.\n\nspinlocks. They are usually acquired for a very short time The simplest locks are interval(nolongerthanseveral���cycles)toprotectparticularmemorycellsfrom concurrent updates.\n\nSpinlocksarebasedonatomic ���instructions,suchascompare-and-swap.1 They only support the exclusive locking mode. If the required resource is already locked, the process busy-waits, repeating the command (it “spins” in the loop, hence the name). Ifthelockcannotbeacquiredwithinthespecifiedtimeinterval,theprocess pauses for a while and then starts another loop.\n\nThis strategy makes sense if the probability of a conflict is estimated as very low, so after an unsuccessful attempt the lock is likely to be acquired within several instructions.\n\nSpinlockshaveneitherdeadlockdetectionnorinstrumentation. Fromthepractical standpoint,we should simply know about their existence; the whole responsibility for their correct implementation lies with Postgre��� developers.\n\n1 backend/storage/lmgr/s_lock.c\n\n276\n\n15.2 Lightweight Locks\n\n15.2 Lightweight Locks\n\nlightweight locks, or lwlocks.1 Acquired for the time Next, there are so-called needed to process a data structure (for example, a hash table or a list of pointers), lightweight locks are typically short; however, they can take longer when used to protect �/� operations.\n\nLightweight locks support two modes: exclusive (for data modification) and shared (for read-only operations). There is no queue as such: if several processes are wait- ing on a lock, one of them will get access to the resource in a more or less random fashion. In high-load systems with multiple concurrent processes, it can lead to some unpleasant effects.\n\nDeadlock checks are not provided; we have to trust Postgre��� developers that lightweight locks are implemented correctly. However, these locks do have instru- mentation, so, unlike spinlocks, they can be observed.\n\n15.3 Examples\n\nTo get some idea of how and where spinlocks and lightweight locks can be used, let’s take a look at two shared memory structures: buffer cache and ��� buffers. I will name only some of the locks; the full picture is too complex and is likely to interest only Postgre��� core developers.\n\nBuffer Cache\n\nTo access a hash table must acquire a reading or in the exclusive mode if any modifications are expected.\n\nused to locate a particular buffer in the cache, the process BufferMapping lightweight lock either in the shared mode for\n\n1 backend/storage/lmgr/lwlock.c\n\n277\n\np. ���\n\np. ���\n\nChapter 15 Locks on Memory Structures\n\nBufferMapping ×128\n\nbuffer strategy\n\nfree buffers\n\nclock hand\n\nhash table\n\nbuffer pin\n\nbuffer header\n\nBufferIO\n\nBufferContent\n\nThe hash table is accessed very frequently,so this lock often becomes a bottleneck. To maximize granularity, it is structured as a tranche of ��� individual lightweight locks, each protecting a separate part of the hash table.1\n\nA hash table lock was converted into a tranche of �� locks as early as ����,in Postgre��� �.�; ten years later, when version �.� was released, the size of the tranche was increased to ���, but it may still be not enough for modern multi-core systems.\n\nbuffer header spinlock2 To get access to the buffer header,the process acquires a (the name is arbitrary, as spinlocks have no user-visible names). Some operations, such as incrementing the usage counter, do not require explicit locks and can be performed using atomic ��� instructions.\n\nBufferContent lock in the header To read a page in a buffer,the process acquires a of this buffer.3 It is usually held only while tuple pointers are being read; later on, will be enough. If the buffer content the protection provided by has to be modified, the BufferContent lock must be acquired in the exclusive mode.\n\nbuffer pinning\n\n1 backend/storage/buffer/bufmgr.c\n\ninclude/storage/buf_internals.h, BufMappingPartitionLock function\n\n2 backend/storage/buffer/bufmgr.c, LockBufHdr function 3 include/storage/buf_internals.h\n\n278\n\n15.3 Examples\n\nWhen a buffer is read from disk (or written to disk), Postgre��� also acquires a BufferIO lock in the buffer header; it is virtually an attribute used as a lock rather than an actual lock.1 It signals other processes requesting access to this page that they have to wait until the �/� operation is complete.\n\nThe pointer to free buffers and the clock hand of the eviction mechanism are pro- tected by a single common\n\nbuffer strategy spinlock.2\n\nWAL Buffers\n\nWALBufMapping\n\nWALWrite\n\nhash table\n\ninsert position\n\nWALInsert ×8\n\nPrevBytePos\n\nCurBytePos\n\nW�� cache also uses a hash table to map pages to buffers. Unlike the buffer cache hash table, it is protected by a single WALBufMapping lightweight lock because ��� cache is smaller (it usually takes 1 of the buffer cache size) and buffer access 32 is more ordered.3\n\nWriting of ��� pages to disk is protected by a WALWrite lightweight lock, which ensures that this operation is performed by one process at a time.\n\nTo create a ��� entry, the process first reserves some space within the ��� page and then fills it with data. Space reservation is strictly ordered; the process must insert position spinlock that protects the insertion pointer.4 But acquire an\n\n1 backend/storage/buffer/bufmgr.c, StartBufferIO function 2 backend/storage/buffer/freelist.c 3 backend/access/transam/xlog.c,AdvanceXLInsertBuffer function 4 backend/access/transam/xlog.c, ReserveXLogInsertLocation function\n\n279\n\noff\n\n1s p. ���\n\nv. �.�\n\nChapter 15 Locks on Memory Structures\n\nonce the space is reserved,it can be filled by several concurrent processes. For this purpose,each process must acquire any of the eight lightweight locks constituting the WALInsert tranche.1\n\n15.4 Monitoring Waits\n\nWithout doubt, locks are indispensable for correct Postgre��� operation, but they can lead to undesirable waits. It is useful to track such waits to understand their origin.\n\nlog_lock_waits The easiest way to get an overview of long-term locks is to turn the parameter on; it enables extensive logging of all the locks that cause a transaction deadlock_timeout. This data is displayed when a deadlock to wait for more than check\n\ncompletes, hence the parameter name.\n\nHowever, the pg_stat_activity view provides much more useful and complete in- formation. Whenever a process—either a system process or a backend—cannot proceed with its task because it is waiting for something, this wait is reflected in the wait_event_type and wait_event fields, which show the type and name of the wait, respectively.\n\nAll waits can be classified as follows.2\n\nWaits on various locks constitute quite a large group:\n\nLock — heavyweight locks\n\nLWLock — lightweight locks\n\nBufferPin — pinned buffers\n\nBut processes can be waiting for other events too:\n\nIO — input/output, when it is required to read or write some data\n\n1 backend/access/transam/xlog.c, WALInsertLockAcquire function 2 postgresql.org/docs/14/monitoring-stats#WAIT-EVENT-TABLE.html\n\n280\n\n15.4 Monitoring Waits\n\nClient — data sent by the client (psql spends in this state most of the time)\n\nIPC — data sent by another process\n\nExtension — a specific event registered by an extension\n\nSometimes a process simply does not perform any useful work. Such waits are usu- ally “normal,” meaning that they do not indicate any issues. This group comprises the following waits:\n\nActivity — background processes in their main cycle\n\nTimeout — timer\n\nLocks of each wait type are further classified by wait names. For example,waits on lightweight locks get the name of the lock or the corresponding tranche.1\n\nYou should bear in mind that the pg_stat_activity view displays only those waits that are handled in the source code in an appropriate way.2 Unless the name of the wait appears in this view, the process is not in the state of wait of any known type. Such time should be considered unaccounted for; it does not necessarily mean that the process is not waiting on anything—we simply do not know what is happening at the moment.\n\n=> SELECT backend_type, wait_event_type AS event_type, wait_event FROM pg_stat_activity;\n\nbackend_type\n\n| event_type |\n\nwait_event\n\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−\n\nlogical replication launcher | Activity | Activity autovacuum launcher | client backend | Activity background writer | Activity checkpointer | Activity walwriter\n\n| LogicalLauncherMain | AutoVacuumMain | | BgWriterMain | CheckpointerMain | WalWriterMain\n\n(6 rows)\n\nHere all the background processes were idle when the view was sampled,while the client backend was busy executing the query and was not waiting on anything.\n\n1 postgresql.org/docs/14/monitoring-stats#WAIT-EVENT-LWLOCK-TABLE.html 2 include/utils/wait_event.h\n\n281\n\nChapter 15 Locks on Memory Structures\n\n15.5 Sampling\n\nUnfortunately, the pg_stat_activity view shows only the current information on waits; statistics are not accumulated. The only way to collect wait data over time is to sample the view at regular intervals.\n\nWe have to take into account the stochastic nature of sampling. The shorter the wait as compared to the sampling interval,the lower the chance to detect this wait. Thus, longer sampling intervals require more samples to reflect the actual state of things (but as you increase the sampling rate, the overhead also rises). For the same reason, sampling is virtually useless for analyzing short-lived sessions.\n\nPostgre��� provides no built-in tools for sampling; however, we can still try it out using the pg_wait_sampling1 extension. To do so, we have to specify its library in the shared_preload_libraries parameter and restart the server:\n\n=> ALTER SYSTEM SET shared_preload_libraries = 'pg_wait_sampling';\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\nNow let’s install the extension into the database:\n\n=> CREATE EXTENSION pg_wait_sampling;\n\nThis extension can display the history of waits, which is saved in its ring buffer. However, it is much more interesting to get the waiting profile—the accumulated statistics for the whole duration of the session.\n\nFor example, let’s take a look at the waits during benchmarking. We have to start the pgbench utility and determine its process �� while it is running:\n\npostgres$ /usr/local/pgsql/bin/pgbench -T 60 internals\n\n=> SELECT pid FROM pg_stat_activity WHERE application_name = 'pgbench';\n\npid −−−−−−− 36367 (1 row)\n\nOnce the test is complete, the waits profile will look as follows:\n\n1 github.com/postgrespro/pg_wait_sampling\n\n282\n\n15.5 Sampling\n\n=> SELECT pid, event_type, event, count FROM pg_wait_sampling_profile WHERE pid = 36367 ORDER BY count DESC LIMIT 4;\n\npid\n\n| event_type |\n\nevent\n\n| count\n\n−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−+−−−−−−−\n\n36367 | IO 36367 | IO 36367 | Client 36367 | IO\n\n| | WALSync | | WALWrite | | ClientRead | DataFileRead |\n\n3478 52 30 2\n\n(4 rows)\n\npg_wait_sampling.profile_period parameter)samplesaretaken Bydefault(setbythe ��� times per second. So to estimate the duration of waits in seconds, you have to divide the count value by ���.\n\nmost of the waits are related to flushing ��� entries to disk. In this particular case, It is a good illustration of the unaccounted-for wait time: the WALSync event was not instrumented until Postgre��� ��; for lower versions,a waits profile would not contain the first row, although the wait itself would still be there.\n\nAnd here is how the profile will look like if we artificially slow down the file system for each �/� operation to take �.� seconds (I use slowfs1 for this purpose) :\n\npostgres$ /usr/local/pgsql/bin/pgbench -T 60 internals\n\n=> SELECT pid FROM pg_stat_activity WHERE application_name = 'pgbench';\n\npid −−−−−−− 36747 (1 row)\n\n=> SELECT pid, event_type, event, count FROM pg_wait_sampling_profile WHERE pid = 36747 ORDER BY count DESC LIMIT 4;\n\npid\n\n| event_type |\n\nevent\n\n| count\n\n−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−\n\n36747 | IO 36747 | LWLock 36747 | IO 36747 | IO\n\n| | WALWrite | | WALWrite | WALSync | | DataFileExtend |\n\n3603 2095 22 19\n\n(4 rows)\n\n1 github.com/nirs/slowfs\n\n283\n\n10ms\n\nv. ��",
      "page_number": 267
    },
    {
      "number": 15,
      "title": "Locks on Memory Structures",
      "start_page": 279,
      "end_page": 288,
      "detection_method": "regex_chapter",
      "content": "Chapter 15 Locks on Memory Structures\n\nNow �/� operations are the slowest ones—mainly those that are related to writing ��� files to disk in the synchronous mode. Since ��� writing is protected by a WALWrite lightweight lock, the corresponding row also appears in the profile.\n\nClearly,the same lock is acquired in the previous example too,but since the wait is shorter than the sampling interval, it either is sampled very few times or does not make it into the profile at all. It illustrates once again that to analyze short waits you have to sample them for quite a long time.\n\n284\n\nPart IV\n\nQuery Execution\n\n16\n\nQuery Execution Stages\n\n16.1 Demo Database\n\nThe examples in the previous parts of the book were based on simple tables with onlyahandfulofrows. Thisandsubsequentpartsdealwithqueryexecution,which is more demanding in this respect: we need related tables that have a much larger number of rows. Instead of inventing a new data set for each example, I took an existing demo database that illustrates passenger air traffic in Russia.1 It has sev- eral versions; we will use the bigger one created on August 15,2017. To install this version,you have to extract the file containing the database copy from the archive2 and run this file in psql.\n\nWhen developing this demo database, we tried to make its schema simple enough to be understood without extra explanations; at the same time, we wanted it to be complex enough to allow writing meaningful queries. The database is filled with true-to-life data, which makes the examples more comprehensive and should be interesting to work with.\n\nHere I will cover the main database objects only briefly; if you would like to re- view the whole schema, you can take a look at its full description referenced in the footnote.\n\nThe main entity is a booking (mapped to the bookings table). One booking can include several passengers, each with a separate electronic ticket (tickets). A pas- senger does not constitute a separate entity; for the purpose of our experiments, we will assume that all passengers are unique.\n\n1 postgrespro.com/community/demodb 2 edu.postgrespro.com/demo-big-en-20170815.zip\n\n287\n\nen\n\nChapter 16 Query Execution Stages\n\nEach ticket includes one or more flight segments (mapped to the ticket_flights ta- ble). A single ticket can have several flight segments in two cases: either it is a round-trip ticket, or it is issued for connecting flights. Although there is no cor- responding constraint in the schema, all tickets in a booking are assumed to have the same flight segments.\n\nEach flight (flights) goes from one airport (airports) to another. Flights with the sameflightnumberhavethesamepointsofdepartureanddestinationbutdifferent departure dates.\n\nThe routes view is based on the flights table; it displays the information on routes that does not depend on particular flight dates.\n\nAtcheck-in,eachpassengerisissuedaboarding pass(boarding_passes)withaseat number. A passenger can check in for a flight only if this flight is included into the ticket. Flight-seat combinations must be unique, so it is impossible to issue two boarding passes for the same seat.\n\nThe number of seats (seats) in an aircraft and their distribution between different travel classes depend on the particular model of the aircraft (aircrafts) that per- forms the flight. It is assumed that each aircraft model can have only one cabin configuration.\n\nSome tables have surrogate primary keys, while others use natural ones (some of them being composite). It is done solely for demonstration purposes and is by no means an example to follow.\n\nThe demo database can be thought of as a dump of a real system: it contains a snapshot of data taken at a particular time in the past. To display this time, you can call the bookings.now() function. Use this function in demo queries that would demand the now() function in real life.\n\nThenamesofairports,cities,andaircraftmodelsarestoredintheairports_dataand aircrafts_data tables; they are provided in two languages, English and Russian. To construct examples for this chapter, I will typically query the airports and aircrafts views shown in the entity-relationship diagram; these views choose the output bookings.lang parameter value. The names of some base language based on the tables can still appear in query plans though.\n\n288\n\n16.1 Demo Database\n\nBookings#book_ref∗book_date∗total_amount\n\nAirports#airport_code∗airport_name∗city∗coordinates∗timezone\n\nTickets#ticket_no∗book_ref∗passenger_id∗passenger_name∗contact_data\n\nTicket_flights#ticket_no#flight_id∗fare_conditions∗amount\n\nFlights#flight_id∗flight_no∗scheduled_departure∗scheduled_arrival∗departure_airport∗arrival_airport∗status∗aircraft_code∘actual_departure∘actual_arrival\n\nAircrafts#aircraft_code∗model∗range\n\nBoarding_passes#ticket_no#flight_id∗boarding_no∗seat_no\n\nSeats#aircraft_code#seat_no∗fare_conditions\n\n1\n\n289\n\nChapter 16 Query Execution Stages\n\n16.2 Simple Query Protocol\n\nA simple version of the client-server protocol1 enables ��� query execution: it sends the text of a query to the server and gets the full execution result in response, no matter how many rows it contains.2 A query sent to the server passes several stages: it is parsed, transformed, planned, and then executed.\n\nParsing\n\nFirst of all,Postgre��� has to parse3 the query text to understand what needs to be executed.\n\nLexical and syntactic analyisis. The lexer splits the query text into a set of lexemes4 (such as keywords, string literals, and numeric literals), while the parser validates this set against the ��� language grammar.5 Postgre��� relies on standard parsing tools, namely Flex and Bison utilities.\n\nThe parsed query is reflected in the backend’s memory as an abstract syntax tree.\n\nFor example, let’s take a look at the following query:\n\nSELECT schemaname, tablename FROM pg_tables WHERE tableowner = 'postgres' ORDER BY tablename;\n\nThelexersinglesoutfivekeywords,fiveidentifiers,astringliteral,andthreesingle- letter lexemes (a comma, an equals sign, and a semicolon). The parser uses these lexemes to build the parse tree, which is shown in the illustration below in a very simplified form. The captions next to the tree nodes specify the corresponding parts of the query:\n\n1 postgresql.org/docs/14/protocol.html 2 backend/tcop/postgres.c, exec_simple_query function 3 postgresql.org/docs/14/parser-stage.html\n\nbackend/parser/README\n\n4 backend/parser/scan.l 5 backend/parser/gram.y\n\n290\n\n16.2 Simple Query Protocol\n\nSELECT\n\nQUERY\n\nTARGETENTRY\n\nFROMEXPR\n\nSORTGROUPCLAUSE\n\nschemaname, tablename\n\nFROM\n\nORDER BY tablename\n\nRTE\n\nOPEXPR\n\npg_tables\n\ntableowner ='postgres'\n\npg_table\n\nWHERE tableowner = 'postgres'\n\nA rather obscure ��� abbreviation stands for Range Table Entry. Postgre��� source code uses the term range table to refer to tables, subqueries, join results—in other words, to any sets of rows that can be processed by ��� operators.1\n\nSemantic analysis. The purpose of semantic analysis2 is to determine whether the database contains any tables or other objects that this query refers to by name, and whether the user has permission to access these objects. All the information required for semantic analysis is stored\n\nin the system catalog.\n\nHaving received the parse tree, the semantic analyzer performs its further restruc- turing, which includes adding references to specific database objects, data types, and other information.\n\nIf you enable the debug_print_parse parameter, you can view the full parse tree in the server log, but it has little practical sense.\n\nTransformation\n\nAt the next stage, the query can be transformed (rewritten).3\n\n1 include/nodes/parsenodes.h 2 backend/parser/analyze.c 3 postgresql.org/docs/14/rule-system.html\n\n291\n\np. ��\n\nv. ��\n\nChapter 16 Query Execution Stages\n\nPostgre��� core uses transformations for several purposes. One of them is to re- place the name of the view in the parse tree with the subtree corresponding to the base query of this view.\n\nAnother case of using transformations is row-level security implementation.1\n\nThe ������ and ����� clauses of recursive queries stage.2\n\nalso get transformed during this\n\nIn the example above, pg_tables is a view; if we placed its definition into the query text, it would look as follows:\n\nSELECT schemaname, tablename FROM (\n\n-- pg_tables SELECT n.nspname AS schemaname,\n\nc.relname AS tablename, pg_get_userbyid(c.relowner) AS tableowner, ...\n\nFROM pg_class c\n\nLEFT JOIN pg_namespace n ON n.oid = c.relnamespace LEFT JOIN pg_tablespace t ON t.oid = c.reltablespace\n\nWHERE c.relkind = ANY (ARRAY['r'::char, 'p'::char])\n\n) WHERE tableowner = 'postgres' ORDER BY tablename;\n\nHowever, the server does not process the text representation of the query; all ma- nipulations are performed on the parse tree. The illustration shows a reduced version of the transformed tree (you can view its full version in the server log if you enable the debug_print_rewritten parameter).\n\nThe parse tree reflects the syntactic structure of the query, but it says nothing about the order in which the operations should be performed.\n\nPostgre��� also supports custom transformations, which the user can implement via the rewrite rule system.3\n\n1 backend/rewrite/rowsecurity.c 2 backend/rewrite/rewriteSearchCycle.c 3 postgresql.org/docs/14/rules.html\n\n292\n\n16.2 Simple Query Protocol\n\nQUERY\n\nTARGETENTRY\n\nFROMEXPR\n\nSORTGROUPCLAUSE\n\nRTE\n\nOPEXPR\n\npg_tables\n\ntableowner ='postgres'\n\nQUERY\n\nTARGETENTRY\n\nFROMEXPR\n\nJOINEXPR\n\nOPEXPR\n\nc.relkind = ANY (ARRAY[...])\n\nJOINEXPR\n\nRTE\n\npg_tablespace\n\nOPEXPR\n\nt.oid = c.reltablespace\n\nRTE\n\nRTE\n\nOPEXPR\n\npg_class\n\npg_namespace\n\nn.oid = c.relnamespace\n\nThe rule system support was proclaimed as one of the main objectives of Postgres devel- opment;1 it was still an academic project when the rules were first implemented,but since then they have been redesigned multiple times. The rule system is a very powerful mech- anism,but it is rather hard to comprehend and debug. It was even proposed to remove the rules from Postgre��� altogether, but the idea did not find unanimous support. In most cases, it is safer and easier to use triggers instead of rules.\n\n1 M. Stonebraker, L.A. Rowe. The Design of Postgres\n\n293\n\nChapter 16 Query Execution Stages\n\nPlanning\n\nS�� is a declarative language: queries specify what data to fetch, but not how to fetch it.\n\nAny query has several execution paths. Each operation shown in the parse tree can be completed in a number of ways: for example, the result can be retrieved by reading the whole table (and filtering out redundancies),or by finding the required rows via an index scan. Data sets are always joined in pairs, so there is a huge number of options that differ in the order of joins. Besides, there are various join algorithms: for example, the executor can scan the rows of the first data set and search for the matching rows in the other set, or both data sets can be first sorted and then merged together. For each algorithm, we can find a use case where it performs better than others.\n\nThe execution times of optimal and non-optimal plans can differ by orders of mag- nitude, so the planner1 that optimizes the parsed query is one of the most complex components of the system.\n\nPlan tree. The execution plan is also represented as a tree,but its nodes deal with physical operations on data rather than logical ones.\n\nIf you would like to explore full plan trees, you can dump them into the server log by enabling the debug_print_plan parameter. But in practice it is usually enough to view the text representation of the plan displayed by the ������� command.2\n\nThe following illustration highlights the main nodes of the tree. It is exactly these nodes that are shown in the output of the ������� command provided below.\n\nFor now, let’s pay attention to the following two points:\n\nThe tree contains only two queried tables out of three: the planner saw that one of the tables is not required for retrieving the result and removed it from the plan tree.\n\nFor each node of the tree, the planner provides the estimated cost and the\n\nnumber of rows expected to be processed.\n\n1 postgresql.org/docs/14/planner-optimizer.html 2 postgresql.org/docs/14/using-explain.html\n\n294",
      "page_number": 279
    },
    {
      "number": 16,
      "title": "Query Execution Stages",
      "start_page": 289,
      "end_page": 312,
      "detection_method": "regex_chapter",
      "content": "16.2 Simple Query Protocol\n\n=> EXPLAIN SELECT schemaname, tablename FROM pg_tables WHERE tableowner = 'postgres' ORDER BY tablename;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\n(cost=21.03..21.04 rows=1 width=128)\n\nSort Key: c.relname −> Nested Loop Left Join\n\n(cost=0.00..21.02 rows=1 width=128)\n\nJoin Filter: (n.oid = c.relnamespace) −> Seq Scan on pg_class c\n\n(cost=0.00..19.93 rows=1 width=72) Filter: ((relkind = ANY ('{r,p}'::\"char\"[])) AND (pg_g... (cost=0.00..1.04 rows=4 wid...\n\n−> Seq Scan on pg_namespace n\n\n(7 rows)\n\nSeq Scan Nested Loop\n\nnodes shown in the query plan correspond to reading the table,while the\n\nnode represents the join operation.\n\nPLANNEDSTMT\n\nSORT\n\nTARGETENTRY\n\nNESTLOOP\n\nTARGETENTRY\n\nSEQSCAN\n\npg_class\n\nSEQSCAN\n\npg_namespace\n\nOPEXPR\n\nn.oid = c.relnamespace\n\nOPEXPR\n\nrelkind =ANY ('r,p'::\"char\"[])AND pg_get_userbyid(relowner) ='postgres'::name\n\nPlan search. Postgre��� uses a cost-based optimizer;1 it goes over potential plans and estimates the resources required for their execution (such as �/� operations or ��� cycles). Normalized to a numeric value,this estimation is called the cost of the plan. Of all the considered plans, the one with the lowest cost is selected.\n\n1 backend/optimizer/README\n\n295\n\np. ��� p. ���\n\nv. ��\n\nChapter 16 Query Execution Stages\n\nThe problem is that the number of potentially available plans grows exponentially with the number of joined tables, so it is impossible to consider them all—even for relativelysimplequeries. Thesearchistypicallynarroweddownusingthedynamic programming algorithm combined with some heuristics. It allows the planner to find a mathematically accurate solution for queries with a larger number of tables within acceptable time.\n\nAn accurate solution does not guarantee that the selected plan is really the optimal one, as the planner uses simplified mathematical models and may lack reliable input data.\n\nManaging the order of joins. A query can be structured in a way that limits the search scope to some extent (at a risk of missing the optimal plan).\n\nCommon table expressions\n\nand the main query can be optimized separately;\n\nto guarantee such behavior, you can specify the ������������ clause.1\n\nSubqueries run within non-��� functions are always optimized separately. (S�� functions can sometimes be inlined into the main query.2)\n\nIf you set the join_collapse_limit parameter and use explicit ���� clauses in the query, the order of some joins will be defined by the query syntax structure; the from_collapse_limit parameter has the same effect on subqueries.3\n\nThe latter point may have to be explained. Let’s take a look at the query that does not specify any explicit joins for tables listed in the ���� clause:\n\nSELECT ... FROM a, b, c, d, e WHERE ...\n\nHere the planner will have to consider all the possible pairs of joins. The query is represented by the following part of the parse tree (shown schematically):\n\n1 postgresql.org/docs/14/queries-with.html 2 wiki.postgresql.org/wiki/Inlining_of_SQL_functions 3 postgresql.org/docs/14/explicit-joins.html\n\n296\n\n16.2 Simple Query Protocol\n\nFROMEXPR\n\nA\n\nB\n\nC\n\nD\n\nE\n\nIn the next example, joins have a certain structure defined by the ���� clause:\n\nSELECT ... FROM a, b JOIN c ON ..., d, e WHERE ...\n\nThe parse tree reflects this structure:\n\nFROMEXPR\n\nA\n\nJOINEXPR\n\nD\n\nE\n\nB\n\nC\n\nThe planner typically flattens the join tree, so that it looks like the one in the first example. The algorithm recursively traverses the tree and replaces each �������� node with a flat list of its elements.1\n\nHowever, such collapsing is performed only if the resulting flat list has no more join_collapse_limit elements. In this particular case, the �������� node would than not be collapsed if the join_collapse_limit value were less than five.\n\nFor the planner, it means the following:\n\nTable � must be joined with table � (or vice versa, � must be joined with �; the\n\norder of joins within a pair is not restricted).\n\nTables �, �, � and the result of joining � and � can be joined in any order.\n\n1 backend/optimizer/plan/initsplan.c, deconstruct_jointree function\n\n297\n\n8\n\n8\n\non 12\n\nChapter 16 Query Execution Stages\n\nIf the join_collapse_limit parameter is set to one, the order defined by explicit ���� clauses is preserved.\n\nAs for ��������� ���� operands,they are never collapsed,regardless of the value of the join_collapse_limit parameter.\n\nfrom_collapse_limit parameter controls subquery flattening in a similar way. The Although subqueries do not look like ���� clauses,the similarity becomes apparent at the parse tree level.\n\nHere is a sample query:\n\nSELECT ... FROM a,\n\n(\n\nSELECT ... FROM b, c WHERE ...\n\n) bc, d, e WHERE ...\n\nThe corresponding join tree is shown below. The only difference here is that this tree contains the �������� node instead of �������� (hence the parameter name).\n\nFROMEXPR\n\nA\n\nFROMEXPR\n\nD\n\nE\n\nB\n\nC\n\nGenetic query optimization. Once flattened, the tree may contain too many ele- ments at one level—either tables or join results, which have to be optimized sepa- rately. Planning time depends exponentially on the number of data sets that have to be joined, so it can grow beyond all reasonable limits.\n\nIf the the\n\ngeqo parameter is enabled and the number of elements at one level exceeds geqo_threshold value, the planner will use the genetic algorithm to optimize the\n\n298\n\n16.2 Simple Query Protocol\n\nquery.1 This algorithm is much faster than its dynamic programming counterpart, but it cannot guarantee that the found plan will be optimal. So the rule of thumb is to avoid using the genetic algorithm by reducing the number of elements that have to be optimized.\n\nThe genetic algorithm has several configurable parameters,2 but I am not going to cover them here.\n\nChoosing the best plan. Whether the plan can be considered optimal or not de- pends on how a particular client is going to use the query result. If the client needs the full result at once (for example, to create a report), the plan should optimize retrieval of all the rows. But if the priority is to return the first rows as soon as possible (for example, to display them on screen), the optimal plan might be com- pletely different.\n\nTo make this choice, Postgre��� calculates two components of the cost:\n\n=> EXPLAIN SELECT schemaname, tablename FROM pg_tables WHERE tableowner = 'postgres' ORDER BY tablename;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\n(cost=21.03..21.04 rows=1 width=128)\n\nSort Key: c.relname −> Nested Loop Left Join\n\n(cost=0.00..21.02 rows=1 width=128)\n\nJoin Filter: (n.oid = c.relnamespace) −> Seq Scan on pg_class c\n\n(cost=0.00..19.93 rows=1 width=72) Filter: ((relkind = ANY ('{r,p}'::\"char\"[])) AND (pg_g... (cost=0.00..1.04 rows=4 wid...\n\n−> Seq Scan on pg_namespace n\n\n(7 rows)\n\nThe first component (the startup cost) represents the price you pay to prepare for node execution, while the second component (the total cost) comprises all the ex- penses incurred by fetching the result.\n\n1 postgresql.org/docs/14/geqo.html\n\nbackend/optimizer/geqo/geqo_main.c\n\n2 postgresql.org/docs/14/runtime-config-query#RUNTIME-CONFIG-QUERY-GEQO.html\n\n299\n\n0.1\n\np. ���\n\nChapter 16 Query Execution Stages\n\nIt is sometimes stated that the startup cost is the cost of retrieving the first row of the result set, but it is not quite accurate.\n\nTo single out the preferred plans, the optimizer checks whether the query uses a cursor (either via the ������� command provided in ��� or declared explicitly in ��/pg���).1 If not, the client is assumed to need the whole result at once, and the optimizer chooses the plan with the least total cost.\n\nIf the query is executed with a cursor, the selected plan must optimize retrieval cursor_tuple_fraction of all rows. To be more exact, Postgre��� chooses the of only plan with the smallest value of the following expression:2\n\nstartup cost + cursor_tuple_fraction(total cost − startup cost)\n\nAn outline of cost estimation. To estimate the total cost of a plan, we have to get cost estimations for all its nodes. The cost of a node depends on its type (it is ob- vious that the cost of reading heap data is not the same as the sorting cost) and on the amount of data processed by this node (larger data volumes typically in- cur higher costs). While node types are known, the amount of data can only be projected based on the estimated cardinality of input sets (the number of rows the node takes as input) and the selectivity of the node (the fraction of rows remaining attheoutput). Thesecalculationsrelyonthecollectedstatistics ,suchastablesizes and data distribution in table columns.\n\nThus, the performed optimization depends on correctness of statistical data that is gathered and updated by autovacuum.\n\nIf cardinality estimation is accurate for each node, the calculated cost is likely to adequately reflect the actual cost. The main planning flaws usually result from incorrect estimation of cardinality and selectivity, which can be caused by inaccu- rate or outdated statistics, inability to use it, or—to a lesser extent—by imperfect planning models.\n\nCardinality estimation. To calculate the cardinality of a node, the planner has to recursively complete the following steps:\n\n1 backend/optimizer/plan/planner.c, standard_planner function 2 backend/optimizer/util/pathnode.c, compare_fractional_path_costs function\n\n300\n\n16.2 Simple Query Protocol\n\n� Estimate the cardinality of each child node and assess the number of input\n\nrows that the node will receive from them.\n\n� Estimate the selectivity of the node,that is,the fraction of input rows that will\n\nremain at the output.\n\nThe cardinality of the node is the product of these two values.\n\nSelectivity is represented by a number from � to �. The smaller the number, the higher the selectivity,and vice versa,a number that is close to one denotes low selectivity. It may seem illogical, but the idea is that a highly selective condition rejects almost all the rows, while the one that dismisses only a few has low selectivity.\n\nFirst, the planner estimates cardinalities of leaf nodes that define data access methods. These calculations rely on the collected statistics, such as the total size of the table.\n\nSelectivity of filter conditions depends on their types. In the most trivial case, it can be assumed to be a constant value, although the planner tries to use all the availableinformationtorefinetheestimation. Ingeneral,itisenoughtoknowhow to estimate simple filter conditions; if a condition includes logical operations, its selectivity is calculated by the following formulas:1\n\nselxandy = selx sely selxory = 1 − (1 − selx)(1 − sely) = selx +sely −selx sely\n\nUnfortunately, these formulas assume that each other. For correlated predicates, such estimations will be inaccurate.\n\npredicates x and y do not depend on\n\nTo estimate the cardinality of joins, the planner has to get the cardinality of the Cartesian product (that is, the product of cardinalities of two data sets) and es- timate the selectivity of join conditions, which is again dependent on condition types.\n\nCardinality of other nodes (such as sorting or aggregation) is estimated in a similar manner.\n\nIt is important to note that incorrect cardinality estimation for lower plan nodes affects all the subsequent calculations, leading to inaccurate total cost estimation\n\n1 backend/optimizer/path/clausesel.c, clauselist_selectivity_ext & clauselist_selectivity_or functions\n\n301\n\np. ���\n\nChapter 16 Query Execution Stages\n\nand a poor plan choice. To make things worse,the planner has no statistics on join results, only on tables.\n\nCost estimation. The process of estimating the cost is also recursive. To calculate the cost of a subtree, it is required to calculate and sum up the costs of all its child nodes and then add the cost of the parent node itself.\n\nTo estimate the cost of a node, Postgre��� applies the mathematical model of the operation performed by this node, using the already estimated node cardinality as input. For each node, both startup and total costs are calculated.\n\nSomeoperationshavenoprerequisites,sotheirexecutionstartsimmediately; such nodes have zero startup cost.\n\nOther operations, on the contrary, need to wait for some preliminary actions to complete. For example, a sort node usually has to wait for all the data from its child nodes before it can proceed to its own tasks. The startup cost of such nodes is usually higher than zero: this price has to be paid even if the above node (or the client) needs only one row of the whole output.\n\nAll calculations performed by the planner are simply estimations, which may have nothing to do with the actual execution time. Their only purpose is to enable com- parison of different plans for the same query in the same conditions. In other cases, it makes no sense to compare queries (especially different ones) in terms of their cost. For example, the cost could have been underestimated because of outdated statistics; once the statistics are refreshed,the calculated figure may rise,but since the estimation becomes more accurate, the server will choose a better plan.\n\nExecution\n\nThe plan built during query optimization now has to be executed.1\n\nThe executor opens a portal in the backend’s memory;2 it is an object that keeps the state of the query currently being executed. This state is represented as a tree\n\n1 postgresql.org/docs/14/executor.html\n\nbackend/executor/README\n\n2 backend/utils/mmgr/portalmem.c\n\n302\n\n16.2 Simple Query Protocol\n\nSORT\n\nNESTLOOP\n\nSEQSCAN\n\nSEQSCAN\n\npg_class\n\npg_namespace\n\nthat repeats the structure of the plan tree. The nodes of this tree operate like an assembly line, requesting and sending rows from one another.\n\nQuery execution starts at the root. The root node (which represents the ���� op- eration in this example) pulls the data from its child node. Having received all the rows, it sorts them and passes them on to the client.\n\nSome nodes (like the �������� node shown in this illustration) join data sets re- ceived from different sources. Such a node pulls the data from two child nodes, and, having received a pair of rows that satisfy the join condition, passes the re- sulting row upwards right away (unlike sorting, which has to get all the rows first). At this point, the execution of the node is interrupted until its parent requests the next row. If only a partial result is required (for example, there is a ����� clause in the query), the operation will not be performed in full.\n\nThe two ������� leaf nodes of the tree are responsible for table scans. When the parent node requests the data from these nodes, they fetch the subsequent row from the corresponding table.\n\nThus, some nodes do not store any rows, passing them upwards immediately, but others (such as ����) have to keep potentially large volumes of data. For this pur- work_mem chunk is allocated in the backend’s memory; if it is not enough, pose, a the remaining data is spilled into temporary files on disk.1\n\nA plan can have several nodes that need a data storage,so Postgre��� may allocate several memory chunks, each of the work_mem size. The total size of ��� that a query can use is not limited in any way.\n\n1 backend/utils/sort/tuplestore.c\n\n303\n\n4MB\n\nChapter 16 Query Execution Stages\n\n16.3 Extended Query Protocol\n\nWhen using the simple query protocol, each command (even if it is being repeated multiple times) has to go through all the aforementioned stages:\n\n� parsing\n\n� transformation\n\n� planning\n\n� execution\n\nHowever, there is no point in parsing one and the same query time and again. Re- peated parsing of queries that differ only in constants does not make much sense either—the parse tree structure still remains the same.\n\nAnother downside of the simple query protocol is that the client receives the whole result at once, regardless of the number of rows it may contain.\n\nIn general,it is possible to get over these limitations using ��� commands. To deal with the first one,you can ������� the query before running the ������� command; the second concern can be addressed by creating a cursor with ������� and return- ing rows via �����. But in this case, naming of these newly created objects must be handled by the client,while the server gets additional overhead of parsing extra commands.\n\nThe extended client-server protocol provides an alternative solution,enabling pre- cise control over separate operator execution stages at the command level of the protocol itself.\n\nPreparation\n\nDuring the preparation stage,the query is parsed and transformed as usual,but the resulting parse tree is kept in the backend’s memory.\n\nPostgre��� has no global cache for queries. The disadvantage of this architecture is obvious: each backend has to parse all the incoming queries, even if the same query has been already parsed by another backend. But there are some benefits\n\n304\n\n16.3 Extended Query Protocol\n\ntoo. Globalcachecaneasilybecomeabottleneckbecauseoflocks. Aclientrunning multiple small but different queries (like the ones varying only in constants) gen- erates much traffic and can negatively affect performance of the whole instance. In Postgre���, queries are parsed locally, so there is no impact on other processes.\n\nA prepared query can be parameterized. Here is a simple example using ��� com- mands(althoughitisnotthesameaspreparationattheprotocollevel,theultimate effect is the same):\n\n=> PREPARE plane(text) AS SELECT * FROM aircrafts WHERE aircraft_code = $1;\n\nAll the named prepared statements are shown in the pg_prepared_statements view:\n\n=> SELECT name, statement, parameter_types FROM pg_prepared_statements \\gx\n\n−[ RECORD 1 ]−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− name statement\n\n| plane | PREPARE plane(text) AS | SELECT * FROM aircrafts WHERE aircraft_code = $1;\n\n+\n\nparameter_types | {text}\n\nYou will not find any unnamed statements here (the ones that use the extended query protocol or ��/pg���). The statements prepared by other backends are not displayed either: it is impossible to access the other session’s memory.\n\nParameter Binding\n\nBefore a prepared statement gets executed,the actual parameter values have to be bound.\n\n=> EXECUTE plane('733');\n\naircraft_code |\n\nmodel\n\n| range\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−\n\n733\n\n| Boeing 737−300 |\n\n4200\n\n(1 row)\n\nThe advantage of binding parameters in prepared statements over concatenating literals with query strings is that it makes ��� injections absolutely impossible: a bound parameter value cannot modify the already built parse tree in any way.\n\n305\n\np. ���\n\nChapter 16 Query Execution Stages\n\nTo reach the same security level without prepared statements, you would have to carefully escape each value received from an untrusted source.\n\nPlanning and Execution\n\nWhen it comes to prepared statement execution, query planning is performed based on the actual parameter values; then the plan is passed on to the executor.\n\nDifferent parameter values may imply different optimal plans, so it is important to take the exact values into account. For example, when looking for expensive bookings,the planner assumes that there are not so many matching rows and uses an index scan:\n\n=> CREATE INDEX ON bookings(total_amount);\n\n=> EXPLAIN SELECT * FROM bookings WHERE total_amount > 1000000;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings\n\n(cost=86.49..9245.82 rows=4395 wid...\n\nRecheck Cond: (total_amount > '1000000'::numeric) −> Bitmap Index Scan on bookings_total_amount_idx\n\n(cost=0.00....\n\nIndex Cond: (total_amount > '1000000'::numeric)\n\n(4 rows)\n\nBut if the provided condition is satisfied by all the bookings, there is no point in using an index, as the whole table has to be scanned:\n\n=> EXPLAIN SELECT * FROM bookings WHERE total_amount > 100;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on bookings\n\n(cost=0.00..39835.88 rows=2111110 width=21)\n\nFilter: (total_amount > '100'::numeric)\n\n(2 rows)\n\nInsomecases,theplannermaykeepboththeparsetreeandthequeryplantoavoid repeated planning. Such a plan does not take parameter values into account, so it is called a generic plan (as compared to custom plans based on the actual values).1\n\n1 backend/utils/cache/plancache.c, choose_custom_plan function\n\n306\n\n16.3 Extended Query Protocol\n\nAn obvious case when the server can use a generic plan without compromising performance is a query with no parameters.\n\nThe first five optimizations of parameterized prepared statements always rely on the actual parameter values; the planner calculates the average cost of custom plans based on these values. Starting from the sixth execution, if the generic plan turns out to be more efficient than custom plans on average (taking into account thatcustomplanshavetobebuiltaneweverytime),1 theplannerkeepsthegeneric plan and continues using it, skipping the optimization stage.\n\nTheplanepreparedstatementhasalreadybeenexecutedonce. Afterthenextthree executions,the server still uses custom plans—you can tell by the parameter value in the query plan:\n\n=> EXECUTE plane('763');\n\n=> EXECUTE plane('773');\n\n=> EXPLAIN EXECUTE plane('319');\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on aircrafts_data ml\n\n(cost=0.00..1.39 rows=1 width=52)\n\nFilter: ((aircraft_code)::text = '319'::text)\n\n(2 rows)\n\nAfterthefifthexecution,theplannerswitchestothegenericplan: itdoesnotdiffer from the custom ones and has the same cost, but the backend can build it once and skip the optimization stage, thus reducing planning overhead. The ������� command now shows that the parameter is referred to by position rather than by its value:\n\n=> EXECUTE plane('320');\n\n=> EXPLAIN EXECUTE plane('321');\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on aircrafts_data ml\n\n(cost=0.00..1.39 rows=1 width=52)\n\nFilter: ((aircraft_code)::text = $1)\n\n(2 rows)\n\n1 backend/utils/cache/plancache.c, cached_plan_cost function\n\n307\n\nv. �� auto\n\nv. ��\n\nChapter 16 Query Execution Stages\n\nWe can easily imagine an unhappy turn of events when the first several custom plans are more expensive than the generic plan; subsequent plans could have been more efficient, but the planner will not consider them at all. Besides, it compares estimations rather than actual costs, which can also lead to miscalculations.\n\nHowever, and select either the generic or a custom plan by setting the rameter accordingly:\n\nif the planner makes a mistake, you can override the automatic decision plan_cache_mode pa-\n\n=> SET plan_cache_mode = 'force_custom_plan';\n\n=> EXPLAIN EXECUTE plane('CN1');\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on aircrafts_data ml\n\n(cost=0.00..1.39 rows=1 width=52)\n\nFilter: ((aircraft_code)::text = 'CN1'::text)\n\n(2 rows)\n\nAmong other things, the pg_prepared_statements view plans:\n\nshows statistics on chosen\n\n=> SELECT name, generic_plans, custom_plans FROM pg_prepared_statements;\n\nname\n\n| generic_plans | custom_plans\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\nplane |\n\n1 |\n\n6\n\n(1 row)\n\nGetting the Results\n\nThe extended query protocol allows retrieving data in batches rather than all at once. ���cursorshavealmostthesameeffect(exceptthatthereissomeextrawork for the server, and the planner optimizes fetching of the first cursor_tuple_fraction rows, not the whole result set):\n\n=> BEGIN;\n\n=> DECLARE cur CURSOR FOR\n\nSELECT * FROM aircrafts ORDER BY aircraft_code;\n\n308\n\n16.3 Extended Query Protocol\n\n=> FETCH 3 FROM cur;\n\naircraft_code |\n\nmodel\n\n| range\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−+−−−−−−−\n\n319 320 321\n\n| Airbus A319−100 | | Airbus A320−200 | | Airbus A321−200 |\n\n6700 5700 5600\n\n(3 rows)\n\n=> FETCH 2 FROM cur;\n\naircraft_code |\n\nmodel\n\n| range\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−\n\n733 763\n\n| Boeing 737−300 | | Boeing 767−300 |\n\n4200 7900\n\n(2 rows)\n\n=> COMMIT;\n\nIf the query returns many rows and the client needs them all, the system through- put highly depends on the batch size. The more rows in a batch,the less communi- cation overhead is incurred by accessing the server and getting the response. But as the batch size grows, these benefits become less tangible: while the difference between fetching rows one by one and in batches of ten rows can be enormous, it is much less noticeable if you compare batches of ��� and ���� rows.\n\n309\n\np. ��\n\n17\n\nStatistics\n\n17.1 Basic Statistics\n\nBasic relation-level statistics1 are stored in the pg_class table of the system catalog and include the following data:\n\nnumber of tuples in a relation (reltuples)\n\nrelation size, in pages (relpages)\n\nnumber of pages tagged in the visibility map (relallvisible)\n\nHere are these values for the flights table:\n\n=> SELECT reltuples, relpages, relallvisible FROM pg_class WHERE relname = 'flights';\n\nreltuples | relpages | relallvisible −−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−\n\n214867 |\n\n2624 |\n\n2624\n\n(1 row)\n\nIf the query does not impose any filter conditions, the reltuples value serves as the cardinality estimation:\n\n=> EXPLAIN SELECT * FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=63)\n\n(1 row)\n\n1 postgresql.org/docs/14/planner-stats.html\n\n310\n\n17.1 Basic Statistics\n\n, both manual and automatic.1 Fur- Statistics are collected during table analysis thermore, since basic statistics are of paramount importance, this data is calcu- latedduringsomeotheroperationsaswell(������ ����and�������,2 ������ ����� and �������3) and is refined during vacuuming.4\n\ndefault_statistics_target random rows are sampled. The For analysis purposes,300× sample size required to build statistics of a particular accuracy has low dependency on the volume of analyzed data, so the size of the table is not taken into account.5\n\nSampled rows are picked from the same number (300 × default_statistics_target) of random pages.6 Obviously, if the table itself is smaller, fewer pages may be read, and fewer rows will be selected for analysis.\n\nInlargetables,statisticscollectiondoesnotincludealltherows,soestimationscan diverge from actual values. It is perfectly normal: if the data is changing,statistics cannot be accurate all the time anyway. Accuracy of up to an order of magnitude is usually enough to choose an adequate plan.\n\nLet’s create a copy of the flights table with autovacuum disabled, so that we can control the autoanalysis start time:\n\n=> CREATE TABLE flights_copy(LIKE flights) WITH (autovacuum_enabled = false);\n\nThere is no statistics for the new table yet:\n\n=> SELECT reltuples, relpages, relallvisible FROM pg_class WHERE relname = 'flights_copy';\n\nreltuples | relpages | relallvisible −−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−\n\n−1 |\n\n0 |\n\n0\n\n(1 row)\n\n1 backend/commands/analyze.c, do_analyze_rel function 2 backend/commands/cluster.c, copy_table_data function 3 backend/catalog/heap.c, index_update_stats function 4 backend/access/heap/vacuumlazy.c, heap_vacuum_rel function 5 backend/commands/analyze.c, std_typanalyze function 6 backend/commands/analyze.c, acquire_sample_rows function\n\nbackend/utils/misc/sampling.c\n\n311\n\np. ���\n\n100\n\nv. ��\n\np. ���\n\nChapter 17 Statistics\n\nThe value reltuples = −1 analyzed yet and a really empty table without any rows.\n\nis used to differentiate between a table that has not been\n\nIt is highly likely that some rows will get inserted into the table right after its cre- ation. So being unaware of the current state of things, the planner assumes that the table contains 10 pages:\n\n=> EXPLAIN SELECT * FROM flights_copy;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights_copy\n\n(cost=0.00..14.10 rows=410 width=170)\n\n(1 row)\n\nThenumberofrowsisestimatedbasedonthesizeofasinglerow,whichisshownin the plan as width. Row width is typically an average value calculated during analy- sis, but since no statistics have been collected yet, here it is just an approximation based on the column data types.1\n\nNow let’s copy the data from the flights table and perform the analysis:\n\n=> INSERT INTO flights_copy SELECT * FROM flights;\n\nINSERT 0 214867\n\n=> ANALYZE flights_copy;\n\nThe collected statistics reflects the actual number of rows (the table size is small enough for the analyzer to gather statistics on all the data):\n\n=> SELECT reltuples, relpages, relallvisible FROM pg_class WHERE relname = 'flights_copy';\n\nreltuples | relpages | relallvisible −−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−\n\n214867 |\n\n2624 |\n\n0\n\n(1 row)\n\nThe relallvisible value is used to estimate the cost is updated by ������:\n\nof an index-only scan. This value\n\n=> VACUUM flights_copy;\n\n1 backend/access/table/tableam.c, table_block_relation_estimate_size function\n\n312\n\n17.1 Basic Statistics\n\n=> SELECT relallvisible FROM pg_class WHERE relname = 'flights_copy';\n\nrelallvisible −−−−−−−−−−−−−−−\n\n2624\n\n(1 row)\n\nNow let’s double the number of rows without updating statistics and check the cardinality estimation in the query plan:\n\n=> INSERT INTO flights_copy SELECT * FROM flights;\n\n=> SELECT count(*) FROM flights_copy;\n\ncount −−−−−−−− 429734 (1 row)\n\n=> EXPLAIN SELECT * FROM flights_copy;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights_copy\n\n(cost=0.00..9545.34 rows=429734 width=63)\n\n(1 row)\n\nDespite the outdated pg_class data, the estimation turns out to be accurate:\n\n=> SELECT reltuples, relpages FROM pg_class WHERE relname = 'flights_copy';\n\nreltuples | relpages −−−−−−−−−−−+−−−−−−−−−−\n\n214867 |\n\n2624\n\n(1 row)\n\nThe thing is that if the planner sees a gap between relpages and the actual file size, it can scale the reltuples value to improve estimation accuracy.1 Since the file size has doubled as compared to relpages,the planner adjusts the estimated number of rows, assuming that data density remains the same:\n\n=> SELECT reltuples *\n\n(pg_relation_size('flights_copy') / 8192) / relpages AS tuples\n\nFROM pg_class WHERE relname = 'flights_copy';\n\n1 backend/access/table/tableam.c, table_block_relation_estimate_size function\n\n313\n\nChapter 17 Statistics\n\ntuples −−−−−−−− 429734 (1 row)\n\nNaturally,such an adjustment may not always work (for example,if we delete some rows,the estimation will remain the same),but in some cases it allows the planner to hold on until significant changes trigger the next analysis run.\n\n17.2 NULL Values\n\nFrowned upon by theoreticians,1 ���� values still play an important role in rela- tional databases: they provide a convenient way to reflect the fact that a value is either unknown or does not exist.\n\nHowever,a special value demands special treatment. Apart from theoretical incon- sistencies, there are also multiple practical challenges that have to be taken into account. Regular Boolean logic is replaced by the three-valued one, so ��� �� be- haves unexpectedly. It is unclear whether ���� values should be treated as greater than or less than regular values (hence the ����� ����� and ����� ���� clauses for sorting). It is not quite obvious whether ���� values must be taken into account by aggregate functions. Strictly speaking, ���� values are not values at all, so the planner requires additional information to process them.\n\nApart from the simplest basic statistics collected at the relation level, the analyzer also gathers statistics for each column of the relation. This data is stored in the pg_statistic table of the system catalog,2 but you can also access it via the pg_stats view, which provides this information in a more convenient format.\n\nThefractionof����valuesbelongstocolumn-levelstatistics; calculatedduringthe analysis, it is shown as the null_frac attribute.\n\nFor example,when searching for the flights that have not departed yet,we can rely on their departure times being undefined:\n\n=> EXPLAIN SELECT * FROM flights WHERE actual_departure IS NULL;\n\n1 sigmodrecord.org/publications/sigmodRecord/0809/p20.date.pdf 2 include/catalog/pg_statistic.h\n\n314\n\n17.3 Distinct Values\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..4772.67 rows=16702 width=63)\n\nFilter: (actual_departure IS NULL)\n\n(2 rows)\n\nTo estimate the result, the planner multiplies the total number of rows by the frac- tion of ���� values:\n\n=> SELECT round(reltuples * s.null_frac) AS rows FROM pg_class\n\nJOIN pg_stats s ON s.tablename = relname\n\nWHERE s.tablename = 'flights'\n\nAND s.attname = 'actual_departure';\n\nrows −−−−−−− 16702 (1 row)\n\nAnd here is the actual row count:\n\n=> SELECT count(*) FROM flights WHERE actual_departure IS NULL;\n\ncount −−−−−−− 16348 (1 row)\n\n17.3 Distinct Values\n\nThe n_distinct field of the pg_stats view shows the number of distinct values in a column.\n\nIf n_distinct is negative, its absolute value denotes the fraction of distinct values in a column rather than their actual count. For example, −1 indicates that all col- umn values are unique, while −3 means that each value appears in three rows on average. The analyzer uses fractions if the estimated number of distinct values ex- ceeds 10% of the total row count; in this case, further data updates are unlikely to change this ratio.1\n\n1 backend/commands/analyze.c, compute_distinct_stats function\n\n315\n\nChapter 17 Statistics\n\ny c n e u q e r f\n\nnull_frac\n\nn_distinct\n\nvalues\n\nIf uniform data distribution is expected, the number of distinct values is used in- stead. For example, when estimating the cardinality of the “column = expression” condition,the planner assumes that the expression can take any column value with equal probability if its exact value is unknown at the planning stage:1\n\n=> EXPLAIN SELECT * FROM flights WHERE departure_airport = (\n\nSELECT airport_code FROM airports WHERE city = 'Saint Petersburg'\n\n);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=30.56..5340.40 rows=2066 width=63)\n\nFilter: (departure_airport = $0) InitPlan 1 (returns $0)\n\n−> Seq Scan on airports_data ml\n\n(cost=0.00..30.56 rows=1 wi...\n\nFilter: ((city −>> lang()) = 'Saint Petersburg'::text)\n\n(5 rows)\n\nHere the InitPlan node is executed only once, and the calculated value is used in the main plan.\n\n1 backend/utils/adt/selfuncs.c, var_eq_non_const function\n\n316\n\n17.4 Most Common Values\n\n=> SELECT round(reltuples / s.n_distinct) AS rows FROM pg_class\n\nJOIN pg_stats s ON s.tablename = relname\n\nWHERE s.tablename = 'flights'\n\nAND s.attname = 'departure_airport';\n\nrows −−−−−− 2066 (1 row)\n\nIf the estimated number of distinct values is incorrect (because a limited number of rows have been analyzed), it can be overridden at the column level:\n\nALTER TABLE ...\n\nALTER COLUMN ... SET (n_distinct = ...);\n\nIf all data always had uniform distribution,this information (coupledwith minimal and maximal values) would be sufficient. However, for non-uniform distribution (which is much more common in practice), such estimation is inaccurate:\n\n=> SELECT min(cnt), round(avg(cnt)) avg, max(cnt) FROM (\n\nSELECT departure_airport, count(*) cnt FROM flights GROUP BY departure_airport\n\n) t;\n\nmin | avg\n\n|\n\nmax\n\n−−−−−+−−−−−−+−−−−−−− 113 | 2066 | 20875\n\n(1 row)\n\n17.4 Most Common Values\n\nIf data distribution is non-uniform,the estimation is fine-tuned based on statistics on most common values (���) and their frequencies. The pg_stats view displays these arrays in the most_common_vals and most_common_freqs fields, respectively.\n\nHere is an example of such statistics on various types of aircraft:\n\n317\n\nChapter 17 Statistics\n\ny c n e u q e r f\n\n[most_common_vals]\n\n] s q e r f _ n o m m o c _ t s o m\n\n[\n\nnull_frac\n\n=> SELECT most_common_vals AS mcv,\n\nleft(most_common_freqs::text,60) || '...' AS mcf\n\nFROM pg_stats WHERE tablename = 'flights' AND attname = 'aircraft_code' \\gx\n\n−[ RECORD 1 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− mcv | {CN1,CR2,SU9,321,733,763,319,773} mcf | {0.27886668,0.27266666,0.26176667,0.057166666,0.037666667,0....\n\nTo estimate the selectivity of the “column = value” condition, it is enough to find this value in the most_common_vals array and take its frequency from the most_common_freqs array element with the same index:1\n\n=> EXPLAIN SELECT * FROM flights WHERE aircraft_code = '733';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..5309.84 rows=8093 width=63)\n\nFilter: (aircraft_code = '733'::bpchar)\n\n(2 rows)\n\n1 backend/utils/adt/selfuncs.c, var_eq_const function\n\n318\n\nvalues",
      "page_number": 289
    },
    {
      "number": 17,
      "title": "Statistics",
      "start_page": 313,
      "end_page": 336,
      "detection_method": "regex_chapter",
      "content": "17.4 Most Common Values\n\n=> SELECT round(reltuples * s.most_common_freqs[\n\narray_position((s.most_common_vals::text::text[]),'733')\n\n]) FROM pg_class\n\nJOIN pg_stats s ON s.tablename = relname\n\nWHERE s.tablename = 'flights'\n\nAND s.attname = 'aircraft_code';\n\nround −−−−−−− 8093 (1 row)\n\nIt is obvious that such estimation will be close to the actual value:\n\n=> SELECT count(*) FROM flights WHERE aircraft_code = '733';\n\ncount −−−−−−− 8263 (1 row)\n\nThe ��� list is also used to estimate selectivity of inequality conditions. For ex- ample, a condition like “column < value” requires the analyzer to search through most_common_vals for all the values that are smaller than the target one and sum up the corresponding frequencies listed in most_common_freqs.1\n\nM�� statistics work best when distinct values are not too many. The maximum size default_statistics_target parameter, which also limits the of arrays is defined by the number of rows to be randomly sampled for the purpose of analysis.\n\nIn some cases, it makes sense to increase the default parameter value, thus ex- panding the ��� list and improving the accuracy of estimations. You can do it at the column level:\n\nALTER TABLE ...\n\nALTER COLUMN ... SET STATISTICS ...;\n\nThe sample size will also grow, but only for the specified table.\n\n1 backend/utils/adt/selfuncs.c, scalarineqsel function\n\n319\n\n100\n\nChapter 17 Statistics\n\nSince the ��� array stores actual values, it may take quite a lot of space. To keep the pg_statistic size under control and avoid loading the planner with useless work, values that are larger than � k� are excluded from analysis and statistics. But since such large values are likely to be unique, they would probably not make it into most_common_vals anyway.\n\n17.5 Histogram\n\nIf distinct values are too many to be stored in an array, Postgre��� employs a histogram. In this case, values are distributed between several buckets of the his- togram. The number of buckets is also limited by the default_statistics_target pa- rameter.\n\nThe bucket width is selected in such a way that each bucket gets approximately the same number of values (this property is reflected in the diagram by the equality of areas of big hatched rectangles). The values included into ��� lists are not taken into account. As a result,the cumulative frequency of values in each bucket equals\n\n1 number of buckets\n\n.\n\nThe histogram is stored in the histogram_bounds field of the pg_stats view as an array of buckets’ boundary values:\n\n=> SELECT left(histogram_bounds::text,60) || '...' AS hist_bounds FROM pg_stats s WHERE s.tablename = 'boarding_passes' AND s.attname = 'seat_no';\n\nhist_bounds −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− {10B,10E,10F,10F,11H,12B,13B,14B,14H,15G,16B,17B,17H,19B,19B...\n\n(1 row)\n\nCombined with the ��� list, the histogram is used for operations like estimating the selectivity of greater than and less than conditions.1 For example, let’s take a look at the number of boarding passes issued for back rows:\n\n1 backend/utils/adt/selfuncs.c, ineq_histogram_selectivity function\n\n320\n\n17.5 Histogram\n\ny c n e u q e r f\n\n[mcv]\n\n]\n\nf c m\n\n[\n\nnull_frac\n\n[histogram_bounds]\n\nvalues\n\n=> EXPLAIN SELECT * FROM boarding_passes WHERE seat_no > '30B';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on boarding_passes\n\n(cost=0.00..157350.10 rows=2983242 ...\n\nFilter: ((seat_no)::text > '30B'::text)\n\n(2 rows)\n\nI have intentionally selected the seat number that lies right on the boundary be- tween two histogram buckets.\n\nN number of buckets\n\n, where N is the The selectivity of this condition will be estimated at number of buckets holding the values that satisfy the condition (that is, the ones located to the right of the specified value). It must also be taken into account that ���s are not included into the histogram.\n\nIncidentally, ���� values do not appear in the histogram either, but the seat_no column contains no such values anyway:\n\n=> SELECT s.null_frac FROM pg_stats s WHERE s.tablename = 'boarding_passes' AND s.attname = 'seat_no';\n\n321\n\nChapter 17 Statistics\n\nnull_frac −−−−−−−−−−−\n\n0\n\n(1 row)\n\nFirst, let’s find the fraction of ���s that satisfy the condition:\n\n=> SELECT sum(s.most_common_freqs[\n\narray_position((s.most_common_vals::text::text[]),v)\n\n]) FROM pg_stats s, unnest(s.most_common_vals::text::text[]) v WHERE s.tablename = 'boarding_passes' AND s.attname = 'seat_no'\n\nAND v > '30B';\n\nsum −−−−−−−−−−−− 0.21226665\n\n(1 row)\n\nThe overall ��� share (ignored by the histogram) is:\n\n=> SELECT sum(s.most_common_freqs[\n\narray_position((s.most_common_vals::text::text[]),v)\n\n]) FROM pg_stats s, unnest(s.most_common_vals::text::text[]) v WHERE s.tablename = 'boarding_passes' AND s.attname = 'seat_no';\n\nsum −−−−−−−−−−−− 0.67816657\n\n(1 row)\n\nSince the values that conform to the specified condition take exactly 𝑁 buckets (out of ��� buckets possible), we get the following estimation:\n\n=> SELECT round( reltuples * ( 0.21226665 -- MCV share\n\n+ (1 - 0.67816657 - 0) * (51 / 100.0) -- histogram share\n\n)) FROM pg_class WHERE relname = 'boarding_passes';\n\nround −−−−−−−−− 2983242\n\n(1 row)\n\n322\n\n17.5 Histogram\n\ny c n e u q e r f\n\nnull_frac\n\nx\n\nvalues\n\nIn the generic case of non-boundary values, the planner applies linear interpola- tion to take into account the fraction of the bucket that contains the target value.\n\nHere is the actual number of back seats:\n\n=> SELECT count(*) FROM boarding_passes WHERE seat_no > '30B';\n\ncount −−−−−−−−− 2993735\n\n(1 row)\n\nAs you increase the default_statistics_target value, estimation accuracy may im- prove,but as our example shows,the histogram combined with the ��� list usually gives good results even if the column contains many unique values:\n\n=> SELECT n_distinct FROM pg_stats WHERE tablename = 'boarding_passes' AND attname = 'seat_no';\n\nn_distinct −−−−−−−−−−−−\n\n461\n\n(1 row)\n\nIt makes sense to improve estimation accuracy only if it leads to better planning. Increasing the default_statistics_target value without giving it much thought may\n\n323\n\nChapter 17 Statistics\n\nslow down planning and analysis without bringing any benefits in return. That said, reducing this parameter value (down to zero) can lead to a bad plan choice, even though it does speed up planning and analysis. Such savings are usually un- justified.\n\n17.6 Statistics for Non-Scalar Data Types\n\nFor non-scalar data types, Postgre��� can gather statistics not only on the distri- bution of values, but also on the distribution of elements used to construct these values. It improves planning accuracy when you query columns that do not con- form to the first normal form.\n\nThe most_common_elems and most_common_elem_freqs arrays show the list of\n\nthe most common elements and the frequency of their usage.\n\nThese statistics are collected and used to estimate selectivity of operations on arrays1 and tsvector2 data types.\n\nThe elem_count_histogram array shows the histogram of the number of distinct\n\nelements in a value.\n\nThis data is collected and used for estimating selectivity of operations on ar- rays only.\n\nFor range types, Postgre��� builds distribution histograms for range length and lower and upper boundaries of the range. These histograms are used for estimating selectivity of various operations on these types,3 but the pg_stats view does not display them. data types.4\n\nFor range types, Postgre��� builds distribution histograms for range length and lower and upper boundaries of the range. These histograms are used for estimating selectivity of various operations on these types,3 but the pg_stats view does not display them. data types.4\n\n1 postgresql.org/docs/14/arrays.html\n\nbackend/utils/adt/array_typanalyze.c backend/utils/adt/array_selfuncs.c\n\n2 postgresql.org/docs/14/datatype-textsearch.html\n\nbackend/tsearch/ts_typanalyze.c backend/tsearch/ts_selfuncs.c\n\n3 postgresql.org/docs/14/rangetypes.html\n\nbackend/utils/adt/rangetypes_typanalyze.c backend/utils/adt/rangetypes_selfuncs.c\n\n4 backend/utils/adt/multirangetypes_selfuncs.c\n\n324\n\n17.7 Average Field Width\n\n17.7 Average Field Width\n\nThe avg_width field of the pg_stats view shows the average size of values stored in a column. Naturally, for types like integer or char(3) this size is always the same, but for data types of variable length, such as text, it can vary a lot from column to column:\n\n=> SELECT attname, avg_width FROM pg_stats WHERE (tablename, attname) IN ( VALUES\n\n('tickets', 'passenger_name'), ('ticket_flights','fare_conditions')\n\n);\n\nattname\n\n| avg_width\n\n−−−−−−−−−−−−−−−−−+−−−−−−−−−−−\n\nfare_conditions | | passenger_name\n\n8 16\n\n(2 rows)\n\nThis statistic is used to estimate the amount of memory required for operations like sorting or hashing.\n\n17.8 Correlation\n\nThecorrelationfieldofthepg_statsviewshowsthecorrelationbetweenthephysical order of data and the logical order defined by comparison operations. If values are stored strictly in ascending order, their correlation will be close to 1; if they are arranged in descending order, their correlation will be close to −1. The more chaotic is data distribution on disk, the closer is the correlation to zero.\n\n=> SELECT attname, correlation FROM pg_stats WHERE tablename = 'airports_data' ORDER BY abs(correlation) DESC;\n\nattname\n\n| correlation\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−− coordinates | airport_code | −0.21120238 city −0.1970127 | airport_name | −0.18223621 0.17961165 | timezone\n\n(5 rows)\n\n325\n\np. ���\n\nChapter 17 Statistics\n\nNote that this statistic is not gathered for the coordinates column: less than and greater than operators are not defined for the point type.\n\n. Correlation is used for cost estimation of index scans\n\n17.9 Expression Statistics\n\nColumn-level statistics can be used only if either the left or the right part of the comparison operation refers to the column itself and does not contain any expres- sions. For example, the planner cannot predict how computing a function of a column will affect statistics, so for conditions like “function-call = constant” the selectivity is always estimated at �.�%:1\n\n=> EXPLAIN SELECT * FROM flights WHERE extract(\n\nmonth FROM scheduled_departure AT TIME ZONE 'Europe/Moscow'\n\n) = 1;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..6384.17 rows=1074 width=63)\n\nFilter: (EXTRACT(month FROM (scheduled_departure AT TIME ZONE ...\n\n(2 rows)\n\n=> SELECT round(reltuples * 0.005) FROM pg_class WHERE relname = 'flights';\n\nround −−−−−−− 1074 (1 row)\n\nThe planner knows nothing about semantics of functions, even of standard ones. Our general knowledge suggests that the flights performed in January will make roughly 1 of the total number of flights,which exceeds the projected value by one 12 order of magnitude.\n\nTo improve the estimation,we have to collect expression statistics rather than rely on the column-level one. There are two ways to do it.\n\n1 backend/utils/adt/selfuncs.c, eqsel function\n\n326\n\n17.9 Expression Statistics\n\nExtended Expression Statistics\n\nThe first option is to use extended expression statistics.1 Such statistics are not col- lected by default; you have to manually create the corresponding database object by running the ������ ���������� command:\n\n=> CREATE STATISTICS flights_expr ON (extract(\n\nmonth FROM scheduled_departure AT TIME ZONE 'Europe/Moscow'\n\n)) FROM flights;\n\nOnce the data is gathered, the estimation accuracy improves:\n\n=> ANALYZE flights;\n\n=> EXPLAIN SELECT * FROM flights WHERE extract(\n\nmonth FROM scheduled_departure AT TIME ZONE 'Europe/Moscow'\n\n) = 1;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..6384.17 rows=16667 width=63)\n\nFilter: (EXTRACT(month FROM (scheduled_departure AT TIME ZONE ...\n\n(2 rows)\n\nFor the collected statistics to be applied, the query must specify the expression in exactly the same form that was used by the ������ ���������� command.\n\nThe size limit for extended statistics ����� ���������� command. For example:\n\ncan be adjusted separately, by running the\n\n=> ALTER STATISTICS flights_expr SET STATISTICS 42;\n\nAll the metadata related to extended statistics is stored in the pg_statistic_ext table of the system catalog, while the collected data itself resides in a separate table called pg_statistic_ext_data . This separation is used to implement access control for sensitive information.\n\nExtended expression statistics available to a particular user can be displayed in a more convenient format in a separate view:\n\n1 postgresql.org/docs/14/planner-stats#PLANNER-STATS-EXTENDED.html\n\nbackend/statistics/README\n\n327\n\nv. ��\n\nv. ��\n\nv. ��\n\np. ���\n\nChapter 17 Statistics\n\n=> SELECT left(expr,50) || '...' AS expr,\n\nnull_frac, avg_width, n_distinct, most_common_vals AS mcv, left(most_common_freqs::text,50) || '...' AS mcf, correlation\n\nFROM pg_stats_ext_exprs WHERE statistics_name = 'flights_expr' \\gx\n\n−[ RECORD 1 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− | EXTRACT(month FROM (scheduled_departure AT TIME ZO... expr | 0 null_frac | 8 avg_width | 12 n_distinct | {8,9,12,3,1,5,6,7,11,10,4,2} mcv mcf | {0.12053333,0.11326667,0.0802,0.07976667,0.0775666... correlation | 0.08355749\n\nStatistics for Expression Indexes\n\nAnother way to improve cardinality estimation is to use special statistics collected for expression indexes ; these statistics are gathered automatically when such an index is created, just like it is done for a table. If the index is really needed, this approach turns out to be very convenient.\n\n=> DROP STATISTICS flights_expr;\n\n=> CREATE INDEX ON flights(extract(\n\nmonth FROM scheduled_departure AT TIME ZONE 'Europe/Moscow'\n\n));\n\n=> ANALYZE flights;\n\n=> EXPLAIN SELECT * FROM flights WHERE extract(\n\nmonth FROM scheduled_departure AT TIME ZONE 'Europe/Moscow'\n\n) = 1;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights\n\n(cost=324.86..3247.92 rows=17089 wi... Recheck Cond: (EXTRACT(month FROM (scheduled_departure AT TIME... (cost=0.00..320.5... −> Bitmap Index Scan on flights_extract_idx Index Cond: (EXTRACT(month FROM (scheduled_departure AT TI...\n\n(4 rows)\n\n328\n\n17.10 Multivariate Statistics\n\nStatistics on expression indexes are stored in the same way as statistics on tables. For example, you can get the number of distinct values by specifying the index name as tablename when querying pg_stats:\n\n=> SELECT n_distinct FROM pg_stats WHERE tablename = 'flights_extract_idx';\n\nn_distinct −−−−−−−−−−−−\n\n12\n\n(1 row)\n\nof index-related statistics using the ����� ����� com- You can adjust the accuracy mand. If you do not know the column name that corresponds to the indexed ex- pression, you have to first find it out. For example:\n\n=> SELECT attname FROM pg_attribute WHERE attrelid = 'flights_extract_idx'::regclass;\n\nattname −−−−−−−−− extract\n\n(1 row)\n\n=> ALTER INDEX flights_extract_idx\n\nALTER COLUMN extract SET STATISTICS 42;\n\n17.10 Multivariate Statistics\n\nItis alsopossibletocollectmultivariatestatistics,whichspanseveraltablecolumns. As a prerequisite, you have to manually create the corresponding extended statis- tics using the ������ ���������� command.\n\nPostgre��� implements three types of multivariate statistics.\n\nFunctional Dependencies Between Columns\n\nIf values in one column depend (fully or partially) on values in another column and the filter conditions include both these columns, cardinality will be underes- timated.\n\n329\n\nv. ��\n\nv. ��\n\np. ���\n\nChapter 17 Statistics\n\nLet’s consider a query with two filter conditions:\n\n=> SELECT count(*) FROM flights WHERE flight_no = 'PG0007' AND departure_airport = 'VKO';\n\ncount −−−−−−−\n\n396\n\n(1 row)\n\nThe value is hugely underestimated:\n\n=> EXPLAIN SELECT * FROM flights WHERE flight_no = 'PG0007' AND departure_airport = 'VKO';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights\n\n(cost=10.49..816.84 rows=15 width=63)\n\nRecheck Cond: (flight_no = 'PG0007'::bpchar) Filter: (departure_airport = 'VKO'::bpchar) −> Bitmap Index Scan on flights_flight_no_scheduled_departure_key\n\n(cost=0.00..10.49 rows=276 width=0) Index Cond: (flight_no = 'PG0007'::bpchar)\n\n(6 rows)\n\nIt is a well-known problem of correlated predicates. The planner assumes that pred- icates do not depend on each other, so the overall selectivity is estimated at the product of selectivities of filter conditions combined by logical ��� . The plan above clearly illustrates this issue: the value estimated by the Bitmap Index Scan node for the condition on the flight_no column is significantly reduced once the Bitmap Heap Scan node filters the results by the condition on the departure_airport column.\n\nHowever,we do understand that airports are unambiguously defined by flight num- bers: the second condition is virtually redundant (unless there is a mistake in the airport name,of course). In such cases,we can improve the estimation by applying extended statistics on functional dependencies.\n\nLet’s create an extended statistic on the functional dependency between the two columns:\n\n=> CREATE STATISTICS flights_dep(dependencies) ON flight_no, departure_airport FROM flights;\n\n330\n\n17.10 Multivariate Statistics\n\nThe next analysis run gathers this statistic, and the estimation improves:\n\n=> ANALYZE flights;\n\n=> EXPLAIN SELECT * FROM flights WHERE flight_no = 'PG0007'\n\nAND departure_airport = 'VKO';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights\n\n(cost=10.57..819.51 rows=277 width=63)\n\nRecheck Cond: (flight_no = 'PG0007'::bpchar) Filter: (departure_airport = 'VKO'::bpchar) −> Bitmap Index Scan on flights_flight_no_scheduled_departure_key\n\n(cost=0.00..10.50 rows=277 width=0) Index Cond: (flight_no = 'PG0007'::bpchar)\n\n(6 rows)\n\nThe collected statistics is stored in the system catalog and can be accessed like this:\n\n=> SELECT dependencies FROM pg_stats_ext WHERE statistics_name = 'flights_dep';\n\ndependencies −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− {\"2 => 5\": 1.000000, \"5 => 2\": 0.010200}\n\n(1 row)\n\nHere � and � are column numbers stored in the pg_attribute table, whereas the corresponding values define the degree of functional dependency: from � (no de- pendency) to � (values in the second columns fully depend on values in the first column).\n\nMultivariate Number of Distinct Values\n\nStatistics on the number of unique combinations of values stored in different columns improves cardinality estimation of a ����� �� operation performed on several columns.\n\nFor example, here the estimated number of possible pairs of departure and arrival airports is the square of the total number of airports; however, the actual value is much smaller, as not all the pairs are connected by direct flights:\n\n331\n\nv. ��\n\nChapter 17 Statistics\n\n=> SELECT count(*) FROM (\n\nSELECT DISTINCT departure_airport, arrival_airport FROM flights\n\n) t;\n\ncount −−−−−−−\n\n618\n\n(1 row)\n\n=> EXPLAIN SELECT DISTINCT departure_airport, arrival_airport FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate\n\n(cost=5847.01..5955.16 rows=10816 width=8)\n\nGroup Key: departure_airport, arrival_airport −> Seq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=8)\n\n(3 rows)\n\nLet’s define and collect an extended statistic on distinct values:\n\n=> CREATE STATISTICS flights_nd(ndistinct) ON departure_airport, arrival_airport FROM flights;\n\n=> ANALYZE flights;\n\nThe cardinality estimation has improved:\n\n=> EXPLAIN SELECT DISTINCT departure_airport, arrival_airport FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate\n\n(cost=5847.01..5853.19 rows=618 width=8)\n\nGroup Key: departure_airport, arrival_airport −> Seq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=8)\n\n(3 rows)\n\nYou can view the collected statistic in the system catalog:\n\n=> SELECT n_distinct FROM pg_stats_ext WHERE statistics_name = 'flights_nd';\n\nn_distinct −−−−−−−−−−−−−−− {\"5, 6\": 618}\n\n(1 row)\n\n332\n\n17.10 Multivariate Statistics\n\nMultivariate MCV Lists\n\nIf the distribution of values is non-uniform, it may be not enough to rely on the functional dependency alone, as the estimation accuracy will highly depend on a particular pair of values. For example, the planner underestimates the number of flights performed by Boeing ��� from Sheremetyevo airport:\n\n=> SELECT count(*) FROM flights WHERE departure_airport = 'SVO' AND aircraft_code = '733';\n\ncount −−−−−−− 2037 (1 row)\n\n=> EXPLAIN SELECT * FROM flights WHERE departure_airport = 'SVO' AND aircraft_code = '733';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..5847.00 rows=736 width=63)\n\nFilter: ((departure_airport = 'SVO'::bpchar) AND (aircraft_cod...\n\n(2 rows)\n\nIn this case,you can improve the estimation by collecting statistics on multivariate ��� lists:1\n\n=> CREATE STATISTICS flights_mcv(mcv) ON departure_airport, aircraft_code FROM flights;\n\n=> ANALYZE flights;\n\nThe new cardinality estimation is much more accurate:\n\n=> EXPLAIN SELECT * FROM flights WHERE departure_airport = 'SVO' AND aircraft_code = '733';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..5847.00 rows=1927 width=63)\n\nFilter: ((departure_airport = 'SVO'::bpchar) AND (aircraft_cod...\n\n(2 rows)\n\n1 backend/statistics/README.mcv\n\nbackend/statistics/mcv.c\n\n333\n\nv. ��\n\n100\n\nv. ��\n\nv. ��\n\nChapter 17 Statistics\n\nTo get this estimation, the planner relies on the frequency values stored in the system catalog:\n\n=> SELECT values, frequency FROM pg_statistic_ext stx\n\nJOIN pg_statistic_ext_data stxd ON stx.oid = stxd.stxoid, pg_mcv_list_items(stxdmcv) m\n\nWHERE stxname = 'flights_mcv' AND values = '{SVO,773}';\n\nvalues\n\n|\n\nfrequency\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−− {SVO,773} | 0.005266666666666667\n\n(1 row)\n\ndefault_statistics_target values Just like a regular ��� list, a multivariate list holds (if this parameter is also set at the column level, the largest of its values is used).\n\nIf required, you can also change the size of the list, expression statistics:\n\nlike it is done for extended\n\nALTER STATISTICS ... SET STATISTICS ...;\n\nIn all these examples, I have used only two columns, but you can collect multivari- ate statistics on a larger number of columns too.\n\nTo combine statistics of several types in one object, you can provide a comma- separated list of these types in its definition. If no type is specified, Postgre��� will collect statistics of all the possible types for the specified columns.\n\nApart from the actual column names, expressions, just like expression statistics.\n\nmultivariate statistics can also use arbitrary\n\n334\n\n18\n\nTable Access Methods\n\n18.1 Pluggable Storage Engines\n\nThe data layout used by Postgre��� is neither the only possible nor the best one for you to create all load types. Following the idea of extensibility, Postgre��� allows and plug in various table access methods (pluggable storage engines), but there is only one available out of the box at the moment:\n\n=> SELECT amname, amhandler FROM pg_am WHERE amtype = 't';\n\namname |\n\namhandler\n\n−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−\n\nheap (1 row)\n\n| heap_tableam_handler\n\nYou can specify the engine to use when creating a table (������ ����� ... �����); default_table_access_method parameter otherwise, the default engine listed in the will be applied.\n\nFor the Postgre��� core to work with various engines in the same way, table ac- cess methods must implement a special interface.1 The function specified in the amhandler column returns the interface structure2 that contains all the informa- tion required by the core.\n\nThe following core components can be used by all table access methods:\n\ntransaction manager, including ���� and snapshot isolation support\n\nbuffer manager\n\n1 postgresql.org/docs/14/tableam.html 2 include/access/tableam.h\n\n335\n\nv. ��\n\nheap\n\nChapter 18 Table Access Methods\n\n�/� subsystem\n\n�����\n\noptimizer and executor\n\nindex support\n\nThese components always remain at the disposal of the engine, even if it does not use them all.\n\nIn their turn, engines define:\n\ntuple format and data structure\n\ntable scan implementation and cost estimation\n\nimplementation of insert, delete, update, and lock operations\n\nvisibility rules\n\nvacuum and analysis procedures\n\nHistorically,Postgre��� used a single built-in data storage without any proper pro- gramming interface,so now it is very hard to come up with a good design that takes all the specifics of the standard engine into account and does not interfere with other methods.\n\nFor example,it is still unclear how to deal with the ���. New access methods may need to log their own operations that the core is unaware of. The existing generic ��� mechanism1 is usually a bad choice, as it incurs too much overhead. You can add yet another interface for handling new types of ��� entries, but then crash recovery will depend on external code, which is highly undesirable. The only plausible solution so far is patching the core for each particular engine.\n\nFor this reason, I did not strive to provide any strict distinction between table ac- cess methods and the core. Many features described in the previous parts of the book formally belong to the heap access method rather than to the core itself. This method is likely to always remain the ultimate standard engine for Postgre���, while other methods will fill separate niches to address challenges of specific load types.\n\n1 postgresql.org/docs/14/generic-wal.html\n\n336\n\n18.2 Sequential Scans\n\nOf all the new engines that are currently being developed, I would like to mention the following:\n\nZheap is aimed at fighting table bloating.1 It implements in-place row updates and moves historic ����-related data into a separate undo storage. Such an engine will be useful for loads that involve frequent data updates.\n\nZheap architecture will seem familiar to Oracle users, although it does have some nuances (for example, the interface of index access methods does not allow creating indexes with their own versioning).\n\nZedstore implements columnar storage,2 which is likely to be most efficient with\n\n���� queries.\n\nThe stored data is structured as a �-tree of tuple ��s; each column is stored in its own �-tree associated with the main one. In the future,it might be possible to store several columns in one �-tree, thus getting a hybrid storage.\n\n18.2 Sequential Scans\n\nThe storage engine defines the physical layout of table data and provides an access method to it. The only supported method is a sequential scan, which reads the file (or files) of the table’s main fork in full. In each read page, the visibility of each tuple is checked; those tuples that do not satisfy the query are filtered out.\n\ntable page\n\na tuple to be filtered out\n\n1 github.com/EnterpriseDB/zheap 2 github.com/greenplum-db/postgres/tree/zedstore\n\n337\n\np. ���\n\np. ��\n\np. ���\n\np. ���\n\nChapter 18 Table Access Methods\n\nA scanning process goes through the buffer cache; to ensure that large tables do not oust useful data, a small-sized buffer ring is employed. Other processes that are scanning the same table join this buffer ring, thus avoiding extra disk reads; such scans are called synchronized. Thus, scanning does not always have to begin at the start of the file.\n\nSequential scanning is the most efficient way to read the whole table or the best part of it. In other words, sequential scans bring the most value when the selec- tivity is low. (If the selectivity is high, meaning that the query has to select only a .) few rows, it is preferable to use an index\n\nCost Estimation\n\nIn the query execution plan,a sequential scan is represented by the Seq Scan node:\n\n=> EXPLAIN SELECT * FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=63)\n\n(1 row)\n\nThe estimated number of rows is provided as part of the basic statistics:\n\n=> SELECT reltuples FROM pg_class WHERE relname = 'flights';\n\nreltuples −−−−−−−−−−−\n\n214867\n\n(1 row)\n\nWhen estimating the cost, the optimizer takes the following two components into account: disk �/� and ��� resources.1\n\nI/� cost is calculated by multiplying the number of pages in a table and the cost of reading a single page assuming that pages are being read sequentially. When\n\n1 backend/optimizer/path/costsize.c, cost_seqscan function\n\n338\n\n18.2 Sequential Scans\n\nthe buffer manager requests a page, the operating system actually reads more data from disk, so several subsequent pages are highly likely to be found in the operating system cache. For this reason, the cost of reading a single page seq_page_cost) is using sequential scanning (which the planner estimates at lower than the random access cost (defined by the\n\nrandom_page_cost value).\n\nThe default settings work well for ���s; if you are using ���s, it makes sense to significantly reduce the random_page_cost value (the seq_page_cost param- eter is usually left as is, serving as a reference value). Since the optimal ratio between these parameters depends on the hardware, they are usually set at the tablespace level (����� ���������� ... ���).\n\n=> SELECT relpages,\n\ncurrent_setting('seq_page_cost') AS seq_page_cost, relpages * current_setting('seq_page_cost')::real AS total\n\nFROM pg_class WHERE relname = 'flights';\n\nrelpages | seq_page_cost | total −−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−\n\n2624 | 1\n\n|\n\n2624\n\n(1 row)\n\nThese calculations clearly show the consequences of table bloating caused by untimely vacuuming: the larger the main fork of the table, the more pages have to be scanned, regardless of the number of live tuples they contain.\n\nC�� resource estimation comprises the costs of processing each tuple (which the\n\nplanner estimates at\n\ncpu_tuple_cost):\n\n=> SELECT reltuples,\n\ncurrent_setting('cpu_tuple_cost') AS cpu_tuple_cost, reltuples * current_setting('cpu_tuple_cost')::real AS total\n\nFROM pg_class WHERE relname = 'flights';\n\nreltuples | cpu_tuple_cost |\n\ntotal\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n214867 | 0.01\n\n| 2148.67\n\n(1 row)\n\nThe sum of these two estimates represents the total cost of the plan. The startup cost is zero because sequential scans have no prerequisites.\n\n339\n\n1 4\n\np. ���\n\n0.01\n\np. ���\n\n0.0025\n\nChapter 18 Table Access Methods\n\nIf the scanned table needs to be filtered, the applied filter conditions appear in the plan under the Filter section of the Seq Scan node. The estimated row count depends on the selectivity of these conditions, while the cost estimation includes the related computation expenses.\n\nThe ������� ������� command displays both the actual number of returned rows and the number of rows that have been filtered out:\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT * FROM flights WHERE status = 'Scheduled';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..5309.84 rows=15383 width=63) (actual rows=15383 loops=1) Filter: ((status)::text = 'Scheduled'::text) Rows Removed by Filter: 199484\n\n(5 rows)\n\nLet’s take a look at a more complex execution plan that uses aggregation:\n\n=> EXPLAIN SELECT count(*) FROM seats;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nAggregate\n\n(cost=24.74..24.75 rows=1 width=8)\n\n−> Seq Scan on seats\n\n(cost=0.00..21.39 rows=1339 width=0)\n\n(2 rows)\n\nThe plan consists of two nodes: the upper node (Aggregate), which computes the count function, pulls the data from the lower node (Seq Scan), which scans the table.\n\nThe startup cost of the Aggregate node includes the aggregation itself: it is im- possible to return the first row (which is the only one in this case) without getting all the rows from the lower node. The aggregation cost is estimated based on the cpu_operator_cost) for each execution cost of a conditional operation (estimated at input row:1\n\n1 backend/optimizer/path/costsize.c, cost_agg function\n\n340\n\n18.2 Sequential Scans\n\n=> SELECT reltuples,\n\ncurrent_setting('cpu_operator_cost') AS cpu_operator_cost, round((\n\nreltuples * current_setting('cpu_operator_cost')::real\n\n)::numeric, 2) AS cpu_cost\n\nFROM pg_class WHERE relname = 'seats';\n\nreltuples | cpu_operator_cost | cpu_cost −−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−\n\n1339 | 0.0025\n\n|\n\n3.35\n\n(1 row)\n\nThe received estimate is added to the total cost of the Seq Scan node.\n\nThe total cost of the Aggregate node also includes the cost of processing a row to be returned, which is estimated at\n\ncpu_tuple_cost:\n\n=> WITH t(cpu_cost) AS (\n\nSELECT round((\n\nreltuples * current_setting('cpu_operator_cost')::real\n\n)::numeric, 2) FROM pg_class WHERE relname = 'seats'\n\n) SELECT 21.39 + t.cpu_cost AS startup_cost,\n\nround((\n\n21.39 + t.cpu_cost + 1 * current_setting('cpu_tuple_cost')::real\n\n)::numeric, 2) AS total_cost\n\nFROM t;\n\nstartup_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n24.74 |\n\n24.75\n\n(1 row)\n\nThus, cost estimation dependencies can be pictured as follows:\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nAggregate\n\n(cost=24.74..24.75 rows=1 width=8) −> Seq Scan on seats\n\n(cost=0.00..21.39 rows=1339 width=0)\n\n(4 rows)\n\n341\n\n0.01\n\nv. �.�\n\nv. �� on\n\nChapter 18 Table Access Methods\n\n18.3 Parallel Plans\n\nparallel query execution.1 The leading process that performs Postgre��� supports the query spawns (via postmaster) several worker processes that execute one and the same parallel part of the plan simultaneously. The results are passed to the leader, which puts them together in the Gather2 node. When not accepting the data, the leader may also participate in the execution of the parallel part of the plan.\n\nIf required, by turning off the\n\nyou can forbid the leader’s contributions to the parallel plan execution\n\nparallel_leader_participation parameter.\n\nsequential part of the plan\n\nGather\n\nparallel part of the plan\n\nparallel part of the plan\n\nparallel part of the plan\n\nworker\n\nleader\n\nworker\n\nNaturally, starting these processes and sending data between them is not free, so not all queries by far should be parallelized.\n\nBesides, not all parts of the plan can be processed concurrently, even if parallel execution is allowed. Some of the operations are performed by the leader alone,in the sequential mode.\n\n1 postgresql.org/docs/14/parallel-query.html backend/access/transam/README.parallel\n\n2 backend/executor/nodeGather.c\n\n342",
      "page_number": 313
    },
    {
      "number": 18,
      "title": "Table Access Methods",
      "start_page": 337,
      "end_page": 358,
      "detection_method": "regex_chapter",
      "content": "18.4 Parallel Sequential Scans\n\nPostgre��� does not support the other approach to parallel plan execution, which con- sists in performing data processing by several workers that virtually form an assembly line (roughly speaking, each plan node is performed by a separate process); this mechanism was deemed inefficient by Postgre��� developers.\n\n18.4 Parallel Sequential Scans\n\nOne of the nodes designed for parallel processing is the Parallel Seq Scan node, which performs a parallel sequential scan.\n\nThe name sounds a bit controversial (is the scan sequential or parallel after all?), butnevertheless,it reflectstheessenceof the operation. If wetakea look at the file access, table pages are read sequentially, following the order in which they would have been read by a simple sequential scan. However, this operation is performed by several concurrent processes. To avoid scanning one and the same page twice, the executor synchronizes these processes via shared memory.\n\nA subtle aspect here is that the operating system does not get the big picture typi- cal of sequential scanning; instead, it sees several processes that perform random reads. Therefore,dataprefetchingthatusuallyspeedsupsequentialscansbecomes virtually useless. To minimize this unpleasant effect, Postgre��� assigns each pro- cess not just one but several consecutive pages to read.1\n\nAs such, parallel scanning does not make much sense because the usual read costs are further increased by the overhead incurred by data transfer from process to pro- cess. However, if workers perform any post-processing on the fetched rows (such as aggregation), the total execution time may turn out to be much shorter.\n\nCost Estimation\n\nLet’s take a look at a simple query that performs aggregation on a large table. The execution plan is parallelized:\n\n1 backend/access/heap/heapam.c,\n\ntable_block_parallelscan_startblock_init & table_block_parallel-\n\nscan_nextpage functions\n\n343\n\nv. ��\n\nChapter 18 Table Access Methods\n\n=> EXPLAIN SELECT count(*) FROM bookings;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n(cost=25442.58..25442.59 rows=1 width=8)\n\n−> Gather\n\n(cost=25442.36..25442.57 rows=2 width=8)\n\nWorkers Planned: 2 −> Partial Aggregate\n\n(cost=24442.36..24442.37 rows=1 width=8) −> Parallel Seq Scan on bookings\n\n(cost=0.00..22243.29 rows=879629 width=0)\n\n(7 rows)\n\nAllthenodesbelowGatherbelongtotheparallelpartoftheplan. Theyareexecuted by each of the workers (two of them are planned here) and possibly by the leader process (unless this functionality is turned off by the parallel_leader_participation parameter). The Gather node itself and all the nodes above it make the sequential part of the plan and are executed by the leader process alone.\n\nThe Parallel Seq Scan node represents a parallel heap scan. The rows field shows the estimated average number of rows to be processed by a single process. All in all, the execution must be performed by three processes (one leader and two workers), but the leader process will handle fewer rows: its share gets smaller as the number of workers grows.1 In this particular case, the factor is �.�.\n\n=> SELECT reltuples::numeric, round(reltuples / 2.4) AS per_process FROM pg_class WHERE relname = 'bookings';\n\nreltuples | per_process −−−−−−−−−−−+−−−−−−−−−−−−−\n\n2111110 |\n\n879629\n\n(1 row)\n\nThe Parallel Seq Scan cost is calculated similar to that of a sequential scan. The re- ceived value is smaller,as each process handles fewer rows; the �/� part is included in full since the whole table still has to be read, page by page:\n\n=> SELECT round((\n\nrelpages reltuples / 2.4 * current_setting('cpu_tuple_cost')::real\n\ncurrent_setting('seq_page_cost')::real +\n\n)::numeric, 2) FROM pg_class WHERE relname = 'bookings';\n\n1 backend/optimizer/path/costsize.c, get_parallel_divisor function\n\n344\n\n18.4 Parallel Sequential Scans\n\nround −−−−−−−−−− 22243.29\n\n(1 row)\n\nNext, the Partial Aggregate node performs aggregation of the fetched data; in this particular case, it counts the number of rows.\n\nThe aggregation cost is estimated in the usual manner and is added to the cost estimation of the table scan:\n\n=> WITH t(startup_cost) AS (\n\nSELECT 22243.29 + round((\n\nreltuples / 2.4 * current_setting('cpu_operator_cost')::real\n\n)::numeric, 2) FROM pg_class WHERE relname = 'bookings'\n\n) SELECT startup_cost,\n\nstartup_cost + round((\n\n1 * current_setting('cpu_tuple_cost')::real\n\n)::numeric, 2) AS total_cost\n\nFROM t;\n\nstartup_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n24442.36 |\n\n24442.37\n\n(1 row)\n\nThe next node (Gather) is executed by the leader process. This node is responsible for launching workers and gathering the data they return.\n\nFor the purpose of planning, the cost estimation of starting processes (regardless parallel_setup_cost parameter, while the cost of of their number) is defined by the each row transfer between the processes is estimated at\n\nparallel_tuple_cost.\n\nIn this example, the startup cost (spent on starting the processes) prevails; this value is added to the startup cost of the Partial Aggregate node. The total cost also includes the cost of transferring two rows; this value is added to the total cost of the Partial Aggregate node:1\n\n1 backend/optimizer/path/costsize.c, cost_gather function\n\n345\n\n1000 0.1\n\nChapter 18 Table Access Methods\n\n=> SELECT\n\n24442.36 + round(\n\ncurrent_setting('parallel_setup_cost')::numeric,\n\n2) AS setup_cost, 24442.37 + round(\n\ncurrent_setting('parallel_setup_cost')::numeric + 2 * current_setting('parallel_tuple_cost')::numeric,\n\n2) AS total_cost;\n\nsetup_cost | total_cost −−−−−−−−−−−−+−−−−−−−−−−−−\n\n25442.36 |\n\n25442.57\n\n(1 row)\n\nLast but not least, the Finalize Aggregate node aggregates all the partial results received by the Gather node from the parallel processes.\n\nThe final aggregation is estimated just like any other. Its startup cost is based on the cost of aggregating three rows; this value is added to the total cost of Gather (since all the rows are needed to compute the result). The total cost of Finalize Aggregate also includes the cost of returning one row.\n\n=> WITH t(startup_cost) AS ( SELECT 25442.57 + round((\n\n3 * current_setting('cpu_operator_cost')::real\n\n)::numeric, 2) FROM pg_class WHERE relname = 'bookings'\n\n) SELECT startup_cost,\n\nstartup_cost + round((\n\n1 * current_setting('cpu_tuple_cost')::real\n\n)::numeric, 2) AS total_cost\n\nFROM t;\n\nstartup_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n25442.58 |\n\n25442.59\n\n(1 row)\n\nDependencies between cost estimations are determined by whether the node has to accumulate the data before passing the result to its parent node. Aggregation cannot return the result until it gets all the input rows, so its startup cost is based onthetotal costofthelowernode. TheGathernode,onthecontrary,startssending rows upstream as soon as they are fetched. Therefore, the startup cost of this op- eration depends on the startup cost of the lower node, while its total cost is based on the lower node’s total cost.\n\n346\n\n18.5 Parallel Execution Limitations\n\nHere is the dependency graph:\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n(cost=25442.58..25442.59 rows=1 width=8) −> Gather\n\n(cost=25442.36..25442.57 rows=2 width=8) Workers Planned: 2 −> Partial Aggregate\n\n(cost=24442.36..24442.37 rows=1 width=8) −> Parallel Seq Scan on bookings\n\n(cost=0.00..22243.29 rows=879629 width=0)\n\n(9 rows)\n\n18.5 Parallel Execution Limitations\n\nNumber of Background Workers\n\nThe number of processes is controlled by a hierarchy of three parameters. The maximal number of background workers running concurrently is defined by the max_worker_processes value.\n\nHowever,parallel query execution is not the only operation that needs background workers. For example, they also participate in logical replication and can be used by extensions. The number of processes allocated specifically for parallel plan ex- ecution is limited to the\n\nmax_parallel_workers value.\n\nOut of this number,up to leader.\n\nmax_parallel_workers_per_gather processes can serve one\n\nThe choice of these parameter values depends on the following factors:\n\nHardware capabilities: the system must have free cores dedicated to parallel\n\nexecution.\n\nTable sizes: the database must contain large tables.\n\nA typical load: there must be queries that potentially benefit from parallel\n\nexecution.\n\n347\n\n8\n\n8\n\n2\n\n8MB\n\nChapter 18 Table Access Methods\n\nThese criteria are typically met by ���� systems rather than ���� ones.\n\nThe planner will not consider parallel execution at all if the estimated volume of heap data to be read does not exceed the\n\nmin_parallel_table_scan_size value.\n\nUnless the number of processes for a particular table is explicitly specified in the parallel_workers storage parameter, it will be calculated by the following formula:\n\n1 +\n\n⌊\n\nlog3 (\n\ntable size\n\nmin_parallel_table_scan_size)⌋\n\nIt means that each time a table grows three times, Postgre��� assigns one more parallel worker for its processing. The default settings give us these figures:\n\ntable, ��\n\nnumber of processes\n\n8 �� �� ��� ��� ����\n\n� � � � � �\n\nIn any case, the number of parallel workers cannot exceed the limit defined by the max_parallel_workers_per_gather parameter.\n\nIf we query a small table of �� ��, only one worker will be planned and launched:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=2 loops=1)\n\nWorkers Planned: 1 Workers Launched: 1 −> Partial Aggregate (actual rows=1 loops=2)\n\n−> Parallel Seq Scan on flights (actual rows=107434 lo...\n\n(6 rows)\n\n348\n\n18.5 Parallel Execution Limitations\n\nA query on a table of ��� �� gets only two processes because it hits the limit of max_parallel_workers_per_gather workers:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM bookings;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=3 loops=1)\n\nWorkers Planned: 2 Workers Launched: 2 −> Partial Aggregate (actual rows=1 loops=3)\n\n−> Parallel Seq Scan on bookings (actual rows=703703 l...\n\n(6 rows)\n\nIf we remove this limit, we will get the estimated three processes:\n\n=> ALTER SYSTEM SET max_parallel_workers_per_gather = 4;\n\n=> SELECT pg_reload_conf();\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM bookings;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=4 loops=1)\n\nWorkers Planned: 3 Workers Launched: 3 −> Partial Aggregate (actual rows=1 loops=4)\n\n−> Parallel Seq Scan on bookings (actual rows=527778 l...\n\n(6 rows)\n\nIf the number of slots that are free during query execution turns out to be smaller than the planned value, only the available number of workers will be launched.\n\nLet’s limit the total number of parallel processes to five and run two queries simul- taneously:\n\n=> ALTER SYSTEM SET max_parallel_workers = 5;\n\n=> SELECT pg_reload_conf();\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM bookings;\n\n349\n\n2\n\nChapter 18 Table Access Methods\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM bookings;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=3 loops=1)\n\nWorkers Planned: 3 Workers Launched: 2 −> Partial Aggregate (actual rows=1 loops=3)\n\n−> Parallel Seq Scan on bookings (actual rows=7037...\n\n(6 rows)\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=4 loops=1)\n\nWorkers Planned: 3 Workers Launched: 3 −> Partial Aggregate (actual rows=1 loops=4)\n\n−> Parallel Seq Scan on bookings (actual rows=527778 l...\n\n(6 rows)\n\nAlthough three processes were expected in both cases,one of the queries managed to get only two slots.\n\nLet’s restore the default settings:\n\n=> ALTER SYSTEM RESET ALL;\n\n=> SELECT pg_reload_conf();\n\nNon-Parallelizable Queries\n\nNot all queries can be parallelized.1 In particular,parallel plans cannot be used for the following query types:\n\nQueries that modify or lock data (������, ������, ������ ��� ������, and the\n\nlike).\n\n1 postgresql.org/docs/14/when-can-parallel-query-be-used.html\n\n350\n\n18.5 Parallel Execution Limitations\n\nThis restriction does not apply to subqueries within the following commands:\n\n– ������ ����� ��,\n\n������ ����, ������ ������������ ����\n\n– ������� ������������ ����\n\nHowever, row insertion is still performed sequentially in all these cases.\n\nQueries that can be paused. It applies to queries run within cursors,including\n\n��� loops in ��/pg���.\n\nQueries that call �������� ������ functions. By default, these are all user- defined functions and a few standard ones. You can get the full list of unsafe functions by querying the system catalog:\n\nSELECT * FROM pg_proc WHERE proparallel = 'u';\n\nQuerieswithinfunctionsifthesefunctionsarecalledfromaparallelizedquery\n\n(to avoid recursive growth of the number of workers).\n\nSome of these limitations may be removed in the future versions of Postgre���. the ability to parallelize queries at the Serializable isolation level is For example, already there.\n\nParallel insertion of rows using such commands as ������ and ���� is currently under development.1\n\nA query may remain unparallelized for several reasons:\n\nThis type of a query does not support parallelization at all.\n\nParallel plan usage is forbidden by the server configuration (for example, be-\n\ncause of the imposed table size limit).\n\nA parallel plan is more expensive than a sequential one.\n\nTo check whether a query can be parallelized at all, you can temporarily switch force_parallel_mode parameter. Then the planner will build parallel plans on the whenever possible:\n\n1 commitfest.postgresql.org/32/2844 commitfest.postgresql.org/32/2841 commitfest.postgresql.org/32/2610\n\n351\n\nv. ��\n\nv. ��\n\nv. ��\n\noff\n\nChapter 18 Table Access Methods\n\n=> EXPLAIN SELECT * FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=63)\n\n(1 row)\n\n=> SET force_parallel_mode = on;\n\n=> EXPLAIN SELECT * FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nGather\n\n(cost=1000.00..27259.37 rows=214867 width=63)\n\nWorkers Planned: 1 Single Copy: true −> Seq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=63)\n\n(4 rows)\n\nParallel Restricted Queries\n\nThe bigger is the parallel part of the plan, the more performance gains can be po- tentially achieved. However, certain operations are executed strictly sequentially bytheleaderprocessalone,1 eventhoughtheydonotinterferewithparallelization as such. In other words,they cannot appear in the plan tree below the Gather node.\n\nNon-expandable subqueries. The most obvious example of a non-expandable sub- query2 is scanning a ��� result (represented in the plan by the CTE Scan node):\n\n=> EXPLAIN (costs off) WITH t AS MATERIALIZED ( SELECT * FROM flights\n\n) SELECT count(*) FROM t;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nAggregate CTE t\n\n−> Seq Scan on flights\n\n−> CTE Scan on t\n\n(4 rows)\n\n1 postgresql.org/docs/14/parallel-safety.html 2 backend/optimizer/plan/subselect.c\n\n352\n\n18.5 Parallel Execution Limitations\n\nIf limitation does not apply.\n\na ��� is not materialized, the plan does not contain the CTE Scan node, so this\n\nNote,however,that a ��� itself can be computed in the parallel mode if it turns out to be less expensive:\n\n=> EXPLAIN (costs off) WITH t AS MATERIALIZED (\n\nSELECT count(*) FROM flights\n\n) SELECT * FROM t;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nCTE Scan on t\n\nCTE t\n\n−> Finalize Aggregate\n\n−> Gather\n\nWorkers Planned: 1 −> Partial Aggregate\n\n−> Parallel Seq Scan on flights\n\n(7 rows)\n\nAnother example of a non-expandable subquery is shown under by the SubPlan node in the plan below:\n\n=> EXPLAIN (costs off) SELECT * FROM flights f WHERE f.scheduled_departure > ( -- SubPlan\n\nSELECT min(f2.scheduled_departure) FROM flights f2 WHERE f2.aircraft_code = f.aircraft_code\n\n);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights f\n\nFilter: (scheduled_departure > (SubPlan 1)) SubPlan 1\n\n−> Aggregate\n\n−> Seq Scan on flights f2\n\nFilter: (aircraft_code = f.aircraft_code)\n\n(6 rows)\n\nThe first two rows represent the plan of the main query: the flights table is scanned sequentially, and each of its rows is checked against the provided filter. The filter\n\n353\n\nv. ��\n\nChapter 18 Table Access Methods\n\ncondition includes a subquery; the plan of this subquery starts on the third row. So theSubPlannodeisexecutedseveraltimes,onceforeach rowfetchedbysequential scanning in this case.\n\nThe upper Seq Scan node of this plan cannot participate in parallel execution be- cause it relies on the data returned by the SubPlan node.\n\nLast but not least, here is one more non-expandable subquery represented by the InitPlan node:\n\n=> EXPLAIN (costs off) SELECT * FROM flights f WHERE f.scheduled_departure > ( -- SubPlan\n\nSELECT min(f2.scheduled_departure) FROM flights f2 WHERE EXISTS ( -- InitPlan\n\nSELECT * FROM ticket_flights tf WHERE tf.flight_id = f.flight_id\n\n)\n\n);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights f\n\nFilter: (scheduled_departure > (SubPlan 2)) SubPlan 2\n\n−> Finalize Aggregate\n\nInitPlan 1 (returns $1)\n\n−> Seq Scan on ticket_flights tf\n\nFilter: (flight_id = f.flight_id)\n\n−> Gather\n\nWorkers Planned: 1 Params Evaluated: $1 −> Partial Aggregate\n\n−> Result\n\nOne−Time Filter: $1 −> Parallel Seq Scan on flights f2\n\n(14 rows)\n\nUnlike the SubPlan node,InitPlan is evaluatedonly once (in this particular example, once per each execution of the SubPlan 2 node).\n\nThe parent node of InitPlan cannot participate in parallel execution (but those nodes that receive the result of the InitPlan evaluation can, like in this example).\n\n354\n\n18.5 Parallel Execution Limitations\n\nTemporary tables. Temporary tables do not support parallel scanning,as they can be accessed exclusively by the process that has created them. Their pages are pro- buffer cache. Making the local cache accessible to several pro- cessed in the local cesses would require a locking mechanism like in the shared cache, which would make its other benefits less prominent.\n\n=> CREATE TEMPORARY TABLE flights_tmp AS SELECT * FROM flights;\n\n=> EXPLAIN (costs off) SELECT count(*) FROM flights_tmp;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nAggregate\n\n−> Seq Scan on flights_tmp\n\n(2 rows)\n\nParallel restricted functions. Functions defined as �������� ���������� are allowed only in the sequential part of the plan. You can get the list of such functions from the system catalog by running the following query:\n\nSELECT * FROM pg_proc WHERE proparallel = 'r';\n\nOnly label your functions as �������� ���������� (to say nothing of �������� ����) if you are fully aware of all the implications and have carefully studied all the im- posed restrictions.1\n\n1 postgresql.org/docs/14/parallel-safety#PARALLEL-LABELING.html\n\n355\n\np. ��� p. ���\n\nv. �.�\n\np. ���\n\n19\n\nIndex Access Methods\n\n19.1 Indexes and Extensibility\n\nIndexes are database objects that mainly serve the purpose of accelerating data ac- cess. These are auxiliary structures: any index can be deleted and recreated based on heap data. In addition to data access speedup, indexes are also used to enforce some integrity constraints.\n\nThe Postgre��� core provides six built-in index access methods (index types):\n\n=> SELECT amname FROM pg_am WHERE amtype = 'i';\n\namname −−−−−−−− btree hash gist gin spgist brin\n\n(6 rows)\n\nthat new access methods can be added without Postgre���’s extensibility implies modifying the core. One such extension (the bloom method) is included into the standard set of modules.\n\nDespite all the differences between various index types, all of them eventually match a key (such as a value of an indexed column) against heap tuples that con- tain this key. Tuples are referred to by six-byte tuple ��s, or ���s. Knowing the key or some information about the key,it is possible to quickly read the tuples that are likely to contain the required data without scanning the whole table.\n\n356\n\n19.1 Indexes and Extensibility\n\nTo ensure that a new access method can be added as an extension, Postgre��� im- plements a common indexing engine. Its main objective is to retrieve and process ���s returned by a particular access method:\n\nread data from the corresponding heap tuples\n\ncheck tuple visibility\n\nagainst a particular snapshot\n\nrecheck conditions if their evaluation by the method is indecisive\n\nThe indexing engine also participates in execution of plans built at the optimiza- tion stage. When assessing various execution paths, the optimizer needs to know the properties of all potentially applicable access methods: can the method return the data in the required order,or do we need a separate sorting stage? is it possible to return several first values right away, or do we have to wait for the whole result set to be fetched? and so on.\n\nItisnotonlytheoptimizerthatneedstoknowspecificsoftheaccessmethod. Index creation poses more questions to answer: does the access method support multi- column indexes? can this index guarantee uniqueness?\n\nThe indexing engine allows using a variety of access methods; in order to be sup- ported, an access method must implement a particular interface to declare its fea- tures and properties.\n\nAccess methods are used to address the following tasks:\n\nimplement algorithms for building indexes, as well as inserting and deleting\n\nindex entries\n\ndistribute index entries between pages (to be further handled by the buffer\n\ncache manager\n\n)\n\nimplement the algorithm of vacuuming\n\nacquire locks\n\nto ensure correct concurrent operation\n\ngenerate ��� entries\n\nsearch indexed data by the key\n\nestimate index scan costs\n\n357\n\np. ��\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\nChapter 19 Index Access Methods\n\nExtensibility also manifests itself as the ability to add new data types, which the access method knows nothing of in advance. Therefore, access methods have to define their own interfaces for plugging in arbitrary data types.\n\nTo enable usage of a new data type with a particular access method,you have to im- plement the corresponding interface—that is, provide operators that can be used with an index, and possibly some auxiliary support functions. Such a set of opera- tors and functions is called an operator class.\n\nThe indexing logic is partially implemented by the access method itself, but some of it is outsourced to operator classes. This distribution is rather arbitrary: while �-trees have all the logic wired into the access method, some other methods may provide only the main framework, leaving all the implementation details at the discretion of particular operator classes. One and the same data type is often sup- ported by several operator classes, and the user can select the one with the most suitable behavior.\n\nHere is a small fraction of the overall picture:\n\nbool_ops\n\nboolean\n\nint4_ops\n\ninteger\n\nbtree\n\ntext_ops\n\ntext\n\nIndexing engine\n\ntext_pattern_ops\n\ngist_int4_ops\n\ngist\n\ngist_text_ops\n\npoint_ops\n\npoint\n\naccess methods\n\noperator classes\n\ndata types\n\n358\n\n19.2 Operator Classes and Families\n\n19.2 Operator Classes and Families\n\nOperator Classes\n\nAn access method interface1 is implemented by an operator class,2 which is a set of operators and support functions applied by the access method to a particular data type.\n\nClasses of operators are stored in the pg_opclass table in the system catalog. The following query returns the complete data for the above illustration:\n\n=> SELECT amname, opcname, opcintype::regtype FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid;\n\namname |\n\nopcname\n\n|\n\nopcintype\n\n−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nbtree hash btree btree ... brin brin brin\n\n| array_ops | array_ops | bit_ops | bool_ops\n\n| pg_lsn_minmax_multi_ops | pg_lsn_bloom_ops | box_inclusion_ops\n\n| anyarray | anyarray | bit | boolean\n\n| pg_lsn | pg_lsn | box\n\n(177 rows)\n\nIn most cases,we do not have to know anything about operator classes. We simply create an index that uses some operator class by default.\n\nFor example,here are �-tree operator classes that support the text type. One of the classes is always marked as the default one:\n\n=> SELECT opcname, opcdefault FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid\n\nWHERE amname = 'btree'\n\nAND opcintype = 'text'::regtype;\n\n1 postgresql.org/docs/14/xindex.html 2 postgresql.org/docs/14/indexes-opclass.html\n\n359\n\nChapter 19 Index Access Methods\n\nopcname\n\n| opcdefault\n\n−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n| t text_ops | f varchar_ops text_pattern_ops | f varchar_pattern_ops | f\n\n(4 rows)\n\nA typical command for index creation looks as follows:\n\nCREATE INDEX ON aircrafts(model, range);\n\nBut it is just a shorthand notation that expands to the following syntax:\n\nCREATE INDEX ON aircrafts USING btree -- the default access method (\n\nmodel text_ops, -- the default operator class for text range int4_ops -- the default operator class for integer\n\n);\n\nIf you would like to use an index of a different type or achieve some custom behav- ior, you have to specify the desired access method or operator class explicitly.\n\nEach operator class defined for a particular access method and data type must contain a set of operators that take parameters of this type and implement the semantics of this access method.\n\nFor example, the btree access method defines five mandatory comparison opera- tors. Any btree operator class must contain all the five:\n\n=> SELECT opcname, amopstrategy, amopopr::regoperator FROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_opclass opc ON opcfamily = opf.oid JOIN pg_amop amop ON amopfamily = opcfamily\n\nWHERE amname = 'btree'\n\nAND opcname IN ('text_ops', 'text_pattern_ops') AND amoplefttype = 'text'::regtype AND amoprighttype = 'text'::regtype\n\nORDER BY opcname, amopstrategy;\n\n360\n\n19.2 Operator Classes and Families\n\n| amopstrategy | −−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−\n\nopcname\n\namopopr\n\n| text_ops | text_ops | text_ops | text_ops text_ops | text_pattern_ops | text_pattern_ops | text_pattern_ops | text_pattern_ops | text_pattern_ops |\n\n1 | <(text,text) 2 | <=(text,text) 3 | =(text,text) 4 | >=(text,text) 5 | >(text,text) 1 | ~<~(text,text) 2 | ~<=~(text,text) 3 | =(text,text) 4 | ~>=~(text,text) 5 | ~>~(text,text)\n\n(10 rows)\n\nThe semantics of an operator implied by the access method is reflected by the strat- egy number shown as amopstrategy.1 For example, strategy � for btree means less than, � denotes less than or equal to, and so on. Operators themselves can have arbitrary names.\n\nThe example above shows two kinds of operators. The difference between regular operators and those with a tilde is that the latter do not take collation2 into account and perform bitwise comparison of strings. Nevertheless, both flavors implement the same logical operations of comparison.\n\nThe text_pattern_ops operator class is designed to address the limitation in support ofthe~~operator(whichcorrespondstothe����operator). Inadatabaseusingany collation other than C, this operator cannot use a regular index on a text field:\n\n=> SHOW lc_collate;\n\nlc_collate −−−−−−−−−−−−− en_US.UTF−8\n\n(1 row)\n\n=> CREATE INDEX ON tickets(passenger_name);\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE passenger_name LIKE 'ELENA%';\n\n1 postgresql.org/docs/14/xindex#XINDEX-STRATEGIES.html 2 postgresql.org/docs/14/collation.html\n\npostgresql.org/docs/14/indexes-collations.html\n\n361\n\nChapter 19 Index Access Methods\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on tickets\n\nFilter: (passenger_name ~~ 'ELENA%'::text)\n\n(2 rows)\n\nAn index with the text_pattern_ops operator class behaves differently:\n\n=> CREATE INDEX tickets_passenger_name_pattern_idx ON tickets(passenger_name text_pattern_ops);\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE passenger_name LIKE 'ELENA%';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on tickets\n\nFilter: (passenger_name ~~ 'ELENA%'::text) −> Bitmap Index Scan on tickets_passenger_name_pattern_idx\n\nIndex Cond: ((passenger_name ~>=~ 'ELENA'::text) AND (passenger_name ~<~ 'ELENB'::text))\n\n(5 rows)\n\nNotehowthefilterexpressionhaschangedintheIndexCondcondition. Thesearch now uses only the template’s prefix before %, while false-positive hits are filtered out during a recheck based on the Filter condition. The operator class for the btree access method does not provide an operator for comparing templates,and the only way to apply a �-tree here is to rewrite this condition using comparison opera- tors. The operators of the text_pattern_ops class do not take collation into account, which gives us an opportunity to use an equivalent condition instead.1\n\nAn index can be used to speed up access by a filter condition if the following two prerequisites are met:\n\n� the condition is written as “indexed-column operator expression” (if the oper- ator has a commuting counterpart specified,2 the condition can also have the form of “expression operator indexed-column”)3\n\n� and the operator belongs to the operator class specified for the indexed-column\n\nin the index declaration.\n\n1 backend/utils/adt/like_support.c 2 postgresql.org/docs/14/xoper-optimization#id-1.8.3.18.6.html 3 backend/optimizer/path/indxpath.c, match_clause_to_indexcol function\n\n362\n\n19.2 Operator Classes and Families\n\nFor example, the following query can use an index:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE 'ELENA BELOVA' = passenger_name;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using tickets_passenger_name_idx on tickets Index Cond: (passenger_name = 'ELENA BELOVA'::text)\n\n(2 rows)\n\nNote the position of arguments in the IndexCond condition: at the execution stage, the indexed field must be on the left. When the arguments are permuted,the oper- ator is replaced by a commuting one; in this particular case,it is the same operator because the equality relation is commutative.\n\nIn the next query, it is technically impossible to use a regular index because the column name in the condition is replaced by a function call:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE initcap(passenger_name) = 'Elena Belova';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on tickets\n\nFilter: (initcap(passenger_name) = 'Elena Belova'::text)\n\n(2 rows)\n\nHere you can use an expression index,1 which has an arbitrary expression specified in its declaration instead of a column:\n\n=> CREATE INDEX ON tickets( (initcap(passenger_name)) );\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE initcap(passenger_name) = 'Elena Belova';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on tickets\n\nRecheck Cond: (initcap(passenger_name) = 'Elena Belova'::text) −> Bitmap Index Scan on tickets_initcap_idx\n\nIndex Cond: (initcap(passenger_name) = 'Elena Belova'::text)\n\n(4 rows)\n\nAn index expression can depend only on heap tuple values and must be affected by neither other data stored in the database nor configuration parameters (such\n\n1 postgresql.org/docs/14/indexes-expressional.html\n\n363\n\nChapter 19 Index Access Methods\n\nas locale settings). In other words, if the expression contains any function calls, these functions must be ���������,1 and they must observe this volatility category. Otherwise,an index scan and a heap scan may return different results for the same query.\n\nApart from regular operators, an operator class can provide support functions2 re- quired by the access method. For example, the btree access method defines five support functions;3 the first one (which compares two values) is mandatory, while all the rest can be absent:\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_opclass opc ON opcfamily = opf.oid JOIN pg_amproc amproc ON amprocfamily = opcfamily\n\nWHERE amname = 'btree'\n\nAND opcname = 'text_ops' AND amproclefttype = 'text'::regtype AND amprocrighttype = 'text'::regtype\n\nORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−− 1 | bttextcmp 2 | bttextsortsupport 4 | btvarstrequalimage\n\n(3 rows)\n\nOperator Families\n\nEach operator class always belongs to some operator family4 (listed in the system catalog in the pg_opfamily table). Afamily can comprise several classes that handle similar data types in the same way.\n\nFor example, the integer_ops family includes several classes for integral data types that have the same semantics but differ in size:\n\n1 postgresql.org/docs/14/xfunc-volatility.html 2 postgresql.org/docs/14/xindex#XINDEX-SUPPORT.html 3 postgresql.org/docs/14/btree-support-funcs.html 4 postgresql.org/docs/14/xindex#XINDEX-OPFAMILY.html\n\n364",
      "page_number": 337
    },
    {
      "number": 19,
      "title": "Index Access Methods",
      "start_page": 359,
      "end_page": 376,
      "detection_method": "regex_chapter",
      "content": "19.2 Operator Classes and Families\n\n=> SELECT opcname, opcintype::regtype FROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_opclass opc ON opcfamily = opf.oid\n\nWHERE amname = 'btree'\n\nAND opfname = 'integer_ops';\n\nopcname\n\n| opcintype\n\n−−−−−−−−−−+−−−−−−−−−−− int2_ops | smallint int4_ops | integer int8_ops | bigint\n\n(3 rows)\n\nThe datetime_ops family comprises operator classes that process dates:\n\n=> SELECT opcname, opcintype::regtype FROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_opclass opc ON opcfamily = opf.oid\n\nWHERE amname = 'btree'\n\nAND opfname = 'datetime_ops';\n\nopcname\n\n|\n\nopcintype\n\n−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\ndate_ops timestamptz_ops | timestamp with time zone timestamp_ops\n\n| date\n\n| timestamp without time zone\n\n(3 rows)\n\nWhile each operator class supports a single data type, a family can comprise oper- ator classes for different data types:\n\n=> SELECT opcname, amopopr::regoperator FROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_opclass opc ON opcfamily = opf.oid JOIN pg_amop amop ON amopfamily = opcfamily\n\nWHERE amname = 'btree'\n\nAND opfname = 'integer_ops' AND amoplefttype = 'integer'::regtype AND amopstrategy = 1\n\nORDER BY opcname;\n\n365\n\nv. �.�\n\nChapter 19 Index Access Methods\n\nopcname\n\n|\n\namopopr\n\n−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−\n\nint2_ops | <(integer,bigint) int2_ops | <(integer,smallint) int2_ops | <(integer,integer) int4_ops | <(integer,bigint) int4_ops | <(integer,smallint) int4_ops | <(integer,integer) int8_ops | <(integer,bigint) int8_ops | <(integer,smallint) int8_ops | <(integer,integer)\n\n(9 rows)\n\nThanks to such grouping of various operators into a single family, the planner can do without type casting when an index is used for conditions involving values of different types.\n\n19.3 Indexing Engine Interface\n\nJust like tains the name of the function that implements the interface:1\n\nfor table access methods, the amhandler column of the pg_am table con-\n\n=> SELECT amname, amhandler FROM pg_am WHERE amtype = 'i';\n\namname |\n\namhandler\n\n−−−−−−−−+−−−−−−−−−−−−−\n\n| bthandler btree | hashhandler hash | gisthandler gist gin | ginhandler spgist | spghandler brin\n\n| brinhandler\n\n(6 rows)\n\nThisfunctionfillsplaceholdersintheinterfacestructure2 withactualvalues. Some of them are functions responsible for separate tasks related to index access (for example, they can perform an index scan and return heap tuple ��s), while others are index method properties that the indexing engine must be aware of.\n\n1 postgresql.org/docs/14/indexam.html 2 include/access/amapi.h\n\n366\n\n19.3 Indexing Engine Interface\n\nAll properties are grouped into three categories:1\n\naccess method properties\n\nproperties of a particular index\n\ncolumn-level properties of an index\n\nThe distinction between access method and index-level properties is provided with a view to the future: right now,all the indexes based on a particular access method always have the same properties at these two levels.\n\nAccess Method Properties\n\nThe following five properties �-tree method here):\n\nare defined at the access method level (shown for the\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'btree';\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | t | can_order | t | can_unique | can_multi_col | t | t | can_exclude | t | can_include\n\nbtree btree btree btree btree (5 rows)\n\nC�� O���� Theabilitytoreceivesorteddata.2 Thispropertyiscurrentlysupported\n\nonly by �-trees.\n\nTo get the results in the required order,you can always scan the table and then sort the fetched data:\n\n1 backend/utils/adt/amutils.c, indexam_property function 2 postgresql.org/docs/14/indexes-ordering.html\n\n367\n\nv. ��\n\nChapter 19 Index Access Methods\n\n=> EXPLAIN (costs off) SELECT * FROM seats ORDER BY seat_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\nSort Key: seat_no −> Seq Scan on seats\n\n(3 rows)\n\nBut if there is an index that supports this property, the data can be returned in the desired order at once:\n\n=> EXPLAIN (costs off) SELECT * FROM seats ORDER BY aircraft_code;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using seats_pkey on seats\n\n(1 row)\n\nC�� U����� Support for unique and primary key constraints.1 This property ap-\n\nplies only to �-trees.\n\nEach time a unique or primary key constraint is declared,Postgre��� automat- ically creates a unique index to support this constraint.\n\n=> INSERT INTO bookings(book_ref, book_date, total_amount) VALUES ('000004', now(), 100.00);\n\nERROR: \"bookings_pkey\" DETAIL:\n\nduplicate key value violates unique constraint\n\nKey (book_ref)=(000004) already exists.\n\nThat said, if you simply create a unique index without explicitly declaring an integrity constraint, the effect will seem to be exactly the same: the indexed column will not allow duplicates. So what is the difference?\n\nAn integrity constraint defines the property that must never be violated,while an index is just a mechanism to guarantee it. In theory, a constraint could be imposed using other means.\n\nFor example, Postgre��� does not support global indexes for partitioned ta- bles, but nevertheless, you can create a unique constraint on such tables (if it\n\n1 postgresql.org/docs/14/indexes-unique.html\n\n368\n\n19.3 Indexing Engine Interface\n\nincludes the partition key). In this case, the global uniqueness is ensured by local unique indexes of each partition, as different partitions cannot have the same partition keys.\n\nC�� M���� C�� The ability to build a multicolumn index.1\n\nA multicolumn index can speed up search by several conditions imposed on different table columns. For example, the ticket_flights table has a composite primary key, so the corresponding index is built on more than one column:\n\n=> \\d ticket_flights_pkey\n\nIndex \"bookings.ticket_flights_pkey\"\n\nColumn\n\n|\n\nType\n\n| Key? | Definition\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−+−−−−−−−−−−−−\n\nticket_no | character(13) | yes | yes flight_id | integer\n\n| ticket_no | flight_id\n\nprimary key, btree, for table \"bookings.ticket_flights\"\n\nA flight search by a ticket number and a flight �� is performed using an index:\n\n=> EXPLAIN (costs off) SELECT * FROM ticket_flights WHERE ticket_no = '0005432001355'\n\nAND flight_id = 51618;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using ticket_flights_pkey on ticket_flights\n\nIndex Cond: ((ticket_no = '0005432001355'::bpchar) AND (flight_id = 51618))\n\n(3 rows)\n\nAs a rule, a multicolumn index can speed up search even if filter conditions involve only some of its columns. In the case of a �-tree, the search will be efficient if the filter condition spans a range of columns that appear first in the index declaration:\n\n=> EXPLAIN (costs off) SELECT * FROM ticket_flights WHERE ticket_no = '0005432001355';\n\n1 postgresql.org/docs/14/indexes-multicolumn.html\n\n369\n\nv. �� p. ���\n\nChapter 19 Index Access Methods\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using ticket_flights_pkey on ticket_flights Index Cond: (ticket_no = '0005432001355'::bpchar)\n\n(2 rows)\n\nIn all other cases (for example,if the condition includes only flights_id),search will be virtually limited to the initial columns (if the query includes the cor- responding conditions), while other conditions will only be used to filter out the returned results. Indexes of other types may behave differently though.\n\nC�� E������ Support for ������� constraints.1\n\nAn ������� constraint guarantees that a condition defined by an operator will not be satisfied for any pair of table rows. To impose this constraint, Post- gre��� automatically creates an index; there must be an operator class that contains the operator used in the constraint’s condition.\n\nIt is the intersection operator && that usually serves this purpose. For in- stance, you can use it to explicitly declare that a conference room cannot be booked twice for the same time, or that buildings on a map cannot overlap.\n\nWith the equality operator, the exclusion constraint takes on the meaning of uniqueness: the table is forbidden to have two rows with the same key val- ues. Nevertheless, it is not the same as a ������ constraint: in particular, the exclusion constraint key cannot be referred to from foreign keys, and neither can it be used in the �� �������� clause.\n\nC�� I������ Theability\n\ntoaddnon-keycolumnstoanindex,whichmakethisindex\n\ncovering.\n\nUsing this property, you can extend a unique index with additional columns. Such an index still guarantees that all the key column values are unique,while data retrieval from the included columns incurs no heap access:\n\n=> CREATE UNIQUE INDEX ON flights(flight_id) INCLUDE (status);\n\n=> EXPLAIN (costs off) SELECT status FROM flights WHERE flight_id = 51618;\n\n1 postgresql.org/docs/14/ddl-constraints#DDL-CONSTRAINTS-EXCLUSION.html\n\n370\n\n19.3 Indexing Engine Interface\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Only Scan using flights_flight_id_status_idx on flights\n\nIndex Cond: (flight_id = 51618)\n\n(2 rows)\n\nIndex-Level Properties\n\nHere are the properties related to an index (shown for an existing one):\n\n=> FROM unnest(array[\n\nSELECT p.name, pg_index_has_property('seats_pkey', p.name)\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| t clusterable | t index_scan bitmap_scan | t backward_scan | t\n\n(4 rows)\n\nC���������� The ability to physically move heap tuples in accordance with the or-\n\nder in which their ��s are returned by an index scan.\n\nThis property shows whether the ������� command is supported.\n\nI���� S��� Index scan support.\n\nThis property implies that the access method can return ���s one by one. Strange as it may seem, some indexes do not provide this functionality.\n\nB����� S��� Bitmap scan support.\n\nThis property defines whether the access method can build and return a bitmap of all ���s at once.\n\nB������� S��� The ability to return results in reverse order as compared to the\n\none specified at index creation.\n\nThis property makes sense only if the access method supports index scans.\n\n371\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\nChapter 19 Index Access Methods\n\nColumn-Level Properties\n\nAnd finally, let’s take a look at the column properties:\n\n=> SELECT p.name,\n\npg_index_column_has_property('seats_pkey', 1, p.name)\n\nFROM unnest(array[\n\n'asc', 'desc', 'nulls_first', 'nulls_last', 'orderable', 'distance_orderable', 'returnable', 'search_array', 'search_nulls'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n| t asc | f desc | f nulls_first | t nulls_last | t orderable distance_orderable | f | t returnable | t search_array | t search_nulls\n\n(9 rows)\n\nA��, D���, N���� F����, N���� L��� Ordering column values.\n\nThese properties define whether column values should be stored in ascending or descending order, and whether ���� values should appear before or after regular values. All these properties are applicable only to �-trees.\n\nO�������� The ability to sort column values using the ����� �� clause.\n\nThis property is applicable only to �-trees.\n\nD������� O�������� Support for ordering operators.1\n\nUnlike regular indexing operators that return logical values, ordering opera- tors return a real number that denotes the “distance” from one argument to another. Indexes support such operators specified in the ����� �� clause of a query.\n\nFor example, the ordering operator <-> can find the airports located at the shortest distance to the specified point:\n\n1 postgresql.org/docs/14/xindex#XINDEX-ORDERING-OPS.html\n\n372\n\n19.3 Indexing Engine Interface\n\n=> CREATE INDEX ON airports_data USING gist(coordinates);\n\n=> EXPLAIN (costs off) SELECT * FROM airports ORDER BY coordinates <-> point (43.578,57.593) LIMIT 3;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nLimit\n\n−> Index Scan using airports_data_coordinates_idx on airpo... Order By: (coordinates <−> '(43.578,57.593)'::point)\n\n(3 rows)\n\nR��������� The ability to return data without accessing the table (index-only scan\n\nsupport).\n\nThis property defines whether an index structure allows retrieving indexed values. It is not always possible: for example, some indexes may store hash codes rather than actual values. In this case,the C�� I������ property will not be available either.\n\nS����� A���� Support for searching several elements in an array.\n\nAn explicit use of arrays is not the only case when it might be necessary. For example, the planner transforms the �� (list) expression into an array scan:\n\n=> EXPLAIN (costs off) SELECT * FROM bookings WHERE book_ref IN ('C7C821', 'A5D060', 'DDE1BB');\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using bookings_pkey on bookings\n\nIndex Cond: (book_ref = ANY ('{C7C821,A5D060,DDE1BB}'::bpchar[]))\n\n(3 rows)\n\nIftheindexmethoddoesnotsupportsuchoperators,theexecutormayhaveto perform several iterations to find particular values (which can make the index scan less efficient).\n\nS����� N���� Search for �� ���� and �� ��� ���� conditions.\n\nShould we index ���� values? On the one hand, it allows us to perform index scans for conditions like �� [���] ����, as well as use the index as a covering\n\n373\n\np. ���\n\nChapter 19 Index Access Methods\n\none if no filter conditions are provided (in this case, the index has to return the data of all the heap tuples, including those that contain ���� values). But on the other hand, skipping ���� values can reduce the index size.\n\nThe decision remains at the discretion of access method developers, but more often than not ���� values do get indexed.\n\nIf you do not need ���� values in an index, you can exclude them by building a partial index1 that covers only those rows that are required. For example:\n\n=> CREATE INDEX ON flights(actual_arrival) WHERE actual_arrival IS NOT NULL;\n\n=> EXPLAIN (costs off) SELECT * FROM flights WHERE actual_arrival = '2017-06-13 10:33:00+03';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using flights_actual_arrival_idx on flights\n\nIndex Cond: (actual_arrival = '2017−06−13 10:33:00+03'::ti...\n\n(2 rows)\n\nA partial index is smaller than the full one, and it gets updated only if the modified row is indexed, which can sometimes lead to tangible performance gains. Obviously, apart from ���� checks, the ����� clause can provide any condition (that can be used with immutable functions).\n\nThe ability to build partial indexes is provided by the indexing engine, so it does not depend on the access method.\n\nNaturally,the interface includes only those properties of index methods that must be known in advance for a correct decision to be taken. For example, it does not list any properties that enable such features as support for predicate locks or non- blocking index creation (������������). Such properties are defined in the code of the functions that implement the interface.\n\n1 postgresql.org/docs/14/indexes-partial.html\n\n374\n\n20\n\nIndex Scans\n\n20.1 Regular Index Scans\n\nThere are two basic ways of accessing ���s provided by an index. The first one is to perform an index scan. Most of the index access methods (but not all of them) have the I���� S���\n\nproperty to support this operation.\n\nIndex scans are represented in the plan by the Index Scan1 node:\n\n=> EXPLAIN SELECT * FROM bookings WHERE book_ref = '9AC0C6' AND total_amount = 48500.00;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using bookings_pkey on bookings\n\n(cost=0.43..8.45 rows=1 width=21) Index Cond: (book_ref = '9AC0C6'::bpchar) Filter: (total_amount = 48500.00)\n\n(4 rows)\n\nDuring an index scan, the access method returns ���s one by one.2 Upon receiv- ing a ���, the indexing engine accesses the heap page this ��� refers to, gets the corresponding tuple, and, if the visibility rules are met, returns the requested set of fields of this tuple. This process continues until the access method runs out of ���s that matches the query.\n\nThe Index Cond line includes only those filter conditions that can be checked using an index. Other conditions that have to be rechecked against the heap are listed separately in the Filter line.\n\n1 backend/executor/nodeIndexscan.c 2 backend/access/index/indexam.c, index_getnext_tid function\n\n375\n\np. ���\n\np. ���\n\nChapter 20 Index Scans\n\nAs this example shows, both index and heap access operations are handled by a common Index Scan node rather by two different ones. But there is also a sepa- rate Tid Scan node,1 which fetches tuples from the heap if their ��s are known in advance:\n\n=> EXPLAIN SELECT * FROM bookings WHERE ctid = '(0,1)'::tid;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nTid Scan on bookings\n\n(cost=0.00..4.01 rows=1 width=21)\n\nTID Cond: (ctid = '(0,1)'::tid)\n\n(2 rows)\n\nCost Estimation\n\nCost estimation of an index scan comprises the estimated costs of index access operations and heap page reads.\n\nObviously,the index-related part of the estimation fully depends on the particular access method. For �-trees, the cost is mostly incurred by fetching index pages and processing their entries. The number of pages and rows to be read can be . determined by the total volume of data and the selectivity of the applied filters Index pages are accessed at random (pages that follow each other in the logical structure are physically scattered on disk). The estimation is further increased by ��� resources spent on getting from the root to the leaf node and computing all the required expressions.2\n\nThe heap-related part of the estimation includes the cost of heap page access and the ��� time required to process all the fetched tuples. It is important to note that �/� estimation depends on both the index scan selectivity and the correlation between the physical order of tuples on disk and the order in which the access method returns their ��s.\n\n1 backend/executor/nodeTidscan.c 2 backend/utils/adt/selfuncs.c, btcostestimate function postgresql.org/docs/14/index-cost-estimation.html\n\n376\n\n20.1 Regular Index Scans\n\nGood Scenario: High Correlation\n\nIfthephysicalorderoftuplesondiskhasaperfectcorrelationwiththelogicalorder of ���s in the index, each page will be accessed only once: the Index Scan node will sequentially go from one page to another, reading the tuples one by one.\n\nheap page\n\na tuple matching filter conditions\n\nPostgre��� collects statistics on correlation\n\n:\n\n=> SELECT attname, correlation FROM pg_stats WHERE tablename = 'bookings' ORDER BY abs(correlation) DESC;\n\nattname\n\n| correlation\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−− book_ref 1 | total_amount | 0.0026738467 8.02188e−05 | book_date\n\n(3 rows)\n\nThe correlation is high if the corresponding absolute value is close to one (like in the case of book_ref); values that are close to zero are a sign of chaotic data distribution.\n\nIn this particular case,high correlation in the book_ref column is of course due to the fact that the data has been loaded into the table in ascending order based on this column, and there have been no updates yet. We would see the same picture if we executed the ������� command for the index created on this column.\n\n377\n\np. ���\n\np. ���\n\np. ���\n\n0.005\n\n0.0025\n\nChapter 20 Index Scans\n\nHowever, the perfect correlation does not guarantee that all queries will be returning re- sults in ascending order of book_ref values. First of all,anyrow update moves the resulting tuple to the end of the table. Second,the plan that relies on an index scan based on some other column returns the results in a different order. And even a sequential scan may not start at the beginning of the table . So if you need a particular order,you should explicitly define it in the ����� ��clause.\n\nHere is an example of an index scan that processes a large number of rows:\n\n=> EXPLAIN SELECT * FROM bookings WHERE book_ref < '100000';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using bookings_pkey on bookings\n\n(cost=0.43..4638.91 rows=132999 width=21) Index Cond: (book_ref < '100000'::bpchar)\n\n(3 rows)\n\nThe condition’s selectivity is estimated as follows:\n\n=> SELECT round(132999::numeric/reltuples::numeric, 4) FROM pg_class WHERE relname = 'bookings';\n\nround −−−−−−−− 0.0630 (1 row)\n\nto 1 16\n\nThisvalueisclose range from ������ to ������.\n\n,whichwecouldhaveguessedknowingthatbook_refvalues\n\nFor �-trees, the index-related part of the �/� cost estimation includes the cost of reading all the required pages. Index entries that satisfy any condition supported by �-trees are stored in pages bound into an ordered list, so the number of index pages to be read is estimated at the index size multiplied by the selectivity. But since these pages are not physically ordered,reading happens in a random fashion.\n\nC��resourcesarespentonprocessingalltheindexentriesthatareread(thecostof cpu_index_tuple_cost value) and com- processing a single entry is estimated at the puting the condition for each of these entries (in this case, the condition contains a single operator; its cost is estimated at the\n\ncpu_operator_cost value).\n\n378\n\n20.1 Regular Index Scans\n\nTable access is regarded as sequential reading of the required number of pages. In the case of a perfect correlation, heap tuples will follow each other on disk, so the number of pages is estimated at the size of the table multiplied by the selectivity.\n\nThe �/� cost is further extended by the expenses incurred by tuple processing; they are estimated at the\n\ncpu_tuple_cost value per tuple.\n\n=> WITH costs(idx_cost, tbl_cost) AS (\n\nSELECT (\n\nSELECT round(\n\ncurrent_setting('random_page_cost')::real * pages + current_setting('cpu_index_tuple_cost')::real * tuples + current_setting('cpu_operator_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0630 AS pages, reltuples * 0.0630 AS tuples FROM pg_class WHERE relname = 'bookings_pkey'\n\n) c\n\n), (\n\nSELECT round(\n\ncurrent_setting('seq_page_cost')::real * pages + current_setting('cpu_tuple_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0630 AS pages, reltuples * 0.0630 AS tuples FROM pg_class WHERE relname = 'bookings'\n\n) c\n\n)\n\n) SELECT idx_cost, tbl_cost, idx_cost + tbl_cost AS total FROM costs;\n\nidx_cost | tbl_cost | total −−−−−−−−−−+−−−−−−−−−−+−−−−−−−\n\n2457 |\n\n2177 |\n\n4634\n\n(1 row)\n\nThese calculations illustrate the logic behind the cost estimation, so the result is aligned with the estimation provided by the planner, even if it is approximated. Getting the exact value would require taking other details into account, which we are not going to discuss here.\n\n379\n\n0.01\n\nChapter 20 Index Scans\n\nBad Scenario: Low Correlation\n\nEverything changes if the correlation is low. Let’s create an index on the book_date column, which has almost zero correlation with this index, and then take a look at the query that selects almost the same fraction of rows as in the previous example. Index access turns out to be so expensive that the planner chooses it only if all the other alternatives are explicitly forbidden:\n\n=> CREATE INDEX ON bookings(book_date);\n\n=> SET enable_seqscan = off;\n\n=> SET enable_bitmapscan = off;\n\n=> EXPLAIN SELECT * FROM bookings WHERE book_date < '2016-08-23 12:00:00+03';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using bookings_book_date_idx on bookings\n\n(cost=0.43..56957.48 rows=132403 width=21) Index Cond: (book_date < '2016−08−23 12:00:00+03'::timestamp w...\n\n(3 rows)\n\nThethingisthatlowcorrelationincreasesthechancesofthenexttuplereturnedby the access method to be located in a different page. Therefore,the Index Scan node has to hop between pages instead of reading them sequentially; in the worst-case scenario, the number of page accesses can reach the number of fetched tuples.\n\nHowever, we cannot simply replace seq_page_cost with random_page_cost and rel- pages with reltuples in the good-scenario calculations. The cost that we see in the plan is much lower than the value we would have estimated this way:\n\n380\n\n20.1 Regular Index Scans\n\n=> WITH costs(idx_cost, tbl_cost) AS (\n\nSELECT\n\n( SELECT round(\n\ncurrent_setting('random_page_cost')::real * pages + current_setting('cpu_index_tuple_cost')::real * tuples + current_setting('cpu_operator_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0630 AS pages, reltuples * 0.0630 AS tuples FROM pg_class WHERE relname = 'bookings_pkey'\n\n) c\n\n), ( SELECT round(\n\ncurrent_setting('random_page_cost')::real * tuples + current_setting('cpu_tuple_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0630 AS pages, reltuples * 0.0630 AS tuples FROM pg_class WHERE relname = 'bookings'\n\n) c\n\n)\n\n) SELECT idx_cost, tbl_cost, idx_cost + tbl_cost AS total FROM costs;\n\nidx_cost | tbl_cost | total −−−−−−−−−−+−−−−−−−−−−+−−−−−−−−\n\n2457 |\n\n533330 | 535787\n\n(1 row)\n\nThe reason is that the model takes caching into account. Frequently used pages are kept in the buffer cache (and in the �� cache), so the bigger the cache size, the more chances to find the required page in it, thus avoiding an extra disk access op- effective_cache_size eration. For planning purposes,the cache size is defined by the parameter. The smaller its value, the more pages are expected to be read.\n\nThe graph that follows shows the dependency between the estimation of the num- ber of pages to be read and the table size (for the selectivity of 1 and the page 2 containing 10 rows).1 The dashed lines show the access count in the best scenario possible (half of the page count if the correlation is perfect) and in the worst sce- nario (half of the row count if there is zero correlation and no cache).\n\n1 backend/optimizer/path/costsize.c, index_pages_fetched function\n\n381\n\n4GB\n\nChapter 20 Index Scans\n\npage access count\n\nofrowcount\n\n0.5\n\n0.5 sel=\n\npagecount\n\n0.5ofpagecount\n\neffective_cache_size\n\ntablesize\n\nIt is assumed that the effective_cache_size value indicates the total volume of mem- ory that can be used for caching (including both the Postgre��� buffer cache and �� cache). But since this parameter is used solely for estimation purposes and does not affect memory allocation itself, you do not have to take actual figures into ac- count when changing this setting.\n\nIf you reduce effective_cache_size to the minimum,the plan estimation will be close to the low-end value shown above for the no-caching case:\n\n=> SET effective_cache_size = '8kB';\n\n=> EXPLAIN SELECT * FROM bookings WHERE book_date < '2016-08-23 12:00:00+03';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using bookings_book_date_idx on bookings\n\n(cost=0.43..532745.48 rows=132403 width=21) Index Cond: (book_date < '2016−08−23 12:00:00+03'::timestamp w...\n\n(3 rows)\n\n382",
      "page_number": 359
    },
    {
      "number": 20,
      "title": "Index Scans",
      "start_page": 377,
      "end_page": 400,
      "detection_method": "regex_chapter",
      "content": "20.2 Index-Only Scans\n\n=> RESET effective_cache_size;\n\n=> RESET enable_seqscan;\n\n=> RESET enable_bitmapscan;\n\nThe planner calculates the table �/� cost for both worst-case and best-case scenar- ios and then takes an intermediate value based on the actual correlation.1\n\nThus,an index scan can be a good choice if only a fraction of rows has to be read. If heap tuples are correlated with the order in which the access method returns their ��s, this fraction can be quite substantial. However, if the correlation is low, index scanning becomes much less attractive for queries with low selectivity.\n\n20.2 Index-Only Scans\n\nIf an index contains all the heap data required by the query, it is called a covering index for this particular query. If such an index is available, extra table access can be avoided: instead of ���s, the access method can return the actual data directly. Such a type of an index scan is called an index-only scan.2 It can be used by those access methods that support the R���������\n\nproperty.\n\nIn the plan, this operation is represented by the Index Only Scan3 node:\n\n=> EXPLAIN SELECT book_ref FROM bookings WHERE book_ref < '100000';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Only Scan using bookings_pkey on bookings\n\n(cost=0.43..3791.91 rows=132999 width=7) Index Cond: (book_ref < '100000'::bpchar)\n\n(3 rows)\n\nThe name suggests that this node never has to access the heap, but it is not so. , so the access In Postgre���, indexes contain no information on tuple visibility method returns the data of all the heap tuples that satisfy the filter condition,even\n\n1 backend/optimizer/path/costsize.c, cost_index function 2 postgresql.org/docs/14/indexes-index-only-scans.html 3 backend/executor/nodeIndexonlyscan.c\n\n383\n\np. ���\n\np. ��\n\np. ��\n\nChapter 20 Index Scans\n\nif the current transaction cannot see them. Their visibility is then checked by the indexing engine.\n\nHowever, if this method had to access the table to check visibility of each tuple, it would not be any different from a regular index scan. Instead, it employs the visibility map provided for tables, in which the vacuum process marks the pages that contain only all-visible tuples (that is, those tuples that are accessible to all transactions, regardless of the snapshot used). If the ��� returned by the index access method belongs to such a page, there is no need to check its visibility.\n\nThe cost estimation of an index-only scan depends on the fraction of all-visible pages in the heap. Postgre��� collects such statistics:\n\n=> SELECT relpages, relallvisible FROM pg_class WHERE relname = 'bookings';\n\nrelpages | relallvisible −−−−−−−−−−+−−−−−−−−−−−−−−−\n\n13447 |\n\n13446\n\n(1 row)\n\nThe cost estimation of an index-only scan differs from that of a regular index scan: its �/� cost related to table access is taken in proportion to the fraction of pages that do not appear in the visibility map. (The cost estimation of tuple processing is the same.)\n\nSince in this particular example all pages contain only all-visible tuples, the cost of heap �/� is in fact excluded from the cost estimation:\n\n=> WITH costs(idx_cost, tbl_cost) AS (\n\nSELECT (\n\nSELECT round(\n\ncurrent_setting('random_page_cost')::real * pages + current_setting('cpu_index_tuple_cost')::real * tuples + current_setting('cpu_operator_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0630 AS pages, reltuples * 0.0630 AS tuples\n\nFROM pg_class WHERE relname = 'bookings_pkey'\n\n) c\n\n) AS idx_cost,\n\n384\n\n20.2 Index-Only Scans\n\n(\n\nSELECT round(\n\n(1 - frac_visible) * -- fraction of non-all-visible pages current_setting('seq_page_cost')::real * pages + current_setting('cpu_tuple_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0630 AS pages,\n\nreltuples * 0.0630 AS tuples,\n\nrelallvisible::real/relpages::real AS frac_visible\n\nFROM pg_class WHERE relname = 'bookings'\n\n) c\n\n) AS tbl_cost\n\n) SELECT idx_cost, tbl_cost, idx_cost + tbl_cost AS total FROM costs;\n\nidx_cost | tbl_cost | total −−−−−−−−−−+−−−−−−−−−−+−−−−−−−\n\n2457 |\n\n1330 |\n\n3787\n\n(1 row)\n\nAny unvacuumed changes that have not disappeared behind the database horizon p. ��� yet increase the estimated cost of the plan (and, consequently, make this plan less attractive to the optimizer). The ������� ������� command can show the actual heap access count.\n\nIn a newly created table, Postgre��� has to check visibility of all the tuples:\n\n=> CREATE TEMP TABLE bookings_tmp WITH (autovacuum_enabled = off) AS\n\nSELECT * FROM bookings ORDER BY book_ref;\n\n=> ALTER TABLE bookings_tmp ADD PRIMARY KEY(book_ref);\n\n=> ANALYZE bookings_tmp;\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT book_ref FROM bookings_tmp WHERE book_ref < '100000';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Only Scan using bookings_tmp_pkey on bookings_tmp\n\n(cost=0.43..4638.91 rows=132999 width=7) (actual rows=132109 l... Index Cond: (book_ref < '100000'::bpchar) Heap Fetches: 132109\n\n(4 rows)\n\n385\n\nv. ��\n\nChapter 20 Index Scans\n\nBut once the table has been vacuumed,such a check becomes redundant and is not performed as long as all the pages remain all-visible.\n\n=> VACUUM bookings_tmp;\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT book_ref FROM bookings_tmp WHERE book_ref < '100000';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Only Scan using bookings_tmp_pkey on bookings_tmp\n\n(cost=0.43..3787.91 rows=132999 width=7) (actual rows=132109 l... Index Cond: (book_ref < '100000'::bpchar) Heap Fetches: 0\n\n(4 rows)\n\nIndexes with the Include Clause\n\nIt is not always possible to extend an index with all the columns required by a query:\n\nFor a unique index, adding a new column would compromise the uniqueness\n\nof the original key columns.\n\nThe index access method may not provide an operator class for the data type\n\nof the column to be added.\n\nyou can still include columns into an index without making them a In this case, part of the index key. It will of course be impossible to perform an index scan based on the included columns, but if a query references these columns, the index will function as a covering one.\n\nThe following example shows how to replace an automatically created primary key index by another index with an included column:\n\n=> CREATE UNIQUE INDEX ON bookings(book_ref) INCLUDE (book_date);\n\n=> BEGIN;\n\n=> ALTER TABLE bookings\n\nDROP CONSTRAINT bookings_pkey CASCADE;\n\n386\n\n20.3 Bitmap Scans\n\nNOTICE: tickets ALTER TABLE\n\ndrop cascades to constraint tickets_book_ref_fkey on table\n\n=> ALTER TABLE bookings ADD CONSTRAINT bookings_pkey PRIMARY KEY USING INDEX bookings_book_ref_book_date_idx; -- a new index\n\nNOTICE: \"bookings_book_ref_book_date_idx\" to \"bookings_pkey\" ALTER TABLE\n\nALTER TABLE / ADD CONSTRAINT USING INDEX will rename index\n\n=> ALTER TABLE tickets\n\nADD FOREIGN KEY (book_ref) REFERENCES bookings(book_ref);\n\n=> COMMIT;\n\n=> EXPLAIN SELECT book_ref, book_date FROM bookings WHERE book_ref < '100000';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Only Scan using bookings_pkey on bookings\n\n(cost=0.43..437...\n\nIndex Cond: (book_ref < '100000'::bpchar)\n\n(2 rows)\n\nSuch indexes are often called covering, but it is not quite correct. An index is considered covering if the set of its columns covers all the columns required by a particular query. It does not matter whether it involves any columns added by the ������� clause,or only key columns are being used. Moreover,one and the same index can be covering for one query but not for the other.\n\n20.3 Bitmap Scans\n\nThe efficiency of an index scan is limited: as the correlation decreases,the number of accesses to heap pages rises,and scanning becomes random rather than sequen- tial. Toovercomethis limitation,Postgre��� can fetch all the ���sbeforeaccessing the table and sort them in ascending order based on their page numbers.1 This is exactly how bitmap scanning works,which is yet another common approach to pro- cessing ���s. It can be used by those access methods that support the B����� S��� p. ��� property.\n\n1 backend/access/index/indexam.c, index_getbitmap function\n\n387\n\n1\n\nChapter 20 Index Scans\n\nUnlike a regular index scan, this operation is represented in the query plan by two nodes:\n\n=> CREATE INDEX ON bookings(total_amount);\n\n=> EXPLAIN SELECT * FROM bookings WHERE total_amount = 48500.00;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings\n\n(cost=54.63..7040.42 rows=2865 wid...\n\nRecheck Cond: (total_amount = 48500.00) −> Bitmap Index Scan on bookings_total_amount_idx\n\n(cost=0.00..53.92 rows=2865 width=0) Index Cond: (total_amount = 48500.00)\n\n(5 rows)\n\nThe Bitmap Index Scan1 node gets the bitmap of all ���s2 from the access method.\n\nThe bitmap consists of separate segments, each corresponding to a single heap page. All these segments have the same size, which is enough for all the page tuples, no matter how many of them are present. This number is limited because a tuple header is quite large; a standard-size page can accommodate ��� tuples at the most, which fit �� bytes.3\n\nThen the Bitmap Heap Scan4 traverses the bitmap segment by segment, reads the corresponding pages, and checks all their tuples that are marked all-visible. Thus, pages are read in ascending order based on their numbers,and each of them is read exactly once.\n\nThat said, this process is not the same as sequential scanning since the accessed pages rarely follow each other. Regular prefetching performed by the operating system does not help in this case, so the Bitmap Heap Scan node implements its effective_io_concurrency pages—and it own prefetching by asynchronously reading is the only node that does it. This mechanism relies on the posix_fadvise function implemented by some operating systems. If your system supports this function, it makes sense to configure the effective_io_concurrency parameter at the tablespace level in accordance with the hardware capabilities.\n\n1 backend/executor/nodeBitmapIndexscan.c 2 backend/access/index/indexam.c, index_getbitmap function 3 backend/nodes/tidbitmap.c 4 backend/executor/nodeBitmapHeapscan.c\n\n388\n\n20.3 Bitmap Scans\n\nAsynchronous prefetching is also used by some other internal processes:\n\nfor index pages when heap rows are being deleted1\n\nfor heap pages during analysis (�������)2\n\nThe prefetch depth is defined by the\n\nmaintenance_io_concurrency.\n\nBitmap Accuracy\n\nThe more pages contain the tuples that satisfy the filter condition of the query,the bigger is the bitmap. It is built in the local memory of the backend, and its size is work_mem parameter. Once the maximum allowed size is reached, limited by the some bitmap segments become lossy: each bit of a lossy segment corresponds to a whole page, while the segment itself comprises a range of pages.3 As a result, the size of the bitmap becomes smaller at the expense of its accuracy.\n\nThe ������� ������� command shows the accuracy of the built bitmap:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM bookings WHERE total_amount > 150000.00;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings (actual rows=242691 loops=1)\n\nRecheck Cond: (total_amount > 150000.00) Heap Blocks: exact=13447 −> Bitmap Index Scan on bookings_total_amount_idx (actual rows...\n\nIndex Cond: (total_amount > 150000.00)\n\n(5 rows)\n\nHere we have enough memory for an exact bitmap.\n\nIf we decrease the work_mem value, some of the bitmap segments become lossy:\n\n=> SET work_mem = '512kB';\n\n1 backend/access/heap/heapam.c, index_delete_prefetch_buffer function 2 backend/commands/analyze.c, acquire_sample_rows function 3 backend/nodes/tidbitmap.c, tbm_lossify function\n\n389\n\nv. ��\n\nv. ��\n\n10\n\n4MB\n\nChapter 20 Index Scans\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM bookings WHERE total_amount > 150000.00;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings (actual rows=242691 loops=1)\n\nRecheck Cond: (total_amount > 150000.00) Rows Removed by Index Recheck: 1145721 Heap Blocks: exact=5178 lossy=8269 −> Bitmap Index Scan on bookings_total_amount_idx (actual rows...\n\nIndex Cond: (total_amount > 150000.00)\n\n(6 rows)\n\n=> RESET work_mem;\n\nWhen reading a heap page that corresponds to a lossy bitmap segment,Postgre��� has to recheck the filter condition for each tuple in the page. The condition to be rechecked is always displayed in the plan as Recheck Cond, even if this recheck is not performed. The number of tuples filtered out during a recheck is displayed separately (as Rows Removed by Index Recheck).\n\nIf the size of the result set is too big,the bitmap may not fit the work_mem memory chunk, even if all its segments are lossy. Then this limit is ignored,and the bitmap takes as much space as required. Postgre��� neither further reduces the bitmap accuracy nor flushes any of its segments to disk.\n\nOperations on Bitmaps\n\nIf the query applies conditions to several table columns that have separate indexes createdonthem,abitmapscancanuseseveralindexestogether.1 Alltheseindexes have their own bitmaps built on the fly; the bitmaps are then combined together bit by bit,using either logical conjunction (if the expressions are connected by���) or logical disjunction (if the expressions are connected by ��). For example:\n\n=> EXPLAIN (costs off) SELECT * FROM bookings WHERE book_date < '2016-08-28' AND total_amount > 250000;\n\n1 postgresql.org/docs/14/indexes-ordering.html\n\n390\n\n20.3 Bitmap Scans\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings\n\nRecheck Cond: ((total_amount > '250000'::numeric) AND (book_da... −> BitmapAnd\n\n−> Bitmap Index Scan on bookings_total_amount_idx\n\nIndex Cond: (total_amount > '250000'::numeric)\n\n−> Bitmap Index Scan on bookings_book_date_idx\n\nIndex Cond: (book_date < '2016−08−28 00:00:00+03'::tim...\n\n(7 rows)\n\nHere the BitmapAnd node combines two bitmaps using the bitwise ��� operation.\n\nAs two bitmaps are being merged into one,1 exact segments remain exact when merged together (if the new bitmap fits the work_mem memory chunk), but if any segment in a pair is lossy, the resulting segment will be lossy too.\n\nCost Estimation\n\nLet’s take a look at the query that uses a bitmap scan:\n\n=> EXPLAIN SELECT * FROM bookings WHERE total_amount = 28000.00;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings\n\n(cost=599.48..14444.96 rows=31878 ...\n\nRecheck Cond: (total_amount = 28000.00) −> Bitmap Index Scan on bookings_total_amount_idx\n\n(cost=0.00..591.51 rows=31878 width=0) Index Cond: (total_amount = 28000.00)\n\n(5 rows)\n\nThe approximate selectivity of the condition used by the planner equals\n\n=> SELECT round(31878::numeric/reltuples::numeric, 4) FROM pg_class WHERE relname = 'bookings';\n\nround −−−−−−−− 0.0151 (1 row)\n\n1 backend/nodes/tidbitmap.c, tbm_union & tbm_intersect functions\n\n391\n\nChapter 20 Index Scans\n\nThe total cost of the Bitmap Index Scan node is estimated in the same way as the cost of a regular index scan that does not take heap access into account:\n\n=> SELECT round(\n\ncurrent_setting('random_page_cost')::real * pages + current_setting('cpu_index_tuple_cost')::real * tuples + current_setting('cpu_operator_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0151 AS pages, reltuples * 0.0151 AS tuples FROM pg_class WHERE relname = 'bookings_total_amount_idx'\n\n) c;\n\nround −−−−−−−\n\n589\n\n(1 row)\n\nThe �/� cost estimation for the Bitmap Heap Scan node differs from that for a perfect-correlation case of a regular index scan. A bitmap allows reading heap pages in ascending order based on their numbers, without getting back to one and the same page, but the tuples that satisfy the filter condition do not follow each other anymore. Instead of reading a strictly sequential page range that is quite compact, Postgre��� is likely to access far more pages.\n\nThe number of pages to be read is estimated by the following formula:1\n\nmin\n\n(\n\n2 relpages ⋅ reltuples ⋅ sel 2 relpages + reltuples ⋅ sel\n\n,relpages\n\n)\n\nThe estimated cost of reading a single page falls between seq_page_cost and ran- dom_page_cost, depending on the ratio of the fraction of fetched pages to the total number of pages in the table:\n\n1 backend/optimizer/path/costsize.c, compute_bitmap_pages function\n\n392\n\n20.3 Bitmap Scans\n\n=> WITH t AS (\n\nSELECT relpages,\n\nleast(\n\n(2 * relpages * reltuples * 0.0151) / (2 * relpages + reltuples * 0.0151), relpages\n\n) AS pages_fetched, round(reltuples * 0.0151) AS tuples_fetched, current_setting('random_page_cost')::real AS rnd_cost, current_setting('seq_page_cost')::real AS seq_cost\n\nFROM pg_class WHERE relname = 'bookings'\n\n) SELECT pages_fetched,\n\nrnd_cost - (rnd_cost - seq_cost) * sqrt(pages_fetched / relpages) AS cost_per_page, tuples_fetched\n\nFROM t;\n\npages_fetched | cost_per_page | tuples_fetched −−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n13447 |\n\n1 |\n\n31878\n\n(1 row)\n\nAs usual, the �/� estimation is increased by the cost of processing each fetched tuple. If an exact bitmap is used, the number of tuples is estimated at the total number of tuples in the table multiplied by the selectivity of filter conditions. But ifanybitmapsegmentsarelossy,Postgre���hastoaccessthecorrespondingpages to recheck all their tuples.\n\na lossybitmap segment\n\nan exact segment\n\nThus, the estimation takes into account the expected fraction of lossy bitmap seg- ments (which can be calculated based on the total number of selected rows and the bitmap size limit defined by work_mem).1\n\n1 backend/optimizer/path/costsize.c, compute_bitmap_pages function\n\n393\n\nv. ��\n\nChapter 20 Index Scans\n\nThe total cost of condition rechecks also increases the estimation (regardless of the bitmap accuracy).\n\nThe startup cost estimation of the Bitmap Heap Scan node is based on the total cost of the Bitmap Index Scan node, which is extended by the cost of bitmap processing:\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings\n\n(cost=599.48..14444.96 rows=31878 width=21) Recheck Cond: (total_amount = 28000.00) −> Bitmap Index Scan on bookings_total_amount_idx\n\n(cost=0.00..591.51 rows=31878 width=0) Index Cond: (total_amount = 28000.00)\n\n(6 rows)\n\nHere the bitmap is exact, and the cost is estimated roughly as follows:1\n\n=> WITH t AS (\n\nSELECT 1 AS cost_per_page,\n\n13447 AS pages_fetched, 31878 AS tuples_fetched\n\n), costs(startup_cost, run_cost) AS (\n\nSELECT\n\n( SELECT round(\n\n589 /* cost estimation for the child node */ + 0.1 * current_setting('cpu_operator_cost')::real * reltuples * 0.0151\n\n) FROM pg_class WHERE relname = 'bookings_total_amount_idx'\n\n), ( SELECT round(\n\ncost_per_page * pages_fetched + current_setting('cpu_tuple_cost')::real * tuples_fetched + current_setting('cpu_operator_cost')::real * tuples_fetched\n\n) FROM t\n\n)\n\n) SELECT startup_cost, run_cost,\n\nstartup_cost + run_cost AS total_cost\n\nFROM costs;\n\n1 backend/optimizer/path/costsize.c, cost_bitmap_heap_scan function\n\n394\n\n20.4 Parallel Index Scans\n\nstartup_cost | run_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−\n\n597 |\n\n13845 |\n\n14442\n\n(1 row)\n\nIf the query plan combines several bitmaps, the sum of the costs of separate index scans is increased by a (small) cost of merging them together.1\n\n20.4 Parallel Index Scans\n\nAll the index scanning modes bitmap scan—have their own flavors for parallel\n\n—a regular index scan, an index-only scan, and a\n\nplans.\n\nThe cost of parallel execution is estimated in the same way as that of sequen- tial one, but (just like in the case of a parallel sequential scan) ��� resources are distributed between all parallel processes, thus reducing the total cost. The �/� component of the cost is not distributed because processes are synchronized to perform page access sequentially.\n\nNow let me show you several examples of parallel plans without breaking down their cost estimation.\n\nA parallel index scan:\n\n=> EXPLAIN SELECT sum(total_amount) FROM bookings WHERE book_ref < '400000';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n(cost=19192.81..19192.82 rows=1 width=32)\n\n−> Gather\n\n(cost=19192.59..19192.80 rows=2 width=32)\n\nWorkers Planned: 2 −> Partial Aggregate\n\n(cost=18192.59..18192.60 rows=1 widt...\n\n−> Parallel Index Scan using bookings_pkey on bookings\n\n(cost=0.43..17642.82 rows=219907 width=6) Index Cond: (book_ref < '400000'::bpchar)\n\n(7 rows)\n\n1 backend/optimizer/path/costsize.c, cost_bitmap_and_node & cost_bitmap_or_node functions\n\n395\n\nv. �.� p. ���\n\nChapter 20 Index Scans\n\nWhile a parallel scan of a �-tree is in progress, the �� of the current index page is kept in the server’s shared memory. The initial value is set by the process that starts the scan: it traverses the tree from the root to the first suitable leaf page and saves its ��. Workers access subsequent index pages as needed,replacing the saved ��. Having fetched a page, the worker iterates through all its suitable entries and reads the corresponding heap tuples. The scanning completes when the worker has read the whole range of values that satisfy the query filter.\n\nA parallel index-only scan:\n\n=> EXPLAIN SELECT sum(total_amount) FROM bookings WHERE total_amount < 50000.00;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n(cost=23370.60..23370.61 rows=1 width=32)\n\n−> Gather\n\n(cost=23370.38..23370.59 rows=2 width=32)\n\nWorkers Planned: 2 −> Partial Aggregate\n\n(cost=22370.38..22370.39 rows=1 widt... −> Parallel Index Only Scan using bookings_total_amoun...\n\n(cost=0.43..21387.27 rows=393244 width=6) Index Cond: (total_amount < 50000.00)\n\n(7 rows)\n\nA parallel index-only scan skips heap access for all-visible pages; it is the only difference it has from a parallel index scan.\n\nA parallel bitmap scan:\n\n=> EXPLAIN SELECT sum(total_amount) FROM bookings WHERE book_date < '2016-10-01';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n(cost=21492.21..21492.22 rows=1 width=32)\n\n−> Gather\n\n(cost=21491.99..21492.20 rows=2 width=32)\n\nWorkers Planned: 2 −> Partial Aggregate\n\n(cost=20491.99..20492.00 rows=1 widt...\n\n−> Parallel Bitmap Heap Scan on bookings\n\n(cost=4891.17..20133.01 rows=143588 width=6) Recheck Cond: (book_date < '2016−10−01 00:00:00+03... −> Bitmap Index Scan on bookings_book_date_idx\n\n(cost=0.00..4805.01 rows=344611 width=0) Index Cond: (book_date < '2016−10−01 00:00:00+...\n\n(10 rows)\n\n396\n\n20.5 Comparison of Various Access Methods\n\nA bitmap scan implies that a bitmap is always built sequentially, by a single leader process; for this reason, the name of the Bitmap Index Scan node does not contain the word Parallel. When the bitmap is ready, the Parallel Bitmap Heap Scan node starts a parallel heap scan. Workers access subsequent heap pages and process them concurrently.\n\n20.5 Comparison of Various Access Methods\n\nThe following illustration shows how costs of various access methods depend on selectivity of filter conditions:\n\ncost\n\nindexscan bitmapindexscan\n\nseq scan\n\nindex-onlyscan\n\nselectivity\n\n0\n\n1\n\nIt is a qualitative diagram; the actual figures are of course dependent on the par- ticular table and server configuration.\n\nSequential scanning does not depend on selectivity, and starting from a certain fraction of selected rows, it is usually more efficient than other methods.\n\nThe cost of an index scan is affected by the correlation between the physical order of tuples and the order in which their ��s are returned by the access method. If the correlation is perfect, an index scan can be quite efficient even if the fraction\n\n397\n\nChapter 20 Index Scans\n\nof selected rows is rather high. However, for low correlation (which is much more common) it can quickly become even more expensive than a sequential scan. That said, index scanning is still an absolute leader when it comes to selecting a single row using an index (typically a unique one).\n\nIf applicable, index-only scans can show great performance and beat sequential scans even if all the rows are selected. However, their performance is highly de- pendent on the visibility map, and in the worst-case scenario an index-only scan can degrade to a regular index scan.\n\nThe cost of a bitmap scan is affected by the size of available memory,but to a much lesser extent than an index scan cost depends on correlation. If the correlation is low, the bitmap scan turns out to be much cheaper.\n\nEach access method has its own perfect usage scenarios; there is no such method that always outperforms other methods. The planner has to do extensive calcu- lations to estimate the efficiency of each method in each particular case. Clearly, the accuracy of these estimations highly depends on the accuracy of the collected statistics.\n\n398\n\n21\n\nNested Loop\n\n21.1 Join Types and Methods\n\nJoins are a key feature of the ��� language; they serve as the foundation for its power and flexibility. Sets of rows (either retrieved from tables directly or received as the result of some other operations) are always joined pairwise.\n\nThere are several types of joins:\n\nInner joins. An inner join (specified as ����� ����, or simply ����) comprises those pairs of rows of two sets that satisfy a particular join condition. The join con- dition combines some columns of one set of rows with some columns of the other set; all the columns involved constitute the join key.\n\nIf the join condition demands that join keys of two sets be equal, such a join is called an equi-join; this is the most common join type.\n\nA Cartesian product (����� ����) of two sets comprises all the possible pairs of rows of these sets—it is a special case of an inner join with a true condition.\n\nOuter joins. A left outer join (specified as ���� ����� ����, or simply ���� ����) ex- tends the result of an inner join by those rows of the left set that have no match in the right set (the corresponding right-side columns are filled with ���� values).\n\nThesameisalsotrueforarightouterjoin(���������),downtothepermutation of sets.\n\nA full outer join (specified as ���� ����) comprises left and right outer joins, adding both right-side and left-side rows for which no match has been found.\n\n399\n\nChapter 21 Nested Loop\n\nAnti-Joins and Semi-Joins. A semi-join looks a lot like an inner join, but it includes only those rows of the left set that have a match in the right set (a row is included only once even if there are several matches).\n\nAn anti-join includes those rows of a set that have no match in the other set.\n\nThe ��� language has no explicit semi- and anti-joins, but the same outcome can be achieved using predicates like ������ and ��� ������.\n\nAll these joins are logical operations. For example,an inner join is often described as a Cartesian product that has been cleared of the rows that do not satisfy the join condition. But at the physical level, an inner join is typically achieved via less expensive means.\n\nPostgre��� provides several join methods:\n\na nested loop join\n\na hash join\n\na merge join\n\nJoin methods are algorithms that implement logical operations of ��� joins. These basic algorithms often have special flavors tailored for particular join types, even though they may support only some of them. For example, a nested loop supports an inner join (represented in the plan by a Nested Loop node) and a left outer join (represented by a Nested Loop Left Join node), but it cannot be used for full joins.\n\nSome flavors of the same algorithms can also be used by other operations, such as aggregation.\n\nDifferent join methods perform best in different conditions; it is the job of the planner to choose the most cost-effective one.\n\n21.2 Nested Loop Joins\n\nThe basic algorithm of the nested loop join functions as follows. The outer loop traverses all the rows of the first set (called the outer set). For each of these rows,\n\n400\n\n21.2 Nested Loop Joins\n\nthenestedloopgoesthroughtherowsofthesecondset(calledtheinner set)tofind the ones that satisfy the join condition. Each found pair is returned immediately as part of the query result.1\n\nThe algorithm accesses the inner set as many times as there are rows in the outer set. Therefore, the efficiency of nested loop joins depends on several factors:\n\ncardinality of the outer set of rows\n\navailability of an access method that can efficiently fetch the needed rows of\n\nthe inner set\n\nrecurrent access to the same rows of the inner set\n\nCartesian Product\n\nA nested loop join is the most efficient way to find a Cartesian product, regardless of the number of rows in the sets:\n\n=> EXPLAIN SELECT * FROM aircrafts_data a1\n\nCROSS JOIN aircrafts_data a2\n\nWHERE a2.range > 5000;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n(cost=0.00..2.78 rows=45 width=144)\n\n−> Seq Scan on aircrafts_data a1\n\n(cost=0.00..1.09 rows=9 width=72)\n\nouter set\n\n−> Materialize\n\n(cost=0.00..1.14 rows=5 width=72)\n\n−> Seq Scan on aircrafts_data a2\n\n(cost=0.00..1.11 rows=5 width=72) Filter: (range > 5000)\n\ninner set\n\n(7 rows)\n\nThe Nested Loop node performs a join using the algorithm described above. It al- ways has two child nodes: the one that is displayed higher in the plan corresponds to the outer set of rows, while the lower one represents the inner set.\n\n1 backend/executor/nodeNestloop.c\n\n401\n\n4MB\n\nChapter 21 Nested Loop\n\nIn this example,the inner set is represented by the Materialize node.1 This node re- turns the rows received from its child node, having saved them for future use (the work_mem; then rows are accumulated in memory until their total size reaches Postgre��� starts spilling them into a temporary file on disk). If accessed again, the node reads the accumulated rows without calling the child node. Thus, the ex- ecutor can avoid scanning the full table again and read only those rows that satisfy the condition.\n\nA similar plan can also be built for a query that uses a regular equi-join:\n\n=> EXPLAIN SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nWHERE t.ticket_no = '0005432000284';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=0.99..25.05 rows=3 width=136)\n\nNested Loop\n\n−> Index Scan using tickets_pkey on tickets t\n\n(cost=0.43..8.45 rows=1 width=104) Index Cond: (ticket_no = '0005432000284'::bpchar)\n\n−> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) Index Cond: (ticket_no = '0005432000284'::bpchar)\n\n(7 rows)\n\nHaving recognized the equality of the two values, the planner replaces the join condition tf.ticket_no = t.ticket_no by the tf.ticket_no = constant condition, virtually reducing an equi-join to a Cartesian product.2\n\nCardinality estimation. The cardinality of a Cartesian product is estimated at the product of cardinalities of the joined data sets: 3 = 1 × 3.\n\nCost estimation. The startup cost of the join operation combines the startup costs of all child nodes.\n\n1 backend/executor/nodeMaterial.c 2 backend/optimizer/path/equivclass.c\n\n402\n\n21.2 Nested Loop Joins\n\nThe full cost of the join includes the following components:\n\nthe cost of fetching all the rows of the outer set\n\nthecostofasingleretrievalofalltherowsoftheinnerset(sincethecardinality\n\nestimation of the outer set equals one)\n\nthe cost of processing each row to be returned\n\nHere is a dependency graph for the cost estimation:\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=0.99..25.05 rows=3 width=136)\n\nNested Loop\n\n−> Index Scan using tickets_pkey on tickets t\n\n(cost=0.43..8.45 rows=1 width=104) Index Cond: (ticket_no = '0005432000284'::bpchar)\n\n× 1\n\n−> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) Index Cond: (ticket_no = '0005432000284'::bpchar)\n\n(7 rows)\n\nThe cost of the join is calculated as follows:\n\n=> SELECT 0.43 + 0.56 AS startup_cost,\n\nround((\n\n8.45 + 16.57 + 3 * current_setting('cpu_tuple_cost')::real\n\n)::numeric, 2) AS total_cost;\n\nstartup_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0.99 |\n\n25.05\n\n(1 row)\n\nNow let’s get back to the previous example:\n\n=> EXPLAIN SELECT * FROM aircrafts_data a1\n\nCROSS JOIN aircrafts_data a2\n\nWHERE a2.range > 5000;\n\n403\n\nChapter 21 Nested Loop\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n(cost=0.00..2.78 rows=45 width=144)\n\n−> Seq Scan on aircrafts_data a1\n\n(cost=0.00..1.09 rows=9 width=72)\n\n−> Materialize\n\n(cost=0.00..1.14 rows=5 width=72)\n\n−> Seq Scan on aircrafts_data a2\n\n(cost=0.00..1.11 rows=5 width=72) Filter: (range > 5000)\n\n(7 rows)\n\nThe plan now contains the Materialize node; having once accumulated the rows received from its child node, Materialize returns them much faster for all the sub- sequent calls.\n\nIn general, the total cost of a join comprises the following expenses:1\n\nthe cost of fetching all the rows of the outer set\n\nthe cost of the initial fetch of all the rows of the inner set (during which ma-\n\nterialization is performed)\n\n(N−1)-foldcostofrepeatfetchesofrowsoftheinnerset(hereN isthenumber\n\nof rows in the outer set)\n\nthe cost of processing each row to be returned\n\nThe dependency graph here is as follows:\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n(cost=0.00..2.78 rows=45 width=144)\n\n−> Seq Scan on aircrafts_data a1\n\n(cost=0.00..1.09 rows=9 width=72)\n\n−> Materialize\n\n× 9\n\n(cost=0.00..1.14 rows=5 width=72) −> Seq Scan on aircrafts_data a2\n\n(cost=0.00..1.11 rows=5 width=72) Filter: (range > 5000)\n\n(8 rows)\n\n1 backend/optimizer/path/costsize.c, initial_cost_nestloop andfinal_cost_nestloop function\n\n404\n\n21.2 Nested Loop Joins\n\nIn this example, materialization reduces the cost of repeat data fetches. The cost of the first Materialize call is shown in the plan, but all the subsequent calls are not listed. I will not provide any calculations here,1 but in this particular case the estimation is �.����.\n\nThus, the cost of the join performed in this example is calculated as follows:\n\n=> SELECT 0.00 + 0.00 AS startup_cost,\n\nround((\n\n1.09 + (1.14 + 8 * 0.0125) + 45 * current_setting('cpu_tuple_cost')::real\n\n)::numeric, 2) AS total_cost;\n\nstartup_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0.00 |\n\n2.78\n\n(1 row)\n\nParameterized Joins\n\nNow let’s consider a more common example that does not boil down to a Cartesian product:\n\n=> CREATE INDEX ON tickets(book_ref);\n\n=> EXPLAIN SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nWHERE t.book_ref = '03A76D';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=0.99..45.68 rows=6 width=136)\n\nNested Loop\n\n−> Index Scan using tickets_book_ref_idx on tickets t\n\n(cost=0.43..12.46 rows=2 width=104) Index Cond: (book_ref = '03A76D'::bpchar)\n\n−> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) Index Cond: (ticket_no = t.ticket_no)\n\n(7 rows)\n\n1 backend/optimizer/path/costsize.c, cost_rescan function\n\n405\n\nChapter 21 Nested Loop\n\nHere the Nested Loop node traverses the rows of the outer set (tickets),and for each of these rows it searches for the corresponding rows of the inner set (flights), pass- ing the ticket number (t.ticket_no) to the condition as a parameter. When the inner node (Index Scan) is called, it has to deal with the condition ticket_no = constant.\n\nCardinality estimation. The planner estimates that the filter condition by a book- ing number is satisfied by two rows of the outer set (rows=2), and each of these rows matches three rows of the inner set on average (rows=3).\n\nJoin selectivity is a fraction of the Cartesian product of the two sets that remains after the join. It is obvious that we must exclude those rows of both sets that con- tain ���� values in the join key since the equality condition will never be satisfied for them.\n\nThe estimated cardinality equals the cardinality of the Cartesian product (that is, the product of cardinalities of the two sets) multiplied by the selectivity.1\n\nHere the estimated cardinality of the first (outer) set is two rows. Since no condi- tions are applied to the second (inner) set except for the join condition itself, the cardinality of the second set is taken as the cardinality of the ticket_flights table.\n\nSince the joined tables are connected by a foreign key, the selectivity estimation relies on the fact that each row of the child table has exactly one matching row in the parent table. So the selectivity is taken as the inverse of the size of the table referred to by the foreign key.2\n\nThus, for the case when the ticket_no columns contain no ���� values, the estima- tion is as follows:\n\n=> SELECT round(2 * tf.reltuples * (1.0 / t.reltuples)) AS rows FROM pg_class t, pg_class tf WHERE t.relname = 'tickets'\n\nAND tf.relname = 'ticket_flights';\n\nrows −−−−−−\n\n6\n\n(1 row)\n\n1 backend/optimizer/path/costsize.c, calc_joinrel_size_estimate function 2 backend/optimizer/path/costsize.c, get_foreign_key_join_selectivity function\n\n406",
      "page_number": 377
    },
    {
      "number": 21,
      "title": "Nested Loop",
      "start_page": 401,
      "end_page": 420,
      "detection_method": "regex_chapter",
      "content": "21.2 Nested Loop Joins\n\nClearly, tables can be also joined without using foreign keys. Then the selectivity will be taken as the estimated selectivities of the particular join conditions.1\n\nFor the equi-join in this example, the generic formula for selectivity estimation , 1 thatassumesuniformdistributionofvalueslooksasfollows: min( nd2),where nd1 and nd2 represent the number of distinct values of the join key in the first and second set, respectively.2\n\n1 nd1\n\nStatisticsondistinctvaluesshowthatticketnumbersintheticketstableareunique (which is only to be expected, as the ticket_no column is the primary key), and the ticket_flights has about three matching rows for each ticket:\n\n=> SELECT t.n_distinct, tf.n_distinct FROM pg_stats t, pg_stats tf WHERE t.tablename = 'tickets' AND t.attname = 'ticket_no'\n\nAND tf.tablename = 'ticket_flights' AND tf.attname = 'ticket_no';\n\nn_distinct | n_distinct −−−−−−−−−−−−+−−−−−−−−−−−−−\n\n−1 | −0.30362356\n\n(1 row)\n\nThe result would match the estimation for the join with the foreign key:\n\n=> SELECT round(2 * tf.reltuples *\n\nleast(1.0/t.reltuples, 1.0/tf.reltuples/0.30362356)\n\n) AS rows FROM pg_class t, pg_class tf WHERE t.relname = 'tickets' AND tf.relname = 'ticket_flights';\n\nrows −−−−−−\n\n6\n\n(1 row)\n\nThe planner tries to refine this baseline estimation whenever possible. It cannot use histograms at the moment, but it takes ��� lists into account if such statistics have been collected on the join key for both tables.3 The selectivity of the rows that appear in the list can be estimated more accurately, and only the remaining rows will have to rely on calculations that are based on uniform distribution.\n\n1 backend/optimizer/path/clausesel.c, clauselist_selectivity function 2 backend/utils/adt/selfuncs.c, eqjoinsel function 3 backend/utils/adt/selfuncs.c, eqjoinsel function\n\n407\n\np. ���\n\np. ���\n\nChapter 21 Nested Loop\n\nIngeneral,joinselectivityestimationislikelytobemoreaccurateiftheforeignkey is defined. It is especially true for composite join keys, as the selectivity is often largely underestimated in this case.\n\nUsing the ������� ������� command, you can view not only the actual number of rows, but also the number of times the inner loop has been executed:\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nWHERE t.book_ref = '03A76D';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n(cost=0.99..45.68 rows=6 width=136)\n\n(actual rows=8 loops=1) −> Index Scan using tickets_book_ref_idx on tickets t\n\n(cost=0.43..12.46 rows=2 width=104) (actual rows=2 loops=1) Index Cond: (book_ref = '03A76D'::bpchar)\n\n−> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) (actual rows=4 loops=2) Index Cond: (ticket_no = t.ticket_no)\n\n(8 rows)\n\nThe outer set contains two rows (actual rows=2); the estimation has been correct. So the Index Scan node was executed twice (loops=2),and each time it selected four rows on average (actual rows=4). Hence the total number of found rows: actual rows=8.\n\nI do not show the execution time of each stage of the plan (������ ���) for the output to fit the limited width of the page; besides, on some platforms an output with timing enabled can significantly slow down query execution. But if we did include it, Postgre��� would display an average value, just like for the row count. To get the total execution time, you should multiply this value by the number of iterations (loops).\n\nCost estimation. The cost estimation formula here is the same as in the previous examples.\n\nLet’s recall our query plan:\n\n408\n\n21.2 Nested Loop Joins\n\n=> EXPLAIN SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nWHERE t.book_ref = '03A76D';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=0.99..45.68 rows=6 width=136)\n\nNested Loop\n\n−> Index Scan using tickets_book_ref_idx on tickets t\n\n(cost=0.43..12.46 rows=2 width=104) Index Cond: (book_ref = '03A76D'::bpchar)\n\n−> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) Index Cond: (ticket_no = t.ticket_no)\n\n(7 rows)\n\nIn this case, the cost of each subsequent scan of the inner set is the same as that of the first scan. So we ultimately get the following figures:\n\n=> SELECT 0.43 + 0.56 AS startup_cost,\n\nround((\n\n12.46 + 2 * 16.57 + 6 * current_setting('cpu_tuple_cost')::real\n\n)::numeric, 2) AS total_cost;\n\nstartup_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0.99 |\n\n45.66\n\n(1 row)\n\nCaching Rows (Memoization)\n\nIf the inner set is repeatedly scanned with the same parameter values (thus giving the same results), it may turn out to be beneficial to cache the rows of this set.\n\nSuch caching is performed by the Memoize1 node. Being similar to the Materialize node, it is designed to handle parameterized joins and has a much more complex implementation:\n\n1 backend/executor/nodeMemoize.c\n\n409\n\nv. ��\n\n4MB 1.0\n\nChapter 21 Nested Loop\n\nThe Materialize node simply materializes all the rows returned by its child node, while Memoize ensures that the rows returned for different parameter values are kept separately.\n\nIn the event of an overflow, the Materialize storage starts spilling rows to disk, while Memoize keeps all the rows in memory (there would otherwise be no point in caching).\n\nHere is an example of a query that uses Memoize:\n\n=> EXPLAIN SELECT * FROM flights f\n\nJOIN aircrafts_data a ON f.aircraft_code = a.aircraft_code\n\nWHERE f.flight_no = 'PG0003';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n(cost=5.44..387.10 rows=113 width=135)\n\n−> Bitmap Heap Scan on flights f\n\n(cost=5.30..382.22 rows=113 width=63) Recheck Cond: (flight_no = 'PG0003'::bpchar) −> Bitmap Index Scan on flights_flight_no_scheduled_depart...\n\n(cost=0.00..5.27 rows=113 width=0) Index Cond: (flight_no = 'PG0003'::bpchar) (cost=0.15..0.27 rows=1 width=72)\n\n−> Memoize\n\nCache Key: f.aircraft_code Cache Mode: logical −> Index Scan using aircrafts_pkey on aircrafts_data a\n\n(cost=0.14..0.26 rows=1 width=72) Index Cond: (aircraft_code = f.aircraft_code)\n\n(13 rows)\n\nwork_mem × The size of the memory chunk used to store cached rows equals hash_mem_multiplier. As implied by the second parameter’s name, cached rows × are stored in a hash table (with open addressing).1 The hash key (shown as Cache Key in the plan) is the parameter value (or several values if there are more than one parameter).\n\nAll the hash keys are bound into a list; one of its ends is considered cold (since it contains the keys that have not been used for a long time), while the other is hot (it stores recently used keys).\n\n1 include/lib/simplehash.h\n\n410\n\n21.2 Nested Loop Joins\n\nIf a call on the Memoize node shows that the passed parameter values correspond to the already cached rows,these rows will be passed on to the parent node (Nested Loop) without checking the child node. The used hash key is then moved to the hot end of the list.\n\nIf the cache does not contain the required rows,the Memoize node pulls them from its child node, caches them, and passes them on to the node above. The corre- sponding hash key also becomes hot.\n\nAs newdata is being cached,it can fill all the available memory. Tofree some space, the rows that correspond to cold keys get evicted. This eviction algorithm differs from the one used in the buffer\n\ncache but serves the same purpose.\n\nSome parameter values may turn out to have so many matching rows that they do not fit into the allocated memory chunk, even if all the other rows are already evicted. Such parameters are skipped—it makes no sense to cache only some of the rows since the next call will still have to get all the rows from the child node.\n\nCost and cardinality estimations. These calculations are quite similar to what we have already seen above. We just have to bear in mind that the cost of the Memoize node shown in the plan has nothing to do with its actual cost: it is simply the cost of its child node increased by the\n\ncpu_tuple_cost value.1\n\nWe have already come across a similar situation for the Materialize node: its cost is only calculated for subsequent scans2 and is not reflected in the plan.\n\nClearly,it only makes sense to use Memoize if it is cheaper than its child node. The cost of each subsequent Memoize scan depends on the expected cache access pro- file and the size of the memory chunk that can be used for caching. The calculated value is highly dependent on the accurate estimation of the number of distinct parameter values to be used in the scans of the inner set of rows.3 Based on this number,you can weigh the probabilities of the rows to be cached and to be evicted from the cache. The expected hits reduce the estimated cost, while potential evic- tions increase it. We will skip the details of these calculations here.\n\n1 backend/optimizer/util/pathnode.c, create_memoize_path function 2 backend/optimizer/path/costsize.c, cost_memoize_rescan function 3 backend/utils/adt/selfuncs.c, estimate_num_groups function\n\n411\n\np. ���\n\n0.01\n\non\n\nChapter 21 Nested Loop\n\nTo figure out what is actually going on during query execution, we will use the ������� ������� command, as usual:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM flights f\n\nJOIN aircrafts_data a ON f.aircraft_code = a.aircraft_code\n\nWHERE f.flight_no = 'PG0003';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop (actual rows=113 loops=1) −> Bitmap Heap Scan on flights f (actual rows=113 loops=1) Recheck Cond: (flight_no = 'PG0003'::bpchar) Heap Blocks: exact=2 −> Bitmap Index Scan on flights_flight_no_scheduled_depart...\n\n(actual rows=113 loops=1) Index Cond: (flight_no = 'PG0003'::bpchar)\n\n−> Memoize (actual rows=1 loops=113)\n\nCache Key: f.aircraft_code Cache Mode: logical Hits: 112 Usage: 1kB −> Index Scan using aircrafts_pkey on aircrafts_data a\n\nMisses: 1\n\nEvictions: 0\n\nOverflows: 0\n\nMemory\n\n(actual rows=1 loops=1) Index Cond: (aircraft_code = f.aircraft_code)\n\n(16 rows)\n\nThis query selects the flights that follow the same route and are performed by air- craft of a particular type, so all the calls on the Memoize node use the same hash key. The first row has to be fetched from the table (Misses: 1), but all the subse- quent rows are found in the cache (Hits: 112). The whole operation takes just � k� of memory.\n\nThe other two displayed values are zero: they represent the number of evictions and the number of cache overflows when it was impossible to cache all the rows related to a particular set of parameters. Large figures would indicate that the allocated cache is too small, which might be caused by inaccurate estimation of the number of distinct parameter values. Then the use of the Memoize node can turn out to be quite expensive. In the extreme case, you can forbid the planner to use caching by turning off the\n\nenable_memoize parameter.\n\n412\n\n21.2 Nested Loop Joins\n\nOuter Joins\n\nThe nested loop join can be used to perform the left outer join:\n\n=> EXPLAIN SELECT * FROM ticket_flights tf\n\nLEFT JOIN boarding_passes bp ON bp.ticket_no = tf.ticket_no AND bp.flight_id = tf.flight_id\n\nWHERE tf.ticket_no = '0005434026720';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop Left Join\n\n(cost=1.12..33.35 rows=3 width=57)\n\nJoin Filter: ((bp.ticket_no = tf.ticket_no) AND (bp.flight_id = tf.flight_id)) −> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) Index Cond: (ticket_no = '0005434026720'::bpchar)\n\n−> Materialize\n\n(cost=0.56..16.62 rows=3 width=25)\n\n−> Index Scan using boarding_passes_pkey on boarding_passe...\n\n(cost=0.56..16.61 rows=3 width=25) Index Cond: (ticket_no = '0005434026720'::bpchar)\n\n(10 rows)\n\nHere the join operation is represented by the Nested Loop Left Join node. The plan- ner has chosen a non-parameterized join with a filter: it performs identical scans of the inner set of rows (so this set is hidden behind the Materialize node) and re- turns the rows that satisfy the filter condition (Join Filter).\n\nThe cardinality of the outer join is estimated just like the one of the inner join, except that the calculated estimation is compared with the cardinality of the outer set of rows, and the bigger value is taken as the final result.1 In other words, the outer join never reduces the number of rows (but can increase it).\n\nThe cost estimation is similar to that of the inner join.\n\nWe must also keep in mind that the planner can select different plans for inner and outer joins. Even this simple example will have a different Join Filter if the planner is forced to use a nested loop join:\n\n1 backend/optimizer/path/costsize.c, calc_joinrel_size_estimate function\n\n413\n\nChapter 21 Nested Loop\n\n=> SET enable_mergejoin = off;\n\n=> EXPLAIN SELECT * FROM ticket_flights tf\n\nJOIN boarding_passes bp ON bp.ticket_no = tf.ticket_no AND bp.flight_id = tf.flight_id\n\nWHERE tf.ticket_no = '0005434026720';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n(cost=1.12..33.33 rows=3 width=57)\n\nJoin Filter: (tf.flight_id = bp.flight_id) −> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) Index Cond: (ticket_no = '0005434026720'::bpchar)\n\n−> Materialize\n\n(cost=0.56..16.62 rows=3 width=25)\n\n−> Index Scan using boarding_passes_pkey on boarding_passe...\n\n(cost=0.56..16.61 rows=3 width=25) Index Cond: (ticket_no = '0005434026720'::bpchar)\n\n(9 rows)\n\n=> RESET enable_mergejoin;\n\nA slight difference in the total cost is caused by the fact that the outer join must also check ticket numbers to get the correct result if there is no match in the outer set of rows.\n\nRight joins are not supported,1 as the nested loop algorithm treats the inner and outer sets differently. The outer set is scanned in full; as forthe inner set,the index access allows reading only those rows that satisfy the join condition,so some of its rows may be skipped altogether.\n\nA full join is not supported for the same reason.\n\nAnti- and Semi-joins\n\nAnti-joins and semi-joins are similar in the sense that for each row of the first (outer) set it is enough to find only one matching row in the second (inner) set.\n\nAnanti-joinreturnstherowsofthefirstsetonlyiftheyhavenomatchinthesecond set: as soon as the executor finds the first matching row in the second set, it can\n\n1 backend/optimizer/path/joinpath.c, match_unsorted_outer function\n\n414\n\n21.2 Nested Loop Joins\n\nexit the current loop: the corresponding row of the first set must be excluded from the result.\n\nAnti-joins can be used to compute the ��� ������ predicate.\n\nFor example, let’s find aircraft models with undefined cabin configuration. The corresponding plan contains the Nested Loop Anti Join node:\n\n=> EXPLAIN SELECT * FROM aircrafts a WHERE NOT EXISTS (\n\nSELECT * FROM seats s WHERE s.aircraft_code = a.aircraft_code\n\n);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop Anti Join\n\n(cost=0.28..4.65 rows=1 width=40)\n\n−> Seq Scan on aircrafts_data ml −> Index Only Scan using seats_pkey on seats s\n\n(cost=0.00..1.09 rows=9 widt...\n\n(cost=0.28..5.55 rows=149 width=4) Index Cond: (aircraft_code = ml.aircraft_code)\n\n(5 rows)\n\nAn alternative query without the ��� ������ predicate will have the same plan:\n\n=> EXPLAIN SELECT a.* FROM aircrafts a\n\nLEFT JOIN seats s ON a.aircraft_code = s.aircraft_code\n\nWHERE s.aircraft_code IS NULL;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop Anti Join\n\n(cost=0.28..4.65 rows=1 width=40)\n\n−> Seq Scan on aircrafts_data ml −> Index Only Scan using seats_pkey on seats s\n\n(cost=0.00..1.09 rows=9 widt...\n\n(cost=0.28..5.55 rows=149 width=4) Index Cond: (aircraft_code = ml.aircraft_code)\n\n(5 rows)\n\nA semi-join returns those rows of the first set that have at least one match in the second set (again, there is no need to check the set for other matches—the result is already known).\n\nA semi-join can be used to compute the ������ predicate. Let’s find the aircraft models with seats installed in the cabin:\n\n415\n\nChapter 21 Nested Loop\n\n=> EXPLAIN SELECT * FROM aircrafts a WHERE EXISTS (\n\nSELECT * FROM seats s WHERE s.aircraft_code = a.aircraft_code\n\n);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop Semi Join\n\n(cost=0.28..6.67 rows=9 width=40)\n\n−> Seq Scan on aircrafts_data ml −> Index Only Scan using seats_pkey on seats s\n\n(cost=0.00..1.09 rows=9 widt...\n\n(cost=0.28..5.55 rows=149 width=4) Index Cond: (aircraft_code = ml.aircraft_code)\n\n(5 rows)\n\nThe Nested Loop Semi Join node represents the same-name join method. This plan (just like the anti-join plans above) provides the basic estimation of the number of rows in the seats table (rows=149), although it is enough to retrieve only one of them. The actual query execution stops after fetching the first row, of course:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM aircrafts a WHERE EXISTS (\n\nSELECT * FROM seats s WHERE s.aircraft_code = a.aircraft_code\n\n);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop Semi Join (actual rows=9 loops=1)\n\n−> Seq Scan on aircrafts_data ml (actual rows=9 loops=1) −> Index Only Scan using seats_pkey on seats s\n\n(actual rows=1 loops=9) Index Cond: (aircraft_code = ml.aircraft_code) Heap Fetches: 0\n\n(6 rows)\n\nCardinality estimation. The selectivity of a semi-join is estimated in the usual manner, except that the cardinality of the inner set is taken as one. For anti-joins, the estimated selectivity is subtracted from one, just like for negation.1\n\n1 backend/optimizer/path/costsize.c, calc_joinrel_size_estimate function\n\n416\n\n21.2 Nested Loop Joins\n\nCostestimation. Foranti-andsemi-joins,thecostestimationreflectsthefactthat the scan of the second set stops as soon as the first matching row is found.1\n\nNon-Equi-joins\n\nThe nested loop algorithm allows joining sets of rows based on any join condition.\n\nObviously, if the inner set is a base table with an index created on it, and the join condition uses an operator that belongs to an operator class of this index, the ac- cess to the inner set can be quite efficient. But it is always possible to perform the join by calculating a Cartesian product of rows filtered by some condition—which can be absolutely arbitrary in this case. Like in the following query, which selects pairs of airports that are located close to each other:\n\n=> CREATE EXTENSION earthdistance CASCADE;\n\n=> EXPLAIN (costs off) SELECT * FROM airports a1\n\nJOIN airports a2 ON a1.airport_code != a2.airport_code\n\nAND a1.coordinates <@> a2.coordinates < 100;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\nJoin Filter: ((ml.airport_code <> ml_1.airport_code) AND ((ml.coordinates <@> ml_1.coordinates) < '100'::double precisi... −> Seq Scan on airports_data ml −> Materialize\n\n−> Seq Scan on airports_data ml_1\n\n(6 rows)\n\nParallel Mode\n\nA nested loop join can participate in parallel\n\nplan execution.2\n\nIt is only the outer set that can be processed in parallel, as it can be scanned by several workers simultaneously. Having fetched an outer row, each worker then has to search for the matching rows in the inner set, which is done sequentially.\n\n1 backend/optimizer/path/costsize.c, final_cost_nestloop function 2 backend/optimizer/path/joinpath.c, consider_parallel_nestloop function\n\n417\n\np. ���\n\nv. �.�\n\np. ���\n\np. ���\n\nChapter 21 Nested Loop\n\nThe query shown below includes several joins; it searches for passengers that have tickets for a particular flight:\n\n=> EXPLAIN (costs off) SELECT t.passenger_name FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no JOIN flights f ON f.flight_id = tf.flight_id\n\nWHERE f.flight_id = 12345;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n−> Index Only Scan using flights_flight_id_status_idx on fligh...\n\nIndex Cond: (flight_id = 12345)\n\n−> Gather\n\nWorkers Planned: 2 −> Nested Loop\n\n−> Parallel Seq Scan on ticket_flights tf\n\nFilter: (flight_id = 12345)\n\n−> Index Scan using tickets_pkey on tickets t Index Cond: (ticket_no = tf.ticket_no)\n\n(10 rows)\n\nAt the upper level, the nested loop join is performed sequentially. The outer set consists of a single row of the flights table fetched by a unique key, so the use of a nested loop is justified even for a large number of inner rows.\n\nThe inner set is retrieved using a parallel plan. Each of the workers scans its own share of rows of the ticket_flights table and joins them with tickets using the nested loop algorithm.\n\n418\n\n22\n\nHashing\n\n22.1 Hash Joins\n\nOne-Pass Hash Joins\n\nA hash join searches for matching rows using a pre-built hash table. Here is an example of a plan with such a join:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Join\n\nHash Cond: (tf.ticket_no = t.ticket_no) −> Seq Scan on ticket_flights tf −> Hash\n\n−> Seq Scan on tickets t\n\n(5 rows)\n\nAt the first stage, the Hash Join node1 calls the Hash node,2 which pulls the whole inner set of rows from its child node and places it into a hash table.\n\nStoring pairs of hash keys and values, the hash table enables fast access to a value by its key; the search time does not depend on the size of the hash table, as hash keys are distributed more or less uniformly between a limited number of buckets. The bucket to which a given key goes is determined by the hash function of the hash key; since the number of buckets is always a power of two, it is enough to take the required number of bits of the computed value.\n\n1 backend/executor/nodeHashjoin.c 2 backend/executor/nodeHash.c\n\n419\n\np. ���\n\nv. ��\n\n4MB 1.0\n\nChapter 22 Hashing\n\nJust like the buffer table that resolves hash collisions by chaining.1\n\ncache,this implementation uses a dynamically extendible hash\n\nAtthefirst stageofajoinoperation,theinnersetisscanned,andthehashfunction is computed for each of its rows. The columns referenced in the join condition (Hash Cond) serve as the hash key, while the hash table itself stores all the queried fields of the inner set.\n\nA hash join is most efficient if the whole hash table can be accommodated in ���, as the executor manages to process the data in one batch in this case. The size work_mem × of the memory chunk allocated for this purpose is limited by the hash_mem_multiplier value.\n\nwork_mem×hash_mem_multiplier\n\ninner set\n\nouter set\n\nLet’s run ������� ������� to take a look at statistics on memory usage of a query:\n\n=> SET work_mem = '256MB';\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM bookings b\n\nJOIN tickets t ON b.book_ref = t.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Join (actual rows=2949857 loops=1) Hash Cond: (t.book_ref = b.book_ref) −> Seq Scan on tickets t (actual rows=2949857 loops=1) −> Hash (actual rows=2111110 loops=1)\n\nBuckets: 4194304 −> Seq Scan on bookings b (actual rows=2111110 loops=1)\n\nBatches: 1\n\nMemory Usage: 145986kB\n\n(6 rows)\n\n1 backend/utils/hash/dynahash.c\n\n420\n\n22.1 Hash Joins\n\nUnlike a nested loop join, which treats inner and outer sets differently, a hash join canswapthemaround. Thesmallersetisusuallyusedastheinnerone,asitresults in a smaller hash table.\n\nIn this example,the whole table fits into the allocated cache: it takes about ��� �� (Memory Usage) and contains � � = ��� buckets. So the join is performed in one pass (Batches).\n\nBut if the query referred to only one column, the hash table would fit ��� ��:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT b.book_ref FROM bookings b\n\nJOIN tickets t ON b.book_ref = t.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Join (actual rows=2949857 loops=1) Hash Cond: (t.book_ref = b.book_ref) −> Index Only Scan using tickets_book_ref_idx on tickets t\n\n(actual rows=2949857 loops=1) Heap Fetches: 0\n\n−> Hash (actual rows=2111110 loops=1)\n\nBuckets: 4194304 −> Seq Scan on bookings b (actual rows=2111110 loops=1)\n\nBatches: 1\n\nMemory Usage: 113172kB\n\n(8 rows)\n\n=> RESET work_mem;\n\nIt is yet another reason to avoid referring to superfluous fields in a query (which can happen if you are using an asterisk, to give one example).\n\nThe chosen number of buckets should guarantee that each bucket holds only one row on average when the hash table is completely filled with data. Higher density would increase the rate of hash collisions, making the search less efficient, while a less compact hash table would take up too much memory. The estimated number of buckets is increased up to the nearest power of two.1\n\n(If the estimated hash table size exceeds the memory limit based on the average width of a single row, two-pass hashing will be applied.)\n\nA hash join cannot start returning results until the hash table is fully built.\n\n1 backend/executor/nodeHash.c, ExecChooseHashTableSize function\n\n421\n\np. ���\n\nChapter 22 Hashing\n\nAt the second stage (the hash table is already built by this time), the Hash Join node calls on its second child node to get the outer set of rows. For each scanned row, the hash table is searched for a match. It requires calculating the hash key for the columns of the outer set that are included into the join condition.\n\nouter set\n\nThe found matches are returned to the parent node.\n\nCost estimation. We have already covered cardinality estimation depend on the join method, I will now focus on cost estimation.\n\n; since it does not\n\nThe cost of the Hash node is represented by the total cost of its child node. It is a dummy number that simply fills the slot in the plan.1 All the actual estimations are included into the cost of the Hash Join node.2\n\nHere is an example:\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT * FROM flights f\n\nJOIN seats s ON s.aircraft_code = f.aircraft_code;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=38.13..278507.28 rows=16518865 width=78)\n\nHash Join\n\n(actual rows=16518865 loops=1) Hash Cond: (f.aircraft_code = s.aircraft_code) −> Seq Scan on flights f\n\n(cost=0.00..4772.67 rows=214867 widt...\n\n(actual rows=214867 loops=1)\n\n−> Hash\n\n(cost=21.39..21.39 rows=1339 width=15)\n\n(actual rows=1339 loops=1) Buckets: 2048 −> Seq Scan on seats s\n\nBatches: 1\n\nMemory Usage: 79kB\n\n(cost=0.00..21.39 rows=1339 width=15)\n\n(actual rows=1339 loops=1)\n\n(10 rows)\n\n1 backend/optimizer/plan/createplan.c, create_hashjoin_plan function 2 backend/optimizer/path/costsize.c, initial_cost_hashjoin and final_cost_hashjoin functions\n\n422\n\n22.1 Hash Joins\n\nThe startup cost of the join reflects primarily the cost of hash table creation and includes the following components:\n\nthetotalcostoffetchingtheinnerset,whichisrequiredtobuildthehashtable\n\nthe cost of calculating the hash function of all the columns included into the cpu_operator_cost per op- 0.0025\n\nthe cost of insertion of all the inner rows into the hash table (estimated at\n\ncpu_tuple_cost per inserted row)\n\nthe startup cost of fetching the outer set of rows,which is required to start the\n\njoin operation\n\nThe total cost comprises the startup cost and the cost of the join itself, namely:\n\nthe cost of computing the hash function of all the columns included into the\n\njoin key, for each row of the outer set (cpu_operator_cost)\n\nthecostofjoinconditionrechecks,whicharerequiredtoaddresspossiblehash\n\ncollisions (estimated at cpu_operator_cost per each checked operator)\n\nthe processing cost for each resulting row (cpu_tuple_cost)\n\nThe number of required rechecks is the hardest to estimate. It is calculated by multiplying the number of rows of the outer set by some fraction of the inner set (stored in the hash table). To estimate this fraction, the planner has to take into account that data distribution may be non-uniform. I will spare you the details of these computations;1 in this particular case,this fraction is estimated at �.������.\n\nThus, the cost of our query is estimated as follows:\n\n=> WITH cost(startup) AS (\n\nSELECT round((\n\n21.39 + current_setting('cpu_operator_cost')::real * 1339 + current_setting('cpu_tuple_cost')::real * 1339 + 0.00\n\n)::numeric, 2)\n\n)\n\n1 backend/utils/adt/selfuncs.c, estimate_hash_bucket_stats function\n\n423\n\n0.01\n\nChapter 22 Hashing\n\nSELECT startup,\n\nstartup + round((\n\n4772.67 + current_setting('cpu_operator_cost')::real * 214867 + current_setting('cpu_operator_cost')::real * 214867 * 1339 *\n\n0.150112 +\n\ncurrent_setting('cpu_tuple_cost')::real * 16518865\n\n)::numeric, 2) AS total\n\nFROM cost;\n\nstartup |\n\ntotal\n\n−−−−−−−−−+−−−−−−−−−−−\n\n38.13 | 278507.26\n\n(1 row)\n\nAnd here is the dependency graph:\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Join\n\n(cost=38.13..278507.28 rows=16518865 width=78) Hash Cond: (f.aircraft_code = s.aircraft_code) −> Seq Scan on flights f\n\n(cost=0.00..4772.67 rows=214867 width=63)\n\n−> Hash\n\n(cost=21.39..21.39 rows=1339 width=15) −> Seq Scan on seats s\n\n(cost=0.00..21.39 rows=1339 width=15)\n\n(9 rows)\n\nTwo-Pass Hash Joins\n\nIf the planner’s estimations show that the hash table will not fit the allocated mem- ory, the inner set of rows is split into batches to be processed separately. The num- ber of batches (just like the number of buckets) is always a power of two; the batch to use is determined by the corresponding number of bits of the hash key.1\n\nAny two matching rows belong to one and the same batch: rows placed into differ- ent batches cannot have the same hash code.\n\n1 backend/executor/nodeHash.c, ExecHashGetBucketAndBatch function\n\n424\n\n22.1 Hash Joins\n\nAll batches hold an equal number of hash keys. If the data is distributed uniformly, batch sizes will also be roughly the same. The planner can control memory con- sumption by choosing an appropriate number of batches.1\n\nAt the first stage, the executor scans the inner set of rows to build the hash table. If the scanned row belongs to the first batch, it is added to the hash table and kept in ���. Otherwise,it is written into a temporary file (there is a separate file for each batch).2\n\nThe total volume of temporary files that a session can store on disk is limited by the temp_file_limit parameter (temporary tables are not included into this limit). As soon as the session reaches this value, the query is aborted.\n\ninner set\n\nouter set\n\nAt the second stage, the outer set is scanned. If the row belongs to the first batch, it is matched against the hash table, which contains the first batch of rows of the inner set (there can be no matches in other batches anyway).\n\nIftherowbelongstoadifferentbatch,itisstoredinatemporaryfile,whichisagain created separately for each batch. Thus, N batches can use 2(N − 1) files (or fewer if some of the batches turn out to be empty).\n\nOncethesecondstageiscomplete,thememoryallocatedforthehashtableisfreed. At this point, we already have the result of the join for one of the batches.\n\n1 backend/executor/nodeHash.c, ExecChooseHashTableSize function 2 backend/executor/nodeHash.c, ExecHashTableInsert function\n\n425\n\n−1\n\nChapter 22 Hashing\n\ninner set\n\nouter set\n\nBoth stages are repeated for each of the batches saved on disk: the rows of the inner set are transferred from the temporary file to the hash table; then the rows of the outer set related to the same batch are read from another temporary file and matched against this hash table. Once processed, temporary files get deleted.\n\nouter set\n\nUnlike a similar output for a one-pass join, the output of the ������� command for a two-pass join contains more than one batch. If run with the ������� option, this command also displays statistics on disk access:\n\n=> EXPLAIN (analyze, buffers, costs off, timing off, summary off) SELECT * FROM bookings b\n\nJOIN tickets t ON b.book_ref = t.book_ref;\n\n426",
      "page_number": 401
    },
    {
      "number": 22,
      "title": "Hashing",
      "start_page": 421,
      "end_page": 444,
      "detection_method": "regex_chapter",
      "content": "22.1 Hash Joins\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Join (actual rows=2949857 loops=1) Hash Cond: (t.book_ref = b.book_ref) Buffers: shared hit=7236 read=55626, temp read=55126 written=55126 −> Seq Scan on tickets t (actual rows=2949857 loops=1)\n\nBuffers: shared read=49415\n\n−> Hash (actual rows=2111110 loops=1)\n\nBuckets: 65536 Buffers: shared hit=7236 read=6211, temp written=10858 −> Seq Scan on bookings b (actual rows=2111110 loops=1)\n\nBatches: 64\n\nMemory Usage: 2277kB\n\nBuffers: shared hit=7236 read=6211\n\n(11 rows)\n\nI have already shown this query above with an increased work_mem setting. The default value of � �� is too small for the whole hash table to fit ���; in this exam- ple, the data is split into �� batches, and the hash table uses �� � = ��� buckets. As the hash table is being built (the Hash node), the data is written into temporary files (temp written); at the join stage (the Hash Join node), temporary files are both read and written (temp read, written).\n\nlog_temp_files param- To collect more statistics on temporary files, you can set the eter to zero. Then the server log will list all the temporary files and their sizes (as they appeared at the time of deletion).\n\nDynamic Adjustments\n\nThe planned course of events may be disrupted by two issues: inaccurate statistics and non-uniform data distribution.\n\nIf the distribution of values in the join key columns is non-uniform, different batches will have different sizes.\n\nIf some batch (except for the very first one) turns out to be too large, all its rows will have to be written to disk and then read from disk. It is the outer set that causes most of the trouble, as it is typically bigger. So if there are regular, non- statistics on ���s of the outer set (that is,the outer set is represented multivariate\n\n427\n\n−1\n\np. ���\n\np. ���\n\nChapter 22 Hashing\n\nby a table, and the join is performed by a single column),rows with hash codes cor- responding to ���s are considered to be a part of the first batch.1 This technique (called skew optimization) can reduce the �/� overhead of a two-pass join to some extent.\n\nBecause of these two factors, the size of some (or all) batches may exceed the esti- mation. Thenthecorrespondinghashtablewillnotfittheallocatedmemorychunk and will surpass the defined limits.\n\nSoifthehashtablebeingbuiltturnsouttoobig,thenumberofbatchesisincreased (doubled) on the fly. Each batch is virtually split into two new ones: about half of the rows (assuming that the distribution is uniform) is left in the hash table, while the other half is saved into a new temporary file.2\n\nSuch a split can happen even if a one-pass join has been originally planned. In fact, one- and two-pass joins use one and the same algorithm implemented by the same code; I single them out here solely for smoother narration.\n\nThe number of batches cannot be reduced. If it turns out that the planner has overestimated the data size, batches will not be merged together.\n\nIn the case of non-uniform distribution,increasing the number of batches may not help. For example,if the key column contains one and the same value in all its rows, they will be placed into the same batch since the hash function will be returning the same value over and over again. Unfortunately, the hash table will continue growing in this case, regardless of the imposed restrictions.\n\nIn theory, this issue could be addressed by a multi-pass join, which would perform partial scans of the batch, but it is not supported.\n\nTo demonstrate a dynamic increase in the number of batches, we first have to per- : form some manipulations\n\n=> CREATE TABLE bookings_copy (LIKE bookings INCLUDING INDEXES) WITH (autovacuum_enabled = off);\n\n=> INSERT INTO bookings_copy SELECT * FROM bookings;\n\nINSERT 0 2111110\n\n1 backend/executor/nodeHash.c, ExecHashBuildSkewHash function 2 backend/executor/nodeHash.c, ExecHashIncreaseNumBatches function\n\n428\n\n22.1 Hash Joins\n\n=> DELETE FROM bookings_copy WHERE random() < 0.9;\n\nDELETE 1899232\n\n=> ANALYZE bookings_copy;\n\n=> INSERT INTO bookings_copy SELECT * FROM bookings ON CONFLICT DO NOTHING;\n\nINSERT 0 1899232\n\n=> SELECT reltuples FROM pg_class WHERE relname = 'bookings_copy';\n\nreltuples −−−−−−−−−−−\n\n211878\n\n(1 row)\n\nAs a result, we get a new table called bookings_copy. It is an exact copy of the bookings table, but the planner underestimates the number of rows in it by ten times. A similar situation may occur if the hash table is generated for a set of rows produced by another join operation, so there is no reliable statistics available.\n\nThis miscalculation makes the planner think that � buckets are enough, but while the join is being performed, this number grows to ��:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM bookings_copy b\n\nJOIN tickets t ON b.book_ref = t.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Join (actual rows=2949857 loops=1) Hash Cond: (t.book_ref = b.book_ref) −> Seq Scan on tickets t (actual rows=2949857 loops=1) −> Hash (actual rows=2111110 loops=1) Buckets: 65536 (originally 65536)\n\nBatches: 32 (originally 8)\n\nMemory Usage: 4040kB\n\n−> Seq Scan on bookings_copy b (actual rows=2111110 loops=1)\n\n(7 rows)\n\nCost estimation. I have already used this example to demonstrate cost estimation for a one-pass join, but now I am going to reduce the size of available memory to the minimum, so the planner will have to use two batches. It increases the cost of the join:\n\n429\n\nChapter 22 Hashing\n\n=> SET work_mem = '64kB';\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT * FROM flights f\n\nJOIN seats s ON s.aircraft_code = f.aircraft_code;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=45.13..283139.28 rows=16518865 width=78)\n\nHash Join\n\n(actual rows=16518865 loops=1) Hash Cond: (f.aircraft_code = s.aircraft_code) −> Seq Scan on flights f\n\n(cost=0.00..4772.67 rows=214867 widt...\n\n(actual rows=214867 loops=1)\n\n−> Hash\n\n(cost=21.39..21.39 rows=1339 width=15)\n\n(actual rows=1339 loops=1) Buckets: 2048 −> Seq Scan on seats s\n\nBatches: 2\n\nMemory Usage: 55kB\n\n(cost=0.00..21.39 rows=1339 width=15)\n\n(actual rows=1339 loops=1)\n\n(10 rows)\n\n=> RESET work_mem;\n\nThe cost of the second pass is incurred by spilling rows into temporary files and reading them from these files.\n\nThe startup cost of a two-pass join is based on that of a one-pass join, which is increased by the estimated cost of writing as many pages as required to store all the necessary fields of all the rows of the inner set.1 Although the first batch is not written to disk when the hash table is being built, the estimation does not take it into account and hence does not depend on the number of batches.\n\nIn its turn, the total cost comprises the total cost of a one-pass join and the esti- mated costs of reading the rows of the inner set previously stored on disk, as well as reading and writing the rows of the outer set.\n\nBoth writing and reading are estimated at seq_page_cost per page,as �/� operations are assumed to be sequential.\n\nInthisparticularcase,thenumberofpagesrequiredfortheinnersetisestimatedat �, while the data of the outer set is expected to fit ���� pages. Having added these estimations to the one-pass join cost calculated above, we get the same figures as shown in the query plan:\n\n1 backend/optimizer/path/costsize.c, page_size function\n\n430\n\n22.1 Hash Joins\n\n=> SELECT 38.13 + -- startup cost of a one-pass join\n\ncurrent_setting('seq_page_cost')::real * 7 AS startup,\n\n278507.28 + -- total cost of a one-pass join\n\ncurrent_setting('seq_page_cost')::real * 2 * (7 + 2309) AS total;\n\nstartup |\n\ntotal\n\n−−−−−−−−−+−−−−−−−−−−−\n\n45.13 | 283139.28\n\n(1 row)\n\nThus, if there is not enough memory, the join is performed in two passes and be- comes less efficient. Therefore, it is important to observe the following points:\n\nThe query must be composed in a way that excludes redundant fields from the\n\nhash table.\n\nThe planner must choose the smaller of the two sets of rows when building\n\nthe hash table.\n\nUsing Hash Joins in Parallel Plans\n\nThe hash join algorithm described above can also be used in parallel plans. First, several parallel processes build their own (absolutely identical) hash tables for the inner set, independently of each other; then they start processing the outer set concurrently. The performance gain here is due to each process scanning only its own share of outer rows.\n\nThe following plan uses a regular one-pass hash join:\n\n=> SET work_mem = '128MB';\n\n=> SET enable_parallel_hash = off;\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM bookings b\n\nJOIN tickets t ON t.book_ref = b.book_ref;\n\n431\n\nv. �.�\n\nv. ��\n\nChapter 22 Hashing\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=3 loops=1)\n\nWorkers Planned: 2 Workers Launched: 2 −> Partial Aggregate (actual rows=1 loops=3)\n\n−> Hash Join (actual rows=983286 loops=3)\n\nHash Cond: (t.book_ref = b.book_ref) −> Parallel Index Only Scan using tickets_book_ref...\n\nHeap Fetches: 0\n\n−> Hash (actual rows=2111110 loops=3)\n\nBuckets: 4194304 113172kB −> Seq Scan on bookings b (actual rows=2111110...\n\nBatches: 1\n\nMemory Usage:\n\n(13 rows)\n\n=> RESET enable_parallel_hash;\n\nHere each process hashes the bookings table, then retrieves its own share of outer rows via the Parallel Index Only Scan node, and matches these rows against the resulting hash table.\n\nThe hash table memory limit is applied to each parallel process separately, so the total size of memory allocated for this purpose will be three times bigger than in- dicated in the plan (Memory Usage).\n\nParallel One-Pass Hash Joins\n\nEven though a regular hash join can be quite efficient in parallel plans (especially for small inner sets, for which parallel processing does not make much sense), larger data sets are better handled by a special parallel hash join algorithm.\n\nAn important distinction of the parallel version of the algorithm is that the hash table is created in the shared memory, which is allocated dynamically and can be accessed by all parallel processes that contribute to the join operation. Instead of several separate hash tables, a single common one is built, which uses the total amount of memory dedicated to all the participating processes. It increases the chance of completing the join in one pass.\n\n432\n\n22.1 Hash Joins\n\nAtthefirst stage(representedintheplanbytheParallel Hashnode),alltheparallel processesbuildacommonhashtable,takingadvantageoftheparallelaccesstothe inner set.1\n\nwork_mem×hash_mem_multiplier ×numberofprocesses\n\ninner set\n\nouter set\n\nTo move on from here, each parallel process must complete its share of first-stage processing.2\n\nAt the second stage (the Parallel Hash Join node), the processes are again run in parallel to match their shares of rows of the outer set against the hash table,which is already built by this time.3\n\nouter set\n\nHere is an example of such a plan:\n\n=> SET work_mem = '64MB';\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM bookings b\n\nJOIN tickets t ON t.book_ref = b.book_ref;\n\n1 backend/executor/nodeHash.c, MultiExecParallelHash function 2 backend/storage/ipc/barrier.c 3 backend/executor/nodeHashjoin.c, ExecParallelHashJoin function\n\n433\n\non\n\nv. ��\n\nChapter 22 Hashing\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=3 loops=1)\n\nWorkers Planned: 2 Workers Launched: 2 −> Partial Aggregate (actual rows=1 loops=3)\n\n−> Parallel Hash Join (actual rows=983286 loops=3)\n\nHash Cond: (t.book_ref = b.book_ref) −> Parallel Index Only Scan using tickets_book_ref...\n\nHeap Fetches: 0\n\n−> Parallel Hash (actual rows=703703 loops=3)\n\nBuckets: 4194304 115392kB −> Parallel Seq Scan on bookings b (actual row...\n\nBatches: 1\n\nMemory Usage:\n\n(13 rows)\n\n=> RESET work_mem;\n\nIt is the same query that I showed in the previous section,but the parallel hash join was turned off by the\n\nenable_parallel_hash parameter at that time.\n\nAlthough the available memory is down by half as compared to a regular hash join demonstrated before, the operation still completes in one pass because it uses the memory allocated for all the parallel processes (Memory Usage). The hash table gets a bit bigger, but since it is the only one we have now, the total memory usage has decreased.\n\nParallel Two-Pass Hash Joins\n\nThe consolidated memory of all the parallel processes may still be not enough to accommodate the whole hash table. It can become clear either at the planning stage or later,during query execution. The two-pass algorithm applied in this case is quite different from what we have seen so far.\n\nThe key distinction of this algorithm is that it creates several smaller hash ta- bles instead of a single big one. Each process gets its own table and processes its own batches independently. (But since separate hash tables are still located in the shared memory, any process can get access to any of these tables.) If planning\n\n434\n\n22.1 Hash Joins\n\nshows that more than one batch will be required,1 a separate hash table is built for each process right away. If the decision is taken at the execution stage, the hash table is rebuilt.2\n\nThus, at the first stage processes scan the inner set in parallel, splitting it into batches and writing them into temporary files.3 Since each process reads only its own share of the inner set, none of them builds a full hash table for any of the batches(evenforthefirstone). Thefullsetofrowsofanybatchisonlyaccumulated inthefilewrittenbyalltheparallelprocessesinasynchronizedmanner.4 Sounlike the non-parallel and one-pass parallel versions of the algorithm, the parallel two- pass hash join writes all the batches to disk, including the first one.\n\ninner set\n\nouter set\n\nOnce all the processes have completed hashing of the inner set, the second stage begins.5\n\nIf the non-parallel version of the algorithm were employed, the rows of the outer set that belong to the first batch would be matched against the hash table right away. But in the case of the parallel version, the memory does not contain the hash table yet, so the workers process the batches independently. Therefore, the second stage starts by a parallel scan of the outer set to distribute its rows into batches, and each batch is written into a separate temporary file.6 The scanned\n\n1 backend/executor/nodeHash.c, ExecChooseHashTableSize function 2 backend/executor/nodeHash.c, ExecParallelHashIncreaseNumBatches function 3 backend/executor/nodeHash.c, MultiExecParallelHash function 4 backend/utils/sort/sharedtuplestore.c 5 backend/executor/nodeHashjoin.c, ExecParallelHashJoin function 6 backend/executor/nodeHashjoin.c, ExecParallelHashJoinPartitionOuter function\n\n435\n\nChapter 22 Hashing\n\nrows are not inserted into the hash table (as it happens at the first stage), so the number of batches never rises.\n\nOnce all the processes have completed the scan of the outer set, we get 2N tempo- rary files on disk; they contain the batches of the inner and outer sets.\n\ninner set\n\nouter set\n\nThen each process chooses one of the batches and performs the join: it loads the inner set of rows into a hash table in memory, scans the rows of the outer set, and matches them against the hash table. When the batch join is complete,the process chooses the next batch that has not been processed yet.1\n\ninner set\n\nouter set\n\nIf no more unprocessed batches are left, the process that has completed its own batch starts processing one of the batches that is currently being handled by an- other process; such concurrent processing is possible because all the hash tables are located in the shared memory.\n\n1 backend/executor/nodeHashjoin.c, ExecParallelHashJoinNewBatch function\n\n436\n\n22.1 Hash Joins\n\nouter set\n\nThis approach is more efficient than using a single big hash table for all the pro- cesses: it is easier to set up parallel processing, and synchronization is cheaper.\n\nModifications\n\nThe hash join algorithm supports any types of joins: apart from the inner join, it can also handle left, right, and full outer joins, as well as semi- and anti-joins. But as I have already mentioned, the join condition is limited to the equality operator.\n\nWe have already observed some of these operations loop join. Here is an example of the right outer join:\n\nwhen dealing with the nested\n\n=> EXPLAIN (costs off) SELECT * FROM bookings b\n\nLEFT OUTER JOIN tickets t ON t.book_ref = b.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Right Join\n\nHash Cond: (t.book_ref = b.book_ref) −> Seq Scan on tickets t −> Hash\n\n−> Seq Scan on bookings b\n\n(5 rows)\n\nNote that the logical left join specified in the ��� query got transformed into a physical operation of the right join in the execution plan.\n\nAt the logical level, bookings is the outer table (constituting the left side of the join operation), while the tickets table is the inner one. Therefore, bookings with no tickets must also be included into the join result.\n\n437\n\np. ���\n\nChapter 22 Hashing\n\nAt the physical level, inner and outer sets are assigned based on the cost of the join rather than their location in the query text. It usually means that the set with a smaller hash table will be used as the inner one. This is exactlywhat is happening here: the bookings table is used as the inner set, and the left join is changed to the right one.\n\nAnd vice versa,if the query specifies the right outer join (to display the tickets that are not related to any bookings), the execution plan uses the left join:\n\n=> EXPLAIN (costs off) SELECT * FROM bookings b\n\nRIGHT OUTER JOIN tickets t ON t.book_ref = b.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Left Join\n\nHash Cond: (t.book_ref = b.book_ref) −> Seq Scan on tickets t −> Hash\n\n−> Seq Scan on bookings b\n\n(5 rows)\n\nTo complete the picture, I will provide an example of a query plan with the full outer join:\n\n=> EXPLAIN (costs off) SELECT * FROM bookings b\n\nFULL OUTER JOIN tickets t ON t.book_ref = b.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Full Join\n\nHash Cond: (t.book_ref = b.book_ref) −> Seq Scan on tickets t −> Hash\n\n−> Seq Scan on bookings b\n\n(5 rows)\n\nParallel hash joins are currently not supported for right and full joins.1\n\nNotethatthenextexampleusesthebookingstableastheouterset,buttheplanner would have preferred the right join if it were supported:\n\n1 commitfest.postgresql.org/33/2903\n\n438\n\n22.2 Distinct Values and Grouping\n\n=> EXPLAIN (costs off) SELECT sum(b.total_amount) FROM bookings b\n\nLEFT OUTER JOIN tickets t ON t.book_ref = b.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n−> Gather\n\nWorkers Planned: 2 −> Partial Aggregate\n\n−> Parallel Hash Left Join\n\nHash Cond: (b.book_ref = t.book_ref) −> Parallel Seq Scan on bookings b −> Parallel Hash\n\n−> Parallel Index Only Scan using tickets_book...\n\n(9 rows)\n\n22.2 Distinct Values and Grouping\n\nAlgorithms that group values for aggregation and remove duplicates are very sim- ilar to join algorithms. One of the approaches they can use consists in building a hash table on the required columns. Values are included into the hash table only if it contains no such values yet. As a result, the hash table accumulates all the distinct values.\n\nThe node that performs hash aggregation is called HashAggregate.1\n\nLet’s consider some situations that may require this node.\n\nThe number of seats in each travel class (����� ��):\n\n=> EXPLAIN (costs off) SELECT fare_conditions, count(*) FROM seats GROUP BY fare_conditions;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate\n\nGroup Key: fare_conditions −> Seq Scan on seats\n\n(3 rows)\n\n1 backend/executor/nodeAgg.c\n\n439\n\n4MB 1.0\n\nChapter 22 Hashing\n\nThe list of travel classes (��������):\n\n=> EXPLAIN (costs off) SELECT DISTINCT fare_conditions FROM seats;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate\n\nGroup Key: fare_conditions −> Seq Scan on seats\n\n(3 rows)\n\nTravel classes combined with one more value (�����):\n\n=> EXPLAIN (costs off) SELECT fare_conditions FROM seats UNION SELECT NULL;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate\n\nGroup Key: seats.fare_conditions −> Append\n\n−> Seq Scan on seats −> Result\n\n(5 rows)\n\nThe Append node combines both sets but does not get rid of any duplicates, which must not appear in the ����� result. They have to be removed separately by the HashAggregate node.\n\nThe memory chunk allocated for the hash table is limited by the hash_mem_multiplier value, just like in the case of a hash join.\n\nwork_mem ×\n\nIf the hash table fits the allocated memory, aggregation uses a single batch:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT DISTINCT amount FROM ticket_flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate (actual rows=338 loops=1)\n\nGroup Key: amount Batches: 1 −> Seq Scan on ticket_flights (actual rows=8391852 loops=1)\n\nMemory Usage: 61kB\n\n(4 rows)\n\n440\n\n22.2 Distinct Values and Grouping\n\nThere are not so many distinct values in the amounts field, so the hash table takes only �� k� (Memory Usage).\n\nthe hash table fills up the allocated memory, all the further values are As soon as spilled into temporary files and grouped into partitions based on several bits of their hash values. The number of partitions is a power of two and is chosen in such a way that each of their hash tables fits the allocated memory. The accuracy of the estimation is of course dependent on the quality of the collected statistics, so the received number is multiplied by �.� to further reduce partition sizes and raise the chances of processing each partition in one pass.1\n\nOnce the whole set is scanned, the node returns aggregation results for those val- ues that have made it into the hash table.\n\nThen the hash table is cleared,and each of the partitions saved into temporary files at the previous stage is scanned and processed just like any other set of rows. If the hash table still exceeds the allocated memory,the rows that are subject to overflow will be partitioned again and written to disk for further processing.\n\nTo avoid excessive �/�, the two-pass hash join algorithm moves ���s into the first batch. Aggregation, however, does not require this optimization: those rows that fit the allocated memory will not be split into partitions, and ���s are likely to occur early enough to get into ���.\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT DISTINCT flight_id FROM ticket_flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate (actual rows=150588 loops=1)\n\nGroup Key: flight_id Batches: 5 −> Seq Scan on ticket_flights (actual rows=8391852 loops=1)\n\nMemory Usage: 4145kB\n\nDisk Usage: 98184kB\n\n(4 rows)\n\nIn this example,the number of distinct ��s is relatively high,so the hash table does not fit the allocated memory. It takes five batches to perform the query: one for the initial data set and four for the partitions written to disk.\n\n1 backend/executor/nodeAgg.c, hash_choose_num_partitions function\n\n441\n\nv. ��\n\n23\n\nSorting and Merging\n\n23.1 Merge Joins\n\nA merge join processes data sets sorted by the join key and returns the result that is sorted in a similar way. Input sets may come pre-sorted following an index scan; otherwise, the executor has to sort them before the actual merge begins.1\n\nMerging Sorted Sets\n\nLet’s take a look at an example of a merge join; it is represented in the execution plan by the Merge Join node:2\n\n=> EXPLAIN (costs off) SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nORDER BY t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nMerge Join\n\nMerge Cond: (t.ticket_no = tf.ticket_no) −> Index Scan using tickets_pkey on tickets t −> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(4 rows)\n\nTheoptimizerprefersthisjoinmethodbecauseitreturnsasortedresult,asdefined by the ����� �� clause. When choosing a plan, the optimizer notes the sort order of the data sets and does not perform any sorting unless it is really required. For\n\n1 backend/optimizer/path/joinpath.c, generate_mergejoin_paths function 2 backend/executor/nodeMergejoin.c\n\n442\n\n23.1 Merge Joins\n\nexample, if the data set produced by a merge join already has an appropriate sort order, it can be used in the subsequent merge join as is:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON t.ticket_no = tf.ticket_no JOIN boarding_passes bp ON bp.ticket_no = tf.ticket_no AND bp.flight_id = tf.flight_id\n\nORDER BY t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nMerge Join\n\nMerge Cond: (tf.ticket_no = t.ticket_no) −> Merge Join\n\nMerge Cond: ((tf.ticket_no = bp.ticket_no) AND (tf.flight_... −> Index Scan using ticket_flights_pkey on ticket_flights tf −> Index Scan using boarding_passes_pkey on boarding_passe...\n\n−> Index Scan using tickets_pkey on tickets t\n\n(7 rows)\n\nThe first tables to be joined are ticket_flights and boarding_passes; both of them have a composite primary key (ticket_no,flight_id), and the result is sorted by these two columns. The produced set of rows is then joined with the tickets table, which is sorted by the ticket_no column.\n\nThejoinrequiresonlyonepassoverbothdatasetsanddoesnottakeanyadditional memory. It uses two pointers to the current rows (which are originally the first ones) of the inner and outer sets.\n\nIf the keys of the current rows do not match, one of the pointers (that references the row with the smaller key) is going to be advanced to the next row until it finds a match. The joined rows are returned to the upper node, and the pointer of the inner set is advanced by one place. The operation continues until one of the sets is over.\n\nThis algorithm copes with duplicates of the inner set,but the outer set can contain them too. Therefore, the algorithm has to be improved: if the key remains the same after the outer pointer is advanced, the inner pointer gets back to the first matching row. Thus, each row of the outer set will be matched to all the rows of the inner set with the same key.1\n\n1 backend/executor/nodeMergejoin.c, ExecMergeJoin function\n\n443\n\np. ���\n\nChapter 23 Sorting and Merging\n\nFor the outer join,the algorithm is further tweaked a bit,but it is still based on the same principle.\n\nMerge join conditions can use only the equality operator, which means that only equi-joins are supported (although support for other condition types is currently under way too).1\n\nCost estimation. Let’s take a closer look at the previous example:\n\n=> EXPLAIN SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nORDER BY t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nMerge Join\n\n(cost=0.99..822355.54 rows=8391852 width=136)\n\nMerge Cond: (t.ticket_no = tf.ticket_no) −> Index Scan using tickets_pkey on tickets t\n\n(cost=0.43..139110.29 rows=2949857 width=104)\n\n−> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..570972.46 rows=8391852 width=32)\n\n(6 rows)\n\nThe startup cost of the join includes at least the startup costs of all the child nodes.\n\nIn general,it may be required to scan some fraction of the outer or inner set before the first match is found. It is possible to estimate this fraction by comparing (based )thesmallestjoinkeysinthetwosets.2 Butinthisparticularcase, onthehistogram the range of ticket numbers is the same in both tables.\n\nThe total cost comprises the cost of fetching the data from the child nodes and the computation cost.\n\nSince the join algorithm stops as soon as one of the sets is over (unless the outer join is performed, of course), the other set may be scanned only partially. To esti- mate the size of the scanned part, we can compare the maximal key values in the two sets. In this example, both sets will be read in full, so the total cost of the join includes the sum of the total costs of both child nodes.\n\n1 For example, see commitfest.postgresql.org/33/3160 2 backend/utils/adt/selfuncs.c, mergejoinscansel function\n\n444\n\n23.1 Merge Joins\n\nMoreover, if there are any duplicates, some of the rows of the inner set may be scanned several times. The estimated number of repeat scans equals the difference between the cardinalities of the join result and the inner set.1 In this query, these cardinalities are the same, which means that the sets contain no duplicates.\n\nThe algorithm compares join keys of the two sets. The cost of one comparison is cpu_operator_cost value, while the estimated number of compar- estimated at the isons can be taken as the sum of rows of both sets (increased by the number of repeat reads caused by duplicates). The processing cost of each row included into the result is estimated at the\n\ncpu_tuple_cost value, as usual.\n\nThus, in this example the cost of the join is estimated as follows:2\n\n=> SELECT 0.43 + 0.56 AS startup,\n\nround((\n\n139110.29 + 570972.46 + current_setting('cpu_tuple_cost')::real * 8391852 + current_setting('cpu_operator_cost')::real * (2949857 + 8391852)\n\n)::numeric, 2) AS total;\n\nstartup |\n\ntotal\n\n−−−−−−−−−+−−−−−−−−−−−\n\n0.99 | 822355.54\n\n(1 row)\n\nParallel Mode\n\nAlthoughthemergejoin hasnoparallel flavor,itcan stillbeusedin parallelplans.3\n\nThe outer set can be scanned by several workers in parallel, but the inner set is always scanned by each worker in full.\n\nSince the parallel hash join\n\nis almost always cheaper, I will turn it off for a while:\n\n=> SET enable_hashjoin = off;\n\nHere is an example of a parallel plan that uses a merge join:\n\n1 backend/optimizer/path/costsize.c, final_cost_mergejoin function 2 backend/optimizer/path/costsize.c, initial_cost_mergejoin & final_cost_mergejoin functions 3 backend/optimizer/path/joinpath.c, consider_parallel_mergejoin function\n\n445\n\n0.0025\n\n0.01\n\nv. �.�\n\np. ���\n\nChapter 23 Sorting and Merging\n\n=> EXPLAIN (costs off) SELECT count(*), sum(tf.amount) FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n−> Gather\n\nWorkers Planned: 2 −> Partial Aggregate −> Merge Join\n\nMerge Cond: (tf.ticket_no = t.ticket_no) −> Parallel Index Scan using ticket_flights_pkey o... −> Index Only Scan using tickets_pkey on tickets t\n\n(8 rows)\n\nFull and right outer merge joins are not allowed in parallel plans.\n\nModifications\n\nThe merge join algorithm can be used with any types of joins. The only restriction is that join conditions of full and right outer joins must contain merge-compatible expressions (“outer-column equals inner-column”or“column equals constant”).1 In- ner and left outer joins simply filter the join result by irrelevant conditions,but for full and right joins such filtering is inapplicable.\n\nHere is an example of a full join that uses the merge algorithm:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets t\n\nFULL JOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nORDER BY t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\nSort Key: t.ticket_no −> Merge Full Join\n\nMerge Cond: (t.ticket_no = tf.ticket_no) −> Index Scan using tickets_pkey on tickets t −> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(6 rows)\n\n1 backend/optimizer/path/joinpath.c, select_mergejoin_clauses function\n\n446\n\n23.2 Sorting\n\nInner and left merge joins preserve the sort order. Full and right outer joins, how- ever, cannot guarantee it because ���� values can be wedged in between the or- dered values of the outer set, which breaks the sort order.1 To restore the required order, the planner introduces the Sort node here. Naturally, it increases the cost of the plan, making the hash join more attractive, so the planner has selected this plan only because hash joins are currently disabled.\n\nBut the next example cannot do without a hash join: the nested loop does not allow full joins at all, while merging cannot be used because of an unsupported join condition. So the hash join is used regardless of the enable_hashjoin parameter value:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets t\n\nFULL JOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nAND tf.amount > 0\n\nORDER BY t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\nSort Key: t.ticket_no −> Hash Full Join\n\nHash Cond: (tf.ticket_no = t.ticket_no) Join Filter: (tf.amount > '0'::numeric) −> Seq Scan on ticket_flights tf −> Hash\n\n−> Seq Scan on tickets t\n\n(8 rows)\n\nLet’s restore the ability to use hash joins that we have previously disabled:\n\n=> RESET enable_hashjoin;\n\n23.2 Sorting\n\nIf one of the sets (or possibly both of them) is not sorted by the join key, it must be reordered before the join operation begins. This sorting operation is represented in the plan by the Sort node:2\n\n1 backend/optimizer/path/pathkeys.c, build_join_pathkeys function 2 backend/executor/nodeSort.c\n\n447\n\nChapter 23 Sorting and Merging\n\n=> EXPLAIN (costs off) SELECT * FROM flights f\n\nJOIN airports_data dep ON f.departure_airport = dep.airport_code\n\nORDER BY dep.airport_code;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nMerge Join\n\nMerge Cond: (f.departure_airport = dep.airport_code) −> Sort\n\nSort Key: f.departure_airport −> Seq Scan on flights f\n\n−> Sort\n\nSort Key: dep.airport_code −> Seq Scan on airports_data dep\n\n(8 rows)\n\nSuch sorting can also be applied outside the context of joins if the ����� �� clause is specified, both in a regular query and within a window function:\n\n=> EXPLAIN (costs off) SELECT flight_id,\n\nrow_number() OVER (PARTITION BY flight_no ORDER BY flight_id)\n\nFROM flights f;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nWindowAgg −> Sort\n\nSort Key: flight_no, flight_id −> Seq Scan on flights f\n\n(4 rows)\n\nHere the WindowAgg node1 computes a window function on the data set that has been pre-sorted by the Sort node.\n\nThe planner has several sort methods in its toolbox. The example that I have al- ready shown uses two of them (Sort Method). These details can be displayed by the ������� ������� command, as usual:\n\n=> EXPLAIN (analyze,costs off,timing off,summary off) SELECT * FROM flights f\n\nJOIN airports_data dep ON f.departure_airport = dep.airport_code\n\nORDER BY dep.airport_code;\n\n1 backend/executor/nodeWindowAgg.c\n\n448\n\n23.2 Sorting\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nMerge Join (actual rows=214867 loops=1)\n\nMerge Cond: (f.departure_airport = dep.airport_code) −> Sort (actual rows=214867 loops=1) Sort Key: f.departure_airport Sort Method: external merge −> Seq Scan on flights f (actual rows=214867 loops=1)\n\nDisk: 17136kB\n\n−> Sort (actual rows=104 loops=1) Sort Key: dep.airport_code Sort Method: quicksort −> Seq Scan on airports_data dep (actual rows=104 loops=1)\n\nMemory: 52kB\n\n(10 rows)\n\nQuicksort\n\nwork_mem chunk,the classic quicksort method is If the data set to be sorted fits the applied. This algorithm is described in all textbooks, so I am not going to explain it here.\n\nAs for the implementation, sorting is performed by a dedicated component1 that chooses the most suitable algorithm depending on the amount of available mem- ory and some other factors.\n\nCost estimation. Let’s take a look at how a small table is sorted. In this case, sort- ing is performed in memory using the quicksort algorithm:\n\n=> EXPLAIN SELECT * FROM airports_data ORDER BY airport_code;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\n(cost=7.52..7.78 rows=104 width=145)\n\nSort Key: airport_code −> Seq Scan on airports_data\n\n(cost=0.00..4.04 rows=104 width=...\n\n(3 rows)\n\n1 backend/utils/sort/tuplesort.c\n\n449\n\n4MB\n\n0.0025\n\nChapter 23 Sorting and Merging\n\nThe computational complexity of sorting n values is known to be O(nlog2 n). A cpu_operator_cost value. single comparison operation is estimated at the doubled Since the whole data set must be scanned and sorted before the result can be re- trieved, the startup cost of sorting includes the total cost of the child node and all the expenses incurred by comparison operations.\n\nThe total cost of sorting also includes the cost of processing each row to be re- turned,which is estimated at cpu_operator_cost (and not at the usual cpu_tuple_cost value, as the overhead incurred by the Sort node is insignificant).1\n\nFor this example, the costs are calculated as follows:\n\n=> WITH costs(startup) AS (\n\nSELECT 4.04 + round((\n\ncurrent_setting('cpu_operator_cost')::real * 2 *\n\n104 * log(2, 104)\n\n)::numeric, 2)\n\n) SELECT startup,\n\nstartup + round((\n\ncurrent_setting('cpu_operator_cost')::real * 104\n\n)::numeric, 2) AS total\n\nFROM costs;\n\nstartup | total −−−−−−−−−+−−−−−−−\n\n7.52 |\n\n7.78\n\n(1 row)\n\nTop-N Heapsort\n\nIf a data set needs to be sorted only partially (as defined by the ����� clause), the heapsort method can be applied (it is represented in the plan as top-N heapsort). To be more exact, this algorithm is used if sorting reduces the number of rows at least by half, or if the allocated memory cannot accommodate the whole input set (while the output set fits it).\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT * FROM seats ORDER BY seat_no LIMIT 100;\n\n1 backend/optimizer/path/costsize.c, cost_sort function\n\n450",
      "page_number": 421
    },
    {
      "number": 23,
      "title": "Sorting and Merging",
      "start_page": 445,
      "end_page": 470,
      "detection_method": "regex_chapter",
      "content": "23.2 Sorting\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nLimit\n\n(cost=72.57..72.82 rows=100 width=15)\n\n(actual rows=100 loops=1) −> Sort\n\n(cost=72.57..75.91 rows=1339 width=15)\n\n(actual rows=100 loops=1) Sort Key: seat_no Sort Method: top−N heapsort −> Seq Scan on seats\n\nMemory: 33kB\n\n(cost=0.00..21.39 rows=1339 width=15)\n\n(actual rows=1339 loops=1)\n\n(8 rows)\n\nTo find k highest (or lowest) values out of n, the executor adds the first k rows into a data structure called heap. Then the rest of the rows get added one by one, and the smallest (or largest) value is removed from the heap after each iteration. Once all the rows are processed, the heap contains k sought-after values.\n\nThe heap term here denotes a well-known data structure and has nothing to do with database tables, which are often referred to by the same name.\n\nCost estimation. The computational complexity of the algorithm is estimated at O(nlog2 k), but each particular operation is more expensive as compared to the quicksort algorithm. Therefore, the formula uses nlog2 2k.1\n\n=> WITH costs(startup) AS (\n\nSELECT 21.39 + round((\n\ncurrent_setting('cpu_operator_cost')::real * 2 *\n\n1339 * log(2, 2 * 100)\n\n)::numeric, 2)\n\n) SELECT startup,\n\nstartup + round((\n\ncurrent_setting('cpu_operator_cost')::real * 100\n\n)::numeric, 2) AS total\n\nFROM costs;\n\nstartup | total −−−−−−−−−+−−−−−−−\n\n72.57 | 72.82\n\n(1 row)\n\n1 backend/optimizer/path/costsize.c, cost_sort function\n\n451\n\nChapter 23 Sorting and Merging\n\nExternal Sorting\n\nIf the scan shows that the data set is too big to be sorted in memory, the sorting node switches over to external merge sorting (labeled as external merge in the plan).\n\nThe rows that are already scanned are sorted in memory by the quicksort algorithm and written into a temporary file.\n\n1\n\n2\n\n3\n\n4\n\n5\n\n1\n\nSubsequent rows are then read into the freed memory, and this procedure is re- peated until all the data is written into several pre-sorted files.\n\n2\n\n3\n\n4\n\n5\n\n1\n\n2\n\nNext, these files are merged into one. This operation is performed by roughly the same algorithm that is used for merge joins; the main difference is that it can pro- cess more than two files at a time.\n\nA merge operation does not need too much memory. In fact, it is enough to have room for one row per file. The first rows are read from each file, the row with the lowest value (or the highest one, depending on the sort order) is returned as a partial result, and the freed memory is filled with the next row fetched from the same file.\n\n452\n\n23.2 Sorting\n\nIn practice, rows are read in batches of �� pages rather than one by one, which reduces the number of �/� operations. The number of files that are merged in a single iteration depends on the available memory, but it is never smaller than six. The upper boundary is also limited (by ���) since efficiency suffers when there are too many files.1\n\nSorting algorithms have long-established terminology. External sorting was originally performed using magnetic tapes,and Postgre��� keeps a similar name for the component that controls temporary files.2 Partially sorted data sets are called “runs.”3 The number of runs participating in the merge is referred to as the“merge order.” I did not use these terms, but they are worth knowing if you want to understand Postgre��� code and comments.\n\nIfthesortedtemporaryfilescannotbemergedallatonce,theyhavetobeprocessed in several passes,their partial results being written into new temporary files. Each iteration increases the volume of data to be read and written, so the more ��� is available, the faster the external sorting completes.\n\n1\n\n2\n\n3\n\n4\n\n5\n\n1+2+3\n\n4\n\n5\n\n1+2+3\n\n4+5\n\n1 backend/utils/sort/tuplesort.c, tuplesort_merge_order function 2 backend/utils/sort/logtape.c 3 Donald E. Knuth. The Art of Computer Programming. Volume III. Sorting and Searching\n\n453\n\nChapter 23 Sorting and Merging\n\nThe next iteration merges newly created temporary files.\n\n1+2+3\n\n4+5\n\nThe final merge is typically deferred and performed on the fly when the upper node pulls the data.\n\nLet’srunthe��������������commandtoseehowmuchdiskspacehasbeenusedby external sorting. The ������� option displays buffer usage statistics for temporary files (temp read and written). The number of written buffers will be (roughly) the same as the number of read ones; converted to kilobytes, this value is shown as Disk in the plan:\n\n=> EXPLAIN (analyze, buffers, costs off, timing off, summary off) SELECT * FROM flights ORDER BY scheduled_departure;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort (actual rows=214867 loops=1) Sort Key: scheduled_departure Sort Method: external merge Buffers: shared hit=2627, temp read=2142 written=2150 −> Seq Scan on flights (actual rows=214867 loops=1)\n\nDisk: 17136kB\n\nBuffers: shared hit=2624\n\n(6 rows)\n\nTo print more details on using temporary files into the server log, you can enable the log_temp_files parameter.\n\nCost estimation. Let’s take the same plan with external sorting as an example:\n\n=> EXPLAIN SELECT * FROM flights ORDER BY scheduled_departure;\n\n454\n\n23.2 Sorting\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\n(cost=31883.96..32421.12 rows=214867 width=63)\n\nSort Key: scheduled_departure −> Seq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=63)\n\n(3 rows)\n\nHere the regular cost of comparisons (their number is the same as in the case of a quicksort operation in memory) is extended by the �/� cost.1 All the input data has to be first written into temporary files on disk and then read from disk during the merge operation (possibly more than once if all the created files cannot be merged in one iteration).\n\nIt is assumed that three quarters of disk operations (both reads and writes) are sequential, while one quarter is random.\n\nThe volume of data written to disk depends on the number of rows to be sorted and the number of columns used in the query.2 In this example,the query displays all the columns of the flights table, so the size of the data spilled to disk is almost the same as the size of the whole table if its tuple and page metadata are not taken into account (���� pages instead of ����).\n\nHere sorting is completed in one iteration.\n\nTherefore, the sorting cost is estimated in this plan as follows:\n\n=> WITH costs(startup) AS ( SELECT 4772.67 + round((\n\ncurrent_setting('cpu_operator_cost')::real * 2 *\n\n214867 * log(2, 214867) +\n\n(current_setting('seq_page_cost')::real * 0.75 +\n\ncurrent_setting('random_page_cost')::real * 0.25) *\n\n2 * 2309 * 1 -- one iteration\n\n)::numeric, 2)\n\n) SELECT startup,\n\nstartup + round((\n\ncurrent_setting('cpu_operator_cost')::real * 214867\n\n)::numeric, 2) AS total\n\nFROM costs;\n\n1 backend/optimizer/path/costsize.c, cost_sort function 2 backend/optimizer/path/costsize.c, relation_byte_size function\n\n455\n\nv. ��\n\nChapter 23 Sorting and Merging\n\n| −−−−−−−−−−+−−−−−−−−−− 31883.96 | 32421.13\n\nstartup\n\ntotal\n\n(1 row)\n\nIncremental Sorting\n\nIf a data set has to be sorted by keys K1 …Km …Kn, and this data set is known to be already sorted by the first m keys, you do not have to re-sort it from scratch. Instead,you can split this set into groups by the same first keys K1 …Km (values in these groups already follow the defined order), and then sort each of these groups separately by the remaining Km+1 …Kn keys. This method is called the incremental sort.\n\nIncremental sorting is less memory-intensive than other sorting algorithms, as it splits the set into several smaller groups; besides, it allows the executor to start returning results after the first group is processed, without waiting for the whole set to be sorted.\n\nInPostgre���,theimplementationisabitmoresubtle:1 whilerelativelybiggroups of rows are processed separately, smaller groups are combined together and are sorted in full. It reduces the overhead incurred by invoking the sorting procedure.2\n\nThe execution plan represents incremental sorting by the Incremental Sort node:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM bookings ORDER BY total_amount, book_date;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIncremental Sort (actual rows=2111110 loops=1)\n\nSort Key: total_amount, book_date Presorted Key: total_amount Full−sort Groups: 2823 Memory: 30kB Pre−sorted Groups: 2624\n\nSort Method: quicksort\n\nPeak Memory: 30kB\n\nSort Method: quicksort\n\nAverage\n\nAverage\n\n1 backend/executor/nodeIncrementalSort.c 2 backend/utils/sort/tuplesort.c\n\n456\n\n23.2 Sorting\n\nMemory: 3152kB −> Index Scan using bookings_total_amount_idx on bookings (ac...\n\nPeak Memory: 3259kB\n\n(8 rows)\n\nAs the plan shows, the data set is pre-sorted by the total_amount field, as it is the result of an index scan run on this column (Presorted Key). The ������� ������� command also displays run-time statistics. The Full-sort Groups row is related to small groups that were united to be sorted in full, while the Presorted Groups row displays the data on large groups with partially ordered data, which required in- cremental sorting by the book_date column only. In both cases, the in-memory quicksort method was applied. The difference in group sizes is due to non-uniform distribution of booking costs.\n\nIncremental sorting\n\ncan be used to compute window functions too:\n\n=> EXPLAIN (costs off) SELECT row_number() OVER (ORDER BY total_amount, book_date) FROM bookings;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nWindowAgg\n\n−> Incremental Sort\n\nSort Key: total_amount, book_date Presorted Key: total_amount −> Index Scan using bookings_total_amount_idx on bookings\n\n(5 rows)\n\nCost estimation. Cost calculations for incremental sorting1 are based on the ex- pectednumberofgroups2 andtheestimatedsortingcostofanaverage-sizedgroup (which we have already reviewed).\n\nThe startup cost reflects the cost estimation of sorting the first group,which allows the node to start returning sorted rows; the total cost includes the sorting cost of all groups.\n\nWe are not going to explore these calculations any further here.\n\n1 backend/optimizer/path/costsize.c, cost_incremental_sort function 2 backend/utils/adt/selfuncs.c, estimate_num_groups function\n\n457\n\nv. ��\n\nv. ��\n\np. ��� 1000\n\nChapter 23 Sorting and Merging\n\nParallel Mode\n\nSorting can also be performed concurrently. But although parallel workers do pre- sort their data shares, the Gather node knows nothing about their sort order and can only accumulate them on a first-come, first-serve basis. To preserve the sort order, the executor has to apply the Gather Merge node.1\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM flights ORDER BY scheduled_departure LIMIT 10;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nLimit (actual rows=10 loops=1)\n\n−> Gather Merge (actual rows=10 loops=1)\n\nWorkers Planned: 1 Workers Launched: 1 −> Sort (actual rows=7 loops=2)\n\nSort Key: scheduled_departure Sort Method: top−N heapsort Worker 0: −> Parallel Seq Scan on flights (actual rows=107434 lo...\n\nMemory: 27kB\n\nSort Method: top−N heapsort\n\nMemory: 27kB\n\n(9 rows)\n\nThe Gather Merge node uses a binary heap2 to adjust the order of rows fetched by several workers. It virtually merges several sorted sets of rows, just like external sorting would do, but is designed for a different use case: Gather Merge typically handles a small fixed number of data sources and fetches rows one by one rather than block by block.\n\nCost estimation. The startup cost of the Gather Merge node is based on the startup cost of its child node. Just like for the Gather node , this value is increased by the cost of launching parallel processes (estimated at\n\nparallel_setup_cost).\n\n1 backend/executor/nodeGatherMerge.c 2 backend/lib/binaryheap.c\n\n458\n\n23.2 Sorting\n\nThe received value is then further extended by the cost of building a binary heap, which requires sorting n values, where n is the number of parallel workers (that is, cpu_operator_cost, nlog2 n). A single comparison operation is estimated at doubled and total share of such operations is typically negligible since n is quite small.\n\nThe total cost includes the expenses incurred by fetching all the data by several processesthatperformtheparallelpartoftheplan,andthecostoftransferringthis parallel_tuple_cost increased datatotheleader. Asinglerowtransferisestimatedat by �%, to compensate for possible waits on getting the next values.\n\nThe expenses incurred by binary heap updates must also be taken into account in total cost calculations: each input row requires log2 n comparison operations and certain additional actions (they are estimated at cpu_operator_cost).1\n\nLet’s take a look at yet another plan that uses the Gather Merge node. Note that the , and then the Sort node workers here first perform partial aggregation by hashing sorts the received results (it is cheap because few rows are left after aggregation) to be passed further to the leader process, which gathers the full result in the Gather Merge node. As forthefinal aggregation,it is performedon the sortedlist of values:\n\n=> EXPLAIN SELECT amount, count(*) FROM ticket_flights GROUP BY amount;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize GroupAggregate\n\n(cost=123399.62..123485.00 rows=337 wid...\n\nGroup Key: amount −> Gather Merge\n\n(cost=123399.62..123478.26 rows=674 width=14)\n\nWorkers Planned: 2 −> Sort\n\n(cost=122399.59..122400.44 rows=337 width=14)\n\nSort Key: amount −> Partial HashAggregate Group Key: amount −> Parallel Seq Scan on ticket_flights\n\n(cost=122382.07..122385.44 r...\n\n(cost=0.00...\n\n(9 rows)\n\nHere we have three parallel processes (including the leader), and the cost of the Gather Merge node is calculated as follows:\n\n1 backend/optimizer/path/costsize.c, cost_gather_merge function\n\n459\n\n0.0025\n\n0.1\n\np. ���\n\nChapter 23 Sorting and Merging\n\n=> WITH costs(startup, run) AS (\n\nSELECT round((\n\n-- launching processes current_setting('parallel_setup_cost')::real + -- building the heap current_setting('cpu_operator_cost')::real * 2 * 3 * log(2, 3)\n\n)::numeric, 2), round((\n\n-- passing rows current_setting('parallel_tuple_cost')::real * 1.05 * 674 + -- updating the heap current_setting('cpu_operator_cost')::real * 2 * 674 * log(2, 3) + current_setting('cpu_operator_cost')::real * 674\n\n)::numeric, 2)\n\n) SELECT 122399.59 + startup AS startup, 122400.44 + startup + run AS total\n\nFROM costs;\n\n| −−−−−−−−−−−+−−−−−−−−−−− 123399.61 | 123478.26\n\nstartup\n\ntotal\n\n(1 row)\n\n23.3 Distinct Values and Grouping\n\nAs we have just seen, grouping values to perform aggregation (and to eliminate duplicates) can be performed not only by hashing, but also by sorting. In a sorted list, groups of duplicate values can be singled out in one pass.\n\nRetrieval of distinct values from a sorted list is represented in the plan by a very simple node called Unique1:\n\n=> EXPLAIN (costs off) SELECT DISTINCT book_ref FROM bookings ORDER BY book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nResult\n\n−> Unique\n\n−> Index Only Scan using bookings_pkey on bookings\n\n(3 rows)\n\n1 backend/executor/nodeUnique.c\n\n460\n\n23.3 Distinct Values and Grouping\n\nAggregation is performed in the GroupAggregate node:1\n\n=> EXPLAIN (costs off) SELECT book_ref, count(*) FROM bookings GROUP BY book_ref ORDER BY book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nGroupAggregate\n\nGroup Key: book_ref −> Index Only Scan using bookings_pkey on bookings\n\n(3 rows)\n\nIn parallel plans, this node is called Partial GroupAggregate, while the node that completes aggregation is called Finalize GroupAggregate.\n\ncan be combined in a single node if group- Both hashing and sorting strategies ing is performed by several column sets (specified in the �������� ����, ����, or ������ clauses). Without getting into rather complex details of this algorithm, I will simply provide an example that performs grouping by three different columns in conditions of scarce memory:\n\n=> SET work_mem = '64kB';\n\n=> EXPLAIN (costs off) SELECT count(*) FROM flights GROUP BY GROUPING SETS (aircraft_code, flight_no, departure_airport);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nMixedAggregate\n\nHash Key: departure_airport Group Key: aircraft_code Sort Key: flight_no\n\nGroup Key: flight_no\n\n−> Sort\n\nSort Key: aircraft_code −> Seq Scan on flights\n\n(8 rows)\n\n=> RESET work_mem;\n\nHere is what happens while this query is being executed. The aggregation node, which is shown in the plan as MixedAggregate, receives the data set sorted by the aircraft_code column.\n\n1 backend/executor/nodeAgg.c, agg_retrieve_direct function\n\n461\n\nv. ��\n\nChapter 23 Sorting and Merging\n\nFirst, this set is scanned, and the values are grouped by the aircraft_code column (Group Key). As the scan progresses,the rows are reordered by the flight_no column (like it is done by a regular Sort node: either via the quicksort method if the mem- ory is sufficient, or using external sorting on disk); at the same time, the executor places these rows into a hash table that uses departure_airport as its key (like it is done by hash aggregation: either in memory, or using temporary files).\n\nAt the second stage, the executor scans the data set that has just been sorted by the flight_no column and groups the values by the same column (Sort Key and the nested Group Key node). If the rows had to be grouped by yet another column,they would be resorted again as required.\n\nFinally, the hash table prepared at the first stage is scanned, and the values are grouped by the departure_airport column (Hash Key).\n\n23.4 Comparison of Join Methods\n\nAs we have seen, two data sets can be joined using three different methods, and each of them has its own pros and cons.\n\nThenestedloopjoindoesnothaveanyprerequisitesandcanstartreturningthefirst rows of the result set right away. It is the only join method that does not have to fully scan the inner set (as long as index access is available for it). These properties make the nested loop algorithm (combined with indexes) an ideal choice for short ���� queries, which deal with rather small sets of rows.\n\nThe weak point of the nested loop becomes apparent as the data volume grows. For a Cartesian product, this algorithm has quadratic complexity—the cost is propor- tionate to the product of sizes of the data sets being joined. However,the Cartesian product is not so common in practice; for each row of the outer set, the executor typically accesses a certain number of rows of the inner set using an index, and this average number does not depend on the total size of the data set (for exam- ple, an average number of tickets in a booking does not change as the number of bookings and bought tickets grows). Thus, the complexity of the nested loop al- gorithm often shows linear growth rather than quadratic one, even if with a high linear coefficient.\n\n462\n\n23.4 Comparison of Join Methods\n\nAn important distinction of the nested loop algorithm is its universal applicability: it supports all join conditions, whereas other methods can only deal with equi- joins. It allows running queries with any types of conditions (except for the full join, which cannot be used with the nested loop), but you must keep in mind that a non-equi-join of a large data set is highly likely to be performed slower than desired.\n\nA hash join works best on large data sets. If ��� is sufficient, it requires only one pass over two data sets,so its complexity is linear. Combined with sequential table scans, this algorithm is typically used for ���� queries, which compute the result based on a large volume of data.\n\nHowever, if the response time is more important than throughput, a hash join is not the best choice: it will not start returning the resulting rows until the whole hash table is built.\n\nThe hash join algorithm is only applicable to equi-joins. Another restriction is that the data type of the join key must support hashing (but almost all of them do).\n\nThenestedloopjoin cansometimesbeatthehashjoin,takingadvantageofcaching the rows of the inner set in the Memoize node (which is also based on a hash table). While the hash join always scans the inner set in full, the nested loop algorithm does not have to, which may result in some cost reduction.\n\nA merge join can perfectly handle both short ���� queries and long ���� ones. It has linear complexity (the sets to be joined have to be scanned only once), does not require much memory, and returns the results without any preprocessing; however, the data sets must already have the required sort order. The most cost- effective way to do it is to fetch the data via an index scan. It is a natural choice if the row count is low; for larger data sets,index scans can still be efficient,but only if the heap access is minimal or does not happen at all.\n\nIf no suitable indexes are available, the sets have to be sorted, but this operation is memory-intensive, and its complexity is higher than linear: O(nlog2 n). In this case,a hash join is almost always cheaper than a merge join—unless the result has to be sorted.\n\nAn added bonus of a merge join is the equivalence of the inner and outer sets. The efficiency of both nested loop and hash joins is highly dependent on whether the planner can assign inner and outer sets correctly.\n\n463\n\nv. ��\n\nChapter 23 Sorting and Merging\n\nMerge joins are limited to equi-joins. Besides, the data type must have a �-tree operator class.\n\nThe following graph illustrates approximate dependencies between the costs of various join methods and the fraction of rows to be joined.\n\ncost\n\nmergejoin+sort\n\nnestedloop\n\nhashjoin\n\nmerge join + index\n\nselectivity\n\n0\n\n1\n\nIftheselectivityishigh,thenestedloopjoinusesindexaccessforbothtables; then the planner switches to the full scan of the outer table, which is reflected by the linear part of the graph.\n\nHere the hash join is using a full scan for both tables. The “step” on the graph corresponds to the moment when the hash table fills the whole memory and the batches start getting spilled to disk.\n\nIf an index scan is used, the cost of a merge join shows small linear growth. If the work_mem size is big enough,a hash join is usually more efficient,but a merge join beats it when it comes to temporary files.\n\nThe upper graph of the sort-merge join shows that the costs rise when indexes are unavailable and the data has to be sorted. Just like in the case of a hash join, the\n\n464\n\n23.4 Comparison of Join Methods\n\n“step”on the graph is caused by insufficient memory,as it leads to using temporary files for sorting.\n\nIt is merely an example; in each particular case the ratio between the costs will be different.\n\n465\n\nPart V\n\nTypes of Indexes\n\n24\n\nHash\n\n24.1 Overview\n\nA hash index1 provides the ability to quickly find a tuple �� (���) by a particular index key. Roughly speaking, it is simply a hash table stored on disk. The only operation supported by a hash index is search by the equality condition.\n\nWhen a value is inserted into an index,2 the hash function of the index key is computed. In Postgre���, hash functions return ��-bit or ��-bit integers; several lowest bits of these values are used as the number of the corresponding bucket. The ��� and the hash code of the key are added into the chosen bucket. The key itself is not stored in the index because it is more convenient to deal with small fixed-length values.\n\nThehashtableofanindexisexpandeddynamically.3 Theminimalnumberofbuck- ets is two. As the number of indexed tuples grows,one of the buckets gets split into two. This operation uses one more bit of the hash code, so the elements are redis- tributed only between the two buckets resulting from the split; the composition of other buckets of the hash table remains the same.4\n\nThe index search operation5 calculates the hash function of the index key and the corresponding bucket number. Of all the bucket contents, the search will return only those ���s that correspond to the hash code of the key. As bucket elements\n\n1 postgresql.org/docs/14/hash-index.html\n\nbackend/access/hash/README 2 backend/access/hash/hashinsert.c 3 backend/access/hash/hashpage.c, _hash_expandtable function 4 backend/access/hash/hashpage.c, _hash_getbucketbuf_from_hashkey function 5 backend/access/hash/hashsearch.c\n\n469\n\np. ���\n\nv. ��\n\nv. ��\n\nChapter 24 Hash\n\nare ordered by the keys’ hash codes, binary search can return matching ���s quite efficiently.\n\nSince keys are not stored in the hash table, the index access method may return redundant ���s because of hash collisions. Therefore, the indexing engine has to recheck all the results fetched by the access method. An index-only scan is not supported for the same reason.\n\n24.2 Page Layout\n\nUnlike a regular hash table, the hash index is stored on disk. Therefore, all the data has to be arranged into pages, preferably in such a way that index operations (search, insertion, deletion) require access to as few pages as possible.\n\nA hash index uses four types of pages:\n\nmetapage—page zero that provides the “table of contents” of an index\n\nbucket pages—the main pages of an index, one per bucket\n\noverflow pages—additional pages that are used when the main bucket page\n\ncannot accommodate all the elements\n\nbitmappages—pagescontainingthebitarrayusedtotrackoverflowpagesthat\n\nhave been freed and can be reused\n\nWe can peek into index pages\n\nusing the pageinspect extension.\n\nLet’s begin with an empty table:\n\n=> CREATE EXTENSION pageinspect;\n\n=> CREATE TABLE t(n integer);\n\n=> ANALYZE t;\n\n=> CREATE INDEX ON t USING hash(n);\n\nI have analyzed the table, so the created index will have the minimal size possible; otherwise, the number of buckets would have been selected based on the assump- tion that the table contains ten pages.1\n\n1 backend/access/table/tableam.c, table_block_relation_estimate_size function\n\n470\n\n24.2 Page Layout\n\nThe index contains four pages: the metapage, two bucket pages, and one bitmap page (created at once for future use):\n\n=> SELECT page, hash_page_type(get_raw_page('t_n_idx', page)) FROM generate_series(0,3) page;\n\npage | hash_page_type −−−−−−+−−−−−−−−−−−−−−−−\n\n0 | metapage 1 | bucket 2 | bucket 3 | bitmap\n\n(4 rows)\n\nmeta- page\n\nbucket 0 bucket 0\n\nbucket 1 bucket 1\n\nbitmap\n\nThe metapage contains all the control information about the index. We are inter- ested only in a few values at the moment:\n\n=> SELECT ntuples, ffactor, maxbucket FROM hash_metapage_info(get_raw_page('t_n_idx', 0));\n\nntuples | ffactor | maxbucket −−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−\n\n0 |\n\n307 |\n\n1\n\n(1 row)\n\nThe estimated number of rows per bucket is shown in the ffactor field. This value fillfactor storage parameter value. By is calculated based on the block size and the absolutely uniform data distribution and no hash collisions you could use a higher fillfactor value, but in real-life databases it increases the risk of page overflows.\n\nThe worst scenario for a hash index is a large skew in data distribution, when a key is repeated multiple times. Since the hash function will be returning one and the same value, all the data will be placed into the same bucket, and increasing the number of buckets will not help.\n\nNow the index is empty, as shown by the ntuples field. Let’s cause a bucket page overflow by inserting multiple rows with the same value of the index key. An over- flow page appears in the index:\n\n471\n\n75\n\nv. ��\n\nChapter 24 Hash\n\n=> INSERT INTO t(n)\n\nSELECT 0 FROM generate_series(1,500); -- the same value\n\n=> SELECT page, hash_page_type(get_raw_page('t_n_idx', page)) FROM generate_series(0,4) page;\n\npage | hash_page_type −−−−−−+−−−−−−−−−−−−−−−−\n\n0 | metapage 1 | bucket 2 | bucket 3 | bitmap 4 | overflow\n\n(5 rows)\n\noverflow\n\nmeta- page\n\nbucket 0 bucket 0\n\nbucket 1 bucket 1\n\nbitmap\n\nbucket 1 bucket 1 bucket 1 bucket 1\n\nThe combined statistics on all the pages shows that bucket � is empty,while all the values have been placed into bucket �: some of them are located in the main page, and those that did not fit it can be found in the overflow page.\n\n=> SELECT page, live_items, free_size, hasho_bucket FROM (VALUES (1), (2), (4)) p(page),\n\nhash_page_stats(get_raw_page('t_n_idx', page));\n\npage | live_items | free_size | hasho_bucket −−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n1 | 2 | 4 |\n\n0 | 407 | 93 |\n\n8148 | 8 | 6288 |\n\n0 1 1\n\n(3 rows)\n\nIt is clear that if the elements of one and the same bucket are spread over several pages,performance will suffer. A hash index shows best results if data distribution is uniform.\n\nNow let’s take a look at how a bucket can be split. It happens when the number of rows in the index exceeds the estimated ffactor value for the available buckets. Here we have two buckets, and the ffactor is ���, so it will happen when the ���th row is inserted into the index:\n\n472\n\n24.2 Page Layout\n\n=> SELECT ntuples, ffactor, maxbucket, ovflpoint FROM hash_metapage_info(get_raw_page('t_n_idx', 0));\n\nntuples | ffactor | maxbucket | ovflpoint −−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−\n\n500 |\n\n307 |\n\n1 |\n\n1\n\n(1 row)\n\n=> INSERT INTO t(n)\n\nSELECT n FROM generate_series(1,115) n; -- now values are different\n\n=> SELECT ntuples, ffactor, maxbucket, ovflpoint FROM hash_metapage_info(get_raw_page('t_n_idx', 0));\n\nntuples | ffactor | maxbucket | ovflpoint −−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−\n\n615 |\n\n307 |\n\n2 |\n\n2\n\n(1 row)\n\nThe maxbucket value has been increased to two: now we have three buckets, num- bered from � to �. But even though we have added only one bucket, the number of pages has doubled:\n\n=> SELECT page, hash_page_type(get_raw_page('t_n_idx', page)) FROM generate_series(0,6) page;\n\npage | hash_page_type −−−−−−+−−−−−−−−−−−−−−−−\n\n0 | metapage 1 | bucket 2 | bucket 3 | bitmap 4 | overflow 5 | bucket 6 | unused\n\n(7 rows)\n\nmeta- page\n\nbucket 0 bucket 0\n\nbucket 1 bucket 1\n\nbitmap\n\nbucket 1 bucket 1 bucket 1 bucket 1\n\nbucket 2 bucket 2\n\nOne of the new pages is used by bucket �,while the other one remains free and will be used by bucket � as soon as it appears.\n\n=> SELECT page, live_items, free_size, hasho_bucket FROM (VALUES (1), (2), (4), (5)) p(page),\n\nhash_page_stats(get_raw_page('t_n_idx', page));\n\n473\n\nv. ��\n\nChapter 24 Hash\n\npage | live_items | free_size | hasho_bucket −−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n1 | 2 | 4 | 5 |\n\n27 | 407 | 158 | 23 |\n\n7608 | 8 | 4988 | 7688 |\n\n0 1 1 2\n\n(4 rows)\n\nThus, from the point of view of the operating system, the hash index grows in spurts, although from the logical standpoint the hash table shows gradual growth.\n\nTo level out this growth to some extent and avoid allocating too many pages at a time, starting from the tenth increase pages get allocated in four equal batches rather than all at once.\n\nTwo more fields of the metapage,which are virtually bit masks,provide the details on bucket addresses:\n\n=> SELECT maxbucket, highmask::bit(4), lowmask::bit(4) FROM hash_metapage_info(get_raw_page('t_n_idx', 0));\n\nmaxbucket | highmask | lowmask −−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−\n\n2 | 0011\n\n| 0001\n\n(1 row)\n\nA bucket number is defined by the hash code bits that correspond to the highmask. But if the received bucket number does not exist (exceeds maxbucket),the lowmask bits are taken.1 In this particular case, we take two lowest bits, which gives us the values from � to �; but if we got �, we would take only one lowest bit, that is, use bucket � instead of bucket �.\n\nEach time the size is doubled, new bucket pages are allocated as a single continu- ous chunk,while overflow and bitmap pages get inserted between these fragments as required. The metapage keeps the number of pages inserted into each of the chunks in the spares array, which gives us an opportunity to calculate the number of its main page based on the bucket number using simple arithmetic.2\n\nIn this particular case, the first increase was followed by insertion of two pages (a bitmap page and an overflow page), but no new additions have happened after the second increase yet:\n\n1 backend/access/hash/hashutil.c, _hash_hashkey2bucket function 2 include/access/hash.h, BUCKET_TO_BLKNO macro\n\n474\n\n24.2 Page Layout\n\n=> SELECT spares[2], spares[3] FROM hash_metapage_info(get_raw_page('t_n_idx', 0));\n\nspares | spares −−−−−−−−+−−−−−−−−\n\n2 |\n\n2\n\n(1 row)\n\nThe metapage also stores an array of pointers to bitmap pages:\n\n=> SELECT mapp[1] FROM hash_metapage_info(get_raw_page('t_n_idx', 0));\n\nmapp −−−−−−\n\n3\n\n(1 row)\n\nspares\n\nmeta- page\n\nbucket 0 bucket 0\n\nbucket 1 bucket 1\n\nbitmap\n\nbucket 1 bucket 1\n\nbucket 2 bucket 2\n\nmmap\n\nThe space within index pages is freed when pointers to dead tuples are removed. It happens during page pruning (which is triggered by an attempt to insert an ele- ment into a completely filled page)1 or when routine vacuuming is performed.\n\nHowever, a hash index cannot shrink: once allocated, index pages will not be re- turned to the operating system. The main pages are permanently assigned to their buckets, even if they contain no elements at all; the cleared overflow pages are tracked in the bitmap and can be reused (possibly by another bucket). The only way to reduce the physical size of an index is to rebuild it using the ������� or ������ ����\n\ncommands.\n\nThe query plan has no indication of the index type:\n\n=> CREATE INDEX ON flights USING hash(flight_no);\n\n1 backend/access/hash/hashinsert.c, _hash_vacuum_one_page function\n\n475\n\np. ���\n\np. ���\n\np. ���\n\nChapter 24 Hash\n\n=> EXPLAIN (costs off) SELECT * FROM flights WHERE flight_no = 'PG0001';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights\n\nRecheck Cond: (flight_no = 'PG0001'::bpchar) −> Bitmap Index Scan on flights_flight_no_idx\n\nIndex Cond: (flight_no = 'PG0001'::bpchar)\n\n(4 rows)\n\n24.3 Operator Class\n\nPriortoPostgre�����,hashindexeswerenotlogged,thatis,theywereneitherpro- tected against failures nor replicated, and consequently, it was not recommended to use them. But even then they had their own value. The thing is that the hashing and grouping), and algorithm is widely used (in particular, to perform hash joins the system must know which hash function can be used for a certain data type. However, this correspondence is not static: it cannot be defined once and for all since Postgre��� allows adding new data types on the fly. Therefore, it is main- tained by the operator class of the hash index and a particular data type. The hash function itself is represented by the support function of the class:\n\n=> SELECT opfname AS opfamily_name,\n\namproc::regproc AS opfamily_procedure\n\nFROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_amproc amproc ON amprocfamily = opf.oid\n\nWHERE amname = 'hash' AND amprocnum = 1 ORDER BY opfamily_name, opfamily_procedure;\n\nopfamily_name\n\n| opfamily_procedure\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−\n\n| hash_aclitem aclitem_ops | hash_array array_ops | hashchar bool_ops bpchar_ops | hashbpchar bpchar_pattern_ops | hashbpchar ...\n\n476\n\n24.4 Properties\n\ntimetz_ops uuid_ops xid8_ops xid_ops (38 rows)\n\n| timetz_hash | uuid_hash | hashint8 | hashint4\n\nThese functions return ��-bit integers. Although they are not documented, they can be used to calculate the hash code for a value of the corresponding type.\n\nFor example, the text_ops family uses the hashtext function:\n\n=> SELECT hashtext('one'), hashtext('two');\n\n| −−−−−−−−−−−−+−−−−−−−−−−−− 1793019229 | 1590507854\n\nhashtext\n\nhashtext\n\n(1 row)\n\nThe operator class of the hash index provides only the equal to operator:\n\n=> SELECT opfname AS opfamily_name,\n\nleft(amopopr::regoperator::text, 20) AS opfamily_operator\n\nFROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_amop amop ON amopfamily = opf.oid\n\nWHERE amname = 'hash' ORDER BY opfamily_name, opfamily_operator;\n\nopfamily_name\n\n|\n\nopfamily_operator\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−\n\naclitem_ops array_ops bool_ops ... uuid_ops xid8_ops xid_ops (48 rows)\n\n| =(aclitem,aclitem) | =(anyarray,anyarray) | =(boolean,boolean)\n\n| =(uuid,uuid) | =(xid8,xid8) | =(xid,xid)\n\n24.4 Properties\n\nLet’s take a look at the index-level properties to the system.\n\nthat the hash access method imparts\n\n477\n\np. ���\n\np. ���\n\nChapter 24 Hash\n\nAccess Method Properties\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'hash';\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | f | can_order | f | can_unique | can_multi_col | f | t | can_exclude | f | can_include\n\nhash hash hash hash hash\n\n(5 rows)\n\nIt is clear that hash indexes cannot be used for row ordering: the hash function mixes the data more or less randomly.\n\nUnique constraints are not supported either. However, hash indexes can enforce constraints,and since the only supported function is equal to,this exclu- exclusion sion attains the meaning of uniqueness:\n\n=> ALTER TABLE aircrafts_data\n\nADD CONSTRAINT unique_range EXCLUDE USING hash(range WITH =);\n\n=> INSERT INTO aircrafts_data\n\nVALUES ('744','{\"ru\": \"Boeing 747-400\"}',11100);\n\nERROR: \"unique_range\" DETAIL: (range)=(11100).\n\nconflicting key value violates exclusion constraint\n\nKey (range)=(11100) conflicts with existing key\n\nMulticolumn indexes and additional ������� columns are not supported either.\n\nIndex-Level Properties\n\n=> FROM unnest(array[\n\nSELECT p.name, pg_index_has_property('flights_flight_no_idx', p.name)\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\n478",
      "page_number": 445
    },
    {
      "number": 24,
      "title": "Hash",
      "start_page": 471,
      "end_page": 482,
      "detection_method": "regex_chapter",
      "content": "24.4 Properties\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| f clusterable | t index_scan bitmap_scan | t backward_scan | t\n\n(4 rows)\n\nThe hash index supports both a regular index scan and a bitmap scan.\n\nTable clusterization by the hash index is not supported. It is quite logical, as it is hard to imagine why it may be necessary to physically order heap data based on the hash function value.\n\nColumn-Level Properties\n\nColumn-level properties are virtually defined by the index access method and al- ways take the same values.\n\n=> SELECT p.name,\n\npg_index_column_has_property('flights_flight_no_idx', 1, p.name)\n\nFROM unnest(array[\n\n'asc', 'desc', 'nulls_first', 'nulls_last', 'orderable', 'distance_orderable', 'returnable', 'search_array', 'search_nulls'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n| f asc | f desc | f nulls_first | f nulls_last orderable | f distance_orderable | f | f returnable | f search_array | f search_nulls\n\n(9 rows)\n\nSince the hash function does not preserve the order of values, all the properties related to ordering are inapplicable to the hash index.\n\n479\n\nChapter 24 Hash\n\nThe hash index cannot participate in an index-only scan, as it does not store the index key and requires heap access.\n\nThe hash index does not support ���� values, since the equal to operation is inap- plicable to them.\n\nSearch for elements in an array is not implemented either.\n\n480\n\n25\n\nB-tree\n\n25.1 Overview\n\nA �-tree (implemented as the btree access method) is a data structure that enables you to quickly find the required element in leaf nodes of the tree by going down from its root.1 For the search path to be unambiguously identified, all tree ele- ments must be ordered. B-trees are designed for ordinal data types, whose values can be compared and sorted.\n\nThe following schematic diagram of an index build over airport codes shows inner nodes as horizontal rectangles; leaf nodes are aligned vertically.\n\nAER OVB AER OVB\n\nAER DME AER DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nAER AER\n\nDME DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nBZK BZK\n\nHMA HMA\n\nLED LED\n\nOVS OVS\n\nSGC SGC\n\nKJA KJA\n\nNUX NUX\n\nPEE PEE\n\n1 postgresql.org/docs/14/btree.html backend/access/nbtree/README\n\nSVO SVO\n\nSVX SVX\n\nVKO VKO\n\n481\n\nChapter 25 B-tree\n\nEach tree node contains several elements, which consist of an index key and a pointer. Inner node elements reference nodes of the next level; leaf node elements reference heap tuples (the illustration does not show these references).\n\nB-trees have the following important properties:\n\nThey are balanced, which means that all leaf nodes of a tree are located at the\n\nsame depth. Therefore, they guarantee equal search time for all values.\n\nThey have plenty of branches, that is, each node contains many elements, of- ten hundreds of them (the illustration shows three-element nodes solely for clarity). As a result, �-tree depth is always small, even for very large tables. We cannot say with absolute certainty what the letter � in the name of this structure stands for. Both balanced and bushy fit equally well. Surprisingly, you can often see it interpreted as binary, which is certainly incorrect.\n\nData in an index is sorted either in ascending or in descending order, both within each node and across all nodes of the same level. Peer nodes are bound into a bidirectional list,so it is possible to get an ordered set of data by simply scanning the list one way or the other,without having to start at the root each time.\n\n25.2 Search and Insertions\n\nSearch by Equality\n\nLet’s take a look at how we can search for a value in a tree by condition “indexed- column = expression”.1 We will try to find the ��� airport (Krasnoyarsk).\n\nThe search starts at the root node, and the access method must determine which child node to descend to. It chooses the Ki key, for which Ki ⩽ expression < Ki+1 is satisfied.\n\nThe root node contains the keys��� and ���. The condition��� ⩽ ��� < ��� holds true,so we need to descend into the child node referenced by the element with the ��� key.\n\n1 backend/access/nbtree/nbtsearch.c, _bt_search function\n\n482\n\n25.2 Search and Insertions\n\nAER OVB AER OVB\n\nAER DME AER DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nAER AER\n\nDME DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nBZK BZK\n\nHMA HMA\n\nLED LED\n\nOVS OVS\n\nSGC SGC\n\nSVX SVX\n\nKJA KJA\n\nNUX NUX\n\nPEE PEE\n\nVKO VKO\n\nThis procedure is repeated recursively until we get to the leaf node that contains the required tuple ��. In this particular case, the child node satisfies the condi- tion ��� ⩽ ��� < ���, so we have to descend into the leaf node referenced by the element with the ��� key.\n\nAs you can notice, the leftmost keys in the inner nodes of the tree are redundant: to choose the child node of the root, it is enough to have condition ��� < ��� satisfied. B-trees do not store such keys, so in the illustrations that follow I will leave the corresponding elements empty.\n\nThe required element in the leaf node can be quickly found by binary search.\n\nHowever, the search procedure is not as trivial as it seems. It must be taken into account that the sort order of data in an index can be either ascending, like shown above, or descending. Even a unique index can have several matching values, and all of them must be returned. Moreover,there may be so many duplicates that they do not fit a single node, so the neighboring leaf node will have to be processed too.\n\nSince an index can contain non-unique values, it would be more accurate to call its order non-descending rather than ascending (and non-ascending rather than descending). But , which lets us I will stick to a simpler term. Besides, the tuple �� is a part of an index key consider index entries to be unique even if the values are actually the same.\n\n483\n\np. ���\n\nv. ��\n\nChapter 25 B-tree\n\nOn top of that, while the search is in progress, other processes may modify the data,pages may get split into two,and the tree structure may change. All the algo- rithms are designed to minimize contention between these concurrent operations whenever possible and avoid excessive locks,but we are not going to get into these technicalities here.\n\nSearch by Inequality\n\nIf the search is performed by condition“indexed-column ⩽ expression”(or“indexed- column ⩾ expression”),we must first search the index for the value that satisfies the equality condition and then traverse its leaf nodes in the required direction until the end of the tree is reached.\n\nThis diagram illustrates the search for airport codes that are less than or equal to ��� (Domodedovo).\n\nAER OVB AER OVB\n\nAER DME AER DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nAER AER\n\nDME DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nBZK BZK\n\nHMA HMA\n\nLED LED\n\nOVS OVS\n\nSGC SGC\n\nKJA KJA\n\nNUX NUX\n\nPEE PEE\n\nFor less than and greater than operators, the procedure is the same, except that the first found value must be excluded.\n\nSearch by Range\n\nWhen searching by range “expression1 ⩽ indexed-column ⩽ expression2”, we must first find expression1 and then traverse the leaf nodes in the right direction until\n\n484\n\nSVO SVO\n\nSVX SVX\n\nVKO VKO\n\n25.2 Search and Insertions\n\nwe get to expression2. This diagram illustrates the process of searching for airport codes in the range between ��� (Saint Petersburg) and ��� (Rostov-on-Don), in- clusive.\n\nAER OVB AER OVB\n\nAER DME AER DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nAER AER\n\nDME DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nBZK BZK\n\nHMA HMA\n\nLED LED\n\nOVS OVS\n\nSGC SGC\n\nSVX SVX\n\nKJA KJA\n\nNUX NUX\n\nPEE PEE\n\nVKO VKO\n\nInsertions\n\nThe insert position of a new element is unambiguously defined by the order of keys. For example, if you insert the ��� airport code (Saratov) into the table, the new element will appear in the last but one leaf node, between ��� and ���.\n\nBut what if the leaf node does not have enough space for a new element? For ex- ample (assuming that a node can accommodate three elements at the most), if we insert the ��� airport code (Tyumen), the last leaf node will be overfilled. In this case, the node is split into two, some of the elements of the old node are moved into the new node, and a pointer to the new child node is added into the parent node. Obviously, the parent can get overfilled too. Then it is also split into two nodes, and so on. If it comes to splitting the root, one more node is created above the resulting nodes to become the new root of the tree. The tree depth is increased by one level in this case.\n\nInthisexample,theinsertionofthe���airportledtotwonodesplits; theresulting new nodes are highlighted in the diagram below. To make sure that any node can\n\n485\n\np. ���\n\nChapter 25 B-tree\n\nbesplit,abidirectionallistbindsthenodesatalllevels,notonlythoseatthelowest level.\n\nAER OVB AER OVB\n\nSVO SVO\n\nAER DME AER DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO TJM SVO TJM\n\nAER AER\n\nDME DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nBZK BZK\n\nHMA HMA\n\nLED LED\n\nOVS OVS\n\nRTW RTW\n\nSVX SVX\n\nKJA KJA\n\nNUX NUX\n\nPEE PEE\n\nSGC SGC\n\nThe described procedure of insertions and splits guarantees that the tree remains balanced, and since the number of elements that a node can accommodate is typ- ically quite large, the tree depth rarely increases.\n\nThe problem is that once split, nodes can never be merged together, even if they contain very few elements after vacuuming. This limitation pertains not to the �- tree data structure as such, but rather to its Postgre��� implementation. So if the node turns out to be full when an insertion is attempted, the access method first redundant data in order to clear some space and avoid an extra split. tries to prune\n\n25.3 Page Layout\n\nEach node of a �-tree takes one page. The page’s size defines the node’s capacity.\n\nBecause of page splits, the root of the tree can be represented by different pages at different times. But the search algorithm must always start the scan at the root. It finds the �� of the current root page in the zero index page (which is called a metapage). The metapage also contains some other metadata.\n\n486\n\nTJM TJM\n\nVKO VKO\n\n25.3 Page Layout\n\nmetapage\n\n2\n\nAER OVB AER OVB\n\nSVO SVO\n\n1\n\nAER DME AER DME\n\nKZN OVB KZN OVB\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nSVO TJM SVO TJM\n\nAER AER\n\nDME DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nTJM TJM\n\n0\n\nBZK BZK\n\nHMA HMA\n\nLED LED\n\nOVS OVS\n\nRTW RTW\n\nSVX SVX\n\nVKO VKO\n\nDME DME\n\nKJA KJA\n\nNUX NUX\n\nPEE PEE\n\nSGC SGC\n\nTJM TJM\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nData layout in index pages is a bit different from what we have seen so far. All the pages, except the rightmost ones at each level, contain an additional “high key,” which is guaranteed to be not smaller than any key in this page. In the above diagram high keys are highlighted.\n\nLet’s use the pageinspect extension to take a look at a page of a real index built upon six-digit booking references. The metapage lists the root page �� and the depth of the tree (level numbering starts from leaf nodes and is zero-based):\n\n=> SELECT root, level FROM bt_metap('bookings_pkey');\n\nroot | level −−−−−−+−−−−−−−\n\n290 | (1 row)\n\n2\n\n487\n\nChapter 25 B-tree\n\nThe keys stored in index entries are displayed as sequences of bytes, which is not really convenient:\n\n=> SELECT data FROM bt_page_items('bookings_pkey',290) WHERE itemoffset = 2;\n\ndata −−−−−−−−−−−−−−−−−−−−−−−−− 0f 30 43 39 41 42 31 00\n\n(1 row)\n\nTo decipher these values, we will have to write an adhoc function. It will not sup- port all platforms and may not work for some particular scenarios,but it will do for the examples in this chapter:\n\n=> CREATE FUNCTION data_to_text(data text) RETURNS text AS $$ DECLARE\n\nraw bytea := ('\\x'||replace(data,' ',''))::bytea; pos integer := 0; len integer; res text := '';\n\nBEGIN\n\nWHILE (octet_length(raw) > pos) LOOP\n\nlen := (get_byte(raw,pos) - 3) / 2; EXIT WHEN len <= 0; IF pos > 0 THEN\n\nres := res || ', ';\n\nEND IF; res := res || (\n\nSELECT string_agg( chr(get_byte(raw, i)),'') FROM generate_series(pos+1,pos+len) i\n\n); pos := pos + len + 1;\n\nEND LOOP; RETURN res;\n\nEND; $$ LANGUAGE plpgsql;\n\nNow we can take a look at the contents of the root page:\n\n488\n\n25.3 Page Layout\n\n=> SELECT itemoffset, ctid, data_to_text(data) FROM bt_page_items('bookings_pkey',290);\n\nitemoffset |\n\nctid\n\n| data_to_text\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−\n\n| 1 | (3,0) | 0C9AB1 2 | (289,1) | 192F03 3 | (575,1) | 25D715 4 | (860,1) 5 | (1145,1) | 32785C\n\n...\n\n17 | (4565,1) | C993F6 18 | (4850,1) | D63931 19 | (5135,1) | E2CB14 20 | (5420,1) | EF6FEA 21 | (5705,1) | FC147D\n\n(21 rows)\n\nAs I have said, the first entry contains no key. The ctid column provides links to child pages.\n\nSuppose we are looking for booking ������. In this case, we have to choose entry �� (since ������ ⩽ ������ < ������) and go down to page ����.\n\n=> SELECT itemoffset, ctid, data_to_text(data) FROM bt_page_items('bookings_pkey',5135);\n\nitemoffset |\n\nctid\n\n| data_to_text\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−\n\n1 | (5417,1) | EF6FEA 2 | (5132,0) | 3 | (5133,1) | E2D71D 4 | (5134,1) | E2E2F4 5 | (5136,1) | E2EDE7\n\nhigh key\n\n...\n\n282 | (5413,1) | EF41BE 283 | (5414,1) | EF4D69 284 | (5415,1) | EF58D4 285 | (5416,1) | EF6410\n\n(285 rows)\n\nThe first entry in this page contains the high key, which may seem a bit unexpected. Logically, it should have been placed at the end of the page, but from the imple- mentation standpoint it is more convenient to have it at the beginning to avoid moving it each time the page content changes.\n\n489\n\nChapter 25 B-tree\n\nHere we choose entry � (since ������ ⩽ ������ < ������) and go down to page �����.\n\n=> SELECT itemoffset, ctid, data_to_text(data) FROM bt_page_items('bookings_pkey',5133);\n\nitemoffset |\n\nctid\n\n| data_to_text\n\n−−−−−−−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n1 | (11921,1) 2 | (11919,76) 3 | (11919,77) 4 | (11919,78) 5 | (11919,79)\n\n| E2E2F4 | E2D71D | E2D725 | E2D72D | E2D733\n\n...\n\n363 | (11921,123) | E2E2C9 364 | (11921,124) | E2E2DB 365 | (11921,125) | E2E2DF 366 | (11921,126) | E2E2E5 367 | (11921,127) | E2E2ED\n\n(367 rows)\n\nIt is a leaf page of the index. The first entry is the high key; all the other entries point to heap tuples.\n\nAnd here is our booking:\n\n=> SELECT * FROM bookings WHERE ctid = '(11919,77)';\n\nbook_ref |\n\nbook_date\n\n| total_amount\n\n−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\nE2D725 (1 row)\n\n| 2017−01−25 04:10:00+03 |\n\n28000.00\n\nThis is roughly what happens at the low level when we search for a booking by its code:\n\n=> EXPLAIN (costs off) SELECT * FROM bookings WHERE book_ref = 'E2D725';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using bookings_pkey on bookings\n\nIndex Cond: (book_ref = 'E2D725'::bpchar)\n\n(2 rows)\n\n490",
      "page_number": 471
    },
    {
      "number": 25,
      "title": "B-tree",
      "start_page": 483,
      "end_page": 508,
      "detection_method": "regex_chapter",
      "content": "25.3 Page Layout\n\nDeduplication\n\nNon-unique indexes can contain a lot of duplicate keys that point to different heap tuples. Since non-unique keys appear more than once and hence take much space, duplicates are collapsed into a single index entry, which contains the key and the list of the corresponding tuple ��s.1 In some cases, this procedure (which is called deduplication) can significantly reduce the index size.\n\nHowever, unique indexes can also contain duplicates because of ����: an index keeps references to all versions of table rows. The mechanism of ��� updates can help you fight index bloating caused by referencing outdated and typically short- lived row versions, but sometimes it may be inapplicable. In this case, deduplica- tion can buy some time required to vacuum redundant heap tuples and avert extra page splits.\n\nTo avoid wasting resources on deduplication when it brings no immediate bene- fits, collapsing is only performed if the leaf page does not have enough space to accommodate one more tuple.2 Then page pruning and deduplication3 can free some space and prevent an undesired page split. However, if duplicates are rare, you can disable the deduplication feature by turning off the deduplicate_items stor- age parameter.\n\nSome indexes do not support deduplication. The main limitation is that the equal- ity of keys must be checked by simple binary comparison of their inner represen- tation. Not all data types by far can be compared this way. For instance, floating- point numbers (float and double precision) have two different representations for zero. Arbitrary-precision numbers (numeric) can represent one and the same num- ber in different scales, while the jsonb type can use such numbers. Neither is deduplicationpossiblefortexttypesifyouusenondeterministiccollations,4 which allow the same symbols to be represented by different byte sequences (standard collations are deterministic).\n\nBesides, deduplication is currently not supported for composite types, ranges, and arrays, as well as for indexes declared with the ������� clause.\n\n1 postgresql.org/docs/14/btree-implementation#BTREE-DEDUPLICATION.html 2 backend/access/nbtree/nbtinsert.c, _bt_delete_or_dedup_one_page function 3 backend/access/nbtree/nbtdedup.c, _bt_dedup_pass function 4 postgresql.org/docs/14/collation.html\n\n491\n\nv. ��\n\np. ���\n\nv. ��\n\nChapter 25 B-tree\n\nTo check whether a particular index can use deduplication, you can take a look at the allequalimage field in its metapage:\n\n=> CREATE INDEX ON tickets(book_ref);\n\n=> SELECT allequalimage FROM bt_metap('tickets_book_ref_idx');\n\nallequalimage −−−−−−−−−−−−−−−\n\nt\n\n(1 row)\n\nIn this case, deduplication is supported. And indeed, we can see that one of the leaf pages contains both index entries with a single tuple �� (htid) and those with a list of ��s (tids):\n\n=> SELECT itemoffset, htid, left(tids::text,27) tids,\n\ndata_to_text(data) AS data\n\nFROM bt_page_items('tickets_book_ref_idx',1) WHERE itemoffset > 1;\n\nitemoffset |\n\nhtid\n\n|\n\ntids\n\n|\n\ndata\n\n−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−\n\n2 | (32965,40) | 3 | (47429,51) | 4 | (3648,56) 5 | (6498,47)\n\n| {\"(3648,56)\",\"(3648,57)\"} |\n\n| 000004 | 00000F | 000010 | 000012\n\n...\n\n271 | (21492,46) | | 000890 272 | (26601,57) | {\"(26601,57)\",\"(26601,58)\"} | 0008AC | 0008B6 273 | (25669,37) |\n\n(272 rows)\n\nCompact Storage of Inner Index Entries\n\nDeduplication enables accommodating more entries in leaf pages of an index. But even though leaf pages make up the bulk of an index, data compaction performed in inner pages to prevent extra splits is just as important, as search efficiency is directly dependent on tree depth.\n\nInner index entries contain index keys, but their values are only used to deter- mine the subtree to descend into during search. In multicolumn indexes,it is often enough to take the first key attribute (or several first ones). Other attributes can be truncated to save space in the page.\n\n492\n\n25.4 Operator Class\n\nSuch suffix truncation happens when a leaf page is being split and the inner page has to accommodate a new pointer.1\n\nIn theory, we could even go one step further and keep only the meaningful part of the attribute, for example, the first few symbols of a row that are enough to differentiate be- tween subtrees. But it is not implemented yet: an index entry either contains the whole attribute or excludes this attribute altogether.\n\nFor instance, here are several entries of the root page of an index built over the tickets table on the columns containing booking references and passenger names:\n\n=> CREATE INDEX tickets_bref_name_idx ON tickets(book_ref, passenger_name);\n\n=> SELECT itemoffset, ctid, data_to_text(data) FROM bt_page_items('tickets_bref_name_idx',229) WHERE itemoffset BETWEEN 8 AND 13;\n\nitemoffset |\n\nctid\n\n|\n\ndata_to_text\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n8 | (1607,1) | 1A98A0 9 | (1833,2) | 1E57D1, SVETLANA MAKSIMOVA\n\n10 | (2054,1) | 220797 11 | (2282,1) | 25DB06 12 | (2509,2) | 299FE4, YURIY AFANASEV 13 | (2736,1) | 2D62C9\n\n(6 rows)\n\nWe can see that some index entries do not have the second attribute.\n\nNaturally,leaf pages must keep all key attributes and ������� column values,if any. Otherwise,itwouldbeimpossibletoperformindex-onlyscans. Theonlyexception is high keys; they can be kept partially.\n\n25.4 Operator Class\n\nComparison Semantics\n\nApart from hashing values, the system must also know how to order values of var- ious types, including custom ones. It is indispensable for sorting, grouping, merge\n\n1 backend/access/nbtree/nbtinsert.c, _bt_split function\n\n493\n\nChapter 25 B-tree\n\njoins, and some other operations. And just like in the case of hashing, comparison operators for a particular data type are defined by an operator class.1\n\nAn operator class allows us to abstract from names (such as >, <, =) and can even provide several ways to order values of one and the same type.\n\nHerearethemandatorycomparisonoperatorsthatmustbedefinedinanyoperator class of the btree method (shown for the bool_ops family):\n\n=> SELECT amopopr::regoperator AS opfamily_operator,\n\namopstrategy\n\nFROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_amop amop ON amopfamily = opf.oid\n\nWHERE amname = 'btree' AND opfname = 'bool_ops' ORDER BY amopstrategy;\n\nopfamily_operator\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n<(boolean,boolean) | <=(boolean,boolean) | =(boolean,boolean) | >=(boolean,boolean) | | >(boolean,boolean)\n\n1 2 3 4 5\n\n(5 rows)\n\nEach of these five comparison operators corresponds to one of the strategies,2 which defines their semantics:\n\n� less than\n\n� less than or equal to\n\n� equal to\n\n� greater than or equal to\n\n� greater than\n\nA �-tree operator class also includes several support functions.3 The first one must return 1 if its first argument is greater than the second one, −1 if it is less than the second one, and 0 if the arguments are equal.\n\n1 postgresql.org/docs/14/btree-behavior.html 2 postgresql.org/docs/14/xindex#XINDEX-STRATEGIES.html 3 postgresql.org/docs/14/btree-support-funcs.html\n\n494\n\n25.4 Operator Class\n\nOther support functions are optional, but they improve performance of the access method.\n\nTo better understand this mechanism, we can define a new data type with a non- default collation. The documentation gives an example for complex numbers,1 but it is written in C. Fortunately, a �-tree operator class can be implemented using interpreted languages too, so I will take advantage of it and make an example that is as simple as possible (even if knowingly inefficient).\n\nLet’s define a new composite type for information units:\n\n=> CREATE TYPE capacity_units AS ENUM (\n\n'B', 'kB', 'MB', 'GB', 'TB', 'PB'\n\n);\n\n=> CREATE TYPE capacity AS (\n\namount integer, unit capacity_units\n\n);\n\nNow create a table with a column of the new type and fill it with random values:\n\n=> CREATE TABLE test AS\n\nSELECT ( (random()*1023)::integer, u.unit )::capacity AS cap FROM generate_series(1,100),\n\nunnest(enum_range(NULL::capacity_units)) AS u(unit);\n\nBy default, values of composite types are sorted in lexicographical order, which is not the same as the natural order in this particular case:\n\n=> SELECT * FROM test ORDER BY cap;\n\ncap −−−−−−−−−−−\n\n(1,B) (3,GB) (4,MB) (9,kB) ... (1017,kB) (1017,GB) (1018,PB) (1020,MB) (600 rows)\n\n1 postgresql.org/docs/14/xindex#XINDEX-EXAMPLE.html\n\n495\n\nChapter 25 B-tree\n\nNow let’s get down to creating our operator class. We will start with defining a function that converts the volume into bytes:\n\n=> CREATE FUNCTION capacity_to_bytes(a capacity) RETURNS numeric AS $$ SELECT a.amount::numeric *\n\n1024::numeric ^ ( array_position(enum_range(a.unit), a.unit) - 1 );\n\n$$ LANGUAGE sql STRICT IMMUTABLE;\n\n=> SELECT capacity_to_bytes('(1,kB)'::capacity);\n\ncapacity_to_bytes −−−−−−−−−−−−−−−−−−−−−−− 1024.0000000000000000\n\n(1 row)\n\nCreate a support function for the future operator class:\n\n=> CREATE FUNCTION capacity_cmp(a capacity, b capacity) RETURNS integer AS $$ SELECT sign(capacity_to_bytes(a) - capacity_to_bytes(b)); $$ LANGUAGE sql STRICT IMMUTABLE;\n\nNow it is easy to define comparison operators using this support function. I delib- erately use peculiar names to demonstrate that they can be arbitrary:\n\n=> CREATE FUNCTION capacity_lt(a capacity, b capacity) RETURNS boolean AS $$ BEGIN\n\nRETURN capacity_cmp(a,b) < 0;\n\nEND; $$ LANGUAGE plpgsql IMMUTABLE STRICT;\n\n=> CREATE OPERATOR #<# ( LEFTARG = capacity, RIGHTARG = capacity, FUNCTION = capacity_lt\n\n);\n\nThe other four operators are defined in a similar way.\n\n=> CREATE FUNCTION capacity_le(a capacity, b capacity) RETURNS boolean AS $$ BEGIN\n\nRETURN capacity_cmp(a,b) <= 0;\n\nEND; $$ LANGUAGE plpgsql IMMUTABLE STRICT;\n\n496\n\n25.4 Operator Class\n\n=> CREATE OPERATOR #<=# (\n\nLEFTARG = capacity, RIGHTARG = capacity, FUNCTION = capacity_le\n\n);\n\n=> CREATE FUNCTION capacity_eq(a capacity, b capacity) RETURNS boolean AS $$ BEGIN\n\nRETURN capacity_cmp(a,b) = 0;\n\nEND; $$ LANGUAGE plpgsql IMMUTABLE STRICT;\n\n=> CREATE OPERATOR #=# ( LEFTARG = capacity, RIGHTARG = capacity, FUNCTION = capacity_eq, MERGES -- can be used in merge joins\n\n);\n\n=> CREATE FUNCTION capacity_ge(a capacity, b capacity) RETURNS boolean AS $$ BEGIN\n\nRETURN capacity_cmp(a,b) >= 0;\n\nEND; $$ LANGUAGE plpgsql IMMUTABLE STRICT;\n\n=> CREATE OPERATOR #>=# (\n\nLEFTARG = capacity, RIGHTARG = capacity, FUNCTION = capacity_ge\n\n);\n\n=> CREATE FUNCTION capacity_gt(a capacity, b capacity) RETURNS boolean AS $$ BEGIN\n\nRETURN capacity_cmp(a,b) > 0;\n\nEND; $$ LANGUAGE plpgsql IMMUTABLE STRICT;\n\n=> CREATE OPERATOR #># ( LEFTARG = capacity, RIGHTARG = capacity, FUNCTION = capacity_gt\n\n);\n\nAt this stage, we can already compare capacities:\n\n497\n\nChapter 25 B-tree\n\n=> SELECT (1,'MB')::capacity #># (512, 'kB')::capacity;\n\n?column? −−−−−−−−−−\n\nt\n\n(1 row)\n\nOnce the operator class is created, sorting will also start working as expected:\n\n=> CREATE OPERATOR CLASS capacity_ops DEFAULT FOR TYPE capacity -- to be used by default USING btree AS\n\nOPERATOR 1 #<#, OPERATOR 2 #<=#, OPERATOR 3 #=#, OPERATOR 4 #>=#, OPERATOR 5 #>#, FUNCTION 1 capacity_cmp(capacity,capacity);\n\n=> SELECT * FROM test ORDER BY cap;\n\ncap −−−−−−−−−−−\n\n(1,B) (21,B) (27,B) (35,B) (46,B) ... (1002,PB) (1013,PB) (1014,PB) (1014,PB) (1018,PB) (600 rows)\n\nOur operator class is used by default when a new index is created, and this index returns the results in the correct order:\n\n=> CREATE INDEX ON test(cap);\n\n=> SELECT * FROM test WHERE cap #<# (100,'B')::capacity ORDER BY cap;\n\ncap −−−−−−−− (1,B) (21,B) (27,B) (35,B)\n\n498\n\n25.4 Operator Class\n\n(46,B) (57,B) (68,B) (70,B) (72,B) (76,B) (78,B) (94,B) (12 rows)\n\n=> EXPLAIN (costs off) SELECT * FROM test WHERE cap #<# (100,'B')::capacity ORDER BY cap;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Only Scan using test_cap_idx on test\n\nIndex Cond: (cap #<# '(100,B)'::capacity)\n\n(2 rows)\n\nThe ������ clause joins for this data type.\n\nspecified in the equality operator declaration enables merge\n\nMulticolumn Indexes and Sorting\n\nLet’s take a closer look at sorting multicolumn indexes.\n\nFirst and foremost, it is very important to choose the optimal order of columns whendeclaringanindex: datasortingwithinpageswillbeginwiththefirstcolumn, then move on to the second one, and so on. Multicolumn indexes can guarantee efficient search only if the provided filter condition spans a continuous sequence of columns starting from the very first one: the first column,the first two columns, the range between the first and the third columns, etc. Other types of conditions can only be used to filter out redundant values that have been fetched based on other criteria.\n\nHere is the order of index entries in the first leaf page of the index that has been created on the tickets table and includes booking references and passenger names:\n\n=> SELECT itemoffset, data_to_text(data) FROM bt_page_items('tickets_bref_name_idx',1) WHERE itemoffset > 1;\n\n499\n\np. ���\n\nChapter 25 B-tree\n\nitemoffset |\n\ndata_to_text\n\n−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n2 | 000004, PETR MAKAROV 3 | 00000F, ANNA ANTONOVA 4 | 000010, ALEKSANDR SOKOLOV 5 | 000010, LYUDMILA BOGDANOVA 6 | 000012, TAMARA ZAYCEVA 7 | 000026, IRINA PETROVA 8 | 00002D, ALEKSANDR SMIRNOV\n\n...\n\n187 | 0003EF, VLADIMIR CHERNOV 188 | 00040C, ANTONINA KOROLEVA 189 | 00040C, DMITRIY FEDOROV 190 | 00041E, EGOR FEDOROV 191 | 00041E, ILYA STEPANOV 192 | 000447, VIKTOR VASILEV 193 | 00044D, NADEZHDA KULIKOVA\n\n(192 rows)\n\nIn this case, an efficient search for tickets is only possible either by the booking reference and the passenger name, or by the booking reference alone.\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE book_ref = '000010';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using tickets_book_ref_idx on tickets\n\nIndex Cond: (book_ref = '000010'::bpchar)\n\n(2 rows)\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE book_ref = '000010' AND passenger_name = 'LYUDMILA BOGDANOVA';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using tickets_bref_name_idx on tickets\n\nIndex Cond: ((book_ref = '000010'::bpchar) AND (passenger_name...\n\n(2 rows)\n\nBut if we decide to look for a passenger name, we have to scan all the rows:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE passenger_name = 'LYUDMILA BOGDANOVA';\n\n500\n\n25.4 Operator Class\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nGather\n\nWorkers Planned: 2 −> Parallel Seq Scan on tickets\n\nFilter: (passenger_name = 'LYUDMILA BOGDANOVA'::text)\n\n(4 rows)\n\nEven if the planner chooses to perform an index scan, all index entries will still have to be traversed.1 Unfortunately, the plan will not show that the condition is actually used only for filtering the result.\n\nIf the first column does not have too manydistinctvalues v1,v2,…,vn,it could be beneficial to perform several passes over the corresponding subtrees, virtually replacing a single search by condition “col2 = value”with a series of searches by the following conditions:\n\ncol1 = v1 ��� col2 = value col1 = v2 ��� col2 = value ⋯ col1 = vn ��� col2 = value\n\nThis type of an index access is called a Skip Scan, but it is not implemented yet.2\n\nAnd vice versa, if an index is created on passenger names and booking numbers, it will better suit queries by either the passenger name alone or both the passenger name and booking reference:\n\n=> CREATE INDEX tickets_name_bref_idx ON tickets(passenger_name, book_ref);\n\n=> SELECT itemoffset, data_to_text(data) FROM bt_page_items('tickets_name_bref_idx',1) WHERE itemoffset > 1;\n\nitemoffset |\n\ndata_to_text\n\n−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n2 | ADELINA ABRAMOVA, E37EDB 3 | ADELINA AFANASEVA, 1133B7 4 | ADELINA AFANASEVA, 4F3370 5 | ADELINA AKIMOVA, 7D2881 6 | ADELINA ALEKSANDROVA, 3C3ADD 7 | ADELINA ALEKSANDROVA, 52801E\n\n...\n\n1 backend/access/nbtree/nbtsearch.c, _bt_first function 2 commitfest.postgresql.org/34/1741\n\n501\n\nChapter 25 B-tree\n\n185 | ADELINA LEBEDEVA, 0A00E3 186 | ADELINA LEBEDEVA, DAEADE 187 | ADELINA LEBEDEVA, DFD7E5 188 | ADELINA LOGINOVA, 8022F3 189 | ADELINA LOGINOVA, EE67B9 190 | ADELINA LUKYANOVA, 292786 191 | ADELINA LUKYANOVA, 54D3F9\n\n(190 rows)\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE passenger_name = 'LYUDMILA BOGDANOVA';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on tickets\n\nRecheck Cond: (passenger_name = 'LYUDMILA BOGDANOVA'::text) −> Bitmap Index Scan on tickets_name_bref_idx\n\nIndex Cond: (passenger_name = 'LYUDMILA BOGDANOVA'::text)\n\n(4 rows)\n\nIn addition to the column order, you should also pay attention to the sort order when creating a new index. By default, values are sorted in ascending order (���), but you can reverse it (����) if required. It does not matter much if an index is built over a single column, as it can be scanned in any direction. But in a multicolumn index the order becomes important.\n\nOur newly created index can be used to retrieve the data sorted by both columns either in ascending or in descending order:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets ORDER BY passenger_name, book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using tickets_name_bref_idx on tickets\n\n(1 row)\n\n=> EXPLAIN (costs off) SELECT * FROM tickets ORDER BY passenger_name DESC, book_ref DESC;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan Backward using tickets_name_bref_idx on tickets\n\n(1 row)\n\nButthisindexcannotreturnthedatarightawayifitneedstobesortedinascending order by one column and in descending order by the other column at the same time.\n\n502\n\n25.4 Operator Class\n\nIn this case, the index provides partially ordered data that has to be further sorted by the second attribute:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets ORDER BY passenger_name ASC, book_ref DESC;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIncremental Sort\n\nSort Key: passenger_name, book_ref DESC Presorted Key: passenger_name −> Index Scan using tickets_name_bref_idx on tickets\n\n(4 rows)\n\nThe location of ���� values also affects the ability to use index for sorting. By default, ���� values are considered “greater” than regular values for the purpose of sorting, that is, they are located in the right side of the tree if the sort order is ascending and in the left side if the sort order is descending. The location of ���� values can be changed using the ����� ���� and ����� ����� clauses.\n\nIn the next example, the index does not satisfy the ����� �� clause, so the result has to be sorted:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets ORDER BY passenger_name NULLS FIRST, book_ref DESC;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nGather Merge\n\nWorkers Planned: 2 −> Sort\n\nSort Key: passenger_name NULLS FIRST, book_ref DESC −> Parallel Seq Scan on tickets\n\n(5 rows)\n\nBut if we create an index that follows the desired order, it will be used:\n\n=> CREATE INDEX tickets_name_bref_idx2 ON tickets(passenger_name NULLS FIRST, book_ref DESC);\n\n=> EXPLAIN (costs off) SELECT * FROM tickets ORDER BY passenger_name NULLS FIRST, book_ref DESC;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using tickets_name_bref_idx2 on tickets\n\n(1 row)\n\n503\n\np. ���\n\np. ���\n\nChapter 25 B-tree\n\n25.5 Properties\n\nLet’s take a look at the interface properties of �-trees.\n\nAccess Method Properties\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'btree';\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | can_order | t | t | can_unique | can_multi_col | t | t | can_exclude | t | can_include\n\nbtree btree btree btree btree (5 rows)\n\nB-trees can order data and ensure its uniqueness. It is the only access method with such properties.\n\nMany access methods support multicolumn indexes,but since values in �-trees are ordered, you have to pay close attention to the order of columns in an index.\n\nFormally, exclusion constraints are supported, but they are limited to equality conditions, which makes them analogous to unique constraints. It is much more preferable to use a full-fledged unique constraint instead.\n\nB-tree indexes can also be extended with additional ������� columns that do not participate in search.\n\nIndex-Level Properties\n\n=> FROM unnest(array[\n\nSELECT p.name, pg_index_has_property('flights_pkey', p.name)\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\n504\n\n25.5 Properties\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| t clusterable | t index_scan bitmap_scan | t backward_scan | t\n\n(4 rows)\n\nB-tree indexes can be used for clusterization.\n\nBoth index scans and bitmap scans are supported. Since leaf pages are bound into a bidirectional list, an index can also be traversed backwards, which results in the reverse sort order:\n\n=> EXPLAIN (costs off) SELECT * FROM bookings ORDER BY book_ref DESC;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan Backward using bookings_pkey on bookings\n\n(1 row)\n\nColumn-Level Properties\n\n=> SELECT p.name,\n\npg_index_column_has_property('flights_pkey', 1, p.name)\n\nFROM unnest(array[\n\n'asc', 'desc', 'nulls_first', 'nulls_last', 'orderable', 'distance_orderable', 'returnable', 'search_array', 'search_nulls'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nasc | t desc | f nulls_first | f nulls_last | t | t orderable distance_orderable | f | t returnable | t search_array | t search_nulls\n\n(9 rows)\n\n505\n\nChapter 25 B-tree\n\nThe O�������� property indicates that the data stored in a �-tree is ordered, while the first four properties (A�� and D���, N���� F���� and N���� L���) define the ac- tual order in a particular column. In this example, column values are sorted in ascending order with ���� values listed last.\n\nThe S����� N���� property indicates whether ���� values can be searched.\n\nB-trees do not support ordering operators (D������� O��������), even though it has been attempted to implement them.1\n\nB-treessupportsearchingformultipleelementsinanarray(theS�����A����prop- erty) and can return the resulting data without heap access (R���������).\n\n1 commitfest.postgresql.org/27/1804\n\n506\n\n26\n\nGiST\n\n26.1 Overview\n\nGi�� (Generalized Search Tree)1 is an access method that is virtually a generaliza- tion of a balanced search tree for data types that support relative positioning of values. B-tree applicability is limited to ordinal types that allow comparison oper- ations (but the support provided for such types is extremely efficient). As for �i��, its operator class allows defining arbitrary criteria for data distribution in the tree. A �i�� index can accommodate an �-tree for spatial data, an ��-tree for sets, and a signature tree for any data types (including texts and images).\n\nThanks to extensibility, you can create a new access method in Postgre��� from scratch byimplementing the interfaceof the indexing engine. However,apart from designing the indexing logic, you have to define the page layout, an efficient lock- ing strategy, and ��� support. It all takes strong programming skills and much implementation efforts. Gi�� simplifies this task,addressing all the low-level tech- nicalities and providing the basis for the search algorithm. To use the �i�� method with a new data type, you just need to add a new operator class that includes a dozensupportfunctions. Unlikethetrivialoperatorclassprovidedfor�-trees,such a class contains most of the indexing logic. Gi�� can be regarded as a framework for building new access methods in this respect.\n\nSpeaking in the most general terms, each entry that belongs to a leaf node (a leaf entry) contains a predicate (a logical condition) and a heap tuple ��. The index key must satisfy the predicate; it does not matter whether the key itself is a part of this entry or not.\n\n1 postgresql.org/docs/14/gist.html backend/access/gist/README\n\n507\n\nChapter 26 GiST\n\nEachentryinaninnerleaf(aninnerentry)alsocontainsapredicateandareference to a child node; all the indexed data of the child subtree must satisfy this predicate. In other words, the predicate of an inner entry is the union of all the predicates of its child entries. This important property of �i�� serves the purpose of simple ranking used by �-trees.\n\nGi�� tree search relies on the consistency function,which is one of the support func- tions defined by the operator class.\n\nThe consistency function is called on an index entry to determine whether the predicate of this entry is “consistent” with the search condition (“indexed-column operator expression”). For an inner entry,it shows whether we have to descend into the corresponding subtree; for a leaf entry,it checks whether its index key satisfies the condition.\n\nThe search starts at the root node,1 which is typical of a tree search. The consis- tency function determines which child nodes must be traversed and which can be skipped. Then this procedure is repeated for each of the found child nodes; unlike a �-tree, a �i�� index may have several such nodes. Leaf node entries selected by the consistency function are returned as results.\n\nThe search is always depth-first: the algorithm tries to get to a leaf page as soon as possible. Therefore, it can start returning results right away, which makes a lot of sense if the user needs to get only the first few rows.\n\nTo insert a new value into a �i�� tree, it is impossible to use the consistency func- tion, since we need to choose exactly one node to descend to.2 This node must have the minimal insert cost; it is determined by the penalty function of the opera- tor class.\n\nJust like in the case of a �-tree, the selected node may turn out to have no free space, which leads to a split.3 This operation needs two more functions. One of them distributes the entries between the old and new nodes; the other forms the union of the two predicates to update the predicate of the parent node.\n\n1 backend/access/gist/gistget.c, gistgettuple function 2 backend/access/gist/gistutil.c, gistchoose function 3 backend/access/gist/gistsplit.c, gistSplitByKey function\n\n508\n\n26.2 R-Trees for Points\n\nAs new values are being added, the existing predicates expand, and they are typ- ically narrowed down only if the page is split or the whole index is rebuilt. Thus, frequent updates of a �i�� index can lead to its performance degradation.\n\nSince all these theoretical discussions may seem too vague, and the exact logic mostlydependsonaparticularoperatorclassanyway,Iwillprovideseveralspecific examples.\n\n26.2 R-Trees for Points\n\nThe first example deals with indexing points (or other geometries) on a plane. A regular �-tree cannot be used for this data type, as there are no comparison op- erators defined for points. Clearly, we could have implemented such operators on our own, but geometries need index support for totally different operations. I will go over just two of them: search for objects contained within a particular area and nearest neighbor search.\n\nAn �-tree draws rectangles on a plane; taken together, they must cover all the in- dexed points. An index entry stores the bounding box, and the predicate can be defined as follows: the point lies within this bounding box.\n\nThe root of an �-tree contains several large rectangles (that may also overlap). Childnodesholdsmallerrectanglesthatfittheirparentnodes; together,theycover all the underlying points.\n\nLeaf nodes should contain the indexed points themselves, but �i�� requires that all entries have the same data type; therefore, leaf entries are also represented by rectangles, which are simply reduced to points.\n\nTo better visualize this structure, let’s take a look at three levels of an �-tree built over airport coordinates. For this example,I have extended the airports table of the demo database up to five thousand rows.1 I have also reduced the fillfactor value to make the tree deeper; the default value would have given us a single-level tree.\n\n1 You can download the corresponding file at edu.postgrespro.ru/internals-14/extra_airports.copy\n\n(I have used the data available at the openflights.org website).\n\n509\n\n90\n\nChapter 26 GiST\n\n=> CREATE TABLE airports_big AS SELECT * FROM airports_data;\n\n=> COPY airports_big FROM\n\n'/home/student/internals/airports/extra_airports.copy';\n\n=> CREATE INDEX airports_gist_idx ON airports_big USING gist(coordinates) WITH (fillfactor=10);\n\nAt the upper level, all the points are included into several (partially overlapping) bounding boxes:\n\nAt the next level, big rectangles are split into smaller ones:\n\n510\n\n26.2 R-Trees for Points\n\nFinally, at the inner level of the tree each bounding box contains as many points as a single page can accommodate:\n\nThis index uses the point_ops operator class, which is the only one available for points.\n\nRectangles and any other geometries can be indexed in the same manner, but in- stead of the object itself the index has to store its bounding box.\n\nPage Layout\n\nYou can study\n\n�i�� pages using the pageinspect extension.\n\nUnlike �-tree indexes, �i�� has no metapage, and the zero page is always the root of the tree. If the root page gets split, the old root is moved into a separate page, and the new root takes its place.\n\nHere is the contents of the root page:\n\n=> SELECT ctid, keys FROM gist_page_items(\n\nget_raw_page('airports_gist_idx', 0), 'airports_gist_idx'\n\n);\n\n511\n\nv. ��\n\nChapter 26 GiST\n\nctid\n\n|\n\nkeys\n\n−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n(207,65535) | (coordinates)=((50.84510040283203,78.246101379395)) (400,65535) | (coordinates)=((179.951004028,73.51780700683594)) (206,65535) | (coordinates)=((−1.5908199548721313,40.63980103)) (466,65535) | (coordinates)=((−1.0334999561309814,82.51779937740001))\n\n(4 rows)\n\nThese four rows correspond to the four rectangles of the upper level shown in the first picture. Unfortunately, the keys are displayed here as points (which makes sense for leaf pages), not as rectangles (which would be more logical for inner pages). But we can always get raw data and interpret it on our own.\n\nTo extract more detailed information, you can use the gevel extension,1 which is not in- cluded into the standard Postgre��� distribution.\n\nOperator Class\n\nThe following query returns the list of support functions that implement the logic of search and insert operations for trees:2\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'gist' AND opcname = 'point_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−\n\n1 | gist_point_consistent 2 | gist_box_union 3 | gist_point_compress 5 | gist_box_penalty 6 | gist_box_picksplit 7 | gist_box_same 8 | gist_point_distance 9 | gist_point_fetch\n\n11 | gist_point_sortsupport\n\n(9 rows)\n\n1 sigaev.ru/git/gitweb.cgi?p=gevel.git 2 postgresql.org/docs/14/gist-extensibility.html\n\n512\n\n26.2 R-Trees for Points\n\nI have already listed the mandatory functions above:\n\n� consistency function used to traverse the tree during search\n\n� union function that merges rectangles\n\n� penalty function used to choose the subtree to descend to when inserting an\n\nentry\n\n� picksplit function that distributes entries between new pages after a page split\n\n� same function that checks two keys for equality\n\nThe point_ops operator class includes the following operators:\n\n=> SELECT amopopr::regoperator, amopstrategy AS st, oprcode::regproc,\n\nleft(obj_description(opr.oid, 'pg_operator'), 19) description\n\nFROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'gist' AND opcname = 'point_ops' ORDER BY amopstrategy;\n\namopopr\n\n| st |\n\noprcode\n\n|\n\ndescription\n\n−−−−−−−−−−−−−−−−−−−+−−−−+−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−\n\n<<(point,point) >>(point,point) ~=(point,point) <<|(point,point) |>>(point,point) <−>(point,point) <@(point,box) <^(point,point) >^(point,point) <@(point,polygon) | 48 | pt_contained_poly <@(point,circle)\n\n1 | point_left | 5 | point_right | | 6 | point_eq | 10 | point_below | 11 | point_above | 15 | point_distance | 28 | on_pb | 29 | point_below | 30 | point_above\n\n| is left of | is right of | same as | is below | is above | distance between | point inside box | deprecated, use <<| | deprecated, use |>> | is contained by | 68 | pt_contained_circle | is contained by\n\n(11 rows)\n\nOperatornamesdonotusuallytellusmuchaboutoperatorsemantics,sothisquery also displays the names of the underlying functions and their descriptions. One way or another, all the operators deal with relative positioning of geometries (left of, right of, above, below, contains, is contained) and the distance between them.\n\nAs compared to �-trees, �i�� offers more strategies. Some of the strategy numbers are common to several types of indexes,1 while others are calculated by formulas\n\n1 include/access/stratnum.h\n\n513\n\nChapter 26 GiST\n\n(for example, ��, ��, and �� virtually represent one and the same strategy: is con- tained for rectangles, polygons, and circles). Besides, �i�� supports some obsolete operator names (<<| and |>>).\n\nOperator classes may implement only some of the available strategies. For ex- ample, the contains strategy is not supported by the operator class for points, but it is available in classes defined for geometries with measurable area (box_ops, poly_ops, and circle_ops).\n\nSearch for Contained Elements\n\nA typical query that can be sped up by an index returns all points of the specified area.\n\nFor example,let’s find all the airports located within one degree from the centre of Moscow:\n\n=> SELECT airport_code, airport_name->>'en' FROM airports_big WHERE coordinates <@ '<(37.622513,55.753220),1.0>'::circle;\n\nairport_code |\n\n?column?\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSVO VKO DME BKA ZIA CKL OSF\n\n| Sheremetyevo International Airport | Vnukovo International Airport | Domodedovo International Airport | Bykovo Airport | Zhukovsky International Airport | Chkalovskiy Air Base | Ostafyevo International Airport\n\n(7 rows)\n\n=> EXPLAIN (costs off) SELECT airport_code FROM airports_big WHERE coordinates <@ '<(37.622513,55.753220),1.0>'::circle;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on airports_big\n\nRecheck Cond: (coordinates <@ '<(37.622513,55.75322),1>'::circle) −> Bitmap Index Scan on airports_gist_idx\n\nIndex Cond: (coordinates <@ '<(37.622513,55.75322),1>'::ci...\n\n(4 rows)\n\n514\n\n26.2 R-Trees for Points\n\nWe can take a closer look at this operator using a trivial example shown in the figure below:\n\n9\n\n8 7\n\n6 5\n\n4 3\n\n2\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nIf bounding boxes are selected this way, the index structure will be as follows:\n\n0,0–3,4 0,0–3,4\n\n5,3–9,9 5,3–9,9\n\n0,0–3,2 0,0–3,2\n\n0,3–3,4 0,3–3,4\n\n5,3–8,5 5,3–8,5\n\n6,6–9,9 6,6–9,9\n\n0,0 0,0\n\n1,2 1,2\n\n3,1 3,1\n\n0,4 0,4\n\n3,3 3,3\n\n5,3 5,3\n\n8,5 8,5\n\n6,6 6,6\n\n8,9 8,9\n\n9,7 9,7\n\nThe contains operator <@ determines whether a particular point is located within the specified rectangle. The consistency function for this operator1 returns “yes” if the rectangle of the index entry has any common points with this rectangle. It means that for leaf node entries, which store rectangles reduced to points, this function determines whether the point is contained within the specified rectangle.\n\n1 backend/access/gist/gistproc.c, gist_point_consistent function\n\n515\n\nChapter 26 GiST\n\nFor example,let’s find the inner points of rectangle (�,�)–(�,�),which is hatched in the figure below:\n\n9\n\n8 7\n\n6 5\n\n4 3\n\n2\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0,0–3,4 0,0–3,4\n\n5,3–9,9 5,3–9,9\n\n0,0–3,2 0,0–3,2\n\n0,3–3,4 0,3–3,4\n\n5,3–8,5 5,3–8,5\n\n6,6–9,9 6,6–9,9\n\n0,0 0,0\n\n1,2 1,2\n\n3,1 3,1\n\n0,4 0,4\n\n3,3 3,3\n\n5,3 5,3\n\n8,5 8,5\n\n6,6 6,6\n\n8,9 8,9\n\nThe search starts at the root node. The bounding box overlaps with (�,�)–(�,�), but does not overlap with (�,�)–(�,�). It means that we do not have to descend into the second subtree.\n\nAt the next level,the bounding box overlaps with (�,�)–(�,�) and touches (�,�)–(�,�), so we have to check both subtrees.\n\nOnce we get to leaf nodes, we just have to go through all the points that they con- tain and return those that satisfy the consistency function.\n\nA �-tree search always selects exactly one child node. A �i�� search,however,may have to scan several subtrees, especially if their bounding boxes overlap.\n\n516\n\n9,7 9,7",
      "page_number": 483
    },
    {
      "number": 26,
      "title": "GiST",
      "start_page": 509,
      "end_page": 544,
      "detection_method": "regex_chapter",
      "content": "26.2 R-Trees for Points\n\nNearest Neighbor Search\n\nMost of the operators supported by indexes (such as = or <@ shown in the previous example) are typically called search operators, as they define search conditions in queries. Such operators are predicates, that is, they return a logical value.\n\nBut there is also a group of ordering operators, which return the distance between arguments. Such operators are used in the ����� �� clause and are typically sup- ported by indexes that have the D������� O�������� property, which enables you to quickly find the specified number of nearest neighbors. This type of search is known as k-��, or k-nearest neighbor search.\n\nFor example, we can find �� airports closest to Kostroma:\n\n=> SELECT airport_code, airport_name->>'en' FROM airports_big ORDER BY coordinates <-> '(40.926780,57.767943)'::point LIMIT 10;\n\nairport_code |\n\n?column?\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nKMW IAR IWA VGD RYB GOJ CEE CKL ZIA BKA\n\n| Kostroma Sokerkino Airport | Tunoshna Airport | Ivanovo South Airport | Vologda Airport | Staroselye Airport | Nizhny Novgorod Strigino International Airport | Cherepovets Airport | Chkalovskiy Air Base | Zhukovsky International Airport | Bykovo Airport\n\n(10 rows)\n\n=> EXPLAIN (costs off) SELECT airport_code FROM airports_big ORDER BY coordinates <-> '(40.926780,57.767943)'::point LIMIT 5;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nLimit\n\n−> Index Scan using airports_gist_idx on airports_big\n\nOrder By: (coordinates <−> '(40.92678,57.767943)'::point)\n\n(3 rows)\n\nSince an index scan returns the results one by one and can be stopped any time, several first values can be found very quickly.\n\n517\n\np. ���\n\nChapter 26 GiST\n\nIt would be very hard to achieve efficient search without index support. We would have to find all the points that appear in a particular area and then gradually expand this area until the requested number of results is returned. It would require several index scans,not to mention the problem of choosing the size of the original area and its increments.\n\nYou can see the operator type in the system catalog (“s” stands for search, “o” de- notes ordering operators):\n\n=> SELECT amopopr::regoperator, amoppurpose, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily\n\nWHERE amname = 'gist' AND opcname = 'point_ops' ORDER BY amopstrategy;\n\namopopr\n\n| amoppurpose | amopstrategy\n\n−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n| s <<(point,point) | s >>(point,point) | s ~=(point,point) | s <<|(point,point) | s |>>(point,point) | o <−>(point,point) | s <@(point,box) | s <^(point,point) | s >^(point,point) <@(point,polygon) | s | s <@(point,circle)\n\n| | | | | | | | | | |\n\n1 5 6 10 11 15 28 29 30 48 68\n\n(11 rows)\n\nTo support such queries, an operator class must define an additional support func- tion: it is the distance function, which is called on the index entry to calculate the distance from the value stored in this entry to some other value.\n\nFor a leaf element representing an indexed value, this function must return the distance to this value. In the case of points,1 it is a regular Euclidean distance, which equals √(x2 − x1)2 + (y2 − y1)2.\n\nFor an inner element, the function must return the minimal of all the possible dis- tances from its child leaf elements. Since it is quite costly to scan all the child entries, the function can optimistically underestimate the distance (sacrificing\n\n1 backend/utils/adt/geo_ops.c, point_distance function\n\n518\n\n26.2 R-Trees for Points\n\nefficiency), but it must never return a bigger value—it would compromise search correctness.\n\nTherefore,for an inner element represented by a bounding box,the distance to the point is understood in the regular mathematical sense: it is either the minimal distance between the point and the rectangle or zero if the point is inside the rect- angle.1 This value can be easily calculated without traversing all the child points of the rectangle, and it is guaranteed to be not greater than the distance to any of these points.\n\nLet’s consider the algorithm of searching for three nearest neighbors of point (�,�):\n\n9\n\n8\n\n7 6 5\n\n4 3\n\n2\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nThe search starts at the root node, which holds two bounding boxes. The distance from the specified point to rectangle (�,�)–(�,�) is taken as the distance to the rect- angle’s corner (�,�), which equals �.�. The distance to (�,�)–(�,�) is �.�. (I am going to round all the values here to the first decimal place; such accuracy will be enough for this example.)\n\nChild nodes get traversed in the order of distance increase. Thus, we first descend into the right child node,which contains two rectangles: (�,�)–(�,�) and (�,�)–(�,�). The distance to the first one is �.�; the distance to the second one is �.�.\n\nOnce again, we choose the right subtree and get into the leaf node that contains three points: (�,�) at the distance of �.�, (�,�) at the distance of �.�, and (�,�) at the distance of �.�.\n\n1 backend/utils/adt/geo_ops.c, box_closest_point function\n\n519\n\nChapter 26 GiST\n\n5.0\n\n0,0–3,4 0,0–3,4\n\n5,3–9,9 5,3–9,9\n\n0.0\n\n0,0–3,2 0,0–3,2\n\n0,3–3,4 0,3–3,4\n\n3.0\n\n5,3–8,5 5,3–8,5\n\n6,6–9,9 6,6–9,9\n\n0.0\n\n0,0 0,0\n\n1,2 1,2\n\n3,1 3,1\n\n0,4 0,4\n\n3,3 3,3\n\n5,3 5,3\n\n8,5 8,5\n\n6,6 6,6\n\n8,9 8,9\n\n9,7 9,7\n\n2.0 2.2 3.2\n\nThus, we have received the first two points: (�,�) and (�,�). But the distance to the third point of this node is greater than the distance to rectangle (�,�)–(�,�).\n\nSo now we have to descend into the left child node,which contains two points. The distance to point (�,�) is �.�,while the distance to (�,�) is �.�. It turns out that point (�,�) in the previous child node is closer to point (�,�) than any of the nodes of the left subtree, so we can return it as the third result.\n\n5.0\n\n0,0–3,4 0,0–3,4\n\n5,3–9,9 5,3–9,9\n\n0.0\n\n0,0–3,2 0,0–3,2\n\n0,3–3,4 0,3–3,4\n\n3.0\n\n5,3–8,5 5,3–8,5\n\n6,6–9,9 6,6–9,9\n\n0.0\n\n0,0 0,0\n\n1,2 1,2\n\n3,1 3,1\n\n0,4 0,4\n\n3,3 3,3\n\n5,3 5,3\n\n8,5 8,5\n\n6,6 6,6\n\n8,9 8,9\n\n9,7 9,7\n\n5.1 3.6\n\n2.0 2.2 3.2\n\n520\n\n26.2 R-Trees for Points\n\nThis example illustrates the requirements that must be satisfied by the distance function for inner entries. Because of the reduced distance (�.� instead of �.�) to rectangle (�,�)–(�,�), an extra node had to be scanned, so search efficiency has declined; however, the algorithm itself remained correct.\n\nInsertion\n\nWhen a new key is getting inserted into an �-tree, the node to be used for this key is determined by the penalty function: the size of the bounding box must be increased as little as possible.1\n\nFor example, point (�,�) will be added to rectangle (�,�)–(�,�) because its area will increase by only � units, while rectangle (�,�)–(�,�) would have to be increased by �� units. At the next (leaf) level, the point will be added to rectangle (�,�)–(�,�), following the same logic.\n\n9\n\n9\n\n8\n\n8\n\n7\n\n7\n\n6 5\n\n6 5\n\n4 3\n\n4 3\n\n2\n\n2\n\n1 0\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nAssuming that a page holds three elements at the most, it has to be split in two, and the elements have to be distributed between the new pages. In this example, the result seems obvious, but in the general case the data distribution task is not\n\n1 backend/access/gist/gistproc.c, gist_box_penalty function\n\n521\n\np. ���\n\nChapter 26 GiST\n\nso trivial. First and foremost, the picksplit function attempts to minimize overlaps between bounding boxes, aiming at getting smaller rectangles and uniform distri- bution of points between pages.1\n\n9\n\n8\n\n7\n\n6 5\n\n4 3\n\n2\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nExclusion Constraints\n\nGi�� indexes can also be used in exclusion constraints.\n\nAn exclusion constraint guarantees that the specified fields of any two heap tuples do not match each other in the sense of some operator. The following conditions must be satisfied:\n\nThe exclusion constraint must be supported by the indexing method (the C��\n\nE������ property).\n\nThe operator must belong to the operator class of this indexing method.\n\nThe operator must be commutative, that is, the condition “a operator b =\n\nb operator a” must be true.\n\nFor the hash and btree access methods considered above,the only suitable operator is equal to. It virtually turns an exclusion constraint into a unique one,which is not particularly useful.\n\n1 backend/access/gist/gistproc.c, gist_box_picksplit function\n\n522\n\n26.2 R-Trees for Points\n\nThe gist method has two more applicable strategies:\n\noverlapping: the && operator\n\nadjacency: the -|- operator (defined for intervals)\n\nTo try it out,let’s create a constraint that forbids placing airports too close to each other. This condition can be formulated as follows: circles of a particular radius with centers lying at the airports’ coordinates must not overlap:\n\n=> ALTER TABLE airports_data ADD EXCLUDE USING gist (circle(coordinates,0.2) WITH &&);\n\n=> INSERT INTO airports_data(\n\nairport_code, airport_name, city, coordinates, timezone\n\n) VALUES (\n\n'ZIA', '{}', '{\"en\": \"Moscow\"}', point(38.1517, 55.5533), 'Europe/Moscow'\n\n);\n\nERROR: \"airports_data_circle_excl\" DETAIL: precision))=(<(38.1517,55.5533),0.2>) conflicts with existing key (circle(coordinates, 0.2::double precision))=(<(37.90629959106445,55.40879821777344),0.2>).\n\nconflicting key value violates exclusion constraint\n\nKey (circle(coordinates, 0.2::double\n\nWhen an exclusion constraint is defined, an index to enforce it is added automati- cally. Here it is a �i�� index built over an expression.\n\nLet’s take a look at a more complex example. Suppose we need to allow close prox- imity of airports, but only if they belong to the same city. A possible solution is to define a new integrity constraint that can be formulated as follows: it is forbidden to have pairs of rows with intersections (&&) of circles if their centers lie at the airports’ coordinates and the corresponding cities have different names (!=).\n\nAn attempt to create such a constraint results in an error because there is no op- erator class for the text data type:\n\n=> ALTER TABLE airports_data DROP CONSTRAINT airports_data_circle_excl; -- delete old data\n\n=> ALTER TABLE airports_data ADD EXCLUDE USING gist (\n\ncircle(coordinates,0.2) WITH &&,\n\n(city->>'en') WITH !=\n\n);\n\n523\n\nChapter 26 GiST\n\nERROR: method \"gist\" HINT: default operator class for the data type.\n\ndata type text has no default operator class for access\n\nYou must specify an operator class for the index or define a\n\nHowever, �i�� does provide strategies like strictly left of, strictly right of, and same, which can also be applied to regular ordinal data types, such as numbers or text strings. Thebtree_gistextensionisspecificallyintendedtoimplement�i��support for operations that are typically used with �-trees:\n\n=> CREATE EXTENSION btree_gist;\n\n=> ALTER TABLE airports_data ADD EXCLUDE USING gist (\n\ncircle(coordinates,0.2) WITH &&,\n\n(city->>'en') WITH !=\n\n);\n\nALTER TABLE\n\nThe constraint is created. Now we cannot add Zhukovsky airport belonging to a town with the same name because Moscow airports are too close:\n\n=> INSERT INTO airports_data(\n\nairport_code, airport_name, city, coordinates, timezone\n\n) VALUES (\n\n'ZIA', '{}', '{\"en\": \"Zhukovsky\"}', point(38.1517, 55.5533), 'Europe/Moscow'\n\n);\n\nERROR: \"airports_data_circle_expr_excl\" DETAIL: 'en'::text))=(<(38.1517,55.5533),0.2>, Zhukovsky) conflicts with existing key (circle(coordinates, 0.2::double precision), (city −>> 'en'::text))=(<(37.90629959106445,55.40879821777344),0.2>, Moscow).\n\nconflicting key value violates exclusion constraint\n\nKey (circle(coordinates, 0.2::double precision), (city −>>\n\nBut we can do it if we specify Moscow as this airport’s city:\n\n=> INSERT INTO airports_data(\n\nairport_code, airport_name, city, coordinates, timezone\n\n) VALUES (\n\n'ZIA', '{}', '{\"en\": \"Moscow\"}', point(38.1517, 55.5533), 'Europe/Moscow'\n\n);\n\nINSERT 0 1\n\n524\n\n26.2 R-Trees for Points\n\nIt is important to remember that even though GiST supports greater than,less than, and equal to operations, �-trees are much more efficient in this respect, especially when accessing a range of values. So it makes sense to use the trick with the btree_gist extension shown above only if the �i�� index is really needed for other legitimate reasons.\n\nProperties\n\nAccess method properties. Here are the properties of the gist method:\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'gist';\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | f | can_order | can_unique | f | can_multi_col | t | t | can_exclude | t | can_include\n\ngist gist gist gist gist\n\n(5 rows)\n\nUnique constraints and sorting are not supported.\n\nA �i�� index can be created\n\nwith additional ������� columns.\n\nAsweknow,wecanbuildanindexoverseveralcolumns,aswellasuseitinintegrity constraints.\n\nIndex-level properties. These properties are defined at the index level:\n\n=> FROM unnest(array[\n\nSELECT p.name, pg_index_has_property('airports_gist_idx', p.name)\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\n525\n\nv. ��\n\nChapter 26 GiST\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| t clusterable | t index_scan bitmap_scan | t backward_scan | f\n\n(4 rows)\n\nA �i�� index can be used for clusterization.\n\nAs for data retrieval methods, both regular (row-by-row) index scans and bitmap scans are supported. However, backward scanning of �i�� indexes is not allowed.\n\nColumn-level properties. Most of the column properties are defined at the access method level, and they remain the same:\n\n=> SELECT p.name,\n\npg_index_column_has_property('airports_gist_idx', 1, p.name)\n\nFROM unnest(array[\n\n'orderable', 'search_array', 'search_nulls'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n| f orderable search_array | f search_nulls | t\n\n(3 rows)\n\nAll sort-related properties are disabled.\n\nN��� values are allowed, but �i�� is not really efficient at handling them. It is assumed that a ���� value does not increase the bounding box; such values get inserted into random subtrees, so they have to be searched for in the whole tree.\n\nHowever,a couple of column-level properties do depend on the particular operator class:\n\n=> SELECT p.name,\n\npg_index_column_has_property('airports_gist_idx', 1, p.name)\n\nFROM unnest(array[\n\n'returnable', 'distance_orderable'\n\n]) p(name);\n\n526\n\n26.3 RD-Trees for Full-Text Search\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nreturnable | t distance_orderable | t\n\n(2 rows)\n\nIndex-only scans are allowed, since leaf nodes keep full index keys.\n\nAs we have seen above, this operator class provides the distance operator for near- est neighbor search. The distance to a ���� value is considered to be ����; such values are returned last (similar to the ����� ���� clause in �-trees).\n\nHowever, there is no distance operator for range types (which represent segments, that is, linear geometries rather than areal ones), so this property is different for an index built for such types:\n\n=> CREATE TABLE reservations(during tsrange);\n\n=> CREATE INDEX ON reservations USING gist(during);\n\n=> SELECT p.name,\n\npg_index_column_has_property('reservations_during_idx', 1, p.name)\n\nFROM unnest(array[\n\n'returnable', 'distance_orderable'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nreturnable | t distance_orderable | f\n\n(2 rows)\n\n26.3 RD-Trees for Full-Text Search\n\nAbout Full-Text Search\n\nThe objective of full-text search1 is to select those documents from the provided set that match the search query.\n\n1 postgresql.org/docs/14/textsearch.html\n\n527\n\nChapter 26 GiST\n\nTo be searched, the document is cast to the tsvector type, which contains lexemes and their positions in the document. Lexemes are words converted into a format that is suitable for search. By default, all words are normalized to lowercase, and their endings are cut off:\n\n=> SET default_text_search_config = english;\n\n=> SELECT to_tsvector(\n\n'No one can tell me, nobody knows, ' || 'Where the wind comes from, where the wind goes.'\n\n);\n\nto_tsvector −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 'come':11 'goe':16 'know':7 'nobodi':6 'one':2 'tell':4 'wind':10,15\n\n(1 row)\n\nThe so-called stop words(like“the”or“from”) are filtered out: they are assumed to occur too often for the search to return any meaningful results for them. Naturally, all these transformations are configurable.\n\nA search query is represented by another type: tsquery. Any query includes one or more lexemes bound by logical connectives: & (���), | (��), ! (���). You can also use parentheses to define operator precedence.\n\n=> SELECT to_tsquery('wind & (comes | goes)');\n\nto_tsquery −−−−−−−−−−−−−−−−−−−−−−−−−−−−− 'wind' & ( 'come' | 'goe' )\n\n(1 row)\n\nThe only operator used for full-text search is the match operator @@:\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'gist' AND opcname = 'tsvector_ops' ORDER BY amopstrategy;\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n@@(tsvector,tsquery) | ts_match_vq |\n\n1\n\n(1 row)\n\n528\n\n26.3 RD-Trees for Full-Text Search\n\nThis operator determines whether the document satisfies the query. Here is an example:\n\n=> SELECT to_tsvector('Where the wind comes from, where the wind goes')\n\n@@ to_tsquery('wind & coming');\n\n?column? −−−−−−−−−−\n\nt\n\n(1 row)\n\nIt is by no means an exhaustive should be sufficient for understanding indexing fundamentals.\n\ndescription of full-text search,but this information\n\nIndexing tsvector Data\n\nTo work fast, full-text search has to be supported by an index.1 Since it is not documents themselves but tsvector values that get indexed, you have two options here: either build an index on an expression and perform a type cast, or add a separate column of the tsvector type and index this column. The benefit of the first approach is that it does not waste any space on storing tsvector values, which are actuallynotneededassuch. Butitisslowerthanthesecondoption,astheindexing engine has to recheck all the heap tuples returned by the access method. It means that the tsvector value has to be calculated again for each rechecked row,and as we soon will see, �i�� rechecks all rows.\n\nLet’s construct a simple example. We are going to create a two-column table: the first column will store the document, while the second one will hold the tsvector value. Wecanuseatriggertoupdatethesecondcolumn,2 butitismoreconvenient to simply declare this column as generated:\n\n3\n\n=> CREATE TABLE ts(\n\ndoc text, doc_tsv tsvector GENERATED ALWAYS AS (\n\nto_tsvector('pg_catalog.english', doc)\n\n) STORED\n\n);\n\n1 postgresql.org/docs/14/textsearch-indexes.html 2 postgresql.org/docs/14/textsearch-features#TEXTSEARCH-UPDATE-TRIGGERS.html 3 postgresql.org/docs/14/ddl-generated-columns.html\n\n529\n\np. ���\n\np. ���\n\nv. ��\n\nenglish\n\nChapter 26 GiST\n\n=> CREATE INDEX ts_gist_idx ON ts USING gist(doc_tsv);\n\nIn the examples above,I used the to_tsvector function with a single argument,having set the default_text_search_config parameter to define the full-text search configuration. The volatility category of this function flavor is ������, since it is implicitly dependent on the parameter value. But here I apply another flavor that defines the configuration explicitly; this flavor is ��������� and can be used in generation expressions.\n\nLet’s insert several rows:\n\n=> INSERT INTO ts(doc) VALUES\n\n('Old MacDonald had a farm'), ('And on his farm he had some cows'), ('Here a moo, there a moo'), ('Everywhere a moo moo'), ('Old MacDonald had a farm'), ('And on his farm he had some chicks'), ('Here a cluck, there a cluck'), ('Everywhere a cluck cluck'), ('Old MacDonald had a farm'), ('And on his farm he had some pigs'), ('Here an oink, there an oink'), ('Everywhere an oink oink')\n\nRETURNING doc_tsv;\n\ndoc_tsv −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 'farm':5 'macdonald':2 'old':1 'cow':8 'farm':4 'moo':3,6 'everywher':1 'moo':3,4 'farm':5 'macdonald':2 'old':1 'chick':8 'farm':4 'cluck':3,6 'cluck':3,4 'everywher':1 'farm':5 'macdonald':2 'old':1 'farm':4 'pig':8 'oink':3,6 'everywher':1 'oink':3,4\n\n(12 rows) INSERT 0 12\n\nAs such, an �-tree is of no good for indexing documents, since the concept of bounding boxes makes no sense for them. Therefore, its ��-tree (Russian Doll)\n\n530\n\n26.3 RD-Trees for Full-Text Search\n\nmodification is used. Instead of a bounding box, such a tree uses a bounding set, that is, a set that contains all the elements of its child sets. For full-text search, such a set contains lexemes of the document, but in the general case a bounding set can be arbitrary.\n\nThere are several ways to represent bounding sets in index entries. The simplest one is to enumerate all the elements of the set.\n\nHere is how it might look like:\n\ncow,everywher, cow,everywher, farm,macdonald, farm,macdonald, moo,old moo,old\n\nchick,cluck, chick,cluck, everywher,farm, everywher,farm, oink,pig oink,pig\n\nfarm, farm, macdonald,old macdonald,old\n\ncow,everywher, cow,everywher, farm,moo farm,moo\n\nchick,cluck, chick,cluck, everywher,farm everywher,farm\n\neverywher,farm, everywher,farm, oink,pig oink,pig\n\nfarm,macdonald,old farm,macdonald,old\n\ncow,farm cow,farm\n\nchick,farm chick,farm\n\nfarm,pig farm,pig\n\nfarm,macdonald,old farm,macdonald,old\n\nmoo moo\n\ncluck cluck\n\noink oink\n\nfarm,macdonald,old farm,macdonald,old\n\neverywher,moo everywher,moo\n\ncluck,everywher cluck,everywher\n\neverywher,oink everywher,oink\n\nTo find the documents that satisfy the ���_��� @@ ��_�������(’���’) condition, we need to descend into the nodes whose child entries are known to contain the “cow” lexeme.\n\nThe problems of such representation are obvious. The number of lexemes in a document can be enormous, while the page size is limited. Even if each particular document does not have too many distinct lexemes when taken separately, their united sets at upper levels of the tree may still turn out too big.\n\n531\n\np. ���\n\nChapter 26 GiST\n\ncow,everywher, cow,everywher, farm,macdonald, farm,macdonald, moo,old moo,old\n\nchick,cluck, chick,cluck, everywher,farm, everywher,farm, oink,pig oink,pig\n\nfarm, farm, macdonald,old macdonald,old\n\ncow,everywher, cow,everywher, farm,moo farm,moo\n\nchick,cluck, chick,cluck, everywher,farm everywher,farm\n\neverywher,farm, everywher,farm, oink,pig oink,pig\n\nfarm,macdonald,old farm,macdonald,old\n\ncow,farm cow,farm\n\nchick,farm chick,farm\n\nfarm,pig farm,pig\n\nfarm,macdonald,old farm,macdonald,old\n\nmoo moo\n\ncluck cluck\n\noink oink\n\nfarm,macdonald,old farm,macdonald,old\n\neverywher,moo everywher,moo\n\ncluck,everywher cluck,everywher\n\neverywher,oink everywher,oink\n\nFull-text search uses another solution, namely a more compact signature tree. It should be well familiar to anyone who had to deal with the Bloom filter.\n\nEach lexeme can be represented by its signature: a bit string of a particular length in which only one of the bits is set to 1. The bit that should be set is determined by the hash function of the lexeme.\n\nA document’s signature is the result of a bitwise �� operation on signatures of all the lexemes in this document.\n\nSuppose we have assigned the following signatures to our lexemes:\n\nchick cluck cow everywher farm\n\n1000000 0001000 0000010 0010000 0000100\n\nmacdonald\n\n0100000\n\nmoo oink old\n\n0000100 0000010 0000001\n\npig\n\n0010000\n\n532\n\n26.3 RD-Trees for Full-Text Search\n\nThen the documents’\n\nOld MacDonald had a farm\n\n0100101\n\nsignatures will be as follows:\n\nAnd on his farm he had some cows Here a moo, there a moo Everywhere a moo moo And on his farm he had some chicks Here a cluck, there a cluck Everywhere a cluck cluck And on his farm he had some pigs Here an oink, there an oink Everywhere an oink oink\n\n0000110 0000100 0010100 1000100 0001000 0011000 0010100 0000010 0010010\n\nAnd the index tree can be represented like this:\n\n0110111 0110111\n\n1011110 1011110\n\n0100101 0100101\n\n0010110 0010110\n\n1011100 1011100\n\n0010110 0010110\n\n0100101 0100101\n\n0000110 0000110\n\n1000100 1000100\n\n0010100 0010100\n\n0100101 0100101\n\n0000100 0000100\n\n0001000 0001000\n\n0000010 0000010\n\n0100101 0100101\n\n0010100 0010100\n\n0011000 0011000\n\n0010010 0010010\n\nThe advantages of this approach are obvious: index entries have the same size, which is quite small, so the index turns out quite compact. But there are certain disadvantages too. To begin with, it is impossible to perform an index-only scan because the index does not store index keys anymore,and each returned ��� has to be rechecked by the table. The accuracy also suffers: the index may return many false positives, which have to be filtered out during a recheck.\n\nLet’s take another look at the ���_��� @@ ��_�������(’����’) condition. The sig- nature of a query is calculated in the same way as that of a document; in this\n\n533\n\nv. ��\n\nChapter 26 GiST\n\nparticular case it equals 0000010. The consistency function1 must find all the child nodes that have the same bits set in their signatures:\n\n0110111 0110111\n\n1011110 1011110\n\n0100101 0100101\n\n0010110 0010110\n\n1011100 1011100\n\n0010110 0010110\n\n0100101 0100101\n\n0000110 0000110\n\n1000100 1000100\n\n0010100 0010100\n\n0100101 0100101\n\n0000100 0000100\n\n0001000 0001000\n\n0000010 0000010\n\n0100101 0100101\n\n0010100 0010100\n\n0011000 0011000\n\n0010010 0010010\n\nAs compared with the previous example, more nodes have to be scanned here be- cause of false-positive hits. Since the signature’s capacity is limited, some of the lexemes in a large set are bound to have the same signatures. In this example,such lexemes are“cow”and“oink.” It means that one and the same signature can match different documents; here the signature of the query corresponds to three of them.\n\nFalse positives reduce index efficiency but do not affect its correctness in any way: since false negatives are guaranteed to be ruled out, the required value cannot be missed.\n\nClearly,thesignaturesizeisactuallybigger. Bydefault,ittakes���bytes(���bits), so the probability of collisions is much lower than in this example. If required, you can further increase the signature size up to about ���� bytes using the operator class parameter:\n\nCREATE INDEX ... USING gist(column tsvector_ops(siglen = size));\n\n1 backend/utils/adt/tsgistidx.c, gtsvector_consistent function\n\n534\n\n26.3 RD-Trees for Full-Text Search\n\nBesides, if values are small enough (a bit smaller than 1 of the page, which takes 16 about ��� bytes for a standard page),1 it is tsvector values themselves rather than their signatures that the tsvector_ops operator class keeps in leaf pages of an index.\n\nTo see how indexing works on real data, we can take the pgsql-hackers mailing list archive.2 It contains ���,��� emails together with their send dates, subjects, author names, and body texts.\n\nLet’s add a column of the tsvector type and build an index. Here I combine three values (subject,author,and body text) into a single vector to show that documents can be generated dynamically and do not have to be stored in a single column.\n\n=> ALTER TABLE mail_messages ADD COLUMN tsv tsvector GENERATED ALWAYS AS ( to_tsvector(\n\n'pg_catalog.english', subject||' '||author||' '||body_plain\n\n) ) STORED;\n\nNOTICE: DETAIL:\n\nword is too long to be indexed Words longer than 2047 characters are ignored.\n\n...\n\nNOTICE: DETAIL: ALTER TABLE\n\nword is too long to be indexed Words longer than 2047 characters are ignored.\n\n=> CREATE INDEX mail_gist_idx ON mail_messages USING gist(tsv);\n\n=> SELECT pg_size_pretty(pg_relation_size('mail_gist_idx'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n127 MB (1 row)\n\nAs the column was being filled, a certain number of largest words were filtered out because of their size. But once the index is ready, it can be used in search queries:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM mail_messages WHERE tsv @@ to_tsquery('magic & value');\n\n1 backend/utils/adt/tsgistidx.c, gtsvector_compress function 2 edu.postgrespro.ru/mail_messages.sql.gz\n\n535\n\np. ���\n\nChapter 26 GiST\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using mail_gist_idx on mail_messages\n\n(actual rows=898 loops=1) Index Cond: (tsv @@ to_tsquery('magic & value'::text)) Rows Removed by Index Recheck: 7859\n\n(4 rows)\n\nTogether with ��� rows that satisfy the condition,the access method also returned ���� rows to be later filtered out by a recheck. If we increase the signature capacity, the accuracy (and, consequently, the index efficiency) will be improved, but the index size will grow:\n\n=> DROP INDEX mail_messages_tsv_idx;\n\n=> CREATE INDEX ON mail_messages USING gist(tsv tsvector_ops(siglen=248));\n\n=> SELECT pg_size_pretty(pg_relation_size('mail_messages_tsv_idx'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n139 MB (1 row)\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM mail_messages WHERE tsv @@ to_tsquery('magic & value');\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using mail_messages_tsv_idx on mail_messages\n\n(actual rows=898 loops=1) Index Cond: (tsv @@ to_tsquery('magic & value'::text)) Rows Removed by Index Recheck: 2060\n\n(4 rows)\n\nProperties\n\nI have already shown the access method properties ,and most of them are the same for all operator classes. But the following two column-level properties are worth mentioning:\n\n536\n\n26.4 Other Data Types\n\n=> SELECT p.name,\n\npg_index_column_has_property('mail_messages_tsv_idx', 1, p.name)\n\nFROM unnest(array[\n\n'returnable', 'distance_orderable'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n| f returnable distance_orderable | f\n\n(2 rows)\n\nIndex-only scans are now impossible,as the original value cannot be restored from its signature. It is perfectly fine in this particular case: the tsvector value is only used for search, while we need to retrieve the document itself.\n\nThe ordering operator for the tsvector_ops class is not defined either.\n\n26.4 Other Data Types\n\nIhaveconsideredonlytwomostprominentexamples. Theyshowthateventhough the �i�� method is based on a balanced tree, it can be used for various data types thanks to different support function implementations in different operator classes. When we speak about a �i�� index,we must always specify the operator class,since it is crucial for index properties.\n\nHere are several more data types currently supported by the �i�� access method.\n\nGeometric data types. Apart from points, �i�� can index other geometric objects: rectangles,circles,polygons. All these objects are represented by their bound- ing boxes for this purpose.\n\nThe cube extension adds the same-name data type that represents multidi- mensional cubes. They are indexed using �-trees with bounding boxes of the corresponding dimension.\n\nRange types. Postgre��� provides several built-in numeric and temporal range types, such as int4range and tstzrange.1 Custom range types can be defined using the ������ ���� �� ����� command.\n\n1 postgresql.org/docs/14/rangetypes.html\n\n537\n\nv. ��\n\nChapter 26 GiST\n\nAny range types, both standard and custom, are supported by �i�� via the range_ops operator class.1 For indexing, a one-dimensional �-tree is applied: bounding boxes are transformed to bounding segments in this case.\n\nare supported as well; they rely on the multirange_ops class. Multirange types A bounding range comprises all the ranges that are part of a multirange value.\n\nThesegextensionprovidesthesame-namedatatypeforintervalswithbounds defined with particular accuracy. It is not considered to be a range type, but it virtually is, so it is indexed in exactly the same manner.\n\nOrdinal types. Let’s recall the btree_gist extension once again: it provides operator classes for the �i�� method to support various ordinal data types, which are typically indexed by a �-tree. Such operator classes can be used to build a multicolumn index when the data type in one of the columns is not supported by �-trees.\n\nNetwork address types. The inet data type has built-in �i�� support, which is im-\n\nplemented via the inet_ops2 operator class.\n\nInteger arrays. The intarray extension expands the functionality of integer arrays to add �i�� support for them. There are two classes of operators. For small arrays, you can use gist__int_ops, which implements the ��-tree with full rep- resentation of keys in index entries. Large arrays will benefit from a more compact but less precise signature ��-tree based on the gist__bigint_ops op- erator class.\n\nExtra underscores in the names of operator classes belong to the names of arrays of basic types. For instance, alongside the more common int4[] notation, an integer array can be denoted as _int4. There are no _int and _bigint types though.\n\nLtree. The ltree extension adds the same-name data type for tree-like structures with labels. Gi�� support is provided via signature ��-trees that use the gist_ltree_ops operator class for ltree values and the gist__ltree_ops operator class for arrays of the ltree type.\n\n1 backend/utils/adt/rangetypes_gist.c 2 backend/utils/adt/network_gist.c\n\n538\n\n26.4 Other Data Types\n\nKey–value storage. The hstore extension provides the hstore data type for storing key–value pairs. The gist_hstore_ops operator class implements index support based on a signature ��-tree.\n\nTrigrams. The pg_trgm\n\nextension adds the gist_trgm_ops class, which implements\n\nindex support for comparing text strings and wildcard search.\n\n539\n\np. ���\n\n27\n\nSP-GiST\n\n27.1 Overview\n\nThe first letters in the ��-�i�� name stand for Space Partitioning. The space here is understood as an arbitrary set of values on which the search is performed; it is not necessarily the space in the conventional sense of the word (such as a two- dimensional plane). The �i�� part of the name hints at certain similarity between �i�� and ��-�i�� methods: both of them are generalized search trees and serve as frameworks for indexing various data types.\n\nThe idea behind the ��-�i�� method1 is to split the search space into several non- overlapping regions, which are in turn can be recursively split into sub-regions. Such partitioning produces non-balanced trees (which differ from �-trees and �i�� trees) and can be used to implement such well-known structures as quadtrees,k-� trees, and radix trees (tries).\n\nNon-balanced trees typically have few branches and, consequently, large depth. For example, a quadtree node has four child nodes at the most, while a node of a k-� tree can have only two. It does not pose any problems if the tree is kept in memory; but when stored on disk, tree nodes have to be packed into pages as densely as possible to minimize �/�, and this task is not so trivial. B-tree and �i�� indexes do not have to take care of it because each of their tree nodes takes the whole page.\n\nAn inner node of an ��-�i�� tree contains a value that satisfies the condition that holds true for all its child nodes. Such a value is often called a prefix; it plays the\n\n1 postgresql.org/docs/14/spgist.html backend/access/spgist/README\n\n540\n\n27.2 Quadtrees for Points\n\nsame role as the predicate in �i�� indexes. Pointers to ��-�i�� child nodes may have labels.\n\nLeaf node elements contain an indexed value (or some part of it) and the corre- sponding ���.\n\nJust like �i��, the ��-�i�� access method implements only the main algorithms, taking care of such low-level details as concurrent access, locks, and logging. New data types and algorithms of space partitioning can be added via the operator class interface. The operator class provides most of the logic and defines many aspects of indexing functionality.\n\nIn ��-�i��, the search is depth-first, starting at the root node.1 The nodes that are worth descending into are chosen by the consistency function, similar to the one usedin�i��. Foraninnernodeofthetree,thisfunctionreturnsasetofchildnodes whose values do not contradict the search predicate. The consistency function does not descend into these nodes: it merely assesses the corresponding labels and prefixes. For leaf nodes, it determines whether the indexed value of this node matches the search predicate.\n\nIn a non-balanced tree, search time can vary depending on the branch depth.\n\nThere are two support functions that participate in insertion of values into an ��-�i�� index. As the tree is being traversed from the root node, the choose func- tion takes one of the following decisions: send the new value into an existing child node, create a new child node for this value, or split the current node (if the value does not match this node’s prefix). If the chosen leaf page does not have enough space, the picksplit function determines which nodes should be moved to a new page.\n\nNow I will provide some examples to illustrate these algorithms.\n\n27.2 Quadtrees for Points\n\nQuadtrees are used for indexing points on a two-dimensional plane. The plane is recursively split into four regions (quadrants) with respect to the selected point.\n\n1 backend/access/spgist/spgscan.c, spgWalk function\n\n541\n\nChapter 27 SP-GiST\n\nThis point is called a centroid; it serves as the node prefix, that is, the condition that defines the location of child values.\n\nThe root node splits the plane into four quadrants.\n\nThen each of them is further split into its own quadrants.\n\n542\n\n27.2 Quadtrees for Points\n\nThis procedure goes on until the desired number of partitions is reached.\n\nThis example uses an index built on an extended airports table . The illustrations show that branch depth depends on point density in the corresponding quadrants. Forvisualclarity,Isetasmallvalueofthefillfactor storageparameter,whichmakes the tree deeper:\n\n=> CREATE INDEX airports_quad_idx ON airports_big USING spgist(coordinates) WITH (fillfactor = 10);\n\nThe default operator class for points is quad_point_ops.\n\nOperator Class\n\nI have already mentioned ��-�i�� support functions:1 the consistency function for search and the picksplit function for insertions.\n\nNow let’s take a look at the list of support functions of the quad_point_ops operator class.2 All of them are mandatory.\n\n1 postgresql.org/docs/14/spgist-extensibility.html 2 backend/access/spgist/spgquadtreeproc.c\n\n543\n\np. ���\n\n80\n\np. ���\n\nChapter 27 SP-GiST\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'spgist' AND opcname = 'quad_point_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n1 | spg_quad_config 2 | spg_quad_choose 3 | spg_quad_picksplit 4 | spg_quad_inner_consistent 5 | spg_quad_leaf_consistent\n\n(5 rows)\n\nThese functions perform the following tasks:\n\n� The config function reports basic information about the operator class to the\n\naccess method.\n\n� The choose function select the node for insertions.\n\n� The picksplit function distributes nodes between pages after a page split.\n\n� The inner_consistent function checks whether the value of the inner node sat-\n\nisfies the search predicate.\n\n� The leaf_consistent function determines whether the value stored in the leaf\n\nnode satisfies the search predicate.\n\nThere are also several optional functions.\n\nThe quad_point_ops operator class supports the same strategies as �i��:\n\n1\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'spgist' AND opcname = 'quad_point_ops' ORDER BY amopstrategy;\n\n1 include/access/stratnum.h\n\n544\n\n27.2 Quadtrees for Points\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n| <<(point,point) | >>(point,point) | ~=(point,point) | <@(point,box) | <<|(point,point) | point_below |>>(point,point) | point_above | <−>(point,point) | point_distance | | <^(point,point) | >^(point,point)\n\n| point_left | point_right | point_eq | on_pb\n\n| point_below | point_above\n\n1 5 6 8 10 11 15 29 30\n\n(9 rows)\n\nFor example, you can use the above operator >^ to find the airports located to the North of Dikson:\n\n=> SELECT airport_code, airport_name->>'en' FROM airports_big WHERE coordinates >^ '(80.3817,73.5167)'::point;\n\nairport_code |\n\n?column?\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−− | Thule Air Base | Eureka Airport | Alert Airport | Resolute Bay Airport | Svalbard Airport, Longyear | Qaanaaq Airport | Grise Fiord Airport | Dikson Airport\n\nTHU YEU YLT YRB LYR NAQ YGZ DKS\n\n(8 rows)\n\n=> EXPLAIN (costs off) SELECT airport_code FROM airports_big WHERE coordinates >^ '(80.3817,73.5167)'::point;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on airports_big\n\nRecheck Cond: (coordinates >^ '(80.3817,73.5167)'::point) −> Bitmap Index Scan on airports_quad_idx\n\nIndex Cond: (coordinates >^ '(80.3817,73.5167)'::point)\n\n(4 rows)\n\nLet’s take a closer look at the structure and inner workings of a quadtree. We will use the same simple example with several points that we discussed in the chapter related to �i��.\n\n545\n\np. ���\n\nChapter 27 SP-GiST\n\nHere is how the plane can be partitioned in this case:\n\n9\n\n9\n\n8\n\n8\n\n7\n\n��\n\n�\n\n7\n\n6 5\n\n6 5\n\n4 3\n\n2\n\n���\n\n��\n\n4 3\n\n2\n\n1 0\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nThe left illustration shows quadrant numbering at one of the tree levels; in the il- lustrations that follow, I will place child nodes from left to right in the same order for the sake of clarity. Points that lie on the boundaries are included into the quad- rant with the smaller number. The right illustration shows the final partitioning.\n\nYou can see a possible structure of this index below. Each inner node references fourchildnodesatthemost,andeachofthesepointersislabeledwiththequadrant number:\n\n5,5\n\n�\n\n���\n\n��\n\n7,7\n\n5,3\n\n3,2\n\n�\n\n���\n\n�\n\n��\n\n��\n\n�� ���\n\n8,9\n\n8,5\n\n6,6\n\n3,3\n\n3,1\n\n0,0\n\n9,7\n\n1,2\n\n546\n\n8\n\n0,4\n\n9\n\n27.2 Quadtrees for Points\n\nPage Layout\n\nUnlike �-tree and �i�� indexes, ��-�i�� has no one-to-one correspondence be- tween tree nodes and pages. Since inner nodes usually do not have too many children, several nodes have to be packed into a single page. Different types of nodes are stored in different pages: inner nodes are stored in inner pages, while leaf nodes go to leaf pages.\n\nIndex entries stored in inner pages hold the value used as a prefix, as well as a set of pointers to child nodes; each pointer may be accompanied by a label.\n\nLeaf page entries consist of a value and a ���.\n\nAll leaf nodes related to a particular inner node are stored together in a single page and are bound into a list. If the page cannot accommodate another node, this list can be moved to a different page,1 or the page can be split; one way or the other, a list never stretches over several pages.\n\nTo save space,the algorithm tries to add new nodes into the same pages until these pages are completely filled. The numbers of the last pages used are cached by back- ends and are periodically saved in the zero page, which is called a metapage. The metapage contains no reference to the root node, which we would have seen in a �-tree; the root of an ��-�i�� index is always located in the first page.\n\nUnfortunately, the pageinspect extension does not provide any functions for exploring ��-�i��,but we can use an external extension called gevel.2 It was attempted to integrate its functionality into pageinspect, but with no success.3\n\nLet’s get back to our example. The illustration below shows how tree nodes can be distributed between pages. The quad_point_ops operator class does not actually use labels. Since a node can have four child nodes at the most, the index keeps a fixed-size array of four pointers, some of which may be empty.\n\n1 backend/access/spgist/spgdoinsert.c, moveLeafs function 2 sigaev.ru/git/gitweb.cgi?p=gevel.git 3 commitfest.postgresql.org/15/1207\n\n547\n\nChapter 27 SP-GiST\n\nroot page\n\n5,55,5\n\ninner pages\n\n7,7 7,7\n\n3,2 3,2\n\nleaf pages\n\n8,9 8,9\n\n9,7 9,7\n\n8,5 8,5\n\n6,6 6,6\n\n5,3 5,3\n\n3,3 3,3\n\n3,1 3,1\n\n0,0 0,0\n\n1,2 1,2\n\n0,40,4\n\nSearch\n\nLet’s use the same example to take a look at the algorithm of searching for points located above point (�,�).\n\n9\n\n8\n\n7\n\n6 5\n\n4 3\n\n2\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nThe search starts at the root. The inner consistency function1 determines the child nodes to be descended into. Point (�,�) is compared with the root node’s centroid\n\n1 backend/access/spgist/spgquadtreeproc.c, spg_quad_inner_consistent function\n\n548\n\n27.2 Quadtrees for Points\n\n(�,�) to choose the quadrants that may contain the sought-after points; in this ex- ample, these are quadrants � and ��.\n\nOnce inside the node with centroid (�,�), we again have to choose the child nodes to descend into. They belong to quadrants � and ��, but since quadrant �� is empty, we only need to check one leaf node. The leaf consistency function1 compares the points of this node with point (�,�) specified in the query. The above condition is satisfied only for (�,�).\n\nNow we just have to go back one level and check the node that corresponds to quadrant �� of the root node. It is empty, so the search is complete.\n\n5,5\n\n�\n\n���\n\n��\n\n7,7\n\n5,3\n\n3,2\n\n�\n\n���\n\n�\n\n��\n\n��\n\n�� ���\n\n8,9\n\n8,5\n\n6,6\n\n3,3\n\n3,1\n\n0,0\n\n0,4\n\n9,7\n\n1,2\n\nInsertion\n\nWhen a value gets inserted into an ��-�i�� tree,2 each action that follows is deter- mined by the choice function.3 In this particular case, it simply directs the point to one of the existing nodes that corresponds to its quadrant.\n\nFor example, let’s add value (�,�):\n\n1 backend/access/spgist/spgquadtreeproc.c, spg_quad_leaf_consistent function 2 backend/access/spgist/spgdoinsert.c, spgdoinsert function 3 backend/access/spgist/spgquadtreeproc.c, spg_quad_choose function\n\n549\n\nChapter 27 SP-GiST\n\n9\n\n8\n\n7\n\n6 5\n\n4 3\n\n2\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nThe value belongs to quadrant �� and will be added to the corresponding tree node:\n\n5,5\n\n�\n\n���\n\n��\n\n7,7\n\n5,3\n\n3,2\n\n�\n\n���\n\n7,1\n\n�\n\n��\n\n��\n\n�� ���\n\n8,9\n\n8,5\n\n6,6\n\n3,3\n\n3,1\n\n0,0\n\n9,7\n\n1,2\n\nIf the list of leaf nodes in the selected quadrant becomes too big after insertion (it must fit a single page), the page is split. The picksplit function1 determines the new centroid by calculating the average value of all points’ coordinates, thus distributing the child nodes between new quadrants more or less uniformly.\n\n1 backend/access/spgist/spgquadtreeproc.c, spg_quad_picksplit function\n\n550\n\n0,4\n\n27.2 Quadtrees for Points\n\nThe following picture illustrates the page overflow caused by point (�,�) insertion:\n\n9\n\n9\n\n8\n\n8\n\n7\n\n7\n\n6 5\n\n6 5\n\n4 3\n\n4 3\n\n2\n\n2\n\n1 0\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA new inner node with centroid (�,�) is added into the tree, while points (�,�), (�,�), and (�,�) get redistributed between the new quadrants:\n\n5,5\n\n�\n\n���\n\n��\n\n7,7\n\n5,3\n\n3,2\n\n�\n\n���\n\n7,1\n\n�\n\n��\n\n��\n\n�� ���\n\n8,9\n\n8,5\n\n6,6\n\n3,3\n\n3,1\n\n1,1\n\n0,4\n\n9,7\n\n�\n\n���\n\n1,2\n\n0,0\n\n2,1\n\n551\n\nv. ��\n\nChapter 27 SP-GiST\n\nProperties\n\nAccess method properties. The spgist method reports the following properties:\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'spgist';\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | f spgist | can_order spgist | can_unique | f spgist | can_multi_col | f | t spgist | can_exclude | t spgist | can_include\n\n(5 rows)\n\nNo support is provided for sorting and uniqueness properties. Multicolumn in- dexes are not supported either.\n\nExclusion constraints are supported, just like in �i��.\n\nAn ��-�i�� index can\n\nbe created with additional ������� columns.\n\nIndex-level properties. Unlike�i��, ��-�i��indexesdonotsupportclusterization:\n\n=> FROM unnest(array[\n\nSELECT p.name, pg_index_has_property('airports_quad_idx', p.name)\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| f clusterable | t index_scan bitmap_scan | t backward_scan | f\n\n(4 rows)\n\nBoth ways of getting ���s (either one by one or as a bitmap) are supported. Back- ward scanning is unavailable, as it does not make any sense for ��-�i��.\n\n552",
      "page_number": 509
    },
    {
      "number": 27,
      "title": "SP-GiST",
      "start_page": 545,
      "end_page": 564,
      "detection_method": "regex_chapter",
      "content": "27.2 Quadtrees for Points\n\nColumn-level properties. For the most part,column-level properties are the same:\n\n=> SELECT p.name,\n\npg_index_column_has_property('airports_quad_idx', 1, p.name)\n\nFROM unnest(array[\n\n'orderable', 'search_array', 'search_nulls'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\norderable | f search_array | f search_nulls | t\n\n(3 rows)\n\nSorting is not supported, so all the related properties do not make any sense and are disabled.\n\nI have not said anything about ���� values so far, but as we can see in the index properties, they are supported. Unlike �i��, ��-�i�� indexes do not store ���� values in the main tree. Instead,a separate tree is created; its root is located in the second index page. Thus, the first three pages always have the same meaning: the metapage, the root of the main tree, and the root of the tree for ���� values.\n\nSome column-level properties may depend on the particular operator class:\n\n=> SELECT p.name,\n\npg_index_column_has_property('airports_quad_idx', 1, p.name)\n\nFROM unnest(array[\n\n'returnable', 'distance_orderable'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nreturnable | t distance_orderable | t\n\n(2 rows)\n\nLike in all the other examples in this chapter,this index can be used for index-only scans.\n\nBut in general,an operator class does not necessarily store full values in leaf pages, as it can recheck them by the table instead. It allows using ��-�i�� indexes in Post��� for potentially large geometry values, to give one example.\n\n553\n\nv. ��\n\nv. ��\n\nChapter 27 SP-GiST\n\nNearest neighbor search is supported the operator class.\n\n; we have seen the ordering operator <-> in\n\n27.3 K-Dimensional Trees for Points\n\nPoints on a plane can also be indexed using another approach to partitioning: we can split the plane into two sub-regions instead of four. Such partitioning is im- plemented by the kd_point_ops1 operator class:\n\n=> CREATE INDEX airports_kd_idx ON airports_big USING spgist(coordinates kd_point_ops);\n\nNote that indexed values, prefixes, and labels may have different data types. For this operator class, values are represented as points, prefixes are real numbers, while labels are not provided (as in quad_point_ops).\n\nLet’s select some coordinate on the Y-axis (it defines the latitude in the example with airports). This coordinate splits the plane into two sub-regions,the upper and the lower one:\n\n1 backend/access/spgist/spgkdtreeproc.c\n\n554\n\n27.3 K-Dimensional Trees for Points\n\nFor each of these sub-regions, select coordinates on the X-axis (longitude) that split them into two sub-regions, left and right:\n\nWe will continue splitting each of the resulting sub-regions, taking turns between horizontal and vertical partitioning, until the points in each part fit a single index page:\n\nAll inner leaf nodes of the tree built this way will have only two child nodes. The methodcanbeeasilygeneralizedforspacewitharbitrarydimensions,sosuchtrees are often referred to as k-dimensional (k-� trees).\n\n555\n\nChapter 27 SP-GiST\n\n27.4 Radix Trees for Strings\n\nThe text_ops operator class for ��-�i�� implements a radix tree for strings.1 Here the prefix of an inner node is really a prefix, which is common to all the strings in the child nodes.\n\nPointers to child nodes are marked by the first byte of the values that follow the prefix.\n\nFor clarity,I use a single character to denote a prefix,but it is true onlyfor �-byte encodings. Ingeneral,theoperatorclassprocessesastringasasequenceofbytes. Besides,aprefixcan take several other values with special semantics,so there are actually two bytes allocated per prefix.\n\nChild nodes store parts of values that follow the prefix and the label. Leaf nodes keep only suffixes.\n\nHere is an example of a radix tree built over several names:\n\nV\n\nA\n\nL\n\nADI\n\nD\n\nS\n\nM\n\nS\n\nL\n\nIM\n\nE\n\nILISA\n\nIR\n\nN\n\nR\n\nTIN\n\nIY\n\nTINA\n\n1 backend/access/spgist/spgtextproc.c\n\n556\n\nLAV\n\n27.4 Radix Trees for Strings\n\nTo reconstruct the full value of an index key in a leaf page, we can concatenate all prefixes and labels, starting from the root node.\n\nOperator Class\n\nThe text_ops operator class supports comparison operators typically used with or- dinal data types, including text strings:\n\n=> SELECT oprname, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'spgist' AND opcname = 'text_ops' ORDER BY amopstrategy;\n\noprname |\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−+−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n~<~ ~<=~ = ~>=~ ~>~ < <= >= > ^@\n\n| text_pattern_lt | | text_pattern_le | | texteq | | text_pattern_ge | | text_pattern_gt | | | text_lt | | text_le | | text_ge | | text_gt | | starts_with\n\n1 2 3 4 5 11 12 14 15 28\n\n(10 rows)\n\nRegular operators process characters, while operators with tildes deal with bytes. Theydonottakecollationintoaccount(justlikethetext_pattern_ops operatorclass for �-tree), so they can be used to speed up search by the ���� condition:\n\n=> CREATE INDEX tickets_spgist_idx ON tickets\n\nUSING spgist(passenger_name);\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE passenger_name LIKE 'IVAN%';\n\n557\n\np. ���\n\nv. ��\n\nChapter 27 SP-GiST\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on tickets\n\nFilter: (passenger_name ~~ 'IVAN%'::text) −> Bitmap Index Scan on tickets_spgist_idx\n\nIndex Cond: ((passenger_name ~>=~ 'IVAN'::text) AND (passenger_name ~<~ 'IVAO'::text))\n\n(5 rows)\n\nIf you use regular operators >= and < together with a collation other than “C,” the index becomes virtually useless, as it deals with bytes rather than characters.\n\nFor such cases of is more suitable:\n\nprefix search, the operator class provides the ^@ operator, which\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE passenger_name ^@ 'IVAN';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on tickets\n\nRecheck Cond: (passenger_name ^@ 'IVAN'::text) −> Bitmap Index Scan on tickets_spgist_idx\n\nIndex Cond: (passenger_name ^@ 'IVAN'::text)\n\n(4 rows)\n\nAradix tree representation can sometimes turn out to be much more compact than a �-tree, as it does not keep full values: it reconstructs them as required while the tree is being traversed.\n\nSearch\n\nLet’s run the following query on the names table:\n\nSELECT * FROM names WHERE name ~>=~ 'VALERIY'\n\nAND name ~<~ 'VLADISLAV';\n\n558\n\n27.4 Radix Trees for Strings\n\nFirst, the inner consistency function1 is called on the root to determine the child nodes to descend into. This function concatenates prefix � and labels � and �. The received value �� goes into the query condition; string literals are truncated there, so that their length does not exceed the length of the value being checked: �� ~>=~ '��' ��� �� ~<~ '��'. The condition is satisfied, so the child node with label � needs to be checked. The �� value is checked in the same way. It is also a match, so the node with label � must be checked too.\n\nNow let’s take the node that corresponds to value ��. Its prefix is empty, so for the three child nodes the inner consistency function reconstructs values ���, ���, and ��� by concatenating �� received at the previous step and the label. The condition ��� ~>=~ '���'��� ��� ~<~ '���' is not true, but the other two values are suitable.\n\nAs the tree is being traversed this way, the algorithm filters out non-matching branchesandgetstoleafnodes. Theleafconsistencyfunction2 checkswhetherthe value reconstructed during the tree traversal satisfies the query condition. Match- ing values are returned as the result of an index scan.\n\nV\n\nA\n\nL\n\nADI\n\nD L\n\nS\n\nM\n\nS\n\nIM\n\nE\n\nILISA\n\nIR\n\nLAV\n\nN\n\nR\n\nTIN\n\nIY\n\nTINA\n\n1 backend/access/spgist/spgtextproc.c, spg_text_inner_consistent function 2 backend/access/spgist/spgtextproc.c, spg_text_leaf_consistent function\n\n559\n\nChapter 27 SP-GiST\n\nNote that although the query uses greater than and less than operators, which are common to �-trees, range search by ��-�i�� is much less efficient. In a �-tree, it is enough to descend into a single boundary value of the range and then scan the list of leaf pages.\n\nInsertion\n\nThe choice function of operator classes for points can always direct a new value into one of the existing sub-regions (a quadrant or one of the halves). But it is not true for radix trees: a new value may not match any of the existing prefixes, and the inner node has to be split in this case.\n\nLet’s add the name ����� to an already built tree.\n\nThe choice function1 manages to descend from the root to the next node (� + �), but the remaining part of the value ��� does not match the ��� prefix. The node has to be split in two: one of the resulting nodes will contain the common part of the prefix (��), while the rest of the prefix will be moved one level down:\n\nADI\n\nAD\n\nM\n\nS\n\nI\n\nIR\n\nLAV\n\nM\n\nS\n\nIR\n\nLAV\n\nThen the choice function is called again on the same node. The prefix now cor- responds to the value, but there is no child node with a suitable label (�), so the function decides to create such a node. The final result is shown in the illustra- tion below; the nodes that have been added or modified during the insertion are highlighted.\n\n1 backend/access/spgist/spgtextproc.c, spg_text_choose function\n\n560\n\n27.4 Radix Trees for Strings\n\nV\n\nA\n\nL\n\nAD\n\nD L\n\nS\n\nA\n\nI\n\nIM\n\nE\n\nILISA\n\nN\n\nR\n\nM\n\nS\n\nTIN\n\nIY\n\nIR\n\nLAV\n\nTINA\n\nProperties\n\nI have already described the access method and index-level properties above; they are common to all the classes. Most of the column-level properties also remain the same.\n\n=> SELECT p.name,\n\npg_index_column_has_property('tickets_spgist_idx', 1, p.name)\n\nFROM unnest(array[\n\n'returnable', 'distance_orderable'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nreturnable | t distance_orderable | f\n\n(2 rows)\n\nEven though indexed values are not explicitly stored in the tree, index-only scans are supported, since values are reconstructed as the tree is being traversed from the root to leaf nodes.\n\n561\n\nv. ��\n\nChapter 27 SP-GiST\n\nAs for the distance operator,it is not defined for strings,so nearest neighbor search is not provided by this operator class.\n\nIt does not mean that the concept of distance cannot be implemented for strings. For example, the pg_trgm extension adds a distance operator based on trigrams: the fewer common trigrams are found in two strings, the farther they are assumed to be located from each other. Then there is the Levenshtein distance, which is defined as the minimal number of single-character edits required to convert one string into another. A function that calculates such a distance is provided in the fuzzystrmatch extension. But none of the extensions provides an operator class with ��-�i�� support.\n\n27.5 Other Data Types\n\nS�-�i�� operator classes are not limited to indexing points and text strings that we have discussed above.\n\nGeometric types. The box_ops1 operator class implements a quadtree for rectan- gles. Rectangles are represented by points in a four-dimensional space,so the area is split into sixteen partitions.\n\nThepoly_ops classcanbeusedtoindexpolygons. Itisafuzzyoperatorclass: it actually uses bounding boxes instead of polygons, just like box_ops, and then rechecks the result by the table.\n\nWhether to choose �i�� or ��-�i�� largely depends on the nature of data to be indexed. For example, Post��� documentation recommends ��-�i�� for ob- jects with large overlaps (also known as “spaghetti data”).2\n\nRange types. The quadtree for ranges offers the range_ops operator class.3 An in- terval is defined by a two-dimensional point: the X-axis represents the lower boundary, while the Y-axis represents the upper boundary.\n\nNetwork address types. For the inet data type, the inet_ops4 operator class imple-\n\nments a radix tree.\n\n1 backend/utils/adt/geo_spgist.c 2 postgis.net/docs/using_postgis_dbmanagement.html#spgist_indexes 3 backend/utils/adt/rangetypes_spgist.c 4 backend/utils/adt/network_spgist.c\n\n562\n\n28\n\nGIN\n\n28.1 Overview\n\nAccording to its authors, ��� stands for a potent and undaunted spirit, not for an alcoholic beverage.1 But there is also a formal interpretation: this acronym is ex- panded as Generalized Inverted Index.\n\nThe ��� access method is designed for data types representing non-atomic values made up of separate elements (for example, documents consist of lexemes in the context of full-text search). Unlike �i��, which indexes values as a whole, ��� in- dexes only their elements; each element is mapped to all the values that contain it.\n\nWe can compare this method to a book’s index, which comprises all the important terms and lists all the pages where these terms are mentioned. To be convenient to use, it must be compiled in alphabetical order,otherwise it would be impossible to navigate through quickly. In a similar way, ��� relies on the fact that all elements of compound values can be sorted; its main data structure is �-tree.\n\nThe implementation of the ��� tree of elements is less complex than that of a reg- ular �-tree: it has been designed to contain rather small sets of elements repeated multiple times.\n\nThis assumption leads to two important conclusions:\n\nAn element must be stored in an index only once.\n\nEachelementismappedtoalistof���s,whichiscalledapostinglist. Ifthislist is rather short, it is stored together with the element; longer lists are moved\n\n1 postgresql.org/docs/14/gin.html backend/access/gin/README\n\n563\n\np. ���\n\np. ���\n\np. ���\n\nChapter 28 GIN\n\ninto a separate posting tree, which is actually a �-tree. Just like element trees, posting lists are sorted; it does not matter much from the user’s perspective but helps to speed up data access and reduce index size.\n\nThere is no point in removing elements from a tree.\n\nEven if the list of ���s for a particular element is empty, the same element is likely to appear again as part of some other value.\n\nThus,an index is a tree of elements whose leaf entries are bound to either flat lists or trees of ���s.\n\nJust like �i�� and ��-�i�� access methods, ��� can be used to index a whole vari- ety of data types via a simplified interface of operator classes. Operators of such classes usually check whether the indexed composite value matches a particular set of elements (just like the @@ operator checks whether a document satisfies a full-text search query).\n\nTo index a particular data type, the ��� method must be able to split composite values into elements, sort these elements, and check whether the found value sat- isfies the query. These operations are implemented by support functions of the operator class.\n\n28.2 Index for Full-Text Search\n\nG�� is mainly applied to speed up full-text search, so I will go on with the example used to illustrate �i�� indexing. As you can guess, compound values in this case are documents, while elements of these values are lexemes.\n\nLet’s build a ��� index on the “Old MacDonald” table:\n\n=> CREATE INDEX ts_gin_idx ON ts USING gin(doc_tsv);\n\nA possible structure of this index is shown below. Unlike in the previous illustra- tions, here I provide actual ��� values (shown with a grey background), as they are very important for understanding the algorithms. These values suggest that heap tuples have the following ��s:\n\n564\n\n28.2 Index for Full-Text Search\n\n=> SELECT ctid, * FROM ts;\n\nctid |\n\ndoc\n\n|\n\ndoc_tsv\n\n−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n(0,1) | Old MacDonald had a farm (0,2) | And on his farm he had some cows (0,3) | Here a moo, there a moo (0,4) | Everywhere a moo moo (1,1) | Old MacDonald had a farm (1,2) | And on his farm he had some chicks | 'chick':8 'farm':4 (1,3) | Here a cluck, there a cluck (1,4) | Everywhere a cluck cluck (2,1) | Old MacDonald had a farm (2,2) | And on his farm he had some pigs (2,3) | Here an oink, there an oink (2,4) | Everywhere an oink oink\n\n| 'farm':5 'macdonald':2 'old':1 | 'cow':8 'farm':4 | 'moo':3,6 | 'everywher':1 'moo':3,4 | 'farm':5 'macdonald':2 'old':1\n\n| 'cluck':3,6 | 'cluck':3,4 'everywher':1 | 'farm':5 'macdonald':2 'old':1 | 'farm':4 'pig':8 | 'oink':3,6 | 'everywher':1 'oink':3,4\n\n(12 rows)\n\nmetapage\n\neverywher everywher\n\noink oink\n\nchick\n\ncluck\n\ncow\n\neverywher\n\nfarm macdonald moo\n\noink\n\nold\n\npig\n\n1,2\n\n1,3\n\n0,2\n\n0,4\n\n0,1\n\n0,3\n\n2,3\n\n0,1\n\n2,2\n\n1,4\n\n1,4\n\n1,1\n\n0,4\n\n2,4\n\n1,1\n\n2,4\n\n2,1\n\n2,1\n\n1,2\n\n0,1\n\n0,2\n\n1,1\n\n1,2\n\n2,1\n\n2,2\n\n565\n\np. ���\n\np. ���\n\np. ��\n\nChapter 28 GIN\n\nNote some differences from a regular �-tree index here. The leftmost keys in inner �-tree nodes are empty,as they are actually redundant; in a ��� index,they are not storedatall. Forthisreason,referencestochildnodesareshiftedtoo. Thehighkey is used in both indexes, but in ��� it takes its legitimate rightmost position. Same- level nodes in a �-tree are bound into a bidirectional list; ��� uses a unidirectional list, since the tree is always traversed in only one direction.\n\nIn this theoretical example,all posting lists fit regular pages,except the one for the “farm” lexeme. This lexeme occurred in as many as six documents, so its ��s were moved into a separate posting tree.\n\nPage Layout\n\nG�� page layout is very similar to that of a �-tree. We can peek into an index using the pageinspect extension. Let’s create a ��� index on the table that stores emails of the pgsql-hackers\n\nmailing list:\n\n=> CREATE INDEX mail_gin_idx ON mail_messages USING gin(tsv);\n\nThe zero page (the metapage) contains the basic statistics, such as the number of elements and pages of other types:\n\n=> SELECT * FROM gin_metapage_info(get_raw_page('mail_gin_idx',0)) \\gx\n\n−[ RECORD 1 ]−−−−+−−−−−−−−−−− | 4294967295 pending_head | 4294967295 pending_tail | 0 tail_free_size n_pending_pages | 0 n_pending_tuples | 0 n_total_pages n_entry_pages n_data_pages n_entries version\n\n| 22957 | 13522 | 9434 | 999109 | 2\n\nG�� uses the special space that define the page type:\n\nof index pages; for example, this space stores the bits\n\n566\n\n28.2 Index for Full-Text Search\n\n=> SELECT flags, count(*) FROM generate_series(0,22956) AS p, -- n_total_pages\n\ngin_page_opaque_info(get_raw_page('mail_gin_idx',p))\n\nGROUP BY flags ORDER BY 2;\n\nflags\n\n| count\n\n−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−\n\n| {meta} | {} {data} | {data,leaf,compressed} | {leaf} (5 rows)\n\n1 137 1525 7909 | 13385\n\nThe page with the meta attribute is of course the metapage. Pages with the data attribute belong to posting lists, while pages without this attribute are related to element trees. Leaf pages have the leaf attribute.\n\nIn the next example,another pageinspect function returns the information on ���s that are stored in trees’leaf pages. Each entry of such a tree is virtually a small list of ���s rather than a single ���:\n\n=> SELECT left(tids::text,60)||'...' tids FROM gin_leafpage_items(get_raw_page('mail_gin_idx',24));\n\ntids −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− {\"(4771,4)\",\"(4775,2)\",\"(4775,5)\",\"(4777,4)\",\"(4779,1)\",\"(47... {\"(5004,2)\",\"(5011,2)\",\"(5013,1)\",\"(5013,2)\",\"(5013,3)\",\"(50... {\"(5435,6)\",\"(5438,3)\",\"(5439,3)\",\"(5439,4)\",\"(5439,5)\",\"(54... ... {\"(9789,4)\",\"(9791,6)\",\"(9792,4)\",\"(9794,4)\",\"(9794,5)\",\"(97... {\"(9937,4)\",\"(9937,6)\",\"(9938,4)\",\"(9939,1)\",\"(9939,5)\",\"(99... {\"(10116,5)\",\"(10118,1)\",\"(10118,4)\",\"(10119,2)\",\"(10121,2)\"...\n\n(27 rows)\n\nPosting lists are ordered, which allows them to be compressed (hence the same- name attribute). Instead of a six-byte ���, they store its difference with the pre- vious value, which is represented by a variable number of bytes:1 the smaller this difference, the less space the data takes.\n\n1 backend/access/gin/ginpostinglist.c\n\n567\n\nChapter 28 GIN\n\nOperator Class\n\nHere is the list of support functions for ��� operator classes:1\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'gin' AND opcname = 'tsvector_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n1 | gin_cmp_tslexeme 2 | pg_catalog.gin_extract_tsvector 3 | pg_catalog.gin_extract_tsquery 4 | pg_catalog.gin_tsquery_consistent 5 | gin_cmp_prefix 6 | gin_tsquery_triconsistent\n\n(6 rows)\n\nThe first support function compares two elements (two lexemes in this case). If the lexemes were represented by a regular ��� type supported by �-tree, ��� would automatically use comparison operators defined in the �-tree operator class.\n\nThe fifth (optional) function is used in partial search to check whether an index element partially matches the search key. In this particular case, partial search consists in searching lexemesbya prefix. For example,the query“c:*”corresponds to all lexemes starting with letter “c.”\n\nThe second function extracts lexemes from the document, while the third one ex- tracts lexemes from the search query. The use of different functions is justified because, at the very least, the document and the query are represented by differ- ent data types, namely tsvector and tsquery. Besides, the function for the search query determines how the search will be performed. If the query requires the doc- ument to contain a particular lexeme, the search will be limited to the documents that contain at least one lexeme specified in the query. If there is no such condi- tion (for example, if you need documents that do not contain a particular lexeme), all the documents have to be scanned—which is of course much more expensive.\n\n1 postgresql.org/docs/14/gin-extensibility.html\n\nbackend/utils/adt/tsginidx.c\n\n568\n\n28.2 Index for Full-Text Search\n\nIf the query contains any other search keys, the index is first scanned by these keys, and then these intermediate results are rechecked. Thus,there is no need to scan the index in full.\n\nThe fourth and sixth functions are consistency functions, which determine whether the found document satisfies the search query. As input, the fourth func- tion gets the exact information on which lexemes specified in the query appear in the document. The sixth function operates in the context of uncertainty and can be called when it is not yet clear whether some of the lexemes are present in the document or not. An operator class does not have to implement both functions: it is enough to provide only one of them, but search efficiency may suffer in this case.\n\nThe tsvector_ops operator class supports only one operator that matches the docu- mentagainstthesearchquery: @@,1 whichisalsoincludedintothe�i��operator class.\n\nSearch\n\nLet’s take a look at the search algorithm for the “everywhere | oink” query, where two lexemes are connected by the �� operator. First, a support function2 extracts lexemes “everywher” and “oink” (search keys) from the search string of the tsquery type.\n\nSince the query demands particular lexemes to be present, ���s of the documents that contain at least one key specified in the query are bound into a list. For this purpose, the ���s that correspond to each search key are searched in the tree of lexemes and are added into a common list. All the ���s stored in an index are ordered, which allows merging\n\nseveral sorted streams of ���s into one.3\n\nNote that at this point it does not matter yet whether the keys were combined by ���, ��, or any other operator: the search engine deals with the list of keys and knows nothing about the search query semantics.\n\n1 backend/utils/adt/tsvector_op.c, ts_match_vq function 2 backend/utils/adt/tsginidx.c, gin_extract_tsquery function 3 backend/access/gin/ginget.c, keyGetItem function\n\n569\n\nv. ��\n\np. ���\n\nChapter 28 GIN\n\neverywher everywher\n\noink oink\n\nchick\n\ncluck\n\ncow\n\neverywher\n\nfarm macdonald moo\n\noink\n\nold\n\n1,2\n\n1,3\n\n0,2\n\n0,4\n\n0,1\n\n0,3\n\n2,3\n\n0,1\n\n1,4\n\n1,4\n\n1,1\n\n0,4\n\n2,4\n\n1,1\n\n2,4\n\n2,1\n\n2,1\n\n1,2\n\n0,1\n\n0,2\n\n1,1\n\n1,2\n\n2,1\n\n2,2\n\nEach found ��� that corresponds to a document is checked by the consistency func- tion.1 It is this function that interprets the search query and leaves only those ���s that satisfy the query (or at least may satisfy it and have to be rechecked by the ta- ble).\n\nIn this particular case, the consistency function leaves all the ���s:\n\n���\n\n“everywher”\n\n“oink”\n\nconsistency function\n\n(�,�) (�,�) (�,�) (�,�)\n\n(cid:51) (cid:51)\n\n– (cid:51)\n\n– – (cid:51) (cid:51)\n\n(cid:51) (cid:51) (cid:51) (cid:51)\n\nInstead of a regular lexeme, search queries can contain a prefix. It is useful if an application user can enter the first letters of a word into the search field,expecting\n\n1 backend/utils/adt/tsginidx.c, gin_tsquery_triconsistent function\n\n570\n\npig\n\n2,2\n\n28.2 Index for Full-Text Search\n\nto get the results right away. For example, the “pig:*” query will match all the documents that contain lexemes starting with “pig”: here we get “pigs,” and we would also get “pigeons” if old MacDonald had bred them on his farm.\n\nSuch partial search matches indexed lexemes against the search key using a special supportfunction;1 inadditiontoprefixmatching,thisfunctioncanalsoimplement other logic for partial search.\n\nFrequent and Rare Lexemes\n\nIfsearchedlexemesoccurinadocumentmultipletimes,thecreatedlistof���swill turn out long, which is of course inefficient. Fortunately, it can often be avoided if the query also contains some rare lexemes.\n\nLet’sconsiderthe“farm & cluck”query. The“cluck”lexemeoccurstwotimes,while the “farm” lexeme appears six times. Instead of treating both lexemes equally and building the full list of ���s by them, the rare “cluck” lexeme is considered manda- tory,whilethemorefrequent“farm”lexemeistreatedasoptional,asitisclear that (taking the query semantics into account) a document with the “farm” lexeme can satisfy the query only if it contains the “cluck” lexeme too.\n\nThus, an index scan determines the first document that contains “cluck”; its ��� is (�,�). Then we have to find out whether this document also contains the “farm” lexeme, but all the documents whose ���s are smaller than (�,�) can be skipped. Since frequent lexemes are likely to correspond to many ���s, chances are high that they are stored in a separate tree, so some pages can be skipped as well. In this particular case, the search in the tree of “farm” lexemes starts with (�,�).\n\nThis procedure is repeated for the subsequent values of the mandatory lexeme.\n\nClearly, this optimization can also be applied to more complex search scenarios that involve more than two lexemes. The algorithm sorts the lexemes in the or- der of their frequency, adds them one by one to the list of mandatory lexemes, and stops when the remaining lexemes are no longer able to guarantee that the document satisfies the query.2\n\n1 backend/utils/adt/tsginidx.c, gin_cmp_prefix function 2 backend/access/gin/ginget.c, startScanKey function\n\n571\n\nChapter 28 GIN\n\nFor example, let’s consider the query “farm & ( cluck | chick )”. The least frequent lexeme is“chick”; it is added to the list of mandatory lexemes right away. To check whether other lexemes can be considered optional, the consistency function takes false for the mandatory lexeme and true for all the other lexemes. The function returns true AND (true OR false) = true, which means that the remaining lexemes are “self-sufficient,” and at least one of them must become mandatory.\n\nThe next least frequent lexeme (“cluck”) is added into the list, and now the consis- tency function returns true AND (false OR false) = false. Thus, “chick” and “cluck” lexemes become mandatory, while “farm” remains optional.\n\neverywher everywher\n\noink oink\n\nchick\n\ncluck\n\ncow\n\neverywher\n\nfarm macdonald moo\n\noink\n\nold\n\n1,2\n\n1,3\n\n0,2\n\n0,4\n\n0,1\n\n0,3\n\n2,3\n\n0,1\n\n1,4\n\n1,4\n\n1,1\n\n0,4\n\n2,4\n\n1,1\n\n2,4\n\n2,1\n\n2,1\n\n1,2\n\n0,1\n\n0,2\n\n1,1\n\n1,2\n\n2,1\n\n2,2\n\nThe length of the posting list is three, as the mandatory lexemes have occurred three times:\n\n���\n\n“chick”\n\n“cluck”\n\n“farm”\n\nconsistency function\n\n(�,�) (�,�) (�,�)\n\n(cid:51)\n\n– –\n\n– (cid:51) (cid:51)\n\n(cid:51)\n\n– –\n\n(cid:51)\n\n– –\n\n572\n\npig\n\n2,2",
      "page_number": 545
    },
    {
      "number": 28,
      "title": "GIN",
      "start_page": 565,
      "end_page": 592,
      "detection_method": "regex_chapter",
      "content": "28.2 Index for Full-Text Search\n\nThus, if lexeme frequencies are known , it is possible to merge trees of lexemes in the most efficient way, starting from rare lexemes and skipping those page ranges of frequent lexemes that are sure to be redundant. It reduces the number of times the consistency function has to be called.\n\nTo make sure that this optimization really works, let’s query archive. We will need to specify two lexemes, a common and a rare one:\n\nthe pgsql-hackers\n\n=> SELECT word, ndoc FROM ts_stat('SELECT tsv FROM mail_messages') WHERE word IN ('wrote', 'tattoo');\n\n| −−−−−−−−+−−−−−−−−\n\nword\n\nndoc\n\nwrote tattoo |\n\n| 231173 2\n\n(2 rows)\n\nIt turns out that a document that contains them both does exist:\n\n=> \\timing on\n\n=> SELECT count(*) FROM mail_messages WHERE tsv @@ to_tsquery('wrote & tattoo');\n\ncount −−−−−−−\n\n1\n\n(1 row) Time: 0,631 ms\n\nThis query is performed almost just as fast as the search for a single word“tattoo”:\n\n=> SELECT count(*) FROM mail_messages WHERE tsv @@ to_tsquery('tattoo');\n\ncount −−−−−−−\n\n2\n\n(1 row) Time: 2,227 ms\n\nBut if we were looking for a single word “wrote,” the search would take much longer:\n\n=> SELECT count(*) FROM mail_messages WHERE tsv @@ to_tsquery('wrote');\n\n573\n\np. ���\n\np. ���\n\non p. ���\n\nChapter 28 GIN\n\ncount −−−−−−−− 231173 (1 row) Time: 343,556 ms\n\n=> \\timing off\n\nInsertions\n\nA ��� index cannot contain duplicates;1 if an element to be added is already present in the index, its ��� is simply added to the posting list or tree of an al- ready existing element. A posting list is a part of an index entry that cannot take toomuchspaceinapage,soiftheallottedspaceisexceeded,thelististransformed into a tree.2\n\nWhen a new element (or a new ���) is being added into a tree, a page overflow can occur; in this case, the page is split into two, and the elements are redistributed between them.3\n\nBut each document typically contains many lexemes that have to be indexed. So even if we create or modify just one document, the index tree still undergoes a lot of modifications. That is why ��� updates are rather slow.\n\nThe illustration below shows the state of the tree after the row“Everywhere clucks, moos,and oinks”with ��� (�,�) was inserted into the table. The posting lists of lex- emes“cluck,”“moo,”and“oink”were extended; the list of the“everywher”lexeme exceeded the maximal size and was split off as a separate tree.\n\nHowever, if an index gets updated to incorporate changes related to several doc- uments at once, the total amount of work is likely to be reduced as compared to consecutive changes, as these documents may contain some common lexemes.\n\nfastupdate storage parameter. Deferred in- This optimization is controlled by the dex updates are accumulated in an unordered pending list, which is physically stored in separate list pages outside the element tree. When this list becomes\n\n1 backend/access/gin/gininsert.c, ginEntryInsert function 2 backend/access/gin/gininsert.c, addItemPointersToLeafTuple function 3 backend/access/gin/ginbtree.c, ginInsertValue function\n\n574\n\n28.2 Index for Full-Text Search\n\nbig enough, all its contents is transferred into the index in one go, and the list is cleared.1 The maximal size of the list is defined either by the gin_pending_list_limit parameter or by the same-name index storage parameter.\n\neverywher everywher\n\noink oink\n\nchick\n\ncluck\n\ncow\n\neverywher\n\nfarm macdonald moo\n\noink\n\nold\n\npig\n\n1,2\n\n1,3\n\n0,2\n\n0,1\n\n0,3\n\n2,3\n\n0,1\n\n2,2\n\n1,4\n\n1,1\n\n0,4\n\n2,4\n\n1,1\n\n4,1\n\n2,1\n\n4,1\n\n4,1\n\n2,1\n\n2,4\n\n1,2\n\n0,4\n\n1,4\n\n2,4\n\n4,1\n\n0,1\n\n0,2\n\n1,1\n\n1,2\n\n2,1\n\n2,2\n\nBy default, such deferred updates are enabled, but you should keep in mind that they slow down search: apart from the tree itself, the whole unordered list of lex- emes has to be scanned. Besides, insertion time becomes less predictable, as any change can lead to an overflow that incurs an expensive merge procedure. The latter is partially smoothed by the fact that the merge can be also performed asyn- chronously during index vacuuming.\n\nWhen a new index is created,2 the elements also get added in batches rather than one by one, which would be too slow. Instead of being saved into an unordered maintenance_work_mem memory list on disk, all the changes are accumulated in a chunk and get transferred into an index once this chunk has no more free space. The more memory is allocated for this operation, the faster the index is built.\n\n1 backend/access/gin/ginfast.c, ginInsertCleanup function 2 backend/access/gin/gininsert.c, ginbuild function\n\n575\n\n4MB\n\n64MB\n\np. ���\n\np. ���\n\n0\n\nChapter 28 GIN\n\nThe examples provided in this chapter prove ��� superiority over �i�� signature trees when it comes to search precision. For this reason, it is ��� that is typically used for full-text search. However, the problem of slow ��� updates may tip the scale in favor of �i�� if the data is being actively updated.\n\nLimiting Result Set Size\n\nThe ��� access method always returns the result as a bitmap; it is impossible to get ���s one by one. In other words,the B����� S��� property is supported,but the I���� S��� property is not.\n\nThe reason for this limitation is the unordered list of deferred updates. In the case of an index access, this list is scanned to build a bitmap, and then this bitmap is updated with the data of the tree. If the unordered list gets merged with the tree (as the result of an index update or during vacuuming) while search is in progress, one and the same value can be returned twice, which is unacceptable. But in the case of a bitmap it does not pose any problems: the same bit will simply be set twice.\n\nConsequently, using the ����� clause with a ��� index is not quite efficient, as the bitmap still has to be built in full, which contributes a fair share to the total cost:\n\n=> EXPLAIN SELECT * FROM mail_messages WHERE tsv @@ to_tsquery('hacker') LIMIT 1000;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=481.41..1964.22 rows=1000 width=1258)\n\nLimit\n\n−> Bitmap Heap Scan on mail_messages\n\n(cost=481.41..74939.28 rows=50214 width=1258) Recheck Cond: (tsv @@ to_tsquery('hacker'::text)) −> Bitmap Index Scan on mail_gin_idx\n\n(cost=0.00..468.85 rows=50214 width=0) Index Cond: (tsv @@ to_tsquery('hacker'::text))\n\n(7 rows)\n\nTherefore, the ��� method offers a special feature that limits the number of re- gin_fuzzy_search_limit sults returned by an index scan. This limit is imposed by the parameter, which is turned off by default. If this parameter is enabled, the index\n\n576\n\n28.2 Index for Full-Text Search\n\naccess method will randomly skip some values to get roughly the specified number of rows (hence the name “fuzzy”):1\n\n=> SET gin_fuzzy_search_limit = 1000;\n\n=> SELECT count(*) FROM mail_messages WHERE tsv @@ to_tsquery('hacker');\n\ncount −−−−−−−\n\n727\n\n(1 row)\n\n=> SELECT count(*) FROM mail_messages WHERE tsv @@ to_tsquery('hacker');\n\ncount −−−−−−−\n\n791\n\n(1 row)\n\n=> RESET gin_fuzzy_search_limit;\n\nNote that there are no ����� clauses in these queries. It is the only legitimate way to get different data when using an index scan and a heap scan. The planner knows nothing about such behavior of ��� indexes and does not take this parameter value into account when estimating the cost.\n\nProperties\n\nAll the properties of the gin access method are the same at all levels; they do not depend on a particular operator class.\n\nAccess Method Properties\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'gin';\n\n1 backend/access/gin/ginget.c, dropItem macro\n\n577\n\nChapter 28 GIN\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | f | can_order | f | can_unique | can_multi_col | t | f | can_exclude | f | can_include\n\ngin gin gin gin gin\n\n(5 rows)\n\nG�� supports neither sorting nor unique constraints.\n\nMulticolumn indexes are supported, but it is worth mentioning that the order of their columns is irrelevant. Unlike a regular �-tree, a multicolumn ��� index does not store composite keys; instead, it extends separate elements with the corre- sponding column number.\n\nExclusion constraints cannot be supported because the I���� S��� property is un- available.\n\nG�� does not support additional ������� columns. Such columns simply do not make much sense here, as it is hardly possible to use a ��� index as covering: it contains only separate elements of an index value, while the value itself is stored in the table.\n\nIndex-Level Properties\n\n=> FROM unnest(array[\n\nSELECT p.name, pg_index_has_property('mail_gin_idx', p.name)\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| f clusterable | f index_scan bitmap_scan | t backward_scan | f\n\n(4 rows)\n\nFetching results one by one is not supported: the index access always returns a bitmap.\n\nFor the same reason,it makes no sense to reorder tables by a ��� index: the bitmap always corresponds to the physical layout of data in a table, whichever it is.\n\n578\n\n28.2 Index for Full-Text Search\n\nBackward scanning is not supported: this feature is useful for regular index scans, not for bitmap scans.\n\nColumn-Level Properties\n\n=> SELECT p.name,\n\npg_index_column_has_property('mail_gin_idx', 1, p.name)\n\nFROM unnest(array[\n\n'orderable', 'search_array', 'search_nulls', 'returnable', 'distance_orderable'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n| f orderable | f search_array | f search_nulls returnable | f distance_orderable | f\n\n(5 rows)\n\nNone of the column-level properties are available: neither sorting (for obvious rea- sons) nor using the index as covering (since the document itself is not stored in the index). N��� support is not available either (it does not make sense for elements of non-atomic types).\n\nGIN Limitations and RUM Index\n\nPotent as it is, ��� still cannot address all the challenges of full-text search. Al- though the tsvector type does indicate positions of lexemes, this information does not make it into an index. Therefore, ��� cannot be used to speed up phrase search, which takes lexeme proximity into account. Moreover, search engines usually re- turn results by relevance (whatever this term might mean), and since ��� does not supportorderingoperators,theonlysolutionherewouldbecomputingtheranking function for each resulting row, which is of course very slow.\n\nThese drawbacks have been addressed by the ��� access method (whose name makes us doubt developers’ sincerity when it comes to the true meaning of ���).\n\n579\n\nChapter 28 GIN\n\nThis access method is provided as an extension; you can either download the cor- responding package from the ���� repository1 or get the source code itself.2\n\nR�� is based on ���, but they have two major differences. First, ��� does not provide deferred updates, so it supports regular index scans in addition to bitmap scansandimplementsorderingoperators. Second,���indexkeyscanbeextended with additional information. This feature resembles ������� columns to some ex- tent, but here additional information is bound to a particular key. In the context of full-text search, ��� operator class maps lexeme occurrences to their positions in the document, which speeds up phrase search and result ranking.\n\nThe downsides of this approach are slow updates and larger index sizes. Besides, since the rum access method is provided as an extension, it relies on the generic ��� mechanism,3 which is slower than the built-in logging and generates bigger volumes of ���.\n\n28.3 Trigrams\n\nThe pg_trgm4 extension can assess word similarity by comparing the number of co- inciding three-letter sequences (trigrams). Word similarity can be used alongside full-text search to return some results even if the words to search for have been entered with typos.\n\nThe gin_trgm_ops operator class implements text string indexing. To single out el- ements of text values, it extracts various three-letter substrings rather than words or lexemes (only letters and digits are taken into account; other characters are ignored). Within an index, trigrams are represented as integers. Note that for non-Latin characters, which take from two to four bytes in the ���-� encoding, such representation does not allow decoding the original symbols.\n\n=> CREATE EXTENSION pg_trgm;\n\n=> SELECT unnest(show_trgm('macdonald')),\n\nunnest(show_trgm('McDonald'));\n\n1 postgresql.org/download 2 github.com/postgrespro/rum 3 postgresql.org/docs/14/generic-wal.html 4 postgresql.org/docs/14/pgtrgm.html\n\n580\n\n28.3 Trigrams\n\nunnest | unnest −−−−−−−−+−−−−−−−− m | mc | | ald | cdo | don | ld | mcd | nal | ona | (10 rows)\n\nm ma acd ald cdo don ld mac nal ona\n\nThis class supports operators for both precise and fuzzy comparison of strings and words.\n\n=> SELECT amopopr::regoperator, oprcode::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'gin' AND opcname = 'gin_trgm_ops' ORDER BY amopstrategy;\n\namopopr\n\n|\n\noprcode\n\n−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n%(text,text) ~~(text,text) ~~*(text,text) | texticlike ~(text,text) ~*(text,text) %>(text,text) %>>(text,text) | strict_word_similarity_commutator_op =(text,text)\n\n| similarity_op | textlike\n\nLIKE and ILIKE\n\n| textregexeq | texticregexeq | word_similarity_commutator_op\n\nregular expressions\n\n| texteq\n\n(8 rows)\n\nTo perform fuzzy comparison,we can define the distance between strings as a ratio of common trigrams to the total number of trigrams in the query string. But as I have already shown, ��� does not support ordering operators, so all operators in the class must be Boolean. Therefore, for %, %>, and %>> operators that imple- ment strategies of fuzzy comparison, the consistency function returns true if the computed distance does not exceed the defined threshold.\n\n581\n\nChapter 28 GIN\n\nFor = and ���� operators, the consistency function demands that the value con- tains all the trigrams of the query string. Matching a document against a regular expression requires a much more complex check.\n\nIn any case, trigram search is always fuzzy, and the results have to be rechecked.\n\n28.4 Indexing Arrays\n\nThe array data type is also supported by ���. Built over array elements,a ��� index can be used to quickly determine whether an array overlaps with or is contained in another array:\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'gin' AND opcname = 'array_ops' ORDER BY amopstrategy;\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n| &&(anyarray,anyarray) | arrayoverlap @>(anyarray,anyarray) | arraycontains | <@(anyarray,anyarray) | arraycontained | | =(anyarray,anyarray)\n\n| array_eq\n\n1 2 3 4\n\n(4 rows)\n\nAs an example, let’s take the routes view of the demo database that shows the in- formation on flights. The days_of_week column is an array of days of the week on which flights are performed. To build an index, we first have to materialize the view:\n\n=> CREATE TABLE routes_tbl AS SELECT * FROM routes;\n\nSELECT 710\n\n=> CREATE INDEX ON routes_tbl USING gin(days_of_week);\n\nLet’s use the created index to select the flights that depart on Tuesdays,Thursdays, and Sundays. I turn off sequential scanning; otherwise, the planner would not use the index for such a small table:\n\n582\n\n28.4 Indexing Arrays\n\n=> SET enable_seqscan = off;\n\n=> EXPLAIN (costs off) SELECT * FROM routes_tbl WHERE days_of_week = ARRAY[2,4,7];\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on routes_tbl\n\nRecheck Cond: (days_of_week = '{2,4,7}'::integer[]) −> Bitmap Index Scan on routes_tbl_days_of_week_idx\n\nIndex Cond: (days_of_week = '{2,4,7}'::integer[])\n\n(4 rows)\n\nIt turns out that there are eleven such flights:\n\n=> SELECT flight_no, departure_airport, arrival_airport,\n\ndays_of_week FROM routes_tbl WHERE days_of_week = ARRAY[2,4,7];\n\nflight_no | departure_airport | arrival_airport | days_of_week −−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\nPG0023 PG0123 PG0155 PG0260 PG0261 PG0310 PG0370 PG0371 PG0448 PG0482 PG0651 (11 rows)\n\n| OSW | NBC | ARH | STW | SVO | UUD | DME | KRO | VKO | DME | UIK\n\n| KRO | ROV | TJM | CEK | GDZ | NYM | KRO | DME | STW | KEJ | KHV\n\n| {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7}\n\nThe built index contains only seven elements: integer numbers from � to � that represent days of the week.\n\nThe query execution is quite similar to what I have shown before for the full-text search. In this particular case, the search query is represented by a regular array rather than by a special data type; it is assumed that the indexed array must con- tain all the specified elements. An important distinction here is that the equality condition also requires the indexed array to contain no other elements. The consis- tency function1 knows about this requirement thanks to the strategy number, but it cannot verify that there are no unwanted elements, so it requests the indexing engine to recheck the results by the table:\n\n1 backend/access/gin/ginarrayproc.c, ginarrayconsistent function\n\n583\n\np. ���\n\nChapter 28 GIN\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM routes_tbl WHERE days_of_week = ARRAY[2,4,7];\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on routes_tbl (actual rows=11 loops=1) Recheck Cond: (days_of_week = '{2,4,7}'::integer[]) Rows Removed by Index Recheck: 482 Heap Blocks: exact=16 −> Bitmap Index Scan on routes_tbl_days_of_week_idx (actual ro...\n\nIndex Cond: (days_of_week = '{2,4,7}'::integer[])\n\n(6 rows)\n\nIt may be useful to extend the ��� index with additional columns. For example, to enable search for the flights that depart on Tuesdays, Thursdays, and Sundays from Moscow, the index lacks the departure_city column. But there are no operator classes implemented for regular scalar data types:\n\n=> CREATE INDEX ON routes_tbl USING gin(days_of_week, departure_city);\n\nERROR: method \"gin\" HINT: default operator class for the data type.\n\ndata type text has no default operator class for access\n\nYou must specify an operator class for the index or define a\n\nSuch situations can be addressed by the btree_gin extension. It adds ��� operator classes that simulate regular �-tree processing by representing a scalar value as a composite value with a single element.\n\n=> CREATE EXTENSION btree_gin;\n\n=> CREATE INDEX ON routes_tbl USING gin(days_of_week,departure_city);\n\n=> EXPLAIN (costs off) SELECT * FROM routes_tbl WHERE days_of_week = ARRAY[2,4,7] AND departure_city = 'Moscow';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on routes_tbl\n\nRecheck Cond: ((days_of_week = '{2,4,7}'::integer[]) AND (departure_city = 'Moscow'::text)) −> Bitmap Index Scan on routes_tbl_days_of_week_departure_city...\n\nIndex Cond: ((days_of_week = '{2,4,7}'::integer[]) AND (departure_city = 'Moscow'::text))\n\n(6 rows)\n\n584\n\n28.5 Indexing JSON\n\n=> RESET enable_seqscan;\n\nThe remark made about btree_gist holds true for btree_gin as well: a �-tree is much moreefficientwhenitcomestocomparisonoperations,soitmakessensetousethe btree_gin extension only when a ��� index is really needed. For instance, a search by less than or less than or equal to conditions can be performed by a backward scan in a �-tree, but not in ���.\n\n28.5 Indexing JSON\n\nOne more non-atomic data type with built-in ��� support is jsonb.1 It offers a whole range of operators for ����, and some of them can perform faster using ���.\n\nThere are two operator classes that extract different sets of elements from a ���� document:\n\n=> SELECT opcname FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid\n\nWHERE amname = 'gin' AND opcintype = 'jsonb'::regtype;\n\nopcname −−−−−−−−−−−−−−−−\n\njsonb_ops jsonb_path_ops\n\n(2 rows)\n\njsonb_ops Operator Class\n\nThe jsonb_ops operator class is the default one. All the keys, values, and array elementsoftheoriginal����documentareconvertedintoindexentries.2 Itspeeds up queries that check for inclusion of ���� values (@>),existence of keys (?, ?|,and ?&), or ���� path matches (@? and @@):\n\n1 postgresql.org/docs/14/datatype-json.html 2 backend/utils/adt/jsonb_gin.c, gin_extract_jsonb function\n\n585\n\nChapter 28 GIN\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'gin' AND opcname = 'jsonb_ops' ORDER BY amopstrategy;\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n| @>(jsonb,jsonb) | ?(jsonb,text) | ?|(jsonb,text[]) ?&(jsonb,text[]) | @?(jsonb,jsonpath) | jsonb_path_exists_opr | | @@(jsonb,jsonpath) | jsonb_path_match_opr\n\n| jsonb_contains | jsonb_exists | jsonb_exists_any | jsonb_exists_all\n\n7 9 10 11 15 16\n\n(6 rows)\n\nLet’s convert several rows of the routes view into the ���� format:\n\n=> CREATE TABLE routes_jsonb AS SELECT to_jsonb(t) route FROM (\n\nSELECT departure_airport_name, arrival_airport_name, days_of_week FROM routes ORDER BY flight_no LIMIT 4\n\n) t;\n\n=> SELECT ctid, jsonb_pretty(route) FROM routes_jsonb;\n\nctid\n\n|\n\njsonb_pretty\n\n−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− + + + + + +\n\n(0,1) | {\n\n| | | | | | } (0,2) | {\n\n\"days_of_week\": [\n\n6\n\n], \"arrival_airport_name\": \"Surgut Airport\", \"departure_airport_name\": \"Ust−Ilimsk Airport\"\n\n| | | | | | }\n\n\"days_of_week\": [\n\n7\n\n], \"arrival_airport_name\": \"Ust−Ilimsk Airport\", \"departure_airport_name\": \"Surgut Airport\"\n\n586\n\n+ + + + + +\n\n28.5 Indexing JSON\n\n(0,3) | {\n\n| | | | | | | } (0,4) | {\n\n| | | | | | | }\n\n+ + + + ], + \"arrival_airport_name\": \"Sochi International Airport\", + + \"departure_airport_name\": \"Ivanovo South Airport\"\n\n\"days_of_week\": [\n\n2, 6\n\n+ + + + + ], \"arrival_airport_name\": \"Ivanovo South Airport\", + \"departure_airport_name\": \"Sochi International Airport\"+\n\n\"days_of_week\": [\n\n3, 7\n\n(4 rows)\n\n=> CREATE INDEX ON routes_jsonb USING gin(route);\n\nThe created index can be illustrated as follows:\n\narrival_airport_name arrival_airport_name\n\nIvanovo-Yuzhny Ivanovo-Yuzhny\n\n2 3 6 7\n\ne m a n _ t r o p r i a _ l a v i r r a\n\nk e e w _ f o _ s y a d\n\ne m a n _ t r o p r i a _ e r u t r a p e d\n\ny n h z u Y - o v o n a v I\n\ni\n\nh c o S\n\nt u g r u S\n\nk s m\n\ni l I - t s U\n\n0,3\n\n0,4\n\n0,1\n\n0,2\n\n0,1\n\n0,1\n\n0,1\n\n0,3\n\n0,3\n\n0,1\n\n0,1\n\n0,3\n\n0,4\n\n0,2\n\n0,2\n\n0,2\n\n0,4\n\n0,4\n\n0,2\n\n0,2\n\n0,3\n\n0,3\n\n0,3\n\n0,4\n\n0,4\n\n0,4\n\nLet’s consider a query with condition route @>'{\"days_of_week\": [6]}',which selects ���� documents containing the specified path (that is, the flights performed on Saturdays).\n\n587\n\nChapter 28 GIN\n\nThe support function1 extracts the search keys from the ���� value of the search query: “days_of_week” and “6”. These keys are searched in the element tree, and the documents that contain at least one of them are checked by the consistency function.2 For the contains strategy, this function demands that all the search keys are available, but the results still have to be rechecked by the table: from the point of view of an index,the specified path can also correspond to documents like {\"days_of_week\": [2],\"foo\": [6]}.\n\njsonb_path_ops Operator Class\n\nThe second class called jsonb_path_ops contains fewer operators:\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'gin' AND opcname = 'jsonb_path_ops' ORDER BY amopstrategy;\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n@>(jsonb,jsonb) | @?(jsonb,jsonpath) | jsonb_path_exists_opr | | @@(jsonb,jsonpath) | jsonb_path_match_opr\n\n| jsonb_contains\n\n7 15 16\n\n(3 rows)\n\nIf this class is used, the index will contain paths from the root of the document to all the values and all the array elements rather than isolated ���� fragments.3 It makes the search much more precise and efficient, but there is no speedup for operations with arguments represented by separate keys instead of paths.\n\nAs a path can be quite lengthy, it is not paths themselves but their hashes that actually get indexed.\n\nLet’s create an index for the same table using this operator class:\n\n1 backend/utils/adt/jsonb_gin.c, gin_extract_jsonb_query function 2 backend/utils/adt/jsonb_gin.c, gin_consistent_jsonb function 3 backend/utils/adt/jsonb_gin.c, gin_extract_jsonb_path function\n\n588\n\n28.5 Indexing JSON\n\n=> CREATE INDEX ON routes_jsonb USING gin(route jsonb_path_ops);\n\nThe created index can be represented by the following tree:\n\nHASH( ... ) HASH( ... ) HASH( ... ) HASH( ... )\n\n) y n h z u Y - o v o n a v I\n\ne m a n _ t r o p r i a _ e r u t r a p e d (\n\n)\n\ni\n\nh c o S\n\ne m a n _ t r o p r i a _ e r u t r a p e d (\n\n) k s m\n\ni l I - t s U\n\ne m a n _ t r o p r i a _ e r u t r a p e d (\n\n)\n\nt u g r u S\n\ne m a n _ t r o p r i a _ e r u t r a p e d (\n\n) 3\n\n,\n\nk e e w _ f o _ s y a d (\n\n)\n\nt u g r u S\n\n,\n\ne m a n _ t r o p r i a _ l a v i r r a\n\n)\n\ni\n\nh c o S\n\n,\n\ne m a n _ t r o p r i a _ l a v i r r a\n\n) k s m\n\ni l I - t s U\n\n,\n\ne m a n _ t r o p r i a _ l a v i r r a\n\n) 7\n\n,\n\nk e e w _ f o _ s y a d (\n\n) y n h z u Y - o v o n a v I\n\n,\n\ne m a n _ t r o p r i a _ l a v i r r a\n\n) 6\n\n,\n\nk e e w _ f o _ s y a d (\n\n) 2\n\n,\n\nk e e w _ f o _ s y a d (\n\n(\n\n(\n\n(\n\n(\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\n0,3\n\n0,4\n\n0,1\n\n0,2\n\n0,4\n\n0,1\n\n0,3\n\n0,2\n\n0,2\n\n0,4\n\n0,1\n\n0,3\n\n0,4\n\n0,3\n\nWhen executing a query with the same condition route @> '{\"days_of_week\": [6]}', the support function1 extracts the whole path “days_of_week, �” rather than its separate components. The ���s of the two matching documents will be found in the element tree right away.\n\nClearly, these entries will be checked by the consistency function2 and then rechecked by the indexing engine (to rule out hash collisions, for example). But the search through the tree is much more efficient, so it makes sense to always choose the jsonb_path_ops class if the index support provided by its operators is sufficient for queries.\n\n1 backend/utils/adt/jsonb_gin.c, gin_extract_jsonb_query_path function 2 backend/utils/adt/jsonb_gin.c, gin_consistent_jsonb_path function\n\n589\n\nv. ��\n\nChapter 28 GIN\n\n28.6 Indexing Other Data Types\n\nG�� support via extensions is also provided for the following data types:\n\nArrays of integers. The intarray extension adds the gin__int_ops operator class for integer arrays. It is very similar to the standard array_ops operator class,but it supportsthematchoperator@@,whichmatchesadocumentagainstasearch query.\n\nKey–value storage. The hstore extension implements a storage for key–value pairs and provides the gin_hstore_ops operator class. Both keys and values get in- dexed.\n\nJSON query language. An external jsquery extension provides its own query lan-\n\nguage and ��� index support for ����.\n\nAfter the ���:���� standard was adopted and the ���/���� query language was implemented in Postgre���, the standard built-in capabilities seem to be a better choice.\n\n590\n\n29\n\nBRIN\n\n29.1 Overview\n\nUnlike other indexes that are optimized to quickly find the required rows, ����1 is designed to filter out unnecessary rows. This access method was created primarily for large tables of several terabytes and up, so a smaller index size takes priority over search accuracy.\n\nTo speed up search, the whole table is split into ranges, hence the name: Block Range Index. Each range comprises several pages. The index does not store ���s, keeping only a summary on the data of each range. For ordinal data types, it is the minimal and the maximal values in the simplest case, but different operator classes may collect different information on values in a range.\n\nThe number of pages in a range is defined at the time of the index creation based on the value of the\n\npages_per_range storage parameter.\n\nIf a query condition references an indexed column, all the ranges that are guaran- teed to have no matches can be skipped. The pages of all the other ranges are bitmap; all the rows of these pages have to be returned by the index as a lossy rechecked.\n\nThus, ���� works well for columns with localized values (that is, for columns in which values stored close to each other have similar summary information prop- erties). For ordinal data types, it means that values must be stored in ascending or descending order, that is, have high correlation between their physical location\n\n1 postgresql.org/docs/14/brin.html backend/access/brin/README\n\n591\n\n128\n\np. ���\n\np. ���\n\nChapter 29 BRIN\n\nand the logical order defined by the greater than and less than operations. For other types of summary information,“similar properties” may vary.\n\nIt will not be wrong to think of ���� as an accelerator of sequential heap scans rather than an index in the conventional sense of the word. It can be regarded as an alternative to partitioning, with each range representing a virtual partition.\n\n29.2 Example\n\nOur demo database contains no tables that are large enough for ����, but we can imagine that analytical reports demand that we have a denormalized table con- taining summary information on all the departed and arrived flights of a particular airport, down to the occupied seats. The data for each airport is updated daily, as soon as it is midnight in the corresponding timezone. The added data is neither updated nor deleted.\n\nThe table looks as follows:\n\nCREATE TABLE flights_bi( airport_code char(3), airport_coord point, airport_utc_offset interval, -- timezone flight_no char(6), flight_type text, scheduled_time timestamptz, actual_time timestamptz, aircraft_code char(3), seat_no varchar(4), fare_conditions varchar(10), -- travel class passenger_id varchar(20), passenger_name text\n\n-- airport coordinates\n\n-- departure or arrival\n\n);\n\nData loading can be emulated using nested loops:1 the outer loop will correspond to days (the demo database stores annual data), while the inner loop will be based on timezones. As a result, the loaded data will be more or less ordered at least by time and airports, even though it is not explicitly sorted within the loop.\n\n1 edu.postgrespro.ru/internals-14/flights_bi.sql\n\n592\n\n29.2 Example\n\nI will load an existing copy of the database that takes roughly � �� and contains about �� million rows:1\n\npostgres$ pg_restore -d demo -c flights_bi.dump\n\n=> ANALYZE flights_bi;\n\n=> SELECT count(*) FROM flights_bi;\n\ncount −−−−−−−−−− 30517076\n\n(1 row)\n\n=> SELECT pg_size_pretty(pg_total_relation_size('flights_bi'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n4129 MB\n\n(1 row)\n\nWe can hardly call it a large table, but this data volume will be enough to demon- strate how ���� works. I will create an index in advance:\n\n=> CREATE INDEX ON flights_bi USING brin(scheduled_time);\n\n=> SELECT pg_size_pretty(pg_total_relation_size(\n\n'flights_bi_scheduled_time_idx'\n\n));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n184 kB (1 row)\n\nIt takes very little space with the default settings.\n\nA �-tree index is a thousand times bigger, even if data deduplication is enabled. True, its efficiency is also much higher, but an additional volume can turn out to be unaffordable luxury for really large tables.\n\n=> CREATE INDEX flights_bi_btree_idx ON flights_bi(scheduled_time);\n\n=> SELECT pg_size_pretty(pg_total_relation_size(\n\n'flights_bi_btree_idx'\n\n));\n\n1 edu.postgrespro.ru/internals-\\oldstylenums{14}/flights_bi.dump.\n\n593\n\nv. ��\n\nChapter 29 BRIN\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n210 MB (1 row)\n\n=> DROP INDEX flights_bi_btree_idx;\n\n29.3 Page Layout\n\nThe zero page of a ���� index is the metapage that keeps information on the index structure.\n\nAt a certain offset from the metadata, there are pages with summary information. Each index entry in such a page contains a summary of a particular block range.\n\nThe space between the metapage and the summary information is taken by the rangemap,whichissometimesalsoreferredtoasareversemap(hencethecommon revmap abbreviation). It is effectively an array of pointers to the corresponding index rows; the index number in this array corresponds to the range number.\n\nmetapage\n\nrevmap\n\n1 .. 10 1 .. 10\n\n11 .. 20 11 .. 20\n\n21 .. 30 21 .. 30\n\n71 .. 80 71 .. 80\n\n31 .. 40 31 .. 40\n\n41 .. 50 41 .. 50\n\n51 .. 60 51 .. 60\n\n61 .. 70 61 .. 70\n\nAs the table is expanding,the size of the range map also grows. If the map does not fittheallottedpages,itovertakesthenextpage,andalltheindexentriespreviously located in this page are transferred to other pages. Since a page can accommodate many pointers, such transfers are quite rare.\n\n594\n\n29.3 Page Layout\n\nB��� index pages can be displayed by the pageinspect extension, as usual. The metadata includes the size of the range and the number of pages reserved for the range map:\n\n=> SELECT pagesperrange, lastrevmappage FROM brin_metapage_info(get_raw_page( 'flights_bi_scheduled_time_idx', 0\n\n));\n\npagesperrange | lastrevmappage −−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n128 |\n\n4\n\n(1 row)\n\nHere the range map takes four pages, from the first to the fourth one. We can take a look at the pointers to index entries containing summarized data:\n\n=> SELECT * FROM brin_revmap_data(get_raw_page(\n\n'flights_bi_scheduled_time_idx', 1\n\n));\n\npages −−−−−−−−−− (6,197) (6,198) (6,199) ... (6,195) (6,196)\n\n(1360 rows)\n\nIf the range is not summarized yet, the pointer in the range map is ����.\n\nAnd here are the summaries for several ranges:\n\n=> SELECT itemoffset, blknum, value FROM brin_page_items(\n\nget_raw_page('flights_bi_scheduled_time_idx', 6), 'flights_bi_scheduled_time_idx'\n\n) ORDER BY blknum LIMIT 3 \\gx\n\n595\n\np. ���\n\nChapter 29 BRIN\n\n−[ RECORD 1 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− itemoffset | 197 | 0 blknum | {2016−08−15 02:45:00+03 .. 2016−08−15 16:20:00+03} value −[ RECORD 2 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− itemoffset | 198 | 128 blknum value | {2016−08−15 05:50:00+03 .. 2016−08−15 18:55:00+03} −[ RECORD 3 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− itemoffset | 199 | 256 blknum | {2016−08−15 07:15:00+03 .. 2016−08−15 18:50:00+03} value\n\n29.4 Search\n\nIf a query condition is supported by the ���� index,1 the executor scans the range mapandthesummaryinformationforeachrange. Ifthedatainarangemaymatch thesearchkey,allthepagesthatbelongtothisrangeareaddedtothebitmap. Since ���� does not keep ��s of separate tuples, the bitmap is always lossy.\n\nMatching the data against the search key is performed by the consistency function, which interprets range summary information. Non-summarized ranges are always added to the bitmap.\n\nThe received bitmap is used to scan the table in the usual manner . It is important to mention that heap page reads happen sequentially, block range by block range, and prefetching is employed.\n\n29.5 Summary Information Updates\n\nValue Insertion\n\nAs a new tuple is added into a heap page, the summary information in the corre- sponding index range is updated.2 The range number is calculated based on the\n\n1 backend/access/brin/brin.c, bringetbitmap function 2 backend/access/brin/brin.c, brininsert function\n\n596\n\n29.5 Summary Information Updates\n\npage number using simple arithmetic operations,and the summary information is then located by the range map.\n\nTo determine whether the current summary information has to be expanded, the addition function is employed. If an expansion is required and the page has enough free space, it is done in-place (without adding a new index entry).\n\nSuppose we have added a tuple with value �� to page ��. The range number is cal- culated by integer division of page number by the size of the range. Assuming that the range size equals four pages, we get range number �; since range numbering is zero-based, we take the fourth pointer in the range map. The minimal value in this range is ��, the maximal one is ��. The added value falls outside these limits, so the maximal value is increased:\n\nmetapage\n\nrevmap\n\n1 .. 10 1 .. 10\n\n11 .. 20 11 .. 20\n\n21 .. 30 21 .. 30\n\n71 .. 80 71 .. 80\n\n31 .. 42 31 .. 42\n\n41 .. 50 41 .. 50\n\n51 .. 60 51 .. 60\n\n61 .. 70 61 .. 70\n\nIf an in-place update is impossible, a new entry is added, and the range map is modified.\n\nRange Summarization\n\nEverything said above applies to scenarios when a new tuple appears in an already summarized range. When an index is being built, all the existing ranges are sum- marized, but as the table grows, new pages may fall outside these ranges.\n\nautosummarize storage parameter enabled, the new If an index is created with the range will be summarized at once. But in data warehouses, where rows are usually added in large batches rather than one by one, this mode can seriously slow down insertion.\n\n597\n\noff\n\np. ���\n\np. ���\n\nChapter 29 BRIN\n\nBy default, new ranges are not summarized right away. It does not affect index correctness because ranges with no summary information are always scanned. or Summarization is performed asynchronously, either during table vacuuming when initiated manually by calling the brin_summarize_new_values function (or the brin_summarize_range function that processes a single range).\n\nRange summarization1 does not lock the table for updates. At the beginning of this process, a placeholder entry is inserted into the index for this range. If the data in the range is changed while this range is being scanned, the placeholder will be updated with the summary information on these changes. Then the union function will unite this data with the summary information on the corresponding range.\n\nIn theory, summary information could sometimes shrink after some rows are deleted. But while �i�� indexes can redistribute data after a page split, summary information of ���� indexes never shrinks and can only get wider. Shrinking is usually not required here because a data storage is typically used only for ap- pending new data. You can manually delete summary information by calling the brin_desummarize_range function for this range to be summarized again, but there is no clue as to which ranges might benefit from it.\n\nThus, ���� is primarily targeted at tables of very large size, which either have min- imal updates that add new rows mostly to the end of the file, or are not updated at all. It is mainly used in data warehouses for building analytical reports.\n\n29.6 Minmax Classes\n\nFor data types that allow comparing values,summary information includes at least the maximal and minimal values. The corresponding operator classes contain the word minmax in their names:2\n\n=> SELECT opcname FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid\n\nWHERE amname = 'brin' AND opcname LIKE '%minmax_ops' ORDER BY opcname;\n\n1 backend/access/brin/brin.c, summarize_range function 2 backend/access/brin/brin_minmax.c\n\n598\n\n29.6 Minmax Classes\n\nopcname −−−−−−−−−−−−−−−−−−−−−−−−\n\nbit_minmax_ops bpchar_minmax_ops bytea_minmax_ops char_minmax_ops ... timestamptz_minmax_ops timetz_minmax_ops uuid_minmax_ops varbit_minmax_ops\n\n(26 rows)\n\nHere are the support functions of these operator classes:\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'brin' AND opcname = 'numeric_minmax_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−\n\n1 | brin_minmax_opcinfo 2 | brin_minmax_add_value 3 | brin_minmax_consistent 4 | brin_minmax_union\n\n(4 rows)\n\nThe first function returns the operator class metadata, and all the other functions have already been described: they insert new values, check consistency, and per- form union operations.\n\nThe minmax class includes the same comparison operators that we have seen for �-trees\n\n:\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'brin' AND opcname = 'numeric_minmax_ops' ORDER BY amopstrategy;\n\n599\n\np. ���\n\np. ��\n\nChapter 29 BRIN\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n<(numeric,numeric) | numeric_lt | <=(numeric,numeric) | numeric_le | =(numeric,numeric) | numeric_eq | >=(numeric,numeric) | numeric_ge | | numeric_gt | >(numeric,numeric)\n\n1 2 3 4 5\n\n(5 rows)\n\nChoosing Columns to be Indexed\n\nWhich columns does it make sense to index using this operator class? As men- tioned earlier, such indexes work well if the physical location of rows correlates with the logical order of values.\n\nLet’s check it for the above example.\n\n=> SELECT attname, correlation, n_distinct FROM pg_stats WHERE tablename = 'flights_bi' ORDER BY correlation DESC NULLS LAST;\n\nattname\n\n|\n\ncorrelation\n\n|\n\nn_distinct\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n25926 | scheduled_time 34469 | actual_time 3 | fare_conditions 2 | flight_type 11 airport_utc_offset | 8 | aircraft_code 104 | airport_code 461 | seat_no 710 | flight_no | −0.00046121294 | 2.610987e+06 passenger_id 8618 | passenger_name 0 | airport_coord\n\n0.9999949 | 0.9999948 | 0.7976897 | 0.4981733 | 0.4440067 | 0.19249801 | 0.061483838 | 0.0024594965 | 0.0020146023 |\n\n−0.012388787 | |\n\n(12 rows)\n\nThe data is ordered by time (both scheduled and actual time; there is little dif- ference, if any): new entries are added in chronological order, and as the data is of the table se- neither updated nor deleted, all the rows get into the main fork quentially, one after another.\n\n600",
      "page_number": 565
    },
    {
      "number": 29,
      "title": "BRIN",
      "start_page": 593,
      "end_page": 629,
      "detection_method": "regex_chapter",
      "content": "29.6 Minmax Classes\n\nColumns fare_conditions, flight_type,and airport_utc_offset have relatively high cor- relation, but they store too few distinct values.\n\nThe correlation in other columns is too low for their indexing with the minmax operator class to be of any interest.\n\nRange Size and Search Efficiency\n\nAn appropriate range size can be determined based on the number of pages used to store particular values.\n\nLet’s take a look at the scheduled_time column and get the information on all the flights performed in �� hours. We first have to find out how many table pages are taken by the data related to this time interval.\n\nTo get this number,we can use the fact that a ��� consists of a page number and an offset. Unfortunately, there is no built-in function to break down a ��� into these two components,so we will have to write our own clumsy function to perform type casting via a text representation:\n\n=> CREATE FUNCTION tid2page(t tid) RETURNS integer LANGUAGE sql RETURN (t::text::point)[0]::integer;\n\nNow we can see how days are distributed through the table:\n\n=> SELECT min(numblk), round(avg(numblk)) avg, max(numblk) FROM (\n\nSELECT count(distinct tid2page(ctid)) numblk FROM flights_bi GROUP BY scheduled_time::date\n\n) t;\n\nmin\n\n| avg\n\n| max\n\n−−−−−−+−−−−−−+−−−−−− 1192 | 1447 | 1512\n\n(1 row)\n\nAs we can notice,the data distribution is not quite uniform. With a standard range size of ��� pages, each day will take from � to �� ranges. While fetching the data for a particular day, the index scan will return both the rows that are really needed and some rows related to other days that got into the same ranges. The bigger\n\n601\n\np. ���\n\nChapter 29 BRIN\n\nthe range size, the more extra boundary values will be read; we can change their number by reducing or increasing the range size.\n\nLet’s try out a query for some particular day (I have already created an index with the default settings). For simplicity, I will forbid parallel execution:\n\n=> SET max_parallel_workers_per_gather = 0;\n\n=> \\set d '2016-08-15 02:45:00+03'\n\n=> EXPLAIN (analyze, buffers, costs off, timing off, summary off) SELECT * FROM flights_bi WHERE scheduled_time >= :'d'::timestamptz\n\nAND scheduled_time < :'d'::timestamptz + interval '1 day';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights_bi (actual rows=81964 loops=1)\n\nRecheck Cond: ((scheduled_time >= '2016−08−15 02:45:00+03'::ti... Rows Removed by Index Recheck: 11606 Heap Blocks: lossy=1536 Buffers: shared hit=1561 −> Bitmap Index Scan on flights_bi_scheduled_time_idx\n\n(actual rows=15360 loops=1) Index Cond: ((scheduled_time >= '2016−08−15 02:45:00+03'::... Buffers: shared hit=25\n\nPlanning:\n\nBuffers: shared hit=1\n\n(11 rows)\n\nWe can define an efficiency factor of a ���� index for a particular query as a ratio between the number of pages skipped in an index scan and the total number of pages in the table. If the efficiency factor is zero, the index access degrades to sequential scanning (without taking overhead costs into account ). The higher the efficiency factor, the fewer pages have to be read. But as some pages contain the data to be returned and cannot be skipped, the efficiency factor is always smaller than one.\n\nIn this particular case,the efficiency factor is 528417−1561 the number of pages in the table.\n\n528417\n\n≈ �.���,where ���,��� is\n\nHowever, we cannot draw any meaningful conclusions based on a single value. Even if we had uniform data and ideal correlation, the efficiency would still vary because, at the very least, range boundaries will not match page boundaries. We\n\n602\n\n29.6 Minmax Classes\n\ncan get the full picture only if we treat the efficiency factor as a random value and analyze its distribution.\n\nForourexample,wecanselectallthedifferentdaysoftheyear,checktheexecution plan for each value, and calculate statistics based on this selection. We can easily automate this process because the ������� command can return the results in the ���� format, which is convenient to parse. I will not provide all the code here, but the following snippet contains all the important details:\n\n=> DO $$ DECLARE\n\nplan jsonb;\n\nBEGIN\n\nEXECUTE\n\n'EXPLAIN (analyze, buffers, timing off, costs off, format json)\n\nSELECT * FROM flights_bi WHERE scheduled_time >= $1\n\nAND scheduled_time < $1 + interval ''1 day'''\n\nUSING '2016-08-15 02:45:00+03'::timestamptz INTO plan; RAISE NOTICE 'shared hit=%, read=%',\n\nplan -> 0 -> 'Plan' ->> 'Shared Hit Blocks', plan -> 0 -> 'Plan' ->> 'Shared Read Blocks';\n\nEND; $$;\n\nNOTICE: DO\n\nshared hit=1561, read=0\n\nThe results can be visually displayed as a box plot, also known as a “box-and- whiskers.” The whiskers here denote the first and fourth quartiles (that is,the right whisker gets ��% of the largest values, while the left one gets ��% of the small- est values). The box itself holds the remaining ��% of values and has the median value marked. What is more important, this compact representation enables us to visually compare different results. The following illustration shows the efficiency factor distribution for the default range size and for two other sizes that are four times larger and smaller.\n\nAs we could have expected, the search accuracy and efficiency are high even for rather large ranges.\n\nThe dashed line here marks the average value of the maximal efficiency factor pos- sible for this query, assuming that one day takes roughly 1 365\n\nof the table.\n\n603\n\nChapter 29 BRIN\n\n32 pages/range, 529 kB\n\n128 pages/range, 184 kB\n\n512 pages/range, 72 kB\n\n0,990\n\n0,992\n\n0,994\n\n0,996\n\n0,998\n\n1,000\n\nefficiency factor\n\nNote that the rise in efficiency comes at the expense of the index size increase. B��� is quite flexible in letting you find the balance between the two.\n\nProperties\n\nB��� properties are hardwired and do not depend on operator classes.\n\nAccess Method Properties\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'brin';\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | can_order | f | f | can_unique | can_multi_col | t | f | can_exclude | f | can_include\n\nbrin brin brin brin brin\n\n(5 rows)\n\nObviously, neither sorting nor uniqueness properties are supported. Since a ���� index always returns a bitmap,exclusion constraints are not supported either. Nei- ther do additional ������� columns make any sense, as even indexing keys are not stored in ���� indexes.\n\n604\n\n29.6 Minmax Classes\n\nHowever, we can create a multicolumn ���� index. In this case, summary informa- tion for each column is collected and stored in a separate index entry,but they still have a common range mapping. Such an index is useful if the same range size is applicable to all the indexed columns.\n\nAlternatively, we can create separate ���� indexes for several columns and take advantage of the fact that bitmaps\n\ncan be merged together. For example:\n\n=> CREATE INDEX ON flights_bi USING brin(airport_utc_offset);\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM flights_bi WHERE scheduled_time >= :'d'::timestamptz\n\nAND scheduled_time < :'d'::timestamptz + interval '1 day' AND airport_utc_offset = '08:00:00';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights_bi (actual rows=1658 loops=1)\n\nRecheck Cond: ((scheduled_time >= '2016−08−15 02:45:00+03'::ti... Rows Removed by Index Recheck: 14077 Heap Blocks: lossy=256 −> BitmapAnd (actual rows=0 loops=1)\n\n−> Bitmap Index Scan on flights_bi_scheduled_time_idx (act... Index Cond: ((scheduled_time >= '2016−08−15 02:45:00+0... −> Bitmap Index Scan on flights_bi_airport_utc_offset_idx ...\n\nIndex Cond: (airport_utc_offset = '08:00:00'::interval)\n\n(9 rows)\n\nIndex-Level Properties\n\n=> SELECT p.name, pg_index_has_property(\n\n'flights_bi_scheduled_time_idx', p.name\n\n) FROM unnest(array[\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| f clusterable | f index_scan bitmap_scan | t backward_scan | f\n\n(4 rows)\n\n605\n\np. ���\n\nChapter 29 BRIN\n\nObviously, bitmap scanning is the only supported access type.\n\nLack of clusterization may seem puzzling. Since ���� is sensitive to the physical order of rows,it is quite logical to assume that it should support reordering,which would maximize its efficiency. But clusterization of large tables is anyway a luxury, taking into account all the processing and extra disk space required to rebuild a table. Besides, as the example of the flights_bi table shows, some ordering in data storages can occur naturally.\n\nColumn-Level Properties\n\n=> SELECT p.name, pg_index_column_has_property( 'flights_bi_scheduled_time_idx', 1, p.name\n\n) FROM unnest(array[\n\n'orderable', 'distance_orderable', 'returnable', 'search_array', 'search_nulls'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\norderable | f distance_orderable | f | f returnable | f search_array | t search_nulls\n\n(5 rows)\n\nThe only available column-level property is ���� support. To track ���� values in a range, summary information provides a separate attribute:\n\n=> SELECT hasnulls, allnulls, value FROM brin_page_items(\n\nget_raw_page('flights_bi_airport_utc_offset_idx', 6), 'flights_bi_airport_utc_offset_idx'\n\n) WHERE itemoffset= 1;\n\nhasnulls | allnulls |\n\nvalue\n\n−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−\n\nf\n\n| f\n\n| {03:00:00 .. 03:00:00}\n\n(1 row)\n\n606\n\n29.7 Minmax-Multi Classes\n\n29.7 Minmax-Multi Classes\n\nThe established correlation can be easily disrupted by data updates. The reason is not an actual modification of a particular value but rather the ���� design itself : an old version of a row may be deleted in one page, while its new version may be inserted into any location that is currently free, so the original row order cannot be preserved.\n\nTo minimize this effect to some extent, we can reduce the value of the fillfactor storage parameter to leave more space in the page for future updates. But is it really worth increasing the size of an already huge table? Besides, deletions will anyway free some space in existing pages,thus preparing traps for new tuples that would otherwise get to the end of the file.\n\nSuch a situation can be easily emulated. Let’s delete �.�% of randomly chosen rows and vacuum the table to clean up some space for new tuples:\n\n=> WITH t AS ( SELECT ctid FROM flights_bi TABLESAMPLE BERNOULLI(0.1) REPEATABLE(0)\n\n) DELETE FROM flights_bi WHERE ctid IN (SELECT ctid FROM t);\n\nDELETE 30180\n\n=> VACUUM flights_bi;\n\nNow let’s add some data for a new day in one of the timezones. I will simply copy the data of the previous day:\n\n=> INSERT INTO flights_bi SELECT airport_code, airport_coord, airport_utc_offset,\n\nflight_no, flight_type, scheduled_time + interval '1 day', actual_time + interval '1 day', aircraft_code, seat_no,\n\nfare_conditions, passenger_id, passenger_name\n\nFROM flights_bi WHERE date_trunc('day', scheduled_time) = '2017-08-15'\n\nAND airport_utc_offset = '03:00:00';\n\nINSERT 0 40532\n\n607\n\nv. ��\n\np. ��\n\nChapter 29 BRIN\n\nThe performed deletion was enough to free some space in all or almost all the ranges. Getting into pages located somewhere in the middle of the file,new tuples have automatically expanded the ranges. For example, the summary information related to the first range used to cover less than a day, but now it comprises the whole year:\n\n=> SELECT value FROM brin_page_items(\n\nget_raw_page('flights_bi_scheduled_time_idx', 6), 'flights_bi_scheduled_time_idx'\n\n) WHERE blknum = 0;\n\nvalue −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− {2016−08−15 02:45:00+03 .. 2017−08−16 09:35:00+03}\n\n(1 row)\n\nThe smaller the date specified in the query, the more ranges have to be scanned. The graph shows the magnitude of the disaster:\n\n128 pages/range, 248 kB\n\n0\n\n0,1\n\n0,2\n\n0,3\n\n0,4\n\n0,5\n\n0,6\n\n0,7\n\n0,8\n\n0,9\n\n1\n\nefficiency factor\n\nTo address this issue, we have to make the summary information a bit more so- phisticated: instead of a single continuous range, we have to store several smaller ones that cover all the values when taken together. Then one of the ranges can cover the main set of data, while the rest will handle occasional outliers.\n\nSuch functionality is provided by minmax-multi operator classes:1\n\n=> SELECT opcname FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid\n\nWHERE amname = 'brin' AND opcname LIKE '%minmax_multi_ops' ORDER BY opcname;\n\n1 backend/access/brin/brin_minmax_multi.c\n\n608\n\n29.7 Minmax-Multi Classes\n\nopcname −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\ndate_minmax_multi_ops float4_minmax_multi_ops float8_minmax_multi_ops inet_minmax_multi_ops ... time_minmax_multi_ops timestamp_minmax_multi_ops timestamptz_minmax_multi_ops timetz_minmax_multi_ops uuid_minmax_multi_ops\n\n(19 rows)\n\nAs compared to minmax operator classes, minmax-multi classes have one more sup- port function that computes the distance between values; it is used to determine the range length, which the operator class strives to reduce:\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'brin' AND opcname = 'numeric_minmax_multi_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 1 | brin_minmax_multi_opcinfo 2 | brin_minmax_multi_add_value 3 | brin_minmax_multi_consistent 4 | brin_minmax_multi_union 5 | brin_minmax_multi_options\n\n11 | brin_minmax_multi_distance_numeric\n\n(6 rows)\n\nThe operators of such classes are absolutely the same as those of the minmax classes.\n\nMinmax-multi classes can take the values_per_range parameter, which defines the maximal allowed number of summarized values per range. A summarized value is represented by two numbers (an interval),while a separate point requires just one. If there are not enough values, some of the intervals are reduced.1\n\n1 backend/access/brin/brin_minmax_multi.c, reduce_expanded_ranges function\n\n609\n\n32\n\nChapter 29 BRIN\n\nLet’s build a minmax-multi index instead of the existing one. We will limit the num- ber of allowed values per range to ��:\n\n=> DROP INDEX flights_bi_scheduled_time_idx;\n\n=> CREATE INDEX ON flights_bi USING brin(\n\nscheduled_time timestamptz_minmax_multi_ops(\n\nvalues_per_range = 16\n\n)\n\n);\n\nThe graph shows that the new index brings the efficiency back to the original level. Quite expectedly, it leads to an increase in the index size:\n\nminmax-multi 656 kB\n\nminmax 184 kB\n\n0,990\n\n0,992\n\n0,994\n\n0,996\n\n0,998\n\n1,000\n\nefficiency factor\n\n29.8 Inclusion Classes\n\nThe difference between minmax and inclusion operator classes is roughly the same as the difference between �-trees and �i�� indexes: the latter are designed for data types that do not support comparison operations, although mutual alignment of values still makes sense for them. Summary information for a particular range provided by inclusion operator classes is represented by the bounding box of the values in this range.\n\nHere are these operator classes; they are not numerous:\n\n=> SELECT opcname FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid\n\nWHERE amname = 'brin' AND opcname LIKE '%inclusion_ops' ORDER BY opcname;\n\n610\n\n29.8 Inclusion Classes\n\nopcname −−−−−−−−−−−−−−−−−−−−−\n\nbox_inclusion_ops inet_inclusion_ops range_inclusion_ops\n\n(3 rows)\n\nThe list of support functions is extended by one more mandatory function that merges two values, and by a bunch of optional ones:\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'brin' AND opcname = 'box_inclusion_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n1 | brin_inclusion_opcinfo 2 | brin_inclusion_add_value 3 | brin_inclusion_consistent 4 | brin_inclusion_union\n\n11 | bound_box 13 | box_contain\n\n(6 rows)\n\nWhen dealing with values that can be compared, we relied on their correlation; but for other data types, no such statistic is collected,1 so it is hard to predict the efficiency of an inclusion-based ���� index.\n\nWhat is worse, correlation greatly affects cost estimation of an index scan. If such statistic is unavailable, it is taken as zero.2 Thus, the planner has no way to tell between exact and fuzzy inclusion indexes, so it typically avoids using them alto- gether.\n\nPost��� collects\n\nstatistics on correlation of spatial data.\n\nIn this particular case, we can presume that it makes sense to build an index over airport coordinates, as longitude must correlate with the timezone.\n\n1 backend/commands/analyze.c, compute_scalar_stats function 2 backend/utils/adt/selfuncs.c, brincostestimate function\n\n611\n\nv. �.�.�\n\nChapter 29 BRIN\n\nUnlike �i�� predicates, ���� summary information has the same type as the in- dexed data; therefore, it is not so easy to build an index for points. But we can create an expression index by converting points into dummy rectangles:\n\n=> CREATE INDEX ON flights_bi USING brin(box(airport_coord)) WITH (pages_per_range = 8);\n\n=> SELECT pg_size_pretty(pg_total_relation_size(\n\n'flights_bi_box_idx'\n\n));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n3816 kB\n\n(1 row)\n\nAn index built over timezones with the same range size takes approximately the same volume (���� k�).\n\nThe operators included into this class are similar to �i�� operators. For example, a ���� index can be used to speed up search for points in a certain area:\n\n=> SELECT airport_code, airport_name FROM airports WHERE box(coordinates) <@ box '135,45,140,50';\n\nairport_code |\n\nairport_name\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−\n\nKHV\n\n| Khabarovsk−Novy Airport\n\n(1 row)\n\nBut as mentioned earlier, the planner refuses to use an index scan unless we turn off sequential scanning:\n\n=> EXPLAIN (costs off) SELECT * FROM flights_bi WHERE box(airport_coord) <@ box '135,45,140,50';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights_bi\n\nFilter: (box(airport_coord) <@ '(140,50),(135,45)'::box)\n\n(2 rows)\n\n=> SET enable_seqscan = off;\n\n612\n\n29.9 Bloom Classes\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM flights_bi WHERE box(airport_coord) <@ box '135,45,140,50';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights_bi (actual rows=511414 loops=1)\n\nRecheck Cond: (box(airport_coord) <@ '(140,50),(135,45)'::box) Rows Removed by Index Recheck: 630756 Heap Blocks: lossy=19656 −> Bitmap Index Scan on flights_bi_box_idx (actual rows=196560...\n\nIndex Cond: (box(airport_coord) <@ '(140,50),(135,45)'::box)\n\n(6 rows)\n\n=> RESET enable_seqscan;\n\n29.9 Bloom Classes\n\nOperator classes based on the Bloom filter enable ���� usage for any data types that support the equal to operation and have a hash function defined. They can also be applied to regular ordinal types if values are localized in separate ranges but their physical location has no correlation with the logical order.\n\nThe names of such operator classes contain the word bloom:1\n\n=> SELECT opcname FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid\n\nWHERE amname = 'brin' AND opcname LIKE '%bloom_ops' ORDER BY opcname;\n\nopcname −−−−−−−−−−−−−−−−−−−−−−−\n\nbpchar_bloom_ops bytea_bloom_ops char_bloom_ops ... timestamptz_bloom_ops timetz_bloom_ops uuid_bloom_ops\n\n(24 rows)\n\n1 backend/access/brin/brin_bloom.c\n\n613\n\nv. ��\n\n−0.1\n\np. ���\n\nChapter 29 BRIN\n\nThe classic Bloom filter is a data structure that enables you to quickly check whether an element belongs to a set. This filter is very compact, but it allows false positives: a set may be assumed to contain more elements than it actually does. But what is more important, false negatives are ruled out: the filter cannot decide that an element is not present in the set if it is actually there.\n\nThe filter is an array of m bits (also called a signature), which is originally filled with zeros. We select k different hash functions to map any element of the set to k bits of the signature. When an element is added to the set, each of the bits in the signature is set to one. Consequently, if all the bits that correspond to an element are set to one, the element may be present in the set; if there is at least one zero bit, the element is guaranteed to be absent.\n\nIn the case of ���� indexes, the filter processes a set of values of an indexed col- umn that belong to a particular range; the summary information for this range is represented by the built Bloom filter.\n\nThe bloom extension1 provides its own index access method based on the Bloom filter. It builds a filter for each table row and deals with a set of column values of each row. Such an index is designed for indexing several columns at a time and can be used in adhoc queries,when the columns to be referenced in filter conditions are not known in advance. A ���� indexcan also be built on several columns,but its summaryinformation will contain several independent Bloom filters for each of these columns.\n\nThe accuracy of the Bloom filter depends on the signature length. In theoretical terms,theoptimalnumberofsignaturebitscanbeestimatedatm = −nlog2 p ,where n is the number of elements in the set and p is the probability of false positives.\n\nln2\n\nThese two settings can be adjusted using the corresponding operator class param- eters:\n\n\n\nn_distinct_per_range defines the number of elements in a set; in this case, it is the number of distinct values in one range of an indexed column. This param- eter value is interpreted just like statistics on distinct values: negative values indicate the fraction of rows in the range, not their absolute number.\n\n1 postgresql.org/docs/14/bloom.html\n\n614\n\n29.9 Bloom Classes\n\n\n\nfalse_positive_rate defines the probability of false positives.\n\nA near-zero value means that an index scan will almost certainly skip the ranges that have no searched values. But it does not guarantee exact search, as the scanned ranges will also contain extra rows that do not match the query. Such behavior is due to range width and physical data location rather than to the actual filter properties.\n\nThe list of support functions is extended by a hash function:\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'brin' AND opcname = 'numeric_bloom_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n1 | brin_bloom_opcinfo 2 | brin_bloom_add_value 3 | brin_bloom_consistent 4 | brin_bloom_union 5 | brin_bloom_options\n\n11 | hash_numeric\n\n(6 rows)\n\nSince the Bloom filter is based on hashing,only the equality operator is supported:\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'brin' AND opcname = 'numeric_bloom_ops' ORDER BY amopstrategy;\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n=(numeric,numeric) | numeric_eq |\n\n1\n\n(1 row)\n\nLet’s take the flight_no column that stores flight numbers; it has near-zero corre- lation, so it is useless for a regular range operator class. We will keep the default\n\n615\n\n0.01\n\nChapter 29 BRIN\n\nfalse-positive setting; as for the number of distinct values in a range, it can be easily calculated. For example, for an eight-page range we will get the following value:\n\n=> SELECT max(nd) FROM (\n\nSELECT count(distinct flight_no) nd FROM flights_bi GROUP BY tid2page(ctid) / 8\n\n) t;\n\nmax −−−−− 22 (1 row)\n\nFor smaller ranges, this number will be even lower (but in any case, the operator class does not allow values smaller than ��).\n\nWe just have to create an index and check the execution plan:\n\n=> CREATE INDEX ON flights_bi USING brin(\n\nflight_no bpchar_bloom_ops(\n\nn_distinct_per_range = 22)\n\n) WITH (pages_per_range = 8);\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM flights_bi WHERE flight_no = 'PG0001';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights_bi (actual rows=5192 loops=1)\n\nRecheck Cond: (flight_no = 'PG0001'::bpchar) Rows Removed by Index Recheck: 122894 Heap Blocks: lossy=2168 −> Bitmap Index Scan on flights_bi_flight_no_idx (actual rows=...\n\nIndex Cond: (flight_no = 'PG0001'::bpchar)\n\n(6 rows)\n\n=> RESET max_parallel_workers_per_gather;\n\nThe graph shows that for some flight numbers (represented by separate points that do not belong to any whiskers) the index does not work very well, but its overall efficiency is quite high:\n\n616\n\n2 pages/range, 14,8 MB\n\n4 pages/range, 7,4 Mb\n\n8 pages/range, 3,7MB\n\n0,70\n\n0,75\n\n0,80\n\n0,85\n\n0,90\n\n0,95\n\n29.9 Bloom Classes\n\n1,00\n\nefficiency factor\n\n617\n\nConclusion\n\nWell, now our journey is coming to an end. I hope that you have found the book useful—or at least interesting—and have learned something new from it (I myself did learn a lot while I was writing it).\n\nMostofthecoveredinformationislikelytoremainup-to-dateforquitealongtime, but some details will inevitably change very fast. I believe that the biggest value of this book is not a set of particular facts but rather the approach to exploring the system that I show. Neither this book nor the documentation should be taken for granted. Contemplate, experiment, verify all the facts yourself: Postgre��� provides all the tools that you need for it, and I tried to show how to use them. It is usually almost as easy as asking a question on a forum or googling the answer, but is definitely much more reliable and useful.\n\nFor the same reason, I wanted to encourage you to look into the code. Do not get intimidated by its complexity: simply try it out. Open source is a great advantage, so take this opportunity.\n\nI will be happy to get your feedback; you can send your comments and ideas to edu@postgrespro.ru. I am going to update the book regularly, so your comments can help me improve future editions. The latest online version of the book is avail- able for free at postgrespro.com/community/books/internals.\n\nGood luck!\n\n618\n\nIndex\n\nA Aborting transactions 84, 88, 91,\n\n251, 271\n\nAccess method\n\nindex 356, 417 properties 366 table 335 Aggregate 340–341 Aggregation 340, 345 hashing 439, 459 sorting 459\n\nAlignment 75 Analysis 128, 311, 389, 470 Anomaly\n\ndirty read 46, 48, 52 lost update 48, 58, 60 non-repeatable read 49, 54, 61 phantom read 49, 61, 270 read skew 56, 58, 62 read-only transaction 65, 68,\n\n270\n\nwrite skew 64, 67, 270\n\nAppend 440 Array 538, 582, 590 “Asterisk,” the reasons not to use it\n\n37, 421, 455\n\nAtomicity 47, 91 autoprewarm leader 187–189 autoprewarm worker 189 autosummarize 597 autovacuum 129 autovacuum launcher 129–131\n\nautovacuum worker 130\n\nautovacuum_analyze_scale_factor\n\n133\n\nautovacuum_analyze_threshold 133\n\nautovacuum_enabled 121, 131\n\nautovacuum_freeze_max_age 149,\n\n154–155\n\nautovacuum_freeze_min_age 155\n\nautovacuum_freeze_table_age 155\n\nautovacuum_max_workers 130, 139,\n\n144\n\nautovacuum_multix-\n\nact_freeze_max_age 247\n\nautovacuum_naptime 130–131\n\nautovacuum_vacuum_cost_delay 139,\n\n144, 155\n\nautovacuum_vacuum_cost_limit 139,\n\n144\n\nautovacuum_vacuum_in- sert_scale_factor 132–133\n\nautovacuum_vacuum_insert_thresh-\n\nold 132–133\n\nautovacuum_vacuum_scale_factor\n\n131–132\n\nautovacuum_vacuum_threshold\n\n131–132\n\nautovacuum_work_mem 130\n\nautovacuum_freeze_max_age 154\n\n619\n\nIndex\n\nB Backend 39 Background worker 127, 130, 347 Background writing 205\n\nsetup 208\n\nBatch processing 166, 257 bgwriter 205, 208–210, 224 bgwriter_delay 208 bgwriter_lru_maxpages 208, 210 bgwriter_lru_multiplier 208 Binding 305 Bison 290 Bitmap 388\n\nNULL values 75\n\nBitmap Heap Scan 330, 388, 392, 394 Bitmap Index Scan 330, 388, 392,\n\n394, 397\n\nBitmapAnd 391 Bloating 105, 119, 165, 339, 475, 486, 491, 509\n\nBlock see page bloom 614 Bloom filter 532, 613 Box plot 603 BRIN 591\n\nefficiency factor 602 operator class 596, 598, 608,\n\n611–612, 614\n\npages 594 properties 604\n\nB-tree 481, 524, 563, 566, 584\n\noperator class 493, 568, 599 pages 486 properties 504\n\nbtree_gin 584 btree_gist 524, 538\n\n620\n\nBuffer cache 38, 171, 192, 198, 277,\n\n338, 357, 381 configuration 184 eviction 179 local 189, 355 Buffer pin 173, 175, 278 Buffer ring 181, 338\n\nC Cardinality 300, 310, 381\n\njoin 406\n\nCartesian product 399, 401 Checkpoint 198, 216 monitoring 208 setup 205\n\ncheckpoint_completion_target 205–206\n\ncheckpointer 198–199, 204, 206, 208–210, 216\n\ncheckpoint_timeout 206, 209 checkpoint_warning 208 client_encoding 580 CLOG 81, 155, 192, 195, 198 Cluster 23 Cmin and cmax 101 Collation 361, 495, 557 Combo-identifier 101 Commit 81, 195, 251\n\nasynchronous 212 synchronous 212\n\ncommit_delay 212 commit_siblings 212 Consistency 45, 47 Correlated predicates 301, 330 Correlation 325, 376, 387, 591, 600,\n\n611\n\nCost 295, 299, 302\n\ncpu_index_tuple_cost 378 cpu_operator_cost 340, 378, 423, 445,\n\n450, 459\n\ncpu_tuple_cost 339, 341, 379, 411,\n\n423, 445, 450\n\nCTE Scan 352–353 CTID 75, 112 cube 537 Cursor 100, 176, 300, 308, 351 cursor_tuple_fraction 300, 308\n\nD Database 23 data_checksums 217 Deadlocks 232, 258, 267–268 deadlock_timeout 259, 267, 280 debug_print_parse 291 debug_print_plan 294 debug_print_rewritten 292 deduplicate_items 491 Deduplication 491, 563 default_statistics_target 311,\n\n319–320, 323, 334\n\ndefault_table_access_method 335 default_text_search_config 530 default_transaction_isolation 70 Demo database 287, 509, 592 Dirty read 48, 52 Durability 47\n\nE effective_cache_size 381–382 effective_io_concurrency 388 enable_bitmapscan 380 enable_hashjoin 445, 447 enable_memoize 412 enable_mergejoin 413\n\nIndex\n\nenable_parallel_hash 431, 434 enable_seqscan 262, 380, 582, 612 Equi-join 399, 437, 444, 469 Eviction 179, 194, 205 Execution 302, 306\n\nF false_positive_rate 615 fastupdate 268, 574 fdatasync 216 fillfactor 108–109, 115–116, 147,\n\n150, 158, 271, 471, 509, 543, 607 Finalize Aggregate 346 Finalize GroupAggregate 461 Flex 290 force_parallel_mode 351 Foreign keys 242, 244, 406 Fork 28\n\nfree space map 30, 108, 120 initialization 30 main 29, 74, 600 visibility map 31, 108, 149–150,\n\n163, 384 Freezing 146, 162, 177, 246\n\nmanual 155\n\nfrom_collapse_limit 296, 298 fsync 216 Full page image 202 full_page_writes 219, 221 Full-text search 527\n\nindexing 529, 564 partial 570 phrase 579 ranking 579\n\nfuzzystrmatch 562\n\n621\n\nIndex\n\nG Gather 342, 344–346, 352, 458 Gather Merge 458–459 geqo 298 geqo_threshold 298 Getting the result 308 gevel 512, 547 GIN 563\n\ndeferred update 267 fast update 574 operator class 564, 568, 582,\n\n585 pages 566 properties 577 gin_fuzzy_search_limit 576 gin_pending_list_limit 575 GiST 507, 610\n\noperator class 508, 512, 612 pages 511 properties 525, 536\n\nGroupAggregate 461 Grouping 439, 460\n\nH Hash 419, 422, 427, 469 operator class 476 page 470 properties 477 Hash Join 419, 422, 427 Hash table 174, 277, 279, 410, 419,\n\n469\n\nHashAggregate 439–440 hash_mem_multiplier 410, 420, 433,\n\n440\n\nHeader\n\npage 72, 122 row version 75\n\n622\n\ntuple 241\n\nHigh key 487, 489, 566 Hint bits see information bits Histogram 320 Horizon 102–103, 108, 123, 165, 385 HOT updates 112, 491 hstore 539, 590\n\nI idle_in_transaction_session_timeout\n\n166\n\nignore_checksum_failure 218 Incremental Sort 456 Index 356, 362\n\ncovering 370, 383, 386, 504, 552 include 525 integrity constraint 368, 370, 478, 504, 522, 525, 552\n\nmulticolumn 369, 499,\n\n503–504, 525, 578, 605 on expression 328, 363, 612 ordering 367, 372, 481, 495,\n\n499, 503–504\n\npartial 374 pruning 118, 486, 491 statistics 328 unique 242, 368, 370, 483, 491,\n\n504\n\nvacuuming 475, 575 versioning 86\n\nIndex Only Scan 383 Index Scan 375–377, 380, 406, 408 Indexing engine 357, 366 Information bits 75, 79, 82, 95, 219,\n\n241\n\nInitPlan 316, 354 Instance 23\n\nintarray 538, 590 Integrity constraints 45 Isolation 47\n\nsnapshot 51, 67, 94, 241\n\nJ Join\n\nanti- and semi- 400, 414 cost estimation 402, 408, 411,\n\n422, 429, 444, 449, 451, 454, 457–458\n\ndifferent methods 462 hashing 419, 424 inner 399 merging 442, 499 nested loop 400 order 294, 296, 421, 444 outer 399, 413, 444 parallel hash 432, 434 parameterized 405 join_collapse_limit 296–298 JSON 585, 590 jsquery 590\n\nK k-D tree 554\n\nL Locks 50, 229, 357 advisory 268 escalation 241, 273 heavyweight 231, 242 lightweight 277 memory 173 no waits 166, 256 non-relation 265 page 267\n\nIndex\n\npredicate 270 queue 237, 247, 253 relation 128, 159, 164, 224, 234 relation extension 267 row 167, 241 spinlocks 276 tranche 278 transaction ID 233 tuple 247\n\nlock_timeout 257–258 log_autovacuum_min_duration 143 log_checkpoints 208 logical 222, 226 log_lock_waits 280 log_temp_files 427, 454 Lost update 48, 58, 60 ltree 538\n\nM maintenance_io_concurrency 389 maintenance_work_mem 126, 140,\n\n143, 575\n\nMap\n\nbitmap 470, 591, 596, 605 free space 30, 108, 120 freeze 31, 149, 152, 163 visibility 31, 108, 149–150, 163,\n\n384\n\nMaterialization 352, 402, 409 Materialize 402, 404–405, 409–411,\n\n413\n\nmax_connections 232, 273 max_locks_per_transaction 232 max_parallel_processes 187 max_parallel_workers 347 max_parallel_workers_per_gather\n\n347–349\n\n623\n\nIndex\n\nmax_pred_locks_per_page 273 max_pred_locks_per_relation 274 max_pred_locks_per_transaction\n\n273–274\n\nmax_wal_senders 222 max_wal_size 206, 209 max_worker_processes 130, 347 Memoize 409–412, 463 Merge Join 442 Merging 442, 452, 458 minimal 216, 222, 224–225 min_parallel_index_scan_size 127 min_parallel_table_scan_size 348 min_wal_size 207 MixedAggregate 461 Multitransactions 245\n\nwraparound 246\n\nMultiversion concurrency control 52, 74, 119, 491\n\nN n_distinct_per_range 614 Nearest neighbor search 372, 517,\n\n527, 554\n\nNested Loop 295, 400–401, 406, 411 Nested Loop Anti Join 415 Nested Loop Left Join 400, 413 Nested Loop Semi Join 416 Non-repeatable read 49, 54, 61 Non-uniform distribution 317, 427,\n\n471\n\nNULL 75, 314, 373, 503, 506, 526,\n\n553, 606\n\nO OID 24 old_snapshot_threshold 166\n\n624\n\nOperator class 359, 417, 564\n\nparameters 534, 609, 614 support functions 364\n\nOperator family 364 Optimization see planning\n\nP Page 32, 470 dirty 172 fragmentation 75, 110 full image 202 header 157, 163 prefetching 388 split 118, 484–485, 521, 550,\n\n574\n\npageinspect 72, 76, 80, 86, 148, 194, 243, 470, 511, 547, 566, 595\n\npages_per_range 591 Parallel Bitmap Heap Scan 397 Parallel execution 342, 347, 395,\n\n417, 431, 445, 458, 461\n\nlimitations 350\n\nParallel Hash 433 Parallel Hash Join 433 Parallel Index Only Scan 432 Parallel Seq Scan 343–344 parallel_leader_participation 342,\n\n344\n\nparallel_setup_cost 345, 458 parallel_tuple_cost 345, 459 parallel_workers 348 Parsing 290 Partial Aggregate 345 Partial GroupAggregate 461 pgbench 214, 219, 282 pg_buffercache 173, 185 pg_checksums 217\n\npg_controldata 201 PGDATA 23 pg_dump 106 pg_prewarm 187 pg_prewarm.autoprewarm 187 pg_prewarm.autoprewarm_interval\n\n187\n\npg_rewind 193 pgrowlocks 246, 263 pgstattuple 159–160 pg_test_fsync 216 pg_trgm 539, 562, 580 pg_visibility 122, 149 pg_wait_sampling 282 pg_wait_sampling.profile_period 283 pg_waldump 197, 204, 223 Phantom read 49, 61, 270 Plan 294\n\ngeneric and custom 306\n\nplan_cache_mode 308 Planning 294, 306 Pointers to tuples 74 Portal 302 postgres 37 postmaster 37–39, 130, 201, 203, 342 Preparing a statement 304 ProcArray 82, 96 Process 37 Protocol 40\n\nextended query 304 simple query 290\n\nPruning 108, 115, 118, 486, 491 psql 17, 20, 24, 92–93, 281, 287\n\nQ Quadtree 541\n\nIndex\n\nR random_page_cost 339, 380, 392 RD-tree 530 Read Committed 49, 51–54, 56, 58,\n\n61–62, 70–71, 94, 102, 104, 106, 123, 251 Read skew 56, 58, 62 Read Uncommitted 48–49, 51–52 Read-only transaction anomaly 65,\n\n68, 270\n\nRecheck 357, 375, 390, 529, 536,\n\n553, 583\n\nRecovery 201 Relation 27 Repeatable Read 49, 51–52, 61–62, 64–65, 67, 69–71, 94, 103, 106, 156, 251, 271\n\nreplica 222, 224–226 Rewriting see transformation Row version see tuple RTE 291 R-Tree 509 Rule system 292 RUM 579\n\nS Savepoint 88 Scan\n\nbitmap 371, 387, 479, 505, 526,\n\n552, 576, 606\n\ncost estimation 338, 343, 376,\n\n384, 391, 611\n\nindex 272, 371, 375, 505, 526,\n\n552\n\nindex-only 312, 373, 383, 479,\n\n527, 533, 553, 561 method comparison 397\n\n625\n\nIndex\n\nparallel index 395 parallel sequential 343 sequential 271, 337 skip 501\n\nSchema 25 search_path 25 seg 538 Segment 28, 196 Selectivity 300, 338\n\njoin 406\n\nSeq Scan 295, 338, 340–341, 354 seq_page_cost 339, 380, 392, 430 Serializable 50–51, 67, 69–71, 94,\n\n103, 106, 251, 270–271, 274, 351\n\nServer 23 shared_buffers 184 shared_preload_libraries 187, 282 Signature 532, 614 slowfs 283 Snapshot 94, 97, 224 export 106 system catalog 105 Sort 447–448, 450, 459, 462 Sorting 372, 442, 447, 481, 499, 503\n\nexternal 452 heapsort 450 incremental 456 parallel 458 quicksort 449\n\nSpecial space 73 SP-GiST 540\n\noperator class 541, 543, 557 pages 547 properties 552, 561\n\nSplit\n\n626\n\nbucket 469, 472 page 484–485, 521, 550, 574\n\nstartup 201–203 Starvation 247, 253 statement_timeout 258 Statistics 128, 300 basic 310, 384 correlation 325, 377, 600, 611 distinct values 315, 331, 601 expression 326, 334 extended 327 field width 325 histogram 320, 444 most common values 317, 333,\n\n407, 427 multivariate 329 non-scalar data types 324 NULL fraction 314\n\nSubPlan 353–354 Subtransaction 88, 198 Support functions 364 Synchronization 212, 216 synchronous_commit 211–213 System catalog 24, 224, 291\n\nT Tablespace 26 temp_buffers 190 temp_file_limit 190, 425 Tid Scan 376 Timeline 196 TOAST 25, 32, 87, 182 track_commit_timestamp 96 track_counts 129 track_io_timing 179 Transaction 46, 78, 94\n\nabort 84, 88, 91, 251, 271\n\nage 145 commit 81, 195, 212, 251 status 96, 195 subtransaction 88, 198 virtual 87, 233\n\nTransaction ID\n\nlock 233 wraparound 145, 153\n\nTransformation 291 Tree\n\nbalanced 482, 486, 507, 563 non-balanced 540 parse 290 plan 294 radix 556 signature 532\n\nTrigrams 580 Truncation\n\nheap 128 suffix 493\n\nTuple 74\n\ninsert only 127, 132\n\nTuple ID 74, 356, 564 Tuple pointer 110\n\nU Unique 460\n\nV Vacuum 103, 176, 311, 357, 385,\n\n475, 598 aggressive 151 autovacuum 129, 259 full 158 monitoring 140, 161 phases 126 routine 120\n\nIndex\n\nvacuum_cost_delay 138, 155 vacuum_cost_limit 138–139 vacuum_cost_page_dirty 138 vacuum_cost_page_hit 138 vacuum_cost_page_miss 138 vacuum_failsafe_age 149, 155 vacuum_freeze_min_age 149–150,\n\n152, 156\n\nvacuum_freeze_table_age 149,\n\n151–152 vacuum_index_cleanup 156 vacuum_multixact_failsafe_age 247 vacuum_multixact_freeze_min_age\n\n247\n\nvacuum_multixact_freeze_table_age\n\n247 vacuum_truncate 128 vacuum_freeze_min_age 150 values_per_range 609 Virtual transaction 87 Visibility 95, 100, 337, 357, 375, 384 Volatility 57, 364, 374\n\nW Wait-for graph 258 Waits 280\n\nsampling 282 unaccounted-for time 281, 283\n\nWAL see write-ahead log wal_buffers 193 wal_compression 219 wal_keep_size 208 wal_level 222 wal_log_hints 219 wal_recycle 207 wal_segment_size 196 walsender 211, 222\n\n627\n\nIndex\n\nwal_skip_threshold 222–223\n\nwal_sync_method 216\n\nwalwriter 212–213\n\nwal_writer_delay 212–213\n\nwal_writer_flush_after 213\n\nWindowAgg 448\n\nwork_mem 19, 303, 389–391, 393,\n\n402, 410, 420, 427, 431, 433,\n\n628\n\n440, 449, 461 Write skew 64, 67, 270 Write-ahead log 39, 191, 279, 336,\n\n357 levels 221\n\nX Xmin and xmax 75, 77, 81, 83, 95,\n\n145, 241, 246",
      "page_number": 593
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Egor RogovPostgreSQL 14 Internals\n\nPostgres Professional Moscow, ����",
      "content_length": 69,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "The elephant on the cover is a fragment of an illustration from Edward Topsell’s The History of Four-footed Beasts and Serpents, published in London in ����\n\nPostgreSQL 14 Internals by Egor Rogov\n\nTranslated from Russian by Liudmila Mantrova\n\n© Postgres Professional, 2022–2023\n\nISBN 978-5-6045970-4-0\n\nThis book in ��� is available at postgrespro.com/community/books/internals",
      "content_length": 377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "Contents at a Glance\n\nAbout This Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n\nPart I Isolation and MVCC 43 2 Isolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3 Pages and Tuples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 4 Snapshots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 5 Page Pruning and HOT Updates . . . . . . . . . . . . . . . . . . . . . . . . 108 6 Vacuum and Autovacuum . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 7 Freezing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 8 Rebuilding Tables and Indexes . . . . . . . . . . . . . . . . . . . . . . . . 158\n\nPart II Buffer Cache and WAL 169 9 Buffer Cache . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171 10 Write-Ahead Log . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 11 WAL Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n\nPart III Locks 227 12 Relation-Level Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 13 Row-Level Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241 14 Miscellaneous Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265 . . . . . . . . . . . . . . . . . . . . . . . . . 276 15 Locks on Memory Structures\n\nPart IV Query Execution 285 16 Query Execution Stages . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287 17 Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310 18 Table Access Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335 19 Index Access Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356 20 Index Scans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375 21 Nested Loop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399\n\n3",
      "content_length": 2039,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Contents at a Glance\n\n22 Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419 23 Sorting and Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442\n\nPart V Types of Indexes 467 24 Hash . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469 25 B-tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481 26 GiST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507 27 SP-GiST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540 28 GIN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563 29 BRIN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591\n\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618 Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619\n\n4",
      "content_length": 947,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Table of Contents\n\nAbout This Book\n\n1 Introduction\n\n23 1.1 Data Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 System Catalog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Schemas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Tablespaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Files and Forks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 Pages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 TOAST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 1.2 Processes and Memory . . . . . . . . . . . . . . . . . . . . . . . . . . 37 1.3 Clients and the Client-Server Protocol . . . . . . . . . . . . . . . . . 39\n\nPart I Isolation and MVCC\n\n2 Isolation\n\n45 2.1 Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 47 2.2 Isolation Levels and Anomalies in SQL Standard . . . . . . . . . . . Lost Update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 Dirty Reads and Read Uncommitted . . . . . . . . . . . . . . . . . . 48 Non-Repeatable Reads and Read Committed . . . . . . . . . . . . . 49 Phantom Reads and Repeatable Read . . . . . . . . . . . . . . . . . . 49 No Anomalies and Serializable . . . . . . . . . . . . . . . . . . . . . 50 Why These Anomalies? . . . . . . . . . . . . . . . . . . . . . . . . . 50 51 Read Committed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 61 Repeatable Read . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Serializable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 2.4 Which Isolation Level to Use? . . . . . . . . . . . . . . . . . . . . . . 70\n\n2.3 Isolation Levels in PostgreSQL . . . . . . . . . . . . . . . . . . . . .\n\n17\n\n43\n\n5",
      "content_length": 1977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Table of Contents\n\n3 Pages and Tuples\n\n72 3.1 Page Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 Page Header Special Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 Tuples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 Item Pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 Free Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 77 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 Insert Commit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 Delete . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 Abort 84 Update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 3.4 Indexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 87 3.5 TOAST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 3.6 Virtual Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.7 Subtransactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 Savepoints 91 Errors and Atomicity . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n3.2 Row Version Layout 3.3 Operations on Tuples\n\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n4 Snapshots\n\n94 4.1 What is a Snapshot? . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 4.2 Row Version Visibility . . . . . . . . . . . . . . . . . . . . . . . . . . 95 4.3 Snapshot Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 4.4 Visibility of Transactions’ Own Changes . . . . . . . . . . . . . . . . 100 4.5 Transaction Horizon . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 4.6 System Catalog Snapshots . . . . . . . . . . . . . . . . . . . . . . . . 105 4.7 Exporting Snapshots . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n\n5 Page Pruning and HOT Updates\n\n108 5.1 Page Pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 5.2 HOT Updates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 5.3 Page Pruning for HOT Updates . . . . . . . . . . . . . . . . . . . . . 115 5.4 HOT Chain Splits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 . . . . . . . . . . . . . . . . . . . . . . . . 118 5.5 Page Pruning for Indexes\n\n6",
      "content_length": 2598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Table of Contents\n\n6 Vacuum and Autovacuum\n\n120 6.1 Vacuum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 6.2 Database Horizon Revisited . . . . . . . . . . . . . . . . . . . . . . . 123 6.3 Vacuum Phases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 Heap Scan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 Index Vacuuming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 Heap Vacuuming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 Heap Truncation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 6.4 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 6.5 Automatic Vacuum and Analysis . . . . . . . . . . . . . . . . . . . . 129 About the Autovacuum Mechanism . . . . . . . . . . . . . . . . . . . 129 Which Tables Need to be Vacuumed? . . . . . . . . . . . . . . . . . . 131 Which Tables Need to Be Analyzed? . . . . . . . . . . . . . . . . . . 133 Autovacuum in Action . . . . . . . . . . . . . . . . . . . . . . . . . . 134 6.6 Managing the Load . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 Vacuum Throttling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 Autovacuum Throttling . . . . . . . . . . . . . . . . . . . . . . . . . 139 6.7 Monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 Monitoring Vacuum . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 Monitoring Autovacuum . . . . . . . . . . . . . . . . . . . . . . . . . 143\n\n7 Freezing\n\n145 7.1 Transaction ID Wraparound . . . . . . . . . . . . . . . . . . . . . . . 145 7.2 Tuple Freezing and Visibility Rules . . . . . . . . . . . . . . . . . . . 146 7.3 Managing Freezing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 Minimal Freezing Age . . . . . . . . . . . . . . . . . . . . . . . . . . 150 Age for Aggressive Freezing . . . . . . . . . . . . . . . . . . . . . . . 151 Age for Forced Autovacuum . . . . . . . . . . . . . . . . . . . . . . . 153 Age for Failsafe Freezing . . . . . . . . . . . . . . . . . . . . . . . . . 155 7.4 Manual Freezing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 Freezing by Vacuum . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 Freezing Data at the Initial Loading . . . . . . . . . . . . . . . . . . 156\n\n8 Rebuilding Tables and Indexes\n\n158 8.1 Full Vacuuming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 Why is Routine Vacuuming not Enough? . . . . . . . . . . . . . . . . 158 Estimating Data Density . . . . . . . . . . . . . . . . . . . . . . . . . 159\n\n7",
      "content_length": 2621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Table of Contents\n\nFreezing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 8.2 Other Rebuilding Methods . . . . . . . . . . . . . . . . . . . . . . . . 164 Alternatives to Full Vacuuming . . . . . . . . . . . . . . . . . . . . . 164 Reducing Downtime During Rebuilding . . . . . . . . . . . . . . . . 164 8.3 Precautions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 Read-Only Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 Data Updates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\n\nPart II Buffer Cache and WAL\n\n9 Buffer Cache\n\n171 9.1 Caching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171 9.2 Buffer Cache Design . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174 9.3 Cache Hits 9.4 Cache Misses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178 Buffer Search and Eviction . . . . . . . . . . . . . . . . . . . . . . . . 179 9.5 Bulk Eviction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 9.6 Choosing the Buffer Cache Size . . . . . . . . . . . . . . . . . . . . . 184 9.7 Cache Warming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187 9.8 Local Cache . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\n\n10 Write-Ahead Log\n\n191 10.1 Logging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 10.2 WAL Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192 Logical Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192 Physical Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 10.3 Checkpoint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 10.4 Recovery 10.5 Background Writing . . . . . . . . . . . . . . . . . . . . . . . . . . . 204 10.6 WAL Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 . . . . . . . . . . . . . . . . . . . . . . . . 205 Configuring Checkpoints Configuring Background Writing . . . . . . . . . . . . . . . . . . . . 208 Monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\n\n11 WAL Modes\n\n211 11.1 Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n\n8\n\n169",
      "content_length": 2388,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Table of Contents\n\n11.2 Fault Tolerance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 Caching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 Data Corruption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217 Non-Atomic Writes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219 11.3 WAL Levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221 Minimal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222 Replica . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 Logical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226\n\nPart III Locks\n\n227\n\n12 Relation-Level Locks\n\n229 12.1 About Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 12.2 Heavyweight Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231 12.3 Locks on Transaction IDs . . . . . . . . . . . . . . . . . . . . . . . . 233 12.4 Relation-Level Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . 234 12.5 Wait Queue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n\n13 Row-Level Locks\n\n241 13.1 Lock Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241 13.2 Row-Level Locking Modes . . . . . . . . . . . . . . . . . . . . . . . . 242 Exclusive Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242 Shared Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245 13.3 Multitransactions 13.4 Wait Queue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247 Exclusive Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247 Shared Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256 13.5 No-Wait Locks 13.6 Deadlocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258 Deadlocks by Row Updates . . . . . . . . . . . . . . . . . . . . . . . 260 Deadlocks Between Two UPDATE Statements . . . . . . . . . . . . . 261\n\n14 Miscellaneous Locks\n\n265 14.1 Non-Object Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265 14.2 Relation Extension Locks . . . . . . . . . . . . . . . . . . . . . . . . 267 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 14.3 Page Locks\n\n9",
      "content_length": 2399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Table of Contents\n\n14.4 Advisory Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268 14.5 Predicate Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270\n\n15 Locks on Memory Structures\n\n276 15.1 Spinlocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276 15.2 Lightweight Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277 15.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277 Buffer Cache . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277 WAL Buffers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279 15.4 Monitoring Waits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280 15.5 Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n\nPart IV Query Execution\n\n16 Query Execution Stages\n\n287 16.1 Demo Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287 16.2 Simple Query Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . 290 Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290 Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291 Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294 Execution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302 . . . . . . . . . . . . . . . . . . . . . . . . 304 Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304 Parameter Binding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305 Planning and Execution . . . . . . . . . . . . . . . . . . . . . . . . . 306 Getting the Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308\n\n16.3 Extended Query Protocol\n\n17 Statistics\n\n310 17.1 Basic Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310 17.2 NULL Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314 17.3 Distinct Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315 17.4 Most Common Values . . . . . . . . . . . . . . . . . . . . . . . . . . 317 17.5 Histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320 17.6 Statistics for Non-Scalar Data Types . . . . . . . . . . . . . . . . . . 324 17.7 Average Field Width . . . . . . . . . . . . . . . . . . . . . . . . . . . 325 17.8 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\n\n10\n\n285",
      "content_length": 2438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Table of Contents\n\n17.9 Expression Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . 326 Extended Expression Statistics . . . . . . . . . . . . . . . . . . . . . 327 Statistics for Expression Indexes . . . . . . . . . . . . . . . . . . . . 328 17.10 Multivariate Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . 329 Functional Dependencies Between Columns . . . . . . . . . . . . . . 329 Multivariate Number of Distinct Values . . . . . . . . . . . . . . . . 331 Multivariate MCV Lists . . . . . . . . . . . . . . . . . . . . . . . . . . 333\n\n18 Table Access Methods\n\n335 18.1 Pluggable Storage Engines . . . . . . . . . . . . . . . . . . . . . . . . 335 18.2 Sequential Scans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337 Cost Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338 18.3 Parallel Plans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342 . . . . . . . . . . . . . . . . . . . . . . . . 343 18.4 Parallel Sequential Scans Cost Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 . . . . . . . . . . . . . . . . . . . . . 347 Number of Background Workers . . . . . . . . . . . . . . . . . . . . 347 Non-Parallelizable Queries . . . . . . . . . . . . . . . . . . . . . . . 350 . . . . . . . . . . . . . . . . . . . . . . . 352 Parallel Restricted Queries\n\n18.5 Parallel Execution Limitations\n\n19 IndexAccess Methods\n\n356 19.1 Indexes and Extensibility . . . . . . . . . . . . . . . . . . . . . . . . 356 . . . . . . . . . . . . . . . . . . . . . 359 19.2 Operator Classes and Families Operator Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364 Operator Families 19.3 Indexing Engine Interface . . . . . . . . . . . . . . . . . . . . . . . . 366 Access Method Properties . . . . . . . . . . . . . . . . . . . . . . . . 367 Index-Level Properties . . . . . . . . . . . . . . . . . . . . . . . . . . 371 . . . . . . . . . . . . . . . . . . . . . . . . 372 Column-Level Properties\n\n20 Index Scans\n\n375 20.1 Regular Index Scans . . . . . . . . . . . . . . . . . . . . . . . . . . . 375 Cost Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376 Good Scenario: High Correlation . . . . . . . . . . . . . . . . . . . . 377 Bad Scenario: Low Correlation . . . . . . . . . . . . . . . . . . . . . 380 20.2 Index-Only Scans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383 Indexes with the Include Clause . . . . . . . . . . . . . . . . . . . . 386\n\n11",
      "content_length": 2572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Table of Contents\n\n20.3 Bitmap Scans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387 Bitmap Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389 . . . . . . . . . . . . . . . . . . . . . . . . . 390 Operations on Bitmaps Cost Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391 20.4 Parallel Index Scans . . . . . . . . . . . . . . . . . . . . . . . . . . . 395 . . . . . . . . . . . . . . . . 397 20.5 Comparison of Various Access Methods\n\n21 Nested Loop\n\n399 21.1 Join Types and Methods . . . . . . . . . . . . . . . . . . . . . . . . . 399 21.2 Nested Loop Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401 Cartesian Product Parameterized Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . 405 Caching Rows (Memoization) . . . . . . . . . . . . . . . . . . . . . . 409 Outer Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413 Anti- and Semi-joins . . . . . . . . . . . . . . . . . . . . . . . . . . . 414 Non-Equi-joins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417 Parallel Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417\n\n22 Hashing\n\n419 22.1 Hash Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419 One-Pass Hash Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . 419 Two-Pass Hash Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . 424 Dynamic Adjustments . . . . . . . . . . . . . . . . . . . . . . . . . . 427 Hash Joins in Parallel Plans . . . . . . . . . . . . . . . . . . . . . . . 431 Parallel One-Pass Hash Joins . . . . . . . . . . . . . . . . . . . . . . 432 Parallel Two-Pass Hash Joins . . . . . . . . . . . . . . . . . . . . . . 434 Modifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437 22.2 Distinct Values and Grouping . . . . . . . . . . . . . . . . . . . . . . 439\n\n23 Sorting and Merging\n\n442 23.1 Merge Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442 Merging Sorted Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 442 Parallel Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445 Modifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446 23.2 Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447 Quicksort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450 Top-N Heapsort\n\n12",
      "content_length": 2561,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Table of Contents\n\nExternal Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452 Incremental Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . 456 Parallel Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458 23.3 Distinct Values and Grouping . . . . . . . . . . . . . . . . . . . . . . 460 . . . . . . . . . . . . . . . . . . . . . . 462 23.4 Comparison of Join Methods\n\nPart V Types of Indexes\n\n467\n\n24 Hash\n\n469 24.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469 24.2 Page Layout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470 24.3 Operator Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476 24.4 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477 Access Method Properties . . . . . . . . . . . . . . . . . . . . . . . . 478 Index-Level Properties . . . . . . . . . . . . . . . . . . . . . . . . . . 478 . . . . . . . . . . . . . . . . . . . . . . . . 479 Column-Level Properties\n\n25 B-tree\n\n481 25.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481 25.2 Search and Insertions . . . . . . . . . . . . . . . . . . . . . . . . . . 482 Search by Equality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482 Search by Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . 484 Search by Range . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484 Insertions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485 25.3 Page Layout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486 Deduplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491 Compact Storage of Inner Index Entries . . . . . . . . . . . . . . . . 492 25.4 Operator Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493 Comparison Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . 493 Multicolumn Indexes and Sorting . . . . . . . . . . . . . . . . . . . . 499 25.5 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504 Access Method Properties . . . . . . . . . . . . . . . . . . . . . . . . 504 Index-Level Properties . . . . . . . . . . . . . . . . . . . . . . . . . . 504 . . . . . . . . . . . . . . . . . . . . . . . . 505 Column-Level Properties\n\n13",
      "content_length": 2331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Table of Contents\n\n26 GiST\n\n507 26.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509 26.2 R-Trees for Points Page Layout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511 Operator Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512 Search for Contained Elements . . . . . . . . . . . . . . . . . . . . . 514 Nearest Neighbor Search . . . . . . . . . . . . . . . . . . . . . . . . 517 Insertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521 Exclusion Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . 522 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525 26.3 RD-Trees for Full-Text Search . . . . . . . . . . . . . . . . . . . . . . 527 About Full-Text Search . . . . . . . . . . . . . . . . . . . . . . . . . . 527 Indexing tsvector Data . . . . . . . . . . . . . . . . . . . . . . . . . . 529 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536 26.4 Other Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537\n\n27 SP-GiST\n\n540 27.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540 27.2 Quadtrees for Points . . . . . . . . . . . . . . . . . . . . . . . . . . . 541 Operator Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543 Page Layout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547 Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548 Insertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552 27.3 K-Dimensional Trees for Points . . . . . . . . . . . . . . . . . . . . . 554 27.4 Radix Trees for Strings . . . . . . . . . . . . . . . . . . . . . . . . . . 556 Operator Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557 Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558 Insertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561 27.5 Other Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562\n\n28 GIN\n\n563 28.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563 28.2 Index for Full-Text Search . . . . . . . . . . . . . . . . . . . . . . . . 564 Page Layout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566 Operator Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568\n\n14",
      "content_length": 2657,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Table of Contents\n\nSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 569 Frequent and Rare Lexemes . . . . . . . . . . . . . . . . . . . . . . . 571 Insertions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574 Limiting Result Set Size . . . . . . . . . . . . . . . . . . . . . . . . . 576 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577 GIN Limitations and RUM Index . . . . . . . . . . . . . . . . . . . . 579 28.3 Trigrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580 28.4 Indexing Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 582 28.5 Indexing JSON . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585 jsonb_ops Operator Class . . . . . . . . . . . . . . . . . . . . . . . . 585 jsonb_path_ops Operator Class . . . . . . . . . . . . . . . . . . . . . 588 28.6 Indexing Other Data Types . . . . . . . . . . . . . . . . . . . . . . . 590\n\n29 BRIN\n\n591 29.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591 29.2 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592 29.3 Page Layout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594 29.4 Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596 29.5 Summary Information Updates . . . . . . . . . . . . . . . . . . . . . 596 Value Insertion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596 Range Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . 597 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598 Choosing Columns to be Indexed . . . . . . . . . . . . . . . . . . . . 600 Range Size and Search Efficiency . . . . . . . . . . . . . . . . . . . . 601 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604 29.7 Minmax-Multi Classes . . . . . . . . . . . . . . . . . . . . . . . . . . 607 29.8 Inclusion Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 610 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 613 29.9 Bloom Classes\n\n29.6 Minmax Classes\n\nConclusion\n\n618\n\nIndex\n\n619\n\n15",
      "content_length": 2166,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "About This Book\n\nBooks are not made to be believed, but to be subjected to inquiry.\n\n— Umberto Eco, The Name of the Rose\n\nFor Whom Is This Book?\n\nThis book is for those who will not settle for a black-box approach when work- ing with a database. If you are eager to learn, prefer not to take expert advice for granted, and would like to figure out everything yourself, follow along.\n\nI assume that the reader has already tried using Postgre��� and has at least some general understanding of how it works. Entry-level users may find the text a bit difficult. For example,I will not tell anything about how to install the server,enter psql commands, or set configuration parameters.\n\nI hope that the book will also be useful for those who are familiar with another database system, but switch over to Postgre��� and would like to understand how they differ. A book like this would have saved me a lot of time several years ago. And that’s exactly why I finally wrote it.\n\nWhat This Book Will Not Provide\n\nThis book is not a collection of recipes. You cannot find ready-made solutions for every occasion, but if you understand inner mechanisms of a complex system, you will be able to analyze and critically evaluate other people’s experience and come to your own conclusions. For this reason, I explain such details that may at first seem to be of no practical use.\n\nBut this book is not a tutorial either. While delving deeply into some fields (in which I am more interested myself), it may say nothing at all about the other.\n\n17",
      "content_length": 1529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "About This Book\n\nBy no means is this book a reference. I tried to be precise, but I did not aim at replacing documentation,so I could easily leave out some details that I considered insignificant. In any unclear situation read the documentation.\n\nThis book will not teach you how to develop the Postgre��� core. I do not expect any knowledge of the C language, as this book is mainly intended for database administrators and application developers. But I do provide multiple references to the source code, which can give you as many details as you like, and even more.\n\nWhat This Book Does Provide\n\nIn the introductory chapter, I briefly touch upon the main database concepts that will serve as the foundation for all the further narration. I do not expect you to get much new information from this chapter but still include it to complete the big picture. Besides, this overview can be found useful by those who are migrating from other database systems.\n\nPart I is devoted to questions of data consistency and isolation. I first cover them from the user’s perspective (you will learn which isolation levels are available and what are the implications) and then dwell on their internals. For this purpose, I have to explain implementation details of multiversion concurrency control and snapshot isolation, paying special attention to cleanup of outdated row versions.\n\nPart II describes buffer cache and ���, which is used to restore data consistency after a failure.\n\nPart III goes into details about the structure and usage of various types of locks: lightweight locks for ���, heavyweight locks for relations, and row-level locks.\n\nPartIVexplainshowtheserverplansandexecutes���queries. Iwilltellyouwhich data access methods are available, which join methods can be used, and how the collected statistics are applied.\n\nPart V extends the discussion of indexes from the already covered B-trees to other access methods. I will explain some general principles of extensibility that define the boundaries between the core of the indexing system, index access methods, and data types (which will bring us to the concept of operator classes), and then elaborate on each of the available methods.\n\n18",
      "content_length": 2197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Conventions\n\nPostgre��� includes multiple “introspective” extensions, which are not used in routine work, but give us an opportunity to peek into the server’s internal behav- ior. This book uses quite a few of them. Apart from letting us explore the server internals, these extensions can also facilitate troubleshooting in complex usage scenarios.\n\nConventions\n\nI tried to write this book in a way that would allow reading it page by page, from start to finish. But it is hardly possible to uncover all the truth at once, so I had to getbacktooneandthesametopicseveraltimes. Writingthat“itwillbeconsidered later” over and over again would inevitably make the text much longer, that’s why in such cases I simply put the page number in the margin to refer you to further discussion. A similar number pointing backwards will take you to the page where something has been already said on the subject.\n\nBoth the text and all the code examples in this book apply to Postgre��� ��. Next to some paragraphs, you can see a version number in the page margin. It means that the provided information is relevant starting from the indicated Postgre��� version, while all the previous versions either did not have the described feature at all, or used a different implementation. Such notes can be useful for those who have not upgraded their systems to the latest release yet.\n\nI also use the margins to show the default values of the discussed parameters. The names of both regular and storage parameters are printed in italics:\n\nwork_mem.\n\nIn footnotes, I provide multiple links to various sources of information. There are severalofthem,butfirstandforemost,IlistthePostgre���documentation,1 which isa wellspringof knowledge. Being an essential part ofthe project,it is always kept up-to-date by Postgre��� developers themselves. However, the primary reference is definitely the source code.2 It is amazing how many answers you can find by simply reading comments and browsing through ������ files, even if you do not know C. Sometimes I also refer to commitfest entries:3 you can always trace the\n\n1 postgresql.org/docs/14/index.html 2 git.postgresql.org/gitweb/?p=postgresql.git;a=summary 3 commitfest.postgresql.org\n\n19\n\np. ��\n\nv. ��\n\n4MB",
      "content_length": 2231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "About This Book\n\nhistory of all changes and understand the logic of decisions taken by developers if you read the related discussions in the psql-hackers mailing list, but it requires digging through piles of emails.\n\nSide notes that can lead the discussion astray (which I could not help but include into the book) are printed like this, so they can be easily skipped.\n\nNaturally, the book contains multiple code examples, mainly in ���. The code is provided with the prompt =>; the server response follows if necessary:\n\n=> SELECT now();\n\nnow −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 2023−03−06 14:00:08.008545+03\n\n(1 row)\n\nIf you carefully repeat all the provided commands in Postgre��� ��,you should get exactly the same results (down to transaction ��s and other inessential details). Anyway, all the code examples in this book have been generated by the script con- taining exactly these commands.\n\nWhen it is required to illustrate concurrent execution of several transactions, the code run in another session is indented and marked off by a vertical line.\n\n=> SHOW server_version;\n\nserver_version −−−−−−−−−−−−−−−−\n\n14.7 (1 row)\n\nTo try out such commands (which is useful for self-study, just like any experimen- tation), it is convenient to open two psql terminals.\n\nThe names of commands and various database objects (such as tables and columns, functions,or extensions) are highlighted in the text using a sans-serif font: ������, pg_class.\n\nIf a utility is called from the operating system, it is shown with a prompt that ends with $:\n\n20",
      "content_length": 1540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Acknowledgments\n\npostgres$ whoami\n\npostgres\n\nI use Linux, but without any technicalities; having some basic understanding of this operating system will be enough.\n\nAcknowledgments\n\nIt is impossible to write a book alone, and now I have an excellent opportunity to thank good people.\n\nI am very grateful to Pavel Luzanov who found the right moment and offered me to start doing something really worthwhile.\n\nI am obliged to Postgres Professional for the opportunity to work on this book be- yond my free time. But there are actual people behind the company,so I would like to express my gratitude to Oleg Bartunov for sharing ideas and infinite energy,and to Ivan Panchenko for thorough support and LATEX.\n\nI would like to thank my colleagues from the education team for the creative atmo- sphere and discussions that shaped the scope and format of our training courses, which also got reflected in the book. Special thanks to Pavel Tolmachev for his meticulous review of the drafts.\n\nMany chapters of this book were first published as articles in the Habr blog,1 and I am grateful to the readers for their comments and feedback. It showed the impor- tanceofthiswork,highlightedsomegapsinmyknowledge,andhelpedmeimprove the text.\n\nI would also like to thank Liudmila Mantrova who has put much effort into pol- ishing this book’s language. If you do not stumble over every other sentence, the credit goes to her. Besides, Liudmila took the trouble to translate this book into English, for which I am very grateful too.\n\n1 habr.com/en/company/postgrespro/blog\n\n21",
      "content_length": 1559,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "About This Book\n\nIdonotprovideanynames,buteachfunctionorfeaturementionedinthisbookhas required years of work done by particular people. I admire Postgre��� developers, and I am very glad to have the honor of calling many of them my colleagues.\n\n22",
      "content_length": 247,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "1\n\nIntroduction\n\n1.1 Data Organization\n\nDatabases\n\nPostgre��� is a program that belongs to the class of database management sys- tems. When this program is running, we call it a Postgre��� server, or instance.\n\nData managed by Postgre��� is stored in databases.1 A single Postgre��� instance can serve several databases at a time; together they are called a database cluster.\n\nTo be able to use the cluster,you must first initialize2 (create) it. The directory that contains all the files related to the cluster is usually called ������, after the name of the environment variable pointing to this directory.\n\nInstallations from pre-built packages can add their own “abstraction layers” over the reg- ular Postgre��� mechanism by explicitly setting all the parameters required by utilities. In this case, the database server runs as an operating system service, and you may never come across the ������ variable directly. But the term itself is well-established, so I am going to use it.\n\nAfter cluster initialization, ������ contains three identical databases:\n\ntemplate0 is used for cases like restoring data from a logical backup or creating a\n\ndatabase with a different encoding; it must never be modified.\n\ntemplate1 serves as a template for all the other databases that a user can create in\n\nthe cluster.\n\n1 postgresql.org/docs/14/managing-databases.html 2 postgresql.org/docs/14/app-initdb.html\n\n23",
      "content_length": 1405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Chapter 1 Introduction\n\npostgres is a regular database that you can use at your discretion.\n\nPostgreSQLinstance\n\nCREATE DATABASE\n\ndatabase cluster\n\npostgres\n\ntemplate0\n\ntemplate1\n\nnewdb\n\nSystem Catalog\n\nMetadata of all cluster objects (such as tables, indexes, data types, or functions) is stored in tables that belong to the system catalog.1 Each database has its own set of tables (and views) that describe the objects of this database. Several system catalog tables are common to the whole cluster; they do not belong to any partic- ular database (technically, a dummy database with a zero �� is used), but can be accessed from all of them.\n\nThesystemcatalogcanbeviewedusingregular���queries,whileallmodifications in it are performed by ��� commands. The psql client also offers a whole range of commands that display the contents of the system catalog.\n\nNames of all system catalog tables begin with pg_, like in pg_database. Column names start with a three-letter prefix that usually corresponds to the table name, like in datname.\n\nIn all system catalog tables, the column declared as the primary key is called oid (object identifier); its type, which is also called oid, is a ��-bit integer.\n\n1 postgresql.org/docs/14/catalogs.html\n\n24",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "1.1 Data Organization\n\nThe implementation of oid object identifiers is virtually the same as that of sequences,but it appeared in Postgre��� much earlier. What makes it special is that the generated unique ��s issued by a common counter are used in different tables of the system catalog. When an assigned �� exceeds the maximum value,the counter is reset. To ensure that all values in a particular table are unique,the next issued oid is checked by the unique index; if it is already used in this table, the counter is incremented, and the check is repeated.1\n\nSchemas\n\nSchemas2 are namespaces that store all objects of a database. Apart from user schemas, Postgre��� offers several predefined ones:\n\npublic is the default schema for user objects unless other settings are specified.\n\npg_catalog is used for system catalog tables.\n\ninformation_schema provides an alternative view for the system catalog as defined\n\nby the ��� standard.\n\npg_toast is used for objects related to ����� .\n\npg_temp comprises temporary tables. Although different users create temporary tables in different schemas called pg_temp_N, everyone refers to their objects using the pg_temp alias.\n\nEach schema is confined to a particular database, and all database objects belong to this or that schema.\n\nIf the schema is not specified explicitly when an object is accessed, Postgre��� se- lectsthefirstsuitableschemafromthesearchpath. Thesearchpathisbasedonthe value of the search_path parameter, which is implicitly extended with pg_catalog and (if necessary) pg_temp schemas. It means that different schemas can contain objects with the same names.\n\n1 backend/catalog/catalog.c, GetNewOidWithIndex function 2 postgresql.org/docs/14/ddl-schemas.html\n\n25\n\np. ��",
      "content_length": 1734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Chapter 1 Introduction\n\nTablespaces\n\nUnlike databases and schemas, which determine logical distribution of objects, tablespaces define physical data layout. A tablespace is virtually a directory in a file system. You can distribute your data between tablespaces in such a way that archive data is stored on slow disks, while the data that is being actively updated goes to fast disks.\n\nOneandthesametablespacecanbeusedbydifferentdatabases,andeachdatabase can store data in several tablespaces. It means that logical structure and physical data layout do not depend on each other.\n\nEach database has the so-called default tablespace. All database objects are cre- ated in this tablespace unless another location is specified. System catalog objects related to this database are also stored there.\n\npg_global\n\ncommon cluster objects\n\npg_default\n\nxyzzy\n\npg_catalog\n\npublic\n\nplugh\n\npg_catalog\n\npublic\n\npostgres\n\ntemplate1\n\n26",
      "content_length": 921,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "1.1 Data Organization\n\nDuring cluster initialization, two tablespaces are created:\n\npg_default is located in the ������/base directory; it is used as the default ta- blespace unless another tablespace is explicitly selected for this purpose.\n\npg_global is located in the ������/global directory; it stores system catalog objects\n\nthat are common to the whole cluster.\n\nWhen creating a custom tablespace,you can specify any directory; Postgre��� will create a symbolic link to this location in the ������/pg_tblspc directory. In fact, all paths used by Postgre��� are relative to the ������ directory, which allows you to move it to a different location (provided that you have stopped the server, of course).\n\nThe illustration on the previous page puts together databases, schemas, and ta- blespaces. Here the postgres database uses tablespace xyzzy as the default one, whereas the template1 database uses pg_default. Various database objects are shown at the intersections of tablespaces and schemas.\n\nRelations\n\nFor all of their differences, tables and indexes—the most important database objects—have one thing in common: they consist of rows. This point is quite self-evident when we think of tables, but it is equally true for �-tree nodes, which contain indexed values and references to other nodes or table rows.\n\nSome other objects also have the same structure; for example, sequences (virtual- ly one-row tables) and materialized views (which can be thought of as tables that “keep” the corresponding queries). Besides, there are regular views, which do not store any data but otherwise are very similar to tables.\n\nIn Postgre���, all these objects are referred to by the generic term relation.\n\nIn my opinion, it is not a happy term because it confuses database tables with “genuine” relations defined in the relational theory. Here we can feel the academic legacy of the project and the inclination of its founder,Michael Stonebraker,to see everything as a rela- tion. In one of his works,he even introduced the concept of an“ordered relation”to denote a table in which the order of rows is defined by an index.\n\n27",
      "content_length": 2126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "Chapter 1 Introduction\n\nThe system catalog table for relations was originally called pg_relation,but following the object orientation trend, it was soon renamed to pg_class, which we are now used to. Its columns still have the ��� prefix though.\n\nFiles and Forks\n\nAll information associated with a relation is stored in several different forks,1 each containing data of a particular type.\n\nAt first, a fork is represented by a single file. Its filename consists of a numeric �� (oid), which can be extended by a suffix that corresponds to the fork’s type.\n\nThe file grows over time, and when its size reaches � ��, another file of this fork is created (such files are sometimes called segments). The sequence number of the segment is added to the end of its filename.\n\nThefilesizelimitof���washistoricallyestablishedtosupportvariousfilesystems that could not handle large files. You can change this limit when building Post- gre��� (./configure --with-segsize).\n\nthe main fork\n\n12345.2\n\n12345.1\n\n12345\n\nfree space map\n\n12345_fsm.1\n\n12345_fsm\n\nvisibilitymap\n\n12345_vm\n\n1 postgresql.org/docs/14/storage-file-layout.html\n\n28",
      "content_length": 1121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "1.1 Data Organization\n\nThus, a single relation is represented on disk by several files. Even a small table without indexes will have at least three files, by the number of mandatory forks.\n\nEach tablespace directory (except for pg_global) contains separate subdirectories for particular databases. All files of the objects belonging to the same tablespace and database are located in the same subdirectory. You must take it into account because too many files in a single directory may not be handled well by file systems.\n\nThere are several standard types of forks.\n\nThe main fork represents actual data: table rows or index rows. This fork is avail-\n\nable for any relations (except for views, which contain no data).\n\nFiles of the main fork are named by their numeric ��s, which are stored as relfilenode values in the pg_class table.\n\nLet’s take a look at the path to a file that belongs to a table created in the pg_default tablespace:\n\n=> CREATE UNLOGGED TABLE t(\n\na integer, b numeric, c text, d json\n\n);\n\n=> INSERT INTO t VALUES (1, 2.0, 'foo', '{}');\n\n=> SELECT pg_relation_filepath('t');\n\npg_relation_filepath −−−−−−−−−−−−−−−−−−−−−−\n\nbase/16384/16385\n\n(1 row)\n\nThe base directory corresponds to the pg_default tablespace, the next sub- directory is used for the database, and it is here that we find the file we are looking for:\n\n=> SELECT oid FROM pg_database WHERE datname = 'internals';\n\noid −−−−−−− 16384 (1 row)\n\n29",
      "content_length": 1429,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "p. ���\n\nChapter 1 Introduction\n\n=> SELECT relfilenode FROM pg_class WHERE relname = 't';\n\nrelfilenode −−−−−−−−−−−−−\n\n16385\n\n(1 row)\n\nHere is the corresponding file in the file system:\n\n=> SELECT size FROM pg_stat_file('/usr/local/pgsql/data/base/16384/16385');\n\nsize −−−−−− 8192 (1 row)\n\nThe initialization fork1 is available only for unlogged tables (created with the ��- ������ clause) and their indexes. Such objects are the same as regular ones, except that any actions performed on them are not written into the write- ahead log. It makes these operations considerably faster, but you will not be able to restore consistent data in case of a failure. Therefore, Postgre��� sim- ply deletes all forks of such objects during recovery and overwrites the main fork with the initialization fork, thus creating a dummy file.\n\nThe t table is created as unlogged, so the initialization fork is present. It has the same name as the main fork, but with the _init suffix:\n\n=> SELECT size FROM pg_stat_file('/usr/local/pgsql/data/base/16384/16385_init');\n\nsize −−−−−−\n\n0\n\n(1 row)\n\nThe free space map2\n\nIts volume changes all the time, growing after vacuuming and getting smaller when new row versions appear. The free space map is used to quickly find a page that can accommodate new data being inserted.\n\nkeeps track of available space within pages.\n\n1 postgresql.org/docs/14/storage-init.html 2 postgresql.org/docs/14/storage-fsm.html backend/storage/freespace/README\n\n30",
      "content_length": 1466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "1.1 Data Organization\n\nAll files related to the free space map have the _fsm suffix. Initially, no such files are created; they appear only when necessary. The easiest way to get them is to vacuum a table\n\n:\n\n=> VACUUM t;\n\n=> SELECT size FROM pg_stat_file('/usr/local/pgsql/data/base/16384/16385_fsm');\n\nsize −−−−−−− 24576 (1 row)\n\nTo speed up search, the free space map is organized as a tree; it takes at least three pages (hence its file size for an almost empty table).\n\nThe free space map is provided for both tables and indexes. But since an index row cannot be added into an arbitrary page (for example, �-trees define the place of insertion by the sort order), Postgre��� tracks only those pages that have been fully emptied and can be reused in the index structure.\n\nThe visibility map1 can quickly show whether a page needs to be vacuumed or\n\nfrozen. For this purpose, it provides two bits for each table page.\n\nThe first bit is set for pages that contain only up-to-date row versions. Vac- uum skips such pages because there is nothing to clean up. Besides, when a transaction tries to read a row from such a page, there is no point in checking its visibility, so an index-only scan can be used.\n\nThe second bit the term freeze\n\nis set for pages that contain only frozen row versions. I will use map to refer to this part of the fork.\n\nVisibility map files have the _vm suffix. They are usually the smallest ones:\n\n=> SELECT size FROM pg_stat_file('/usr/local/pgsql/data/base/16384/16385_vm');\n\nsize −−−−−− 8192 (1 row)\n\nThe visibility map is provided for tables, but not for indexes.\n\n1 postgresql.org/docs/14/storage-vm.html\n\n31\n\np. ���\n\np. ���\n\np. ���\n\nv. �.� p. ���\n\np. ��",
      "content_length": 1687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "p. ��\n\np. ���\n\np. ���\n\nChapter 1 Introduction\n\nPages\n\n(or blocks), which represent To facilitate �/�, all files are logically split into pages the minimum amount of data that can be read or written. Consequently, many internal Postgre��� algorithms are tuned for page processing.\n\nThe page size is usually � k�. It can be configured to some extent (up to �� k�),but only at build time (./configure --with-blocksize), and nobody usually does it. Once built and launched, the instance can work only with pages of the same size; it is impossible to create tablespaces that support different page sizes.\n\nRegardless of the fork they belong to, all the files are handled by the server in roughly the same way. Pages are first moved to the buffer cache (where they can be read and updated by processes) and then flushed back to disk as required.\n\nTOAST\n\nEach row must fit a single page: there is no way to continue a row on the next page. To store long rows,Postgre��� uses a special mechanism called �����1 (The Oversized Attributes Storage Technique).\n\nT���� implies several strategies. You can move long attribute values into a sep- arate service table, having sliced them into smaller “toasts.” Another option is to compress a long value in such a way that the row fits the page. Or you can do both: first compress the value, and then slice and move it.\n\nIf the main table contains potentially long attributes, a separate ����� table is created for it right away, one for all the attributes. For example, if a table has a column of the numeric or text type,a ����� table will be created even if this column will never store any long values.\n\nFor indexes, the ����� mechanism can offer only compression; moving long at- tributes into a separate table is not supported. It limits the size of the keys that can be indexed (the actual implementation depends on a particular operator class\n\n1 postgresql.org/docs/14/storage-toast.html\n\ninclude/access/heaptoast.h\n\n32\n\n).",
      "content_length": 1963,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "1.1 Data Organization\n\nBy default, the ����� strategy is selected based on the data type of a column. The easiest way to review the used strategies is to run the \\d+ command in psql, but I will query the system catalog to get an uncluttered output:\n\n=> SELECT attname, atttypid::regtype,\n\nCASE attstorage\n\nWHEN 'p' THEN 'plain' WHEN 'e' THEN 'external' WHEN 'm' THEN 'main' WHEN 'x' THEN 'extended'\n\nEND AS storage FROM pg_attribute WHERE attrelid = 't'::regclass AND attnum > 0;\n\nattname | atttypid | storage −−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−\n\na b c d\n\n| integer | numeric | text | json\n\n| plain | main | extended | extended\n\n(4 rows)\n\nPostgre��� supports the following strategies:\n\nplain means that ����� is not used (this strategy is applied to data types that are\n\nknown to be “short,” such as the integer type).\n\nextended allowsbothcompressingattributesandstoringtheminaseparate�����\n\ntable.\n\nexternal implies that long attributes are stored in the ����� table in an uncom-\n\npressed state.\n\nmain requires long attributes to be compressed first; they will be moved to the\n\n����� table only if compression did not help.\n\nIn general terms, the algorithm looks as follows.1 Postgre��� aims at having at least four rows in a page. So if the size of the row exceeds one fourth of the page, excluding the header (for a standard-size page it is about ���� bytes), we must ap- ply the ����� mechanism to some of the values. Following the workflow described below, we stop as soon as the row length does not exceed the threshold anymore:\n\n1 backend/access/heap/heaptoast.c\n\n33",
      "content_length": 1569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "v. ��\n\nChapter 1 Introduction\n\n1. First of all, we go through attributes with external and extended strategies, starting from the longest ones. Extended attributes get compressed,and if the resulting value (on its own, without taking other attributes into account) ex- ceedsonefourthofthepage,itismovedtothe�����tablerightaway. External attributes are handled in the same way, except that the compression stage is skipped.\n\n2. If the row still does not fit the page after the first pass,we move the remaining attributes that use external or extended strategies into the ����� table,one by one.\n\n3. If it did not help either, we try to compress the attributes that use the main\n\nstrategy, keeping them in the table page.\n\n4. If the row is still not short enough, the main attributes are moved into the\n\n����� table.\n\nThe threshold value the toast_tuple_target storage parameter.\n\nis ���� bytes, but it can be redefined at the table level using\n\nIt may sometimes be useful to change the default strategy for some of the col- umns. If it is known in advance that the data in a particular column cannot be compressed (for example, the column stores ���� images), you can set the external strategy for this column; it allows you to avoid futile attempts to compress the data. The strategy can be changed as follows:\n\n=> ALTER TABLE t ALTER COLUMN d SET STORAGE external;\n\nIf we repeat the query, we will get the following result:\n\nattname | atttypid | storage −−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−\n\na b c d\n\n| integer | numeric | text | json\n\n| plain | main | extended | external\n\n(4 rows)\n\nT���� tables reside in a separate schema called pg_toast; it is not included into the search path, so ����� tables are usually hidden. For temporary tables, pg_toast_temp_N schemas are used, by analogy with pg_temp_N.\n\n34",
      "content_length": 1801,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "1.1 Data Organization\n\nLet’s take a look at the inner mechanics of the process. Suppose table t contains three potentially long attributes; it means that there must be a corresponding ����� table. Here it is:\n\n=> SELECT relnamespace::regnamespace, relname FROM pg_class WHERE oid = (\n\nSELECT reltoastrelid FROM pg_class WHERE relname = 't'\n\n);\n\nrelnamespace |\n\nrelname\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\npg_toast\n\n| pg_toast_16385\n\n(1 row)\n\n=> \\d+ pg_toast.pg_toast_16385\n\nTOAST table \"pg_toast.pg_toast_16385\"\n\nColumn\n\n|\n\nType\n\n| Storage\n\n−−−−−−−−−−−−+−−−−−−−−−+−−−−−−−−−\n\nchunk_id chunk_seq chunk_data | bytea\n\n| oid | plain | integer | plain | plain\n\nOwning table: \"public.t\" Indexes:\n\n\"pg_toast_16385_index\" PRIMARY KEY, btree (chunk_id, chunk_seq)\n\nAccess method: heap\n\nIt is only logical that the resulting chunks of the toasted row use the plain strategy: there is no second-level �����.\n\nApart from the ����� table itself, Postgre��� creates the corresponding index in the same schema. This index is always used to access ����� chunks. The name of the index is displayed in the output, but you can also view it by running the following query:\n\n=> SELECT indexrelid::regclass FROM pg_index WHERE indrelid = (\n\nSELECT oid FROM pg_class WHERE relname = 'pg_toast_16385'\n\n);\n\nindexrelid −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− pg_toast.pg_toast_16385_index\n\n(1 row)\n\n35",
      "content_length": 1364,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Chapter 1 Introduction\n\n=> \\d pg_toast.pg_toast_16385_index\n\nUnlogged index \"pg_toast.pg_toast_16385_index\"\n\nColumn\n\n|\n\nType\n\n| Key? | Definition\n\n−−−−−−−−−−−+−−−−−−−−−+−−−−−−+−−−−−−−−−−−−\n\nchunk_id | yes | oid chunk_seq | integer | yes\n\n| chunk_id | chunk_seq\n\nprimary key, btree, for table \"pg_toast.pg_toast_16385\"\n\nThus, a ����� table increases the minimum number of fork files used by the table up to eight: three for the main table, three for the ����� table, and two for the ����� index.\n\nColumn c uses the extended strategy, so its values will be compressed:\n\n=> UPDATE t SET c = repeat('A',5000);\n\n=> SELECT * FROM pg_toast.pg_toast_16385;\n\nchunk_id | chunk_seq | chunk_data −−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−− (0 rows)\n\nThe ����� table is empty: repeated symbols have been compressed by the �� al- gorithm, so the value fits the table page.\n\nAnd now let’s construct this value of random symbols:\n\n=> UPDATE t SET c = (\n\nSELECT string_agg( chr(trunc(65+random()*26)::integer), '') FROM generate_series(1,5000)\n\n) RETURNING left(c,10) || '...' || right(c,10);\n\n?column? −−−−−−−−−−−−−−−−−−−−−−−−− YEYNNDTSZR...JPKYUGMLDX\n\n(1 row) UPDATE 1\n\nThis sequence cannot be compressed, so it gets into the ����� table:\n\n=> SELECT chunk_id,\n\nchunk_seq, length(chunk_data), left(encode(chunk_data,'escape')::text, 10) || '...' || right(encode(chunk_data,'escape')::text, 10)\n\nFROM pg_toast.pg_toast_16385;\n\n36",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "1.2 Processes and Memory\n\nchunk_id | chunk_seq | length |\n\n?column?\n\n−−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−\n\n16390 | 16390 | 16390 |\n\n0 | 1 | 2 |\n\n1996 | YEYNNDTSZR...TXLNDZOXMY 1996 | EWEACUJGZD...GDBWMUWTJY 1008 | GSGDYSWTKF...JPKYUGMLDX\n\n(3 rows)\n\nWe can see that the characters are sliced into chunks. The chunk size is selected in suchawaythatthepageofthe�����tablecanaccommodatefourrows. Thisvalue varies a little from version to version depending on the size of the page header.\n\nWhen a long attribute is accessed, Postgre��� automatically restores the original value and returns it to the client; it all happens seamlessly for the application. If long attributes do not participate in the query, the ����� table will not be read at all. It is one of the reasons why you should avoid using the asterisk in production solutions.\n\nIf required chunks only, even if the value has been compressed.\n\nthe client queries one of the first chunks of a long value,Postgre��� will read the\n\nNevertheless, data compression and slicing require a lot of resources; the same goes for restoring the original values. That’s why it is not a good idea to keep bulky data in Postgre���, especially if this data is being actively used and does not require transactional logic (like scanned accounting documents). Apotentially better alternative is to store such data in the file system, keeping in the database only the names of the corresponding files. But then the database system cannot guarantee data consistency.\n\n1.2 Processes and Memory\n\nA Postgre��� server instance consists of several interacting processes.\n\nThe first process launched at the server start is postgres, which is traditionally called postmaster. It spawns all the other processes (Unix-like systems use the fork system call for this purpose) and supervises them: if any process fails, postmas- ter restarts it (or the whole server if there is a risk that the shared data has been damaged).\n\n37\n\nv. ��",
      "content_length": 1978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "p. ���\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\nChapter 1 Introduction\n\nBecause of its simplicity, the process model has been used in Postgre��� from the very beginning, and ever since there have been unending discussions about switching over to threads.\n\nThe current model has several drawbacks: static shared memory allocation does not allow resizing structures like buffer cache on the fly; parallel algorithms are hard to imple- ment and less efficient than they could be; sessions are tightly bound to processes. Using threads sounds promising, even though it involves some challenges related to isolation, OS compatibility, and resource management. However, their implementation would re- quire a radical code overhaul and years of work,so conservative views prevail for now: no such changes are expected in the near future.\n\nServer operation is maintained by background processes. Here are the main ones:\n\nstartup restores the system after a failure.\n\nautovacuum removes\n\nstale data from tables and indexes.\n\nwal writer writes ��� entries to disk .\n\ncheckpointer executes checkpoints\n\n.\n\nwriter flushes dirty pages to disk\n\n.\n\nstats collector collects usage statistics for the instance.\n\nwal sender sends ��� entries to a replica.\n\nwal receiver gets ��� entries on a replica.\n\nSome of these processes are terminated once the task is complete, others run in the background all the time, and some can be switched off.\n\nEach process is managed by configuration parameters, sometimes by dozens of them. To set up the server in a comprehensive manner, you have to be aware of its inner workings. But general considerations will only help you select more or less adequate initial values; later on, these settings have to be fine-tuned based on monitoring data.\n\nTo enable process interaction, postmaster allocates shared memory, which is avail- able to all the processes.\n\nSince disks (especially ���, but ��� too) are much slower than ���, Postgre��� some part of the shared ��� is reserved for recently read pages, in uses caching: hope that they will be needed more than once and the overhead of repeated disk\n\n38",
      "content_length": 2109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "1.3 Clients and the Client-Server Protocol\n\naccess will be reduced. Modified data is also flushed to disk after some delay, not immediately.\n\nBuffercachetakesthegreaterpartofthesharedmemory,whichalsocontainsother buffers used by the server to speed up disk access.\n\nThe operating system has its own cache too. Postgre��� (almost) never bypasses the operating system mechanisms to use direct �/�, so it results in double caching.\n\npostmaster\n\nPostgreSQL instance\n\nclient client client application application application\n\nbackend backend backend\n\nbackground processes\n\nshared memory\n\nbuffer cache\n\ncache\n\noperating system\n\nIn case of a failure (such as a power outage or an operating system crash), the data kept in ��� is lost, including that of the buffer cache. The files that remain on disk have their pages written at different points in time. To be able to restore data consistency, Postgre��� maintains the write-ahead log (���) during its operation, which makes it possible to repeat lost operations when necessary.\n\n1.3 Clients and the Client-Server Protocol\n\nAnother task of the postmaster process is to listen for incoming connections. Once a new client appears, postmaster spawns a separate backend process.1 The client\n\n1 backend/tcop/postgres.c, PostgresMain function\n\n39\n\np. ���",
      "content_length": 1292,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "p. ��� p. ���\n\np. ��\n\nChapter 1 Introduction\n\nestablishes a connection and starts a session with this backend. The session con- tinues until the client disconnects or the connection is lost.\n\nThe server has to spawn a separate backend for each client. If many clients are trying to connect, it can turn out to be a problem.\n\nEach process needs ��� to cache catalog tables, prepared statements , inter- , and other data. The more connections are open, the\n\nmediate query results more memory is required.\n\nIf connections are short and frequent (a client performs a small query and disconnects), the cost of establishing a connection, spawning a new process, and performing pointless local caching is unreasonably high.\n\nThemoreprocessesarestarted,themoretimeisrequiredtoscantheirlist,and As a result, performance may decline this operation is performed very often. as the number of clients grows.\n\nThis problem can be resolved by connection pooling, which limits the number of spawned backends. Postgre��� has no such built-in functionality, so we have to rely on third-party solutions: pooling managers integrated into the application server or external tools (such as PgBouncer1 or Odyssey2). This approach usually means that each server backend can execute transactions of different clients, one after another. It imposes some restrictions on application development since it is only allowed to use resources that are local to a transaction, not to the whole session.\n\nTo understand each other, a client and a server must use one and the same inter- facing protocol.3 It is usually based on the standard libpq library,but there are also other custom implementations.\n\nSpeaking in the most general terms, the protocol allows clients to connect to the server and execute ��� queries.\n\nA connection is always established to a particular database on behalf of a particu- lar role, or user. Although the server supports a database cluster, it is required to establish a separate connection to each database that you would like to use in your\n\n1 pgbouncer.org 2 github.com/yandex/odyssey 3 postgresql.org/docs/14/protocol.html\n\n40",
      "content_length": 2125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "1.3 Clients and the Client-Server Protocol\n\napplication. Atthispoint,authenticationisperformed: thebackendprocessverifies the user’s identity (for example, by asking for the password) and checks whether this user has the right to connect to the server and to the specified database.\n\nS�� queries are passed to the backend process as text strings. The process parses the text, optimizes the query, executes it, and returns the result to the client.\n\n41",
      "content_length": 451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Part I\n\nIsolation and MVCC",
      "content_length": 26,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "2\n\nIsolation\n\n2.1 Consistency\n\nThe key feature of relational databases is their ability to ensure data consistency, that is, data correctness.\n\nIt is a known fact that at the database level it is possible to create integrity con- straints, such as ��� ���� or ������. The database system ensures that these con- straints are never broken, so data integrity is never compromised.\n\nIf all the required constraints could be formulated at the database level, consis- tency would be guaranteed. But some conditions are too complex for that, for example, they touch upon several tables at once. And even if a constraint can be defined in the database, but for some reason it is not, it does not mean that this constraint may be violated.\n\nThus, data consistency is stricter than integrity, but the database system has no idea what “consistency” actually means. If an application breaks it without break- ing the integrity,there is no way for the database system to find out. Consequently, it is the application that must lay down the criteria for data consistency, and we have to believe that it is written correctly and will never have any errors.\n\nBut if the application always executes only correct sequences of operators, where does the database system come into play?\n\nFirst of all,a correct sequence of operators can temporarily break data consistency, and—strange as it may seem—it is perfectly normal.\n\nAhackneyedbutclearexampleisatransferoffundsfromoneaccounttoanother. A consistency rule may sound as follows: a money transfer must never change the total\n\n45",
      "content_length": 1562,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Chapter 2 Isolation\n\nbalance of the affected accounts. It is quite difficult (although possible) to formulate this rule as an integrity constraint in ���, so let’s assume that it is defined at the applicationlevelandremainsopaquetothedatabasesystem. Atransferconsistsof two operations: the first one draws some money from one of the accounts,whereas the second one adds this sum to another account. The first operation breaks data consistency, whereas the second one restores it.\n\nIf the first operation succeeds, but the second one does not (because of some fail- ure),data consistency will be broken. Such situations are unacceptable,but it takes a great deal of effort to detect and address them at the application level. Luckily it is not required—the problem can be completely solved by the database system itself if it knows that these two operations constitute an indivisible whole, that is, a transaction.\n\nBut there is also a more subtle aspect here. Being absolutely correct on their own, transactions can start operating incorrectly when run in parallel. That’s because operations belonging to different transactions often get intermixed. There would be no such issues if the database system first completed all operations of one trans- actionandthenmovedontothenextone,butperformanceofsequentialexecution would be implausibly low.\n\nA truly simultaneous execution of transactions can only be achieved on systems with suit- able hardware: a multi-core processor, a disk array, and so on. But the same reasoning is also true for a server that executes commands sequentially in the time-sharing mode. For generalization purposes,both these situations are sometimes referred to as concurrent execution.\n\nCorrect transactions that behave incorrectly when run together result in concur- rency anomalies, or phenomena.\n\nHere is a simple example. To get consistent data from the database, the applica- tion must not see any changes made by other uncommitted transactions, at the very minimum. Otherwise (if some transactions are rolled back), it would see the database state that has never existed. Such an anomaly is called a dirty read. There are also many other anomalies, which are more complex.\n\nWhen running transactions concurrently, the database must guarantee that the result of such execution will be the same as the outcome of one of the possible se-\n\n46",
      "content_length": 2368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "2.2 Isolation Levels and Anomalies in SQL Standard\n\nquential executions. In other words, it must isolate transactions from one another, thus taking care of any possible anomalies.\n\nTo sum it up, a transaction is a set of operations that takes the database from one correct state to another correct state (consistency), provided that it is executed in full (atomicity) and without being affected by other transactions (isolation). This definition combines the requirements implied by the first three letters of the ���� acronym. They are so intertwined that it makes sense to discuss them together. In fact, the durability requirement is hardly possible to split off either: after a crash, the system may still contain some changes made by uncommitted transactions, and you have to do something about it to restore data consistency.\n\nThus, the database system helps the application maintain data consistency by tak- ing transaction boundaries into account, even though it has no idea about the im- plied consistency rules.\n\nUnfortunately, full isolation is hard to implement and can negatively affect per- formance. Most real-life systems use weaker isolation levels, which prevent some anomalies, but not all of them. It means that the job of maintaining data consis- tency partially falls on the application. And that’s exactly why it is very important to understand which isolation level is used in the system, what is guaranteed at this level and what is not, and how to ensure that your code will be correct in such conditions.\n\n2.2 Isolation Levels and Anomalies in SQL Standard\n\nThe���standardspecifiesfourisolationlevels.1 Theselevelsaredefinedbythelist of anomalies that may or may not occur during concurrent transaction execution. So when talking about isolation levels, we have to start with anomalies.\n\nWe should bear in mind that the standard is a theoretical construct: it affects the practice, but the practice still diverges from it in lots of ways. That’s why all ex-\n\n1 postgresql.org/docs/14/transaction-iso.html\n\n47\n\np. ���",
      "content_length": 2043,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "Chapter 2 Isolation\n\namples here are rather hypothetical. Dealing with transactions on bank accounts, these examples are quite self-explanatory,but I have to admit that they have noth- ing to do with real banking operations.\n\nIt is interesting that the actual database theory also diverges from the standard: it was developed after the standard had been adopted, and the practice was already well ahead.\n\nLost Update\n\nThe lost update anomaly occurs when two transactions read one and the same table row,thenoneofthetransactionsupdatesthisrow,andfinallytheothertransaction updates the same row without taking into account any changes made by the first transaction.\n\nSuppose that two transactions are going to increase the balance of one and the same account by $���. The first transaction reads the current value ($�,���), then the second transaction reads the same value. The first transaction increases the balance (making it $�,���) and writes the new value into the database. The second transaction does the same: it gets $�,��� after increasing the balance and writes this value. As a result, the customer loses $���.\n\nLost updates are forbidden by the standard at all isolation levels.\n\nDirty Reads and Read Uncommitted\n\nThe dirty read anomaly occurs when a transaction reads uncommitted changes made by another transaction.\n\nFor example, the first transaction transfers $��� to an empty account but does not commit this change. Another transaction reads the account state (which has been updated but not committed) and allows the customer to withdraw the money— even though the first transaction gets interrupted and its changes are rolled back, so the account is empty.\n\nThe standard allows dirty reads at the Read Uncommitted level.\n\n48",
      "content_length": 1744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "2.2 Isolation Levels and Anomalies in SQL Standard\n\nNon-Repeatable Reads and Read Committed\n\nThe non-repeatable read anomaly occurs when a transaction reads one and the samerowtwice,whereasanothertransactionupdates(ordeletes)thisrowbetween these reads and commitsthe change. As a result,the first transaction gets different results.\n\nFor example, suppose there is a consistency rule that forbids having a negative bal- ance in bank accounts. The first transaction is going to reduce the account balance by $���. It checks the current value, gets $�,���, and decides that this operation is possible. At the same time, another transaction withdraws all the money from this account and commits the changes. If the first transaction checked the bal- ance again at this point, it would get $� (but the decision to withdraw the money is already taken, and this operation causes an overdraft).\n\nThe standard allows non-repeatable reads at the Read Uncommitted and Read Com- mitted levels.\n\nPhantom Reads and Repeatable Read\n\nThephantomread anomalyoccurswhenoneandthesametransactionexecutestwo identical queries returning a set of rows that satisfy a particular condition, while another transaction adds some other rows satisfying this condition and commits the changes in the time interval between these queries. As a result, the first trans- action gets two different sets of rows.\n\nFor example, suppose there is a consistency rule that forbids a customer to have more than three accounts. The first transaction is going to open a new account, so it checks how many accounts are currently available (let’s say there are two of them) and decides that this operation is possible. At this very moment,the second transaction also opens a new account for this client and commits the changes. If the first transaction double-checked the number of open accounts, it would get three (but it is already opening another account, and the client ends up having four of them).\n\nThe standard allows phantom reads at the Read Uncommitted,Read Committed,and Repeatable Read isolation levels.\n\n49",
      "content_length": 2073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Chapter 2 Isolation\n\nNo Anomalies and Serializable\n\nThe standard also defines the Serializable level, which does not allow any anoma- lies. It is not the same as the ban on lost updates and dirty, non-repeatable, and phantom reads. In fact, there is a much higher number of known anomalies than the standard specifies, and an unknown number of still unknown ones.\n\nThe Serializable level must prevent any anomalies. It means that the application developer does not have to take isolation into account. If transactions execute correct operator sequences when run on their own, concurrent execution cannot break data consistency either.\n\nTo illustrate this idea, I will use a well-known table provided in the standard; the last column is added here for clarity:\n\nlost update\n\ndirty read\n\nnon-repeatable read\n\nphantom read\n\nother anomalies\n\nRead Uncommitted Read Committed Repeatable Read Serializable\n\n— — — —\n\nyes — — —\n\nyes yes — —\n\nyes yes yes —\n\nyes yes yes —\n\nWhy These Anomalies?\n\nOf all the possible anomalies,why does the standard mentions only some,and why exactly these ones?\n\nNo one seems to know it for sure. But it is not unlikely that other anomalies were simply not considered when the first versions of the standard were adopted, as theory was far behind practice at that time.\n\nBesides, it was assumed that isolation had to be based on locks. The widely used two-phase locking protocol (���) requires transactions to lock the affected rows dur- ing execution and release the locks upon completion. In simplistic terms,the more locks a transaction acquires, the better it is isolated from other transactions. And consequently, the worse is the system performance, as transactions start queuing to get access to the same rows instead of running concurrently.\n\n50",
      "content_length": 1775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "2.3 Isolation Levels in PostgreSQL\n\nI believe that to a great extent the difference between the standard isolation levels is defined by the number of locks required for their implementation.\n\nIf the rows to be updated are locked for writes but not for reads, we get the Read Uncommitted isolation level, which allows reading data before it is committed.\n\nIf the rows to be updated are locked for both reads and writes, we get the Read Committed level: it is forbidden to read uncommitted data, but a query can return different values if it is run more than once (non-repeatable reads).\n\nLocking the rows to be read and to be updated for all operations gives us the Re- peatable Read level: a repeated query will return the same result.\n\nHowever, the Serializable level poses a problem: it is impossible to lock a row that does not exist yet. It leaves an opportunity for phantom reads to occur: a transac- tion can add a row that satisfies the condition of the previous query, and this row will appear in the next query result.\n\nThus,regular locks cannot provide full isolation: to achieve it,we have to lock con- ditions (predicates) rather than rows. Such predicate locks were introduced as early as ���� when System R was being developed; however,their practical applicability islimitedtosimpleconditionsforwhichitisclearwhethertwodifferentpredicates have never may conflict. As far as I know, predicate locks in their intended form been implemented in any system.\n\n2.3 Isolation Levels in PostgreSQL\n\nOver time,lock-based protocols for transaction management got replaced with the Snapshot Isolation (��) protocol. The idea behind this approach is that each trans- action accesses a consistent snapshot of data as it appeared at a particular point in time. The snapshot includes all the current changes committed before the snap- shot was taken.\n\nSnapshot isolation minimizes the number of required locks. In fact, a row will be locked only by concurrent update attempts. In all other cases, operations can be executed concurrently: writes never lock reads, and reads never lock anything.\n\n51\n\np. ���\n\np. ���",
      "content_length": 2112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "p. ���\n\np. ��\n\nChapter 2 Isolation\n\nPostgre��� uses a multiversion flavor of the �� protocol. Multiversion concurrency control implies that at any moment the database system can contain several ver- sions of one and the same row, so Postgre��� can include an appropriate version into the snapshot rather than abort transactions that attempt to read stale data.\n\nBased on snapshots, Postgre��� isolation differs from the requirements specified in the standard—in fact, it is even stricter. Dirty reads are forbidden by design. Technically,youcanspecifytheRead Uncommittedlevel,butitsbehaviorwillbethe same as that of Read Committed, so I am not going to mention this level anymore. Repeatable Read allows neither non-repeatable nor phantom reads (even though it does not guarantee full isolation). But in some cases, there is a risk of losing changes at the Read Committed level.\n\nlost updates\n\ndirty reads\n\nnon-repeatable reads\n\nphantom reads\n\nother anomalies\n\nRead Committed Repeatable Read Serializable\n\nyes — —\n\n— — —\n\nyes — —\n\nyes — —\n\nyes yes —\n\nBefore exploring the internal mechanisms of isolation, three isolation levels from the user’s perspective.\n\nlet’s discuss each of the\n\nFor this purpose,we are going to create the accounts table; Alice and Bob will have $�,��� each, but Bob will have two accounts:\n\n=> CREATE TABLE accounts(\n\nid integer PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY, client text, amount numeric\n\n);\n\n=> INSERT INTO accounts VALUES\n\n(1, 'alice', 1000.00), (2, 'bob', 100.00), (3, 'bob', 900.00);\n\nRead Committed\n\nNo dirtyreads. a transaction. By default, it uses the Read Committed1 isolation level:\n\nIt is easy to check that reading dirty data is not allowed. Let’s start\n\n1 postgresql.org/docs/14/transaction-iso.html#XACT-READ-COMMITTED\n\n52",
      "content_length": 1775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "2.3 Isolation Levels in PostgreSQL\n\n=> BEGIN;\n\n=> SHOW transaction_isolation;\n\ntransaction_isolation −−−−−−−−−−−−−−−−−−−−−−−\n\nread committed\n\n(1 row)\n\nTo be more exact, the default level is set by the following parameter, which can be changed as required:\n\n=> SHOW default_transaction_isolation;\n\ndefault_transaction_isolation −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nread committed\n\n(1 row)\n\nTheopenedtransactionwithdrawssomefundsfromthecustomeraccountbutdoes not commit these changes yet. It will see its own changes though, as it is always allowed:\n\n=> UPDATE accounts SET amount = amount - 200 WHERE id = 1;\n\n=> SELECT * FROM accounts WHERE client = 'alice';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n1 | alice\n\n| 800.00\n\n(1 row)\n\nIn the second session, we start another transaction that will also run at the Read Committed level:\n\n=> BEGIN;\n\n=> SELECT * FROM accounts WHERE client = 'alice';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−−\n\n1 | alice\n\n| 1000.00\n\n(1 row)\n\nPredictably, the second transaction does not see any uncommitted changes—dirty reads are forbidden.\n\n53",
      "content_length": 1074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Chapter 2 Isolation\n\nNon-repeatablereads. Nowletthefirsttransactioncommitthechanges. Thenthe second transaction will repeat the same query:\n\n=> COMMIT;\n\n=> SELECT * FROM accounts WHERE client = 'alice';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n1 | alice\n\n| 800.00\n\n(1 row)\n\n=> COMMIT;\n\nThe query receives an updated version of the data—and it is exactly what is under- stood by the non-repeatable read anomaly, which is allowed at the Read Committed level.\n\nA practical insight: in a transaction, you must not take any decisions based on the data read by the previous operator, as everything can change in between. Here is an example whose variations appear in the application code so often that it can be considered a classic anti-pattern:\n\nIF (SELECT amount FROM accounts WHERE id = 1) >= 1000 THEN\n\nUPDATE accounts SET amount = amount - 1000 WHERE id = 1;\n\nEND IF;\n\nDuring the time that passes between the check and the update, other transactions can freely change the state of the account, so such a “check” is absolutely useless. For better understanding,you can imagine that random operators of other transac- tions are“wedged”between the operators of the current transaction. For example, like this:\n\nIF (SELECT amount FROM accounts WHERE id = 1) >= 1000 THEN\n\nUPDATE accounts SET amount = amount - 200 WHERE id = 1;\n\nCOMMIT;\n\nUPDATE accounts SET amount = amount - 1000 WHERE id = 1;\n\nEND IF;\n\n54",
      "content_length": 1408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "2.3 Isolation Levels in PostgreSQL\n\nIf everything goes wrong as soon as the operators are rearranged, then the code is incorrect. Do not delude yourself that you will never get into this trouble: any- thing that can go wrong will go wrong. Such errors are very hard to reproduce,and consequently, fixing them is a real challenge.\n\nHow can you correct this code? There are several options:\n\nReplace procedural code with declarative one.\n\nFor example, in this particular case it is easy to turn an �� statement into a ����� constraint:\n\nALTER TABLE accounts\n\nADD CHECK amount >= 0;\n\nNow you do not need any checks in the code: it is enough to simply run the command and handle the exception that will be raised if an integrity con- straint violation is attempted.\n\nUse a single ��� operator.\n\nData consistency can be compromised if a transaction gets committed within the time gap between operators of another transaction, thus changing data visibility. If there is only one operator, there are no such gaps.\n\nPostgre��� has enough capabilities to solve complex tasks with a single ��� statement. In particular, it offers common table expressions (���) that can contain operators like ������, ������, ������, as well as the ������ �� �������� operator that implements the following logic: insert the row if it does not exist, otherwise perform an update.\n\nApply explicit locks.\n\nThe last resort is to manually set an exclusive lock on all the required rows (������ ��� ������) or even on the whole table (���� �����) . This approach always works, but it nullifies all the advantages of ����: some operations that could be executed concurrently will run sequentially.\n\n55\n\np. ��� p. ���",
      "content_length": 1683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Chapter 2 Isolation\n\nRead skew. However, it is not all that simple. The Postgre��� implementation allows other, less known anomalies, which are not regulated by the standard.\n\nSuppose the first transaction has started a money transfer between Bob’s accounts:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount - 100 WHERE id = 2;\n\nMeanwhile, the other transaction starts looping through all Bob’s accounts to cal- culate their total balance. It begins with the first account (seeing its previous state, of course):\n\n=> BEGIN;\n\n=> SELECT amount FROM accounts WHERE id = 2;\n\namount −−−−−−−− 100.00 (1 row)\n\nAt this moment, the first transaction completes successfully:\n\n=> UPDATE accounts SET amount = amount + 100 WHERE id = 3;\n\n=> COMMIT;\n\nThe second transaction reads the state of the second account (and sees the already updated value):\n\n=> SELECT amount FROM accounts WHERE id = 3;\n\namount −−−−−−−−− 1000.00\n\n(1 row)\n\n=> COMMIT;\n\nAs a result, the second transaction gets $�,��� because it has read incorrect data. Such an anomaly is called read skew.\n\nHow can you avoid this anomaly at the Read Committed level? The answer is obvi- ous: use a single operator. For example, like this:\n\nSELECT sum(amount) FROM accounts WHERE client = 'bob';\n\n56",
      "content_length": 1246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "2.3 Isolation Levels in PostgreSQL\n\nI have been stating so far that data visibility can change only between operators, but is it really so? What if the query is running for a long time? Can it see different parts of data in different states in this case?\n\nLet’s check it out. A convenient way to do it is to add a delay to an operator by callingthepg_sleepfunction. Thenthefirstrowwillbereadatonce,butthesecond row will have to wait for two seconds:\n\n=> SELECT amount, pg_sleep(2) -- two seconds FROM accounts WHERE client = 'bob';\n\nWhile this statement is being executed, let’s start another transaction to transfer the money back:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount + 100 WHERE id = 2;\n\n=> UPDATE accounts SET amount = amount - 100 WHERE id = 3;\n\n=> COMMIT;\n\nThe result shows that the operator has seen all the data in the state that corre- sponds to the beginning of its execution, which is certainly correct:\n\namount\n\n| pg_sleep\n\n−−−−−−−−−+−−−−−−−−−−\n\n0.00 | 1000.00 |\n\n(2 rows)\n\nBut it is not all that simple either. If the query contains a function that is de- clared ��������, and this function executes another query, then the data seen by this nested query will not be consistent with the result of the main query.\n\nLet’s check the balance in Bob’s accounts using the following function:\n\n=> CREATE FUNCTION get_amount(id integer) RETURNS numeric AS $$\n\nSELECT amount FROM accounts a WHERE a.id = get_amount.id;\n\n$$ VOLATILE LANGUAGE sql;\n\n=> SELECT get_amount(id), pg_sleep(2) FROM accounts WHERE client = 'bob';\n\nWe will transfer the money between the accounts once again while our delayed query is being executed:\n\n57",
      "content_length": 1645,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Chapter 2 Isolation\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount + 100 WHERE id = 2;\n\n=> UPDATE accounts SET amount = amount - 100 WHERE id = 3;\n\n=> COMMIT;\n\nIn this case, we are going to get inconsistent data—$��� has been lost:\n\nget_amount | pg_sleep −−−−−−−−−−−−+−−−−−−−−−−\n\n100.00 | 800.00 |\n\n(2 rows)\n\nI would like to emphasize that this effect is possible only at the Read Committed isolation level, and only if the function is ��������. The trouble is that Postgre��� uses exactly this isolation level and this volatility category by default. So we have to admit that the trap is set in a very cunning way.\n\nRead skew instead of lost updates. The read skew anomaly can also occur within a single operator during an update—even though in a somewhat unexpected way.\n\nLet’s see what happens if two transactions try to modify one and the same row. Bob currently has a total of $�,��� in two accounts:\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n2 | bob 3 | bob\n\n| 200.00 | 800.00\n\n(2 rows)\n\nStart a transaction that will reduce Bob’s balance:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount - 100 WHERE id = 3;\n\nAt the same time, the other transaction will be calculating the interest for all cus- tomer accounts with the total balance of $�,��� or more:\n\n58",
      "content_length": 1321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "2.3 Isolation Levels in PostgreSQL\n\n=> UPDATE accounts SET amount = amount * 1.01 WHERE client IN ( SELECT client FROM accounts GROUP BY client HAVING sum(amount) >= 1000\n\n);\n\nThe ������ operator execution virtually consists of two stages. First,the rows to be updated are selected based on the provided condition. Since the first transaction is not committed yet, the second transaction cannot see its result, so the selection of rows picked for interest accrual is not affected. Thus, Bob’s accounts satisfy the condition, and his balance must be increased by $�� once the ������ operation completes.\n\nAt the second stage, the selected rows are updated one by one. The second trans- action has to wait because the row with id = 3 is locked: it is being updated by the first transaction.\n\nMeanwhile, the first transaction commits its changes:\n\n=> COMMIT;\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client |\n\namount\n\n−−−−+−−−−−−−−+−−−−−−−−−−\n\n2 | bob 3 | bob\n\n| 202.0000 | 707.0000\n\n(2 rows)\n\nOn the one hand,the ������ command must not see any changes made by the first transaction. But on the other hand, it must not lose any committed changes.\n\nOnce the lock is released, the ������ operator re-reads the row to be updated (but only this row!). As a result, Bob gets $� of interest, based on the total of $���. But if he had $���, his accounts should not have been included into the query results in the first place.\n\nThus, our transaction has returned incorrect data: different rows have been read from different snapshots. Instead of a lost update, we observe the read skew anomaly again.\n\n59\n\np. ���",
      "content_length": 1617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "p. ��\n\nChapter 2 Isolation\n\nLost updates. However,the trick of re-reading the locked row will not help against lost updates if the data is modified by different ��� operators.\n\nHere is an example that we have already seen. (outside of the database) the current balance of Alice’s account:\n\nThe application reads and registers\n\n=> BEGIN;\n\n=> SELECT amount FROM accounts WHERE id = 1;\n\namount −−−−−−−− 800.00 (1 row)\n\nMeanwhile, the other transaction does the same:\n\n=> BEGIN;\n\n=> SELECT amount FROM accounts WHERE id = 1;\n\namount −−−−−−−− 800.00 (1 row)\n\nThefirsttransactionincreasesthepreviouslyregisteredvalueby$���andcommits this change:\n\n=> UPDATE accounts SET amount = 800.00 + 100 WHERE id = 1 RETURNING amount;\n\namount −−−−−−−− 900.00 (1 row) UPDATE 1\n\n=> COMMIT;\n\nThe second transaction does the same:\n\n=> UPDATE accounts SET amount = 800.00 + 100 WHERE id = 1 RETURNING amount;\n\namount −−−−−−−− 900.00 (1 row) UPDATE 1\n\n60",
      "content_length": 930,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "2.3 Isolation Levels in PostgreSQL\n\n=> COMMIT;\n\nUnfortunately, Alice has lost $���. The database system does not know that the registered value of $��� is somehow related to accounts.amount, so it cannot pre- vent the lost update anomaly. At the Read Committed isolation level, this code is incorrect.\n\nRepeatable Read\n\nNo non-repeatable and phantom reads. As its name suggests,the Repeatable Read1 isolation level must guarantee repeatable reading. Let’s check it and make sure that phantom reads cannot occur either. For this purpose, we are going to start a transaction that will revert Bob’s accounts to their previous state and create a new account for Charlie:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = 200.00 WHERE id = 2;\n\n=> UPDATE accounts SET amount = 800.00 WHERE id = 3;\n\n=> INSERT INTO accounts VALUES\n\n(4, 'charlie', 100.00);\n\n=> SELECT * FROM accounts ORDER BY id;\n\nid | client\n\n| amount\n\n−−−−+−−−−−−−−−+−−−−−−−−\n\n| 900.00 1 | alice | 200.00 2 | bob 3 | bob | 800.00 4 | charlie | 100.00\n\n(4 rows)\n\nInthesecondsession,let’sstartanothertransaction,withtheRepeatableReadlevel explicitly specified in the ����� command (the level of the first transaction is not important):\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT * FROM accounts ORDER BY id;\n\n1 postgresql.org/docs/14/transaction-iso.html#XACT-REPEATABLE-READ\n\n61",
      "content_length": 1343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "p. ��\n\nChapter 2 Isolation\n\nid | client |\n\namount\n\n−−−−+−−−−−−−−+−−−−−−−−−−\n\n1 | alice 2 | bob 3 | bob\n\n| 900.00 | 202.0000 | 707.0000\n\n(3 rows)\n\nNow the first transaction commits its changes, and the second transaction repeats the same query:\n\n=> COMMIT;\n\n=> SELECT * FROM accounts ORDER BY id;\n\nid | client |\n\namount\n\n−−−−+−−−−−−−−+−−−−−−−−−−\n\n1 | alice 2 | bob 3 | bob\n\n| 900.00 | 202.0000 | 707.0000\n\n(3 rows)\n\n=> COMMIT;\n\nThesecondtransactionstillseesthesamedataasbefore: neithernewrowsnorrow updates are visible. At this isolation level,you do not have to worry that something will change between operators.\n\nSerialization failures instead of lost updates. As we have already seen ,if two trans- actions update one and the same row at the Read Committed level, it can cause the read skew anomaly: the waiting transaction has to re-read the locked row, so it sees the state of this row at a different point in time as compared to other rows.\n\nSuch an anomaly is not allowed at the Repeatable Read isolation level,and if it does happen,thetransactioncanonlybeabortedwithaserializationfailure. Let’scheck it out by repeating the scenario with interest accrual:\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n2 | bob 3 | bob\n\n| 200.00 | 800.00\n\n(2 rows)\n\n=> BEGIN;\n\n62",
      "content_length": 1318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "2.3 Isolation Levels in PostgreSQL\n\n=> UPDATE accounts SET amount = amount - 100.00 WHERE id = 3;\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> UPDATE accounts SET amount = amount * 1.01 WHERE client IN ( SELECT client FROM accounts GROUP BY client HAVING sum(amount) >= 1000\n\n);\n\n=> COMMIT;\n\nERROR:\n\ncould not serialize access due to concurrent update\n\n=> ROLLBACK;\n\nThe data remains consistent:\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n2 | bob 3 | bob\n\n| 200.00 | 700.00\n\n(2 rows)\n\nThe same error will be raised by any concurrent row updates, even if they affect different columns.\n\nWe will also get this error if we try to update the balance based on the previously stored value:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT amount FROM accounts WHERE id = 1;\n\namount −−−−−−−− 900.00 (1 row)\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n63",
      "content_length": 905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Chapter 2 Isolation\n\n=> SELECT amount FROM accounts WHERE id = 1;\n\namount −−−−−−−− 900.00 (1 row)\n\n=> UPDATE accounts SET amount = 900.00 + 100.00 WHERE id = 1 RETURNING amount;\n\namount −−−−−−−−− 1000.00\n\n(1 row) UPDATE 1\n\n=> COMMIT;\n\n=> UPDATE accounts SET amount = 900.00 + 100.00 WHERE id = 1 RETURNING amount;\n\nERROR:\n\ncould not serialize access due to concurrent update\n\n=> ROLLBACK;\n\nA practical insight: if your application is using the Repeatable Read isolation level for write transactions, it must be ready to retry transactions that have been com- pleted with a serialization failure. For read-only transactions, such an outcome is impossible.\n\nWrite skew. As we have seen, the Postgre��� implementation of the Repeatable Read isolation level prevents all the anomalies described in the standard. But not all possible ones: no one knows how many of them exist. However, one important fact is proved for sure: snapshot isolation does not prevent only two anomalies, no matter how many other anomalies are out there.\n\nThe first one is write skew.\n\nLet’s define the following consistency rule: it is allowed to have a negative balance in some of the customer’s accounts as long as the total balance is non-negative.\n\nThe first transaction gets the total balance of Bob’s accounts:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n64",
      "content_length": 1335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "2.3 Isolation Levels in PostgreSQL\n\n=> SELECT sum(amount) FROM accounts WHERE client = 'bob';\n\nsum −−−−−−−− 900.00 (1 row)\n\nThe second transaction gets the same sum:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT sum(amount) FROM accounts WHERE client = 'bob';\n\nsum −−−−−−−− 900.00 (1 row)\n\nThe first transaction fairly assumes that it can debit one of the accounts by $���:\n\n=> UPDATE accounts SET amount = amount - 600.00 WHERE id = 2;\n\nThe second transaction comes to the same conclusion, but debits the other ac- count:\n\n=> UPDATE accounts SET amount = amount - 600.00 WHERE id = 3;\n\n=> COMMIT;\n\n=> COMMIT;\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−−\n\n2 | bob 3 | bob\n\n| −400.00 100.00 |\n\n(2 rows)\n\nBob’s total balance is now negative, although both transactions would have been correct if run separately.\n\nRead-only transaction anomaly. The read-only transaction anomaly is the second and the last one allowed at the Repeatable Read isolation level. To observe this anomaly, we have to run three transactions: two of them are going to update the data, while the third one will be read-only.\n\n65",
      "content_length": 1157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Chapter 2 Isolation\n\nBut first let’s restore Bob’s balance:\n\n=> UPDATE accounts SET amount = 900.00 WHERE id = 2;\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n3 | bob 2 | bob\n\n| 100.00 | 900.00\n\n(2 rows)\n\nThe first transaction calculates the interest to be accrued on Bob’s total balance and adds this sum to one of his accounts:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ; -- 1\n\n=> UPDATE accounts SET amount = amount + (\n\nSELECT sum(amount) FROM accounts WHERE client = 'bob'\n\n) * 0.01 WHERE id = 2;\n\nThen the second transaction withdraws some money from Bob’s other account and commits this change:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ; -- 2\n\n=> UPDATE accounts SET amount = amount - 100.00 WHERE id = 3;\n\n=> COMMIT;\n\nIf the first transaction gets committed at this point,there will be no anomalies: we couldassumethatthefirsttransactioniscommittedbeforethesecondone(butnot vice versa—the first transaction had seen the state of account with id = 3 before any updates were made by the second transaction).\n\nBut let’s imagine that at this very moment we start a ready-only transaction to query an account that is not affected by the first two transactions:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ; -- 3\n\n=> SELECT * FROM accounts WHERE client = 'alice';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−−\n\n1 | alice\n\n| 1000.00\n\n(1 row)\n\nAnd only now will the first transaction get committed:\n\n66",
      "content_length": 1448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "2.3 Isolation Levels in PostgreSQL\n\n=> COMMIT;\n\nWhich state should the third transaction see at this point? Having started,it could see the changes made by the second transaction (which had already been commit- ted), but not by the first one (which had not been committed yet). But as we have already established, the second transaction should be treated as if it were started afterthefirstone. Anystateseenbythethirdtransactionwillbeinconsistent—this is exactly what is meant by the read-only transaction anomaly:\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n2 | bob 3 | bob\n\n| 900.00 0.00 |\n\n(2 rows)\n\n=> COMMIT;\n\nSerializable\n\nThe Serializable1 isolation level prevents all possible anomalies. This level is vir- tually built on top of snapshot isolation. Those anomalies that do not occur at the Repeatable Read isolation level (such as dirty, non-repeatable, or phantom reads) cannot occur at the Serializable level either. And those two anomalies that do occur (write skew and read-only transaction anomalies) get detected in a special way to abort the transaction, causing an already familiar serialization failure.\n\nNo anomalies. Let’s make sure that our write skew scenario with a serialization failure:\n\nwill eventually end\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE;\n\n=> SELECT sum(amount) FROM accounts WHERE client = 'bob';\n\nsum −−−−−−−−−− 910.0000\n\n(1 row)\n\n1 postgresql.org/docs/14/transaction-iso.html#XACT-SERIALIZABLE\n\n67\n\np. ��",
      "content_length": 1492,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Chapter 2 Isolation\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE;\n\n=> SELECT sum(amount) FROM accounts WHERE client = 'bob';\n\nsum −−−−−−−−−− 910.0000\n\n(1 row)\n\n=> UPDATE accounts SET amount = amount - 600.00 WHERE id = 2;\n\n=> UPDATE accounts SET amount = amount - 600.00 WHERE id = 3;\n\n=> COMMIT;\n\nCOMMIT\n\n=> COMMIT;\n\nERROR: among transactions DETAIL: commit attempt. HINT:\n\ncould not serialize access due to read/write dependencies\n\nReason code: Canceled on identification as a pivot, during\n\nThe transaction might succeed if retried.\n\nThe scenario with the read-only transaction anomaly will lead to the same error.\n\nDeferring a read-only transaction. To avoid situations when a read-only transac- tion can cause an anomaly that compromises data consistency, Postgre��� offers an interesting solution: this transaction can be deferred until its execution be- comes safe. It is the only case when a ������ statement can be blocked by row updates.\n\nWe are going to check it out by repeating the scenario that demonstrated the read- only transaction anomaly:\n\n=> UPDATE accounts SET amount = 900.00 WHERE id = 2;\n\n=> UPDATE accounts SET amount = 100.00 WHERE id = 3;\n\n=> SELECT * FROM accounts WHERE client = 'bob' ORDER BY id;\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n2 | bob 3 | bob\n\n| 900.00 | 100.00\n\n(2 rows)\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE; -- 1\n\n68",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "2.3 Isolation Levels in PostgreSQL\n\n=> UPDATE accounts SET amount = amount + (\n\nSELECT sum(amount) FROM accounts WHERE client = 'bob'\n\n) * 0.01 WHERE id = 2;\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE; -- 2\n\n=> UPDATE accounts SET amount = amount - 100.00 WHERE id = 3;\n\n=> COMMIT;\n\nLet’s explicitly declare the third transaction as ���� ���� and ����������:\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE READ ONLY DEFERRABLE; -- 3\n\n=> SELECT * FROM accounts WHERE client = 'alice';\n\nAn attempt to run the query blocks the transaction—otherwise, it would have caused an anomaly.\n\nAnd only when the first transaction is committed, the third one can continue its execution:\n\n=> COMMIT;\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−−\n\n1 | alice\n\n| 1000.00\n\n(1 row)\n\n=> SELECT * FROM accounts WHERE client = 'bob';\n\nid | client |\n\namount\n\n−−−−+−−−−−−−−+−−−−−−−−−−\n\n2 | bob 3 | bob\n\n| 910.0000 0.00 |\n\n(2 rows)\n\n=> COMMIT;\n\nThus,if an application uses the Serializable isolation level,it must be ready to retry transactions that have ended with a serialization failure. (The Repeatable Read level requires the same approach unless the application is limited to read-only transactions.)\n\nThe Serializable isolation level brings ease of programming, but the price you pay is the overhead incurred by anomaly detection and forced termination of a certain\n\n69",
      "content_length": 1338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "p. ���\n\nread committed\n\nv. ��\n\nChapter 2 Isolation\n\nfraction of transactions. You can lower this impact by explicitly using the ���� ���� clause when declaring read-only transactions. But the main questions is, of course, how big the fraction of aborted transactions is—since these transactions will have to be retried. It would have been not so bad if Postgre��� aborted only those transactions that result in data conflicts and are really incompatible. But such an approach would inevitably be too resource-intensive, as it would involve tracking operations on each row.\n\nThe current implementation allows false positives: Postgre��� can abort some ab- solutely safe transactions that are simply out of luck. Their “luck” depends on many factors, such as the presence of appropriate indexes or the amount of ��� available, so the actual behavior is hard to predict in advance.\n\nIf you use the Serializable level, it must be observed by all transactions of the ap- plication. When combined with other levels, Serializable behaves as Repeatable Read without any notice. So if you decide to use the Serializable level, it makes default_transaction_isolation parameter value accordingly— sense to modify the even though someone can still overwrite it by explicitly setting a different level.\n\nfor example, queries run at the Serializable level There are also other restrictions; cannot be executed on replicas. And although the functionality of this level is constantly being improved, the current limitations and overhead make it less at- tractive.\n\n2.4 Which Isolation Level to Use?\n\nRead CommittedisthedefaultisolationlevelinPostgre���,andapparentlyitisthis level that is used in the vast majority of applications. This level can be convenient because it allows aborting transactions only in case of a failure; it does not abort anytransactionstopreservedataconsistency. Inotherwords,serializationfailures cannot occur, so you do not have to take care of transaction retries.\n\nThe downside of this level is a large number of possible anomalies, which have been discussed in detail above. A developer has to keep them in mind all the time and write the code in a way that prevents their occurrence. If it is impossible to define all the needed actions in a single ��� statement, then you have to resort to explicit locking. The toughest part is that the code is hard to test for errors\n\n70",
      "content_length": 2389,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "2.4 Which Isolation Level to Use?\n\nrelated to data inconsistency; such errors can appear in unpredictable and barely reproducible ways, so they are very hard to fix too.\n\nThe Repeatable Read isolation level eliminates some of the inconsistency prob- lems, but alas, not all of them. Therefore, you must not only remember about the remaining anomalies,but also modify the application to correctly handle serializa- tion failures, which is certainly inconvenient. However, for read-only transactions this level is a perfect complement to the Read Committed level; it can be very useful for cases like building reports that involve multiple ��� queries.\n\nAnd finally, the Serializable isolation level allows you not to worry about data con- sistency at all, which simplifies writing the code to a great extent. The only thing required from the application is the ability to retry any transaction that is aborted with a serialization failure. However, the number of aborted transactions and as- sociated overhead can significantly reduce system throughput. You should also keep in mind that the Serializable level is not supported on replicas and cannot be combined with other isolation levels.\n\n71",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "p. ���\n\n3\n\nPages and Tuples\n\n3.1 Page Structure\n\nEach page has a certain inner layout that usually consists of the following parts:1\n\npage header\n\nan array of item pointers\n\nfree space\n\nitems (row versions)\n\nspecial space\n\nPage Header\n\nThe page header is located in the lowest addresses and has a fixed size. It stores various information about the page , such as its checksum and the sizes of all the other parts of the page.\n\nThese sizes can be easily displayed using the pageinspect extension.2 Let’s take a look at the first page of the table (page numbering is zero-based):\n\n1 postgresql.org/docs/14/storage-page-layout.html\n\ninclude/storage/bufpage.h\n\n2 postgresql.org/docs/14/pageinspect.html\n\n72",
      "content_length": 703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "3.1 Page Structure\n\n=> CREATE EXTENSION pageinspect;\n\n=> SELECT lower, upper, special, pagesize FROM page_header(get_raw_page('accounts',0));\n\nlower | upper | special | pagesize −−−−−−−+−−−−−−−+−−−−−−−−−+−−−−−−−−−−\n\n152 |\n\n6904 |\n\n8192 |\n\n8192\n\n(1 row)\n\n0\n\nheader\n\n24\n\nan array of item pointers\n\nlower\n\nfree space\n\nupper\n\nitems\n\nspecial\n\nspecial space\n\npagesize\n\nSpecial Space\n\nThe special space is located in the opposite part of the page, taking its highest ad- dresses. It is used by some indexes to store auxiliary information; in other indexes and table pages this space is zero-sized.\n\nIn general,the layout of index pages is quite diverse; their content largely depends on a particular index type. Even one and the same index can have different kinds of pages: for example, �-trees have a metadata page of a special structure (page zero) and regular pages that are very similar to table pages.\n\nTuples\n\nRows contain the actual data stored in the database,together with some additional information. They are located just before the special space.\n\n73",
      "content_length": 1056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "p. ��\n\nChapter 3 Pages and Tuples\n\nIn the case of tables, we have to deal with row versions rather than rows because multiversion concurrency control implies having several versions of one and the same row. Indexes do not use this ���� mechanism; instead, they have to ref- erence all the available row versions, falling back on visibility rules to select the appropriate ones.\n\nBoth table row versions and index entries are often referred to as tuples. This term is borrowed from the relational theory—it is yet another legacy of Postgre���’s academic past.\n\nItem Pointers\n\nThe array of pointers to tuples serves as the page’s table of contents. It is located right after the header.\n\nIndex entries have to refer to particular heap tuples somehow. Postgre��� em- ploys six-byte tuple identifiers (���s) for this purpose. Each ��� consists of the page and a reference to a particular row version located in this number of the main fork page.\n\nIn theory, tuples could be referred to by their offset from the start of the page. But then it would be impossible to move tuples within pages without breaking these references, which in turn would lead to page fragmentation and other unpleasant consequences.\n\nFor this reason,Postgre��� uses indirect addressing: a tuple identifier refers to the corresponding pointer number, and this pointer specifies the current offset of the tuple. If the tuple is moved within the page, its ��� still remains the same; it is enough to modify the pointer, which is also located in this page.\n\nEach pointer takes exactly four bytes and contains the following data:\n\ntuple offset from the start of the page\n\ntuple length\n\nseveral bits defining the tuple status\n\n74",
      "content_length": 1693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "3.2 Row Version Layout\n\nFree Space\n\nPages can have some free space left between pointers and tuples (which is reflected in the free space map ). There is no page fragmentation: all the free space available is always aggregated into one chunk.1\n\n3.2 Row Version Layout\n\nEach row version contains a header followed by actual data. The header consists of multiple fields, including the following:\n\nxmin,xmax represent transaction ��s; they are used to differentiate between this\n\nand other versions of one and the same row.\n\ninfomask provides a set of information bits that define version properties.\n\nctid is a pointer to the next updated version of the same row.\n\nnull bitmap is an array of bits marking the columns that can contain ���� values.\n\nAs a result, the header turns out quite big: it requires at least �� bytes for each tu- ple,and this value is often exceeded because of the null bitmap and the mandatory padding used for data alignment. In a “narrow” table, the size of various metadata can easily beat the size of the actual data stored.\n\nDatalayoutondiskfullycoincideswithdatarepresentationin���. Thepagealong with its tuples is read into the buffer cache as is, without any transformations. That’s why data files are incompatible between different platforms.2\n\nOne of the sources of incompatibility is the byte order. For example, the x�� ar- chitecture is little-endian, z/�rchitecture is big-endian, and ��� has configurable byte order.\n\nAnother reason is data alignment by machine word boundaries, which is required by many architectures. For example, in a ��-bit x�� system, integer numbers (the integer type, takes four bytes) are aligned by the boundary of four-byte words,\n\n1 backend/storage/page/bufpage.c, PageRepairFragmentation function 2 include/access/htup_details.h\n\n75\n\np. ��",
      "content_length": 1805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Chapter 3 Pages and Tuples\n\njust like double-precision floating-point numbers (the double precision type, eight bytes). But in a ��-bit system, double values are aligned by the boundary of eight- byte words.\n\nData alignment makes the size of a tuple dependent on the order of fields in the table. This effect is usually negligible,but in some cases it can lead to a significant size increase. Here is an example:\n\n=> CREATE TABLE padding(\n\nb1 boolean, i1 integer, b2 boolean, i2 integer\n\n);\n\n=> INSERT INTO padding VALUES (true,1,false,2);\n\n=> SELECT lp_len FROM heap_page_items(get_raw_page('padding', 0));\n\nlp_len −−−−−−−−\n\n40 (1 row)\n\nI have used the heap_page_items function of the pageinspect extension to display some details about pointers and tuples.\n\nIn Postgre���, tables are often referred to as heap. This is yet another obscure term that hints at the similarity between space allocation for tuples and dynamic memory alloca- tion. Some analogy can certainly be seen,but tables are managed by completely different algorithms. We can interpret this term in the sense that“everything is piled up into a heap,” by contrast with ordered indexes.\n\nThe size of the row is �� bytes. Its header takes �� bytes, a column of the integer type takes � bytes, and boolean columns take � byte each. It makes �� bytes, and � bytes are wasted on four-byte alignment of integer columns.\n\nIf we rebuild the table, the space will be used more efficiently:\n\n=> DROP TABLE padding;\n\n=> CREATE TABLE padding(\n\ni1 integer, i2 integer, b1 boolean, b2 boolean\n\n);\n\n76",
      "content_length": 1554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "3.3 Operations on Tuples\n\n=> INSERT INTO padding VALUES (1,2,true,false);\n\n=> SELECT lp_len FROM heap_page_items(get_raw_page('padding', 0));\n\nlp_len −−−−−−−−\n\n34 (1 row)\n\nAnother possible micro-optimization is to start the table with the fixed-length columns that cannot contain ���� values. Access to such columns will be more efficient because it is possible to cache their offset within the tuple.1\n\n3.3 Operations on Tuples\n\nTo identify different versions of one and the same row, Postgre��� marks each of them with two values: xmin and xmax. These values define “validity time” of each rowversion,butinsteadoftheactualtime,theyrelyonever-increasingtransaction ��s.\n\nWhen a row is created, its xmin value is set to the transaction �� of the ������ com- mand.\n\nWhenarowisdeleted,thexmaxvalueofitscurrentversionissettothetransaction �� of the ������ command.\n\nWith a certain degree of abstraction, the ������ command can be regarded as two separate operations: ������ and ������. First, the xmax value of the current row version is set to the transaction �� of the ������ command. Then a new ver- sion of this row is created; its xmin value will be the same as the xmax value of the previous version.\n\nNow let’s get down to some low-level details of different operations on tuples.2\n\nFor these experiments, we will need a two-column table with an index created on one of the columns:\n\n1 backend/access/common/heaptuple.c, heap_deform_tuple function 2 backend/access/transam/README\n\n77\n\np. ���",
      "content_length": 1495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Chapter 3 Pages and Tuples\n\n=> CREATE TABLE t(\n\nid integer GENERATED ALWAYS AS IDENTITY, s text\n\n);\n\n=> CREATE INDEX ON t(s);\n\nInsert\n\nStart a transaction and insert one row:\n\n=> BEGIN;\n\n=> INSERT INTO t(s) VALUES ('FOO');\n\nHere is the current transaction ��:\n\n=> -- txid_current() before v.13 SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n776\n\n(1 row)\n\nTo denote the concept of a transaction,Postgre��� uses the term xact,which can be found both in ��� function names and in the source code. Consequently, a transaction �� can be called xact ��, ����, or simply ���. We are going to come across these abbreviations over and over again.\n\nLet’s take a look at the page contents. The heap_page_items function can give us all the required information, but it shows the data “as is,” so the output format is a bit hard to comprehend:\n\n=> SELECT * FROM heap_page_items(get_raw_page('t',0)) \\gx\n\n−[ RECORD 1 ]−−−−−−−−−−−−−−−−−−− lp lp_off lp_flags lp_len t_xmin t_xmax t_field3 t_ctid\n\n| 1 | 8160 | 1 | 32 | 776 | 0 | 0 | (0,1)\n\n78",
      "content_length": 1050,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "3.3 Operations on Tuples\n\nt_infomask2 | 2 t_infomask t_hoff t_bits t_oid t_data\n\n| 2050 | 24 | | | \\x0100000009464f4f\n\nTo make it more readable, we can leave out some information and expand a few columns:\n\n=> SELECT '(0,'||lp||')' AS ctid,\n\nCASE lp_flags\n\nWHEN 0 THEN 'unused' WHEN 1 THEN 'normal' WHEN 2 THEN 'redirect to '||lp_off WHEN 3 THEN 'dead'\n\nEND AS state, t_xmin as xmin, t_xmax as xmax, (t_infomask & 256) > 0 (t_infomask & 512) > 0 (t_infomask & 1024) > 0 AS xmax_committed, (t_infomask & 2048) > 0 AS xmax_aborted\n\nAS xmin_committed, AS xmin_aborted,\n\nFROM heap_page_items(get_raw_page('t',0)) \\gx\n\n−[ RECORD 1 ]−−+−−−−−−− | (0,1) ctid | normal state | 776 xmin xmax | 0 xmin_committed | f xmin_aborted | f xmax_committed | f | t xmax_aborted\n\nThis is what has been done here:\n\nThe lp pointer is converted to the standard format of a tuple ��: (page number,\n\npointer number).\n\nThelp_flagsstateisspelledout. Hereitissettothenormalvalue,whichmeans\n\nthat it really points to a tuple.\n\nOf all the information bits, we have singled out just two pairs so far. The xmin_committed and xmin_aborted bits show whether the xmin transaction is\n\n79",
      "content_length": 1149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "v. ��\n\nChapter 3 Pages and Tuples\n\ncommitted or aborted. The xmax_committed and xmax_aborted bits give simi- lar information about the xmax transaction.\n\nextension provides the heap_tuple_infomask_flags function that explains The pageinspect all the information bits, but I am going to retrieve only those that are required at the moment, showing them in a more concise form.\n\nLet’s get back to our experiment. The ������ command has added pointer � to the heap page; it refers to the first tuple, which is currently the only one.\n\nThe xmin field of the tuple is set to the current transaction ��. This transaction is still active, so the xmin_committed and xmin_aborted bits are not set yet.\n\nThe xmax field contains �, which is a dummy number showing that this tuple has not been deleted and represents the current version of the row. Transactions will ignore this number because the xmax_aborted bit is set.\n\nIt may seem strange that the bit corresponding to an aborted transaction is set for the transaction that has not happened yet. But there is no difference between such transac- tions from the isolation standpoint: an aborted transaction leaves no trace, hence it has never existed.\n\nWe will use this query more than once,so I am going to wrap it into a function. And\n\nwhilebeingatit,Iwillalsomaketheoutputmoreconcisebyhidingtheinformation bit columns and displaying the status of transactions together with their ��s.\n\n=> CREATE FUNCTION heap_page(relname text, pageno integer) RETURNS TABLE(ctid tid, state text, xmin text, xmax text) AS $$ SELECT (pageno,lp)::text::tid AS ctid,\n\nCASE lp_flags\n\nWHEN 0 THEN 'unused' WHEN 1 THEN 'normal' WHEN 2 THEN 'redirect to '||lp_off WHEN 3 THEN 'dead'\n\nEND AS state, t_xmin || CASE\n\nWHEN (t_infomask & 256) > 0 THEN ' c' WHEN (t_infomask & 512) > 0 THEN ' a' ELSE '' END AS xmin,\n\n80",
      "content_length": 1835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "3.3 Operations on Tuples\n\nt_xmax || CASE\n\nWHEN (t_infomask & 1024) > 0 THEN ' c' WHEN (t_infomask & 2048) > 0 THEN ' a' ELSE '' END AS xmax\n\nFROM heap_page_items(get_raw_page(relname,pageno)) ORDER BY lp; $$ LANGUAGE sql;\n\nNow it is much clearer what is happening in the tuple header:\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin | xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−+−−−−−−\n\n(0,1) | normal | 776\n\n| 0 a\n\n(1 row)\n\nYou can get similar but less detailed information from the table itself by querying the xmin and xmax pseudocolumns:\n\n=> SELECT xmin, xmax, * FROM t;\n\nxmin | xmax | id |\n\ns\n\n−−−−−−+−−−−−−+−−−−+−−−−− 0 |\n\n776 | (1 row)\n\n1 | FOO\n\nCommit\n\nOnce a transaction has been completed successfully, its status has to be stored somehow—it must be registered that the transaction is committed. For this pur- pose, Postgre��� employs a special ���� (commit log) structure.1 It is stored as files in the ������/pg_xact directory rather than as a system catalog table.\n\nPreviously, these files were located in ������/pg_clog, but in version �� this directory got renamed:2 it was not uncommon for database administrators unfamiliar with Postgre��� to delete it in search of free disk space, thinking that a “log” is something unnecessary.\n\n1 include/access/clog.h\n\nbackend/access/transam/clog.c 2 commitfest.postgresql.org/13/750\n\n81",
      "content_length": 1339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "p. ���\n\nChapter 3 Pages and Tuples\n\nC��� is split into several files solely for convenience. by page via buffers in the server’s shared memory.1\n\nThese files are accessed page\n\nJust like a tuple header, ���� contains two bits for each transaction: committed and aborted.\n\nOnce committed, a transaction is marked in ���� with the committed bit. When any other transaction accesses a heap page, it has to answer the question: has the xmin transaction already finished?\n\nIf not, then the created tuple must not be visible.\n\nTo check whether the transaction is still active, Postgre��� uses yet another structure located in the shared memory of the instance; it is called ProcArray. This structure contains the list of all the active processes,with the correspond- ing current (active) transaction specified for each process.\n\nIf yes,was it committed or aborted? In the latter case,the corresponding tuple\n\ncannot be visible either.\n\nIt is this check that requires ����. But even though the most recent ���� pages are stored in memory buffers, it is still expensive to perform this check every time. Once determined, the transaction status is written into the tuple header—morespecifically,intoxmin_committedandxmin_abortedinformation bits, which are also called hint bits. If one of these bits is set, then the xmin transaction status is considered to be already known,and the next transaction will have to access neither ���� nor ProcArray.\n\nWhy aren’t these bits set by the transaction that performs row insertion? The prob- lem is that it is not known yet at that time whether this transaction will complete successfully. Andwhenitiscommitted,itisalreadyunclearwhichtuplesandpages have been changed. If a transaction affects many pages,it may be too expensive to trackthem. Besides,someofthesepagesmaybenotinthecacheanymore; reading them again to simply update the hint bits would seriously slow down the commit.\n\n1 backend/access/transam/clog.c\n\n82",
      "content_length": 1949,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "3.3 Operations on Tuples\n\nThe flip side of this cost reduction is that any transaction (even a read-only ������ command) can start setting hint bits, thus leaving a trail of dirtied pages in the buffer cache.\n\nFinally, let’s commit the transaction started with the ������ statement:\n\n=> COMMIT;\n\nNothing has changed in the page (but we know that the transaction status has al- ready been written into ����):\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin | xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−+−−−−−−\n\n(0,1) | normal | 776\n\n| 0 a\n\n(1 row)\n\nNowthefirsttransactionthataccessesthepage(ina“standard”way,withoutusing pageinspect) has to determine the status of the xmin transaction and update the hint bits:\n\n=> SELECT * FROM t;\n\nid |\n\ns\n\n−−−−+−−−−− 1 | FOO\n\n(1 row)\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−− (0,1) | normal | 776 c | 0 a\n\n(1 row)\n\nDelete\n\nWhen a row is deleted,the xmax field of its current version is set to the transaction �� that performs the deletion, and the xmax_aborted bit is unset.\n\n83",
      "content_length": 1069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "p. ���\n\nChapter 3 Pages and Tuples\n\nWhile this transaction is active,the xmax value serves as a row lock. If another transaction is going to update or delete this row, it will have to wait until the xmax transaction is complete.\n\nLet’s delete a row:\n\n=> BEGIN;\n\n=> DELETE FROM t;\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n777\n\n(1 row)\n\nThe transaction �� has already been written into the xmax field, but the informa- tion bits have not been set yet:\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−− (0,1) | normal | 776 c | 777\n\n(1 row)\n\nAbort\n\nThe mechanism of aborting a transaction is similar to that of commit and happens just as fast, but instead of committed it sets the aborted bit in ����. Although the corresponding command is called ��������, no actual data rollback is happening: all the changes made by the aborted transaction in data pages remain in place.\n\n=> ROLLBACK;\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−− (0,1) | normal | 776 c | 777\n\n(1 row)\n\n84",
      "content_length": 1110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "3.3 Operations on Tuples\n\nWhenthepageisaccessed,thetransactionstatusischecked,andthetuplereceives the xmax_aborted hint bit. The xmax number itself still remains in the page, but no one is going to pay attention to it anymore:\n\n=> SELECT * FROM t;\n\nid |\n\ns\n\n−−−−+−−−−− 1 | FOO\n\n(1 row)\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−− (0,1) | normal | 776 c | 777 a\n\n(1 row)\n\nUpdate\n\nAn update is performed in such a way as if the current tuple is deleted, and then a new one is inserted:\n\n=> BEGIN;\n\n=> UPDATE t SET s = 'BAR';\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n778\n\n(1 row)\n\nThe query returns a single row (its new version):\n\n=> SELECT * FROM t;\n\nid |\n\ns\n\n−−−−+−−−−− 1 | BAR\n\n(1 row)\n\n85",
      "content_length": 781,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "p. ���\n\nChapter 3 Pages and Tuples\n\nBut the page keeps both versions:\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−− (0,1) | normal | 776 c | 778 | 0 a (0,2) | normal | 778\n\n(2 rows)\n\nThe xmax field of the previously deleted version contains the current transaction ��. This value is written on top of the old one because the previous transaction was aborted. The xmax_aborted bit is unset since the status of the current transaction is still unknown.\n\nTo complete this experiment, let’s commit the transaction.\n\n=> COMMIT;\n\n3.4 Indexes\n\nRegardlessoftheirtype,indexesdonotuserowversioning; eachrowisrepresented by exactly one tuple. In other words, index row headers do not contain xmin and xmax fields. Index entries point to all the versions of the corresponding table row . To figure out which row version is visible, transactions have to access the table (unless the required page appears in the visibility map).\n\nFor convenience, let’s create a simple function that will use pageinspect to display all the index entries in the page (�-tree index pages store them as a flat list):\n\n=> CREATE FUNCTION index_page(relname text, pageno integer) RETURNS TABLE(itemoffset smallint, htid tid) AS $$ SELECT itemoffset,\n\nhtid -- ctid before v.13\n\nFROM bt_page_items(relname,pageno); $$ LANGUAGE sql;\n\nThe page references both heap tuples, the current and the previous one:\n\n86",
      "content_length": 1425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "3.5 TOAST\n\n=> SELECT * FROM index_page('t_s_idx',1);\n\nitemoffset | htid −−−−−−−−−−−−+−−−−−−−\n\n1 | (0,2) 2 | (0,1)\n\n(2 rows)\n\nSince ��� < ���, the pointer to the second tuple comes first in the index.\n\n3.5 TOAST\n\nA ����� table is virtually a regular table, and it has its own versioning that does not depend on row versions of the main table. However, rows of ����� tables are handled in such a way that they are never updated; they can be either added or deleted, so their versioning is somewhat artificial.\n\nEachdatamodificationresultsincreationofanewtupleinthemaintable. Butifan updatedoesnotaffectanylongvaluesstoredin�����,thenewtuplewillreference an existing toasted value. Only when a long value gets updated will Postgre��� create both a new tuple in the main table and new “toasts.”\n\n3.6 Virtual Transactions\n\nTo consume transaction ��s sparingly, Postgre��� offers a special optimization.\n\nIf a transaction is read-only,it does not affect row visibility in any way. That’s why such a transaction is given a virtual ���1 at first, which consists of the backend process �� and a sequential number. Assigning a virtual ��� does not require any synchronization between different processes,so it happens very fast. At this point, the transaction has no real �� yet:\n\n=> BEGIN;\n\n1 backend/access/transam/xact.c\n\n87\n\np. ��\n\np. ���",
      "content_length": 1332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Chapter 3 Pages and Tuples\n\n=> -- txid_current_if_assigned() before v.13 SELECT pg_current_xact_id_if_assigned();\n\npg_current_xact_id_if_assigned −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n(1 row)\n\nAt different points in time, the system can contain some virtual ���s that have already been used. And it is perfectly normal: virtual ���s exist only in ���, and only while the corresponding transactions are active; they are never written into data pages and never get to disk.\n\nOnce the transaction starts modifying data, it receives an actual unique ��:\n\n=> UPDATE accounts SET amount = amount - 1.00;\n\n=> SELECT pg_current_xact_id_if_assigned();\n\npg_current_xact_id_if_assigned −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n780\n\n(1 row)\n\n=> COMMIT;\n\n3.7 Subtransactions\n\nSavepoints\n\nS�� supports savepoints, which enable canceling some of the operations within a transaction without aborting this transaction as a whole. But such a scenario does not fit the course of action described above: the status of a transaction applies to all its operations, and no physical data rollback is performed.\n\nTo implement this functionality, a transaction containing a savepoint is split into several subtransactions,1 so their status can be managed separately.\n\n1 backend/access/transam/subtrans.c\n\n88",
      "content_length": 1270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "3.7 Subtransactions\n\nSubtransactions have their own ��s (which are bigger than the �� of the main trans- action). The status of a subtransaction is written into ���� in the usual manner; however, committed subtransactions receive both the committed and the aborted bits at once. The final decision depends on the status of the main transaction: if it is aborted, all its subtransactions will be considered aborted too.\n\nThe information about subtransactions is stored under the ������/pg_subtrans di- rectory. File access is arranged via buffers that are located in the instance’s shared memory and have the same structure as ���� buffers.1\n\nDo not confuse subtransactions with autonomous ones. Unlike subtransactions, the latter do not depend on each other in any way. Vanilla Postgre��� does not support autonomous transactions, and it is probably for the best: they are required in very rare cases, but their availability in other database systems often provokes misuse, which can cause a lot of trouble.\n\nLet’s truncate the table, start a new transaction, and insert a row:\n\n=> TRUNCATE TABLE t;\n\n=> BEGIN;\n\n=> INSERT INTO t(s) VALUES ('FOO');\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n782\n\n(1 row)\n\nNow create a savepoint and insert another row:\n\n=> SAVEPOINT sp;\n\n=> INSERT INTO t(s) VALUES ('XYZ');\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n782\n\n(1 row)\n\nNote that the pg_current_xact_id function returns the �� of the main transaction, not that of a subtransaction.\n\n1 backend/access/transam/slru.c\n\n89",
      "content_length": 1576,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Chapter 3 Pages and Tuples\n\n=> SELECT * FROM heap_page('t',0) p\n\nLEFT JOIN t ON p.ctid = t.ctid;\n\nctid\n\n| state\n\n| xmin | xmax | id |\n\ns\n\n−−−−−−−+−−−−−−−−+−−−−−−+−−−−−−+−−−−+−−−−−\n\n(0,1) | normal | 782 (0,2) | normal | 783\n\n| 0 a | 0 a\n\n| |\n\n2 | FOO 3 | XYZ\n\n(2 rows)\n\nLet’s roll back to the savepoint and insert the third row:\n\n=> ROLLBACK TO sp;\n\n=> INSERT INTO t(s) VALUES ('BAR');\n\n=> SELECT * FROM heap_page('t',0) p\n\nLEFT JOIN t ON p.ctid = t.ctid;\n\nctid\n\n| state\n\n| xmin | xmax | id |\n\ns\n\n−−−−−−−+−−−−−−−−+−−−−−−+−−−−−−+−−−−+−−−−−\n\n(0,1) | normal | 782 (0,2) | normal | 783 (0,3) | normal | 784\n\n| 0 a | 0 a | 0 a\n\n| | |\n\n2 | FOO\n\n|\n\n4 | BAR\n\n(3 rows)\n\nThe page still contains the row added by the aborted subtransaction.\n\nCommit the changes:\n\n=> COMMIT;\n\n=> SELECT * FROM t;\n\nid |\n\ns\n\n−−−−+−−−−− 2 | FOO 4 | BAR\n\n(2 rows)\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−− (0,1) | normal | 782 c | 0 a (0,2) | normal | 783 a | 0 a (0,3) | normal | 784 c | 0 a\n\n(3 rows)\n\n90",
      "content_length": 1030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "3.7 Subtransactions\n\nNow we can clearly see that each subtransaction has its own status.\n\nS�� does not allow using subtransactions directly, that is, you cannot start a new transaction before completing the current one:\n\n=> BEGIN;\n\nBEGIN\n\n=> BEGIN;\n\nWARNING: BEGIN\n\nthere is already a transaction in progress\n\n=> COMMIT;\n\nCOMMIT\n\n=> COMMIT;\n\nWARNING: COMMIT\n\nthere is no transaction in progress\n\nSubtransactions are employed implicitly: to implement savepoints, handle excep- tions in ��/pg���, and in some other, more exotic cases.\n\nErrors and Atomicity\n\nWhat happens if an error occurs during execution of a statement?\n\n=> BEGIN;\n\n=> SELECT * FROM t;\n\nid |\n\ns\n\n−−−−+−−−−− 2 | FOO 4 | BAR\n\n(2 rows)\n\n=> UPDATE t SET s = repeat('X', 1/(id-4));\n\nERROR:\n\ndivision by zero\n\nAfter a failure, the whole transaction is considered aborted and cannot perform any further operations:\n\n=> SELECT * FROM t;\n\nERROR: of transaction block\n\ncurrent transaction is aborted, commands ignored until end\n\n91",
      "content_length": 988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Chapter 3 Pages and Tuples\n\nAnd even if you try to commit the changes, Postgre��� will report that the trans- action is rolled back:\n\n=> COMMIT;\n\nROLLBACK\n\nWhy is it forbidden to continue transaction execution after a failure? Since the already executed operations are never rolled back, we would get access to some changes made before the error—it would break the atomicity of the statement,and hence that of the transaction itself.\n\nFor example,in our experiment the operator has managed to update one of the two rows before the failure:\n\n=> SELECT * FROM heap_page('t',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−− (0,1) | normal | 782 c | 785 (0,2) | normal | 783 a | 0 a (0,3) | normal | 784 c | 0 a | 0 a (0,4) | normal | 785\n\n(4 rows)\n\nOn a side note, psql provides a special mode that allows you to continue a transac- tion after a failure as if the erroneous statement were rolled back:\n\n=> \\set ON_ERROR_ROLLBACK on\n\n=> BEGIN;\n\n=> UPDATE t SET s = repeat('X', 1/(id-4));\n\nERROR:\n\ndivision by zero\n\n=> SELECT * FROM t;\n\nid |\n\ns\n\n−−−−+−−−−− 2 | FOO 4 | BAR\n\n(2 rows)\n\n=> COMMIT;\n\nCOMMIT\n\n92",
      "content_length": 1120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "3.7 Subtransactions\n\nAs you can guess, psql simply adds an implicit savepoint before each command when run in this mode; in case of a failure, a rollback is initiated. This mode is not used by default because issuing savepoints (even if they are not rolled back to) incurs significant overhead.\n\n93",
      "content_length": 298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "p. ��\n\n4\n\nSnapshots\n\n4.1 What is a Snapshot?\n\nA data page can contain several versions of one and the same row, although each transaction must see only one of them at the most. Together, visible versions of all the different rows constitute a snapshot . A snapshot includes only the current data committed by the time it was taken, thus providing a consistent (in the ���� sense) view of the data for this particular moment.\n\nToensureisolation,eachtransactionusesitsownsnapshot. Itmeansthatdifferent transactionscanseedifferentsnapshotstakenatdifferentpointsintime,whichare nevertheless consistent.\n\nAt the Read Committed isolation level, a snapshot is taken at the beginning of each statement, and it remains active only for the duration of this statement.\n\nAt the Repeatable Read and Serializable levels, a snapshot is taken at the begin- ning of the first statement of a transaction, and it remains active until the whole transaction is complete.\n\nsnapshot1\n\nsnapshot2\n\nsnapshot\n\nstatement1\n\nstatement2\n\nstatement1\n\nstatement2\n\nxid\n\nRead Committed\n\nRepeatable Read, Serializable\n\n94\n\nxid",
      "content_length": 1090,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "4.2 Row Version Visibility\n\n4.2 Row Version Visibility\n\nA snapshot is not a physical copy of all the required tuples. Instead, it is defined by several numbers, while tuple visibility is determined by certain rules.\n\nTuple visibility is defined by xmin and xmax fields of the tuple header (that is, ��s of transactions that perform insertion and deletion) and the corresponding hint bits. Since xmin–xmax intervals do not intersect, each row is represented in any snapshot by only one of its versions.\n\nThe exact visibility rules are quite complex,1 as they take into account a variety of differentscenariosandcornercases. Veryroughly,wecandescribethemasfollows: a tuple is visible in a snapshot that includes xmin transaction changes but excludes xmax transaction changes (in other words, the tuple has already appeared and has not been deleted yet).\n\nIn their turn, transaction changes are visible in a snapshot if this transaction was committed before the snapshot creation. As an exception, transactions can see their own uncommitted changes. If a transaction is aborted, its changes will not be visible in any snapshot.\n\nLet’s take a look at a simple example. In this illustration line segments represent transactions (from their start time till commit time):\n\nsnapshot\n\nxid\n\n1\n\n2\n\n3\n\nHere visibility rules are applied to transactions as follows:\n\nTransaction � was committed before the snapshot creation, so its changes are\n\nvisible.\n\n1 backend/access/heap/heapam_visibility.c\n\n95",
      "content_length": 1486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "off\n\np. ���\n\nChapter 4 Snapshots\n\nTransaction � was active at the time of the snapshot creation, so its changes\n\nare not visible.\n\nTransaction � was started after the snapshot creation, so its changes are not visible either (it makes no difference whether this transaction is completed or not).\n\n4.3 Snapshot Structure\n\nUnfortunately,the previous illustration has nothing to do with the way Postgre��� actually sees this picture.1 The problem is that the system does not know when transactions got committed. It is only known when they were started (this moment isdefinedbythetransaction ��),whiletheircompletionisnotregisteredanywhere.\n\nCommit times can be tracked2 if you enable the track_commit_timestamp parameter, but they do not participate in visibility checks in any way (although it can still be useful to track them for other purposes, for example, to apply in external replication solutions).\n\nBesides, Postgre��� always logs commit and rollback times in the corresponding ��� en- tries\n\n, but this information is used only for point-in-time recovery.\n\nIt is only the current status of a transaction that we can learn. This information is available in the server’s shared memory: the ProcArray structure contains the list of all the active sessions and their transactions. Once a transaction is complete, it is impossible to find out whether it was active at the time of the snapshot creation.\n\nSo to create a snapshot,it is not enough to register the moment when it was taken: it is also necessary to collect the status of all the transactions at that moment. Otherwise, later it will be impossible to understand which tuples must be visible in the snapshot, and which must be excluded.\n\nTakealookattheinformationavailabletothesystemwhenthesnapshotwastaken and some time afterwards (the white circle denotes an active transaction, whereas the black circles stand for completed ones):\n\n1 include/utils/snapshot.h\n\nbackend/utils/time/snapmgr.c\n\n2 backend/access/transam/commit_ts.c\n\n96",
      "content_length": 1995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "4.3 Snapshot Structure\n\nxid\n\nxid\n\n1\n\n2\n\n3\n\n1\n\n2\n\n3\n\nat snapshot creation…\n\n…and some time later\n\nSuppose we did not know that at the time the snapshot was taken the first transac- tion was still being executed and the third transaction had not started yet. Then it would seem that they were just like the second transaction (which was committed at that time), and it would be impossible to filter them out.\n\nFor this reason,Postgre��� cannot create a snapshot that shows a consistent state ofdataatsomearbitrarypointinthepast,evenifalltherequiredtuplesarepresent in heap pages. Consequently, it is impossible to implement retrospective queries (which are sometimes also called temporal or flashback queries).\n\nIntriguingly, such functionality was declared as one of the objectives of Postgres and was implemented at the very start, but it was removed from the database system when the project support was passed on to the community.1\n\nThus, a snapshot consists of several values saved at the time of its creation:2\n\nxmin is the snapshot’s lower boundary,which is represented by the �� of the oldest\n\nactive transaction.\n\nAll the transactions with smaller ��s are included into the snapshot) or aborted (so their changes are ignored).\n\nare either committed (so their changes\n\nxmax is the snapshot’s upper boundary, which is represented by the value that exceedsthe��ofthelatestcommittedtransactionbyone. Theupperboundary defines the moment when the snapshot was taken.\n\nAll the transactions whose ��s are equal to or greater than xmax are either still running or do not exist, so their changes cannot be visible.\n\nxip_list\n\nis the list of ��s of all the active transactions except for virtual ones,which\n\ndo not affect visibility in any way.\n\n1 Joseph M. Hellerstein, Looking Back at Postgres. https://arxiv.org/pdf/1901.01973.pdf 2 backend/storage/ipc/procarray.c, GetSnapshotData function\n\n97\n\np. ���\n\np. ��",
      "content_length": 1908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Chapter 4 Snapshots\n\nSnapshots also include several other parameters, but we will ignore them for now.\n\nIn a graphical form, a snapshot can be represented as a rectangle that comprises transactions from xmin to xmax:\n\nxip_list\n\nxmin\n\nxmax\n\nxid\n\n1\n\n2\n\n3\n\nTo understand how visibility rules are defined by the snapshot, we are going to reproduce the above scenario on the accounts table.\n\n=> TRUNCATE TABLE accounts;\n\nThe first transaction inserts the first row into the table and remains open:\n\n=> BEGIN;\n\n=> INSERT INTO accounts VALUES (1, 'alice', 1000.00);\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n790\n\n(1 row)\n\nThe second transaction inserts the second row and commits this change immedi- ately:\n\n=> BEGIN;\n\n=> INSERT INTO accounts VALUES (2, 'bob', 100.00);\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n791\n\n(1 row)\n\n=> COMMIT;\n\n98",
      "content_length": 899,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "4.3 Snapshot Structure\n\nAt this point, let’s create a new snapshot in another session. We could simply run any query for this purpose, but we will use a special function to take a look at this snapshot right away:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> -- txid_current_snapshot() before v.13 SELECT pg_current_snapshot();\n\npg_current_snapshot −−−−−−−−−−−−−−−−−−−−−\n\n790:792:790\n\n(1 row)\n\nThis function displays the following snapshot components, separated by colons: xmin, xmax, and xip_list (the list of active transactions; in this particular case it consists of a single item).\n\nOnce the snapshot is taken, commit the first transaction:\n\n=> COMMIT;\n\nThe third transaction is started after the snapshot creation. It modifies the second row, so a new tuple appears:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount + 100 WHERE id = 2;\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n792\n\n(1 row)\n\n=> COMMIT;\n\nOur snapshot sees only one tuple:\n\n=> SELECT ctid, * FROM accounts;\n\nctid\n\n| id | client | amount\n\n−−−−−−−+−−−−+−−−−−−−−+−−−−−−−−\n\n(0,2) |\n\n2 | bob\n\n| 100.00\n\n(1 row)\n\n99",
      "content_length": 1117,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "Chapter 4 Snapshots\n\nBut the table contains three of them:\n\n=> SELECT * FROM heap_page('accounts',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−\n\n(0,1) | normal | 790 c | 0 a (0,2) | normal | 791 c | 792 c (0,3) | normal | 792 c | 0 a\n\n(3 rows)\n\nSo how does Postgre��� choose which versions to show? By the above rules, changes are included into a snapshot only if they are made by committed trans- actions that satisfy the following criteria:\n\nIf xid < xmin,changes are shown unconditionally (like in the case of the trans-\n\naction that created the accounts table).\n\nIfxmin ⩽ xid < xmax,changesareshownonlyifthecorrespondingtransaction\n\n��s are not in xip_list.\n\nThe first row (�,�) is invisible because it is inserted by a transaction that appears in xip_list (even though this transaction falls into the snapshot range).\n\nThe latest version of the second row (�,�) is invisible because the corresponding transaction �� is above the upper boundary of the snapshot.\n\nBut the first version of the second row (�,�) is visible: row insertion was performed by a transaction that falls into the snapshot range and does not appear in xip_list (the insertion is visible),while row deletion was performed by a transaction whose �� is above the upper boundary of the snapshot (the deletion is invisible).\n\n=> COMMIT;\n\n4.4 Visibility of Transactions’Own Changes\n\nThings get a bit more complicated when it comes to defining visibility rules for transactions’ own changes: in some cases, only part of such changes must be vis- ible. For example, a cursor that was opened at a particular point in time must not see any changes that happened later, regardless of the isolation level.\n\n100",
      "content_length": 1697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "4.4 Visibility of Transactions’ Own Changes\n\nTo address such situations, tuple headers provide a special field (displayed as cmin and cmax pseudocolumns) that shows the sequence number of the operation within the transaction. The cmin column identifies insertion, while cmax is used for deletion operations. To save space, these values are stored in a single field of the tuple header rather than in two different ones. It is assumed that one and the same row almost never gets both inserted and deleted within a single transaction. (If it does happen, Postgre��� writes a special combo identifier into this field, and the actual cmin and cmax values are stored by the backend in this case.1)\n\nAs an illustration, let’s start a transaction and insert a row into the table:\n\n=> BEGIN;\n\n=> INSERT INTO accounts VALUES (3, 'charlie', 100.00);\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n793\n\n(1 row)\n\nOpen a cursor to run the query that returns the number of rows in this table:\n\n=> DECLARE c CURSOR FOR SELECT count(*) FROM accounts;\n\nInsert one more row:\n\n=> INSERT INTO accounts VALUES (4, 'charlie', 200.00);\n\nNow extend the output by another column to display the cmin value for the rows inserted by our transaction (it makes no sense for other rows):\n\n=> SELECT xmin, CASE WHEN xmin = 793 THEN cmin END cmin, * FROM accounts;\n\nxmin | cmin | id | client\n\n| amount\n\n−−−−−−+−−−−−−+−−−−+−−−−−−−−−+−−−−−−−−−\n\n790 | 792 | 793 | 793 | (4 rows)\n\n| | 0 | 1 |\n\n| 1000.00 1 | alice 200.00 | 2 | bob 100.00 3 | charlie | 200.00 4 | charlie |\n\n1 backend/utils/time/combocid.c\n\n101",
      "content_length": 1601,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "Chapter 4 Snapshots\n\nThe cursor query gets only three rows; the row inserted when the cursor was al- ready open does not make it into the snapshot because the cmin < 1 condition is not satisfied:\n\n=> FETCH c;\n\ncount −−−−−−−\n\n3\n\n(1 row)\n\nNaturally, this cmin number is also stored in the snapshot, but it is impossible to display it using any ��� means.\n\n4.5 Transaction Horizon\n\nAs mentioned earlier, the lower boundary of the snapshot is represented by xmin, which is the �� of the oldest transaction that was active at the moment of the snap- shot creation. This value is very important because it defines the horizon of the transaction that uses this snapshot.\n\nIf a transaction has no active snapshot (for example, at the Read Committed isola- tion level between statement execution), its horizon is defined by its own �� if it is assigned.\n\nAll the transactions that are beyond the horizon (those with xid < xmin) are gu- ranteed to be committed. It means that a transaction can see only the current row versions beyond its horizon.\n\nAs you can guess, this term is inspired by the concept of event horizon in physics.\n\nPostgre��� tracks the current horizons of all its processes; transactions can see their own horizons in the pg_stat_activity table:\n\n=> BEGIN;\n\n=> SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid();\n\nbackend_xmin −−−−−−−−−−−−−−\n\n793\n\n(1 row)\n\n102",
      "content_length": 1390,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "4.5 Transaction Horizon\n\nVirtual transactions have no real ��s, but they still use snapshots just like regular transactions, so they have their own horizons. The only exception is virtual trans- actions without an active snapshot: the concept of the horizon makes no sense for them, and they are fully “transparent” to the system when it comes to snapshots and visibility (even though pg_stat_activity.backend_xmin may still contain an xmin of an old snapshot).\n\nWe can also define the database horizon in a similar manner. For this purpose, we should take the horizons of all the transactions in this database and select the most remote one, which has the oldest xmin.1 Beyond this horizon, outdated heap tuples will never be visible to any transaction in this database. Such tuples can be safely cleaned up by vacuum—this is exactly why the concept of the horizon is so important from a practical standpoint.\n\ndatabase horizon\n\noutdated tuples that can be vacuumed\n\nxid\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\nLet’s draw some conclusions:\n\nIf a transaction (no matter whether it is real or virtual) at the Repeatable Read or Serializable isolation level is running for a long time, it thereby holds the database horizon and defers vacuuming.\n\n1 backend/storage/ipc/procarray.c, ComputeXidHorizons function\n\n103",
      "content_length": 1304,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Chapter 4 Snapshots\n\nA real transaction at the Read Committed isolation level holds the database horizon in the same way,even if it is not executing any operators (being in the “idle in transaction” state).\n\nA virtual transaction at the Read Committed isolation level holds the horizon\n\nonly while executing operators.\n\nThere is only one horizon for the whole database, so if it is being held by a trans- action, it is impossible to vacuum any data within this horizon—even if this data has not been accessed by this transaction.\n\nCluster-wide tables of the system catalog have a separate horizon that takes into account all transactions in all databases. Temporary tables, on the contrary, do not have to pay attention to any transactions except those that are being executed by the current process.\n\nLet’s get back to our current experiment. The active transaction of the first session still holds the database horizon; we can see it by incrementing the transaction counter:\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n794\n\n(1 row)\n\n=> SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid();\n\nbackend_xmin −−−−−−−−−−−−−−\n\n793\n\n(1 row)\n\nAnd only when this transaction is complete, the horizon moves forward, and out- dated tuples can be vacuumed:\n\n=> COMMIT;\n\n=> SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid();\n\nbackend_xmin −−−−−−−−−−−−−−\n\n795\n\n(1 row)\n\n104",
      "content_length": 1430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "4.6 System Catalog Snapshots\n\nIn a perfect world, you should avoid combining long transactions with frequent updates (that spawn new row versions), as it will lead to table and index bloating.\n\n4.6 System Catalog Snapshots\n\nAlthough the system catalog consists of regular tables, they cannot be accessed via a snapshot used by a transaction or an operator. The snapshot must be “fresh” enoughtoincludeallthelatestchanges,otherwisetransactionscouldseeoutdated definitions of table columns or miss newly added integrity constraints.\n\nHere is a simple example:\n\n=> BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT 1; -- a snapshot for the transaction is taken\n\n=> ALTER TABLE accounts\n\nALTER amount SET NOT NULL;\n\n=> INSERT INTO accounts(client, amount)\n\nVALUES ('alice', NULL);\n\nERROR: violates not−null constraint DETAIL:\n\nnull value in column \"amount\" of relation \"accounts\"\n\nFailing row contains (1, alice, null).\n\n=> ROLLBACK;\n\nThe integrity constraint that appeared after the snapshot creation was visible to the ������ command. It may seem that such behavior breaks isolation, but if the inserting transaction had managed to reach the accounts table before the ����� ��- ��� command,the latter would have been blocked until this transaction completed.\n\nIn general, the server behaves as if a separate snapshot is created for each system catalog query. But the implementation is, of course, much more complex1 since frequent snapshot creation would negatively affect performance; besides, many system catalog objects get cached, and it must also be taken into account.\n\n1 backend/utils/time/snapmgr.c, GetCatalogSnapshot function\n\n105\n\np. ���\n\np. ���",
      "content_length": 1666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Chapter 4 Snapshots\n\n4.7 Exporting Snapshots\n\nIn some situations, concurrent transactions must see one and the same snapshot by all means. For example, if the pg_dump utility is run in the parallel mode, all its processes must see the same database state to produce a consistent backup.\n\nWe cannot assume that snapshots will be identical simply because transactions were started “simultaneously.” To ensure that all the transactions see the same data, we must employ the snapshot export mechanism.\n\nThe pg_export_snapshot function returns a snapshot ��, which can be passed to another transaction (outside of the database system):\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT count(*) FROM accounts;\n\ncount −−−−−−−\n\n4\n\n(1 row)\n\n=> SELECT pg_export_snapshot();\n\npg_export_snapshot −−−−−−−−−−−−−−−−−−−−− 00000004−0000006E−1\n\n(1 row)\n\nBeforeexecutingthefirststatement,theothertransactioncanimportthesnapshot by running the �������������� �������� command. The isolation level must be set to Repeatable Read or Serializable because operators use their own snapshots at the Read Committed level:\n\n=> DELETE FROM accounts;\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SET TRANSACTION SNAPSHOT '00000004-0000006E-1';\n\nNow the second transaction is going to use the snapshot of the first transaction, and consequently, it will see four rows (instead of zero):\n\n106",
      "content_length": 1367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "4.7 Exporting Snapshots\n\n=> SELECT count(*) FROM accounts;\n\ncount −−−−−−−\n\n4\n\n(1 row)\n\nClearly, the second transaction will not see any changes made by the first transac- tion after the snapshot export (and vice versa): regular visibility rules still apply.\n\nThe exported snapshot’s lifetime is the same as that of the exporting transaction.\n\n=> COMMIT;\n\n=> COMMIT;\n\n107",
      "content_length": 370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "100\n\np. ���\n\np. ��\n\n5\n\nPage Pruning and HOT Updates\n\n5.1 Page Pruning\n\nWhile a heap page is being read or updated, Postgre��� can perform some quick page cleanup, or pruning.1 It happens in the following cases:\n\nThe previous ������ operation did not find enough space to place a new tuple\n\ninto the same page. This event is reflected in the page header.\n\nThe heap page contains more data than allowed by the\n\nfillfactor storage pa-\n\nrameter.\n\nAn ������ operation can add a new row into the page only if this page is filled for less than fillfactor percent. The rest of the space is kept for ������ opera- tions (no such space is reserved by default).\n\nPage pruning removes the tuples that cannot be visible in any snapshot anymore (that is, that are beyond the database horizon ). It never goes beyond a single heap page, but in return it is performed very fast. Pointers to pruned tuples remain in place since they may be referenced from an index—which is already a different page.\n\nFor the same reason, neither the visibility map nor the free space map is refreshed (so the recovered space is set aside for updates, not for insertions).\n\nSince a page can be pruned during reads, any ������ statement can cause page modifications. This is yet another such case in addition to deferred setting of in- formation bits.\n\n1 backend/access/heap/pruneheap.c, heap_page_prune_opt function\n\n108",
      "content_length": 1386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "5.1 Page Pruning\n\nLet’s take a look at how page pruning actually works. We are going to create a two-column table and build an index on each of the columns:\n\n=> CREATE TABLE hot(id integer, s char(2000)) WITH (fillfactor = 75);\n\n=> CREATE INDEX hot_id ON hot(id);\n\n=> CREATE INDEX hot_s ON hot(s);\n\nIf the s column contains only Latin letters, each heap tuple will have a fixed size of ���� bytes, plus �� bytes of the header. The fillfactor storage parameter is set to ��%. It means that the page has enough free space for four tuples, but we can insert only three.\n\nLet’s insert a new row and update it several times:\n\n=> INSERT INTO hot VALUES (1, 'A');\n\n=> UPDATE hot SET s = 'B';\n\n=> UPDATE hot SET s = 'C';\n\n=> UPDATE hot SET s = 'D';\n\nNow the page contains four tuples:\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−− (0,1) | normal | 801 c | 802 c (0,2) | normal | 802 c | 803 c (0,3) | normal | 803 c | 804 | 0 a (0,4) | normal | 804\n\n(4 rows)\n\nExpectedly, we have just exceeded the fillfactor threshold. You can tell it by the difference between the pagesize and upper values—it is bigger than ��% of the page size, which is ���� bytes:\n\n=> SELECT upper, pagesize FROM page_header(get_raw_page('hot',0));\n\nupper | pagesize −−−−−−−+−−−−−−−−−−\n\n64 |\n\n8192\n\n(1 row)\n\nThe next page access triggers page pruning that removes all the outdated tuples. Then a new tuple (�,�) is added into the freed space:\n\n109\n\np. ��",
      "content_length": 1475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Chapter 5 Page Pruning and HOT Updates\n\n=> UPDATE hot SET s = 'E';\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−\n\n(0,1) | dead (0,2) | dead (0,3) | dead (0,4) | normal | 804 c | 805 | 0 a (0,5) | normal | 805\n\n| | |\n\n| | |\n\n(5 rows)\n\nThe remaining heap tuples are physically moved towards the highest addresses so that all the free space is aggregated into a single continuous chunk. The tuple pointers are also modified accordingly. As a result, there is no free space fragmen- tation in the page.\n\nThe pointers to the pruned tuples cannot be removed yet because they are still ref- erenced from the indexes; Postgre��� changes their status from normal to dead. Let’s take a look at the first page of the hot_s index (the zero page is used for meta- data):\n\n=> SELECT * FROM index_page('hot_s',1);\n\nitemoffset | htid −−−−−−−−−−−−+−−−−−−−\n\n1 | (0,1) 2 | (0,2) 3 | (0,3) 4 | (0,4) 5 | (0,5)\n\n(5 rows)\n\nWe can see the same picture in the other index too:\n\n=> SELECT * FROM index_page('hot_id',1);\n\nitemoffset | htid −−−−−−−−−−−−+−−−−−−−\n\n1 | (0,1) 2 | (0,2) 3 | (0,3) 4 | (0,4) 5 | (0,5)\n\n(5 rows)\n\n110",
      "content_length": 1156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "5.1 Page Pruning\n\nAn index scan can return (�,�), (�,�), and (�,�) as tuple identifiers. The server tries to read the corresponding heap tuple but sees that the pointer has the dead status; it means that this tuple does not exist anymore and should be ignored. And while being at it, the server also changes the pointer status in the index page to avoid repeated heap page access.1\n\nLet’s extend the function pointer is dead:\n\ndisplaying index pages so that it also shows whether the\n\n=> DROP FUNCTION index_page(text, integer);\n\n=> CREATE FUNCTION index_page(relname text, pageno integer) RETURNS TABLE(itemoffset smallint, htid tid, dead boolean) AS $$ SELECT itemoffset, htid, dead -- starting from v.13\n\nFROM bt_page_items(relname,pageno); $$ LANGUAGE sql;\n\n=> SELECT * FROM index_page('hot_id',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,1) | f 2 | (0,2) | f 3 | (0,3) | f 4 | (0,4) | f 5 | (0,5) | f\n\n(5 rows)\n\nAll the pointers in the index page are active so far. But as soon as the first index scan occurs, their status changes:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM hot WHERE id = 1;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using hot_id on hot (actual rows=1 loops=1)\n\nIndex Cond: (id = 1)\n\n(2 rows)\n\n1 backend/access/index/indexam.c, index_fetch_heap function\n\n111\n\nv. ��",
      "content_length": 1379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "p. ���\n\nChapter 5 Page Pruning and HOT Updates\n\n=> SELECT * FROM index_page('hot_id',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,1) | t 2 | (0,2) | t 3 | (0,3) | t 4 | (0,4) | t 5 | (0,5) | f\n\n(5 rows)\n\nAlthough the heap tuple referenced by the fourth pointer is still unpruned and has the normal status, it is already beyond the database horizon. That’s why this pointer is also marked as dead in the index.\n\n5.2 HOT Updates\n\nIt would be very inefficient to keep references to all heap tuples in an index.\n\nTo begin with,each row modification triggers updates of all the indexes created on the table: once a new heap tuple appears, each index must include a reference to this tuple, even if the modified fields are not indexed.\n\nFurthermore, indexes accumulate references to historic heap tuples, so they have to be pruned together with these tuples.\n\nThings get worse as you create more indexes on a table.\n\nBut if the updated column is not a part of any index, there is no point in creating another index entry that contains the same key value. To avoid such redundancies, Postgre��� provides an optimization called Heap-Only Tuple updates.1\n\nIf such an update is performed,an index page contains only one entry for each row. This entry points to the very first row version; all the subsequent versions located in the same page are bound into a chain by ctid pointers in the tuple headers.\n\nRow versions that are not referenced from any index are tagged with the Heap-Only Tuple bit. If a version is included into the ��� chain, it is tagged with the Heap Hot Updated bit.\n\n1 backend/access/heap/README.HOT\n\n112",
      "content_length": 1638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "5.2 HOT Updates\n\nIf an index scan accesses a heap page and finds a row version marked as Heap Hot Updated,it means that the scan should continue,so it goes further along the chain of ��� updates. Obviously, all the fetched row versions are checked for visibility before the result is returned to the client.\n\nTo take a look at how ��� updates are performed, let’s delete one of the indexes and truncate the table.\n\n=> DROP INDEX hot_s;\n\n=> TRUNCATE TABLE hot;\n\nFor convenience, we will redefine the heap_page function so that its output in- cludes three more fields: ctid and the two bits related to ��� updates:\n\n=> DROP FUNCTION heap_page(text,integer);\n\n=> CREATE FUNCTION heap_page(relname text, pageno integer) RETURNS TABLE(\n\nctid tid, state text, xmin text, xmax text, hhu text, hot text, t_ctid tid\n\n) AS $$ SELECT (pageno,lp)::text::tid AS ctid,\n\nCASE lp_flags\n\nWHEN 0 THEN 'unused' WHEN 1 THEN 'normal' WHEN 2 THEN 'redirect to '||lp_off WHEN 3 THEN 'dead'\n\nEND AS state, t_xmin || CASE\n\nWHEN (t_infomask & 256) > 0 THEN ' c' WHEN (t_infomask & 512) > 0 THEN ' a' ELSE '' END AS xmin, t_xmax || CASE\n\nWHEN (t_infomask & 1024) > 0 THEN ' c' WHEN (t_infomask & 2048) > 0 THEN ' a' ELSE '' END AS xmax, CASE WHEN (t_infomask2 & 16384) > 0 THEN 't' END AS hhu, CASE WHEN (t_infomask2 & 32768) > 0 THEN 't' END AS hot, t_ctid\n\nFROM heap_page_items(get_raw_page(relname,pageno)) ORDER BY lp; $$ LANGUAGE sql;\n\n113",
      "content_length": 1417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "Chapter 5 Page Pruning and HOT Updates\n\nLet’s repeat the insert and update operations:\n\n=> INSERT INTO hot VALUES (1, 'A');\n\n=> UPDATE hot SET s = 'B';\n\nThe page now contains a chain of ��� updates:\n\nThe Heap Hot Updated bit shows that the executor should follow the ����\n\nchain.\n\nThe Heap Only Tuple bit indicates that this tuple is not referenced from any\n\nindexes.\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax | hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | normal | 812 c | 813 | 0 a (0,2) | normal | 813\n\n| t |\n\n| | t\n\n| (0,2) | (0,2)\n\n(2 rows)\n\nAs we make further updates, the chain will grow—but only within the page limits:\n\n=> UPDATE hot SET s = 'C';\n\n=> UPDATE hot SET s = 'D';\n\n=> SELECT * FROM heap_page('hot',0);\n\n| xmax −−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\nctid\n\n| state\n\n| xmin\n\n| hhu | hot | t_ctid\n\n(0,1) | normal | 812 c | 813 c | t (0,2) | normal | 813 c | 814 c | t | t (0,3) | normal | 814 c | 815 | | 0 a (0,4) | normal | 815\n\n| | t | t | t\n\n| (0,2) | (0,3) | (0,4) | (0,4)\n\n(4 rows)\n\nThe index still contains only one reference, which points to the head of this chain:\n\n=> SELECT * FROM index_page('hot_id',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,1) | f\n\n(1 row)\n\n114",
      "content_length": 1296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "5.3 Page Pruning for HOT Updates\n\nA ��� update is possible if the modified fields are not a part of any index. Other- wise, some of the indexes would contain a reference to a heap tuple that appears in the middle of the chain, which contradicts the idea of this optimization. Since a ��� chain can grow only within a single page, traversing the whole chain never requires access to other pages and thus does not hamper performance.\n\n5.3 Page Pruning for HOT Updates\n\nA special case of page pruning—which is nevertheless important—is pruning of ��� update chains.\n\nIn the example above, the fillfactor threshold is already exceeded, so the next up- date should trigger page pruning. But this time the page contains a chain of ��� updates. The head of this chain must always remain in its place since it is refer- enced from the index, but other pointers can be released because they are sure to have no external references.\n\nToavoidmovingthehead,Postgre���usesdualaddressing: thepointerreferenced from the index (which is (�,�) in this case) receives the redirect status since it points to the tuple that currently starts the chain:\n\n=> UPDATE hot SET s = 'E';\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmax | hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | redirect to 4 | (0,2) | normal (0,3) | unused (0,4) | normal\n\n| | 0 a |\n\n| 816 | | 815 c | 816\n\n| | | | t\n\n| | t | | t\n\n| | (0,2) | | (0,2)\n\n(4 rows)\n\nThe tuples (�,�), (�,�), and (�,�) have been pruned; the head pointer � remains for redirection purposes, while pointers � and � have been deallocated (received the unused status) since they are guaranteed to have no references from indexes. The new tuple is written into the freed space as tuple (�,�).\n\n115",
      "content_length": 1780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Chapter 5 Page Pruning and HOT Updates\n\nLet’s perform some more updates:\n\n=> UPDATE hot SET s = 'F';\n\n=> UPDATE hot SET s = 'G';\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmax\n\n| hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | redirect to 4 | (0,2) | normal (0,3) | normal (0,4) | normal (0,5) | normal\n\n|\n\n|\n\n| 816 c | 817 c | t | t | 817 c | 818 | 815 c | 816 c | t | 818\n\n| 0 a\n\n|\n\n| | t | t | t | t\n\n| | (0,3) | (0,5) | (0,2) | (0,5)\n\n(5 rows)\n\nThe next update is going to trigger page pruning:\n\n=> UPDATE hot SET s = 'H';\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmax | hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | redirect to 5 | (0,2) | normal (0,3) | unused (0,4) | unused (0,5) | normal\n\n| | 0 a | |\n\n| 819 | | | 818 c | 819\n\n| | | | | t\n\n| | t | | | t\n\n| | (0,2) | | | (0,2)\n\n(5 rows)\n\nAgain, some of the tuples are pruned, and the pointer to the head of the chain is shifted accordingly.\n\nIf unindexed columns are modified frequently, it makes sense to reduce the fillfac- tor value, thus reserving some space in the page for updates. Obviously, you have to keep in mind that the lower the fillfactor value is, the more free space is left in the page, so the physical size of the table grows.\n\n116",
      "content_length": 1341,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "5.4 HOT Chain Splits\n\n5.4 HOT Chain Splits\n\nIf the page has no more space to accommodate a new tuple, the chain will be cut off. Postgre��� will have to add a separate index entry to refer to the tuple located in another page.\n\nTo observe this situation, let’s start a concurrent transaction with a snapshot that blocks page pruning:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT 1;\n\nNow we are going to perform some updates in the first session:\n\n=> UPDATE hot SET s = 'I';\n\n=> UPDATE hot SET s = 'J';\n\n=> UPDATE hot SET s = 'K';\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmax\n\n| hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | redirect to 2 | (0,2) | normal (0,3) | normal (0,4) | normal (0,5) | normal\n\n|\n\n|\n\n| 819 c | 820 c | t | 820 c | 821 c | t | t | 821 c | 822 | | 0 a | 822\n\n| | t | t | t | t\n\n| | (0,3) | (0,4) | (0,5) | (0,5)\n\n(5 rows)\n\nWhen the next update happens,this page will not be able to accommodate another tuple, and page pruning will not manage to free any space:\n\n=> UPDATE hot SET s = 'L';\n\n=> COMMIT; -- the snapshot is not required anymore\n\n117",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "Chapter 5 Page Pruning and HOT Updates\n\n=> SELECT * FROM heap_page('hot',0);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmax\n\n| hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | redirect to 2 | (0,2) | normal (0,3) | normal (0,4) | normal (0,5) | normal\n\n|\n\n|\n\n| 819 c | 820 c | t | 820 c | 821 c | t | 821 c | 822 c | t | 822 c | 823\n\n|\n\n| | t | t | t | t\n\n| | (0,3) | (0,4) | (0,5) | (1,1)\n\n(5 rows)\n\nTuple (�,�) contains the (�,�) reference that goes to page �:\n\n=> SELECT * FROM heap_page('hot',1);\n\nctid\n\n| state\n\n| xmin | xmax | hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−+−−−−−−+−−−−−−+−−−−−+−−−−−+−−−−−−−− | 0 a\n\n(1,1) | normal | 823\n\n|\n\n|\n\n| (1,1)\n\n(1 row)\n\nHowever, this reference is not used: the Heap Hot Updated bit is not set for tuple (�,�). As for tuple (�,�), it can be accessed from the index that now has two entries. Each of them points to the head of their own ��� chain:\n\n=> SELECT * FROM index_page('hot_id',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,1) | f 2 | (1,1) | f\n\n(2 rows)\n\n5.5 Page Pruning for Indexes\n\nI have declared that page pruning is confined to a single heap page and does not affect indexes. However, indexes have their own pruning,1 which also cleans up a single page—an index one in this case.\n\nIndex pruning happens when an insertion into a �-tree is about to split the page into two,as the original page does nothaveenough space anymore. The problem is that even if some index entries are deleted later, two separate index pages will not\n\n1 postgresql.org/docs/14/btree-implementation.html#BTREE-DELETION\n\n118",
      "content_length": 1596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "5.5 Page Pruning for Indexes\n\nbe merged into one. It leads to index bloating, and once bloated, the index cannot shrink even if a large part of the data is deleted. But if pruning can remove some of the tuples, a page split may be deferred.\n\nThere are two types of tuples that can be pruned from an index.\n\nFirst of all, Postgre��� prunes those tuples that have been tagged as dead.1 As I have already said, Postgre��� sets such a tag during an index scan if it detects an index entry pointing to a tuple that is not visible in any snapshot anymore or simply does not exist.\n\n, Postgre��� checks those index entries that ref- If no tuples are known to be dead erence different versions of one and the same table row.2 Because of ����, update operations may generate a large number of row versions, and many of them are soon likely to disappear behind the database horizon. H�� updates cushion this effect, but they are not always applicable: if the column to update is a part of an index,the corresponding references are propagated to all the indexes. Before split- ting the page, it makes sense to search for the rows that are not tagged as dead yet but can already be pruned. To achieve this, Postgre��� has to check visibility of heap tuples. Such checks require table access, so they are performed only for “promising” index tuples, which have been created as copies of the existing ones for ���� purposes. It is cheaper to perform such a check than to allow an extra page split.\n\n1 backend/access/nbtree/README, Simple deletion section 2 backend/access/nbtree/README, Bottom-Up deletion section\n\ninclude/access/tableam.h\n\n119\n\nv. ��",
      "content_length": 1637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "p. ���\n\np. ��\n\n6\n\nVacuum and Autovacuum\n\n6.1 Vacuum\n\nPage pruning happens very fast, but it frees only part of the space that can be po- tentially reclaimed. Operating within a single heap page, it does not touch upon indexes (or vice versa, it cleans up an index page without affecting the table).\n\nRoutine vacuuming,1 which is the main vacuuming procedure, is performed by the ������command.2 Itprocessesthewholetableandeliminatesbothoutdatedheap tuples and all the corresponding index entries.\n\nVacuuming is performed in parallel with other processes in the database system. While being vacuumed, tables and indexes can be used in the usual manner, both for read and write operations (but concurrent execution of such commands as ���- ��� �����, ����� �����, and some others is not allowed\n\n).\n\n. Pages tracked To avoid scanning extra pages, Postgre��� uses a visibility map in this map are skipped since they are sure to contain only the current tuples, so a page will only be vacuumed if it does not appear in this map. If all the tuples remaininginapageaftervacuumingarebeyondthedatabasehorizon,thevisibility map is refreshed to include this page.\n\nThe free space map also gets updated to reflect the space that has been cleared.\n\nLet’s create a table with an index on it:\n\n1 postgresql.org/docs/14/routine-vacuuming.html 2 postgresql.org/docs/14/sql-vacuum.html\n\nbackend/commands/vacuum.c\n\n120",
      "content_length": 1400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "6.1 Vacuum\n\n=> CREATE TABLE vac(\n\nid integer, s char(100)\n\n) WITH (autovacuum_enabled = off);\n\n=> CREATE INDEX vac_s ON vac(s);\n\nThe autovacuum_enabled storage parameter turns off autovacuum; we are doing it here solely for the purpose of experimentation to precisely control vacuuming start time.\n\nLet’s insert a row and make a couple of updates:\n\n=> INSERT INTO vac(id,s) VALUES (1,'A');\n\n=> UPDATE vac SET s = 'B';\n\n=> UPDATE vac SET s = 'C';\n\nNow the table contains three tuples:\n\n=> SELECT * FROM heap_page('vac',0);\n\n| xmax −−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\nctid\n\n| state\n\n| xmin\n\n| hhu | hot | t_ctid\n\n(0,1) | normal | 826 c | 827 c | | (0,2) | normal | 827 c | 828 | | 0 a (0,3) | normal | 828\n\n| | |\n\n| (0,2) | (0,3) | (0,3)\n\n(3 rows)\n\nEach tuple is referenced from the index:\n\n=> SELECT * FROM index_page('vac_s',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,1) | f 2 | (0,2) | f 3 | (0,3) | f\n\n(3 rows)\n\nVacuuming has removed all the dead tuples, leaving only the current one:\n\n=> VACUUM vac;\n\n121",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Chapter 6 Vacuum and Autovacuum\n\n=> SELECT * FROM heap_page('vac',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax | hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | unused | (0,2) | unused | (0,3) | normal | 828 c | 0 a\n\n| |\n\n| | |\n\n| | |\n\n| | | (0,3)\n\n(3 rows)\n\nIn the case of page pruning, the first two pointers would be considered dead, but here they have the unused status since no index entries are referring to them now:\n\n=> SELECT * FROM index_page('vac_s',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,3) | f\n\n(1 row)\n\nPointers with the unused status are treated as free and can be reused by new row versions.\n\nNow the heap page appears in the visibility map; we can check it using the pg_vis- ibility extension:\n\n=> CREATE EXTENSION pg_visibility;\n\n=> SELECT all_visible FROM pg_visibility_map('vac',0);\n\nall_visible −−−−−−−−−−−−−\n\nt\n\n(1 row)\n\nThepageheaderhasalsoreceivedanattributeshowingthatallitstuplesarevisible in all snapshots:\n\n=> SELECT flags & 4 > 0 AS all_visible FROM page_header(get_raw_page('vac',0));\n\nall_visible −−−−−−−−−−−−−\n\nt\n\n(1 row)\n\n122",
      "content_length": 1111,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "6.2 Database Horizon Revisited\n\n6.2 Database Horizon Revisited\n\nVacuuming detects dead tuples based on the database horizon. This concept is so fundamental that it makes sense to get back to it once again.\n\nLet’s restart our experiment from the very beginning:\n\n=> TRUNCATE vac;\n\n=> INSERT INTO vac(id,s) VALUES (1,'A');\n\n=> UPDATE vac SET s = 'B';\n\nBut this time, before updating the row, we are going to open another transaction that will hold the database horizon (it can be almost any transaction , except for a virtual one executed at the Read Committed isolation level). For example, this transaction can modify some rows in another table.\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = 0;\n\n=> UPDATE vac SET s = 'C';\n\nNow our table contains three tuples,and the index contains three references. Let’s vacuum the table and see what changes:\n\n=> VACUUM vac;\n\n=> SELECT * FROM heap_page('vac',0);\n\n| xmax −−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\nctid\n\n| state\n\n| xmin\n\n| hhu | hot | t_ctid\n\n(0,1) | unused | | (0,2) | normal | 833 c | 835 c | | (0,3) | normal | 835 c | 0 a\n\n|\n\n| | |\n\n| | (0,3) | (0,3)\n\n(3 rows)\n\n=> SELECT * FROM index_page('vac_s',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,2) | f 2 | (0,3) | f\n\n(2 rows)\n\n123\n\np. ���",
      "content_length": 1276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Chapter 6 Vacuum and Autovacuum\n\nWhile the previous run left only one tuple in the page, now we have two of them: ������ has decided that version (�,�) cannot be removed yet. The reason is the database horizon, which is defined by an unfinished transaction in this case:\n\n=> SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid();\n\nbackend_xmin −−−−−−−−−−−−−−\n\n834\n\n(1 row)\n\nWe can use the ������� clause when calling ������ to observe what is going on:\n\n=> VACUUM VERBOSE vac;\n\nINFO: INFO: in 1 out of 1 pages DETAIL: Skipped 0 pages due to buffer pins, 0 frozen pages. CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s. VACUUM\n\nvacuuming \"public.vac\" table \"vac\": found 0 removable, 2 nonremovable row versions\n\n1 dead row versions cannot be removed yet, oldest xmin: 834\n\nThe output shows the following information:\n\n������ has detected no tuples that can be removed (0 ���������).\n\nTwo tuples must not be removed (2 ������������).\n\nOne of the nonremovable tuples is dead (1 ����), the other is in use.\n\nThe current horizon respected by ������ (������ ����) is the horizon of the\n\nactive transaction.\n\nOnce the active transaction completes, the database horizon moves forward, and vacuuming can continue:\n\n=> COMMIT;\n\n124",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "6.2 Database Horizon Revisited\n\n=> VACUUM VERBOSE vac;\n\nINFO: INFO: DETAIL: INFO: DETAIL: INFO: DETAIL: 0 index pages were newly deleted. 0 index pages are currently deleted, of which 0 are currently reusable. CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s. INFO: in 1 out of 1 pages DETAIL: Skipped 0 pages due to buffer pins, 0 frozen pages. CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s. VACUUM\n\nvacuuming \"public.vac\" scanned index \"vac_s\" to remove 1 row versions\n\nCPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s table \"vac\": removed 1 dead item identifiers in 1 pages CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s index \"vac_s\" now contains 1 row versions in 2 pages\n\n1 index row versions were removed.\n\ntable \"vac\": found 1 removable, 1 nonremovable row versions\n\n0 dead row versions cannot be removed yet, oldest xmin: 836\n\n������ has detected and removed a dead tuple beyond the new database horizon.\n\nNow the page contains no outdated row versions; the only version remaining is the current one:\n\n=> SELECT * FROM heap_page('vac',0);\n\nctid\n\n| state\n\n| xmin\n\n| xmax | hhu | hot | t_ctid\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−+−−−−−+−−−−−+−−−−−−−−\n\n(0,1) | unused | (0,2) | unused | (0,3) | normal | 835 c | 0 a\n\n| |\n\n| | |\n\n| | |\n\n| | | (0,3)\n\n(3 rows)\n\nThe index also contains only one entry:\n\n=> SELECT * FROM index_page('vac_s',1);\n\nitemoffset | htid\n\n| dead\n\n−−−−−−−−−−−−+−−−−−−−+−−−−−−\n\n1 | (0,3) | f\n\n(1 row)\n\n125",
      "content_length": 1439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "64MB\n\nChapter 6 Vacuum and Autovacuum\n\n6.3 Vacuum Phases\n\nThe mechanism of vacuuming seems quite simple, but this impression is mislead- ing. After all, both tables and indexes have to be processed concurrently, without blocking other processes. To enable such operation, vacuuming of each table is carried out in several phases.1\n\nIt all starts with scanning a table in search of dead tuples; if found, they are first removed from indexes and then from the table itself. If too many dead tuples have to be vacuumed in one go, this process is repeated. Eventually, heap truncation may be performed.\n\nHeap Scan\n\nIn the first phase, a heap scan is performed.2 The scanning process takes the vis- ibility map into account: all pages tracked in this map are skipped because they are sure to contain no outdated tuples. If a tuple is beyond the horizon and is not required anymore, its �� is added to a special tid array. Such tuples cannot be re- moved yet because they may still be referenced from indexes.\n\nThe tid array resides in the local memory of the ������ process; the size of the maintenance_work_mem parameter. The allocated memory chunk is defined by the whole chunk is allocated at once rather than on demand. However, the allocated memory never exceeds the volume required in the worst-case scenario, so if the table is small, vacuuming may use less memory than specified in this parameter.\n\nIndex Vacuuming\n\nThe first phase can have two outcomes: either the table is scanned in full, or the memory allocated for the tid array is filled up before this operation completes. In any case, index vacuuming begins.3 In this phase, each of the indexes created on\n\n1 backend/access/heap/vacuumlazy.c, heap_vacuum_rel function 2 backend/access/heap/vacuumlazy.c, lazy_scan_heap function 3 backend/access/heap/vacuumlazy.c, lazy_vacuum_all_indexes function\n\n126",
      "content_length": 1862,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "6.3 Vacuum Phases\n\nthe table is fully scanned to find all the entries that refer to the tuples registered in the tid array. These entries are removed from index pages.\n\nAn index can help you quickly get to a heap tuple by its index key, but there is no way to quickly find an index entry by the corresponding tuple ��. This functionality is currently being implemented for �-trees,1 but this work is not completed yet.\n\nmin_parallel_index_scan_size value,they If there are several indexes bigger than the can be vacuumed by background workers running in parallel. Unless the level of parallelism is explicitly defined by the parallel N clause, ������ launches one worker per suitable index (within the general limits imposed on the number of background workers).2 One index cannot be processed by several workers.\n\nDuring the index vacuuming phase, Postgre��� updates the free space map and calculates statistics on vacuuming. However, this phase is skipped if rows are only inserted (and are neither deleted nor updated) because the table contains no dead tuples in this case. Then an index scan will be forced only once at the very end, as part of a separate phase of index cleanup.3\n\nThe index vacuuming phase leaves no references to outdated heap tuples in in- dexes,butthetuplesthemselvesarestillpresentinthetable. Itisperfectlynormal: index scans cannot find any dead tuples, while sequential scans of the table rely on visibility rules to filter them out.\n\nHeap Vacuuming\n\nThen the heap vacuuming phase begins.4 The table is scanned again to remove the tuples registered in the tid array and free the corresponding pointers. Now that all the related index references have been removed, it can be done safely.\n\nThe space recovered by ������ is reflected in the free space map, while the pages that now contain only the current tuples visible in all snapshots are tagged in the visibility map.\n\n1 commitfest.postgresql.org/21/1802 2 postgresql.org/docs/14/bgworker.html 3 backend/access/heap/vacuumlazy.c, lazy_cleanup_all_indexes function\n\nbackend/access/nbtree/nbtree.c, btvacuumcleanup function 4 backend/access/heap/vacuumlazy.c, lazy_vacuum_heap function\n\n127\n\n512kB v. ��",
      "content_length": 2182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "p. ���\n\nv. ��\n\np. ���\n\nChapter 6 Vacuum and Autovacuum\n\nIf the table was not read in full during the heap scan phase, the tid array is cleared, and the heap scan is resumed from where it left off last time.\n\nHeap Truncation\n\nVacuumed heap pages contain some free space; occasionally, you may be lucky to clear the whole page. If you get several empty pages at the end of the file, vacuum- ing can “bite off” this tail and return the reclaimed space to the operating system. It happens during heap truncation,1 which is the final vacuum phase.\n\nHeaptruncationrequiresashortexclusive processes for too long, attempts to acquire a lock do not exceed five seconds.\n\nlockonthetable. Toavoidholdingother\n\nSincethetablehastobelocked,truncationisonlyperformediftheemptytailtakes at least 1 of the table or has reached the length of �,��� pages. These thresholds 16 are hardcoded and cannot be configured.\n\nIf,despite all these precautions,table locks still cause any issues ,truncation can be disabled altogether using the vacuum_truncate and toast.vacuum_truncate storage parameters.\n\n6.4 Analysis\n\nWhen talking about vacuuming, we have to mention yet another task that is closely related to it, even though there is no formal connection between them. It is analysis,2 or gathering statistical information for the query planner. The collected statistics include the number of rows (pg_class.reltuples) and pages (pg_class.relpages) in relations, data distribution within columns, and some other information.\n\nYou can run the analysis manually using the������� command,3 or combine it with vacuuming by calling ������ �������. However, these two tasks are still performed sequentially, so there is no difference in terms of performance.\n\n1 backend/access/heap/vacuumlazy.c, lazy_truncate_heap function 2 postgresql.org/docs/14/routine-vacuuming.html#VACUUM-FOR-STATISTICS 3 backend/commands/analyze.c\n\n128",
      "content_length": 1897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "6.5 Automatic Vacuum and Analysis\n\nHistorically, ������ ������� appeared first, in version �.�, while a separate ������� com- mand was not implemented until version �.�. In earlier versions, statistics were collected by a ��� script.\n\nAutomatic vacuum and analysis are set up in a similar way, so it makes sense to discuss them together.\n\n6.5 Automatic Vacuum and Analysis\n\nUnless the database horizon is held up for a long time, routine vacuuming should cope with its work. But how often do we need to call the ������ command?\n\nIf a frequently updated table is vacuumed too seldom, it will grow bigger than de- sired. Besides,it may accumulate too many changes,and then the next������ run will have to make several passes over the indexes.\n\nIfthetableisvacuumedtoooften,theserverwillbebusywithmaintenanceinstead of useful work.\n\nFurthermore,typicalworkloadsmaychangeovertime,sohavingafixedvacuuming schedule will not help anyway: the more often the table is updated,the more often it has to be vacuumed.\n\nThis problem is solved by autovacuum,1 which launches vacuum and analysis pro- cesses based on the intensity of table updates.\n\nAbout the Autovacuum Mechanism\n\nautovacuum configuration parameter is on), the au- When autovacuum is enabled ( tovacuum launcher process is always running in the system. This process defines the autovacuum schedule and maintains the list of“active”databases based on us- track_counts parameter is enabled. age statistics. Such statistics are collected if the Do not switch off these parameters, otherwise autovacuum will not work.\n\n1 postgresql.org/docs/14/routine-vacuuming.html#AUTOVACUUM\n\n129\n\non\n\non",
      "content_length": 1638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "1min\n\n3\n\nChapter 6 Vacuum and Autovacuum\n\nautovacuum_naptime, the autovacuum launcher starts an autovacuum Once in worker1 for each active database in the list (these workers are spawned by post- master, as usual). Consequently, if there are N active databases in the cluster, N workers are spawned within the autovacuum_naptime interval. But the total num- berofautovacuumworkersrunninginparallelcannotexceedthethresholddefined by the\n\nautovacuum_max_workers parameter.\n\nAutovacuum workers are very similar to regular background workers, but they appeared much earlier than this general mechanism of task management. It was decided to leave the autovacuum implementation unchanged, so autovacuum workers do not use max_worker_processes slots.\n\nOnce started,the background worker connects to the specified database and builds two lists:\n\nthe list of all tables, materialized views, and ����� tables to be vacuumed\n\nthe list of all tables and materialized views to be analyzed (����� tables are\n\nnot analyzed because they are always accessed via an index)\n\nThen the selected objects are vacuumed or analyzed one by one (or undergo both operations), and once the job is complete, the worker is terminated.\n\nAutomatic vacuuming works similar to the manual one initiated by the ������ command, but there are some nuances:\n\nManual vacuuming accumulates tuple ��s in a memory chunk of the mainte- nance_work_mem size. However, using the same limit for autovacuum is un- desirable, as it can result in excessive memory consumption: there may be several autovacuum workers running in parallel, and each of them will get maintenance_work_mem of memory at once. Instead, Postgre��� provides a separate memory limit for autovacuum processes, which is defined by the au- tovacuum_work_mem parameter. autovacuum_work_mem parameter falls back on the regular By default, the maintenance_work_mem limit, so if the autovacuum_max_workers value is high, you may have to adjust the autovacuum_work_mem value accordingly.\n\nManual vacuuming accumulates tuple ��s in a memory chunk of the mainte- nance_work_mem size. However, using the same limit for autovacuum is un- desirable, as it can result in excessive memory consumption: there may be several autovacuum workers running in parallel, and each of them will get maintenance_work_mem of memory at once. Instead, Postgre��� provides a separate memory limit for autovacuum processes, which is defined by the au- tovacuum_work_mem parameter. autovacuum_work_mem parameter falls back on the regular By default, the maintenance_work_mem limit, so if the autovacuum_max_workers value is high, you may have to adjust the autovacuum_work_mem value accordingly.\n\n1 backend/postmaster/autovacuum.c\n\n130",
      "content_length": 2725,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "6.5 Automatic Vacuum and Analysis\n\nConcurrent processing of several indexes created on one table can be per- formed only by manual vacuuming; using autovacuum for this purpose would result in a large number of parallel processes, so it is not allowed.\n\nIfaworkerfailstocompleteallthescheduledtaskswithintheautovacuum_naptime interval, the autovacuum launcher spawns another worker to be run in parallel in that database. The second worker will build its own lists of objects to be vacuumed and analyzed and will start processing them. There is no parallelism at the table level; only different tables can be processed concurrently.\n\nWhich Tables Need to be Vacuumed?\n\nYou can disable autovacuum at the table level—although it is hard to imagine why it could be necessary. There are two storage parameters provided for this purpose, one for regular tables and the other for ����� tables:\n\nautovacuum_enabled\n\ntoast.autovacuum_enabled\n\nIn usual circumstances, autovacuum is triggered either by tuples or by insertion of new rows.\n\naccumulation of dead\n\nDead tuple accumulation. Dead tuples are constantly being counted by the statis- tics collector; their current number is shown in the system catalog table called pg_stat_all_tables.\n\nIt is assumed that dead tuples have to be vacuumed if they exceed the threshold defined by the following two parameters:\n\n\n\nautovacuum_vacuum_threshold, which specifies the number of dead tuples (an absolute value)\n\n\n\nautovacuum_vacuum_scale_factor, which sets the fraction of dead tuples in a table\n\n131\n\np. ���\n\n50\n\n0.2",
      "content_length": 1555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "v. ��\n\np. ��� p. ���\n\n1000\n\n0.2\n\nChapter 6 Vacuum and Autovacuum\n\nVacuuming is required if the following condition is satisfied:\n\npg_stat_all_tables.n_dead_tup >\n\nautovacuum_vacuum_threshold +\n\nautovacuum_vacuum_scale_factor × pg_class.reltuples\n\nThe main parameter here is of course autovacuum_vacuum_scale_factor: its value is important for large tables (and it is large tables that are likely to cause the majority of issues). The default value of ��% seems too big and may have to be significantly reduced.\n\nFor different tables, optimal parameter values may vary: they largely depend on the table size and workload type. It makes sense to set more or less adequate initial values and then override them for particular tables using storage parameters:\n\nautovacuum_vacuum_threshold and toast.autovacuum_vacuum_threshold\n\nautovacuum_vacuum_scale_factor and toast.autovacuum_vacuum_scale_factor\n\nRow insertions. If rows are only inserted and are neither deleted nor updated, the table contains no dead tuples. But such tables should also be vacuumed to freeze and update the visibility map (thus enabling index-only heap tuples in advance scans\n\n).\n\nA table will be vacuumed if the number of rows inserted since the previous vacu- uming exceeds the threshold defined by another similar pair of parameters:\n\n\n\nautovacuum_vacuum_insert_threshold\n\n\n\nautovacuum_vacuum_insert_scale_factor\n\nThe formula is as follows:\n\npg_stat_all_tables.n_ins_since_vacuum >\n\nautovacuum_vacuum_insert_threshold +\n\nautovacuum_vacuum_insert_scale_factor × pg_class.reltuples\n\n132",
      "content_length": 1557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "6.5 Automatic Vacuum and Analysis\n\nLike in the previous example,you can override these values at the table level using storage parameters:\n\nautovacuum_vacuum_insert_threshold and its ����� counterpart\n\nautovacuum_vacuum_insert_scale_factor and its ����� counterpart\n\nWhich Tables Need to Be Analyzed?\n\nAutomatic analysis needs to process only modified rows, so the calculations are a bit simpler than those for autovacuum.\n\nIt is assumed that a table has to be analyzed if the number of rows modified since the previous analysis exceeds the threshold defined by the following two configu- ration parameters:\n\n\n\nautovacuum_analyze_threshold\n\n\n\nautovacuum_analyze_scale_factor\n\nAutoanalysis is triggered if the following condition is met:\n\npg_stat_all_tables.n_mod_since_analyze >\n\nautovacuum_analyze_threshold +\n\nautovacuum_analyze_scale_factor × pg_class.reltuples\n\nTo override autoanalysis settings for particular tables,you can use the same-name storage parameters:\n\nautovacuum_analyze_threshold\n\nautovacuum_analyze_scale_factor\n\nSince ����� tables are not analyzed, they have no corresponding parameters.\n\n133\n\n50\n\n0.1",
      "content_length": 1121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Chapter 6 Vacuum and Autovacuum\n\nAutovacuum in Action\n\nTo formalize everything said in this section,let’s create two views that show which tables currently need to be vacuumed and analyzed.1 The function used in these views returns the current value of the passed parameter, taking into account that this value can be redefined at the table level:\n\n=> CREATE FUNCTION p(param text, c pg_class) RETURNS float AS $$\n\nSELECT coalesce(\n\n-- use storage parameter if set (SELECT option_value\n\nFROM WHERE\n\npg_options_to_table(c.reloptions) option_name = CASE\n\n-- for TOAST tables the parameter name is different WHEN c.relkind = 't' THEN 'toast.' ELSE ''\n\nEND || param\n\n), -- else take the configuration parameter value current_setting(param)\n\n)::float;\n\n$$ LANGUAGE sql;\n\nThis is how a vacuum-related view can look like:\n\n=> CREATE VIEW need_vacuum AS WITH c AS (\n\nSELECT c.oid,\n\ngreatest(c.reltuples, 0) reltuples, p('autovacuum_vacuum_threshold', c) threshold, p('autovacuum_vacuum_scale_factor', c) scale_factor, p('autovacuum_vacuum_insert_threshold', c) ins_threshold, p('autovacuum_vacuum_insert_scale_factor', c) ins_scale_factor\n\nFROM pg_class c WHERE c.relkind IN ('r','m','t')\n\n) SELECT st.schemaname || '.' || st.relname AS tablename,\n\nst.n_dead_tup AS dead_tup, c.threshold + c.scale_factor * c.reltuples AS max_dead_tup, st.n_ins_since_vacuum AS ins_tup, c.ins_threshold + c.ins_scale_factor * c.reltuples AS max_ins_tup, st.last_autovacuum\n\nFROM pg_stat_all_tables st\n\nJOIN c ON c.oid = st.relid;\n\n1 backend/postmaster/autovacuum.c, relation_needs_vacanalyze function\n\n134",
      "content_length": 1580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "6.5 Automatic Vacuum and Analysis\n\nThe max_dead_tup column shows the number of dead tuples that will trigger au- tovacuum, whereas the max_ins_tup column shows the threshold value related to insertion.\n\nHere is a similar view for analysis:\n\n=> CREATE VIEW need_analyze AS WITH c AS (\n\nSELECT c.oid,\n\ngreatest(c.reltuples, 0) reltuples, p('autovacuum_analyze_threshold', c) threshold, p('autovacuum_analyze_scale_factor', c) scale_factor\n\nFROM pg_class c WHERE c.relkind IN ('r','m')\n\n) SELECT st.schemaname || '.' || st.relname AS tablename,\n\nst.n_mod_since_analyze AS mod_tup, c.threshold + c.scale_factor * c.reltuples AS max_mod_tup, st.last_autoanalyze\n\nFROM pg_stat_all_tables st\n\nJOIN c ON c.oid = st.relid;\n\nThe max_mod_tup column shows the threshold value for autoanalysis.\n\nTo speed up the experiment, we will be starting autovacuum every second:\n\n=> ALTER SYSTEM SET autovacuum_naptime = '1s';\n\n=> SELECT pg_reload_conf();\n\nLet’s truncate the vac table and then insert �,��� rows. Note that autovacuum is turned off at the table level.\n\n=> TRUNCATE TABLE vac;\n\n=> INSERT INTO vac(id,s)\n\nSELECT id, 'A' FROM generate_series(1,1000) id;\n\nHere is what our vacuum-related view will show:\n\n=> SELECT * FROM need_vacuum WHERE tablename = 'public.vac' \\gx −[ RECORD 1 ]−−−+−−−−−−−−−−− | public.vac tablename | 0 dead_tup | 50 max_dead_tup | 1000 ins_tup | 1000 max_ins_tup last_autovacuum |\n\n135",
      "content_length": 1398,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "v. ��\n\nChapter 6 Vacuum and Autovacuum\n\nThe actual threshold value is max_dead_tup = 50, although the formula listed above suggests that it should be 50 + 0.2 × 1000 = 250. The thing is that statistics on this table are not available yet since the ������ command does not update it:\n\n=> SELECT reltuples FROM pg_class WHERE relname = 'vac';\n\nreltuples −−−−−−−−−−−\n\n−1\n\n(1 row)\n\nThe pg_class.reltuples value is set to −1; this special constant is used instead of zero to differentiate between a table without any statistics and a really empty table that has already been analyzed. For the purpose of calculation, the negative value is taken as zero, which gives us 50 + 0.2 × 0 = 50.\n\nThe value of max_ins_tup = 1000 differs from the projected value of �,��� for the same reason.\n\nLet’s have a look at the analysis view:\n\n=> SELECT * FROM need_analyze WHERE tablename = 'public.vac' \\gx\n\n−[ RECORD 1 ]−−−−+−−−−−−−−−−− | public.vac tablename | 1006 mod_tup max_mod_tup | 50 last_autoanalyze |\n\nWe have updated (inserted in this case) �,��� rows; as a result, the threshold is exceeded: since the table size is unknown, it is currently set to ��. It means that autoanalysis will be triggered immediately when we turn it on:\n\n=> ALTER TABLE vac SET (autovacuum_enabled = on);\n\nOnce the table analysis completes, the threshold is reset to an adequate value of ��� rows.\n\n=> SELECT reltuples FROM pg_class WHERE relname = 'vac';\n\nreltuples −−−−−−−−−−−\n\n1000\n\n(1 row)\n\n136",
      "content_length": 1465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "6.5 Automatic Vacuum and Analysis\n\n=> SELECT * FROM need_analyze WHERE tablename = 'public.vac' \\gx\n\n−[ RECORD 1 ]−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− tablename mod_tup max_mod_tup last_autoanalyze | 2023−03−06 14:00:45.533464+03\n\n| public.vac | 0 | 150\n\nLet’s get back to autovacuum:\n\n=> SELECT * FROM need_vacuum WHERE tablename = 'public.vac' \\gx\n\n−[ RECORD 1 ]−−−+−−−−−−−−−−− | public.vac tablename | 0 dead_tup | 250 max_dead_tup | 1000 ins_tup max_ins_tup | 1200 last_autovacuum |\n\nThe max_dead_tup and max_ins_tup values have also been updated based on the actual table size discovered by the analysis.\n\nVacuuming will be started if at least one of the following conditions is met:\n\nMore than ��� dead tuples are accumulated.\n\nMore than ��� rows are inserted into the table.\n\nLet’s turn off autovacuum again and update ��� rows so that the threshold value is exceeded by one:\n\n=> ALTER TABLE vac SET (autovacuum_enabled = off);\n\n=> UPDATE vac SET s = 'B' WHERE id <= 251;\n\n=> SELECT * FROM need_vacuum WHERE tablename = 'public.vac' \\gx\n\n−[ RECORD 1 ]−−−+−−−−−−−−−−− | public.vac tablename | 251 dead_tup | 250 max_dead_tup | 1000 ins_tup max_ins_tup | 1200 last_autovacuum |\n\nNow the trigger condition is satisfied. Let’s enable autovacuum; after a while, we will see that the table has been processed, and its usage statistics has been reset:\n\n137\n\nv. ��",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "200 0\n\np. ��� 1 2 20\n\nChapter 6 Vacuum and Autovacuum\n\n=> ALTER TABLE vac SET (autovacuum_enabled = on);\n\n=> SELECT * FROM need_vacuum WHERE tablename = 'public.vac' \\gx\n\n−[ RECORD 1 ]−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− tablename dead_tup max_dead_tup ins_tup max_ins_tup last_autovacuum | 2023−03−06 14:00:51.736815+03\n\n| public.vac | 0 | 250 | 0 | 1200\n\n6.6 Managing the Load\n\nOperating at the page level, vacuuming does not block other processes; but never- theless, it increases the system load and can have a noticeable impact on perfor- mance.\n\nVacuum Throttling\n\nTo control vacuuming intensity, Postgre��� makes regular pauses in table pro- vacuum_cost_limit units of work, the process falls cessing. After completing about asleep and remains idle for the\n\nvacuum_cost_delay time interval.\n\nThe default zero value of vacuum_cost_delay means that routine vacuuming actu- ally never sleeps, so the exact vacuum_cost_limit value makes no difference. It is assumed that if administrators have to resort to manual vacuuming,they are likely to expect its completion as soon as possible.\n\nIf the sleep time is set, then the process will pause each time it has spent vac- uum_cost_limit units of work on page processing in the buffer cache . The cost of vacuum_cost_page_hit units if the page is found in each page read is estimated at vacuum_cost_page_miss units otherwise.1 If a clean page is dirt- the buffer cache,or ied by vacuum, it adds another\n\nvacuum_cost_page_dirty units.2\n\n1 backend/storage/buffer/bufmgr.c, ReadBuffer_common function 2 backend/storage/buffer/bufmgr.c, MarkBufferDirty function\n\n138",
      "content_length": 1609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "6.6 Managing the Load\n\nIf you keep the default value of the vacuum_cost_limit parameter, ������ can pro- cess up to ��� pages per cycle in the best-case scenario (if all the pages are cached, and no pages are dirtied by������) and only nine pages in the worst case (if all the pages are read from disk and become dirty).\n\nAutovacuum Throttling\n\nThrottling for autovacuum1 is quite similar to ������ throttling. However, auto- vacuum can be run with a different intensity as it has its own set of parameters:\n\n\n\nautovacuum_vacuum_cost_limit\n\n\n\nautovacuum_vacuum_cost_delay\n\nIf any of these parameters is set to −1,it falls back on the corresponding parameter for regular ������. Thus, the autovacuum_vacuum_cost_limit parameter relies on the vacuum_cost_limit value by default.\n\nPrior to version ��, the default value of autovacuum_vacuum_cost_delay was �� ms, and it led to very poor performance on modern hardware.\n\nAutovacuumworkunitsarelimitedtoautovacuum_vacuum_cost_limit percycle,and since they are shared between all the workers, the overall impact on the system re- mains roughly the same,regardless of their number. So if you need to speed up au- tovacuum, both the autovacuum_max_workers and autovacuum_vacuum_cost_limit values should be increased proportionally.\n\nIf required, you can override these settings for particular tables by setting the fol- lowing storage parameters:\n\nautovacuum_vacuum_cost_delay and toast.autovacuum_vacuum_cost_delay\n\nautovacuum_vacuum_cost_limit and toast.autovacuum_vacuum_cost_limit\n\n1 backend/postmaster/autovacuum.c, autovac_balance_cost function\n\n139\n\n−1\n\n2ms",
      "content_length": 1606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "v. �.�\n\nv. ��\n\nChapter 6 Vacuum and Autovacuum\n\n6.7 Monitoring\n\nIf vacuuming is monitored, you can detect situations when dead tuples cannot be removed in one go, as references to them do not fit the maintenance_work_mem memory chunk. In this case, all the indexes will have to be fully scanned several times. It can take a substantial amount of time for large tables, thus creating a significant load on the system. Even though queries will not be blocked, extra �/� operations can seriously limit system throughput.\n\nSuch issues can be corrected either by vacuuming the table more often (so that each run cleans up fewer tuples) or by allocating more memory.\n\nMonitoring Vacuum\n\nWhen run with the������� clause,the������ command performs the cleanup and displays the status report, whereas the pg_stat_progress_vacuum view shows the current state of the started process.\n\nThere is also a similar view for analysis is usually performed very fast and is unlikely to cause any issues.\n\n(pg_stat_progress_analyze), even though it\n\nLet’s insert more rows into the table and update them all so that ������ has to run for a noticeable period of time:\n\n=> TRUNCATE vac;\n\n=> INSERT INTO vac(id,s)\n\nSELECT id, 'A' FROM generate_series(1,500000) id;\n\n=> UPDATE vac SET s\n\n= 'B';\n\nFor the purpose of this demonstration, we will limit the amount of memory allo- cated for the tid array by � ��:\n\n=> ALTER SYSTEM SET maintenance_work_mem = '1MB';\n\n=> SELECT pg_reload_conf();\n\nLaunch the ������ command and query the pg_stat_progress_vacuum view several times while it is running:\n\n140",
      "content_length": 1573,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "6.7 Monitoring\n\n=> VACUUM VERBOSE vac;\n\n=> SELECT * FROM pg_stat_progress_vacuum \\gx\n\n−[ RECORD 1 ]−−−−−−+−−−−−−−−−−−−−−−−−− pid datid datname relid phase heap_blks_total heap_blks_scanned heap_blks_vacuumed | 0 index_vacuum_count | 0 max_dead_tuples num_dead_tuples\n\n| 14531 | 16391 | internals | 16479 | vacuuming indexes | 17242 | 3009\n\n| 174761 | 174522\n\n=> SELECT * FROM pg_stat_progress_vacuum \\gx\n\n−[ RECORD 1 ]−−−−−−+−−−−−−−−−−−−−−−−−− pid datid datname relid phase heap_blks_total heap_blks_scanned heap_blks_vacuumed | 6017 index_vacuum_count | 2 max_dead_tuples num_dead_tuples\n\n| 14531 | 16391 | internals | 16479 | vacuuming indexes | 17242 | 17242\n\n| 174761 | 150956\n\nIn particular, this view shows:\n\nphase—thenameofthecurrentvacuumphase(Ihavedescribedthemainones, but there are actually more of them1)\n\nheap_blks_total—the total number of pages in a table\n\nheap_blks_scanned—the number of scanned pages\n\nheap_blks_vacuumed—the number of vacuumed pages\n\nindex_vacuum_count—the number of index scans\n\n1 postgresql.org/docs/14/progress-reporting.html#VACUUM-PHASES\n\n141",
      "content_length": 1081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "Chapter 6 Vacuum and Autovacuum\n\nThe overall vacuuming progress is defined by the ratio of heap_blks_vacuumed to heap_blks_total, but you have to keep in mind that it changes in spurts because of index scans. In fact,it is more important to pay attention to the number of vacuum cycles: if this value is greater than one, it means that the allocated memory was not enough to complete vacuuming in one go.\n\nYoucanseethewholepictureintheoutputofthe�������������command,which has already finished by this time:\n\nINFO: INFO: DETAIL: INFO: 3009 pages DETAIL: INFO: DETAIL: INFO: 3009 pages DETAIL: INFO: DETAIL: INFO: 2603 pages DETAIL: INFO: 932 pages DETAIL: 433 index pages were newly deleted. 433 index pages are currently deleted, of which 0 are currently reusable. CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s. INFO: nonremovable row versions in 17242 out of 17242 pages DETAIL: xmin: 851 Skipped 0 pages due to buffer pins, 0 frozen pages. CPU: user: 0.20 s, system: 0.03 s, elapsed: 0.53 s. VACUUM\n\nvacuuming \"public.vac\" scanned index \"vac_s\" to remove 174522 row versions\n\nCPU: user: 0.02 s, system: 0.00 s, elapsed: 0.05 s table \"vac\": removed 174522 dead item identifiers in\n\nCPU: user: 0.00 s, system: 0.01 s, elapsed: 0.07 s\n\nscanned index \"vac_s\" to remove 174522 row versions\n\nCPU: user: 0.02 s, system: 0.00 s, elapsed: 0.05 s table \"vac\": removed 174522 dead item identifiers in\n\nCPU: user: 0.00 s, system: 0.00 s, elapsed: 0.01 s\n\nscanned index \"vac_s\" to remove 150956 row versions\n\nCPU: user: 0.02 s, system: 0.00 s, elapsed: 0.04 s table \"vac\": removed 150956 dead item identifiers in\n\nCPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s\n\nindex \"vac_s\" now contains 500000 row versions in\n\n500000 index row versions were removed.\n\ntable \"vac\": found 500000 removable, 500000\n\n0 dead row versions cannot be removed yet, oldest\n\nAll in all, there have been three index scans; each scan has removed ���,��� pointers to dead tuples at the most. This value is defined by the number of tid pointers (each of them takes � bytes) that can fit into an array of the main-\n\n142\n\nindex vacuum\n\ntable vacuum\n\nindex vacuum\n\ntable vacuum\n\nindex vacuum\n\ntable vacuum",
      "content_length": 2172,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "6.7 Monitoring\n\ntenance_work_mem size. The maximum size possible is shown by pg_stat_prog- ress_vacuum.max_dead_tuples,but the actually used space is always a bit smaller. It guarantees that when the next page is read, all its pointers to dead tuples, no mat- ter how many of them are located in this page, will fit into the remaining memory.\n\nMonitoring Autovacuum\n\nThe main approach to monitoring autovacuum is to print its status information (which is similar to the output of the ������ ������� command) into the server log_autovacuum_min_duration parameter is set to log for further analysis. If the zero, all autovacuum runs are logged:\n\n=> ALTER SYSTEM SET log_autovacuum_min_duration = 0;\n\n=> SELECT pg_reload_conf();\n\n=> UPDATE vac SET s = 'C';\n\nUPDATE 500000\n\npostgres$ tail -n 13 /home/postgres/logfile\n\n2023−03−06 14:01:13.727 MSK [17351] LOG: \"internals.public.vac\": index scans: 3 pages: 0 removed, 17242 remain, 0 skipped due to pins, 0 skipped frozen tuples: 500000 removed, 500000 remain, 0 are dead but not yet removable, oldest xmin: 853 index scan needed: 8622 pages from table (50.01% of total) had 500000 dead item identifiers removed index \"vac_s\": pages: 1428 in total, 496 newly deleted, 929 currently deleted, 433 reusable avg read rate: 12.404 MB/s, avg write rate: 14.810 MB/s buffer usage: 46038 hits, 5670 misses, 6770 dirtied WAL usage: 40390 records, 15062 full page images, 89188595 bytes system usage: CPU: user: 0.31 s, system: 0.33 s, elapsed: 3.57 s 2023−03−06 14:01:14.117 MSK [17351] LOG: \"internals.public.vac\" avg read rate: 41.081 MB/s, avg write rate: 0.020 MB/s buffer usage: 15355 hits, 2035 misses, 1 dirtied system usage: CPU: user: 0.14 s, system: 0.00 s, elapsed: 0.38 s\n\nautomatic vacuum of table\n\nautomatic analyze of table\n\n143\n\n−1",
      "content_length": 1783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Chapter 6 Vacuum and Autovacuum\n\nTo track the list of tables that have to be vacuumed and analyzed, you can use the need_vacuum and need_analyze views, which we have already reviewed. If this list grows, it means that autovacuum does not cope with the load and has to be sped up by either reducing the gap (autovacuum_vacuum_cost_delay) or increasing the amount of work done between the gaps (autovacuum_vacuum_cost_limit). It is not unlikely that the degree of parallelism will also have to be increased (autovac- uum_max_workers).\n\n144",
      "content_length": 537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "7\n\nFreezing\n\n7.1 Transaction ID Wraparound\n\nIn Postgre���, a transaction �� takes �� bits. Four billions seems to be quite a big number, but it can be exhausted very fast if the system is being actively used. For example, for an average load of �,��� transactions per second (excluding virtual ones), it will happen in about six weeks of continuous operation.\n\nOnce all the numbers are used up, the counter has to be reset to start the next round (this situation is called a “wraparound”). But a transaction with a smaller �� can only be considered older than another transaction with a bigger �� if the assigned numbers are always increasing. So the counter cannot simply start using the same numbers anew after being reset.\n\nAllocating �� bits for transaction ��s would have eliminated this problem alto- gether, so why doesn’t Postgre��� take advantage of it? The thing is that each tuple header has to store ��s for two transactions: xmin and xmax. The header is quite big already (at least �� bytes if data alignment is taken into account), and adding more bits would have given another � bytes.\n\nPostgre��� does implement ��-bit transaction ��s1 that extend a regular �� by a ��-bit epoch, but they are used only internally and never get into data pages.\n\nTocorrectlyhandlewraparound,Postgre���hastocomparetheageoftransactions (defined as the number of subsequent transactions that have appeared since the startofthistransaction)ratherthantransaction��s. Thus,insteadofthetermsless than and greater than we should use the concepts of older (precedes) and younger (follows).\n\n1 include/access/transam.h, FullTransactionId type\n\n145\n\np. ��",
      "content_length": 1643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "p. ���\n\nChapter 7 Freezing\n\nIn the code,this comparison is implemented by simply using the ��-bit arithmetic: first the difference between ��-bit transaction ��s is found, and then this result is compared to zero.1\n\nTo better visualize this idea, you can imagine a sequence of transaction ��s as a clock face. For each transaction,half of the circle in the clockwise direction will be in the future, while the other half will be in the past.\n\nT1\n\nT1\n\nT1T1\n\nT2\n\np a s t\n\nf u t u r e\n\nHowever, this visualization has an unpleasant catch. An old transaction (��) is in the remote past as compared to more recent transactions. But sooner or later a new transaction will see it in the half of the circle related to the future. If it were really so,it would have a catastrophic impact: from now on,all newer transactions would not see the changes made by transaction ��.\n\n7.2 Tuple Freezing and Visibility Rules\n\nTo prevent such “time travel,” vacuuming performs one more task (in addition to pagecleanup):2 itsearchesfortuplesthatarebeyondthedatabasehorizon(sothey are visible in all snapshots) and tags them in a special way, that is, freezes\n\nFor frozen tuples, visibility rules do not have to take xmin into account since such tuples are known to be visible in all snapshots, so this transaction �� can be safely reused.\n\n1 backend/access/transam/transam.c, TransactionIdPrecedes function 2 postgresql.org/docs/14/routine-vacuuming.html#VACUUM-FOR-WRAPAROUND\n\n146\n\nT2\n\nT3\n\nthem.",
      "content_length": 1476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "7.2 Tuple Freezing and Visibility Rules\n\nYou can imagine that the xmin transaction �� is replaced in frozen tuples by a hy- pothetical “minus infinity” (pictured as a snowflake below); it is a sign that this tuple is created by a transaction that is so far in the past that its actual �� does not matter anymore. Yet in reality xmin remains unchanged, whereas the freezing attribute is defined by a combination of two hint bits: committed and aborted.\n\nT1(cid:94)\n\n(cid:94)\n\nT1(cid:94)\n\nT2\n\n(cid:94)\n\n(cid:94)\n\nT3\n\nT3\n\nT2\n\nT4\n\nT1\n\nT4\n\nT1\n\nT4\n\nMany sources (including the documentation) mention FrozenTransactionId = 2. It is the “minus infinity”that I have already referred to—this value used to replace xmin in versions prior to �.�, but now hint bits are employed instead. As a result, the original transaction �� remains in the tuple,which is convenient for both debugging and support. Old systems can still contain the obsolete FrozenTransactionId, even if they have been upgraded to higher versions.\n\nThe xmax transaction �� does not participate in freezing in any way. It is only present in outdated tuples, and once such tuples stop being visible in all snap- shots (which means that the xmax �� is beyond the database horizon), they will be vacuumed away.\n\nLet’s create a new table for our experiments. The fillfactor parameter should be set to the lowest value so that each page can accommodate only two tuples—it will be easier to track the progress this way. We will also disable autovacuum to make sure that the table is only cleaned up on demand.\n\n=> CREATE TABLE tfreeze(\n\nid integer, s char(300)\n\n) WITH (fillfactor = 10, autovacuum_enabled = off);\n\n(cid:94)\n\n147",
      "content_length": 1678,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Chapter 7 Freezing\n\nWe are going to create yet another flavor of the function that displays heap pages using pageinspect. Dealing with a range of pages, it will show the values of the freezing attribute (f) and the xmin transaction age for each tuple (it will have to call the age system function—the age itself is not stored in heap pages, of course):\n\n=> CREATE FUNCTION heap_page(\n\nrelname text, pageno_from integer, pageno_to integer\n\n) RETURNS TABLE(\n\nctid tid, state text, xmin text, xmin_age integer, xmax text\n\n) AS $$ SELECT (pageno,lp)::text::tid AS ctid,\n\nCASE lp_flags\n\nWHEN 0 THEN 'unused' WHEN 1 THEN 'normal' WHEN 2 THEN 'redirect to '||lp_off WHEN 3 THEN 'dead'\n\nEND AS state, t_xmin || CASE\n\nWHEN (t_infomask & 256+512) = 256+512 THEN ' f' WHEN (t_infomask & 256) > 0 THEN ' c' WHEN (t_infomask & 512) > 0 THEN ' a' ELSE '' END AS xmin, age(t_xmin) AS xmin_age, t_xmax || CASE\n\nWHEN (t_infomask & 1024) > 0 THEN ' c' WHEN (t_infomask & 2048) > 0 THEN ' a' ELSE '' END AS xmax\n\nFROM generate_series(pageno_from, pageno_to) p(pageno), heap_page_items(get_raw_page(relname, pageno))\n\nORDER BY pageno, lp; $$ LANGUAGE sql;\n\nNow let’s insert some rows into the table and run the ������ command that will immediately create the visibility map.\n\n=> CREATE EXTENSION IF NOT EXISTS pg_visibility;\n\n=> INSERT INTO tfreeze(id, s)\n\nSELECT id, 'FOO'||id FROM generate_series(1,100) id;\n\nINSERT 0 100\n\n148",
      "content_length": 1408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "7.3 Managing Freezing\n\nWe are going to observe the first two heap pages using the pg_visibility exten- sion. When vacuuming completes, both pages get tagged in the visibility map (all_visible) but not in the freeze map (all_frozen ), as they still contain some un- frozen tuples:\n\n=> VACUUM tfreeze;\n\n=> SELECT * FROM generate_series(0,1) g(blkno),\n\npg_visibility_map('tfreeze',g.blkno)\n\nORDER BY g.blkno;\n\nblkno | all_visible | all_frozen −−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0 | t 1 | t\n\n| f | f\n\n(2 rows)\n\nThe xmin_age of the transaction that has created the rows equals 1 because it is the latest transaction performed in the system:\n\n=> SELECT * FROM heap_page('tfreeze',0,1);\n\nctid\n\n| state\n\n| xmin\n\n| xmin_age | xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−−−−+−−−−−−\n\n(0,1) | normal | 856 c | (0,2) | normal | 856 c | (1,1) | normal | 856 c | (1,2) | normal | 856 c |\n\n1 | 0 a 1 | 0 a 1 | 0 a 1 | 0 a\n\n(4 rows)\n\n7.3 Managing Freezing\n\nThere are four main parameters that control freezing. All of them represent trans- action age and define when the following events happen:\n\nFreezing starts (vacuum_freeze_min_age).\n\nAggressive freezing is performed (vacuum_freeze_table_age).\n\nFreezing is forced (autovacuum_freeze_max_age).\n\nFreezing receives priority\n\n(vacuum_failsafe_age).\n\n149\n\nv. �.�\n\nv. ��",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "50 million\n\np. ���\n\nChapter 7 Freezing\n\nMinimal Freezing Age\n\nvacuum_freeze_min_age parameter defines the minimal freezing age of xmin The transactions. The lower its value, the higher the overhead: if a row is “hot” and is actively being changed,then freezing all its newly created versions will be a wasted effort. Setting this parameter to a relatively high value allows you to wait for a while.\n\nTo observe the freezing process, let’s reduce this parameter value to one:\n\n=> ALTER SYSTEM SET vacuum_freeze_min_age = 1;\n\n=> SELECT pg_reload_conf();\n\nNow update one row in the zero page. The new row version will get into the same page because the fillfactor value is quite small:\n\n=> UPDATE tfreeze SET s = 'BAR' WHERE id = 1;\n\nThe age of all transactions has been increased by one,and the heap pages now look as follows:\n\n=> SELECT * FROM heap_page('tfreeze',0,1);\n\nctid\n\n| state\n\n| xmin\n\n| xmin_age | xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−−−−+−−−−−−\n\n(0,1) | normal | 856 c | (0,2) | normal | 856 c | (0,3) | normal | 857 | (1,1) | normal | 856 c | (1,2) | normal | 856 c |\n\n2 | 857 2 | 0 a 1 | 0 a 2 | 0 a 2 | 0 a\n\n(5 rows)\n\nAt this point, the tuples that are older than vacuum_freeze_min_age = 1 are subject : to freezing. But vacuum will not process any pages tagged in the visibility map\n\n=> SELECT * FROM generate_series(0,1) g(blkno), pg_visibility_map('tfreeze',g.blkno)\n\nORDER BY g.blkno;\n\nblkno | all_visible | all_frozen −−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0 | f 1 | t\n\n| f | f\n\n(2 rows)\n\n150",
      "content_length": 1507,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "7.3 Managing Freezing\n\nThe previous ������ command has removed the visibility bit of the zero page, so the tuple that has an appropriate xmin age in this page will be frozen. But the first page will be skipped altogether:\n\n=> VACUUM tfreeze;\n\n=> SELECT * FROM heap_page('tfreeze',0,1);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmin_age | xmax\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−−−−−+−−−−−−\n\n(0,1) | redirect to 3 | (0,2) | normal (0,3) | normal (1,1) | normal (1,2) | normal\n\n| | 856 f | | 857 c | | 856 c | | 856 c |\n\n|\n\n2 | 0 a 1 | 0 a 2 | 0 a 2 | 0 a\n\n(5 rows)\n\nNow the zero page appears in the visibility map again, and if nothing changes in it, vacuuming will not return to this page anymore:\n\n=> SELECT * FROM generate_series(0,1) g(blkno), pg_visibility_map('tfreeze',g.blkno)\n\nORDER BY g.blkno;\n\nblkno | all_visible | all_frozen −−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0 | t 1 | t\n\n| f | f\n\n(2 rows)\n\nAge for Aggressive Freezing\n\nAs we have just seen, if a page contains only the current tuples that are visible in all snapshots, vacuuming will not freeze them. To overcome this constraint, Post- vacuum_freeze_table_age parameter. It defines the transaction gre��� provides the age that allows vacuuming to ignore the visibility map, so any heap page can be frozen.\n\nFor each table,the system catalog keeps a transaction �� for which it is known that all the older transactions are sure to be frozen. It is stored as relfrozenxid:\n\n151\n\n150 million",
      "content_length": 1443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "v. �.�\n\nChapter 7 Freezing\n\n=> SELECT relfrozenxid, age(relfrozenxid) FROM pg_class WHERE relname = 'tfreeze';\n\nrelfrozenxid | age −−−−−−−−−−−−−−+−−−−−\n\n854 |\n\n4\n\n(1 row)\n\nIt is the age of this transaction that is compared to the vacuum_freeze_table_age value to decide whether the time has come for aggressive freezing.\n\n, there is no need to perform a full table scan during vac- Thanks to the freeze map uuming: it is enough to check only those pages that do not appear in the map. Apart from this important optimization,the freeze map also brings fault tolerance: if vacuuming is interrupted,its next run will not have to get back to the pages that have already been processed and are tagged in the map.\n\nPostgre��� performs aggressive freezing of all pages in a table each time when the number of transactions in the system reaches the vacuum_freeze_table_age − vacuum_freeze_min_age limit (if the default values are used, it happens after each ��� million transactions). Thus, if the vacuum_freeze_min_age value is too big, it can lead to excessive freezing and increased overhead.\n\nTo freeze the whole table, let’s reduce the vacuum_freeze_table_age value to four; then the condition for aggressive freezing will be satisfied:\n\n=> ALTER SYSTEM SET vacuum_freeze_table_age = 4;\n\n=> SELECT pg_reload_conf();\n\nRun the ������ command:\n\n=> VACUUM VERBOSE tfreeze;\n\nINFO: INFO: versions in 50 out of 50 pages DETAIL: Skipped 0 pages due to buffer pins, 0 frozen pages. CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s. VACUUM\n\naggressively vacuuming \"public.tfreeze\" table \"tfreeze\": found 0 removable, 100 nonremovable row\n\n0 dead row versions cannot be removed yet, oldest xmin: 858\n\n152",
      "content_length": 1692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "7.3 Managing Freezing\n\nNow that the whole table has been analyzed, the relfrozenxid value can be ad- vanced—heap pages are guaranteed to have no older unfrozen xmin transactions:\n\n=> SELECT relfrozenxid, age(relfrozenxid) FROM pg_class WHERE relname = 'tfreeze';\n\nrelfrozenxid | age −−−−−−−−−−−−−−+−−−−−\n\n857 |\n\n1\n\n(1 row)\n\nThe first page now contains only frozen tuples:\n\n=> SELECT * FROM heap_page('tfreeze',0,1);\n\nctid\n\n|\n\nstate\n\n| xmin\n\n| xmin_age | xmax\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−+−−−−−−−−−−+−−−−−−\n\n(0,1) | redirect to 3 | (0,2) | normal (0,3) | normal (1,1) | normal (1,2) | normal\n\n| | 856 f | | 857 c | | 856 f | | 856 f |\n\n|\n\n2 | 0 a 1 | 0 a 2 | 0 a 2 | 0 a\n\n(5 rows)\n\nBesides, this page is tagged in the freeze map:\n\n=> SELECT * FROM generate_series(0,1) g(blkno),\n\npg_visibility_map('tfreeze',g.blkno)\n\nORDER BY g.blkno;\n\nblkno | all_visible | all_frozen −−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0 | t 1 | t\n\n| f | t\n\n(2 rows)\n\nAge for Forced Autovacuum\n\nSometimes it is not enough to configure the two parameters discussed above to timely freeze tuples. Autovacuum might be switched off, while regular ������ is not being called at all (it is a very bad idea, but technically it is possible). Besides,\n\n153",
      "content_length": 1220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "p. ���\n\n200 million\n\nChapter 7 Freezing\n\nsome inactive databases (like template0) may not be vacuumed handle such situations by forcing autovacuum in the aggressive mode.\n\n. Postgre��� can\n\nAutovacuum is forced1 (even if it is switched off) when there is a risk that the autovacu- age of some unfrozen transaction ��s in the database will exceed the um_freeze_max_age value. The decision is taken based on the age of the oldest pg_class.relfrozenxid transaction in all the tables, as all the older transactions are guaranteed to be frozen. The �� of this transaction is stored in the system catalog:\n\n=> SELECT datname, datfrozenxid, age(datfrozenxid) FROM pg_database;\n\ndatname\n\n| datfrozenxid | age\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−+−−−−−\n\npostgres | template1 | template0 | internals |\n\n726 | 132 726 | 132 726 | 132 726 | 132\n\n(4 rows)\n\ndatfrozenxid\n\nall row versions in the database are guaranteed to be frozen\n\nxid\n\nrelfrozenxid of table 1\n\nrelfrozenxid of table 3\n\nrelfrozenxid of table 2\n\nThe autovacuum_freeze_max_age limit is set to � billion transactions (a bit less than half of the circle), while the default value is �� times smaller. It is done for good reason: a big value increases the risk of transaction �� wraparound, as Postgre��� may fail to timely freeze all the required tuples. In this case, the server must stop immediately to prevent possible issues and will have to be restarted by an admin- istrator.\n\n1 backend/access/transam/varsup.c, SetTransactionIdLimit function\n\n154",
      "content_length": 1495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "7.4 Manual Freezing\n\nThe autovacuum_freeze_max_age value also affects the size of ����. There is no needtokeepthestatusoffrozentransactions,andallthetransactionsthatprecede theonewiththeoldestdatfrozenxidintheclusteraresuretobefrozen. Those���� files that are not required anymore are removed by autovacuum.1\n\nChanging the autovacuum_freeze_max_age parameter requires a server restart. However,all the freezing settings discussed above can also be adjusted at the table level via the corresponding storage parameters. Note that the names of all these parameters start with “auto”:\n\nautovacuum_freeze_min_age and toast.autovacuum_freeze_min_age\n\nautovacuum_freeze_table_age and toast.autovacuum_freeze_table_age\n\nautovacuum_freeze_max_age and toast.autovacuum_freeze_max_age\n\nAge for Failsafe Freezing\n\nIf autovacuum is already struggling to prevent transaction �� wraparound and it is clearly a race against time, a safety switch is pulled: autovacuum will ignore the autovacuum_vacuum_cost_delay (vacuum_cost_delay) value and will stop vacuuming indexes to freeze heap tuples as soon as possible.\n\nA failsafe freezing mode2 is enabled if there is a risk that the age of an unfrozen vacuum_failsafe_agevalue. Itisassumed transactioninthedatabasewillexceedthe that this value must be higher than autovacuum_freeze_max_age.\n\n7.4 Manual Freezing\n\nIt is sometimes more convenient to manage freezing manually rather than rely on autovacuum.\n\n1 backend/commands/vacuum.c, vac_truncate_clog function 2 backend/access/heap/vacuumlazy.c, lazy_check_wraparound_failsafe function\n\n155\n\np. ��\n\nv. ��\n\n1.6 billion",
      "content_length": 1600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "v. ��\n\np. ���\n\nChapter 7 Freezing\n\nFreezing by Vacuum\n\nYou can initiate freezing by calling the ������ command with the ������ op- tion. It will freeze all the heap tuples regardless of their transaction age, as if vacuum_freeze_min_age = 0.\n\nof such a call is to freeze heap tuples as soon as possible, it makes If the purpose sense to disable index vacuuming,like it is done in the failsafe mode. You can do it either explicitly, by running the ������ (freeze, index_cleanup false) command, or via the vacuum_index_cleanup storage parameter. It is rather obvious that it should not be done on a regular basis since in this case������ will not be coping well with its main task of page cleanup.\n\nFreezing Data at the Initial Loading\n\nThe data that is not expected to change can be frozen at once, while it is being loaded into the database. It is done by running the ���� command with the ������ option.\n\nTuples can be frozen during the initial loading only if the resulting table has been created or truncated within the same transaction,as both these operations acquire an exclusive lock on the table. This restriction is necessary because frozen tuples are expected to be visible in all snapshots, regardless of the isolation level; other- wise,transactions would suddenly see freshly-frozen tuples right as they are being uploaded. But if the lock is acquired, other transactions will not be able to get access to this table.\n\nNevertheless, it is still technically possible to break isolation. Let’s start a new transaction at the Repeatable Read isolation level in a separate session:\n\n=> BEGIN ISOLATION LEVEL REPEATABLE READ;\n\n=> SELECT 1; -- the snapshot is built\n\nTruncatethetfreezetableandinsertnewrowsintothistablewithinthesametrans- action. (If the read-only transaction had already accessed the tfreeze table, the �������� command will be blocked.)\n\n156",
      "content_length": 1867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "7.4 Manual Freezing\n\n=> BEGIN;\n\n=> TRUNCATE tfreeze;\n\n=> COPY tfreeze FROM stdin WITH FREEZE; 1 FOO 2 BAR 3 BAZ \\.\n\n=> COMMIT;\n\nNow the reading transaction sees the new data as well:\n\n=> SELECT count(*) FROM tfreeze;\n\ncount −−−−−−−\n\n3\n\n(1 row)\n\n=> COMMIT;\n\nIt does break isolation, but since data loading is unlikely to happen regularly, in most cases it will not cause any issues.\n\nIf you load data with freezing ers receive the visibility attribute:\n\n, the visibility map is created at once, and page head-\n\n=> SELECT * FROM pg_visibility_map('tfreeze',0);\n\nall_visible | all_frozen −−−−−−−−−−−−−+−−−−−−−−−−−−\n\nt\n\n| t\n\n(1 row)\n\n=> SELECT flags & 4 > 0 AS all_visible FROM page_header(get_raw_page('tfreeze',0));\n\nall_visible −−−−−−−−−−−−−\n\nt\n\n(1 row)\n\nif the data has been loaded with freezing, the table will not be processed by Thus, vacuum (as long as the data remains unchanged). Unfortunately,this functionality is not supported for ����� tables yet: if an oversized value is loaded, vacuum will have to rewrite the whole ����� table to set visibility attributes in all page headers.\n\n157\n\nv. �� p. ���\n\nv. ��",
      "content_length": 1116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "p. ���\n\n8\n\nRebuilding Tables and Indexes\n\n8.1 Full Vacuuming\n\nWhy is Routine Vacuuming not Enough?\n\nRoutine vacuuming can free more space than page pruning, but sometimes it may still be not enough.\n\nIf table or index files have grown in size, ������ can clean up some space within pages, but it can rarely reduce the number of pages. The reclaimed space can only be returned to the operating system if several empty pages appear at the very end of the file, which does not happen too often.\n\nAn excessive size can lead to unpleasant consequences:\n\nFull table (or index) scan will take longer.\n\nA bigger buffer cache may be required (pages are cached as a whole, so data\n\ndensity decreases).\n\nB-trees can get an extra level, which slows down index access.\n\nFiles take up extra space on disk and in backups.\n\nIf the fraction of useful data in a file has dropped below some reasonable level, an administrator can perform full vacuuming by running the ������ ���� command.1 In this case, the table and all its indexes are rebuilt from scratch, and the data is packed as densely as possible (taking the fillfactor\n\nparameter into account).\n\n1 postgresql.org/docs/14/routine-vacuuming.html#VACUUM-FOR-SPACE-RECOVERY\n\n158",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "8.1 Full Vacuuming\n\nWhen full vacuuming is performed, Postgre��� first fully rebuilds the table and then each of its indexes. While an object is being rebuilt, both old and new files have to be stored on disk,1 so this process may require a lot of free space.\n\nYou should also keep in mind that this operation fully blocks access to the table, both for reads and writes.\n\nEstimating Data Density\n\nFor the purpose of illustration, let’s insert some rows into the table:\n\n=> TRUNCATE vac;\n\n=> INSERT INTO vac(id,s)\n\nSELECT id, id::text FROM generate_series(1,500000) id;\n\nStorage density can be estimated using the pgstattuple extension:\n\n=> CREATE EXTENSION pgstattuple;\n\n=> SELECT * FROM pgstattuple('vac') \\gx\n\n−[ RECORD 1 ]−−−−−−+−−−−−−−−− | 70623232 table_len | 500000 tuple_count | 64500000 tuple_len | 91.33 tuple_percent | 0 dead_tuple_count | 0 dead_tuple_len dead_tuple_percent | 0 free_space free_percent\n\n| 381844 | 0.54\n\nThe function reads the whole table and displays statistics on space distribution in its files. The tuple_percent field shows the percentage of space taken up by use- ful data (heap tuples). This value is inevitably less than ���% because of various metadata within pages, but in this example it is still quite high.\n\nFor indexes, the displayed information differs a bit, but the avg_leaf_density field has the same meaning: it shows the percentage of useful data (in �-tree leaf pages).\n\n1 backend/commands/cluster.c\n\n159",
      "content_length": 1453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "Chapter 8 Rebuilding Tables and Indexes\n\n=> SELECT * FROM pgstatindex('vac_s') \\gx\n\n−[ RECORD 1 ]−−−−−−+−−−−−−−−−− | 4 version | 3 tree_level | 114302976 index_size | 2825 root_block_no | 376 internal_pages | 13576 leaf_pages | 0 empty_pages | 0 deleted_pages avg_leaf_density | 53.88 leaf_fragmentation | 10.59\n\nThe previously used pgstattuple functions read the table or index in full to get the precise statistics. For large objects, it can turn out to be too expensive, so the extension also provides another function called pgstattuple_approx, which skips the pages tracked in the visibility map to show approximate figures.\n\nAfaster but even less accurate method is to roughly estimate the ratio between the data volume and the file size using the system catalog.1\n\nHere are the current sizes of the table and its index:\n\n=> SELECT pg_size_pretty(pg_table_size('vac')) AS table_size, pg_size_pretty(pg_indexes_size('vac')) AS index_size;\n\ntable_size | index_size −−−−−−−−−−−−+−−−−−−−−−−−−\n\n67 MB (1 row)\n\n| 109 MB\n\nNow let’s delete ��% of all the rows:\n\n=> DELETE FROM vac WHERE id % 10 != 0;\n\nDELETE 450000\n\nRoutine vacuuming does not affect the file size because there are no empty pages at the end of the file:\n\n=> VACUUM vac;\n\n1 wiki.postgresql.org/wiki/Show_database_bloat\n\n160",
      "content_length": 1288,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "8.1 Full Vacuuming\n\n=> SELECT pg_size_pretty(pg_table_size('vac')) AS table_size, pg_size_pretty(pg_indexes_size('vac')) AS index_size;\n\ntable_size | index_size −−−−−−−−−−−−+−−−−−−−−−−−−\n\n67 MB (1 row)\n\n| 109 MB\n\nHowever, data density has dropped about �� times:\n\n=> SELECT vac.tuple_percent, vac_s.avg_leaf_density FROM pgstattuple('vac') vac, pgstatindex('vac_s') vac_s;\n\ntuple_percent | avg_leaf_density −−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−\n\n9.13 |\n\n6.71\n\n(1 row)\n\nThe table and the index are currently located in the following files:\n\n=> SELECT pg_relation_filepath('vac') AS vac_filepath,\n\npg_relation_filepath('vac_s') AS vac_s_filepath \\gx\n\n−[ RECORD 1 ]−−+−−−−−−−−−−−−−−−−− vac_filepath | base/16391/16514 vac_s_filepath | base/16391/16515\n\nLet’s check what we will get after ������ ����. While the command is running, its progress can be tracked in the pg_stat_progress_cluster view (which is similar to the pg_stat_progress_vacuum view provided for ������):\n\n=> VACUUM FULL vac;\n\n=> SELECT * FROM pg_stat_progress_cluster \\gx\n\n−[ RECORD 1 ]−−−−−−−+−−−−−−−−−−−−−−−−− pid datid datname relid command phase cluster_index_relid | 0 heap_tuples_scanned | 50000 heap_tuples_written | 50000 heap_blks_total heap_blks_scanned index_rebuild_count | 0\n\n| 19488 | 16391 | internals | 16479 | VACUUM FULL | rebuilding index\n\n| 8621 | 8621\n\n161\n\nv. ��",
      "content_length": 1346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Chapter 8 Rebuilding Tables and Indexes\n\nExpectedly, ������ ���� phases1 differ from those of routine vacuuming.\n\nFull vacuuming has replaced old files with new ones:\n\n=> SELECT pg_relation_filepath('vac') AS vac_filepath,\n\npg_relation_filepath('vac_s') AS vac_s_filepath \\gx\n\n−[ RECORD 1 ]−−+−−−−−−−−−−−−−−−−− | base/16391/16526 vac_filepath vac_s_filepath | base/16391/16529\n\nBoth index and table sizes are much smaller now:\n\n=> SELECT pg_size_pretty(pg_table_size('vac')) AS table_size, pg_size_pretty(pg_indexes_size('vac')) AS index_size;\n\ntable_size | index_size −−−−−−−−−−−−+−−−−−−−−−−−−\n\n6904 kB\n\n| 6504 kB\n\n(1 row)\n\nAs a result, data density has increased. For the index, it is even higher than the original one: it is more efficient to create a �-tree from scratch based on the avail- able data than to insert entries row by row into an already existing index:\n\n=> SELECT vac.tuple_percent,\n\nvac_s.avg_leaf_density\n\nFROM pgstattuple('vac') vac,\n\npgstatindex('vac_s') vac_s;\n\ntuple_percent | avg_leaf_density −−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−\n\n91.23 |\n\n91.08\n\n(1 row)\n\nFreezing\n\nWhen the table is being rebuilt, Postgre��� freezes its tuples because this opera- tion costs almost nothing compared to the rest of the work:\n\n1 postgresql.org/docs/14/progress-reporting.html#CLUSTER-PHASES\n\n162",
      "content_length": 1301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "8.1 Full Vacuuming\n\n=> SELECT * FROM heap_page('vac',0,0) LIMIT 5;\n\nctid\n\n| state\n\n| xmin\n\n| xmin_age | xmax\n\n−−−−−−−+−−−−−−−−+−−−−−−−+−−−−−−−−−−+−−−−−−\n\n(0,1) | normal | 861 f | (0,2) | normal | 861 f | (0,3) | normal | 861 f | (0,4) | normal | 861 f | (0,5) | normal | 861 f |\n\n5 | 0 a 5 | 0 a 5 | 0 a 5 | 0 a 5 | 0 a\n\n(5 rows)\n\nBut pages are registered neither in the visibility map nor in the freeze map,and the page header does not receive the visibility attribute (as it happens when the ���� command is executed with the ������ option\n\n):\n\n=> SELECT * FROM pg_visibility_map('vac',0);\n\nall_visible | all_frozen −−−−−−−−−−−−−+−−−−−−−−−−−−\n\nf\n\n| f\n\n(1 row)\n\n=> SELECT flags & 4 > 0 all_visible FROM page_header(get_raw_page('vac',0));\n\nall_visible −−−−−−−−−−−−−\n\nf\n\n(1 row)\n\nThe situation improves only after ������ is called (or autovacuum is triggered):\n\n=> VACUUM vac;\n\n=> SELECT * FROM pg_visibility_map('vac',0);\n\nall_visible | all_frozen −−−−−−−−−−−−−+−−−−−−−−−−−−\n\nt\n\n| t\n\n(1 row)\n\n=> SELECT flags & 4 > 0 AS all_visible FROM page_header(get_raw_page('vac',0));\n\nall_visible −−−−−−−−−−−−−\n\nt\n\n(1 row)\n\n163\n\np. ���",
      "content_length": 1125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "p. ���\n\np. ���\n\np. ��\n\np. ���\n\nChapter 8 Rebuilding Tables and Indexes\n\nIt essentially means that even if all tuples in a page are beyond the database hori- zon, such a page will still have to be rewritten.\n\n8.2 Other Rebuilding Methods\n\nAlternatives to Full Vacuuming\n\nIn addition to������ ����,there are several other commands that can fully rebuild tablesandindexes. Allofthemexclusivelylockthetable,allofthemdeleteolddata files and recreate them anew.\n\nThe ������� command is fully analogous to ������ ����,but it also reorders tuples in files based on one of the available indexes. In some cases,it can help the planner use index scans more efficiently . But you should bear in mind that clusterization is not supported: all further table updates will be breaking the physical order of tuples.\n\nProgrammatically, ������ ���� is simply a special instance of the ������� com- mand that does not require tuple reordering.1\n\nThe ������� command rebuilds one or more indexes.2 In fact, ������ ���� and ������� use this command under the hood when rebuilding indexes.\n\nThe �������� command3 deletes all table rows; it is a logical equivalent of ������ runwithoutthe�����clause. Butwhile������ simplymarksheaptuplesasdeleted (so they still have to be vacuumed), �������� creates a new empty file, which is usually faster.\n\nReducing Downtime During Rebuilding\n\n������ ���� is not meant to be run regularly, as it exclusively locks the table (even for queries) for the whole duration of its operation. This is usually not an option for highly available systems.\n\n1 backend/commands/cluster.c 2 backend/commands/indexcmds.c 3 backend/commands/tablecmds.c, ExecuteTruncate function\n\n164",
      "content_length": 1680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "8.3 Precautions\n\nThere are several extensions (such as pg_repack1) that can rebuild tables and in- dexeswithalmostzerodowntime. Anexclusivelockisstillrequired,butonlyatthe beginning and at the end of this process, and only for a short time. It is achieved by a more complex implementation: all the changes made on the original table while it is being rebuilt are saved by a trigger and then applied to the new table. To complete the operation, the utility replaces one table with the other in the system catalog.\n\nAn unconventional solution is offered by the pgcompacttable utility.2 It performs multiple fake row updates (that do not change any data) so that current row ver- sions gradually move towards the start of the file.\n\nBetween these update series, vacuuming removes outdated tuples and truncates the file little by little. This approach takes much more time and resources, but it requires no extra space for rebuilding the table and does not lead to load spikes. Short-time exclusive locks are still acquired while the table is being truncated, but vacuuming handles them rather smoothly.\n\n8.3 Precautions\n\nRead-Only Queries\n\nOne of the reasons for file bloating is executing long-running transactions that hold the database horizon\n\nalongside intensive data updates.\n\nAs such, long-running (read-only) transactions do not cause any issues. So a com- monapproachistosplittheloadbetweendifferentsystems: keepfast����queries on the primary server and direct all ���� transactions to a replica. Although it makes the solution more expensive and complicated, such measures may turn out to be indispensable.\n\nIn some cases, long transactions are the result of application or driver bugs rather than a necessity. If an issue cannot be resolved in a civilized way,the administrator can resort to the following two parameters:\n\n1 github.com/reorg/pg_repack 2 github.com/dataegret/pgcompacttable\n\n165\n\np. ���\n\np. ���",
      "content_length": 1918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "v. �.�\n\nv. �.�\n\nChapter 8 Rebuilding Tables and Indexes\n\nThe old_snapshot_threshold\n\nparameter defines the maximum lifetime of a snapshot. Once this time is up, the server has the right to remove outdated tuples; if a long-running transaction still requires them, it will get an error (“snapshot too old”).\n\nTheidle_in_transaction_session_timeout\n\nparameterlimitsthelifetimeofanidle\n\ntransaction. The transaction is aborted upon reaching this threshold.\n\nData Updates\n\nAnother reason for bloating is simultaneous modification of a large number of tu- ples. If alltable rowsget updated,thenumber oftuples candouble,and vacuuming will not have enough time to interfere. Page pruning can reduce this problem, but not resolve it entirely.\n\nLet’s extend the output with another column to keep track of the processed rows:\n\n=> ALTER TABLE vac ADD processed boolean DEFAULT false;\n\n=> SELECT pg_size_pretty(pg_table_size('vac'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n6936 kB\n\n(1 row)\n\nOnce all the rows are updated, the table gets almost two times bigger:\n\n=> UPDATE vac SET processed = true;\n\nUPDATE 50000\n\n=> SELECT pg_size_pretty(pg_table_size('vac'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n14 MB (1 row)\n\nTo address this situation, you can reduce the number of changes performed by a single transaction, spreading them out over time; then vacuuming can delete out- dated tuples and free some space for new ones within the already existing pages. Assuming that each row update can be committed separately, we can use the fol- lowing query that selects a batch of rows of the specified size as a template:\n\n166",
      "content_length": 1598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "8.3 Precautions\n\nSELECT ID FROM table WHERE filtering the already processed rows LIMIT batch size FOR UPDATE SKIP LOCKED\n\nThis code snippet selects and immediately locks a set of rows that does not ex- ceed the specified size. The rows that are already locked by other transactions are skipped : they will get into another batch next time. It is a rather flexible and con- venient solution that allows you to easily change the batch size and restart the operation in case of a failure. Let’s unset the processed attribute and perform full vacuuming to restore the original size of the table:\n\n=> UPDATE vac SET processed = false;\n\n=> VACUUM FULL vac;\n\nOnce the first batch is updated, the table size grows a bit:\n\n=> WITH batch AS (\n\nSELECT id FROM vac WHERE NOT processed LIMIT 1000 FOR UPDATE SKIP LOCKED\n\n) UPDATE vac SET processed = true WHERE id IN (SELECT id FROM batch);\n\nUPDATE 1000\n\n=> SELECT pg_size_pretty(pg_table_size('vac'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n7064 kB\n\n(1 row)\n\nBut from now on,the size remains almost the same because new tuples replace the removed ones:\n\n=> VACUUM vac;\n\n=> WITH batch AS (\n\nSELECT id FROM vac WHERE NOT processed LIMIT 1000 FOR UPDATE SKIP LOCKED\n\n) UPDATE vac SET processed = true WHERE id IN (SELECT id FROM batch);\n\nUPDATE 1000\n\n167\n\np. ���",
      "content_length": 1292,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "Chapter 8 Rebuilding Tables and Indexes\n\n=> SELECT pg_size_pretty(pg_table_size('vac'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n7072 kB\n\n(1 row)\n\n168",
      "content_length": 144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "Part II\n\nBuffer Cache and WAL",
      "content_length": 29,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "9\n\nBuffer Cache\n\n9.1 Caching\n\nIn modern computing systems, caching is omnipresent—both at the hardware and at the software level. The processor alone can have up to three or four levels of cache. R��� controllers and disks add their own cache too.\n\nCaching is used to even out performance difference between fast and slow types of memory. Fast memory is expensive and has smaller volume, while slow memory is bigger and cheaper. Therefore, fast memory cannot accommodate all the data stored in slow memory. But in most cases only a small portion of data is being actively used at each particular moment,so allocating some fast memory for cache to keep hot data can significantly reduce the overhead incurred by slow memory access.\n\nIn Postgre���, buffer cache1 holds relation pages, thus balancing access times to disks (milliseconds) and ��� (nanoseconds).\n\nThe operating system has its own cache that serves the same purpose. For this reason, database systems are usually designed to avoid double caching: the data stored on disk is usually queried directly, bypassing the �� cache. But Postgre��� uses a different approach: it reads and writes all data via buffered file operations.\n\nIt will reduce the overhead, as Double caching can be avoided if you apply direct �/�. Postgre��� will use direct memory access (���) instead of copying buffered pages into the �� address space; besides, you will gain immediate control over physical writes on disk. However, direct �/� does not support data prefetching enabled by bufferization, so , which requires massive code you have to implement it separately via asynchronous �/�\n\n1 backend/storage/buffer/README\n\n171\n\np. ���",
      "content_length": 1668,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "Chapter 9 Buffer Cache\n\nmodifications in Postgre��� core, as well as handling �� incompatibilities when it comes to direct and asynchronous �/� support. But once the asynchronous communication is set up,you can enjoy additional benefits of no-wait disk access.\n\nThe Postgre��� community has already started this major effort,1 but it will take a long time for the actual results to appear.\n\n9.2 Buffer Cache Design\n\nBuffer cache is located in the server’s shared memory and is accessible to all the processes. It takes the major part of the shared memory and is surely one of the most important and complex data structures in Postgre���. Understanding how cache works is important in its own right, but even more so as many other struc- tures (such as subtransactions, ���� transaction status, and ��� entries) use a similar caching mechanism, albeit a simpler one.\n\nThe name of this cache is inspired by its inner structure,as it consists of an array of buffers. Each buffer reserves a memory chuck that can accommodate a single data page together with its header.2\n\nbuffer cache\n\nheader\n\npage\n\nA header contains some information about the buffer and the page in it, such as:\n\nphysical location of the page (file ��, fork, and block number in the fork)\n\nthe attribute showing that the data in the page has been modified and sooner\n\nor later has to be written back to disk (such a page is called dirty)\n\nbuffer usage count\n\npin count (or reference count)\n\n1 www.postgresql.org/message-id/flat/20210223100344.llw5an2aklengrmn%40alap3.anarazel.de 2 include/storage/buf_internals.h\n\n172",
      "content_length": 1583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "9.2 Buffer Cache Design\n\nTo get access to a relation’s data page, a process requests it from the buffer man- ager1 and receives the �� of the buffer that contains this page. Then it reads the cached data and modifies it right in the cache if needed. While the page is in use, its buffer is pinned. Pins forbid eviction of the cached page and can be applied together with other locks\n\n. Each pin increments the usage count as well.\n\nAs long as the page is cached, its usage does not incur any file operations.\n\nWe can explore the buffer cache using the pg_buffercache extension:\n\n=> CREATE EXTENSION pg_buffercache;\n\nLet’s create a table and insert a row:\n\n=> CREATE TABLE cacheme(\n\nid integer\n\n) WITH (autovacuum_enabled = off);\n\n=> INSERT INTO cacheme VALUES (1);\n\nNow the buffer cache contains a heap page with the newly inserted row. You can see it for yourself by selecting all the buffers related to a particular table. We will need such a query again, so let’s wrap it into a function:\n\n=> CREATE FUNCTION buffercache(rel regclass) RETURNS TABLE(\n\nbufferid integer, relfork text, relblk bigint, isdirty boolean, usagecount smallint, pins integer\n\n) AS $$ SELECT bufferid,\n\nCASE relforknumber\n\nWHEN 0 THEN 'main' WHEN 1 THEN 'fsm' WHEN 2 THEN 'vm'\n\nEND, relblocknumber, isdirty, usagecount, pinning_backends FROM pg_buffercache WHERE relfilenode = pg_relation_filenode(rel) ORDER BY relforknumber, relblocknumber; $$ LANGUAGE sql;\n\n1 backend/storage/buffer/bufmgr.c\n\n173\n\np. ���",
      "content_length": 1483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Chapter 9 Buffer Cache\n\n=> SELECT * FROM buffercache('cacheme');\n\nbufferid | relfork | relblk | isdirty | usagecount | pins −−−−−−−−−−+−−−−−−−−−+−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−−+−−−−−− 0 | t\n\n268 | main\n\n|\n\n|\n\n1 |\n\n0\n\n(1 row)\n\nThe page is dirty: it has been modified, but is not written to disk yet. Its usage count is set to one.\n\n9.3 Cache Hits\n\nWhen the buffer manager has to read a page,1 it first checks the buffer cache.\n\nAll buffer ��s are stored in a hash table,2 which is used to speed up their search.\n\nMany modern programming languages include hash tables as one of the basic data types. Hash tables are often called associative arrays, and indeed, from the user’s perspective they do look like an array; however, their index (a hash key) can be of any data type, for example, a text string rather than an integer.\n\nWhile the range of possible key values can be quite large, hash tables never contain that many different values at a time. The idea of hashing is to convert a key value into an integer number using a hash function. This number (or some of its bits) is used as an index of a regular array. The elements of this array are called hash table buckets.\n\nA good hash function distributes hash keys between buckets more or less uniformly,but it can still assign the same number to different keys,thus placing them into the same bucket; it is called a collision. For this reason,values are stored in buckets together with hash keys; to access a hashed value by its key, Postgre��� has to check all the keys in the bucket.\n\nThere are multiple implementations of hash tables; of all the possible options,the buffer cache uses the extendible table that resolves hash collisions by chaining.3\n\nA hash key consists of the �� of the relation file, the type of the fork, and the �� of the page within this fork’s file. Thus, knowing the page, Postgre��� can quickly find the buffer containing this page or make sure that the page is not currently cached.\n\n1 backend/storage/buffer/bufmgr.c, ReadBuffer_common function 2 backend/storage/buffer/buf_table.c 3 backend/utils/hash/dynahash.c\n\n174",
      "content_length": 2101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "9.3 Cache Hits\n\n3501,0,3\n\n2610,0,7\n\nhash table\n\nThe buffer cache implementation has long been criticized for relying on a hash table: this structure is of no use when it comes to finding all the buffers taken bypages of a particular relation,which is required to remove pages from cache when running ���� and �������� commands or truncating a table duringvacuuming.1 Yet no one has suggested an adequate alternative so far.\n\nIfthehashtablecontainstherequiredbuffer��,thebuffermanagerpinsthisbuffer and returns its �� to the process. Then this process can start using the cached page without incurring any �/� traffic.\n\nTo pin a buffer, Postgre��� has to increment the pin counter in its header; a buffer can be pinned by several processes at a time. While its pin counter is greater than zero, the buffer is assumed to be in use, and no radical changes in its contents are allowed. For example, a new tuple can appear (it will be invisible following the visibility rules), but the page itself cannot be replaced.\n\nWhen run with the analyze and buffers options, the ������� command executes the displayed query plan and shows the number of used buffers:\n\n1 backend/storage/buffer/bufmgr.c, DropRelFileNodeBuffers function\n\n175",
      "content_length": 1225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Chapter 9 Buffer Cache\n\n=> EXPLAIN (analyze, buffers, costs off, timing off, summary off)\n\nSELECT * FROM cacheme;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Seq Scan on cacheme (actual rows=1 loops=1)\n\nBuffers: shared hit=1\n\nPlanning:\n\nBuffers: shared hit=12 read=7\n\n(4 rows)\n\nHere hit=1 means that the only page that had to be read was found in the cache.\n\nBuffer pinning increases the usage count by one:\n\n=> SELECT * FROM buffercache('cacheme');\n\nbufferid | relfork | relblk | isdirty | usagecount | pins −−−−−−−−−−+−−−−−−−−−+−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−−+−−−−−− 0 | t\n\n268 | main\n\n|\n\n|\n\n2 |\n\n0\n\n(1 row)\n\nTo observe pinning in action during query execution, let’s open a cursor—it will hold the buffer pin, as it has to provide quick access to the next row in the result set:\n\n=> BEGIN;\n\n=> DECLARE c CURSOR FOR SELECT * FROM cacheme;\n\n=> FETCH c;\n\nid −−−− 1\n\n(1 row)\n\n=> SELECT * FROM buffercache('cacheme');\n\nbufferid | relfork | relblk | isdirty | usagecount | pins −−−−−−−−−−+−−−−−−−−−+−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−−+−−−−−− 0 | t\n\n268 | main\n\n|\n\n|\n\n3 |\n\n1\n\n(1 row)\n\nIf a process cannot use a pinned buffer, it usually skips it and simply chooses an- other one. We can see it during table vacuuming:\n\n176",
      "content_length": 1228,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "9.3 Cache Hits\n\n=> VACUUM VERBOSE cacheme;\n\nINFO: INFO: versions in 1 out of 1 pages DETAIL: 877 Skipped 1 page due to buffer pins, 0 frozen pages. CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s. VACUUM\n\nvacuuming \"public.cacheme\" table \"cacheme\": found 0 removable, 0 nonremovable row\n\n0 dead row versions cannot be removed yet, oldest xmin:\n\nThe page was skipped because its tuples could not be physically removed from the pinned buffer.\n\nBut if it is exactly this buffer that is required,the process joins the queue and waits for exclusive access to this buffer. An example of such an operation is vacuuming with freezing.1\n\nOnce the cursor closes or moves on to another page, the buffer gets unpinned. In this example, it happens at the end of the transaction:\n\n=> COMMIT;\n\n=> SELECT * FROM buffercache('cacheme');\n\nbufferid | relfork | relblk | isdirty | usagecount | pins −−−−−−−−−−+−−−−−−−−−+−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−−+−−−−−− 0 | t 0 | f\n\n268 | main 310 | vm\n\n| |\n\n| |\n\n3 | 2 |\n\n0 0\n\n(2 rows)\n\nPage modifications are protected by the same pinning mechanism. For example, let’s insert another row into the table (it will get into the same page):\n\n=> INSERT INTO cacheme VALUES (2);\n\n=> SELECT * FROM buffercache('cacheme');\n\nbufferid | relfork | relblk | isdirty | usagecount | pins −−−−−−−−−−+−−−−−−−−−+−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−−+−−−−−− 0 | t 0 | f\n\n268 | main 310 | vm\n\n| |\n\n| |\n\n4 | 2 |\n\n0 0\n\n(2 rows)\n\n1 backend/storage/buffer/bufmgr.c, LockBufferForCleanup function\n\n177\n\np. ���",
      "content_length": 1503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "p. ��\n\nChapter 9 Buffer Cache\n\nPostgre��� does not perform any immediate writes to disk: a page remains dirty in the buffer cache for a while, providing some performance gains for both reads and writes.\n\n9.4 Cache Misses\n\nIf the hash table has no entry related to the queried page, it means that this page is not cached. In this case, a new buffer is assigned (and immediately pinned), the page is read into this buffer,and the hash table references are modified accordingly.\n\nLet’s restart the instance to clear its buffer cache:\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\nAn attempt to read a page will result in a cache miss, and the page will be loaded into a new buffer:\n\n=> EXPLAIN (analyze, buffers, costs off, timing off, summary off)\n\nSELECT * FROM cacheme;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Seq Scan on cacheme (actual rows=2 loops=1)\n\nBuffers: shared read=1 dirtied=1\n\nPlanning:\n\nBuffers: shared hit=15 read=7\n\n(4 rows)\n\nInstead of hit, the plan now shows the read status, which denotes a cache miss. . Besides, this page has become dirty, as the query has modified some hint bits\n\nA buffer cache query shows that the usage count for the newly added page is set to one:\n\n=> SELECT * FROM buffercache('cacheme');\n\nbufferid | relfork | relblk | isdirty | usagecount | pins −−−−−−−−−−+−−−−−−−−−+−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−−+−−−−−− 0 | t\n\n98 | main\n\n|\n\n|\n\n1 |\n\n0\n\n(1 row)\n\n178",
      "content_length": 1420,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "9.4 Cache Misses\n\nThepg_statio_all_tablesviewcontainsthecompletestatisticsonbuffercacheusage by tables:\n\n=> SELECT heap_blks_read, heap_blks_hit FROM pg_statio_all_tables WHERE relname = 'cacheme';\n\nheap_blks_read | heap_blks_hit −−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−− 2 |\n\n5\n\n(1 row)\n\nPostgre���providessimilarviewsforindexesandsequences. Theycanalsodisplay statistics on �/� operations, but only if\n\ntrack_io_timing is enabled.\n\nBuffer Search and Eviction\n\nChoosing a buffer for a page is not so trivial.1 There are two possible scenarios:\n\n1. Right after the server start all the buffers are empty and are bound into a list.\n\nWhile some buffers are still free, the next page read from disk will occupy the first buffer, and it will be removed from the list.\n\nA buffer can return to the list2 only if its page disappears, without being re- placed by another page. It can happen if you call ���� or �������� commands, or if the table is truncated during vacuuming.\n\n2. Sooner or later no free buffers will be left (since the size of the database is usually bigger than the memory chunk allocated for cache). Then the buffer manager will have to select one of the buffers that is already in use and evict the cached page from this buffer. It is performed using the clock sweep algo- rithm, which is well illustrated by the clock metaphor. Pointing to one of the buffers, the clock hand starts going around the buffer cache and reduces the usage countforeach cached page by one as it passes. The first unpinned buffer with the zero count found by the clock hand will be cleared.\n\n1 backend/storage/buffer/freelist.c, StrategyGetBuffer function 2 backend/storage/buffer/freelist.c, StrategyFreeBuffer function\n\n179\n\noff",
      "content_length": 1711,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "p. ���\n\nChapter 9 Buffer Cache\n\nThus, the usage count is incremented each time the buffer is accessed (that is, pinned), and reduced when the buffer manager is searching for pages to evict. Asaresult,theleastrecentlyusedpagesareevictedfirst,whilethosethathave been accessed more often will remain in the cache longer.\n\nAs you can guess,if all the buffers have a non-zero usage count,the clock hand has to complete more than one full circle before any of them finally reaches the zero value. To avoid running multiple laps, Postgre��� limits the usage count by �.\n\nOnce the buffer to evict is found, the reference to the page that is still in this buffer must be removed from the hash table.\n\nBut if this buffer is dirty, that is, it contains some modified data, the old page cannot be simply thrown away—the buffer manager has to write it to disk first.\n\nfree buffers\n\nclock hand\n\nThen the buffer manager reads a new page into the found buffer—no matter if it had to be cleared or was still free. It uses buffered �/� for this purpose, so the page will be read from disk only if the operating system cannot find it in its own cache.\n\n180",
      "content_length": 1137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "9.5 Bulk Eviction\n\nThose database systems that use direct �/� and do not depend on the �� cache differentiate between logical reads (from ���, that is, from the buffer cache) and physical reads (from disk). From the standpoint of Postgre���, a page can be either read from the buffer cache or requested from the operating system, but there is no way to tell whether it was found in ��� or read from disk in the latter case.\n\nThe hash table is updated to refer to the new page, and the buffer gets pinned. Its usage count is incremented and is now set to one, which gives this buffer some time to increase this value while the clock hand is traversing the buffer cache.\n\n9.5 Bulk Eviction\n\nIf bulk reads or writes are performed, there is a risk that one-time data can quickly oust useful pages from the buffer cache.\n\nAs a precaution, bulk operations use rather small buffer rings, and eviction is per- formed within their boundaries, without affecting other buffers.\n\nAlongside the“buffer ring,”the code also uses the term“ring buffer”. However,this synonym is rather ambiguous because the ring buffer itself consists of several buffers (that belong to the buffer cache). The term “buffer ring”is more accurate in this respect.\n\nA buffer ring of a particular size consists of an array of buffers that are used one after another. At first,the buffer ring is empty,and individual buffers join it one by one, after being selected from the buffer cache in the usual manner. Then eviction comes into play, but only within the ring limits.1\n\nBuffers added into a ring are not excluded from the buffer cache and can still be used by other operations. So if the buffer to be reused turns out to be pinned, or its usage count is higher than one, it will be simply detached from the ring and replaced by another buffer.\n\nPostgre��� supports three eviction strategies.\n\nBulk reads strategy is used for sequential scans\n\nof large tables if their size exceeds\n\n1 4\n\nof the buffer cache. The ring buffer takes ��� k� (�� standard pages).\n\n1 backend/storage/buffer/freelist.c, GetBufferFromRing function\n\n181\n\np. ���",
      "content_length": 2101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "p. ��\n\nChapter 9 Buffer Cache\n\nThisstrategydoesnotallowwritingdirtypagestodisktofreeabuffer; instead, the buffer is excluded from the ring and replaced by another one. As a result, reading does not have to wait for writing to complete,so it is performed faster.\n\nIf it turns out that the table is already being scanned, the process that starts another scan joins the existing buffer ring and gets access to the currently available data,without incurring extra �/� operations.1 When the first process completes the scan, the second one gets back to the skipped part of the table.\n\nBulk writes strategy is applied by ���� ����, ������ ����� �� ������, and ������ ��- ���������� ���� commands, as well as by those ����� ����� flavors that cause table rewrites. The allocated ring is quite big, its default size being �� �� (���� standard pages), but it never exceeds 1 of the total size of the buffer 8 cache.\n\nVacuuming strategy is used by the process of vacuuming when it performs a full table scan without taking the visibility map into account. The ring buffer is assigned ��� k� of ��� (�� standard pages).\n\nBuffer rings do not always prevent undesired eviction. If ������ or ������ com- mands affect a lot of rows,the performed table scan applies the bulk reads strategy, but since the pages are constantly being modified, buffer rings virtually become useless.\n\nAnother example worth mentioning is storing oversized data in ����� tables. In spite of a potentially large volume of data that has to be read, toasted values are always accessed via an index, so they bypass buffer rings.\n\nLet’s take a closer look at the bulk reads strategy. For simplicity, we will create a tableinsuchawaythataninsertedrowtakesthewholepage. Bydefault,thebuffer cache size is ��,��� pages,� k� each. So the table must take more than ���� pages for the scan to use a buffer ring.\n\n=> CREATE TABLE big(\n\nid integer PRIMARY KEY GENERATED ALWAYS AS IDENTITY, s char(1000)\n\n) WITH (fillfactor = 10);\n\n1 backend/access/common/syncscan.c\n\n182",
      "content_length": 2019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "9.5 Bulk Eviction\n\n=> INSERT INTO big(s)\n\nSELECT 'FOO' FROM generate_series(1,4096+1);\n\nLet’s analyze the table:\n\n=> ANALYZE big;\n\n=> SELECT relname, relfilenode, relpages FROM pg_class WHERE relname IN ('big', 'big_pkey');\n\nrelname\n\n| relfilenode | relpages\n\n−−−−−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−\n\n| big big_pkey |\n\n16546 | 16551 |\n\n4097 14\n\n(2 rows)\n\nRestart the server to clear the cache,as now it contains some heap pages that have been read during analysis.\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\nOnce the server is restarted, let’s read the whole table:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT id FROM big;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Seq Scan on big (actual rows=4097 loops=1)\n\n(1 row)\n\nHeap pages take only �� buffers, which make up the buffer ring for this operation:\n\n=> SELECT count(*) FROM pg_buffercache WHERE relfilenode = pg_relation_filenode('big'::regclass);\n\ncount −−−−−−−\n\n32\n\n(1 row)\n\nBut in the case of an index scan the buffer ring is not used:\n\n183",
      "content_length": 1037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "128MB\n\nChapter 9 Buffer Cache\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM big ORDER BY id;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using big_pkey on big (actual rows=4097 loops=1)\n\n(1 row)\n\nAs a result, the buffer cache ends up containing the whole table and the whole index:\n\n=> SELECT relfilenode, count(*) FROM pg_buffercache WHERE relfilenode IN (\n\npg_relation_filenode('big'), pg_relation_filenode('big_pkey')\n\n) GROUP BY relfilenode;\n\nrelfilenode | count −−−−−−−−−−−−−+−−−−−−− 16546 | 16551 |\n\n4097 14\n\n(2 rows)\n\n9.6 Choosing the Buffer Cache Size\n\nshared_buffers parameter. Its default The size of the buffer cache is defined by the value is known to be low,so it makes sense to increase it right after the Postgre��� installation. You will have to reload the server in this case because shared memory is allocated for cache at the server start.\n\nBut how can we determine an appropriate value?\n\nEven a very large database has a limited set of hot data that is being used simulta- neously. In the perfect world,it is this set that must fit the buffer cache (with some space being reserved for one-time data). If the cache size is smaller, the actively used pages will be evicting each other all the time,thus leading to excessive �/� op- erations. But thoughtless increase of the cache size is not a good idea either: ��� is a scarce resource, and besides, larger cache incurs higher maintenance costs.\n\n184",
      "content_length": 1486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "9.6 Choosing the Buffer Cache Size\n\nThe optimal buffer cache size differs from system to system: it depends on things like the total size of the available memory, data profiles, and workload types. Un- fortunately, there is no magic value or formula to suit everyone equally well.\n\nYou should also keep in mind that a cache miss in Postgre��� does not necessarily trigger a physical �/� operation. If the buffer cache is quite small, the �� cache uses the remaining free memory and can smooth things out to some extent. But unlike the database, the operating system knows nothing about the read data, so it applies a different eviction strategy.\n\nA typical recommendation is to start with 1 4 required.\n\nof ��� and then adjust this setting as\n\nThe best approach is experimentation: you can increase or decrease the cache size and compare the system performance. Naturally, it requires having a test system that is fully analogous to the production one, and you must be able to reproduce typical workloads.\n\nYou can also run some analysis using the pg_buffercache extension. For example, explore buffer distribution depending on their usage:\n\n=> SELECT usagecount, count(*) FROM pg_buffercache GROUP BY usagecount ORDER BY usagecount;\n\nusagecount | count −−−−−−−−−−−−+−−−−−−−\n\n1 | 2 | 3 | 4 | 5 |\n\n4128 50 4 4 73 | 12125\n\n(6 rows)\n\nN��� usage count values correspond to free buffers. They are quite expected in this case because the server was restarted and remained idle most of the time. The majority of the used buffers contain pages of the system catalog tables read by the backend to fill its system catalog cache and to perform queries.\n\nWe can check what fraction of each relation is cached,and whether this data is hot (a page is considered hot here if its usage count is bigger than one):\n\n185",
      "content_length": 1801,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "Chapter 9 Buffer Cache\n\n=> SELECT c.relname, count(*) blocks, round( 100.0 * 8192 * count(*) /\n\npg_table_size(c.oid) ) AS \"% of rel\",\n\nround( 100.0 * 8192 * count(*) FILTER (WHERE b.usagecount > 1) /\n\npg_table_size(c.oid) ) AS \"% hot\"\n\nFROM pg_buffercache b\n\nJOIN pg_class c ON pg_relation_filenode(c.oid) = b.relfilenode\n\nWHERE b.reldatabase IN (\n\n0, -- cluster-wide objects (SELECT oid FROM pg_database WHERE datname = current_database())\n\n) AND b.usagecount IS NOT NULL GROUP BY c.relname, c.oid ORDER BY 2 DESC LIMIT 10;\n\nrelname\n\n| blocks | % of rel | % hot\n\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−+−−−−−−−−−−+−−−−−−−\n\n| big | pg_attribute | big_pkey | pg_proc | pg_operator | pg_class pg_proc_oid_index | pg_attribute_relid_attnum_index | | pg_proc_proname_args_nsp_index | pg_amproc\n\n4097 | 30 | 14 | 13 | 11 | 10 | 9 | 8 | 6 | 5 |\n\n100 | 48 | 100 | 12 | 61 | 59 | 82 | 73 | 18 | 56 |\n\n1 47 0 6 50 59 45 64 6 56\n\n(10 rows)\n\nThisexampleshowsthatthe bigtableanditsindexarefullycached,buttheirpages are not being actively used.\n\nAnalyzing data from different angles, you can gain some useful insights. However, make sure to follow these simple rules when running pg_buffercache queries:\n\nRepeat such queries several times since the returned figures will vary to some\n\nextent.\n\nDo not run such queries non-stop because the pg_buffercache extension locks\n\nthe viewed buffers, even if only briefly.\n\n186",
      "content_length": 1407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "9.7 Cache Warming\n\n9.7 Cache Warming\n\nAfter a server restart, the cache requires some time to warm up, that is, to accu- mulate the actively used data. It may be helpful to cache certain tables right away, and the pg_prewarm extension serves exactly this purpose:\n\n=> CREATE EXTENSION pg_prewarm;\n\nApart from loading tables into the buffer cache (or into the �� cache only), this extension can write the current cache state to disk and then restore it after the server restart. To enable this functionality,you have to add this extension’s library to shared_preload_libraries and restart the server:\n\n=> ALTER SYSTEM SET shared_preload_libraries = 'pg_prewarm';\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\npg_prewarm.autoprewarm setting has not changed, a process called auto- If the prewarm leader will be started automatically after the server is reloaded; once in pg_prewarm.autoprewarm_interval seconds,this process will flush the list of cached pages to disk (using one of the max_parallel_processes slots).\n\npostgres$ ps -o pid,command \\ --ppid `head -n 1 /usr/local/pgsql/data/postmaster.pid` | \\ grep prewarm\n\n23124 postgres: autoprewarm leader\n\nNow that the server has been restarted, the big table is not cached anymore:\n\n=> SELECT count(*) FROM pg_buffercache WHERE relfilenode = pg_relation_filenode('big'::regclass);\n\ncount −−−−−−−\n\n0\n\n(1 row)\n\n187\n\nv. ��\n\non\n\n300s",
      "content_length": 1387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Chapter 9 Buffer Cache\n\nIf you have well-grounded assumptions that the whole table is going to be actively used and disk access will make response times unacceptably high,you can load this table into the buffer cache in advance:\n\n=> SELECT pg_prewarm('big');\n\npg_prewarm −−−−−−−−−−−−\n\n4097\n\n(1 row)\n\n=> SELECT count(*) FROM pg_buffercache WHERE relfilenode = pg_relation_filenode('big'::regclass);\n\ncount −−−−−−− 4097 (1 row)\n\nThe list of pages is dumped into the ������/autoprewarm.blocks file. You can wait until the autoprewarm leader completes for the first time, but we will initiate the dump manually:\n\n=> SELECT autoprewarm_dump_now();\n\nautoprewarm_dump_now −−−−−−−−−−−−−−−−−−−−−−\n\n4224\n\n(1 row)\n\nThe number of flushed pages is bigger than ���� because all the used buffers are taken into account. The file is written in a text format; it contains the ��s of the database, tablespace, and file, as well as the fork and segment numbers:\n\npostgres$ head -n 10 /usr/local/pgsql/data/autoprewarm.blocks\n\n<<4224>> 0,1664,1262,0,0 0,1664,1260,0,0 16391,1663,1259,0,0 16391,1663,1259,0,1 16391,1663,1259,0,2 16391,1663,1259,0,3 16391,1663,1249,0,0 16391,1663,1249,0,1 16391,1663,1249,0,2\n\n188",
      "content_length": 1192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "9.8 Local Cache\n\nLet’s restart the server again.\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\nThe table appears in the cache right away:\n\n=> SELECT count(*) FROM pg_buffercache WHERE relfilenode = pg_relation_filenode('big'::regclass);\n\ncount −−−−−−− 4097 (1 row)\n\nIt is again the autoprewarm leader that does all the preliminary work: it reads the file,sorts the pages by databases,reorders them (so that disk reads happen sequen- tially if possible), and then passes them to the autoprewarm worker for processing.\n\n9.8 Local Cache\n\nTemporary tables do not follow the workflow described above. Since temporary data is visible to a single process only,there is no point in loading it into the shared buffer cache. Therefore, temporary data uses the local cache of the process that owns the table.1\n\nIn general, local buffer cache works similar to the shared one:\n\nPage search is performed via a hash table.\n\nEviction follows the standard algorithm (except that buffer rings are not used).\n\nPages can be pinned to avoid eviction.\n\nHowever, local cache implementation is much simpler because it has to handle neither locks on memory structures (buffers can be accessed by a single process only) nor fault tolerance (temporary data exists till the end of the session at the most).\n\n1 backend/storage/buffer/localbuf.c\n\n189\n\np. ��� p. ���",
      "content_length": 1342,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "8MB\n\nChapter 9 Buffer Cache\n\nSince only few sessions typically use temporary tables, local cache memory is as- signed on demand. The maximum size of the local cache available to a session is limited by the\n\ntemp_buffers parameter.\n\nDespite a similar name, the temp_file_limit parameter has nothing to do with temporary tables; it is related to files that maybe created during queryexecution to temporarilystore intermediate data.\n\nIn the ������� command output, all calls to the local buffer cache are tagged as local instead of shared:\n\n=> CREATE TEMPORARY TABLE tmp AS SELECT 1;\n\n=> EXPLAIN (analyze, buffers, costs off, timing off, summary off)\n\nSELECT * FROM tmp;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Seq Scan on tmp (actual rows=1 loops=1)\n\nBuffers: local hit=1\n\nPlanning:\n\nBuffers: shared hit=12 read=7\n\n(4 rows)\n\n190",
      "content_length": 840,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "10\n\nWrite-Ahead Log\n\n10.1 Logging\n\nIn case of a failure, such as a power outage, an �� error, or a database server crash, all the contents of ��� will be lost; only the data written to disk will persist. To starttheserverafterafailure,youhavetorestoredataconsistency. Ifthediskitself has been damaged, the same issue has to be resolved by backup recovery.\n\nIn theory, you could maintain data consistency on disk at all times. But in prac- tice it means that the server has to constantly write random pages to disk (even though sequential writing is cheaper), and the order of such writes must guaran- tee that consistency is not compromised at any particular moment (which is hard to achieve, especially if you deal with complex index structures).\n\nJust like the majority of database systems, Postgre��� uses a different approach.\n\nWhile the server is running, some of the current data is available only in ���, its writing to permanent storage being deferred. Therefore, the data stored on disk is always inconsistent during server operation, as pages are never flushed all at once. But each change that happens in ��� (such as a page update performed in the buffer cache) is logged: Postgre��� creates a log entry that contains all the essential information required to repeat this operation if the need arises.1\n\nA log entry related to a page modification must be written to disk ahead of the modified page itself. Hence the name of the log: write-ahead log, or ���. This requirement guarantees that in case of a failure Postgre��� can read ��� entries from disk and replay them to repeat the already completed operations whose re- sults were still in ��� and did not make it to disk before the crash.\n\n1 postgresql.org/docs/14/wal-intro.html\n\n191",
      "content_length": 1750,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Chapter 10 Write-Ahead Log\n\nKeeping a write-ahead log is usually more efficient than writing random pages to disk. W�� entries constitute a continuous stream of data, which can be handled even by ���s. Besides, ��� entries are often smaller than the page size.\n\nIt is required to log all operations that can potentially break data consistency in case of a failure. In particular, the following actions are recorded in ���:\n\npage modifications performed in the buffer cache—since writes are deferred\n\ntransaction commits and rollbacks—since the status change happens in ����\n\nbuffers and does not make it to disk right away\n\nfile operations (like creation and deletion of files and directories when ta- bles get added or removed)—since such operations must be in sync with data changes\n\nThe following actions are not logged:\n\noperations on �������� tables\n\noperations on temporary tables—since their lifetime is anyway limited by the\n\nsession that spawns them\n\nPrior to Postgre��� ��, hash indexes were not logged either. Their only purpose was to match hash functions to different data types.\n\nApart from crash recovery, ��� can also be used for point-in-time recovery from a backup and replication.\n\n10.2 WAL Structure\n\nLogical Structure\n\nSpeaking about its logical structure, we can describe ���1 as a stream of log en- triesofvariablelength. Eachentrycontainssomedataaboutaparticularoperation\n\n1 postgresql.org/docs/14/wal-internals.html\n\nbackend/access/transam/README\n\n192",
      "content_length": 1476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "10.2 WAL Structure\n\npreceded by a standard header.1 Among other things, the header provides the fol- lowing information:\n\ntransaction �� related to the entry\n\nthe resource manager that interprets the entry2\n\nthe checksum to detect data corruption\n\nentry length\n\na reference to the previous ��� entry\n\nW�� is usually read in the forward direction, but some utilities like pg_rewind may scan it backwards.\n\nW�� data itself can have different formats and meaning. For example, it can be a page fragment that has to replace some part of the page at the specified offset. The corresponding resource manager must know how to interpret and replay a particu- lar entry. There are separate managers for tables, various index types, transaction status, and other entities.\n\nW�� files take up special buffers in the server’s shared memory. The size of the wal_buffers parameter. By default, this size is cache used by ��� is defined by the chosen automatically as 1 32\n\nof the total buffer cache size.\n\nW�� cache is quite similar to buffer cache,but it usually operates in the ring buffer mode: new entries are added to its head,while older entries are saved to disk start- ing at the tail. If ��� cache is too small, disk synchronization will be performed more often than necessary.\n\nUnder low load,the insert position (the buffer’s head) is almost always the same as the position of the entries that have already been saved to disk (the buffer’s tail):\n\n=> SELECT pg_current_wal_lsn(), pg_current_wal_insert_lsn();\n\npg_current_wal_lsn | pg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3DF56000\n\n| 0/3DF57968\n\n(1 row)\n\n1 include/access/xlogrecord.h 2 include/access/rmgrlist.h\n\n193\n\n−1",
      "content_length": 1705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "Chapter 10 Write-Ahead Log\n\nPrior to Postgre��� ��, all function names contained the ���� acronym instead of ���.\n\nTo refer to a particular entry, Postgre��� uses a special data type: pg_lsn (log se- quence number, ���). It represents a ��-bit offset in bytes from the start of the ��� to an entry. An ��� is displayed as two ��-bit numbers in the hexadecimal notation separated by a slash.\n\nLet’s create a table:\n\n=> CREATE TABLE wal(id integer);\n\n=> INSERT INTO wal VALUES (1);\n\nStart a transaction and note the ��� of the ��� insert position:\n\n=> BEGIN;\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3DF708D8\n\n(1 row)\n\nNow run some arbitrary command, for example, update a row:\n\n=> UPDATE wal SET id = id + 1;\n\nThe page modification is performed in the buffer cache in ���. This change is logged in a ��� page, also in ���. As a result, the insert ��� is advanced:\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3DF70920\n\n(1 row)\n\nTo ensure that the modified data page is flushed to disk strictly after the corre- sponding ��� entry, the page header stores the ��� of the latest ��� entry related to this page. You can view this ��� using pageinspect:\n\n=> SELECT lsn FROM page_header(get_raw_page('wal',0));\n\nlsn −−−−−−−−−−−− 0/3DF70920\n\n(1 row)\n\n194",
      "content_length": 1356,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "10.2 WAL Structure\n\nThere is only one ��� for the whole database cluster, and new entries constantly get appended to it. For this reason, the ��� stored in the page may turn out to be smaller than the one returned by the pg_current_wal_insert_lsn function some time ago. But if nothing has happened in the system, these numbers will be the same.\n\nNow let’s commit the transaction:\n\n=> COMMIT;\n\nThe commit operation is also logged, and the insert ��� changes again:\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3DF70948\n\n(1 row)\n\nA commit updates transaction status in ���� pages , which are kept in their own cache.1 The ���� cache usually takes ��� pages in the shared memory.2 To make sure that a ���� page is not flushed to disk before the corresponding ��� entry, the ��� of the latest ��� entry has to be tracked for ���� pages too. But this in- formation is stored in ���, not in the page itself.\n\n��� entries will make it to disk; then it will be possible to evict ���� At some point and data pages from the cache. If they had to be evicted earlier,it would have been discovered, and ��� entries would have been forced to disk first.3\n\nIf you know two ��� positions, you can calculate the size of ��� entries between them (in bytes) by simply subtracting one position from the other. You just have to cast them to the pg_lsn type:\n\n=> SELECT '0/3DF70948'::pg_lsn - '0/3DF708D8'::pg_lsn;\n\n?column? −−−−−−−−−−\n\n112\n\n(1 row)\n\n1 backend/access/transam/slru.c 2 backend/access/transam/clog.c, CLOGShmemBuffers function 3 backend/storage/buffer/bufmgr.c, FlushBuffer function\n\n195\n\np. ��\n\np. ���",
      "content_length": 1649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "16MB\n\nv. ��\n\nv. ��\n\nChapter 10 Write-Ahead Log\n\nIn this particular case, ��� entries related to ������ and ������ operations took about a hundred of bytes.\n\nYou can use the same approach to estimate the volume of ��� entries generated by a particular workload per unit of time. This information will be required for the checkpoint setup.\n\nPhysical Structure\n\nOn disk, the ��� is stored in the ������/pg_wal directory as separate files, or seg- ments. Their size is shown by the read-only\n\nwal_segment_size parameter.\n\nFor high-load systems, it makes sense to increase the segment size since it may reduce the overhead, but this setting can be modified only during cluster initial- ization (initdb --wal-segsize).\n\nW�� entries get into the current file until it runs out of space; then Postgre��� starts a new file.\n\nWe can learn in which file a particular entry is located, and at what offset from the start of the file:\n\n=> SELECT file_name, upper(to_hex(file_offset)) file_offset FROM pg_walfile_name_offset('0/3DF708D8');\n\nfile_name\n\n| file_offset\n\n−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−\n\n00000001000000000000003D | F708D8\n\n(1 row) timeline\n\nlog sequence number\n\nThe name of the file consists of two parts. The highest eight hexadecimal digits define the timeline used for recovery from a backup, while the rest represent the highest ��� bits (the lowest ��� bits are shown in the file_offset field).\n\nTo view the current ��� files\n\n, you can call the following function:\n\n=> SELECT * FROM pg_ls_waldir() WHERE name = '00000001000000000000003D';\n\n196",
      "content_length": 1555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "10.3 Checkpoint\n\nname\n\n|\n\nsize\n\n|\n\nmodification\n\n−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−− 00000001000000000000003D | 16777216 | 2023−03−06 14:01:48+03\n\n(1 row)\n\nNow let’s take a look at the headers of the newly created ��� entries using the pg_waldump utility, which can filter ��� entries both by the ��� range (like in this example) and by a particular transaction ��.\n\nThe pg_waldump utility should be started on behalf of the postgres �� user, as it needs access to ��� files on disk.\n\npostgres$ /usr/local/pgsql/bin/pg_waldump \\ -p /usr/local/pgsql/data/pg_wal -s 0/3DF708D8 -e 0/3DF70948#\n\nrmgr: Heap 0/3DF708D8, prev 0/3DF708B0, desc: HOT_UPDATE off 1 xmax 886 flags 0x40 ; new off 2 xmax 0, blkref #0: rel 1663/16391/16562 blk 0 rmgr: Transaction len (rec/tot): 886, lsn: 0/3DF70920, prev 0/3DF708D8, desc: COMMIT 2023−03−06 14:01:48.875861 MSK\n\nlen (rec/tot):\n\n69/\n\n69, tx:\n\n886, lsn:\n\n34/\n\n34, tx:\n\nHere we can see the headers of two entries.\n\nThe first one is the ���_������ The blkref field shows the filename and the page �� of the updated heap page:\n\noperation handled by the Heap resource manager.\n\n=> SELECT pg_relation_filepath('wal');\n\npg_relation_filepath −−−−−−−−−−−−−−−−−−−−−−\n\nbase/16391/16562\n\n(1 row)\n\nThe second entry is the ������ operation supervised by the Transaction resource manager.\n\n10.3 Checkpoint\n\nTo restore data consistency after a failure (that is,to perform recovery),Postgre��� has to replay the ��� in the forward direction and apply the entries that represent lost changes to the corresponding pages. To find out what has been lost, the ���\n\n197\n\np. ���",
      "content_length": 1617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "v. �.�\n\nChapter 10 Write-Ahead Log\n\nof the page stored on disk is compared to the ��� of the ��� entry. But at which point should we start the recovery? If we start too late, the pages written to disk before this point will fail to receive all the changes, which will lead to irreversible data corruption. Starting from the very beginning is unrealistic: it is impossible to store such a potentially huge volume of data,and neither is it possible to accept such a long recovery time. We need a checkpoint that is gradually moving forward, thusmakingitsafetostarttherecoveryfromthispointandremovealltheprevious ��� entries.\n\nThe most straightforward way to create a checkpoint is to periodically suspend all system operations and force all dirty pages to disk. This approach is of course unacceptable, as the system will hang for an indefinite but quite significant time.\n\nFor this reason, the checkpoint is spread out over time, virtually constituting an interval. Checkpointexecutionisperformedbyaspecialbackgroundprocesscalled checkpointer.1\n\nCheckpoint start. The checkpointer process flushes to disk everything that can be written instantaneously: ���� transaction status, subtransactions’ metadata, and a few other structures.\n\nCheckpoint execution. Most of the checkpoint execution time is spent on flushing\n\ndirty pages to disk.2\n\nFirst, a special tag is set in the headers of all the buffers that were dirty at the checkpoint start. It happens very fast since no �/� operations are involved.\n\nThen checkpointer traverses all the buffers and writes the tagged ones to disk. Their pages are not evicted from the cache: they are simply written down, so usage and pin counts can be ignored.\n\nPages are processed in the order of their ��s to avoid random writing if pos- sible. For better load balancing, Postgre��� alternates between different ta- blespaces (as they may be located on different physical devices).\n\n1 backend/postmaster/checkpointer.c\n\nbackend/access/transam/xlog.c, CreateCheckPoint function\n\n2 backend/storage/buffer/bufmgr.c, BufferSync function\n\n198",
      "content_length": 2073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "10.3 Checkpoint\n\nBackends can also write tagged buffers to disk—if they get to them first. In any case,buffertagsareremovedatthisstage,soforthepurposeofthecheckpoint each buffer will be written only once.\n\nNaturally,pages can still be modified in the buffer cache while the checkpoint is in progress. But since new dirty buffers are not tagged, checkpointer will ignore them.\n\nCheckpoint completion. When all the buffers that were dirty at the start of the checkpoint are written to disk, the checkpoint is considered complete. From now on (but not earlier!), the start of the checkpoint will be used as a new starting point of recovery. All the ��� entries written before this point are not required anymore.\n\nstart of recovery\n\nrequired WALfiles\n\nfailure\n\ntime\n\ncheckpoint\n\nstart of recovery\n\nrequired WALfiles\n\nfailure\n\ntime\n\ncheckpoint\n\ncheckpoint\n\nFinally, checkpointer creates a ��� entry that corresponds to the checkpoint completion, specifying the checkpoint’s start ���. Since the checkpoint logs nothing when it starts, this ��� can belong to a ��� entry of any type.\n\nThe ������/global/pg_control file also gets updated to refer to the latest com- pleted checkpoint. (Until this process is over, pg_control keeps the previous checkpoint.)\n\n199",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Chapter 10 Write-Ahead Log\n\nPGDATA/global/pg_control\n\nLatest checkpoint location:\n\n0/3E7EF818\n\nLatest checkpoint's REDO location:\n\n0/3E7EF7E0\n\ncheckpoint start\n\ncheckpoint finish\n\nCHECKPOINT\n\nTo figure out once and for all what points where, let’s take a look at a simple ex- ample. We will make several cached pages dirty:\n\n=> UPDATE big SET s = 'FOO';\n\n=> SELECT count(*) FROM pg_buffercache WHERE isdirty;\n\ncount −−−−−−− 4119 (1 row)\n\nNote the current ��� position:\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3E7EF7E0\n\n(1 row)\n\nNow let’s complete the checkpoint manually. All the dirty pages will be flushed to disk; since nothing happens in the system, new dirty pages will not appear:\n\n=> CHECKPOINT;\n\n=> SELECT count(*) FROM pg_buffercache WHERE isdirty;\n\ncount −−−−−−−\n\n0\n\n(1 row)\n\nLet’s see how the checkpoint is reflected in the ���:\n\n200",
      "content_length": 902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "10.4 Recovery\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3E7EF890\n\n(1 row)\n\npostgres$ /usr/local/pgsql/bin/pg_waldump \\ -p /usr/local/pgsql/data/pg_wal -s 0/3E7EF7E0 -e 0/3E7EF890\n\nrmgr: Standby 0/3E7EF7E0, prev 0/3E7EF7B8, desc: RUNNING_XACTS nextXid 888 latestCompletedXid 887 oldestRunningXid 888 rmgr: XLOG 0/3E7EF818, prev 0/3E7EF7E0, desc: CHECKPOINT_ONLINE redo 0/3E7EF7E0; tli 1; prev tli 1; fpw true; xid 0:888; oid 24754; multi 1; offset 0; oldest xid 726 in DB 1; oldest multi 1 in DB 1; oldest/newest commit timestamp xid: 0/0; oldest running xid 888; online\n\nlen (rec/tot):\n\n50/\n\n50, tx:\n\n0, lsn:\n\nlen (rec/tot):\n\n114/\n\n114, tx:\n\n0, lsn:\n\nThe latest ��� entry is related to the checkpoint completion (����������_������). The start ��� of this checkpoint is specified after the word redo; this position cor- responds to the latest inserted ��� entry at the time of the checkpoint start.\n\nThe same information can also be found in the pg_control file:\n\npostgres$ /usr/local/pgsql/bin/pg_controldata \\ -D /usr/local/pgsql/data | egrep 'Latest.*location'\n\nLatest checkpoint location: Latest checkpoint's REDO location:\n\n0/3E7EF818 0/3E7EF7E0\n\n10.4 Recovery\n\nThe first process launched at the server start is postmaster. In its turn, postmaster spawns the startup process,1 which takes care of data recovery in case of a failure.\n\nTo determine whether recovery is needed, the startup process reads the pg_control file and checks the cluster status. The pg_controldata utility enables us to view the content of this file:\n\n1 backend/postmaster/startup.c\n\nbackend/access/transam/xlog.c, StartupXLOG function\n\n201",
      "content_length": 1673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "p. ��\n\nChapter 10 Write-Ahead Log\n\npostgres$ /usr/local/pgsql/bin/pg_controldata \\ -D /usr/local/pgsql/data | grep state\n\nDatabase cluster state:\n\nin production\n\nA properly stopped server has the “shut down” status; the “in production” status of a non-running server indicates a failure. In this case,the startup process will au- tomatically initiate recovery from the start ��� of the latest completed checkpoint found in the same pg_control file.\n\nIf the ������ directory contains a backup_label file related to a backup, the start ��� posi- tion is taken from that file.\n\nThe startup process reads ��� entries one by one, starting from the defined posi- tion, and applies them to data pages if the ��� of the page is smaller than the ��� of the ��� entry. If the page contains a bigger ���, ��� should not be applied; in fact, it must not be applied because its entries are designed to be replayed strictly sequentially.\n\nHowever, some ��� entries constitute a full page image, or ���. Entries of this type can be applied to any state of the page since all the page contents will be erased anyway. Such modifications are called idempotent. Another example of an idempo- tent operation is registering transaction status changes: each transaction status is defined in ���� by certain bits that are set regardless of their previous values, so there is no need to keep the ��� of the latest change in ���� pages.\n\nW�� entries are applied to pages in the buffer cache, just like regular page updates during normal operation.\n\nFiles get restored from ��� in a similar manner: for example, if a ��� entry shows that the file must exit, but it is missing for some reason, it will be created anew.\n\nOnce the recovery is over,all unlogged relations are overwritten by the correspond- ing initialization forks.\n\nFinally, the checkpoint is executed to secure the recovered state on disk.\n\nThe job of the startup process is now complete.\n\nIn its classic form, the recovery process consists of two phases. In the roll-forward phase, ��� entries are replayed, repeating the lost operations. In the roll-back phase, the server aborts the transactions that were not yet committed at the time of the failure.\n\n202",
      "content_length": 2198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "10.4 Recovery\n\nIn Postgre���, the second phase is not required. After the recovery, the ���� will contain neither commit nor abort bits for an unfinished transaction (which technically denotes an active transaction), but since it is known for sure that the transaction is not running anymore, it will be considered aborted.1\n\nWe can simulate a failure by forcing the server to stop in the immediate mode:\n\npostgres$ pg_ctl stop -m immediate\n\nHere is the new cluster state:\n\npostgres$ /usr/local/pgsql/bin/pg_controldata \\ -D /usr/local/pgsql/data | grep 'state'\n\nDatabase cluster state:\n\nin production\n\nWhen we launch the server,the startup process sees that a failure has occurred and enters the recovery mode:\n\npostgres$ pg_ctl start -l /home/postgres/logfile\n\npostgres$ tail -n 6 /home/postgres/logfile\n\nLOG: 14:01:49 MSK LOG: in progress LOG: LOG: LOG: system: 0.00 s, elapsed: 0.00 s LOG:\n\ndatabase system was interrupted; last known up at 2023−03−06\n\ndatabase system was not properly shut down; automatic recovery\n\nredo starts at 0/3E7EF7E0 invalid record length at 0/3E7EF890: wanted 24, got 0 redo done at 0/3E7EF818 system usage: CPU: user: 0.00 s,\n\ndatabase system is ready to accept connections\n\nIf the server is being stopped normally,postmaster disconnects all clients and then executes the final checkpoint to flush all dirty pages to disk.\n\nNote the current ��� position:\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/3E7EF908\n\n(1 row)\n\n1 backend/access/heap/heapam_visibility.c, HeapTupleSatisfiesMVCC function\n\n203",
      "content_length": 1583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "Chapter 10 Write-Ahead Log\n\nNow let’s stop the server properly:\n\npostgres$ pg_ctl stop\n\nHere is the new cluster state:\n\npostgres$ /usr/local/pgsql/bin/pg_controldata \\ -D /usr/local/pgsql/data | grep state\n\nDatabase cluster state:\n\nshut down\n\nAt the end of the ���, we can see the ����������_�������� entry, which denotes the final checkpoint:\n\npostgres$ /usr/local/pgsql/bin/pg_waldump \\ -p /usr/local/pgsql/data/pg_wal -s 0/3E7EF908\n\nrmgr: XLOG 0/3E7EF908, prev 0/3E7EF890, desc: CHECKPOINT_SHUTDOWN redo 0/3E7EF908; tli 1; prev tli 1; fpw true; xid 0:888; oid 24754; multi 1; offset 0; oldest xid 726 in DB 1; oldest multi 1 in DB 1; oldest/newest commit timestamp xid: 0/0; oldest running xid 0; shutdown pg_waldump: fatal: error in WAL record at 0/3E7EF908: invalid record length at 0/3E7EF980: wanted 24, got 0\n\nlen (rec/tot):\n\n114/\n\n114, tx:\n\n0, lsn:\n\nThe latest pg_waldump message shows that the utility has read the ��� to the end.\n\nLet’s start the instance again:\n\npostgres$ pg_ctl start -l /home/postgres/logfile\n\n10.5 Background Writing\n\nIf the backend needs to evict a dirty page from a buffer, it has to write this page to disk. Such a situation is undesired because it leads to waits—it is much better to perform writing asynchronously in the background.\n\nThis job is partially handled by checkpointer, but it is still not enough.\n\n204",
      "content_length": 1350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "10.6 WAL Setup\n\nTherefore, Postgre��� provides another process called bgwriter,1 specifically for backgroundwriting. Itreliesonthesamebuffersearchalgorithmaseviction,except for the two main differences:\n\nThe bgwriter process uses its own clock hand that never lags behind that of\n\neviction and typically overtakes it.\n\nAs the buffers are being traversed, the usage count is not reduced.\n\nA dirty page is flushed to disk if the buffer is not pinned and has zero usage count. Thus, bgwriter runs before eviction and proactively writes to disk those pages that are highly likely to be evicted soon.\n\nIt raises the odds of the buffers selected for eviction being clean.\n\n10.6 WAL Setup\n\nConfiguring Checkpoints\n\nThe checkpoint duration (to be more exact, the duration of writing dirty buffers to checkpoint_completion_target parameter. Its value specifies disk) is defined by the the fraction of time between the starts of two neighboring checkpoints that is allot- ted to writing. Avoid setting this parameter to one: as a result,the next checkpoint may be due before the previous one is complete. No disaster will happen, as it is impossible to execute more than one checkpoint at a time, but normal operation may still be disrupted.\n\nWhen configuring other parameters, we can use the following approach. First, we define an appropriate volume of ��� files to be stored between two neighboring checkpoints. The bigger the volume, the smaller the overhead, but this value will anyway be limited by the available free space and the acceptable recovery time.\n\nTo estimate the time required to generate this volume by normal load, you need to note the initial insert ��� and check the difference between this and the current insert positions from time to time.\n\n1 backend/postmaster/bgwriter.c\n\n205\n\n0.9 v. ��",
      "content_length": 1803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "5min p. ���\n\n1GB\n\nv. ��\n\nChapter 10 Write-Ahead Log\n\nThe received figure is assumed to be a typical interval between checkpoints, so we checkpoint_timeout parameter value. The default setting is likely will use it as the it is usually increased, for example, to �� minutes. to be too small;\n\nHowever, it is quite possible (and even probable) that the load will sometimes be higher, so the size of ��� files generated during this interval will be too big. In this case,the checkpoint must be executed more often. To set up such a trigger,we max_wal_size parameter. will limit the size of ��� files required for recovery by the When this threshold is exceeded, the server invokes an extra checkpoint.1\n\nrequired for recovery contain all the entries both for the latest completed W�� files checkpointandforthecurrentone,whichisnotcompletedyet. Sotoestimatetheir total volume you should multiply the calculated ��� size between checkpoints by 1 + checkpoint_completion_target.\n\nPrior to version ��, Postgre��� kept ��� files for two completed checkpoints, so the mul- tiplier was 2 + checkpoint_completion_target.\n\nFollowing this approach,most checkpoints are executed on schedule,once per the checkpoint_timeout interval; but should the load increase, the checkpoint is trig- gered when ��� size exceeds the max_wal_size value.\n\nThe actual progress is periodically checked against the expected figures:2\n\nThe actual progress is defined by the fraction of cached pages that have already\n\nbeen processed.\n\nThe expected progress (by time)\n\nis defined by the fraction of time that has al- ready elapsed, assuming that the checkpoint must be completed within the checkpoint_timeout × checkpoint_completion_target interval.\n\nThe expected progress (by size) is defined by the fraction of the already filled ��� files, their expected number being estimated based on the max_wal_size × checkpoint_completion_target value.\n\nIf dirty pages get written to disk ahead of schedule, checkpointer is paused for a while; if there is any delay by either of the parameters, it catches up as soon as\n\n1 backend/access/transam/xlog.c, LogCheckpointNeeded & CalculateCheckpointSegments functions 2 backend/postmaster/checkpointer.c, IsCheckpointOnSchedule function\n\n206",
      "content_length": 2244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "10.6 WAL Setup\n\npossible.1 Since both time and data size are taken into account, Postgre��� can manage scheduled and on-demand checkpoints using the same approach.\n\nOnce the checkpoint has been completed, ��� files that are not required for recov- ery anymore are deleted;2 however, several files (up to min_wal_size in total) are kept for reuse and are simply renamed.\n\nSuch renaming tion,but you can turn off this feature using the need it.\n\nreduces the overhead incurred by constant file creation and dele- wal_recycle parameter if you do not\n\nThe following figure shows how the size of ��� files stored on disk changes under normal conditions.\n\nWALsize\n\ne z i s _ l a w _ x a m\n\ntime\n\ncheckpoint_timeout\n\nthe size of WALgenerated between the starts of two checkpoints\n\nIt is important to keep in mind that the actual size of ��� files on disk may exceed the max_wal_size value:\n\nThe max_wal_size parameter specifies the desired target value rather than a\n\nhard limit. If the load spikes, writing may lag behind the schedule.\n\nThe server has no right to delete ��� files that are yet to be replicated or han- dledbycontinuousarchiving. Ifenabled,thisfunctionalitymustbeconstantly monitored, as it can easily cause a disk overflow.\n\n1 backend/postmaster/checkpointer.c, CheckpointWriteDelay function 2 backend/access/transam/xlog.c, RemoveOldXlogFiles function\n\n207\n\n80MB\n\nv. �� on",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "v. �� 0MB\n\n200ms\n\n2\n\n100\n\n30s\n\noff\n\nChapter 10 Write-Ahead Log\n\nYou can reserve a certain amount of space\n\nfor ��� files by configuring the\n\nwal_keep_size parameter.\n\nConfiguring Background Writing\n\nOnce checkpointer is configured, you should also set up bgwriter. Together, these processes must be able to cope with writing dirty buffers to disk before backends need to reuse them.\n\nDuring its operation, bgwriter makes periodic pauses, sleeping for units of time.\n\nbgwriter_delay\n\nThe number of pages written between two pauses depends on the average number of buffers accessed by backends since the previous run (Postgre��� uses a moving average to level out possible spikes and avoid depending on very old data at the bgwriter_lru_multiplier. same time). The calculated number is then multiplied by But in any case, the number of pages written in a single run cannot exceed the bgwriter_lru_maxpages value.\n\nIf no dirty buffers are detected (that is, nothing happens in the system), bgwriter sleeps until one of the backends accesses a buffer. Then it wakes up and continues its regular operation.\n\nMonitoring\n\nCheckpoint settings can and should be tuned based on monitoring data.\n\nIf size-triggered checkpoints have to be performed more often than defined by the checkpoint_warning parameter, Postgre��� issues a warning. This setting should be brought in line with the expected peak load.\n\nThe into the server log. Let’s turn it on:\n\nlog_checkpoints parameter enables printing checkpoint-related information\n\n=> ALTER SYSTEM SET log_checkpoints = on;\n\n=> SELECT pg_reload_conf();\n\nNow we will modify some data and execute a checkpoint:\n\n208",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "10.6 WAL Setup\n\n=> UPDATE big SET s = 'BAR';\n\n=> CHECKPOINT;\n\nThe server log shows the number of written buffers, some statistics on ��� file changes after the checkpoint, the duration of the checkpoint, and the distance (in bytes) between the starts of two neighboring checkpoints:\n\npostgres$ tail -n 2 /home/postgres/logfile\n\nLOG: LOG: added, 1 removed, 0 recycled; write=0.076 s, sync=0.009 s, total=0.099 s; sync files=3, longest=0.007 s, average=0.003 s; distance=9213 kB, estimate=9213 kB\n\ncheckpoint starting: immediate force wait checkpoint complete: wrote 4100 buffers (25.0%); 0 WAL file(s)\n\nThe most useful data that can affect your configuration decisions is statistics on background writing and checkpoint execution provided in the pg_stat_bgwriter view.\n\nPrior to version 9.2, both tasks were performed by bgwriter; then a separate checkpointer process was introduced, but the common view remained unchanged.\n\n=> SELECT * FROM pg_stat_bgwriter \\gx\n\n−[ RECORD 1 ]−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− checkpoints_timed checkpoints_req checkpoint_write_time | 33111 checkpoint_sync_time buffers_checkpoint buffers_clean maxwritten_clean buffers_backend buffers_backend_fsync | 0 buffers_alloc stats_reset\n\n| 0 | 14\n\n| 221 | 14253 | 13066 | 122 | 84226\n\n| 86700 | 2023−03−06 14:00:07.369124+03\n\nAmong other things, this view displays the number of completed checkpoints:\n\nThe checkpoints_timed field shows scheduled checkpoints (which are triggered\n\nwhen the checkpoint_timeout interval is reached).\n\nThe checkpoints_req field shows on-demand checkpoints (including those trig-\n\ngered when the max_wal_size size is reached).\n\n209",
      "content_length": 1643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "Chapter 10 Write-Ahead Log\n\nA large checkpoint_req value (as compared to checkpoints_timed) indicates that checkpoints are performed more often than expected.\n\nThe following statistics on the number of written pages are also very important:\n\nbuffers_checkpoint pages written by checkpointer\n\nbuffers_backend pages written by backends\n\nbuffers_clean pages written by bgwriter\n\nIn a well-configured system, the buffers_backend value must be considerably lower than the sum of buffers_checkpoint and buffers_clean.\n\nWhen setting up background writing, pay attention to the maxwritten_clean value: it shows how many times bgwriter had to stop because of exceeding the threshold defined by bgwriter_lru_maxpages.\n\nThe following call will drop the collected statistics:\n\n=> SELECT pg_stat_reset_shared('bgwriter');\n\n210",
      "content_length": 813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "11\n\nWAL Modes\n\n11.1 Performance\n\nWhile the server is running normally, ��� files are being constantly written to disk. However, these writes are sequential: there is almost no random access, so even ���s can cope with this task. Since this type of load is very different from a typical data file access, it may be worth setting up a separate physical storage for ��� files and replacing the ������/pg_wal catalog by a symbolic link to a directory in a mounted file system.\n\nThere are a couple of situations when ��� files have to be both written and read. The first one is the obvious case of crash recovery; the second one is stream replication. The walsender1 process reads ��� entries directly from files.2 So if a replica does not receive ��� entries while the required pages are still in the �� buffers of the primary server, the data has to be read from disk. But the access will still be sequential rather than random.\n\n��� entries can be written in one of the following modes:\n\nThe synchronous mode forbids any further operations until a transaction com-\n\nmit saves all the related ��� entries to disk.\n\nThe asynchronous mode implies instant transaction commits, with ��� en-\n\ntries being written to disk later in the background.\n\nThe current mode is defined by the\n\nsynchronous_commit parameter.\n\n1 backend/replication/walsender.c 2 backend/access/transam/xlogreader.c\n\n211\n\non",
      "content_length": 1386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "0s 5\n\n200ms\n\nChapter 11 WAL Modes\n\nSynchronous mode. To reliably register the fact of a commit, it is not enough to simply pass ��� entries to the operating system; you have to make sure that disk synchronization has completed successfully. Since synchronization im- plies actual �/� operations (which are quite slow), it is beneficial to perform it as seldom as possible.\n\nFor this purpose, the backend that completes the transaction and writes ��� commit_delay param- entries to disk can make a small pause as defined by the commit_siblings active eter. However, it will only happen if there are at least transactions in the system:1 during this pause, some of them may finish, and the server will manage to synchronize all the ��� entries in one go. It is a lot like holding doors of an elevator for someone to rush in.\n\nBy default, there is no pause. It makes sense to modify the commit_delay pa- rameter only for systems that perform a lot of short ���� transactions.\n\nAfter a potential pause, the process that completes the transaction flushes all the accumulated ��� entries to disk and performs synchronization (it is important to save the commit entry and all the previous entries related to this transaction; the rest is written just because it does not increase the cost).\n\nFrom this time on,the����’s durability requirement is guaranteed—the trans- action is considered to be reliably committed.2 That’s why the synchronous mode is the default one.\n\nThe downside of the synchronous commit is longer latencies (the ������ com- mand does not return control until the end of synchronization) and lower sys- tem throughput, especially for ���� loads.\n\nAsynchronous mode. To enable asynchronous commits,3 you have to turn off the\n\nsynchronous_commit parameter.\n\nIn the asynchronous mode, ��� entries are written to disk by the walwriter4 process, which alternates between work and sleep. The duration of pauses is defined by the\n\nwal_writer_delay value.\n\n1 backend/access/transam/xlog.c, XLogFlush function 2 backend/access/transam/xlog.c, RecordTransactionCommit function 3 postgresql.org/docs/14/wal-async-commit.html 4 backend/postmaster/walwriter.c\n\n212",
      "content_length": 2164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "11.1 Performance\n\nWaking up from a pause,the process checks the cache for new completely filled ��� pages. If any such pages have appeared, the process writes them to disk, skipping the current page. Otherwise, it writes the current half-empty page since it has woken up anyway.1\n\nThe purpose of this algorithm is to avoid flushing one and the same page several times, which brings noticeable performance gains for workloads with intensive data changes.\n\nAlthough ��� cache is used as a ring buffer, walwriter stops when it reaches the last page of the cache; after a pause, the next writing cycle starts from the first page. So in the worst case walwriter needs three runs to get to a particular ��� entry: first,it will write all full pages located at the end of the cache,then it will get back to the beginning,and finally,it will handle the underfilled page containing the entry. But in most cases it takes one or two cycles.\n\nSynchronization is performed each time the data is written, and once again at the end of the writing cycle.\n\nwal_writer_flush_after amount of\n\nAsynchronous commits are faster than synchronous ones since they do not have to wait for physical writes to disk. But reliability suffers: you can lose the data committed within the 3 × wal_writer_delay timeframe before a failure (which is 0.6 seconds by default).\n\nIn the real world, these two modes complement each other. In the synchronous mode, ��� entries related to a long transaction can still be written asynchronously to free ��� buffers. And vice versa, a ��� entry related to a page that is about to be evicted from the buffer cache will be immediately flushed to disk even in the asynchronous mode—otherwise, it is impossible to continue operation.\n\nIn most cases, a hard choice between performance and durability has to be made by the system designer.\n\nThe synchronous_commit parameter can also be set for particular transactions. If it is possible to classify all transactions at the application level as either absolutely critical (such as handling financial data) or less important, you can boost perfor- mance while risking to lose only non-critical transactions.\n\n1 backend/access/transam/xlog.c, XLogBackgroundFlush function\n\n213\n\n1MB",
      "content_length": 2227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "Chapter 11 WAL Modes\n\nTo get some idea of potential performance gains of the asynchronous commit,let’s compare latency and throughput in the two modes using a pgbench test.1\n\nFirst, initialize the required tables:\n\npostgres$ /usr/local/pgsql/bin/pgbench -i internals\n\nStart a ��-second test in the synchronous mode:\n\npostgres$ /usr/local/pgsql/bin/pgbench -T 30 internals\n\npgbench (14.7) starting vacuum...end. transaction type: <builtin: TPC−B (sort of)> scaling factor: 1 query mode: simple number of clients: 1 number of threads: 1 duration: 30 s number of transactions actually processed: 20123 latency average = 1.491 ms initial connection time = 2.507 ms tps = 670.809688 (without initial connection time)\n\nAnd now run the same test in the asynchronous mode:\n\n=> ALTER SYSTEM SET synchronous_commit = off;\n\n=> SELECT pg_reload_conf();\n\npostgres$ /usr/local/pgsql/bin/pgbench -T 30 internals\n\npgbench (14.7) starting vacuum...end. transaction type: <builtin: TPC−B (sort of)> scaling factor: 1 query mode: simple number of clients: 1 number of threads: 1 duration: 30 s number of transactions actually processed: 61809 latency average = 0.485 ms initial connection time = 1.915 ms tps = 2060.399861 (without initial connection time)\n\n1 postgresql.org/docs/14/pgbench.html\n\n214",
      "content_length": 1281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "11.2 Fault Tolerance\n\nIn the asynchronous mode, this simple benchmark shows a significantly lower la- tency and higher throughput (���). Naturally, each particular system will have its own figures depending on the current load, but it is clear that the impact on short ���� transactions can be quite tangible.\n\nLet’s restore the default settings:\n\n=> ALTER SYSTEM RESET synchronous_commit;\n\n=> SELECT pg_reload_conf();\n\n11.2 Fault Tolerance\n\nIt is self-evident that write-ahead logging must guarantee crash recovery under any circumstances (unless the persistent storage itself is broken). There are many factors that can affect data consistency, but I will cover only the most important ones: caching, data corruption, and non-atomic writes.1\n\nCaching\n\nBefore reaching a non-volatile storage (such as a hard disk),data can pass through various caches.\n\nA disk write simply instructs the operating system to place the data into its cache (which is also prone to crashes, just like any other part of ���). The actual writing is performed asynchronously, as defined by the settings of the �/� scheduler of the operating system.\n\nOnce the scheduler decides to flush the accumulated data, this data is moved to the cache of a storage device (like an ���). Storage devices can also defer writing, for example, to group of adjacent pages together. A ���� controller adds one more caching level between the disk and the operating system.\n\nUnless special measures are taken, the moment when the data is reliably stored on disk remains unknown. It is usually not so important because we have the ���,\n\n1 postgresql.org/docs/14/wal-reliability.html\n\n215",
      "content_length": 1643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "on\n\nChapter 11 WAL Modes\n\nbut ��� entries themselves must be reliably saved on disk right away.1 It is equally true for the asynchronous mode—otherwise,it is impossible to guarantee that ��� entries get do disk ahead of the modified data.\n\nThe checkpointer process must also save the data in a reliable way, ensuring that dirty pages make it to disk from the �� cache. Besides,it has to synchronize all the file operations that have been performed by other processes (such as page writes or file deletions): when the checkpoint completes, the results of all these actions must be already saved on disk.2\n\nThere are also some other situations that demand fail-safe writing,such as execut- ing unlogged operations at the minimal ��� level.\n\nOperating systems provide various means to guarantee immediate writing of data into a non-volatile storage. All of them boil down to the following two main ap- proaches: either a separate synchronization command is called after writing (such as fsync or fdatasync), or the requirement to perform synchronization (or even di- rect writing that bypasses �� cache) is specified when the file is being opened or written into.\n\nThe pg_test_fsync utility can help you determine the best way to synchronize the ��� depending on your �� and file system; the preferred method can be specified in the wal_sync_method parameter. For other operations, an appropriate synchro- nization method is selected automatically and cannot be configured.3\n\nA subtle aspect here is that in each particular case the most suitable method de- pends on the hardware. For example, if you use a controller with a backup battery, you can take advantage of its cache, as the battery will protect the data in case of a power outage.\n\nYou should keep in mind that the asynchronous commit and lack of synchroniza- fsync tion are two totally different stories. Turning off synchronization (by the parameter) boosts system performance, yet any failure will lead to fatal data loss. The asynchronous mode guarantees crash recovery up to a consistent state, but some of the latest data updates may be missing.\n\n1 backend/access/transam/xlog.c, issue_xlog_fsync function 2 backend/storage/sync/sync.c 3 backend/storage/file/fd.c, pg_fsync function\n\n216",
      "content_length": 2251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "11.2 Fault Tolerance\n\nData Corruption\n\nTechnical equipment is imperfect, and data can get damaged both in memory and on disk,or while it is being transferred via interface cables. Such errors are usually handled at the hardware level, yet some can escape.\n\nTocatchissuesingoodtime,Postgre���alwaysprotects���entriesbychecksums.\n\nChecksumscanbecalculatedfordatapagesaswell.1 Itisdoneeitherduringcluster pg_checksums2 utility when the server is stopped.3 initialization or by running the\n\nIn production systems, checksums must always be enabled, despite some (minor) calculation and verification overhead. It raises the chance of timely corruption discovery, even though some corner cases still remain:\n\nChecksum verification is performed only when the page is accessed, so data corruption can go unnoticed for a long time, up to the point when it gets into all backups and leaves no source of correct data.\n\nA zeroed page is considered correct, so if the file system zeroes out a page by\n\nmistake, this issue will not be discovered.\n\nChecksums are calculated only for the main fork of relations; other forks and\n\nfiles (such as transaction status in ����) remain unprotected.\n\nLet’s take a look at the read-only data_checksums parameter to make sure that checksums are enabled:\n\n=> SHOW data_checksums;\n\ndata_checksums −−−−−−−−−−−−−−−−\n\non\n\n(1 row)\n\nNow stop the server and zero out several bytes in the zero page of the main fork of the table:\n\n1 backend/storage/page/README 2 postgresql.org/docs/14/app-pgchecksums.html 3 commitfest.postgresql.org/27/2260\n\n217\n\nv. ��",
      "content_length": 1568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "off\n\nChapter 11 WAL Modes\n\n=> SELECT pg_relation_filepath('wal');\n\npg_relation_filepath −−−−−−−−−−−−−−−−−−−−−−\n\nbase/16391/16562\n\n(1 row)\n\npostgres$ pg_ctl stop\n\npostgres$ dd if=/dev/zero of=/usr/local/pgsql/data/base/16391/16562 \\ oflag=dsync conv=notrunc bs=1 count=8\n\n8+0 records in 8+0 records out 8 bytes copied, 0,00776573 s, 1,0 kB/s\n\nStart the server again:\n\npostgres$ pg_ctl start -l /home/postgres/logfile\n\nIn fact, we could have left the server running—it is enough to write the page to disk and evict it from cache (otherwise, the server will continue using its cached version). But such a workflow is harder to reproduce.\n\nNow let’s attempt to read the table:\n\n=> SELECT * FROM wal LIMIT 1;\n\nWARNING: expected 28733 ERROR:\n\npage verification failed, calculated checksum 20397 but\n\ninvalid page in block 0 of relation base/16391/16562\n\nIf the data cannot be restored from a backup, it makes sense to at least try to read the damaged page (risking to get garbled output). For this purpose, you have to enable the\n\nignore_checksum_failure parameter:\n\n=> SET ignore_checksum_failure = on;\n\n=> SELECT * FROM wal LIMIT 1;\n\nWARNING: expected 28733\n\npage verification failed, calculated checksum 20397 but\n\nid −−−− 2\n\n(1 row)\n\nEverything went fine in this case because we have damaged a non-critical part of the page header (the ��� of the latest ��� entry), not the data itself.\n\n218",
      "content_length": 1389,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "11.2 Fault Tolerance\n\nNon-Atomic Writes\n\nA database page usually takes � k�, but at the low level writing is performed by blocks, which are often smaller (typically ��� bytes or � k�). Thus, if a failure oc- curs, a page may be written only partially. It makes no sense to apply regular ��� entries to such a page during recovery.\n\nTo avoid partial writes, Postgre��� saves a full page image (���) in the ��� when this page is modified for the first time after the checkpoint start. This behavior is full_page_writes parameter,but turning it off can lead to fatal data controlled by the corruption.\n\nIf the recovery process comes across an ��� in the ���,it will unconditionally write it to disk (without checking its ���); just like any ��� entry, ���s are protected by checksums,so their damage cannot go unnoticed. Regular ��� entries will then be applied to this state, which is guaranteed to be correct.\n\nThere is no separate ��� entry type for setting hint bits : this operation is consid- ered non-critical because any query that accesses a page will set the required bits anew. However, any hint bit change will affect the page’s checksum. So if check- wal_log_hints parameter is on), hint bit modifications sums are enabled (or if the are logged as ���s.1\n\nEven though the logging mechanism excludes empty space from an ���,2 the size of the generated ��� files still significantly increases. The situation can be greatly improved if you enable ��� compression via the\n\nwal_compression parameter.\n\nLet’s run a simple experiment using the pgbench utility. We will perform a check- point and immediately start a benchmark test with a hard-set number of transac- tions:\n\n=> CHECKPOINT;\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/42CE5DA8\n\n(1 row)\n\n1 backend/storage/buffer/bufmgr.c, MarkBufferDirtyHint function 2 backend/access/transam/xloginsert.c, XLogRecordAssemble function\n\n219\n\np. ���\n\non\n\np. ��\n\noff\n\noff",
      "content_length": 1973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "Chapter 11 WAL Modes\n\npostgres$ /usr/local/pgsql/bin/pgbench -t 20000 internals\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/449113E0\n\n(1 row)\n\nHere is the size of the generated ��� entries:\n\n=> SELECT pg_size_pretty('0/449755C0'::pg_lsn - '0/42CE5DA8'::pg_lsn);\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n29 MB (1 row)\n\nIn this example, ���s take more than half of the total ��� size. You can see it for yourselfinthecollectedstatisticsthatshowthenumberof ��� entries(N),thesize of regular entries (Record size), and the ��� size for each resource type (Type):\n\npostgres$ /usr/local/pgsql/bin/pg_waldump --stats \\ -p /usr/local/pgsql/data/pg_wal -s 0/42CE5DA8 -e 0/449755C0\n\n(%) Type −−− −−−− XLOG 3,31) Transaction 20004 ( 15,41) 0,00) Storage 0,00) CLOG Standby 0,00) 24774 ( 19,09) Heap2 80234 ( 61,81) Heap 0,38) Btree\n\nN − 4294 (\n\n1 ( 1 ( 6 (\n\nRecord size −−−−−−−−−−−\n\n(%) −−− 2,50) 8,10) 0,00) 0,00) 0,00) 1536253 ( 18,27) 5946242 ( 70,73) 0,39)\n\n210406 ( 680536 ( 42 ( 30 ( 416 (\n\n(%) FPI size −−−−−−−− −−− 19820068 ( 93,78) 0,00) 0 ( 0,00) 0 ( 0,00) 0 ( 0,00) 0 ( 0,12) 24576 ( 1,40) 295664 ( 4,70) 993860 (\n\n494 (\n\n32747 (\n\nTotal\n\n−−−−−− 129808\n\n−−−−−−−−\n\n8406672 [28,46%]\n\n−−−−−−−− 21134168 [71,54%]\n\nThis ratio will be smaller if data pages get modified between checkpoints several times. It is yet another reason to perform checkpoints less often.\n\nWe will repeat the same experiment to see if compression can help.\n\n=> ALTER SYSTEM SET wal_compression = on;\n\n=> SELECT pg_reload_conf();\n\n=> CHECKPOINT;\n\n220",
      "content_length": 1565,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "11.3 WAL Levels\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/44D4C228\n\n(1 row)\n\npostgres$ /usr/local/pgsql/bin/pgbench -t 20000 internals\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/457653B0\n\n(1 row)\n\nHere is the ��� size with compression enabled:\n\n=> SELECT pg_size_pretty('0/457653B0'::pg_lsn - '0/44D4C228'::pg_lsn);\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n10 MB (1 row)\n\npostgres$ /usr/local/pgsql/bin/pg_waldump --stats \\ -p /usr/local/pgsql/data/pg_wal -s 0/44D4C228 -e 0/457653B0\n\n(%) Type −−− −−−− XLOG 0,29) Transaction 20001 ( 16,73) 0,00) Storage Standby 0,00) 18946 ( 15,84) Heap2 80141 ( 67,02) Heap 0,12) Btree\n\nN − 344 (\n\n1 ( 5 (\n\nRecord size −−−−−−−−−−−\n\n(%) −−− 0,22) 8,68) 0,00) 0,00) 1207425 ( 15,42) 5918020 ( 75,56) 0,11)\n\n17530 ( 680114 ( 42 ( 330 (\n\nFPI size −−−−−−−−\n\n(%) −−− 435492 ( 17,75) 0,00) 0,00) 0,00) 4,14) 1627008 ( 66,31) 289654 ( 11,80)\n\n0 ( 0 ( 0 ( 101601 (\n\n143 (\n\n8443 (\n\nTotal\n\n−−−−−− 119581\n\n−−−−−−−−\n\n7831904 [76,14%]\n\n−−−−−−−−\n\n2453755 [23,86%]\n\nTo sum it up,when there is a large number of ���s caused by enabled checksums or full_page_writes (that is,almost always),it makes sense to use compression despite some additional ��� overhead.\n\n11.3 WAL Levels\n\nThe main objective of write-ahead logging is to enable crash recovery. But if you extend the scope of logged information, a ��� can be used for other purposes too.\n\n221",
      "content_length": 1470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "replica\n\nv. �� 2MB\n\nv. ��\n\n10\n\nChapter 11 WAL Modes\n\nPostgre���providesminimal,replica,andlogicallogginglevels. Eachlevelincludes everything that is logged on the previous one and adds some more information.\n\nThe level in use is defined by the server restart.\n\nwal_level parameter; its modification requires a\n\nMinimal\n\nThe minimal level guarantees only crash recovery. To save space, the operations on relations that have been created or truncated within the current transaction are not logged if they incur insertion of large volumes of data (like in the case of ������ ����� �� ������ and ������ ����� commands).1 Instead of being logged, all the required data is immediately flushed to disk, and system catalog changes become visible right after the transaction commit.\n\nIf such an operation is interrupted by a failure, the data that has already made it to disk remains invisible and does not affect consistency. If a failure occurs when the operation is complete, all the data required for applying the subsequent ��� entries is already saved to disk.\n\nThe volume of data optimization to take effect is defined by the\n\nthat has to be written into a newly created relation for this\n\nwal_skip_threshold parameter.\n\nLet’s see what gets logged at the minimal level.\n\na higher replica level is used, which supports data replication. If you By default, choose the minimal level, you also have to set the allowed number of walsender processes to zero in the\n\nmax_wal_senders parameter:\n\n=> ALTER SYSTEM SET wal_level = minimal;\n\n=> ALTER SYSTEM SET max_wal_senders = 0;\n\nThe server has to be restarted for these changes to take effect:\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\nNote the current ��� position:\n\n=> SELECT pg_current_wal_insert_lsn();\n\n1 include/utils/rel.h, RelationNeedsWAL macro\n\n222",
      "content_length": 1810,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "11.3 WAL Levels\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/45767698\n\n(1 row)\n\nTruncate the table and keep inserting new rows within the same transaction until the wal_skip_threshold is exceeded:\n\n=> BEGIN;\n\n=> TRUNCATE TABLE wal;\n\n=> INSERT INTO wal\n\nSELECT id FROM generate_series(1,100000) id;\n\n=> COMMIT;\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/45767840\n\n(1 row)\n\nInstead of creating a new table, I run the �������� command as it generates fewer ��� entries.\n\nLet’s examine the generated ��� using the already familiar pg_waldump utility.\n\npostgres$ /usr/local/pgsql/bin/pg_waldump \\ -p /usr/local/pgsql/data/pg_wal -s 0/45767698 -e 0/45767840#\n\nrmgr: Storage 0/45767698, prev 0/45767660, desc: CREATE base/16391/24784 rmgr: Heap 0/457676C8, prev 0/45767698, desc: UPDATE off 45 xmax 122844 flags 0x60 ; new off 48 xmax 0, blkref #0: rel 1663/16391/1259 blk 0 rmgr: Btree 0/45767748, prev 0/457676C8, desc: INSERT_LEAF off 176, blkref #0: rel 1663/16391/2662 blk 2 rmgr: Btree 0/45767788, prev 0/45767748, desc: INSERT_LEAF off 147, blkref #0: rel 1663/16391/2663 blk 2 rmgr: Btree 0/457677C8, prev 0/45767788, desc: INSERT_LEAF off 254, blkref #0: rel 1663/16391/3455 blk 4 rmgr: Transaction len (rec/tot): 122844, lsn: 0/45767808, prev 0/457677C8, desc: COMMIT 2023−03−06 14:03:58.395214 MSK; rels: base/16391/24783\n\n42, tx:\n\n42/\n\n0, lsn:\n\nlen (rec/tot):\n\n122844, lsn:\n\n123, tx:\n\nlen (rec/tot):\n\n123/\n\n64/\n\nlen (rec/tot):\n\n64, tx:\n\n122844, lsn:\n\n64/\n\n64, tx:\n\n122844, lsn:\n\nlen (rec/tot):\n\n64/\n\n122844, lsn:\n\nlen (rec/tot):\n\n64, tx:\n\n54/\n\n54, tx:\n\n223",
      "content_length": 1629,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "p. ���\n\np. ���\n\np. ��\n\nChapter 11 WAL Modes\n\nThe first entry logs creation of a new file for the relation (since �������� rewrites the table).\n\nvirtually\n\nThe next four entries are associated with system catalog operations. They reflect the changes in the pg_class table and its three indexes.\n\nFinally, there is a commit-related entry. Data insertion is not logged.\n\nReplica\n\nDuring crash recovery, ��� entries are replayed to restore the data on disk up to a consistent state. Backup recovery works in a similar way, but it can also restore the database state up to the specified recovery target point using a ��� archive. The number of archived ��� entries can be quite high (for example, they can span several days), so the recovery period will include multiple checkpoints. Therefore, the minimal ��� level is not enough: it is impossible to repeat an operation if it is unlogged. For backup recovery, ��� files must include all the operations.\n\nThe same is true for replication: unlogged commands will not be sent to a replica and will not be replayed on it.\n\nThings get even more complicated if a replica is used for executing queries. First of all, it needs to have the information on exclusive locks acquired on the primary server since they may conflict with queries on the replica. Second, it must be able tocapturesnapshots ,whichrequirestheinformationonactivetransactions. When we deal with a replica, both local transactions and those running on the primary server have to be taken into account.\n\nThe only way to send this data to a replica is to periodically write it into ��� files.1 It is done by the bgwriter2 process, once in �� seconds (the interval is hard-coded).\n\nThe ability to perform data recovery from a backup and use physical replication is guaranteed at the replica level.\n\n1 backend/storage/ipc/standby, LogStandbySnapshot function 2 backend/postmaster/bgwriter.c\n\n224",
      "content_length": 1899,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "11.3 WAL Levels\n\nThe replica level ured above and restart the server:\n\nis used by default, so we can simply reset the parameters config-\n\n=> ALTER SYSTEM RESET wal_level;\n\n=> ALTER SYSTEM RESET max_wal_senders;\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\nLet’s repeat the same workflow as before (but now we will insert only one row to get a neater output):\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/45D88E48\n\n(1 row)\n\n=> BEGIN;\n\n=> TRUNCATE TABLE wal;\n\n=> INSERT INTO wal VALUES (42);\n\n=> COMMIT;\n\n=> SELECT pg_current_wal_insert_lsn();\n\npg_current_wal_insert_lsn −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n0/45D89108\n\n(1 row)\n\nCheck out the generated ��� entries.\n\nApart from what we have seen at the minimal level, we have also got the following entries:\n\nreplication-related entries of the Standby resource manager: �������_�����\n\n(active transactions) and ����\n\nthe entry that logs the ������+���� operation,which initializes a new page and\n\ninserts a new row into this page\n\n225\n\nv. ��",
      "content_length": 1042,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "Chapter 11 WAL Modes\n\npostgres$ /usr/local/pgsql/bin/pg_waldump \\ -p /usr/local/pgsql/data/pg_wal -s 0/45D88E48 -e 0/45D89108\n\nrmgr: Standby 0/45D88E48, prev 0/45D88DD0, desc: LOCK xid 122846 db 16391 rel 16562 rmgr: Storage 0/45D88E78, prev 0/45D88E48, desc: CREATE base/16391/24786 rmgr: Heap 0/45D88EA8, prev 0/45D88E78, desc: UPDATE off 49 xmax 122846 flags 0x60 ; new off 50 xmax 0, blkref #0: rel 1663/16391/1259 blk 0 rmgr: Btree 0/45D88F28, prev 0/45D88EA8, desc: INSERT_LEAF off 178, blkref #0: rel 1663/16391/2662 blk 2 rmgr: Btree 0/45D88F68, prev 0/45D88F28, desc: INSERT_LEAF off 149, blkref #0: rel 1663/16391/2663 blk 2 rmgr: Btree 0/45D88FA8, prev 0/45D88F68, desc: INSERT_LEAF off 256, blkref #0: rel 1663/16391/3455 blk 4 rmgr: Heap 0/45D88FE8, prev 0/45D88FA8, desc: INSERT+INIT off 1 flags 0x00, blkref #0: rel 1663/16391/24786 blk 0 rmgr: Standby 0/45D89028, prev 0/45D88FE8, desc: LOCK xid 122846 db 16391 rel 16562 rmgr: Standby 0/45D89058, prev 0/45D89028, desc: RUNNING_XACTS nextXid 122847 latestCompletedXid 122845 oldestRunningXid 122846; 1 xacts: 122846 122846, lsn: rmgr: Transaction len (rec/tot): 0/45D89090, prev 0/45D89058, desc: COMMIT 2023−03−06 14:04:14.538399 MSK; rels: base/16391/24785; inval msgs: catcache 51 catcache 50 relcache 16562\n\n122846, lsn:\n\nlen (rec/tot):\n\n42, tx:\n\n42/\n\n122846, lsn:\n\n42, tx:\n\n42/\n\nlen (rec/tot):\n\n123, tx:\n\n122846, lsn:\n\nlen (rec/tot):\n\n123/\n\n122846, lsn:\n\n64, tx:\n\n64/\n\nlen (rec/tot):\n\n64, tx:\n\n122846, lsn:\n\n64/\n\nlen (rec/tot):\n\n122846, lsn:\n\n64/\n\nlen (rec/tot):\n\n64, tx:\n\n122846, lsn:\n\nlen (rec/tot):\n\n59, tx:\n\n59/\n\n0, lsn:\n\nlen (rec/tot):\n\n42, tx:\n\n42/\n\n54, tx:\n\n54/\n\nlen (rec/tot):\n\n0, lsn:\n\n114, tx:\n\n114/\n\nLogical\n\nLast but not least,the logical level enables logical decoding and logical replication. It has to be activated on the publishing server.\n\nIf we take a look at ��� entries, we will see that this level is almost the same as replica: it adds the entries related to replication sources and some arbitrary logical entries that may be generated by applications. For the most part, logical decoding depends on the information about active transactions (�������_�����) because it requires capturing a snapshot to track system catalog changes.\n\n226",
      "content_length": 2230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "Part III\n\nLocks",
      "content_length": 15,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "12\n\nRelation-Level Locks\n\n12.1 About Locks\n\nLocks control concurrent access to shared resources.\n\nConcurrent access implies that several processes try to get one and the same re- source at the same time. It makes no difference whether these processes are ex- ecuted in parallel (if the hardware permits) or sequentially in the time-sharing mode. If there is no concurrent access, there is no need to acquire locks (for exam- ple, shared buffer cache requires locking, while local cache can do without it).\n\nBefore accessing a resource, the process must acquire a lock on it; when the oper- ation is complete, this lock must be released for the resource to become available to other processes. If locks are managed by the database system, the established order of operations is maintained automatically; if locks are controlled by the ap- plication, the protocol must be enforced by the application itself.\n\nAtalowlevel,alockissimplyachunkofsharedmemorythatdefinesthelockstatus (whether it is acquired or not); it can also provide some additional information, such as the process number or acquisition time.\n\nAs you can guess, a shared memory segment is a resource in its own right. Concurrent access to such resources is regulated bysynchronization primitives (such as semaphores or mutexes) provided by the operating system. They guarantee strictly consecutive execution of the code that accesses a shared resource. At the lowest level,these primitives are based on atomic ��� instructions (such as test-and-set or compare-and-swap).\n\nIn general, we can use locks to protect any resource as long as it can be unambigu- ously identified and assigned a particular lock address.\n\n229",
      "content_length": 1681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "Chapter 12 Relation-Level Locks\n\nFor example, we can lock a database object, such as a table (identified by oid in the system catalog), a data page (identified by a filename and a position within this file),a row version (identified by a page and an offset within this page). We can also lock a memory structure,such as a hash table or a buffer (identified by an assigned ��). We can even lock an abstract resource that has no physical representation.\n\nBut it is not always possible to acquire a lock at once: a resource can be already locked by someone else. Then the process either joins the queue (if it is allowed for this particular lock type) or tries again some time later. Either way, it has to wait for the lock to be released.\n\nI would like to single out two factors that can greatly affect locking efficiency.\n\nGranularity, or the“grain size”of a lock. Granularity is important if resources form\n\na hierarchy.\n\nFor example, a table consists of pages, which, in their turn, consist of tu- ples. All these objects can be protected by locks. Table-level locks are coarse- grained; they forbid concurrent access even if the processes need to get to different pages or rows.\n\nRow-level locks are fine-grained, so they do not have this drawback; however, the number of locks grows. To avoid using too much memory for lock-related metadata, Postgre��� can apply various methods, one of them being lock es- calation: if the number of fine-grained locks exceeds a certain threshold, they are replaced by a single lock of coarser granularity.\n\nA set of modes in which a lock can be acquired.\n\nAs a rule, only two modes are applied. The exclusive mode is incompatible with all the other modes, including itself. The shared mode allows a resource to be locked by several processes at a time. The shared mode can be used for reading, while the exclusive mode is applied for writing.\n\nIn general, there may be other modes too. Names of modes are unimportant, it is their compatibility matrix that matters.\n\nFiner granularity and support for multiple compatible modes give more opportu- nities for concurrent execution.\n\n230",
      "content_length": 2120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "12.2 Heavyweight Locks\n\nAll locks can be classified by their duration.\n\nLong-term locks are acquired for a potentially long time (in most cases,till the end of the transaction); they typically protect such resources as relations and rows. These locks are usually managed by Postgre��� automatically, but a user still has some control over this process.\n\nLong-term locks offer multiple modes that enable various concurrent oper- ations on data. They usually have extensive infrastructure (including such features as wait queues, deadlock detection, and instrumentation) since its maintenance is anyway much cheaper than operations on protected data.\n\nShort-term locks are acquired for fractions of a second and rarely last longer than several ��� instructions; they usually protect data structures in the shared memory. Postgre��� manages such locks in a fully automated way.\n\nShort-term locks typically offer very few modes and only basic infrastructure, which may have no instrumentation at all.\n\nPostgre��� supports various types of locks.1 Heavyweight locks (which are acquired locks are considered long-term. Short- on relations and other objects) and row-level term locks comprise various locks on memory structures . Besides, there is also a distinct group of predicate locks\n\n, which, despite their name, are not locks at all.\n\n12.2 Heavyweight Locks\n\nHeavyweight locks are long-term ones. Acquired at the object level,they are mainly used for relations, but can also be applied to some other types of objects. Heavy- weightlockstypicallyprotectobjectsfromconcurrentupdatesorforbidtheirusage during restructuring,but they can address other needs too. Such a vague definition is deliberate: locks of this type are used for all kinds of purposes. The only thing they have in common is their internal structure.\n\nUnless explicitly specified otherwise, the term lock usually implies a heavyweight lock.\n\n1 backend/storage/lmgr/README\n\n231\n\np. ��� p. ��� p. ���",
      "content_length": 1963,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "64 100\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\np. ���\n\nChapter 12 Relation-Level Locks\n\nHeavyweightlocksarelocatedintheserver’ssharedmemory1 andcanbedisplayed inthepg_locksview. Theirtotalnumberislimitedbythe max_locks_per_transaction max_connections. value multiplied by\n\nAll transactions use a common pool of locks, so one transaction can acquire more than max_locks_per_transaction locks. What really matters is that the total number of locks in the system does not exceed the defined limit. Since the pool is initial- ized when the server is launched,changing any of these two parameters requires a server restart.\n\nIf a resource is already locked in an incompatible mode, the process trying to ac- quire another lock joins the queue. Waiting processes do not waste ��� time: they fall asleep until the lock is released and the operating system wakes them up.\n\nTwo transactions can find themselves in a deadlock if the first transaction is unable to continue its operation until it gets a resource locked by the other transaction, which, in its turn, needs a resource locked by the first transaction. This case is rather simple; a deadlock can also involve more than two transactions. Since dead- locks cause infinite waits, Postgre��� detects them automatically and aborts one of the affected transactions to ensure that normal operation can continue.\n\nDifferent types of heavyweight locks serve different purposes, protect different re- sources, and support different modes, so we will consider them separately.\n\nThe following list provides the names of lock types as they appear in the locktype column of the pg_locks view:\n\ntransactionid and virtualxid — a lock\n\non a transaction ��\n\nrelation — a relation-level lock\n\ntuple — a lock acquired on a tuple\n\nobject — a lock on an object\n\nthat is not a relation\n\nextend — a relation extension lock\n\npage — a page-level lock\n\nused by some index types\n\nadvisory — an advisory lock\n\n1 backend/storage/lmgr/lock.c\n\n232",
      "content_length": 1984,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "12.3 Locks on Transaction IDs\n\nAlmost all heavyweight locks are acquired automatically as needed and are re- leased automatically when the corresponding transaction completes. There are some exceptions though: for example, a relation-level lock can be set explicitly, while advisory locks are always managed by users.\n\n12.3 Locks on Transaction IDs\n\nEach transaction always holds an exclusive lock on its own �� (both virtual if available).\n\nand real,\n\nPostgre��� offers two locking modes for this purpose, exclusive and shared. Their compatibility matrix is very simple: the shared mode is compatible with itself, while the exclusive mode cannot be combined with any mode.\n\nShared Exclusive\n\nShared\n\n×\n\nExclusive\n\n×\n\n×\n\nTo track completion of a particular transaction, a process can request a lock on this transaction’s ��, in any mode. Since the transaction itself is already holding an exclusive lock on its own ��, another lock is impossible to acquire. The process requesting this lock joins the queue and falls asleep. Once the transaction com- pletes, the lock is released, and the queued process wakes up. Clearly, it will not manage to acquire the lock because the corresponding resource has already disap- peared, but this lock is not what is actually needed anyway.\n\nLet’s start a transaction in a separate session and get the process �� (���) of the backend:\n\n=> BEGIN;\n\n=> SELECT pg_backend_pid();\n\npg_backend_pid −−−−−−−−−−−−−−−−\n\n28980\n\n(1 row)\n\nThe started transaction holds an exclusive lock on its own virtual ��:\n\n233\n\np. ��",
      "content_length": 1543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "Chapter 12 Relation-Level Locks\n\n=> SELECT locktype, virtualxid, mode, granted FROM pg_locks WHERE pid = 28980;\n\nlocktype\n\n| virtualxid |\n\nmode\n\n| granted\n\n−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−\n\nvirtualxid | 5/2\n\n| ExclusiveLock | t\n\n(1 row)\n\nHere locktype is the type of the lock, virtualxid is the virtual transaction �� (which identifies the locked resource), and mode is the locking mode (exclusive in this case). The granted flag shows whether the requested lock has been acquired.\n\nOnce the transaction gets a real ��, the corresponding lock is added to this list:\n\n=> SELECT pg_current_xact_id();\n\npg_current_xact_id −−−−−−−−−−−−−−−−−−−−\n\n122849\n\n(1 row)\n\n=> SELECT locktype, virtualxid, transactionid AS xid, mode, granted FROM pg_locks WHERE pid = 28980;\n\nlocktype\n\n| virtualxid |\n\nxid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−\n\nvirtualxid transactionid |\n\n| 5/2\n\n| ExclusiveLock | t | | 122849 | ExclusiveLock | t\n\n(2 rows)\n\nNow this transaction holds exclusive locks on both its ��s.\n\n12.4 Relation-Level Locks\n\nPostgre��� provides as many as eight modes in which a relation (a table, an index, or any other object) can be locked.1 Such a variety allows you to maximize the number of concurrent commands that can be run on a relation.\n\nThe next page shows the compatibility matrix extended with examples of com- mands that require the corresponding locking modes. There is no point in mem- orizing all these modes or trying to find the logic behind their naming, but it is\n\n1 postgresql.org/docs/14/explicit-locking#LOCKING-TABLES.html\n\n234",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "12.4 Relation-Level Locks\n\ndefinitely useful to look through this data, draw some general conclusions, and refer to this table as required.\n\nAS RS RE SUE S SRE E AE\n\nAccess Share\n\n× SELECT\n\nRow Share\n\n× × SELECTFOR UPDATE/SHARE\n\nRow Exclusive\n\n× × × × INSERT,UPDATE,DELETE\n\nShare Update Exclusive\n\n× × × × × VACUUM,CREATE INDEX CONCURRENTLY\n\nShare\n\n× ×\n\n× × × CREATE INDEX\n\nShare Row Exclusive\n\n× × × × × × CREATE TRIGGER\n\nExclusive\n\n× × × × × × × REFRESH MAT.VIEW CONCURRENTLY\n\nAccess Exclusive\n\n× × × × × × × × DROP,TRUNCATE,VACUUM FULL,\n\nLOCKTABLE,REFRESH MAT.VIEW\n\nThe Access Share mode is the weakest one; it can be used with any other mode except Access Exclusive, which is incompatible with all the modes. Thus, a ������ command can be run in parallel with almost any operation, but it does not let you drop a table that is being queried.\n\nThe first four modes allow concurrent heap modifications, while the other four do not. For example, the ������ ����� command uses the Share mode, which is com- patible with itself (so you can create several indexes on a table concurrently) and with the modes used by read-only operations. As a result, ������ commands can run in parallel with index creation,while ������, ������,and ������ commands will be blocked.\n\nConversely, unfinished transactions that modify heap data block the ������ ����� command. Instead, you can call ������ ����� ������������, which uses a weaker Share Update Exclusive mode: it takes longer to create an index (and this operation can even fail), but in return, concurrent data updates are allowed.\n\nThe ����� ����� command has multiple flavors that use different locking modes (Share Update Exclusive, Share Row Exclusive, Access Exclusive). All of them are described in the documentation.1\n\n1 postgresql.org/docs/14/sql-altertable.html\n\n235",
      "content_length": 1818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "Chapter 12 Relation-Level Locks\n\nExamples in this part of the book rely on the accounts table again:\n\n=> TRUNCATE accounts;\n\n=> INSERT INTO accounts(id, client, amount) VALUES\n\n100.00), (1, 'alice', (2, 'bob', 200.00), (3, 'charlie', 300.00);\n\nWe will have to access the pg_locks table more than once,so let’s create a view that shows all ��s in a single column, thus making the output more concise:\n\n=> CREATE VIEW locks AS SELECT pid, locktype, CASE locktype\n\nWHEN 'relation' THEN relation::regclass::text WHEN 'transactionid' THEN transactionid::text WHEN 'virtualxid' THEN virtualxid\n\nEND AS lockid, mode, granted\n\nFROM pg_locks ORDER BY 1, 2, 3;\n\nThe transaction that is still running in the first session updates a row. This opera- tion locks the accounts table and all its indexes, which results in two new locks of the relation type acquired in the Row Exclusive mode:\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\n=> SELECT locktype, lockid, mode, granted FROM locks WHERE pid = 28980;\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\nrelation relation transactionid | 122849 virtualxid\n\n| accounts | RowExclusiveLock | t | accounts_pkey | RowExclusiveLock | t | t | t\n\n| ExclusiveLock | ExclusiveLock\n\n| 5/2\n\n(4 rows)\n\n236",
      "content_length": 1305,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "12.5 Wait Queue\n\n12.5 Wait Queue\n\nHeavyweight locks form a fair wait queue.1 A process joins the queue if it attempts to acquire a lock that is incompatible either with the current lock or with the locks requested by other processes already in the queue.\n\nWhile the first session is working on an update, let’s try to create an index on this table in another session:\n\n=> SELECT pg_backend_pid();\n\npg_backend_pid −−−−−−−−−−−−−−−−\n\n29459\n\n(1 row)\n\n=> CREATE INDEX ON accounts(client);\n\nThe command hangs,waiting for the resource to be released. The transaction tries to lock the table in the Share mode but cannot do it:\n\n=> SELECT locktype, lockid, mode, granted FROM locks WHERE pid = 29459;\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−\n\nrelation virtualxid | 6/3\n\n| accounts | ShareLock\n\n| f | ExclusiveLock | t\n\n(2 rows)\n\nNowletthethirdsessionstartthe������ ����command. Itwillalsojointhequeue because it requires the Access Exclusive mode, which conflicts with all the other modes:\n\n=> SELECT pg_backend_pid();\n\npg_backend_pid −−−−−−−−−−−−−−−−\n\n29662\n\n(1 row)\n\n=> VACUUM FULL accounts;\n\n1 backend/storage/lmgr/lock.c, LockAcquire function\n\n237",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "Chapter 12 Relation-Level Locks\n\n=> SELECT locktype, lockid, mode, granted FROM locks WHERE pid = 29662;\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\nrelation transactionid | 122853 virtualxid\n\n| accounts | AccessExclusiveLock | f | t | ExclusiveLock | t | ExclusiveLock\n\n| 7/4\n\n(3 rows)\n\nAll the subsequent contenders will now have to join the queue, regardless of their locking mode. Even simple ������ queries will honestly follow ������ ����, al- though they are compatible with the Row Exclusive lock held by the first session performing the update.\n\n=> SELECT pg_backend_pid();\n\npg_backend_pid −−−−−−−−−−−−−−−−\n\n29872\n\n(1 row)\n\n=> SELECT * FROM accounts;\n\n=> SELECT locktype, lockid, mode, granted FROM locks WHERE pid = 29872;\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\nrelation virtualxid | 8/3\n\n| accounts | AccessShareLock | f | t\n\n| ExclusiveLock\n\n(2 rows)\n\nT1\n\nUPDATE\n\nT2\n\nrelation\n\nT3\n\nCREATE INDEX\n\nT4\n\nVACUUM FULL\n\nSELECT\n\n238",
      "content_length": 1050,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "12.5 Wait Queue\n\npg_blocking_pids function gives a high-level overview of all waits. It shows The the ��s of all processes queued before the specified one that are already holding or would like to acquire an incompatible lock:\n\n=> SELECT pid,\n\npg_blocking_pids(pid), wait_event_type, state, left(query,50) AS query\n\nFROM pg_stat_activity WHERE pid IN (28980,29459,29662,29872) \\gx\n\n−[ RECORD 1 ]−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− pid pg_blocking_pids | {} wait_event_type | Client state | idle in transaction | UPDATE accounts SET amount = amount + 100.00 WHERE query −[ RECORD 2 ]−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− pid pg_blocking_pids | {28980} | Lock wait_event_type | active state query | CREATE INDEX ON accounts(client); −[ RECORD 3 ]−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− pid pg_blocking_pids | {28980,29459} wait_event_type state query −[ RECORD 4 ]−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− pid pg_blocking_pids | {29662} wait_event_type state query\n\n| 28980\n\n| 29459\n\n| 29662\n\n| Lock | active | VACUUM FULL accounts;\n\n| 29872\n\n| Lock | active | SELECT * FROM accounts;\n\nTogetmoredetails,youcanreviewtheinformationprovidedinthepg_lockstable.1\n\nOnce the transaction is completed (either committed or aborted), all its locks are released.2 The first process in the queue gets the requested lock and wakes up.\n\n1 wiki.postgresql.org/wiki/Lock_dependency_information 2 backend/storage/lmgr/lock.c, LockReleaseAll & LockRelease functions\n\n239\n\nv. �.�",
      "content_length": 1530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "Chapter 12 Relation-Level Locks\n\nHere the transaction commit in the first session leads to sequential execution of all the queued processes:\n\n=> ROLLBACK;\n\nROLLBACK\n\nCREATE INDEX\n\nVACUUM\n\nid | client\n\n| amount\n\n−−−−+−−−−−−−−−+−−−−−−−−\n\n| 100.00 1 | alice 2 | bob | 200.00 3 | charlie | 300.00\n\n(3 rows)\n\n240",
      "content_length": 307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "13\n\nRow-Level Locks\n\n13.1 Lock Design\n\nThanks to snapshot isolation, heap tuples do not have to be locked for reading. However, two write transactions must not be allowed to modify one and the same row at the same time. Rows must be locked in this case, but heavyweight locks are not a very good choice for this purpose: each of them takes space in the server’s shared memory (hundreds of bytes, not to mention all the supporting infrastruc- ture),andPostgre���internalmechanismsarenotdesignedtohandleahugenum- ber of concurrent heavyweight locks.\n\nSome database systems solvethis problem bylock escalation: if row-levellocks are too many, they are replaced by a single lock of finer granularity (for example, by a page-level or table-level lock). It simplifies the implementation, but can greatly limit system throughput.\n\nIn Postgre���, the information on whether a particular row is locked is kept only in the header of its current heap tuple. Row-level locks are virtually attributes in heap pages rather than actual locks, and they are not reflected in ��� in any way.\n\nA row is typically locked when it is being updated or deleted. In both cases, the current version of the row is marked as deleted. The attribute used for this pur- pose is the current transaction’s �� specified in the xmax field, and it is the same �� (combined with additional hint bits) that indicates that the row is locked. If a transaction wants to modify a row but sees an active transaction �� in the xmax field of its current version, it has to wait for this transaction to complete. Once it is over, all the locks are released, and the waiting transaction can proceed.\n\nThis mechanism allows locking as many rows as required at no extra cost.\n\n241\n\np. ��",
      "content_length": 1738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "Chapter 13 Row-Level Locks\n\nThe downside of this solution is that other processes cannot form a queue, as ��� contains no information about such locks. Therefore, heavyweight locks are still required: a process waiting for a row to be released requests a lock on the �� of the transaction currently busy with this row. Once the transaction completes, the row becomes available again. Thus, the number of heavyweight locks is proportional to the number of concurrent processes rather than rows being modified.\n\n13.2 Row-Level Locking Modes\n\nRow-level locks support four modes.1 Two of them implement exclusive locks that can be acquired by only one transaction at a time, while the other two provide shared locks that can be held by several transactions simultaneously.\n\nHere is the compatibility matrix of these modes:\n\nKeyShare\n\nShare\n\nNo Key Update\n\nUpdate\n\nKeyShare\n\n×\n\nShare\n\n×\n\n×\n\nNo KeyUpdate\n\n×\n\n×\n\n×\n\nUpdate\n\n×\n\n×\n\n×\n\n×\n\nExclusive Modes\n\nThe Update mode allows modifying any tuple fields and even deleting the whole tuple, while the No Key Update mode permits only those changes that do not in- volve any fields related to unique indexes (in other words, foreign keys must not be affected).\n\nThe ������ command automatically chooses the weakest locking mode possible; keys usually remain unchanged, so rows are typically locked in the No Key Update mode.\n\n1 postgresql.org/docs/14/explicit-locking#LOCKING-ROWS.html\n\n242",
      "content_length": 1428,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "13.2 Row-Level Locking Modes\n\nLet’s create a function that uses pageinspect to display some tuple metadata that we are interested in, namely the xmax field and several hint bits:\n\n=> CREATE FUNCTION row_locks(relname text, pageno integer) RETURNS TABLE(\n\nctid tid, xmax text, lock_only text, is_multi text, keys_upd text, keyshr text, shr text\n\n) AS $$ SELECT (pageno,lp)::text::tid,\n\nt_xmax, & 128 CASE WHEN t_infomask CASE WHEN t_infomask & 4096 CASE WHEN t_infomask2 & 8192 CASE WHEN t_infomask CASE WHEN t_infomask\n\n= 128 = 4096 = 8192 = 16\n\nTHEN 't' END, THEN 't' END, THEN 't' END, THEN 't' END,\n\n& 16 & 16+64 = 16+64 THEN 't' END\n\nFROM heap_page_items(get_raw_page(relname,pageno)) ORDER BY lp; $$ LANGUAGE sql;\n\nNow start a transaction on the accounts table to update the balance of the first account (the key remains the same) and the �� of the second account (the key gets updated):\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\n=> UPDATE accounts SET id = 20 WHERE id = 2;\n\nThe page now contains the following metadata:\n\n=> SELECT * FROM row_locks('accounts',0) LIMIT 2;\n\nctid\n\n|\n\nxmax\n\n| lock_only | is_multi | keys_upd | keyshr | shr\n\n−−−−−−−+−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−+−−−−−\n\n(0,1) | 122858 | (0,2) | 122858 |\n\n| |\n\n| | t\n\n| |\n\n| |\n\n(2 rows)\n\nThe locking mode is defined by the keys_updated hint bit.\n\n=> ROLLBACK;\n\n243",
      "content_length": 1387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "Chapter 13 Row-Level Locks\n\nThe ������ ��� command uses the same xmax field as a locking attribute, but in this case the xmax_lock_only hint bit must also be set. This bit indicates that the tuple is locked but not deleted, which means that it is still current:\n\n=> BEGIN;\n\n=> SELECT * FROM accounts WHERE id = 1 FOR NO KEY UPDATE;\n\n=> SELECT * FROM accounts WHERE id = 2 FOR UPDATE;\n\n=> SELECT * FROM row_locks('accounts',0) LIMIT 2;\n\nctid\n\n|\n\nxmax\n\n| lock_only | is_multi | keys_upd | keyshr | shr\n\n−−−−−−−+−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−+−−−−−\n\n(0,1) | 122859 | t (0,2) | 122859 | t\n\n| |\n\n| | t\n\n| |\n\n| |\n\n(2 rows)\n\n=> ROLLBACK;\n\nShared Modes\n\nThe Share mode can be applied when a row needs to be read, but its modification by another transaction must be forbidden. The Key Share mode allows updating any tuple fields except key attributes.\n\nOf all the shared modes, the Postgre��� core uses only Key Share, which is applied when foreign keys are being checked. Since it is compatible with the No Key Update exclusive mode, foreign key checks do not interfere with concurrent updates of non-key attributes. As for applications, they can use any shared modes they like.\n\nLet me stress once again that simple ������ commands never use row-level locks.\n\n=> BEGIN;\n\n=> SELECT * FROM accounts WHERE id = 1 FOR KEY SHARE;\n\n=> SELECT * FROM accounts WHERE id = 2 FOR SHARE;\n\nHere is what we see in the heap tuples:\n\n=> SELECT * FROM row_locks('accounts',0) LIMIT 2;\n\nctid\n\n|\n\nxmax\n\n| lock_only | is_multi | keys_upd | keyshr | shr\n\n−−−−−−−+−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−+−−−−−\n\n(0,1) | 122860 | t (0,2) | 122860 | t\n\n| |\n\n| |\n\n| t | t\n\n| | t\n\n(2 rows)\n\n244",
      "content_length": 1693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "13.3 Multitransactions\n\nThexmax_keyshr_lockbitissetforbothoperations,butyoucanrecognizetheShare mode by other hint bits.1\n\n13.3 Multitransactions\n\nAs we have seen,the locking attribute is represented by the xmax field,which is set to the �� of the transaction that has acquired the lock. So how is this attribute set for a shared lock held by several transactions at a time?\n\nWhen dealing with shared locks, Postgre��� applies so-called multitransactions (multixacts).2 Amultitransaction is a group of transactions that is assigned a sepa- rate ��. Detailed information on group members and their locking modes is stored in files under the ������/pg_multixact directory. For faster access,locked pages are cached in the shared memory of the server;3 all changes are logged to ensure fault tolerance.\n\nMultixact ��s have the same ��-bit length as regular transaction ��s, but they are issued independently. It means that transactions and multitransactions can po- tentially have the same ��s. To differentiate between the two, Postgre��� uses an additional hint bit: xmax_is_multi.\n\nLet’s add one more exclusive lock acquired by another transaction (Key Share and No Key Update modes are compatible):\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\n=> SELECT * FROM row_locks('accounts',0) LIMIT 2;\n\nctid\n\n|\n\nxmax\n\n| lock_only | is_multi | keys_upd | keyshr | shr\n\n−−−−−−−+−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−+−−−−−\n\n(0,1) | 1 (0,2) | 122860 | t\n\n|\n\n| t |\n\n| |\n\n| | t\n\n| | t\n\n(2 rows)\n\n1 include/access/htup_details.h 2 backend/access/transam/multixact.c 3 backend/access/transam/slru.c\n\n245",
      "content_length": 1632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "p. ���\n\nChapter 13 Row-Level Locks\n\nThe xmax_is_multi bit shows that the first row uses a multitransaction �� instead of a regular one.\n\nWithout going into further implementation details, let’s display the information on all the possible row-level locks using the pgrowlocks extension:\n\n=> CREATE EXTENSION pgrowlocks;\n\n=> SELECT * FROM pgrowlocks('accounts') \\gx\n\n−[ RECORD 1 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−− locked_row | (0,1) locker multi xids modes pids −[ RECORD 2 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−− locked_row | (0,2) locker multi xids modes pids\n\n| 1 | t | {122860,122861} | {\"Key Share\",\"No Key Update\"} | {30423,30723}\n\n| 122860 | f | {122860} | {\"For Share\"} | {30423}\n\nIt looks a lot like querying the pg_locks view, but the pgrowlocks function has to access heap pages, as ��� contains no information on row-level locks.\n\n=> COMMIT;\n\n=> ROLLBACK;\n\nSince multixact ��s are ��-bit, they are subject to wraparound because of counter limits, just like regular transaction ��s. Therefore, Postgre��� has to process mul- tixact ��s in a way similar to freezing: old multixact ��s are replaced with new ones (or with a regular transaction �� if only one transaction is holding the lock by that time).1\n\nBut while regular transaction ��s are frozen only in the xmin field (as a non-empty xmax indicates that the tuple is outdated and will soon be removed), it is the xmax field that has to be frozen for multitransactions: the current row version may be repeatedly locked by new transactions in a shared mode.\n\n1 backend/access/heap/heapam.c, FreezeMultiXactId function\n\n246",
      "content_length": 1572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "13.4 Wait Queue\n\nFreezing of multitransactions can be managed by server parameters, which are similar to those provided for regular freezing: vacuum_multixact_freeze_min_age, vacuum_multixact_freeze_table_age, autovacuum_multixact_freeze_max_age, as well as vacuum_multixact_failsafe_age\n\n.\n\n13.4 Wait Queue\n\nExclusive Modes\n\nSince a row-level lock is just an attribute, the queue is arranged in a not-so-trivial way. When a transaction is about to modify a row, it has to follow these steps:1\n\n� If the xmax field and the hint bits indicate that the row is locked in an incom- patible mode,acquire an exclusive heavyweight lock on the tuple that is being modified.\n\n� If necessary, wait until all the incompatible locks are released by requesting a lockonthe��ofthexmaxtransaction(orseveraltransactionsifxmaxcontains a mutixact ��).\n\n� Write its own �� into xmax in the tuple header and set the required hint bits.\n\n� Release the tuple lock if it was acquired in the first step.\n\nA tuple lock is yet another kind of heavyweight locks,which has the tuple type (not to be confused with a regular row-level lock).\n\nIt may seem that steps � and � are redundant and it is enough to simply wait until all the locking transactions are over. However, if several transactions are trying to update one and the same row, all of them will be waiting on the transaction currentlyprocessingthisrow. Onceitcompletes,theywillfindthemselvesinarace condition for the right to lock the row, and some “unlucky” transactions may have to wait for an indefinitely long time. Such a situation is called resource starvation.\n\nA tuple lock identifies the first transaction in the queue and guarantees that it will be the next one to get the lock.\n\n1 backend/access/heap/README.tuplock\n\n247\n\nv. ��",
      "content_length": 1771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "Chapter 13 Row-Level Locks\n\nBut you can see it for yourself. Since Postgre��� acquires many different locks during its operation,and each of them is reflected in a separate row in the pg_locks table, I am going to create yet another view on top of pg_locks. It will show this information in a more concise form,keeping only those locks that we are currently interested in (the ones related to the accounts table and to the transaction itself, except for any locks on virtual ��s):\n\n=> CREATE VIEW locks_accounts AS SELECT pid, locktype, CASE locktype\n\nWHEN 'relation' THEN relation::regclass::text WHEN 'transactionid' THEN transactionid::text WHEN 'tuple' THEN relation::regclass||'('||page||','||tuple||')'\n\nEND AS lockid, mode, granted\n\nFROM pg_locks WHERE locktype in ('relation','transactionid','tuple')\n\nAND (locktype != 'relation' OR relation = 'accounts'::regclass)\n\nORDER BY 1, 2, 3;\n\nLet’s start the first transaction and update a row:\n\n=> BEGIN;\n\n=> SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122863 |\n\n30723\n\n(1 row)\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\nThetransactionhascompletedallthefourstepsoftheworkflowandisnowholding a lock on the table:\n\n=> SELECT * FROM locks_accounts WHERE pid = 30723;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30723 | relation 30723 | transactionid | 122863\n\n| accounts | RowExclusiveLock | t | t\n\n| ExclusiveLock\n\n(2 rows)\n\nStart the second transaction and try to update the same row. The transaction will hang, waiting on a lock:\n\n248",
      "content_length": 1639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "13.4 Wait Queue\n\n=> BEGIN;\n\n=> SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122864 |\n\n30794\n\n(1 row)\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\nT1 No KeyUpdate\n\nT2\n\nctid xmin xmax (0,1) T1\n\ndata\n\ntuple (0,1)\n\nThe second transaction only gets as far as the second step. For this reason, apart from locking the table and its own ��, it adds two more locks, which are also re- flected in the pg_locks view: the tuple lock acquired at the first step and the lock of the �� of the second transaction requested at the second step:\n\n=> SELECT * FROM locks_accounts WHERE pid = 30794;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30794 | relation 30794 | transactionid | 122863 30794 | transactionid | 122864 30794 | tuple\n\n| RowExclusiveLock | t | f | ShareLock | t | ExclusiveLock | t | accounts(0,1) | ExclusiveLock\n\n| accounts\n\n(4 rows)\n\nThe third transaction will get stuck on the first step. It will try to acquire a lock on the tuple and will stop at this point:\n\n=> BEGIN; => SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122865 |\n\n30865\n\n(1 row) => UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\n249",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "Chapter 13 Row-Level Locks\n\n=> SELECT * FROM locks_accounts WHERE pid = 30865;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30865 | relation 30865 | transactionid | 122865 30865 | tuple\n\n| accounts\n\n| RowExclusiveLock | t | t | ExclusiveLock | f | accounts(0,1) | ExclusiveLock\n\n(3 rows)\n\nThe fourth and all the subsequent transactions trying to update this row will not differ from the third transaction in this respect: all of them will be waiting on the same tuple lock.\n\n=> BEGIN;\n\n=> SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122866 |\n\n30936\n\n(1 row)\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\n=> SELECT * FROM locks_accounts WHERE pid = 30865;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30865 | relation 30865 | transactionid | 122865 30865 | tuple\n\n| accounts\n\n| RowExclusiveLock | t | t | ExclusiveLock | f | accounts(0,1) | ExclusiveLock\n\n(3 rows)\n\nT1 No KeyUpdate\n\nT2\n\nctid xmin xmax (0,1) T1\n\ndata\n\nT3\n\ntuple (0,1)\n\nT4\n\nTo get the full picture of the current waits,you can extend the pg_stat_activity view with the information on locking processes:\n\n250",
      "content_length": 1300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "13.4 Wait Queue\n\n=> SELECT pid,\n\nwait_event_type, wait_event, pg_blocking_pids(pid)\n\nFROM pg_stat_activity WHERE pid IN (30723,30794,30865,30936);\n\npid\n\n| wait_event_type |\n\nwait_event\n\n| pg_blocking_pids\n\n−−−−−−−+−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−\n\n30723 | Client 30794 | Lock 30865 | Lock 30936 | Lock\n\n| ClientRead | transactionid | {30723} | {30794} | tuple | {30794,30865} | tuple\n\n| {}\n\n(4 rows)\n\nIf the first transaction is aborted, everything will work as expected: all the subse- quent transactions will move one step further without jumping the queue.\n\nAnd yet it is more likely that the first transaction will be committed. At the Repeat- able Read or Serializable isolation levels, it would result in a serialization failure, so the second transaction would have to be aborted1 (and all the subsequent trans- actions in the queue would get aborted too). But at the Read Committed isolation level the modified row will be re-read, and its update will be retried.\n\nSo, the first transaction is committed:\n\n=> COMMIT;\n\nThe second transaction wakes up and successfully completes the third and the fourth steps of the workflow:\n\nUPDATE 1\n\n=> SELECT * FROM locks_accounts WHERE pid = 30794;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30794 | relation 30794 | transactionid | 122864\n\n| accounts | RowExclusiveLock | t | t\n\n| ExclusiveLock\n\n(2 rows)\n\nAs soon as the second transaction releases the tuple lock, the third one also wakes up, but it sees that the xmax field of the new tuple contains a different �� already.\n\n1 backend/executor/nodeModifyTable.c, ExecUpdate function\n\n251",
      "content_length": 1678,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "Chapter 13 Row-Level Locks\n\nAt this point, the above workflow is over. At the Read Committed isolation level, one more attempt to lock the row is performed,1 but it does not follow the outlined steps. The third transaction is now waiting for the second one to complete without trying to acquire a tuple lock:\n\n=> SELECT * FROM locks_accounts WHERE pid = 30865;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30865 | relation 30865 | transactionid | 122864 30865 | transactionid | 122865\n\n| accounts | RowExclusiveLock | t | f | ShareLock | t | ExclusiveLock\n\n(3 rows)\n\nThe fourth transaction does the same:\n\n=> SELECT * FROM locks_accounts WHERE pid = 30936;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30936 | relation 30936 | transactionid | 122864 30936 | transactionid | 122866\n\n| accounts | RowExclusiveLock | t | f | ShareLock | t | ExclusiveLock\n\n(3 rows)\n\nNow both the third and the fourth transactions are waiting for the second one to complete, risking to get into a race condition. The queue has virtually fallen apart.\n\nT3\n\nT4\n\nT2 No KeyUpdate\n\nctid xmin xmax (0,1) T1 (0,2) T2\n\nT1\n\ndata\n\nIf other transactions had joined the queue while it still existed, all of them would have been dragged into this race.\n\n1 backend/access/heap/heapam_handler.c, heapam_tuple_lock function\n\n252",
      "content_length": 1431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "13.4 Wait Queue\n\nConclusion: it is not a good idea to update one and the same table row in mul- tiple concurrent processes. Under high load, this hotspot can quickly turn into a bottleneck that causes performance issues.\n\nLet’s commit all the started transactions.\n\n=> COMMIT;\n\nUPDATE 1\n\n=> COMMIT;\n\nUPDATE 1\n\n=> COMMIT;\n\nShared Modes\n\nPostgre��� acquires shared locks only for referential integrity checks. Using them in a high-load application can lead to resource starvation, and a two-level locking model cannot prevent such an outcome.\n\nLet’s recall the steps a transaction should take to lock a row:\n\n� If the xmax field and hint bits indicate that the row is locked in the exclusive\n\nmode, acquire an exclusive heavyweight tuple lock.\n\n� If required, wait for all the incompatible locks to be released by requesting a lockonthe��ofthexmaxtransaction(orseveraltransactionsifxmaxcontains a multixact ��).\n\n� Write its own �� into xmax in the tuple header and set the required hint bits.\n\n� Release the tuple lock if it was acquired in the first step.\n\nThe first two steps imply that if the locking modes are compatible, the transaction will jump the queue.\n\nLet’s repeat our experiment from the very beginning.\n\n=> TRUNCATE accounts;\n\n253",
      "content_length": 1243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Chapter 13 Row-Level Locks\n\n=> INSERT INTO accounts(id, client, amount) VALUES\n\n(1,'alice',100.00), (2,'bob',200.00), (3,'charlie',300.00);\n\nStart the first transaction:\n\n=> BEGIN;\n\n=> SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122869 |\n\n30723\n\n(1 row)\n\nThe row is now locked in a shared mode:\n\n=> SELECT * FROM accounts WHERE id = 1 FOR SHARE;\n\nThe second transaction tries to update the same row, but it is not allowed: Share and No Key Update modes are incompatible:\n\n=> BEGIN;\n\n=> SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122870 |\n\n30794\n\n(1 row)\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\nWaiting for the first transaction to complete,the second transaction is holding the tuple lock, just like in the previous example:\n\n=> SELECT * FROM locks_accounts WHERE pid = 30794;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30794 | relation 30794 | transactionid | 122869 30794 | transactionid | 122870 30794 | tuple\n\n| accounts\n\n| RowExclusiveLock | t | f | ShareLock | t | ExclusiveLock | t | accounts(0,1) | ExclusiveLock\n\n(4 rows)\n\n254",
      "content_length": 1259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "13.4 Wait Queue\n\nT1\n\nT2\n\nShare ctid xmin xmax (0,1) T1\n\ndata\n\ntuple (0,1)\n\nNow let the third transaction lock the row in a shared mode. Such a lock is com- patible with the already acquired lock, so this transaction jumps the queue:\n\n=> BEGIN;\n\n=> SELECT txid_current(), pg_backend_pid();\n\ntxid_current | pg_backend_pid −−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n122871 |\n\n30865\n\n(1 row)\n\n=> SELECT * FROM accounts WHERE id = 1 FOR SHARE;\n\nWe have got two transactions locking the same row:\n\n=> SELECT * FROM pgrowlocks('accounts') \\gx\n\n−[ RECORD 1 ]−−−−−−−−−−−−−−− locked_row | (0,1) locker multi xids modes pids\n\n| 2 | t | {122869,122871} | {Share,Share} | {30723,30865}\n\nT1\n\nT3\n\nT2\n\nShare ctid xmin xmax (0,1) multi\n\ndata\n\ntuple (0,1)\n\n255",
      "content_length": 732,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "Chapter 13 Row-Level Locks\n\nIf the first transaction completes at this point, the second one will wake up to see that the row is still locked and will get back to the queue—but this time it will find itself behind the third transaction:\n\n=> COMMIT;\n\n=> SELECT * FROM locks_accounts WHERE pid = 30794;\n\npid\n\n|\n\nlocktype\n\n|\n\nlockid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n30794 | relation 30794 | transactionid | 122870 30794 | transactionid | 122871 30794 | tuple\n\n| accounts\n\n| RowExclusiveLock | t | t | ExclusiveLock | f | ShareLock | t\n\n| accounts(0,1) | ExclusiveLock\n\n(4 rows)\n\nAnd only when the third transaction completes will the second one be able to per- form an update (unless other shared locks appear within this time interval).\n\n=> COMMIT;\n\nUPDATE 1\n\n=> COMMIT;\n\nForeignkeychecksareunlikelytocauseanyissues,askeyattributesusuallyremain unchanged and Key Share can be used together with No Key Update. But in most cases, you should avoid shared row-level locks in applications.\n\n13.5 No-Wait Locks\n\nS�� commands usually wait forthe requested resources to be freed. But sometimes it makes sense to cancel the operation if the lock cannot be acquired immediately. For this purpose, commands like ������, ����, and ����� offer the ������ clause.\n\nLet’s lock a row:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 1;\n\n256",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "13.5 No-Wait Locks\n\nThe command with the ������ clause immediately completes with an error if the requested resource is locked:\n\n=> SELECT * FROM accounts FOR UPDATE NOWAIT;\n\nERROR:\n\ncould not obtain lock on row in relation \"accounts\"\n\nSuch an error can be captured and handled by the application code.\n\nThe ������ and ������ commands do not have the ������ clause. Instead, you can try to lock the row using the ������ ��� ������ ������ command and then update or delete it if the attempt is successful.\n\nIn some rare cases, it may be convenient to skip the already locked rows and start processing the available ones right away. This is exactlywhat ��������� does when run with the ���� ������ clause:\n\n=> SELECT * FROM accounts ORDER BY id FOR UPDATE SKIP LOCKED LIMIT 1;\n\nid | client | amount −−−−+−−−−−−−−+−−−−−−−−\n\n2 | bob\n\n| 200.00\n\n(1 row)\n\nIn this example, the first (already locked) row was skipped, and the query locked and returned the second row.\n\nThis approach enables us to process rows in batches or set up parallel processing of event queues. However, avoid inventing other use cases for this command—most tasks can be addressed using much simpler methods.\n\nLast but not least, you can avoid long waits by setting a timeout:\n\n=> SET lock_timeout = '1s';\n\n=> ALTER TABLE accounts DROP COLUMN amount;\n\nERROR:\n\ncanceling statement due to lock timeout\n\nThecommand completeswith anerror because ithas failedto acquirea lock within one second. A timeout can be set not only at the session level, but also at lower levels, for example, for a particular transaction.\n\n257\n\np. ���",
      "content_length": 1588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "Chapter 13 Row-Level Locks\n\nThis method prevents long waits during table processing when the command re- quiring an exclusive lock is executed under load. If an error occurs, this command can be retried after a while.\n\nWhile statement_timeout limits the total time of operator execution, the lock_timeout pa- rameter defines the maximum time that can be spent waiting on a lock.\n\n=> ROLLBACK;\n\n13.6 Deadlocks\n\nA transaction may sometimes require a resource that is currently being used by another transaction, which, in its turn, may be waiting on a resource locked by the third transaction, and so on. Such transactions get queued using heavyweight locks.\n\nBut occasionally a transaction already in the queue may need yet another resource, so it has to join the same queue again and wait for this resource to be released. A deadlock1 occurs: the queue now has a circular dependency that cannot resolve on its own.\n\nFor better visualization,let’s draw a wait-for graph. Its nodes represent active pro- cesses, while the edges shown as arrows point from the processes waiting on locks to the processes holding these locks. If the graph has a cycle, that is, a node can reach itself following the arrows, it means that a deadlock has occurred.\n\nThe illustrations here show transactions rather than processes. This substitution is usually acceptable because one transaction is executed by one process, and locks can only be acquired within a transaction. But in general,it is more correct to talk about processes,as some locks may not be released right away when the transaction is complete.\n\nIf a deadlock has occurred, and none of its participants has set a timeout, transac- tions will be waiting on each other forever. That’s why the lock manager2 performs automatic deadlock detection.\n\nHowever, this check requires some effort, which should not be wasted each time a lock is requested (after all, deadlocks do not happen too often). So if the process\n\n1 postgresql.org/docs/14/explicit-locking#LOCKING-DEADLOCKS.html 2 backend/storage/lmgr/README\n\n258",
      "content_length": 2054,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "13.6 Deadlocks\n\nT3\n\nT2\n\nresource 3\n\nT1\n\nresource 2\n\nresource 1\n\nmakes an unsuccessful attempt to acquire a lock and falls asleep after joining the deadlock_timeout queue,Postgre��� automatically sets a timeout as defined by the parameter.1 If the resource becomes available earlier—great, then the extra cost of the check will be avoided. But if the wait continues after the deadlock_timeout units of time, the waiting process wakes up and initiates the check.2\n\nThis check effectively consists in building a wait-for graph and searching it for cy- cles.3 To “freeze” the current state of the graph, Postgre��� stops any processing of heavyweight locks for the whole duration of the check.\n\nIf no deadlocks are detected, the process falls asleep again; sooner or later its turn will come.\n\nIf a deadlock is detected, one of the transactions will be forced to terminate, thus releasing its locks and enabling other transactions to continue their execution. In most cases, it is the transaction initiating the check that gets interrupted, but if the cycle includes an autovacuum process that is not currently freezing tuples to prevent wraparound, the server terminates autovacuum as having lower priority.\n\nDeadlocksusuallyindicatebadapplicationdesign. Todiscoversuchsituations,you have two things to watch out for: the corresponding messages in the server log and an increasing deadlocks value in the pg_stat_database table.\n\n1 backend/storage/lmgr/proc.c, ProcSleep function 2 backend/storage/lmgr/proc.c, CheckDeadLock function 3 backend/storage/lmgr/deadlock.c\n\n259\n\n1s",
      "content_length": 1572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "Chapter 13 Row-Level Locks\n\nDeadlocks by Row Updates\n\nAlthough deadlocks are ultimately caused by heavyweight locks, it is mostly row- level locks acquired in different order that lead to them.\n\nSuppose a transaction is going to transfer $��� between two accounts. It starts by drawing this sum from the first account:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount - 100.00 WHERE id = 1;\n\nUPDATE 1\n\nAt the same time, another transaction is going to transfer $�� from the second account to the first one. It begins by drawing this sum from the second account:\n\n=> BEGIN;\n\n=> UPDATE accounts SET amount = amount - 10.00 WHERE id = 2;\n\nUPDATE 1\n\nNow the first transaction attempts to increase the amount in the second account but sees that the corresponding row is locked:\n\n=> UPDATE accounts SET amount = amount + 100.00 WHERE id = 2;\n\nThen the second transaction tries to update the first account but also gets locked:\n\n=> UPDATE accounts SET amount = amount + 10.00 WHERE id = 1;\n\nThis circular wait will never resolve on its own. Unable to obtain the resource within one second,the first transaction initiates a deadlock check and gets aborted by the server:\n\nERROR: DETAIL: blocked by process 30723. Process 30723 waits for ShareLock on transaction 122876; blocked by process 30423. HINT: CONTEXT:\n\ndeadlock detected\n\nProcess 30423 waits for ShareLock on transaction 122877;\n\nSee server log for query details.\n\nwhile updating tuple (0,2) in relation \"accounts\"\n\nNow the second transaction can continue. It wakes up and performs an update:\n\n260",
      "content_length": 1550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "13.6 Deadlocks\n\nUPDATE 1\n\nLet’s complete the transactions.\n\n=> ROLLBACK;\n\n=> ROLLBACK;\n\nTherightwaytoperformsuchoperationsistolockresourcesinthesameorder. For example, in this particular case the accounts could have been locked in ascending order based on their numbers.\n\nDeadlocks Between Two UPDATE Statements\n\nIn some cases deadlocks seem impossible, and yet they do occur.\n\nWe usually assume that ��� commands are atomic, but are they really? Let’s take a closer look at ������: this command locks rows as they are being updated rather than all at once,and it does not happen simultaneously. So if one ������ command modifies several rows in one order while the other is doing the same in a different order, a deadlock can occur.\n\nLet’s reproduce this scenario. First, we are going to build an index on the amount column, in descending order:\n\n=> CREATE INDEX ON accounts(amount DESC);\n\nTo be able to observe the process, we can write a function that slows things down:\n\n=> CREATE FUNCTION inc_slow(n numeric) RETURNS numeric AS $$\n\nSELECT pg_sleep(1); SELECT n + 100.00;\n\n$$ LANGUAGE sql;\n\nThe first ������ command is going to update all the tuples. The execution plan relies on a sequential scan of the whole table.\n\n261",
      "content_length": 1226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "p. ���\n\nChapter 13 Row-Level Locks\n\n=> EXPLAIN (costs off) UPDATE accounts SET amount = inc_slow(amount);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nUpdate on accounts\n\n−> Seq Scan on accounts\n\n(2 rows)\n\nTo make sure that the heap page stores the rows in ascending order based on the amount column, we have to truncate the table and insert the rows anew:\n\n=> TRUNCATE accounts;\n\n=> INSERT INTO accounts(id, client, amount) VALUES\n\n(1,'alice',100.00), (2,'bob',200.00), (3,'charlie',300.00);\n\n=> ANALYZE accounts;\n\n=> SELECT ctid, * FROM accounts;\n\nctid\n\n| id | client\n\n| amount\n\n−−−−−−−+−−−−+−−−−−−−−−+−−−−−−−−\n\n(0,1) | (0,2) | (0,3) | (3 rows)\n\n| 100.00 1 | alice 2 | bob | 200.00 3 | charlie | 300.00\n\nThe sequential scan will update the rows in the same order (it is not always true for large tables\n\nthough).\n\nLet’s start the update:\n\n=> UPDATE accounts SET amount = inc_slow(amount);\n\nMeanwhile, we are going to forbid sequential scans in another session:\n\n=> SET enable_seqscan = off;\n\nAs a result, the planner chooses an index scan for the next ������ command.\n\n=> EXPLAIN (costs off) UPDATE accounts SET amount = inc_slow(amount) WHERE amount > 100.00;\n\n262",
      "content_length": 1163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "13.6 Deadlocks\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nUpdate on accounts\n\n−> Index Scan using accounts_amount_idx on accounts\n\nIndex Cond: (amount > 100.00)\n\n(3 rows)\n\nThe second and third rows satisfy the condition; since the index is descending,the rows will get updated in the reverse order.\n\nLet’s start the next update:\n\n=> UPDATE accounts SET amount = inc_slow(amount) WHERE amount > 100.00;\n\nThe pgrowlocks extension shows that the first operator has already updated the first row (�,�), while the second one has managed to update the last row (�,�):\n\n=> SELECT locked_row, locker, modes FROM pgrowlocks('accounts');\n\nlocked_row | locker |\n\nmodes\n\n−−−−−−−−−−−−+−−−−−−−−+−−−−−−−−−−−−−−−−−−−\n\n(0,1) (0,3) (2 rows)\n\n| 122883 | {\"No Key Update\"} | 122884 | {\"No Key Update\"}\n\nfirst second\n\nAnother second passes. The first operator has updated the second row, and the other one would like to do it too, but it is not allowed.\n\n=> SELECT locked_row, locker, modes FROM pgrowlocks('accounts');\n\nlocked_row | locker |\n\nmodes\n\n−−−−−−−−−−−−+−−−−−−−−+−−−−−−−−−−−−−−−−−−−\n\n(0,1) (0,2) (0,3) (3 rows)\n\n| 122883 | {\"No Key Update\"} | 122883 | {\"No Key Update\"} | 122884 | {\"No Key Update\"}\n\nthe first one wins\n\nNow the first operator would like to update the last table row, but it is already locked by the second operator. A deadlock has occurred.\n\nOne of the transactions is aborted:\n\n263",
      "content_length": 1414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "Chapter 13 Row-Level Locks\n\nERROR: DETAIL: blocked by process 30723. Process 30723 waits for ShareLock on transaction 122884; blocked by process 30794. HINT: CONTEXT:\n\ndeadlock detected\n\nProcess 30794 waits for ShareLock on transaction 122883;\n\nSee server log for query details.\n\nwhile updating tuple (0,2) in relation \"accounts\"\n\nAnd the other completes its execution:\n\nUPDATE 3\n\nAlthough such situations seem impossible, they do occur in high-load systems when batch row updates are performed.\n\n264",
      "content_length": 500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "14\n\nMiscellaneous Locks\n\n14.1 Non-Object Locks\n\nTo lock a resource that is not considered a relation, Postgre��� uses heavyweight locks of the object type.1 You can lock almost anything that is stored in the sys- tem catalog: tablespaces, subscriptions, schemas, roles, policies, enumerated data types, and so on.\n\nLet’s start a transaction that creates a table:\n\n=> BEGIN;\n\n=> CREATE TABLE example(n integer);\n\nNow take a look at non-relation locks in the pg_locks table:\n\n=> SELECT database,\n\n(\n\nSELECT datname FROM pg_database WHERE oid = database\n\n) AS dbname, classid, (\n\nSELECT relname FROM pg_class WHERE oid = classid\n\n) AS classname, objid, mode, granted\n\nFROM pg_locks WHERE locktype = 'object'\n\nAND pid = pg_backend_pid() \\gx\n\n1 backend/storage/lmgr/lmgr.c, LockDatabaseObject & LockSharedObject functions\n\n265",
      "content_length": 821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "Chapter 14 Miscellaneous Locks\n\n−[ RECORD 1 ]−−−−−−−−−−−−−− | 16391 database | internals dbname | 2615 classid classname | pg_namespace objid mode granted\n\n| 2200 | AccessShareLock | t\n\nThe locked resource is defined here by three values:\n\ndatabase — the oid of the database that contains the object being locked (or zero\n\nif this object is common to the whole cluster)\n\nclassid — the oid listed in pg_class that corresponds to the name of the system\n\ncatalog table defining the type of the resource\n\nobjid — the oid listed in the system catalog table referenced by classid\n\nThe database value points to the internals database; it is the database to which the current session is connected. The classid column points to the pg_namespace table, which lists schemas.\n\nNow we can decipher the objid:\n\n=> SELECT nspname FROM pg_namespace WHERE oid = 2200;\n\nnspname −−−−−−−−− public (1 row)\n\nThus,Postgre��� has locked the public schema to make sure that no one can delete it while the transaction is still running.\n\nSimilarly, object deletion requires exclusive locks on both the object itself and all the resources it depends on.1\n\n=> ROLLBACK;\n\n1 backend/catalog/dependency.c, performDeletion function\n\n266",
      "content_length": 1203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "14.2 Relation Extension Locks\n\n14.2 Relation Extension Locks\n\nAs the number of tuples in a relation grows,Postgre��� inserts newtuples into free space in the already available pages whenever possible. But it is clear that at some point it will have to add new pages, that is, to extend the relation. In terms of the physical layout, new pages get added to the end of the corresponding file (which, in turn, can lead to creation of a new file).\n\nFor new pages to be added by only one process at a time,this operation is protected by a special heavyweight lock of the extend type.1 Such a lock is also used by index vacuuming to forbid adding new pages during an index scan.\n\nRelation extension locks behave a bit differently from what we have seen so far:\n\nThey are released as soon as the extension is created, without waiting for the\n\ntransaction to complete.\n\nThey cannot cause a deadlock,so they are not included into the wait-for graph.\n\nHowever,a deadlock check will still be performed if the procedure of extending a relation is taking longer than deadlock_timeout. It is not a typical situation, but it can happen if a large number of processes perform multiple insertions concurrently. In this case,the check can be called multiple times,virtually paralyzing normal system operation.\n\nTo minimize this risk,heap files are extended byseveral pages at once (in proportion to the number of processes awaiting the lock, but by not more than ��� pages per operation).2 An exception to this rule is �-tree index files, which are extended by one page at a time.3\n\n14.3 Page Locks\n\nApage-level heavyweight lock of the page type4 is applied only by ��� indexes,and only in the following case.\n\n1 backend/storage/lmgr/lmgr.c, LockRelationForExtension function 2 backend/access/heap/hio.c, RelationAddExtraBlocks function 3 backend/access/nbtree/nbtpage.c, _bt_getbuf function 4 backend/storage/lmgr/lmgr.c, LockPage function\n\n267\n\nv. �.�",
      "content_length": 1935,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "on\n\nChapter 14 Miscellaneous Locks\n\nG�� indexes can speed up search of elements in compound values,such as words in textdocuments. Theycanberoughlydescribedas�-treesthatstoreseparatewords rather than the whole documents. When a new document is added, the index has to be thoroughly updated to include each word that appears in this document.\n\nTo improve performance, ��� indexes allow deferred insertion, which is controlled fastupdate storage parameter. New words are first quickly added into an by the unordered pendinglist,and aftera while allthe accumulatedentries aremovedinto the main index structure. Since different documents are likely to contain duplicate words, this approach proves to be quite cost-effective.\n\nTo avoid concurrent transfer of words by several processes, the index metapage is locked in the exclusive mode until all the words are moved from the pending list to the main index. This lock does not interfere with regular index usage.\n\nJust like relation extension locks, page locks are released immediately when the taskiscomplete,withoutwaitingfortheendofthetransaction,sotheynevercause deadlocks.\n\n14.4 Advisory Locks\n\nUnlike other heavyweight locks (such as relation locks), advisory locks1 are never acquired automatically: they are controlled by the application developer. These locks are convenient to use if the application requires dedicated locking logic for some particular purpose.\n\nSupposeweneedtolockaresourcethatdoesnotcorrespondtoanydatabaseobject (which we could lock using ������ ��� or ���� ����� commands). In this case, the resource needs to be assigned a numeric ��. If the resource has a unique name,the easiest way to do it is to generate a hash code for this name:\n\n=> SELECT hashtext('resource1');\n\nhashtext −−−−−−−−−−− 991601810\n\n(1 row)\n\n1 postgresql.org/docs/14/explicit-locking#ADVISORY-LOCKS.html\n\n268",
      "content_length": 1857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "14.4 Advisory Locks\n\nPostgre��� providesawholeclassoffunctionsformanagingadvisorylocks.1 Their names begin with the pg_advisory prefix and can contain the following words that hint at the function purpose:\n\nlock — acquire a lock\n\ntry — acquire a lock if it can be done without waits\n\nunlock — release the lock\n\nshare — use a shared locking mode (by default, the exclusive mode is used)\n\nxact — acquire and hold a lock till the end of the transaction (by default, the lock\n\nis held till the end of the session)\n\nLet’s acquire an exclusive lock until the end of the session:\n\n=> BEGIN;\n\n=> SELECT pg_advisory_lock(hashtext('resource1'));\n\n=> SELECT locktype, objid, mode, granted FROM pg_locks WHERE locktype = 'advisory' AND pid = pg_backend_pid();\n\nlocktype |\n\nobjid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−\n\nadvisory | 991601810 | ExclusiveLock | t\n\n(1 row)\n\nFor advisory locks to actually work, other processes must also observe the estab- lished order when accessing the resource; it must be guaranteed by the application.\n\nThe acquired lock will be held even after the transaction is complete:\n\n=> COMMIT;\n\n=> SELECT locktype, objid, mode, granted FROM pg_locks WHERE locktype = 'advisory' AND pid = pg_backend_pid();\n\nlocktype |\n\nobjid\n\n|\n\nmode\n\n| granted\n\n−−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−\n\nadvisory | 991601810 | ExclusiveLock | t\n\n(1 row)\n\nOnce the operation on the resource is over, the lock has to be explicitly released:\n\n=> SELECT pg_advisory_unlock(hashtext('resource1'));\n\n1 postgresql.org/docs/14/functions-admin#FUNCTIONS-ADVISORY-LOCKS.html\n\n269",
      "content_length": 1606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "p. ��\n\np. ��\n\nChapter 14 Miscellaneous Locks\n\n14.5 Predicate Locks\n\nThe term predicate lock appeared as early as the first attempts to implement full isolation based on locks.1 The problem confronted at that time was that locking all the rows to be read and updated still could not guarantee full isolation. Indeed, if new rows that satisfy the filter condition get inserted into the table, they will become phantoms.\n\nFor this reason, it was suggested to lock conditions (predicates) rather than rows. If you run a query with the a > 10 predicate, locking this predicate will not allow adding new rows into the table if they satisfy this condition, so phantoms will be avoided. The trouble is that if a query with a different predicate appears, such as a < 20, you have to find out whether these predicates overlap. In theory, this problem is algorithmically unsolvable; in practice, it can be solved only for a very simple class of predicates (like in this example).\n\nIn Postgre���, the Serializable isolation level is implemented in a different way: it uses the Serializable Snapshot Isolation (���) protocol.2 The term predicate lock still remains, but its sense has radically changed. In fact, such “locks” do not lock anything: they are used to track data dependencies between different transactions.\n\nsnapshot isolation at the Repeatable Read level allows no anoma- It is proved that lies except for the write skew and the read-only transaction anomaly. These two anomalies result in certain patterns in the data dependence graph that can be dis- covered at a relatively low cost.\n\nThe problem is that we must differentiate between two types of dependencies:\n\nThefirsttransactionreadsarowthatislaterupdatedbythesecondtransaction\n\n(�� dependency).\n\nThefirsttransactionmodifiesarowthatislaterreadbythesecondtransaction\n\n(�� dependency).\n\n1 K. P. Eswaran, J. N. Gray, R. A. Lorie, I. L. Traiger. The notions of consistency and predicate locks in a\n\ndatabase system\n\n2 backend/storage/lmgr/README-SSI backend/storage/lmgr/predicate.c\n\n270",
      "content_length": 2041,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "14.5 Predicate Locks\n\nW� dependencies can be detected using regular locks, but �� dependencies have to be tracked via predicate locks. Such tracking is turned on automatically at the Serializable isolation level, and that’s exactly why it is important to use this level for all transactions (or at least all the interconnected ones). If any transaction is running at a different level, it will not set (or check) predicate locks, so the Serial- izable level will be downgraded to Repeatable Read.\n\nIwouldliketostressonceagainthatdespitetheirname,predicatelocksdonotlock anything. Instead, a transaction is checked for “dangerous” dependencies when it is about to be committed, and if Postgre��� suspects an anomaly, this transaction will be aborted.\n\nLet’s create a table with an index that will span several pages (it can be achieved by using a low fillfactor value):\n\n=> CREATE TABLE pred(n numeric, s text);\n\n=> INSERT INTO pred(n) SELECT n FROM generate_series(1,10000) n;\n\n=> CREATE INDEX ON pred(n) WITH (fillfactor = 10);\n\n=> ANALYZE pred;\n\nIf the query performs a sequential scan, a predicate lock is acquired on the whole table (even if some of the rows do not satisfy the provided filter conditions).\n\n=> SELECT pg_backend_pid();\n\npg_backend_pid −−−−−−−−−−−−−−−−\n\n34753\n\n(1 row)\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE;\n\n=> EXPLAIN (analyze, costs off, timing off, summary off)\n\nSELECT * FROM pred WHERE n > 100;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Seq Scan on pred (actual rows=9900 loops=1)\n\nFilter: (n > '100'::numeric) Rows Removed by Filter: 100\n\n(3 rows)\n\n271",
      "content_length": 1596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "Chapter 14 Miscellaneous Locks\n\nAlthough predicate locks have their own infrastructure, the pg_locks view displays them together with heavyweight locks. All predicate locks are always acquired in the SIRead mode, which stands for Serializable Isolation Read:\n\n=> SELECT relation::regclass, locktype, page, tuple FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 34753 ORDER BY 1, 2, 3, 4;\n\nrelation | locktype | page | tuple −−−−−−−−−−+−−−−−−−−−−+−−−−−−+−−−−−−−\n\npred (1 row)\n\n| relation |\n\n|\n\n=> ROLLBACK;\n\nNote that predicate locks may be held longer than the transaction duration, as they are used to track dependencies between transactions. But anyway, they are managed automatically.\n\nIf the query performs an index scan, the situation improves. For a �-tree index, it is enough to set a predicate lock on the read heap tuples and on the scanned leaf pages of the index. It will “lock” the whole range that has been read, not only the exact values.\n\n=> BEGIN ISOLATION LEVEL SERIALIZABLE;\n\n=> EXPLAIN (analyze, costs off, timing off, summary off)\n\nSELECT * FROM pred WHERE n BETWEEN 1000 AND 1001;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using pred_n_idx on pred (actual rows=2 loops=1)\n\nIndex Cond: ((n >= '1000'::numeric) AND (n <= '1001'::numeric))\n\n(2 rows)\n\n=> SELECT relation::regclass, locktype, page, tuple FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 34753 ORDER BY 1, 2, 3, 4;\n\nrelation\n\n| locktype | page | tuple\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−+−−−−−−−\n\npred pred pred_n_idx | page\n\n| tuple | tuple\n\n| | |\n\n4 | 4 | 28 |\n\n96 97\n\n(3 rows)\n\n272",
      "content_length": 1614,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "14.5 Predicate Locks\n\nThe number of leaf pages corresponding to the already scanned tuples can change: for example, an index page can be split when new rows get inserted into the table. However, Postgre��� takes it into account and locks newly appeared pages too:\n\n=> INSERT INTO pred\n\nSELECT 1000+(n/1000.0) FROM generate_series(1,999) n;\n\n=> SELECT relation::regclass, locktype, page, tuple FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 34753 ORDER BY 1, 2, 3, 4;\n\nrelation\n\n| locktype | page | tuple\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−+−−−−−−−\n\npred pred pred_n_idx | page pred_n_idx | page pred_n_idx | page pred_n_idx | page pred_n_idx | page\n\n| tuple | tuple\n\n| | | | | | |\n\n4 | 4 | 28 | 266 | 267 | 268 | 269 |\n\n96 97\n\n(7 rows)\n\nEach read tuple is locked separately, and there may be quite a few of such tuples. Predicate locks use their own pool allocated at the server start. The total number max_pred_locks_per_transactionvaluemultiplied ofpredicatelocksislimitedbythe max_connections (despite the parameter names, predicate locks are not being by counted per separate transactions).\n\nHere we get the same problem as with row-level locks,but it is solved in a different way: lock escalation is applied.1\n\nAs soon as the number of tuple locks related to one page exceeds the value of the max_pred_locks_per_page parameter, they are replaced by a single page-level lock.\n\n=> EXPLAIN (analyze, costs off, timing off, summary off)\n\nSELECT * FROM pred WHERE n BETWEEN 1000 AND 1002;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using pred_n_idx on pred (actual rows=3 loops=1)\n\nIndex Cond: ((n >= '1000'::numeric) AND (n <= '1002'::numeric))\n\n(2 rows)\n\n1 backend/storage/lmgr/predicate.c, PredicateLockAcquire function\n\n273\n\n64 100\n\nv. �� 2",
      "content_length": 1792,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "v. �� −2\n\n64\n\nv. ��\n\nChapter 14 Miscellaneous Locks\n\nInstead of three locks of the tuple type we now have one lock of the page type:\n\n=> SELECT relation::regclass, locktype, page, tuple FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 34753 ORDER BY 1, 2, 3, 4;\n\nrelation\n\n| locktype | page | tuple\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−+−−−−−−−\n\npred | page pred_n_idx | page pred_n_idx | page pred_n_idx | page pred_n_idx | page pred_n_idx | page\n\n| | | | | |\n\n4 | 28 | 266 | 267 | 268 | 269 |\n\n(6 rows)\n\n=> ROLLBACK;\n\n. If the number of such Escalation of page-level locks follows the same principle max_pred_locks_per_relation value, they locks for a particular relation exceeds the get replaced by a single relation-level lock. (If this parameter is set to a negative max_pred_locks_per_transaction divided by the value, the threshold is calculated as absolute value of max_pred_locks_per_relation; thus, the default threshold is ��).\n\nLock escalation is sure to lead to multiple false-positive serialization errors,which negatively affects system throughput. So you have to find an appropriate balance between performance and spending the available ��� on locks.\n\nPredicate locks support the following index types:\n\n�-trees\n\nhash indexes, �i��, and ���\n\nIf an index scan is performed, but the index does not support predicate locks, the wholeindexwillbelocked. Itisonlytobeexpectedthatthenumberoftransactions aborted for no good reason will also increase in this case.\n\nFor more efficient operation at the Serializable level, it makes sense to explicitly declare read-only transactions as such using the ���� ���� clause. If the lock man- ager sees that a read-only transaction will not conflict with other transactions,1 it\n\n1 backend/storage/lmgr/predicate.c, SxactIsROSafe macro\n\n274",
      "content_length": 1786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "14.5 Predicate Locks\n\ncan release the already set predicate locks and refrain from acquiring new ones. And if such a transaction is also declared ����������, the read-only transaction p. �� anomaly will be avoided too.\n\n275",
      "content_length": 223,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "15\n\nLocks on Memory Structures\n\n15.1 Spinlocks\n\nTo protect data structures in shared memory, Postgre��� uses several types of lighter and less expensive locks rather than regular heavyweight ones.\n\nspinlocks. They are usually acquired for a very short time The simplest locks are interval(nolongerthanseveral���cycles)toprotectparticularmemorycellsfrom concurrent updates.\n\nSpinlocksarebasedonatomic ���instructions,suchascompare-and-swap.1 They only support the exclusive locking mode. If the required resource is already locked, the process busy-waits, repeating the command (it “spins” in the loop, hence the name). Ifthelockcannotbeacquiredwithinthespecifiedtimeinterval,theprocess pauses for a while and then starts another loop.\n\nThis strategy makes sense if the probability of a conflict is estimated as very low, so after an unsuccessful attempt the lock is likely to be acquired within several instructions.\n\nSpinlockshaveneitherdeadlockdetectionnorinstrumentation. Fromthepractical standpoint,we should simply know about their existence; the whole responsibility for their correct implementation lies with Postgre��� developers.\n\n1 backend/storage/lmgr/s_lock.c\n\n276",
      "content_length": 1176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "15.2 Lightweight Locks\n\n15.2 Lightweight Locks\n\nlightweight locks, or lwlocks.1 Acquired for the time Next, there are so-called needed to process a data structure (for example, a hash table or a list of pointers), lightweight locks are typically short; however, they can take longer when used to protect �/� operations.\n\nLightweight locks support two modes: exclusive (for data modification) and shared (for read-only operations). There is no queue as such: if several processes are wait- ing on a lock, one of them will get access to the resource in a more or less random fashion. In high-load systems with multiple concurrent processes, it can lead to some unpleasant effects.\n\nDeadlock checks are not provided; we have to trust Postgre��� developers that lightweight locks are implemented correctly. However, these locks do have instru- mentation, so, unlike spinlocks, they can be observed.\n\n15.3 Examples\n\nTo get some idea of how and where spinlocks and lightweight locks can be used, let’s take a look at two shared memory structures: buffer cache and ��� buffers. I will name only some of the locks; the full picture is too complex and is likely to interest only Postgre��� core developers.\n\nBuffer Cache\n\nTo access a hash table must acquire a reading or in the exclusive mode if any modifications are expected.\n\nused to locate a particular buffer in the cache, the process BufferMapping lightweight lock either in the shared mode for\n\n1 backend/storage/lmgr/lwlock.c\n\n277\n\np. ���",
      "content_length": 1487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "p. ���\n\nChapter 15 Locks on Memory Structures\n\nBufferMapping ×128\n\nbuffer strategy\n\nfree buffers\n\nclock hand\n\nhash table\n\nbuffer pin\n\nbuffer header\n\nBufferIO\n\nBufferContent\n\nThe hash table is accessed very frequently,so this lock often becomes a bottleneck. To maximize granularity, it is structured as a tranche of ��� individual lightweight locks, each protecting a separate part of the hash table.1\n\nA hash table lock was converted into a tranche of �� locks as early as ����,in Postgre��� �.�; ten years later, when version �.� was released, the size of the tranche was increased to ���, but it may still be not enough for modern multi-core systems.\n\nbuffer header spinlock2 To get access to the buffer header,the process acquires a (the name is arbitrary, as spinlocks have no user-visible names). Some operations, such as incrementing the usage counter, do not require explicit locks and can be performed using atomic ��� instructions.\n\nBufferContent lock in the header To read a page in a buffer,the process acquires a of this buffer.3 It is usually held only while tuple pointers are being read; later on, will be enough. If the buffer content the protection provided by has to be modified, the BufferContent lock must be acquired in the exclusive mode.\n\nbuffer pinning\n\n1 backend/storage/buffer/bufmgr.c\n\ninclude/storage/buf_internals.h, BufMappingPartitionLock function\n\n2 backend/storage/buffer/bufmgr.c, LockBufHdr function 3 include/storage/buf_internals.h\n\n278",
      "content_length": 1474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "15.3 Examples\n\nWhen a buffer is read from disk (or written to disk), Postgre��� also acquires a BufferIO lock in the buffer header; it is virtually an attribute used as a lock rather than an actual lock.1 It signals other processes requesting access to this page that they have to wait until the �/� operation is complete.\n\nThe pointer to free buffers and the clock hand of the eviction mechanism are pro- tected by a single common\n\nbuffer strategy spinlock.2\n\nWAL Buffers\n\nWALBufMapping\n\nWALWrite\n\nhash table\n\ninsert position\n\nWALInsert ×8\n\nPrevBytePos\n\nCurBytePos\n\nW�� cache also uses a hash table to map pages to buffers. Unlike the buffer cache hash table, it is protected by a single WALBufMapping lightweight lock because ��� cache is smaller (it usually takes 1 of the buffer cache size) and buffer access 32 is more ordered.3\n\nWriting of ��� pages to disk is protected by a WALWrite lightweight lock, which ensures that this operation is performed by one process at a time.\n\nTo create a ��� entry, the process first reserves some space within the ��� page and then fills it with data. Space reservation is strictly ordered; the process must insert position spinlock that protects the insertion pointer.4 But acquire an\n\n1 backend/storage/buffer/bufmgr.c, StartBufferIO function 2 backend/storage/buffer/freelist.c 3 backend/access/transam/xlog.c,AdvanceXLInsertBuffer function 4 backend/access/transam/xlog.c, ReserveXLogInsertLocation function\n\n279",
      "content_length": 1457,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "off\n\n1s p. ���\n\nv. �.�\n\nChapter 15 Locks on Memory Structures\n\nonce the space is reserved,it can be filled by several concurrent processes. For this purpose,each process must acquire any of the eight lightweight locks constituting the WALInsert tranche.1\n\n15.4 Monitoring Waits\n\nWithout doubt, locks are indispensable for correct Postgre��� operation, but they can lead to undesirable waits. It is useful to track such waits to understand their origin.\n\nlog_lock_waits The easiest way to get an overview of long-term locks is to turn the parameter on; it enables extensive logging of all the locks that cause a transaction deadlock_timeout. This data is displayed when a deadlock to wait for more than check\n\ncompletes, hence the parameter name.\n\nHowever, the pg_stat_activity view provides much more useful and complete in- formation. Whenever a process—either a system process or a backend—cannot proceed with its task because it is waiting for something, this wait is reflected in the wait_event_type and wait_event fields, which show the type and name of the wait, respectively.\n\nAll waits can be classified as follows.2\n\nWaits on various locks constitute quite a large group:\n\nLock — heavyweight locks\n\nLWLock — lightweight locks\n\nBufferPin — pinned buffers\n\nBut processes can be waiting for other events too:\n\nIO — input/output, when it is required to read or write some data\n\n1 backend/access/transam/xlog.c, WALInsertLockAcquire function 2 postgresql.org/docs/14/monitoring-stats#WAIT-EVENT-TABLE.html\n\n280",
      "content_length": 1514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "15.4 Monitoring Waits\n\nClient — data sent by the client (psql spends in this state most of the time)\n\nIPC — data sent by another process\n\nExtension — a specific event registered by an extension\n\nSometimes a process simply does not perform any useful work. Such waits are usu- ally “normal,” meaning that they do not indicate any issues. This group comprises the following waits:\n\nActivity — background processes in their main cycle\n\nTimeout — timer\n\nLocks of each wait type are further classified by wait names. For example,waits on lightweight locks get the name of the lock or the corresponding tranche.1\n\nYou should bear in mind that the pg_stat_activity view displays only those waits that are handled in the source code in an appropriate way.2 Unless the name of the wait appears in this view, the process is not in the state of wait of any known type. Such time should be considered unaccounted for; it does not necessarily mean that the process is not waiting on anything—we simply do not know what is happening at the moment.\n\n=> SELECT backend_type, wait_event_type AS event_type, wait_event FROM pg_stat_activity;\n\nbackend_type\n\n| event_type |\n\nwait_event\n\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−\n\nlogical replication launcher | Activity | Activity autovacuum launcher | client backend | Activity background writer | Activity checkpointer | Activity walwriter\n\n| LogicalLauncherMain | AutoVacuumMain | | BgWriterMain | CheckpointerMain | WalWriterMain\n\n(6 rows)\n\nHere all the background processes were idle when the view was sampled,while the client backend was busy executing the query and was not waiting on anything.\n\n1 postgresql.org/docs/14/monitoring-stats#WAIT-EVENT-LWLOCK-TABLE.html 2 include/utils/wait_event.h\n\n281",
      "content_length": 1761,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "Chapter 15 Locks on Memory Structures\n\n15.5 Sampling\n\nUnfortunately, the pg_stat_activity view shows only the current information on waits; statistics are not accumulated. The only way to collect wait data over time is to sample the view at regular intervals.\n\nWe have to take into account the stochastic nature of sampling. The shorter the wait as compared to the sampling interval,the lower the chance to detect this wait. Thus, longer sampling intervals require more samples to reflect the actual state of things (but as you increase the sampling rate, the overhead also rises). For the same reason, sampling is virtually useless for analyzing short-lived sessions.\n\nPostgre��� provides no built-in tools for sampling; however, we can still try it out using the pg_wait_sampling1 extension. To do so, we have to specify its library in the shared_preload_libraries parameter and restart the server:\n\n=> ALTER SYSTEM SET shared_preload_libraries = 'pg_wait_sampling';\n\npostgres$ pg_ctl restart -l /home/postgres/logfile\n\nNow let’s install the extension into the database:\n\n=> CREATE EXTENSION pg_wait_sampling;\n\nThis extension can display the history of waits, which is saved in its ring buffer. However, it is much more interesting to get the waiting profile—the accumulated statistics for the whole duration of the session.\n\nFor example, let’s take a look at the waits during benchmarking. We have to start the pgbench utility and determine its process �� while it is running:\n\npostgres$ /usr/local/pgsql/bin/pgbench -T 60 internals\n\n=> SELECT pid FROM pg_stat_activity WHERE application_name = 'pgbench';\n\npid −−−−−−− 36367 (1 row)\n\nOnce the test is complete, the waits profile will look as follows:\n\n1 github.com/postgrespro/pg_wait_sampling\n\n282",
      "content_length": 1751,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "15.5 Sampling\n\n=> SELECT pid, event_type, event, count FROM pg_wait_sampling_profile WHERE pid = 36367 ORDER BY count DESC LIMIT 4;\n\npid\n\n| event_type |\n\nevent\n\n| count\n\n−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−+−−−−−−−\n\n36367 | IO 36367 | IO 36367 | Client 36367 | IO\n\n| | WALSync | | WALWrite | | ClientRead | DataFileRead |\n\n3478 52 30 2\n\n(4 rows)\n\npg_wait_sampling.profile_period parameter)samplesaretaken Bydefault(setbythe ��� times per second. So to estimate the duration of waits in seconds, you have to divide the count value by ���.\n\nmost of the waits are related to flushing ��� entries to disk. In this particular case, It is a good illustration of the unaccounted-for wait time: the WALSync event was not instrumented until Postgre��� ��; for lower versions,a waits profile would not contain the first row, although the wait itself would still be there.\n\nAnd here is how the profile will look like if we artificially slow down the file system for each �/� operation to take �.� seconds (I use slowfs1 for this purpose) :\n\npostgres$ /usr/local/pgsql/bin/pgbench -T 60 internals\n\n=> SELECT pid FROM pg_stat_activity WHERE application_name = 'pgbench';\n\npid −−−−−−− 36747 (1 row)\n\n=> SELECT pid, event_type, event, count FROM pg_wait_sampling_profile WHERE pid = 36747 ORDER BY count DESC LIMIT 4;\n\npid\n\n| event_type |\n\nevent\n\n| count\n\n−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−\n\n36747 | IO 36747 | LWLock 36747 | IO 36747 | IO\n\n| | WALWrite | | WALWrite | WALSync | | DataFileExtend |\n\n3603 2095 22 19\n\n(4 rows)\n\n1 github.com/nirs/slowfs\n\n283\n\n10ms\n\nv. ��",
      "content_length": 1563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "Chapter 15 Locks on Memory Structures\n\nNow �/� operations are the slowest ones—mainly those that are related to writing ��� files to disk in the synchronous mode. Since ��� writing is protected by a WALWrite lightweight lock, the corresponding row also appears in the profile.\n\nClearly,the same lock is acquired in the previous example too,but since the wait is shorter than the sampling interval, it either is sampled very few times or does not make it into the profile at all. It illustrates once again that to analyze short waits you have to sample them for quite a long time.\n\n284",
      "content_length": 584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "Part IV\n\nQuery Execution",
      "content_length": 24,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "16\n\nQuery Execution Stages\n\n16.1 Demo Database\n\nThe examples in the previous parts of the book were based on simple tables with onlyahandfulofrows. Thisandsubsequentpartsdealwithqueryexecution,which is more demanding in this respect: we need related tables that have a much larger number of rows. Instead of inventing a new data set for each example, I took an existing demo database that illustrates passenger air traffic in Russia.1 It has sev- eral versions; we will use the bigger one created on August 15,2017. To install this version,you have to extract the file containing the database copy from the archive2 and run this file in psql.\n\nWhen developing this demo database, we tried to make its schema simple enough to be understood without extra explanations; at the same time, we wanted it to be complex enough to allow writing meaningful queries. The database is filled with true-to-life data, which makes the examples more comprehensive and should be interesting to work with.\n\nHere I will cover the main database objects only briefly; if you would like to re- view the whole schema, you can take a look at its full description referenced in the footnote.\n\nThe main entity is a booking (mapped to the bookings table). One booking can include several passengers, each with a separate electronic ticket (tickets). A pas- senger does not constitute a separate entity; for the purpose of our experiments, we will assume that all passengers are unique.\n\n1 postgrespro.com/community/demodb 2 edu.postgrespro.com/demo-big-en-20170815.zip\n\n287",
      "content_length": 1545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "en\n\nChapter 16 Query Execution Stages\n\nEach ticket includes one or more flight segments (mapped to the ticket_flights ta- ble). A single ticket can have several flight segments in two cases: either it is a round-trip ticket, or it is issued for connecting flights. Although there is no cor- responding constraint in the schema, all tickets in a booking are assumed to have the same flight segments.\n\nEach flight (flights) goes from one airport (airports) to another. Flights with the sameflightnumberhavethesamepointsofdepartureanddestinationbutdifferent departure dates.\n\nThe routes view is based on the flights table; it displays the information on routes that does not depend on particular flight dates.\n\nAtcheck-in,eachpassengerisissuedaboarding pass(boarding_passes)withaseat number. A passenger can check in for a flight only if this flight is included into the ticket. Flight-seat combinations must be unique, so it is impossible to issue two boarding passes for the same seat.\n\nThe number of seats (seats) in an aircraft and their distribution between different travel classes depend on the particular model of the aircraft (aircrafts) that per- forms the flight. It is assumed that each aircraft model can have only one cabin configuration.\n\nSome tables have surrogate primary keys, while others use natural ones (some of them being composite). It is done solely for demonstration purposes and is by no means an example to follow.\n\nThe demo database can be thought of as a dump of a real system: it contains a snapshot of data taken at a particular time in the past. To display this time, you can call the bookings.now() function. Use this function in demo queries that would demand the now() function in real life.\n\nThenamesofairports,cities,andaircraftmodelsarestoredintheairports_dataand aircrafts_data tables; they are provided in two languages, English and Russian. To construct examples for this chapter, I will typically query the airports and aircrafts views shown in the entity-relationship diagram; these views choose the output bookings.lang parameter value. The names of some base language based on the tables can still appear in query plans though.\n\n288",
      "content_length": 2175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "16.1 Demo Database\n\nBookings#book_ref∗book_date∗total_amount\n\nAirports#airport_code∗airport_name∗city∗coordinates∗timezone\n\nTickets#ticket_no∗book_ref∗passenger_id∗passenger_name∗contact_data\n\nTicket_flights#ticket_no#flight_id∗fare_conditions∗amount\n\nFlights#flight_id∗flight_no∗scheduled_departure∗scheduled_arrival∗departure_airport∗arrival_airport∗status∗aircraft_code∘actual_departure∘actual_arrival\n\nAircrafts#aircraft_code∗model∗range\n\nBoarding_passes#ticket_no#flight_id∗boarding_no∗seat_no\n\nSeats#aircraft_code#seat_no∗fare_conditions\n\n1\n\n289",
      "content_length": 551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "Chapter 16 Query Execution Stages\n\n16.2 Simple Query Protocol\n\nA simple version of the client-server protocol1 enables ��� query execution: it sends the text of a query to the server and gets the full execution result in response, no matter how many rows it contains.2 A query sent to the server passes several stages: it is parsed, transformed, planned, and then executed.\n\nParsing\n\nFirst of all,Postgre��� has to parse3 the query text to understand what needs to be executed.\n\nLexical and syntactic analyisis. The lexer splits the query text into a set of lexemes4 (such as keywords, string literals, and numeric literals), while the parser validates this set against the ��� language grammar.5 Postgre��� relies on standard parsing tools, namely Flex and Bison utilities.\n\nThe parsed query is reflected in the backend’s memory as an abstract syntax tree.\n\nFor example, let’s take a look at the following query:\n\nSELECT schemaname, tablename FROM pg_tables WHERE tableowner = 'postgres' ORDER BY tablename;\n\nThelexersinglesoutfivekeywords,fiveidentifiers,astringliteral,andthreesingle- letter lexemes (a comma, an equals sign, and a semicolon). The parser uses these lexemes to build the parse tree, which is shown in the illustration below in a very simplified form. The captions next to the tree nodes specify the corresponding parts of the query:\n\n1 postgresql.org/docs/14/protocol.html 2 backend/tcop/postgres.c, exec_simple_query function 3 postgresql.org/docs/14/parser-stage.html\n\nbackend/parser/README\n\n4 backend/parser/scan.l 5 backend/parser/gram.y\n\n290",
      "content_length": 1565,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "16.2 Simple Query Protocol\n\nSELECT\n\nQUERY\n\nTARGETENTRY\n\nFROMEXPR\n\nSORTGROUPCLAUSE\n\nschemaname, tablename\n\nFROM\n\nORDER BY tablename\n\nRTE\n\nOPEXPR\n\npg_tables\n\ntableowner ='postgres'\n\npg_table\n\nWHERE tableowner = 'postgres'\n\nA rather obscure ��� abbreviation stands for Range Table Entry. Postgre��� source code uses the term range table to refer to tables, subqueries, join results—in other words, to any sets of rows that can be processed by ��� operators.1\n\nSemantic analysis. The purpose of semantic analysis2 is to determine whether the database contains any tables or other objects that this query refers to by name, and whether the user has permission to access these objects. All the information required for semantic analysis is stored\n\nin the system catalog.\n\nHaving received the parse tree, the semantic analyzer performs its further restruc- turing, which includes adding references to specific database objects, data types, and other information.\n\nIf you enable the debug_print_parse parameter, you can view the full parse tree in the server log, but it has little practical sense.\n\nTransformation\n\nAt the next stage, the query can be transformed (rewritten).3\n\n1 include/nodes/parsenodes.h 2 backend/parser/analyze.c 3 postgresql.org/docs/14/rule-system.html\n\n291\n\np. ��",
      "content_length": 1280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "v. ��\n\nChapter 16 Query Execution Stages\n\nPostgre��� core uses transformations for several purposes. One of them is to re- place the name of the view in the parse tree with the subtree corresponding to the base query of this view.\n\nAnother case of using transformations is row-level security implementation.1\n\nThe ������ and ����� clauses of recursive queries stage.2\n\nalso get transformed during this\n\nIn the example above, pg_tables is a view; if we placed its definition into the query text, it would look as follows:\n\nSELECT schemaname, tablename FROM (\n\n-- pg_tables SELECT n.nspname AS schemaname,\n\nc.relname AS tablename, pg_get_userbyid(c.relowner) AS tableowner, ...\n\nFROM pg_class c\n\nLEFT JOIN pg_namespace n ON n.oid = c.relnamespace LEFT JOIN pg_tablespace t ON t.oid = c.reltablespace\n\nWHERE c.relkind = ANY (ARRAY['r'::char, 'p'::char])\n\n) WHERE tableowner = 'postgres' ORDER BY tablename;\n\nHowever, the server does not process the text representation of the query; all ma- nipulations are performed on the parse tree. The illustration shows a reduced version of the transformed tree (you can view its full version in the server log if you enable the debug_print_rewritten parameter).\n\nThe parse tree reflects the syntactic structure of the query, but it says nothing about the order in which the operations should be performed.\n\nPostgre��� also supports custom transformations, which the user can implement via the rewrite rule system.3\n\n1 backend/rewrite/rowsecurity.c 2 backend/rewrite/rewriteSearchCycle.c 3 postgresql.org/docs/14/rules.html\n\n292",
      "content_length": 1564,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "16.2 Simple Query Protocol\n\nQUERY\n\nTARGETENTRY\n\nFROMEXPR\n\nSORTGROUPCLAUSE\n\nRTE\n\nOPEXPR\n\npg_tables\n\ntableowner ='postgres'\n\nQUERY\n\nTARGETENTRY\n\nFROMEXPR\n\nJOINEXPR\n\nOPEXPR\n\nc.relkind = ANY (ARRAY[...])\n\nJOINEXPR\n\nRTE\n\npg_tablespace\n\nOPEXPR\n\nt.oid = c.reltablespace\n\nRTE\n\nRTE\n\nOPEXPR\n\npg_class\n\npg_namespace\n\nn.oid = c.relnamespace\n\nThe rule system support was proclaimed as one of the main objectives of Postgres devel- opment;1 it was still an academic project when the rules were first implemented,but since then they have been redesigned multiple times. The rule system is a very powerful mech- anism,but it is rather hard to comprehend and debug. It was even proposed to remove the rules from Postgre��� altogether, but the idea did not find unanimous support. In most cases, it is safer and easier to use triggers instead of rules.\n\n1 M. Stonebraker, L.A. Rowe. The Design of Postgres\n\n293",
      "content_length": 892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "Chapter 16 Query Execution Stages\n\nPlanning\n\nS�� is a declarative language: queries specify what data to fetch, but not how to fetch it.\n\nAny query has several execution paths. Each operation shown in the parse tree can be completed in a number of ways: for example, the result can be retrieved by reading the whole table (and filtering out redundancies),or by finding the required rows via an index scan. Data sets are always joined in pairs, so there is a huge number of options that differ in the order of joins. Besides, there are various join algorithms: for example, the executor can scan the rows of the first data set and search for the matching rows in the other set, or both data sets can be first sorted and then merged together. For each algorithm, we can find a use case where it performs better than others.\n\nThe execution times of optimal and non-optimal plans can differ by orders of mag- nitude, so the planner1 that optimizes the parsed query is one of the most complex components of the system.\n\nPlan tree. The execution plan is also represented as a tree,but its nodes deal with physical operations on data rather than logical ones.\n\nIf you would like to explore full plan trees, you can dump them into the server log by enabling the debug_print_plan parameter. But in practice it is usually enough to view the text representation of the plan displayed by the ������� command.2\n\nThe following illustration highlights the main nodes of the tree. It is exactly these nodes that are shown in the output of the ������� command provided below.\n\nFor now, let’s pay attention to the following two points:\n\nThe tree contains only two queried tables out of three: the planner saw that one of the tables is not required for retrieving the result and removed it from the plan tree.\n\nFor each node of the tree, the planner provides the estimated cost and the\n\nnumber of rows expected to be processed.\n\n1 postgresql.org/docs/14/planner-optimizer.html 2 postgresql.org/docs/14/using-explain.html\n\n294",
      "content_length": 2006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "16.2 Simple Query Protocol\n\n=> EXPLAIN SELECT schemaname, tablename FROM pg_tables WHERE tableowner = 'postgres' ORDER BY tablename;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\n(cost=21.03..21.04 rows=1 width=128)\n\nSort Key: c.relname −> Nested Loop Left Join\n\n(cost=0.00..21.02 rows=1 width=128)\n\nJoin Filter: (n.oid = c.relnamespace) −> Seq Scan on pg_class c\n\n(cost=0.00..19.93 rows=1 width=72) Filter: ((relkind = ANY ('{r,p}'::\"char\"[])) AND (pg_g... (cost=0.00..1.04 rows=4 wid...\n\n−> Seq Scan on pg_namespace n\n\n(7 rows)\n\nSeq Scan Nested Loop\n\nnodes shown in the query plan correspond to reading the table,while the\n\nnode represents the join operation.\n\nPLANNEDSTMT\n\nSORT\n\nTARGETENTRY\n\nNESTLOOP\n\nTARGETENTRY\n\nSEQSCAN\n\npg_class\n\nSEQSCAN\n\npg_namespace\n\nOPEXPR\n\nn.oid = c.relnamespace\n\nOPEXPR\n\nrelkind =ANY ('r,p'::\"char\"[])AND pg_get_userbyid(relowner) ='postgres'::name\n\nPlan search. Postgre��� uses a cost-based optimizer;1 it goes over potential plans and estimates the resources required for their execution (such as �/� operations or ��� cycles). Normalized to a numeric value,this estimation is called the cost of the plan. Of all the considered plans, the one with the lowest cost is selected.\n\n1 backend/optimizer/README\n\n295\n\np. ��� p. ���",
      "content_length": 1298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "v. ��\n\nChapter 16 Query Execution Stages\n\nThe problem is that the number of potentially available plans grows exponentially with the number of joined tables, so it is impossible to consider them all—even for relativelysimplequeries. Thesearchistypicallynarroweddownusingthedynamic programming algorithm combined with some heuristics. It allows the planner to find a mathematically accurate solution for queries with a larger number of tables within acceptable time.\n\nAn accurate solution does not guarantee that the selected plan is really the optimal one, as the planner uses simplified mathematical models and may lack reliable input data.\n\nManaging the order of joins. A query can be structured in a way that limits the search scope to some extent (at a risk of missing the optimal plan).\n\nCommon table expressions\n\nand the main query can be optimized separately;\n\nto guarantee such behavior, you can specify the ������������ clause.1\n\nSubqueries run within non-��� functions are always optimized separately. (S�� functions can sometimes be inlined into the main query.2)\n\nIf you set the join_collapse_limit parameter and use explicit ���� clauses in the query, the order of some joins will be defined by the query syntax structure; the from_collapse_limit parameter has the same effect on subqueries.3\n\nThe latter point may have to be explained. Let’s take a look at the query that does not specify any explicit joins for tables listed in the ���� clause:\n\nSELECT ... FROM a, b, c, d, e WHERE ...\n\nHere the planner will have to consider all the possible pairs of joins. The query is represented by the following part of the parse tree (shown schematically):\n\n1 postgresql.org/docs/14/queries-with.html 2 wiki.postgresql.org/wiki/Inlining_of_SQL_functions 3 postgresql.org/docs/14/explicit-joins.html\n\n296",
      "content_length": 1808,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "16.2 Simple Query Protocol\n\nFROMEXPR\n\nA\n\nB\n\nC\n\nD\n\nE\n\nIn the next example, joins have a certain structure defined by the ���� clause:\n\nSELECT ... FROM a, b JOIN c ON ..., d, e WHERE ...\n\nThe parse tree reflects this structure:\n\nFROMEXPR\n\nA\n\nJOINEXPR\n\nD\n\nE\n\nB\n\nC\n\nThe planner typically flattens the join tree, so that it looks like the one in the first example. The algorithm recursively traverses the tree and replaces each �������� node with a flat list of its elements.1\n\nHowever, such collapsing is performed only if the resulting flat list has no more join_collapse_limit elements. In this particular case, the �������� node would than not be collapsed if the join_collapse_limit value were less than five.\n\nFor the planner, it means the following:\n\nTable � must be joined with table � (or vice versa, � must be joined with �; the\n\norder of joins within a pair is not restricted).\n\nTables �, �, � and the result of joining � and � can be joined in any order.\n\n1 backend/optimizer/plan/initsplan.c, deconstruct_jointree function\n\n297\n\n8",
      "content_length": 1038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "8\n\non 12\n\nChapter 16 Query Execution Stages\n\nIf the join_collapse_limit parameter is set to one, the order defined by explicit ���� clauses is preserved.\n\nAs for ��������� ���� operands,they are never collapsed,regardless of the value of the join_collapse_limit parameter.\n\nfrom_collapse_limit parameter controls subquery flattening in a similar way. The Although subqueries do not look like ���� clauses,the similarity becomes apparent at the parse tree level.\n\nHere is a sample query:\n\nSELECT ... FROM a,\n\n(\n\nSELECT ... FROM b, c WHERE ...\n\n) bc, d, e WHERE ...\n\nThe corresponding join tree is shown below. The only difference here is that this tree contains the �������� node instead of �������� (hence the parameter name).\n\nFROMEXPR\n\nA\n\nFROMEXPR\n\nD\n\nE\n\nB\n\nC\n\nGenetic query optimization. Once flattened, the tree may contain too many ele- ments at one level—either tables or join results, which have to be optimized sepa- rately. Planning time depends exponentially on the number of data sets that have to be joined, so it can grow beyond all reasonable limits.\n\nIf the the\n\ngeqo parameter is enabled and the number of elements at one level exceeds geqo_threshold value, the planner will use the genetic algorithm to optimize the\n\n298",
      "content_length": 1237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "16.2 Simple Query Protocol\n\nquery.1 This algorithm is much faster than its dynamic programming counterpart, but it cannot guarantee that the found plan will be optimal. So the rule of thumb is to avoid using the genetic algorithm by reducing the number of elements that have to be optimized.\n\nThe genetic algorithm has several configurable parameters,2 but I am not going to cover them here.\n\nChoosing the best plan. Whether the plan can be considered optimal or not de- pends on how a particular client is going to use the query result. If the client needs the full result at once (for example, to create a report), the plan should optimize retrieval of all the rows. But if the priority is to return the first rows as soon as possible (for example, to display them on screen), the optimal plan might be com- pletely different.\n\nTo make this choice, Postgre��� calculates two components of the cost:\n\n=> EXPLAIN SELECT schemaname, tablename FROM pg_tables WHERE tableowner = 'postgres' ORDER BY tablename;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\n(cost=21.03..21.04 rows=1 width=128)\n\nSort Key: c.relname −> Nested Loop Left Join\n\n(cost=0.00..21.02 rows=1 width=128)\n\nJoin Filter: (n.oid = c.relnamespace) −> Seq Scan on pg_class c\n\n(cost=0.00..19.93 rows=1 width=72) Filter: ((relkind = ANY ('{r,p}'::\"char\"[])) AND (pg_g... (cost=0.00..1.04 rows=4 wid...\n\n−> Seq Scan on pg_namespace n\n\n(7 rows)\n\nThe first component (the startup cost) represents the price you pay to prepare for node execution, while the second component (the total cost) comprises all the ex- penses incurred by fetching the result.\n\n1 postgresql.org/docs/14/geqo.html\n\nbackend/optimizer/geqo/geqo_main.c\n\n2 postgresql.org/docs/14/runtime-config-query#RUNTIME-CONFIG-QUERY-GEQO.html\n\n299",
      "content_length": 1807,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "0.1\n\np. ���\n\nChapter 16 Query Execution Stages\n\nIt is sometimes stated that the startup cost is the cost of retrieving the first row of the result set, but it is not quite accurate.\n\nTo single out the preferred plans, the optimizer checks whether the query uses a cursor (either via the ������� command provided in ��� or declared explicitly in ��/pg���).1 If not, the client is assumed to need the whole result at once, and the optimizer chooses the plan with the least total cost.\n\nIf the query is executed with a cursor, the selected plan must optimize retrieval cursor_tuple_fraction of all rows. To be more exact, Postgre��� chooses the of only plan with the smallest value of the following expression:2\n\nstartup cost + cursor_tuple_fraction(total cost − startup cost)\n\nAn outline of cost estimation. To estimate the total cost of a plan, we have to get cost estimations for all its nodes. The cost of a node depends on its type (it is ob- vious that the cost of reading heap data is not the same as the sorting cost) and on the amount of data processed by this node (larger data volumes typically in- cur higher costs). While node types are known, the amount of data can only be projected based on the estimated cardinality of input sets (the number of rows the node takes as input) and the selectivity of the node (the fraction of rows remaining attheoutput). Thesecalculationsrelyonthecollectedstatistics ,suchastablesizes and data distribution in table columns.\n\nThus, the performed optimization depends on correctness of statistical data that is gathered and updated by autovacuum.\n\nIf cardinality estimation is accurate for each node, the calculated cost is likely to adequately reflect the actual cost. The main planning flaws usually result from incorrect estimation of cardinality and selectivity, which can be caused by inaccu- rate or outdated statistics, inability to use it, or—to a lesser extent—by imperfect planning models.\n\nCardinality estimation. To calculate the cardinality of a node, the planner has to recursively complete the following steps:\n\n1 backend/optimizer/plan/planner.c, standard_planner function 2 backend/optimizer/util/pathnode.c, compare_fractional_path_costs function\n\n300",
      "content_length": 2214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "16.2 Simple Query Protocol\n\n� Estimate the cardinality of each child node and assess the number of input\n\nrows that the node will receive from them.\n\n� Estimate the selectivity of the node,that is,the fraction of input rows that will\n\nremain at the output.\n\nThe cardinality of the node is the product of these two values.\n\nSelectivity is represented by a number from � to �. The smaller the number, the higher the selectivity,and vice versa,a number that is close to one denotes low selectivity. It may seem illogical, but the idea is that a highly selective condition rejects almost all the rows, while the one that dismisses only a few has low selectivity.\n\nFirst, the planner estimates cardinalities of leaf nodes that define data access methods. These calculations rely on the collected statistics, such as the total size of the table.\n\nSelectivity of filter conditions depends on their types. In the most trivial case, it can be assumed to be a constant value, although the planner tries to use all the availableinformationtorefinetheestimation. Ingeneral,itisenoughtoknowhow to estimate simple filter conditions; if a condition includes logical operations, its selectivity is calculated by the following formulas:1\n\nselxandy = selx sely selxory = 1 − (1 − selx)(1 − sely) = selx +sely −selx sely\n\nUnfortunately, these formulas assume that each other. For correlated predicates, such estimations will be inaccurate.\n\npredicates x and y do not depend on\n\nTo estimate the cardinality of joins, the planner has to get the cardinality of the Cartesian product (that is, the product of cardinalities of two data sets) and es- timate the selectivity of join conditions, which is again dependent on condition types.\n\nCardinality of other nodes (such as sorting or aggregation) is estimated in a similar manner.\n\nIt is important to note that incorrect cardinality estimation for lower plan nodes affects all the subsequent calculations, leading to inaccurate total cost estimation\n\n1 backend/optimizer/path/clausesel.c, clauselist_selectivity_ext & clauselist_selectivity_or functions\n\n301\n\np. ���",
      "content_length": 2094,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "Chapter 16 Query Execution Stages\n\nand a poor plan choice. To make things worse,the planner has no statistics on join results, only on tables.\n\nCost estimation. The process of estimating the cost is also recursive. To calculate the cost of a subtree, it is required to calculate and sum up the costs of all its child nodes and then add the cost of the parent node itself.\n\nTo estimate the cost of a node, Postgre��� applies the mathematical model of the operation performed by this node, using the already estimated node cardinality as input. For each node, both startup and total costs are calculated.\n\nSomeoperationshavenoprerequisites,sotheirexecutionstartsimmediately; such nodes have zero startup cost.\n\nOther operations, on the contrary, need to wait for some preliminary actions to complete. For example, a sort node usually has to wait for all the data from its child nodes before it can proceed to its own tasks. The startup cost of such nodes is usually higher than zero: this price has to be paid even if the above node (or the client) needs only one row of the whole output.\n\nAll calculations performed by the planner are simply estimations, which may have nothing to do with the actual execution time. Their only purpose is to enable com- parison of different plans for the same query in the same conditions. In other cases, it makes no sense to compare queries (especially different ones) in terms of their cost. For example, the cost could have been underestimated because of outdated statistics; once the statistics are refreshed,the calculated figure may rise,but since the estimation becomes more accurate, the server will choose a better plan.\n\nExecution\n\nThe plan built during query optimization now has to be executed.1\n\nThe executor opens a portal in the backend’s memory;2 it is an object that keeps the state of the query currently being executed. This state is represented as a tree\n\n1 postgresql.org/docs/14/executor.html\n\nbackend/executor/README\n\n2 backend/utils/mmgr/portalmem.c\n\n302",
      "content_length": 2011,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "16.2 Simple Query Protocol\n\nSORT\n\nNESTLOOP\n\nSEQSCAN\n\nSEQSCAN\n\npg_class\n\npg_namespace\n\nthat repeats the structure of the plan tree. The nodes of this tree operate like an assembly line, requesting and sending rows from one another.\n\nQuery execution starts at the root. The root node (which represents the ���� op- eration in this example) pulls the data from its child node. Having received all the rows, it sorts them and passes them on to the client.\n\nSome nodes (like the �������� node shown in this illustration) join data sets re- ceived from different sources. Such a node pulls the data from two child nodes, and, having received a pair of rows that satisfy the join condition, passes the re- sulting row upwards right away (unlike sorting, which has to get all the rows first). At this point, the execution of the node is interrupted until its parent requests the next row. If only a partial result is required (for example, there is a ����� clause in the query), the operation will not be performed in full.\n\nThe two ������� leaf nodes of the tree are responsible for table scans. When the parent node requests the data from these nodes, they fetch the subsequent row from the corresponding table.\n\nThus, some nodes do not store any rows, passing them upwards immediately, but others (such as ����) have to keep potentially large volumes of data. For this pur- work_mem chunk is allocated in the backend’s memory; if it is not enough, pose, a the remaining data is spilled into temporary files on disk.1\n\nA plan can have several nodes that need a data storage,so Postgre��� may allocate several memory chunks, each of the work_mem size. The total size of ��� that a query can use is not limited in any way.\n\n1 backend/utils/sort/tuplestore.c\n\n303\n\n4MB",
      "content_length": 1759,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "Chapter 16 Query Execution Stages\n\n16.3 Extended Query Protocol\n\nWhen using the simple query protocol, each command (even if it is being repeated multiple times) has to go through all the aforementioned stages:\n\n� parsing\n\n� transformation\n\n� planning\n\n� execution\n\nHowever, there is no point in parsing one and the same query time and again. Re- peated parsing of queries that differ only in constants does not make much sense either—the parse tree structure still remains the same.\n\nAnother downside of the simple query protocol is that the client receives the whole result at once, regardless of the number of rows it may contain.\n\nIn general,it is possible to get over these limitations using ��� commands. To deal with the first one,you can ������� the query before running the ������� command; the second concern can be addressed by creating a cursor with ������� and return- ing rows via �����. But in this case, naming of these newly created objects must be handled by the client,while the server gets additional overhead of parsing extra commands.\n\nThe extended client-server protocol provides an alternative solution,enabling pre- cise control over separate operator execution stages at the command level of the protocol itself.\n\nPreparation\n\nDuring the preparation stage,the query is parsed and transformed as usual,but the resulting parse tree is kept in the backend’s memory.\n\nPostgre��� has no global cache for queries. The disadvantage of this architecture is obvious: each backend has to parse all the incoming queries, even if the same query has been already parsed by another backend. But there are some benefits\n\n304",
      "content_length": 1635,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "16.3 Extended Query Protocol\n\ntoo. Globalcachecaneasilybecomeabottleneckbecauseoflocks. Aclientrunning multiple small but different queries (like the ones varying only in constants) gen- erates much traffic and can negatively affect performance of the whole instance. In Postgre���, queries are parsed locally, so there is no impact on other processes.\n\nA prepared query can be parameterized. Here is a simple example using ��� com- mands(althoughitisnotthesameaspreparationattheprotocollevel,theultimate effect is the same):\n\n=> PREPARE plane(text) AS SELECT * FROM aircrafts WHERE aircraft_code = $1;\n\nAll the named prepared statements are shown in the pg_prepared_statements view:\n\n=> SELECT name, statement, parameter_types FROM pg_prepared_statements \\gx\n\n−[ RECORD 1 ]−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− name statement\n\n| plane | PREPARE plane(text) AS | SELECT * FROM aircrafts WHERE aircraft_code = $1;\n\n+\n\nparameter_types | {text}\n\nYou will not find any unnamed statements here (the ones that use the extended query protocol or ��/pg���). The statements prepared by other backends are not displayed either: it is impossible to access the other session’s memory.\n\nParameter Binding\n\nBefore a prepared statement gets executed,the actual parameter values have to be bound.\n\n=> EXECUTE plane('733');\n\naircraft_code |\n\nmodel\n\n| range\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−\n\n733\n\n| Boeing 737−300 |\n\n4200\n\n(1 row)\n\nThe advantage of binding parameters in prepared statements over concatenating literals with query strings is that it makes ��� injections absolutely impossible: a bound parameter value cannot modify the already built parse tree in any way.\n\n305\n\np. ���",
      "content_length": 1692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "Chapter 16 Query Execution Stages\n\nTo reach the same security level without prepared statements, you would have to carefully escape each value received from an untrusted source.\n\nPlanning and Execution\n\nWhen it comes to prepared statement execution, query planning is performed based on the actual parameter values; then the plan is passed on to the executor.\n\nDifferent parameter values may imply different optimal plans, so it is important to take the exact values into account. For example, when looking for expensive bookings,the planner assumes that there are not so many matching rows and uses an index scan:\n\n=> CREATE INDEX ON bookings(total_amount);\n\n=> EXPLAIN SELECT * FROM bookings WHERE total_amount > 1000000;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings\n\n(cost=86.49..9245.82 rows=4395 wid...\n\nRecheck Cond: (total_amount > '1000000'::numeric) −> Bitmap Index Scan on bookings_total_amount_idx\n\n(cost=0.00....\n\nIndex Cond: (total_amount > '1000000'::numeric)\n\n(4 rows)\n\nBut if the provided condition is satisfied by all the bookings, there is no point in using an index, as the whole table has to be scanned:\n\n=> EXPLAIN SELECT * FROM bookings WHERE total_amount > 100;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on bookings\n\n(cost=0.00..39835.88 rows=2111110 width=21)\n\nFilter: (total_amount > '100'::numeric)\n\n(2 rows)\n\nInsomecases,theplannermaykeepboththeparsetreeandthequeryplantoavoid repeated planning. Such a plan does not take parameter values into account, so it is called a generic plan (as compared to custom plans based on the actual values).1\n\n1 backend/utils/cache/plancache.c, choose_custom_plan function\n\n306",
      "content_length": 1753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "16.3 Extended Query Protocol\n\nAn obvious case when the server can use a generic plan without compromising performance is a query with no parameters.\n\nThe first five optimizations of parameterized prepared statements always rely on the actual parameter values; the planner calculates the average cost of custom plans based on these values. Starting from the sixth execution, if the generic plan turns out to be more efficient than custom plans on average (taking into account thatcustomplanshavetobebuiltaneweverytime),1 theplannerkeepsthegeneric plan and continues using it, skipping the optimization stage.\n\nTheplanepreparedstatementhasalreadybeenexecutedonce. Afterthenextthree executions,the server still uses custom plans—you can tell by the parameter value in the query plan:\n\n=> EXECUTE plane('763');\n\n=> EXECUTE plane('773');\n\n=> EXPLAIN EXECUTE plane('319');\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on aircrafts_data ml\n\n(cost=0.00..1.39 rows=1 width=52)\n\nFilter: ((aircraft_code)::text = '319'::text)\n\n(2 rows)\n\nAfterthefifthexecution,theplannerswitchestothegenericplan: itdoesnotdiffer from the custom ones and has the same cost, but the backend can build it once and skip the optimization stage, thus reducing planning overhead. The ������� command now shows that the parameter is referred to by position rather than by its value:\n\n=> EXECUTE plane('320');\n\n=> EXPLAIN EXECUTE plane('321');\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on aircrafts_data ml\n\n(cost=0.00..1.39 rows=1 width=52)\n\nFilter: ((aircraft_code)::text = $1)\n\n(2 rows)\n\n1 backend/utils/cache/plancache.c, cached_plan_cost function\n\n307",
      "content_length": 1710,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "v. �� auto\n\nv. ��\n\nChapter 16 Query Execution Stages\n\nWe can easily imagine an unhappy turn of events when the first several custom plans are more expensive than the generic plan; subsequent plans could have been more efficient, but the planner will not consider them at all. Besides, it compares estimations rather than actual costs, which can also lead to miscalculations.\n\nHowever, and select either the generic or a custom plan by setting the rameter accordingly:\n\nif the planner makes a mistake, you can override the automatic decision plan_cache_mode pa-\n\n=> SET plan_cache_mode = 'force_custom_plan';\n\n=> EXPLAIN EXECUTE plane('CN1');\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on aircrafts_data ml\n\n(cost=0.00..1.39 rows=1 width=52)\n\nFilter: ((aircraft_code)::text = 'CN1'::text)\n\n(2 rows)\n\nAmong other things, the pg_prepared_statements view plans:\n\nshows statistics on chosen\n\n=> SELECT name, generic_plans, custom_plans FROM pg_prepared_statements;\n\nname\n\n| generic_plans | custom_plans\n\n−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\nplane |\n\n1 |\n\n6\n\n(1 row)\n\nGetting the Results\n\nThe extended query protocol allows retrieving data in batches rather than all at once. ���cursorshavealmostthesameeffect(exceptthatthereissomeextrawork for the server, and the planner optimizes fetching of the first cursor_tuple_fraction rows, not the whole result set):\n\n=> BEGIN;\n\n=> DECLARE cur CURSOR FOR\n\nSELECT * FROM aircrafts ORDER BY aircraft_code;\n\n308",
      "content_length": 1494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "16.3 Extended Query Protocol\n\n=> FETCH 3 FROM cur;\n\naircraft_code |\n\nmodel\n\n| range\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−+−−−−−−−\n\n319 320 321\n\n| Airbus A319−100 | | Airbus A320−200 | | Airbus A321−200 |\n\n6700 5700 5600\n\n(3 rows)\n\n=> FETCH 2 FROM cur;\n\naircraft_code |\n\nmodel\n\n| range\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−\n\n733 763\n\n| Boeing 737−300 | | Boeing 767−300 |\n\n4200 7900\n\n(2 rows)\n\n=> COMMIT;\n\nIf the query returns many rows and the client needs them all, the system through- put highly depends on the batch size. The more rows in a batch,the less communi- cation overhead is incurred by accessing the server and getting the response. But as the batch size grows, these benefits become less tangible: while the difference between fetching rows one by one and in batches of ten rows can be enormous, it is much less noticeable if you compare batches of ��� and ���� rows.\n\n309",
      "content_length": 887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "p. ��\n\n17\n\nStatistics\n\n17.1 Basic Statistics\n\nBasic relation-level statistics1 are stored in the pg_class table of the system catalog and include the following data:\n\nnumber of tuples in a relation (reltuples)\n\nrelation size, in pages (relpages)\n\nnumber of pages tagged in the visibility map (relallvisible)\n\nHere are these values for the flights table:\n\n=> SELECT reltuples, relpages, relallvisible FROM pg_class WHERE relname = 'flights';\n\nreltuples | relpages | relallvisible −−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−\n\n214867 |\n\n2624 |\n\n2624\n\n(1 row)\n\nIf the query does not impose any filter conditions, the reltuples value serves as the cardinality estimation:\n\n=> EXPLAIN SELECT * FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=63)\n\n(1 row)\n\n1 postgresql.org/docs/14/planner-stats.html\n\n310",
      "content_length": 896,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "17.1 Basic Statistics\n\n, both manual and automatic.1 Fur- Statistics are collected during table analysis thermore, since basic statistics are of paramount importance, this data is calcu- latedduringsomeotheroperationsaswell(������ ����and�������,2 ������ ����� and �������3) and is refined during vacuuming.4\n\ndefault_statistics_target random rows are sampled. The For analysis purposes,300× sample size required to build statistics of a particular accuracy has low dependency on the volume of analyzed data, so the size of the table is not taken into account.5\n\nSampled rows are picked from the same number (300 × default_statistics_target) of random pages.6 Obviously, if the table itself is smaller, fewer pages may be read, and fewer rows will be selected for analysis.\n\nInlargetables,statisticscollectiondoesnotincludealltherows,soestimationscan diverge from actual values. It is perfectly normal: if the data is changing,statistics cannot be accurate all the time anyway. Accuracy of up to an order of magnitude is usually enough to choose an adequate plan.\n\nLet’s create a copy of the flights table with autovacuum disabled, so that we can control the autoanalysis start time:\n\n=> CREATE TABLE flights_copy(LIKE flights) WITH (autovacuum_enabled = false);\n\nThere is no statistics for the new table yet:\n\n=> SELECT reltuples, relpages, relallvisible FROM pg_class WHERE relname = 'flights_copy';\n\nreltuples | relpages | relallvisible −−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−\n\n−1 |\n\n0 |\n\n0\n\n(1 row)\n\n1 backend/commands/analyze.c, do_analyze_rel function 2 backend/commands/cluster.c, copy_table_data function 3 backend/catalog/heap.c, index_update_stats function 4 backend/access/heap/vacuumlazy.c, heap_vacuum_rel function 5 backend/commands/analyze.c, std_typanalyze function 6 backend/commands/analyze.c, acquire_sample_rows function\n\nbackend/utils/misc/sampling.c\n\n311\n\np. ���\n\n100",
      "content_length": 1888,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "v. ��\n\np. ���\n\nChapter 17 Statistics\n\nThe value reltuples = −1 analyzed yet and a really empty table without any rows.\n\nis used to differentiate between a table that has not been\n\nIt is highly likely that some rows will get inserted into the table right after its cre- ation. So being unaware of the current state of things, the planner assumes that the table contains 10 pages:\n\n=> EXPLAIN SELECT * FROM flights_copy;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights_copy\n\n(cost=0.00..14.10 rows=410 width=170)\n\n(1 row)\n\nThenumberofrowsisestimatedbasedonthesizeofasinglerow,whichisshownin the plan as width. Row width is typically an average value calculated during analy- sis, but since no statistics have been collected yet, here it is just an approximation based on the column data types.1\n\nNow let’s copy the data from the flights table and perform the analysis:\n\n=> INSERT INTO flights_copy SELECT * FROM flights;\n\nINSERT 0 214867\n\n=> ANALYZE flights_copy;\n\nThe collected statistics reflects the actual number of rows (the table size is small enough for the analyzer to gather statistics on all the data):\n\n=> SELECT reltuples, relpages, relallvisible FROM pg_class WHERE relname = 'flights_copy';\n\nreltuples | relpages | relallvisible −−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−\n\n214867 |\n\n2624 |\n\n0\n\n(1 row)\n\nThe relallvisible value is used to estimate the cost is updated by ������:\n\nof an index-only scan. This value\n\n=> VACUUM flights_copy;\n\n1 backend/access/table/tableam.c, table_block_relation_estimate_size function\n\n312",
      "content_length": 1580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "17.1 Basic Statistics\n\n=> SELECT relallvisible FROM pg_class WHERE relname = 'flights_copy';\n\nrelallvisible −−−−−−−−−−−−−−−\n\n2624\n\n(1 row)\n\nNow let’s double the number of rows without updating statistics and check the cardinality estimation in the query plan:\n\n=> INSERT INTO flights_copy SELECT * FROM flights;\n\n=> SELECT count(*) FROM flights_copy;\n\ncount −−−−−−−− 429734 (1 row)\n\n=> EXPLAIN SELECT * FROM flights_copy;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights_copy\n\n(cost=0.00..9545.34 rows=429734 width=63)\n\n(1 row)\n\nDespite the outdated pg_class data, the estimation turns out to be accurate:\n\n=> SELECT reltuples, relpages FROM pg_class WHERE relname = 'flights_copy';\n\nreltuples | relpages −−−−−−−−−−−+−−−−−−−−−−\n\n214867 |\n\n2624\n\n(1 row)\n\nThe thing is that if the planner sees a gap between relpages and the actual file size, it can scale the reltuples value to improve estimation accuracy.1 Since the file size has doubled as compared to relpages,the planner adjusts the estimated number of rows, assuming that data density remains the same:\n\n=> SELECT reltuples *\n\n(pg_relation_size('flights_copy') / 8192) / relpages AS tuples\n\nFROM pg_class WHERE relname = 'flights_copy';\n\n1 backend/access/table/tableam.c, table_block_relation_estimate_size function\n\n313",
      "content_length": 1329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "Chapter 17 Statistics\n\ntuples −−−−−−−− 429734 (1 row)\n\nNaturally,such an adjustment may not always work (for example,if we delete some rows,the estimation will remain the same),but in some cases it allows the planner to hold on until significant changes trigger the next analysis run.\n\n17.2 NULL Values\n\nFrowned upon by theoreticians,1 ���� values still play an important role in rela- tional databases: they provide a convenient way to reflect the fact that a value is either unknown or does not exist.\n\nHowever,a special value demands special treatment. Apart from theoretical incon- sistencies, there are also multiple practical challenges that have to be taken into account. Regular Boolean logic is replaced by the three-valued one, so ��� �� be- haves unexpectedly. It is unclear whether ���� values should be treated as greater than or less than regular values (hence the ����� ����� and ����� ���� clauses for sorting). It is not quite obvious whether ���� values must be taken into account by aggregate functions. Strictly speaking, ���� values are not values at all, so the planner requires additional information to process them.\n\nApart from the simplest basic statistics collected at the relation level, the analyzer also gathers statistics for each column of the relation. This data is stored in the pg_statistic table of the system catalog,2 but you can also access it via the pg_stats view, which provides this information in a more convenient format.\n\nThefractionof����valuesbelongstocolumn-levelstatistics; calculatedduringthe analysis, it is shown as the null_frac attribute.\n\nFor example,when searching for the flights that have not departed yet,we can rely on their departure times being undefined:\n\n=> EXPLAIN SELECT * FROM flights WHERE actual_departure IS NULL;\n\n1 sigmodrecord.org/publications/sigmodRecord/0809/p20.date.pdf 2 include/catalog/pg_statistic.h\n\n314",
      "content_length": 1886,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "17.3 Distinct Values\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..4772.67 rows=16702 width=63)\n\nFilter: (actual_departure IS NULL)\n\n(2 rows)\n\nTo estimate the result, the planner multiplies the total number of rows by the frac- tion of ���� values:\n\n=> SELECT round(reltuples * s.null_frac) AS rows FROM pg_class\n\nJOIN pg_stats s ON s.tablename = relname\n\nWHERE s.tablename = 'flights'\n\nAND s.attname = 'actual_departure';\n\nrows −−−−−−− 16702 (1 row)\n\nAnd here is the actual row count:\n\n=> SELECT count(*) FROM flights WHERE actual_departure IS NULL;\n\ncount −−−−−−− 16348 (1 row)\n\n17.3 Distinct Values\n\nThe n_distinct field of the pg_stats view shows the number of distinct values in a column.\n\nIf n_distinct is negative, its absolute value denotes the fraction of distinct values in a column rather than their actual count. For example, −1 indicates that all col- umn values are unique, while −3 means that each value appears in three rows on average. The analyzer uses fractions if the estimated number of distinct values ex- ceeds 10% of the total row count; in this case, further data updates are unlikely to change this ratio.1\n\n1 backend/commands/analyze.c, compute_distinct_stats function\n\n315",
      "content_length": 1264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "Chapter 17 Statistics\n\ny c n e u q e r f\n\nnull_frac\n\nn_distinct\n\nvalues\n\nIf uniform data distribution is expected, the number of distinct values is used in- stead. For example, when estimating the cardinality of the “column = expression” condition,the planner assumes that the expression can take any column value with equal probability if its exact value is unknown at the planning stage:1\n\n=> EXPLAIN SELECT * FROM flights WHERE departure_airport = (\n\nSELECT airport_code FROM airports WHERE city = 'Saint Petersburg'\n\n);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=30.56..5340.40 rows=2066 width=63)\n\nFilter: (departure_airport = $0) InitPlan 1 (returns $0)\n\n−> Seq Scan on airports_data ml\n\n(cost=0.00..30.56 rows=1 wi...\n\nFilter: ((city −>> lang()) = 'Saint Petersburg'::text)\n\n(5 rows)\n\nHere the InitPlan node is executed only once, and the calculated value is used in the main plan.\n\n1 backend/utils/adt/selfuncs.c, var_eq_non_const function\n\n316",
      "content_length": 1019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "17.4 Most Common Values\n\n=> SELECT round(reltuples / s.n_distinct) AS rows FROM pg_class\n\nJOIN pg_stats s ON s.tablename = relname\n\nWHERE s.tablename = 'flights'\n\nAND s.attname = 'departure_airport';\n\nrows −−−−−− 2066 (1 row)\n\nIf the estimated number of distinct values is incorrect (because a limited number of rows have been analyzed), it can be overridden at the column level:\n\nALTER TABLE ...\n\nALTER COLUMN ... SET (n_distinct = ...);\n\nIf all data always had uniform distribution,this information (coupledwith minimal and maximal values) would be sufficient. However, for non-uniform distribution (which is much more common in practice), such estimation is inaccurate:\n\n=> SELECT min(cnt), round(avg(cnt)) avg, max(cnt) FROM (\n\nSELECT departure_airport, count(*) cnt FROM flights GROUP BY departure_airport\n\n) t;\n\nmin | avg\n\n|\n\nmax\n\n−−−−−+−−−−−−+−−−−−−− 113 | 2066 | 20875\n\n(1 row)\n\n17.4 Most Common Values\n\nIf data distribution is non-uniform,the estimation is fine-tuned based on statistics on most common values (���) and their frequencies. The pg_stats view displays these arrays in the most_common_vals and most_common_freqs fields, respectively.\n\nHere is an example of such statistics on various types of aircraft:\n\n317",
      "content_length": 1229,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "Chapter 17 Statistics\n\ny c n e u q e r f\n\n[most_common_vals]\n\n] s q e r f _ n o m m o c _ t s o m\n\n[\n\nnull_frac\n\n=> SELECT most_common_vals AS mcv,\n\nleft(most_common_freqs::text,60) || '...' AS mcf\n\nFROM pg_stats WHERE tablename = 'flights' AND attname = 'aircraft_code' \\gx\n\n−[ RECORD 1 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− mcv | {CN1,CR2,SU9,321,733,763,319,773} mcf | {0.27886668,0.27266666,0.26176667,0.057166666,0.037666667,0....\n\nTo estimate the selectivity of the “column = value” condition, it is enough to find this value in the most_common_vals array and take its frequency from the most_common_freqs array element with the same index:1\n\n=> EXPLAIN SELECT * FROM flights WHERE aircraft_code = '733';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..5309.84 rows=8093 width=63)\n\nFilter: (aircraft_code = '733'::bpchar)\n\n(2 rows)\n\n1 backend/utils/adt/selfuncs.c, var_eq_const function\n\n318\n\nvalues",
      "content_length": 986,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "17.4 Most Common Values\n\n=> SELECT round(reltuples * s.most_common_freqs[\n\narray_position((s.most_common_vals::text::text[]),'733')\n\n]) FROM pg_class\n\nJOIN pg_stats s ON s.tablename = relname\n\nWHERE s.tablename = 'flights'\n\nAND s.attname = 'aircraft_code';\n\nround −−−−−−− 8093 (1 row)\n\nIt is obvious that such estimation will be close to the actual value:\n\n=> SELECT count(*) FROM flights WHERE aircraft_code = '733';\n\ncount −−−−−−− 8263 (1 row)\n\nThe ��� list is also used to estimate selectivity of inequality conditions. For ex- ample, a condition like “column < value” requires the analyzer to search through most_common_vals for all the values that are smaller than the target one and sum up the corresponding frequencies listed in most_common_freqs.1\n\nM�� statistics work best when distinct values are not too many. The maximum size default_statistics_target parameter, which also limits the of arrays is defined by the number of rows to be randomly sampled for the purpose of analysis.\n\nIn some cases, it makes sense to increase the default parameter value, thus ex- panding the ��� list and improving the accuracy of estimations. You can do it at the column level:\n\nALTER TABLE ...\n\nALTER COLUMN ... SET STATISTICS ...;\n\nThe sample size will also grow, but only for the specified table.\n\n1 backend/utils/adt/selfuncs.c, scalarineqsel function\n\n319\n\n100",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "Chapter 17 Statistics\n\nSince the ��� array stores actual values, it may take quite a lot of space. To keep the pg_statistic size under control and avoid loading the planner with useless work, values that are larger than � k� are excluded from analysis and statistics. But since such large values are likely to be unique, they would probably not make it into most_common_vals anyway.\n\n17.5 Histogram\n\nIf distinct values are too many to be stored in an array, Postgre��� employs a histogram. In this case, values are distributed between several buckets of the his- togram. The number of buckets is also limited by the default_statistics_target pa- rameter.\n\nThe bucket width is selected in such a way that each bucket gets approximately the same number of values (this property is reflected in the diagram by the equality of areas of big hatched rectangles). The values included into ��� lists are not taken into account. As a result,the cumulative frequency of values in each bucket equals\n\n1 number of buckets\n\n.\n\nThe histogram is stored in the histogram_bounds field of the pg_stats view as an array of buckets’ boundary values:\n\n=> SELECT left(histogram_bounds::text,60) || '...' AS hist_bounds FROM pg_stats s WHERE s.tablename = 'boarding_passes' AND s.attname = 'seat_no';\n\nhist_bounds −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− {10B,10E,10F,10F,11H,12B,13B,14B,14H,15G,16B,17B,17H,19B,19B...\n\n(1 row)\n\nCombined with the ��� list, the histogram is used for operations like estimating the selectivity of greater than and less than conditions.1 For example, let’s take a look at the number of boarding passes issued for back rows:\n\n1 backend/utils/adt/selfuncs.c, ineq_histogram_selectivity function\n\n320",
      "content_length": 1730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "17.5 Histogram\n\ny c n e u q e r f\n\n[mcv]\n\n]\n\nf c m\n\n[\n\nnull_frac\n\n[histogram_bounds]\n\nvalues\n\n=> EXPLAIN SELECT * FROM boarding_passes WHERE seat_no > '30B';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on boarding_passes\n\n(cost=0.00..157350.10 rows=2983242 ...\n\nFilter: ((seat_no)::text > '30B'::text)\n\n(2 rows)\n\nI have intentionally selected the seat number that lies right on the boundary be- tween two histogram buckets.\n\nN number of buckets\n\n, where N is the The selectivity of this condition will be estimated at number of buckets holding the values that satisfy the condition (that is, the ones located to the right of the specified value). It must also be taken into account that ���s are not included into the histogram.\n\nIncidentally, ���� values do not appear in the histogram either, but the seat_no column contains no such values anyway:\n\n=> SELECT s.null_frac FROM pg_stats s WHERE s.tablename = 'boarding_passes' AND s.attname = 'seat_no';\n\n321",
      "content_length": 1006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "Chapter 17 Statistics\n\nnull_frac −−−−−−−−−−−\n\n0\n\n(1 row)\n\nFirst, let’s find the fraction of ���s that satisfy the condition:\n\n=> SELECT sum(s.most_common_freqs[\n\narray_position((s.most_common_vals::text::text[]),v)\n\n]) FROM pg_stats s, unnest(s.most_common_vals::text::text[]) v WHERE s.tablename = 'boarding_passes' AND s.attname = 'seat_no'\n\nAND v > '30B';\n\nsum −−−−−−−−−−−− 0.21226665\n\n(1 row)\n\nThe overall ��� share (ignored by the histogram) is:\n\n=> SELECT sum(s.most_common_freqs[\n\narray_position((s.most_common_vals::text::text[]),v)\n\n]) FROM pg_stats s, unnest(s.most_common_vals::text::text[]) v WHERE s.tablename = 'boarding_passes' AND s.attname = 'seat_no';\n\nsum −−−−−−−−−−−− 0.67816657\n\n(1 row)\n\nSince the values that conform to the specified condition take exactly 𝑁 buckets (out of ��� buckets possible), we get the following estimation:\n\n=> SELECT round( reltuples * ( 0.21226665 -- MCV share\n\n+ (1 - 0.67816657 - 0) * (51 / 100.0) -- histogram share\n\n)) FROM pg_class WHERE relname = 'boarding_passes';\n\nround −−−−−−−−− 2983242\n\n(1 row)\n\n322",
      "content_length": 1058,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "17.5 Histogram\n\ny c n e u q e r f\n\nnull_frac\n\nx\n\nvalues\n\nIn the generic case of non-boundary values, the planner applies linear interpola- tion to take into account the fraction of the bucket that contains the target value.\n\nHere is the actual number of back seats:\n\n=> SELECT count(*) FROM boarding_passes WHERE seat_no > '30B';\n\ncount −−−−−−−−− 2993735\n\n(1 row)\n\nAs you increase the default_statistics_target value, estimation accuracy may im- prove,but as our example shows,the histogram combined with the ��� list usually gives good results even if the column contains many unique values:\n\n=> SELECT n_distinct FROM pg_stats WHERE tablename = 'boarding_passes' AND attname = 'seat_no';\n\nn_distinct −−−−−−−−−−−−\n\n461\n\n(1 row)\n\nIt makes sense to improve estimation accuracy only if it leads to better planning. Increasing the default_statistics_target value without giving it much thought may\n\n323",
      "content_length": 899,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "Chapter 17 Statistics\n\nslow down planning and analysis without bringing any benefits in return. That said, reducing this parameter value (down to zero) can lead to a bad plan choice, even though it does speed up planning and analysis. Such savings are usually un- justified.\n\n17.6 Statistics for Non-Scalar Data Types\n\nFor non-scalar data types, Postgre��� can gather statistics not only on the distri- bution of values, but also on the distribution of elements used to construct these values. It improves planning accuracy when you query columns that do not con- form to the first normal form.\n\nThe most_common_elems and most_common_elem_freqs arrays show the list of\n\nthe most common elements and the frequency of their usage.\n\nThese statistics are collected and used to estimate selectivity of operations on arrays1 and tsvector2 data types.\n\nThe elem_count_histogram array shows the histogram of the number of distinct\n\nelements in a value.\n\nThis data is collected and used for estimating selectivity of operations on ar- rays only.\n\nFor range types, Postgre��� builds distribution histograms for range length and lower and upper boundaries of the range. These histograms are used for estimating selectivity of various operations on these types,3 but the pg_stats view does not display them. data types.4\n\nFor range types, Postgre��� builds distribution histograms for range length and lower and upper boundaries of the range. These histograms are used for estimating selectivity of various operations on these types,3 but the pg_stats view does not display them. data types.4\n\n1 postgresql.org/docs/14/arrays.html\n\nbackend/utils/adt/array_typanalyze.c backend/utils/adt/array_selfuncs.c\n\n2 postgresql.org/docs/14/datatype-textsearch.html\n\nbackend/tsearch/ts_typanalyze.c backend/tsearch/ts_selfuncs.c\n\n3 postgresql.org/docs/14/rangetypes.html\n\nbackend/utils/adt/rangetypes_typanalyze.c backend/utils/adt/rangetypes_selfuncs.c\n\n4 backend/utils/adt/multirangetypes_selfuncs.c\n\n324",
      "content_length": 1983,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "17.7 Average Field Width\n\n17.7 Average Field Width\n\nThe avg_width field of the pg_stats view shows the average size of values stored in a column. Naturally, for types like integer or char(3) this size is always the same, but for data types of variable length, such as text, it can vary a lot from column to column:\n\n=> SELECT attname, avg_width FROM pg_stats WHERE (tablename, attname) IN ( VALUES\n\n('tickets', 'passenger_name'), ('ticket_flights','fare_conditions')\n\n);\n\nattname\n\n| avg_width\n\n−−−−−−−−−−−−−−−−−+−−−−−−−−−−−\n\nfare_conditions | | passenger_name\n\n8 16\n\n(2 rows)\n\nThis statistic is used to estimate the amount of memory required for operations like sorting or hashing.\n\n17.8 Correlation\n\nThecorrelationfieldofthepg_statsviewshowsthecorrelationbetweenthephysical order of data and the logical order defined by comparison operations. If values are stored strictly in ascending order, their correlation will be close to 1; if they are arranged in descending order, their correlation will be close to −1. The more chaotic is data distribution on disk, the closer is the correlation to zero.\n\n=> SELECT attname, correlation FROM pg_stats WHERE tablename = 'airports_data' ORDER BY abs(correlation) DESC;\n\nattname\n\n| correlation\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−− coordinates | airport_code | −0.21120238 city −0.1970127 | airport_name | −0.18223621 0.17961165 | timezone\n\n(5 rows)\n\n325",
      "content_length": 1388,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "p. ���\n\nChapter 17 Statistics\n\nNote that this statistic is not gathered for the coordinates column: less than and greater than operators are not defined for the point type.\n\n. Correlation is used for cost estimation of index scans\n\n17.9 Expression Statistics\n\nColumn-level statistics can be used only if either the left or the right part of the comparison operation refers to the column itself and does not contain any expres- sions. For example, the planner cannot predict how computing a function of a column will affect statistics, so for conditions like “function-call = constant” the selectivity is always estimated at �.�%:1\n\n=> EXPLAIN SELECT * FROM flights WHERE extract(\n\nmonth FROM scheduled_departure AT TIME ZONE 'Europe/Moscow'\n\n) = 1;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..6384.17 rows=1074 width=63)\n\nFilter: (EXTRACT(month FROM (scheduled_departure AT TIME ZONE ...\n\n(2 rows)\n\n=> SELECT round(reltuples * 0.005) FROM pg_class WHERE relname = 'flights';\n\nround −−−−−−− 1074 (1 row)\n\nThe planner knows nothing about semantics of functions, even of standard ones. Our general knowledge suggests that the flights performed in January will make roughly 1 of the total number of flights,which exceeds the projected value by one 12 order of magnitude.\n\nTo improve the estimation,we have to collect expression statistics rather than rely on the column-level one. There are two ways to do it.\n\n1 backend/utils/adt/selfuncs.c, eqsel function\n\n326",
      "content_length": 1530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "17.9 Expression Statistics\n\nExtended Expression Statistics\n\nThe first option is to use extended expression statistics.1 Such statistics are not col- lected by default; you have to manually create the corresponding database object by running the ������ ���������� command:\n\n=> CREATE STATISTICS flights_expr ON (extract(\n\nmonth FROM scheduled_departure AT TIME ZONE 'Europe/Moscow'\n\n)) FROM flights;\n\nOnce the data is gathered, the estimation accuracy improves:\n\n=> ANALYZE flights;\n\n=> EXPLAIN SELECT * FROM flights WHERE extract(\n\nmonth FROM scheduled_departure AT TIME ZONE 'Europe/Moscow'\n\n) = 1;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..6384.17 rows=16667 width=63)\n\nFilter: (EXTRACT(month FROM (scheduled_departure AT TIME ZONE ...\n\n(2 rows)\n\nFor the collected statistics to be applied, the query must specify the expression in exactly the same form that was used by the ������ ���������� command.\n\nThe size limit for extended statistics ����� ���������� command. For example:\n\ncan be adjusted separately, by running the\n\n=> ALTER STATISTICS flights_expr SET STATISTICS 42;\n\nAll the metadata related to extended statistics is stored in the pg_statistic_ext table of the system catalog, while the collected data itself resides in a separate table called pg_statistic_ext_data . This separation is used to implement access control for sensitive information.\n\nExtended expression statistics available to a particular user can be displayed in a more convenient format in a separate view:\n\n1 postgresql.org/docs/14/planner-stats#PLANNER-STATS-EXTENDED.html\n\nbackend/statistics/README\n\n327\n\nv. ��\n\nv. ��\n\nv. ��",
      "content_length": 1684,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "p. ���\n\nChapter 17 Statistics\n\n=> SELECT left(expr,50) || '...' AS expr,\n\nnull_frac, avg_width, n_distinct, most_common_vals AS mcv, left(most_common_freqs::text,50) || '...' AS mcf, correlation\n\nFROM pg_stats_ext_exprs WHERE statistics_name = 'flights_expr' \\gx\n\n−[ RECORD 1 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− | EXTRACT(month FROM (scheduled_departure AT TIME ZO... expr | 0 null_frac | 8 avg_width | 12 n_distinct | {8,9,12,3,1,5,6,7,11,10,4,2} mcv mcf | {0.12053333,0.11326667,0.0802,0.07976667,0.0775666... correlation | 0.08355749\n\nStatistics for Expression Indexes\n\nAnother way to improve cardinality estimation is to use special statistics collected for expression indexes ; these statistics are gathered automatically when such an index is created, just like it is done for a table. If the index is really needed, this approach turns out to be very convenient.\n\n=> DROP STATISTICS flights_expr;\n\n=> CREATE INDEX ON flights(extract(\n\nmonth FROM scheduled_departure AT TIME ZONE 'Europe/Moscow'\n\n));\n\n=> ANALYZE flights;\n\n=> EXPLAIN SELECT * FROM flights WHERE extract(\n\nmonth FROM scheduled_departure AT TIME ZONE 'Europe/Moscow'\n\n) = 1;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights\n\n(cost=324.86..3247.92 rows=17089 wi... Recheck Cond: (EXTRACT(month FROM (scheduled_departure AT TIME... (cost=0.00..320.5... −> Bitmap Index Scan on flights_extract_idx Index Cond: (EXTRACT(month FROM (scheduled_departure AT TI...\n\n(4 rows)\n\n328",
      "content_length": 1524,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "17.10 Multivariate Statistics\n\nStatistics on expression indexes are stored in the same way as statistics on tables. For example, you can get the number of distinct values by specifying the index name as tablename when querying pg_stats:\n\n=> SELECT n_distinct FROM pg_stats WHERE tablename = 'flights_extract_idx';\n\nn_distinct −−−−−−−−−−−−\n\n12\n\n(1 row)\n\nof index-related statistics using the ����� ����� com- You can adjust the accuracy mand. If you do not know the column name that corresponds to the indexed ex- pression, you have to first find it out. For example:\n\n=> SELECT attname FROM pg_attribute WHERE attrelid = 'flights_extract_idx'::regclass;\n\nattname −−−−−−−−− extract\n\n(1 row)\n\n=> ALTER INDEX flights_extract_idx\n\nALTER COLUMN extract SET STATISTICS 42;\n\n17.10 Multivariate Statistics\n\nItis alsopossibletocollectmultivariatestatistics,whichspanseveraltablecolumns. As a prerequisite, you have to manually create the corresponding extended statis- tics using the ������ ���������� command.\n\nPostgre��� implements three types of multivariate statistics.\n\nFunctional Dependencies Between Columns\n\nIf values in one column depend (fully or partially) on values in another column and the filter conditions include both these columns, cardinality will be underes- timated.\n\n329\n\nv. ��\n\nv. ��",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "p. ���\n\nChapter 17 Statistics\n\nLet’s consider a query with two filter conditions:\n\n=> SELECT count(*) FROM flights WHERE flight_no = 'PG0007' AND departure_airport = 'VKO';\n\ncount −−−−−−−\n\n396\n\n(1 row)\n\nThe value is hugely underestimated:\n\n=> EXPLAIN SELECT * FROM flights WHERE flight_no = 'PG0007' AND departure_airport = 'VKO';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights\n\n(cost=10.49..816.84 rows=15 width=63)\n\nRecheck Cond: (flight_no = 'PG0007'::bpchar) Filter: (departure_airport = 'VKO'::bpchar) −> Bitmap Index Scan on flights_flight_no_scheduled_departure_key\n\n(cost=0.00..10.49 rows=276 width=0) Index Cond: (flight_no = 'PG0007'::bpchar)\n\n(6 rows)\n\nIt is a well-known problem of correlated predicates. The planner assumes that pred- icates do not depend on each other, so the overall selectivity is estimated at the product of selectivities of filter conditions combined by logical ��� . The plan above clearly illustrates this issue: the value estimated by the Bitmap Index Scan node for the condition on the flight_no column is significantly reduced once the Bitmap Heap Scan node filters the results by the condition on the departure_airport column.\n\nHowever,we do understand that airports are unambiguously defined by flight num- bers: the second condition is virtually redundant (unless there is a mistake in the airport name,of course). In such cases,we can improve the estimation by applying extended statistics on functional dependencies.\n\nLet’s create an extended statistic on the functional dependency between the two columns:\n\n=> CREATE STATISTICS flights_dep(dependencies) ON flight_no, departure_airport FROM flights;\n\n330",
      "content_length": 1714,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "17.10 Multivariate Statistics\n\nThe next analysis run gathers this statistic, and the estimation improves:\n\n=> ANALYZE flights;\n\n=> EXPLAIN SELECT * FROM flights WHERE flight_no = 'PG0007'\n\nAND departure_airport = 'VKO';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights\n\n(cost=10.57..819.51 rows=277 width=63)\n\nRecheck Cond: (flight_no = 'PG0007'::bpchar) Filter: (departure_airport = 'VKO'::bpchar) −> Bitmap Index Scan on flights_flight_no_scheduled_departure_key\n\n(cost=0.00..10.50 rows=277 width=0) Index Cond: (flight_no = 'PG0007'::bpchar)\n\n(6 rows)\n\nThe collected statistics is stored in the system catalog and can be accessed like this:\n\n=> SELECT dependencies FROM pg_stats_ext WHERE statistics_name = 'flights_dep';\n\ndependencies −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− {\"2 => 5\": 1.000000, \"5 => 2\": 0.010200}\n\n(1 row)\n\nHere � and � are column numbers stored in the pg_attribute table, whereas the corresponding values define the degree of functional dependency: from � (no de- pendency) to � (values in the second columns fully depend on values in the first column).\n\nMultivariate Number of Distinct Values\n\nStatistics on the number of unique combinations of values stored in different columns improves cardinality estimation of a ����� �� operation performed on several columns.\n\nFor example, here the estimated number of possible pairs of departure and arrival airports is the square of the total number of airports; however, the actual value is much smaller, as not all the pairs are connected by direct flights:\n\n331\n\nv. ��",
      "content_length": 1603,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "Chapter 17 Statistics\n\n=> SELECT count(*) FROM (\n\nSELECT DISTINCT departure_airport, arrival_airport FROM flights\n\n) t;\n\ncount −−−−−−−\n\n618\n\n(1 row)\n\n=> EXPLAIN SELECT DISTINCT departure_airport, arrival_airport FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate\n\n(cost=5847.01..5955.16 rows=10816 width=8)\n\nGroup Key: departure_airport, arrival_airport −> Seq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=8)\n\n(3 rows)\n\nLet’s define and collect an extended statistic on distinct values:\n\n=> CREATE STATISTICS flights_nd(ndistinct) ON departure_airport, arrival_airport FROM flights;\n\n=> ANALYZE flights;\n\nThe cardinality estimation has improved:\n\n=> EXPLAIN SELECT DISTINCT departure_airport, arrival_airport FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate\n\n(cost=5847.01..5853.19 rows=618 width=8)\n\nGroup Key: departure_airport, arrival_airport −> Seq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=8)\n\n(3 rows)\n\nYou can view the collected statistic in the system catalog:\n\n=> SELECT n_distinct FROM pg_stats_ext WHERE statistics_name = 'flights_nd';\n\nn_distinct −−−−−−−−−−−−−−− {\"5, 6\": 618}\n\n(1 row)\n\n332",
      "content_length": 1246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "17.10 Multivariate Statistics\n\nMultivariate MCV Lists\n\nIf the distribution of values is non-uniform, it may be not enough to rely on the functional dependency alone, as the estimation accuracy will highly depend on a particular pair of values. For example, the planner underestimates the number of flights performed by Boeing ��� from Sheremetyevo airport:\n\n=> SELECT count(*) FROM flights WHERE departure_airport = 'SVO' AND aircraft_code = '733';\n\ncount −−−−−−− 2037 (1 row)\n\n=> EXPLAIN SELECT * FROM flights WHERE departure_airport = 'SVO' AND aircraft_code = '733';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..5847.00 rows=736 width=63)\n\nFilter: ((departure_airport = 'SVO'::bpchar) AND (aircraft_cod...\n\n(2 rows)\n\nIn this case,you can improve the estimation by collecting statistics on multivariate ��� lists:1\n\n=> CREATE STATISTICS flights_mcv(mcv) ON departure_airport, aircraft_code FROM flights;\n\n=> ANALYZE flights;\n\nThe new cardinality estimation is much more accurate:\n\n=> EXPLAIN SELECT * FROM flights WHERE departure_airport = 'SVO' AND aircraft_code = '733';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..5847.00 rows=1927 width=63)\n\nFilter: ((departure_airport = 'SVO'::bpchar) AND (aircraft_cod...\n\n(2 rows)\n\n1 backend/statistics/README.mcv\n\nbackend/statistics/mcv.c\n\n333\n\nv. ��",
      "content_length": 1437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "100\n\nv. ��\n\nv. ��\n\nChapter 17 Statistics\n\nTo get this estimation, the planner relies on the frequency values stored in the system catalog:\n\n=> SELECT values, frequency FROM pg_statistic_ext stx\n\nJOIN pg_statistic_ext_data stxd ON stx.oid = stxd.stxoid, pg_mcv_list_items(stxdmcv) m\n\nWHERE stxname = 'flights_mcv' AND values = '{SVO,773}';\n\nvalues\n\n|\n\nfrequency\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−− {SVO,773} | 0.005266666666666667\n\n(1 row)\n\ndefault_statistics_target values Just like a regular ��� list, a multivariate list holds (if this parameter is also set at the column level, the largest of its values is used).\n\nIf required, you can also change the size of the list, expression statistics:\n\nlike it is done for extended\n\nALTER STATISTICS ... SET STATISTICS ...;\n\nIn all these examples, I have used only two columns, but you can collect multivari- ate statistics on a larger number of columns too.\n\nTo combine statistics of several types in one object, you can provide a comma- separated list of these types in its definition. If no type is specified, Postgre��� will collect statistics of all the possible types for the specified columns.\n\nApart from the actual column names, expressions, just like expression statistics.\n\nmultivariate statistics can also use arbitrary\n\n334",
      "content_length": 1280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "18\n\nTable Access Methods\n\n18.1 Pluggable Storage Engines\n\nThe data layout used by Postgre��� is neither the only possible nor the best one for you to create all load types. Following the idea of extensibility, Postgre��� allows and plug in various table access methods (pluggable storage engines), but there is only one available out of the box at the moment:\n\n=> SELECT amname, amhandler FROM pg_am WHERE amtype = 't';\n\namname |\n\namhandler\n\n−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−\n\nheap (1 row)\n\n| heap_tableam_handler\n\nYou can specify the engine to use when creating a table (������ ����� ... �����); default_table_access_method parameter otherwise, the default engine listed in the will be applied.\n\nFor the Postgre��� core to work with various engines in the same way, table ac- cess methods must implement a special interface.1 The function specified in the amhandler column returns the interface structure2 that contains all the informa- tion required by the core.\n\nThe following core components can be used by all table access methods:\n\ntransaction manager, including ���� and snapshot isolation support\n\nbuffer manager\n\n1 postgresql.org/docs/14/tableam.html 2 include/access/tableam.h\n\n335\n\nv. ��\n\nheap",
      "content_length": 1202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "Chapter 18 Table Access Methods\n\n�/� subsystem\n\n�����\n\noptimizer and executor\n\nindex support\n\nThese components always remain at the disposal of the engine, even if it does not use them all.\n\nIn their turn, engines define:\n\ntuple format and data structure\n\ntable scan implementation and cost estimation\n\nimplementation of insert, delete, update, and lock operations\n\nvisibility rules\n\nvacuum and analysis procedures\n\nHistorically,Postgre��� used a single built-in data storage without any proper pro- gramming interface,so now it is very hard to come up with a good design that takes all the specifics of the standard engine into account and does not interfere with other methods.\n\nFor example,it is still unclear how to deal with the ���. New access methods may need to log their own operations that the core is unaware of. The existing generic ��� mechanism1 is usually a bad choice, as it incurs too much overhead. You can add yet another interface for handling new types of ��� entries, but then crash recovery will depend on external code, which is highly undesirable. The only plausible solution so far is patching the core for each particular engine.\n\nFor this reason, I did not strive to provide any strict distinction between table ac- cess methods and the core. Many features described in the previous parts of the book formally belong to the heap access method rather than to the core itself. This method is likely to always remain the ultimate standard engine for Postgre���, while other methods will fill separate niches to address challenges of specific load types.\n\n1 postgresql.org/docs/14/generic-wal.html\n\n336",
      "content_length": 1626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "18.2 Sequential Scans\n\nOf all the new engines that are currently being developed, I would like to mention the following:\n\nZheap is aimed at fighting table bloating.1 It implements in-place row updates and moves historic ����-related data into a separate undo storage. Such an engine will be useful for loads that involve frequent data updates.\n\nZheap architecture will seem familiar to Oracle users, although it does have some nuances (for example, the interface of index access methods does not allow creating indexes with their own versioning).\n\nZedstore implements columnar storage,2 which is likely to be most efficient with\n\n���� queries.\n\nThe stored data is structured as a �-tree of tuple ��s; each column is stored in its own �-tree associated with the main one. In the future,it might be possible to store several columns in one �-tree, thus getting a hybrid storage.\n\n18.2 Sequential Scans\n\nThe storage engine defines the physical layout of table data and provides an access method to it. The only supported method is a sequential scan, which reads the file (or files) of the table’s main fork in full. In each read page, the visibility of each tuple is checked; those tuples that do not satisfy the query are filtered out.\n\ntable page\n\na tuple to be filtered out\n\n1 github.com/EnterpriseDB/zheap 2 github.com/greenplum-db/postgres/tree/zedstore\n\n337\n\np. ���\n\np. ��",
      "content_length": 1375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "p. ���\n\np. ���\n\nChapter 18 Table Access Methods\n\nA scanning process goes through the buffer cache; to ensure that large tables do not oust useful data, a small-sized buffer ring is employed. Other processes that are scanning the same table join this buffer ring, thus avoiding extra disk reads; such scans are called synchronized. Thus, scanning does not always have to begin at the start of the file.\n\nSequential scanning is the most efficient way to read the whole table or the best part of it. In other words, sequential scans bring the most value when the selec- tivity is low. (If the selectivity is high, meaning that the query has to select only a .) few rows, it is preferable to use an index\n\nCost Estimation\n\nIn the query execution plan,a sequential scan is represented by the Seq Scan node:\n\n=> EXPLAIN SELECT * FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=63)\n\n(1 row)\n\nThe estimated number of rows is provided as part of the basic statistics:\n\n=> SELECT reltuples FROM pg_class WHERE relname = 'flights';\n\nreltuples −−−−−−−−−−−\n\n214867\n\n(1 row)\n\nWhen estimating the cost, the optimizer takes the following two components into account: disk �/� and ��� resources.1\n\nI/� cost is calculated by multiplying the number of pages in a table and the cost of reading a single page assuming that pages are being read sequentially. When\n\n1 backend/optimizer/path/costsize.c, cost_seqscan function\n\n338",
      "content_length": 1509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "18.2 Sequential Scans\n\nthe buffer manager requests a page, the operating system actually reads more data from disk, so several subsequent pages are highly likely to be found in the operating system cache. For this reason, the cost of reading a single page seq_page_cost) is using sequential scanning (which the planner estimates at lower than the random access cost (defined by the\n\nrandom_page_cost value).\n\nThe default settings work well for ���s; if you are using ���s, it makes sense to significantly reduce the random_page_cost value (the seq_page_cost param- eter is usually left as is, serving as a reference value). Since the optimal ratio between these parameters depends on the hardware, they are usually set at the tablespace level (����� ���������� ... ���).\n\n=> SELECT relpages,\n\ncurrent_setting('seq_page_cost') AS seq_page_cost, relpages * current_setting('seq_page_cost')::real AS total\n\nFROM pg_class WHERE relname = 'flights';\n\nrelpages | seq_page_cost | total −−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−\n\n2624 | 1\n\n|\n\n2624\n\n(1 row)\n\nThese calculations clearly show the consequences of table bloating caused by untimely vacuuming: the larger the main fork of the table, the more pages have to be scanned, regardless of the number of live tuples they contain.\n\nC�� resource estimation comprises the costs of processing each tuple (which the\n\nplanner estimates at\n\ncpu_tuple_cost):\n\n=> SELECT reltuples,\n\ncurrent_setting('cpu_tuple_cost') AS cpu_tuple_cost, reltuples * current_setting('cpu_tuple_cost')::real AS total\n\nFROM pg_class WHERE relname = 'flights';\n\nreltuples | cpu_tuple_cost |\n\ntotal\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−−−\n\n214867 | 0.01\n\n| 2148.67\n\n(1 row)\n\nThe sum of these two estimates represents the total cost of the plan. The startup cost is zero because sequential scans have no prerequisites.\n\n339\n\n1 4\n\np. ���\n\n0.01",
      "content_length": 1847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "p. ���\n\n0.0025\n\nChapter 18 Table Access Methods\n\nIf the scanned table needs to be filtered, the applied filter conditions appear in the plan under the Filter section of the Seq Scan node. The estimated row count depends on the selectivity of these conditions, while the cost estimation includes the related computation expenses.\n\nThe ������� ������� command displays both the actual number of returned rows and the number of rows that have been filtered out:\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT * FROM flights WHERE status = 'Scheduled';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..5309.84 rows=15383 width=63) (actual rows=15383 loops=1) Filter: ((status)::text = 'Scheduled'::text) Rows Removed by Filter: 199484\n\n(5 rows)\n\nLet’s take a look at a more complex execution plan that uses aggregation:\n\n=> EXPLAIN SELECT count(*) FROM seats;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nAggregate\n\n(cost=24.74..24.75 rows=1 width=8)\n\n−> Seq Scan on seats\n\n(cost=0.00..21.39 rows=1339 width=0)\n\n(2 rows)\n\nThe plan consists of two nodes: the upper node (Aggregate), which computes the count function, pulls the data from the lower node (Seq Scan), which scans the table.\n\nThe startup cost of the Aggregate node includes the aggregation itself: it is im- possible to return the first row (which is the only one in this case) without getting all the rows from the lower node. The aggregation cost is estimated based on the cpu_operator_cost) for each execution cost of a conditional operation (estimated at input row:1\n\n1 backend/optimizer/path/costsize.c, cost_agg function\n\n340",
      "content_length": 1673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "18.2 Sequential Scans\n\n=> SELECT reltuples,\n\ncurrent_setting('cpu_operator_cost') AS cpu_operator_cost, round((\n\nreltuples * current_setting('cpu_operator_cost')::real\n\n)::numeric, 2) AS cpu_cost\n\nFROM pg_class WHERE relname = 'seats';\n\nreltuples | cpu_operator_cost | cpu_cost −−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−\n\n1339 | 0.0025\n\n|\n\n3.35\n\n(1 row)\n\nThe received estimate is added to the total cost of the Seq Scan node.\n\nThe total cost of the Aggregate node also includes the cost of processing a row to be returned, which is estimated at\n\ncpu_tuple_cost:\n\n=> WITH t(cpu_cost) AS (\n\nSELECT round((\n\nreltuples * current_setting('cpu_operator_cost')::real\n\n)::numeric, 2) FROM pg_class WHERE relname = 'seats'\n\n) SELECT 21.39 + t.cpu_cost AS startup_cost,\n\nround((\n\n21.39 + t.cpu_cost + 1 * current_setting('cpu_tuple_cost')::real\n\n)::numeric, 2) AS total_cost\n\nFROM t;\n\nstartup_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n24.74 |\n\n24.75\n\n(1 row)\n\nThus, cost estimation dependencies can be pictured as follows:\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nAggregate\n\n(cost=24.74..24.75 rows=1 width=8) −> Seq Scan on seats\n\n(cost=0.00..21.39 rows=1339 width=0)\n\n(4 rows)\n\n341\n\n0.01",
      "content_length": 1201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "v. �.�\n\nv. �� on\n\nChapter 18 Table Access Methods\n\n18.3 Parallel Plans\n\nparallel query execution.1 The leading process that performs Postgre��� supports the query spawns (via postmaster) several worker processes that execute one and the same parallel part of the plan simultaneously. The results are passed to the leader, which puts them together in the Gather2 node. When not accepting the data, the leader may also participate in the execution of the parallel part of the plan.\n\nIf required, by turning off the\n\nyou can forbid the leader’s contributions to the parallel plan execution\n\nparallel_leader_participation parameter.\n\nsequential part of the plan\n\nGather\n\nparallel part of the plan\n\nparallel part of the plan\n\nparallel part of the plan\n\nworker\n\nleader\n\nworker\n\nNaturally, starting these processes and sending data between them is not free, so not all queries by far should be parallelized.\n\nBesides, not all parts of the plan can be processed concurrently, even if parallel execution is allowed. Some of the operations are performed by the leader alone,in the sequential mode.\n\n1 postgresql.org/docs/14/parallel-query.html backend/access/transam/README.parallel\n\n2 backend/executor/nodeGather.c\n\n342",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "18.4 Parallel Sequential Scans\n\nPostgre��� does not support the other approach to parallel plan execution, which con- sists in performing data processing by several workers that virtually form an assembly line (roughly speaking, each plan node is performed by a separate process); this mechanism was deemed inefficient by Postgre��� developers.\n\n18.4 Parallel Sequential Scans\n\nOne of the nodes designed for parallel processing is the Parallel Seq Scan node, which performs a parallel sequential scan.\n\nThe name sounds a bit controversial (is the scan sequential or parallel after all?), butnevertheless,it reflectstheessenceof the operation. If wetakea look at the file access, table pages are read sequentially, following the order in which they would have been read by a simple sequential scan. However, this operation is performed by several concurrent processes. To avoid scanning one and the same page twice, the executor synchronizes these processes via shared memory.\n\nA subtle aspect here is that the operating system does not get the big picture typi- cal of sequential scanning; instead, it sees several processes that perform random reads. Therefore,dataprefetchingthatusuallyspeedsupsequentialscansbecomes virtually useless. To minimize this unpleasant effect, Postgre��� assigns each pro- cess not just one but several consecutive pages to read.1\n\nAs such, parallel scanning does not make much sense because the usual read costs are further increased by the overhead incurred by data transfer from process to pro- cess. However, if workers perform any post-processing on the fetched rows (such as aggregation), the total execution time may turn out to be much shorter.\n\nCost Estimation\n\nLet’s take a look at a simple query that performs aggregation on a large table. The execution plan is parallelized:\n\n1 backend/access/heap/heapam.c,\n\ntable_block_parallelscan_startblock_init & table_block_parallel-\n\nscan_nextpage functions\n\n343\n\nv. ��",
      "content_length": 1952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "Chapter 18 Table Access Methods\n\n=> EXPLAIN SELECT count(*) FROM bookings;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n(cost=25442.58..25442.59 rows=1 width=8)\n\n−> Gather\n\n(cost=25442.36..25442.57 rows=2 width=8)\n\nWorkers Planned: 2 −> Partial Aggregate\n\n(cost=24442.36..24442.37 rows=1 width=8) −> Parallel Seq Scan on bookings\n\n(cost=0.00..22243.29 rows=879629 width=0)\n\n(7 rows)\n\nAllthenodesbelowGatherbelongtotheparallelpartoftheplan. Theyareexecuted by each of the workers (two of them are planned here) and possibly by the leader process (unless this functionality is turned off by the parallel_leader_participation parameter). The Gather node itself and all the nodes above it make the sequential part of the plan and are executed by the leader process alone.\n\nThe Parallel Seq Scan node represents a parallel heap scan. The rows field shows the estimated average number of rows to be processed by a single process. All in all, the execution must be performed by three processes (one leader and two workers), but the leader process will handle fewer rows: its share gets smaller as the number of workers grows.1 In this particular case, the factor is �.�.\n\n=> SELECT reltuples::numeric, round(reltuples / 2.4) AS per_process FROM pg_class WHERE relname = 'bookings';\n\nreltuples | per_process −−−−−−−−−−−+−−−−−−−−−−−−−\n\n2111110 |\n\n879629\n\n(1 row)\n\nThe Parallel Seq Scan cost is calculated similar to that of a sequential scan. The re- ceived value is smaller,as each process handles fewer rows; the �/� part is included in full since the whole table still has to be read, page by page:\n\n=> SELECT round((\n\nrelpages reltuples / 2.4 * current_setting('cpu_tuple_cost')::real\n\ncurrent_setting('seq_page_cost')::real +\n\n)::numeric, 2) FROM pg_class WHERE relname = 'bookings';\n\n1 backend/optimizer/path/costsize.c, get_parallel_divisor function\n\n344",
      "content_length": 1903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "18.4 Parallel Sequential Scans\n\nround −−−−−−−−−− 22243.29\n\n(1 row)\n\nNext, the Partial Aggregate node performs aggregation of the fetched data; in this particular case, it counts the number of rows.\n\nThe aggregation cost is estimated in the usual manner and is added to the cost estimation of the table scan:\n\n=> WITH t(startup_cost) AS (\n\nSELECT 22243.29 + round((\n\nreltuples / 2.4 * current_setting('cpu_operator_cost')::real\n\n)::numeric, 2) FROM pg_class WHERE relname = 'bookings'\n\n) SELECT startup_cost,\n\nstartup_cost + round((\n\n1 * current_setting('cpu_tuple_cost')::real\n\n)::numeric, 2) AS total_cost\n\nFROM t;\n\nstartup_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n24442.36 |\n\n24442.37\n\n(1 row)\n\nThe next node (Gather) is executed by the leader process. This node is responsible for launching workers and gathering the data they return.\n\nFor the purpose of planning, the cost estimation of starting processes (regardless parallel_setup_cost parameter, while the cost of of their number) is defined by the each row transfer between the processes is estimated at\n\nparallel_tuple_cost.\n\nIn this example, the startup cost (spent on starting the processes) prevails; this value is added to the startup cost of the Partial Aggregate node. The total cost also includes the cost of transferring two rows; this value is added to the total cost of the Partial Aggregate node:1\n\n1 backend/optimizer/path/costsize.c, cost_gather function\n\n345\n\n1000 0.1",
      "content_length": 1447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "Chapter 18 Table Access Methods\n\n=> SELECT\n\n24442.36 + round(\n\ncurrent_setting('parallel_setup_cost')::numeric,\n\n2) AS setup_cost, 24442.37 + round(\n\ncurrent_setting('parallel_setup_cost')::numeric + 2 * current_setting('parallel_tuple_cost')::numeric,\n\n2) AS total_cost;\n\nsetup_cost | total_cost −−−−−−−−−−−−+−−−−−−−−−−−−\n\n25442.36 |\n\n25442.57\n\n(1 row)\n\nLast but not least, the Finalize Aggregate node aggregates all the partial results received by the Gather node from the parallel processes.\n\nThe final aggregation is estimated just like any other. Its startup cost is based on the cost of aggregating three rows; this value is added to the total cost of Gather (since all the rows are needed to compute the result). The total cost of Finalize Aggregate also includes the cost of returning one row.\n\n=> WITH t(startup_cost) AS ( SELECT 25442.57 + round((\n\n3 * current_setting('cpu_operator_cost')::real\n\n)::numeric, 2) FROM pg_class WHERE relname = 'bookings'\n\n) SELECT startup_cost,\n\nstartup_cost + round((\n\n1 * current_setting('cpu_tuple_cost')::real\n\n)::numeric, 2) AS total_cost\n\nFROM t;\n\nstartup_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n25442.58 |\n\n25442.59\n\n(1 row)\n\nDependencies between cost estimations are determined by whether the node has to accumulate the data before passing the result to its parent node. Aggregation cannot return the result until it gets all the input rows, so its startup cost is based onthetotal costofthelowernode. TheGathernode,onthecontrary,startssending rows upstream as soon as they are fetched. Therefore, the startup cost of this op- eration depends on the startup cost of the lower node, while its total cost is based on the lower node’s total cost.\n\n346",
      "content_length": 1705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "18.5 Parallel Execution Limitations\n\nHere is the dependency graph:\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n(cost=25442.58..25442.59 rows=1 width=8) −> Gather\n\n(cost=25442.36..25442.57 rows=2 width=8) Workers Planned: 2 −> Partial Aggregate\n\n(cost=24442.36..24442.37 rows=1 width=8) −> Parallel Seq Scan on bookings\n\n(cost=0.00..22243.29 rows=879629 width=0)\n\n(9 rows)\n\n18.5 Parallel Execution Limitations\n\nNumber of Background Workers\n\nThe number of processes is controlled by a hierarchy of three parameters. The maximal number of background workers running concurrently is defined by the max_worker_processes value.\n\nHowever,parallel query execution is not the only operation that needs background workers. For example, they also participate in logical replication and can be used by extensions. The number of processes allocated specifically for parallel plan ex- ecution is limited to the\n\nmax_parallel_workers value.\n\nOut of this number,up to leader.\n\nmax_parallel_workers_per_gather processes can serve one\n\nThe choice of these parameter values depends on the following factors:\n\nHardware capabilities: the system must have free cores dedicated to parallel\n\nexecution.\n\nTable sizes: the database must contain large tables.\n\nA typical load: there must be queries that potentially benefit from parallel\n\nexecution.\n\n347\n\n8\n\n8\n\n2",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "8MB\n\nChapter 18 Table Access Methods\n\nThese criteria are typically met by ���� systems rather than ���� ones.\n\nThe planner will not consider parallel execution at all if the estimated volume of heap data to be read does not exceed the\n\nmin_parallel_table_scan_size value.\n\nUnless the number of processes for a particular table is explicitly specified in the parallel_workers storage parameter, it will be calculated by the following formula:\n\n1 +\n\n⌊\n\nlog3 (\n\ntable size\n\nmin_parallel_table_scan_size)⌋\n\nIt means that each time a table grows three times, Postgre��� assigns one more parallel worker for its processing. The default settings give us these figures:\n\ntable, ��\n\nnumber of processes\n\n8 �� �� ��� ��� ����\n\n� � � � � �\n\nIn any case, the number of parallel workers cannot exceed the limit defined by the max_parallel_workers_per_gather parameter.\n\nIf we query a small table of �� ��, only one worker will be planned and launched:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=2 loops=1)\n\nWorkers Planned: 1 Workers Launched: 1 −> Partial Aggregate (actual rows=1 loops=2)\n\n−> Parallel Seq Scan on flights (actual rows=107434 lo...\n\n(6 rows)\n\n348",
      "content_length": 1346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "18.5 Parallel Execution Limitations\n\nA query on a table of ��� �� gets only two processes because it hits the limit of max_parallel_workers_per_gather workers:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM bookings;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=3 loops=1)\n\nWorkers Planned: 2 Workers Launched: 2 −> Partial Aggregate (actual rows=1 loops=3)\n\n−> Parallel Seq Scan on bookings (actual rows=703703 l...\n\n(6 rows)\n\nIf we remove this limit, we will get the estimated three processes:\n\n=> ALTER SYSTEM SET max_parallel_workers_per_gather = 4;\n\n=> SELECT pg_reload_conf();\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM bookings;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=4 loops=1)\n\nWorkers Planned: 3 Workers Launched: 3 −> Partial Aggregate (actual rows=1 loops=4)\n\n−> Parallel Seq Scan on bookings (actual rows=527778 l...\n\n(6 rows)\n\nIf the number of slots that are free during query execution turns out to be smaller than the planned value, only the available number of workers will be launched.\n\nLet’s limit the total number of parallel processes to five and run two queries simul- taneously:\n\n=> ALTER SYSTEM SET max_parallel_workers = 5;\n\n=> SELECT pg_reload_conf();\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM bookings;\n\n349\n\n2",
      "content_length": 1558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "Chapter 18 Table Access Methods\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM bookings;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=3 loops=1)\n\nWorkers Planned: 3 Workers Launched: 2 −> Partial Aggregate (actual rows=1 loops=3)\n\n−> Parallel Seq Scan on bookings (actual rows=7037...\n\n(6 rows)\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=4 loops=1)\n\nWorkers Planned: 3 Workers Launched: 3 −> Partial Aggregate (actual rows=1 loops=4)\n\n−> Parallel Seq Scan on bookings (actual rows=527778 l...\n\n(6 rows)\n\nAlthough three processes were expected in both cases,one of the queries managed to get only two slots.\n\nLet’s restore the default settings:\n\n=> ALTER SYSTEM RESET ALL;\n\n=> SELECT pg_reload_conf();\n\nNon-Parallelizable Queries\n\nNot all queries can be parallelized.1 In particular,parallel plans cannot be used for the following query types:\n\nQueries that modify or lock data (������, ������, ������ ��� ������, and the\n\nlike).\n\n1 postgresql.org/docs/14/when-can-parallel-query-be-used.html\n\n350",
      "content_length": 1236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "18.5 Parallel Execution Limitations\n\nThis restriction does not apply to subqueries within the following commands:\n\n– ������ ����� ��,\n\n������ ����, ������ ������������ ����\n\n– ������� ������������ ����\n\nHowever, row insertion is still performed sequentially in all these cases.\n\nQueries that can be paused. It applies to queries run within cursors,including\n\n��� loops in ��/pg���.\n\nQueries that call �������� ������ functions. By default, these are all user- defined functions and a few standard ones. You can get the full list of unsafe functions by querying the system catalog:\n\nSELECT * FROM pg_proc WHERE proparallel = 'u';\n\nQuerieswithinfunctionsifthesefunctionsarecalledfromaparallelizedquery\n\n(to avoid recursive growth of the number of workers).\n\nSome of these limitations may be removed in the future versions of Postgre���. the ability to parallelize queries at the Serializable isolation level is For example, already there.\n\nParallel insertion of rows using such commands as ������ and ���� is currently under development.1\n\nA query may remain unparallelized for several reasons:\n\nThis type of a query does not support parallelization at all.\n\nParallel plan usage is forbidden by the server configuration (for example, be-\n\ncause of the imposed table size limit).\n\nA parallel plan is more expensive than a sequential one.\n\nTo check whether a query can be parallelized at all, you can temporarily switch force_parallel_mode parameter. Then the planner will build parallel plans on the whenever possible:\n\n1 commitfest.postgresql.org/32/2844 commitfest.postgresql.org/32/2841 commitfest.postgresql.org/32/2610\n\n351\n\nv. ��\n\nv. ��\n\nv. ��\n\noff",
      "content_length": 1651,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "Chapter 18 Table Access Methods\n\n=> EXPLAIN SELECT * FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=63)\n\n(1 row)\n\n=> SET force_parallel_mode = on;\n\n=> EXPLAIN SELECT * FROM flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nGather\n\n(cost=1000.00..27259.37 rows=214867 width=63)\n\nWorkers Planned: 1 Single Copy: true −> Seq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=63)\n\n(4 rows)\n\nParallel Restricted Queries\n\nThe bigger is the parallel part of the plan, the more performance gains can be po- tentially achieved. However, certain operations are executed strictly sequentially bytheleaderprocessalone,1 eventhoughtheydonotinterferewithparallelization as such. In other words,they cannot appear in the plan tree below the Gather node.\n\nNon-expandable subqueries. The most obvious example of a non-expandable sub- query2 is scanning a ��� result (represented in the plan by the CTE Scan node):\n\n=> EXPLAIN (costs off) WITH t AS MATERIALIZED ( SELECT * FROM flights\n\n) SELECT count(*) FROM t;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nAggregate CTE t\n\n−> Seq Scan on flights\n\n−> CTE Scan on t\n\n(4 rows)\n\n1 postgresql.org/docs/14/parallel-safety.html 2 backend/optimizer/plan/subselect.c\n\n352",
      "content_length": 1344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "18.5 Parallel Execution Limitations\n\nIf limitation does not apply.\n\na ��� is not materialized, the plan does not contain the CTE Scan node, so this\n\nNote,however,that a ��� itself can be computed in the parallel mode if it turns out to be less expensive:\n\n=> EXPLAIN (costs off) WITH t AS MATERIALIZED (\n\nSELECT count(*) FROM flights\n\n) SELECT * FROM t;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nCTE Scan on t\n\nCTE t\n\n−> Finalize Aggregate\n\n−> Gather\n\nWorkers Planned: 1 −> Partial Aggregate\n\n−> Parallel Seq Scan on flights\n\n(7 rows)\n\nAnother example of a non-expandable subquery is shown under by the SubPlan node in the plan below:\n\n=> EXPLAIN (costs off) SELECT * FROM flights f WHERE f.scheduled_departure > ( -- SubPlan\n\nSELECT min(f2.scheduled_departure) FROM flights f2 WHERE f2.aircraft_code = f.aircraft_code\n\n);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights f\n\nFilter: (scheduled_departure > (SubPlan 1)) SubPlan 1\n\n−> Aggregate\n\n−> Seq Scan on flights f2\n\nFilter: (aircraft_code = f.aircraft_code)\n\n(6 rows)\n\nThe first two rows represent the plan of the main query: the flights table is scanned sequentially, and each of its rows is checked against the provided filter. The filter\n\n353\n\nv. ��",
      "content_length": 1268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "Chapter 18 Table Access Methods\n\ncondition includes a subquery; the plan of this subquery starts on the third row. So theSubPlannodeisexecutedseveraltimes,onceforeach rowfetchedbysequential scanning in this case.\n\nThe upper Seq Scan node of this plan cannot participate in parallel execution be- cause it relies on the data returned by the SubPlan node.\n\nLast but not least, here is one more non-expandable subquery represented by the InitPlan node:\n\n=> EXPLAIN (costs off) SELECT * FROM flights f WHERE f.scheduled_departure > ( -- SubPlan\n\nSELECT min(f2.scheduled_departure) FROM flights f2 WHERE EXISTS ( -- InitPlan\n\nSELECT * FROM ticket_flights tf WHERE tf.flight_id = f.flight_id\n\n)\n\n);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights f\n\nFilter: (scheduled_departure > (SubPlan 2)) SubPlan 2\n\n−> Finalize Aggregate\n\nInitPlan 1 (returns $1)\n\n−> Seq Scan on ticket_flights tf\n\nFilter: (flight_id = f.flight_id)\n\n−> Gather\n\nWorkers Planned: 1 Params Evaluated: $1 −> Partial Aggregate\n\n−> Result\n\nOne−Time Filter: $1 −> Parallel Seq Scan on flights f2\n\n(14 rows)\n\nUnlike the SubPlan node,InitPlan is evaluatedonly once (in this particular example, once per each execution of the SubPlan 2 node).\n\nThe parent node of InitPlan cannot participate in parallel execution (but those nodes that receive the result of the InitPlan evaluation can, like in this example).\n\n354",
      "content_length": 1411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "18.5 Parallel Execution Limitations\n\nTemporary tables. Temporary tables do not support parallel scanning,as they can be accessed exclusively by the process that has created them. Their pages are pro- buffer cache. Making the local cache accessible to several pro- cessed in the local cesses would require a locking mechanism like in the shared cache, which would make its other benefits less prominent.\n\n=> CREATE TEMPORARY TABLE flights_tmp AS SELECT * FROM flights;\n\n=> EXPLAIN (costs off) SELECT count(*) FROM flights_tmp;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nAggregate\n\n−> Seq Scan on flights_tmp\n\n(2 rows)\n\nParallel restricted functions. Functions defined as �������� ���������� are allowed only in the sequential part of the plan. You can get the list of such functions from the system catalog by running the following query:\n\nSELECT * FROM pg_proc WHERE proparallel = 'r';\n\nOnly label your functions as �������� ���������� (to say nothing of �������� ����) if you are fully aware of all the implications and have carefully studied all the im- posed restrictions.1\n\n1 postgresql.org/docs/14/parallel-safety#PARALLEL-LABELING.html\n\n355\n\np. ��� p. ���",
      "content_length": 1162,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "v. �.�\n\np. ���\n\n19\n\nIndex Access Methods\n\n19.1 Indexes and Extensibility\n\nIndexes are database objects that mainly serve the purpose of accelerating data ac- cess. These are auxiliary structures: any index can be deleted and recreated based on heap data. In addition to data access speedup, indexes are also used to enforce some integrity constraints.\n\nThe Postgre��� core provides six built-in index access methods (index types):\n\n=> SELECT amname FROM pg_am WHERE amtype = 'i';\n\namname −−−−−−−− btree hash gist gin spgist brin\n\n(6 rows)\n\nthat new access methods can be added without Postgre���’s extensibility implies modifying the core. One such extension (the bloom method) is included into the standard set of modules.\n\nDespite all the differences between various index types, all of them eventually match a key (such as a value of an indexed column) against heap tuples that con- tain this key. Tuples are referred to by six-byte tuple ��s, or ���s. Knowing the key or some information about the key,it is possible to quickly read the tuples that are likely to contain the required data without scanning the whole table.\n\n356",
      "content_length": 1131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "19.1 Indexes and Extensibility\n\nTo ensure that a new access method can be added as an extension, Postgre��� im- plements a common indexing engine. Its main objective is to retrieve and process ���s returned by a particular access method:\n\nread data from the corresponding heap tuples\n\ncheck tuple visibility\n\nagainst a particular snapshot\n\nrecheck conditions if their evaluation by the method is indecisive\n\nThe indexing engine also participates in execution of plans built at the optimiza- tion stage. When assessing various execution paths, the optimizer needs to know the properties of all potentially applicable access methods: can the method return the data in the required order,or do we need a separate sorting stage? is it possible to return several first values right away, or do we have to wait for the whole result set to be fetched? and so on.\n\nItisnotonlytheoptimizerthatneedstoknowspecificsoftheaccessmethod. Index creation poses more questions to answer: does the access method support multi- column indexes? can this index guarantee uniqueness?\n\nThe indexing engine allows using a variety of access methods; in order to be sup- ported, an access method must implement a particular interface to declare its fea- tures and properties.\n\nAccess methods are used to address the following tasks:\n\nimplement algorithms for building indexes, as well as inserting and deleting\n\nindex entries\n\ndistribute index entries between pages (to be further handled by the buffer\n\ncache manager\n\n)\n\nimplement the algorithm of vacuuming\n\nacquire locks\n\nto ensure correct concurrent operation\n\ngenerate ��� entries\n\nsearch indexed data by the key\n\nestimate index scan costs\n\n357\n\np. ��\n\np. ���\n\np. ���\n\np. ���\n\np. ���",
      "content_length": 1711,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "Chapter 19 Index Access Methods\n\nExtensibility also manifests itself as the ability to add new data types, which the access method knows nothing of in advance. Therefore, access methods have to define their own interfaces for plugging in arbitrary data types.\n\nTo enable usage of a new data type with a particular access method,you have to im- plement the corresponding interface—that is, provide operators that can be used with an index, and possibly some auxiliary support functions. Such a set of opera- tors and functions is called an operator class.\n\nThe indexing logic is partially implemented by the access method itself, but some of it is outsourced to operator classes. This distribution is rather arbitrary: while �-trees have all the logic wired into the access method, some other methods may provide only the main framework, leaving all the implementation details at the discretion of particular operator classes. One and the same data type is often sup- ported by several operator classes, and the user can select the one with the most suitable behavior.\n\nHere is a small fraction of the overall picture:\n\nbool_ops\n\nboolean\n\nint4_ops\n\ninteger\n\nbtree\n\ntext_ops\n\ntext\n\nIndexing engine\n\ntext_pattern_ops\n\ngist_int4_ops\n\ngist\n\ngist_text_ops\n\npoint_ops\n\npoint\n\naccess methods\n\noperator classes\n\ndata types\n\n358",
      "content_length": 1318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "19.2 Operator Classes and Families\n\n19.2 Operator Classes and Families\n\nOperator Classes\n\nAn access method interface1 is implemented by an operator class,2 which is a set of operators and support functions applied by the access method to a particular data type.\n\nClasses of operators are stored in the pg_opclass table in the system catalog. The following query returns the complete data for the above illustration:\n\n=> SELECT amname, opcname, opcintype::regtype FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid;\n\namname |\n\nopcname\n\n|\n\nopcintype\n\n−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nbtree hash btree btree ... brin brin brin\n\n| array_ops | array_ops | bit_ops | bool_ops\n\n| pg_lsn_minmax_multi_ops | pg_lsn_bloom_ops | box_inclusion_ops\n\n| anyarray | anyarray | bit | boolean\n\n| pg_lsn | pg_lsn | box\n\n(177 rows)\n\nIn most cases,we do not have to know anything about operator classes. We simply create an index that uses some operator class by default.\n\nFor example,here are �-tree operator classes that support the text type. One of the classes is always marked as the default one:\n\n=> SELECT opcname, opcdefault FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid\n\nWHERE amname = 'btree'\n\nAND opcintype = 'text'::regtype;\n\n1 postgresql.org/docs/14/xindex.html 2 postgresql.org/docs/14/indexes-opclass.html\n\n359",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "Chapter 19 Index Access Methods\n\nopcname\n\n| opcdefault\n\n−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n| t text_ops | f varchar_ops text_pattern_ops | f varchar_pattern_ops | f\n\n(4 rows)\n\nA typical command for index creation looks as follows:\n\nCREATE INDEX ON aircrafts(model, range);\n\nBut it is just a shorthand notation that expands to the following syntax:\n\nCREATE INDEX ON aircrafts USING btree -- the default access method (\n\nmodel text_ops, -- the default operator class for text range int4_ops -- the default operator class for integer\n\n);\n\nIf you would like to use an index of a different type or achieve some custom behav- ior, you have to specify the desired access method or operator class explicitly.\n\nEach operator class defined for a particular access method and data type must contain a set of operators that take parameters of this type and implement the semantics of this access method.\n\nFor example, the btree access method defines five mandatory comparison opera- tors. Any btree operator class must contain all the five:\n\n=> SELECT opcname, amopstrategy, amopopr::regoperator FROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_opclass opc ON opcfamily = opf.oid JOIN pg_amop amop ON amopfamily = opcfamily\n\nWHERE amname = 'btree'\n\nAND opcname IN ('text_ops', 'text_pattern_ops') AND amoplefttype = 'text'::regtype AND amoprighttype = 'text'::regtype\n\nORDER BY opcname, amopstrategy;\n\n360",
      "content_length": 1411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "19.2 Operator Classes and Families\n\n| amopstrategy | −−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−\n\nopcname\n\namopopr\n\n| text_ops | text_ops | text_ops | text_ops text_ops | text_pattern_ops | text_pattern_ops | text_pattern_ops | text_pattern_ops | text_pattern_ops |\n\n1 | <(text,text) 2 | <=(text,text) 3 | =(text,text) 4 | >=(text,text) 5 | >(text,text) 1 | ~<~(text,text) 2 | ~<=~(text,text) 3 | =(text,text) 4 | ~>=~(text,text) 5 | ~>~(text,text)\n\n(10 rows)\n\nThe semantics of an operator implied by the access method is reflected by the strat- egy number shown as amopstrategy.1 For example, strategy � for btree means less than, � denotes less than or equal to, and so on. Operators themselves can have arbitrary names.\n\nThe example above shows two kinds of operators. The difference between regular operators and those with a tilde is that the latter do not take collation2 into account and perform bitwise comparison of strings. Nevertheless, both flavors implement the same logical operations of comparison.\n\nThe text_pattern_ops operator class is designed to address the limitation in support ofthe~~operator(whichcorrespondstothe����operator). Inadatabaseusingany collation other than C, this operator cannot use a regular index on a text field:\n\n=> SHOW lc_collate;\n\nlc_collate −−−−−−−−−−−−− en_US.UTF−8\n\n(1 row)\n\n=> CREATE INDEX ON tickets(passenger_name);\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE passenger_name LIKE 'ELENA%';\n\n1 postgresql.org/docs/14/xindex#XINDEX-STRATEGIES.html 2 postgresql.org/docs/14/collation.html\n\npostgresql.org/docs/14/indexes-collations.html\n\n361",
      "content_length": 1605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "Chapter 19 Index Access Methods\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on tickets\n\nFilter: (passenger_name ~~ 'ELENA%'::text)\n\n(2 rows)\n\nAn index with the text_pattern_ops operator class behaves differently:\n\n=> CREATE INDEX tickets_passenger_name_pattern_idx ON tickets(passenger_name text_pattern_ops);\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE passenger_name LIKE 'ELENA%';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on tickets\n\nFilter: (passenger_name ~~ 'ELENA%'::text) −> Bitmap Index Scan on tickets_passenger_name_pattern_idx\n\nIndex Cond: ((passenger_name ~>=~ 'ELENA'::text) AND (passenger_name ~<~ 'ELENB'::text))\n\n(5 rows)\n\nNotehowthefilterexpressionhaschangedintheIndexCondcondition. Thesearch now uses only the template’s prefix before %, while false-positive hits are filtered out during a recheck based on the Filter condition. The operator class for the btree access method does not provide an operator for comparing templates,and the only way to apply a �-tree here is to rewrite this condition using comparison opera- tors. The operators of the text_pattern_ops class do not take collation into account, which gives us an opportunity to use an equivalent condition instead.1\n\nAn index can be used to speed up access by a filter condition if the following two prerequisites are met:\n\n� the condition is written as “indexed-column operator expression” (if the oper- ator has a commuting counterpart specified,2 the condition can also have the form of “expression operator indexed-column”)3\n\n� and the operator belongs to the operator class specified for the indexed-column\n\nin the index declaration.\n\n1 backend/utils/adt/like_support.c 2 postgresql.org/docs/14/xoper-optimization#id-1.8.3.18.6.html 3 backend/optimizer/path/indxpath.c, match_clause_to_indexcol function\n\n362",
      "content_length": 1880,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "19.2 Operator Classes and Families\n\nFor example, the following query can use an index:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE 'ELENA BELOVA' = passenger_name;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using tickets_passenger_name_idx on tickets Index Cond: (passenger_name = 'ELENA BELOVA'::text)\n\n(2 rows)\n\nNote the position of arguments in the IndexCond condition: at the execution stage, the indexed field must be on the left. When the arguments are permuted,the oper- ator is replaced by a commuting one; in this particular case,it is the same operator because the equality relation is commutative.\n\nIn the next query, it is technically impossible to use a regular index because the column name in the condition is replaced by a function call:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE initcap(passenger_name) = 'Elena Belova';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on tickets\n\nFilter: (initcap(passenger_name) = 'Elena Belova'::text)\n\n(2 rows)\n\nHere you can use an expression index,1 which has an arbitrary expression specified in its declaration instead of a column:\n\n=> CREATE INDEX ON tickets( (initcap(passenger_name)) );\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE initcap(passenger_name) = 'Elena Belova';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on tickets\n\nRecheck Cond: (initcap(passenger_name) = 'Elena Belova'::text) −> Bitmap Index Scan on tickets_initcap_idx\n\nIndex Cond: (initcap(passenger_name) = 'Elena Belova'::text)\n\n(4 rows)\n\nAn index expression can depend only on heap tuple values and must be affected by neither other data stored in the database nor configuration parameters (such\n\n1 postgresql.org/docs/14/indexes-expressional.html\n\n363",
      "content_length": 1836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "Chapter 19 Index Access Methods\n\nas locale settings). In other words, if the expression contains any function calls, these functions must be ���������,1 and they must observe this volatility category. Otherwise,an index scan and a heap scan may return different results for the same query.\n\nApart from regular operators, an operator class can provide support functions2 re- quired by the access method. For example, the btree access method defines five support functions;3 the first one (which compares two values) is mandatory, while all the rest can be absent:\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_opclass opc ON opcfamily = opf.oid JOIN pg_amproc amproc ON amprocfamily = opcfamily\n\nWHERE amname = 'btree'\n\nAND opcname = 'text_ops' AND amproclefttype = 'text'::regtype AND amprocrighttype = 'text'::regtype\n\nORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−− 1 | bttextcmp 2 | bttextsortsupport 4 | btvarstrequalimage\n\n(3 rows)\n\nOperator Families\n\nEach operator class always belongs to some operator family4 (listed in the system catalog in the pg_opfamily table). Afamily can comprise several classes that handle similar data types in the same way.\n\nFor example, the integer_ops family includes several classes for integral data types that have the same semantics but differ in size:\n\n1 postgresql.org/docs/14/xfunc-volatility.html 2 postgresql.org/docs/14/xindex#XINDEX-SUPPORT.html 3 postgresql.org/docs/14/btree-support-funcs.html 4 postgresql.org/docs/14/xindex#XINDEX-OPFAMILY.html\n\n364",
      "content_length": 1586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "19.2 Operator Classes and Families\n\n=> SELECT opcname, opcintype::regtype FROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_opclass opc ON opcfamily = opf.oid\n\nWHERE amname = 'btree'\n\nAND opfname = 'integer_ops';\n\nopcname\n\n| opcintype\n\n−−−−−−−−−−+−−−−−−−−−−− int2_ops | smallint int4_ops | integer int8_ops | bigint\n\n(3 rows)\n\nThe datetime_ops family comprises operator classes that process dates:\n\n=> SELECT opcname, opcintype::regtype FROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_opclass opc ON opcfamily = opf.oid\n\nWHERE amname = 'btree'\n\nAND opfname = 'datetime_ops';\n\nopcname\n\n|\n\nopcintype\n\n−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\ndate_ops timestamptz_ops | timestamp with time zone timestamp_ops\n\n| date\n\n| timestamp without time zone\n\n(3 rows)\n\nWhile each operator class supports a single data type, a family can comprise oper- ator classes for different data types:\n\n=> SELECT opcname, amopopr::regoperator FROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_opclass opc ON opcfamily = opf.oid JOIN pg_amop amop ON amopfamily = opcfamily\n\nWHERE amname = 'btree'\n\nAND opfname = 'integer_ops' AND amoplefttype = 'integer'::regtype AND amopstrategy = 1\n\nORDER BY opcname;\n\n365",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "v. �.�\n\nChapter 19 Index Access Methods\n\nopcname\n\n|\n\namopopr\n\n−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−\n\nint2_ops | <(integer,bigint) int2_ops | <(integer,smallint) int2_ops | <(integer,integer) int4_ops | <(integer,bigint) int4_ops | <(integer,smallint) int4_ops | <(integer,integer) int8_ops | <(integer,bigint) int8_ops | <(integer,smallint) int8_ops | <(integer,integer)\n\n(9 rows)\n\nThanks to such grouping of various operators into a single family, the planner can do without type casting when an index is used for conditions involving values of different types.\n\n19.3 Indexing Engine Interface\n\nJust like tains the name of the function that implements the interface:1\n\nfor table access methods, the amhandler column of the pg_am table con-\n\n=> SELECT amname, amhandler FROM pg_am WHERE amtype = 'i';\n\namname |\n\namhandler\n\n−−−−−−−−+−−−−−−−−−−−−−\n\n| bthandler btree | hashhandler hash | gisthandler gist gin | ginhandler spgist | spghandler brin\n\n| brinhandler\n\n(6 rows)\n\nThisfunctionfillsplaceholdersintheinterfacestructure2 withactualvalues. Some of them are functions responsible for separate tasks related to index access (for example, they can perform an index scan and return heap tuple ��s), while others are index method properties that the indexing engine must be aware of.\n\n1 postgresql.org/docs/14/indexam.html 2 include/access/amapi.h\n\n366",
      "content_length": 1345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "19.3 Indexing Engine Interface\n\nAll properties are grouped into three categories:1\n\naccess method properties\n\nproperties of a particular index\n\ncolumn-level properties of an index\n\nThe distinction between access method and index-level properties is provided with a view to the future: right now,all the indexes based on a particular access method always have the same properties at these two levels.\n\nAccess Method Properties\n\nThe following five properties �-tree method here):\n\nare defined at the access method level (shown for the\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'btree';\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | t | can_order | t | can_unique | can_multi_col | t | t | can_exclude | t | can_include\n\nbtree btree btree btree btree (5 rows)\n\nC�� O���� Theabilitytoreceivesorteddata.2 Thispropertyiscurrentlysupported\n\nonly by �-trees.\n\nTo get the results in the required order,you can always scan the table and then sort the fetched data:\n\n1 backend/utils/adt/amutils.c, indexam_property function 2 postgresql.org/docs/14/indexes-ordering.html\n\n367\n\nv. ��",
      "content_length": 1279,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "Chapter 19 Index Access Methods\n\n=> EXPLAIN (costs off) SELECT * FROM seats ORDER BY seat_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\nSort Key: seat_no −> Seq Scan on seats\n\n(3 rows)\n\nBut if there is an index that supports this property, the data can be returned in the desired order at once:\n\n=> EXPLAIN (costs off) SELECT * FROM seats ORDER BY aircraft_code;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using seats_pkey on seats\n\n(1 row)\n\nC�� U����� Support for unique and primary key constraints.1 This property ap-\n\nplies only to �-trees.\n\nEach time a unique or primary key constraint is declared,Postgre��� automat- ically creates a unique index to support this constraint.\n\n=> INSERT INTO bookings(book_ref, book_date, total_amount) VALUES ('000004', now(), 100.00);\n\nERROR: \"bookings_pkey\" DETAIL:\n\nduplicate key value violates unique constraint\n\nKey (book_ref)=(000004) already exists.\n\nThat said, if you simply create a unique index without explicitly declaring an integrity constraint, the effect will seem to be exactly the same: the indexed column will not allow duplicates. So what is the difference?\n\nAn integrity constraint defines the property that must never be violated,while an index is just a mechanism to guarantee it. In theory, a constraint could be imposed using other means.\n\nFor example, Postgre��� does not support global indexes for partitioned ta- bles, but nevertheless, you can create a unique constraint on such tables (if it\n\n1 postgresql.org/docs/14/indexes-unique.html\n\n368",
      "content_length": 1530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "19.3 Indexing Engine Interface\n\nincludes the partition key). In this case, the global uniqueness is ensured by local unique indexes of each partition, as different partitions cannot have the same partition keys.\n\nC�� M���� C�� The ability to build a multicolumn index.1\n\nA multicolumn index can speed up search by several conditions imposed on different table columns. For example, the ticket_flights table has a composite primary key, so the corresponding index is built on more than one column:\n\n=> \\d ticket_flights_pkey\n\nIndex \"bookings.ticket_flights_pkey\"\n\nColumn\n\n|\n\nType\n\n| Key? | Definition\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−+−−−−−−−−−−−−\n\nticket_no | character(13) | yes | yes flight_id | integer\n\n| ticket_no | flight_id\n\nprimary key, btree, for table \"bookings.ticket_flights\"\n\nA flight search by a ticket number and a flight �� is performed using an index:\n\n=> EXPLAIN (costs off) SELECT * FROM ticket_flights WHERE ticket_no = '0005432001355'\n\nAND flight_id = 51618;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using ticket_flights_pkey on ticket_flights\n\nIndex Cond: ((ticket_no = '0005432001355'::bpchar) AND (flight_id = 51618))\n\n(3 rows)\n\nAs a rule, a multicolumn index can speed up search even if filter conditions involve only some of its columns. In the case of a �-tree, the search will be efficient if the filter condition spans a range of columns that appear first in the index declaration:\n\n=> EXPLAIN (costs off) SELECT * FROM ticket_flights WHERE ticket_no = '0005432001355';\n\n1 postgresql.org/docs/14/indexes-multicolumn.html\n\n369",
      "content_length": 1598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "v. �� p. ���\n\nChapter 19 Index Access Methods\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using ticket_flights_pkey on ticket_flights Index Cond: (ticket_no = '0005432001355'::bpchar)\n\n(2 rows)\n\nIn all other cases (for example,if the condition includes only flights_id),search will be virtually limited to the initial columns (if the query includes the cor- responding conditions), while other conditions will only be used to filter out the returned results. Indexes of other types may behave differently though.\n\nC�� E������ Support for ������� constraints.1\n\nAn ������� constraint guarantees that a condition defined by an operator will not be satisfied for any pair of table rows. To impose this constraint, Post- gre��� automatically creates an index; there must be an operator class that contains the operator used in the constraint’s condition.\n\nIt is the intersection operator && that usually serves this purpose. For in- stance, you can use it to explicitly declare that a conference room cannot be booked twice for the same time, or that buildings on a map cannot overlap.\n\nWith the equality operator, the exclusion constraint takes on the meaning of uniqueness: the table is forbidden to have two rows with the same key val- ues. Nevertheless, it is not the same as a ������ constraint: in particular, the exclusion constraint key cannot be referred to from foreign keys, and neither can it be used in the �� �������� clause.\n\nC�� I������ Theability\n\ntoaddnon-keycolumnstoanindex,whichmakethisindex\n\ncovering.\n\nUsing this property, you can extend a unique index with additional columns. Such an index still guarantees that all the key column values are unique,while data retrieval from the included columns incurs no heap access:\n\n=> CREATE UNIQUE INDEX ON flights(flight_id) INCLUDE (status);\n\n=> EXPLAIN (costs off) SELECT status FROM flights WHERE flight_id = 51618;\n\n1 postgresql.org/docs/14/ddl-constraints#DDL-CONSTRAINTS-EXCLUSION.html\n\n370",
      "content_length": 1993,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "19.3 Indexing Engine Interface\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Only Scan using flights_flight_id_status_idx on flights\n\nIndex Cond: (flight_id = 51618)\n\n(2 rows)\n\nIndex-Level Properties\n\nHere are the properties related to an index (shown for an existing one):\n\n=> FROM unnest(array[\n\nSELECT p.name, pg_index_has_property('seats_pkey', p.name)\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| t clusterable | t index_scan bitmap_scan | t backward_scan | t\n\n(4 rows)\n\nC���������� The ability to physically move heap tuples in accordance with the or-\n\nder in which their ��s are returned by an index scan.\n\nThis property shows whether the ������� command is supported.\n\nI���� S��� Index scan support.\n\nThis property implies that the access method can return ���s one by one. Strange as it may seem, some indexes do not provide this functionality.\n\nB����� S��� Bitmap scan support.\n\nThis property defines whether the access method can build and return a bitmap of all ���s at once.\n\nB������� S��� The ability to return results in reverse order as compared to the\n\none specified at index creation.\n\nThis property makes sense only if the access method supports index scans.\n\n371\n\np. ���\n\np. ���\n\np. ���",
      "content_length": 1346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "p. ���\n\np. ���\n\nChapter 19 Index Access Methods\n\nColumn-Level Properties\n\nAnd finally, let’s take a look at the column properties:\n\n=> SELECT p.name,\n\npg_index_column_has_property('seats_pkey', 1, p.name)\n\nFROM unnest(array[\n\n'asc', 'desc', 'nulls_first', 'nulls_last', 'orderable', 'distance_orderable', 'returnable', 'search_array', 'search_nulls'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n| t asc | f desc | f nulls_first | t nulls_last | t orderable distance_orderable | f | t returnable | t search_array | t search_nulls\n\n(9 rows)\n\nA��, D���, N���� F����, N���� L��� Ordering column values.\n\nThese properties define whether column values should be stored in ascending or descending order, and whether ���� values should appear before or after regular values. All these properties are applicable only to �-trees.\n\nO�������� The ability to sort column values using the ����� �� clause.\n\nThis property is applicable only to �-trees.\n\nD������� O�������� Support for ordering operators.1\n\nUnlike regular indexing operators that return logical values, ordering opera- tors return a real number that denotes the “distance” from one argument to another. Indexes support such operators specified in the ����� �� clause of a query.\n\nFor example, the ordering operator <-> can find the airports located at the shortest distance to the specified point:\n\n1 postgresql.org/docs/14/xindex#XINDEX-ORDERING-OPS.html\n\n372",
      "content_length": 1471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "19.3 Indexing Engine Interface\n\n=> CREATE INDEX ON airports_data USING gist(coordinates);\n\n=> EXPLAIN (costs off) SELECT * FROM airports ORDER BY coordinates <-> point (43.578,57.593) LIMIT 3;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nLimit\n\n−> Index Scan using airports_data_coordinates_idx on airpo... Order By: (coordinates <−> '(43.578,57.593)'::point)\n\n(3 rows)\n\nR��������� The ability to return data without accessing the table (index-only scan\n\nsupport).\n\nThis property defines whether an index structure allows retrieving indexed values. It is not always possible: for example, some indexes may store hash codes rather than actual values. In this case,the C�� I������ property will not be available either.\n\nS����� A���� Support for searching several elements in an array.\n\nAn explicit use of arrays is not the only case when it might be necessary. For example, the planner transforms the �� (list) expression into an array scan:\n\n=> EXPLAIN (costs off) SELECT * FROM bookings WHERE book_ref IN ('C7C821', 'A5D060', 'DDE1BB');\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using bookings_pkey on bookings\n\nIndex Cond: (book_ref = ANY ('{C7C821,A5D060,DDE1BB}'::bpchar[]))\n\n(3 rows)\n\nIftheindexmethoddoesnotsupportsuchoperators,theexecutormayhaveto perform several iterations to find particular values (which can make the index scan less efficient).\n\nS����� N���� Search for �� ���� and �� ��� ���� conditions.\n\nShould we index ���� values? On the one hand, it allows us to perform index scans for conditions like �� [���] ����, as well as use the index as a covering\n\n373\n\np. ���",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "Chapter 19 Index Access Methods\n\none if no filter conditions are provided (in this case, the index has to return the data of all the heap tuples, including those that contain ���� values). But on the other hand, skipping ���� values can reduce the index size.\n\nThe decision remains at the discretion of access method developers, but more often than not ���� values do get indexed.\n\nIf you do not need ���� values in an index, you can exclude them by building a partial index1 that covers only those rows that are required. For example:\n\n=> CREATE INDEX ON flights(actual_arrival) WHERE actual_arrival IS NOT NULL;\n\n=> EXPLAIN (costs off) SELECT * FROM flights WHERE actual_arrival = '2017-06-13 10:33:00+03';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using flights_actual_arrival_idx on flights\n\nIndex Cond: (actual_arrival = '2017−06−13 10:33:00+03'::ti...\n\n(2 rows)\n\nA partial index is smaller than the full one, and it gets updated only if the modified row is indexed, which can sometimes lead to tangible performance gains. Obviously, apart from ���� checks, the ����� clause can provide any condition (that can be used with immutable functions).\n\nThe ability to build partial indexes is provided by the indexing engine, so it does not depend on the access method.\n\nNaturally,the interface includes only those properties of index methods that must be known in advance for a correct decision to be taken. For example, it does not list any properties that enable such features as support for predicate locks or non- blocking index creation (������������). Such properties are defined in the code of the functions that implement the interface.\n\n1 postgresql.org/docs/14/indexes-partial.html\n\n374",
      "content_length": 1744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "20\n\nIndex Scans\n\n20.1 Regular Index Scans\n\nThere are two basic ways of accessing ���s provided by an index. The first one is to perform an index scan. Most of the index access methods (but not all of them) have the I���� S���\n\nproperty to support this operation.\n\nIndex scans are represented in the plan by the Index Scan1 node:\n\n=> EXPLAIN SELECT * FROM bookings WHERE book_ref = '9AC0C6' AND total_amount = 48500.00;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using bookings_pkey on bookings\n\n(cost=0.43..8.45 rows=1 width=21) Index Cond: (book_ref = '9AC0C6'::bpchar) Filter: (total_amount = 48500.00)\n\n(4 rows)\n\nDuring an index scan, the access method returns ���s one by one.2 Upon receiv- ing a ���, the indexing engine accesses the heap page this ��� refers to, gets the corresponding tuple, and, if the visibility rules are met, returns the requested set of fields of this tuple. This process continues until the access method runs out of ���s that matches the query.\n\nThe Index Cond line includes only those filter conditions that can be checked using an index. Other conditions that have to be rechecked against the heap are listed separately in the Filter line.\n\n1 backend/executor/nodeIndexscan.c 2 backend/access/index/indexam.c, index_getnext_tid function\n\n375\n\np. ���",
      "content_length": 1308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "p. ���\n\nChapter 20 Index Scans\n\nAs this example shows, both index and heap access operations are handled by a common Index Scan node rather by two different ones. But there is also a sepa- rate Tid Scan node,1 which fetches tuples from the heap if their ��s are known in advance:\n\n=> EXPLAIN SELECT * FROM bookings WHERE ctid = '(0,1)'::tid;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nTid Scan on bookings\n\n(cost=0.00..4.01 rows=1 width=21)\n\nTID Cond: (ctid = '(0,1)'::tid)\n\n(2 rows)\n\nCost Estimation\n\nCost estimation of an index scan comprises the estimated costs of index access operations and heap page reads.\n\nObviously,the index-related part of the estimation fully depends on the particular access method. For �-trees, the cost is mostly incurred by fetching index pages and processing their entries. The number of pages and rows to be read can be . determined by the total volume of data and the selectivity of the applied filters Index pages are accessed at random (pages that follow each other in the logical structure are physically scattered on disk). The estimation is further increased by ��� resources spent on getting from the root to the leaf node and computing all the required expressions.2\n\nThe heap-related part of the estimation includes the cost of heap page access and the ��� time required to process all the fetched tuples. It is important to note that �/� estimation depends on both the index scan selectivity and the correlation between the physical order of tuples on disk and the order in which the access method returns their ��s.\n\n1 backend/executor/nodeTidscan.c 2 backend/utils/adt/selfuncs.c, btcostestimate function postgresql.org/docs/14/index-cost-estimation.html\n\n376",
      "content_length": 1733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "20.1 Regular Index Scans\n\nGood Scenario: High Correlation\n\nIfthephysicalorderoftuplesondiskhasaperfectcorrelationwiththelogicalorder of ���s in the index, each page will be accessed only once: the Index Scan node will sequentially go from one page to another, reading the tuples one by one.\n\nheap page\n\na tuple matching filter conditions\n\nPostgre��� collects statistics on correlation\n\n:\n\n=> SELECT attname, correlation FROM pg_stats WHERE tablename = 'bookings' ORDER BY abs(correlation) DESC;\n\nattname\n\n| correlation\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−− book_ref 1 | total_amount | 0.0026738467 8.02188e−05 | book_date\n\n(3 rows)\n\nThe correlation is high if the corresponding absolute value is close to one (like in the case of book_ref); values that are close to zero are a sign of chaotic data distribution.\n\nIn this particular case,high correlation in the book_ref column is of course due to the fact that the data has been loaded into the table in ascending order based on this column, and there have been no updates yet. We would see the same picture if we executed the ������� command for the index created on this column.\n\n377\n\np. ���",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "p. ���\n\np. ���\n\n0.005\n\n0.0025\n\nChapter 20 Index Scans\n\nHowever, the perfect correlation does not guarantee that all queries will be returning re- sults in ascending order of book_ref values. First of all,anyrow update moves the resulting tuple to the end of the table. Second,the plan that relies on an index scan based on some other column returns the results in a different order. And even a sequential scan may not start at the beginning of the table . So if you need a particular order,you should explicitly define it in the ����� ��clause.\n\nHere is an example of an index scan that processes a large number of rows:\n\n=> EXPLAIN SELECT * FROM bookings WHERE book_ref < '100000';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using bookings_pkey on bookings\n\n(cost=0.43..4638.91 rows=132999 width=21) Index Cond: (book_ref < '100000'::bpchar)\n\n(3 rows)\n\nThe condition’s selectivity is estimated as follows:\n\n=> SELECT round(132999::numeric/reltuples::numeric, 4) FROM pg_class WHERE relname = 'bookings';\n\nround −−−−−−−− 0.0630 (1 row)\n\nto 1 16\n\nThisvalueisclose range from ������ to ������.\n\n,whichwecouldhaveguessedknowingthatbook_refvalues\n\nFor �-trees, the index-related part of the �/� cost estimation includes the cost of reading all the required pages. Index entries that satisfy any condition supported by �-trees are stored in pages bound into an ordered list, so the number of index pages to be read is estimated at the index size multiplied by the selectivity. But since these pages are not physically ordered,reading happens in a random fashion.\n\nC��resourcesarespentonprocessingalltheindexentriesthatareread(thecostof cpu_index_tuple_cost value) and com- processing a single entry is estimated at the puting the condition for each of these entries (in this case, the condition contains a single operator; its cost is estimated at the\n\ncpu_operator_cost value).\n\n378",
      "content_length": 1904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "20.1 Regular Index Scans\n\nTable access is regarded as sequential reading of the required number of pages. In the case of a perfect correlation, heap tuples will follow each other on disk, so the number of pages is estimated at the size of the table multiplied by the selectivity.\n\nThe �/� cost is further extended by the expenses incurred by tuple processing; they are estimated at the\n\ncpu_tuple_cost value per tuple.\n\n=> WITH costs(idx_cost, tbl_cost) AS (\n\nSELECT (\n\nSELECT round(\n\ncurrent_setting('random_page_cost')::real * pages + current_setting('cpu_index_tuple_cost')::real * tuples + current_setting('cpu_operator_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0630 AS pages, reltuples * 0.0630 AS tuples FROM pg_class WHERE relname = 'bookings_pkey'\n\n) c\n\n), (\n\nSELECT round(\n\ncurrent_setting('seq_page_cost')::real * pages + current_setting('cpu_tuple_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0630 AS pages, reltuples * 0.0630 AS tuples FROM pg_class WHERE relname = 'bookings'\n\n) c\n\n)\n\n) SELECT idx_cost, tbl_cost, idx_cost + tbl_cost AS total FROM costs;\n\nidx_cost | tbl_cost | total −−−−−−−−−−+−−−−−−−−−−+−−−−−−−\n\n2457 |\n\n2177 |\n\n4634\n\n(1 row)\n\nThese calculations illustrate the logic behind the cost estimation, so the result is aligned with the estimation provided by the planner, even if it is approximated. Getting the exact value would require taking other details into account, which we are not going to discuss here.\n\n379\n\n0.01",
      "content_length": 1467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "Chapter 20 Index Scans\n\nBad Scenario: Low Correlation\n\nEverything changes if the correlation is low. Let’s create an index on the book_date column, which has almost zero correlation with this index, and then take a look at the query that selects almost the same fraction of rows as in the previous example. Index access turns out to be so expensive that the planner chooses it only if all the other alternatives are explicitly forbidden:\n\n=> CREATE INDEX ON bookings(book_date);\n\n=> SET enable_seqscan = off;\n\n=> SET enable_bitmapscan = off;\n\n=> EXPLAIN SELECT * FROM bookings WHERE book_date < '2016-08-23 12:00:00+03';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using bookings_book_date_idx on bookings\n\n(cost=0.43..56957.48 rows=132403 width=21) Index Cond: (book_date < '2016−08−23 12:00:00+03'::timestamp w...\n\n(3 rows)\n\nThethingisthatlowcorrelationincreasesthechancesofthenexttuplereturnedby the access method to be located in a different page. Therefore,the Index Scan node has to hop between pages instead of reading them sequentially; in the worst-case scenario, the number of page accesses can reach the number of fetched tuples.\n\nHowever, we cannot simply replace seq_page_cost with random_page_cost and rel- pages with reltuples in the good-scenario calculations. The cost that we see in the plan is much lower than the value we would have estimated this way:\n\n380",
      "content_length": 1427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "20.1 Regular Index Scans\n\n=> WITH costs(idx_cost, tbl_cost) AS (\n\nSELECT\n\n( SELECT round(\n\ncurrent_setting('random_page_cost')::real * pages + current_setting('cpu_index_tuple_cost')::real * tuples + current_setting('cpu_operator_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0630 AS pages, reltuples * 0.0630 AS tuples FROM pg_class WHERE relname = 'bookings_pkey'\n\n) c\n\n), ( SELECT round(\n\ncurrent_setting('random_page_cost')::real * tuples + current_setting('cpu_tuple_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0630 AS pages, reltuples * 0.0630 AS tuples FROM pg_class WHERE relname = 'bookings'\n\n) c\n\n)\n\n) SELECT idx_cost, tbl_cost, idx_cost + tbl_cost AS total FROM costs;\n\nidx_cost | tbl_cost | total −−−−−−−−−−+−−−−−−−−−−+−−−−−−−−\n\n2457 |\n\n533330 | 535787\n\n(1 row)\n\nThe reason is that the model takes caching into account. Frequently used pages are kept in the buffer cache (and in the �� cache), so the bigger the cache size, the more chances to find the required page in it, thus avoiding an extra disk access op- effective_cache_size eration. For planning purposes,the cache size is defined by the parameter. The smaller its value, the more pages are expected to be read.\n\nThe graph that follows shows the dependency between the estimation of the num- ber of pages to be read and the table size (for the selectivity of 1 and the page 2 containing 10 rows).1 The dashed lines show the access count in the best scenario possible (half of the page count if the correlation is perfect) and in the worst sce- nario (half of the row count if there is zero correlation and no cache).\n\n1 backend/optimizer/path/costsize.c, index_pages_fetched function\n\n381\n\n4GB",
      "content_length": 1681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "Chapter 20 Index Scans\n\npage access count\n\nofrowcount\n\n0.5\n\n0.5 sel=\n\npagecount\n\n0.5ofpagecount\n\neffective_cache_size\n\ntablesize\n\nIt is assumed that the effective_cache_size value indicates the total volume of mem- ory that can be used for caching (including both the Postgre��� buffer cache and �� cache). But since this parameter is used solely for estimation purposes and does not affect memory allocation itself, you do not have to take actual figures into ac- count when changing this setting.\n\nIf you reduce effective_cache_size to the minimum,the plan estimation will be close to the low-end value shown above for the no-caching case:\n\n=> SET effective_cache_size = '8kB';\n\n=> EXPLAIN SELECT * FROM bookings WHERE book_date < '2016-08-23 12:00:00+03';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using bookings_book_date_idx on bookings\n\n(cost=0.43..532745.48 rows=132403 width=21) Index Cond: (book_date < '2016−08−23 12:00:00+03'::timestamp w...\n\n(3 rows)\n\n382",
      "content_length": 1019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "20.2 Index-Only Scans\n\n=> RESET effective_cache_size;\n\n=> RESET enable_seqscan;\n\n=> RESET enable_bitmapscan;\n\nThe planner calculates the table �/� cost for both worst-case and best-case scenar- ios and then takes an intermediate value based on the actual correlation.1\n\nThus,an index scan can be a good choice if only a fraction of rows has to be read. If heap tuples are correlated with the order in which the access method returns their ��s, this fraction can be quite substantial. However, if the correlation is low, index scanning becomes much less attractive for queries with low selectivity.\n\n20.2 Index-Only Scans\n\nIf an index contains all the heap data required by the query, it is called a covering index for this particular query. If such an index is available, extra table access can be avoided: instead of ���s, the access method can return the actual data directly. Such a type of an index scan is called an index-only scan.2 It can be used by those access methods that support the R���������\n\nproperty.\n\nIn the plan, this operation is represented by the Index Only Scan3 node:\n\n=> EXPLAIN SELECT book_ref FROM bookings WHERE book_ref < '100000';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Only Scan using bookings_pkey on bookings\n\n(cost=0.43..3791.91 rows=132999 width=7) Index Cond: (book_ref < '100000'::bpchar)\n\n(3 rows)\n\nThe name suggests that this node never has to access the heap, but it is not so. , so the access In Postgre���, indexes contain no information on tuple visibility method returns the data of all the heap tuples that satisfy the filter condition,even\n\n1 backend/optimizer/path/costsize.c, cost_index function 2 postgresql.org/docs/14/indexes-index-only-scans.html 3 backend/executor/nodeIndexonlyscan.c\n\n383\n\np. ���\n\np. ��",
      "content_length": 1785,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "p. ��\n\nChapter 20 Index Scans\n\nif the current transaction cannot see them. Their visibility is then checked by the indexing engine.\n\nHowever, if this method had to access the table to check visibility of each tuple, it would not be any different from a regular index scan. Instead, it employs the visibility map provided for tables, in which the vacuum process marks the pages that contain only all-visible tuples (that is, those tuples that are accessible to all transactions, regardless of the snapshot used). If the ��� returned by the index access method belongs to such a page, there is no need to check its visibility.\n\nThe cost estimation of an index-only scan depends on the fraction of all-visible pages in the heap. Postgre��� collects such statistics:\n\n=> SELECT relpages, relallvisible FROM pg_class WHERE relname = 'bookings';\n\nrelpages | relallvisible −−−−−−−−−−+−−−−−−−−−−−−−−−\n\n13447 |\n\n13446\n\n(1 row)\n\nThe cost estimation of an index-only scan differs from that of a regular index scan: its �/� cost related to table access is taken in proportion to the fraction of pages that do not appear in the visibility map. (The cost estimation of tuple processing is the same.)\n\nSince in this particular example all pages contain only all-visible tuples, the cost of heap �/� is in fact excluded from the cost estimation:\n\n=> WITH costs(idx_cost, tbl_cost) AS (\n\nSELECT (\n\nSELECT round(\n\ncurrent_setting('random_page_cost')::real * pages + current_setting('cpu_index_tuple_cost')::real * tuples + current_setting('cpu_operator_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0630 AS pages, reltuples * 0.0630 AS tuples\n\nFROM pg_class WHERE relname = 'bookings_pkey'\n\n) c\n\n) AS idx_cost,\n\n384",
      "content_length": 1704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "20.2 Index-Only Scans\n\n(\n\nSELECT round(\n\n(1 - frac_visible) * -- fraction of non-all-visible pages current_setting('seq_page_cost')::real * pages + current_setting('cpu_tuple_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0630 AS pages,\n\nreltuples * 0.0630 AS tuples,\n\nrelallvisible::real/relpages::real AS frac_visible\n\nFROM pg_class WHERE relname = 'bookings'\n\n) c\n\n) AS tbl_cost\n\n) SELECT idx_cost, tbl_cost, idx_cost + tbl_cost AS total FROM costs;\n\nidx_cost | tbl_cost | total −−−−−−−−−−+−−−−−−−−−−+−−−−−−−\n\n2457 |\n\n1330 |\n\n3787\n\n(1 row)\n\nAny unvacuumed changes that have not disappeared behind the database horizon p. ��� yet increase the estimated cost of the plan (and, consequently, make this plan less attractive to the optimizer). The ������� ������� command can show the actual heap access count.\n\nIn a newly created table, Postgre��� has to check visibility of all the tuples:\n\n=> CREATE TEMP TABLE bookings_tmp WITH (autovacuum_enabled = off) AS\n\nSELECT * FROM bookings ORDER BY book_ref;\n\n=> ALTER TABLE bookings_tmp ADD PRIMARY KEY(book_ref);\n\n=> ANALYZE bookings_tmp;\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT book_ref FROM bookings_tmp WHERE book_ref < '100000';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Only Scan using bookings_tmp_pkey on bookings_tmp\n\n(cost=0.43..4638.91 rows=132999 width=7) (actual rows=132109 l... Index Cond: (book_ref < '100000'::bpchar) Heap Fetches: 132109\n\n(4 rows)\n\n385",
      "content_length": 1482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "v. ��\n\nChapter 20 Index Scans\n\nBut once the table has been vacuumed,such a check becomes redundant and is not performed as long as all the pages remain all-visible.\n\n=> VACUUM bookings_tmp;\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT book_ref FROM bookings_tmp WHERE book_ref < '100000';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Only Scan using bookings_tmp_pkey on bookings_tmp\n\n(cost=0.43..3787.91 rows=132999 width=7) (actual rows=132109 l... Index Cond: (book_ref < '100000'::bpchar) Heap Fetches: 0\n\n(4 rows)\n\nIndexes with the Include Clause\n\nIt is not always possible to extend an index with all the columns required by a query:\n\nFor a unique index, adding a new column would compromise the uniqueness\n\nof the original key columns.\n\nThe index access method may not provide an operator class for the data type\n\nof the column to be added.\n\nyou can still include columns into an index without making them a In this case, part of the index key. It will of course be impossible to perform an index scan based on the included columns, but if a query references these columns, the index will function as a covering one.\n\nThe following example shows how to replace an automatically created primary key index by another index with an included column:\n\n=> CREATE UNIQUE INDEX ON bookings(book_ref) INCLUDE (book_date);\n\n=> BEGIN;\n\n=> ALTER TABLE bookings\n\nDROP CONSTRAINT bookings_pkey CASCADE;\n\n386",
      "content_length": 1453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "20.3 Bitmap Scans\n\nNOTICE: tickets ALTER TABLE\n\ndrop cascades to constraint tickets_book_ref_fkey on table\n\n=> ALTER TABLE bookings ADD CONSTRAINT bookings_pkey PRIMARY KEY USING INDEX bookings_book_ref_book_date_idx; -- a new index\n\nNOTICE: \"bookings_book_ref_book_date_idx\" to \"bookings_pkey\" ALTER TABLE\n\nALTER TABLE / ADD CONSTRAINT USING INDEX will rename index\n\n=> ALTER TABLE tickets\n\nADD FOREIGN KEY (book_ref) REFERENCES bookings(book_ref);\n\n=> COMMIT;\n\n=> EXPLAIN SELECT book_ref, book_date FROM bookings WHERE book_ref < '100000';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Only Scan using bookings_pkey on bookings\n\n(cost=0.43..437...\n\nIndex Cond: (book_ref < '100000'::bpchar)\n\n(2 rows)\n\nSuch indexes are often called covering, but it is not quite correct. An index is considered covering if the set of its columns covers all the columns required by a particular query. It does not matter whether it involves any columns added by the ������� clause,or only key columns are being used. Moreover,one and the same index can be covering for one query but not for the other.\n\n20.3 Bitmap Scans\n\nThe efficiency of an index scan is limited: as the correlation decreases,the number of accesses to heap pages rises,and scanning becomes random rather than sequen- tial. Toovercomethis limitation,Postgre��� can fetch all the ���sbeforeaccessing the table and sort them in ascending order based on their page numbers.1 This is exactly how bitmap scanning works,which is yet another common approach to pro- cessing ���s. It can be used by those access methods that support the B����� S��� p. ��� property.\n\n1 backend/access/index/indexam.c, index_getbitmap function\n\n387",
      "content_length": 1717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "1\n\nChapter 20 Index Scans\n\nUnlike a regular index scan, this operation is represented in the query plan by two nodes:\n\n=> CREATE INDEX ON bookings(total_amount);\n\n=> EXPLAIN SELECT * FROM bookings WHERE total_amount = 48500.00;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings\n\n(cost=54.63..7040.42 rows=2865 wid...\n\nRecheck Cond: (total_amount = 48500.00) −> Bitmap Index Scan on bookings_total_amount_idx\n\n(cost=0.00..53.92 rows=2865 width=0) Index Cond: (total_amount = 48500.00)\n\n(5 rows)\n\nThe Bitmap Index Scan1 node gets the bitmap of all ���s2 from the access method.\n\nThe bitmap consists of separate segments, each corresponding to a single heap page. All these segments have the same size, which is enough for all the page tuples, no matter how many of them are present. This number is limited because a tuple header is quite large; a standard-size page can accommodate ��� tuples at the most, which fit �� bytes.3\n\nThen the Bitmap Heap Scan4 traverses the bitmap segment by segment, reads the corresponding pages, and checks all their tuples that are marked all-visible. Thus, pages are read in ascending order based on their numbers,and each of them is read exactly once.\n\nThat said, this process is not the same as sequential scanning since the accessed pages rarely follow each other. Regular prefetching performed by the operating system does not help in this case, so the Bitmap Heap Scan node implements its effective_io_concurrency pages—and it own prefetching by asynchronously reading is the only node that does it. This mechanism relies on the posix_fadvise function implemented by some operating systems. If your system supports this function, it makes sense to configure the effective_io_concurrency parameter at the tablespace level in accordance with the hardware capabilities.\n\n1 backend/executor/nodeBitmapIndexscan.c 2 backend/access/index/indexam.c, index_getbitmap function 3 backend/nodes/tidbitmap.c 4 backend/executor/nodeBitmapHeapscan.c\n\n388",
      "content_length": 2038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "20.3 Bitmap Scans\n\nAsynchronous prefetching is also used by some other internal processes:\n\nfor index pages when heap rows are being deleted1\n\nfor heap pages during analysis (�������)2\n\nThe prefetch depth is defined by the\n\nmaintenance_io_concurrency.\n\nBitmap Accuracy\n\nThe more pages contain the tuples that satisfy the filter condition of the query,the bigger is the bitmap. It is built in the local memory of the backend, and its size is work_mem parameter. Once the maximum allowed size is reached, limited by the some bitmap segments become lossy: each bit of a lossy segment corresponds to a whole page, while the segment itself comprises a range of pages.3 As a result, the size of the bitmap becomes smaller at the expense of its accuracy.\n\nThe ������� ������� command shows the accuracy of the built bitmap:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM bookings WHERE total_amount > 150000.00;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings (actual rows=242691 loops=1)\n\nRecheck Cond: (total_amount > 150000.00) Heap Blocks: exact=13447 −> Bitmap Index Scan on bookings_total_amount_idx (actual rows...\n\nIndex Cond: (total_amount > 150000.00)\n\n(5 rows)\n\nHere we have enough memory for an exact bitmap.\n\nIf we decrease the work_mem value, some of the bitmap segments become lossy:\n\n=> SET work_mem = '512kB';\n\n1 backend/access/heap/heapam.c, index_delete_prefetch_buffer function 2 backend/commands/analyze.c, acquire_sample_rows function 3 backend/nodes/tidbitmap.c, tbm_lossify function\n\n389\n\nv. ��\n\nv. ��\n\n10\n\n4MB",
      "content_length": 1616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "Chapter 20 Index Scans\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM bookings WHERE total_amount > 150000.00;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings (actual rows=242691 loops=1)\n\nRecheck Cond: (total_amount > 150000.00) Rows Removed by Index Recheck: 1145721 Heap Blocks: exact=5178 lossy=8269 −> Bitmap Index Scan on bookings_total_amount_idx (actual rows...\n\nIndex Cond: (total_amount > 150000.00)\n\n(6 rows)\n\n=> RESET work_mem;\n\nWhen reading a heap page that corresponds to a lossy bitmap segment,Postgre��� has to recheck the filter condition for each tuple in the page. The condition to be rechecked is always displayed in the plan as Recheck Cond, even if this recheck is not performed. The number of tuples filtered out during a recheck is displayed separately (as Rows Removed by Index Recheck).\n\nIf the size of the result set is too big,the bitmap may not fit the work_mem memory chunk, even if all its segments are lossy. Then this limit is ignored,and the bitmap takes as much space as required. Postgre��� neither further reduces the bitmap accuracy nor flushes any of its segments to disk.\n\nOperations on Bitmaps\n\nIf the query applies conditions to several table columns that have separate indexes createdonthem,abitmapscancanuseseveralindexestogether.1 Alltheseindexes have their own bitmaps built on the fly; the bitmaps are then combined together bit by bit,using either logical conjunction (if the expressions are connected by���) or logical disjunction (if the expressions are connected by ��). For example:\n\n=> EXPLAIN (costs off) SELECT * FROM bookings WHERE book_date < '2016-08-28' AND total_amount > 250000;\n\n1 postgresql.org/docs/14/indexes-ordering.html\n\n390",
      "content_length": 1781,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "20.3 Bitmap Scans\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings\n\nRecheck Cond: ((total_amount > '250000'::numeric) AND (book_da... −> BitmapAnd\n\n−> Bitmap Index Scan on bookings_total_amount_idx\n\nIndex Cond: (total_amount > '250000'::numeric)\n\n−> Bitmap Index Scan on bookings_book_date_idx\n\nIndex Cond: (book_date < '2016−08−28 00:00:00+03'::tim...\n\n(7 rows)\n\nHere the BitmapAnd node combines two bitmaps using the bitwise ��� operation.\n\nAs two bitmaps are being merged into one,1 exact segments remain exact when merged together (if the new bitmap fits the work_mem memory chunk), but if any segment in a pair is lossy, the resulting segment will be lossy too.\n\nCost Estimation\n\nLet’s take a look at the query that uses a bitmap scan:\n\n=> EXPLAIN SELECT * FROM bookings WHERE total_amount = 28000.00;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings\n\n(cost=599.48..14444.96 rows=31878 ...\n\nRecheck Cond: (total_amount = 28000.00) −> Bitmap Index Scan on bookings_total_amount_idx\n\n(cost=0.00..591.51 rows=31878 width=0) Index Cond: (total_amount = 28000.00)\n\n(5 rows)\n\nThe approximate selectivity of the condition used by the planner equals\n\n=> SELECT round(31878::numeric/reltuples::numeric, 4) FROM pg_class WHERE relname = 'bookings';\n\nround −−−−−−−− 0.0151 (1 row)\n\n1 backend/nodes/tidbitmap.c, tbm_union & tbm_intersect functions\n\n391",
      "content_length": 1471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "Chapter 20 Index Scans\n\nThe total cost of the Bitmap Index Scan node is estimated in the same way as the cost of a regular index scan that does not take heap access into account:\n\n=> SELECT round(\n\ncurrent_setting('random_page_cost')::real * pages + current_setting('cpu_index_tuple_cost')::real * tuples + current_setting('cpu_operator_cost')::real * tuples\n\n) FROM (\n\nSELECT relpages * 0.0151 AS pages, reltuples * 0.0151 AS tuples FROM pg_class WHERE relname = 'bookings_total_amount_idx'\n\n) c;\n\nround −−−−−−−\n\n589\n\n(1 row)\n\nThe �/� cost estimation for the Bitmap Heap Scan node differs from that for a perfect-correlation case of a regular index scan. A bitmap allows reading heap pages in ascending order based on their numbers, without getting back to one and the same page, but the tuples that satisfy the filter condition do not follow each other anymore. Instead of reading a strictly sequential page range that is quite compact, Postgre��� is likely to access far more pages.\n\nThe number of pages to be read is estimated by the following formula:1\n\nmin\n\n(\n\n2 relpages ⋅ reltuples ⋅ sel 2 relpages + reltuples ⋅ sel\n\n,relpages\n\n)\n\nThe estimated cost of reading a single page falls between seq_page_cost and ran- dom_page_cost, depending on the ratio of the fraction of fetched pages to the total number of pages in the table:\n\n1 backend/optimizer/path/costsize.c, compute_bitmap_pages function\n\n392",
      "content_length": 1407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "20.3 Bitmap Scans\n\n=> WITH t AS (\n\nSELECT relpages,\n\nleast(\n\n(2 * relpages * reltuples * 0.0151) / (2 * relpages + reltuples * 0.0151), relpages\n\n) AS pages_fetched, round(reltuples * 0.0151) AS tuples_fetched, current_setting('random_page_cost')::real AS rnd_cost, current_setting('seq_page_cost')::real AS seq_cost\n\nFROM pg_class WHERE relname = 'bookings'\n\n) SELECT pages_fetched,\n\nrnd_cost - (rnd_cost - seq_cost) * sqrt(pages_fetched / relpages) AS cost_per_page, tuples_fetched\n\nFROM t;\n\npages_fetched | cost_per_page | tuples_fetched −−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n13447 |\n\n1 |\n\n31878\n\n(1 row)\n\nAs usual, the �/� estimation is increased by the cost of processing each fetched tuple. If an exact bitmap is used, the number of tuples is estimated at the total number of tuples in the table multiplied by the selectivity of filter conditions. But ifanybitmapsegmentsarelossy,Postgre���hastoaccessthecorrespondingpages to recheck all their tuples.\n\na lossybitmap segment\n\nan exact segment\n\nThus, the estimation takes into account the expected fraction of lossy bitmap seg- ments (which can be calculated based on the total number of selected rows and the bitmap size limit defined by work_mem).1\n\n1 backend/optimizer/path/costsize.c, compute_bitmap_pages function\n\n393\n\nv. ��",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "Chapter 20 Index Scans\n\nThe total cost of condition rechecks also increases the estimation (regardless of the bitmap accuracy).\n\nThe startup cost estimation of the Bitmap Heap Scan node is based on the total cost of the Bitmap Index Scan node, which is extended by the cost of bitmap processing:\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on bookings\n\n(cost=599.48..14444.96 rows=31878 width=21) Recheck Cond: (total_amount = 28000.00) −> Bitmap Index Scan on bookings_total_amount_idx\n\n(cost=0.00..591.51 rows=31878 width=0) Index Cond: (total_amount = 28000.00)\n\n(6 rows)\n\nHere the bitmap is exact, and the cost is estimated roughly as follows:1\n\n=> WITH t AS (\n\nSELECT 1 AS cost_per_page,\n\n13447 AS pages_fetched, 31878 AS tuples_fetched\n\n), costs(startup_cost, run_cost) AS (\n\nSELECT\n\n( SELECT round(\n\n589 /* cost estimation for the child node */ + 0.1 * current_setting('cpu_operator_cost')::real * reltuples * 0.0151\n\n) FROM pg_class WHERE relname = 'bookings_total_amount_idx'\n\n), ( SELECT round(\n\ncost_per_page * pages_fetched + current_setting('cpu_tuple_cost')::real * tuples_fetched + current_setting('cpu_operator_cost')::real * tuples_fetched\n\n) FROM t\n\n)\n\n) SELECT startup_cost, run_cost,\n\nstartup_cost + run_cost AS total_cost\n\nFROM costs;\n\n1 backend/optimizer/path/costsize.c, cost_bitmap_heap_scan function\n\n394",
      "content_length": 1369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "20.4 Parallel Index Scans\n\nstartup_cost | run_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−\n\n597 |\n\n13845 |\n\n14442\n\n(1 row)\n\nIf the query plan combines several bitmaps, the sum of the costs of separate index scans is increased by a (small) cost of merging them together.1\n\n20.4 Parallel Index Scans\n\nAll the index scanning modes bitmap scan—have their own flavors for parallel\n\n—a regular index scan, an index-only scan, and a\n\nplans.\n\nThe cost of parallel execution is estimated in the same way as that of sequen- tial one, but (just like in the case of a parallel sequential scan) ��� resources are distributed between all parallel processes, thus reducing the total cost. The �/� component of the cost is not distributed because processes are synchronized to perform page access sequentially.\n\nNow let me show you several examples of parallel plans without breaking down their cost estimation.\n\nA parallel index scan:\n\n=> EXPLAIN SELECT sum(total_amount) FROM bookings WHERE book_ref < '400000';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n(cost=19192.81..19192.82 rows=1 width=32)\n\n−> Gather\n\n(cost=19192.59..19192.80 rows=2 width=32)\n\nWorkers Planned: 2 −> Partial Aggregate\n\n(cost=18192.59..18192.60 rows=1 widt...\n\n−> Parallel Index Scan using bookings_pkey on bookings\n\n(cost=0.43..17642.82 rows=219907 width=6) Index Cond: (book_ref < '400000'::bpchar)\n\n(7 rows)\n\n1 backend/optimizer/path/costsize.c, cost_bitmap_and_node & cost_bitmap_or_node functions\n\n395\n\nv. �.� p. ���",
      "content_length": 1552,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "Chapter 20 Index Scans\n\nWhile a parallel scan of a �-tree is in progress, the �� of the current index page is kept in the server’s shared memory. The initial value is set by the process that starts the scan: it traverses the tree from the root to the first suitable leaf page and saves its ��. Workers access subsequent index pages as needed,replacing the saved ��. Having fetched a page, the worker iterates through all its suitable entries and reads the corresponding heap tuples. The scanning completes when the worker has read the whole range of values that satisfy the query filter.\n\nA parallel index-only scan:\n\n=> EXPLAIN SELECT sum(total_amount) FROM bookings WHERE total_amount < 50000.00;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n(cost=23370.60..23370.61 rows=1 width=32)\n\n−> Gather\n\n(cost=23370.38..23370.59 rows=2 width=32)\n\nWorkers Planned: 2 −> Partial Aggregate\n\n(cost=22370.38..22370.39 rows=1 widt... −> Parallel Index Only Scan using bookings_total_amoun...\n\n(cost=0.43..21387.27 rows=393244 width=6) Index Cond: (total_amount < 50000.00)\n\n(7 rows)\n\nA parallel index-only scan skips heap access for all-visible pages; it is the only difference it has from a parallel index scan.\n\nA parallel bitmap scan:\n\n=> EXPLAIN SELECT sum(total_amount) FROM bookings WHERE book_date < '2016-10-01';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n(cost=21492.21..21492.22 rows=1 width=32)\n\n−> Gather\n\n(cost=21491.99..21492.20 rows=2 width=32)\n\nWorkers Planned: 2 −> Partial Aggregate\n\n(cost=20491.99..20492.00 rows=1 widt...\n\n−> Parallel Bitmap Heap Scan on bookings\n\n(cost=4891.17..20133.01 rows=143588 width=6) Recheck Cond: (book_date < '2016−10−01 00:00:00+03... −> Bitmap Index Scan on bookings_book_date_idx\n\n(cost=0.00..4805.01 rows=344611 width=0) Index Cond: (book_date < '2016−10−01 00:00:00+...\n\n(10 rows)\n\n396",
      "content_length": 1944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "20.5 Comparison of Various Access Methods\n\nA bitmap scan implies that a bitmap is always built sequentially, by a single leader process; for this reason, the name of the Bitmap Index Scan node does not contain the word Parallel. When the bitmap is ready, the Parallel Bitmap Heap Scan node starts a parallel heap scan. Workers access subsequent heap pages and process them concurrently.\n\n20.5 Comparison of Various Access Methods\n\nThe following illustration shows how costs of various access methods depend on selectivity of filter conditions:\n\ncost\n\nindexscan bitmapindexscan\n\nseq scan\n\nindex-onlyscan\n\nselectivity\n\n0\n\n1\n\nIt is a qualitative diagram; the actual figures are of course dependent on the par- ticular table and server configuration.\n\nSequential scanning does not depend on selectivity, and starting from a certain fraction of selected rows, it is usually more efficient than other methods.\n\nThe cost of an index scan is affected by the correlation between the physical order of tuples and the order in which their ��s are returned by the access method. If the correlation is perfect, an index scan can be quite efficient even if the fraction\n\n397",
      "content_length": 1160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "Chapter 20 Index Scans\n\nof selected rows is rather high. However, for low correlation (which is much more common) it can quickly become even more expensive than a sequential scan. That said, index scanning is still an absolute leader when it comes to selecting a single row using an index (typically a unique one).\n\nIf applicable, index-only scans can show great performance and beat sequential scans even if all the rows are selected. However, their performance is highly de- pendent on the visibility map, and in the worst-case scenario an index-only scan can degrade to a regular index scan.\n\nThe cost of a bitmap scan is affected by the size of available memory,but to a much lesser extent than an index scan cost depends on correlation. If the correlation is low, the bitmap scan turns out to be much cheaper.\n\nEach access method has its own perfect usage scenarios; there is no such method that always outperforms other methods. The planner has to do extensive calcu- lations to estimate the efficiency of each method in each particular case. Clearly, the accuracy of these estimations highly depends on the accuracy of the collected statistics.\n\n398",
      "content_length": 1156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "21\n\nNested Loop\n\n21.1 Join Types and Methods\n\nJoins are a key feature of the ��� language; they serve as the foundation for its power and flexibility. Sets of rows (either retrieved from tables directly or received as the result of some other operations) are always joined pairwise.\n\nThere are several types of joins:\n\nInner joins. An inner join (specified as ����� ����, or simply ����) comprises those pairs of rows of two sets that satisfy a particular join condition. The join con- dition combines some columns of one set of rows with some columns of the other set; all the columns involved constitute the join key.\n\nIf the join condition demands that join keys of two sets be equal, such a join is called an equi-join; this is the most common join type.\n\nA Cartesian product (����� ����) of two sets comprises all the possible pairs of rows of these sets—it is a special case of an inner join with a true condition.\n\nOuter joins. A left outer join (specified as ���� ����� ����, or simply ���� ����) ex- tends the result of an inner join by those rows of the left set that have no match in the right set (the corresponding right-side columns are filled with ���� values).\n\nThesameisalsotrueforarightouterjoin(���������),downtothepermutation of sets.\n\nA full outer join (specified as ���� ����) comprises left and right outer joins, adding both right-side and left-side rows for which no match has been found.\n\n399",
      "content_length": 1418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "Chapter 21 Nested Loop\n\nAnti-Joins and Semi-Joins. A semi-join looks a lot like an inner join, but it includes only those rows of the left set that have a match in the right set (a row is included only once even if there are several matches).\n\nAn anti-join includes those rows of a set that have no match in the other set.\n\nThe ��� language has no explicit semi- and anti-joins, but the same outcome can be achieved using predicates like ������ and ��� ������.\n\nAll these joins are logical operations. For example,an inner join is often described as a Cartesian product that has been cleared of the rows that do not satisfy the join condition. But at the physical level, an inner join is typically achieved via less expensive means.\n\nPostgre��� provides several join methods:\n\na nested loop join\n\na hash join\n\na merge join\n\nJoin methods are algorithms that implement logical operations of ��� joins. These basic algorithms often have special flavors tailored for particular join types, even though they may support only some of them. For example, a nested loop supports an inner join (represented in the plan by a Nested Loop node) and a left outer join (represented by a Nested Loop Left Join node), but it cannot be used for full joins.\n\nSome flavors of the same algorithms can also be used by other operations, such as aggregation.\n\nDifferent join methods perform best in different conditions; it is the job of the planner to choose the most cost-effective one.\n\n21.2 Nested Loop Joins\n\nThe basic algorithm of the nested loop join functions as follows. The outer loop traverses all the rows of the first set (called the outer set). For each of these rows,\n\n400",
      "content_length": 1663,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "21.2 Nested Loop Joins\n\nthenestedloopgoesthroughtherowsofthesecondset(calledtheinner set)tofind the ones that satisfy the join condition. Each found pair is returned immediately as part of the query result.1\n\nThe algorithm accesses the inner set as many times as there are rows in the outer set. Therefore, the efficiency of nested loop joins depends on several factors:\n\ncardinality of the outer set of rows\n\navailability of an access method that can efficiently fetch the needed rows of\n\nthe inner set\n\nrecurrent access to the same rows of the inner set\n\nCartesian Product\n\nA nested loop join is the most efficient way to find a Cartesian product, regardless of the number of rows in the sets:\n\n=> EXPLAIN SELECT * FROM aircrafts_data a1\n\nCROSS JOIN aircrafts_data a2\n\nWHERE a2.range > 5000;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n(cost=0.00..2.78 rows=45 width=144)\n\n−> Seq Scan on aircrafts_data a1\n\n(cost=0.00..1.09 rows=9 width=72)\n\nouter set\n\n−> Materialize\n\n(cost=0.00..1.14 rows=5 width=72)\n\n−> Seq Scan on aircrafts_data a2\n\n(cost=0.00..1.11 rows=5 width=72) Filter: (range > 5000)\n\ninner set\n\n(7 rows)\n\nThe Nested Loop node performs a join using the algorithm described above. It al- ways has two child nodes: the one that is displayed higher in the plan corresponds to the outer set of rows, while the lower one represents the inner set.\n\n1 backend/executor/nodeNestloop.c\n\n401",
      "content_length": 1430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "4MB\n\nChapter 21 Nested Loop\n\nIn this example,the inner set is represented by the Materialize node.1 This node re- turns the rows received from its child node, having saved them for future use (the work_mem; then rows are accumulated in memory until their total size reaches Postgre��� starts spilling them into a temporary file on disk). If accessed again, the node reads the accumulated rows without calling the child node. Thus, the ex- ecutor can avoid scanning the full table again and read only those rows that satisfy the condition.\n\nA similar plan can also be built for a query that uses a regular equi-join:\n\n=> EXPLAIN SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nWHERE t.ticket_no = '0005432000284';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=0.99..25.05 rows=3 width=136)\n\nNested Loop\n\n−> Index Scan using tickets_pkey on tickets t\n\n(cost=0.43..8.45 rows=1 width=104) Index Cond: (ticket_no = '0005432000284'::bpchar)\n\n−> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) Index Cond: (ticket_no = '0005432000284'::bpchar)\n\n(7 rows)\n\nHaving recognized the equality of the two values, the planner replaces the join condition tf.ticket_no = t.ticket_no by the tf.ticket_no = constant condition, virtually reducing an equi-join to a Cartesian product.2\n\nCardinality estimation. The cardinality of a Cartesian product is estimated at the product of cardinalities of the joined data sets: 3 = 1 × 3.\n\nCost estimation. The startup cost of the join operation combines the startup costs of all child nodes.\n\n1 backend/executor/nodeMaterial.c 2 backend/optimizer/path/equivclass.c\n\n402",
      "content_length": 1704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "21.2 Nested Loop Joins\n\nThe full cost of the join includes the following components:\n\nthe cost of fetching all the rows of the outer set\n\nthecostofasingleretrievalofalltherowsoftheinnerset(sincethecardinality\n\nestimation of the outer set equals one)\n\nthe cost of processing each row to be returned\n\nHere is a dependency graph for the cost estimation:\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=0.99..25.05 rows=3 width=136)\n\nNested Loop\n\n−> Index Scan using tickets_pkey on tickets t\n\n(cost=0.43..8.45 rows=1 width=104) Index Cond: (ticket_no = '0005432000284'::bpchar)\n\n× 1\n\n−> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) Index Cond: (ticket_no = '0005432000284'::bpchar)\n\n(7 rows)\n\nThe cost of the join is calculated as follows:\n\n=> SELECT 0.43 + 0.56 AS startup_cost,\n\nround((\n\n8.45 + 16.57 + 3 * current_setting('cpu_tuple_cost')::real\n\n)::numeric, 2) AS total_cost;\n\nstartup_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0.99 |\n\n25.05\n\n(1 row)\n\nNow let’s get back to the previous example:\n\n=> EXPLAIN SELECT * FROM aircrafts_data a1\n\nCROSS JOIN aircrafts_data a2\n\nWHERE a2.range > 5000;\n\n403",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "Chapter 21 Nested Loop\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n(cost=0.00..2.78 rows=45 width=144)\n\n−> Seq Scan on aircrafts_data a1\n\n(cost=0.00..1.09 rows=9 width=72)\n\n−> Materialize\n\n(cost=0.00..1.14 rows=5 width=72)\n\n−> Seq Scan on aircrafts_data a2\n\n(cost=0.00..1.11 rows=5 width=72) Filter: (range > 5000)\n\n(7 rows)\n\nThe plan now contains the Materialize node; having once accumulated the rows received from its child node, Materialize returns them much faster for all the sub- sequent calls.\n\nIn general, the total cost of a join comprises the following expenses:1\n\nthe cost of fetching all the rows of the outer set\n\nthe cost of the initial fetch of all the rows of the inner set (during which ma-\n\nterialization is performed)\n\n(N−1)-foldcostofrepeatfetchesofrowsoftheinnerset(hereN isthenumber\n\nof rows in the outer set)\n\nthe cost of processing each row to be returned\n\nThe dependency graph here is as follows:\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n(cost=0.00..2.78 rows=45 width=144)\n\n−> Seq Scan on aircrafts_data a1\n\n(cost=0.00..1.09 rows=9 width=72)\n\n−> Materialize\n\n× 9\n\n(cost=0.00..1.14 rows=5 width=72) −> Seq Scan on aircrafts_data a2\n\n(cost=0.00..1.11 rows=5 width=72) Filter: (range > 5000)\n\n(8 rows)\n\n1 backend/optimizer/path/costsize.c, initial_cost_nestloop andfinal_cost_nestloop function\n\n404",
      "content_length": 1394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "21.2 Nested Loop Joins\n\nIn this example, materialization reduces the cost of repeat data fetches. The cost of the first Materialize call is shown in the plan, but all the subsequent calls are not listed. I will not provide any calculations here,1 but in this particular case the estimation is �.����.\n\nThus, the cost of the join performed in this example is calculated as follows:\n\n=> SELECT 0.00 + 0.00 AS startup_cost,\n\nround((\n\n1.09 + (1.14 + 8 * 0.0125) + 45 * current_setting('cpu_tuple_cost')::real\n\n)::numeric, 2) AS total_cost;\n\nstartup_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0.00 |\n\n2.78\n\n(1 row)\n\nParameterized Joins\n\nNow let’s consider a more common example that does not boil down to a Cartesian product:\n\n=> CREATE INDEX ON tickets(book_ref);\n\n=> EXPLAIN SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nWHERE t.book_ref = '03A76D';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=0.99..45.68 rows=6 width=136)\n\nNested Loop\n\n−> Index Scan using tickets_book_ref_idx on tickets t\n\n(cost=0.43..12.46 rows=2 width=104) Index Cond: (book_ref = '03A76D'::bpchar)\n\n−> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) Index Cond: (ticket_no = t.ticket_no)\n\n(7 rows)\n\n1 backend/optimizer/path/costsize.c, cost_rescan function\n\n405",
      "content_length": 1353,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "Chapter 21 Nested Loop\n\nHere the Nested Loop node traverses the rows of the outer set (tickets),and for each of these rows it searches for the corresponding rows of the inner set (flights), pass- ing the ticket number (t.ticket_no) to the condition as a parameter. When the inner node (Index Scan) is called, it has to deal with the condition ticket_no = constant.\n\nCardinality estimation. The planner estimates that the filter condition by a book- ing number is satisfied by two rows of the outer set (rows=2), and each of these rows matches three rows of the inner set on average (rows=3).\n\nJoin selectivity is a fraction of the Cartesian product of the two sets that remains after the join. It is obvious that we must exclude those rows of both sets that con- tain ���� values in the join key since the equality condition will never be satisfied for them.\n\nThe estimated cardinality equals the cardinality of the Cartesian product (that is, the product of cardinalities of the two sets) multiplied by the selectivity.1\n\nHere the estimated cardinality of the first (outer) set is two rows. Since no condi- tions are applied to the second (inner) set except for the join condition itself, the cardinality of the second set is taken as the cardinality of the ticket_flights table.\n\nSince the joined tables are connected by a foreign key, the selectivity estimation relies on the fact that each row of the child table has exactly one matching row in the parent table. So the selectivity is taken as the inverse of the size of the table referred to by the foreign key.2\n\nThus, for the case when the ticket_no columns contain no ���� values, the estima- tion is as follows:\n\n=> SELECT round(2 * tf.reltuples * (1.0 / t.reltuples)) AS rows FROM pg_class t, pg_class tf WHERE t.relname = 'tickets'\n\nAND tf.relname = 'ticket_flights';\n\nrows −−−−−−\n\n6\n\n(1 row)\n\n1 backend/optimizer/path/costsize.c, calc_joinrel_size_estimate function 2 backend/optimizer/path/costsize.c, get_foreign_key_join_selectivity function\n\n406",
      "content_length": 2011,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "21.2 Nested Loop Joins\n\nClearly, tables can be also joined without using foreign keys. Then the selectivity will be taken as the estimated selectivities of the particular join conditions.1\n\nFor the equi-join in this example, the generic formula for selectivity estimation , 1 thatassumesuniformdistributionofvalueslooksasfollows: min( nd2),where nd1 and nd2 represent the number of distinct values of the join key in the first and second set, respectively.2\n\n1 nd1\n\nStatisticsondistinctvaluesshowthatticketnumbersintheticketstableareunique (which is only to be expected, as the ticket_no column is the primary key), and the ticket_flights has about three matching rows for each ticket:\n\n=> SELECT t.n_distinct, tf.n_distinct FROM pg_stats t, pg_stats tf WHERE t.tablename = 'tickets' AND t.attname = 'ticket_no'\n\nAND tf.tablename = 'ticket_flights' AND tf.attname = 'ticket_no';\n\nn_distinct | n_distinct −−−−−−−−−−−−+−−−−−−−−−−−−−\n\n−1 | −0.30362356\n\n(1 row)\n\nThe result would match the estimation for the join with the foreign key:\n\n=> SELECT round(2 * tf.reltuples *\n\nleast(1.0/t.reltuples, 1.0/tf.reltuples/0.30362356)\n\n) AS rows FROM pg_class t, pg_class tf WHERE t.relname = 'tickets' AND tf.relname = 'ticket_flights';\n\nrows −−−−−−\n\n6\n\n(1 row)\n\nThe planner tries to refine this baseline estimation whenever possible. It cannot use histograms at the moment, but it takes ��� lists into account if such statistics have been collected on the join key for both tables.3 The selectivity of the rows that appear in the list can be estimated more accurately, and only the remaining rows will have to rely on calculations that are based on uniform distribution.\n\n1 backend/optimizer/path/clausesel.c, clauselist_selectivity function 2 backend/utils/adt/selfuncs.c, eqjoinsel function 3 backend/utils/adt/selfuncs.c, eqjoinsel function\n\n407\n\np. ���\n\np. ���",
      "content_length": 1852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "Chapter 21 Nested Loop\n\nIngeneral,joinselectivityestimationislikelytobemoreaccurateiftheforeignkey is defined. It is especially true for composite join keys, as the selectivity is often largely underestimated in this case.\n\nUsing the ������� ������� command, you can view not only the actual number of rows, but also the number of times the inner loop has been executed:\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nWHERE t.book_ref = '03A76D';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n(cost=0.99..45.68 rows=6 width=136)\n\n(actual rows=8 loops=1) −> Index Scan using tickets_book_ref_idx on tickets t\n\n(cost=0.43..12.46 rows=2 width=104) (actual rows=2 loops=1) Index Cond: (book_ref = '03A76D'::bpchar)\n\n−> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) (actual rows=4 loops=2) Index Cond: (ticket_no = t.ticket_no)\n\n(8 rows)\n\nThe outer set contains two rows (actual rows=2); the estimation has been correct. So the Index Scan node was executed twice (loops=2),and each time it selected four rows on average (actual rows=4). Hence the total number of found rows: actual rows=8.\n\nI do not show the execution time of each stage of the plan (������ ���) for the output to fit the limited width of the page; besides, on some platforms an output with timing enabled can significantly slow down query execution. But if we did include it, Postgre��� would display an average value, just like for the row count. To get the total execution time, you should multiply this value by the number of iterations (loops).\n\nCost estimation. The cost estimation formula here is the same as in the previous examples.\n\nLet’s recall our query plan:\n\n408",
      "content_length": 1813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 410,
      "content": "21.2 Nested Loop Joins\n\n=> EXPLAIN SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nWHERE t.book_ref = '03A76D';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=0.99..45.68 rows=6 width=136)\n\nNested Loop\n\n−> Index Scan using tickets_book_ref_idx on tickets t\n\n(cost=0.43..12.46 rows=2 width=104) Index Cond: (book_ref = '03A76D'::bpchar)\n\n−> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) Index Cond: (ticket_no = t.ticket_no)\n\n(7 rows)\n\nIn this case, the cost of each subsequent scan of the inner set is the same as that of the first scan. So we ultimately get the following figures:\n\n=> SELECT 0.43 + 0.56 AS startup_cost,\n\nround((\n\n12.46 + 2 * 16.57 + 6 * current_setting('cpu_tuple_cost')::real\n\n)::numeric, 2) AS total_cost;\n\nstartup_cost | total_cost −−−−−−−−−−−−−−+−−−−−−−−−−−−\n\n0.99 |\n\n45.66\n\n(1 row)\n\nCaching Rows (Memoization)\n\nIf the inner set is repeatedly scanned with the same parameter values (thus giving the same results), it may turn out to be beneficial to cache the rows of this set.\n\nSuch caching is performed by the Memoize1 node. Being similar to the Materialize node, it is designed to handle parameterized joins and has a much more complex implementation:\n\n1 backend/executor/nodeMemoize.c\n\n409\n\nv. ��",
      "content_length": 1338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "4MB 1.0\n\nChapter 21 Nested Loop\n\nThe Materialize node simply materializes all the rows returned by its child node, while Memoize ensures that the rows returned for different parameter values are kept separately.\n\nIn the event of an overflow, the Materialize storage starts spilling rows to disk, while Memoize keeps all the rows in memory (there would otherwise be no point in caching).\n\nHere is an example of a query that uses Memoize:\n\n=> EXPLAIN SELECT * FROM flights f\n\nJOIN aircrafts_data a ON f.aircraft_code = a.aircraft_code\n\nWHERE f.flight_no = 'PG0003';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n(cost=5.44..387.10 rows=113 width=135)\n\n−> Bitmap Heap Scan on flights f\n\n(cost=5.30..382.22 rows=113 width=63) Recheck Cond: (flight_no = 'PG0003'::bpchar) −> Bitmap Index Scan on flights_flight_no_scheduled_depart...\n\n(cost=0.00..5.27 rows=113 width=0) Index Cond: (flight_no = 'PG0003'::bpchar) (cost=0.15..0.27 rows=1 width=72)\n\n−> Memoize\n\nCache Key: f.aircraft_code Cache Mode: logical −> Index Scan using aircrafts_pkey on aircrafts_data a\n\n(cost=0.14..0.26 rows=1 width=72) Index Cond: (aircraft_code = f.aircraft_code)\n\n(13 rows)\n\nwork_mem × The size of the memory chunk used to store cached rows equals hash_mem_multiplier. As implied by the second parameter’s name, cached rows × are stored in a hash table (with open addressing).1 The hash key (shown as Cache Key in the plan) is the parameter value (or several values if there are more than one parameter).\n\nAll the hash keys are bound into a list; one of its ends is considered cold (since it contains the keys that have not been used for a long time), while the other is hot (it stores recently used keys).\n\n1 include/lib/simplehash.h\n\n410",
      "content_length": 1764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "21.2 Nested Loop Joins\n\nIf a call on the Memoize node shows that the passed parameter values correspond to the already cached rows,these rows will be passed on to the parent node (Nested Loop) without checking the child node. The used hash key is then moved to the hot end of the list.\n\nIf the cache does not contain the required rows,the Memoize node pulls them from its child node, caches them, and passes them on to the node above. The corre- sponding hash key also becomes hot.\n\nAs newdata is being cached,it can fill all the available memory. Tofree some space, the rows that correspond to cold keys get evicted. This eviction algorithm differs from the one used in the buffer\n\ncache but serves the same purpose.\n\nSome parameter values may turn out to have so many matching rows that they do not fit into the allocated memory chunk, even if all the other rows are already evicted. Such parameters are skipped—it makes no sense to cache only some of the rows since the next call will still have to get all the rows from the child node.\n\nCost and cardinality estimations. These calculations are quite similar to what we have already seen above. We just have to bear in mind that the cost of the Memoize node shown in the plan has nothing to do with its actual cost: it is simply the cost of its child node increased by the\n\ncpu_tuple_cost value.1\n\nWe have already come across a similar situation for the Materialize node: its cost is only calculated for subsequent scans2 and is not reflected in the plan.\n\nClearly,it only makes sense to use Memoize if it is cheaper than its child node. The cost of each subsequent Memoize scan depends on the expected cache access pro- file and the size of the memory chunk that can be used for caching. The calculated value is highly dependent on the accurate estimation of the number of distinct parameter values to be used in the scans of the inner set of rows.3 Based on this number,you can weigh the probabilities of the rows to be cached and to be evicted from the cache. The expected hits reduce the estimated cost, while potential evic- tions increase it. We will skip the details of these calculations here.\n\n1 backend/optimizer/util/pathnode.c, create_memoize_path function 2 backend/optimizer/path/costsize.c, cost_memoize_rescan function 3 backend/utils/adt/selfuncs.c, estimate_num_groups function\n\n411\n\np. ���\n\n0.01",
      "content_length": 2367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "on\n\nChapter 21 Nested Loop\n\nTo figure out what is actually going on during query execution, we will use the ������� ������� command, as usual:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM flights f\n\nJOIN aircrafts_data a ON f.aircraft_code = a.aircraft_code\n\nWHERE f.flight_no = 'PG0003';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop (actual rows=113 loops=1) −> Bitmap Heap Scan on flights f (actual rows=113 loops=1) Recheck Cond: (flight_no = 'PG0003'::bpchar) Heap Blocks: exact=2 −> Bitmap Index Scan on flights_flight_no_scheduled_depart...\n\n(actual rows=113 loops=1) Index Cond: (flight_no = 'PG0003'::bpchar)\n\n−> Memoize (actual rows=1 loops=113)\n\nCache Key: f.aircraft_code Cache Mode: logical Hits: 112 Usage: 1kB −> Index Scan using aircrafts_pkey on aircrafts_data a\n\nMisses: 1\n\nEvictions: 0\n\nOverflows: 0\n\nMemory\n\n(actual rows=1 loops=1) Index Cond: (aircraft_code = f.aircraft_code)\n\n(16 rows)\n\nThis query selects the flights that follow the same route and are performed by air- craft of a particular type, so all the calls on the Memoize node use the same hash key. The first row has to be fetched from the table (Misses: 1), but all the subse- quent rows are found in the cache (Hits: 112). The whole operation takes just � k� of memory.\n\nThe other two displayed values are zero: they represent the number of evictions and the number of cache overflows when it was impossible to cache all the rows related to a particular set of parameters. Large figures would indicate that the allocated cache is too small, which might be caused by inaccurate estimation of the number of distinct parameter values. Then the use of the Memoize node can turn out to be quite expensive. In the extreme case, you can forbid the planner to use caching by turning off the\n\nenable_memoize parameter.\n\n412",
      "content_length": 1875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "21.2 Nested Loop Joins\n\nOuter Joins\n\nThe nested loop join can be used to perform the left outer join:\n\n=> EXPLAIN SELECT * FROM ticket_flights tf\n\nLEFT JOIN boarding_passes bp ON bp.ticket_no = tf.ticket_no AND bp.flight_id = tf.flight_id\n\nWHERE tf.ticket_no = '0005434026720';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop Left Join\n\n(cost=1.12..33.35 rows=3 width=57)\n\nJoin Filter: ((bp.ticket_no = tf.ticket_no) AND (bp.flight_id = tf.flight_id)) −> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) Index Cond: (ticket_no = '0005434026720'::bpchar)\n\n−> Materialize\n\n(cost=0.56..16.62 rows=3 width=25)\n\n−> Index Scan using boarding_passes_pkey on boarding_passe...\n\n(cost=0.56..16.61 rows=3 width=25) Index Cond: (ticket_no = '0005434026720'::bpchar)\n\n(10 rows)\n\nHere the join operation is represented by the Nested Loop Left Join node. The plan- ner has chosen a non-parameterized join with a filter: it performs identical scans of the inner set of rows (so this set is hidden behind the Materialize node) and re- turns the rows that satisfy the filter condition (Join Filter).\n\nThe cardinality of the outer join is estimated just like the one of the inner join, except that the calculated estimation is compared with the cardinality of the outer set of rows, and the bigger value is taken as the final result.1 In other words, the outer join never reduces the number of rows (but can increase it).\n\nThe cost estimation is similar to that of the inner join.\n\nWe must also keep in mind that the planner can select different plans for inner and outer joins. Even this simple example will have a different Join Filter if the planner is forced to use a nested loop join:\n\n1 backend/optimizer/path/costsize.c, calc_joinrel_size_estimate function\n\n413",
      "content_length": 1842,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "Chapter 21 Nested Loop\n\n=> SET enable_mergejoin = off;\n\n=> EXPLAIN SELECT * FROM ticket_flights tf\n\nJOIN boarding_passes bp ON bp.ticket_no = tf.ticket_no AND bp.flight_id = tf.flight_id\n\nWHERE tf.ticket_no = '0005434026720';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n(cost=1.12..33.33 rows=3 width=57)\n\nJoin Filter: (tf.flight_id = bp.flight_id) −> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..16.58 rows=3 width=32) Index Cond: (ticket_no = '0005434026720'::bpchar)\n\n−> Materialize\n\n(cost=0.56..16.62 rows=3 width=25)\n\n−> Index Scan using boarding_passes_pkey on boarding_passe...\n\n(cost=0.56..16.61 rows=3 width=25) Index Cond: (ticket_no = '0005434026720'::bpchar)\n\n(9 rows)\n\n=> RESET enable_mergejoin;\n\nA slight difference in the total cost is caused by the fact that the outer join must also check ticket numbers to get the correct result if there is no match in the outer set of rows.\n\nRight joins are not supported,1 as the nested loop algorithm treats the inner and outer sets differently. The outer set is scanned in full; as forthe inner set,the index access allows reading only those rows that satisfy the join condition,so some of its rows may be skipped altogether.\n\nA full join is not supported for the same reason.\n\nAnti- and Semi-joins\n\nAnti-joins and semi-joins are similar in the sense that for each row of the first (outer) set it is enough to find only one matching row in the second (inner) set.\n\nAnanti-joinreturnstherowsofthefirstsetonlyiftheyhavenomatchinthesecond set: as soon as the executor finds the first matching row in the second set, it can\n\n1 backend/optimizer/path/joinpath.c, match_unsorted_outer function\n\n414",
      "content_length": 1726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "21.2 Nested Loop Joins\n\nexit the current loop: the corresponding row of the first set must be excluded from the result.\n\nAnti-joins can be used to compute the ��� ������ predicate.\n\nFor example, let’s find aircraft models with undefined cabin configuration. The corresponding plan contains the Nested Loop Anti Join node:\n\n=> EXPLAIN SELECT * FROM aircrafts a WHERE NOT EXISTS (\n\nSELECT * FROM seats s WHERE s.aircraft_code = a.aircraft_code\n\n);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop Anti Join\n\n(cost=0.28..4.65 rows=1 width=40)\n\n−> Seq Scan on aircrafts_data ml −> Index Only Scan using seats_pkey on seats s\n\n(cost=0.00..1.09 rows=9 widt...\n\n(cost=0.28..5.55 rows=149 width=4) Index Cond: (aircraft_code = ml.aircraft_code)\n\n(5 rows)\n\nAn alternative query without the ��� ������ predicate will have the same plan:\n\n=> EXPLAIN SELECT a.* FROM aircrafts a\n\nLEFT JOIN seats s ON a.aircraft_code = s.aircraft_code\n\nWHERE s.aircraft_code IS NULL;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop Anti Join\n\n(cost=0.28..4.65 rows=1 width=40)\n\n−> Seq Scan on aircrafts_data ml −> Index Only Scan using seats_pkey on seats s\n\n(cost=0.00..1.09 rows=9 widt...\n\n(cost=0.28..5.55 rows=149 width=4) Index Cond: (aircraft_code = ml.aircraft_code)\n\n(5 rows)\n\nA semi-join returns those rows of the first set that have at least one match in the second set (again, there is no need to check the set for other matches—the result is already known).\n\nA semi-join can be used to compute the ������ predicate. Let’s find the aircraft models with seats installed in the cabin:\n\n415",
      "content_length": 1662,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "Chapter 21 Nested Loop\n\n=> EXPLAIN SELECT * FROM aircrafts a WHERE EXISTS (\n\nSELECT * FROM seats s WHERE s.aircraft_code = a.aircraft_code\n\n);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop Semi Join\n\n(cost=0.28..6.67 rows=9 width=40)\n\n−> Seq Scan on aircrafts_data ml −> Index Only Scan using seats_pkey on seats s\n\n(cost=0.00..1.09 rows=9 widt...\n\n(cost=0.28..5.55 rows=149 width=4) Index Cond: (aircraft_code = ml.aircraft_code)\n\n(5 rows)\n\nThe Nested Loop Semi Join node represents the same-name join method. This plan (just like the anti-join plans above) provides the basic estimation of the number of rows in the seats table (rows=149), although it is enough to retrieve only one of them. The actual query execution stops after fetching the first row, of course:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM aircrafts a WHERE EXISTS (\n\nSELECT * FROM seats s WHERE s.aircraft_code = a.aircraft_code\n\n);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop Semi Join (actual rows=9 loops=1)\n\n−> Seq Scan on aircrafts_data ml (actual rows=9 loops=1) −> Index Only Scan using seats_pkey on seats s\n\n(actual rows=1 loops=9) Index Cond: (aircraft_code = ml.aircraft_code) Heap Fetches: 0\n\n(6 rows)\n\nCardinality estimation. The selectivity of a semi-join is estimated in the usual manner, except that the cardinality of the inner set is taken as one. For anti-joins, the estimated selectivity is subtracted from one, just like for negation.1\n\n1 backend/optimizer/path/costsize.c, calc_joinrel_size_estimate function\n\n416",
      "content_length": 1627,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "21.2 Nested Loop Joins\n\nCostestimation. Foranti-andsemi-joins,thecostestimationreflectsthefactthat the scan of the second set stops as soon as the first matching row is found.1\n\nNon-Equi-joins\n\nThe nested loop algorithm allows joining sets of rows based on any join condition.\n\nObviously, if the inner set is a base table with an index created on it, and the join condition uses an operator that belongs to an operator class of this index, the ac- cess to the inner set can be quite efficient. But it is always possible to perform the join by calculating a Cartesian product of rows filtered by some condition—which can be absolutely arbitrary in this case. Like in the following query, which selects pairs of airports that are located close to each other:\n\n=> CREATE EXTENSION earthdistance CASCADE;\n\n=> EXPLAIN (costs off) SELECT * FROM airports a1\n\nJOIN airports a2 ON a1.airport_code != a2.airport_code\n\nAND a1.coordinates <@> a2.coordinates < 100;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\nJoin Filter: ((ml.airport_code <> ml_1.airport_code) AND ((ml.coordinates <@> ml_1.coordinates) < '100'::double precisi... −> Seq Scan on airports_data ml −> Materialize\n\n−> Seq Scan on airports_data ml_1\n\n(6 rows)\n\nParallel Mode\n\nA nested loop join can participate in parallel\n\nplan execution.2\n\nIt is only the outer set that can be processed in parallel, as it can be scanned by several workers simultaneously. Having fetched an outer row, each worker then has to search for the matching rows in the inner set, which is done sequentially.\n\n1 backend/optimizer/path/costsize.c, final_cost_nestloop function 2 backend/optimizer/path/joinpath.c, consider_parallel_nestloop function\n\n417\n\np. ���\n\nv. �.�\n\np. ���",
      "content_length": 1758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "p. ���\n\nChapter 21 Nested Loop\n\nThe query shown below includes several joins; it searches for passengers that have tickets for a particular flight:\n\n=> EXPLAIN (costs off) SELECT t.passenger_name FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no JOIN flights f ON f.flight_id = tf.flight_id\n\nWHERE f.flight_id = 12345;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nNested Loop\n\n−> Index Only Scan using flights_flight_id_status_idx on fligh...\n\nIndex Cond: (flight_id = 12345)\n\n−> Gather\n\nWorkers Planned: 2 −> Nested Loop\n\n−> Parallel Seq Scan on ticket_flights tf\n\nFilter: (flight_id = 12345)\n\n−> Index Scan using tickets_pkey on tickets t Index Cond: (ticket_no = tf.ticket_no)\n\n(10 rows)\n\nAt the upper level, the nested loop join is performed sequentially. The outer set consists of a single row of the flights table fetched by a unique key, so the use of a nested loop is justified even for a large number of inner rows.\n\nThe inner set is retrieved using a parallel plan. Each of the workers scans its own share of rows of the ticket_flights table and joins them with tickets using the nested loop algorithm.\n\n418",
      "content_length": 1174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "22\n\nHashing\n\n22.1 Hash Joins\n\nOne-Pass Hash Joins\n\nA hash join searches for matching rows using a pre-built hash table. Here is an example of a plan with such a join:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Join\n\nHash Cond: (tf.ticket_no = t.ticket_no) −> Seq Scan on ticket_flights tf −> Hash\n\n−> Seq Scan on tickets t\n\n(5 rows)\n\nAt the first stage, the Hash Join node1 calls the Hash node,2 which pulls the whole inner set of rows from its child node and places it into a hash table.\n\nStoring pairs of hash keys and values, the hash table enables fast access to a value by its key; the search time does not depend on the size of the hash table, as hash keys are distributed more or less uniformly between a limited number of buckets. The bucket to which a given key goes is determined by the hash function of the hash key; since the number of buckets is always a power of two, it is enough to take the required number of bits of the computed value.\n\n1 backend/executor/nodeHashjoin.c 2 backend/executor/nodeHash.c\n\n419",
      "content_length": 1145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "p. ���\n\nv. ��\n\n4MB 1.0\n\nChapter 22 Hashing\n\nJust like the buffer table that resolves hash collisions by chaining.1\n\ncache,this implementation uses a dynamically extendible hash\n\nAtthefirst stageofajoinoperation,theinnersetisscanned,andthehashfunction is computed for each of its rows. The columns referenced in the join condition (Hash Cond) serve as the hash key, while the hash table itself stores all the queried fields of the inner set.\n\nA hash join is most efficient if the whole hash table can be accommodated in ���, as the executor manages to process the data in one batch in this case. The size work_mem × of the memory chunk allocated for this purpose is limited by the hash_mem_multiplier value.\n\nwork_mem×hash_mem_multiplier\n\ninner set\n\nouter set\n\nLet’s run ������� ������� to take a look at statistics on memory usage of a query:\n\n=> SET work_mem = '256MB';\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM bookings b\n\nJOIN tickets t ON b.book_ref = t.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Join (actual rows=2949857 loops=1) Hash Cond: (t.book_ref = b.book_ref) −> Seq Scan on tickets t (actual rows=2949857 loops=1) −> Hash (actual rows=2111110 loops=1)\n\nBuckets: 4194304 −> Seq Scan on bookings b (actual rows=2111110 loops=1)\n\nBatches: 1\n\nMemory Usage: 145986kB\n\n(6 rows)\n\n1 backend/utils/hash/dynahash.c\n\n420",
      "content_length": 1402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "22.1 Hash Joins\n\nUnlike a nested loop join, which treats inner and outer sets differently, a hash join canswapthemaround. Thesmallersetisusuallyusedastheinnerone,asitresults in a smaller hash table.\n\nIn this example,the whole table fits into the allocated cache: it takes about ��� �� (Memory Usage) and contains � � = ��� buckets. So the join is performed in one pass (Batches).\n\nBut if the query referred to only one column, the hash table would fit ��� ��:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT b.book_ref FROM bookings b\n\nJOIN tickets t ON b.book_ref = t.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Join (actual rows=2949857 loops=1) Hash Cond: (t.book_ref = b.book_ref) −> Index Only Scan using tickets_book_ref_idx on tickets t\n\n(actual rows=2949857 loops=1) Heap Fetches: 0\n\n−> Hash (actual rows=2111110 loops=1)\n\nBuckets: 4194304 −> Seq Scan on bookings b (actual rows=2111110 loops=1)\n\nBatches: 1\n\nMemory Usage: 113172kB\n\n(8 rows)\n\n=> RESET work_mem;\n\nIt is yet another reason to avoid referring to superfluous fields in a query (which can happen if you are using an asterisk, to give one example).\n\nThe chosen number of buckets should guarantee that each bucket holds only one row on average when the hash table is completely filled with data. Higher density would increase the rate of hash collisions, making the search less efficient, while a less compact hash table would take up too much memory. The estimated number of buckets is increased up to the nearest power of two.1\n\n(If the estimated hash table size exceeds the memory limit based on the average width of a single row, two-pass hashing will be applied.)\n\nA hash join cannot start returning results until the hash table is fully built.\n\n1 backend/executor/nodeHash.c, ExecChooseHashTableSize function\n\n421",
      "content_length": 1852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "p. ���\n\nChapter 22 Hashing\n\nAt the second stage (the hash table is already built by this time), the Hash Join node calls on its second child node to get the outer set of rows. For each scanned row, the hash table is searched for a match. It requires calculating the hash key for the columns of the outer set that are included into the join condition.\n\nouter set\n\nThe found matches are returned to the parent node.\n\nCost estimation. We have already covered cardinality estimation depend on the join method, I will now focus on cost estimation.\n\n; since it does not\n\nThe cost of the Hash node is represented by the total cost of its child node. It is a dummy number that simply fills the slot in the plan.1 All the actual estimations are included into the cost of the Hash Join node.2\n\nHere is an example:\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT * FROM flights f\n\nJOIN seats s ON s.aircraft_code = f.aircraft_code;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=38.13..278507.28 rows=16518865 width=78)\n\nHash Join\n\n(actual rows=16518865 loops=1) Hash Cond: (f.aircraft_code = s.aircraft_code) −> Seq Scan on flights f\n\n(cost=0.00..4772.67 rows=214867 widt...\n\n(actual rows=214867 loops=1)\n\n−> Hash\n\n(cost=21.39..21.39 rows=1339 width=15)\n\n(actual rows=1339 loops=1) Buckets: 2048 −> Seq Scan on seats s\n\nBatches: 1\n\nMemory Usage: 79kB\n\n(cost=0.00..21.39 rows=1339 width=15)\n\n(actual rows=1339 loops=1)\n\n(10 rows)\n\n1 backend/optimizer/plan/createplan.c, create_hashjoin_plan function 2 backend/optimizer/path/costsize.c, initial_cost_hashjoin and final_cost_hashjoin functions\n\n422",
      "content_length": 1633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "22.1 Hash Joins\n\nThe startup cost of the join reflects primarily the cost of hash table creation and includes the following components:\n\nthetotalcostoffetchingtheinnerset,whichisrequiredtobuildthehashtable\n\nthe cost of calculating the hash function of all the columns included into the cpu_operator_cost per op- 0.0025\n\nthe cost of insertion of all the inner rows into the hash table (estimated at\n\ncpu_tuple_cost per inserted row)\n\nthe startup cost of fetching the outer set of rows,which is required to start the\n\njoin operation\n\nThe total cost comprises the startup cost and the cost of the join itself, namely:\n\nthe cost of computing the hash function of all the columns included into the\n\njoin key, for each row of the outer set (cpu_operator_cost)\n\nthecostofjoinconditionrechecks,whicharerequiredtoaddresspossiblehash\n\ncollisions (estimated at cpu_operator_cost per each checked operator)\n\nthe processing cost for each resulting row (cpu_tuple_cost)\n\nThe number of required rechecks is the hardest to estimate. It is calculated by multiplying the number of rows of the outer set by some fraction of the inner set (stored in the hash table). To estimate this fraction, the planner has to take into account that data distribution may be non-uniform. I will spare you the details of these computations;1 in this particular case,this fraction is estimated at �.������.\n\nThus, the cost of our query is estimated as follows:\n\n=> WITH cost(startup) AS (\n\nSELECT round((\n\n21.39 + current_setting('cpu_operator_cost')::real * 1339 + current_setting('cpu_tuple_cost')::real * 1339 + 0.00\n\n)::numeric, 2)\n\n)\n\n1 backend/utils/adt/selfuncs.c, estimate_hash_bucket_stats function\n\n423\n\n0.01",
      "content_length": 1682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "Chapter 22 Hashing\n\nSELECT startup,\n\nstartup + round((\n\n4772.67 + current_setting('cpu_operator_cost')::real * 214867 + current_setting('cpu_operator_cost')::real * 214867 * 1339 *\n\n0.150112 +\n\ncurrent_setting('cpu_tuple_cost')::real * 16518865\n\n)::numeric, 2) AS total\n\nFROM cost;\n\nstartup |\n\ntotal\n\n−−−−−−−−−+−−−−−−−−−−−\n\n38.13 | 278507.26\n\n(1 row)\n\nAnd here is the dependency graph:\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Join\n\n(cost=38.13..278507.28 rows=16518865 width=78) Hash Cond: (f.aircraft_code = s.aircraft_code) −> Seq Scan on flights f\n\n(cost=0.00..4772.67 rows=214867 width=63)\n\n−> Hash\n\n(cost=21.39..21.39 rows=1339 width=15) −> Seq Scan on seats s\n\n(cost=0.00..21.39 rows=1339 width=15)\n\n(9 rows)\n\nTwo-Pass Hash Joins\n\nIf the planner’s estimations show that the hash table will not fit the allocated mem- ory, the inner set of rows is split into batches to be processed separately. The num- ber of batches (just like the number of buckets) is always a power of two; the batch to use is determined by the corresponding number of bits of the hash key.1\n\nAny two matching rows belong to one and the same batch: rows placed into differ- ent batches cannot have the same hash code.\n\n1 backend/executor/nodeHash.c, ExecHashGetBucketAndBatch function\n\n424",
      "content_length": 1295,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "22.1 Hash Joins\n\nAll batches hold an equal number of hash keys. If the data is distributed uniformly, batch sizes will also be roughly the same. The planner can control memory con- sumption by choosing an appropriate number of batches.1\n\nAt the first stage, the executor scans the inner set of rows to build the hash table. If the scanned row belongs to the first batch, it is added to the hash table and kept in ���. Otherwise,it is written into a temporary file (there is a separate file for each batch).2\n\nThe total volume of temporary files that a session can store on disk is limited by the temp_file_limit parameter (temporary tables are not included into this limit). As soon as the session reaches this value, the query is aborted.\n\ninner set\n\nouter set\n\nAt the second stage, the outer set is scanned. If the row belongs to the first batch, it is matched against the hash table, which contains the first batch of rows of the inner set (there can be no matches in other batches anyway).\n\nIftherowbelongstoadifferentbatch,itisstoredinatemporaryfile,whichisagain created separately for each batch. Thus, N batches can use 2(N − 1) files (or fewer if some of the batches turn out to be empty).\n\nOncethesecondstageiscomplete,thememoryallocatedforthehashtableisfreed. At this point, we already have the result of the join for one of the batches.\n\n1 backend/executor/nodeHash.c, ExecChooseHashTableSize function 2 backend/executor/nodeHash.c, ExecHashTableInsert function\n\n425\n\n−1",
      "content_length": 1481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "Chapter 22 Hashing\n\ninner set\n\nouter set\n\nBoth stages are repeated for each of the batches saved on disk: the rows of the inner set are transferred from the temporary file to the hash table; then the rows of the outer set related to the same batch are read from another temporary file and matched against this hash table. Once processed, temporary files get deleted.\n\nouter set\n\nUnlike a similar output for a one-pass join, the output of the ������� command for a two-pass join contains more than one batch. If run with the ������� option, this command also displays statistics on disk access:\n\n=> EXPLAIN (analyze, buffers, costs off, timing off, summary off) SELECT * FROM bookings b\n\nJOIN tickets t ON b.book_ref = t.book_ref;\n\n426",
      "content_length": 734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "22.1 Hash Joins\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Join (actual rows=2949857 loops=1) Hash Cond: (t.book_ref = b.book_ref) Buffers: shared hit=7236 read=55626, temp read=55126 written=55126 −> Seq Scan on tickets t (actual rows=2949857 loops=1)\n\nBuffers: shared read=49415\n\n−> Hash (actual rows=2111110 loops=1)\n\nBuckets: 65536 Buffers: shared hit=7236 read=6211, temp written=10858 −> Seq Scan on bookings b (actual rows=2111110 loops=1)\n\nBatches: 64\n\nMemory Usage: 2277kB\n\nBuffers: shared hit=7236 read=6211\n\n(11 rows)\n\nI have already shown this query above with an increased work_mem setting. The default value of � �� is too small for the whole hash table to fit ���; in this exam- ple, the data is split into �� batches, and the hash table uses �� � = ��� buckets. As the hash table is being built (the Hash node), the data is written into temporary files (temp written); at the join stage (the Hash Join node), temporary files are both read and written (temp read, written).\n\nlog_temp_files param- To collect more statistics on temporary files, you can set the eter to zero. Then the server log will list all the temporary files and their sizes (as they appeared at the time of deletion).\n\nDynamic Adjustments\n\nThe planned course of events may be disrupted by two issues: inaccurate statistics and non-uniform data distribution.\n\nIf the distribution of values in the join key columns is non-uniform, different batches will have different sizes.\n\nIf some batch (except for the very first one) turns out to be too large, all its rows will have to be written to disk and then read from disk. It is the outer set that causes most of the trouble, as it is typically bigger. So if there are regular, non- statistics on ���s of the outer set (that is,the outer set is represented multivariate\n\n427\n\n−1\n\np. ���",
      "content_length": 1855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "p. ���\n\nChapter 22 Hashing\n\nby a table, and the join is performed by a single column),rows with hash codes cor- responding to ���s are considered to be a part of the first batch.1 This technique (called skew optimization) can reduce the �/� overhead of a two-pass join to some extent.\n\nBecause of these two factors, the size of some (or all) batches may exceed the esti- mation. Thenthecorrespondinghashtablewillnotfittheallocatedmemorychunk and will surpass the defined limits.\n\nSoifthehashtablebeingbuiltturnsouttoobig,thenumberofbatchesisincreased (doubled) on the fly. Each batch is virtually split into two new ones: about half of the rows (assuming that the distribution is uniform) is left in the hash table, while the other half is saved into a new temporary file.2\n\nSuch a split can happen even if a one-pass join has been originally planned. In fact, one- and two-pass joins use one and the same algorithm implemented by the same code; I single them out here solely for smoother narration.\n\nThe number of batches cannot be reduced. If it turns out that the planner has overestimated the data size, batches will not be merged together.\n\nIn the case of non-uniform distribution,increasing the number of batches may not help. For example,if the key column contains one and the same value in all its rows, they will be placed into the same batch since the hash function will be returning the same value over and over again. Unfortunately, the hash table will continue growing in this case, regardless of the imposed restrictions.\n\nIn theory, this issue could be addressed by a multi-pass join, which would perform partial scans of the batch, but it is not supported.\n\nTo demonstrate a dynamic increase in the number of batches, we first have to per- : form some manipulations\n\n=> CREATE TABLE bookings_copy (LIKE bookings INCLUDING INDEXES) WITH (autovacuum_enabled = off);\n\n=> INSERT INTO bookings_copy SELECT * FROM bookings;\n\nINSERT 0 2111110\n\n1 backend/executor/nodeHash.c, ExecHashBuildSkewHash function 2 backend/executor/nodeHash.c, ExecHashIncreaseNumBatches function\n\n428",
      "content_length": 2086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "22.1 Hash Joins\n\n=> DELETE FROM bookings_copy WHERE random() < 0.9;\n\nDELETE 1899232\n\n=> ANALYZE bookings_copy;\n\n=> INSERT INTO bookings_copy SELECT * FROM bookings ON CONFLICT DO NOTHING;\n\nINSERT 0 1899232\n\n=> SELECT reltuples FROM pg_class WHERE relname = 'bookings_copy';\n\nreltuples −−−−−−−−−−−\n\n211878\n\n(1 row)\n\nAs a result, we get a new table called bookings_copy. It is an exact copy of the bookings table, but the planner underestimates the number of rows in it by ten times. A similar situation may occur if the hash table is generated for a set of rows produced by another join operation, so there is no reliable statistics available.\n\nThis miscalculation makes the planner think that � buckets are enough, but while the join is being performed, this number grows to ��:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM bookings_copy b\n\nJOIN tickets t ON b.book_ref = t.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Join (actual rows=2949857 loops=1) Hash Cond: (t.book_ref = b.book_ref) −> Seq Scan on tickets t (actual rows=2949857 loops=1) −> Hash (actual rows=2111110 loops=1) Buckets: 65536 (originally 65536)\n\nBatches: 32 (originally 8)\n\nMemory Usage: 4040kB\n\n−> Seq Scan on bookings_copy b (actual rows=2111110 loops=1)\n\n(7 rows)\n\nCost estimation. I have already used this example to demonstrate cost estimation for a one-pass join, but now I am going to reduce the size of available memory to the minimum, so the planner will have to use two batches. It increases the cost of the join:\n\n429",
      "content_length": 1581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "Chapter 22 Hashing\n\n=> SET work_mem = '64kB';\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT * FROM flights f\n\nJOIN seats s ON s.aircraft_code = f.aircraft_code;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=45.13..283139.28 rows=16518865 width=78)\n\nHash Join\n\n(actual rows=16518865 loops=1) Hash Cond: (f.aircraft_code = s.aircraft_code) −> Seq Scan on flights f\n\n(cost=0.00..4772.67 rows=214867 widt...\n\n(actual rows=214867 loops=1)\n\n−> Hash\n\n(cost=21.39..21.39 rows=1339 width=15)\n\n(actual rows=1339 loops=1) Buckets: 2048 −> Seq Scan on seats s\n\nBatches: 2\n\nMemory Usage: 55kB\n\n(cost=0.00..21.39 rows=1339 width=15)\n\n(actual rows=1339 loops=1)\n\n(10 rows)\n\n=> RESET work_mem;\n\nThe cost of the second pass is incurred by spilling rows into temporary files and reading them from these files.\n\nThe startup cost of a two-pass join is based on that of a one-pass join, which is increased by the estimated cost of writing as many pages as required to store all the necessary fields of all the rows of the inner set.1 Although the first batch is not written to disk when the hash table is being built, the estimation does not take it into account and hence does not depend on the number of batches.\n\nIn its turn, the total cost comprises the total cost of a one-pass join and the esti- mated costs of reading the rows of the inner set previously stored on disk, as well as reading and writing the rows of the outer set.\n\nBoth writing and reading are estimated at seq_page_cost per page,as �/� operations are assumed to be sequential.\n\nInthisparticularcase,thenumberofpagesrequiredfortheinnersetisestimatedat �, while the data of the outer set is expected to fit ���� pages. Having added these estimations to the one-pass join cost calculated above, we get the same figures as shown in the query plan:\n\n1 backend/optimizer/path/costsize.c, page_size function\n\n430",
      "content_length": 1907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "22.1 Hash Joins\n\n=> SELECT 38.13 + -- startup cost of a one-pass join\n\ncurrent_setting('seq_page_cost')::real * 7 AS startup,\n\n278507.28 + -- total cost of a one-pass join\n\ncurrent_setting('seq_page_cost')::real * 2 * (7 + 2309) AS total;\n\nstartup |\n\ntotal\n\n−−−−−−−−−+−−−−−−−−−−−\n\n45.13 | 283139.28\n\n(1 row)\n\nThus, if there is not enough memory, the join is performed in two passes and be- comes less efficient. Therefore, it is important to observe the following points:\n\nThe query must be composed in a way that excludes redundant fields from the\n\nhash table.\n\nThe planner must choose the smaller of the two sets of rows when building\n\nthe hash table.\n\nUsing Hash Joins in Parallel Plans\n\nThe hash join algorithm described above can also be used in parallel plans. First, several parallel processes build their own (absolutely identical) hash tables for the inner set, independently of each other; then they start processing the outer set concurrently. The performance gain here is due to each process scanning only its own share of outer rows.\n\nThe following plan uses a regular one-pass hash join:\n\n=> SET work_mem = '128MB';\n\n=> SET enable_parallel_hash = off;\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM bookings b\n\nJOIN tickets t ON t.book_ref = b.book_ref;\n\n431\n\nv. �.�",
      "content_length": 1312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "v. ��\n\nChapter 22 Hashing\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=3 loops=1)\n\nWorkers Planned: 2 Workers Launched: 2 −> Partial Aggregate (actual rows=1 loops=3)\n\n−> Hash Join (actual rows=983286 loops=3)\n\nHash Cond: (t.book_ref = b.book_ref) −> Parallel Index Only Scan using tickets_book_ref...\n\nHeap Fetches: 0\n\n−> Hash (actual rows=2111110 loops=3)\n\nBuckets: 4194304 113172kB −> Seq Scan on bookings b (actual rows=2111110...\n\nBatches: 1\n\nMemory Usage:\n\n(13 rows)\n\n=> RESET enable_parallel_hash;\n\nHere each process hashes the bookings table, then retrieves its own share of outer rows via the Parallel Index Only Scan node, and matches these rows against the resulting hash table.\n\nThe hash table memory limit is applied to each parallel process separately, so the total size of memory allocated for this purpose will be three times bigger than in- dicated in the plan (Memory Usage).\n\nParallel One-Pass Hash Joins\n\nEven though a regular hash join can be quite efficient in parallel plans (especially for small inner sets, for which parallel processing does not make much sense), larger data sets are better handled by a special parallel hash join algorithm.\n\nAn important distinction of the parallel version of the algorithm is that the hash table is created in the shared memory, which is allocated dynamically and can be accessed by all parallel processes that contribute to the join operation. Instead of several separate hash tables, a single common one is built, which uses the total amount of memory dedicated to all the participating processes. It increases the chance of completing the join in one pass.\n\n432",
      "content_length": 1731,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 434,
      "content": "22.1 Hash Joins\n\nAtthefirst stage(representedintheplanbytheParallel Hashnode),alltheparallel processesbuildacommonhashtable,takingadvantageoftheparallelaccesstothe inner set.1\n\nwork_mem×hash_mem_multiplier ×numberofprocesses\n\ninner set\n\nouter set\n\nTo move on from here, each parallel process must complete its share of first-stage processing.2\n\nAt the second stage (the Parallel Hash Join node), the processes are again run in parallel to match their shares of rows of the outer set against the hash table,which is already built by this time.3\n\nouter set\n\nHere is an example of such a plan:\n\n=> SET work_mem = '64MB';\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT count(*) FROM bookings b\n\nJOIN tickets t ON t.book_ref = b.book_ref;\n\n1 backend/executor/nodeHash.c, MultiExecParallelHash function 2 backend/storage/ipc/barrier.c 3 backend/executor/nodeHashjoin.c, ExecParallelHashJoin function\n\n433",
      "content_length": 916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "on\n\nv. ��\n\nChapter 22 Hashing\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate (actual rows=1 loops=1)\n\n−> Gather (actual rows=3 loops=1)\n\nWorkers Planned: 2 Workers Launched: 2 −> Partial Aggregate (actual rows=1 loops=3)\n\n−> Parallel Hash Join (actual rows=983286 loops=3)\n\nHash Cond: (t.book_ref = b.book_ref) −> Parallel Index Only Scan using tickets_book_ref...\n\nHeap Fetches: 0\n\n−> Parallel Hash (actual rows=703703 loops=3)\n\nBuckets: 4194304 115392kB −> Parallel Seq Scan on bookings b (actual row...\n\nBatches: 1\n\nMemory Usage:\n\n(13 rows)\n\n=> RESET work_mem;\n\nIt is the same query that I showed in the previous section,but the parallel hash join was turned off by the\n\nenable_parallel_hash parameter at that time.\n\nAlthough the available memory is down by half as compared to a regular hash join demonstrated before, the operation still completes in one pass because it uses the memory allocated for all the parallel processes (Memory Usage). The hash table gets a bit bigger, but since it is the only one we have now, the total memory usage has decreased.\n\nParallel Two-Pass Hash Joins\n\nThe consolidated memory of all the parallel processes may still be not enough to accommodate the whole hash table. It can become clear either at the planning stage or later,during query execution. The two-pass algorithm applied in this case is quite different from what we have seen so far.\n\nThe key distinction of this algorithm is that it creates several smaller hash ta- bles instead of a single big one. Each process gets its own table and processes its own batches independently. (But since separate hash tables are still located in the shared memory, any process can get access to any of these tables.) If planning\n\n434",
      "content_length": 1773,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "22.1 Hash Joins\n\nshows that more than one batch will be required,1 a separate hash table is built for each process right away. If the decision is taken at the execution stage, the hash table is rebuilt.2\n\nThus, at the first stage processes scan the inner set in parallel, splitting it into batches and writing them into temporary files.3 Since each process reads only its own share of the inner set, none of them builds a full hash table for any of the batches(evenforthefirstone). Thefullsetofrowsofanybatchisonlyaccumulated inthefilewrittenbyalltheparallelprocessesinasynchronizedmanner.4 Sounlike the non-parallel and one-pass parallel versions of the algorithm, the parallel two- pass hash join writes all the batches to disk, including the first one.\n\ninner set\n\nouter set\n\nOnce all the processes have completed hashing of the inner set, the second stage begins.5\n\nIf the non-parallel version of the algorithm were employed, the rows of the outer set that belong to the first batch would be matched against the hash table right away. But in the case of the parallel version, the memory does not contain the hash table yet, so the workers process the batches independently. Therefore, the second stage starts by a parallel scan of the outer set to distribute its rows into batches, and each batch is written into a separate temporary file.6 The scanned\n\n1 backend/executor/nodeHash.c, ExecChooseHashTableSize function 2 backend/executor/nodeHash.c, ExecParallelHashIncreaseNumBatches function 3 backend/executor/nodeHash.c, MultiExecParallelHash function 4 backend/utils/sort/sharedtuplestore.c 5 backend/executor/nodeHashjoin.c, ExecParallelHashJoin function 6 backend/executor/nodeHashjoin.c, ExecParallelHashJoinPartitionOuter function\n\n435",
      "content_length": 1747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "Chapter 22 Hashing\n\nrows are not inserted into the hash table (as it happens at the first stage), so the number of batches never rises.\n\nOnce all the processes have completed the scan of the outer set, we get 2N tempo- rary files on disk; they contain the batches of the inner and outer sets.\n\ninner set\n\nouter set\n\nThen each process chooses one of the batches and performs the join: it loads the inner set of rows into a hash table in memory, scans the rows of the outer set, and matches them against the hash table. When the batch join is complete,the process chooses the next batch that has not been processed yet.1\n\ninner set\n\nouter set\n\nIf no more unprocessed batches are left, the process that has completed its own batch starts processing one of the batches that is currently being handled by an- other process; such concurrent processing is possible because all the hash tables are located in the shared memory.\n\n1 backend/executor/nodeHashjoin.c, ExecParallelHashJoinNewBatch function\n\n436",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "22.1 Hash Joins\n\nouter set\n\nThis approach is more efficient than using a single big hash table for all the pro- cesses: it is easier to set up parallel processing, and synchronization is cheaper.\n\nModifications\n\nThe hash join algorithm supports any types of joins: apart from the inner join, it can also handle left, right, and full outer joins, as well as semi- and anti-joins. But as I have already mentioned, the join condition is limited to the equality operator.\n\nWe have already observed some of these operations loop join. Here is an example of the right outer join:\n\nwhen dealing with the nested\n\n=> EXPLAIN (costs off) SELECT * FROM bookings b\n\nLEFT OUTER JOIN tickets t ON t.book_ref = b.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Right Join\n\nHash Cond: (t.book_ref = b.book_ref) −> Seq Scan on tickets t −> Hash\n\n−> Seq Scan on bookings b\n\n(5 rows)\n\nNote that the logical left join specified in the ��� query got transformed into a physical operation of the right join in the execution plan.\n\nAt the logical level, bookings is the outer table (constituting the left side of the join operation), while the tickets table is the inner one. Therefore, bookings with no tickets must also be included into the join result.\n\n437\n\np. ���",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "Chapter 22 Hashing\n\nAt the physical level, inner and outer sets are assigned based on the cost of the join rather than their location in the query text. It usually means that the set with a smaller hash table will be used as the inner one. This is exactlywhat is happening here: the bookings table is used as the inner set, and the left join is changed to the right one.\n\nAnd vice versa,if the query specifies the right outer join (to display the tickets that are not related to any bookings), the execution plan uses the left join:\n\n=> EXPLAIN (costs off) SELECT * FROM bookings b\n\nRIGHT OUTER JOIN tickets t ON t.book_ref = b.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Left Join\n\nHash Cond: (t.book_ref = b.book_ref) −> Seq Scan on tickets t −> Hash\n\n−> Seq Scan on bookings b\n\n(5 rows)\n\nTo complete the picture, I will provide an example of a query plan with the full outer join:\n\n=> EXPLAIN (costs off) SELECT * FROM bookings b\n\nFULL OUTER JOIN tickets t ON t.book_ref = b.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHash Full Join\n\nHash Cond: (t.book_ref = b.book_ref) −> Seq Scan on tickets t −> Hash\n\n−> Seq Scan on bookings b\n\n(5 rows)\n\nParallel hash joins are currently not supported for right and full joins.1\n\nNotethatthenextexampleusesthebookingstableastheouterset,buttheplanner would have preferred the right join if it were supported:\n\n1 commitfest.postgresql.org/33/2903\n\n438",
      "content_length": 1436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "22.2 Distinct Values and Grouping\n\n=> EXPLAIN (costs off) SELECT sum(b.total_amount) FROM bookings b\n\nLEFT OUTER JOIN tickets t ON t.book_ref = b.book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n−> Gather\n\nWorkers Planned: 2 −> Partial Aggregate\n\n−> Parallel Hash Left Join\n\nHash Cond: (b.book_ref = t.book_ref) −> Parallel Seq Scan on bookings b −> Parallel Hash\n\n−> Parallel Index Only Scan using tickets_book...\n\n(9 rows)\n\n22.2 Distinct Values and Grouping\n\nAlgorithms that group values for aggregation and remove duplicates are very sim- ilar to join algorithms. One of the approaches they can use consists in building a hash table on the required columns. Values are included into the hash table only if it contains no such values yet. As a result, the hash table accumulates all the distinct values.\n\nThe node that performs hash aggregation is called HashAggregate.1\n\nLet’s consider some situations that may require this node.\n\nThe number of seats in each travel class (����� ��):\n\n=> EXPLAIN (costs off) SELECT fare_conditions, count(*) FROM seats GROUP BY fare_conditions;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate\n\nGroup Key: fare_conditions −> Seq Scan on seats\n\n(3 rows)\n\n1 backend/executor/nodeAgg.c\n\n439",
      "content_length": 1296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 441,
      "content": "4MB 1.0\n\nChapter 22 Hashing\n\nThe list of travel classes (��������):\n\n=> EXPLAIN (costs off) SELECT DISTINCT fare_conditions FROM seats;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate\n\nGroup Key: fare_conditions −> Seq Scan on seats\n\n(3 rows)\n\nTravel classes combined with one more value (�����):\n\n=> EXPLAIN (costs off) SELECT fare_conditions FROM seats UNION SELECT NULL;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate\n\nGroup Key: seats.fare_conditions −> Append\n\n−> Seq Scan on seats −> Result\n\n(5 rows)\n\nThe Append node combines both sets but does not get rid of any duplicates, which must not appear in the ����� result. They have to be removed separately by the HashAggregate node.\n\nThe memory chunk allocated for the hash table is limited by the hash_mem_multiplier value, just like in the case of a hash join.\n\nwork_mem ×\n\nIf the hash table fits the allocated memory, aggregation uses a single batch:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT DISTINCT amount FROM ticket_flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate (actual rows=338 loops=1)\n\nGroup Key: amount Batches: 1 −> Seq Scan on ticket_flights (actual rows=8391852 loops=1)\n\nMemory Usage: 61kB\n\n(4 rows)\n\n440",
      "content_length": 1279,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "22.2 Distinct Values and Grouping\n\nThere are not so many distinct values in the amounts field, so the hash table takes only �� k� (Memory Usage).\n\nthe hash table fills up the allocated memory, all the further values are As soon as spilled into temporary files and grouped into partitions based on several bits of their hash values. The number of partitions is a power of two and is chosen in such a way that each of their hash tables fits the allocated memory. The accuracy of the estimation is of course dependent on the quality of the collected statistics, so the received number is multiplied by �.� to further reduce partition sizes and raise the chances of processing each partition in one pass.1\n\nOnce the whole set is scanned, the node returns aggregation results for those val- ues that have made it into the hash table.\n\nThen the hash table is cleared,and each of the partitions saved into temporary files at the previous stage is scanned and processed just like any other set of rows. If the hash table still exceeds the allocated memory,the rows that are subject to overflow will be partitioned again and written to disk for further processing.\n\nTo avoid excessive �/�, the two-pass hash join algorithm moves ���s into the first batch. Aggregation, however, does not require this optimization: those rows that fit the allocated memory will not be split into partitions, and ���s are likely to occur early enough to get into ���.\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT DISTINCT flight_id FROM ticket_flights;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nHashAggregate (actual rows=150588 loops=1)\n\nGroup Key: flight_id Batches: 5 −> Seq Scan on ticket_flights (actual rows=8391852 loops=1)\n\nMemory Usage: 4145kB\n\nDisk Usage: 98184kB\n\n(4 rows)\n\nIn this example,the number of distinct ��s is relatively high,so the hash table does not fit the allocated memory. It takes five batches to perform the query: one for the initial data set and four for the partitions written to disk.\n\n1 backend/executor/nodeAgg.c, hash_choose_num_partitions function\n\n441\n\nv. ��",
      "content_length": 2123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "23\n\nSorting and Merging\n\n23.1 Merge Joins\n\nA merge join processes data sets sorted by the join key and returns the result that is sorted in a similar way. Input sets may come pre-sorted following an index scan; otherwise, the executor has to sort them before the actual merge begins.1\n\nMerging Sorted Sets\n\nLet’s take a look at an example of a merge join; it is represented in the execution plan by the Merge Join node:2\n\n=> EXPLAIN (costs off) SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nORDER BY t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nMerge Join\n\nMerge Cond: (t.ticket_no = tf.ticket_no) −> Index Scan using tickets_pkey on tickets t −> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(4 rows)\n\nTheoptimizerprefersthisjoinmethodbecauseitreturnsasortedresult,asdefined by the ����� �� clause. When choosing a plan, the optimizer notes the sort order of the data sets and does not perform any sorting unless it is really required. For\n\n1 backend/optimizer/path/joinpath.c, generate_mergejoin_paths function 2 backend/executor/nodeMergejoin.c\n\n442",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "23.1 Merge Joins\n\nexample, if the data set produced by a merge join already has an appropriate sort order, it can be used in the subsequent merge join as is:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON t.ticket_no = tf.ticket_no JOIN boarding_passes bp ON bp.ticket_no = tf.ticket_no AND bp.flight_id = tf.flight_id\n\nORDER BY t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nMerge Join\n\nMerge Cond: (tf.ticket_no = t.ticket_no) −> Merge Join\n\nMerge Cond: ((tf.ticket_no = bp.ticket_no) AND (tf.flight_... −> Index Scan using ticket_flights_pkey on ticket_flights tf −> Index Scan using boarding_passes_pkey on boarding_passe...\n\n−> Index Scan using tickets_pkey on tickets t\n\n(7 rows)\n\nThe first tables to be joined are ticket_flights and boarding_passes; both of them have a composite primary key (ticket_no,flight_id), and the result is sorted by these two columns. The produced set of rows is then joined with the tickets table, which is sorted by the ticket_no column.\n\nThejoinrequiresonlyonepassoverbothdatasetsanddoesnottakeanyadditional memory. It uses two pointers to the current rows (which are originally the first ones) of the inner and outer sets.\n\nIf the keys of the current rows do not match, one of the pointers (that references the row with the smaller key) is going to be advanced to the next row until it finds a match. The joined rows are returned to the upper node, and the pointer of the inner set is advanced by one place. The operation continues until one of the sets is over.\n\nThis algorithm copes with duplicates of the inner set,but the outer set can contain them too. Therefore, the algorithm has to be improved: if the key remains the same after the outer pointer is advanced, the inner pointer gets back to the first matching row. Thus, each row of the outer set will be matched to all the rows of the inner set with the same key.1\n\n1 backend/executor/nodeMergejoin.c, ExecMergeJoin function\n\n443",
      "content_length": 2003,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "p. ���\n\nChapter 23 Sorting and Merging\n\nFor the outer join,the algorithm is further tweaked a bit,but it is still based on the same principle.\n\nMerge join conditions can use only the equality operator, which means that only equi-joins are supported (although support for other condition types is currently under way too).1\n\nCost estimation. Let’s take a closer look at the previous example:\n\n=> EXPLAIN SELECT * FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nORDER BY t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nMerge Join\n\n(cost=0.99..822355.54 rows=8391852 width=136)\n\nMerge Cond: (t.ticket_no = tf.ticket_no) −> Index Scan using tickets_pkey on tickets t\n\n(cost=0.43..139110.29 rows=2949857 width=104)\n\n−> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(cost=0.56..570972.46 rows=8391852 width=32)\n\n(6 rows)\n\nThe startup cost of the join includes at least the startup costs of all the child nodes.\n\nIn general,it may be required to scan some fraction of the outer or inner set before the first match is found. It is possible to estimate this fraction by comparing (based )thesmallestjoinkeysinthetwosets.2 Butinthisparticularcase, onthehistogram the range of ticket numbers is the same in both tables.\n\nThe total cost comprises the cost of fetching the data from the child nodes and the computation cost.\n\nSince the join algorithm stops as soon as one of the sets is over (unless the outer join is performed, of course), the other set may be scanned only partially. To esti- mate the size of the scanned part, we can compare the maximal key values in the two sets. In this example, both sets will be read in full, so the total cost of the join includes the sum of the total costs of both child nodes.\n\n1 For example, see commitfest.postgresql.org/33/3160 2 backend/utils/adt/selfuncs.c, mergejoinscansel function\n\n444",
      "content_length": 1902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "23.1 Merge Joins\n\nMoreover, if there are any duplicates, some of the rows of the inner set may be scanned several times. The estimated number of repeat scans equals the difference between the cardinalities of the join result and the inner set.1 In this query, these cardinalities are the same, which means that the sets contain no duplicates.\n\nThe algorithm compares join keys of the two sets. The cost of one comparison is cpu_operator_cost value, while the estimated number of compar- estimated at the isons can be taken as the sum of rows of both sets (increased by the number of repeat reads caused by duplicates). The processing cost of each row included into the result is estimated at the\n\ncpu_tuple_cost value, as usual.\n\nThus, in this example the cost of the join is estimated as follows:2\n\n=> SELECT 0.43 + 0.56 AS startup,\n\nround((\n\n139110.29 + 570972.46 + current_setting('cpu_tuple_cost')::real * 8391852 + current_setting('cpu_operator_cost')::real * (2949857 + 8391852)\n\n)::numeric, 2) AS total;\n\nstartup |\n\ntotal\n\n−−−−−−−−−+−−−−−−−−−−−\n\n0.99 | 822355.54\n\n(1 row)\n\nParallel Mode\n\nAlthoughthemergejoin hasnoparallel flavor,itcan stillbeusedin parallelplans.3\n\nThe outer set can be scanned by several workers in parallel, but the inner set is always scanned by each worker in full.\n\nSince the parallel hash join\n\nis almost always cheaper, I will turn it off for a while:\n\n=> SET enable_hashjoin = off;\n\nHere is an example of a parallel plan that uses a merge join:\n\n1 backend/optimizer/path/costsize.c, final_cost_mergejoin function 2 backend/optimizer/path/costsize.c, initial_cost_mergejoin & final_cost_mergejoin functions 3 backend/optimizer/path/joinpath.c, consider_parallel_mergejoin function\n\n445\n\n0.0025\n\n0.01\n\nv. �.�\n\np. ���",
      "content_length": 1747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "Chapter 23 Sorting and Merging\n\n=> EXPLAIN (costs off) SELECT count(*), sum(tf.amount) FROM tickets t\n\nJOIN ticket_flights tf ON tf.ticket_no = t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize Aggregate\n\n−> Gather\n\nWorkers Planned: 2 −> Partial Aggregate −> Merge Join\n\nMerge Cond: (tf.ticket_no = t.ticket_no) −> Parallel Index Scan using ticket_flights_pkey o... −> Index Only Scan using tickets_pkey on tickets t\n\n(8 rows)\n\nFull and right outer merge joins are not allowed in parallel plans.\n\nModifications\n\nThe merge join algorithm can be used with any types of joins. The only restriction is that join conditions of full and right outer joins must contain merge-compatible expressions (“outer-column equals inner-column”or“column equals constant”).1 In- ner and left outer joins simply filter the join result by irrelevant conditions,but for full and right joins such filtering is inapplicable.\n\nHere is an example of a full join that uses the merge algorithm:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets t\n\nFULL JOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nORDER BY t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\nSort Key: t.ticket_no −> Merge Full Join\n\nMerge Cond: (t.ticket_no = tf.ticket_no) −> Index Scan using tickets_pkey on tickets t −> Index Scan using ticket_flights_pkey on ticket_flights tf\n\n(6 rows)\n\n1 backend/optimizer/path/joinpath.c, select_mergejoin_clauses function\n\n446",
      "content_length": 1516,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "23.2 Sorting\n\nInner and left merge joins preserve the sort order. Full and right outer joins, how- ever, cannot guarantee it because ���� values can be wedged in between the or- dered values of the outer set, which breaks the sort order.1 To restore the required order, the planner introduces the Sort node here. Naturally, it increases the cost of the plan, making the hash join more attractive, so the planner has selected this plan only because hash joins are currently disabled.\n\nBut the next example cannot do without a hash join: the nested loop does not allow full joins at all, while merging cannot be used because of an unsupported join condition. So the hash join is used regardless of the enable_hashjoin parameter value:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets t\n\nFULL JOIN ticket_flights tf ON tf.ticket_no = t.ticket_no\n\nAND tf.amount > 0\n\nORDER BY t.ticket_no;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\nSort Key: t.ticket_no −> Hash Full Join\n\nHash Cond: (tf.ticket_no = t.ticket_no) Join Filter: (tf.amount > '0'::numeric) −> Seq Scan on ticket_flights tf −> Hash\n\n−> Seq Scan on tickets t\n\n(8 rows)\n\nLet’s restore the ability to use hash joins that we have previously disabled:\n\n=> RESET enable_hashjoin;\n\n23.2 Sorting\n\nIf one of the sets (or possibly both of them) is not sorted by the join key, it must be reordered before the join operation begins. This sorting operation is represented in the plan by the Sort node:2\n\n1 backend/optimizer/path/pathkeys.c, build_join_pathkeys function 2 backend/executor/nodeSort.c\n\n447",
      "content_length": 1570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "Chapter 23 Sorting and Merging\n\n=> EXPLAIN (costs off) SELECT * FROM flights f\n\nJOIN airports_data dep ON f.departure_airport = dep.airport_code\n\nORDER BY dep.airport_code;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nMerge Join\n\nMerge Cond: (f.departure_airport = dep.airport_code) −> Sort\n\nSort Key: f.departure_airport −> Seq Scan on flights f\n\n−> Sort\n\nSort Key: dep.airport_code −> Seq Scan on airports_data dep\n\n(8 rows)\n\nSuch sorting can also be applied outside the context of joins if the ����� �� clause is specified, both in a regular query and within a window function:\n\n=> EXPLAIN (costs off) SELECT flight_id,\n\nrow_number() OVER (PARTITION BY flight_no ORDER BY flight_id)\n\nFROM flights f;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nWindowAgg −> Sort\n\nSort Key: flight_no, flight_id −> Seq Scan on flights f\n\n(4 rows)\n\nHere the WindowAgg node1 computes a window function on the data set that has been pre-sorted by the Sort node.\n\nThe planner has several sort methods in its toolbox. The example that I have al- ready shown uses two of them (Sort Method). These details can be displayed by the ������� ������� command, as usual:\n\n=> EXPLAIN (analyze,costs off,timing off,summary off) SELECT * FROM flights f\n\nJOIN airports_data dep ON f.departure_airport = dep.airport_code\n\nORDER BY dep.airport_code;\n\n1 backend/executor/nodeWindowAgg.c\n\n448",
      "content_length": 1389,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 450,
      "content": "23.2 Sorting\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nMerge Join (actual rows=214867 loops=1)\n\nMerge Cond: (f.departure_airport = dep.airport_code) −> Sort (actual rows=214867 loops=1) Sort Key: f.departure_airport Sort Method: external merge −> Seq Scan on flights f (actual rows=214867 loops=1)\n\nDisk: 17136kB\n\n−> Sort (actual rows=104 loops=1) Sort Key: dep.airport_code Sort Method: quicksort −> Seq Scan on airports_data dep (actual rows=104 loops=1)\n\nMemory: 52kB\n\n(10 rows)\n\nQuicksort\n\nwork_mem chunk,the classic quicksort method is If the data set to be sorted fits the applied. This algorithm is described in all textbooks, so I am not going to explain it here.\n\nAs for the implementation, sorting is performed by a dedicated component1 that chooses the most suitable algorithm depending on the amount of available mem- ory and some other factors.\n\nCost estimation. Let’s take a look at how a small table is sorted. In this case, sort- ing is performed in memory using the quicksort algorithm:\n\n=> EXPLAIN SELECT * FROM airports_data ORDER BY airport_code;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\n(cost=7.52..7.78 rows=104 width=145)\n\nSort Key: airport_code −> Seq Scan on airports_data\n\n(cost=0.00..4.04 rows=104 width=...\n\n(3 rows)\n\n1 backend/utils/sort/tuplesort.c\n\n449\n\n4MB",
      "content_length": 1374,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "0.0025\n\nChapter 23 Sorting and Merging\n\nThe computational complexity of sorting n values is known to be O(nlog2 n). A cpu_operator_cost value. single comparison operation is estimated at the doubled Since the whole data set must be scanned and sorted before the result can be re- trieved, the startup cost of sorting includes the total cost of the child node and all the expenses incurred by comparison operations.\n\nThe total cost of sorting also includes the cost of processing each row to be re- turned,which is estimated at cpu_operator_cost (and not at the usual cpu_tuple_cost value, as the overhead incurred by the Sort node is insignificant).1\n\nFor this example, the costs are calculated as follows:\n\n=> WITH costs(startup) AS (\n\nSELECT 4.04 + round((\n\ncurrent_setting('cpu_operator_cost')::real * 2 *\n\n104 * log(2, 104)\n\n)::numeric, 2)\n\n) SELECT startup,\n\nstartup + round((\n\ncurrent_setting('cpu_operator_cost')::real * 104\n\n)::numeric, 2) AS total\n\nFROM costs;\n\nstartup | total −−−−−−−−−+−−−−−−−\n\n7.52 |\n\n7.78\n\n(1 row)\n\nTop-N Heapsort\n\nIf a data set needs to be sorted only partially (as defined by the ����� clause), the heapsort method can be applied (it is represented in the plan as top-N heapsort). To be more exact, this algorithm is used if sorting reduces the number of rows at least by half, or if the allocated memory cannot accommodate the whole input set (while the output set fits it).\n\n=> EXPLAIN (analyze, timing off, summary off) SELECT * FROM seats ORDER BY seat_no LIMIT 100;\n\n1 backend/optimizer/path/costsize.c, cost_sort function\n\n450",
      "content_length": 1564,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "23.2 Sorting\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nLimit\n\n(cost=72.57..72.82 rows=100 width=15)\n\n(actual rows=100 loops=1) −> Sort\n\n(cost=72.57..75.91 rows=1339 width=15)\n\n(actual rows=100 loops=1) Sort Key: seat_no Sort Method: top−N heapsort −> Seq Scan on seats\n\nMemory: 33kB\n\n(cost=0.00..21.39 rows=1339 width=15)\n\n(actual rows=1339 loops=1)\n\n(8 rows)\n\nTo find k highest (or lowest) values out of n, the executor adds the first k rows into a data structure called heap. Then the rest of the rows get added one by one, and the smallest (or largest) value is removed from the heap after each iteration. Once all the rows are processed, the heap contains k sought-after values.\n\nThe heap term here denotes a well-known data structure and has nothing to do with database tables, which are often referred to by the same name.\n\nCost estimation. The computational complexity of the algorithm is estimated at O(nlog2 k), but each particular operation is more expensive as compared to the quicksort algorithm. Therefore, the formula uses nlog2 2k.1\n\n=> WITH costs(startup) AS (\n\nSELECT 21.39 + round((\n\ncurrent_setting('cpu_operator_cost')::real * 2 *\n\n1339 * log(2, 2 * 100)\n\n)::numeric, 2)\n\n) SELECT startup,\n\nstartup + round((\n\ncurrent_setting('cpu_operator_cost')::real * 100\n\n)::numeric, 2) AS total\n\nFROM costs;\n\nstartup | total −−−−−−−−−+−−−−−−−\n\n72.57 | 72.82\n\n(1 row)\n\n1 backend/optimizer/path/costsize.c, cost_sort function\n\n451",
      "content_length": 1476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "Chapter 23 Sorting and Merging\n\nExternal Sorting\n\nIf the scan shows that the data set is too big to be sorted in memory, the sorting node switches over to external merge sorting (labeled as external merge in the plan).\n\nThe rows that are already scanned are sorted in memory by the quicksort algorithm and written into a temporary file.\n\n1\n\n2\n\n3\n\n4\n\n5\n\n1\n\nSubsequent rows are then read into the freed memory, and this procedure is re- peated until all the data is written into several pre-sorted files.\n\n2\n\n3\n\n4\n\n5\n\n1\n\n2\n\nNext, these files are merged into one. This operation is performed by roughly the same algorithm that is used for merge joins; the main difference is that it can pro- cess more than two files at a time.\n\nA merge operation does not need too much memory. In fact, it is enough to have room for one row per file. The first rows are read from each file, the row with the lowest value (or the highest one, depending on the sort order) is returned as a partial result, and the freed memory is filled with the next row fetched from the same file.\n\n452",
      "content_length": 1066,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "23.2 Sorting\n\nIn practice, rows are read in batches of �� pages rather than one by one, which reduces the number of �/� operations. The number of files that are merged in a single iteration depends on the available memory, but it is never smaller than six. The upper boundary is also limited (by ���) since efficiency suffers when there are too many files.1\n\nSorting algorithms have long-established terminology. External sorting was originally performed using magnetic tapes,and Postgre��� keeps a similar name for the component that controls temporary files.2 Partially sorted data sets are called “runs.”3 The number of runs participating in the merge is referred to as the“merge order.” I did not use these terms, but they are worth knowing if you want to understand Postgre��� code and comments.\n\nIfthesortedtemporaryfilescannotbemergedallatonce,theyhavetobeprocessed in several passes,their partial results being written into new temporary files. Each iteration increases the volume of data to be read and written, so the more ��� is available, the faster the external sorting completes.\n\n1\n\n2\n\n3\n\n4\n\n5\n\n1+2+3\n\n4\n\n5\n\n1+2+3\n\n4+5\n\n1 backend/utils/sort/tuplesort.c, tuplesort_merge_order function 2 backend/utils/sort/logtape.c 3 Donald E. Knuth. The Art of Computer Programming. Volume III. Sorting and Searching\n\n453",
      "content_length": 1321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "Chapter 23 Sorting and Merging\n\nThe next iteration merges newly created temporary files.\n\n1+2+3\n\n4+5\n\nThe final merge is typically deferred and performed on the fly when the upper node pulls the data.\n\nLet’srunthe��������������commandtoseehowmuchdiskspacehasbeenusedby external sorting. The ������� option displays buffer usage statistics for temporary files (temp read and written). The number of written buffers will be (roughly) the same as the number of read ones; converted to kilobytes, this value is shown as Disk in the plan:\n\n=> EXPLAIN (analyze, buffers, costs off, timing off, summary off) SELECT * FROM flights ORDER BY scheduled_departure;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort (actual rows=214867 loops=1) Sort Key: scheduled_departure Sort Method: external merge Buffers: shared hit=2627, temp read=2142 written=2150 −> Seq Scan on flights (actual rows=214867 loops=1)\n\nDisk: 17136kB\n\nBuffers: shared hit=2624\n\n(6 rows)\n\nTo print more details on using temporary files into the server log, you can enable the log_temp_files parameter.\n\nCost estimation. Let’s take the same plan with external sorting as an example:\n\n=> EXPLAIN SELECT * FROM flights ORDER BY scheduled_departure;\n\n454",
      "content_length": 1235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "23.2 Sorting\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSort\n\n(cost=31883.96..32421.12 rows=214867 width=63)\n\nSort Key: scheduled_departure −> Seq Scan on flights\n\n(cost=0.00..4772.67 rows=214867 width=63)\n\n(3 rows)\n\nHere the regular cost of comparisons (their number is the same as in the case of a quicksort operation in memory) is extended by the �/� cost.1 All the input data has to be first written into temporary files on disk and then read from disk during the merge operation (possibly more than once if all the created files cannot be merged in one iteration).\n\nIt is assumed that three quarters of disk operations (both reads and writes) are sequential, while one quarter is random.\n\nThe volume of data written to disk depends on the number of rows to be sorted and the number of columns used in the query.2 In this example,the query displays all the columns of the flights table, so the size of the data spilled to disk is almost the same as the size of the whole table if its tuple and page metadata are not taken into account (���� pages instead of ����).\n\nHere sorting is completed in one iteration.\n\nTherefore, the sorting cost is estimated in this plan as follows:\n\n=> WITH costs(startup) AS ( SELECT 4772.67 + round((\n\ncurrent_setting('cpu_operator_cost')::real * 2 *\n\n214867 * log(2, 214867) +\n\n(current_setting('seq_page_cost')::real * 0.75 +\n\ncurrent_setting('random_page_cost')::real * 0.25) *\n\n2 * 2309 * 1 -- one iteration\n\n)::numeric, 2)\n\n) SELECT startup,\n\nstartup + round((\n\ncurrent_setting('cpu_operator_cost')::real * 214867\n\n)::numeric, 2) AS total\n\nFROM costs;\n\n1 backend/optimizer/path/costsize.c, cost_sort function 2 backend/optimizer/path/costsize.c, relation_byte_size function\n\n455",
      "content_length": 1757,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "v. ��\n\nChapter 23 Sorting and Merging\n\n| −−−−−−−−−−+−−−−−−−−−− 31883.96 | 32421.13\n\nstartup\n\ntotal\n\n(1 row)\n\nIncremental Sorting\n\nIf a data set has to be sorted by keys K1 …Km …Kn, and this data set is known to be already sorted by the first m keys, you do not have to re-sort it from scratch. Instead,you can split this set into groups by the same first keys K1 …Km (values in these groups already follow the defined order), and then sort each of these groups separately by the remaining Km+1 …Kn keys. This method is called the incremental sort.\n\nIncremental sorting is less memory-intensive than other sorting algorithms, as it splits the set into several smaller groups; besides, it allows the executor to start returning results after the first group is processed, without waiting for the whole set to be sorted.\n\nInPostgre���,theimplementationisabitmoresubtle:1 whilerelativelybiggroups of rows are processed separately, smaller groups are combined together and are sorted in full. It reduces the overhead incurred by invoking the sorting procedure.2\n\nThe execution plan represents incremental sorting by the Incremental Sort node:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM bookings ORDER BY total_amount, book_date;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIncremental Sort (actual rows=2111110 loops=1)\n\nSort Key: total_amount, book_date Presorted Key: total_amount Full−sort Groups: 2823 Memory: 30kB Pre−sorted Groups: 2624\n\nSort Method: quicksort\n\nPeak Memory: 30kB\n\nSort Method: quicksort\n\nAverage\n\nAverage\n\n1 backend/executor/nodeIncrementalSort.c 2 backend/utils/sort/tuplesort.c\n\n456",
      "content_length": 1669,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "23.2 Sorting\n\nMemory: 3152kB −> Index Scan using bookings_total_amount_idx on bookings (ac...\n\nPeak Memory: 3259kB\n\n(8 rows)\n\nAs the plan shows, the data set is pre-sorted by the total_amount field, as it is the result of an index scan run on this column (Presorted Key). The ������� ������� command also displays run-time statistics. The Full-sort Groups row is related to small groups that were united to be sorted in full, while the Presorted Groups row displays the data on large groups with partially ordered data, which required in- cremental sorting by the book_date column only. In both cases, the in-memory quicksort method was applied. The difference in group sizes is due to non-uniform distribution of booking costs.\n\nIncremental sorting\n\ncan be used to compute window functions too:\n\n=> EXPLAIN (costs off) SELECT row_number() OVER (ORDER BY total_amount, book_date) FROM bookings;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nWindowAgg\n\n−> Incremental Sort\n\nSort Key: total_amount, book_date Presorted Key: total_amount −> Index Scan using bookings_total_amount_idx on bookings\n\n(5 rows)\n\nCost estimation. Cost calculations for incremental sorting1 are based on the ex- pectednumberofgroups2 andtheestimatedsortingcostofanaverage-sizedgroup (which we have already reviewed).\n\nThe startup cost reflects the cost estimation of sorting the first group,which allows the node to start returning sorted rows; the total cost includes the sorting cost of all groups.\n\nWe are not going to explore these calculations any further here.\n\n1 backend/optimizer/path/costsize.c, cost_incremental_sort function 2 backend/utils/adt/selfuncs.c, estimate_num_groups function\n\n457\n\nv. ��",
      "content_length": 1714,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "v. ��\n\np. ��� 1000\n\nChapter 23 Sorting and Merging\n\nParallel Mode\n\nSorting can also be performed concurrently. But although parallel workers do pre- sort their data shares, the Gather node knows nothing about their sort order and can only accumulate them on a first-come, first-serve basis. To preserve the sort order, the executor has to apply the Gather Merge node.1\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM flights ORDER BY scheduled_departure LIMIT 10;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nLimit (actual rows=10 loops=1)\n\n−> Gather Merge (actual rows=10 loops=1)\n\nWorkers Planned: 1 Workers Launched: 1 −> Sort (actual rows=7 loops=2)\n\nSort Key: scheduled_departure Sort Method: top−N heapsort Worker 0: −> Parallel Seq Scan on flights (actual rows=107434 lo...\n\nMemory: 27kB\n\nSort Method: top−N heapsort\n\nMemory: 27kB\n\n(9 rows)\n\nThe Gather Merge node uses a binary heap2 to adjust the order of rows fetched by several workers. It virtually merges several sorted sets of rows, just like external sorting would do, but is designed for a different use case: Gather Merge typically handles a small fixed number of data sources and fetches rows one by one rather than block by block.\n\nCost estimation. The startup cost of the Gather Merge node is based on the startup cost of its child node. Just like for the Gather node , this value is increased by the cost of launching parallel processes (estimated at\n\nparallel_setup_cost).\n\n1 backend/executor/nodeGatherMerge.c 2 backend/lib/binaryheap.c\n\n458",
      "content_length": 1575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "23.2 Sorting\n\nThe received value is then further extended by the cost of building a binary heap, which requires sorting n values, where n is the number of parallel workers (that is, cpu_operator_cost, nlog2 n). A single comparison operation is estimated at doubled and total share of such operations is typically negligible since n is quite small.\n\nThe total cost includes the expenses incurred by fetching all the data by several processesthatperformtheparallelpartoftheplan,andthecostoftransferringthis parallel_tuple_cost increased datatotheleader. Asinglerowtransferisestimatedat by �%, to compensate for possible waits on getting the next values.\n\nThe expenses incurred by binary heap updates must also be taken into account in total cost calculations: each input row requires log2 n comparison operations and certain additional actions (they are estimated at cpu_operator_cost).1\n\nLet’s take a look at yet another plan that uses the Gather Merge node. Note that the , and then the Sort node workers here first perform partial aggregation by hashing sorts the received results (it is cheap because few rows are left after aggregation) to be passed further to the leader process, which gathers the full result in the Gather Merge node. As forthefinal aggregation,it is performedon the sortedlist of values:\n\n=> EXPLAIN SELECT amount, count(*) FROM ticket_flights GROUP BY amount;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nFinalize GroupAggregate\n\n(cost=123399.62..123485.00 rows=337 wid...\n\nGroup Key: amount −> Gather Merge\n\n(cost=123399.62..123478.26 rows=674 width=14)\n\nWorkers Planned: 2 −> Sort\n\n(cost=122399.59..122400.44 rows=337 width=14)\n\nSort Key: amount −> Partial HashAggregate Group Key: amount −> Parallel Seq Scan on ticket_flights\n\n(cost=122382.07..122385.44 r...\n\n(cost=0.00...\n\n(9 rows)\n\nHere we have three parallel processes (including the leader), and the cost of the Gather Merge node is calculated as follows:\n\n1 backend/optimizer/path/costsize.c, cost_gather_merge function\n\n459\n\n0.0025\n\n0.1\n\np. ���",
      "content_length": 2067,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": "Chapter 23 Sorting and Merging\n\n=> WITH costs(startup, run) AS (\n\nSELECT round((\n\n-- launching processes current_setting('parallel_setup_cost')::real + -- building the heap current_setting('cpu_operator_cost')::real * 2 * 3 * log(2, 3)\n\n)::numeric, 2), round((\n\n-- passing rows current_setting('parallel_tuple_cost')::real * 1.05 * 674 + -- updating the heap current_setting('cpu_operator_cost')::real * 2 * 674 * log(2, 3) + current_setting('cpu_operator_cost')::real * 674\n\n)::numeric, 2)\n\n) SELECT 122399.59 + startup AS startup, 122400.44 + startup + run AS total\n\nFROM costs;\n\n| −−−−−−−−−−−+−−−−−−−−−−− 123399.61 | 123478.26\n\nstartup\n\ntotal\n\n(1 row)\n\n23.3 Distinct Values and Grouping\n\nAs we have just seen, grouping values to perform aggregation (and to eliminate duplicates) can be performed not only by hashing, but also by sorting. In a sorted list, groups of duplicate values can be singled out in one pass.\n\nRetrieval of distinct values from a sorted list is represented in the plan by a very simple node called Unique1:\n\n=> EXPLAIN (costs off) SELECT DISTINCT book_ref FROM bookings ORDER BY book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nResult\n\n−> Unique\n\n−> Index Only Scan using bookings_pkey on bookings\n\n(3 rows)\n\n1 backend/executor/nodeUnique.c\n\n460",
      "content_length": 1303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 462,
      "content": "23.3 Distinct Values and Grouping\n\nAggregation is performed in the GroupAggregate node:1\n\n=> EXPLAIN (costs off) SELECT book_ref, count(*) FROM bookings GROUP BY book_ref ORDER BY book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nGroupAggregate\n\nGroup Key: book_ref −> Index Only Scan using bookings_pkey on bookings\n\n(3 rows)\n\nIn parallel plans, this node is called Partial GroupAggregate, while the node that completes aggregation is called Finalize GroupAggregate.\n\ncan be combined in a single node if group- Both hashing and sorting strategies ing is performed by several column sets (specified in the �������� ����, ����, or ������ clauses). Without getting into rather complex details of this algorithm, I will simply provide an example that performs grouping by three different columns in conditions of scarce memory:\n\n=> SET work_mem = '64kB';\n\n=> EXPLAIN (costs off) SELECT count(*) FROM flights GROUP BY GROUPING SETS (aircraft_code, flight_no, departure_airport);\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nMixedAggregate\n\nHash Key: departure_airport Group Key: aircraft_code Sort Key: flight_no\n\nGroup Key: flight_no\n\n−> Sort\n\nSort Key: aircraft_code −> Seq Scan on flights\n\n(8 rows)\n\n=> RESET work_mem;\n\nHere is what happens while this query is being executed. The aggregation node, which is shown in the plan as MixedAggregate, receives the data set sorted by the aircraft_code column.\n\n1 backend/executor/nodeAgg.c, agg_retrieve_direct function\n\n461\n\nv. ��",
      "content_length": 1501,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 463,
      "content": "Chapter 23 Sorting and Merging\n\nFirst, this set is scanned, and the values are grouped by the aircraft_code column (Group Key). As the scan progresses,the rows are reordered by the flight_no column (like it is done by a regular Sort node: either via the quicksort method if the mem- ory is sufficient, or using external sorting on disk); at the same time, the executor places these rows into a hash table that uses departure_airport as its key (like it is done by hash aggregation: either in memory, or using temporary files).\n\nAt the second stage, the executor scans the data set that has just been sorted by the flight_no column and groups the values by the same column (Sort Key and the nested Group Key node). If the rows had to be grouped by yet another column,they would be resorted again as required.\n\nFinally, the hash table prepared at the first stage is scanned, and the values are grouped by the departure_airport column (Hash Key).\n\n23.4 Comparison of Join Methods\n\nAs we have seen, two data sets can be joined using three different methods, and each of them has its own pros and cons.\n\nThenestedloopjoindoesnothaveanyprerequisitesandcanstartreturningthefirst rows of the result set right away. It is the only join method that does not have to fully scan the inner set (as long as index access is available for it). These properties make the nested loop algorithm (combined with indexes) an ideal choice for short ���� queries, which deal with rather small sets of rows.\n\nThe weak point of the nested loop becomes apparent as the data volume grows. For a Cartesian product, this algorithm has quadratic complexity—the cost is propor- tionate to the product of sizes of the data sets being joined. However,the Cartesian product is not so common in practice; for each row of the outer set, the executor typically accesses a certain number of rows of the inner set using an index, and this average number does not depend on the total size of the data set (for exam- ple, an average number of tickets in a booking does not change as the number of bookings and bought tickets grows). Thus, the complexity of the nested loop al- gorithm often shows linear growth rather than quadratic one, even if with a high linear coefficient.\n\n462",
      "content_length": 2240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 464,
      "content": "23.4 Comparison of Join Methods\n\nAn important distinction of the nested loop algorithm is its universal applicability: it supports all join conditions, whereas other methods can only deal with equi- joins. It allows running queries with any types of conditions (except for the full join, which cannot be used with the nested loop), but you must keep in mind that a non-equi-join of a large data set is highly likely to be performed slower than desired.\n\nA hash join works best on large data sets. If ��� is sufficient, it requires only one pass over two data sets,so its complexity is linear. Combined with sequential table scans, this algorithm is typically used for ���� queries, which compute the result based on a large volume of data.\n\nHowever, if the response time is more important than throughput, a hash join is not the best choice: it will not start returning the resulting rows until the whole hash table is built.\n\nThe hash join algorithm is only applicable to equi-joins. Another restriction is that the data type of the join key must support hashing (but almost all of them do).\n\nThenestedloopjoin cansometimesbeatthehashjoin,takingadvantageofcaching the rows of the inner set in the Memoize node (which is also based on a hash table). While the hash join always scans the inner set in full, the nested loop algorithm does not have to, which may result in some cost reduction.\n\nA merge join can perfectly handle both short ���� queries and long ���� ones. It has linear complexity (the sets to be joined have to be scanned only once), does not require much memory, and returns the results without any preprocessing; however, the data sets must already have the required sort order. The most cost- effective way to do it is to fetch the data via an index scan. It is a natural choice if the row count is low; for larger data sets,index scans can still be efficient,but only if the heap access is minimal or does not happen at all.\n\nIf no suitable indexes are available, the sets have to be sorted, but this operation is memory-intensive, and its complexity is higher than linear: O(nlog2 n). In this case,a hash join is almost always cheaper than a merge join—unless the result has to be sorted.\n\nAn added bonus of a merge join is the equivalence of the inner and outer sets. The efficiency of both nested loop and hash joins is highly dependent on whether the planner can assign inner and outer sets correctly.\n\n463\n\nv. ��",
      "content_length": 2436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 465,
      "content": "Chapter 23 Sorting and Merging\n\nMerge joins are limited to equi-joins. Besides, the data type must have a �-tree operator class.\n\nThe following graph illustrates approximate dependencies between the costs of various join methods and the fraction of rows to be joined.\n\ncost\n\nmergejoin+sort\n\nnestedloop\n\nhashjoin\n\nmerge join + index\n\nselectivity\n\n0\n\n1\n\nIftheselectivityishigh,thenestedloopjoinusesindexaccessforbothtables; then the planner switches to the full scan of the outer table, which is reflected by the linear part of the graph.\n\nHere the hash join is using a full scan for both tables. The “step” on the graph corresponds to the moment when the hash table fills the whole memory and the batches start getting spilled to disk.\n\nIf an index scan is used, the cost of a merge join shows small linear growth. If the work_mem size is big enough,a hash join is usually more efficient,but a merge join beats it when it comes to temporary files.\n\nThe upper graph of the sort-merge join shows that the costs rise when indexes are unavailable and the data has to be sorted. Just like in the case of a hash join, the\n\n464",
      "content_length": 1119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 466,
      "content": "23.4 Comparison of Join Methods\n\n“step”on the graph is caused by insufficient memory,as it leads to using temporary files for sorting.\n\nIt is merely an example; in each particular case the ratio between the costs will be different.\n\n465",
      "content_length": 236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 468,
      "content": "Part V\n\nTypes of Indexes",
      "content_length": 24,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 470,
      "content": "24\n\nHash\n\n24.1 Overview\n\nA hash index1 provides the ability to quickly find a tuple �� (���) by a particular index key. Roughly speaking, it is simply a hash table stored on disk. The only operation supported by a hash index is search by the equality condition.\n\nWhen a value is inserted into an index,2 the hash function of the index key is computed. In Postgre���, hash functions return ��-bit or ��-bit integers; several lowest bits of these values are used as the number of the corresponding bucket. The ��� and the hash code of the key are added into the chosen bucket. The key itself is not stored in the index because it is more convenient to deal with small fixed-length values.\n\nThehashtableofanindexisexpandeddynamically.3 Theminimalnumberofbuck- ets is two. As the number of indexed tuples grows,one of the buckets gets split into two. This operation uses one more bit of the hash code, so the elements are redis- tributed only between the two buckets resulting from the split; the composition of other buckets of the hash table remains the same.4\n\nThe index search operation5 calculates the hash function of the index key and the corresponding bucket number. Of all the bucket contents, the search will return only those ���s that correspond to the hash code of the key. As bucket elements\n\n1 postgresql.org/docs/14/hash-index.html\n\nbackend/access/hash/README 2 backend/access/hash/hashinsert.c 3 backend/access/hash/hashpage.c, _hash_expandtable function 4 backend/access/hash/hashpage.c, _hash_getbucketbuf_from_hashkey function 5 backend/access/hash/hashsearch.c\n\n469",
      "content_length": 1582,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 471,
      "content": "p. ���\n\nv. ��\n\nv. ��\n\nChapter 24 Hash\n\nare ordered by the keys’ hash codes, binary search can return matching ���s quite efficiently.\n\nSince keys are not stored in the hash table, the index access method may return redundant ���s because of hash collisions. Therefore, the indexing engine has to recheck all the results fetched by the access method. An index-only scan is not supported for the same reason.\n\n24.2 Page Layout\n\nUnlike a regular hash table, the hash index is stored on disk. Therefore, all the data has to be arranged into pages, preferably in such a way that index operations (search, insertion, deletion) require access to as few pages as possible.\n\nA hash index uses four types of pages:\n\nmetapage—page zero that provides the “table of contents” of an index\n\nbucket pages—the main pages of an index, one per bucket\n\noverflow pages—additional pages that are used when the main bucket page\n\ncannot accommodate all the elements\n\nbitmappages—pagescontainingthebitarrayusedtotrackoverflowpagesthat\n\nhave been freed and can be reused\n\nWe can peek into index pages\n\nusing the pageinspect extension.\n\nLet’s begin with an empty table:\n\n=> CREATE EXTENSION pageinspect;\n\n=> CREATE TABLE t(n integer);\n\n=> ANALYZE t;\n\n=> CREATE INDEX ON t USING hash(n);\n\nI have analyzed the table, so the created index will have the minimal size possible; otherwise, the number of buckets would have been selected based on the assump- tion that the table contains ten pages.1\n\n1 backend/access/table/tableam.c, table_block_relation_estimate_size function\n\n470",
      "content_length": 1549,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 472,
      "content": "24.2 Page Layout\n\nThe index contains four pages: the metapage, two bucket pages, and one bitmap page (created at once for future use):\n\n=> SELECT page, hash_page_type(get_raw_page('t_n_idx', page)) FROM generate_series(0,3) page;\n\npage | hash_page_type −−−−−−+−−−−−−−−−−−−−−−−\n\n0 | metapage 1 | bucket 2 | bucket 3 | bitmap\n\n(4 rows)\n\nmeta- page\n\nbucket 0 bucket 0\n\nbucket 1 bucket 1\n\nbitmap\n\nThe metapage contains all the control information about the index. We are inter- ested only in a few values at the moment:\n\n=> SELECT ntuples, ffactor, maxbucket FROM hash_metapage_info(get_raw_page('t_n_idx', 0));\n\nntuples | ffactor | maxbucket −−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−\n\n0 |\n\n307 |\n\n1\n\n(1 row)\n\nThe estimated number of rows per bucket is shown in the ffactor field. This value fillfactor storage parameter value. By is calculated based on the block size and the absolutely uniform data distribution and no hash collisions you could use a higher fillfactor value, but in real-life databases it increases the risk of page overflows.\n\nThe worst scenario for a hash index is a large skew in data distribution, when a key is repeated multiple times. Since the hash function will be returning one and the same value, all the data will be placed into the same bucket, and increasing the number of buckets will not help.\n\nNow the index is empty, as shown by the ntuples field. Let’s cause a bucket page overflow by inserting multiple rows with the same value of the index key. An over- flow page appears in the index:\n\n471\n\n75\n\nv. ��",
      "content_length": 1526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 473,
      "content": "Chapter 24 Hash\n\n=> INSERT INTO t(n)\n\nSELECT 0 FROM generate_series(1,500); -- the same value\n\n=> SELECT page, hash_page_type(get_raw_page('t_n_idx', page)) FROM generate_series(0,4) page;\n\npage | hash_page_type −−−−−−+−−−−−−−−−−−−−−−−\n\n0 | metapage 1 | bucket 2 | bucket 3 | bitmap 4 | overflow\n\n(5 rows)\n\noverflow\n\nmeta- page\n\nbucket 0 bucket 0\n\nbucket 1 bucket 1\n\nbitmap\n\nbucket 1 bucket 1 bucket 1 bucket 1\n\nThe combined statistics on all the pages shows that bucket � is empty,while all the values have been placed into bucket �: some of them are located in the main page, and those that did not fit it can be found in the overflow page.\n\n=> SELECT page, live_items, free_size, hasho_bucket FROM (VALUES (1), (2), (4)) p(page),\n\nhash_page_stats(get_raw_page('t_n_idx', page));\n\npage | live_items | free_size | hasho_bucket −−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n1 | 2 | 4 |\n\n0 | 407 | 93 |\n\n8148 | 8 | 6288 |\n\n0 1 1\n\n(3 rows)\n\nIt is clear that if the elements of one and the same bucket are spread over several pages,performance will suffer. A hash index shows best results if data distribution is uniform.\n\nNow let’s take a look at how a bucket can be split. It happens when the number of rows in the index exceeds the estimated ffactor value for the available buckets. Here we have two buckets, and the ffactor is ���, so it will happen when the ���th row is inserted into the index:\n\n472",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 474,
      "content": "24.2 Page Layout\n\n=> SELECT ntuples, ffactor, maxbucket, ovflpoint FROM hash_metapage_info(get_raw_page('t_n_idx', 0));\n\nntuples | ffactor | maxbucket | ovflpoint −−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−\n\n500 |\n\n307 |\n\n1 |\n\n1\n\n(1 row)\n\n=> INSERT INTO t(n)\n\nSELECT n FROM generate_series(1,115) n; -- now values are different\n\n=> SELECT ntuples, ffactor, maxbucket, ovflpoint FROM hash_metapage_info(get_raw_page('t_n_idx', 0));\n\nntuples | ffactor | maxbucket | ovflpoint −−−−−−−−−+−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−\n\n615 |\n\n307 |\n\n2 |\n\n2\n\n(1 row)\n\nThe maxbucket value has been increased to two: now we have three buckets, num- bered from � to �. But even though we have added only one bucket, the number of pages has doubled:\n\n=> SELECT page, hash_page_type(get_raw_page('t_n_idx', page)) FROM generate_series(0,6) page;\n\npage | hash_page_type −−−−−−+−−−−−−−−−−−−−−−−\n\n0 | metapage 1 | bucket 2 | bucket 3 | bitmap 4 | overflow 5 | bucket 6 | unused\n\n(7 rows)\n\nmeta- page\n\nbucket 0 bucket 0\n\nbucket 1 bucket 1\n\nbitmap\n\nbucket 1 bucket 1 bucket 1 bucket 1\n\nbucket 2 bucket 2\n\nOne of the new pages is used by bucket �,while the other one remains free and will be used by bucket � as soon as it appears.\n\n=> SELECT page, live_items, free_size, hasho_bucket FROM (VALUES (1), (2), (4), (5)) p(page),\n\nhash_page_stats(get_raw_page('t_n_idx', page));\n\n473",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 475,
      "content": "v. ��\n\nChapter 24 Hash\n\npage | live_items | free_size | hasho_bucket −−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n1 | 2 | 4 | 5 |\n\n27 | 407 | 158 | 23 |\n\n7608 | 8 | 4988 | 7688 |\n\n0 1 1 2\n\n(4 rows)\n\nThus, from the point of view of the operating system, the hash index grows in spurts, although from the logical standpoint the hash table shows gradual growth.\n\nTo level out this growth to some extent and avoid allocating too many pages at a time, starting from the tenth increase pages get allocated in four equal batches rather than all at once.\n\nTwo more fields of the metapage,which are virtually bit masks,provide the details on bucket addresses:\n\n=> SELECT maxbucket, highmask::bit(4), lowmask::bit(4) FROM hash_metapage_info(get_raw_page('t_n_idx', 0));\n\nmaxbucket | highmask | lowmask −−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−\n\n2 | 0011\n\n| 0001\n\n(1 row)\n\nA bucket number is defined by the hash code bits that correspond to the highmask. But if the received bucket number does not exist (exceeds maxbucket),the lowmask bits are taken.1 In this particular case, we take two lowest bits, which gives us the values from � to �; but if we got �, we would take only one lowest bit, that is, use bucket � instead of bucket �.\n\nEach time the size is doubled, new bucket pages are allocated as a single continu- ous chunk,while overflow and bitmap pages get inserted between these fragments as required. The metapage keeps the number of pages inserted into each of the chunks in the spares array, which gives us an opportunity to calculate the number of its main page based on the bucket number using simple arithmetic.2\n\nIn this particular case, the first increase was followed by insertion of two pages (a bitmap page and an overflow page), but no new additions have happened after the second increase yet:\n\n1 backend/access/hash/hashutil.c, _hash_hashkey2bucket function 2 include/access/hash.h, BUCKET_TO_BLKNO macro\n\n474",
      "content_length": 1917,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 476,
      "content": "24.2 Page Layout\n\n=> SELECT spares[2], spares[3] FROM hash_metapage_info(get_raw_page('t_n_idx', 0));\n\nspares | spares −−−−−−−−+−−−−−−−−\n\n2 |\n\n2\n\n(1 row)\n\nThe metapage also stores an array of pointers to bitmap pages:\n\n=> SELECT mapp[1] FROM hash_metapage_info(get_raw_page('t_n_idx', 0));\n\nmapp −−−−−−\n\n3\n\n(1 row)\n\nspares\n\nmeta- page\n\nbucket 0 bucket 0\n\nbucket 1 bucket 1\n\nbitmap\n\nbucket 1 bucket 1\n\nbucket 2 bucket 2\n\nmmap\n\nThe space within index pages is freed when pointers to dead tuples are removed. It happens during page pruning (which is triggered by an attempt to insert an ele- ment into a completely filled page)1 or when routine vacuuming is performed.\n\nHowever, a hash index cannot shrink: once allocated, index pages will not be re- turned to the operating system. The main pages are permanently assigned to their buckets, even if they contain no elements at all; the cleared overflow pages are tracked in the bitmap and can be reused (possibly by another bucket). The only way to reduce the physical size of an index is to rebuild it using the ������� or ������ ����\n\ncommands.\n\nThe query plan has no indication of the index type:\n\n=> CREATE INDEX ON flights USING hash(flight_no);\n\n1 backend/access/hash/hashinsert.c, _hash_vacuum_one_page function\n\n475\n\np. ���",
      "content_length": 1278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 477,
      "content": "p. ���\n\np. ���\n\nChapter 24 Hash\n\n=> EXPLAIN (costs off) SELECT * FROM flights WHERE flight_no = 'PG0001';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights\n\nRecheck Cond: (flight_no = 'PG0001'::bpchar) −> Bitmap Index Scan on flights_flight_no_idx\n\nIndex Cond: (flight_no = 'PG0001'::bpchar)\n\n(4 rows)\n\n24.3 Operator Class\n\nPriortoPostgre�����,hashindexeswerenotlogged,thatis,theywereneitherpro- tected against failures nor replicated, and consequently, it was not recommended to use them. But even then they had their own value. The thing is that the hashing and grouping), and algorithm is widely used (in particular, to perform hash joins the system must know which hash function can be used for a certain data type. However, this correspondence is not static: it cannot be defined once and for all since Postgre��� allows adding new data types on the fly. Therefore, it is main- tained by the operator class of the hash index and a particular data type. The hash function itself is represented by the support function of the class:\n\n=> SELECT opfname AS opfamily_name,\n\namproc::regproc AS opfamily_procedure\n\nFROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_amproc amproc ON amprocfamily = opf.oid\n\nWHERE amname = 'hash' AND amprocnum = 1 ORDER BY opfamily_name, opfamily_procedure;\n\nopfamily_name\n\n| opfamily_procedure\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−\n\n| hash_aclitem aclitem_ops | hash_array array_ops | hashchar bool_ops bpchar_ops | hashbpchar bpchar_pattern_ops | hashbpchar ...\n\n476",
      "content_length": 1561,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 478,
      "content": "24.4 Properties\n\ntimetz_ops uuid_ops xid8_ops xid_ops (38 rows)\n\n| timetz_hash | uuid_hash | hashint8 | hashint4\n\nThese functions return ��-bit integers. Although they are not documented, they can be used to calculate the hash code for a value of the corresponding type.\n\nFor example, the text_ops family uses the hashtext function:\n\n=> SELECT hashtext('one'), hashtext('two');\n\n| −−−−−−−−−−−−+−−−−−−−−−−−− 1793019229 | 1590507854\n\nhashtext\n\nhashtext\n\n(1 row)\n\nThe operator class of the hash index provides only the equal to operator:\n\n=> SELECT opfname AS opfamily_name,\n\nleft(amopopr::regoperator::text, 20) AS opfamily_operator\n\nFROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_amop amop ON amopfamily = opf.oid\n\nWHERE amname = 'hash' ORDER BY opfamily_name, opfamily_operator;\n\nopfamily_name\n\n|\n\nopfamily_operator\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−\n\naclitem_ops array_ops bool_ops ... uuid_ops xid8_ops xid_ops (48 rows)\n\n| =(aclitem,aclitem) | =(anyarray,anyarray) | =(boolean,boolean)\n\n| =(uuid,uuid) | =(xid8,xid8) | =(xid,xid)\n\n24.4 Properties\n\nLet’s take a look at the index-level properties to the system.\n\nthat the hash access method imparts\n\n477\n\np. ���",
      "content_length": 1192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 479,
      "content": "p. ���\n\nChapter 24 Hash\n\nAccess Method Properties\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'hash';\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | f | can_order | f | can_unique | can_multi_col | f | t | can_exclude | f | can_include\n\nhash hash hash hash hash\n\n(5 rows)\n\nIt is clear that hash indexes cannot be used for row ordering: the hash function mixes the data more or less randomly.\n\nUnique constraints are not supported either. However, hash indexes can enforce constraints,and since the only supported function is equal to,this exclu- exclusion sion attains the meaning of uniqueness:\n\n=> ALTER TABLE aircrafts_data\n\nADD CONSTRAINT unique_range EXCLUDE USING hash(range WITH =);\n\n=> INSERT INTO aircrafts_data\n\nVALUES ('744','{\"ru\": \"Boeing 747-400\"}',11100);\n\nERROR: \"unique_range\" DETAIL: (range)=(11100).\n\nconflicting key value violates exclusion constraint\n\nKey (range)=(11100) conflicts with existing key\n\nMulticolumn indexes and additional ������� columns are not supported either.\n\nIndex-Level Properties\n\n=> FROM unnest(array[\n\nSELECT p.name, pg_index_has_property('flights_flight_no_idx', p.name)\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\n478",
      "content_length": 1400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 480,
      "content": "24.4 Properties\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| f clusterable | t index_scan bitmap_scan | t backward_scan | t\n\n(4 rows)\n\nThe hash index supports both a regular index scan and a bitmap scan.\n\nTable clusterization by the hash index is not supported. It is quite logical, as it is hard to imagine why it may be necessary to physically order heap data based on the hash function value.\n\nColumn-Level Properties\n\nColumn-level properties are virtually defined by the index access method and al- ways take the same values.\n\n=> SELECT p.name,\n\npg_index_column_has_property('flights_flight_no_idx', 1, p.name)\n\nFROM unnest(array[\n\n'asc', 'desc', 'nulls_first', 'nulls_last', 'orderable', 'distance_orderable', 'returnable', 'search_array', 'search_nulls'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n| f asc | f desc | f nulls_first | f nulls_last orderable | f distance_orderable | f | f returnable | f search_array | f search_nulls\n\n(9 rows)\n\nSince the hash function does not preserve the order of values, all the properties related to ordering are inapplicable to the hash index.\n\n479",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 481,
      "content": "Chapter 24 Hash\n\nThe hash index cannot participate in an index-only scan, as it does not store the index key and requires heap access.\n\nThe hash index does not support ���� values, since the equal to operation is inap- plicable to them.\n\nSearch for elements in an array is not implemented either.\n\n480",
      "content_length": 301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 482,
      "content": "25\n\nB-tree\n\n25.1 Overview\n\nA �-tree (implemented as the btree access method) is a data structure that enables you to quickly find the required element in leaf nodes of the tree by going down from its root.1 For the search path to be unambiguously identified, all tree ele- ments must be ordered. B-trees are designed for ordinal data types, whose values can be compared and sorted.\n\nThe following schematic diagram of an index build over airport codes shows inner nodes as horizontal rectangles; leaf nodes are aligned vertically.\n\nAER OVB AER OVB\n\nAER DME AER DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nAER AER\n\nDME DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nBZK BZK\n\nHMA HMA\n\nLED LED\n\nOVS OVS\n\nSGC SGC\n\nKJA KJA\n\nNUX NUX\n\nPEE PEE\n\n1 postgresql.org/docs/14/btree.html backend/access/nbtree/README\n\nSVO SVO\n\nSVX SVX\n\nVKO VKO\n\n481",
      "content_length": 815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 483,
      "content": "Chapter 25 B-tree\n\nEach tree node contains several elements, which consist of an index key and a pointer. Inner node elements reference nodes of the next level; leaf node elements reference heap tuples (the illustration does not show these references).\n\nB-trees have the following important properties:\n\nThey are balanced, which means that all leaf nodes of a tree are located at the\n\nsame depth. Therefore, they guarantee equal search time for all values.\n\nThey have plenty of branches, that is, each node contains many elements, of- ten hundreds of them (the illustration shows three-element nodes solely for clarity). As a result, �-tree depth is always small, even for very large tables. We cannot say with absolute certainty what the letter � in the name of this structure stands for. Both balanced and bushy fit equally well. Surprisingly, you can often see it interpreted as binary, which is certainly incorrect.\n\nData in an index is sorted either in ascending or in descending order, both within each node and across all nodes of the same level. Peer nodes are bound into a bidirectional list,so it is possible to get an ordered set of data by simply scanning the list one way or the other,without having to start at the root each time.\n\n25.2 Search and Insertions\n\nSearch by Equality\n\nLet’s take a look at how we can search for a value in a tree by condition “indexed- column = expression”.1 We will try to find the ��� airport (Krasnoyarsk).\n\nThe search starts at the root node, and the access method must determine which child node to descend to. It chooses the Ki key, for which Ki ⩽ expression < Ki+1 is satisfied.\n\nThe root node contains the keys��� and ���. The condition��� ⩽ ��� < ��� holds true,so we need to descend into the child node referenced by the element with the ��� key.\n\n1 backend/access/nbtree/nbtsearch.c, _bt_search function\n\n482",
      "content_length": 1861,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 484,
      "content": "25.2 Search and Insertions\n\nAER OVB AER OVB\n\nAER DME AER DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nAER AER\n\nDME DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nBZK BZK\n\nHMA HMA\n\nLED LED\n\nOVS OVS\n\nSGC SGC\n\nSVX SVX\n\nKJA KJA\n\nNUX NUX\n\nPEE PEE\n\nVKO VKO\n\nThis procedure is repeated recursively until we get to the leaf node that contains the required tuple ��. In this particular case, the child node satisfies the condi- tion ��� ⩽ ��� < ���, so we have to descend into the leaf node referenced by the element with the ��� key.\n\nAs you can notice, the leftmost keys in the inner nodes of the tree are redundant: to choose the child node of the root, it is enough to have condition ��� < ��� satisfied. B-trees do not store such keys, so in the illustrations that follow I will leave the corresponding elements empty.\n\nThe required element in the leaf node can be quickly found by binary search.\n\nHowever, the search procedure is not as trivial as it seems. It must be taken into account that the sort order of data in an index can be either ascending, like shown above, or descending. Even a unique index can have several matching values, and all of them must be returned. Moreover,there may be so many duplicates that they do not fit a single node, so the neighboring leaf node will have to be processed too.\n\nSince an index can contain non-unique values, it would be more accurate to call its order non-descending rather than ascending (and non-ascending rather than descending). But , which lets us I will stick to a simpler term. Besides, the tuple �� is a part of an index key consider index entries to be unique even if the values are actually the same.\n\n483\n\np. ���\n\nv. ��",
      "content_length": 1667,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 485,
      "content": "Chapter 25 B-tree\n\nOn top of that, while the search is in progress, other processes may modify the data,pages may get split into two,and the tree structure may change. All the algo- rithms are designed to minimize contention between these concurrent operations whenever possible and avoid excessive locks,but we are not going to get into these technicalities here.\n\nSearch by Inequality\n\nIf the search is performed by condition“indexed-column ⩽ expression”(or“indexed- column ⩾ expression”),we must first search the index for the value that satisfies the equality condition and then traverse its leaf nodes in the required direction until the end of the tree is reached.\n\nThis diagram illustrates the search for airport codes that are less than or equal to ��� (Domodedovo).\n\nAER OVB AER OVB\n\nAER DME AER DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nAER AER\n\nDME DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nBZK BZK\n\nHMA HMA\n\nLED LED\n\nOVS OVS\n\nSGC SGC\n\nKJA KJA\n\nNUX NUX\n\nPEE PEE\n\nFor less than and greater than operators, the procedure is the same, except that the first found value must be excluded.\n\nSearch by Range\n\nWhen searching by range “expression1 ⩽ indexed-column ⩽ expression2”, we must first find expression1 and then traverse the leaf nodes in the right direction until\n\n484\n\nSVO SVO\n\nSVX SVX\n\nVKO VKO",
      "content_length": 1296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 486,
      "content": "25.2 Search and Insertions\n\nwe get to expression2. This diagram illustrates the process of searching for airport codes in the range between ��� (Saint Petersburg) and ��� (Rostov-on-Don), in- clusive.\n\nAER OVB AER OVB\n\nAER DME AER DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nAER AER\n\nDME DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nBZK BZK\n\nHMA HMA\n\nLED LED\n\nOVS OVS\n\nSGC SGC\n\nSVX SVX\n\nKJA KJA\n\nNUX NUX\n\nPEE PEE\n\nVKO VKO\n\nInsertions\n\nThe insert position of a new element is unambiguously defined by the order of keys. For example, if you insert the ��� airport code (Saratov) into the table, the new element will appear in the last but one leaf node, between ��� and ���.\n\nBut what if the leaf node does not have enough space for a new element? For ex- ample (assuming that a node can accommodate three elements at the most), if we insert the ��� airport code (Tyumen), the last leaf node will be overfilled. In this case, the node is split into two, some of the elements of the old node are moved into the new node, and a pointer to the new child node is added into the parent node. Obviously, the parent can get overfilled too. Then it is also split into two nodes, and so on. If it comes to splitting the root, one more node is created above the resulting nodes to become the new root of the tree. The tree depth is increased by one level in this case.\n\nInthisexample,theinsertionofthe���airportledtotwonodesplits; theresulting new nodes are highlighted in the diagram below. To make sure that any node can\n\n485",
      "content_length": 1508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 487,
      "content": "p. ���\n\nChapter 25 B-tree\n\nbesplit,abidirectionallistbindsthenodesatalllevels,notonlythoseatthelowest level.\n\nAER OVB AER OVB\n\nSVO SVO\n\nAER DME AER DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO TJM SVO TJM\n\nAER AER\n\nDME DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nBZK BZK\n\nHMA HMA\n\nLED LED\n\nOVS OVS\n\nRTW RTW\n\nSVX SVX\n\nKJA KJA\n\nNUX NUX\n\nPEE PEE\n\nSGC SGC\n\nThe described procedure of insertions and splits guarantees that the tree remains balanced, and since the number of elements that a node can accommodate is typ- ically quite large, the tree depth rarely increases.\n\nThe problem is that once split, nodes can never be merged together, even if they contain very few elements after vacuuming. This limitation pertains not to the �- tree data structure as such, but rather to its Postgre��� implementation. So if the node turns out to be full when an insertion is attempted, the access method first redundant data in order to clear some space and avoid an extra split. tries to prune\n\n25.3 Page Layout\n\nEach node of a �-tree takes one page. The page’s size defines the node’s capacity.\n\nBecause of page splits, the root of the tree can be represented by different pages at different times. But the search algorithm must always start the scan at the root. It finds the �� of the current root page in the zero index page (which is called a metapage). The metapage also contains some other metadata.\n\n486\n\nTJM TJM\n\nVKO VKO",
      "content_length": 1405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 488,
      "content": "25.3 Page Layout\n\nmetapage\n\n2\n\nAER OVB AER OVB\n\nSVO SVO\n\n1\n\nAER DME AER DME\n\nKZN OVB KZN OVB\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nSVO TJM SVO TJM\n\nAER AER\n\nDME DME\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nTJM TJM\n\n0\n\nBZK BZK\n\nHMA HMA\n\nLED LED\n\nOVS OVS\n\nRTW RTW\n\nSVX SVX\n\nVKO VKO\n\nDME DME\n\nKJA KJA\n\nNUX NUX\n\nPEE PEE\n\nSGC SGC\n\nTJM TJM\n\nKZN KZN\n\nOVB OVB\n\nROV ROV\n\nSVO SVO\n\nData layout in index pages is a bit different from what we have seen so far. All the pages, except the rightmost ones at each level, contain an additional “high key,” which is guaranteed to be not smaller than any key in this page. In the above diagram high keys are highlighted.\n\nLet’s use the pageinspect extension to take a look at a page of a real index built upon six-digit booking references. The metapage lists the root page �� and the depth of the tree (level numbering starts from leaf nodes and is zero-based):\n\n=> SELECT root, level FROM bt_metap('bookings_pkey');\n\nroot | level −−−−−−+−−−−−−−\n\n290 | (1 row)\n\n2\n\n487",
      "content_length": 984,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 489,
      "content": "Chapter 25 B-tree\n\nThe keys stored in index entries are displayed as sequences of bytes, which is not really convenient:\n\n=> SELECT data FROM bt_page_items('bookings_pkey',290) WHERE itemoffset = 2;\n\ndata −−−−−−−−−−−−−−−−−−−−−−−−− 0f 30 43 39 41 42 31 00\n\n(1 row)\n\nTo decipher these values, we will have to write an adhoc function. It will not sup- port all platforms and may not work for some particular scenarios,but it will do for the examples in this chapter:\n\n=> CREATE FUNCTION data_to_text(data text) RETURNS text AS $$ DECLARE\n\nraw bytea := ('\\x'||replace(data,' ',''))::bytea; pos integer := 0; len integer; res text := '';\n\nBEGIN\n\nWHILE (octet_length(raw) > pos) LOOP\n\nlen := (get_byte(raw,pos) - 3) / 2; EXIT WHEN len <= 0; IF pos > 0 THEN\n\nres := res || ', ';\n\nEND IF; res := res || (\n\nSELECT string_agg( chr(get_byte(raw, i)),'') FROM generate_series(pos+1,pos+len) i\n\n); pos := pos + len + 1;\n\nEND LOOP; RETURN res;\n\nEND; $$ LANGUAGE plpgsql;\n\nNow we can take a look at the contents of the root page:\n\n488",
      "content_length": 1019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 490,
      "content": "25.3 Page Layout\n\n=> SELECT itemoffset, ctid, data_to_text(data) FROM bt_page_items('bookings_pkey',290);\n\nitemoffset |\n\nctid\n\n| data_to_text\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−\n\n| 1 | (3,0) | 0C9AB1 2 | (289,1) | 192F03 3 | (575,1) | 25D715 4 | (860,1) 5 | (1145,1) | 32785C\n\n...\n\n17 | (4565,1) | C993F6 18 | (4850,1) | D63931 19 | (5135,1) | E2CB14 20 | (5420,1) | EF6FEA 21 | (5705,1) | FC147D\n\n(21 rows)\n\nAs I have said, the first entry contains no key. The ctid column provides links to child pages.\n\nSuppose we are looking for booking ������. In this case, we have to choose entry �� (since ������ ⩽ ������ < ������) and go down to page ����.\n\n=> SELECT itemoffset, ctid, data_to_text(data) FROM bt_page_items('bookings_pkey',5135);\n\nitemoffset |\n\nctid\n\n| data_to_text\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−\n\n1 | (5417,1) | EF6FEA 2 | (5132,0) | 3 | (5133,1) | E2D71D 4 | (5134,1) | E2E2F4 5 | (5136,1) | E2EDE7\n\nhigh key\n\n...\n\n282 | (5413,1) | EF41BE 283 | (5414,1) | EF4D69 284 | (5415,1) | EF58D4 285 | (5416,1) | EF6410\n\n(285 rows)\n\nThe first entry in this page contains the high key, which may seem a bit unexpected. Logically, it should have been placed at the end of the page, but from the imple- mentation standpoint it is more convenient to have it at the beginning to avoid moving it each time the page content changes.\n\n489",
      "content_length": 1345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 491,
      "content": "Chapter 25 B-tree\n\nHere we choose entry � (since ������ ⩽ ������ < ������) and go down to page �����.\n\n=> SELECT itemoffset, ctid, data_to_text(data) FROM bt_page_items('bookings_pkey',5133);\n\nitemoffset |\n\nctid\n\n| data_to_text\n\n−−−−−−−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n1 | (11921,1) 2 | (11919,76) 3 | (11919,77) 4 | (11919,78) 5 | (11919,79)\n\n| E2E2F4 | E2D71D | E2D725 | E2D72D | E2D733\n\n...\n\n363 | (11921,123) | E2E2C9 364 | (11921,124) | E2E2DB 365 | (11921,125) | E2E2DF 366 | (11921,126) | E2E2E5 367 | (11921,127) | E2E2ED\n\n(367 rows)\n\nIt is a leaf page of the index. The first entry is the high key; all the other entries point to heap tuples.\n\nAnd here is our booking:\n\n=> SELECT * FROM bookings WHERE ctid = '(11919,77)';\n\nbook_ref |\n\nbook_date\n\n| total_amount\n\n−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\nE2D725 (1 row)\n\n| 2017−01−25 04:10:00+03 |\n\n28000.00\n\nThis is roughly what happens at the low level when we search for a booking by its code:\n\n=> EXPLAIN (costs off) SELECT * FROM bookings WHERE book_ref = 'E2D725';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using bookings_pkey on bookings\n\nIndex Cond: (book_ref = 'E2D725'::bpchar)\n\n(2 rows)\n\n490",
      "content_length": 1201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 492,
      "content": "25.3 Page Layout\n\nDeduplication\n\nNon-unique indexes can contain a lot of duplicate keys that point to different heap tuples. Since non-unique keys appear more than once and hence take much space, duplicates are collapsed into a single index entry, which contains the key and the list of the corresponding tuple ��s.1 In some cases, this procedure (which is called deduplication) can significantly reduce the index size.\n\nHowever, unique indexes can also contain duplicates because of ����: an index keeps references to all versions of table rows. The mechanism of ��� updates can help you fight index bloating caused by referencing outdated and typically short- lived row versions, but sometimes it may be inapplicable. In this case, deduplica- tion can buy some time required to vacuum redundant heap tuples and avert extra page splits.\n\nTo avoid wasting resources on deduplication when it brings no immediate bene- fits, collapsing is only performed if the leaf page does not have enough space to accommodate one more tuple.2 Then page pruning and deduplication3 can free some space and prevent an undesired page split. However, if duplicates are rare, you can disable the deduplication feature by turning off the deduplicate_items stor- age parameter.\n\nSome indexes do not support deduplication. The main limitation is that the equal- ity of keys must be checked by simple binary comparison of their inner represen- tation. Not all data types by far can be compared this way. For instance, floating- point numbers (float and double precision) have two different representations for zero. Arbitrary-precision numbers (numeric) can represent one and the same num- ber in different scales, while the jsonb type can use such numbers. Neither is deduplicationpossiblefortexttypesifyouusenondeterministiccollations,4 which allow the same symbols to be represented by different byte sequences (standard collations are deterministic).\n\nBesides, deduplication is currently not supported for composite types, ranges, and arrays, as well as for indexes declared with the ������� clause.\n\n1 postgresql.org/docs/14/btree-implementation#BTREE-DEDUPLICATION.html 2 backend/access/nbtree/nbtinsert.c, _bt_delete_or_dedup_one_page function 3 backend/access/nbtree/nbtdedup.c, _bt_dedup_pass function 4 postgresql.org/docs/14/collation.html\n\n491\n\nv. ��\n\np. ���",
      "content_length": 2345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 493,
      "content": "v. ��\n\nChapter 25 B-tree\n\nTo check whether a particular index can use deduplication, you can take a look at the allequalimage field in its metapage:\n\n=> CREATE INDEX ON tickets(book_ref);\n\n=> SELECT allequalimage FROM bt_metap('tickets_book_ref_idx');\n\nallequalimage −−−−−−−−−−−−−−−\n\nt\n\n(1 row)\n\nIn this case, deduplication is supported. And indeed, we can see that one of the leaf pages contains both index entries with a single tuple �� (htid) and those with a list of ��s (tids):\n\n=> SELECT itemoffset, htid, left(tids::text,27) tids,\n\ndata_to_text(data) AS data\n\nFROM bt_page_items('tickets_book_ref_idx',1) WHERE itemoffset > 1;\n\nitemoffset |\n\nhtid\n\n|\n\ntids\n\n|\n\ndata\n\n−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−\n\n2 | (32965,40) | 3 | (47429,51) | 4 | (3648,56) 5 | (6498,47)\n\n| {\"(3648,56)\",\"(3648,57)\"} |\n\n| 000004 | 00000F | 000010 | 000012\n\n...\n\n271 | (21492,46) | | 000890 272 | (26601,57) | {\"(26601,57)\",\"(26601,58)\"} | 0008AC | 0008B6 273 | (25669,37) |\n\n(272 rows)\n\nCompact Storage of Inner Index Entries\n\nDeduplication enables accommodating more entries in leaf pages of an index. But even though leaf pages make up the bulk of an index, data compaction performed in inner pages to prevent extra splits is just as important, as search efficiency is directly dependent on tree depth.\n\nInner index entries contain index keys, but their values are only used to deter- mine the subtree to descend into during search. In multicolumn indexes,it is often enough to take the first key attribute (or several first ones). Other attributes can be truncated to save space in the page.\n\n492",
      "content_length": 1612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 494,
      "content": "25.4 Operator Class\n\nSuch suffix truncation happens when a leaf page is being split and the inner page has to accommodate a new pointer.1\n\nIn theory, we could even go one step further and keep only the meaningful part of the attribute, for example, the first few symbols of a row that are enough to differentiate be- tween subtrees. But it is not implemented yet: an index entry either contains the whole attribute or excludes this attribute altogether.\n\nFor instance, here are several entries of the root page of an index built over the tickets table on the columns containing booking references and passenger names:\n\n=> CREATE INDEX tickets_bref_name_idx ON tickets(book_ref, passenger_name);\n\n=> SELECT itemoffset, ctid, data_to_text(data) FROM bt_page_items('tickets_bref_name_idx',229) WHERE itemoffset BETWEEN 8 AND 13;\n\nitemoffset |\n\nctid\n\n|\n\ndata_to_text\n\n−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n8 | (1607,1) | 1A98A0 9 | (1833,2) | 1E57D1, SVETLANA MAKSIMOVA\n\n10 | (2054,1) | 220797 11 | (2282,1) | 25DB06 12 | (2509,2) | 299FE4, YURIY AFANASEV 13 | (2736,1) | 2D62C9\n\n(6 rows)\n\nWe can see that some index entries do not have the second attribute.\n\nNaturally,leaf pages must keep all key attributes and ������� column values,if any. Otherwise,itwouldbeimpossibletoperformindex-onlyscans. Theonlyexception is high keys; they can be kept partially.\n\n25.4 Operator Class\n\nComparison Semantics\n\nApart from hashing values, the system must also know how to order values of var- ious types, including custom ones. It is indispensable for sorting, grouping, merge\n\n1 backend/access/nbtree/nbtinsert.c, _bt_split function\n\n493",
      "content_length": 1640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 495,
      "content": "Chapter 25 B-tree\n\njoins, and some other operations. And just like in the case of hashing, comparison operators for a particular data type are defined by an operator class.1\n\nAn operator class allows us to abstract from names (such as >, <, =) and can even provide several ways to order values of one and the same type.\n\nHerearethemandatorycomparisonoperatorsthatmustbedefinedinanyoperator class of the btree method (shown for the bool_ops family):\n\n=> SELECT amopopr::regoperator AS opfamily_operator,\n\namopstrategy\n\nFROM pg_am am\n\nJOIN pg_opfamily opf ON opfmethod = am.oid JOIN pg_amop amop ON amopfamily = opf.oid\n\nWHERE amname = 'btree' AND opfname = 'bool_ops' ORDER BY amopstrategy;\n\nopfamily_operator\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n<(boolean,boolean) | <=(boolean,boolean) | =(boolean,boolean) | >=(boolean,boolean) | | >(boolean,boolean)\n\n1 2 3 4 5\n\n(5 rows)\n\nEach of these five comparison operators corresponds to one of the strategies,2 which defines their semantics:\n\n� less than\n\n� less than or equal to\n\n� equal to\n\n� greater than or equal to\n\n� greater than\n\nA �-tree operator class also includes several support functions.3 The first one must return 1 if its first argument is greater than the second one, −1 if it is less than the second one, and 0 if the arguments are equal.\n\n1 postgresql.org/docs/14/btree-behavior.html 2 postgresql.org/docs/14/xindex#XINDEX-STRATEGIES.html 3 postgresql.org/docs/14/btree-support-funcs.html\n\n494",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 496,
      "content": "25.4 Operator Class\n\nOther support functions are optional, but they improve performance of the access method.\n\nTo better understand this mechanism, we can define a new data type with a non- default collation. The documentation gives an example for complex numbers,1 but it is written in C. Fortunately, a �-tree operator class can be implemented using interpreted languages too, so I will take advantage of it and make an example that is as simple as possible (even if knowingly inefficient).\n\nLet’s define a new composite type for information units:\n\n=> CREATE TYPE capacity_units AS ENUM (\n\n'B', 'kB', 'MB', 'GB', 'TB', 'PB'\n\n);\n\n=> CREATE TYPE capacity AS (\n\namount integer, unit capacity_units\n\n);\n\nNow create a table with a column of the new type and fill it with random values:\n\n=> CREATE TABLE test AS\n\nSELECT ( (random()*1023)::integer, u.unit )::capacity AS cap FROM generate_series(1,100),\n\nunnest(enum_range(NULL::capacity_units)) AS u(unit);\n\nBy default, values of composite types are sorted in lexicographical order, which is not the same as the natural order in this particular case:\n\n=> SELECT * FROM test ORDER BY cap;\n\ncap −−−−−−−−−−−\n\n(1,B) (3,GB) (4,MB) (9,kB) ... (1017,kB) (1017,GB) (1018,PB) (1020,MB) (600 rows)\n\n1 postgresql.org/docs/14/xindex#XINDEX-EXAMPLE.html\n\n495",
      "content_length": 1292,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 497,
      "content": "Chapter 25 B-tree\n\nNow let’s get down to creating our operator class. We will start with defining a function that converts the volume into bytes:\n\n=> CREATE FUNCTION capacity_to_bytes(a capacity) RETURNS numeric AS $$ SELECT a.amount::numeric *\n\n1024::numeric ^ ( array_position(enum_range(a.unit), a.unit) - 1 );\n\n$$ LANGUAGE sql STRICT IMMUTABLE;\n\n=> SELECT capacity_to_bytes('(1,kB)'::capacity);\n\ncapacity_to_bytes −−−−−−−−−−−−−−−−−−−−−−− 1024.0000000000000000\n\n(1 row)\n\nCreate a support function for the future operator class:\n\n=> CREATE FUNCTION capacity_cmp(a capacity, b capacity) RETURNS integer AS $$ SELECT sign(capacity_to_bytes(a) - capacity_to_bytes(b)); $$ LANGUAGE sql STRICT IMMUTABLE;\n\nNow it is easy to define comparison operators using this support function. I delib- erately use peculiar names to demonstrate that they can be arbitrary:\n\n=> CREATE FUNCTION capacity_lt(a capacity, b capacity) RETURNS boolean AS $$ BEGIN\n\nRETURN capacity_cmp(a,b) < 0;\n\nEND; $$ LANGUAGE plpgsql IMMUTABLE STRICT;\n\n=> CREATE OPERATOR #<# ( LEFTARG = capacity, RIGHTARG = capacity, FUNCTION = capacity_lt\n\n);\n\nThe other four operators are defined in a similar way.\n\n=> CREATE FUNCTION capacity_le(a capacity, b capacity) RETURNS boolean AS $$ BEGIN\n\nRETURN capacity_cmp(a,b) <= 0;\n\nEND; $$ LANGUAGE plpgsql IMMUTABLE STRICT;\n\n496",
      "content_length": 1330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 498,
      "content": "25.4 Operator Class\n\n=> CREATE OPERATOR #<=# (\n\nLEFTARG = capacity, RIGHTARG = capacity, FUNCTION = capacity_le\n\n);\n\n=> CREATE FUNCTION capacity_eq(a capacity, b capacity) RETURNS boolean AS $$ BEGIN\n\nRETURN capacity_cmp(a,b) = 0;\n\nEND; $$ LANGUAGE plpgsql IMMUTABLE STRICT;\n\n=> CREATE OPERATOR #=# ( LEFTARG = capacity, RIGHTARG = capacity, FUNCTION = capacity_eq, MERGES -- can be used in merge joins\n\n);\n\n=> CREATE FUNCTION capacity_ge(a capacity, b capacity) RETURNS boolean AS $$ BEGIN\n\nRETURN capacity_cmp(a,b) >= 0;\n\nEND; $$ LANGUAGE plpgsql IMMUTABLE STRICT;\n\n=> CREATE OPERATOR #>=# (\n\nLEFTARG = capacity, RIGHTARG = capacity, FUNCTION = capacity_ge\n\n);\n\n=> CREATE FUNCTION capacity_gt(a capacity, b capacity) RETURNS boolean AS $$ BEGIN\n\nRETURN capacity_cmp(a,b) > 0;\n\nEND; $$ LANGUAGE plpgsql IMMUTABLE STRICT;\n\n=> CREATE OPERATOR #># ( LEFTARG = capacity, RIGHTARG = capacity, FUNCTION = capacity_gt\n\n);\n\nAt this stage, we can already compare capacities:\n\n497",
      "content_length": 971,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 499,
      "content": "Chapter 25 B-tree\n\n=> SELECT (1,'MB')::capacity #># (512, 'kB')::capacity;\n\n?column? −−−−−−−−−−\n\nt\n\n(1 row)\n\nOnce the operator class is created, sorting will also start working as expected:\n\n=> CREATE OPERATOR CLASS capacity_ops DEFAULT FOR TYPE capacity -- to be used by default USING btree AS\n\nOPERATOR 1 #<#, OPERATOR 2 #<=#, OPERATOR 3 #=#, OPERATOR 4 #>=#, OPERATOR 5 #>#, FUNCTION 1 capacity_cmp(capacity,capacity);\n\n=> SELECT * FROM test ORDER BY cap;\n\ncap −−−−−−−−−−−\n\n(1,B) (21,B) (27,B) (35,B) (46,B) ... (1002,PB) (1013,PB) (1014,PB) (1014,PB) (1018,PB) (600 rows)\n\nOur operator class is used by default when a new index is created, and this index returns the results in the correct order:\n\n=> CREATE INDEX ON test(cap);\n\n=> SELECT * FROM test WHERE cap #<# (100,'B')::capacity ORDER BY cap;\n\ncap −−−−−−−− (1,B) (21,B) (27,B) (35,B)\n\n498",
      "content_length": 848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 500,
      "content": "25.4 Operator Class\n\n(46,B) (57,B) (68,B) (70,B) (72,B) (76,B) (78,B) (94,B) (12 rows)\n\n=> EXPLAIN (costs off) SELECT * FROM test WHERE cap #<# (100,'B')::capacity ORDER BY cap;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Only Scan using test_cap_idx on test\n\nIndex Cond: (cap #<# '(100,B)'::capacity)\n\n(2 rows)\n\nThe ������ clause joins for this data type.\n\nspecified in the equality operator declaration enables merge\n\nMulticolumn Indexes and Sorting\n\nLet’s take a closer look at sorting multicolumn indexes.\n\nFirst and foremost, it is very important to choose the optimal order of columns whendeclaringanindex: datasortingwithinpageswillbeginwiththefirstcolumn, then move on to the second one, and so on. Multicolumn indexes can guarantee efficient search only if the provided filter condition spans a continuous sequence of columns starting from the very first one: the first column,the first two columns, the range between the first and the third columns, etc. Other types of conditions can only be used to filter out redundant values that have been fetched based on other criteria.\n\nHere is the order of index entries in the first leaf page of the index that has been created on the tickets table and includes booking references and passenger names:\n\n=> SELECT itemoffset, data_to_text(data) FROM bt_page_items('tickets_bref_name_idx',1) WHERE itemoffset > 1;\n\n499\n\np. ���",
      "content_length": 1397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 501,
      "content": "Chapter 25 B-tree\n\nitemoffset |\n\ndata_to_text\n\n−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n2 | 000004, PETR MAKAROV 3 | 00000F, ANNA ANTONOVA 4 | 000010, ALEKSANDR SOKOLOV 5 | 000010, LYUDMILA BOGDANOVA 6 | 000012, TAMARA ZAYCEVA 7 | 000026, IRINA PETROVA 8 | 00002D, ALEKSANDR SMIRNOV\n\n...\n\n187 | 0003EF, VLADIMIR CHERNOV 188 | 00040C, ANTONINA KOROLEVA 189 | 00040C, DMITRIY FEDOROV 190 | 00041E, EGOR FEDOROV 191 | 00041E, ILYA STEPANOV 192 | 000447, VIKTOR VASILEV 193 | 00044D, NADEZHDA KULIKOVA\n\n(192 rows)\n\nIn this case, an efficient search for tickets is only possible either by the booking reference and the passenger name, or by the booking reference alone.\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE book_ref = '000010';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using tickets_book_ref_idx on tickets\n\nIndex Cond: (book_ref = '000010'::bpchar)\n\n(2 rows)\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE book_ref = '000010' AND passenger_name = 'LYUDMILA BOGDANOVA';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using tickets_bref_name_idx on tickets\n\nIndex Cond: ((book_ref = '000010'::bpchar) AND (passenger_name...\n\n(2 rows)\n\nBut if we decide to look for a passenger name, we have to scan all the rows:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE passenger_name = 'LYUDMILA BOGDANOVA';\n\n500",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 502,
      "content": "25.4 Operator Class\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nGather\n\nWorkers Planned: 2 −> Parallel Seq Scan on tickets\n\nFilter: (passenger_name = 'LYUDMILA BOGDANOVA'::text)\n\n(4 rows)\n\nEven if the planner chooses to perform an index scan, all index entries will still have to be traversed.1 Unfortunately, the plan will not show that the condition is actually used only for filtering the result.\n\nIf the first column does not have too manydistinctvalues v1,v2,…,vn,it could be beneficial to perform several passes over the corresponding subtrees, virtually replacing a single search by condition “col2 = value”with a series of searches by the following conditions:\n\ncol1 = v1 ��� col2 = value col1 = v2 ��� col2 = value ⋯ col1 = vn ��� col2 = value\n\nThis type of an index access is called a Skip Scan, but it is not implemented yet.2\n\nAnd vice versa, if an index is created on passenger names and booking numbers, it will better suit queries by either the passenger name alone or both the passenger name and booking reference:\n\n=> CREATE INDEX tickets_name_bref_idx ON tickets(passenger_name, book_ref);\n\n=> SELECT itemoffset, data_to_text(data) FROM bt_page_items('tickets_name_bref_idx',1) WHERE itemoffset > 1;\n\nitemoffset |\n\ndata_to_text\n\n−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n2 | ADELINA ABRAMOVA, E37EDB 3 | ADELINA AFANASEVA, 1133B7 4 | ADELINA AFANASEVA, 4F3370 5 | ADELINA AKIMOVA, 7D2881 6 | ADELINA ALEKSANDROVA, 3C3ADD 7 | ADELINA ALEKSANDROVA, 52801E\n\n...\n\n1 backend/access/nbtree/nbtsearch.c, _bt_first function 2 commitfest.postgresql.org/34/1741\n\n501",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 503,
      "content": "Chapter 25 B-tree\n\n185 | ADELINA LEBEDEVA, 0A00E3 186 | ADELINA LEBEDEVA, DAEADE 187 | ADELINA LEBEDEVA, DFD7E5 188 | ADELINA LOGINOVA, 8022F3 189 | ADELINA LOGINOVA, EE67B9 190 | ADELINA LUKYANOVA, 292786 191 | ADELINA LUKYANOVA, 54D3F9\n\n(190 rows)\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE passenger_name = 'LYUDMILA BOGDANOVA';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on tickets\n\nRecheck Cond: (passenger_name = 'LYUDMILA BOGDANOVA'::text) −> Bitmap Index Scan on tickets_name_bref_idx\n\nIndex Cond: (passenger_name = 'LYUDMILA BOGDANOVA'::text)\n\n(4 rows)\n\nIn addition to the column order, you should also pay attention to the sort order when creating a new index. By default, values are sorted in ascending order (���), but you can reverse it (����) if required. It does not matter much if an index is built over a single column, as it can be scanned in any direction. But in a multicolumn index the order becomes important.\n\nOur newly created index can be used to retrieve the data sorted by both columns either in ascending or in descending order:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets ORDER BY passenger_name, book_ref;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using tickets_name_bref_idx on tickets\n\n(1 row)\n\n=> EXPLAIN (costs off) SELECT * FROM tickets ORDER BY passenger_name DESC, book_ref DESC;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan Backward using tickets_name_bref_idx on tickets\n\n(1 row)\n\nButthisindexcannotreturnthedatarightawayifitneedstobesortedinascending order by one column and in descending order by the other column at the same time.\n\n502",
      "content_length": 1713,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 504,
      "content": "25.4 Operator Class\n\nIn this case, the index provides partially ordered data that has to be further sorted by the second attribute:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets ORDER BY passenger_name ASC, book_ref DESC;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIncremental Sort\n\nSort Key: passenger_name, book_ref DESC Presorted Key: passenger_name −> Index Scan using tickets_name_bref_idx on tickets\n\n(4 rows)\n\nThe location of ���� values also affects the ability to use index for sorting. By default, ���� values are considered “greater” than regular values for the purpose of sorting, that is, they are located in the right side of the tree if the sort order is ascending and in the left side if the sort order is descending. The location of ���� values can be changed using the ����� ���� and ����� ����� clauses.\n\nIn the next example, the index does not satisfy the ����� �� clause, so the result has to be sorted:\n\n=> EXPLAIN (costs off) SELECT * FROM tickets ORDER BY passenger_name NULLS FIRST, book_ref DESC;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nGather Merge\n\nWorkers Planned: 2 −> Sort\n\nSort Key: passenger_name NULLS FIRST, book_ref DESC −> Parallel Seq Scan on tickets\n\n(5 rows)\n\nBut if we create an index that follows the desired order, it will be used:\n\n=> CREATE INDEX tickets_name_bref_idx2 ON tickets(passenger_name NULLS FIRST, book_ref DESC);\n\n=> EXPLAIN (costs off) SELECT * FROM tickets ORDER BY passenger_name NULLS FIRST, book_ref DESC;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using tickets_name_bref_idx2 on tickets\n\n(1 row)\n\n503\n\np. ���",
      "content_length": 1665,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 505,
      "content": "p. ���\n\nChapter 25 B-tree\n\n25.5 Properties\n\nLet’s take a look at the interface properties of �-trees.\n\nAccess Method Properties\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'btree';\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | can_order | t | t | can_unique | can_multi_col | t | t | can_exclude | t | can_include\n\nbtree btree btree btree btree (5 rows)\n\nB-trees can order data and ensure its uniqueness. It is the only access method with such properties.\n\nMany access methods support multicolumn indexes,but since values in �-trees are ordered, you have to pay close attention to the order of columns in an index.\n\nFormally, exclusion constraints are supported, but they are limited to equality conditions, which makes them analogous to unique constraints. It is much more preferable to use a full-fledged unique constraint instead.\n\nB-tree indexes can also be extended with additional ������� columns that do not participate in search.\n\nIndex-Level Properties\n\n=> FROM unnest(array[\n\nSELECT p.name, pg_index_has_property('flights_pkey', p.name)\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\n504",
      "content_length": 1331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 506,
      "content": "25.5 Properties\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| t clusterable | t index_scan bitmap_scan | t backward_scan | t\n\n(4 rows)\n\nB-tree indexes can be used for clusterization.\n\nBoth index scans and bitmap scans are supported. Since leaf pages are bound into a bidirectional list, an index can also be traversed backwards, which results in the reverse sort order:\n\n=> EXPLAIN (costs off) SELECT * FROM bookings ORDER BY book_ref DESC;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan Backward using bookings_pkey on bookings\n\n(1 row)\n\nColumn-Level Properties\n\n=> SELECT p.name,\n\npg_index_column_has_property('flights_pkey', 1, p.name)\n\nFROM unnest(array[\n\n'asc', 'desc', 'nulls_first', 'nulls_last', 'orderable', 'distance_orderable', 'returnable', 'search_array', 'search_nulls'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nasc | t desc | f nulls_first | f nulls_last | t | t orderable distance_orderable | f | t returnable | t search_array | t search_nulls\n\n(9 rows)\n\n505",
      "content_length": 1096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 507,
      "content": "Chapter 25 B-tree\n\nThe O�������� property indicates that the data stored in a �-tree is ordered, while the first four properties (A�� and D���, N���� F���� and N���� L���) define the ac- tual order in a particular column. In this example, column values are sorted in ascending order with ���� values listed last.\n\nThe S����� N���� property indicates whether ���� values can be searched.\n\nB-trees do not support ordering operators (D������� O��������), even though it has been attempted to implement them.1\n\nB-treessupportsearchingformultipleelementsinanarray(theS�����A����prop- erty) and can return the resulting data without heap access (R���������).\n\n1 commitfest.postgresql.org/27/1804\n\n506",
      "content_length": 694,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 508,
      "content": "26\n\nGiST\n\n26.1 Overview\n\nGi�� (Generalized Search Tree)1 is an access method that is virtually a generaliza- tion of a balanced search tree for data types that support relative positioning of values. B-tree applicability is limited to ordinal types that allow comparison oper- ations (but the support provided for such types is extremely efficient). As for �i��, its operator class allows defining arbitrary criteria for data distribution in the tree. A �i�� index can accommodate an �-tree for spatial data, an ��-tree for sets, and a signature tree for any data types (including texts and images).\n\nThanks to extensibility, you can create a new access method in Postgre��� from scratch byimplementing the interfaceof the indexing engine. However,apart from designing the indexing logic, you have to define the page layout, an efficient lock- ing strategy, and ��� support. It all takes strong programming skills and much implementation efforts. Gi�� simplifies this task,addressing all the low-level tech- nicalities and providing the basis for the search algorithm. To use the �i�� method with a new data type, you just need to add a new operator class that includes a dozensupportfunctions. Unlikethetrivialoperatorclassprovidedfor�-trees,such a class contains most of the indexing logic. Gi�� can be regarded as a framework for building new access methods in this respect.\n\nSpeaking in the most general terms, each entry that belongs to a leaf node (a leaf entry) contains a predicate (a logical condition) and a heap tuple ��. The index key must satisfy the predicate; it does not matter whether the key itself is a part of this entry or not.\n\n1 postgresql.org/docs/14/gist.html backend/access/gist/README\n\n507",
      "content_length": 1716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 509,
      "content": "Chapter 26 GiST\n\nEachentryinaninnerleaf(aninnerentry)alsocontainsapredicateandareference to a child node; all the indexed data of the child subtree must satisfy this predicate. In other words, the predicate of an inner entry is the union of all the predicates of its child entries. This important property of �i�� serves the purpose of simple ranking used by �-trees.\n\nGi�� tree search relies on the consistency function,which is one of the support func- tions defined by the operator class.\n\nThe consistency function is called on an index entry to determine whether the predicate of this entry is “consistent” with the search condition (“indexed-column operator expression”). For an inner entry,it shows whether we have to descend into the corresponding subtree; for a leaf entry,it checks whether its index key satisfies the condition.\n\nThe search starts at the root node,1 which is typical of a tree search. The consis- tency function determines which child nodes must be traversed and which can be skipped. Then this procedure is repeated for each of the found child nodes; unlike a �-tree, a �i�� index may have several such nodes. Leaf node entries selected by the consistency function are returned as results.\n\nThe search is always depth-first: the algorithm tries to get to a leaf page as soon as possible. Therefore, it can start returning results right away, which makes a lot of sense if the user needs to get only the first few rows.\n\nTo insert a new value into a �i�� tree, it is impossible to use the consistency func- tion, since we need to choose exactly one node to descend to.2 This node must have the minimal insert cost; it is determined by the penalty function of the opera- tor class.\n\nJust like in the case of a �-tree, the selected node may turn out to have no free space, which leads to a split.3 This operation needs two more functions. One of them distributes the entries between the old and new nodes; the other forms the union of the two predicates to update the predicate of the parent node.\n\n1 backend/access/gist/gistget.c, gistgettuple function 2 backend/access/gist/gistutil.c, gistchoose function 3 backend/access/gist/gistsplit.c, gistSplitByKey function\n\n508",
      "content_length": 2195,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 510,
      "content": "26.2 R-Trees for Points\n\nAs new values are being added, the existing predicates expand, and they are typ- ically narrowed down only if the page is split or the whole index is rebuilt. Thus, frequent updates of a �i�� index can lead to its performance degradation.\n\nSince all these theoretical discussions may seem too vague, and the exact logic mostlydependsonaparticularoperatorclassanyway,Iwillprovideseveralspecific examples.\n\n26.2 R-Trees for Points\n\nThe first example deals with indexing points (or other geometries) on a plane. A regular �-tree cannot be used for this data type, as there are no comparison op- erators defined for points. Clearly, we could have implemented such operators on our own, but geometries need index support for totally different operations. I will go over just two of them: search for objects contained within a particular area and nearest neighbor search.\n\nAn �-tree draws rectangles on a plane; taken together, they must cover all the in- dexed points. An index entry stores the bounding box, and the predicate can be defined as follows: the point lies within this bounding box.\n\nThe root of an �-tree contains several large rectangles (that may also overlap). Childnodesholdsmallerrectanglesthatfittheirparentnodes; together,theycover all the underlying points.\n\nLeaf nodes should contain the indexed points themselves, but �i�� requires that all entries have the same data type; therefore, leaf entries are also represented by rectangles, which are simply reduced to points.\n\nTo better visualize this structure, let’s take a look at three levels of an �-tree built over airport coordinates. For this example,I have extended the airports table of the demo database up to five thousand rows.1 I have also reduced the fillfactor value to make the tree deeper; the default value would have given us a single-level tree.\n\n1 You can download the corresponding file at edu.postgrespro.ru/internals-14/extra_airports.copy\n\n(I have used the data available at the openflights.org website).\n\n509\n\n90",
      "content_length": 2026,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 511,
      "content": "Chapter 26 GiST\n\n=> CREATE TABLE airports_big AS SELECT * FROM airports_data;\n\n=> COPY airports_big FROM\n\n'/home/student/internals/airports/extra_airports.copy';\n\n=> CREATE INDEX airports_gist_idx ON airports_big USING gist(coordinates) WITH (fillfactor=10);\n\nAt the upper level, all the points are included into several (partially overlapping) bounding boxes:\n\nAt the next level, big rectangles are split into smaller ones:\n\n510",
      "content_length": 429,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 512,
      "content": "26.2 R-Trees for Points\n\nFinally, at the inner level of the tree each bounding box contains as many points as a single page can accommodate:\n\nThis index uses the point_ops operator class, which is the only one available for points.\n\nRectangles and any other geometries can be indexed in the same manner, but in- stead of the object itself the index has to store its bounding box.\n\nPage Layout\n\nYou can study\n\n�i�� pages using the pageinspect extension.\n\nUnlike �-tree indexes, �i�� has no metapage, and the zero page is always the root of the tree. If the root page gets split, the old root is moved into a separate page, and the new root takes its place.\n\nHere is the contents of the root page:\n\n=> SELECT ctid, keys FROM gist_page_items(\n\nget_raw_page('airports_gist_idx', 0), 'airports_gist_idx'\n\n);\n\n511\n\nv. ��",
      "content_length": 814,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 513,
      "content": "Chapter 26 GiST\n\nctid\n\n|\n\nkeys\n\n−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n(207,65535) | (coordinates)=((50.84510040283203,78.246101379395)) (400,65535) | (coordinates)=((179.951004028,73.51780700683594)) (206,65535) | (coordinates)=((−1.5908199548721313,40.63980103)) (466,65535) | (coordinates)=((−1.0334999561309814,82.51779937740001))\n\n(4 rows)\n\nThese four rows correspond to the four rectangles of the upper level shown in the first picture. Unfortunately, the keys are displayed here as points (which makes sense for leaf pages), not as rectangles (which would be more logical for inner pages). But we can always get raw data and interpret it on our own.\n\nTo extract more detailed information, you can use the gevel extension,1 which is not in- cluded into the standard Postgre��� distribution.\n\nOperator Class\n\nThe following query returns the list of support functions that implement the logic of search and insert operations for trees:2\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'gist' AND opcname = 'point_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−\n\n1 | gist_point_consistent 2 | gist_box_union 3 | gist_point_compress 5 | gist_box_penalty 6 | gist_box_picksplit 7 | gist_box_same 8 | gist_point_distance 9 | gist_point_fetch\n\n11 | gist_point_sortsupport\n\n(9 rows)\n\n1 sigaev.ru/git/gitweb.cgi?p=gevel.git 2 postgresql.org/docs/14/gist-extensibility.html\n\n512",
      "content_length": 1555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 514,
      "content": "26.2 R-Trees for Points\n\nI have already listed the mandatory functions above:\n\n� consistency function used to traverse the tree during search\n\n� union function that merges rectangles\n\n� penalty function used to choose the subtree to descend to when inserting an\n\nentry\n\n� picksplit function that distributes entries between new pages after a page split\n\n� same function that checks two keys for equality\n\nThe point_ops operator class includes the following operators:\n\n=> SELECT amopopr::regoperator, amopstrategy AS st, oprcode::regproc,\n\nleft(obj_description(opr.oid, 'pg_operator'), 19) description\n\nFROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'gist' AND opcname = 'point_ops' ORDER BY amopstrategy;\n\namopopr\n\n| st |\n\noprcode\n\n|\n\ndescription\n\n−−−−−−−−−−−−−−−−−−−+−−−−+−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−\n\n<<(point,point) >>(point,point) ~=(point,point) <<|(point,point) |>>(point,point) <−>(point,point) <@(point,box) <^(point,point) >^(point,point) <@(point,polygon) | 48 | pt_contained_poly <@(point,circle)\n\n1 | point_left | 5 | point_right | | 6 | point_eq | 10 | point_below | 11 | point_above | 15 | point_distance | 28 | on_pb | 29 | point_below | 30 | point_above\n\n| is left of | is right of | same as | is below | is above | distance between | point inside box | deprecated, use <<| | deprecated, use |>> | is contained by | 68 | pt_contained_circle | is contained by\n\n(11 rows)\n\nOperatornamesdonotusuallytellusmuchaboutoperatorsemantics,sothisquery also displays the names of the underlying functions and their descriptions. One way or another, all the operators deal with relative positioning of geometries (left of, right of, above, below, contains, is contained) and the distance between them.\n\nAs compared to �-trees, �i�� offers more strategies. Some of the strategy numbers are common to several types of indexes,1 while others are calculated by formulas\n\n1 include/access/stratnum.h\n\n513",
      "content_length": 2030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 515,
      "content": "Chapter 26 GiST\n\n(for example, ��, ��, and �� virtually represent one and the same strategy: is con- tained for rectangles, polygons, and circles). Besides, �i�� supports some obsolete operator names (<<| and |>>).\n\nOperator classes may implement only some of the available strategies. For ex- ample, the contains strategy is not supported by the operator class for points, but it is available in classes defined for geometries with measurable area (box_ops, poly_ops, and circle_ops).\n\nSearch for Contained Elements\n\nA typical query that can be sped up by an index returns all points of the specified area.\n\nFor example,let’s find all the airports located within one degree from the centre of Moscow:\n\n=> SELECT airport_code, airport_name->>'en' FROM airports_big WHERE coordinates <@ '<(37.622513,55.753220),1.0>'::circle;\n\nairport_code |\n\n?column?\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSVO VKO DME BKA ZIA CKL OSF\n\n| Sheremetyevo International Airport | Vnukovo International Airport | Domodedovo International Airport | Bykovo Airport | Zhukovsky International Airport | Chkalovskiy Air Base | Ostafyevo International Airport\n\n(7 rows)\n\n=> EXPLAIN (costs off) SELECT airport_code FROM airports_big WHERE coordinates <@ '<(37.622513,55.753220),1.0>'::circle;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on airports_big\n\nRecheck Cond: (coordinates <@ '<(37.622513,55.75322),1>'::circle) −> Bitmap Index Scan on airports_gist_idx\n\nIndex Cond: (coordinates <@ '<(37.622513,55.75322),1>'::ci...\n\n(4 rows)\n\n514",
      "content_length": 1580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 516,
      "content": "26.2 R-Trees for Points\n\nWe can take a closer look at this operator using a trivial example shown in the figure below:\n\n9\n\n8 7\n\n6 5\n\n4 3\n\n2\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nIf bounding boxes are selected this way, the index structure will be as follows:\n\n0,0–3,4 0,0–3,4\n\n5,3–9,9 5,3–9,9\n\n0,0–3,2 0,0–3,2\n\n0,3–3,4 0,3–3,4\n\n5,3–8,5 5,3–8,5\n\n6,6–9,9 6,6–9,9\n\n0,0 0,0\n\n1,2 1,2\n\n3,1 3,1\n\n0,4 0,4\n\n3,3 3,3\n\n5,3 5,3\n\n8,5 8,5\n\n6,6 6,6\n\n8,9 8,9\n\n9,7 9,7\n\nThe contains operator <@ determines whether a particular point is located within the specified rectangle. The consistency function for this operator1 returns “yes” if the rectangle of the index entry has any common points with this rectangle. It means that for leaf node entries, which store rectangles reduced to points, this function determines whether the point is contained within the specified rectangle.\n\n1 backend/access/gist/gistproc.c, gist_point_consistent function\n\n515",
      "content_length": 930,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 517,
      "content": "Chapter 26 GiST\n\nFor example,let’s find the inner points of rectangle (�,�)–(�,�),which is hatched in the figure below:\n\n9\n\n8 7\n\n6 5\n\n4 3\n\n2\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0,0–3,4 0,0–3,4\n\n5,3–9,9 5,3–9,9\n\n0,0–3,2 0,0–3,2\n\n0,3–3,4 0,3–3,4\n\n5,3–8,5 5,3–8,5\n\n6,6–9,9 6,6–9,9\n\n0,0 0,0\n\n1,2 1,2\n\n3,1 3,1\n\n0,4 0,4\n\n3,3 3,3\n\n5,3 5,3\n\n8,5 8,5\n\n6,6 6,6\n\n8,9 8,9\n\nThe search starts at the root node. The bounding box overlaps with (�,�)–(�,�), but does not overlap with (�,�)–(�,�). It means that we do not have to descend into the second subtree.\n\nAt the next level,the bounding box overlaps with (�,�)–(�,�) and touches (�,�)–(�,�), so we have to check both subtrees.\n\nOnce we get to leaf nodes, we just have to go through all the points that they con- tain and return those that satisfy the consistency function.\n\nA �-tree search always selects exactly one child node. A �i�� search,however,may have to scan several subtrees, especially if their bounding boxes overlap.\n\n516\n\n9,7 9,7",
      "content_length": 982,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 518,
      "content": "26.2 R-Trees for Points\n\nNearest Neighbor Search\n\nMost of the operators supported by indexes (such as = or <@ shown in the previous example) are typically called search operators, as they define search conditions in queries. Such operators are predicates, that is, they return a logical value.\n\nBut there is also a group of ordering operators, which return the distance between arguments. Such operators are used in the ����� �� clause and are typically sup- ported by indexes that have the D������� O�������� property, which enables you to quickly find the specified number of nearest neighbors. This type of search is known as k-��, or k-nearest neighbor search.\n\nFor example, we can find �� airports closest to Kostroma:\n\n=> SELECT airport_code, airport_name->>'en' FROM airports_big ORDER BY coordinates <-> '(40.926780,57.767943)'::point LIMIT 10;\n\nairport_code |\n\n?column?\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nKMW IAR IWA VGD RYB GOJ CEE CKL ZIA BKA\n\n| Kostroma Sokerkino Airport | Tunoshna Airport | Ivanovo South Airport | Vologda Airport | Staroselye Airport | Nizhny Novgorod Strigino International Airport | Cherepovets Airport | Chkalovskiy Air Base | Zhukovsky International Airport | Bykovo Airport\n\n(10 rows)\n\n=> EXPLAIN (costs off) SELECT airport_code FROM airports_big ORDER BY coordinates <-> '(40.926780,57.767943)'::point LIMIT 5;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nLimit\n\n−> Index Scan using airports_gist_idx on airports_big\n\nOrder By: (coordinates <−> '(40.92678,57.767943)'::point)\n\n(3 rows)\n\nSince an index scan returns the results one by one and can be stopped any time, several first values can be found very quickly.\n\n517\n\np. ���",
      "content_length": 1730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 519,
      "content": "Chapter 26 GiST\n\nIt would be very hard to achieve efficient search without index support. We would have to find all the points that appear in a particular area and then gradually expand this area until the requested number of results is returned. It would require several index scans,not to mention the problem of choosing the size of the original area and its increments.\n\nYou can see the operator type in the system catalog (“s” stands for search, “o” de- notes ordering operators):\n\n=> SELECT amopopr::regoperator, amoppurpose, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily\n\nWHERE amname = 'gist' AND opcname = 'point_ops' ORDER BY amopstrategy;\n\namopopr\n\n| amoppurpose | amopstrategy\n\n−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n| s <<(point,point) | s >>(point,point) | s ~=(point,point) | s <<|(point,point) | s |>>(point,point) | o <−>(point,point) | s <@(point,box) | s <^(point,point) | s >^(point,point) <@(point,polygon) | s | s <@(point,circle)\n\n| | | | | | | | | | |\n\n1 5 6 10 11 15 28 29 30 48 68\n\n(11 rows)\n\nTo support such queries, an operator class must define an additional support func- tion: it is the distance function, which is called on the index entry to calculate the distance from the value stored in this entry to some other value.\n\nFor a leaf element representing an indexed value, this function must return the distance to this value. In the case of points,1 it is a regular Euclidean distance, which equals √(x2 − x1)2 + (y2 − y1)2.\n\nFor an inner element, the function must return the minimal of all the possible dis- tances from its child leaf elements. Since it is quite costly to scan all the child entries, the function can optimistically underestimate the distance (sacrificing\n\n1 backend/utils/adt/geo_ops.c, point_distance function\n\n518",
      "content_length": 1847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 520,
      "content": "26.2 R-Trees for Points\n\nefficiency), but it must never return a bigger value—it would compromise search correctness.\n\nTherefore,for an inner element represented by a bounding box,the distance to the point is understood in the regular mathematical sense: it is either the minimal distance between the point and the rectangle or zero if the point is inside the rect- angle.1 This value can be easily calculated without traversing all the child points of the rectangle, and it is guaranteed to be not greater than the distance to any of these points.\n\nLet’s consider the algorithm of searching for three nearest neighbors of point (�,�):\n\n9\n\n8\n\n7 6 5\n\n4 3\n\n2\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nThe search starts at the root node, which holds two bounding boxes. The distance from the specified point to rectangle (�,�)–(�,�) is taken as the distance to the rect- angle’s corner (�,�), which equals �.�. The distance to (�,�)–(�,�) is �.�. (I am going to round all the values here to the first decimal place; such accuracy will be enough for this example.)\n\nChild nodes get traversed in the order of distance increase. Thus, we first descend into the right child node,which contains two rectangles: (�,�)–(�,�) and (�,�)–(�,�). The distance to the first one is �.�; the distance to the second one is �.�.\n\nOnce again, we choose the right subtree and get into the leaf node that contains three points: (�,�) at the distance of �.�, (�,�) at the distance of �.�, and (�,�) at the distance of �.�.\n\n1 backend/utils/adt/geo_ops.c, box_closest_point function\n\n519",
      "content_length": 1556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 521,
      "content": "Chapter 26 GiST\n\n5.0\n\n0,0–3,4 0,0–3,4\n\n5,3–9,9 5,3–9,9\n\n0.0\n\n0,0–3,2 0,0–3,2\n\n0,3–3,4 0,3–3,4\n\n3.0\n\n5,3–8,5 5,3–8,5\n\n6,6–9,9 6,6–9,9\n\n0.0\n\n0,0 0,0\n\n1,2 1,2\n\n3,1 3,1\n\n0,4 0,4\n\n3,3 3,3\n\n5,3 5,3\n\n8,5 8,5\n\n6,6 6,6\n\n8,9 8,9\n\n9,7 9,7\n\n2.0 2.2 3.2\n\nThus, we have received the first two points: (�,�) and (�,�). But the distance to the third point of this node is greater than the distance to rectangle (�,�)–(�,�).\n\nSo now we have to descend into the left child node,which contains two points. The distance to point (�,�) is �.�,while the distance to (�,�) is �.�. It turns out that point (�,�) in the previous child node is closer to point (�,�) than any of the nodes of the left subtree, so we can return it as the third result.\n\n5.0\n\n0,0–3,4 0,0–3,4\n\n5,3–9,9 5,3–9,9\n\n0.0\n\n0,0–3,2 0,0–3,2\n\n0,3–3,4 0,3–3,4\n\n3.0\n\n5,3–8,5 5,3–8,5\n\n6,6–9,9 6,6–9,9\n\n0.0\n\n0,0 0,0\n\n1,2 1,2\n\n3,1 3,1\n\n0,4 0,4\n\n3,3 3,3\n\n5,3 5,3\n\n8,5 8,5\n\n6,6 6,6\n\n8,9 8,9\n\n9,7 9,7\n\n5.1 3.6\n\n2.0 2.2 3.2\n\n520",
      "content_length": 962,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 522,
      "content": "26.2 R-Trees for Points\n\nThis example illustrates the requirements that must be satisfied by the distance function for inner entries. Because of the reduced distance (�.� instead of �.�) to rectangle (�,�)–(�,�), an extra node had to be scanned, so search efficiency has declined; however, the algorithm itself remained correct.\n\nInsertion\n\nWhen a new key is getting inserted into an �-tree, the node to be used for this key is determined by the penalty function: the size of the bounding box must be increased as little as possible.1\n\nFor example, point (�,�) will be added to rectangle (�,�)–(�,�) because its area will increase by only � units, while rectangle (�,�)–(�,�) would have to be increased by �� units. At the next (leaf) level, the point will be added to rectangle (�,�)–(�,�), following the same logic.\n\n9\n\n9\n\n8\n\n8\n\n7\n\n7\n\n6 5\n\n6 5\n\n4 3\n\n4 3\n\n2\n\n2\n\n1 0\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nAssuming that a page holds three elements at the most, it has to be split in two, and the elements have to be distributed between the new pages. In this example, the result seems obvious, but in the general case the data distribution task is not\n\n1 backend/access/gist/gistproc.c, gist_box_penalty function\n\n521",
      "content_length": 1244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 523,
      "content": "p. ���\n\nChapter 26 GiST\n\nso trivial. First and foremost, the picksplit function attempts to minimize overlaps between bounding boxes, aiming at getting smaller rectangles and uniform distri- bution of points between pages.1\n\n9\n\n8\n\n7\n\n6 5\n\n4 3\n\n2\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nExclusion Constraints\n\nGi�� indexes can also be used in exclusion constraints.\n\nAn exclusion constraint guarantees that the specified fields of any two heap tuples do not match each other in the sense of some operator. The following conditions must be satisfied:\n\nThe exclusion constraint must be supported by the indexing method (the C��\n\nE������ property).\n\nThe operator must belong to the operator class of this indexing method.\n\nThe operator must be commutative, that is, the condition “a operator b =\n\nb operator a” must be true.\n\nFor the hash and btree access methods considered above,the only suitable operator is equal to. It virtually turns an exclusion constraint into a unique one,which is not particularly useful.\n\n1 backend/access/gist/gistproc.c, gist_box_picksplit function\n\n522",
      "content_length": 1075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 524,
      "content": "26.2 R-Trees for Points\n\nThe gist method has two more applicable strategies:\n\noverlapping: the && operator\n\nadjacency: the -|- operator (defined for intervals)\n\nTo try it out,let’s create a constraint that forbids placing airports too close to each other. This condition can be formulated as follows: circles of a particular radius with centers lying at the airports’ coordinates must not overlap:\n\n=> ALTER TABLE airports_data ADD EXCLUDE USING gist (circle(coordinates,0.2) WITH &&);\n\n=> INSERT INTO airports_data(\n\nairport_code, airport_name, city, coordinates, timezone\n\n) VALUES (\n\n'ZIA', '{}', '{\"en\": \"Moscow\"}', point(38.1517, 55.5533), 'Europe/Moscow'\n\n);\n\nERROR: \"airports_data_circle_excl\" DETAIL: precision))=(<(38.1517,55.5533),0.2>) conflicts with existing key (circle(coordinates, 0.2::double precision))=(<(37.90629959106445,55.40879821777344),0.2>).\n\nconflicting key value violates exclusion constraint\n\nKey (circle(coordinates, 0.2::double\n\nWhen an exclusion constraint is defined, an index to enforce it is added automati- cally. Here it is a �i�� index built over an expression.\n\nLet’s take a look at a more complex example. Suppose we need to allow close prox- imity of airports, but only if they belong to the same city. A possible solution is to define a new integrity constraint that can be formulated as follows: it is forbidden to have pairs of rows with intersections (&&) of circles if their centers lie at the airports’ coordinates and the corresponding cities have different names (!=).\n\nAn attempt to create such a constraint results in an error because there is no op- erator class for the text data type:\n\n=> ALTER TABLE airports_data DROP CONSTRAINT airports_data_circle_excl; -- delete old data\n\n=> ALTER TABLE airports_data ADD EXCLUDE USING gist (\n\ncircle(coordinates,0.2) WITH &&,\n\n(city->>'en') WITH !=\n\n);\n\n523",
      "content_length": 1850,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 525,
      "content": "Chapter 26 GiST\n\nERROR: method \"gist\" HINT: default operator class for the data type.\n\ndata type text has no default operator class for access\n\nYou must specify an operator class for the index or define a\n\nHowever, �i�� does provide strategies like strictly left of, strictly right of, and same, which can also be applied to regular ordinal data types, such as numbers or text strings. Thebtree_gistextensionisspecificallyintendedtoimplement�i��support for operations that are typically used with �-trees:\n\n=> CREATE EXTENSION btree_gist;\n\n=> ALTER TABLE airports_data ADD EXCLUDE USING gist (\n\ncircle(coordinates,0.2) WITH &&,\n\n(city->>'en') WITH !=\n\n);\n\nALTER TABLE\n\nThe constraint is created. Now we cannot add Zhukovsky airport belonging to a town with the same name because Moscow airports are too close:\n\n=> INSERT INTO airports_data(\n\nairport_code, airport_name, city, coordinates, timezone\n\n) VALUES (\n\n'ZIA', '{}', '{\"en\": \"Zhukovsky\"}', point(38.1517, 55.5533), 'Europe/Moscow'\n\n);\n\nERROR: \"airports_data_circle_expr_excl\" DETAIL: 'en'::text))=(<(38.1517,55.5533),0.2>, Zhukovsky) conflicts with existing key (circle(coordinates, 0.2::double precision), (city −>> 'en'::text))=(<(37.90629959106445,55.40879821777344),0.2>, Moscow).\n\nconflicting key value violates exclusion constraint\n\nKey (circle(coordinates, 0.2::double precision), (city −>>\n\nBut we can do it if we specify Moscow as this airport’s city:\n\n=> INSERT INTO airports_data(\n\nairport_code, airport_name, city, coordinates, timezone\n\n) VALUES (\n\n'ZIA', '{}', '{\"en\": \"Moscow\"}', point(38.1517, 55.5533), 'Europe/Moscow'\n\n);\n\nINSERT 0 1\n\n524",
      "content_length": 1613,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 526,
      "content": "26.2 R-Trees for Points\n\nIt is important to remember that even though GiST supports greater than,less than, and equal to operations, �-trees are much more efficient in this respect, especially when accessing a range of values. So it makes sense to use the trick with the btree_gist extension shown above only if the �i�� index is really needed for other legitimate reasons.\n\nProperties\n\nAccess method properties. Here are the properties of the gist method:\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'gist';\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | f | can_order | can_unique | f | can_multi_col | t | t | can_exclude | t | can_include\n\ngist gist gist gist gist\n\n(5 rows)\n\nUnique constraints and sorting are not supported.\n\nA �i�� index can be created\n\nwith additional ������� columns.\n\nAsweknow,wecanbuildanindexoverseveralcolumns,aswellasuseitinintegrity constraints.\n\nIndex-level properties. These properties are defined at the index level:\n\n=> FROM unnest(array[\n\nSELECT p.name, pg_index_has_property('airports_gist_idx', p.name)\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\n525\n\nv. ��",
      "content_length": 1331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 527,
      "content": "Chapter 26 GiST\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| t clusterable | t index_scan bitmap_scan | t backward_scan | f\n\n(4 rows)\n\nA �i�� index can be used for clusterization.\n\nAs for data retrieval methods, both regular (row-by-row) index scans and bitmap scans are supported. However, backward scanning of �i�� indexes is not allowed.\n\nColumn-level properties. Most of the column properties are defined at the access method level, and they remain the same:\n\n=> SELECT p.name,\n\npg_index_column_has_property('airports_gist_idx', 1, p.name)\n\nFROM unnest(array[\n\n'orderable', 'search_array', 'search_nulls'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n| f orderable search_array | f search_nulls | t\n\n(3 rows)\n\nAll sort-related properties are disabled.\n\nN��� values are allowed, but �i�� is not really efficient at handling them. It is assumed that a ���� value does not increase the bounding box; such values get inserted into random subtrees, so they have to be searched for in the whole tree.\n\nHowever,a couple of column-level properties do depend on the particular operator class:\n\n=> SELECT p.name,\n\npg_index_column_has_property('airports_gist_idx', 1, p.name)\n\nFROM unnest(array[\n\n'returnable', 'distance_orderable'\n\n]) p(name);\n\n526",
      "content_length": 1325,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 528,
      "content": "26.3 RD-Trees for Full-Text Search\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nreturnable | t distance_orderable | t\n\n(2 rows)\n\nIndex-only scans are allowed, since leaf nodes keep full index keys.\n\nAs we have seen above, this operator class provides the distance operator for near- est neighbor search. The distance to a ���� value is considered to be ����; such values are returned last (similar to the ����� ���� clause in �-trees).\n\nHowever, there is no distance operator for range types (which represent segments, that is, linear geometries rather than areal ones), so this property is different for an index built for such types:\n\n=> CREATE TABLE reservations(during tsrange);\n\n=> CREATE INDEX ON reservations USING gist(during);\n\n=> SELECT p.name,\n\npg_index_column_has_property('reservations_during_idx', 1, p.name)\n\nFROM unnest(array[\n\n'returnable', 'distance_orderable'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nreturnable | t distance_orderable | f\n\n(2 rows)\n\n26.3 RD-Trees for Full-Text Search\n\nAbout Full-Text Search\n\nThe objective of full-text search1 is to select those documents from the provided set that match the search query.\n\n1 postgresql.org/docs/14/textsearch.html\n\n527",
      "content_length": 1301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 529,
      "content": "Chapter 26 GiST\n\nTo be searched, the document is cast to the tsvector type, which contains lexemes and their positions in the document. Lexemes are words converted into a format that is suitable for search. By default, all words are normalized to lowercase, and their endings are cut off:\n\n=> SET default_text_search_config = english;\n\n=> SELECT to_tsvector(\n\n'No one can tell me, nobody knows, ' || 'Where the wind comes from, where the wind goes.'\n\n);\n\nto_tsvector −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 'come':11 'goe':16 'know':7 'nobodi':6 'one':2 'tell':4 'wind':10,15\n\n(1 row)\n\nThe so-called stop words(like“the”or“from”) are filtered out: they are assumed to occur too often for the search to return any meaningful results for them. Naturally, all these transformations are configurable.\n\nA search query is represented by another type: tsquery. Any query includes one or more lexemes bound by logical connectives: & (���), | (��), ! (���). You can also use parentheses to define operator precedence.\n\n=> SELECT to_tsquery('wind & (comes | goes)');\n\nto_tsquery −−−−−−−−−−−−−−−−−−−−−−−−−−−−− 'wind' & ( 'come' | 'goe' )\n\n(1 row)\n\nThe only operator used for full-text search is the match operator @@:\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'gist' AND opcname = 'tsvector_ops' ORDER BY amopstrategy;\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n@@(tsvector,tsquery) | ts_match_vq |\n\n1\n\n(1 row)\n\n528",
      "content_length": 1664,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 530,
      "content": "26.3 RD-Trees for Full-Text Search\n\nThis operator determines whether the document satisfies the query. Here is an example:\n\n=> SELECT to_tsvector('Where the wind comes from, where the wind goes')\n\n@@ to_tsquery('wind & coming');\n\n?column? −−−−−−−−−−\n\nt\n\n(1 row)\n\nIt is by no means an exhaustive should be sufficient for understanding indexing fundamentals.\n\ndescription of full-text search,but this information\n\nIndexing tsvector Data\n\nTo work fast, full-text search has to be supported by an index.1 Since it is not documents themselves but tsvector values that get indexed, you have two options here: either build an index on an expression and perform a type cast, or add a separate column of the tsvector type and index this column. The benefit of the first approach is that it does not waste any space on storing tsvector values, which are actuallynotneededassuch. Butitisslowerthanthesecondoption,astheindexing engine has to recheck all the heap tuples returned by the access method. It means that the tsvector value has to be calculated again for each rechecked row,and as we soon will see, �i�� rechecks all rows.\n\nLet’s construct a simple example. We are going to create a two-column table: the first column will store the document, while the second one will hold the tsvector value. Wecanuseatriggertoupdatethesecondcolumn,2 butitismoreconvenient to simply declare this column as generated:\n\n3\n\n=> CREATE TABLE ts(\n\ndoc text, doc_tsv tsvector GENERATED ALWAYS AS (\n\nto_tsvector('pg_catalog.english', doc)\n\n) STORED\n\n);\n\n1 postgresql.org/docs/14/textsearch-indexes.html 2 postgresql.org/docs/14/textsearch-features#TEXTSEARCH-UPDATE-TRIGGERS.html 3 postgresql.org/docs/14/ddl-generated-columns.html\n\n529\n\np. ���\n\np. ���\n\nv. ��",
      "content_length": 1734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 531,
      "content": "english\n\nChapter 26 GiST\n\n=> CREATE INDEX ts_gist_idx ON ts USING gist(doc_tsv);\n\nIn the examples above,I used the to_tsvector function with a single argument,having set the default_text_search_config parameter to define the full-text search configuration. The volatility category of this function flavor is ������, since it is implicitly dependent on the parameter value. But here I apply another flavor that defines the configuration explicitly; this flavor is ��������� and can be used in generation expressions.\n\nLet’s insert several rows:\n\n=> INSERT INTO ts(doc) VALUES\n\n('Old MacDonald had a farm'), ('And on his farm he had some cows'), ('Here a moo, there a moo'), ('Everywhere a moo moo'), ('Old MacDonald had a farm'), ('And on his farm he had some chicks'), ('Here a cluck, there a cluck'), ('Everywhere a cluck cluck'), ('Old MacDonald had a farm'), ('And on his farm he had some pigs'), ('Here an oink, there an oink'), ('Everywhere an oink oink')\n\nRETURNING doc_tsv;\n\ndoc_tsv −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 'farm':5 'macdonald':2 'old':1 'cow':8 'farm':4 'moo':3,6 'everywher':1 'moo':3,4 'farm':5 'macdonald':2 'old':1 'chick':8 'farm':4 'cluck':3,6 'cluck':3,4 'everywher':1 'farm':5 'macdonald':2 'old':1 'farm':4 'pig':8 'oink':3,6 'everywher':1 'oink':3,4\n\n(12 rows) INSERT 0 12\n\nAs such, an �-tree is of no good for indexing documents, since the concept of bounding boxes makes no sense for them. Therefore, its ��-tree (Russian Doll)\n\n530",
      "content_length": 1461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 532,
      "content": "26.3 RD-Trees for Full-Text Search\n\nmodification is used. Instead of a bounding box, such a tree uses a bounding set, that is, a set that contains all the elements of its child sets. For full-text search, such a set contains lexemes of the document, but in the general case a bounding set can be arbitrary.\n\nThere are several ways to represent bounding sets in index entries. The simplest one is to enumerate all the elements of the set.\n\nHere is how it might look like:\n\ncow,everywher, cow,everywher, farm,macdonald, farm,macdonald, moo,old moo,old\n\nchick,cluck, chick,cluck, everywher,farm, everywher,farm, oink,pig oink,pig\n\nfarm, farm, macdonald,old macdonald,old\n\ncow,everywher, cow,everywher, farm,moo farm,moo\n\nchick,cluck, chick,cluck, everywher,farm everywher,farm\n\neverywher,farm, everywher,farm, oink,pig oink,pig\n\nfarm,macdonald,old farm,macdonald,old\n\ncow,farm cow,farm\n\nchick,farm chick,farm\n\nfarm,pig farm,pig\n\nfarm,macdonald,old farm,macdonald,old\n\nmoo moo\n\ncluck cluck\n\noink oink\n\nfarm,macdonald,old farm,macdonald,old\n\neverywher,moo everywher,moo\n\ncluck,everywher cluck,everywher\n\neverywher,oink everywher,oink\n\nTo find the documents that satisfy the ���_��� @@ ��_�������(’���’) condition, we need to descend into the nodes whose child entries are known to contain the “cow” lexeme.\n\nThe problems of such representation are obvious. The number of lexemes in a document can be enormous, while the page size is limited. Even if each particular document does not have too many distinct lexemes when taken separately, their united sets at upper levels of the tree may still turn out too big.\n\n531",
      "content_length": 1611,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 533,
      "content": "p. ���\n\nChapter 26 GiST\n\ncow,everywher, cow,everywher, farm,macdonald, farm,macdonald, moo,old moo,old\n\nchick,cluck, chick,cluck, everywher,farm, everywher,farm, oink,pig oink,pig\n\nfarm, farm, macdonald,old macdonald,old\n\ncow,everywher, cow,everywher, farm,moo farm,moo\n\nchick,cluck, chick,cluck, everywher,farm everywher,farm\n\neverywher,farm, everywher,farm, oink,pig oink,pig\n\nfarm,macdonald,old farm,macdonald,old\n\ncow,farm cow,farm\n\nchick,farm chick,farm\n\nfarm,pig farm,pig\n\nfarm,macdonald,old farm,macdonald,old\n\nmoo moo\n\ncluck cluck\n\noink oink\n\nfarm,macdonald,old farm,macdonald,old\n\neverywher,moo everywher,moo\n\ncluck,everywher cluck,everywher\n\neverywher,oink everywher,oink\n\nFull-text search uses another solution, namely a more compact signature tree. It should be well familiar to anyone who had to deal with the Bloom filter.\n\nEach lexeme can be represented by its signature: a bit string of a particular length in which only one of the bits is set to 1. The bit that should be set is determined by the hash function of the lexeme.\n\nA document’s signature is the result of a bitwise �� operation on signatures of all the lexemes in this document.\n\nSuppose we have assigned the following signatures to our lexemes:\n\nchick cluck cow everywher farm\n\n1000000 0001000 0000010 0010000 0000100\n\nmacdonald\n\n0100000\n\nmoo oink old\n\n0000100 0000010 0000001\n\npig\n\n0010000\n\n532",
      "content_length": 1375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 534,
      "content": "26.3 RD-Trees for Full-Text Search\n\nThen the documents’\n\nOld MacDonald had a farm\n\n0100101\n\nsignatures will be as follows:\n\nAnd on his farm he had some cows Here a moo, there a moo Everywhere a moo moo And on his farm he had some chicks Here a cluck, there a cluck Everywhere a cluck cluck And on his farm he had some pigs Here an oink, there an oink Everywhere an oink oink\n\n0000110 0000100 0010100 1000100 0001000 0011000 0010100 0000010 0010010\n\nAnd the index tree can be represented like this:\n\n0110111 0110111\n\n1011110 1011110\n\n0100101 0100101\n\n0010110 0010110\n\n1011100 1011100\n\n0010110 0010110\n\n0100101 0100101\n\n0000110 0000110\n\n1000100 1000100\n\n0010100 0010100\n\n0100101 0100101\n\n0000100 0000100\n\n0001000 0001000\n\n0000010 0000010\n\n0100101 0100101\n\n0010100 0010100\n\n0011000 0011000\n\n0010010 0010010\n\nThe advantages of this approach are obvious: index entries have the same size, which is quite small, so the index turns out quite compact. But there are certain disadvantages too. To begin with, it is impossible to perform an index-only scan because the index does not store index keys anymore,and each returned ��� has to be rechecked by the table. The accuracy also suffers: the index may return many false positives, which have to be filtered out during a recheck.\n\nLet’s take another look at the ���_��� @@ ��_�������(’����’) condition. The sig- nature of a query is calculated in the same way as that of a document; in this\n\n533",
      "content_length": 1438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 535,
      "content": "v. ��\n\nChapter 26 GiST\n\nparticular case it equals 0000010. The consistency function1 must find all the child nodes that have the same bits set in their signatures:\n\n0110111 0110111\n\n1011110 1011110\n\n0100101 0100101\n\n0010110 0010110\n\n1011100 1011100\n\n0010110 0010110\n\n0100101 0100101\n\n0000110 0000110\n\n1000100 1000100\n\n0010100 0010100\n\n0100101 0100101\n\n0000100 0000100\n\n0001000 0001000\n\n0000010 0000010\n\n0100101 0100101\n\n0010100 0010100\n\n0011000 0011000\n\n0010010 0010010\n\nAs compared with the previous example, more nodes have to be scanned here be- cause of false-positive hits. Since the signature’s capacity is limited, some of the lexemes in a large set are bound to have the same signatures. In this example,such lexemes are“cow”and“oink.” It means that one and the same signature can match different documents; here the signature of the query corresponds to three of them.\n\nFalse positives reduce index efficiency but do not affect its correctness in any way: since false negatives are guaranteed to be ruled out, the required value cannot be missed.\n\nClearly,thesignaturesizeisactuallybigger. Bydefault,ittakes���bytes(���bits), so the probability of collisions is much lower than in this example. If required, you can further increase the signature size up to about ���� bytes using the operator class parameter:\n\nCREATE INDEX ... USING gist(column tsvector_ops(siglen = size));\n\n1 backend/utils/adt/tsgistidx.c, gtsvector_consistent function\n\n534",
      "content_length": 1454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 536,
      "content": "26.3 RD-Trees for Full-Text Search\n\nBesides, if values are small enough (a bit smaller than 1 of the page, which takes 16 about ��� bytes for a standard page),1 it is tsvector values themselves rather than their signatures that the tsvector_ops operator class keeps in leaf pages of an index.\n\nTo see how indexing works on real data, we can take the pgsql-hackers mailing list archive.2 It contains ���,��� emails together with their send dates, subjects, author names, and body texts.\n\nLet’s add a column of the tsvector type and build an index. Here I combine three values (subject,author,and body text) into a single vector to show that documents can be generated dynamically and do not have to be stored in a single column.\n\n=> ALTER TABLE mail_messages ADD COLUMN tsv tsvector GENERATED ALWAYS AS ( to_tsvector(\n\n'pg_catalog.english', subject||' '||author||' '||body_plain\n\n) ) STORED;\n\nNOTICE: DETAIL:\n\nword is too long to be indexed Words longer than 2047 characters are ignored.\n\n...\n\nNOTICE: DETAIL: ALTER TABLE\n\nword is too long to be indexed Words longer than 2047 characters are ignored.\n\n=> CREATE INDEX mail_gist_idx ON mail_messages USING gist(tsv);\n\n=> SELECT pg_size_pretty(pg_relation_size('mail_gist_idx'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n127 MB (1 row)\n\nAs the column was being filled, a certain number of largest words were filtered out because of their size. But once the index is ready, it can be used in search queries:\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM mail_messages WHERE tsv @@ to_tsquery('magic & value');\n\n1 backend/utils/adt/tsgistidx.c, gtsvector_compress function 2 edu.postgrespro.ru/mail_messages.sql.gz\n\n535",
      "content_length": 1683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 537,
      "content": "p. ���\n\nChapter 26 GiST\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nIndex Scan using mail_gist_idx on mail_messages\n\n(actual rows=898 loops=1) Index Cond: (tsv @@ to_tsquery('magic & value'::text)) Rows Removed by Index Recheck: 7859\n\n(4 rows)\n\nTogether with ��� rows that satisfy the condition,the access method also returned ���� rows to be later filtered out by a recheck. If we increase the signature capacity, the accuracy (and, consequently, the index efficiency) will be improved, but the index size will grow:\n\n=> DROP INDEX mail_messages_tsv_idx;\n\n=> CREATE INDEX ON mail_messages USING gist(tsv tsvector_ops(siglen=248));\n\n=> SELECT pg_size_pretty(pg_relation_size('mail_messages_tsv_idx'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n139 MB (1 row)\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM mail_messages WHERE tsv @@ to_tsquery('magic & value');\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− Index Scan using mail_messages_tsv_idx on mail_messages\n\n(actual rows=898 loops=1) Index Cond: (tsv @@ to_tsquery('magic & value'::text)) Rows Removed by Index Recheck: 2060\n\n(4 rows)\n\nProperties\n\nI have already shown the access method properties ,and most of them are the same for all operator classes. But the following two column-level properties are worth mentioning:\n\n536",
      "content_length": 1354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 538,
      "content": "26.4 Other Data Types\n\n=> SELECT p.name,\n\npg_index_column_has_property('mail_messages_tsv_idx', 1, p.name)\n\nFROM unnest(array[\n\n'returnable', 'distance_orderable'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n| f returnable distance_orderable | f\n\n(2 rows)\n\nIndex-only scans are now impossible,as the original value cannot be restored from its signature. It is perfectly fine in this particular case: the tsvector value is only used for search, while we need to retrieve the document itself.\n\nThe ordering operator for the tsvector_ops class is not defined either.\n\n26.4 Other Data Types\n\nIhaveconsideredonlytwomostprominentexamples. Theyshowthateventhough the �i�� method is based on a balanced tree, it can be used for various data types thanks to different support function implementations in different operator classes. When we speak about a �i�� index,we must always specify the operator class,since it is crucial for index properties.\n\nHere are several more data types currently supported by the �i�� access method.\n\nGeometric data types. Apart from points, �i�� can index other geometric objects: rectangles,circles,polygons. All these objects are represented by their bound- ing boxes for this purpose.\n\nThe cube extension adds the same-name data type that represents multidi- mensional cubes. They are indexed using �-trees with bounding boxes of the corresponding dimension.\n\nRange types. Postgre��� provides several built-in numeric and temporal range types, such as int4range and tstzrange.1 Custom range types can be defined using the ������ ���� �� ����� command.\n\n1 postgresql.org/docs/14/rangetypes.html\n\n537",
      "content_length": 1683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 539,
      "content": "v. ��\n\nChapter 26 GiST\n\nAny range types, both standard and custom, are supported by �i�� via the range_ops operator class.1 For indexing, a one-dimensional �-tree is applied: bounding boxes are transformed to bounding segments in this case.\n\nare supported as well; they rely on the multirange_ops class. Multirange types A bounding range comprises all the ranges that are part of a multirange value.\n\nThesegextensionprovidesthesame-namedatatypeforintervalswithbounds defined with particular accuracy. It is not considered to be a range type, but it virtually is, so it is indexed in exactly the same manner.\n\nOrdinal types. Let’s recall the btree_gist extension once again: it provides operator classes for the �i�� method to support various ordinal data types, which are typically indexed by a �-tree. Such operator classes can be used to build a multicolumn index when the data type in one of the columns is not supported by �-trees.\n\nNetwork address types. The inet data type has built-in �i�� support, which is im-\n\nplemented via the inet_ops2 operator class.\n\nInteger arrays. The intarray extension expands the functionality of integer arrays to add �i�� support for them. There are two classes of operators. For small arrays, you can use gist__int_ops, which implements the ��-tree with full rep- resentation of keys in index entries. Large arrays will benefit from a more compact but less precise signature ��-tree based on the gist__bigint_ops op- erator class.\n\nExtra underscores in the names of operator classes belong to the names of arrays of basic types. For instance, alongside the more common int4[] notation, an integer array can be denoted as _int4. There are no _int and _bigint types though.\n\nLtree. The ltree extension adds the same-name data type for tree-like structures with labels. Gi�� support is provided via signature ��-trees that use the gist_ltree_ops operator class for ltree values and the gist__ltree_ops operator class for arrays of the ltree type.\n\n1 backend/utils/adt/rangetypes_gist.c 2 backend/utils/adt/network_gist.c\n\n538",
      "content_length": 2061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 540,
      "content": "26.4 Other Data Types\n\nKey–value storage. The hstore extension provides the hstore data type for storing key–value pairs. The gist_hstore_ops operator class implements index support based on a signature ��-tree.\n\nTrigrams. The pg_trgm\n\nextension adds the gist_trgm_ops class, which implements\n\nindex support for comparing text strings and wildcard search.\n\n539\n\np. ���",
      "content_length": 368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 541,
      "content": "27\n\nSP-GiST\n\n27.1 Overview\n\nThe first letters in the ��-�i�� name stand for Space Partitioning. The space here is understood as an arbitrary set of values on which the search is performed; it is not necessarily the space in the conventional sense of the word (such as a two- dimensional plane). The �i�� part of the name hints at certain similarity between �i�� and ��-�i�� methods: both of them are generalized search trees and serve as frameworks for indexing various data types.\n\nThe idea behind the ��-�i�� method1 is to split the search space into several non- overlapping regions, which are in turn can be recursively split into sub-regions. Such partitioning produces non-balanced trees (which differ from �-trees and �i�� trees) and can be used to implement such well-known structures as quadtrees,k-� trees, and radix trees (tries).\n\nNon-balanced trees typically have few branches and, consequently, large depth. For example, a quadtree node has four child nodes at the most, while a node of a k-� tree can have only two. It does not pose any problems if the tree is kept in memory; but when stored on disk, tree nodes have to be packed into pages as densely as possible to minimize �/�, and this task is not so trivial. B-tree and �i�� indexes do not have to take care of it because each of their tree nodes takes the whole page.\n\nAn inner node of an ��-�i�� tree contains a value that satisfies the condition that holds true for all its child nodes. Such a value is often called a prefix; it plays the\n\n1 postgresql.org/docs/14/spgist.html backend/access/spgist/README\n\n540",
      "content_length": 1584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 542,
      "content": "27.2 Quadtrees for Points\n\nsame role as the predicate in �i�� indexes. Pointers to ��-�i�� child nodes may have labels.\n\nLeaf node elements contain an indexed value (or some part of it) and the corre- sponding ���.\n\nJust like �i��, the ��-�i�� access method implements only the main algorithms, taking care of such low-level details as concurrent access, locks, and logging. New data types and algorithms of space partitioning can be added via the operator class interface. The operator class provides most of the logic and defines many aspects of indexing functionality.\n\nIn ��-�i��, the search is depth-first, starting at the root node.1 The nodes that are worth descending into are chosen by the consistency function, similar to the one usedin�i��. Foraninnernodeofthetree,thisfunctionreturnsasetofchildnodes whose values do not contradict the search predicate. The consistency function does not descend into these nodes: it merely assesses the corresponding labels and prefixes. For leaf nodes, it determines whether the indexed value of this node matches the search predicate.\n\nIn a non-balanced tree, search time can vary depending on the branch depth.\n\nThere are two support functions that participate in insertion of values into an ��-�i�� index. As the tree is being traversed from the root node, the choose func- tion takes one of the following decisions: send the new value into an existing child node, create a new child node for this value, or split the current node (if the value does not match this node’s prefix). If the chosen leaf page does not have enough space, the picksplit function determines which nodes should be moved to a new page.\n\nNow I will provide some examples to illustrate these algorithms.\n\n27.2 Quadtrees for Points\n\nQuadtrees are used for indexing points on a two-dimensional plane. The plane is recursively split into four regions (quadrants) with respect to the selected point.\n\n1 backend/access/spgist/spgscan.c, spgWalk function\n\n541",
      "content_length": 1974,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 543,
      "content": "Chapter 27 SP-GiST\n\nThis point is called a centroid; it serves as the node prefix, that is, the condition that defines the location of child values.\n\nThe root node splits the plane into four quadrants.\n\nThen each of them is further split into its own quadrants.\n\n542",
      "content_length": 266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 544,
      "content": "27.2 Quadtrees for Points\n\nThis procedure goes on until the desired number of partitions is reached.\n\nThis example uses an index built on an extended airports table . The illustrations show that branch depth depends on point density in the corresponding quadrants. Forvisualclarity,Isetasmallvalueofthefillfactor storageparameter,whichmakes the tree deeper:\n\n=> CREATE INDEX airports_quad_idx ON airports_big USING spgist(coordinates) WITH (fillfactor = 10);\n\nThe default operator class for points is quad_point_ops.\n\nOperator Class\n\nI have already mentioned ��-�i�� support functions:1 the consistency function for search and the picksplit function for insertions.\n\nNow let’s take a look at the list of support functions of the quad_point_ops operator class.2 All of them are mandatory.\n\n1 postgresql.org/docs/14/spgist-extensibility.html 2 backend/access/spgist/spgquadtreeproc.c\n\n543\n\np. ���\n\n80",
      "content_length": 898,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 545,
      "content": "p. ���\n\nChapter 27 SP-GiST\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'spgist' AND opcname = 'quad_point_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n1 | spg_quad_config 2 | spg_quad_choose 3 | spg_quad_picksplit 4 | spg_quad_inner_consistent 5 | spg_quad_leaf_consistent\n\n(5 rows)\n\nThese functions perform the following tasks:\n\n� The config function reports basic information about the operator class to the\n\naccess method.\n\n� The choose function select the node for insertions.\n\n� The picksplit function distributes nodes between pages after a page split.\n\n� The inner_consistent function checks whether the value of the inner node sat-\n\nisfies the search predicate.\n\n� The leaf_consistent function determines whether the value stored in the leaf\n\nnode satisfies the search predicate.\n\nThere are also several optional functions.\n\nThe quad_point_ops operator class supports the same strategies as �i��:\n\n1\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'spgist' AND opcname = 'quad_point_ops' ORDER BY amopstrategy;\n\n1 include/access/stratnum.h\n\n544",
      "content_length": 1385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 546,
      "content": "27.2 Quadtrees for Points\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n| <<(point,point) | >>(point,point) | ~=(point,point) | <@(point,box) | <<|(point,point) | point_below |>>(point,point) | point_above | <−>(point,point) | point_distance | | <^(point,point) | >^(point,point)\n\n| point_left | point_right | point_eq | on_pb\n\n| point_below | point_above\n\n1 5 6 8 10 11 15 29 30\n\n(9 rows)\n\nFor example, you can use the above operator >^ to find the airports located to the North of Dikson:\n\n=> SELECT airport_code, airport_name->>'en' FROM airports_big WHERE coordinates >^ '(80.3817,73.5167)'::point;\n\nairport_code |\n\n?column?\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−− | Thule Air Base | Eureka Airport | Alert Airport | Resolute Bay Airport | Svalbard Airport, Longyear | Qaanaaq Airport | Grise Fiord Airport | Dikson Airport\n\nTHU YEU YLT YRB LYR NAQ YGZ DKS\n\n(8 rows)\n\n=> EXPLAIN (costs off) SELECT airport_code FROM airports_big WHERE coordinates >^ '(80.3817,73.5167)'::point;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on airports_big\n\nRecheck Cond: (coordinates >^ '(80.3817,73.5167)'::point) −> Bitmap Index Scan on airports_quad_idx\n\nIndex Cond: (coordinates >^ '(80.3817,73.5167)'::point)\n\n(4 rows)\n\nLet’s take a closer look at the structure and inner workings of a quadtree. We will use the same simple example with several points that we discussed in the chapter related to �i��.\n\n545\n\np. ���",
      "content_length": 1504,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 547,
      "content": "Chapter 27 SP-GiST\n\nHere is how the plane can be partitioned in this case:\n\n9\n\n9\n\n8\n\n8\n\n7\n\n��\n\n�\n\n7\n\n6 5\n\n6 5\n\n4 3\n\n2\n\n���\n\n��\n\n4 3\n\n2\n\n1 0\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nThe left illustration shows quadrant numbering at one of the tree levels; in the il- lustrations that follow, I will place child nodes from left to right in the same order for the sake of clarity. Points that lie on the boundaries are included into the quad- rant with the smaller number. The right illustration shows the final partitioning.\n\nYou can see a possible structure of this index below. Each inner node references fourchildnodesatthemost,andeachofthesepointersislabeledwiththequadrant number:\n\n5,5\n\n�\n\n���\n\n��\n\n7,7\n\n5,3\n\n3,2\n\n�\n\n���\n\n�\n\n��\n\n��\n\n�� ���\n\n8,9\n\n8,5\n\n6,6\n\n3,3\n\n3,1\n\n0,0\n\n9,7\n\n1,2\n\n546\n\n8\n\n0,4\n\n9",
      "content_length": 817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 548,
      "content": "27.2 Quadtrees for Points\n\nPage Layout\n\nUnlike �-tree and �i�� indexes, ��-�i�� has no one-to-one correspondence be- tween tree nodes and pages. Since inner nodes usually do not have too many children, several nodes have to be packed into a single page. Different types of nodes are stored in different pages: inner nodes are stored in inner pages, while leaf nodes go to leaf pages.\n\nIndex entries stored in inner pages hold the value used as a prefix, as well as a set of pointers to child nodes; each pointer may be accompanied by a label.\n\nLeaf page entries consist of a value and a ���.\n\nAll leaf nodes related to a particular inner node are stored together in a single page and are bound into a list. If the page cannot accommodate another node, this list can be moved to a different page,1 or the page can be split; one way or the other, a list never stretches over several pages.\n\nTo save space,the algorithm tries to add new nodes into the same pages until these pages are completely filled. The numbers of the last pages used are cached by back- ends and are periodically saved in the zero page, which is called a metapage. The metapage contains no reference to the root node, which we would have seen in a �-tree; the root of an ��-�i�� index is always located in the first page.\n\nUnfortunately, the pageinspect extension does not provide any functions for exploring ��-�i��,but we can use an external extension called gevel.2 It was attempted to integrate its functionality into pageinspect, but with no success.3\n\nLet’s get back to our example. The illustration below shows how tree nodes can be distributed between pages. The quad_point_ops operator class does not actually use labels. Since a node can have four child nodes at the most, the index keeps a fixed-size array of four pointers, some of which may be empty.\n\n1 backend/access/spgist/spgdoinsert.c, moveLeafs function 2 sigaev.ru/git/gitweb.cgi?p=gevel.git 3 commitfest.postgresql.org/15/1207\n\n547",
      "content_length": 1971,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 549,
      "content": "Chapter 27 SP-GiST\n\nroot page\n\n5,55,5\n\ninner pages\n\n7,7 7,7\n\n3,2 3,2\n\nleaf pages\n\n8,9 8,9\n\n9,7 9,7\n\n8,5 8,5\n\n6,6 6,6\n\n5,3 5,3\n\n3,3 3,3\n\n3,1 3,1\n\n0,0 0,0\n\n1,2 1,2\n\n0,40,4\n\nSearch\n\nLet’s use the same example to take a look at the algorithm of searching for points located above point (�,�).\n\n9\n\n8\n\n7\n\n6 5\n\n4 3\n\n2\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nThe search starts at the root. The inner consistency function1 determines the child nodes to be descended into. Point (�,�) is compared with the root node’s centroid\n\n1 backend/access/spgist/spgquadtreeproc.c, spg_quad_inner_consistent function\n\n548",
      "content_length": 596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 550,
      "content": "27.2 Quadtrees for Points\n\n(�,�) to choose the quadrants that may contain the sought-after points; in this ex- ample, these are quadrants � and ��.\n\nOnce inside the node with centroid (�,�), we again have to choose the child nodes to descend into. They belong to quadrants � and ��, but since quadrant �� is empty, we only need to check one leaf node. The leaf consistency function1 compares the points of this node with point (�,�) specified in the query. The above condition is satisfied only for (�,�).\n\nNow we just have to go back one level and check the node that corresponds to quadrant �� of the root node. It is empty, so the search is complete.\n\n5,5\n\n�\n\n���\n\n��\n\n7,7\n\n5,3\n\n3,2\n\n�\n\n���\n\n�\n\n��\n\n��\n\n�� ���\n\n8,9\n\n8,5\n\n6,6\n\n3,3\n\n3,1\n\n0,0\n\n0,4\n\n9,7\n\n1,2\n\nInsertion\n\nWhen a value gets inserted into an ��-�i�� tree,2 each action that follows is deter- mined by the choice function.3 In this particular case, it simply directs the point to one of the existing nodes that corresponds to its quadrant.\n\nFor example, let’s add value (�,�):\n\n1 backend/access/spgist/spgquadtreeproc.c, spg_quad_leaf_consistent function 2 backend/access/spgist/spgdoinsert.c, spgdoinsert function 3 backend/access/spgist/spgquadtreeproc.c, spg_quad_choose function\n\n549",
      "content_length": 1249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 551,
      "content": "Chapter 27 SP-GiST\n\n9\n\n8\n\n7\n\n6 5\n\n4 3\n\n2\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nThe value belongs to quadrant �� and will be added to the corresponding tree node:\n\n5,5\n\n�\n\n���\n\n��\n\n7,7\n\n5,3\n\n3,2\n\n�\n\n���\n\n7,1\n\n�\n\n��\n\n��\n\n�� ���\n\n8,9\n\n8,5\n\n6,6\n\n3,3\n\n3,1\n\n0,0\n\n9,7\n\n1,2\n\nIf the list of leaf nodes in the selected quadrant becomes too big after insertion (it must fit a single page), the page is split. The picksplit function1 determines the new centroid by calculating the average value of all points’ coordinates, thus distributing the child nodes between new quadrants more or less uniformly.\n\n1 backend/access/spgist/spgquadtreeproc.c, spg_quad_picksplit function\n\n550\n\n0,4",
      "content_length": 670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 552,
      "content": "27.2 Quadtrees for Points\n\nThe following picture illustrates the page overflow caused by point (�,�) insertion:\n\n9\n\n9\n\n8\n\n8\n\n7\n\n7\n\n6 5\n\n6 5\n\n4 3\n\n4 3\n\n2\n\n2\n\n1 0\n\n1 0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nA new inner node with centroid (�,�) is added into the tree, while points (�,�), (�,�), and (�,�) get redistributed between the new quadrants:\n\n5,5\n\n�\n\n���\n\n��\n\n7,7\n\n5,3\n\n3,2\n\n�\n\n���\n\n7,1\n\n�\n\n��\n\n��\n\n�� ���\n\n8,9\n\n8,5\n\n6,6\n\n3,3\n\n3,1\n\n1,1\n\n0,4\n\n9,7\n\n�\n\n���\n\n1,2\n\n0,0\n\n2,1\n\n551",
      "content_length": 501,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 553,
      "content": "v. ��\n\nChapter 27 SP-GiST\n\nProperties\n\nAccess method properties. The spgist method reports the following properties:\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'spgist';\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | f spgist | can_order spgist | can_unique | f spgist | can_multi_col | f | t spgist | can_exclude | t spgist | can_include\n\n(5 rows)\n\nNo support is provided for sorting and uniqueness properties. Multicolumn in- dexes are not supported either.\n\nExclusion constraints are supported, just like in �i��.\n\nAn ��-�i�� index can\n\nbe created with additional ������� columns.\n\nIndex-level properties. Unlike�i��, ��-�i��indexesdonotsupportclusterization:\n\n=> FROM unnest(array[\n\nSELECT p.name, pg_index_has_property('airports_quad_idx', p.name)\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| f clusterable | t index_scan bitmap_scan | t backward_scan | f\n\n(4 rows)\n\nBoth ways of getting ���s (either one by one or as a bitmap) are supported. Back- ward scanning is unavailable, as it does not make any sense for ��-�i��.\n\n552",
      "content_length": 1341,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 554,
      "content": "27.2 Quadtrees for Points\n\nColumn-level properties. For the most part,column-level properties are the same:\n\n=> SELECT p.name,\n\npg_index_column_has_property('airports_quad_idx', 1, p.name)\n\nFROM unnest(array[\n\n'orderable', 'search_array', 'search_nulls'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\norderable | f search_array | f search_nulls | t\n\n(3 rows)\n\nSorting is not supported, so all the related properties do not make any sense and are disabled.\n\nI have not said anything about ���� values so far, but as we can see in the index properties, they are supported. Unlike �i��, ��-�i�� indexes do not store ���� values in the main tree. Instead,a separate tree is created; its root is located in the second index page. Thus, the first three pages always have the same meaning: the metapage, the root of the main tree, and the root of the tree for ���� values.\n\nSome column-level properties may depend on the particular operator class:\n\n=> SELECT p.name,\n\npg_index_column_has_property('airports_quad_idx', 1, p.name)\n\nFROM unnest(array[\n\n'returnable', 'distance_orderable'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nreturnable | t distance_orderable | t\n\n(2 rows)\n\nLike in all the other examples in this chapter,this index can be used for index-only scans.\n\nBut in general,an operator class does not necessarily store full values in leaf pages, as it can recheck them by the table instead. It allows using ��-�i�� indexes in Post��� for potentially large geometry values, to give one example.\n\n553\n\nv. ��",
      "content_length": 1623,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 555,
      "content": "v. ��\n\nChapter 27 SP-GiST\n\nNearest neighbor search is supported the operator class.\n\n; we have seen the ordering operator <-> in\n\n27.3 K-Dimensional Trees for Points\n\nPoints on a plane can also be indexed using another approach to partitioning: we can split the plane into two sub-regions instead of four. Such partitioning is im- plemented by the kd_point_ops1 operator class:\n\n=> CREATE INDEX airports_kd_idx ON airports_big USING spgist(coordinates kd_point_ops);\n\nNote that indexed values, prefixes, and labels may have different data types. For this operator class, values are represented as points, prefixes are real numbers, while labels are not provided (as in quad_point_ops).\n\nLet’s select some coordinate on the Y-axis (it defines the latitude in the example with airports). This coordinate splits the plane into two sub-regions,the upper and the lower one:\n\n1 backend/access/spgist/spgkdtreeproc.c\n\n554",
      "content_length": 914,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 556,
      "content": "27.3 K-Dimensional Trees for Points\n\nFor each of these sub-regions, select coordinates on the X-axis (longitude) that split them into two sub-regions, left and right:\n\nWe will continue splitting each of the resulting sub-regions, taking turns between horizontal and vertical partitioning, until the points in each part fit a single index page:\n\nAll inner leaf nodes of the tree built this way will have only two child nodes. The methodcanbeeasilygeneralizedforspacewitharbitrarydimensions,sosuchtrees are often referred to as k-dimensional (k-� trees).\n\n555",
      "content_length": 557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 557,
      "content": "Chapter 27 SP-GiST\n\n27.4 Radix Trees for Strings\n\nThe text_ops operator class for ��-�i�� implements a radix tree for strings.1 Here the prefix of an inner node is really a prefix, which is common to all the strings in the child nodes.\n\nPointers to child nodes are marked by the first byte of the values that follow the prefix.\n\nFor clarity,I use a single character to denote a prefix,but it is true onlyfor �-byte encodings. Ingeneral,theoperatorclassprocessesastringasasequenceofbytes. Besides,aprefixcan take several other values with special semantics,so there are actually two bytes allocated per prefix.\n\nChild nodes store parts of values that follow the prefix and the label. Leaf nodes keep only suffixes.\n\nHere is an example of a radix tree built over several names:\n\nV\n\nA\n\nL\n\nADI\n\nD\n\nS\n\nM\n\nS\n\nL\n\nIM\n\nE\n\nILISA\n\nIR\n\nN\n\nR\n\nTIN\n\nIY\n\nTINA\n\n1 backend/access/spgist/spgtextproc.c\n\n556\n\nLAV",
      "content_length": 892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 558,
      "content": "27.4 Radix Trees for Strings\n\nTo reconstruct the full value of an index key in a leaf page, we can concatenate all prefixes and labels, starting from the root node.\n\nOperator Class\n\nThe text_ops operator class supports comparison operators typically used with or- dinal data types, including text strings:\n\n=> SELECT oprname, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'spgist' AND opcname = 'text_ops' ORDER BY amopstrategy;\n\noprname |\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−+−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n~<~ ~<=~ = ~>=~ ~>~ < <= >= > ^@\n\n| text_pattern_lt | | text_pattern_le | | texteq | | text_pattern_ge | | text_pattern_gt | | | text_lt | | text_le | | text_ge | | text_gt | | starts_with\n\n1 2 3 4 5 11 12 14 15 28\n\n(10 rows)\n\nRegular operators process characters, while operators with tildes deal with bytes. Theydonottakecollationintoaccount(justlikethetext_pattern_ops operatorclass for �-tree), so they can be used to speed up search by the ���� condition:\n\n=> CREATE INDEX tickets_spgist_idx ON tickets\n\nUSING spgist(passenger_name);\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE passenger_name LIKE 'IVAN%';\n\n557\n\np. ���",
      "content_length": 1286,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 559,
      "content": "v. ��\n\nChapter 27 SP-GiST\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on tickets\n\nFilter: (passenger_name ~~ 'IVAN%'::text) −> Bitmap Index Scan on tickets_spgist_idx\n\nIndex Cond: ((passenger_name ~>=~ 'IVAN'::text) AND (passenger_name ~<~ 'IVAO'::text))\n\n(5 rows)\n\nIf you use regular operators >= and < together with a collation other than “C,” the index becomes virtually useless, as it deals with bytes rather than characters.\n\nFor such cases of is more suitable:\n\nprefix search, the operator class provides the ^@ operator, which\n\n=> EXPLAIN (costs off) SELECT * FROM tickets WHERE passenger_name ^@ 'IVAN';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on tickets\n\nRecheck Cond: (passenger_name ^@ 'IVAN'::text) −> Bitmap Index Scan on tickets_spgist_idx\n\nIndex Cond: (passenger_name ^@ 'IVAN'::text)\n\n(4 rows)\n\nAradix tree representation can sometimes turn out to be much more compact than a �-tree, as it does not keep full values: it reconstructs them as required while the tree is being traversed.\n\nSearch\n\nLet’s run the following query on the names table:\n\nSELECT * FROM names WHERE name ~>=~ 'VALERIY'\n\nAND name ~<~ 'VLADISLAV';\n\n558",
      "content_length": 1227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 560,
      "content": "27.4 Radix Trees for Strings\n\nFirst, the inner consistency function1 is called on the root to determine the child nodes to descend into. This function concatenates prefix � and labels � and �. The received value �� goes into the query condition; string literals are truncated there, so that their length does not exceed the length of the value being checked: �� ~>=~ '��' ��� �� ~<~ '��'. The condition is satisfied, so the child node with label � needs to be checked. The �� value is checked in the same way. It is also a match, so the node with label � must be checked too.\n\nNow let’s take the node that corresponds to value ��. Its prefix is empty, so for the three child nodes the inner consistency function reconstructs values ���, ���, and ��� by concatenating �� received at the previous step and the label. The condition ��� ~>=~ '���'��� ��� ~<~ '���' is not true, but the other two values are suitable.\n\nAs the tree is being traversed this way, the algorithm filters out non-matching branchesandgetstoleafnodes. Theleafconsistencyfunction2 checkswhetherthe value reconstructed during the tree traversal satisfies the query condition. Match- ing values are returned as the result of an index scan.\n\nV\n\nA\n\nL\n\nADI\n\nD L\n\nS\n\nM\n\nS\n\nIM\n\nE\n\nILISA\n\nIR\n\nLAV\n\nN\n\nR\n\nTIN\n\nIY\n\nTINA\n\n1 backend/access/spgist/spgtextproc.c, spg_text_inner_consistent function 2 backend/access/spgist/spgtextproc.c, spg_text_leaf_consistent function\n\n559",
      "content_length": 1431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 561,
      "content": "Chapter 27 SP-GiST\n\nNote that although the query uses greater than and less than operators, which are common to �-trees, range search by ��-�i�� is much less efficient. In a �-tree, it is enough to descend into a single boundary value of the range and then scan the list of leaf pages.\n\nInsertion\n\nThe choice function of operator classes for points can always direct a new value into one of the existing sub-regions (a quadrant or one of the halves). But it is not true for radix trees: a new value may not match any of the existing prefixes, and the inner node has to be split in this case.\n\nLet’s add the name ����� to an already built tree.\n\nThe choice function1 manages to descend from the root to the next node (� + �), but the remaining part of the value ��� does not match the ��� prefix. The node has to be split in two: one of the resulting nodes will contain the common part of the prefix (��), while the rest of the prefix will be moved one level down:\n\nADI\n\nAD\n\nM\n\nS\n\nI\n\nIR\n\nLAV\n\nM\n\nS\n\nIR\n\nLAV\n\nThen the choice function is called again on the same node. The prefix now cor- responds to the value, but there is no child node with a suitable label (�), so the function decides to create such a node. The final result is shown in the illustra- tion below; the nodes that have been added or modified during the insertion are highlighted.\n\n1 backend/access/spgist/spgtextproc.c, spg_text_choose function\n\n560",
      "content_length": 1415,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 562,
      "content": "27.4 Radix Trees for Strings\n\nV\n\nA\n\nL\n\nAD\n\nD L\n\nS\n\nA\n\nI\n\nIM\n\nE\n\nILISA\n\nN\n\nR\n\nM\n\nS\n\nTIN\n\nIY\n\nIR\n\nLAV\n\nTINA\n\nProperties\n\nI have already described the access method and index-level properties above; they are common to all the classes. Most of the column-level properties also remain the same.\n\n=> SELECT p.name,\n\npg_index_column_has_property('tickets_spgist_idx', 1, p.name)\n\nFROM unnest(array[\n\n'returnable', 'distance_orderable'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nreturnable | t distance_orderable | f\n\n(2 rows)\n\nEven though indexed values are not explicitly stored in the tree, index-only scans are supported, since values are reconstructed as the tree is being traversed from the root to leaf nodes.\n\n561",
      "content_length": 775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 563,
      "content": "v. ��\n\nChapter 27 SP-GiST\n\nAs for the distance operator,it is not defined for strings,so nearest neighbor search is not provided by this operator class.\n\nIt does not mean that the concept of distance cannot be implemented for strings. For example, the pg_trgm extension adds a distance operator based on trigrams: the fewer common trigrams are found in two strings, the farther they are assumed to be located from each other. Then there is the Levenshtein distance, which is defined as the minimal number of single-character edits required to convert one string into another. A function that calculates such a distance is provided in the fuzzystrmatch extension. But none of the extensions provides an operator class with ��-�i�� support.\n\n27.5 Other Data Types\n\nS�-�i�� operator classes are not limited to indexing points and text strings that we have discussed above.\n\nGeometric types. The box_ops1 operator class implements a quadtree for rectan- gles. Rectangles are represented by points in a four-dimensional space,so the area is split into sixteen partitions.\n\nThepoly_ops classcanbeusedtoindexpolygons. Itisafuzzyoperatorclass: it actually uses bounding boxes instead of polygons, just like box_ops, and then rechecks the result by the table.\n\nWhether to choose �i�� or ��-�i�� largely depends on the nature of data to be indexed. For example, Post��� documentation recommends ��-�i�� for ob- jects with large overlaps (also known as “spaghetti data”).2\n\nRange types. The quadtree for ranges offers the range_ops operator class.3 An in- terval is defined by a two-dimensional point: the X-axis represents the lower boundary, while the Y-axis represents the upper boundary.\n\nNetwork address types. For the inet data type, the inet_ops4 operator class imple-\n\nments a radix tree.\n\n1 backend/utils/adt/geo_spgist.c 2 postgis.net/docs/using_postgis_dbmanagement.html#spgist_indexes 3 backend/utils/adt/rangetypes_spgist.c 4 backend/utils/adt/network_spgist.c\n\n562",
      "content_length": 1967,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 564,
      "content": "28\n\nGIN\n\n28.1 Overview\n\nAccording to its authors, ��� stands for a potent and undaunted spirit, not for an alcoholic beverage.1 But there is also a formal interpretation: this acronym is ex- panded as Generalized Inverted Index.\n\nThe ��� access method is designed for data types representing non-atomic values made up of separate elements (for example, documents consist of lexemes in the context of full-text search). Unlike �i��, which indexes values as a whole, ��� in- dexes only their elements; each element is mapped to all the values that contain it.\n\nWe can compare this method to a book’s index, which comprises all the important terms and lists all the pages where these terms are mentioned. To be convenient to use, it must be compiled in alphabetical order,otherwise it would be impossible to navigate through quickly. In a similar way, ��� relies on the fact that all elements of compound values can be sorted; its main data structure is �-tree.\n\nThe implementation of the ��� tree of elements is less complex than that of a reg- ular �-tree: it has been designed to contain rather small sets of elements repeated multiple times.\n\nThis assumption leads to two important conclusions:\n\nAn element must be stored in an index only once.\n\nEachelementismappedtoalistof���s,whichiscalledapostinglist. Ifthislist is rather short, it is stored together with the element; longer lists are moved\n\n1 postgresql.org/docs/14/gin.html backend/access/gin/README\n\n563\n\np. ���",
      "content_length": 1471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 565,
      "content": "p. ���\n\np. ���\n\nChapter 28 GIN\n\ninto a separate posting tree, which is actually a �-tree. Just like element trees, posting lists are sorted; it does not matter much from the user’s perspective but helps to speed up data access and reduce index size.\n\nThere is no point in removing elements from a tree.\n\nEven if the list of ���s for a particular element is empty, the same element is likely to appear again as part of some other value.\n\nThus,an index is a tree of elements whose leaf entries are bound to either flat lists or trees of ���s.\n\nJust like �i�� and ��-�i�� access methods, ��� can be used to index a whole vari- ety of data types via a simplified interface of operator classes. Operators of such classes usually check whether the indexed composite value matches a particular set of elements (just like the @@ operator checks whether a document satisfies a full-text search query).\n\nTo index a particular data type, the ��� method must be able to split composite values into elements, sort these elements, and check whether the found value sat- isfies the query. These operations are implemented by support functions of the operator class.\n\n28.2 Index for Full-Text Search\n\nG�� is mainly applied to speed up full-text search, so I will go on with the example used to illustrate �i�� indexing. As you can guess, compound values in this case are documents, while elements of these values are lexemes.\n\nLet’s build a ��� index on the “Old MacDonald” table:\n\n=> CREATE INDEX ts_gin_idx ON ts USING gin(doc_tsv);\n\nA possible structure of this index is shown below. Unlike in the previous illustra- tions, here I provide actual ��� values (shown with a grey background), as they are very important for understanding the algorithms. These values suggest that heap tuples have the following ��s:\n\n564",
      "content_length": 1803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 566,
      "content": "28.2 Index for Full-Text Search\n\n=> SELECT ctid, * FROM ts;\n\nctid |\n\ndoc\n\n|\n\ndoc_tsv\n\n−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n(0,1) | Old MacDonald had a farm (0,2) | And on his farm he had some cows (0,3) | Here a moo, there a moo (0,4) | Everywhere a moo moo (1,1) | Old MacDonald had a farm (1,2) | And on his farm he had some chicks | 'chick':8 'farm':4 (1,3) | Here a cluck, there a cluck (1,4) | Everywhere a cluck cluck (2,1) | Old MacDonald had a farm (2,2) | And on his farm he had some pigs (2,3) | Here an oink, there an oink (2,4) | Everywhere an oink oink\n\n| 'farm':5 'macdonald':2 'old':1 | 'cow':8 'farm':4 | 'moo':3,6 | 'everywher':1 'moo':3,4 | 'farm':5 'macdonald':2 'old':1\n\n| 'cluck':3,6 | 'cluck':3,4 'everywher':1 | 'farm':5 'macdonald':2 'old':1 | 'farm':4 'pig':8 | 'oink':3,6 | 'everywher':1 'oink':3,4\n\n(12 rows)\n\nmetapage\n\neverywher everywher\n\noink oink\n\nchick\n\ncluck\n\ncow\n\neverywher\n\nfarm macdonald moo\n\noink\n\nold\n\npig\n\n1,2\n\n1,3\n\n0,2\n\n0,4\n\n0,1\n\n0,3\n\n2,3\n\n0,1\n\n2,2\n\n1,4\n\n1,4\n\n1,1\n\n0,4\n\n2,4\n\n1,1\n\n2,4\n\n2,1\n\n2,1\n\n1,2\n\n0,1\n\n0,2\n\n1,1\n\n1,2\n\n2,1\n\n2,2\n\n565",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 567,
      "content": "p. ���\n\np. ���\n\np. ��\n\nChapter 28 GIN\n\nNote some differences from a regular �-tree index here. The leftmost keys in inner �-tree nodes are empty,as they are actually redundant; in a ��� index,they are not storedatall. Forthisreason,referencestochildnodesareshiftedtoo. Thehighkey is used in both indexes, but in ��� it takes its legitimate rightmost position. Same- level nodes in a �-tree are bound into a bidirectional list; ��� uses a unidirectional list, since the tree is always traversed in only one direction.\n\nIn this theoretical example,all posting lists fit regular pages,except the one for the “farm” lexeme. This lexeme occurred in as many as six documents, so its ��s were moved into a separate posting tree.\n\nPage Layout\n\nG�� page layout is very similar to that of a �-tree. We can peek into an index using the pageinspect extension. Let’s create a ��� index on the table that stores emails of the pgsql-hackers\n\nmailing list:\n\n=> CREATE INDEX mail_gin_idx ON mail_messages USING gin(tsv);\n\nThe zero page (the metapage) contains the basic statistics, such as the number of elements and pages of other types:\n\n=> SELECT * FROM gin_metapage_info(get_raw_page('mail_gin_idx',0)) \\gx\n\n−[ RECORD 1 ]−−−−+−−−−−−−−−−− | 4294967295 pending_head | 4294967295 pending_tail | 0 tail_free_size n_pending_pages | 0 n_pending_tuples | 0 n_total_pages n_entry_pages n_data_pages n_entries version\n\n| 22957 | 13522 | 9434 | 999109 | 2\n\nG�� uses the special space that define the page type:\n\nof index pages; for example, this space stores the bits\n\n566",
      "content_length": 1549,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 568,
      "content": "28.2 Index for Full-Text Search\n\n=> SELECT flags, count(*) FROM generate_series(0,22956) AS p, -- n_total_pages\n\ngin_page_opaque_info(get_raw_page('mail_gin_idx',p))\n\nGROUP BY flags ORDER BY 2;\n\nflags\n\n| count\n\n−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−\n\n| {meta} | {} {data} | {data,leaf,compressed} | {leaf} (5 rows)\n\n1 137 1525 7909 | 13385\n\nThe page with the meta attribute is of course the metapage. Pages with the data attribute belong to posting lists, while pages without this attribute are related to element trees. Leaf pages have the leaf attribute.\n\nIn the next example,another pageinspect function returns the information on ���s that are stored in trees’leaf pages. Each entry of such a tree is virtually a small list of ���s rather than a single ���:\n\n=> SELECT left(tids::text,60)||'...' tids FROM gin_leafpage_items(get_raw_page('mail_gin_idx',24));\n\ntids −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− {\"(4771,4)\",\"(4775,2)\",\"(4775,5)\",\"(4777,4)\",\"(4779,1)\",\"(47... {\"(5004,2)\",\"(5011,2)\",\"(5013,1)\",\"(5013,2)\",\"(5013,3)\",\"(50... {\"(5435,6)\",\"(5438,3)\",\"(5439,3)\",\"(5439,4)\",\"(5439,5)\",\"(54... ... {\"(9789,4)\",\"(9791,6)\",\"(9792,4)\",\"(9794,4)\",\"(9794,5)\",\"(97... {\"(9937,4)\",\"(9937,6)\",\"(9938,4)\",\"(9939,1)\",\"(9939,5)\",\"(99... {\"(10116,5)\",\"(10118,1)\",\"(10118,4)\",\"(10119,2)\",\"(10121,2)\"...\n\n(27 rows)\n\nPosting lists are ordered, which allows them to be compressed (hence the same- name attribute). Instead of a six-byte ���, they store its difference with the pre- vious value, which is represented by a variable number of bytes:1 the smaller this difference, the less space the data takes.\n\n1 backend/access/gin/ginpostinglist.c\n\n567",
      "content_length": 1661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 569,
      "content": "Chapter 28 GIN\n\nOperator Class\n\nHere is the list of support functions for ��� operator classes:1\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'gin' AND opcname = 'tsvector_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n1 | gin_cmp_tslexeme 2 | pg_catalog.gin_extract_tsvector 3 | pg_catalog.gin_extract_tsquery 4 | pg_catalog.gin_tsquery_consistent 5 | gin_cmp_prefix 6 | gin_tsquery_triconsistent\n\n(6 rows)\n\nThe first support function compares two elements (two lexemes in this case). If the lexemes were represented by a regular ��� type supported by �-tree, ��� would automatically use comparison operators defined in the �-tree operator class.\n\nThe fifth (optional) function is used in partial search to check whether an index element partially matches the search key. In this particular case, partial search consists in searching lexemesbya prefix. For example,the query“c:*”corresponds to all lexemes starting with letter “c.”\n\nThe second function extracts lexemes from the document, while the third one ex- tracts lexemes from the search query. The use of different functions is justified because, at the very least, the document and the query are represented by differ- ent data types, namely tsvector and tsquery. Besides, the function for the search query determines how the search will be performed. If the query requires the doc- ument to contain a particular lexeme, the search will be limited to the documents that contain at least one lexeme specified in the query. If there is no such condi- tion (for example, if you need documents that do not contain a particular lexeme), all the documents have to be scanned—which is of course much more expensive.\n\n1 postgresql.org/docs/14/gin-extensibility.html\n\nbackend/utils/adt/tsginidx.c\n\n568",
      "content_length": 1914,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 570,
      "content": "28.2 Index for Full-Text Search\n\nIf the query contains any other search keys, the index is first scanned by these keys, and then these intermediate results are rechecked. Thus,there is no need to scan the index in full.\n\nThe fourth and sixth functions are consistency functions, which determine whether the found document satisfies the search query. As input, the fourth func- tion gets the exact information on which lexemes specified in the query appear in the document. The sixth function operates in the context of uncertainty and can be called when it is not yet clear whether some of the lexemes are present in the document or not. An operator class does not have to implement both functions: it is enough to provide only one of them, but search efficiency may suffer in this case.\n\nThe tsvector_ops operator class supports only one operator that matches the docu- mentagainstthesearchquery: @@,1 whichisalsoincludedintothe�i��operator class.\n\nSearch\n\nLet’s take a look at the search algorithm for the “everywhere | oink” query, where two lexemes are connected by the �� operator. First, a support function2 extracts lexemes “everywher” and “oink” (search keys) from the search string of the tsquery type.\n\nSince the query demands particular lexemes to be present, ���s of the documents that contain at least one key specified in the query are bound into a list. For this purpose, the ���s that correspond to each search key are searched in the tree of lexemes and are added into a common list. All the ���s stored in an index are ordered, which allows merging\n\nseveral sorted streams of ���s into one.3\n\nNote that at this point it does not matter yet whether the keys were combined by ���, ��, or any other operator: the search engine deals with the list of keys and knows nothing about the search query semantics.\n\n1 backend/utils/adt/tsvector_op.c, ts_match_vq function 2 backend/utils/adt/tsginidx.c, gin_extract_tsquery function 3 backend/access/gin/ginget.c, keyGetItem function\n\n569\n\nv. ��\n\np. ���",
      "content_length": 2010,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 571,
      "content": "Chapter 28 GIN\n\neverywher everywher\n\noink oink\n\nchick\n\ncluck\n\ncow\n\neverywher\n\nfarm macdonald moo\n\noink\n\nold\n\n1,2\n\n1,3\n\n0,2\n\n0,4\n\n0,1\n\n0,3\n\n2,3\n\n0,1\n\n1,4\n\n1,4\n\n1,1\n\n0,4\n\n2,4\n\n1,1\n\n2,4\n\n2,1\n\n2,1\n\n1,2\n\n0,1\n\n0,2\n\n1,1\n\n1,2\n\n2,1\n\n2,2\n\nEach found ��� that corresponds to a document is checked by the consistency func- tion.1 It is this function that interprets the search query and leaves only those ���s that satisfy the query (or at least may satisfy it and have to be rechecked by the ta- ble).\n\nIn this particular case, the consistency function leaves all the ���s:\n\n���\n\n“everywher”\n\n“oink”\n\nconsistency function\n\n(�,�) (�,�) (�,�) (�,�)\n\n(cid:51) (cid:51)\n\n– (cid:51)\n\n– – (cid:51) (cid:51)\n\n(cid:51) (cid:51) (cid:51) (cid:51)\n\nInstead of a regular lexeme, search queries can contain a prefix. It is useful if an application user can enter the first letters of a word into the search field,expecting\n\n1 backend/utils/adt/tsginidx.c, gin_tsquery_triconsistent function\n\n570\n\npig\n\n2,2",
      "content_length": 982,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 572,
      "content": "28.2 Index for Full-Text Search\n\nto get the results right away. For example, the “pig:*” query will match all the documents that contain lexemes starting with “pig”: here we get “pigs,” and we would also get “pigeons” if old MacDonald had bred them on his farm.\n\nSuch partial search matches indexed lexemes against the search key using a special supportfunction;1 inadditiontoprefixmatching,thisfunctioncanalsoimplement other logic for partial search.\n\nFrequent and Rare Lexemes\n\nIfsearchedlexemesoccurinadocumentmultipletimes,thecreatedlistof���swill turn out long, which is of course inefficient. Fortunately, it can often be avoided if the query also contains some rare lexemes.\n\nLet’sconsiderthe“farm & cluck”query. The“cluck”lexemeoccurstwotimes,while the “farm” lexeme appears six times. Instead of treating both lexemes equally and building the full list of ���s by them, the rare “cluck” lexeme is considered manda- tory,whilethemorefrequent“farm”lexemeistreatedasoptional,asitisclear that (taking the query semantics into account) a document with the “farm” lexeme can satisfy the query only if it contains the “cluck” lexeme too.\n\nThus, an index scan determines the first document that contains “cluck”; its ��� is (�,�). Then we have to find out whether this document also contains the “farm” lexeme, but all the documents whose ���s are smaller than (�,�) can be skipped. Since frequent lexemes are likely to correspond to many ���s, chances are high that they are stored in a separate tree, so some pages can be skipped as well. In this particular case, the search in the tree of “farm” lexemes starts with (�,�).\n\nThis procedure is repeated for the subsequent values of the mandatory lexeme.\n\nClearly, this optimization can also be applied to more complex search scenarios that involve more than two lexemes. The algorithm sorts the lexemes in the or- der of their frequency, adds them one by one to the list of mandatory lexemes, and stops when the remaining lexemes are no longer able to guarantee that the document satisfies the query.2\n\n1 backend/utils/adt/tsginidx.c, gin_cmp_prefix function 2 backend/access/gin/ginget.c, startScanKey function\n\n571",
      "content_length": 2168,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 573,
      "content": "Chapter 28 GIN\n\nFor example, let’s consider the query “farm & ( cluck | chick )”. The least frequent lexeme is“chick”; it is added to the list of mandatory lexemes right away. To check whether other lexemes can be considered optional, the consistency function takes false for the mandatory lexeme and true for all the other lexemes. The function returns true AND (true OR false) = true, which means that the remaining lexemes are “self-sufficient,” and at least one of them must become mandatory.\n\nThe next least frequent lexeme (“cluck”) is added into the list, and now the consis- tency function returns true AND (false OR false) = false. Thus, “chick” and “cluck” lexemes become mandatory, while “farm” remains optional.\n\neverywher everywher\n\noink oink\n\nchick\n\ncluck\n\ncow\n\neverywher\n\nfarm macdonald moo\n\noink\n\nold\n\n1,2\n\n1,3\n\n0,2\n\n0,4\n\n0,1\n\n0,3\n\n2,3\n\n0,1\n\n1,4\n\n1,4\n\n1,1\n\n0,4\n\n2,4\n\n1,1\n\n2,4\n\n2,1\n\n2,1\n\n1,2\n\n0,1\n\n0,2\n\n1,1\n\n1,2\n\n2,1\n\n2,2\n\nThe length of the posting list is three, as the mandatory lexemes have occurred three times:\n\n���\n\n“chick”\n\n“cluck”\n\n“farm”\n\nconsistency function\n\n(�,�) (�,�) (�,�)\n\n(cid:51)\n\n– –\n\n– (cid:51) (cid:51)\n\n(cid:51)\n\n– –\n\n(cid:51)\n\n– –\n\n572\n\npig\n\n2,2",
      "content_length": 1183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 574,
      "content": "28.2 Index for Full-Text Search\n\nThus, if lexeme frequencies are known , it is possible to merge trees of lexemes in the most efficient way, starting from rare lexemes and skipping those page ranges of frequent lexemes that are sure to be redundant. It reduces the number of times the consistency function has to be called.\n\nTo make sure that this optimization really works, let’s query archive. We will need to specify two lexemes, a common and a rare one:\n\nthe pgsql-hackers\n\n=> SELECT word, ndoc FROM ts_stat('SELECT tsv FROM mail_messages') WHERE word IN ('wrote', 'tattoo');\n\n| −−−−−−−−+−−−−−−−−\n\nword\n\nndoc\n\nwrote tattoo |\n\n| 231173 2\n\n(2 rows)\n\nIt turns out that a document that contains them both does exist:\n\n=> \\timing on\n\n=> SELECT count(*) FROM mail_messages WHERE tsv @@ to_tsquery('wrote & tattoo');\n\ncount −−−−−−−\n\n1\n\n(1 row) Time: 0,631 ms\n\nThis query is performed almost just as fast as the search for a single word“tattoo”:\n\n=> SELECT count(*) FROM mail_messages WHERE tsv @@ to_tsquery('tattoo');\n\ncount −−−−−−−\n\n2\n\n(1 row) Time: 2,227 ms\n\nBut if we were looking for a single word “wrote,” the search would take much longer:\n\n=> SELECT count(*) FROM mail_messages WHERE tsv @@ to_tsquery('wrote');\n\n573\n\np. ���\n\np. ���",
      "content_length": 1237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 575,
      "content": "on p. ���\n\nChapter 28 GIN\n\ncount −−−−−−−− 231173 (1 row) Time: 343,556 ms\n\n=> \\timing off\n\nInsertions\n\nA ��� index cannot contain duplicates;1 if an element to be added is already present in the index, its ��� is simply added to the posting list or tree of an al- ready existing element. A posting list is a part of an index entry that cannot take toomuchspaceinapage,soiftheallottedspaceisexceeded,thelististransformed into a tree.2\n\nWhen a new element (or a new ���) is being added into a tree, a page overflow can occur; in this case, the page is split into two, and the elements are redistributed between them.3\n\nBut each document typically contains many lexemes that have to be indexed. So even if we create or modify just one document, the index tree still undergoes a lot of modifications. That is why ��� updates are rather slow.\n\nThe illustration below shows the state of the tree after the row“Everywhere clucks, moos,and oinks”with ��� (�,�) was inserted into the table. The posting lists of lex- emes“cluck,”“moo,”and“oink”were extended; the list of the“everywher”lexeme exceeded the maximal size and was split off as a separate tree.\n\nHowever, if an index gets updated to incorporate changes related to several doc- uments at once, the total amount of work is likely to be reduced as compared to consecutive changes, as these documents may contain some common lexemes.\n\nfastupdate storage parameter. Deferred in- This optimization is controlled by the dex updates are accumulated in an unordered pending list, which is physically stored in separate list pages outside the element tree. When this list becomes\n\n1 backend/access/gin/gininsert.c, ginEntryInsert function 2 backend/access/gin/gininsert.c, addItemPointersToLeafTuple function 3 backend/access/gin/ginbtree.c, ginInsertValue function\n\n574",
      "content_length": 1812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 576,
      "content": "28.2 Index for Full-Text Search\n\nbig enough, all its contents is transferred into the index in one go, and the list is cleared.1 The maximal size of the list is defined either by the gin_pending_list_limit parameter or by the same-name index storage parameter.\n\neverywher everywher\n\noink oink\n\nchick\n\ncluck\n\ncow\n\neverywher\n\nfarm macdonald moo\n\noink\n\nold\n\npig\n\n1,2\n\n1,3\n\n0,2\n\n0,1\n\n0,3\n\n2,3\n\n0,1\n\n2,2\n\n1,4\n\n1,1\n\n0,4\n\n2,4\n\n1,1\n\n4,1\n\n2,1\n\n4,1\n\n4,1\n\n2,1\n\n2,4\n\n1,2\n\n0,4\n\n1,4\n\n2,4\n\n4,1\n\n0,1\n\n0,2\n\n1,1\n\n1,2\n\n2,1\n\n2,2\n\nBy default, such deferred updates are enabled, but you should keep in mind that they slow down search: apart from the tree itself, the whole unordered list of lex- emes has to be scanned. Besides, insertion time becomes less predictable, as any change can lead to an overflow that incurs an expensive merge procedure. The latter is partially smoothed by the fact that the merge can be also performed asyn- chronously during index vacuuming.\n\nWhen a new index is created,2 the elements also get added in batches rather than one by one, which would be too slow. Instead of being saved into an unordered maintenance_work_mem memory list on disk, all the changes are accumulated in a chunk and get transferred into an index once this chunk has no more free space. The more memory is allocated for this operation, the faster the index is built.\n\n1 backend/access/gin/ginfast.c, ginInsertCleanup function 2 backend/access/gin/gininsert.c, ginbuild function\n\n575\n\n4MB\n\n64MB",
      "content_length": 1476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 577,
      "content": "p. ���\n\np. ���\n\n0\n\nChapter 28 GIN\n\nThe examples provided in this chapter prove ��� superiority over �i�� signature trees when it comes to search precision. For this reason, it is ��� that is typically used for full-text search. However, the problem of slow ��� updates may tip the scale in favor of �i�� if the data is being actively updated.\n\nLimiting Result Set Size\n\nThe ��� access method always returns the result as a bitmap; it is impossible to get ���s one by one. In other words,the B����� S��� property is supported,but the I���� S��� property is not.\n\nThe reason for this limitation is the unordered list of deferred updates. In the case of an index access, this list is scanned to build a bitmap, and then this bitmap is updated with the data of the tree. If the unordered list gets merged with the tree (as the result of an index update or during vacuuming) while search is in progress, one and the same value can be returned twice, which is unacceptable. But in the case of a bitmap it does not pose any problems: the same bit will simply be set twice.\n\nConsequently, using the ����� clause with a ��� index is not quite efficient, as the bitmap still has to be built in full, which contributes a fair share to the total cost:\n\n=> EXPLAIN SELECT * FROM mail_messages WHERE tsv @@ to_tsquery('hacker') LIMIT 1000;\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− (cost=481.41..1964.22 rows=1000 width=1258)\n\nLimit\n\n−> Bitmap Heap Scan on mail_messages\n\n(cost=481.41..74939.28 rows=50214 width=1258) Recheck Cond: (tsv @@ to_tsquery('hacker'::text)) −> Bitmap Index Scan on mail_gin_idx\n\n(cost=0.00..468.85 rows=50214 width=0) Index Cond: (tsv @@ to_tsquery('hacker'::text))\n\n(7 rows)\n\nTherefore, the ��� method offers a special feature that limits the number of re- gin_fuzzy_search_limit sults returned by an index scan. This limit is imposed by the parameter, which is turned off by default. If this parameter is enabled, the index\n\n576",
      "content_length": 1973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 578,
      "content": "28.2 Index for Full-Text Search\n\naccess method will randomly skip some values to get roughly the specified number of rows (hence the name “fuzzy”):1\n\n=> SET gin_fuzzy_search_limit = 1000;\n\n=> SELECT count(*) FROM mail_messages WHERE tsv @@ to_tsquery('hacker');\n\ncount −−−−−−−\n\n727\n\n(1 row)\n\n=> SELECT count(*) FROM mail_messages WHERE tsv @@ to_tsquery('hacker');\n\ncount −−−−−−−\n\n791\n\n(1 row)\n\n=> RESET gin_fuzzy_search_limit;\n\nNote that there are no ����� clauses in these queries. It is the only legitimate way to get different data when using an index scan and a heap scan. The planner knows nothing about such behavior of ��� indexes and does not take this parameter value into account when estimating the cost.\n\nProperties\n\nAll the properties of the gin access method are the same at all levels; they do not depend on a particular operator class.\n\nAccess Method Properties\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'gin';\n\n1 backend/access/gin/ginget.c, dropItem macro\n\n577",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 579,
      "content": "Chapter 28 GIN\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | f | can_order | f | can_unique | can_multi_col | t | f | can_exclude | f | can_include\n\ngin gin gin gin gin\n\n(5 rows)\n\nG�� supports neither sorting nor unique constraints.\n\nMulticolumn indexes are supported, but it is worth mentioning that the order of their columns is irrelevant. Unlike a regular �-tree, a multicolumn ��� index does not store composite keys; instead, it extends separate elements with the corre- sponding column number.\n\nExclusion constraints cannot be supported because the I���� S��� property is un- available.\n\nG�� does not support additional ������� columns. Such columns simply do not make much sense here, as it is hardly possible to use a ��� index as covering: it contains only separate elements of an index value, while the value itself is stored in the table.\n\nIndex-Level Properties\n\n=> FROM unnest(array[\n\nSELECT p.name, pg_index_has_property('mail_gin_idx', p.name)\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| f clusterable | f index_scan bitmap_scan | t backward_scan | f\n\n(4 rows)\n\nFetching results one by one is not supported: the index access always returns a bitmap.\n\nFor the same reason,it makes no sense to reorder tables by a ��� index: the bitmap always corresponds to the physical layout of data in a table, whichever it is.\n\n578",
      "content_length": 1489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 580,
      "content": "28.2 Index for Full-Text Search\n\nBackward scanning is not supported: this feature is useful for regular index scans, not for bitmap scans.\n\nColumn-Level Properties\n\n=> SELECT p.name,\n\npg_index_column_has_property('mail_gin_idx', 1, p.name)\n\nFROM unnest(array[\n\n'orderable', 'search_array', 'search_nulls', 'returnable', 'distance_orderable'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n| f orderable | f search_array | f search_nulls returnable | f distance_orderable | f\n\n(5 rows)\n\nNone of the column-level properties are available: neither sorting (for obvious rea- sons) nor using the index as covering (since the document itself is not stored in the index). N��� support is not available either (it does not make sense for elements of non-atomic types).\n\nGIN Limitations and RUM Index\n\nPotent as it is, ��� still cannot address all the challenges of full-text search. Al- though the tsvector type does indicate positions of lexemes, this information does not make it into an index. Therefore, ��� cannot be used to speed up phrase search, which takes lexeme proximity into account. Moreover, search engines usually re- turn results by relevance (whatever this term might mean), and since ��� does not supportorderingoperators,theonlysolutionherewouldbecomputingtheranking function for each resulting row, which is of course very slow.\n\nThese drawbacks have been addressed by the ��� access method (whose name makes us doubt developers’ sincerity when it comes to the true meaning of ���).\n\n579",
      "content_length": 1557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 581,
      "content": "Chapter 28 GIN\n\nThis access method is provided as an extension; you can either download the cor- responding package from the ���� repository1 or get the source code itself.2\n\nR�� is based on ���, but they have two major differences. First, ��� does not provide deferred updates, so it supports regular index scans in addition to bitmap scansandimplementsorderingoperators. Second,���indexkeyscanbeextended with additional information. This feature resembles ������� columns to some ex- tent, but here additional information is bound to a particular key. In the context of full-text search, ��� operator class maps lexeme occurrences to their positions in the document, which speeds up phrase search and result ranking.\n\nThe downsides of this approach are slow updates and larger index sizes. Besides, since the rum access method is provided as an extension, it relies on the generic ��� mechanism,3 which is slower than the built-in logging and generates bigger volumes of ���.\n\n28.3 Trigrams\n\nThe pg_trgm4 extension can assess word similarity by comparing the number of co- inciding three-letter sequences (trigrams). Word similarity can be used alongside full-text search to return some results even if the words to search for have been entered with typos.\n\nThe gin_trgm_ops operator class implements text string indexing. To single out el- ements of text values, it extracts various three-letter substrings rather than words or lexemes (only letters and digits are taken into account; other characters are ignored). Within an index, trigrams are represented as integers. Note that for non-Latin characters, which take from two to four bytes in the ���-� encoding, such representation does not allow decoding the original symbols.\n\n=> CREATE EXTENSION pg_trgm;\n\n=> SELECT unnest(show_trgm('macdonald')),\n\nunnest(show_trgm('McDonald'));\n\n1 postgresql.org/download 2 github.com/postgrespro/rum 3 postgresql.org/docs/14/generic-wal.html 4 postgresql.org/docs/14/pgtrgm.html\n\n580",
      "content_length": 1977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 582,
      "content": "28.3 Trigrams\n\nunnest | unnest −−−−−−−−+−−−−−−−− m | mc | | ald | cdo | don | ld | mcd | nal | ona | (10 rows)\n\nm ma acd ald cdo don ld mac nal ona\n\nThis class supports operators for both precise and fuzzy comparison of strings and words.\n\n=> SELECT amopopr::regoperator, oprcode::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'gin' AND opcname = 'gin_trgm_ops' ORDER BY amopstrategy;\n\namopopr\n\n|\n\noprcode\n\n−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n%(text,text) ~~(text,text) ~~*(text,text) | texticlike ~(text,text) ~*(text,text) %>(text,text) %>>(text,text) | strict_word_similarity_commutator_op =(text,text)\n\n| similarity_op | textlike\n\nLIKE and ILIKE\n\n| textregexeq | texticregexeq | word_similarity_commutator_op\n\nregular expressions\n\n| texteq\n\n(8 rows)\n\nTo perform fuzzy comparison,we can define the distance between strings as a ratio of common trigrams to the total number of trigrams in the query string. But as I have already shown, ��� does not support ordering operators, so all operators in the class must be Boolean. Therefore, for %, %>, and %>> operators that imple- ment strategies of fuzzy comparison, the consistency function returns true if the computed distance does not exceed the defined threshold.\n\n581",
      "content_length": 1363,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 583,
      "content": "Chapter 28 GIN\n\nFor = and ���� operators, the consistency function demands that the value con- tains all the trigrams of the query string. Matching a document against a regular expression requires a much more complex check.\n\nIn any case, trigram search is always fuzzy, and the results have to be rechecked.\n\n28.4 Indexing Arrays\n\nThe array data type is also supported by ���. Built over array elements,a ��� index can be used to quickly determine whether an array overlaps with or is contained in another array:\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'gin' AND opcname = 'array_ops' ORDER BY amopstrategy;\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n| &&(anyarray,anyarray) | arrayoverlap @>(anyarray,anyarray) | arraycontains | <@(anyarray,anyarray) | arraycontained | | =(anyarray,anyarray)\n\n| array_eq\n\n1 2 3 4\n\n(4 rows)\n\nAs an example, let’s take the routes view of the demo database that shows the in- formation on flights. The days_of_week column is an array of days of the week on which flights are performed. To build an index, we first have to materialize the view:\n\n=> CREATE TABLE routes_tbl AS SELECT * FROM routes;\n\nSELECT 710\n\n=> CREATE INDEX ON routes_tbl USING gin(days_of_week);\n\nLet’s use the created index to select the flights that depart on Tuesdays,Thursdays, and Sundays. I turn off sequential scanning; otherwise, the planner would not use the index for such a small table:\n\n582",
      "content_length": 1638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 584,
      "content": "28.4 Indexing Arrays\n\n=> SET enable_seqscan = off;\n\n=> EXPLAIN (costs off) SELECT * FROM routes_tbl WHERE days_of_week = ARRAY[2,4,7];\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on routes_tbl\n\nRecheck Cond: (days_of_week = '{2,4,7}'::integer[]) −> Bitmap Index Scan on routes_tbl_days_of_week_idx\n\nIndex Cond: (days_of_week = '{2,4,7}'::integer[])\n\n(4 rows)\n\nIt turns out that there are eleven such flights:\n\n=> SELECT flight_no, departure_airport, arrival_airport,\n\ndays_of_week FROM routes_tbl WHERE days_of_week = ARRAY[2,4,7];\n\nflight_no | departure_airport | arrival_airport | days_of_week −−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\nPG0023 PG0123 PG0155 PG0260 PG0261 PG0310 PG0370 PG0371 PG0448 PG0482 PG0651 (11 rows)\n\n| OSW | NBC | ARH | STW | SVO | UUD | DME | KRO | VKO | DME | UIK\n\n| KRO | ROV | TJM | CEK | GDZ | NYM | KRO | DME | STW | KEJ | KHV\n\n| {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7} | {2,4,7}\n\nThe built index contains only seven elements: integer numbers from � to � that represent days of the week.\n\nThe query execution is quite similar to what I have shown before for the full-text search. In this particular case, the search query is represented by a regular array rather than by a special data type; it is assumed that the indexed array must con- tain all the specified elements. An important distinction here is that the equality condition also requires the indexed array to contain no other elements. The consis- tency function1 knows about this requirement thanks to the strategy number, but it cannot verify that there are no unwanted elements, so it requests the indexing engine to recheck the results by the table:\n\n1 backend/access/gin/ginarrayproc.c, ginarrayconsistent function\n\n583",
      "content_length": 1841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 585,
      "content": "p. ���\n\nChapter 28 GIN\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM routes_tbl WHERE days_of_week = ARRAY[2,4,7];\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on routes_tbl (actual rows=11 loops=1) Recheck Cond: (days_of_week = '{2,4,7}'::integer[]) Rows Removed by Index Recheck: 482 Heap Blocks: exact=16 −> Bitmap Index Scan on routes_tbl_days_of_week_idx (actual ro...\n\nIndex Cond: (days_of_week = '{2,4,7}'::integer[])\n\n(6 rows)\n\nIt may be useful to extend the ��� index with additional columns. For example, to enable search for the flights that depart on Tuesdays, Thursdays, and Sundays from Moscow, the index lacks the departure_city column. But there are no operator classes implemented for regular scalar data types:\n\n=> CREATE INDEX ON routes_tbl USING gin(days_of_week, departure_city);\n\nERROR: method \"gin\" HINT: default operator class for the data type.\n\ndata type text has no default operator class for access\n\nYou must specify an operator class for the index or define a\n\nSuch situations can be addressed by the btree_gin extension. It adds ��� operator classes that simulate regular �-tree processing by representing a scalar value as a composite value with a single element.\n\n=> CREATE EXTENSION btree_gin;\n\n=> CREATE INDEX ON routes_tbl USING gin(days_of_week,departure_city);\n\n=> EXPLAIN (costs off) SELECT * FROM routes_tbl WHERE days_of_week = ARRAY[2,4,7] AND departure_city = 'Moscow';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on routes_tbl\n\nRecheck Cond: ((days_of_week = '{2,4,7}'::integer[]) AND (departure_city = 'Moscow'::text)) −> Bitmap Index Scan on routes_tbl_days_of_week_departure_city...\n\nIndex Cond: ((days_of_week = '{2,4,7}'::integer[]) AND (departure_city = 'Moscow'::text))\n\n(6 rows)\n\n584",
      "content_length": 1871,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 586,
      "content": "28.5 Indexing JSON\n\n=> RESET enable_seqscan;\n\nThe remark made about btree_gist holds true for btree_gin as well: a �-tree is much moreefficientwhenitcomestocomparisonoperations,soitmakessensetousethe btree_gin extension only when a ��� index is really needed. For instance, a search by less than or less than or equal to conditions can be performed by a backward scan in a �-tree, but not in ���.\n\n28.5 Indexing JSON\n\nOne more non-atomic data type with built-in ��� support is jsonb.1 It offers a whole range of operators for ����, and some of them can perform faster using ���.\n\nThere are two operator classes that extract different sets of elements from a ���� document:\n\n=> SELECT opcname FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid\n\nWHERE amname = 'gin' AND opcintype = 'jsonb'::regtype;\n\nopcname −−−−−−−−−−−−−−−−\n\njsonb_ops jsonb_path_ops\n\n(2 rows)\n\njsonb_ops Operator Class\n\nThe jsonb_ops operator class is the default one. All the keys, values, and array elementsoftheoriginal����documentareconvertedintoindexentries.2 Itspeeds up queries that check for inclusion of ���� values (@>),existence of keys (?, ?|,and ?&), or ���� path matches (@? and @@):\n\n1 postgresql.org/docs/14/datatype-json.html 2 backend/utils/adt/jsonb_gin.c, gin_extract_jsonb function\n\n585",
      "content_length": 1281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 587,
      "content": "Chapter 28 GIN\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'gin' AND opcname = 'jsonb_ops' ORDER BY amopstrategy;\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n| @>(jsonb,jsonb) | ?(jsonb,text) | ?|(jsonb,text[]) ?&(jsonb,text[]) | @?(jsonb,jsonpath) | jsonb_path_exists_opr | | @@(jsonb,jsonpath) | jsonb_path_match_opr\n\n| jsonb_contains | jsonb_exists | jsonb_exists_any | jsonb_exists_all\n\n7 9 10 11 15 16\n\n(6 rows)\n\nLet’s convert several rows of the routes view into the ���� format:\n\n=> CREATE TABLE routes_jsonb AS SELECT to_jsonb(t) route FROM (\n\nSELECT departure_airport_name, arrival_airport_name, days_of_week FROM routes ORDER BY flight_no LIMIT 4\n\n) t;\n\n=> SELECT ctid, jsonb_pretty(route) FROM routes_jsonb;\n\nctid\n\n|\n\njsonb_pretty\n\n−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− + + + + + +\n\n(0,1) | {\n\n| | | | | | } (0,2) | {\n\n\"days_of_week\": [\n\n6\n\n], \"arrival_airport_name\": \"Surgut Airport\", \"departure_airport_name\": \"Ust−Ilimsk Airport\"\n\n| | | | | | }\n\n\"days_of_week\": [\n\n7\n\n], \"arrival_airport_name\": \"Ust−Ilimsk Airport\", \"departure_airport_name\": \"Surgut Airport\"\n\n586\n\n+ + + + + +",
      "content_length": 1357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 588,
      "content": "28.5 Indexing JSON\n\n(0,3) | {\n\n| | | | | | | } (0,4) | {\n\n| | | | | | | }\n\n+ + + + ], + \"arrival_airport_name\": \"Sochi International Airport\", + + \"departure_airport_name\": \"Ivanovo South Airport\"\n\n\"days_of_week\": [\n\n2, 6\n\n+ + + + + ], \"arrival_airport_name\": \"Ivanovo South Airport\", + \"departure_airport_name\": \"Sochi International Airport\"+\n\n\"days_of_week\": [\n\n3, 7\n\n(4 rows)\n\n=> CREATE INDEX ON routes_jsonb USING gin(route);\n\nThe created index can be illustrated as follows:\n\narrival_airport_name arrival_airport_name\n\nIvanovo-Yuzhny Ivanovo-Yuzhny\n\n2 3 6 7\n\ne m a n _ t r o p r i a _ l a v i r r a\n\nk e e w _ f o _ s y a d\n\ne m a n _ t r o p r i a _ e r u t r a p e d\n\ny n h z u Y - o v o n a v I\n\ni\n\nh c o S\n\nt u g r u S\n\nk s m\n\ni l I - t s U\n\n0,3\n\n0,4\n\n0,1\n\n0,2\n\n0,1\n\n0,1\n\n0,1\n\n0,3\n\n0,3\n\n0,1\n\n0,1\n\n0,3\n\n0,4\n\n0,2\n\n0,2\n\n0,2\n\n0,4\n\n0,4\n\n0,2\n\n0,2\n\n0,3\n\n0,3\n\n0,3\n\n0,4\n\n0,4\n\n0,4\n\nLet’s consider a query with condition route @>'{\"days_of_week\": [6]}',which selects ���� documents containing the specified path (that is, the flights performed on Saturdays).\n\n587",
      "content_length": 1061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 589,
      "content": "Chapter 28 GIN\n\nThe support function1 extracts the search keys from the ���� value of the search query: “days_of_week” and “6”. These keys are searched in the element tree, and the documents that contain at least one of them are checked by the consistency function.2 For the contains strategy, this function demands that all the search keys are available, but the results still have to be rechecked by the table: from the point of view of an index,the specified path can also correspond to documents like {\"days_of_week\": [2],\"foo\": [6]}.\n\njsonb_path_ops Operator Class\n\nThe second class called jsonb_path_ops contains fewer operators:\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'gin' AND opcname = 'jsonb_path_ops' ORDER BY amopstrategy;\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n@>(jsonb,jsonb) | @?(jsonb,jsonpath) | jsonb_path_exists_opr | | @@(jsonb,jsonpath) | jsonb_path_match_opr\n\n| jsonb_contains\n\n7 15 16\n\n(3 rows)\n\nIf this class is used, the index will contain paths from the root of the document to all the values and all the array elements rather than isolated ���� fragments.3 It makes the search much more precise and efficient, but there is no speedup for operations with arguments represented by separate keys instead of paths.\n\nAs a path can be quite lengthy, it is not paths themselves but their hashes that actually get indexed.\n\nLet’s create an index for the same table using this operator class:\n\n1 backend/utils/adt/jsonb_gin.c, gin_extract_jsonb_query function 2 backend/utils/adt/jsonb_gin.c, gin_consistent_jsonb function 3 backend/utils/adt/jsonb_gin.c, gin_extract_jsonb_path function\n\n588",
      "content_length": 1854,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 590,
      "content": "28.5 Indexing JSON\n\n=> CREATE INDEX ON routes_jsonb USING gin(route jsonb_path_ops);\n\nThe created index can be represented by the following tree:\n\nHASH( ... ) HASH( ... ) HASH( ... ) HASH( ... )\n\n) y n h z u Y - o v o n a v I\n\ne m a n _ t r o p r i a _ e r u t r a p e d (\n\n)\n\ni\n\nh c o S\n\ne m a n _ t r o p r i a _ e r u t r a p e d (\n\n) k s m\n\ni l I - t s U\n\ne m a n _ t r o p r i a _ e r u t r a p e d (\n\n)\n\nt u g r u S\n\ne m a n _ t r o p r i a _ e r u t r a p e d (\n\n) 3\n\n,\n\nk e e w _ f o _ s y a d (\n\n)\n\nt u g r u S\n\n,\n\ne m a n _ t r o p r i a _ l a v i r r a\n\n)\n\ni\n\nh c o S\n\n,\n\ne m a n _ t r o p r i a _ l a v i r r a\n\n) k s m\n\ni l I - t s U\n\n,\n\ne m a n _ t r o p r i a _ l a v i r r a\n\n) 7\n\n,\n\nk e e w _ f o _ s y a d (\n\n) y n h z u Y - o v o n a v I\n\n,\n\ne m a n _ t r o p r i a _ l a v i r r a\n\n) 6\n\n,\n\nk e e w _ f o _ s y a d (\n\n) 2\n\n,\n\nk e e w _ f o _ s y a d (\n\n(\n\n(\n\n(\n\n(\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\nH S A H\n\n0,3\n\n0,4\n\n0,1\n\n0,2\n\n0,4\n\n0,1\n\n0,3\n\n0,2\n\n0,2\n\n0,4\n\n0,1\n\n0,3\n\n0,4\n\n0,3\n\nWhen executing a query with the same condition route @> '{\"days_of_week\": [6]}', the support function1 extracts the whole path “days_of_week, �” rather than its separate components. The ���s of the two matching documents will be found in the element tree right away.\n\nClearly, these entries will be checked by the consistency function2 and then rechecked by the indexing engine (to rule out hash collisions, for example). But the search through the tree is much more efficient, so it makes sense to always choose the jsonb_path_ops class if the index support provided by its operators is sufficient for queries.\n\n1 backend/utils/adt/jsonb_gin.c, gin_extract_jsonb_query_path function 2 backend/utils/adt/jsonb_gin.c, gin_consistent_jsonb_path function\n\n589",
      "content_length": 1818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 591,
      "content": "v. ��\n\nChapter 28 GIN\n\n28.6 Indexing Other Data Types\n\nG�� support via extensions is also provided for the following data types:\n\nArrays of integers. The intarray extension adds the gin__int_ops operator class for integer arrays. It is very similar to the standard array_ops operator class,but it supportsthematchoperator@@,whichmatchesadocumentagainstasearch query.\n\nKey–value storage. The hstore extension implements a storage for key–value pairs and provides the gin_hstore_ops operator class. Both keys and values get in- dexed.\n\nJSON query language. An external jsquery extension provides its own query lan-\n\nguage and ��� index support for ����.\n\nAfter the ���:���� standard was adopted and the ���/���� query language was implemented in Postgre���, the standard built-in capabilities seem to be a better choice.\n\n590",
      "content_length": 823,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 592,
      "content": "29\n\nBRIN\n\n29.1 Overview\n\nUnlike other indexes that are optimized to quickly find the required rows, ����1 is designed to filter out unnecessary rows. This access method was created primarily for large tables of several terabytes and up, so a smaller index size takes priority over search accuracy.\n\nTo speed up search, the whole table is split into ranges, hence the name: Block Range Index. Each range comprises several pages. The index does not store ���s, keeping only a summary on the data of each range. For ordinal data types, it is the minimal and the maximal values in the simplest case, but different operator classes may collect different information on values in a range.\n\nThe number of pages in a range is defined at the time of the index creation based on the value of the\n\npages_per_range storage parameter.\n\nIf a query condition references an indexed column, all the ranges that are guaran- teed to have no matches can be skipped. The pages of all the other ranges are bitmap; all the rows of these pages have to be returned by the index as a lossy rechecked.\n\nThus, ���� works well for columns with localized values (that is, for columns in which values stored close to each other have similar summary information prop- erties). For ordinal data types, it means that values must be stored in ascending or descending order, that is, have high correlation between their physical location\n\n1 postgresql.org/docs/14/brin.html backend/access/brin/README\n\n591\n\n128\n\np. ���\n\np. ���",
      "content_length": 1490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 593,
      "content": "Chapter 29 BRIN\n\nand the logical order defined by the greater than and less than operations. For other types of summary information,“similar properties” may vary.\n\nIt will not be wrong to think of ���� as an accelerator of sequential heap scans rather than an index in the conventional sense of the word. It can be regarded as an alternative to partitioning, with each range representing a virtual partition.\n\n29.2 Example\n\nOur demo database contains no tables that are large enough for ����, but we can imagine that analytical reports demand that we have a denormalized table con- taining summary information on all the departed and arrived flights of a particular airport, down to the occupied seats. The data for each airport is updated daily, as soon as it is midnight in the corresponding timezone. The added data is neither updated nor deleted.\n\nThe table looks as follows:\n\nCREATE TABLE flights_bi( airport_code char(3), airport_coord point, airport_utc_offset interval, -- timezone flight_no char(6), flight_type text, scheduled_time timestamptz, actual_time timestamptz, aircraft_code char(3), seat_no varchar(4), fare_conditions varchar(10), -- travel class passenger_id varchar(20), passenger_name text\n\n-- airport coordinates\n\n-- departure or arrival\n\n);\n\nData loading can be emulated using nested loops:1 the outer loop will correspond to days (the demo database stores annual data), while the inner loop will be based on timezones. As a result, the loaded data will be more or less ordered at least by time and airports, even though it is not explicitly sorted within the loop.\n\n1 edu.postgrespro.ru/internals-14/flights_bi.sql\n\n592",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 594,
      "content": "29.2 Example\n\nI will load an existing copy of the database that takes roughly � �� and contains about �� million rows:1\n\npostgres$ pg_restore -d demo -c flights_bi.dump\n\n=> ANALYZE flights_bi;\n\n=> SELECT count(*) FROM flights_bi;\n\ncount −−−−−−−−−− 30517076\n\n(1 row)\n\n=> SELECT pg_size_pretty(pg_total_relation_size('flights_bi'));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n4129 MB\n\n(1 row)\n\nWe can hardly call it a large table, but this data volume will be enough to demon- strate how ���� works. I will create an index in advance:\n\n=> CREATE INDEX ON flights_bi USING brin(scheduled_time);\n\n=> SELECT pg_size_pretty(pg_total_relation_size(\n\n'flights_bi_scheduled_time_idx'\n\n));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n184 kB (1 row)\n\nIt takes very little space with the default settings.\n\nA �-tree index is a thousand times bigger, even if data deduplication is enabled. True, its efficiency is also much higher, but an additional volume can turn out to be unaffordable luxury for really large tables.\n\n=> CREATE INDEX flights_bi_btree_idx ON flights_bi(scheduled_time);\n\n=> SELECT pg_size_pretty(pg_total_relation_size(\n\n'flights_bi_btree_idx'\n\n));\n\n1 edu.postgrespro.ru/internals-\\oldstylenums{14}/flights_bi.dump.\n\n593\n\nv. ��",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 595,
      "content": "Chapter 29 BRIN\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n210 MB (1 row)\n\n=> DROP INDEX flights_bi_btree_idx;\n\n29.3 Page Layout\n\nThe zero page of a ���� index is the metapage that keeps information on the index structure.\n\nAt a certain offset from the metadata, there are pages with summary information. Each index entry in such a page contains a summary of a particular block range.\n\nThe space between the metapage and the summary information is taken by the rangemap,whichissometimesalsoreferredtoasareversemap(hencethecommon revmap abbreviation). It is effectively an array of pointers to the corresponding index rows; the index number in this array corresponds to the range number.\n\nmetapage\n\nrevmap\n\n1 .. 10 1 .. 10\n\n11 .. 20 11 .. 20\n\n21 .. 30 21 .. 30\n\n71 .. 80 71 .. 80\n\n31 .. 40 31 .. 40\n\n41 .. 50 41 .. 50\n\n51 .. 60 51 .. 60\n\n61 .. 70 61 .. 70\n\nAs the table is expanding,the size of the range map also grows. If the map does not fittheallottedpages,itovertakesthenextpage,andalltheindexentriespreviously located in this page are transferred to other pages. Since a page can accommodate many pointers, such transfers are quite rare.\n\n594",
      "content_length": 1138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 596,
      "content": "29.3 Page Layout\n\nB��� index pages can be displayed by the pageinspect extension, as usual. The metadata includes the size of the range and the number of pages reserved for the range map:\n\n=> SELECT pagesperrange, lastrevmappage FROM brin_metapage_info(get_raw_page( 'flights_bi_scheduled_time_idx', 0\n\n));\n\npagesperrange | lastrevmappage −−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−\n\n128 |\n\n4\n\n(1 row)\n\nHere the range map takes four pages, from the first to the fourth one. We can take a look at the pointers to index entries containing summarized data:\n\n=> SELECT * FROM brin_revmap_data(get_raw_page(\n\n'flights_bi_scheduled_time_idx', 1\n\n));\n\npages −−−−−−−−−− (6,197) (6,198) (6,199) ... (6,195) (6,196)\n\n(1360 rows)\n\nIf the range is not summarized yet, the pointer in the range map is ����.\n\nAnd here are the summaries for several ranges:\n\n=> SELECT itemoffset, blknum, value FROM brin_page_items(\n\nget_raw_page('flights_bi_scheduled_time_idx', 6), 'flights_bi_scheduled_time_idx'\n\n) ORDER BY blknum LIMIT 3 \\gx\n\n595",
      "content_length": 1008,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 597,
      "content": "p. ���\n\nChapter 29 BRIN\n\n−[ RECORD 1 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− itemoffset | 197 | 0 blknum | {2016−08−15 02:45:00+03 .. 2016−08−15 16:20:00+03} value −[ RECORD 2 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− itemoffset | 198 | 128 blknum value | {2016−08−15 05:50:00+03 .. 2016−08−15 18:55:00+03} −[ RECORD 3 ]−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− itemoffset | 199 | 256 blknum | {2016−08−15 07:15:00+03 .. 2016−08−15 18:50:00+03} value\n\n29.4 Search\n\nIf a query condition is supported by the ���� index,1 the executor scans the range mapandthesummaryinformationforeachrange. Ifthedatainarangemaymatch thesearchkey,allthepagesthatbelongtothisrangeareaddedtothebitmap. Since ���� does not keep ��s of separate tuples, the bitmap is always lossy.\n\nMatching the data against the search key is performed by the consistency function, which interprets range summary information. Non-summarized ranges are always added to the bitmap.\n\nThe received bitmap is used to scan the table in the usual manner . It is important to mention that heap page reads happen sequentially, block range by block range, and prefetching is employed.\n\n29.5 Summary Information Updates\n\nValue Insertion\n\nAs a new tuple is added into a heap page, the summary information in the corre- sponding index range is updated.2 The range number is calculated based on the\n\n1 backend/access/brin/brin.c, bringetbitmap function 2 backend/access/brin/brin.c, brininsert function\n\n596",
      "content_length": 1484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 598,
      "content": "29.5 Summary Information Updates\n\npage number using simple arithmetic operations,and the summary information is then located by the range map.\n\nTo determine whether the current summary information has to be expanded, the addition function is employed. If an expansion is required and the page has enough free space, it is done in-place (without adding a new index entry).\n\nSuppose we have added a tuple with value �� to page ��. The range number is cal- culated by integer division of page number by the size of the range. Assuming that the range size equals four pages, we get range number �; since range numbering is zero-based, we take the fourth pointer in the range map. The minimal value in this range is ��, the maximal one is ��. The added value falls outside these limits, so the maximal value is increased:\n\nmetapage\n\nrevmap\n\n1 .. 10 1 .. 10\n\n11 .. 20 11 .. 20\n\n21 .. 30 21 .. 30\n\n71 .. 80 71 .. 80\n\n31 .. 42 31 .. 42\n\n41 .. 50 41 .. 50\n\n51 .. 60 51 .. 60\n\n61 .. 70 61 .. 70\n\nIf an in-place update is impossible, a new entry is added, and the range map is modified.\n\nRange Summarization\n\nEverything said above applies to scenarios when a new tuple appears in an already summarized range. When an index is being built, all the existing ranges are sum- marized, but as the table grows, new pages may fall outside these ranges.\n\nautosummarize storage parameter enabled, the new If an index is created with the range will be summarized at once. But in data warehouses, where rows are usually added in large batches rather than one by one, this mode can seriously slow down insertion.\n\n597\n\noff",
      "content_length": 1599,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 599,
      "content": "p. ���\n\np. ���\n\nChapter 29 BRIN\n\nBy default, new ranges are not summarized right away. It does not affect index correctness because ranges with no summary information are always scanned. or Summarization is performed asynchronously, either during table vacuuming when initiated manually by calling the brin_summarize_new_values function (or the brin_summarize_range function that processes a single range).\n\nRange summarization1 does not lock the table for updates. At the beginning of this process, a placeholder entry is inserted into the index for this range. If the data in the range is changed while this range is being scanned, the placeholder will be updated with the summary information on these changes. Then the union function will unite this data with the summary information on the corresponding range.\n\nIn theory, summary information could sometimes shrink after some rows are deleted. But while �i�� indexes can redistribute data after a page split, summary information of ���� indexes never shrinks and can only get wider. Shrinking is usually not required here because a data storage is typically used only for ap- pending new data. You can manually delete summary information by calling the brin_desummarize_range function for this range to be summarized again, but there is no clue as to which ranges might benefit from it.\n\nThus, ���� is primarily targeted at tables of very large size, which either have min- imal updates that add new rows mostly to the end of the file, or are not updated at all. It is mainly used in data warehouses for building analytical reports.\n\n29.6 Minmax Classes\n\nFor data types that allow comparing values,summary information includes at least the maximal and minimal values. The corresponding operator classes contain the word minmax in their names:2\n\n=> SELECT opcname FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid\n\nWHERE amname = 'brin' AND opcname LIKE '%minmax_ops' ORDER BY opcname;\n\n1 backend/access/brin/brin.c, summarize_range function 2 backend/access/brin/brin_minmax.c\n\n598",
      "content_length": 2043,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 600,
      "content": "29.6 Minmax Classes\n\nopcname −−−−−−−−−−−−−−−−−−−−−−−−\n\nbit_minmax_ops bpchar_minmax_ops bytea_minmax_ops char_minmax_ops ... timestamptz_minmax_ops timetz_minmax_ops uuid_minmax_ops varbit_minmax_ops\n\n(26 rows)\n\nHere are the support functions of these operator classes:\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'brin' AND opcname = 'numeric_minmax_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−\n\n1 | brin_minmax_opcinfo 2 | brin_minmax_add_value 3 | brin_minmax_consistent 4 | brin_minmax_union\n\n(4 rows)\n\nThe first function returns the operator class metadata, and all the other functions have already been described: they insert new values, check consistency, and per- form union operations.\n\nThe minmax class includes the same comparison operators that we have seen for �-trees\n\n:\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'brin' AND opcname = 'numeric_minmax_ops' ORDER BY amopstrategy;\n\n599\n\np. ���",
      "content_length": 1239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 601,
      "content": "p. ��\n\nChapter 29 BRIN\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n<(numeric,numeric) | numeric_lt | <=(numeric,numeric) | numeric_le | =(numeric,numeric) | numeric_eq | >=(numeric,numeric) | numeric_ge | | numeric_gt | >(numeric,numeric)\n\n1 2 3 4 5\n\n(5 rows)\n\nChoosing Columns to be Indexed\n\nWhich columns does it make sense to index using this operator class? As men- tioned earlier, such indexes work well if the physical location of rows correlates with the logical order of values.\n\nLet’s check it for the above example.\n\n=> SELECT attname, correlation, n_distinct FROM pg_stats WHERE tablename = 'flights_bi' ORDER BY correlation DESC NULLS LAST;\n\nattname\n\n|\n\ncorrelation\n\n|\n\nn_distinct\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n25926 | scheduled_time 34469 | actual_time 3 | fare_conditions 2 | flight_type 11 airport_utc_offset | 8 | aircraft_code 104 | airport_code 461 | seat_no 710 | flight_no | −0.00046121294 | 2.610987e+06 passenger_id 8618 | passenger_name 0 | airport_coord\n\n0.9999949 | 0.9999948 | 0.7976897 | 0.4981733 | 0.4440067 | 0.19249801 | 0.061483838 | 0.0024594965 | 0.0020146023 |\n\n−0.012388787 | |\n\n(12 rows)\n\nThe data is ordered by time (both scheduled and actual time; there is little dif- ference, if any): new entries are added in chronological order, and as the data is of the table se- neither updated nor deleted, all the rows get into the main fork quentially, one after another.\n\n600",
      "content_length": 1478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 602,
      "content": "29.6 Minmax Classes\n\nColumns fare_conditions, flight_type,and airport_utc_offset have relatively high cor- relation, but they store too few distinct values.\n\nThe correlation in other columns is too low for their indexing with the minmax operator class to be of any interest.\n\nRange Size and Search Efficiency\n\nAn appropriate range size can be determined based on the number of pages used to store particular values.\n\nLet’s take a look at the scheduled_time column and get the information on all the flights performed in �� hours. We first have to find out how many table pages are taken by the data related to this time interval.\n\nTo get this number,we can use the fact that a ��� consists of a page number and an offset. Unfortunately, there is no built-in function to break down a ��� into these two components,so we will have to write our own clumsy function to perform type casting via a text representation:\n\n=> CREATE FUNCTION tid2page(t tid) RETURNS integer LANGUAGE sql RETURN (t::text::point)[0]::integer;\n\nNow we can see how days are distributed through the table:\n\n=> SELECT min(numblk), round(avg(numblk)) avg, max(numblk) FROM (\n\nSELECT count(distinct tid2page(ctid)) numblk FROM flights_bi GROUP BY scheduled_time::date\n\n) t;\n\nmin\n\n| avg\n\n| max\n\n−−−−−−+−−−−−−+−−−−−− 1192 | 1447 | 1512\n\n(1 row)\n\nAs we can notice,the data distribution is not quite uniform. With a standard range size of ��� pages, each day will take from � to �� ranges. While fetching the data for a particular day, the index scan will return both the rows that are really needed and some rows related to other days that got into the same ranges. The bigger\n\n601",
      "content_length": 1644,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 603,
      "content": "p. ���\n\nChapter 29 BRIN\n\nthe range size, the more extra boundary values will be read; we can change their number by reducing or increasing the range size.\n\nLet’s try out a query for some particular day (I have already created an index with the default settings). For simplicity, I will forbid parallel execution:\n\n=> SET max_parallel_workers_per_gather = 0;\n\n=> \\set d '2016-08-15 02:45:00+03'\n\n=> EXPLAIN (analyze, buffers, costs off, timing off, summary off) SELECT * FROM flights_bi WHERE scheduled_time >= :'d'::timestamptz\n\nAND scheduled_time < :'d'::timestamptz + interval '1 day';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights_bi (actual rows=81964 loops=1)\n\nRecheck Cond: ((scheduled_time >= '2016−08−15 02:45:00+03'::ti... Rows Removed by Index Recheck: 11606 Heap Blocks: lossy=1536 Buffers: shared hit=1561 −> Bitmap Index Scan on flights_bi_scheduled_time_idx\n\n(actual rows=15360 loops=1) Index Cond: ((scheduled_time >= '2016−08−15 02:45:00+03'::... Buffers: shared hit=25\n\nPlanning:\n\nBuffers: shared hit=1\n\n(11 rows)\n\nWe can define an efficiency factor of a ���� index for a particular query as a ratio between the number of pages skipped in an index scan and the total number of pages in the table. If the efficiency factor is zero, the index access degrades to sequential scanning (without taking overhead costs into account ). The higher the efficiency factor, the fewer pages have to be read. But as some pages contain the data to be returned and cannot be skipped, the efficiency factor is always smaller than one.\n\nIn this particular case,the efficiency factor is 528417−1561 the number of pages in the table.\n\n528417\n\n≈ �.���,where ���,��� is\n\nHowever, we cannot draw any meaningful conclusions based on a single value. Even if we had uniform data and ideal correlation, the efficiency would still vary because, at the very least, range boundaries will not match page boundaries. We\n\n602",
      "content_length": 1973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 604,
      "content": "29.6 Minmax Classes\n\ncan get the full picture only if we treat the efficiency factor as a random value and analyze its distribution.\n\nForourexample,wecanselectallthedifferentdaysoftheyear,checktheexecution plan for each value, and calculate statistics based on this selection. We can easily automate this process because the ������� command can return the results in the ���� format, which is convenient to parse. I will not provide all the code here, but the following snippet contains all the important details:\n\n=> DO $$ DECLARE\n\nplan jsonb;\n\nBEGIN\n\nEXECUTE\n\n'EXPLAIN (analyze, buffers, timing off, costs off, format json)\n\nSELECT * FROM flights_bi WHERE scheduled_time >= $1\n\nAND scheduled_time < $1 + interval ''1 day'''\n\nUSING '2016-08-15 02:45:00+03'::timestamptz INTO plan; RAISE NOTICE 'shared hit=%, read=%',\n\nplan -> 0 -> 'Plan' ->> 'Shared Hit Blocks', plan -> 0 -> 'Plan' ->> 'Shared Read Blocks';\n\nEND; $$;\n\nNOTICE: DO\n\nshared hit=1561, read=0\n\nThe results can be visually displayed as a box plot, also known as a “box-and- whiskers.” The whiskers here denote the first and fourth quartiles (that is,the right whisker gets ��% of the largest values, while the left one gets ��% of the small- est values). The box itself holds the remaining ��% of values and has the median value marked. What is more important, this compact representation enables us to visually compare different results. The following illustration shows the efficiency factor distribution for the default range size and for two other sizes that are four times larger and smaller.\n\nAs we could have expected, the search accuracy and efficiency are high even for rather large ranges.\n\nThe dashed line here marks the average value of the maximal efficiency factor pos- sible for this query, assuming that one day takes roughly 1 365\n\nof the table.\n\n603",
      "content_length": 1831,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 605,
      "content": "Chapter 29 BRIN\n\n32 pages/range, 529 kB\n\n128 pages/range, 184 kB\n\n512 pages/range, 72 kB\n\n0,990\n\n0,992\n\n0,994\n\n0,996\n\n0,998\n\n1,000\n\nefficiency factor\n\nNote that the rise in efficiency comes at the expense of the index size increase. B��� is quite flexible in letting you find the balance between the two.\n\nProperties\n\nB��� properties are hardwired and do not depend on operator classes.\n\nAccess Method Properties\n\n=> SELECT a.amname, p.name, pg_indexam_has_property(a.oid, p.name) FROM pg_am a, unnest(array[\n\n'can_order', 'can_unique', 'can_multi_col', 'can_exclude', 'can_include'\n\n]) p(name) WHERE a.amname = 'brin';\n\namname |\n\nname\n\n| pg_indexam_has_property\n\n−−−−−−−−+−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−− | can_order | f | f | can_unique | can_multi_col | t | f | can_exclude | f | can_include\n\nbrin brin brin brin brin\n\n(5 rows)\n\nObviously, neither sorting nor uniqueness properties are supported. Since a ���� index always returns a bitmap,exclusion constraints are not supported either. Nei- ther do additional ������� columns make any sense, as even indexing keys are not stored in ���� indexes.\n\n604",
      "content_length": 1114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 606,
      "content": "29.6 Minmax Classes\n\nHowever, we can create a multicolumn ���� index. In this case, summary informa- tion for each column is collected and stored in a separate index entry,but they still have a common range mapping. Such an index is useful if the same range size is applicable to all the indexed columns.\n\nAlternatively, we can create separate ���� indexes for several columns and take advantage of the fact that bitmaps\n\ncan be merged together. For example:\n\n=> CREATE INDEX ON flights_bi USING brin(airport_utc_offset);\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM flights_bi WHERE scheduled_time >= :'d'::timestamptz\n\nAND scheduled_time < :'d'::timestamptz + interval '1 day' AND airport_utc_offset = '08:00:00';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights_bi (actual rows=1658 loops=1)\n\nRecheck Cond: ((scheduled_time >= '2016−08−15 02:45:00+03'::ti... Rows Removed by Index Recheck: 14077 Heap Blocks: lossy=256 −> BitmapAnd (actual rows=0 loops=1)\n\n−> Bitmap Index Scan on flights_bi_scheduled_time_idx (act... Index Cond: ((scheduled_time >= '2016−08−15 02:45:00+0... −> Bitmap Index Scan on flights_bi_airport_utc_offset_idx ...\n\nIndex Cond: (airport_utc_offset = '08:00:00'::interval)\n\n(9 rows)\n\nIndex-Level Properties\n\n=> SELECT p.name, pg_index_has_property(\n\n'flights_bi_scheduled_time_idx', p.name\n\n) FROM unnest(array[\n\n'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'\n\n]) p(name);\n\nname\n\n| pg_index_has_property\n\n−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n| f clusterable | f index_scan bitmap_scan | t backward_scan | f\n\n(4 rows)\n\n605\n\np. ���",
      "content_length": 1661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 607,
      "content": "Chapter 29 BRIN\n\nObviously, bitmap scanning is the only supported access type.\n\nLack of clusterization may seem puzzling. Since ���� is sensitive to the physical order of rows,it is quite logical to assume that it should support reordering,which would maximize its efficiency. But clusterization of large tables is anyway a luxury, taking into account all the processing and extra disk space required to rebuild a table. Besides, as the example of the flights_bi table shows, some ordering in data storages can occur naturally.\n\nColumn-Level Properties\n\n=> SELECT p.name, pg_index_column_has_property( 'flights_bi_scheduled_time_idx', 1, p.name\n\n) FROM unnest(array[\n\n'orderable', 'distance_orderable', 'returnable', 'search_array', 'search_nulls'\n\n]) p(name);\n\nname\n\n| pg_index_column_has_property\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\norderable | f distance_orderable | f | f returnable | f search_array | t search_nulls\n\n(5 rows)\n\nThe only available column-level property is ���� support. To track ���� values in a range, summary information provides a separate attribute:\n\n=> SELECT hasnulls, allnulls, value FROM brin_page_items(\n\nget_raw_page('flights_bi_airport_utc_offset_idx', 6), 'flights_bi_airport_utc_offset_idx'\n\n) WHERE itemoffset= 1;\n\nhasnulls | allnulls |\n\nvalue\n\n−−−−−−−−−−+−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−\n\nf\n\n| f\n\n| {03:00:00 .. 03:00:00}\n\n(1 row)\n\n606",
      "content_length": 1391,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 608,
      "content": "29.7 Minmax-Multi Classes\n\n29.7 Minmax-Multi Classes\n\nThe established correlation can be easily disrupted by data updates. The reason is not an actual modification of a particular value but rather the ���� design itself : an old version of a row may be deleted in one page, while its new version may be inserted into any location that is currently free, so the original row order cannot be preserved.\n\nTo minimize this effect to some extent, we can reduce the value of the fillfactor storage parameter to leave more space in the page for future updates. But is it really worth increasing the size of an already huge table? Besides, deletions will anyway free some space in existing pages,thus preparing traps for new tuples that would otherwise get to the end of the file.\n\nSuch a situation can be easily emulated. Let’s delete �.�% of randomly chosen rows and vacuum the table to clean up some space for new tuples:\n\n=> WITH t AS ( SELECT ctid FROM flights_bi TABLESAMPLE BERNOULLI(0.1) REPEATABLE(0)\n\n) DELETE FROM flights_bi WHERE ctid IN (SELECT ctid FROM t);\n\nDELETE 30180\n\n=> VACUUM flights_bi;\n\nNow let’s add some data for a new day in one of the timezones. I will simply copy the data of the previous day:\n\n=> INSERT INTO flights_bi SELECT airport_code, airport_coord, airport_utc_offset,\n\nflight_no, flight_type, scheduled_time + interval '1 day', actual_time + interval '1 day', aircraft_code, seat_no,\n\nfare_conditions, passenger_id, passenger_name\n\nFROM flights_bi WHERE date_trunc('day', scheduled_time) = '2017-08-15'\n\nAND airport_utc_offset = '03:00:00';\n\nINSERT 0 40532\n\n607\n\nv. ��\n\np. ��",
      "content_length": 1604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 609,
      "content": "Chapter 29 BRIN\n\nThe performed deletion was enough to free some space in all or almost all the ranges. Getting into pages located somewhere in the middle of the file,new tuples have automatically expanded the ranges. For example, the summary information related to the first range used to cover less than a day, but now it comprises the whole year:\n\n=> SELECT value FROM brin_page_items(\n\nget_raw_page('flights_bi_scheduled_time_idx', 6), 'flights_bi_scheduled_time_idx'\n\n) WHERE blknum = 0;\n\nvalue −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− {2016−08−15 02:45:00+03 .. 2017−08−16 09:35:00+03}\n\n(1 row)\n\nThe smaller the date specified in the query, the more ranges have to be scanned. The graph shows the magnitude of the disaster:\n\n128 pages/range, 248 kB\n\n0\n\n0,1\n\n0,2\n\n0,3\n\n0,4\n\n0,5\n\n0,6\n\n0,7\n\n0,8\n\n0,9\n\n1\n\nefficiency factor\n\nTo address this issue, we have to make the summary information a bit more so- phisticated: instead of a single continuous range, we have to store several smaller ones that cover all the values when taken together. Then one of the ranges can cover the main set of data, while the rest will handle occasional outliers.\n\nSuch functionality is provided by minmax-multi operator classes:1\n\n=> SELECT opcname FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid\n\nWHERE amname = 'brin' AND opcname LIKE '%minmax_multi_ops' ORDER BY opcname;\n\n1 backend/access/brin/brin_minmax_multi.c\n\n608",
      "content_length": 1422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 610,
      "content": "29.7 Minmax-Multi Classes\n\nopcname −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\ndate_minmax_multi_ops float4_minmax_multi_ops float8_minmax_multi_ops inet_minmax_multi_ops ... time_minmax_multi_ops timestamp_minmax_multi_ops timestamptz_minmax_multi_ops timetz_minmax_multi_ops uuid_minmax_multi_ops\n\n(19 rows)\n\nAs compared to minmax operator classes, minmax-multi classes have one more sup- port function that computes the distance between values; it is used to determine the range length, which the operator class strives to reduce:\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'brin' AND opcname = 'numeric_minmax_multi_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− 1 | brin_minmax_multi_opcinfo 2 | brin_minmax_multi_add_value 3 | brin_minmax_multi_consistent 4 | brin_minmax_multi_union 5 | brin_minmax_multi_options\n\n11 | brin_minmax_multi_distance_numeric\n\n(6 rows)\n\nThe operators of such classes are absolutely the same as those of the minmax classes.\n\nMinmax-multi classes can take the values_per_range parameter, which defines the maximal allowed number of summarized values per range. A summarized value is represented by two numbers (an interval),while a separate point requires just one. If there are not enough values, some of the intervals are reduced.1\n\n1 backend/access/brin/brin_minmax_multi.c, reduce_expanded_ranges function\n\n609\n\n32",
      "content_length": 1504,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 611,
      "content": "Chapter 29 BRIN\n\nLet’s build a minmax-multi index instead of the existing one. We will limit the num- ber of allowed values per range to ��:\n\n=> DROP INDEX flights_bi_scheduled_time_idx;\n\n=> CREATE INDEX ON flights_bi USING brin(\n\nscheduled_time timestamptz_minmax_multi_ops(\n\nvalues_per_range = 16\n\n)\n\n);\n\nThe graph shows that the new index brings the efficiency back to the original level. Quite expectedly, it leads to an increase in the index size:\n\nminmax-multi 656 kB\n\nminmax 184 kB\n\n0,990\n\n0,992\n\n0,994\n\n0,996\n\n0,998\n\n1,000\n\nefficiency factor\n\n29.8 Inclusion Classes\n\nThe difference between minmax and inclusion operator classes is roughly the same as the difference between �-trees and �i�� indexes: the latter are designed for data types that do not support comparison operations, although mutual alignment of values still makes sense for them. Summary information for a particular range provided by inclusion operator classes is represented by the bounding box of the values in this range.\n\nHere are these operator classes; they are not numerous:\n\n=> SELECT opcname FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid\n\nWHERE amname = 'brin' AND opcname LIKE '%inclusion_ops' ORDER BY opcname;\n\n610",
      "content_length": 1212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 612,
      "content": "29.8 Inclusion Classes\n\nopcname −−−−−−−−−−−−−−−−−−−−−\n\nbox_inclusion_ops inet_inclusion_ops range_inclusion_ops\n\n(3 rows)\n\nThe list of support functions is extended by one more mandatory function that merges two values, and by a bunch of optional ones:\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'brin' AND opcname = 'box_inclusion_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\n1 | brin_inclusion_opcinfo 2 | brin_inclusion_add_value 3 | brin_inclusion_consistent 4 | brin_inclusion_union\n\n11 | bound_box 13 | box_contain\n\n(6 rows)\n\nWhen dealing with values that can be compared, we relied on their correlation; but for other data types, no such statistic is collected,1 so it is hard to predict the efficiency of an inclusion-based ���� index.\n\nWhat is worse, correlation greatly affects cost estimation of an index scan. If such statistic is unavailable, it is taken as zero.2 Thus, the planner has no way to tell between exact and fuzzy inclusion indexes, so it typically avoids using them alto- gether.\n\nPost��� collects\n\nstatistics on correlation of spatial data.\n\nIn this particular case, we can presume that it makes sense to build an index over airport coordinates, as longitude must correlate with the timezone.\n\n1 backend/commands/analyze.c, compute_scalar_stats function 2 backend/utils/adt/selfuncs.c, brincostestimate function\n\n611\n\nv. �.�.�",
      "content_length": 1512,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 613,
      "content": "Chapter 29 BRIN\n\nUnlike �i�� predicates, ���� summary information has the same type as the in- dexed data; therefore, it is not so easy to build an index for points. But we can create an expression index by converting points into dummy rectangles:\n\n=> CREATE INDEX ON flights_bi USING brin(box(airport_coord)) WITH (pages_per_range = 8);\n\n=> SELECT pg_size_pretty(pg_total_relation_size(\n\n'flights_bi_box_idx'\n\n));\n\npg_size_pretty −−−−−−−−−−−−−−−−\n\n3816 kB\n\n(1 row)\n\nAn index built over timezones with the same range size takes approximately the same volume (���� k�).\n\nThe operators included into this class are similar to �i�� operators. For example, a ���� index can be used to speed up search for points in a certain area:\n\n=> SELECT airport_code, airport_name FROM airports WHERE box(coordinates) <@ box '135,45,140,50';\n\nairport_code |\n\nairport_name\n\n−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−−−\n\nKHV\n\n| Khabarovsk−Novy Airport\n\n(1 row)\n\nBut as mentioned earlier, the planner refuses to use an index scan unless we turn off sequential scanning:\n\n=> EXPLAIN (costs off) SELECT * FROM flights_bi WHERE box(airport_coord) <@ box '135,45,140,50';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nSeq Scan on flights_bi\n\nFilter: (box(airport_coord) <@ '(140,50),(135,45)'::box)\n\n(2 rows)\n\n=> SET enable_seqscan = off;\n\n612",
      "content_length": 1344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 614,
      "content": "29.9 Bloom Classes\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM flights_bi WHERE box(airport_coord) <@ box '135,45,140,50';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights_bi (actual rows=511414 loops=1)\n\nRecheck Cond: (box(airport_coord) <@ '(140,50),(135,45)'::box) Rows Removed by Index Recheck: 630756 Heap Blocks: lossy=19656 −> Bitmap Index Scan on flights_bi_box_idx (actual rows=196560...\n\nIndex Cond: (box(airport_coord) <@ '(140,50),(135,45)'::box)\n\n(6 rows)\n\n=> RESET enable_seqscan;\n\n29.9 Bloom Classes\n\nOperator classes based on the Bloom filter enable ���� usage for any data types that support the equal to operation and have a hash function defined. They can also be applied to regular ordinal types if values are localized in separate ranges but their physical location has no correlation with the logical order.\n\nThe names of such operator classes contain the word bloom:1\n\n=> SELECT opcname FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid\n\nWHERE amname = 'brin' AND opcname LIKE '%bloom_ops' ORDER BY opcname;\n\nopcname −−−−−−−−−−−−−−−−−−−−−−−\n\nbpchar_bloom_ops bytea_bloom_ops char_bloom_ops ... timestamptz_bloom_ops timetz_bloom_ops uuid_bloom_ops\n\n(24 rows)\n\n1 backend/access/brin/brin_bloom.c\n\n613\n\nv. ��",
      "content_length": 1326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 615,
      "content": "−0.1\n\np. ���\n\nChapter 29 BRIN\n\nThe classic Bloom filter is a data structure that enables you to quickly check whether an element belongs to a set. This filter is very compact, but it allows false positives: a set may be assumed to contain more elements than it actually does. But what is more important, false negatives are ruled out: the filter cannot decide that an element is not present in the set if it is actually there.\n\nThe filter is an array of m bits (also called a signature), which is originally filled with zeros. We select k different hash functions to map any element of the set to k bits of the signature. When an element is added to the set, each of the bits in the signature is set to one. Consequently, if all the bits that correspond to an element are set to one, the element may be present in the set; if there is at least one zero bit, the element is guaranteed to be absent.\n\nIn the case of ���� indexes, the filter processes a set of values of an indexed col- umn that belong to a particular range; the summary information for this range is represented by the built Bloom filter.\n\nThe bloom extension1 provides its own index access method based on the Bloom filter. It builds a filter for each table row and deals with a set of column values of each row. Such an index is designed for indexing several columns at a time and can be used in adhoc queries,when the columns to be referenced in filter conditions are not known in advance. A ���� indexcan also be built on several columns,but its summaryinformation will contain several independent Bloom filters for each of these columns.\n\nThe accuracy of the Bloom filter depends on the signature length. In theoretical terms,theoptimalnumberofsignaturebitscanbeestimatedatm = −nlog2 p ,where n is the number of elements in the set and p is the probability of false positives.\n\nln2\n\nThese two settings can be adjusted using the corresponding operator class param- eters:\n\n\n\nn_distinct_per_range defines the number of elements in a set; in this case, it is the number of distinct values in one range of an indexed column. This param- eter value is interpreted just like statistics on distinct values: negative values indicate the fraction of rows in the range, not their absolute number.\n\n1 postgresql.org/docs/14/bloom.html\n\n614",
      "content_length": 2298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 616,
      "content": "29.9 Bloom Classes\n\n\n\nfalse_positive_rate defines the probability of false positives.\n\nA near-zero value means that an index scan will almost certainly skip the ranges that have no searched values. But it does not guarantee exact search, as the scanned ranges will also contain extra rows that do not match the query. Such behavior is due to range width and physical data location rather than to the actual filter properties.\n\nThe list of support functions is extended by a hash function:\n\n=> SELECT amprocnum, amproc::regproc FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amproc amop ON amprocfamily = opcfamily\n\nWHERE amname = 'brin' AND opcname = 'numeric_bloom_ops' ORDER BY amprocnum;\n\namprocnum |\n\namproc\n\n−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−−−\n\n1 | brin_bloom_opcinfo 2 | brin_bloom_add_value 3 | brin_bloom_consistent 4 | brin_bloom_union 5 | brin_bloom_options\n\n11 | hash_numeric\n\n(6 rows)\n\nSince the Bloom filter is based on hashing,only the equality operator is supported:\n\n=> SELECT amopopr::regoperator, oprcode::regproc, amopstrategy FROM pg_am am\n\nJOIN pg_opclass opc ON opcmethod = am.oid JOIN pg_amop amop ON amopfamily = opcfamily JOIN pg_operator opr ON opr.oid = amopopr\n\nWHERE amname = 'brin' AND opcname = 'numeric_bloom_ops' ORDER BY amopstrategy;\n\namopopr\n\n|\n\noprcode\n\n| amopstrategy\n\n−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−\n\n=(numeric,numeric) | numeric_eq |\n\n1\n\n(1 row)\n\nLet’s take the flight_no column that stores flight numbers; it has near-zero corre- lation, so it is useless for a regular range operator class. We will keep the default\n\n615\n\n0.01",
      "content_length": 1602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 617,
      "content": "Chapter 29 BRIN\n\nfalse-positive setting; as for the number of distinct values in a range, it can be easily calculated. For example, for an eight-page range we will get the following value:\n\n=> SELECT max(nd) FROM (\n\nSELECT count(distinct flight_no) nd FROM flights_bi GROUP BY tid2page(ctid) / 8\n\n) t;\n\nmax −−−−− 22 (1 row)\n\nFor smaller ranges, this number will be even lower (but in any case, the operator class does not allow values smaller than ��).\n\nWe just have to create an index and check the execution plan:\n\n=> CREATE INDEX ON flights_bi USING brin(\n\nflight_no bpchar_bloom_ops(\n\nn_distinct_per_range = 22)\n\n) WITH (pages_per_range = 8);\n\n=> EXPLAIN (analyze, costs off, timing off, summary off) SELECT * FROM flights_bi WHERE flight_no = 'PG0001';\n\nQUERY PLAN −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n\nBitmap Heap Scan on flights_bi (actual rows=5192 loops=1)\n\nRecheck Cond: (flight_no = 'PG0001'::bpchar) Rows Removed by Index Recheck: 122894 Heap Blocks: lossy=2168 −> Bitmap Index Scan on flights_bi_flight_no_idx (actual rows=...\n\nIndex Cond: (flight_no = 'PG0001'::bpchar)\n\n(6 rows)\n\n=> RESET max_parallel_workers_per_gather;\n\nThe graph shows that for some flight numbers (represented by separate points that do not belong to any whiskers) the index does not work very well, but its overall efficiency is quite high:\n\n616",
      "content_length": 1365,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 618,
      "content": "2 pages/range, 14,8 MB\n\n4 pages/range, 7,4 Mb\n\n8 pages/range, 3,7MB\n\n0,70\n\n0,75\n\n0,80\n\n0,85\n\n0,90\n\n0,95\n\n29.9 Bloom Classes\n\n1,00\n\nefficiency factor\n\n617",
      "content_length": 153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 619,
      "content": "Conclusion\n\nWell, now our journey is coming to an end. I hope that you have found the book useful—or at least interesting—and have learned something new from it (I myself did learn a lot while I was writing it).\n\nMostofthecoveredinformationislikelytoremainup-to-dateforquitealongtime, but some details will inevitably change very fast. I believe that the biggest value of this book is not a set of particular facts but rather the approach to exploring the system that I show. Neither this book nor the documentation should be taken for granted. Contemplate, experiment, verify all the facts yourself: Postgre��� provides all the tools that you need for it, and I tried to show how to use them. It is usually almost as easy as asking a question on a forum or googling the answer, but is definitely much more reliable and useful.\n\nFor the same reason, I wanted to encourage you to look into the code. Do not get intimidated by its complexity: simply try it out. Open source is a great advantage, so take this opportunity.\n\nI will be happy to get your feedback; you can send your comments and ideas to edu@postgrespro.ru. I am going to update the book regularly, so your comments can help me improve future editions. The latest online version of the book is avail- able for free at postgrespro.com/community/books/internals.\n\nGood luck!\n\n618",
      "content_length": 1338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 620,
      "content": "Index\n\nA Aborting transactions 84, 88, 91,\n\n251, 271\n\nAccess method\n\nindex 356, 417 properties 366 table 335 Aggregate 340–341 Aggregation 340, 345 hashing 439, 459 sorting 459\n\nAlignment 75 Analysis 128, 311, 389, 470 Anomaly\n\ndirty read 46, 48, 52 lost update 48, 58, 60 non-repeatable read 49, 54, 61 phantom read 49, 61, 270 read skew 56, 58, 62 read-only transaction 65, 68,\n\n270\n\nwrite skew 64, 67, 270\n\nAppend 440 Array 538, 582, 590 “Asterisk,” the reasons not to use it\n\n37, 421, 455\n\nAtomicity 47, 91 autoprewarm leader 187–189 autoprewarm worker 189 autosummarize 597 autovacuum 129 autovacuum launcher 129–131\n\nautovacuum worker 130\n\nautovacuum_analyze_scale_factor\n\n133\n\nautovacuum_analyze_threshold 133\n\nautovacuum_enabled 121, 131\n\nautovacuum_freeze_max_age 149,\n\n154–155\n\nautovacuum_freeze_min_age 155\n\nautovacuum_freeze_table_age 155\n\nautovacuum_max_workers 130, 139,\n\n144\n\nautovacuum_multix-\n\nact_freeze_max_age 247\n\nautovacuum_naptime 130–131\n\nautovacuum_vacuum_cost_delay 139,\n\n144, 155\n\nautovacuum_vacuum_cost_limit 139,\n\n144\n\nautovacuum_vacuum_in- sert_scale_factor 132–133\n\nautovacuum_vacuum_insert_thresh-\n\nold 132–133\n\nautovacuum_vacuum_scale_factor\n\n131–132\n\nautovacuum_vacuum_threshold\n\n131–132\n\nautovacuum_work_mem 130\n\nautovacuum_freeze_max_age 154\n\n619",
      "content_length": 1282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 621,
      "content": "Index\n\nB Backend 39 Background worker 127, 130, 347 Background writing 205\n\nsetup 208\n\nBatch processing 166, 257 bgwriter 205, 208–210, 224 bgwriter_delay 208 bgwriter_lru_maxpages 208, 210 bgwriter_lru_multiplier 208 Binding 305 Bison 290 Bitmap 388\n\nNULL values 75\n\nBitmap Heap Scan 330, 388, 392, 394 Bitmap Index Scan 330, 388, 392,\n\n394, 397\n\nBitmapAnd 391 Bloating 105, 119, 165, 339, 475, 486, 491, 509\n\nBlock see page bloom 614 Bloom filter 532, 613 Box plot 603 BRIN 591\n\nefficiency factor 602 operator class 596, 598, 608,\n\n611–612, 614\n\npages 594 properties 604\n\nB-tree 481, 524, 563, 566, 584\n\noperator class 493, 568, 599 pages 486 properties 504\n\nbtree_gin 584 btree_gist 524, 538\n\n620\n\nBuffer cache 38, 171, 192, 198, 277,\n\n338, 357, 381 configuration 184 eviction 179 local 189, 355 Buffer pin 173, 175, 278 Buffer ring 181, 338\n\nC Cardinality 300, 310, 381\n\njoin 406\n\nCartesian product 399, 401 Checkpoint 198, 216 monitoring 208 setup 205\n\ncheckpoint_completion_target 205–206\n\ncheckpointer 198–199, 204, 206, 208–210, 216\n\ncheckpoint_timeout 206, 209 checkpoint_warning 208 client_encoding 580 CLOG 81, 155, 192, 195, 198 Cluster 23 Cmin and cmax 101 Collation 361, 495, 557 Combo-identifier 101 Commit 81, 195, 251\n\nasynchronous 212 synchronous 212\n\ncommit_delay 212 commit_siblings 212 Consistency 45, 47 Correlated predicates 301, 330 Correlation 325, 376, 387, 591, 600,\n\n611\n\nCost 295, 299, 302",
      "content_length": 1418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 622,
      "content": "cpu_index_tuple_cost 378 cpu_operator_cost 340, 378, 423, 445,\n\n450, 459\n\ncpu_tuple_cost 339, 341, 379, 411,\n\n423, 445, 450\n\nCTE Scan 352–353 CTID 75, 112 cube 537 Cursor 100, 176, 300, 308, 351 cursor_tuple_fraction 300, 308\n\nD Database 23 data_checksums 217 Deadlocks 232, 258, 267–268 deadlock_timeout 259, 267, 280 debug_print_parse 291 debug_print_plan 294 debug_print_rewritten 292 deduplicate_items 491 Deduplication 491, 563 default_statistics_target 311,\n\n319–320, 323, 334\n\ndefault_table_access_method 335 default_text_search_config 530 default_transaction_isolation 70 Demo database 287, 509, 592 Dirty read 48, 52 Durability 47\n\nE effective_cache_size 381–382 effective_io_concurrency 388 enable_bitmapscan 380 enable_hashjoin 445, 447 enable_memoize 412 enable_mergejoin 413\n\nIndex\n\nenable_parallel_hash 431, 434 enable_seqscan 262, 380, 582, 612 Equi-join 399, 437, 444, 469 Eviction 179, 194, 205 Execution 302, 306\n\nF false_positive_rate 615 fastupdate 268, 574 fdatasync 216 fillfactor 108–109, 115–116, 147,\n\n150, 158, 271, 471, 509, 543, 607 Finalize Aggregate 346 Finalize GroupAggregate 461 Flex 290 force_parallel_mode 351 Foreign keys 242, 244, 406 Fork 28\n\nfree space map 30, 108, 120 initialization 30 main 29, 74, 600 visibility map 31, 108, 149–150,\n\n163, 384 Freezing 146, 162, 177, 246\n\nmanual 155\n\nfrom_collapse_limit 296, 298 fsync 216 Full page image 202 full_page_writes 219, 221 Full-text search 527\n\nindexing 529, 564 partial 570 phrase 579 ranking 579\n\nfuzzystrmatch 562\n\n621",
      "content_length": 1511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 623,
      "content": "Index\n\nG Gather 342, 344–346, 352, 458 Gather Merge 458–459 geqo 298 geqo_threshold 298 Getting the result 308 gevel 512, 547 GIN 563\n\ndeferred update 267 fast update 574 operator class 564, 568, 582,\n\n585 pages 566 properties 577 gin_fuzzy_search_limit 576 gin_pending_list_limit 575 GiST 507, 610\n\noperator class 508, 512, 612 pages 511 properties 525, 536\n\nGroupAggregate 461 Grouping 439, 460\n\nH Hash 419, 422, 427, 469 operator class 476 page 470 properties 477 Hash Join 419, 422, 427 Hash table 174, 277, 279, 410, 419,\n\n469\n\nHashAggregate 439–440 hash_mem_multiplier 410, 420, 433,\n\n440\n\nHeader\n\npage 72, 122 row version 75\n\n622\n\ntuple 241\n\nHigh key 487, 489, 566 Hint bits see information bits Histogram 320 Horizon 102–103, 108, 123, 165, 385 HOT updates 112, 491 hstore 539, 590\n\nI idle_in_transaction_session_timeout\n\n166\n\nignore_checksum_failure 218 Incremental Sort 456 Index 356, 362\n\ncovering 370, 383, 386, 504, 552 include 525 integrity constraint 368, 370, 478, 504, 522, 525, 552\n\nmulticolumn 369, 499,\n\n503–504, 525, 578, 605 on expression 328, 363, 612 ordering 367, 372, 481, 495,\n\n499, 503–504\n\npartial 374 pruning 118, 486, 491 statistics 328 unique 242, 368, 370, 483, 491,\n\n504\n\nvacuuming 475, 575 versioning 86\n\nIndex Only Scan 383 Index Scan 375–377, 380, 406, 408 Indexing engine 357, 366 Information bits 75, 79, 82, 95, 219,\n\n241\n\nInitPlan 316, 354 Instance 23",
      "content_length": 1392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 624,
      "content": "intarray 538, 590 Integrity constraints 45 Isolation 47\n\nsnapshot 51, 67, 94, 241\n\nJ Join\n\nanti- and semi- 400, 414 cost estimation 402, 408, 411,\n\n422, 429, 444, 449, 451, 454, 457–458\n\ndifferent methods 462 hashing 419, 424 inner 399 merging 442, 499 nested loop 400 order 294, 296, 421, 444 outer 399, 413, 444 parallel hash 432, 434 parameterized 405 join_collapse_limit 296–298 JSON 585, 590 jsquery 590\n\nK k-D tree 554\n\nL Locks 50, 229, 357 advisory 268 escalation 241, 273 heavyweight 231, 242 lightweight 277 memory 173 no waits 166, 256 non-relation 265 page 267\n\nIndex\n\npredicate 270 queue 237, 247, 253 relation 128, 159, 164, 224, 234 relation extension 267 row 167, 241 spinlocks 276 tranche 278 transaction ID 233 tuple 247\n\nlock_timeout 257–258 log_autovacuum_min_duration 143 log_checkpoints 208 logical 222, 226 log_lock_waits 280 log_temp_files 427, 454 Lost update 48, 58, 60 ltree 538\n\nM maintenance_io_concurrency 389 maintenance_work_mem 126, 140,\n\n143, 575\n\nMap\n\nbitmap 470, 591, 596, 605 free space 30, 108, 120 freeze 31, 149, 152, 163 visibility 31, 108, 149–150, 163,\n\n384\n\nMaterialization 352, 402, 409 Materialize 402, 404–405, 409–411,\n\n413\n\nmax_connections 232, 273 max_locks_per_transaction 232 max_parallel_processes 187 max_parallel_workers 347 max_parallel_workers_per_gather\n\n347–349\n\n623",
      "content_length": 1324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 625,
      "content": "Index\n\nmax_pred_locks_per_page 273 max_pred_locks_per_relation 274 max_pred_locks_per_transaction\n\n273–274\n\nmax_wal_senders 222 max_wal_size 206, 209 max_worker_processes 130, 347 Memoize 409–412, 463 Merge Join 442 Merging 442, 452, 458 minimal 216, 222, 224–225 min_parallel_index_scan_size 127 min_parallel_table_scan_size 348 min_wal_size 207 MixedAggregate 461 Multitransactions 245\n\nwraparound 246\n\nMultiversion concurrency control 52, 74, 119, 491\n\nN n_distinct_per_range 614 Nearest neighbor search 372, 517,\n\n527, 554\n\nNested Loop 295, 400–401, 406, 411 Nested Loop Anti Join 415 Nested Loop Left Join 400, 413 Nested Loop Semi Join 416 Non-repeatable read 49, 54, 61 Non-uniform distribution 317, 427,\n\n471\n\nNULL 75, 314, 373, 503, 506, 526,\n\n553, 606\n\nO OID 24 old_snapshot_threshold 166\n\n624\n\nOperator class 359, 417, 564\n\nparameters 534, 609, 614 support functions 364\n\nOperator family 364 Optimization see planning\n\nP Page 32, 470 dirty 172 fragmentation 75, 110 full image 202 header 157, 163 prefetching 388 split 118, 484–485, 521, 550,\n\n574\n\npageinspect 72, 76, 80, 86, 148, 194, 243, 470, 511, 547, 566, 595\n\npages_per_range 591 Parallel Bitmap Heap Scan 397 Parallel execution 342, 347, 395,\n\n417, 431, 445, 458, 461\n\nlimitations 350\n\nParallel Hash 433 Parallel Hash Join 433 Parallel Index Only Scan 432 Parallel Seq Scan 343–344 parallel_leader_participation 342,\n\n344\n\nparallel_setup_cost 345, 458 parallel_tuple_cost 345, 459 parallel_workers 348 Parsing 290 Partial Aggregate 345 Partial GroupAggregate 461 pgbench 214, 219, 282 pg_buffercache 173, 185 pg_checksums 217",
      "content_length": 1594,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 626,
      "content": "pg_controldata 201 PGDATA 23 pg_dump 106 pg_prewarm 187 pg_prewarm.autoprewarm 187 pg_prewarm.autoprewarm_interval\n\n187\n\npg_rewind 193 pgrowlocks 246, 263 pgstattuple 159–160 pg_test_fsync 216 pg_trgm 539, 562, 580 pg_visibility 122, 149 pg_wait_sampling 282 pg_wait_sampling.profile_period 283 pg_waldump 197, 204, 223 Phantom read 49, 61, 270 Plan 294\n\ngeneric and custom 306\n\nplan_cache_mode 308 Planning 294, 306 Pointers to tuples 74 Portal 302 postgres 37 postmaster 37–39, 130, 201, 203, 342 Preparing a statement 304 ProcArray 82, 96 Process 37 Protocol 40\n\nextended query 304 simple query 290\n\nPruning 108, 115, 118, 486, 491 psql 17, 20, 24, 92–93, 281, 287\n\nQ Quadtree 541\n\nIndex\n\nR random_page_cost 339, 380, 392 RD-tree 530 Read Committed 49, 51–54, 56, 58,\n\n61–62, 70–71, 94, 102, 104, 106, 123, 251 Read skew 56, 58, 62 Read Uncommitted 48–49, 51–52 Read-only transaction anomaly 65,\n\n68, 270\n\nRecheck 357, 375, 390, 529, 536,\n\n553, 583\n\nRecovery 201 Relation 27 Repeatable Read 49, 51–52, 61–62, 64–65, 67, 69–71, 94, 103, 106, 156, 251, 271\n\nreplica 222, 224–226 Rewriting see transformation Row version see tuple RTE 291 R-Tree 509 Rule system 292 RUM 579\n\nS Savepoint 88 Scan\n\nbitmap 371, 387, 479, 505, 526,\n\n552, 576, 606\n\ncost estimation 338, 343, 376,\n\n384, 391, 611\n\nindex 272, 371, 375, 505, 526,\n\n552\n\nindex-only 312, 373, 383, 479,\n\n527, 533, 553, 561 method comparison 397\n\n625",
      "content_length": 1405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 627,
      "content": "Index\n\nparallel index 395 parallel sequential 343 sequential 271, 337 skip 501\n\nSchema 25 search_path 25 seg 538 Segment 28, 196 Selectivity 300, 338\n\njoin 406\n\nSeq Scan 295, 338, 340–341, 354 seq_page_cost 339, 380, 392, 430 Serializable 50–51, 67, 69–71, 94,\n\n103, 106, 251, 270–271, 274, 351\n\nServer 23 shared_buffers 184 shared_preload_libraries 187, 282 Signature 532, 614 slowfs 283 Snapshot 94, 97, 224 export 106 system catalog 105 Sort 447–448, 450, 459, 462 Sorting 372, 442, 447, 481, 499, 503\n\nexternal 452 heapsort 450 incremental 456 parallel 458 quicksort 449\n\nSpecial space 73 SP-GiST 540\n\noperator class 541, 543, 557 pages 547 properties 552, 561\n\nSplit\n\n626\n\nbucket 469, 472 page 484–485, 521, 550, 574\n\nstartup 201–203 Starvation 247, 253 statement_timeout 258 Statistics 128, 300 basic 310, 384 correlation 325, 377, 600, 611 distinct values 315, 331, 601 expression 326, 334 extended 327 field width 325 histogram 320, 444 most common values 317, 333,\n\n407, 427 multivariate 329 non-scalar data types 324 NULL fraction 314\n\nSubPlan 353–354 Subtransaction 88, 198 Support functions 364 Synchronization 212, 216 synchronous_commit 211–213 System catalog 24, 224, 291\n\nT Tablespace 26 temp_buffers 190 temp_file_limit 190, 425 Tid Scan 376 Timeline 196 TOAST 25, 32, 87, 182 track_commit_timestamp 96 track_counts 129 track_io_timing 179 Transaction 46, 78, 94\n\nabort 84, 88, 91, 251, 271",
      "content_length": 1407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 628,
      "content": "age 145 commit 81, 195, 212, 251 status 96, 195 subtransaction 88, 198 virtual 87, 233\n\nTransaction ID\n\nlock 233 wraparound 145, 153\n\nTransformation 291 Tree\n\nbalanced 482, 486, 507, 563 non-balanced 540 parse 290 plan 294 radix 556 signature 532\n\nTrigrams 580 Truncation\n\nheap 128 suffix 493\n\nTuple 74\n\ninsert only 127, 132\n\nTuple ID 74, 356, 564 Tuple pointer 110\n\nU Unique 460\n\nV Vacuum 103, 176, 311, 357, 385,\n\n475, 598 aggressive 151 autovacuum 129, 259 full 158 monitoring 140, 161 phases 126 routine 120\n\nIndex\n\nvacuum_cost_delay 138, 155 vacuum_cost_limit 138–139 vacuum_cost_page_dirty 138 vacuum_cost_page_hit 138 vacuum_cost_page_miss 138 vacuum_failsafe_age 149, 155 vacuum_freeze_min_age 149–150,\n\n152, 156\n\nvacuum_freeze_table_age 149,\n\n151–152 vacuum_index_cleanup 156 vacuum_multixact_failsafe_age 247 vacuum_multixact_freeze_min_age\n\n247\n\nvacuum_multixact_freeze_table_age\n\n247 vacuum_truncate 128 vacuum_freeze_min_age 150 values_per_range 609 Virtual transaction 87 Visibility 95, 100, 337, 357, 375, 384 Volatility 57, 364, 374\n\nW Wait-for graph 258 Waits 280\n\nsampling 282 unaccounted-for time 281, 283\n\nWAL see write-ahead log wal_buffers 193 wal_compression 219 wal_keep_size 208 wal_level 222 wal_log_hints 219 wal_recycle 207 wal_segment_size 196 walsender 211, 222\n\n627",
      "content_length": 1296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 629,
      "content": "Index\n\nwal_skip_threshold 222–223\n\nwal_sync_method 216\n\nwalwriter 212–213\n\nwal_writer_delay 212–213\n\nwal_writer_flush_after 213\n\nWindowAgg 448\n\nwork_mem 19, 303, 389–391, 393,\n\n402, 410, 420, 427, 431, 433,\n\n628\n\n440, 449, 461 Write skew 64, 67, 270 Write-ahead log 39, 191, 279, 336,\n\n357 levels 221\n\nX Xmin and xmax 75, 77, 81, 83, 95,\n\n145, 241, 246",
      "content_length": 352,
      "extraction_method": "Unstructured"
    }
  ]
}