{
  "metadata": {
    "title": "Advanced Applied Deep Learning",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 294,
    "conversion_date": "2025-12-25T18:10:27.671232",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Advanced Applied Deep Learning.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "Advanced Applied Deep Learning\n\nConvolutional Neural Networks and Object Detection — Umberto Michelucci\n\nAdvanced Applied Deep Learning\n\nConvolutional Neural Networks and Object Detection\n\nUmberto Michelucci\n\nAdvanced Applied Deep Learning: Convolutional Neural Networks and Object Detection\n\nUmberto Michelucci TOELT LLC, Dübendorf, Switzerland\n\nISBN-13 (pbk): 978-1-4842-4975-8 https://doi.org/10.1007/978-1-4842-4976-5\n\nISBN-13 (electronic): 978-1-4842-4976-5\n\nCopyright © 2019 by Umberto Michelucci\n\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.\n\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every occurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion and to the benefit of the trademark owner, with no intention of infringement of the trademark.\n\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.\n\nWhile the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.\n\nManaging Director, Apress Media LLC: Welmoed Spahr Acquisitions Editor: Celestin Suresh John Development Editor: Matthew Moodie Coordinating Editor: Aditee Mirashi\n\nCover designed by eStudioCalamar\n\nCover image designed by Freepik (www.freepik.com)\n\nDistributed to the book trade worldwide by Springer Science+Business Media New York, 233 Spring Street, 6th Floor, New York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail orders-ny@springer-sbm.com, or visit www.springeronline.com. Apress Media, LLC is a California LLC and the sole member (owner) is Springer Science + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware corporation.\n\nFor information on translations, please e-mail rights@apress.com, or visit http://www.apress. com/rights-permissions.\n\nApress titles may be purchased in bulk for academic, corporate, or promotional use. eBook versions and licenses are also available for most titles. For more information, reference our Print and eBook Bulk Sales web page at http://www.apress.com/bulk-sales.\n\nAny source code or other supplementary material referenced by the author in this book is available to readers on GitHub via the book’s product page, located at www.apress.com/978-1-4842-4975-8. For more detailed information, please visit http://www.apress.com/source-code.\n\nPrinted on acid-free paper\n\nThis book is dedicated to my wife Francesca and daughter Caterina, who always show me how important it is to have dreams and to follow them.\n\nTable of Contents\n\nAbout the Author ���������������������������������������������������������������������������������xi\n\nAbout the Technical Reviewer �����������������������������������������������������������xiii\n\nAcknowledgments ������������������������������������������������������������������������������xv\n\nIntroduction ��������������������������������������������������������������������������������������xvii\n\nChapter 1: Introduction and Development Environment Setup�������������1\n\nGitHub Repository and Companion Website ����������������������������������������������������������3\n\nMathematical Level Required �������������������������������������������������������������������������������3\n\nPython Development Environment ������������������������������������������������������������������������4\n\nGoogle Colab ����������������������������������������������������������������������������������������������������5\n\nAnaconda ���������������������������������������������������������������������������������������������������������9\n\nDocker Image ������������������������������������������������������������������������������������������������18\n\nWhich Option Should You Choose?����������������������������������������������������������������������25\n\nChapter 2: TensorFlow: Advanced Topics �������������������������������������������27\n\nTensorflow Eager Execution ��������������������������������������������������������������������������������28\n\nEnabling Eager Execution ������������������������������������������������������������������������������29\n\nPolynomial Fitting with Eager Execution �������������������������������������������������������30\n\nMNIST Classification with Eager Execution ���������������������������������������������������34\n\nTensorFlow and Numpy Compatibility �����������������������������������������������������������������39\n\nHardware Acceleration ����������������������������������������������������������������������������������������40\n\nChecking the Availability of the GPU ��������������������������������������������������������������40\n\nDevice Names ������������������������������������������������������������������������������������������������41\n\nv\n\nTable of ConTenTs Table of ConTenTs\n\nExplicit Device Placement �����������������������������������������������������������������������������42\n\nGPU Acceleration Demonstration: Matrix Multiplication ��������������������������������43\n\nEffect of GPU Acceleration on the MNIST Example ����������������������������������������45\n\nTraining Only Specific Layers ������������������������������������������������������������������������������47\n\nTraining Only Specific Layers: An Example ����������������������������������������������������48\n\nRemoving Layers �������������������������������������������������������������������������������������������52\n\nKeras Callback Functions ������������������������������������������������������������������������������������54\n\nCustom Callback Class ����������������������������������������������������������������������������������55\n\nExample of a Custom Callback Class ������������������������������������������������������������57\n\nSave and Load Models ����������������������������������������������������������������������������������������61\n\nSave Your Weights Manually ��������������������������������������������������������������������������67\n\nSaving the Entire Model ��������������������������������������������������������������������������������68\n\nDataset Abstraction ���������������������������������������������������������������������������������������������68\n\nIterating Over a Dataset ���������������������������������������������������������������������������������71\n\nSimple Batching ��������������������������������������������������������������������������������������������72\n\nSimple Batching with the MNIST Dataset ������������������������������������������������������73\n\nUsing tf�data�Dataset in Eager Execution Mode ���������������������������������������������76\n\nConclusions ���������������������������������������������������������������������������������������������������������77\n\nChapter 3: Fundamentals of Convolutional Neural Networks �������������79\n\nKernels and Filters ����������������������������������������������������������������������������������������������79\n\nConvolution ���������������������������������������������������������������������������������������������������������81\n\nExamples of Convolution �������������������������������������������������������������������������������������91\n\nPooling ����������������������������������������������������������������������������������������������������������������99\n\nPadding �������������������������������������������������������������������������������������������������������104\n\nBuilding Blocks of a CNN ����������������������������������������������������������������������������������105\n\nConvolutional Layers �����������������������������������������������������������������������������������105\n\nPooling Layers ���������������������������������������������������������������������������������������������108\n\nStacking Layers Together ����������������������������������������������������������������������������108\n\nvi\n\nTable of ConTenTs Table of ConTenTs\n\nNumber of Weights in a CNN �����������������������������������������������������������������������������109\n\nConvolutional Layer �������������������������������������������������������������������������������������109\n\nPooling Layer �����������������������������������������������������������������������������������������������110\n\nDense Layer �������������������������������������������������������������������������������������������������110\n\nExample of a CNN: MNIST Dataset ��������������������������������������������������������������������110\n\nVisualization of CNN Learning ���������������������������������������������������������������������������115\n\nBrief Digression: keras�backend�function( ) �������������������������������������������������115\n\nEffect of Kernels ������������������������������������������������������������������������������������������118\n\nEffect of Max-Pooling ����������������������������������������������������������������������������������121\n\nChapter 4: Advanced CNNs and Transfer Learning ���������������������������125\n\nConvolution with Multiple Channels ������������������������������������������������������������������125\n\nHistory and Basics of Inception Networks ��������������������������������������������������������129\n\nInception Module: Naïve Version �����������������������������������������������������������������131\n\nNumber of Parameters in the Naïve Inception Module ��������������������������������132\n\nInception Module with Dimension Reduction ����������������������������������������������133\n\nMultiple Cost Functions: GoogLeNet �����������������������������������������������������������������134\n\nExample of Inception Modules in Keras ������������������������������������������������������������136\n\nDigression: Custom Losses in Keras �����������������������������������������������������������������139\n\nHow To Use Pre-Trained Networks ��������������������������������������������������������������������141\n\nTransfer Learning: An Introduction ��������������������������������������������������������������������145\n\nA Dog and Cat Problem �������������������������������������������������������������������������������������149\n\nClassical Approach to Transfer Learning �����������������������������������������������������150\n\nExperimentation with Transfer Learning ������������������������������������������������������157\n\nChapter 5: Cost Functions and Style Transfer �����������������������������������161\n\nComponents of a Neural Network Model ����������������������������������������������������������161\n\nTraining Seen as an Optimization Problem ��������������������������������������������������162\n\nA Concrete Example: Linear Regression ������������������������������������������������������164\n\nvii\n\nTable of ConTenTs Table of ConTenTs\n\nThe Cost Function ���������������������������������������������������������������������������������������������165\n\nMathematical Notation ��������������������������������������������������������������������������������165\n\nTypical Cost Functions ���������������������������������������������������������������������������������166\n\nNeural Style Transfer �����������������������������������������������������������������������������������������176\n\nThe Mathematics Behind NST ���������������������������������������������������������������������178\n\nAn Example of Style Transfer in Keras ���������������������������������������������������������183\n\nNST with Silhouettes �����������������������������������������������������������������������������������190\n\nMasking �������������������������������������������������������������������������������������������������������192\n\nChapter 6: Object Classification: An Introduction �����������������������������195\n\nWhat Is Object Localization? �����������������������������������������������������������������������������196\n\nMost Important Available Datasets ��������������������������������������������������������������199\n\nIntersect Over Union (IoU) ����������������������������������������������������������������������������200\n\nA Naïve Approach to Solving Object Localization (Sliding Window Approach) ��202\n\nProblems and Limitations the with Sliding Window Approach ��������������������204\n\nClassification and Localization ��������������������������������������������������������������������������211\n\nRegion-Based CNN (R-CNN) ������������������������������������������������������������������������������213\n\nFast R-CNN �������������������������������������������������������������������������������������������������������217\n\nFaster R-CNN ����������������������������������������������������������������������������������������������������219\n\nChapter 7: Object Localization: An Implementation in Python ���������221\n\nThe You Only Look Once (YOLO) Method �����������������������������������������������������������222\n\nHow YOLO Works �����������������������������������������������������������������������������������������223\n\nYOLOv2 (Also Known As YOLO9000)�������������������������������������������������������������226\n\nYOLOv3 ��������������������������������������������������������������������������������������������������������227\n\nNon-Maxima Suppression ���������������������������������������������������������������������������228\n\nLoss Function ����������������������������������������������������������������������������������������������228\n\nviii",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-17)",
      "start_page": 9,
      "end_page": 17,
      "detection_method": "topic_boundary",
      "content": "Table of ConTenTs Table of ConTenTs\n\nYOLO Implementation in Python and OpenCV ���������������������������������������������������231\n\nDarknet Implementation of YOLO �����������������������������������������������������������������231\n\nTesting Object Detection with Darknet ��������������������������������������������������������233\n\nTraining a Model for YOLO for Your Specific Images �����������������������������������������240\n\nConcluding Remarks �����������������������������������������������������������������������������������������241\n\nChapter 8: Histology Tissue Classification ���������������������������������������243\n\nData Analysis and Preparation ��������������������������������������������������������������������������244\n\nModel Building ��������������������������������������������������������������������������������������������������253\n\nData Augmentation �������������������������������������������������������������������������������������������264\n\nHorizontal and Vertical Shifts ����������������������������������������������������������������������266\n\nFlipping Images Vertically ����������������������������������������������������������������������������269\n\nRandomly Rotating Images ��������������������������������������������������������������������������269\n\nZooming in Images ��������������������������������������������������������������������������������������272\n\nPutting All Together��������������������������������������������������������������������������������������272\n\nVGG16 with Data Augmentation ������������������������������������������������������������������������273\n\nThe fit( ) Function �����������������������������������������������������������������������������������������273\n\nThe fit_generator( ) Function �����������������������������������������������������������������������274\n\nThe train_on_batch( ) Function ��������������������������������������������������������������������275\n\nTraining the Network �����������������������������������������������������������������������������������276\n\nAnd Now Have Fun… ����������������������������������������������������������������������������������������277\n\nIndex �������������������������������������������������������������������������������������������������279\n\nix\n\nAbout the Author\n\nUmberto Michelucci studied physics and mathematics. He is an expert in numerical simulation, statistics, data science, and machine learning. Over the years, he has continuously expanded his expertise in post-graduate courses and research projects. In addition to several years of research experience at George Washington University (USA) and the University of Augsburg (DE), he has 15 years of practical experience in data warehouse, data science, and machine learning. He is currently responsible for Deep Learning, New Technologies, and Research Cooperation at Helsana Versicherung AG. In 2014, he completed a postgraduate certificate in professional studies in education in England to broaden his knowledge of teaching and pedagogy. He is the author of Applied Deep Learning: A Case-Based Approach to Understanding Deep Neural Networks, published by Springer in 2018. He regularly publishes his research results in leading journals and gives lectures at international conferences. He is also a founder of TOELT llc, a company focusing on research in AI in science.\n\nxi\n\nAbout the Technical Reviewer\n\nJojo Moolayil is an artificial intelligence professional. He has authored three books on machine learning, deep learning, and IoT. He is currently working with Amazon Web Services as a Research Scientist – A.I. in their Vancouver, BC office.\n\nHe was born and raised in Pune, India and\n\ngraduated from the University of Pune with a major in Information Technology Engineering. His passion for problem- solving and data- driven decision led him to start a career with Mu Sigma Inc., the world’s largest pure-play analytics provider. He was responsible for developing machine learning and decision science solutions to large complex problems for Healthcare and Telecom giants. He later worked with Flutura (an IoT analytics startup) and General Electric, with a focus on Industrial A.I in Bangalore, India.\n\nIn his current role with AWS, he works on researching and developing large-scale A.I. solutions for combating fraud and enriching the customer payment experience in the cloud. He is also actively involved as a tech reviewer and AI consultant with leading publishers and has reviewed over a dozen books on machine learning, deep learning, and business analytics.\n\nYou can reach out to Jojo at\n\n\n\nhttps://www.jojomoolayil.com/\n\n\n\nhttps://www.linkedin.com/in/jojo62000\n\n\n\nhttps://twitter.com/jojo62000\n\nxiii\n\nAcknowledgments\n\nWriting this second book on more advanced topics has been a challenge. Finding the right level—finding the right parts to discuss and the right ones to leave out—caused me a few sleepless nights. This would not have been possible without several people who gave me feedback and discussed the chapters with me. I need to thank my editors, from Aditee to Matt, and especially my technical editor Jojo, who read and tried all the code. What patience. The team at Apress has been great. Thanks go to Celestin John, the acquisitions editor, who believed in me. Thanks to everyone for everything; you are great.\n\nOf course, a big thank you goes to my family, who put up with me spending time at the computer writing, testing code, writing more, testing more, and so on and so forth. Thanks to my wife Francesca for her endless patience. I don’t know how she put up with me. I really don’t. To my daughter Caterina goes a special thank you. She shows me everyday how great it is to love something and do it just for fun, and how important it is not too take yourself too seriously.\n\nA special thanks to all the readers who invested part of their lives reading what I wrote. I am really flattered that you are doing that. I would love to hear from you, so get in touch. You can get in touch with me at umberto.michelucci@toelt.ai, or you use the GitHub repository to get in touch with me by opening an issue. Really, do that. I look forward to hearing from you.\n\nxv\n\nIntroduction\n\nThis is the second book I have written, and it covers advanced topics in deep learning. It will require some knowledge to be understood. It’s not for beginners. If you are one, I suggest you check out my first book published by Apress (Applied Deep Learning: A Case-Based Approach, ISBN 978-1-4842-3790-8). To understand this book, you should have some intermediate to advanced experience in Python and some intermediate to advanced deep learning experience (and experience with neural networks in general). This book assumes you know things like regularization, hyperparameter tuning, mini-batch gradient descent, which optimizers are more efficient (does the name Adam tell you something?), and so on. I also use heavily Keras (from TensorFlow), so I suggest you get some experience with that too. It will help you work through the examples of the book.\n\nI tried to tackle advanced topics, like transfer learning or multi-loss function networks, with a practical approach. That means I explain the concepts and then show you how to implement those things in Keras. I invested quite some time in preparing code for you and it’s available on the GitHub repository for the book, so get the code and use it while you’re reading the book. The code for the advanced topics we deal with, is too long to discuss completely, so I only dissect the most important parts. In GitHub, you have all of it.\n\nThis book touches on several research fields, but was not written for very experienced researchers. It has been written for practitioners who want to start doing research; therefore, its goal is to bridge the gap between the beginner and the researcher. Very advanced topics, like in object detection, are not explained in much technical detail, since otherwise the book would turn into a collection of research papers.\n\nxvii\n\nInTroduCTIon InTroduCTIon\n\nKeep in mind that many of the things I describe in the book, like the YOLO object detection algorithm, are only a few years old. For advanced topics, the only way to understand an algorithm is to read the original paper. You should get used to doing that without any book. Here, I try to give you the tools and explain the language that you need to read the research papers. From there, you are on your own.\n\nIf you want to proceed further in your deep learning adventure, you\n\nshould get used to reading research papers. They are not easy to read and will require time. But this book should give you many tools and tips for a head start. Reading this book and understanding all of it will put you at the start of your research career. From there on, start reading research papers. Try to repeat what they did if possible (mostly is not, given the infrastructure needed for deep learning, but you can always try). Understanding algorithms and research papers will give you enough knowledge to evaluate libraries and see what others have done, if you are searching for a way to use a specific algorithm in your projects.\n\nI hope you enjoy the book, that you learn something from it, and that it\n\nhelps you, but more important than anything—I hope you have fun!\n\n—Umberto Michelucci, Zürich, 3rd of July 2019\n\nxviii\n\nCHAPTER 1\n\nIntroduction and Development Environment Setup\n\nThis book assumes that you have some basic know-how in machine learning, neural networks, and TensorFlow.1 It follows my first book, Applied Deep Learning: A Case-Based Approach (ISBN 978-1-4842-3790-8), published by Apress in 2018, and assumes you know and understand what is explained in there. The first volume’s goal is to explain the basic concepts of neural networks and to give you a sound basis in deep learning, and this book’s goal is to explain more advanced topics, like convolutional and recurrent neural networks. To be able to profit from this book, you should have at least a basic knowledge of the following topics:\n\nHow a single neuron and its components work (activation functions, inputs, weights, and bias)\n\nHow to develop a simple neural network with several\n\nlayers in Python with TensorFlow or Keras\n\n1 TensorFlow, the TensorFlow logo, and any related marks are trademarks of Google Inc.\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_1\n\n1\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nWhat an optimizer is and how it works (at least you\n\nshould know how gradient descent works)\n\nWhich advanced optimizers are available and how they\n\nwork (at least RMSProp, Momentum, and Adam)\n\nWhat regularization is and what the most common\n\nmethods are (ℓ1, ℓ2, and dropout)\n\nWhat hyperparameters are\n\nHow to train a network and which hyper-parameters\n\nplay an essential role (for example, the learning rate or the number of epochs)\n\nWhat hyperparameter tuning is and how to do it\n\nIn the next chapters, we switch freely between low-level TensorFlow\n\nAPIs and Keras (introduced in the next chapter) where needed, to be able to concentrate on the more advanced concepts and not on implementation details. We will not discuss why a specific optimizer works better or how neurons work. If any of that is unclear, you should keep my first book close and use it as a reference.\n\nAdditionally, not all the Python code in the book is discussed as extensively as in my first book. You should already understand Python code well. However, all the new concepts are explained. If you have a sound basis, you will understand very well what is going on (and why). This book is not for beginners of deep learning. If you are one, I suggest buying my first book and studying it before starting this one.\n\nI hope that the book will be enjoyable and that you will learn a lot from\n\nit. But most of all, I hope it will be fun.\n\n2\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nGitHub Repository and Companion Website\n\nThe Jupyter Notebooks related to the code I discuss in this book are found on GitHub.2 To find the link to them, go to the Apress web page for this book. Near the cover of the book, a button with the text “Download Code” can be found. It points to the GitHub repository. The notebooks contain specific topics discussed in the book, including exercises of additional material that did not fit in the book. It is even possible to leave feedback directly on GitHub using “Issues” (see https://goo.gl/294qg4 to learn how). It would be great to hear from you. The GitHub repository acts as a companion to the book, meaning it contains more code than is printed in the book. If you are a teacher, I hope you can use these notebooks for your students. The notebooks are the same ones I use in my university courses, and much work has gone into making them useful for teaching.\n\nThe best way to learn is to try. Don’t merely read the book: try, play\n\nwith the code, change it, and apply it to concrete problems.\n\nA companion website is also available, where news about\n\nthe book and additional useful material is found. Its URL is www.applieddeeplearningbook.com.\n\nMathematical Level Required\n\nThere are a few sections that are more mathematically advanced. You should understand most of these concepts without the mathematical details. However, it is essential to know what a matrix is, how to multiply matrices, what a transpose is, and so on. You basically need a sound grasp of linear algebra. If that is not the case, I suggest reviewing a linear algebra book before reading this book. A basic understanding of calculus is also\n\n2 In case you don’t know what GitHub is, you can learn the basics with this guide at https://guides.github.com/activities/hello-world/\n\n3",
      "page_number": 9
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 18-26)",
      "start_page": 18,
      "end_page": 26,
      "detection_method": "topic_boundary",
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nbeneficial. It is important not to skip the mathematical parts. They can help you understand why we do things in specific ways. You should also not be scared by more complex mathematical notations. The goal of this book is not to give you a mathematical foundation; I assume you have that already. Deep learning and neural networks (in general, machine learning) are complex and whoever tries to convince you otherwise is lying or doesn’t understand them.\n\nWe will not spend time justifying or deriving algorithms or equations.\n\nAdditionally, we will not discuss the applicability of specific equations. For example, we will not discuss the problem of differentiability of functions when we calculate derivatives. Just assume we can apply the formulas you find here. Many years of practical implementations have shown the deep learning community that those methods and equations work as expected. These kinds of advanced discussions would require a separate book.\n\nPython Development Environment\n\nIn this book, we work exclusively with TensorFlow and Keras from Google, and we develop our code exclusively with Jupyter Notebooks, so it is crucial to know how to deal with them. There are three main possibilities when working with the code in the book, and in general when working with Python and TensorFlow:\n\nUse Google Colab, a cloud-based Python development\n\nenvironment.\n\n\n\nInstall a Python development environment locally on a laptop or desktop.\n\nUse a Docker image provided by Google, with\n\nTensorFlow installed.\n\nLet’s look at the different options in order to decide which one is the\n\nbest for you.\n\n4\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nGoogle Colab\n\nAs mentioned, Google Colab is a cloud-based environment. That means nothing has to be installed locally. A Google account and a web browser (preferably Google Chrome) are the only things you need. The URL of the service is https://colab.research.google.com/.\n\nJust log in with a Google account or create one if you don’t have one. You will then get a window where you can open existing notebooks, if\n\nyou have some already in the cloud, or create new ones. The window looks like Figure 1-1.\n\nFigure 1-1. The first screen you see when you log in to Google Colab. In this screenshot, the Recent tab is open. Sometimes the Recent tab is opened the first time you log in.\n\n5\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nIn the lower right, you can see the NEW PYTHON 3 NOTEBOOK link (typically in blue). If you click on the small downward triangle, you have the option of creating a Python 2 notebook. In this book, we use Python 3 exclusively. If you click the link, you get an empty Jupyter Notebook, like the one shown in Figure 1-2.\n\nFigure 1-2. The empty Jupyter Notebook you see when you create a new notebook in Google Colab\n\nThe notebook works precisely like a locally installed Jupyter Notebook,\n\nwith the exception that keyboard shortcuts (referred to here as simply shortcuts) are not the same as the ones in a local installation. For example, pressing X to delete a cell does not work here (but works in a local installation). In case you are stuck, and you don’t find the shortcut you want, you can press Ctrl+Shift+P to get a popup where you can search through the shortcuts. Figure 1-3 shows this popup.\n\n6\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nFigure 1-3. The popup to search keyboard shortcuts when pressing Ctrl+Shift+P. Note that you can type a command name to search for it. You don’t need to scroll through them.\n\nFor example, typing DELETE in the popup tells you that, to delete a cell, you need to type Ctrl+M and then D. An exceptional place to start learning what is possible in Google Colab is from this Google notebook:\n\nhttps://Colab.research.Google.com/notebooks/basic_features_\n\noverview.ipynb (https://goo.gl/h9Co1f).\n\n7\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nNote Google Colab has a great feature: it allows you to use Gpu (Graphical processing unit) and tpu (tensor processing unit)3 hardware acceleration for your experimentation. I will explain what difference this makes and how to use this when the time comes, but it will not be necessary to try the code and examples in this book.\n\nBenefits and Drawbacks to Google Colab\n\nGoogle Colab is a great development environment, but it has positive and negative aspects. Here is an overview.\n\nPositives:\n\nYou don’t have to install anything on your laptop/\n\ndesktop.\n\nYou can use GPU and TPU acceleration without buying\n\nexpensive hardware.\n\n\n\nIt has excellent sharing possibilities.\n\nMultiple people can collaboratively edit the same\n\nnotebook at the same time. Like Google Docs, you can set collaborators both within the document (top right, left of the comments button) and within a cell (right of the cell).4\n\n3 In deep learning, most of the calculations are done between tensors (multi- dimensional arrays). GPUs and TPUs are chips that are highly optimized to perform such calculations (like matrix multiplications) between very big tensors (up to a million of elements). When developing networks, it is possible to let GPUs and TPUs perform such expensive calculation in Google Colab, speeding up the training of networks. 4 Google Colab documentation is found at https://goo.gl/bKNWy8\n\n8\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nNegatives:\n\nYou need to be online to use and work with it. If you want to study this book on a train while commuting, you may not be able to do so.\n\n\n\nIf you have sensitive data and you are not allowed to upload it to a cloud service, you cannot work with it.\n\nThis system is designed for research and\n\nexperimentation, so you should not use it as a substitute productive environment.\n\nAnaconda\n\nThe second way of using and testing the code in this book is to have a local installation of Python and TensorFlow on your laptop or desktop. The easiest way to do that is using Anaconda. Here I describe in quite some detail how to do that.\n\nTo set it up, first download and install Anaconda for your system (I used Anaconda on Windows 10, but the code is not dependent on it, so feel free to use a Mac or Linux version if you prefer). You can get the Anaconda from https://anaconda.org/.\n\nOn the right side of the web page (see Figure 1-4), you’ll find a\n\nDownload Anaconda link.\n\nFigure 1-4. On the top-right side of the Anaconda website, you’ll find a link to download the software\n\n9\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nJust follow the instructions to install it. When you start it after the\n\ninstallation, you should see the screen shown in Figure 1-5.\n\nFigure 1-5. The screen you see when you start Anaconda\n\nPython packages (like numpy) are updated regularly and very often. A new version of a package may make your code stop working. Functions are deprecated and removed and new ones are added. To solve this problem, in Anaconda you can create what is called an environment. That is a container that contains a specific Python version and specific versions of the packages you decide to install. This way, you can have a container for Python 2.7 and numpy 1.10 and another with Python 3.6 and numpy 1.13, for example. You may have to work with code that exists already, and that is based on Python 2.7, and therefore you need a container with the right Python version. However, at the same time, it may be that for your projects you need Python 3.6. With containers, you can do all this at the same time. Sometimes different packages conflict, so you must be careful,\n\n10\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nand you should avoid installing all the packages you find interesting in your environment, primarily if you use it for developing under a deadline. There’s nothing worse than discovering that your code is not working anymore, and you don’t know why.\n\nNote When you define an environment, try to install only the packages you need and pay attention when you update them to make sure that the upgrade does not break your code (remember that functions are deprecated, removed, added, or changed very often). Check the updates documentation before upgrading and do it only if you need the updated features.\n\nIn the first book of the series (https://goo.gl/ytiQ1k), I explained how to create an environment with the graphical interface, so you can check that to learn how, or you can read the following page on the Anaconda documentation to understand how to work with environments in detail:\n\nhttps://conda.io/docs/user-guide/tasks/manage-environments.html\n\nIn the next section, we will create an environment and install\n\nTensorFlow in one shot, with one command only.\n\nInstalling TensorFlow the Anaconda Way\n\nInstalling TensorFlow is not complicated and has gotten a lot easier in the last year since my last book. To start (we describe the procedure for Windows here), go into the Start menu in Windows and type Anaconda. You should see the Anaconda Prompt under Apps. (You should see something similar to what is shown in Figure 1-6.)\n\n11\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nFigure 1-6. If you type Anaconda in the Start menu search field in Windows 10 you should see at least two entries: the Anaconda Navigator and the Anaconda Prompt.\n\nStart the Anaconda Prompt (see Figure 1-7). A command-line interface should start. The difference between this and the simple cmd.exe command prompt is that, here, all the Anaconda commands are recognized without having to set up Windows environment variables.\n\nFigure 1-7. This is what you should see when you start the Anaconda Prompt. Note that the username will be different. You will not see “umber” (my username), but your username.\n\n12",
      "page_number": 18
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 27-34)",
      "start_page": 27,
      "end_page": 34,
      "detection_method": "topic_boundary",
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nThen just type the following commands:\n\nconda create -n tensorflow tensorflow conda activate tensorflow\n\nThe first line creates an environment called tensorflow with TensorFlow\n\nalready installed, and the second line activates the environment. Then you only need to install the following packages with this code:\n\nconda install Jupyter conda install matplotlib conda install scikit-learn\n\nNote that sometimes you may get some warnings simply by importing\n\nTensorFlow with this command:\n\nimport tensorflow as tf\n\nThe warnings are due, most probably, by an outdated hdf5 version. To solve this issue (if it happens to you), try to update it using this code (if you don’t get any warning you can skip this step):\n\nconda update hdf5\n\nYou should be all set up. If you have a compatible GPU graphic card installed locally, you can simply install the GPU version of TensorFlow by using this command:\n\nconda create -n tensorflow_gpuenv tensorflow-gpu\n\nThis will create an environment with the GPU version of TensorFlow\n\ninstalled. If you do this, remember to activate the environment and then install all the additional packages as we have done here, in this new environment. Note that to use a GPU, you need additional libraries installed on your system. You can find all the necessary information for the\n\n13\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\ndifferent operating systems (Windows, Mac, and Linux) at https://www. tensorflow.org/install/gpu. Note that the TensorFlow website suggests using a Docker image (discussed later in the chapter) if you’re using a GPU for hardware acceleration.\n\nLocal Jupyter Notebooks\n\nThe last step to be able to type code and let it run is to use a Jupyter Notebook from a local installation. The Jupyter Notebook can be described (according to the official website) as follows:\n\nThe Jupyter Notebook is an open source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. Uses include data cleaning and transformation, numerical simula- tion, statistical modeling, data visualization, machine learn- ing, and much more.\n\nIt is widely used in the machine learning community and is a good idea\n\nto learn how to use it. Check out the Jupyter project website at http:// Jupyter.org/. It is very instructive and includes many examples of what is possible.\n\nAll the code you find in this book has been developed and tested using\n\nJupyter Notebooks. I assume that you have some experience with this web-based development environment. If you need a refresher, I suggest you check out the documentation. You can find it on the Jupyter project website at this address: http://Jupyter.org/documentation.html. To start a notebook in your new environment, you must go back to Anaconda Navigator and click on the triangle to the right of your tensorflow environment (if you used a different name, you have to click on the triangle to the right of your new environment), as shown in Figure 1-8. Then click on the Open with Jupyter Notebook option.\n\n14\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nFigure 1-8. To start a Jupyter Notebook in your new environment, click on the triangle to the right of the TensorFlow environment name and choose Open with Jupyter Notebook\n\nYour browser starts with a list of the folders in your user folder. (If you\n\nare using Windows, this is usually located in c:\\Users\\<YOUR USER NAME>, where you substitute <YOUR USER NAME> with your username.) From there, you should navigate to a folder where you want to save your notebook files. You can create a new one by clicking on the New button, as illustrated in Figure 1-9.\n\nFigure 1-9. To create a new notebook, click on the New button located on the top-right part of the page and choose Python 3\n\n15\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nA new page that should look like the one in Figure 1-10 will open.\n\nFigure 1-10. An empty Jupyter Notebook as it appears immediately after creation\n\nFor example, you can type the following code in the first “cell” (the\n\nrectangular space where you can type).\n\na=1 b=2 print(a+b)\n\nTo evaluate the code press Shift+Enter and you should see the result (3)\n\nimmediately, as shown in Figure 1-11.\n\nFigure 1-11. After typing some code in the cell, pressing Shift+Enter evaluates the code in the cell\n\nThe result of a+b is 3 (as shown in Figure 1-11). A new empty cell is\n\nautomatically created after the result for you to type in.\n\nFor more information on how to add comments, equations, inline plots, and much more, I suggest you visit the Jupyter website and check out their documentation.\n\n16\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nNote check the url of the page. For example, in my case, I have http:// localhost:8888/notebooks/Documents/Data%20Science/ Projects/Applied%20advanced%20deep%20learning%20 (book)/chapter%201/AADL%20-%20Chapter%201%20-%20 Introduction.ipynb. note that the url is merely a concatenation of the folders showing where the notebook is located, separated by forward slashes. a %20 character indicates a space. In this case, my notebook is in the Documents/Data Science/Projects/... folder. I often work with several notebooks at the same time and it’s useful to know where each notebook is located, in case you forget (as I often do).\n\nIn case you forget which folder your notebook is in, you can\n\nBenefits and Drawbacks to Anaconda\n\nLet’s take a look at the positive and the negative sides of Anaconda now.\n\nPositives:\n\nThe system does not require an active Internet\n\nconnection (except when installing), so you can work with it everywhere (on the train, for example).\n\n\n\nIf you are working on sensitive data that you cannot upload to a cloud service, this is the solution for you, since you can work with data locally.\n\nYou can keep close control over which packages you\n\ninstall and on which environment you create.\n\n17\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nNegatives:\n\n\n\nIt is quite annoying to get the TensorFlow GPU version to work (you need additional libraries for it to work) with this method. The TensorFlow website suggests using a Docker image (see the next section) for it.\n\n\n\nIt is complicated to share your work with other people directly. If sharing is essential, you should consider Google Colab.\n\n\n\nIf you are using a corporate laptop that must work behind a firewall or a proxy, it’s challenging to work with Jupyter Notebooks, since sometimes, the notebooks may need to connect to the Internet and, if you are behind a firewall, this may not be possible. Installing packages may also be complicated in this case.\n\nThe performance of your code depends on the power\n\nand memory of your laptop or desktop. If you are using a slow or old machine, your code may be very slow. In this case, Google Colab may be the better option.\n\nDocker Image\n\nThe third option you have is to use a Docker image with TensorFlow installed. Docker (https://www.docker.com/) in a way is a bit like a virtual machine. However, unlike a virtual machine, rather than creating a whole virtual operating system, it merely adds components that are not present on the host machine.5 First, you need to download Docker for your system. A good starting point to learn about it and download it is at https://docs. docker.com/install/.\n\n5 https://opensource.com/resources/what-docker [Last accessed: 19/12/2018]\n\n18\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nFirst, install Docker on your system. Once you have done so, you can\n\naccess all different types of TensorFlow versions by using the following command. You must type this command into a command-line interface (for example, cmd in Windows, Terminal on the Mac, or a shell under Linux):\n\ndocker pull TensorFlow/TensorFlow:<TAG>\n\nYou should substitute <TAG> with the right text (called a tag as you may\n\nimagine), like latest-py3, if you want the latest stable CPU-based build from Python 3.5. You can find an updated list of all tags at https://hub. docker.com/r/TensorFlow/TensorFlow/tags/. In this example, you would need to type:\n\ndocker pull tensorflow/tensorflow:latest-py3\n\nThis command downloads the right image automatically. Docker is efficient, and you can ask it to run the image immediately. If it does not find it locally, it downloads it. You can use the following command to start the image:\n\ndocker run -it -p 8888:8888 tensorflow/tensorflow:latest-py3\n\nIf you haven’t already downloaded it, this command downloads the\n\nlatest TensorFlow version based on Python 3 and starts it. You should see output like the following if everything goes well:\n\nC:\\Users\\umber>docker run -it -p 8888:8888 tensorflow/ tensorflow:latest-py3 Unable to find image 'TensorFlow/TensorFlow:latest-py3' locally latest-py3: Pulling from TensorFlow/TensorFlow 18d680d61657: Already exists 0addb6fece63: Already exists 78e58219b215: Already exists eb6959a66df2: Already exists 3b57572cd8ae: Pull complete\n\n19\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\n56ffb7bbb1f1: Pull complete 1766f64e236d: Pull complete 983abc49e91e: Pull complete a6f427d2463d: Pull complete 1d2078adb47a: Pull complete f644ce975673: Pull complete a4eaf7b16108: Pull complete 8f591b09babe: Pull complete Digest: sha256:1658b00f06cdf8316cd8a905391235dad4bf25a488f1ea98 9a98a9fe9ec0386e Status: Downloaded newer image for TensorFlow/TensorFlow:latest-py3 [I 08:53:35.084 NotebookApp] Writing notebook server cookie secret to /root/.local/share/Jupyter/runtime/notebook_cookie_ secret [I 08:53:35.112 NotebookApp] Serving notebooks from local directory: /notebooks [I 08:53:35.112 NotebookApp] The Jupyter Notebook is running at: [I 08:53:35.112 NotebookApp] http://(9a30b4f7646e or 127.0.0.1):8888/?token=f2ff836cccb1d688f4d9ad8c7ac3af80011f11ea 77edc425 [I 08:53:35.112 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 08:53:35.113 NotebookApp]\n\nCopy/paste this URL into your browser when you connect for\n\nthe first time, to login with a token:\n\nhttp://(9a30b4f7646e or 127.0.0.1):8888/?token=f2ff836c\n\nccb1d688f4d9ad8c7ac3af80011f11ea77edc425\n\nAt this point, you can simply connect to a Jupyter server running from\n\nthe Docker image.\n\n20",
      "page_number": 27
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 35-44)",
      "start_page": 35,
      "end_page": 44,
      "detection_method": "topic_boundary",
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nAt the end of all previous messages, you’ll find the URL you should type in the browser to use Jupyter Notebooks. When you copy the URL, simply substitute cbc82bb4e78c or 127.0.0.1 with 127.0.0.1. Copy it into the URL field of your browser. The page should look like the one shown in Figure 1-12.\n\nFigure 1-12. The navigation window you see when using a Docker image Jupyter instance\n\nIt’s important to note that if you use the notebook out of the box, all\n\nfiles and notebooks that you create will disappear the next time you start the Docker image.\n\nIf you use the Jupyter notebook server as it is, and you create Note new notebooks and files, they will all disappear the next time you start the server. You need to mount a local directory that resides on your machine so that you can save your files locally and not in the image itself.\n\n21\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nLet’s suppose you are using a Windows machine and that your notebooks reside locally at c:\\python. To see and use them while using Jupyter Notebooks from the Docker image, you need to start the Docker instance using the -v option in the following way:\n\ndocker run -it -v c:/python:/notebooks/python -p 8888:8888 TensorFlow/TensorFlow:latest-py3\n\nThis way, you can see all your files that are under c:\\python in a folder called python in the Docker image. You specify the local folder (where the files are local) and the Docker folder name (where you want to see the files while using Jupyter Notebooks from the Docker image) with the -v option:\n\nv <LOCAL FOLDER>:/notebooks/<DOCKER FOLDER>\n\nIn our example, <LOCAL FOLDER> is c:/python (the local folder you want to use for your locally saved notebooks) and <DOCKER FOLDER> is python (where you want Docker to mount the folder with your notebooks). Once you run the code, you should see output like the following:\n\n[I 09:23:49.182 NotebookApp] Writing notebook server cookie secret to /root/.local/share/Jupyter/runtime/notebook_cookie_ secret [I 09:23:49.203 NotebookApp] Serving notebooks from local directory: /notebooks [I 09:23:49.203 NotebookApp] The Jupyter Notebook is running at: [I 09:23:49.203 NotebookApp] http://(93d95a95358a or 127.0.0.1):8888/?token=d564b4b1e806c62560ef9e477bfad99245bf9670 52bebf68 [I 09:23:49.203 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 09:23:49.204 NotebookApp]\n\n22\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nCopy/paste this URL into your browser when you connect for\n\nthe first time, to log in with a token:\n\nhttp://(93d95a95358a or 127.0.0.1):8888/?token=d564b4b1\n\ne806c62560ef9e477bfad99245bf967052bebf68\n\nNow, when you start your browser with the URL given at the end of the last message (where you must substitute 93d95a95358a or 127.0.0.1 with 127.0.0.1), you should see a Python folder named python, as shown in the one circled in Figure 1-13.\n\nFigure 1-13. The folder that you should see when starting the Docker image with the correct -v option. In the folder, you can now see all the files that are saved locally in the c:\\python folder.\n\nYou can now see all your locally saved notebooks, and if you save a notebook in the folder, you will find it again when you restart your Docker image.\n\n23\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nOn a final note, if you have a compatible GPU at your disposal,6 you can directly download the latest GPU TensorFlow version, for example, using the tag, latest-gpu. You can find more information at https://www.TensorFlow.org/install/gpu.\n\nBenefits and Drawbacks to a Docker Image\n\nLet’s take a look at the positive and the negative aspects of this option.\n\nPositives:\n\nYou don’t need to install anything locally, except\n\nDocker.\n\nThe installation process is straightforward.\n\nYou get the latest version of TensorFlow automatically.\n\n\n\nIt is the preferred option to choose if you want to use the GPU version of TensorFlow.\n\nNegatives:\n\nYou cannot develop with this method in several environments and with several versions of the packages.\n\n\n\nInstalling specific package versions is complicated.\n\nSharing notebooks is more complicated than with other\n\noptions.\n\nThe performance of your code is limited by the\n\nhardware on which you are running the Docker image.\n\n6 You can find a list of all compatible GPUs at https://developer.nvidia.com/ cuda-gpus and TensorFlow information at https://www.TensorFlow.org/ install/gpu.\n\n24\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nWhich Option Should You Choose?\n\nYou can quickly start with any of the options described and later continue with another one. Your code will continue to work. The only thing you need to be aware of is that, if you develop extensive amounts of code with GPU support and then try to run this on a system without GPU support, you may need to modify the code extensively. To decide which option is the best one for you, I provided the following questions and answers.\n\nDo you need to work on sensitive data?\n\nIf you need to work on sensitive data (for example, medical data) that you cannot upload on a cloud service, you should choose a local installation with Anaconda or Docker. You cannot use Google Colab.\n\nDo you often work in an environment without an\n\nInternet connection?\n\nIf you want to write code and train your models without an active Internet connection (for example, while commuting), you should choose a local installation of Anaconda or Docker, since Google Colab requires an active Internet connection.\n\nDo you need to work on the same notebook in\n\nparallel with other people?\n\nIf you want to share your work with others and work on it at the same time as others, the best solution is to use Google Colab, since it offers a great sharing experience, one that is missing from the local installation options.\n\n25\n\nChapter 1\n\nIntroduCtIon and development envIronment Setup\n\nYou don’t want to (or can’t) install anything on your\n\nlaptop/desktop?\n\nIf you don’t want to or can’t install anything on your laptop or desktop (maybe it’s a corporate laptop ), you should use Google Colab. You only need an Internet connection and a browser. Keep in mind that some features work only with Google Chrome and not Internet Explorer.\n\nNote the easiest way to get up and running and start developing models with tensorFlow is probably to use Google Colab since it does not require any installation. directly go the website, log in, and start writing code. If you need to work locally, the docker option is probably the easiest solution. It is straightforward to get it up and running and you get to work with the latest version of tensorFlow. If you need the flexibility of many environments and precise control over which version of each package you’re using, your only solution is to perform a complete local installation of a python development environment, like anaconda.\n\n26\n\nCHAPTER 2\n\nTensorFlow: Advanced Topics\n\nThe TensorFlow library has come a long way from its first appearance. Especially in the last year, many more features have become available that can make the life of researchers a lot easier. Things like eager execution and Keras allow scientists to test and experiment much faster and debug models in ways that were not possible before. It is essential for any researcher to know those methods and know when it makes sense to use them. In this chapter, we will look at few of them: eager execution, GPU acceleration, Keras, how to freeze parts of a network and train only specific parts (used very often, especially in transfer learning and image recognition), and finally how to save and restore models already trained. Those technical skills will be very useful, not only to study this book, but in real-life research projects.\n\nThe goal of this chapter is not to teach you how to use Keras from the ground up, or to teach you all the intricacies of the methods, but to show you some advanced techniques to solve some specific problems. Consider the different sections as hints. Remember that is always a good idea to study the official documentation, since methods and functions change very often. In this chapter, I will avoid copying the official documentation, and instead give you few advanced examples of techniques that are very useful and are used very often. To go deeper (pun intended), you should study the official TensorFlow documentation at https://www.tensorflow.org/.\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_2\n\n27\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nTo study and understand advanced topics, a good basis in Tensorflow and Keras is required. A very good resource to get up to speed with Keras is the book Learn Keras for Deep Neural Networks - A Fast-Track Approach to Modern Deep Learning with Python from Jojo John Moolayil (https:// goo.gl/mW4Ubg). If you don’t have much experience, I suggest you get this book and study it before starting this one.\n\nTensorflow Eager Execution\n\nTensorFlow’s eager execution is an imperative programming environment.1 That, loosely explained, means that the commands are evaluated immediately. That also means that a computational graph is built in the background without you noticing it. Operations return concrete values immediately instead of having first open a session, and then run it. This makes it very easy to start with TensorFlow, since it resembles classical Python programming. Eager execution provides the following advantages:\n\nEasier debugging: You can debug your models with\n\nclassical Python debugging tools for immediate checks\n\n\n\nIntuitive interface: You can structure your code naturally, as you would do in a classical Python program\n\nSupport for GPU acceleration is available\n\nTo be able to use this execution mode, you will need the latest version\n\nof TensorFlow. If you have not yet installed it, see Chapter 1 to learn how to do it.\n\n1 https://www.tensorflow.org/guide/eager (accessed 17th January, 2019)\n\n28\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nEnabling Eager Execution\n\nTo enable eager execution, you can use the following code:\n\nimport tensorflow as tf tf.enable_eager_execution()\n\nRemember that you need to do that right at the beginning, after the\n\nimports and before any other command. Otherwise, you will get an error message. If that is the case, you can simply restart the kernel of the notebook.\n\nFor example, you can easily add two tensors\n\nprint(tf.add(1, 2))\n\nand get immediately this result\n\ntf.Tensor(3, shape=(), dtype=int32)\n\nIf you don’t enable eager execution and try the print command again,\n\nyou will get this result\n\nTensor(\"Add:0\", shape=(), dtype=int32)\n\nSince TensorFlow has not yet evaluated the node. You would need the\n\nfollowing code to get the result:\n\nsess = tf.Session() print(sess.run(tf.add(1,2))) sess.close()\n\nThe result will be, of course, 3. This second version of the code creates\n\na graph, then opens a session, and then evaluates it. With eager you get the result immediately. You can easily check if you have enabled eager execution with this:\n\ntf.executing_eagerly()\n\n29\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nIt should return True or False, depending on if you have enabled it or\n\nnot.\n\nPolynomial Fitting with Eager Execution\n\nLet’s check how eager execution works in a practical example.2\n\nKeep in mind you need the following imports:\n\nimport tensorflow as tf import numpy as np import matplotlib.pyplot as plt import tensorflow.contrib.eager as tfe tf.enable_eager_execution()\n\nLet’s generate some fake data for this function\n\ny\n\n=\n\nx\n\n3\n\n\n\n24 x\n\n\n\n2\n\nx\n\n+\n\n2\n\nwith the code\n\nx = np.arange(0, 5, 0.1) y = x**3 - 4*x**2 - 2*x + 2 y_noise = y + np.random.normal(0, 1.5, size=(len(x),))\n\nWe have created two numpy arrays: y, which contains the function evaluated over the array x, and y_noise, which contains y with some noise added. You can see how the data looks in Figure 2-1.\n\n2 You can find the notebook with the code in the book repository. To find it, go to the Apress book website and click on the Download Code button. The link points to the GitHub repository. The notebook is in the Chapter2 folder.\n\n30",
      "page_number": 35
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 45-52)",
      "start_page": 45,
      "end_page": 52,
      "detection_method": "topic_boundary",
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nFigure 2-1. The plot shows the two numpy arrays y (ground truth) and y_noise (ground truth + noise)\n\nNow we need to define a model that we want to fit and define our loss function (the one we want to minimize with TensorFlow). Remember we are facing a regression problem, so we will use the Mean Squared Error (MSE) as our loss function. The functions we need are as follows:\n\nclass Model(object): def __init__(self): self.w = tfe.Variable(tf.random_normal([4])) # The 4\n\nparameters\n\ndef f(self, x): return self.w[0] * x ** 3 + self.w[1] * x ** 2 +\n\nself.w[2] * x + self.w[3]\n\nand\n\ndef loss(model, x, y): err = model.f(x) - y return tf.reduce_mean(tf.square(err))\n\n31\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nNow is easy to minimize the loss function. First let’s define some\n\nvariables we will need:\n\nmodel = Model() grad = tfe.implicit_gradients(loss) optimizer = tf.train.AdamOptimizer()\n\nThen let’s, with a for loop, minimize the loss function:\n\niters = 20000 for i in range(iters): optimizer.apply_gradients(grad(model, x, y)) if i % 1000 == 0: print(\"Iteration {}, loss: {}\".format(i+1, loss(model,\n\nx, y).numpy()))\n\nThis code will produce some outputs showing you the value for the loss function each 1,000 iterations. Note that we are feeding all the data in one batch to the optimizer (since we have only 50 data points, we don’t really need to use mini-batches).\n\nYou should see several output lines like this one:\n\nIteration 20000, loss: 0.004939439240843058\n\nThe loss function plot versus the number of the iterations can be seen\n\nin Figure 2-2 and is decreasing constantly, as expected.\n\n32\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nFigure 2-2. The loss function (MSE) vs. the iteration number is decreasing as expected. That shows clearly that the optimizer is doing a good job finding the best weights to minimize the loss function.\n\nIn Figure 2-3, you can see the function the optimizer was able to find,\n\nby minimizing the weights.\n\nFigure 2-3. The red dashed line is the function obtained by minimizing the loss function with the Adam optimizer. The method worked perfectly and found the right function efficiently.\n\n33\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nWhat you should note is that we did not create a computational graph explicitly and then evaluate it in a session. We simply used the commands as we would with any Python code. For example, in the code\n\nfor i in range(iters): optimizer.apply_gradients(grad(model, x, y))\n\nwe simply call a TensorFlow operation in a loop without the need of a session. With eager execution, it’s easy to start using TensorFlow operations quickly without too much overhead.\n\nMNIST Classification with Eager Execution\n\nTo give another example of how you can build a model with eager execution, let’s build a classifier for the famous MNIST dataset. This is a dataset containing 60000 images of handwritten digits (from 0 to 9), each with a dimension of 28x28 in gray levels (each pixel has a value ranging from 0 to 255). If you have not seen the MNIST dataset, I suggest you check out the original website at https://goo.gl/yF0yH, where you will find all the information. We will implement the following steps:\n\nLoad the dataset.\n\nNormalize the features and one-hot encode the labels.\n\nConvert the data in a tf.data.Dataset object.\n\nBuild a Keras model with two layers, each with 1024\n\nneurons.\n\nDefine the optimizer and the loss function.\n\nMinimize the loss function using the gradients and the\n\noptimizer directly.\n\n34\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nLet’s start. While following the code, note how we implement each piece as we would do with plain numpy, meaning without the need of creating a graph or opening a TensorFlow session.\n\nSo first let’s load the MNIST dataset using the keras.datasets.mnist\n\npackage, reshape it, and one-hot encode the labels.\n\nimport tensorflow as tf import tensorflow.keras as keras\n\nnum_classes = 10\n\nmnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimage_vector_size = 28*28 x_train = x_train.reshape(x_train.shape[0], image_vector_size) x_test = x_test.reshape(x_test.shape[0], image_vector_size)\n\ny_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes)\n\nThen let’s convert the arrays in a tf.data.Dataset object. In case you\n\ndon’t understand what this is, don’t worry, we will look at this more later in this chapter. For the moment, it suffices to know that it is a convenient way to use mini-batches while you train your network.\n\ndataset = tf.data.Dataset.from_tensor_slices( (tf.cast(x_train/255.0, tf.float32), tf.cast(y_train,tf.int64)))\n\ndataset = dataset.shuffle(60000).batch(64)\n\n35\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nNow let’s build the model using a feed-forward neural network with\n\ntwo layers, each with 1024 neurons:\n\nmnist_model = tf.keras.Sequential([ tf.keras.layers.Dense(1024, input_shape=(784,)), tf.keras.layers.Dense(1024), tf.keras.layers.Dense(10) ])\n\nUp to now we have not done anything particularly new, so you should\n\nbe able to follow what we did quite easily. The next step is to define the optimizer (we will use Adam) and the list that will contain the loss function history:\n\noptimizer = tf.train.AdamOptimizer() loss_history = []\n\nAt this point we can start with the actual training. We will have two\n\nnested loops—the first is for the epochs, the second for the batches.\n\nfor i in range(10): # Epochs print (\"\\nEpoch:\", i) for (batch, (images, labels)) in enumerate(dataset.\n\ntake(60000)):\n\nif batch % 100 == 0: print('.', end=\") with tf.GradientTape() as tape: logits = mnist_model(images, training=True) # Prediction\n\nof the model\n\nloss_value = tf.losses.sparse_softmax_cross_entropy(tf.\n\nargmax(labels, axis = 1), logits)\n\nloss_history.append(loss_value.numpy()) grads = tape.gradient(loss_value, mnist_model.variables)\n\n# Evaluation of gradients\n\n36\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\noptimizer.apply_gradients(zip(grads, mnist_model.\n\nvariables),\n\nglobal_step=tf.train.get_or_\n\ncreate_global_step())\n\nThe part of the code that is probably new to you is the part that\n\ncontains these two lines:\n\ngrads = tape.gradient(loss_value, mnist_model.variables) optimizer.apply_gradients(zip(grads, mnist_model.variables), global_step=tf.train.get_or_\n\ncreate_global_step())\n\nThe first line calculates the gradients of the loss_value TensorFlow\n\noperation with respect to the mnist_model.variables (the weights basically), and the second line uses the gradients to let the optimizer update the weights. To understand how Keras evaluates gradients automatically, I suggest you check the official documentation at https:// goo.gl/s9Uqjc. Running the code will finally train the network. As the training progress, you should see output like this for each epoch:\n\nEpoch: 0 ..........\n\nNow to check the accuracy, you can simply run the following two lines\n\n(that should be self-explanatory):\n\nprobs = tf.nn.softmax(mnist_model(x_train)) print(tf.reduce_mean(tf.cast(tf.equal(tf.argmax(probs, axis=1), tf.argmax(y_train, axis = 1)), tf.float32)))\n\nThis will give you as a result a tensor that will contain the accuracy\n\nreached by the model:\n\ntf.Tensor(0.8980333, shape=(), dtype=float32)\n\n37\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nIn this example, we reached 89.8% accuracy, a relatively good result for such a simple network. Of course, you could try to train the model for more epochs or try to change the learning rate, for example. In case you are wondering where we defined the learning rate, we did not. When we define the optimizer as tf.train.AdamOptimizer, TensorFlow will use, if not specified differently, the standard value of 10−3. You can check this by looking at the documentation at https://goo.gl/pU7yrB.\n\nWe could check one prediction easily. Let’s get one image from our\n\ndataset:\n\nimage = x_train[4:5,:] label = y_train[4]\n\nIf we plot the image, we will see the number nine (see Figure 2-4).\n\nFigure 2-4. One image from the MNIST dataset. This happens to be a 9.\n\nWe can easily check what the model predicts:\n\nprint(tf.argmax(tf.nn.softmax(mnist_model(image)), axis = 1))\n\n38",
      "page_number": 45
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 53-60)",
      "start_page": 53,
      "end_page": 60,
      "detection_method": "topic_boundary",
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nThis returns the following, as we expected:\n\ntf.Tensor([9], shape=(1,), dtype=int64)\n\nYou should note how we wrote the code. We did not create a graph explicitly, but we simply used functions and operations as we would have done with numpy. There is no need to think in graphs and sessions. This is how eager execution works.\n\nTensorFlow and Numpy Compatibility\n\nTensorFlow makes switching to and from numpy arrays very easy:\n\nTensorFlow converts numpy arrays to tensors\n\nNumpy converts tensors to numpy arrays\n\nConverting a tensor to a numpy array is very easy and is enough to invoke the .numpy() method. This operation is fast and cheap since the numpy array and the tensor share the memory, so no shifting around in memory is happening. Now this is not possible if you are using GPU hardware acceleration, since numpy arrays cannot be stored in GPU memory and tensors can. Converting will involve copying data from the GPU memory to the CPU memory. Simply something to keep in mind.\n\nNote typically, tensorFlow tensors and numpy arrays share the same memory. Converting one to another is a very cheap operation. But if you use GpU accelerations, tensors may be held in the GpU memory, and numpy arrays cannot, so copying data will be required. this may be more expensive in terms of running time.\n\n39\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nHardware Acceleration Checking the Availability of the GPU\n\nIt is worth it to show briefly how to use GPUs and what difference it may make, just to give you a feeling for it. If you have never seen it, it’s quite impressive. The easiest way to test GPU acceleration is to use Google Colab. Create a new notebook in Google Colab, activate GPU3 acceleration, and import TensorFlow as usual:\n\nimport tensorflow as tf\n\nThen we need to test if we have a GPU at our disposal. This can be\n\neasily done with this code:\n\nprint(tf.test.is_gpu_available())\n\nThis will return True or False depending on if a GPU is available. In a\n\nslightly more sophisticated way, it can be done in this way:\n\ndevice_name = tf.test.gpu_device_name() if device_name != '/device:GPU:0': raise SystemError('GPU device not found.') print('Found GPU at: {}'.format(device_name))\n\nIf you run the code, you may get this error:\n\nSystemErrorTraceback (most recent call last) <ipython-input-1-d1680108c58e> in <module>() 2 device_name = tf.test.gpu_device_name() 3 if device_name != '/device:GPU:0':\n\n3 You can find this article at https://goo.gl/hXKNnf to learn how to do it.\n\n40\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\n----> 4 raise SystemError('GPU device not found') 5 print('Found GPU at: {}'.format(device_name)) SystemError: GPU device not found\n\nThe reason is that you may have not yet configured the notebook (if\n\nyou are in Google Colab) to use a GPU. Or, if you are working on a laptop or desktop, you may have not installed the right TensorFlow version or you may not have a compatible GPU available.\n\nTo enable the GPU hardware acceleration in Google Colab, choose the Edit ➤ Notebook Settings menu option. You are then presented with a window where you can set up the hardware accelerator. By default, it is set to None. If you set it to GPU and run the previous code again, you should get this message:\n\nFound GPU at: /device:GPU:0\n\nDevice Names\n\nNote how the device name, in our case /device:GPU:0, encodes lots of information. This name ends with GPU:<NUMBER>, where <NUMBER> is an integer that can be as big as the number of GPUs you have at your disposal. You can get a list of all the devices you have at your disposal with this code:\n\nlocal_device_protos = device_lib.list_local_devices() print(local_device_protos)\n\nYou will get a list of all the devices. Each list entry will resemble this\n\none (this example refers to a GPU device):\n\nname: \"/device:XLA_GPU:0\" device_type: \"XLA_GPU\" memory_limit: 17179869184 locality { }\n\n41\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nincarnation: 16797530695469281809 physical_device_desc: \"device: XLA_GPU device\"\n\nWith a function like this one:\n\ndef get_available_gpus(): local_device_protos = device_lib.list_local_devices() return [x.name for x in local_device_protos if x.device_\n\ntype.endswith('GPU')]\n\nYou will get an easier-to-read result like this one4:\n\n['/device:XLA_GPU:0', '/device:GPU:0']\n\nExplicit Device Placement\n\nIt is very easy to place an operation on a specific device. That can be achieved using the tf.device context. For example, to place an operation on a CPU, you can use the following code:\n\nwith tf.device(\"/cpu:0\"): # SOME OPERATION\n\nOr to place an operation on a GPU, you can use the code:\n\nwith tf.device('/gpu:0'): # SOME OPERATION\n\nNote Unless explicitly declared, tensorFlow automatically decides on which device each operation must run. don’t assume that if you don’t specify the device explicitly that your code will run on a CpU.\n\n4 The result was obtained when calling the function in a Google Colab notebook.\n\n42\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nGPU Acceleration Demonstration: Matrix Multiplication\n\nIt is interesting to see what effect hardware acceleration may have. To learn more about using GPUs, it is instructive to read the official documentation, which can be found at https://www.TensorFlow.org/guide/using_gpu.\n\nStart with the following code5:\n\nconfig = tf.ConfigProto() config.gpu_options.allow_growth = True sess = tf.Session(config=config)\n\nThe second line is needed since TensorFlow starts to allocate a little\n\nGPU memory. As the session is started and the processes run, more GPU memory is then allocated as needed. Then a session is created. Let’s try to multiply two matrices of dimensions 10000x10000 filled with random values and see if using a GPU makes a difference. The following code will run the multiplication on a GPU:\n\n%%time with tf.device('/gpu:0'): tensor1 = tf.random_normal((10000, 10000)) tensor2 = tf.random_normal((10000, 10000)) prod = tf.linalg.matmul(tensor1, tensor2) prod_sum = tf.reduce_sum(prod)\n\nsess.run(prod_sum)\n\nAnd the following runs it on a CPU:\n\n%%time with tf.device('/cpu:0'):\n\n5 The code has been inspired by the Google code in the Google Colab documentation.\n\n43\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\ntensor1 = tf.random_normal((10000, 10000)) tensor2 = tf.random_normal((10000, 10000)) prod = tf.linalg.matmul(tensor1, tensor2) prod_sum = tf.reduce_sum(prod)\n\nsess.run(prod_sum)\n\nWhen I ran the code, I got 1.86 sec total time on a GPU and 1min 4sec on a CPU: a factor 32 times faster. You can imagine then, when doing such calculations over and over (as is often the case in deep learning), that you’ll get quite a performance boost in your evaluations. Using TPUs is slightly more complicated and goes beyond the scope of this book, so we will skip that.\n\nNote Using a GpU does not always give you a performance boost. when the tensors involved are small, you will not see a huge difference between using a GpU and a CpU. the real difference will become evident when the dimensions of the tensors start to grow.\n\nIf you try to run the same code on smaller tensors, for example 100x100, you will not see any difference at all between using a GPU and a CPU. The tensors are small enough that a CPU will get the result as fast as a GPU. For two 100x100 matrices, GPU and CPU both give a result in roughly 20ms. Typically, practitioners let CPUs do all the preprocessing (for example, normalization, loading of data, etc.) and then let GPUs perform all the big tensor operations during training.\n\nNote typically, you should evaluate only expensive tensor operations (like matrix multiplications or convolution) on GpUs and do all preprocessing (like data loading, cleaning, etc.) on a CpU.\n\n44\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nWe will see later in the book (where applicable) how to do that. But\n\ndon’t be afraid. You will be able to use the code and follow the examples without a GPU at your disposal.\n\nEffect of GPU Acceleration on the MNIST Example\n\nIt is instructive to see the effect of hardware acceleration on the MNIST example. To run the training of the model completely on the CPU we need to force TensorFlow to do it, since otherwise it will try to place expensive operations on a GPU when available. To do that, you can use this code:\n\nwith tf.device('/cpu:0'): for i in range(10): # Loop for the Epochs print (\"\\nEpoch:\", i) for (batch, (images, labels)) in enumerate(dataset.\n\ntake(60000)): # Loop for the mini-batches\n\nif batch % 100 == 0: print('.', end=\") with tf.GradientTape() as tape: logits = mnist_model(images, training=True) loss_value = tf.losses.sparse_softmax_cross_entropy(tf.\n\nargmax(labels, axis = 1), logits)\n\nloss_history.append(loss_value.numpy()) grads = tape.gradient(loss_value, mnist_model.\n\nvariables)\n\noptimizer.apply_gradients(zip(grads, mnist_model.\n\nvariables),\n\nglobal_step=tf.train.get_or_\n\ncreate_global_step())\n\n45\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nThis code, on Google Colab, runs in roughly 8 minutes and 41 seconds.\n\nIf we put all the possible operations on a GPU, using this code:\n\nfor i in range(10): # Loop for the Epochs print (\"\\nEpoch:\", i)\n\nfor (batch, (images, labels)) in enumerate(dataset.\n\ntake(60000)): # Loop for the mini-batches\n\nif batch % 100 == 0: print('.', end=\") labels = tf.cast(labels, dtype = tf.int64)\n\nwith tf.GradientTape() as tape:\n\nwith tf.device('/gpu:0'): logits = mnist_model(images, training=True)\n\nwith tf.device('/cpu:0'): tgmax = tf.argmax(labels, axis = 1, output_type=tf.\n\nint64)\n\nwith tf.device('/gpu:0'): loss_value = tf.losses.sparse_softmax_cross_\n\nentropy(tgmax, logits)\n\nloss_history.append(loss_value.numpy()) grads = tape.gradient(loss_value, mnist_model.\n\nvariables)\n\noptimizer.apply_gradients(zip(grads, mnist_model.\n\nvariables),\n\nglobal_step=tf.train.get_ or_create_global_step())\n\n46",
      "page_number": 53
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 61-68)",
      "start_page": 61,
      "end_page": 68,
      "detection_method": "topic_boundary",
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nIt will run in 1 minute and 24 seconds. The reason that the tf.\n\nargmax() has been placed on a CPU is that at the time of writing the GPU implementation of tf.argmax has a bug and does not work as intended.\n\nYou can clearly see the dramatic effect that GPU acceleration has, even\n\non a simple network like the one we used.\n\nTraining Only Specific Layers\n\nYou should now know that Keras works with layers. When you define one, let’s say a Dense layer, as follows:\n\nlayer1 = Dense(32)\n\nYou can pass a trainable argument (that is Boolean) to a layer\n\nconstructor. This will stop the optimizer to update its weights\n\nlayer1 = dense(32, trainable = False)\n\nBut this would not be very useful. What is needed is the possibility of changing this property after instantiation. This is easy to do. For example, you can use the following code\n\nlayer = Dense(32) # something useful happens here layer.trainable = False\n\nNote For the trainable property’s change to take effect, you need to call the compile() method on your model. otherwise, the change will not have any effect while using the fit() method.\n\n47\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nTraining Only Specific Layers: An Example\n\nTo understand better how this all works, let’s look at an example. Let’s again consider a feed-forward network with two layers:\n\nmodel = Sequential() model.add(Dense(32, activation='relu', input_dim=784, name = 'input')) model.add(Dense(32, activation='relu', name = 'hidden1'))\n\nNote how we created a model with two Dense layers with a name property. One is called input and the second is called hidden1. Now you can check the network structure with model.summary(). In this simple example, you will get the following output:\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== input (Dense) (None, 32) 25120 _______________________________________________________________ hidden1 (Dense) (None, 32) 1056 =============================================================== Total params: 26,176 Trainable params: 26,176 Non-trainable params: 0 _______________________________________________________________\n\nNote how all the parameters are trainable and how you can find the layer name in the first column. Please take note, since assigning each layer a name will be useful in the future. To freeze the layer called hidden1, you simply need to find the layer with the name and change its trainable property as follows:\n\nmodel.get_layer('hidden1').trainable = False\n\n48\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nNow, if you check the model summary again, you will see a different\n\nnumber of trainable parameters:\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== input (Dense) (None, 32) 25120 _______________________________________________________________ hidden1 (Dense) (None, 32) 1056 =============================================================== Total params: 26,176 Trainable params: 25,120 Non-trainable params: 1,056 _______________________________________________________________\n\nAs you can see, the 1056 parameters contained in the hidden1 layer are no longer trainable. The layer is now frozen. If you have not assigned names to the layers and you want to find out what the layers are called, you can use the model.summary() function or you can simply loop through the layers in the model with this:\n\nfor layer in model.layers: print (layer.name)\n\nThis code will give you the following output:\n\ninput hidden1\n\nNote that model.layers is simply a list with layers as elements. As such, you can use the classical way of accessing elements from a list. For example, to access the last layer, you can use:\n\nmodel.layers[-1]\n\n49\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nOr to access the first layer, use:\n\nmodel.layers[0]\n\nTo freeze the last layer, for example, you can simply use:\n\nmodel.layers[-1].trainable = False\n\nNote when you change a property of a layer in Keras, like the trainable property, remember to recompile the model with the compile() function. otherwise, the change will not take effect during the training.\n\nTo summarize, consider the following code6:\n\nx = Input(shape=(4,)) layer = Dense(8) layer.trainable = False y = layer(x) frozen_model = Model(x, y)\n\nNow, if we run the following code:\n\nfrozen_model.compile(optimizer='Adam', loss='mse') frozen_model.fit(data, labels)\n\nIt will not modify the weights of layer. In fact, calling frozen_model.\n\nsummary() gives us this:\n\n6 Check the official documentation for the example at https://keras.io/ getting-started/faq/#how-can-i-freeze-keras-layers.\n\n50\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== input_1 (InputLayer) (None, 4) 0 _______________________________________________________________ dense_6 (Dense) (None, 8) 40 =============================================================== Total params: 40 Trainable params: 0 Non-trainable params: 40 _______________________________________________________________\n\nAs expected, there are no trainable parameters. We can simply modify\n\nthe layer.trainable property:\n\nlayer.trainable = True trainable_model = Model(x, y)\n\nNow we compile and fit the model:\n\ntrainable_model.compile(optimizer='Adam', loss='mse') trainable_model.fit(data, labels)\n\nThis time the weights of layer will be updated. We can check on that\n\nwith trainable_model.summary():\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== input_1 (InputLayer) (None, 4) 0 _______________________________________________________________ dense_6 (Dense) (None, 8) 40 ===============================================================\n\n51\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nTotal params: 40 Trainable params: 40 Non-trainable params: 0 _______________________________________________________________\n\nNow all the parameters are trainable, as we wanted.\n\nRemoving Layers\n\nIt’s very useful to remove one or more of the last layers in a model and add different ones to fine-tune it. The idea is used very often in transfer learning, when you train a network and want to fine- tune its behavior by training only the last few layers. Let’s consider the following model:\n\nmodel = Sequential() model.add(Dense(32, activation='relu', input_dim=784, name = 'input')) model.add(Dense(32, activation='relu', name = 'hidden1')) model.add(Dense(32, activation='relu', name = 'hidden2'))\n\nThe summary() call will give this output:\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== input (Dense) (None, 32) 25120 _______________________________________________________________ hidden1 (Dense) (None, 32) 1056 _______________________________________________________________ hidden2 (Dense) (None, 32) 1056 ===============================================================\n\n52\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nTotal params: 27,232 Trainable params: 27,232 Non-trainable params: 0 _______________________________________________________________\n\nSay you want to build a second model, keeping your trained weights in the input and hidden1 layers, but you want to substitute the hidden2 layer with a different layer (let’s say one with 16 neurons). You can easily do that in the following way:\n\nmodel2 = Sequential() for layer in model.layers[:-1]: model2.add(layer)\n\nThis gives you:\n\nLayer (type) Output Shape Param # =============================================================== input (Dense) (None, 32) 25120 _______________________________________________________________ hidden1 (Dense) (None, 32) 1056 =============================================================== Total params: 26,176 Trainable params: 26,176 Non-trainable params: 0 _______________________________________________________________\n\nAt this point, you can simply add a new layer with the following:\n\nmodel2.add(Dense(16, activation='relu', name = 'hidden3'))\n\n53\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nIt has the following structure:\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== input (Dense) (None, 32) 25120 _______________________________________________________________ hidden1 (Dense) (None, 32) 1056 _______________________________________________________________ hidden3 (Dense) (None, 16) 528 =============================================================== Total params: 26,704 Trainable params: 26,704 Non-trainable params: 0 _______________________________________________________________\n\nAfter that, remember to compile your model. For example, for a\n\nregression problem, your code may look like this:\n\nmodel.compile(loss='mse', optimizer='Adam', metrics=['mse'])\n\nKeras Callback Functions\n\nIt is instructive to understand a bit better what Keras callback functions are since they are used quite often while developing models. This is from the official documentation7:\n\nA callback is a set of functions to be applied at given stages of the training procedure.\n\nThe idea is that you can pass a list of callback functions to the .fit()\n\nmethod of the Sequential or Model classes. Relevant methods of the\n\n7 https://keras.io/callbacks/\n\n54",
      "page_number": 61
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 69-76)",
      "start_page": 69,
      "end_page": 76,
      "detection_method": "topic_boundary",
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\ncallbacks will then be called at each stage of training [https://keras.io/ callbacks/, Accessed 01/02/2019]. Keunwoo Choi has written a nice overview on how to write a callback class that you can find at https:// goo.gl/hL37wq. We summarize it here and expand it with some practical examples.\n\nCustom Callback Class\n\nThe abstract base class, called Callback, can be found at the time of this writing at\n\ntensorflow/python/keras/callbacks.py (https://goo.gl/uMrMbH). To start, you need to define a custom class. The main methods you\n\nwant to redefine are typically the following\n\n\n\non_train_begin: Called at the beginning of training\n\n\n\non_train_end: Called at the end of training\n\n\n\non_epoch_begin: Called at the start of an epoch\n\n\n\non_epoch_end: Called at the end of an epoch\n\n\n\non_batch_begin: Called right before processing a batch\n\n\n\non_batch_end: Called at the end of a batch\n\nThis can be done with the following code:\n\nimport keras class My_Callback(keras.callbacks.Callback): def on_train_begin(self, logs={}): return\n\ndef on_train_end(self, logs={}): return\n\ndef on_epoch_begin(self, epoch, logs={}): return\n\n55\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\ndef on_epoch_end(self, epoch, logs={}): return\n\ndef on_batch_begin(self, batch, logs={}): return\n\ndef on_batch_end(self, batch, logs={}): self.losses.append(logs.get('loss')) return\n\nEach of the methods has slightly different inputs that you may use in\n\nyour class. Let’s look at them briefly (you can find them in the original Python code at https://goo.gl/uMrMbH).\n\non_epoch_begin, on_epoch_end\n\nArguments:\n\nepoch: integer, index of epoch.\n\nlogs: dictionary of logs.\n\non_train_begin, on_train_end\n\nArguments:\n\nlogs: dictionary of logs.\n\non_batch_begin, on_batch_end\n\nArguments:\n\nbatch: integer, index of batch within the current\n\nepoch.\n\nlogs: dictionary of logs.\n\nLet’s see with an example of how we can use this class.\n\n56\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nExample of a Custom Callback Class\n\nLet’s again consider the MNIST example. It’s the same code you have seen by now:\n\nimport tensorflow as tf from tensorflow import keras (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n\ntrain_labels = train_labels[:5000] test_labels = test_labels[:5000]\n\ntrain_images = train_images[:5000].reshape(-1, 28 * 28) / 255.0 test_images = test_images[:5000].reshape(-1, 28 * 28) / 255.0\n\nLet’s define a Sequential model for our example:\n\nmodel = tf.keras.models.Sequential([ keras.layers.Dense(512, activation=tf.keras.activations.\n\nrelu, input_shape=(784,)), keras.layers.Dropout(0.2), \\keras.layers.Dense(10, activation=tf.keras.activations.\n\nsoftmax)\n\n])\n\nmodel.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_\n\ncrossentropy,\n\nmetrics=['accuracy'])\n\n57\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nNow let’s write a custom callback class, redefining only one of the methods to see the inputs. For example, let’s see what the logs variable contains at the beginning of the training:\n\nclass CustomCallback1(keras.callbacks.Callback): def on_train_begin(self, logs={}): print (logs) return\n\nYou can then use it with:\n\nCC1 = CustomCallback1() model.fit(train_images, train_labels, epochs = 2, validation_data = (test_images,test_labels), callbacks = [CC1]) # pass callback to training\n\nRemember to always instantiate the class and pass the CC1 variable,\n\nand not the class itself. You will get the following:\n\nTrain on 5000 samples, validate on 5000 samples {} Epoch 1/2 5000/5000 [==============================] - 1s 274us/step - loss: 0.0976 - acc: 0.9746 - val_loss: 0.2690 - val_acc: 0.9172 Epoch 2/2 5000/5000 [==============================] - 1s 275us/step - loss: 0.0650 - acc: 0.9852 - val_loss: 0.2925 - val_acc: 0.9114 {} <tensorflow.python.keras.callbacks.History at 0x7f795d750208>\n\nThe logs dictionary is empty, as you can see from the {}. Let’s expand\n\nour class a bit:\n\n58\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nclass CustomCallback2(keras.callbacks.Callback): def on_train_begin(self, logs={}): print (logs) return\n\ndef on_epoch_end(self, epoch, logs={}): print (\"Just finished epoch\", epoch) print (logs) return\n\nNow we train the network with this:\n\nCC2 = CustomCallback2() model.fit(train_images, train_labels, epochs = 2, validation_data = (test_images,test_labels), callbacks = [CC2]) # pass callback to training\n\nThis will give the following output (reported here for just one epoch for\n\nbrevity):\n\nTrain on 5000 samples, validate on 5000 samples {} Epoch 1/2 4864/5000 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9879 Just finished epoch 0 {'val_loss': 0.2545496598124504, 'val_acc': 0.9244, 'loss': 0.05098680723309517, 'acc': 0.9878}\n\nNow things are starting to get interesting. The logs dictionary now\n\ncontains a lot more information that we can access and use. In the dictionary, we have val_loss, val_acc, and acc. So let’s customize our output a bit. Let’s set verbose = 0 in the fit() call to suppress the standard output and then generate our own.\n\n59\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nOur new class will be:\n\nclass CustomCallback3(keras.callbacks.Callback): def on_train_begin(self, logs={}): print (logs) return\n\ndef on_epoch_end(self, epoch, logs={}): print (\"Just finished epoch\", epoch) print ('Loss evaluated on the validation dataset\n\n=',logs.get('val_loss'))\n\nprint ('Accuracy reached is', logs.get('acc')) return\n\nWe can train our network with:\n\nCC3 = CustomCallback3() model.fit(train_images, train_labels, epochs = 2, validation_data = (test_images,test_labels), callbacks = [CC3], verbose = 0) # pass callback to\n\ntraining\n\nWe will get this:\n\n{} Just finished epoch 0 Loss evaluated on the validation dataset = 0.2546206972360611\n\nThe empty {} simply indicates the empty logs dictionary that on_train_begin received. Of course, you can print information every few epochs. For example, by modifying the on_epoch_end() function as follows:\n\ndef on_epoch_end(self, epoch, logs={}): if (epoch % 10 == 0): print (\"Just finished epoch\", epoch)\n\n60\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nprint ('Loss evaluated on the validation dataset\n\n=',logs.get('val_loss'))\n\nprint ('Accuracy reached is', logs.get('acc')) return\n\nYou will get the following output if you train your network for 30\n\nepochs:\n\n{} Just finished epoch 0 Loss evaluated on the validation dataset = 0.3692033936366439 Accuracy reached is 0.9932 Just finished epoch 10 Loss evaluated on the validation dataset = 0.3073081444747746 Accuracy reached is 1.0 Just finished epoch 20 Loss evaluated on the validation dataset = 0.31566708440929653 Accuracy reached is 0.9992 <tensorflow.python.keras.callbacks.History at 0x7f796083c4e0>\n\nNow you should start to get an idea as to how you can perform several\n\nthings during the training. A typical use of callbacks that we will look at in the next section is saving your model every few epochs. But you can, for example, save accuracy values in lists to be able to plot them later, or simply plot metrics to see how your training is going.\n\nSave and Load Models\n\nIt is often useful to save a model on disk, in order to be able to continue the training at a later stage, or to reuse a previously trained model. To see how you can do this, let’s consider the MNIST dataset again for the sake\n\n61\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nof giving a concrete example.8 The entire code is available in a dedicated notebook in the book’s GitHub repository in the chapter 2 folder.\n\nYou will need the following imports:\n\nimport os import tensorflow as tf from tensorflow import keras\n\nAgain, let’s load the MNIST dataset and take the first 5000\n\nobservations.\n\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data() train_labels = train_labels[:5000] test_labels = test_labels[:5000] train_images = train_images[:5000].reshape(-1, 28 * 28) / 255.0 test_images = test_images[:5000].reshape(-1, 28 * 28) / 255.0\n\nThen let’s build a simple Keras model using a Dense layer with 512 neurons, a bit of dropout, and the classical 10 neuron output layer for classification (remember the MNIST dataset has 10 classes).\n\nmodel = tf.keras.models.Sequential([ keras.layers.Dense(512, activation=tf.keras.activations.\n\nrelu, input_shape=(784,)), keras.layers.Dropout(0.2), keras.layers.Dense(10, activation=tf.keras.activations.\n\nsoftmax)\n\n])\n\nmodel.compile(optimizer='adam',\n\n8 The example was inspired by the official Keras documentation at https://www. tensorflow.org/tutorials/keras/save_and_restore_models.\n\n62",
      "page_number": 69
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 77-86)",
      "start_page": 77,
      "end_page": 86,
      "detection_method": "topic_boundary",
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nloss=tf.keras.losses.sparse_categorical_\n\ncrossentropy,\n\nmetrics=['accuracy'])\n\nWe added a bit of dropout, since this model has 407’050 trainable parameters. You can check this number simply by using model.summary(). What we need to do is define where we want to save the model on the\n\ndisk. And we can do that (for example) in this way:\n\ncheckpoint_path = \"training/cp.ckpt\" checkpoint_dir = os.path.dirname(checkpoint_path)\n\nAfter that, we need to define a callback (remember what we did in the\n\nlast section) that will save the weights:\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_\n\nonly=True, verbose=1)\n\nNote that now we don’t need to define a class as we did in the previous\n\nsection, since ModelCheckpoint inherits from the Callback class.\n\nThen we can simply train the model, specifying the correct callback\n\nfunction:\n\nmodel.fit(train_images, train_labels, epochs = 10, validation_data = (test_images,test_labels), callbacks = [cp_callback])\n\nIf you run a !ls command, you should see at least three files:\n\n\n\ncp.ckpt.data-00000-of-00001: Contains the weights (in case the number of weights is large, you will get many files like this one)\n\n63\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\n\n\ncp.ckpt.index: This file indicates which weights are in which files\n\n\n\ncheckpoint: This text file contains information about the checkpoint itself\n\nWe can now test our method. The previous code will give you a model\n\nthat will reach an accuracy on the validation dataset of roughly 92%. Now if we define a second model as so:\n\nmodel2 = tf.keras.models.Sequential([ keras.layers.Dense(512, activation=tf.keras.activations.\n\nrelu, input_shape=(784,)), keras.layers.Dropout(0.2), keras.layers.Dense(10, activation=tf.keras.activations.\n\nsoftmax)\n\n])\n\nmodel2.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_\n\ncrossentropy,\n\nmetrics=['accuracy'])\n\nAnd we check its accuracy on the validation dataset with this:\n\nloss, acc = model2.evaluate(test_images, test_labels) print(\"Untrained model, accuracy: {:5.2f}%\".format(100*acc))\n\nWe will get an accuracy of roughly 8.6%. That was expected, since this model has not been trained yet. But now we can load the saved weights in this model and try again.\n\nmodel2.load_weights(checkpoint_path) loss,acc = model2.evaluate(test_images, test_labels) print(\"Second model, accuracy: {:5.2f}%\".format(100*acc))\n\n64\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nWe should get this result:\n\n5000/5000 [==============================] - 0s 50us/step Restored model, accuracy: 92.06%\n\nThat makes again sense, since the new model is now using the weights on the old trained model. Keep in mind that, to load pre-trained weights in a new model, the new model needs to have the exact same architecture as the original one.\n\nNote to use saved weights with a new model, the new model must have the same architecture as the one used to save the weights. Using pre-trained weights can save you a lot of time, since you don't need to waste time training the network again.\n\nAs we will see again and again, the basic idea is to use callbacks and define a custom one that will save our weights. Of course, we can customize our callback function. For example, if want to save the weights every 100 epochs, each time with a different filename so that we can restore a specific checkpoint if needed, we must first define the filename in a dynamic way:\n\ncheckpoint_path = \"training/cp-{epoch:04d}.ckpt\" checkpoint_dir = os.path.dirname(checkpoint_path)\n\nWe should also use the following callback:\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint( checkpoint_path, verbose=1, save_weights_only=True, period=1)\n\nNote that checkpoint_path can contain named formatting options (in the name we have {epoch:04d}), which will be filled by the values of epoch and keys in logs (passed in on_epoch_end, which we saw in the previous\n\n65\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nsection).9 You can check the original code for tf.keras.callbacks. ModelCheckpoint and you will find that the formatting is done in the on_ epoch_end(self, epoch, logs) method:\n\nfilepath = self.filepath.format(epoch=epoch + 1, **logs)\n\nYou can define your filename with the epoch number and the values\n\ncontained in the logs dictionary.\n\nLet’s get back to our example. Let’s start by saving the first version of\n\nthe model:\n\nmodel.save_weights(checkpoint_path.format(epoch=0))\n\nThen we can fit the model as usual:\n\nmodel.fit(train_images, train_labels, epochs = 10, callbacks = [cp_callback], validation_data = (test_images,test_labels), verbose=0)\n\nBe careful since this will save lots of files. In our example, one file every epoch. So, for example, your directory content (obtainable with !ls training) may look like this:\n\ncheckpoint cp-0006.ckpt.data-00000-of-00001 cp-0000.ckpt.data-00000-of-00001 cp-0006.ckpt.index cp-0000.ckpt.index cp-0007.ckpt.data-00000-of-00001 cp-0001.ckpt.data-00000-of-00001 cp-0007.ckpt.index cp-0001.ckpt.index cp-0008.ckpt.data-00000-of-00001 cp-0002.ckpt.data-00000-of-00001 cp-0008.ckpt.index cp-0002.ckpt.index cp-0009.ckpt.data-00000-of-00001 cp-0003.ckpt.data-00000-of-00001 cp-0009.ckpt.index cp-0003.ckpt.index cp-0010.ckpt.data-00000-of-00001\n\n9 Check the official documentation at https://goo.gl/SnKgyQ.\n\n66\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\ncp-0004.ckpt.data-00000-of-00001 cp-0010.ckpt.index cp-0004.ckpt.index cp.ckpt.data-00000-of-00001 cp-0005.ckpt.data-00000-of-00001 cp.ckpt.index cp-0005.ckpt.index\n\nA last tip before moving on is how to get the latest checkpoint, without bothering to search its filename. This can be done easily with the following code:\n\nlatest = tf.train.latest_checkpoint('training') model.load_weights(latest)\n\nThis will load the weights saved in the latest checkpoint automatically.\n\nThe latest variable is simply a string and contains the last checkpoint filename. In our example, that is training/cp- 0010.ckpt.\n\nNote the checkpoint files are binary files that contain the weights of your model. so you will not be able to read them directly, and you should not need to.\n\nSave Your Weights Manually\n\nOf course, you can simply save your model weights manually when you are done training, without defining a callback function:\n\nmodel.save_weights('./checkpoints/my_checkpoint')\n\nThis command will generate three files, all starting with the string you\n\ngave as a name. In this case, it’s my_checkpoint. Running the previous code will generate the three files we described above:\n\ncheckpoint my_checkpoint.data-00000-of-00001 my_checkpoint.index\n\n67\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nReloading the weights in a new model is as simple as this:\n\nmodel.load_weights('./checkpoints/my_checkpoint')\n\nKeep in mind that, to be able to reload saved weights in a new model, the old model must have the same architecture as the new one. It must be exactly the same.\n\nSaving the Entire Model\n\nKeras also allows you to save the entire model on disk: weights, the architecture, and the optimizer. In this way, you can recreate the same model by moving some files. For example, we could use the following code\n\nmodel.save('my_model.h5')\n\nThis will save in one file, called my_model.h5, the entire model. You can simply move the file to a different computer and recreate the same trained model with this code:\n\nnew_model = keras.models.load_model('my_model.h5')\n\nNote that this model will have the same trained weights of your original\n\nmodel, so it’s ready to use. This may be helpful if you want to stop training your model and continue the training on a different machine, for example. Or maybe you must stop the training for a while and continue at a later time.\n\nDataset Abstraction\n\nThe tf.data.Dataset10 is a new abstraction in TensorFlow that is very useful for building data pipelines. It’s also very useful when you are dealing with datasets that do not fit in memory. We will see how to use it in more\n\n10 https://www.tensorflow.org/guide/datasets\n\n68\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\ndetail later in the book. In the next sections, I give you some hints on a couple of ways in which you can use it in your projects. To learn how to use it, a good starting point is to study the official documentation at https://www.tensorflow.org/guide/datasets. Remember: Always start there when you want to learn more about a specific method or feature of TensorFlow.\n\nBasically, a Dataset it is simply a sequence of elements, in which each\n\nelement contains one or more tensors. Typically, each element will be one training example or a batch of them. The basic idea is that first you create a Dataset with some data, and then you chain method calls on it. For example, you apply the Dataset.map() to apply a function to each element. Note that a dataset is made up of elements, each with the same structure.\n\nAs usual, let’s consider an example to understand how this works and\n\nhow to use it. Let’s suppose we have as input a matrix of 10 rows and 10 columns, defined by the following:\n\ninp = tf.random_uniform([10, 10])\n\nWe can simply create a dataset with the following:\n\ndataset = tf.data.Dataset.from_tensor_slices(inp)\n\nUsing print(dataset), will get you this output:\n\n<TensorSliceDataset shapes: (10,), types: tf.float32>\n\nThat tells you that each element in the dataset is a tensor with 10 elements (the rows in the inp tensor). A nice possibility is to apply specific functions to each element in a dataset. For example, we could multiply all elements by two:\n\ndataset2 = dataset.map(lambda x: x*2)\n\n69\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nIn order to check what happened, we could print the first element in\n\neach dataset. This can be easily done (more on that later) with:\n\ndataset.make_one_shot_iterator().get_next()\n\nand\n\ndataset2.make_one_shot_iterator().get_next()\n\nFrom the first line, you will get (your number will be different since we\n\nare dealing with random numbers here):\n\n<tf.Tensor: id=62, shape=(10,), dtype=float32, numpy= array([0.2215631 , 0.32099664, 0.04410303, 0.8502971 , 0.2472974 , 0.25522232, 0.94817066, 0.7719344 , 0.60333145, 0.75336015], dtype=float32)>\n\nAnd from the second line, you get:\n\n<tf.Tensor: id=71, shape=(10,), dtype=float32, numpy= array([0.4431262 , 0.6419933 , 0.08820605, 1.7005942 , 0.4945948 , 0.51044464, 1.8963413 , 1.5438688 , 1.2066629 , 1.5067203 ], dtype=float32)>\n\nAs expected, the second output contains all numbers of the first\n\nmultiplied by two.\n\nNote tf.data.dataset is designed to build data processing pipelines. For example, in image recognition you could do data augmentation, preparation, normalization, and so on in this way.\n\n70\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nI strongly suggest you check the official documentation to get more\n\ninformation on different ways of applying a function to each element. For example, you may need to apply transformation to the data and then flatten the result (see flat_map(), for example).\n\nIterating Over a Dataset\n\nOnce you have your dataset, you probably want to process the elements one by one, or in batches. To do that, you need an iterator. For example, to process the elements that you defined before one by one, you can instantiate a so-called make_one_shot_iterator() as follows:\n\niterator = dataset.make_one_shot_iterator()\n\nThen you can iterate over the elements using the get_next() method:\n\nfor i in range(10): value = print(iterator.get_next())\n\nThis will give you all the elements in the dataset. They will look like this\n\none (note that your number will be different):\n\ntf.Tensor( [0.2215631 0.32099664 0.04410303 0.8502971 0.2472974 0.25522232 0.94817066 0.7719344 0.60333145 0.75336015], shape=(10,), dtype=float32)\n\nNote that once you reach the end of the dataset, using the method\n\nget_next() will raise a tf.errors.OutOfRangeError.\n\n71\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nSimple Batching\n\nThe most fundamental way to batch consists of stacking n consecutive elements of a dataset in a single group. This will be very useful when we train our networks with mini-batches. This can be done using the batch() method. Let’s get back to our example. Remember that our dataset has 10 elements. Suppose we want to create batches, each having two elements. This can be done with this code:\n\nbatched_dataset = dataset.batch(2)\n\nNow let’s define an iterator again with:\n\niterator = batched_dataset.make_one_shot_iterator()\n\nNow let’s check what get_next() will return with this:\n\nprint(iterator.get_next())\n\nThe output will be:\n\ntf.Tensor( [[0.2215631 0.32099664 0.04410303 0.8502971 0.2472974 0.25522232 0.94817066 0.7719344 0.60333145 0.75336015] [0.28381765 0.3738917 0.8146689 0.20919728 0.5753969 0.9356725 0.7362906 0.76200795 0.01308048 0.14003313]], shape=(2, 10),\n\ndtype=float32)\n\nThat is two elements of our dataset.\n\nNote Batching with the batch() method is really useful when we train a neural network with mini-batches. we don’t have to bother creating the batches ourselves, as tf.data.dataset will do it for us.\n\n72",
      "page_number": 77
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 87-98)",
      "start_page": 87,
      "end_page": 98,
      "detection_method": "topic_boundary",
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nSimple Batching with the MNIST Dataset\n\nTo try the following code, you may want to restart the kernel you are using to avoid to conflicts with eager execution from the previous examples. Once you have done that, load the data (as before):\n\nnum_classes = 10\n\nmnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimage_vector_size = 28*28 x_train = x_train.reshape(x_train.shape[0], image_vector_size) x_test = x_test.reshape(x_test.shape[0], image_vector_size)\n\ny_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes)\n\nThen create the training Dataset:\n\nmnist_ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n\nNow build the Keras model using a simple feed-forward network with\n\ntwo layers:\n\nimg = tf.placeholder(tf.float32, shape=(None, 784)) x = Dense(128, activation='relu')(img) # fully-connected layer with 128 units and ReLU activation x = Dense(128, activation='relu')(x) preds = Dense(10, activation='softmax')(x) labels = tf.placeholder(tf.float32, shape=(None, 10)) loss = tf.reduce_mean(categorical_crossentropy(labels, preds))\n\ncorrect_prediction = tf.equal(tf.argmax(preds,1), tf.argmax(labels,1))\n\n73\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\ntrain_step = tf.train.AdamOptimizer(0.001).minimize(loss) init_op = tf.global_variables_initializer()\n\nNow we need to define the batch size:\n\ntrain_batched = mnist_ds_train.batch(1000)\n\nAnd now let’s define the iterator:\n\ntrain_iterator = train_batched.make_initializable_iterator() # So we can restart from the beginning next_batch = train_iterator.get_next() it_init_op = train_iterator.initializer\n\nThe it_init_op operation will be used to reset the iterator and will start from the beginning of each epoch. Note that the next_batch operation has the following structure:\n\n(<tf.Tensor 'IteratorGetNext_6:0' shape=(?, 784) dtype=uint8>, <tf.Tensor 'IteratorGetNext_6:1' shape=(?, 10) dtype=float32>)\n\nSince it contains the images and the labels. During our training, we will\n\nneed to get the batches in this form:\n\ntrain_batch_x, train_batch_y = sess.run(next_batch)\n\nFinally, let’s train our network:\n\nwith tf.Session() as sess: sess.run(init_op)\n\nfor epoch in range(50): sess.run(it_init_op) try:\n\n74\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nwhile True: train_batch_x, train_batch_y = sess.run(next_batch) sess.run(train_step,feed_dict={img: train_batch_x,\n\nlabels: train_batch_y})\n\nexcept tf.errors.OutOfRangeError: pass\n\nif (epoch % 10 == 0 ): print('epoch',epoch) print(sess.run(accuracy,feed_dict={img: x_train, labels: y_train}))\n\nNow, I have used a few tricks here that are good to know. In particular,\n\nsince you don’t know how many batches you have, you can use the following construct to avoid getting error messages:\n\ntry: while True: # Do something except tf.errors.OutOfRangeError: pass\n\nThis way, when you get an OutOfRangeError when you run out of batches, the exception will simply go on without interrupting your code. Note how, for each epoch, we call this code to reset the iterator:\n\nsess.run(it_init_op)\n\nOtherwise, we would get an OutOfRangeError immediately. Running\n\nthis code will get you to roughly 99% accuracy very fast. You should see output like this one (I show the output for epoch 40 only, for brevity):\n\nepoch 40 0.98903334\n\n75\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nThis quick overview of the dataset is not exhaustive by any means, but\n\nshould give you an idea of its power. If you want to learn more, the best place to do so is, as usual, the official documentation.\n\nNote tf.data.Dataset is an extremely convenient way of building pipelines for data, beginning from loading, to manipulating, normalizing, augmenting, and so on. especially in image-recognition problems, this can be very useful. remember that using it means adding nodes to your computational graph. so no data is processed until the session evaluates the graph.\n\nUsing tf.data.Dataset in Eager Execution Mode\n\nThis chapter ends with one final hint. If you are working in eager execution mode, your life with datasets is even easier. For example, to iterate over a batched dataset, you can simply do as you would do with classical Python (for x in ...). To understand what I mean, let’s look at an easy example. First, you need to enable eager execution:\n\nimport tensorflow as tf from tensorflow import keras import tensorflow.contrib.eager as tfe\n\ntf.enable_eager_execution()\n\nThen you can simply do this:\n\ndataset = tf.data.Dataset.from_tensor_slices(tf.random_ uniform([4, 2])) dataset = dataset.batch(2) for batch in dataset: print(batch)\n\n76\n\nChapter 2\n\ntensorFlow: advanCed topiCs\n\nThis can be very useful when you need to iterate over a dataset batch by batch. The output would be as follows (your numbers will be different, due to the tf.random.uniform() call):\n\ntf.Tensor( [[0.07181489 0.46992648] [0.00652897 0.9028846 ]], shape=(2, 2), dtype=float32) tf.Tensor( [[0.9167508 0.8379569 ] [0.33501422 0.3299384 ]], shape=(2, 2), dtype=float32)\n\nConclusions\n\nThis chapter had the goal of showing you a few techniques that we will use in this book and that will be very helpful to your projects. The goal was not to explain those methods in detail, as that would require a separate book. But the chapter should point you in the right direction when trying to do specific things, such as saving the weights of your model regularly. In the next chapters, we will use some of these techniques. If you want to learn a bit more about them, remember to always check the official documentation.\n\n77\n\nCHAPTER 3\n\nFundamentals of Convolutional Neural Networks\n\nIn this chapter, we will look at the main components of a convolutional neural network (CNN): kernels and pooling layers. We will then look at how a typical network looks. We will then try to solve a classification problem with a simple convolutional network and try to visualize the convolutional operation. The purpose of this is to try to understand, at least intuitively, how the learning works.\n\nKernels and Filters\n\nOne of the main components of CNNs are filters, which are square matrices that have dimensions nK × nK, where nK is an integer and is usually a small number, like 3 or 5. Sometimes filters are also called kernels. Using kernels comes from classical image processing techniques. If you have used Photoshop or similar software, you are used to do operations like\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_3\n\n79\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nsharpening, blurring, embossing, and so on.1 All those operations are done with kernels. We will see in this section what exactly kernels are and how they work. Note that in this book we will use both terms (kernels and filters) interchangeably. Let’s define four different filters and let’s check later in the chapter their effect when used in convolution operations. For those examples, we will work with 3 × 3 filters. For the moment, just take the following definitions as a reference and we will see how to use them later in the chapter.\n\nThe following kernel will allow the detection of\n\nhorizontal edges\n\nIH =\n\næ ç ç ç è\n\n1\n\n0 - 1\n\n1\n\n0 - 1\n\n1\n\n0 - 1\n\nö ÷ ÷ ÷ ø\n\nThe following kernel will allow the detection of vertical\n\nedges\n\nIV =\n\næ ç ç ç è\n\n1 0\n\n1 0\n\n1 0\n\n1 - 1 - 1\n\nö ÷ ÷ ÷ ø\n\nThe following kernel will allow the detection of edges\n\nwhen luminosity changes drastically\n\nIL =\n\næ ç ç ç è\n\n1 - 1 - 1\n\n1\n\n8 - 1\n\n1 - 1 - 1\n\nö ÷ ÷ ÷ ø\n\n1 You can find a nice overview on Wikipedia at https://en.wikipedia.org/wiki/ Kernel_(image_processing).\n\n80\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nThe following kernel will blur edges in an image\n\nIB = -\n\n1 9\n\næ ç ç ç è\n\n1 1 1\n\n1 1 1\n\n1 1 1\n\nö ÷ ÷ ÷ ø\n\nIn the next sections, we will apply convolution to a test image with the filters, to see what their effect is.\n\nConvolution\n\nThe first step to understanding CNNs is to understand convolution. The easiest way is to see it in action with a few simple cases. First, in the context of neural networks, convolution is done between tensors. The operation gets two tensors as input and produces a tensor as output. The operation is usually indicated with the operator *.\n\nLet’s see how it works. Consider two tensors, both with dimensions\n\n3 × 3. The convolution operation is done by applying the following formula:\n\næ ç ç ç è\n\na 1\n\na\n\n4\n\na\n\n7\n\na\n\na\n\na\n\n2\n\n5\n\n8\n\na\n\na\n\na\n\n3\n\n6\n\n9\n\nö ÷ ÷ ÷ ø\n\n\n\næ ç ç ç è\n\nk 1\n\nk\n\n4\n\nk 7\n\nk\n\nk\n\nk\n\n2\n\n5\n\n8\n\nk\n\n3\n\nk 6 k\n\n9\n\nö ÷ ÷ ÷ ø\n\n=\n\n9\n\nå\n\nii\n\n= 1\n\nia k i\n\nIn this case, the result is merely the sum of each element, ai, multiplied\n\nby the respective element, ki. In more typical matrix formalism, this formula could be written with a double sum as\n\næ ç ç ç è\n\na\n\na\n\na\n\n11\n\n21\n\n31\n\na\n\n12\n\na\n\n22\n\na\n\n32\n\na\n\n13\n\na\n\n23\n\na\n\n33\n\nö ÷ ÷ ÷ ø\n\n\n\næ ç ç ç è\n\nk 11 k\n\n21\n\nk\n\n3 11\n\nk 12 k k\n\n22\n\n32\n\nk 13 k k\n\n23\n\n33\n\nö ÷ ÷ ÷ ø\n\n=\n\n3\n\n3\n\nåå\n\na k ij ij = 1\n\ni\n\n= 1\n\nj\n\n81\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nHowever, the first version has the advantage of making the\n\nfundamental idea very clear: each element from one tensor is multiplied by the correspondent element (the element in the same position) of the second tensor, and then all the values are summed to get the result.\n\nIn the previous section, we talked about kernels, and the reason is that\n\nconvolution is usually done between a tensor, that we may indicate here with A, and a kernel. Typically, kernels are small, 3 × 3 or 5 × 5, while the input tensors A are normally bigger. In image recognition for example, the input tensors A are the images that may have dimensions as high as 1024 × 1024 × 3, where 1024 × 1024 is the resolution and the last dimension (3) is the number of the color channels, the RGB values.\n\nIn advanced applications, the images may even have higher resolution.\n\nTo understand how to apply convolution when we have matrices with different dimensions, let’s consider a matrix A that is 4 × 4\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1\n\na\n\n5\n\na\n\n9\n\na\n\n13\n\na\n\n2\n\na\n\n6\n\na\n\n10\n\na\n\n14\n\na\n\n3\n\na\n\n7\n\na\n\n11\n\na\n\n15\n\na\n\n4\n\na\n\n8\n\na\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\nAnd a Kernel K that we will take for this example to be 3 × 3\n\nK\n\n=\n\næ ç ç ç è\n\nk 1\n\nk\n\n4\n\nk 7\n\nk\n\nk\n\nk\n\n2\n\n5\n\n8\n\nk\n\n3\n\nk 6 k\n\n9\n\nö ÷ ÷ ÷ ø\n\nThe idea is to start in the top-left corner of the matrix A and select a\n\n3 × 3 region. In the example that would be\n\nA 1\n\n=\n\næ ç ç ç è\n\na 1 a\n\n5\n\na\n\n9\n\na a\n\n2\n\n6\n\na\n\n10\n\na a\n\n3\n\n7\n\na\n\n11\n\nö ÷ ÷ ÷ ø\n\n82\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nAlternatively, the elements marked in boldface here:\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1 a 5\n\na a\n\n9\n\n13\n\na\n\n2\n\na\n\n6\n\na a\n\n10\n\n14\n\na\n\n3\n\na\n\n7\n\na a\n\n11\n\n15\n\na\n\n4\n\na\n\n8\n\na\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\nThen we perform the convolution, as explained at the beginning between this smaller matrix A1 and K, getting (we will indicate the result with B1):\n\nB 1\n\n=\n\n\n\n=\n\nA K a k 1 1\n\n1\n\n+\n\na k 2 2\n\n+\n\na k 3 3\n\n+\n\nk a 4\n\n5\n\n+\n\nk a 5\n\n5\n\n+\n\nk a 7 6\n\n+\n\nk a 7\n\n9\n\n+\n\nk a\n\n8 10\n\n+\n\nk a\n\n9 11\n\nThen we need to shift the selected 3 × 3 region in matrix A of one\n\ncolumn to the right and select the elements marked in bold here:\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1\n\na\n\n5\n\na\n\n9\n\na\n\n13\n\na\n\n2\n\na\n\n6\n\na a\n\n10\n\n14\n\na\n\n3\n\na\n\n7\n\na a\n\n11\n\n15\n\na\n\n4\n\na\n\n8\n\na a\n\n12\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\nThis will give us the second sub-matrix A2:\n\nA 2\n\n=\n\næ ç ç ç è\n\na\n\n2\n\na\n\n6\n\na\n\n10\n\na\n\n3\n\na\n\n7\n\na\n\n11\n\na\n\n4\n\na\n\n8\n\na\n\n12\n\nö ÷ ÷ ÷ ø\n\nWe then perform the convolution between this smaller matrix A2\n\nand K again:\n\nB 2\n\n=\n\nA K a k 2 1\n\n\n\n=\n\n2\n\n+\n\na k 3 2\n\n+\n\na k 4 3\n\n+\n\na k 6 4\n\n+\n\na k 7 5\n\n+\n\na k 8 6\n\n+\n\na k 10 7\n\n+\n\na k 11 8\n\n+\n\na k 12 9\n\n83\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nWe cannot shift our 3 × 3 region anymore to the right, since we have reached the end of the matrix A, so what we do is shift it one row down and start again from the left side. The next selected region would be\n\nA 3\n\n=\n\næ ç ç ç è\n\na\n\n5\n\na\n\n9\n\na\n\n13\n\na\n\n6\n\na\n\n10\n\na\n\n14\n\na\n\n7\n\na\n\n11\n\na\n\n15\n\nö ÷ ÷ ÷ ø\n\nAgain, we perform convolution of A3 with K\n\nB\n\n3\n\n=\n\n\n\n=\n\nA K a k 5 1\n\n3\n\n+\n\na k 6 2\n\n+\n\na k 7 3\n\n+\n\na k 9 4\n\n+\n\na k 10 5\n\n+\n\na k 11 6\n\n+\n\na k 13 7\n\n+\n\na k 14 8\n\n+\n\na k 15 9\n\nAs you may have guessed at this point, the last step is to shift our 3 × 3 selected region to the right of one column and perform convolution again. Our selected region will now be\n\nA 4\n\n=\n\næ ç ç ç è\n\na\n\n6\n\na\n\n10\n\na\n\n14\n\na\n\n7\n\na\n\n11\n\na\n\n15\n\na\n\n8\n\na\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ø\n\nMoreover, the convolution will give this result:\n\nB\n\n4\n\n=\n\nA K a k 6 1\n\n\n\n=\n\n4\n\n+\n\na k 7 2\n\n+\n\na k 8 3\n\n+\n\na k 10 4\n\n+\n\na k 11 5\n\n+\n\na k 12 6\n\n+\n\na k 14 7\n\n+\n\na k 15 8\n\n+\n\na k 16 9\n\nNow we cannot shift our 3 × 3 region anymore, neither right nor down. We have calculated four values: B1, B2, B3, and B4. Those elements will form the resulting tensor of the convolution operation giving us the tensor B:\n\nB\n\n=\n\næ ç è\n\nB 1 B 3\n\nB 2 B\n\n4\n\nö ÷ ø\n\n84\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nThe same process can be applied when tensor A is bigger. You will simply get a bigger resulting B tensor, but the algorithm to get the elements Bi is the same. Before moving on, there is still a small detail that we need to discuss, and that is the concept of stride. In the previous process, we moved our 3 × 3 region always one column to the right and one row down. The number of rows and columns, in this example 1, is called the stride and is often indicated with s. Stride s = 2 means simply that we shift our 3 × 3 region two columns to the right and two rows down at each step. Something else that we need to discuss is the size of the selected region in the input matrix A. The dimensions of the selected region that we shifted around in the process must be the same as of the kernel used. If you use a 5 × 5 kernel, you will need to select a 5 × 5 region in A. In general, given a nK × nK kernel, you select a nK × nK region in A.\n\nIn a more formal definition, convolution with stride s in the neural network context is a process that takes a tensor A of dimensions nA × nA and a kernel K of dimensions nK × nK and gives as output a matrix B of dimensions nB × nB with\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\ns\n\nn\n\nK\n\n+\n\nú ûú1\n\nWhere we have indicated with ⌊x⌋ the integer part of x (in the programming world, this is often called the floor of x). A proof of this formula would take too long to discuss, but it’s easy to see why it is true (try to derive it). To make things a bit easier, suppose that nK is odd. You will see soon why this is important (although not fundamental). Let’s start explaining formally the case with a stride s = 1. The algorithm generates a new tensor B from an input tensor A and a kernel K according to the formula\n\nB ij\n\n=\n\n( A K\n\n) =\n\nij\n\nn\n\n1\n\nn\n\n1 A i 0\n\n1 A i 0\n\nK\n\nK\n\nå å\n\nf\n\n=\n\n0\n\nh\n\n+\n\nf\n\n+ j h\n\nK\n\ni\n\n+\n\n,\n\n+ j h\n\n85",
      "page_number": 87
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 99-108)",
      "start_page": 99,
      "end_page": 108,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nThe formula is cryptic and is very difficult to understand. Let’s study some more examples to grasp the meaning better. In Figure 3-1, you can see a visual explanation of how convolution works. Suppose to have a 3 × 3 filter. Then in the Figure 3-1, you can see that the top left nine elements of the matrix A, marked by a square drawn with a black continuous line, are the one used to generate the first element of the matrix B1 according to this formula. The elements marked by the square drawn with a dotted line are the ones used to generate the second element B2 and so on.\n\nFigure 3-1. A visual explanation of convolution\n\nTo reiterate what we discuss in the example at the beginning, the basic\n\nidea is that each element of the 3 × 3 square from matrix A is multiplied by the corresponding element of the kernel K and all the numbers are summed. The sum is then the element of the new matrix B. After having calculated the value for B1, you shift the region you are considering in the original matrix of one column to the right (the square indicated in Figure 3-1 with a dotted line) and repeat the operation. You continue to shift your region to the right until you reach the border and then you move one element down and start again from the left. You continue in this fashion until the lower right angle of the matrix. The same kernel is used for all the regions in the original matrix.\n\nGiven the kernel IH for example, you can see in Figure 3-2 which element of A are multiplied by which elements in IH and the result for the element B1, that is nothing else as the sum of all the multiplications\n\n= ´ + ´ + ´ + ´ + ´ + ´ + ´ -( 1\n\nB11 1 1 2 1 3 1 1 0 2 0 3 0 4\n\n)+ ´ -( 3 1\n\n)+ ´ -( 2 1\n\n)= - 3\n\n86\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-2. A visualization of convolution with the kernel IH\n\nIn Figure 3-3, you can see an example of convolution with stride s = 2.\n\nFigure 3-3. A visual explanation of convolution with stride s = 2\n\nThe reason that the dimension of the output matrix takes only the floor\n\n(the integer part) of\n\nn\n\nA\n\nK- n s\n\n+1\n\nCan be seen intuitively in Figure 3-4. If s > 1, what can happen, depending on the dimensions of A, is that at a certain point you cannot shift your window on matrix A (the black square you can see in Figure 3-3 for example) anymore, and you cannot cover all of matrix A completely. In Figure 3-4, you can see how you would need an additional column to the right of matrix A (marked by many X) to be able to perform the convolution\n\n87\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\noperation. In Figure 3-4, we chose s = 3, and since we have nA = 5 and nK = 3, B will be a scalar as a result.\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\ns\n\nn\n\nK\n\n+\n\nú ûú= 1\n\nê ëê\n\n5 3 3\n\n+\n\nú ûú= 1\n\nê ëê\n\n5 3\n\nú ûú=\n\n1\n\nFigure 3-4. A visual explanation of why the floor function is needed when evaluating the resulting matrix B dimensions\n\nYou can easily see from Figure 3-4, how with a 3 × 3 region, one can only cover the top-left region of A, since with stride s = 3 you would end up outside A and therefore can consider one region only for the convolution operation. Therefore, you end up with a scalar for the resulting tensor B.\n\nLet’s now look at a few additional examples to make this formula even\n\nmore transparent. Let’s start with a small matrix 3 × 3\n\nA =\n\næ ç ç ç è\n\n1 2 3\n\n4 5 6\n\n7 8 9\n\nö ÷ ÷ ÷ ø\n\nMoreover, let’s consider the kernel\n\nK\n\n=\n\næ ç ç ç è\n\nk 1 k\n\n4\n\nk 7\n\nk k\n\nk\n\n2\n\n5\n\n8\n\nk k 6 k\n\n3\n\n9\n\nö ÷ ÷ ÷ ø\n\n88\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nwith stride s = 1. The convolution will be given by\n\nB A K\n\n= * = × + × 2 1\n\nk 1\n\nk\n\n2\n\n+ × 3\n\nk\n\n3\n\n+ × 4\n\nk\n\n4\n\n+ × 5\n\nk\n\n5\n\n+ × 6\n\nk 6\n\n+ × 7\n\nk 7\n\n+ × 8\n\nk\n\n8\n\n+ × 9\n\nk\n\n9\n\nMoreover, the result B will be a scalar, since nA = 3, nK = 3.\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\ns\n\nn\n\nK\n\n+\n\nú ûú= 1\n\nê ëê\n\n3 3 1\n\n+\n\nú ûú= 1\n\n1\n\nIf you consider a matrix A with dimensions 4 × 4, or nA = 4, nK = 3 and\n\ns = 1, you will get as output a matrix B with dimensions 2 × 2, since\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\ns\n\nn\n\nK\n\n+\n\nú ûú= 1\n\nê ëê\n\n4 3 1\n\n+\n\nú ûú= 1\n\n2\n\nFor example, you can verify that given\n\nA =\n\næ ç ç ç ç è\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10 11 12\n\n13 14 15 16\n\nö ÷ ÷ ÷ ÷ ø\n\nAnd\n\nK =\n\næ ç ç ç è\n\n1 2 3\n\n4 5 6\n\n7 8 9\n\nö ÷ ÷ ÷ ø\n\nWe have with stride s = 1\n\n= * B A K\n\n=\n\næ ç è\n\n348 393 528 573\n\nö ÷ ø\n\n89\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nWe’ll verify one of the elements: B11 with the formula I gave you. We have\n\nB\n\n11\n\n=\n\n2\n\n2\n\nå å A + 1\n\n+ , 1\n\nh\n\nK\n\n+ 1\n\nf\n\n+ , 1\n\nh\n\n=\n\n2\n\nå\n\n(\n\nA\n\n+ 1\n\nf\n\n,\n\n1\n\nK\n\n+ 1\n\nf\n\n, 1\n\n+\n\nA\n\n+ 1\n\nf\n\n,\n\n2\n\nK\n\n+ 1\n\n2 ,\n\n+\n\nA\n\n+ 1\n\nf\n\n3\n\nfK + 1\n\n3 ,\n\n)\n\n=\n\n0\n\nh\n\n=\n\n0\n\nf\n\n=\n\n0\n\n=\n\n( +\n\nA ( (\n\n+\n\n1 1 ,\n\nK\n\n1 1 ,\n\n+\n\nA\n\n1 2 ,\n\nK\n\n1 2 ,\n\nA\n\n, 3 1\n\nK\n\n, 3 1\n\n+\n\nA\n\n, 3 2\n\nK\n\n× +\n\n× +\n\n×\n\n9 7 10 8 11 9\n\n( K A AA )= × + × + 1 1 2 2\n\n)+ K\n\n+\n\n+\n\nK\n\nA\n\n1 3 ,\n\n2 1 ,\n\n1 3 ,\n\n2 1 ,\n\n(\n\nA\n\n+ )=\n\n, 3 3\n\n, 3 2\n\n, 3 3\n\n+\n\n+\n\n=\n\n14 92 242 348\n\n, 2 2\n\nK\n\n, 2 2\n\n+\n\n× 33 3\n\n)+\n\n(\n\n) × + × + × 5 4 6 5 7 6\n\nA\n\nK\n\n, 2 3\n\n, 2 3\n\nNote that the formula I gave you for the convolution works only for\n\nstride s = 1, but can be easily generalized for other values of s.\n\nThis calculation is very easy to implement in Python. The following\n\nfunction can evaluate the convolution of two matrices easily enough for s = 1 (you can do it in Python with existing functions, but I think it’s instructive to see how to do it from scratch):\n\nimport numpy as np def conv_2d(A, kernel): output = np.zeros([A.shape[0]-(kernel.shape[0]-1),\n\nA.shape[1]-(kernel.shape[0]-1)])\n\nfor row in range(1,A.shape[0]-1): for column in range(1, A.shape[1]-1): output[row-1, column-1] = np.tensordot(A[row-\n\n1:row+2, column-1:column+2], kernel)\n\nreturn output\n\nNote that the input matrix A does not even need to a square one, but it is assumed that the kernel is and that its dimension nK is odd. The previous example can be evaluated with the following code:\n\nA = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]]) K = np.array([[1,2,3],[4,5,6],[7,8,9]]) print(conv_2d(A,K))\n\n90\n\n)\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nThis gives the result:\n\n[[ 348. 393.] [ 528. 573.]]\n\nExamples of Convolution\n\nNow let’s try to apply the kernels we defined at the beginning to a test image and see the results. As a test image, let’s create a chessboard of dimensions 160 × 160 pixels with the code:\n\nchessboard = np.zeros([8*20, 8*20]) for row in range(0, 8): for column in range (0, 8): if ((column+8*row) % 2 == 1) and (row % 2 == 0): chessboard[row*20:row*20+20,\n\ncolumn*20:column*20+20] = 1\n\nelif ((column+8*row) % 2 == 0) and (row % 2 == 1): chessboard[row*20:row*20+20,\n\ncolumn*20:column*20+20] = 1\n\n91\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nIn Figure 3-5, you can see how the chessboard looks.\n\nFigure 3-5. The chessboard image generated with code\n\nNow let’s apply convolution to this image with the different kernels\n\nwith stride s = 1.\n\nUsing the kernel, IH will detect the horizontal edges. This can be\n\napplied with the code\n\nedgeh = np.matrix('1 1 1; 0 0 0; -1 -1 -1') outputh = conv_2d (chessboard, edgeh)\n\nIn Figure 3-6, you can see how the output looks. The image can be\n\neasily generated with this code:\n\nImport matplotlib.pyplot as plt plt.imshow(outputh)\n\n92\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-6. The result of performing a convolution between the kernel IH and the chessboard image\n\nNow you can understand why this kernel detects horizontal edges. Additionally, this kernel detects when you go from light to dark or vice versa. Note this image is only 158 × 158 pixels, as expected, since\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\ns\n\nn\n\nK\n\n+\n\nú ûú= 1\n\nê ëê\n\n160 3 1\n\n+\n\nú ûú= 1\n\nê ëê\n\n157 1\n\n+\n\nú ûú= êë 1\n\n158\n\núû=\n\n158\n\nNow let’s apply IV using this code:\n\nedgev = np.matrix('1 0 -1; 1 0 -1; 1 0 -1') outputv = conv_2d (chessboard, edgev)\n\n93\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nThis gives the result shown in Figure 3-7.\n\nFigure 3-7. The result of performing a convolution between the kernel IV and the chessboard image\n\nNow we can use kernel IL :\n\nedgel = np.matrix ('-1 -1 -1; -1 8 -1; -1 -1 -1') outputl = conv_2d (chessboard, edgel)\n\nThat gives the result shown in Figure 3-8.\n\n94\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-8. The result of performing a convolution between the kernel IL and the chessboard image\n\nMoreover, we can apply the blurring kernel IB :\n\nedge_blur = -1.0/9.0*np.matrix('1 1 1; 1 1 1; 1 1 1') output_blur = conv_2d (chessboard, edge_blur)\n\nIn Figure 3-9, you can see two plots—on the left the blurred image and\n\non the right the original one. The images show only a small region of the original chessboard to make the blurring clearer.\n\n95",
      "page_number": 99
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 109-116)",
      "start_page": 109,
      "end_page": 116,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-9. The effect of the blurring kernel IB. On the left is the blurred image and on the right is the original one.\n\nTo finish this section, let’s try to understand better how the edges can\n\nbe detected. Consider the following matrix with a sharp vertical transition, since the left part is full of 10 and the right part full of 0.\n\nex_mat = np.matrix('10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0')\n\nThis looks like this\n\nmatrix([[10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0]])\n\n96\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nLet’s consider the kernel IV . We can perform the convolution with\n\nthis code:\n\nex_out = conv_2d (ex_mat, edgev)\n\nThe result is as follows:\n\narray([[ 0., 0., 30., 30., 0., 0.], [ 0., 0., 30., 30., 0., 0.], [ 0., 0., 30., 30., 0., 0.], [ 0., 0., 30., 30., 0., 0.], [ 0., 0., 30., 30., 0., 0.], [ 0., 0., 30., 30., 0., 0.]])\n\nIn Figure 3-10, you can see the original matrix (on the left) and the output of the convolution on the right. The convolution with the kernel IV has clearly detected the sharp transition in the original matrix, marking with a vertical black line where the transition from black to white happens. For example, consider B11 = 0\n\nB\n\n=\n\n10 10 10\n\n1 - 1 - 1\n\n10 10 10\n\n1 0\n\næ ç ç çç è\n\nö ÷ ÷ ÷ ø ´ +\n\næ ç ç ç è ´- + 1 10 1 10 0 100\n\nö ÷ ÷ ÷ ø ´ +\n\næ ç ç ç è ´ +\n\nö ÷ ÷ ÷ ø ´- + 10 1 10 0 10\n\n\n\n=\n\n=\n\n\n\nI V\n\n10 10 10\n\n1 0\n\n10 10 10\n\n11\n\n10 10 10\n\n1 0\n\n10 10 10\n\n´ +\n\n´ +\n\n´ +\n\n1 10 1 10 0 10\n\n´- =\n\n1 0\n\nNote that in the input matrix\n\næ ç ç ç è\n\n10 10 10\n\n10 10 10\n\n10 10 10\n\nö ÷ ÷ ÷ ø\n\n97\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nthere is no transition, as all the values are the same. On the contrary, if you consider B13 you need to consider this region of the input matrix\n\næ ç ç ç è\n\n10 10 0\n\n10 10 0\n\n10 10 0\n\nö ÷ ÷ ÷ ø\n\nwhere there is a clear transition since the right-most column is made of zeros and the rest of 10. You get now a different result\n\n1 - 1 - 1\n\n1 0\n\n10 10 0\n\n10 10 0\n\næ ç ç ç è\n\nö ÷ ÷ ÷ ø ´\n\nö ÷ ÷ ÷ ø ´ + ´- + 10 1 10 0 0\n\næ ç ç ç è ´ +\n\nö ÷ ÷ ÷ ø\n\næ ç ç ç è ´ + ´- + 1 10 1 10 0 0\n\n1 0\n\n**\n\n10 10 0\n\nB\n\n=\n\nI V\n\n\n\n10 10 0\n\n=\n\n11\n\n10 10 0\n\n10 10 0\n\n1 0\n\n´ +\n\n+ 1 10 11 10 0 0\n\n=\n\n´ + ´- =\n\n1 30\n\nMoreover, this is precisely how, as soon as there is a significant change\n\nin values along the horizontal direction, the convolution returns a high value since the values multiplied by the column with 1 in the kernel will be more significant. When there is a transition from small to high values along the horizontal axis, the elements multiplied by -1 will give a result that is bigger in absolute value. Therefore the final result will be negative and big in absolute value. This is the reason that this kernel can also detect if you pass from a light color to a darker color and vice versa. If you consider the opposite transition (from 0 to 10) in a different hypothetical matrix A, you would have\n\n1 - 1 - 1\n\n1 0\n\n0 10 10\n\n0 10 10\n\nö ÷ ÷ ÷ ø ´ +\n\næ ç ç ç è\n\nö ÷ ÷ ÷ ø 1 0 1++\n\næ ç ç ç è ´- + ´ 1 0 1 10 0 10\n\nö ÷ ÷ ÷ ø ´- + ´ + 0 1 10 0 10\n\næ ç ç ç è\n\n\n\n=\n\n=\n\n**\n\nB\n\n1 0\n\n0 10 10\n\n0 10 10\n\nI V\n\n11\n\n0 10 10\n\n1 0\n\n0 10 10\n\n´ +\n\n= ´ +\n\n´ + 10 0 10\n\n´- = - 1\n\n30\n\nWe move from 0 to 10 along the horizontal direction.\n\n98\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-10. The result of the convolution of the matrix ex_mat with the kernel IV as described in the text\n\nNote how, as expected, the output matrix has dimensions 5 × 5 since\n\nthe original matrix has dimensions 7 × 7 and the kernel is 3 × 3.\n\nPooling\n\nPooling is the second operation that is fundamental in CNNs. This operation is much easier to understand than convolution. To understand it, let’s look at a concrete example and consider what is called max pooling. Consider the 4 × 4 matrix we discussed during our convolution discussion again:\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1\n\na\n\n5\n\na\n\n9\n\na\n\n13\n\na\n\n2\n\na\n\n6\n\na\n\n10\n\na\n\n14\n\na\n\n3\n\na\n\n7\n\na\n\n11\n\na\n\n15\n\na\n\n4\n\na\n\n8\n\na\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\n99\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nTo perform max pooling, we need to define a region of size nK × nK, analogous to what we did for convolution. Let’s consider nK = 2. What we need to do is start on the top-left corner of our matrix A and select a nK × nK region, in our case 2 × 2 from A. Here we would select\n\næ ç è\n\na 1\n\na\n\n5\n\na\n\na\n\n2\n\n6\n\nö ÷ ø\n\nAlternatively, the elements marked in boldface in the matrix A here:\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1 a 5 a\n\n9\n\na\n\n13\n\na\n\n2\n\na a\n\n6\n\n10\n\na\n\n14\n\na\n\n3\n\na\n\n7\n\na\n\n11\n\na\n\n15\n\na\n\n4\n\na\n\n8\n\na\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\nFrom the elements selected, a1, a2, a5 and a6, the max pooling operation selects the maximum value. The result is indicated with B1\n\nB 1\n\n=\n\ni\n\nmax = , 1 2 5 6 , ,\n\na i\n\nWe then need to shift our 2 × 2 window two columns to the right, typically the same number of columns the selected region has, and select the elements marked in bold:\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1\n\na\n\n5\n\na\n\n9\n\na\n\n13\n\na\n\n2\n\na\n\n6\n\na\n\n10\n\na\n\n14\n\na\n\n3\n\na a\n\n7\n\n11\n\na\n\n15\n\na\n\n4\n\na a\n\n8\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\n100\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nOr, in other words, the smaller matrix\n\næ ç è\n\na\n\na\n\n3\n\n7\n\na\n\na\n\n4\n\n8\n\nö ÷ ø\n\nThe max-pooling algorithm will then select the maximum of the values\n\nand give a result that we will indicate with B2\n\nB 2\n\n=\n\ni\n\nmax = , , 3 4 7 8 ,\n\na i\n\nAt this point we cannot shift the 2 × 2 region to the right anymore, so we shift it two rows down and start the process again from the left side of A, selecting the elements marked in bold and getting the maximum and calling it B3.\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1\n\na\n\n5\n\na\n\n9\n\na\n\n13\n\na\n\n2\n\na\n\n6\n\na\n\n10\n\na\n\n14\n\na\n\n3\n\na\n\n7\n\na\n\n11\n\na\n\n15\n\na\n\n4\n\na\n\n8\n\na\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\nThe stride s in this context has the same meaning we have already discussed in convolution. It’s simply the number of rows or columns you move your region when selecting the elements. Finally, we select the last region 2 × 2 in the bottom-lower part of A, selecting the elements a11, a12, a15, and a16. We then get the maximum and call it B4. With the values we obtain in this process, in the example the four values B1, B2, B3 and B4, we will build an output tensor:\n\nB\n\n=\n\næ ç è\n\nB 1 B 3\n\nB 2 B\n\n4\n\nö ÷ ø\n\n101\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nIn the example, we have s = 2. Basically, this operation takes as input\n\na matrix A, a stride s, and a kernel size nK (the dimension of the region we selected in the example before) and returns a new matrix B with dimensions given by the same formula we discussed for convolution:\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\ns\n\nn\n\nK\n\n+\n\nú ûú1\n\nTo reiterate this idea, start from the top-left of matrix A, take a region of dimensions nK × nK, apply the max function to the selected elements, then shift the region of s elements toward the right, select a new region again of dimensions nK × nK, apply the function to its values, and so on. In Figure 3-11 you can see how you would select the elements from matrix A with stride s = 2.\n\nFigure 3-11. A visualization of pooling with stride s = 2\n\nFor example, applying max-pooling to the input A\n\nA =\n\næ ç ç ç ç è\n\n1\n\n3\n\n4\n\n5\n\n4\n\n1\n\n13 15\n\n5\n\n7\n\n11 3\n\n21 6 2 1\n\nö ÷ ÷ ÷ ÷ ø\n\n102\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nWill get you this result (it’s very easy to verify it):\n\nB =\n\næ ç è\n\n4\n\n11\n\n15 21\n\nö ÷ ø\n\nSince four is the maximum of the values marked in bold.\n\nA =\n\næ ç ç ç ç è\n\n1\n\n3\n\n4 4\n\n5 1\n\n13 15\n\n5\n\n7\n\n11 3\n\n21 6\n\n1\n\n2\n\nö ÷ ÷ ÷ ÷ ø\n\nEleven is the maximum of the values marked in bold here:\n\nA =\n\næ ç ç ç ç è\n\n1\n\n3\n\n4\n\n5\n\n4\n\n1\n\n13 15\n\n5\n\n7\n\n11 3 21 6 2 1\n\nö ÷ ÷ ÷ ÷ ø\n\nAnd so on. It’s worth mentioning another way of doing pooling,\n\nalthough it’s not as widely used as max-pooling: average pooling. Instead of returning the maximum of the selected values, it returns the average.\n\nNote the most commonly used pooling operation is max pooling. average pooling is not as widely used but can be found in specific network architectures.\n\n103",
      "page_number": 109
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 117-124)",
      "start_page": 117,
      "end_page": 124,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nPadding\n\nSomething that’s worth mentioning here is padding. Sometimes, when dealing with images, it is not optimal to get a result from a convolution operation that has dimensions that are different from the original image. This is when padding is necessary. The idea is straightforward: you add rows of pixels on the top and bottom and columns of pixels on the right and left of the final images so the resulting matrices are the same size as the original. Some strategies fill the added pixels with zeros, with the values of the closest pixels and so on. For example, in our example, our ex_out matrix with zero padding would like like this\n\narray([[ 0., 0., 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 30., 30., 0., 0., 0.], [ 0., 0., 0., 30., 30., 0., 0., 0.], [ 0., 0., 0., 30., 30., 0., 0., 0.], [ 0., 0., 0., 30., 30., 0., 0., 0.], [ 0., 0., 0., 30., 30., 0., 0., 0.], [ 0., 0., 0., 30., 30., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0., 0., 0.]])\n\nOnly as a reference, in case you use padding p (the width of the rows\n\nand columns you use as padding), the final dimensions of the matrix B, in case of both convolution and pooling, is given by\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\n+\n\n2 p n s K\n\n+\n\nú 1 ûú\n\n104\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nNote when dealing with real images, you always have color images, coded in three channels: rGB. that means that convolution and pooling must be done in three dimensions: width, height, and color channel. this will add a layer of complexity to the algorithms.\n\nBuilding Blocks of a CNN\n\nConvolution and pooling operations are used to build the layers used in CNNs. In CNNs typically you can find the following layers\n\nConvolutional layers\n\nPooling layers\n\nFully connected layers\n\nFully connected layers are precisely what we have seen in all the previous chapters: a layer where neurons are connected to all neurons of previous and subsequent layers. You know them already. The other two require some additional explanation.\n\nConvolutional Layers\n\nA convolutional layer takes as input a tensor (which can be three- dimensional, due to the three color channels), for example an image, applies a certain number of kernels, typically 10, 16, or even more, adds a bias, applies ReLu activation functions (for example) to introduce non- linearity to the result of the convolution, and produces an output matrix B.\n\n105\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nNow in the previous sections, I showed you some examples of applying\n\nconvolutions with just one kernel. How can you apply several kernels at the same time? Well, the answer is straightforward. The final tensor (I use now the word tensor since it will not be a simple matrix anymore) B will have not two dimensions but three. Let’s indicate the number of kernels you want to apply with nc (the c is used since sometimes people talk about channels). You simply apply each filter to the input independently and stack the results. So instead of a single matrix B with dimensions nB × nB you get a final tensor B of dimensions nB × nB × nc. That means that this\n\nB\n\n, i j\n\n1 ,\n\n\" Î[ , n 1 , i j\n\nB\n\n]\n\nWill be the output of convolution of the input image with the first\n\nkernel, and\n\nB\n\n, i j\n\n,\n\n2\n\n\" Î[ , n 1 , i j\n\nB\n\n]\n\nWill be the output of convolution with the second kernel, and so on. The convolution layer simply transforms the input into an output tensor. However, what are the weights in this layer? The weights, or the parameters that the network learns during the training phase, are the elements of the kernel themselves. We discussed that we have nc kernels, each of nK × nK dimensions. That means that we have n nK c layer.\n\n2\n\nparameter in a convolutional\n\nNote the number of parameters that you have in a convolutional layer, n nK c in reducing overfitting, especially when dealing with large input images.\n\n, is independent from the input image size. this fact helps\n\n2\n\n106\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nSometimes this layer is indicated with the word POOL and\n\nthen a number. In our case, we could indicate this layer with POOL1. In Figure 3-12, you can see a representation of a convolutional layer. The input image is transformed by applying convolution with nc kernels in a tensor of dimensions nA × nA × nc.\n\nFigure 3-12. A representation of a convolutional layer2\n\nOf course, a convolutional layer must not necessarily be placed immediately after the inputs. A convolutional layer may get as input the output of any other layer of course. Keep in mind that usually, the input image will have dimensions nA × nA × 3, since an image in color has three channels: Red, Green, and Blue. A complete analysis of the tensors involved in a CNN when considering color images is beyond the scope of this book. Very often in diagrams, the layer is simply indicated as a cube or a square.\n\n2 Cat image source: https://www.shutterstock.com/\n\n107\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nPooling Layers\n\nA pooling layer is usually indicated with POOL and a number: for example, POOL1. It takes as input a tensor and gives as output another tensor after applying pooling to the input.\n\nNote a pooling layer has no parameter to learn, but it introduces additional hyperparameters: nK and stride v. typically, in pooling layers, you don't use any padding, since one of the reasons to use pooling is often to reduce the dimensionality of the tensors.\n\nStacking Layers Together\n\nIn CNNs you usually stack convolutional and pooling layer together. One after the other. In Figure 3-13, you can see a convolutional and a pooling layer stack. A convolutional layer is always followed by a pooling layer. Sometimes the two together are called a layer. The reason is that a pooling layer has no learnable weights and therefore it is merely seen as a simple operation that is associated with the convolutional layer. So be aware when you read papers or blogs and check what they intend.\n\nFigure 3-13. A representation of how to stack convolutional and pooling layers\n\n108\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nTo conclude this part of CNN in Figure 3-14, you can see an example of a CNN. In Figure 3-14, you see an example like the very famous LeNet-5 network, about which you can read more here: https://goo.gl/hM1kAL. You have the inputs, then two times convolution- pooling layer, then three fully connected layers, and then an output layers, with a softmax activation function to perform multiclass classification. I put some indicative numbers in the figure to give you an idea of the size of the different layers.\n\nFigure 3-14. A representation of a CNN similar to the famous LeNet-5 network\n\nNumber of Weights in a CNN\n\nIt is important to point out where the weights in a CNN are in the different layers.\n\nConvolutional Layer\n\nIn a convolutional layer, the parameters that are learned are the filters themselves. For example, if you have 32 filters, each of dimension 5x5, you will get 32x5x5=832 learnable parameters, since for each filter there is also a bias term that you will need to add. Note that this number is not dependent on the input image size. In a typical feed-forward neural network, the number of weights in the first layer is dependent on the input size, but not here.\n\n109\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nThe number of weights in a convolutional layer is, in general terms,\n\ngiven by the following:\n\nn n n K\n\n×\n\n×\n\nC\n\nK\n\n+\n\nn C\n\nPooling Layer\n\nThe pooling layer has no learnable parameters, and as mentioned, this is the reason it’s typically associated with the convolutional layer. In this layer (operation), there are no learnable weights.\n\nDense Layer\n\nIn this layer, the weights are the ones you know from traditional feed- forward networks. So the number depends on the number of neurons and the number of neurons in the preceding and subsequent layers.\n\nNote the only layers in a Cnn that have learnable parameters are the convolutional and dense layers.\n\nExample of a CNN: MNIST Dataset\n\nLet’s start with some coding. We will develop a very simple CNN and try to do classification on the MNIST dataset. You should know the dataset very well by now, from Chapter 2.\n\nWe start, as usual, by importing the necessary packages:\n\nfrom keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D from keras.utils import np_utils\n\n110\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nimport numpy as np import matplotlib.pyplot as plt\n\nWe need an additional step before we can start loading the data:\n\nfrom keras import backend as K K.set_image_dim_ordering('th')\n\nThe reason is the following. When you load images for your model, you\n\nwill need to convert them to tensors, each with three dimensions:\n\nNumber of pixels along the x-axis\n\nNumber of pixels along the y-axis\n\nNumber of color channels (in a gray image, this\n\nnumber is; if you have color images, this number is 3, one for each of the RGB channels)\n\nWhen doing convolution, Keras must know on which axis it finds the information. In particular, it is relevant to define if the index of the color channel’s dimension is the first or the last. To achieve this, we can define the ordering of the data with keras.backend.set_image_dim_ordering(). This function accepts as input a string that can assume two possible values:\n\n\n\n'th' (for the convention used by the library Theano): Theano expects the channel dimensions to be the second one (the first one will be the observation index).\n\n\n\n'tf' (for the convention used by TensorFlow): TensorFlow expects the channel dimension to be the last one.\n\nYou can use both, but simply pay attention when preparing the data to use the right convention. Otherwise, you will get error messages about tensor dimensions. In what follows, we will convert images in tensors having the color channel dimensions as the second one, as you can see later.\n\n111",
      "page_number": 117
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 125-132)",
      "start_page": 125,
      "end_page": 132,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nNow we are ready to load the MNIST data with this code:\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nThe code will deliver the images “flattened,” meaning each image will be a one-dimensional vector of 784 elements (28x28). We need to reshape them as proper images, since our convolutional layers want images as inputs. After that, we need to normalize the data (remember the images are in a grayscale, and each pixel can have a value from 0 to 255).\n\nX_train = X_train.reshape(X_train.shape[0], 1, 28, 28). astype('float32') X_test = X_test.reshape(X_test.shape[0], 1, 28, 28). astype('float32') X_train = X_train / 255.0 X_test = X_test / 255.0\n\nNote how, since we have defined the ordering as 'th', the number of\n\nchannels (in this case 1) is the second element of the X arrays. As a next step, we need to one-hot-encode the labels:\n\ny_train = np_utils.to_categorical(y_train) y_test = np_utils.to_categorical(y_test)\n\nWe know we have 10 classes so we can simply define them:\n\nnum_classes = 10\n\nNow let’s define a function to create and compile our Keras model:\n\ndef baseline_model(): # create model model = Sequential() model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28),\n\nactivation='relu'))\n\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n112\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nmodel.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # Compile model model.compile(loss='categorical_crossentropy',\n\noptimizer='adam', metrics=['accuracy'])\n\nreturn model\n\nYou can see a diagram of this CNN in Figure 3-15.\n\nFigure 3-15. A diagram depicting the CNN we used in the text. The numbers are the dimensions of the tensors produced by each layer.\n\nTo determine what kind of model we have, we simply use the model.summary() call. Let’s first create a model and then check it:\n\nmodel = baseline_model() model.summary()\n\n113\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nThe output (check the diagram form in Figure 3-15) is as follows:\n\n_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 24, 24) 832 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 32, 12, 12) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 32, 12, 12) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 4608) 0 _________________________________________________________________ dense_1 (Dense) (None, 128) 589952 _________________________________________________________________ dense_2 (Dense) (None, 10) 1290 ================================================================= Total params: 592,074 Trainable params: 592,074 Non-trainable params: 0\n\nIn case you are wondering why the max-pooling layer produces tensors\n\nof dimensions 12x12, the reason is that since we haven’t specified the stride, Keras will take as a standard value the dimension of the filter, which in our case is 2x2. Having input tensors that are 24x24 with stride 2 you will get tensors that are 12x12.\n\nThis network is quite simple. In the model we defined just one\n\nconvolutional and pooling layer, we added a bit of dropout, then we added a dense layer with 128 neurons and then an output layer for the softmax with 10 neurons. Now we can simply train it with the fit() method:\n\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=200, verbose=1)\n\n114\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nThis will train the network for only one epoch and should give you\n\noutput similar to this (your numbers may vary a bit):\n\nTrain on 60000 samples, validate on 10000 samples Epoch 1/1 60000/60000 [==============================] - 151s 3ms/step - loss: 0.0735 - acc: 0.9779 - val_loss: 0.0454 - val_acc: 0.9853\n\nWe have already reached good accuracy, without having any overfitting.\n\nNote when you pass to the compile() method the optimizer parameter, keras will use its standard parameters. if you want to change them, you need to define an optimizer separately. For example, to specify an adam optimizer with a starting learning rate of 0.001 you can use AdamOpt = adam(lr=0.001) and then pass it to the compile method with model. compile(optimizer=AdamOpt, loss='categorical_ crossentropy', metrics=['accuracy']).\n\nVisualization of CNN Learning Brief Digression: keras.backend.function( )\n\nSometime it’s useful to get intermediate results from a computational graph. For example, you may be interested in the output of a specific layer for debugging purposes. In low-level TensorFlow, you can simply evaluate in the session the relevant node in the graph, but it’s not so easy to understand how to do it in Keras. To find out, we need to consider what\n\n115\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nthe Keras backend is. The best way of explaining it is to cite the official documentation (https://keras.io/backend/):\n\nKeras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle low-level operations such as tensor products, convolutions, and so on itself. Instead, it relies on a specialized, well opti- mized tensor manipulation library to do so, serving as the “backend engine” of Keras.\n\nTo be complete, it is important to note that Keras uses (at the time of writing) three backends: the TensorFlow backend, the Theano backend, and the CNTK backend. When you want to write your own specific function, you should use the abstract Keras backend API that can be loaded with this code:\n\nfrom keras import backend as K\n\nUnderstanding how to use the Keras backend goes beyond the scope of this book (remember the focus of this book is not Keras), but I suggest you spend some time getting to know it. It may be very useful. For example, to reset a session when using Keras you can use this:\n\nkeras.backend.clear_session()\n\nWhat we are really interested in this chapter is a specific method\n\navailable in the backed: function(). Its arguments are as follows:\n\n\n\ninputs: List of placeholder tensors\n\noutputs: List of output tensors\n\nupdates: List of update ops\n\n\n\n**kwargs: Passed to tf.Session.run\n\n116\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nWe will use only the first two in this chapter. To understand how to use them, let’s consider for example the model we created in the previous sections:\n\nmodel = Sequential() model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu')) model.add(MaxPool2D(pool_size=(2, 2))) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(num_classes, activation='softmax'))\n\nHow do we get, for example, the output of the first convolutional layer?\n\nWe can do this easily by creating a function:\n\nget_1st_layer_output = K.function([model.layers[0]. input],[model.layers[0].output])\n\nThis will use the following arguments\n\n\n\ninputs: model.layers[0].input, which is the input of our network\n\noutputs: model.layers[0].output, which is the output\n\nof the first layer (with index 0)\n\nYou simply ask Keras to evaluate specific nodes in your computational\n\ngraph, given a specific set of inputs. Note that up to now we have only defined a function. Now we need to apply it to a specific dataset. For example, if we want to apply it to one single image, we could do this:\n\nlayer_conv_output = get_1st_layer_output([np.expand_dims(X_ test[21], axis=0)])[0]\n\n117\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nThis multidimensional array will have dimensions (1, 32, 24, 24) as expected: one image, 32 filters, 24x24 output. In the next section, we will use this function to see the effect of the learned filter in the network.\n\nEffect of Kernels\n\nIt is interesting to see what the effect of the learned kernels is on the input image. For this purpose, let’s take an image from the test dataset (if you shuffled your dataset, you may get a different digit at index 21).\n\ntst = X_test[21]\n\nNote how this array has dimensions (1,28,28). This is a six, as you can\n\nsee in Figure 3-16.\n\nFigure 3-16. The first image in the test dataset\n\n118\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nTo get the effect of the first layer (the convolutional one), we can use\n\nthe following code (explained in the previous section)\n\nget_1st_layer_output = K.function([model.layers[0]. input],[model.layers[0].output]) layer_conv_output = get_1st_layer_output([tst])[0]\n\nNote how the layer_conv_output is a multidimensional array, and it\n\nwill contain the convolution of the input image with each filter, stacked on top of each other. Its dimensions are (1,32,24,24). The first number is 1 since we applied the layer only to one single image, the second, 32, is the number of filters we have, and the second is 24 since, as we have discussed, the output tensor dimensions of a conv layer are given by\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\n+\n\n2 p n s K\n\n+\n\nú 1 ûú\n\nMoreover, in our case\n\nnB =\n\nê ëê\n\n28 5 1\n\n+\n\nú ûú= 1\n\n24\n\n119",
      "page_number": 125
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 133-141)",
      "start_page": 133,
      "end_page": 141,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-17. The test image (a 6) convoluted with the first 12 filters learned by the network\n\n120\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nSince in our network, we have nA = 28, p = 0, nK = 5, and stride s = 1. In Figure 3-17, you can see our test image convoluted with the first 12 filters (32 was too many for a figure).\n\nFrom Figure 3-17 you can see how different filters learn to detect\n\ndifferent features. For example, the third filter (as can be seen in Figure 3-18) learned to detect diagonal lines.\n\nFigure 3-18. The test image convoluted with the third filter. It learned to detect diagonal lines.\n\nOther filters learn to detect horizontal lines or other features.\n\nEffect of Max-Pooling\n\nThe subsequent step is to apply max pooling to the output of the convolutional layer. As we discussed, this will reduce the dimensions of the tensors and will try to (intuitively) condense the relevant information.\n\nYou can see the output on the tensor coming from the first 12 filters in\n\nFigure 3-19.\n\n121\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-19. The output of the pooling layer when applied to the first 12 tensors coming from the convolutional layer\n\n122\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nLet’s see how our test image is transformed from one convolutional\n\nand the pooling layer by one of the filters (consider the third, just for illustration purposes). You can easily see the effect in Figure 3-20.\n\nFigure 3-20. The original test image as in the dataset (in panel a); the image convoluted with the third learned filter (panel b); the image convoluted with the third filter after the max pooling layer (panel c)\n\nNote how the resolution of the image is changing, since we are not using any padding. In the next chapter, we will look at more complicated architectures, called inception networks, that are known to work better than traditional CNN (what we have described in this chapter) when dealing with images. In fact, simply adding more and more convolutional layers will not easily increase the accuracy of the predictions, and more complex architecture are known to be much more effective.\n\nNow that we have seen the very basic building blocks of a CNN, we are ready to move to some more advanced topics. In the next chapter, we will look at many exciting topics as inception networks, multiple loss functions, custom loss functions, and transfer learning.\n\n123\n\nCHAPTER 4\n\nAdvanced CNNs and Transfer Learning\n\nIn this chapter, we look at more advanced techniques typically used when developing CNNs. In particular, we will look at a very successful new convolutional network called the inception network that is based on the idea of several convolutional operations done in parallel instead of sequentially. We will then look at how to use the multiple cost function, in a similar fashion as what is done in multi-task learning. The next sections will show you how to use the pre-trained network that Keras makes available, and how to use transfer learning to tune those pre-trained networks for your specific problem. At the end of the chapter, we will look at a technique to implement transfer learning that is very efficient when dealing with big datasets.\n\nConvolution with Multiple Channels\n\nIn the previous chapter, you learned how convolution works. In the examples we have explicitly described how to perform it when the input is a bi-dimensional matrix. But this is not what happens in reality. For example, the input tensors may represent color images, and therefore will have three dimensions: the number of pixels in the x direction (resolution along the x axis), the number of pixels in the y direction (resolution along\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_4\n\n125\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nthe y axis), and the number of color channels, that is three when dealing with RGB images (one channel for the reds, one for the greens, and one for the blues). It can be even worse. A convolutional layer with 32 kernels, each 5 × 5, when expecting an input of images each 28 × 28 (see the MNIST example in the previous chapter) will have an output of dimensions (m, 32, 24, 24), where m is the number of training images. That means that our convolutions will have to be done with tensors with dimensions of 32 × 24 × 24. So how we can perform the convolutional operation on three-dimensional tensors? Well, it is actually quite easy. Mathematically speaking, if kernel K has dimensions nK × nK × nc, and the input tensors A have dimensions nx × ny × nc, the result of our convolution operation will be:\n\nn\n\nx\n\nn\n\ny\n\nn c\n\nååå\n\nK A ijk ijk\n\n= 1\n\nj\n\n= 1\n\nk\n\n= 1\n\nMeaning that we will sum over the channel dimension. In Keras, when\n\nyou define a convolutional layer in 2D, you use the following code:\n\nConv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu')\n\nWhere the first number (32) is the number of filters and (5,5) defines\n\nthe dimensions of the kernels. What Keras does not tell you is that it automatically takes kernels of nc × 5 × 5 where nc is the number of channels of the input tensors. This is why you need to give the first layer the input_ shape parameter. The number of channels is included in this information. But which of the three numbers is the correct one? How can Keras know that the right one is 1 in this case and not a 28?\n\nLet’s look at the concrete example we looked in the previous chapter\n\nmore in depth. Let’s suppose we import the MNIST dataset with this code:\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n126\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nIn the previous chapter, we reshaped the input tensors with\n\nX_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n\nAs you will notice, we added a dimension of 1 before the x and y dimensions of 28. The 1 is the number of channels in the image: since it’s a grayscale image, it has only one channel. But we could have added the number of channels also after the x and y dimensions of 28. That was our choice. We can tell Keras which dimension to take with the code that we discussed in Chapter 3:\n\nK.set_image_dim_ordering('th')\n\nThis line is important, since Keras needs to know which one is the channel dimension in order to be able to extract the right channel dimension for the convolutional operation. Remember that for the kernels we specify only x and y dimensions, so Keras needs to find the third dimension by itself: in this case a 1. You will remember that a value of 'th' will expect the channel dimension to come before the x, y dimensions, while a value of 'tf' will expect the channel dimension to be the last one. So, it is just a matter of being consistent. You tell Keras with the code above, where the channel dimension is and then reshape your data accordingly. Let’s consider a few additional examples to make the concept even clearer.\n\nLet’s suppose we consider the following network with set_image_\n\ndim_ordering('th') (we will neglect the dimension for the number of observations m) when using MNIST images as the input:\n\nInput tensors shape: 1×28×28 Convolutional Layer 1 with 32 kernels, each 5×5: output shape 32×24×24 Convolutional Layer 2 with 16 kernels, each 3×3: output shape 16×22×22\n\n127\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nThe kernels in the second convolutional layer will have dimensions of 32 × 3 × 3. The number of channels coming from the first convolutional layer (32) do not play a role in determining the dimensions of the output of the second convolutional layer, since we sum over that dimension. In fact, if we change the number of kernels in the first layer to 128, we get the following dimensions:\n\nInput tensors shape: 1×28×28 Convolutional Layer 1 with 32 kernels, each 5×5: output shape 128×24×24 Convolutional Layer 2 with 16 kernels, each 3×3: output shape 16×22×22\n\nAs you can see, the output dimensions of the second layer have not\n\nchanged at all.\n\nNote Keras infers automatically the channel dimensions when creating the filters, so you need to tell Keras which one is the right dimension with set_image_dim_ordering() and then reshape your data accordingly.\n\nWHY A 1 × 1 CONVOLUTION REDUCES DIMENSIONALITY\n\nin this chapter we will look at inception networks, and we will use 1 × 1 kernels, with the reasonsing that those reduce dimensionality. at first it seems counter-intuitive, but you need to remember from the previous section discussion, that a filter always has a third dimension. Consider the following set of layers:\n\nInput tensors shape: 1 × 28 × 28\n\nConvolutional Layer 1 with 32 kernels, each 5 × 5: output shape\n\n128 × 24 × 24\n\n128\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nConvolutional Layer 2 with 16 kernels, each 1 × 1: output shape\n\n16 × 24 × 24\n\nnote how the layer with 1 × 1 kernels reduces the dimensions of the previous layer. it changes the dimensions from 128 × 24 × 24 to 16 × 24 × 24. a 1 × 1 kernel will not change the x, y dimensions of the tensors but it will change the channel dimension. this is the reason why, if you read blogs or books on inception networks, you will read that those kernels are used to reduce dimensions of the tensors used.\n\nKernels 1 × 1 does not change the x, y dimensions of tensors, but will change the channel dimension. this is why they are often used to reduce dimensionality of the tensors flowing through a network.\n\nHistory and Basics of Inception Networks\n\nInception networks were first proposed in a famous paper by Szegedy et al. titled Going Deeper with Convolutions.1 This new architecture that we will discuss in detail is the result of the efforts to get better results in image recognition tasks without increasing the computational budget.2 Adding more and more layers will create models with more and more parameters that will be increasingly difficult and slow to train. Additionally the authors wanted to find methods that could be used on machines that may not be as powerful as the ones used in big data centers. As they state in the paper, their models were designed to keep a “computational budget of 1.5 billion multiply-adds at inference time”. It is important that inference is cheap, because then it can be done on devices that are not that powerful; for example, on mobile phones.\n\n1 The original paper can be accessed on the arXiv archive at this link: http://toe.lt/4. 2 With computational budget we determine the time and hardware resources needed to perform a specific computation (for example, training a network).\n\n129",
      "page_number": 133
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 142-149)",
      "start_page": 142,
      "end_page": 149,
      "detection_method": "topic_boundary",
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nNote that the goal of this chapter is not to analyze the entire original paper on inception networks, but to explain the new building blocks and techniques that have been used and show you how to use them in your projects. To develop inception networks, we will need to start using the functional Keras APIs, work with multiple loss functions, and perform operations on the dataset with layers that are evaluated in parallel and not sequentially. We will also not look at all variants of the architecture, because that would simply require us to list the results of a few papers and will not bring any additional value to the reader (that is better served by reading the original papers). If you are interested, the best advice I can give you is to download it and study the original paper. You will find lots of interesting information in there. But at the end of this chapter, you will have the tools to really understand those new networks and will be able to develop one with Keras.\n\nLet’s go back to “classical” CNNs. Typically, those have a standard structure: stacked convolutional layers (with pooling of course) followed by a set of dense layers. It is very tempting to just increase the number of layers or the number of kernels or their size to try to get a better result. This leads to overfitting issues and therefore requires heavy use of regularization techniques (like dropout) to try to counter this problem. Bigger sizes (both in the number of layers and kernel size and numbers) mean of course a larger number of parameters and therefore the need of increasingly high computational resources. To summarize, some of the main problems of “classical” CNNs are as follows:\n\n\n\nIt is very difficult to get the right kernel size. Each image is different. Typically, larger kernels are good for more globally distributed information, and smaller ones for locally distributed information.\n\nDeep CNNs are prone to overfitting.\n\nTraining and inference of networks with many parameters is computationally intensive.\n\n130\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nInception Module: Naïve Version\n\nTo overcome these difficulties, the main idea of Szegedy and the co- authors of the paper is to perform convolution with multiple-size kernels in parallel, to be able to detect features at different sizes at the same time, instead of adding convolutional layer after layer sequentially. Those kinds of networks are said to be going wider instead of deeper.\n\nFor example, we may do convolution with 1 × 1, 3 × 3 and 5 × 5 kernels,\n\nand even max pooling at the same time in parallel, instead of adding several convolutional layers, one after the other. In Figure 4-1, you can see how the different convolutions can be done in parallel in what is called the naïve inception module.\n\nFigure 4-1. Different convolutions with different kernel sizes done in parallel. This is the basic module used in inception networks called the inception module.\n\nIn the example in Figure 4-1, the 1 × 1 kernel will look at very localized information, while the 5 × 5 will be able to spot more global features. In the next section, we will look at how we can develop exactly that with Keras.\n\n131\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nNumber of Parameters in the Naïve Inception Module\n\nLet’s look at the difference in number of parameters between inception and classical CNNs. Let’s suppose we consider the example in Figure 4- 1. Let’s suppose the “previous layer” is the input layer with the MNIST dataset. For the sake of this comparison, we will use 32 kernels for all layers or convolutional operations. The number of parameters for each convolution operation in the naïve inception module is\n\n\n\n1 × 1 convolutions: 64 parameters3\n\n\n\n3 × 3 convolutions: 320 parameters\n\n\n\n5 × 5 convolutions: 832 parameters\n\nRemember that the max-pooling operations have no learnable parameters. In total, we have 1216 learnable parameters. Now let’s suppose we create a network with the three convolutional layers, one after the other. The first one with 32 1 × 1 kernels, then one with 32 3 × 3 kernels, and finally one with 32 5 × 5 kernels. Now the total number of parameters in the layers will be (remember that, for example, the convolutional layer with the 32 3 × 3 kernels will have as input the output of the convolutional layer with the 32 1 × 1 kernels):\n\nLayer with 1 × 1 convolutions: 64 parameters\n\nLayer with 3 × 3 convolutions: 9248 parameters\n\nLayer with 5 × 5 convolutions: 25632 parameters\n\nFor a total of 34944 learnable parameters. Roughly 30 times the\n\nnumber of the parameters as the inception version. You can easily see how such parallel processing reduces drastically the number of parameters that the model must learn.\n\n3 Remember in this case we have one weight and one bias.\n\n132\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nInception Module with Dimension Reduction\n\nIn the naïve inception module, we get a smaller number of learnable parameters with respect to classical CNNs, but we can actually do even better. We can use 1 × 1 convolutions at the right places (mainly before the higher dimension convolutions) to reduce dimensions. This allows us to use an increasing number of such modules without blowing up the computational budget. In Figure 4-2, you can see how such a module could look.\n\nFigure 4-2. Inception module example with dimension reduction\n\nIt is instructive to see how many learnable parameters we have in this module. To see where the dimensionality reduction really helps, let’s suppose that the previous layer is the output of a previous operation and that its output has the dimensions of 256, 28, 28. Now let’s compare the naïve module and the one with dimension reduction pictured in Figure 4- 2.\n\nNaïve module:\n\n\n\n1 × 1 convolutions with 8 kernels: 2056 parameters4\n\n\n\n3 × 3 convolutions with 8 kernels: 18440 parameters\n\n\n\n5 × 5 convolutions with 8 kernels: 51208 parameters\n\n4 Remember in this case we have one weight and one bias.\n\n133\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nFor a total of 71704 learnable parameters. Module with dimension reduction:\n\n\n\n1 × 1 convolutions with 8 kernels: 2056 parameters\n\n\n\n1 × 1 followed by the 3 × 3 convolutions: 2640 parameters\n\n\n\n1 × 1 followed by the 5 × 5 convolutions: 3664 parameters\n\n\n\n3 × 3 max pooling followed by the 1 × 1 convolutions: 2056 parameters\n\nFor a total of 10416 learnable parameters. Comparing the number of learnable parameters, you can see why this module is said to reduce dimensions. Thanks to a smart placement of 1 × 1 convolutions, we can prevent the number of learnable parameters from blowing up without control.\n\nAn inception network is simply built by stacking lots of those modules\n\none after the other.\n\nMultiple Cost Functions: GoogLeNet\n\nIn Figure 4-3, you can see the main structure of the GoogLeNet network that won the imagenet challenge. This network, as described in the paper referenced at the beginning, stacks several inception models one after the other. The problem is, as the authors of the original paper quickly discovered, the middle layers tend to “die”. Meaning they tend to stop playing any role in the learning. To keep them from “dying,” the authors introduced classifiers along the network, as depicted in Figure 4-3.\n\nEach part of the network (PART 1, PART 2, and PART 3 in Figure 4-3) will be trained as a stand-alone classifier. The training of the three parts does not happen independently, but at the same time, in a very similar way to what happens in multi-task learning (MTL).\n\n134\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nFigure 4-3. The high-level architecture of the GoogLeNet network\n\nTo prevent the middle part of the network from not being so effective and effectively dying out, the authors introduced two classifiers along the network, indicated in Figure 4-3 with the yellow boxes. They introduced two intermediate loss functions and then computed the total loss function as a weighted sum of the auxiliary losses, effectively using a total loss evaluated with this formula:\n\nTotal Loss = Cost Function 1 + 0.3 * (Cost Function 2) + 0.3 * (Cost Function 3)\n\nWhere Cost Function 1 is the cost function evaluated with PART 1, Cost Function 2 is evaluated with PART 2, and Cost Function 3 with PART 3. Testing has shown that this is quite effective and you get a much better result than simply training the entire network as a single classifier. Of course, the auxiliary losses are used only in training and not during inference.\n\nThe authors have developed several versions of inception networks, with increasingly complex modules. If you are interested, you should read the original papers as they are very instructive. A second paper with more a complex architecture by the authors can be found at https://arxiv. org/pdf/1512.00567v3.pdf.\n\n135\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nExample of Inception Modules in Keras\n\nUsing the functional APIs of Keras makes building an inception module extremely easy. Let’s look at the necessary code. For space reasons, we will not build a complete model with a dataset, because that would take up too much space and would distract from the main learning goal, which is to see how to use Keras to build a network with layers that are evaluated in parallel instead of sequentially.\n\nLet’s suppose for the sake of this example that our training dataset is the CIFAR10.5 This is made of images, all 32 × 32 with three channels (the images are in color). So first we need to define the input layer of our network:\n\nfrom keras.layers import Input input_img = Input(shape = (32, 32, 3))\n\nThen we simply define one layer after the other:\n\nfrom keras.layers import Conv2D, MaxPooling2D tower_1 = Conv2D(64, (1,1), padding='same', activation='relu') (input_img) tower_1 = Conv2D(64, (3,3), padding='same', activation='relu') (tower_1) tower_2 = Conv2D(64, (1,1), padding='same', activation='relu') (input_img) tower_2 = Conv2D(64, (5,5), padding='same', activation='relu') (tower_2) tower_3 = MaxPooling2D((3,3), strides=(1,1), padding='same') (input_img) tower_3 = Conv2D(64, (1,1), padding='same', activation='relu') (tower_3)\n\n5 You can find all information on the dataset at https://www.cs.toronto. edu/~kriz/cifar.html.\n\n136\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nThis code will build the module depicted in Figure 4-4. The Keras\n\nfunctional APIs are easy to use: you define the layers as functions of another layer. Each function returns a tensor of the appropriate dimensions. The nice thing is that you don’t have to worry about dimensions; you can simply define layer after layer. Just take care to use the right one for the input. For example, with this line:\n\ntower_1 = Conv2D(64, (1,1), padding='same', activation='relu') (input_img)\n\nYou define a tensor, named tower_1, that is evaluated after a\n\nconvolutional operation with the input_img tensor and 64 1 × 1 kernels. Then this line:\n\ntower_1 = Conv2D(64, (3,3), padding='same', activation='relu') (tower_1)\n\nDefines a new tensor that is obtained by the convolution of 64 3 × 3\n\nkernels with the output of the previous line. We have taken the input tensor, performed convolution with 64 1 × 1 kernels, and then performed convolution with 64 3 × 3 kernels again.\n\nFigure 4-4. The inception module built from the given code\n\n137",
      "page_number": 142
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 150-159)",
      "start_page": 150,
      "end_page": 159,
      "detection_method": "topic_boundary",
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nThe concatenation of the layers is easy:\n\nfrom keras.layers import concatenate from tensorflow.keras import optimizers output = concatenate([tower_1, tower_2, tower_3], axis = 3)\n\nNow let’s add the couple of necessary dense layers:\n\nfrom keras.layers import Flatten, Dense output = Flatten()(output) out = Dense(10, activation='softmax')(output)\n\nThen we finally create the model:\n\nfrom keras.models import Model model = Model(inputs = input_img, outputs = out)\n\nThis model can then be compiled and trained as usual. An example of\n\nusage could be\n\nepochs = 50 model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(), metrics=['accuracy']) model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n\nSupposing the training dataset is composed by the arrays (X_train\n\nand y_train) and the validation dataset by (X_test, y_test).\n\nNote have to use the padding='same' option, since all the outputs of the convolutional operations must have the same dimensions.\n\nin all convolutional operations in the inception module, you\n\nThis section gave you a brief introduction to how to develop more complex network architectures using the functional APIs of Keras. You should now have a basic understanding of how inception networks work and their basic building blocks.\n\n138\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nDigression: Custom Losses in Keras\n\nSometimes it is useful to be able to develop custom losses in Keras. From the official Keras documentation (https://keras.io/losses/):\n\nYou can either pass the name of an existing loss function or pass a TensorFlow/Theano symbolic function that returns a scalar for each data point and takes the following two arguments:\n\ny_true: True labels. TensorFlow/Theano tensor.\n\ny_pred: Predictions. TensorFlow/Theano tensor of the same shape as y_true.\n\nLet’s suppose we want to define a loss that calculates the average of the\n\npredictions. We would need to write this\n\nimport keras.backend as K def mean_predictions(y_true, y_pred): return K.mean(y_pred)\n\nAnd then we can simply use it in the compile call as follows:\n\nmodel.compile(optimizer='rmsprop', loss=mean_predictions, metrics=['accuracy'])\n\nAlthough this would not make so much sense as a loss. Now this starts\n\nto get interesting the moment where the loss function can be evaluated only using intermediate results from specific layers. But to do that, we need to use a small trick. Since, as per official documentation, the function can only accept as input true labels and predictions. To do this we need to create a function that return a function that accepts only the true labels and the predictions. Seems convoluted? Let’s look at an example to understand it. Let’s suppose we have this model:\n\n139\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\ninputs = Input(shape=(512,)) x1 = Dense(128, activation=sigmoid)(inputs) x2 = Dense(64, activation=sigmoid)(x1) predictions = Dense(10, activation='softmax')(x2) model = Model(inputs=inputs, outputs=predictions)\n\nWe can define a loss function that depends on x1 with this code6 (what\n\nthe loss is doing is not relevant):\n\ndef custom_loss(layer): def loss(y_true,y_pred): return K.mean(K.square(y_pred - y_true) +\n\nK.square(layer), axis=-1)\n\nreturn loss\n\nThen we can simply use the loss function as before:\n\nmodel.compile(optimizer='adam', loss=custom_loss(x1), metrics=['accuracy'])\n\nThis is an easy way to develop and use custom losses. It is also sometimes useful to be able to train a model with multiple losses, as described in the inception networks. Keras is ready for this. Once you define the loss functions you can use the following syntax\n\nmodel.compile(loss = [loss1,loss2], loss_weights = [l1,l2], ...)\n\nand Keras will then use as loss function\n\nl1*loss1+l2*loss2\n\n6 The code was inspired by http://toe.lt/7.\n\n140\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nConsider that each loss will only affect the weights that are on the path between the inputs and the loss functions. In Figure 4-5, you can see a network divided in different parts: A, B, and C. loss1 is calculated using the output of B, and loss2 of C. Therefore, loss1 will only affect the weights in A and B, while loss2 will affect weights in A, B and C, as you can see in Figure 4-5.\n\nFigure 4-5. A schematic representation of the influence of multiple loss functions on different network parts\n\nAs a side note, this technique is heavily used in what is called multi-\n\ntask learning (MTL).7\n\nHow To Use Pre-Trained Networks\n\nKeras makes pre-trained deep learning models available for you to use. The models, called applications, can be used for predictions on new data. The models have already been trained on big datasets, so there is no need for big datasets or long training sessions. You can find all applications information on the official documentation at https://keras.io/ applications/. At the moment of writing there are 20 models available, each a variation of one of the following:\n\nXception\n\nVGG16\n\n7 You can find more information at https://en.wikipedia.org/wiki/ Multi-task_learning\n\n141\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nVGG19\n\nResNet\n\nResNetV2\n\nResNeXt\n\n\n\nInceptionV3\n\n\n\nInceptionResNetV2\n\nMobileNet\n\nMobileNetV2\n\nDenseNEt\n\nNASNet\n\nLet’s look at one example, and while doing so, let’s discuss the different\n\nparameters used in the functions. The pre-ready models are all in the keras.applications package. Each model has its own package. For example, ResNet50 is in the keras.applications.resnet50. Let’s suppose we have one image we want to classify. We may use the VGG16 network, a well known network that is very successful in image recognition. We can start with the following code\n\nimport tensorflow as tf from tensorflow.keras.applications.vgg16 import VGG16 from tensorflow.keras.preprocessing import image from tensorflow.keras.applications.vgg16 import preprocess_ input , decode_predictions\n\nimport numpy as np\n\nThen we can simply load the model with a simple line\n\nmodel = VGG16(weights='imagenet')\n\n142\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nThe weights parameter is very important. If weights is None the weights are randomly initialized. That means that you get the VGG16 architecture and you can train it yourself. But be aware, it has roughly 138 million parameters, so you will need a really big training dataset and lots of patience (and a really powerful hardware). If you use the value imagenet, the weights are the ones obtained by training the network with the imagenet dataset.8 If you want a pre-trained network, you should use weights = 'imagenet'.\n\nIf you get an error message about certificates and you are on a Mac, there is an easy solution. The command above will try to download the weights over SSL and, if you just installed Python from python.org, the installed certificates will not work on your machine. Simply open a Finder window, navigate to the Applications/Python 3.7 (or the Python version you have installed), and double-click Install Certificates.command. A Terminal window will open, and a script will run. After that, the VGG16() call will work without an error message.\n\nAfter that, we need to tell Keras where the image is (let’s suppose you\n\nhave it in the folder where the Jupyter Notebook is) and load it:\n\nimg_path = 'elephant.jpg' img = image.load_img(img_path, target_size = (224, 224))\n\nYou can find the image in the GitHub repository in the folder for\n\nChapter 4. After that we need\n\nx = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x)\n\nFirst, you convert the image to an array, then you need to expand its dimensions. What is meant is the following: the model works with batches of images, meaning it will expect as input a tensor with four axes (index\n\n8 http://www.image-net.org\n\n143\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nin the batch of images, resolution along x, resolution along y, number of channels). But our image has only three dimensions, the horizontal and vertical resolutions and the number of channels (in our example three, for the RGB channels). We need to add one dimension for the samples dimension. To be more concrete, our image has dimensions (224,244,3), but the model expects a tensor of dimensions (1,224,224,3), so we need to add the first dimension.\n\nThis can be done with the numpy function expand_dims(), which simply inserts a new axis in the tensor.9 As a last step, you need to pre- process the input image, since each model expects something slightly different (normalized between +1 and -1, or between 0 and 1, and so on) with the preprocess_input(x) call.\n\nNow we are ready to let the model predict the class of the image with\n\nthe following:\n\npreds = model.predict(x)\n\nTo get the top three classes of the prediction, we can use the decode_\n\npredictions() function.\n\nprint('Predicted:', decode_predictions(preds, top=3)[0])\n\nIt will produce (with our image) the following predictions:\n\nPredicted: [('n02504013', 'Indian_elephant', 0.7278206), ('n02504458', 'African_elephant', 0.14308284), ('n01871265', 'tusker', 0.12798567)]\n\nThe decode_predictions() returns tuples in the form (class_name, class_description, score). The first cryptic string is the internal class name, the second is the description (what we are interested in), and the last one is the probability. It seems our image, according to the VGG16 network, is with 72.8% probability an Indian elephant. I am not an expert\n\n9 You can check the official documentation for the function at http://toe.lt/5.\n\n144\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\non elephants, but I will trust the model. To use a different pre-trained network (for example ResNet50), you need to change the following imports:\n\nfrom keras.applications.resnet50 import ResNet50 from keras.applications.resnet50 import preprocess_input, decode_predictions\n\nAnd the way you define the model:\n\nmodel = ResNet50(weights='imagenet')\n\nThe rest of the code remains the same.\n\nTransfer Learning: An Introduction\n\nTransfer learning is a technique where a model trained to solve a specific problem is re-purposed10 for a new challenge related to the first problem. Let’s suppose we have a network with many layers. In image recognition typically, the first layers will learn to detect generic features, and the last layers will be able to detect more specific ones.11 Remember that in a classification problem the last layer will have N softmax neurons (assuming we are classifiying N classes), and therefore must learn to be very specific to your problem. You can intuitively understand transfer learning with the following steps, where we introduce some notation we will use in the next sections and chapters. Let’s suppose we have a network with nL layers.\n\n1. We train a base network (or get a pre-trained model) on a big dataset (called a base dataset) related to our\n\n10 The term has been used by Yosinki in https://arxiv.org/abs/1411.1792. 11 You can find a very interesting paper on the subject by Yosinki et al. at https://\n\narxiv.org/abs/1411.1792.\n\n145\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nproblem. For example, if we want to classify dogs’ images, we may train a model in this step on the imagenet dataset (since we basically want to classify images). It is important that, at this step, the dataset has enough data and that the task is related to the problem we want to solve. Getting a network trained for speech recognition will not be good at dog images classification. This network will probably not be that good for your specific problem.\n\n2. We get a new dataset that we call a target dataset (for\n\nexample, dogs’ breeds images) that will be our new training dataset. Typically, this dataset will be much smaller than the one used in Step 1.\n\n3. You then train a new network, called target network,\n\non the target dataset. The target network will typically have the same first nk (with nk < nL) layers of our base network. The learnable parameters of the first layers (let’s say 1 to nk, with nk < nL) are inherited from the base network trained in Step 1, and are not changed during the training of the target network. Only the last and new layers (in our example from layer nK to nL) are trained. The idea is that layers from 1 to nk (from the base network) will learn enough features in Step 1 to distinguish dogs from other animals, and the layers nk to nL (in the target network) will learn the features needed to distinguish different breeds. Sometimes you can even train your entire target network using the weights inherited from the base network as the initial values of the weights, although this requires much more powerful hardware.\n\n146\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nNote layers inherited from the base network frozen, since otherwise it’s very easy to overfit the small dataset.\n\nif the target dataset is small, the best strategy is to keep the\n\nThe idea behind this is that you hope that in Step 1, the base network\n\nhas learned to extract generic features from images well enough and therefore you want to use this learned knowledge and avoid the need to learn it again. But to make predictions better, you want to fine-tune the predictions of your network for your specific case, optimizing how your target network extracts specific features (that typically happens in the last layers of a network) that are related to your problem.\n\nIn other words, you can think it this way. To recognize dog breeds, you\n\nimplicitly follow these steps:\n\n1. You look at an image and decide if it’s a dog or not.\n\n2.\n\nIf you are looking at a dog, you classify it into broad classes (for example, terrier).\n\n3. After that, you classify into sub-classes (for example,\n\na Welsh terrier or Tibetan terrier).\n\nTransfer learning is based on the idea that Steps 1 and possibly 2 can\n\nbe learned from a lot of generic images (for example from the imagenet dataset) from a base network, and that Step 3 can be learned by a much smaller dataset with the help of what has been learned in Step 1 and 2. When the target dataset is much smaller than the base dataset, this is a very powerful tool that will help avoiding overfitting of your training dataset.\n\nThis method is very useful when used with pre-trained models. For\n\nexample, using a VGG16 network trained on imagenet, and then re- training just the last layers is typically an extremely efficient way to solve specific image recognition problems. You get lots of features detection\n\n147",
      "page_number": 150
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 160-167)",
      "start_page": 160,
      "end_page": 167,
      "detection_method": "topic_boundary",
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\ncapabilities for free. Keep in mind that training such networks on the imagenet networks costs several thousands of GPU hours. It’s typically not doable for researchers without the needed hardware and know-how. In the next sections, we will look at how to do exactly that. With Keras, it’s really easy and it will allow you to solve image classification problems with an accuracy that would not otherwise be possible. In Figure 4-6, you can see a schematic representation of the transfer learning process.\n\nFigure 4-6. A schematic representation of the transfer learning process\n\n148\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nA Dog and Cat Problem\n\nThe best way to understand how transfer learning works in practice is to try it in practice. Our goal is to be able to classify images of dogs and cats as best as we can, with the least effort (in computational resources) as possible. To do that, we will use the dataset with dog and cat images that you can find on Kaggle at https://www.kaggle.com/c/dogs-vs-cats. Warning: The download is almost 800MB. In Figure 4-7, you can see some of the images we will have to classify.\n\nFigure 4-7. Random samples of the images contained in the dog versus cat dataset\n\n149\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nClassical Approach to Transfer Learning\n\nThe naïve way of solving this problem is to create a CNN model and train it with the images. First of all, we need to load the images and resize them to make sure they all have the same resolution. If you check the images in the dataset, you will notice that each has a different resolution. To do that let’s resize all the images to (150, 150) pixels. In Python, we would use this:\n\nimport glob import numpy as np import os\n\nimg_res = (150, 150)\n\ntrain_files = glob.glob('training_data/*') train_imgs = [img_to_array(load_img(img, target_size=img_res)) for img in train_files] train_imgs = np.array(train_imgs) train_labels = [fn.split('/')[1].split('.')[0].strip() for fn in train_files]\n\nvalidation_files = glob.glob('validation_data/*') validation_imgs = [img_to_array(load_img(img, target_size=img_ res)) for img in validation_files] validation_imgs = np.array(validation_imgs) validation_labels = [fn.split('/')[1].split('.')[0].strip() for fn in validation_files]\n\nSupposing we have 3000 training images in a folder called training_ data and 1000 validation images in a folder called validation_data, the shapes of the train_imgs and validation_imgs will be as follows:\n\n(3000, 150, 150, 3) (1000, 150, 150, 3)\n\n150\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nAs usual we will need to normalize the images. Each pixel now has a value between 0 and 255 and is an integer. So first we convert the numbers to floating point, and then we normalize them by dividing by 255, so that each value is now between 0 and 1.\n\ntrain_imgs_scaled = train_imgs.astype('float32') validation_imgs_scaled = validation_imgs.astype('float32') train_imgs_scaled /= 255 validation_imgs_scaled /= 255\n\nIf you check the train_labels you will see that they are strings: 'dog'\n\nor 'cat'. We need to transform the labels to integers, in particular into 0 and 1. To do that, we can use the Keras function called LabelEncoder.\n\nfrom sklearn.preprocessing import LabelEncoder le = LabelEncoder() le.fit(train_labels) train_labels_enc = le.transform(train_labels) validation_labels_enc = le.transform(validation_labels)\n\nWe can check the labels with this code:\n\nprint(train_labels[10:15], train_labels_enc[10:15])\n\nWhich will give this:\n\n['cat', 'dog', 'cat', 'cat', 'dog'] [0 1 0 0 1]\n\nNow we are ready to build our model. We can do this easily with the\n\nfollowing code:\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout from tensorflow.keras.models import Sequential from tensorflow.keras import optimizers\n\n151\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nmodel = Sequential()\n\nmodel.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) model.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten()) model.add(Dense(512, activation='relu')) model.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(), metrics=['accuracy'])\n\n152\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nThis is a small network that has this structure:\n\nLayer (type) Output Shape Param # ============================================================== conv2d_3 (Conv2D) (None, 148, 148, 16) 448 ______________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 74, 74, 16) 0 ______________________________________________________________ conv2d_4 (Conv2D) (None, 72, 72, 64) 9280 ______________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 36, 36, 64) 0 ______________________________________________________________ conv2d_5 (Conv2D) (None, 34, 34, 128) 73856 ______________________________________________________________ max_pooling2d_5 (MaxPooling2 (None, 17, 17, 128) 0 ______________________________________________________________ flatten_1 (Flatten) (None, 36992) 0 ______________________________________________________________ dense_2 (Dense) (None, 512) 18940416 ______________________________________________________________ dense_3 (Dense) (None, 1) 513 ============================================================== Total params: 19,024,513 Trainable params: 19,024,513 Non-trainable params: 0 ______________________________________________________________\n\nIn Figure 4-8, you can see a schematic representation of the network to\n\ngive you an idea of the layer sequence.\n\n153\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nFigure 4-8. A schematic representation of the network to give you an idea of the layer sequence\n\nAt this point we can train the network with the following:\n\nbatch_size = 30 num_classes = 2 epochs = 2 input_shape = (150, 150, 3) model.fit(x=train_imgs_scaled, y=train_labels_enc, validation_data=(validation_imgs_scaled, validation_labels_enc), batch_size=batch_size, epochs=epochs, verbose=1)\n\nWith two epochs, we get to about 69% validation accuracy and 70% training accuracy. Not really a good result. Let’s see if we can do better than this in just two epochs. The reason to do this in two epochs is merely a way of checking quickly different possibilities. Training such networks for many epochs can take easily few hours. Note that this model overfit the training data. That becomes clearly visible when training for more epochs, but the main goal here is not to get the best model, but to see how you can use pre-trained model to get better results, so we will ignore this problem.\n\n154\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nNow let’s import the VGG16 pre-trained network.\n\nfrom tensorflow.keras.applications import vgg16 from tensorflow.keras.models import Model import tensorflow.keras as keras\n\nbase_model=vgg16.VGG16(include_top=False, weights='imagenet')\n\nNote that the include_top=False parameter removes the last three fully connected layers of the network. In this way, we can append our own layers to the base network with the code:\n\nfrom tensorflow.keras.layers import Dense,GlobalAveragePooling2D x=base_model.output x=GlobalAveragePooling2D()(x) x=Dense(1024,activation='relu')(x) preds=Dense(1,activation='softmax')(x) model=Model(inputs=base_model.input,outputs=preds)\n\nWe added a pooling layer, then a Dense layer with 1024 neurons, and then an output layer with one neuron with a softmax activation function to do binary classification. We can check the structure with the following:\n\nmodel.summary()\n\nThe output is quite long, but at the end you will find this:\n\nTotal params: 15,242,050 Trainable params: 15,242,050 Non-trainable params: 0\n\n155",
      "page_number": 160
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 168-176)",
      "start_page": 168,
      "end_page": 176,
      "detection_method": "topic_boundary",
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nAll the 22 layers are trainable at the moment. To be able to really do transfer learning, we need to freeze all layers of the VGG16 base network. To do that we can do the following:\n\nfor layer in model.layers[:20]: layer.trainable=False for layer in model.layers[20:]: layer.trainable=True\n\nThis code will set the first 20 layers to a non trainable status, and the last two to a trainable status. Then we can compile our model as follows:\n\nmodel.compile(optimizer='Adam',loss='sparse_categorical_crossen tropy',metrics=['accuracy'])\n\nNote that we used loss='sparse_categorical_crossentropy' to be able to use the labels as they are, without having to hot-encode them. As we have done before, we can now train the network:\n\nmodel.fit(x=train_imgs_scaled, y=train_labels_enc, validation_data=(validation_imgs_scaled, validation_labels_enc), batch_size=batch_size, epochs=epochs, verbose=1)\n\nNote that although we are training only a portion of the network, this will require much more time than the simple network we tried before. The result will be an astounding 88% in two epochs. An incredibly better result than before! Your output should look something like this:\n\nTrain on 3000 samples, validate on 1000 samples Epoch 1/2 3000/3000 [==============================] - 283s 94ms/sample - loss: 0.3563 - acc: 0.8353 - val_loss: 0.2892 - val_acc: 0.8740\n\n156\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nEpoch 2/2 3000/3000 [==============================] - 276s 92ms/sample - loss: 0.2913 - acc: 0.8730 - val_loss: 0.2699 - val_acc: 0.8820\n\nThis was thanks to the pre-trained first layers, which saved us a lot of work.\n\nExperimentation with Transfer Learning\n\nWhat if we want to try different architectures for the target networks, and we want to add a few more layers and try again? The previous approach has a slight downside: we need to train the entire network each time event though only the last layers should be trained. As you see from the section above, one epoch took roughly 4.5 minutes. Can we be more efficient? Turns out we can.\n\nConsider the configuration depicted in Figure 4-9.\n\nFigure 4-9. A schematic representation of a more flexible way of doing transfer learning in practice\n\nThe idea is to generate a new dataset that we will call the feature dataset, with the frozen layers. Since they will not be changed by training, those layers will always generate the same output. We can use this feature dataset as new input for a much smaller network (that we will call the target subnetwork), made by only the new layers we added to the base\n\n157\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nlayer in the previous section. We will need to train only a few layers, and that will be much faster. The generation of the feature dataset will take some time, but this must be done only once. At this point you can test different architecture for the target subnetwork and find the best configuration for your problem. Let’s see how we can do that in Keras. The base dataset preparation is the same as before, so we will not do it again.\n\nLet’s import the VGG16 pre-trained network as before:\n\nfrom tensorflow.keras.applications import vgg16 from tensorflow.keras.models import Model import tensorflow.keras as keras\n\nvgg = vgg16.VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n\noutput = vgg.layers[-1].output output = keras.layers.Flatten()(output) vgg_model = Model(vgg.input, output)\n\nvgg_model.trainable = False for layer in vgg_model.layers: layer.trainable = False\n\nwhere input_shape is (150, 150, 3).\n\nWe can simply generate the features dataset with a few lines (using\n\nthe predict functionality):\n\ndef get_ features(model, input_imgs): features = model.predict(input_imgs, verbose=0) return features\n\ntrain_features_vgg = get_features(vgg_model, train_imgs_scaled) validation_features_vgg = get_features(vgg_model, validation_ imgs_scaled)\n\n158\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\nNote that this will take a few minutes on a modern laptop. On a modern MacBook Pro, this will take 40 CPU minutes, meaning that if you have more cores/threads it will take a fraction of it. On my laptop, it takes effectively four minutes. Remember that since we used the parameter include_top = False, the three dense layers at the end of the network have been removed. The train_features_vgg will contain just the output of the last layer of the base network without the last three dense layers. At this point we can simply build our target subnetwork:\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer from tensorflow.keras.models import Sequential from tensorflow.keras import optimizers\n\ninput_shape = vgg_model.output_shape[1]\n\nmodel = Sequential() model.add(InputLayer(input_shape=(input_shape,))) model.add(Dense(512, activation='relu', input_dim=input_shape)) model.add(Dropout(0.3)) model.add(Dense(512, activation='relu')) model.add(Dropout(0.3)) model.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr =1e-4), metrics=['accuracy'])\n\nmodel.summary()\n\nTraining this network will be much faster than before. You will get in the range of 90% accuracy in a few seconds’ time (remember that you have created a new training dataset this time). But now you can change this network and it will be much faster to test different architectures. This time,\n\n159\n\nChapter 4\n\nadvanCed Cnns and transfer Learning\n\none epoch takes only six seconds, in comparison to the 4.5 minutes in the previous example. This method is much more efficient than the previous one. We split the training in two phases:\n\n1. Creation of the feature dataset. Done only once. (In our example, this needs about four minutes.)\n\n2. Train the new layers as a stand-alone network, using\n\nthe feature dataset as input. (This takes six seconds for each epoch.)\n\nIf we want to train our network for 100 epochs, with this method we would need 14 minutes. With the method described in the previous section, we would need 7.5 hours! The downside is that you need to create the new feature dataset for each dataset you want to use. In our example, we needed to do it for the training and for the validation dataset.\n\n160\n\nCHAPTER 5\n\nCost Functions and Style Transfer\n\nIn this chapter we will look in more depth at the role of the cost function in neural network models. In particular, we will discuss the MSE (mean square error) and the cross-entropy and discuss their origin and their interpretation. We will look at why we can use them to solve problems and how the MSE can be interpreted in a statistical sense, as well as how cross- entropy is related to information theory. Then, to give you an example of a much more advanced use of special loss functions, we will learn how to do neural style transfer, where we will discuss a neural network to paint in the style of famous painters.\n\nComponents of a Neural Network Model\n\nAt this point you have seen and developed several models that try to solve different types of problems. You should know by now that in all the neural network models, there are (at least) three main building blocks:\n\nNetwork architecture (number of layers, type of layers,\n\nactivation functions, etc.)\n\nLoss function (MSE, cross-entropy, etc.)\n\nThe optimizer\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_5\n\n161\n\nChapter 5\n\nCost FunCtions and style transFer\n\nThe optimizer is not typically problem specific. For example, to solve\n\na regression or classification problem, you need to choose a different architecture and loss function, but you can use in both cases the same optimizer. In regression, you may use a feed-forward network and the MSE for the loss function. In classification, you may choose a convolutional neural network and the cross-entropy loss function. But in both you can use the Adam optimizer. The component that plays the biggest role in deciding what a network can learn is the loss function. Change it and you will change what your network will be able to predict and learn.\n\nTraining Seen as an Optimization Problem\n\nLet’s try to understand why this is the case in more detail. From a purely theoretical point of view, training a network means nothing more than solving a really complex optimization problem. The standard formulation of continuous optimization problem is to find the minimum of a given function\n\nmin x\n\nf x(\n\n)\n\nSubject to two constraint types\n\n( g x i ( p x j\n\n)£ )=\n\n0\n\n,\n\n0\n\n,\n\ni\n\n= ¼ , m , 1 = ¼ , , 1\n\nj\n\nn\n\nwhere f : ℝn → ℝ is the continuous function we want to minimize, gi(x) ≤ 0 refers to the inequalities constraints, pj(x) = 0 refers to the equality constraints, and m, n ∈ ℕ+. And of course, is possible to have a problem without constraints. But how does this relate to neural networks? Well, the following parallels can be drawn:\n\n162\n\nChapter 5\n\nCost FunCtions and style transFer\n\nThe function f (x) is the loss function that we have chosen when building the neural network model.\n\nThe input x ∈ ℝn are the weights (the learnable parameters) of our network. Remember that any loss function that we may choose is always a function of the output of the network (that we indicate with ˆy), and the output is always a function of the weights W (the learnable parameters of the network).\n\nWhen we are training a network, we are actually solving an\n\noptimization problem, one where we want to minimize the loss function with respect to the weights. We implicitly have constraints, although we normally don’t declare them explicitly. For example, we may have the constraint that we want the inference time needed for one observation to be less than 10ms. In this case we would have n = 0 (no equality constraints), m = 1 (one inequality constraint) with g1 being the inference running time. To cite Wikipedia1:\n\nA loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some “cost” associated with the event\n\nTypically, a loss function measures how bad your model understands your data. Let’s look at a few simple examples so that you can understand in a concrete case this formulation of the training of a network.\n\n1 https://en.wikipedia.org/wiki/Loss_function\n\n163\n\nChapter 5\n\nCost FunCtions and style transFer\n\nA Concrete Example: Linear Regression\n\nAs you know, you can perform linear regression with a network with just one neuron if you choose as its activation function the identity function2. We indicate the set of observations with x[i] ∈ ℝn with i = 1, …, m where m is the number of observations we have at our disposal. The neuron (and therefore the network) will have the output\n\nˆy\n\n[ ] i\n\n=\n\nn\n\nå\n\nw x k = 1\n\nk\n\n[ ] i k\n\n+\n\nb\n\nwhere we have indicated the weights with w = (w1, …wn). We can choose the loss function as the mean square error (MSE):\n\n( J w b\n\n,\n\n)=\n\nm\n\nå1 m\n\n= 1\n\nk\n\n(\n\nˆ y\n\n[ ] i\n\n\n\ny\n\n[ ] i\n\n)\n\n2\n\nWhere y[i] is the target variable that we want to predict for the ith\n\nobservation. It’s easy to see how the loss function that we have defined is a function of the weights and the bias. In fact, we have\n\n( J w b\n\n,\n\n)=\n\n1 m\n\nm\n\nå\n\ni\n\n= 1\n\n(\n\nˆ y\n\n[ ] i\n\n\n\ny\n\n[ ] i\n\n) =\n\n2\n\n1 m\n\næ å å ç è\n\nm\n\nn\n\nw x k = 1\n\ni\n\n= 1\n\nk\n\n[ ] i k\n\n+ -\n\nb y\n\n[ ] i\n\nö ÷ øø\n\n2\n\nTraining this network as we typically do with (for example) a gradient\n\ndescent algorithm is nothing more than solving an unconstrained\n\n2 This example is discussed in detail in Michelucci, Umberto, 2018. Applied Deep Learning: A Case-Based Approach To Understanding Deep Neural Networks. 1. Auflage. New York: Apress. ISBN 978-1-4842-3789-2. Available from: https:// doi.org/10.1007/978-1-4842-3790-8\n\n164",
      "page_number": 168
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 177-187)",
      "start_page": 177,
      "end_page": 187,
      "detection_method": "topic_boundary",
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\noptimization problem where we have (using the notation we have used at the beginning):\n\nf\n\n:=\n\nJ\n\nThe Cost Function Mathematical Notation\n\nLet’s define some notation that we will use in the next sections. We will use\n\ni[ ]Î is the output of the network for the ith observation. ˆy ˆY Î ´ m k\n\nk\n\n is the tensor containing the output of the network for all\n\nobservations.3 [ ] ´ x i\n\nn n n c y\n\n´\n\nÎ\n\nx\n\nrepresents the ith observation input features (in general,\n\nfor images we would have nc channels, and a resolution of nx × ny). is the tensor containing all input observations.\n\nÎ ´ \n\n´ m n n n c x\n\n´\n\nX W is the set of all learnable parameters that are used in the network\n\ny\n\n(including the biases).\n\nm is the number of observations. nc is the number of image channels (for RGB images it would be 3). nx is the horizontal resolution of the input images. ny is the vertical resolution of the input images. J is the cost function. In general, we will define the so-called cost (or loss) function J\n\ngenerically as follows:\n\n( J X Y, Wˆ(\n\n)\n\n)\n\n3 Remember that the order of the dimensions depends on how you structure your network and you may need to change it. The dimensions here are for illustrative purposes only.\n\n165\n\nChapter 5\n\nCost FunCtions and style transFer\n\nThis function, in addition to the network architecture, will define what kind of problem our neural network model will be able to solve. Note how this function\n\nDepends on the network architecture, since it depends on the network output ˆY (and therefore from the learnable parameters, W)\n\nDepends on the input dataset, since it depends on the\n\ninput X\n\nThis is the function that will be used when finding the best weights. In\n\nalmost all optimizers, the weights are updated using Ñ some form.\n\nW\n\n( J X Yˆ\n\n( , W\n\n)\n\nTypical Cost Functions\n\nThere are several cost functions that you may use when training neural networks, as we have seen in the previous chapters. In the next sections, we will look at two of the most used in detail and try to understand their meaning and origin.\n\nMean Square Error\n\nThe mean square error function\n\n( J w b\n\n,\n\n)=\n\nm\n\nå1 m\n\n= 1\n\nk\n\n(\n\nˆ y\n\n[ ] i\n\n\n\ny\n\n[ ] i\n\n)\n\n2\n\nis probably the most used cost function used when developing models for regression. There are several interpretations of this cost function, but the following two should help you in get an intuitive and a more formal understanding of it.\n\n166\n\n)\n\nin\n\nChapter 5\n\nCost FunCtions and style transFer\n\nIntuitive Explanation\n\nJ is nothing more than the average of the squared difference between the predictions and the measured values. So basically, it measures how far the predictions are from the expected values. A perfect model that would predict the data perfectly ( ˆy = for all i = 1, …, m) would have J = 0. In general, it holds the smallest J the better the predictions are.\n\n[ ] i\n\n[ ] i\n\ny\n\nNote predictions are (and therefore, the better the model is).\n\nin general, it holds that the smaller the Mse, the better the\n\nMinimizing the MSE means finding the parameters so that our\n\nnetwork will give output as close as possible to our training data. Note that you could achieve a similar result by using, for example, the MAE (Mean Absolute Error) given by\n\nMAE\n\n=\n\nå1 m\n\nm\n\n= 1\n\nk\n\nˆ y\n\n[ ] i\n\n\n\ny\n\n[ ] i\n\nAlthough this is not usually done.\n\nMSE as the Second Moment of a Moment-Generating Function\n\nThere is a more formal way of interpreting the MSE. Let’s define the quantity\n\nDY\n\n[ ] i\n\n=\n\n[ ] i -ˆ y\n\ny\n\n[ ] i\n\nLet’s define the moment-generating function\n\nD ( ) = éë : E e\n\nM t Y\n\nD t Y\n\nùû\n\n167\n\nChapter 5\n\nCost FunCtions and style transFer\n\nWhere we have t ∈ ℝ and we have indicated with E[·] the expected value of the variable over all observations. We will skip the discussion about the existence of the expected value, depending on the characteristics of ΔY, since this goes beyond the scope of this book. We can expand etΔY with a Taylor series expansion4 (we will assume we can do that):\n\ne\n\ntDY = +\n\n1\n\nD t Y\n\n+\n\n2 D t Y ! 2\n\n2\n\n+¼\n\nTherefore\n\nD ( ) = éë : E e\n\nM t Y\n\nt\n\nD Y\n\nùû = + [ 1\n\nD tE Y\n\n]+\n\n2 éë D t E Y 2\n\n!\n\n2\n\nùû +¼\n\nE[ΔYn] is called the nth moment of the function MΔY(t). You can see that\n\nthe moments can be easily interpreted (at least the first):\n\nE[ΔY]: First moment of MΔY(t) - Average of ΔY\n\nE[ΔY2]: Second moment of MΔY(t) - is what we defined as the MSE function\n\nE[ΔY3]: Third moment of MΔY(t) - Skeweness5\n\nE[ΔY4]: Fourth moment of MΔY(t) - Kurtosis6\n\nWe can simply write the second moment as the average over the\n\nobservations\n\nD 2 éë E Y\n\nùû\n\n:=\n\n1 m\n\nm\n\nå D\n\nk\n\n= 1\n\nY\n\n[ ] i\n\n2\n\n=\n\n1 m\n\nm\n\nå\n\nk\n\n= 1\n\n(\n\nˆ y\n\n[ ] i\n\n\n\ny\n\n[ ] i\n\n)\n\n2\n\n4 https://en.wikipedia.org/wiki/Taylor_series 5 https://en.m.wikipedia.org/wiki/Skewness. In the case of E[ΔY] = 0. 6 https://en.m.wikipedia.org/wiki/Kurtosis. In the case of E[ΔY] = 0.\n\n168\n\nChapter 5\n\nCost FunCtions and style transFer\n\nIf we assume that our model predict data with E[ΔY] = 0, then the E[ΔY2] (and therefore the MSE) is nothing more than the variance of the distribution of our data points ΔY[i]. In this case, it simply measures how broad our points are spread around the average (that is zero): the perfect prediction. Remember that, if for an observation, we have ΔY[i] = 0, it means we have ˆy , meaning the prediciton is perfect. Just to give the correct terminology, if E[ΔY] is not zero, then the moments are sometimes called the non-central moments. If you are dealing with non-central moments, you cannot interpret them directly as a statistical quantity (as the variance) anymore.\n\n[ ] i\n\n[ ] i\n\n=\n\ny\n\nNote if you are dealing with non-central moments, you cannot interpret them directly as a statistical quantity (as the variance) anymore. if the average of ΔY [i] is zero, then the Mse is simply the variance of the distributions of our predictions. and of course, the smaller the value, the better the predictions are.\n\nCross-Entropy\n\nThere are several ways to understand the cross-entropy loss function, but I think the most fascinating way is obtained by starting the discussion from information theory. In this section, we will discuss some of the fundamental concepts on a more intuitive basis to give you enough information and understanding to get a very powerful understanding of cross-entropy.\n\nSelf-Information or Suprisal of an Event\n\nWe need to start with the concept of self-information, or suprisal of an event. To get an intuitive understanding of it, consider the following: when an unlikely outcome of an event occurs, we associate it with a high level\n\n169\n\nChapter 5\n\nCost FunCtions and style transFer\n\nof information. When an outcome happens all the time, typically it does not have much information associated with it. In other words, we are more surprised when an unlikely event occurs; therefore, it’s also called suprisal of an outcome. How can we formulate this in a mathematical form? Let’s consider a random variable X with n possible outcomes x1, x2, …, xn and probability mass function7 P(X). Let’s indicate the probability of event xi to occur with pi = P(xi). Any monotonically decreasing function I(pi) between 0 and 1 could be used to represent the suprisal (or self-information) of the random variable X. But there is an important property that this function must have: if the events are independent, I should satisfy\n\n( I p p i\n\nj\n\n)= ( I p i\n\n)+ ( I p\n\n)\n\nIf the outcomes i and j are independent. There is immediately a function that comes to mind that has this property: the logarithm. In fact, it’s true that\n\nln\n\n(\n\np p i\n\nj\n\n)=\n\nlog\n\np i\n\n+\n\nlog\n\np\n\nTo have it monotonically decreasing, we can choose the following\n\nformula:\n\n( I p i\n\n)= -log\n\np i\n\n7 In probability and statistics, a probability mass function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value [Stewart, William J. (2011). Probability, Markov Chains, Queues, and Simulation: The Mathematical Basis of Performance Modeling. Princeton University Press. p. 105. ISBN 978-1-4008-3281-1.]\n\n170\n\nChapter 5\n\nCost FunCtions and style transFer\n\nSuprisal Associated with an Event X\n\nIn general, how much information do we have related to a specific event X? This is measured by the expected value over all possible outcomes for X (we will indicate this set with P). Mathematically, we can write this as\n\n( H X\n\n)=\n\n(\n\néë E I X\n\nP\n\n)\n\nùû =\n\nn\n\nå\n\n) ( P x I x i\n\n(\n\ni\n\n)= -\n\nn\n\nå\n\n( P x\n\ni\n\n)\n\nlog\n\nb\n\n( P x\n\ni\n\n)\n\ni\n\n= 1\n\ni\n\n= 1\n\nH(X) is called the Shannon entropy, and b is the basis of the algorithm\n\nand typically is chosen as 2, 10, or e.\n\nCross-Entropy\n\nNow let’s suppose we want to compare two distributions of probabilities for our event X. Let’s analyze what we do when we train a neural network for classification. Consider the following points:\n\nOur examples give us the “real” or expected\n\ndistributions of our events (the true labels). Their distributions will be our P. For example, our observations may contain cat classes (let’s suppose this is class 1) with a certain probability P(x1), where x1 is the outcome “this image has a cat in it”. We have a given probability mass function, P.\n\nThe network we have trained will give us a different probability mass function, Q, since the predictions will not be identical to the training data. Outcome x1 (“the image has a cat in it”) will occur with a different probability, Q(x1). You will remember that when building a network for classification, we use a softmax activation function for the output layer to interpret the output as probabilities. Do you see how everything seems to make suddenly more sense? 171\n\nChapter 5\n\nCost FunCtions and style transFer\n\nWe want to have a prediction that reflects as best as possible the given labels, meaning that we want to have a probability mass function Q that is as similar as possible to P.\n\nTo compare the two probability mass functions (what we are interested\n\nin), we can simply calculate the expected value of the self-information obtained by our network with the distribution obtained by the examples. In a more mathematical form\n\n( H Q P\n\n,\n\n)=\n\n(\n\néë E I Q\n\nP\n\n)\n\nùû =\n\nE\n\nP\n\n[\n\nlog\n\nb\n\nQ\n\n]= -\n\nn\n\nå\n\n( P x\n\ni\n\n)\n\nlog\n\nb\n\n( Q x\n\n)\n\ni\n\n= 1\n\nIf you have any experience in information theory, H(Q, P) will give a measure of the similary of the two probability mass functions, Q and P. To understand why, let’s consider a practical example. X will be the toss of a fair coin. X will have two possible outcomes: x1 will be the head and x2 will be the tail of the coint. The “true” probability mass function is of course a constant one with P(x1) = 0.5 and P(x2) = 0.5. Now let’s consider alternative probability mass functions Qi with (we will consider only nine possible values for illustrative purposes):\n\n\n\ni = 1 → Q1(x1) = 0.1, Q1(x2) = 0.9\n\n\n\ni = 2 → Q2(x1) = 0.2, Q2(x2) = 0.8\n\n\n\ni = 3 → Q3(x1) = 0.3, Q3(x2) = 0.7\n\n\n\ni = 4 → Q4(x1) = 0.4, Q4(x2) = 0.6\n\n\n\ni = 5 → Q5(x1) = 0.5, Q5(x2) = 0.5\n\n\n\ni = 6 → Q6(x1) = 0.6, Q6(x2) = 0.4\n\n\n\ni = 7 → Q7(x1) = 0.7, Q7(x2) = 0.3\n\n\n\ni = 8 → Q8(x1) = 0.8, Q8(x2) = 0.2\n\n\n\ni = 9 → Q9(x1) = 0.9, Q9(x2) = 0.1\n\n172\n\nChapter 5\n\nCost FunCtions and style transFer\n\nLet’s calculate H(Qi, P) for i = 1, …5. We don’t need to calculate H for i = 6, . . , 9 since the function is symmetric, meaning for example that H(Q4, P) = H(Q6, P). In Figure 5-1, you can see the plot of H(Qi, P). You can see how the maxium is reached for i = 5, exactly when the two probability mass functions are the same.\n\nFigure 5-1. H(Qi, P) for i = 1, …5. The minimum is obtained for i = 5, when the two probability mass functions are exactly the same.\n\nNote Cross-entropy H(Q, P) is a measure of how similar the two mass probability functions, Q and P, are.\n\nCross-Entropy for Binary Classification\n\nNow let’s consider a binary classification problem and let’s see how cross- entropy works. Let’s suppose our event X is the classification of a given image in two classes. The possible outcomes are only two: class 1 or class 2. Let’s suppose for illustrative purposes that our image belongs to class 1. Our “true” probability mass function for the image will have P(x1) = 1.0, P(x2) = 0. In other words, our probability mass function P can only be 0 or 1 since we know the true value.\n\n173\n\nChapter 5\n\nCost FunCtions and style transFer\n\nYou will remember that in a binary classification problem we used the\n\nfollowing\n\n(  ˆ y\n\n( ) j\n\n, y\n\n( ) j\n\n)= -\n\n(\n\ny\n\n( ) j\n\nlog\n\nˆ y\n\n( ) j\n\n+ -( 1\n\ny\n\n( ) j\n\n)\n\nlog\n\n( 1\n\nˆ y\n\n( ) j\n\n)\n\n)\n\nWhere y(j) represents the true labels (0 for class 1 and 1 for class 2) and ˆy j( ) is the probability of the image j of being of class 2, or in other words, of the output of the network assuming the value 1. The cost function we will minimize is given by a sum over all observations (or examples)\n\nJ\n\n(\n\nw,\n\nb\n\n)=\n\nm\n\nå1 m\n\nj\n\n= 1\n\n(  ˆ y\n\n( ) j\n\n,\n\ny\n\n( ) j\n\n)\n\nUsing the notation of the previous section, we can write for image j\n\n( p x 1 j\n\n)= - ( ) 1\n\nj\n\ny\n\n2( p x j\n\n)= ( ) y\n\nj\n\nRemember that y(j) can be only 0 or 1; therefore, we have only two\n\npossibilities: pj(x1) = 1, pj(x2) = 0 or pj(x1) = 0, pj(x2) = 1. And we can also write for the prediction of the network\n\n( q x 1 j\n\n)= - ( )ˆ 1\n\nj\n\ny\n\n2( q x j\n\n)= ( )ˆ y\n\nj\n\nRemember: This result is determined by how we built our network (since we used the softmax activation functions in the output layer to have probabilities) and by how we coded our labels (to be 0 and 1, so that they could be interpreted as probabilities). Let’s now write cross-entropy\n\n174\n\nChapter 5\n\nCost FunCtions and style transFer\n\nas defined in the previous section using our neural network notation but summing over all examples (remember that we want to have the entire cross-entropy for all the events, in other words for all images):\n\nm\n\n2\n\n( H Q P,\n\n)= -\n\nåå\n\n( p x j\n\ni\n\n)\n\nlog\n\nb\n\n( q x j\n\ni\n\n)\n\n=\n\n1\n\ni\n\n=\n\n1\n\n=-\n\nm\n\nå\n\n(\n\ny\n\n( ) j\n\nlog\n\nb\n\nˆ y\n\n( ) j\n\n( + - 1\n\ny\n\n( ) jj\n\n)\n\n( ( ) jy log 1 ˆ b )\n\n=\n\n1\n\n( So basically  ˆy\n\n( ) yi ,\n\n( ) i\n\n)\n\nis nothing more than the cross-entropy as it is\n\nderived in information theory.\n\nNote intuitively when we minimize the cross-entropy in a binary classification problem, we minimize the surprise that we may have when our predictions are different from what we expect.\n\nH(Q, P) measures how good our predictions probability mass function\n\n(Q) matches our training examples probability mass function (P).\n\nNote When we design a network for classification using the cross- entropy and we use the softmax activation function in the final layer to interpret the output as probabilities, we simply build a complex classification system that is based on information theory. We should thank shannon8 for classification with neural networks.\n\n8 https://en.wikipedia.org/wiki/Claude_Shannon\n\n175",
      "page_number": 177
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 188-195)",
      "start_page": 188,
      "end_page": 195,
      "detection_method": "topic_boundary",
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nCost Functions: A Final Word\n\nIt should be clear now that the cost function determines what a neural network can learn. Change it and the network will learn completely different things. It should come as no surprise that, to achieve special results, like art for example, it’s simply a matter of choosing the right architecture and the right cost function. In the next part of this chapter, we will look at neural style transfer and it will become immediately clear how choosing the right cost function (multiple ones in this example as we will see) is the key to achieving extraordinary results.\n\nNeural Style Transfer\n\nAt this point you have all the tools to start using networks for more advanced techniques: using pre-trained CNNs, extracting information from the hidden layers, and using custom cost functions. This is starting to be advanced material, so you need to understand all the basics we discussed in the previous chapters very well. If something seems unclear, go back and study the material again.\n\nAn interesting and fun application of CNNs is to make art. Neural style\n\ntransfer (NST) refers to a technique that manipulates digital images, to adopt the appearance or style of another image9. A fun application is to take an image and let the network manipulate it to adopt it to the style of a famous painter, like Van Gogh. NST using deep learning appeared first in a paper by Gatys et al. in 201510. It’s a new technique. The method developed by Gatys used pre-trained deep CNNs to separate the content of an image from the style.\n\n9 https://en.wikipedia.org/wiki/Neural_Style_Transfer 10 Gatys, Leon A.; Ecker, Alexander S.; Bethge, Matthias (26 August 2015). “A Neural\n\nAlgorithm of Artistic Style”. https://arxiv.org/abs/1508.06576\n\n176\n\nChapter 5\n\nCost FunCtions and style transFer\n\nThe idea is that an image is fed into a pre-trained VGG-1911 CNN trained on the imagenet dataset. The author assumed that the content of an image can be found in the network intermediate layers output (the image passed through the learned filters in each layers), while the style lies in the correlations of the different layers output (coded in a Gramian matrix). The pre-trained network can identify the content of images quite well, and therefore the features learned by each layer must relate strongly to the content of the image, and not to the style. In fact, a robust CNN that is good at identifying images does not care much about style. Intuitively, style is contained in how the different filter responses over the space of an image are related. A painter may use brush strokes that are wide or narrow, may use many colors close to each other or just a few, and so on. Remember in a CNN, each layer is simply a collection of image filters; therefore, the output of a given layer is simply a collection of differently filtered versions of the input image10.\n\nAnother way of seeing that is that content is found when you look at\n\nan image from afar (you don’t care much about the details), while style is found when looking at the image at a much closer scale and depends on how different parts of the image relate to each other. Gatys et al. have, in a smart way, simply implemented these ideas mathematically. To give you an idea, look at Figure 5-2. A network has manipulated the original image (upper left) into the style of the Van Gogh painting in the upper right, to obtain the image on the bottom.\n\n11 ”Very Deep CNNS for Large-Scale Visual Recognition”. Robots.ox.ac.uk. 2014. Retrieved 13 February 2019, http://www.robots.ox.ac.uk/~vgg/research/ very_deep/\n\n177\n\nChapter 5\n\nCost FunCtions and style transFer\n\nFigure 5-2. An example of NST. The method has manipulated the original image (upper left) into the style of the Van Gogh painting in the upper right, to obtain the image on the bottom.\n\nThe Mathematics Behind NST\n\nThe original paper used the VGG19 network, which Keras makes available for us to download and use. An input image that we will indicate here with x (I will try to use the original notation as much as possible) is encoded\n\n178\n\nChapter 5\n\nCost FunCtions and style transFer\n\nin each layer of the CNN. A layer with Nl filters (or kernels as they are sometimes called) will have Nl feature maps as output. In the algorithm those outputs will be flattened out in a one-dimensional vector of dimension Ml, where Ml is the height times the width of the output of each filter when applied to the input image. The response of a layer l can then be encoded in a tensor F l Î . Let’s pause a second here and try to understand with a concrete example what we mean.\n\n´ N Ml\n\nl\n\n\n\nLet’s suppose we use as input images in color, each with dimensions\n\n32 × 32. Let’s consider the first convolutional layer in a CNN that has been created with the code:\n\nConv2D(32, (3, 3), padding='same', activation='relu', input_ shape=input_shape))\n\nWhere of course input_shape = (32,32,3). The output of the layer\n\nwill have these dimensions\n\n(None, 32, 32, 32)\n\nWhere of course the None will assume the value of the number of observations used. This is because we used the parameter padding = 'same'. In this case, the output of layer l = 1, are 32 feature maps (or the result of the input image convoluted with the 32 filters) each with dimensions 32 × 32. In this case, we will have Nl = 1 = 32 and Ml = 1 = 32 × 32 = 1024. Each of the 32 × 32 feature maps will be flattened out before calculating the Gramian matrices. You will see clearly how this is done later in the code.\n\nLet’s call the original image p. This is the image we want to change. The image that is generated as output is called x. We will indicate with Pl and Fl their respective features maps obtained from layer l. We define the squre error loss, called the content loss function, as follows:\n\ncontent\n\n(\n\n, p x l ,\n\n)=\n\nå1 2\n\ni j ,\n\n(\n\nl F ij\n\n\n\nl P ij\n\n)\n\n2\n\n179\n\nChapter 5\n\nCost FunCtions and style transFer\n\nIn Keras, we will implement this with the following code:\n\ncontent_loss = tf.add_n([tf.reduce_mean((content_outputs[name]- content_targets[name])**2) for name in content_outputs.\n\nkeys()])\n\nwhere the content_outputs[] and content_targets[] will contain the output of specific layers of VGG19 when applied to the input (content_ outputs) and the generated image (content_targets), respectively (already flattened). Later we will discuss it in more detail; don’t worry for the moment if you don’t understand it completely. You may wonder why ) will we don’t have the factor 1/2 but we don’t need it, since content p x l , , be multiplied by another factor, which will make the 1/2 useless.\n\n(\n\nWe need to calculate the gradient of the loss function with respect to\n\nthe image. This is quite an important point. What this means is that the parameters we want to learn are the pixel values of the image we want to change. The parameters of the network are fixed, and we don’t need to change them. With Keras, we will need to use the tape.gradient function in this form:\n\ntape.gradient(loss, image)\n\nWe will need to define the image as a TensorFlow Variable (more on that later). If you are not familiar with how tape.gradient works, I suggest you check out the official documentation at https://www.tensorflow. org/tutorials/eager/automatic_differentiation.\n\nNote the parameters we want to learn are the pixel values of the image we want to change, not the weights of the network.\n\nNow we need to take care of the style. To do this, we need to define a loss function for the style. To do this, we need to define the Gramian matrix\n\n180\n\nChapter 5\n\nCost FunCtions and style transFer\n\nGl, which is the inner product between the flattened feature maps i and j in layer l. In other words\n\nl G ij\n\n=å\n\nl l F F kj ik\n\nk\n\nWith this newly defined quantity, we will define a Style loss function ( style a x,\n\n), where a is the image from which we want to use the style as\n\n5\n\nstyle\n\n(\n\n, a x\n\n)=\n\nå\n\nw E l = 1\n\nl\n\nl\n\nWhere\n\nE\n\n=\n\nå1 4 2 N M l\n\n2 l\n\ni j ,\n\n(\n\nl - G A ij\n\nl ij\n\n)\n\n2\n\nWhere wl are weights that in the original papers were chosen and equal 1/5. In Keras, we will implement this loss with the code (we will look at the details later):\n\ntf.add_n([tf.reduce_mean((style_outputs[name]-style_ targets[name])**2) for name in style_outputs.keys()])\n\nThe style_outputs and style_targets variables will contain the output of five of the layers of the VGG19 network. In the original paper, the following five layers were used:\n\nl=1 - block1_conv1 l=2 - block2_conv1 l=3 - block3_conv1 l=4 - block4_conv1 l=5 - block5_conv1\n\n181\n\nChapter 5\n\nCost FunCtions and style transFer\n\nThose are the first layers in each block in the VGG19 network.\n\nRemember that you can get the layer names from the VGG19 simply with this code:\n\nvgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n\nprint() for layer in vgg.layers: print(layer.name)\n\nThat would give you this result:\n\ninput_1 block1_conv1 block1_conv2 block1_pool block2_conv1 block2_conv2 block2_pool block3_conv1 block3_conv2 block3_conv3 block3_conv4 block3_pool block4_conv1 block4_conv2 block4_conv3 block4_conv4 block4_pool block5_conv1 block5_conv2 block5_conv3\n\n182\n\nChapter 5\n\nCost FunCtions and style transFer\n\nblock5_conv4 block5_pool\n\nNote that we have no dense layers, since we used include_top=False.\n\nFinally, we will minimize the following loss function\n\n5\n\n\n\ntotal\n\n(\n\n, p x a ,\n\n)=\n\na\n\n\n\nstyle\n\n(\n\n, a x\n\n)+\n\nå b \n\ncontent\n\n(\n\n, p x l ,\n\n)\n\n= 1\n\nWith gradient descent (for example), with respect to the image we want\n\nto change. The constants α and β can be choosen to give more weight to style or content. For the result in Figure 5-1, I chose α = 1.0, β = 104. Other typical values are α = 10−2, β = 104.\n\nAn Example of Style Transfer in Keras\n\nThe code that we will discuss here has been taken from the original TensorFlow NST tutorial and is greatly simplified for this discussion. We will discuss only part of the code to simplify the discussion, since in its entirety the code is relatively long. You can find the entire simplified version in the book’s GitHub repository in the Chapter 5 folder. I suggest you run the code in Google Colab with GPU enabled, since it is computationally quite intensive. To give you an idea, one epoch on my laptop takes roughly 13 seconds, while on Google Colab, it takes 0.5 seconds to work with 512 × 512 pixel images.\n\nTo make sure that you have the latest TensorFlow version installed, you\n\nshould run the following code at the beginning of your notebook:\n\nfrom __future__ import absolute_import, division, print_ function, unicode_literals !pip install tensorflow-gpu==2.0.0-alpha0 import tensorflow as tf\n\n183",
      "page_number": 188
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 196-203)",
      "start_page": 196,
      "end_page": 203,
      "detection_method": "topic_boundary",
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nIf you run the code on Google Colab, you need to save the images you\n\nwant to work with on your Google drive and mount it. To do that, you need to upload two images on your drive:\n\nA style image: For example, a famous painting. This is\n\nthe image you want to get the style from.\n\nA content image: For example, a landscape or a photo\n\nyou took. This is the image you want to modify.\n\nI assume here that you uploaded your images into a folder called data into the root directory of your Google drive. What you need to do now is to mount your Google drive in Google Colab to be able to access the images. To do that, you need the following code:\n\nfrom google.colab import drive drive.mount('/content/drive')\n\nIf you run this code, you need to go to a specific URL (that will be given\n\nto you by Google Colab) where you will receive the code that you need to paste in your notebook. A nice overview on how to do this can be found at http://toe.lt/a. Once mounted, you’ll get a list of the files in the directory with this:\n\n!ls \"/content/drive/My Drive/data\"\n\nWe can define the filenames of the images we will use with\n\ncontent_path = '/content/drive/My Drive/data/landscape.jpg' style_path = '/content/drive/My Drive/data/vangogh_landscape.jpg'\n\nYou need to change the filenames to yours, of course. But you will find the images I used for this example on the GitHub repository if you want to try the exercise with them. You need to create the data directory if you don’t have it and copy the images in there. The images will be loaded with the load_img() function. Note that in the function at the beginning we resize the images to have their maximum dimension equal\n\n184\n\nChapter 5\n\nCost FunCtions and style transFer\n\nto 512 (the complete code for the load_img() function can be found on GitHub). This is a size that is manageable, but if you want to generate better-looking images, you need to increase this value. The image in Figure 5-1 was generated with max_dim = 1024. The function begins with\n\ndef load_img(path_to_img): max_dim = 512 img = tf.io.read_file(path_to_img)\n\nSo, you change the value of the max_dim variable to work with bigger\n\nimages. Now we need to select only the output of some layers, as we described in the previous section. To do that, we put the names of the layers we want to use into two lists:\n\n# Content layer where will pull our feature maps content_layers = ['block5_conv2']\n\n# Style layer we are interested in style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n\nThis way we can select the right layers using the names. What we need is a model that gets input and returns all the feature maps from each layer. To do that, we use the following code\n\ndef vgg_layers(layer_names): vgg = tf.keras.applications.VGG19(include_top=False,\n\nweights='imagenet') vgg.trainable = False\n\noutputs = [vgg.get_layer(name).output for name in layer_names]\n\n185\n\nChapter 5\n\nCost FunCtions and style transFer\n\nmodel = tf.keras.Model([vgg.input], outputs) return model\n\nThis function gets a list as input with the layer names and selects the\n\nnetwork layer output of the given layers with this line:\n\noutputs = [vgg.get_layer(name).output for name in layer_names]\n\nNote that there are no checks, so if you have a wrong layer names you\n\nwill not get the result you expect. But since the layers we need are fixed, you don’t need to check if the names exist in the network. This line\n\nmodel = tf.keras.Model([vgg.input], outputs)\n\ncreates a model with one input (vgg.input) and one or more outputs, depending on the number of layers in the layer_names input list.\n\nTo calculate Gij\n\nl (the Gramian matrix), we use this function\n\ndef gram_matrix(input_tensor): result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor,\n\ninput_tensor)\n\ninput_shape = tf.shape(input_tensor) num_locations = tf.cast(input_shape[1]*input_shape[2],\n\ntf.float32)\n\nreturn result/(num_locations)\n\nwhere the variable num_locations is simply Ml. Now comes the interesting part: the definition of the loss functions. We need to define a class called StyleContentModel that will take our model and return the output of the different layers at each iteration. The class has an __init__ part that we will skip here (you can find the code in the Jupyter Notebook). The interesting part is the call() function:\n\ndef call(self, inputs): inputs = inputs*255.0\n\n186\n\nChapter 5\n\nCost FunCtions and style transFer\n\npreprocessed_input = tf.keras.applications.vgg19.\n\npreprocess_input(inputs)\n\noutputs = self.vgg(preprocessed_input) style_outputs, content_outputs = (outputs[:self.num_style_\n\nlayers],\n\noutputs[self.num_style_\n\nlayers:])\n\nstyle_outputs = [gram_matrix(style_output) for style_output in style_outputs]\n\ncontent_dict = {content_name:value for content_name, value in zip(self.content_layers, content_\n\noutputs)}\n\nstyle_dict = {style_name:value for style_name, value in zip(self.style_layers, style_outputs)}\n\nreturn {'content':content_dict, 'style':style_dict}\n\nThis function will return a dictionary with two elements—content_ dict contains the content layers and their output and style_dict contains the style layers and their outputs. You use this function:\n\nextractor = StyleContentModel(style_layers, content_layers)\n\nAnd then:\n\nstyle_targets = extractor(style_image)['style'] content_targets = extractor(content_image)['content']\n\nThis way, we can get the output of the different layers when applied\n\nto different images. Remember we need the output of the style layers when applied to our Van Gogh painting, but we need the content layer\n\n187\n\nChapter 5\n\nCost FunCtions and style transFer\n\noutput when applied to the landscape (or your image) image. Let’s save the content image (the landscape or your image) in a variable and define a function (it will be useful later9) that will clip the values of an array between 0 and 1:\n\nimage = tf.Variable(content_image) def clip_0_1(image): return tf.clip_by_value(image, clip_value_min=0.0,\n\nclip_value_max=1.0)\n\nThen we can define the two variables α, β as follows:\n\nstyle_weight=1e-2 content_weight=1e4\n\nNow we have everything we need to define the loss function:\n\ndef style_content_loss(outputs): style_outputs = outputs['style'] content_outputs = outputs['content'] style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-\n\nstyle_targets[name])**2)\n\nfor name in style_outputs.keys()]) style_loss *= style_weight / num_style_layers\n\ncontent_loss = tf.add_n([tf.reduce_mean((content_\n\noutputs[name]-content_targets[name])**2)\n\nfor name in content_outputs.\n\nkeys()])\n\ncontent_loss *= content_weight / num_content_layers loss = style_loss + content_loss return loss\n\nThis code is rather self-explanatory, as we have discussed its parts already. This function expects as input the dictionary that we obtain using the StyleContentModel class.\n\n188\n\nChapter 5\n\nCost FunCtions and style transFer\n\nNow let’s create the function that will update the weights:\n\n@tf.function() def train_step(image): with tf.GradientTape() as tape: outputs = extractor(image) loss = style_content_loss(outputs)\n\ngrad = tape.gradient(loss, image) opt.apply_gradients([(grad, image)]) image.assign(clip_0_1(image))\n\nWe use tf.GradientTape to update the image. Note that when you annotate a function with @tf.function, you can still call it like any other function. But it will be compiled into a graph, which means you get the benefits of faster execution, running on GPU or TPU, or exporting to SavedModel (see https://www.tensorflow.org/alpha/guide/ autograph). Remember that the variable extractor has been obtained with this code:\n\nextractor = StyleContentModel(style_layers, content_layers)\n\nAnd is the dictionary with the output of the different layers. Now this code is rather advanced and complicated to understand\n\nat the beginning, so take your time and read the pages with the Jupyter Notebook open at the same time, to be able to follow the code and the explanation. Don’t be discouraged if at the beginning you don’t understand everything. The line:\n\ngrad = tape.gradient(loss, image)\n\nwill calculate the gradients of the loss function with respect to the variable image that we have defined. Each update step can be done with a simple line of code:\n\ntrain_step(image)\n\n189\n\nChapter 5\n\nCost FunCtions and style transFer\n\nNow we can do the final loop easily:\n\nepochs = 20 steps_per_epoch = 100\n\nstep = 0 for n in range(epochs): for m in range(steps_per_epoch): step += 1 train_step(image) print(\".\", end=\") display.clear_output(wait=True) imshow(image.read_value()) plt.title(\"Train step: {}\".format(step)) plt.show()\n\nWhile it’s running, you will see the image change every epoch and you\n\ncan witness how it is changing.\n\nNST with Silhouettes\n\nThere is a fun application that you can do with NST, and that has to do with silhouettes12. A silhouette is an image of something represented as a solid shape of a single color. In Figure 5-3, you can see an example; if you are a fan of Star Wars, you know who it is (hint: Darth Vader13).\n\n12 This part of the chapter has been inspired by the Medium post https://\n\nbecominghuman.ai/creating-intricate-art-with-neural-style-transfer- e5fee5f89481.\n\n13 https://en.wikipedia.org/wiki/Darth_Vader\n\n190\n\nChapter 5\n\nCost FunCtions and style transFer\n\nFigure 5-3. A silhouette of the Star Wars character Darth Vader\n\nYou should search the Internet14 for images that are similar to a mosaic\n\nor stained glass, like the one shown in Figure 5-4.\n\nFigure 5-4. A mosaic-like image\n\n14 Note that all the images used in this chapter were images free of copyright and free to use. If you use images for your papers or block, ensure that you can use them freely or you’ll need to pay royalties.\n\n191",
      "page_number": 196
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 204-211)",
      "start_page": 204,
      "end_page": 211,
      "detection_method": "topic_boundary",
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nThe goal is to obtain an image like the one shown in Figure 5-5.\n\nFigure 5-5. NST done on a silhouette after applying masking (more on this later)\n\nMasking\n\nMasking has several meanings, depending on the field you are using it in. Here I refer to masking as the process of changing parts of an image to absolute white according to a silhouette. The idea is graphically illustrated in Figure 5-6. You can think of it this way: you put a silhouette over your image (they should have the same resolution) and keep only the parts where the silhouette is black.\n\nFigure 5-6. Masking applied to the mosaic image in Figure 5-4\n\n192\n\nChapter 5\n\nCost FunCtions and style transFer\n\nThis is okay, but a bit unsatisfying, since for example you don’t have\n\nedges in the result. The mosaic shapes are simply cut in the middle. Visually this is not so satisfying. But we can use NST to make the end image much nicer. The process is the following:\n\nYou use the mosaic-like image as the style image.\n\nYou use your silhouette image as the content image.\n\nAt the end you apply masking to your end result using\n\nyour silhouette image.\n\nYou can see the result (using the same code) in Figure 5-5. You can see\n\nthat you get nice edges and the mosaic tiles are not cut in half.\n\nYou can find the entire code in the book’s GitHub repository in\n\nChapter 5. But as a reference, let’s suppose that you have your image saved as a numpy array. Let’s suppose that the silhouette is saved in an array called mask and that your image is saved in an array called result. The assumption (and you should check that) is that the mask array will contain only 0 or 255 values (black and white). Then masking is done simply with this:\n\nresult[mask] = 255\n\nThat simply makes white in the result image where there is white in the\n\nsilhouette and leaves the rest untouched.\n\n193\n\nCHAPTER 6\n\nObject Classification: An Introduction\n\nIn this chapter, we will look at more advanced tasks in image processing that can be achieved with neural networks. We will look at semantic segmentation, localization, detection, and instance segmentation. The goal of this chapter is not to make you an expert, since one could easily read many books on the subject, but to give you enough information to be able to understand the algorithms and read the original papers. I hope that, by the end of this chapter, you will understand the difference between the methods, and you will have an intuitive understanding of the building blocks of these methods.\n\nThese algorithms need many advanced techniques that we have looked at in the previous chapters, like multiple loss functions and multi- task learning. We will look at a few more in this chapter. Keep in mind that the original papers on the methods are in some cases just a couple of years old, so to master the subject, you need to get your hands dirty and read the original papers.\n\nTraining and using the networks described in the papers is not doable\n\non a simple laptop and therefore you will find in this chapter (and in the next) less code and examples. I try to point you in the right direction and tell you what pre-trained libraries and networks are available at the time of this writing, in case you want to use those techniques in your own\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_6\n\n195\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nprojects. That will be the subject of the next chapter. Where relevant, I try to point out the differences, advantages, and disadvantages of the different methods. We will look at the most advanced methods in a very superficial way, since the details are so complex that only studying the original papers can give you all the information you need to implement those algorithms yourself.\n\nWhat Is Object Localization?\n\nLet’s start with an intuitive understanding of what object localization is. We have already seen image classification in many forms: it tells us what the content of an image is. That may sound easy, but there are many cases when this is difficult, and not because of the algorithms. For example, consider the case when you have a dog and a cat in an image at the same time. What is the class of the image: cat or dog? And what is the content of the image: A cat or a dog? Of course, both are in there, but classification algorithms give you one class only, so they are unable to tell you that you have two animals in the image. And what if you have many cats and many dogs? What if you have several objects? You get the idea.\n\nIt may be interesting to know where the cat and the dog are in the image. Consider the problem of a self-driving car: it is important to know where a person is, since that could mean the difference between a dead passerby and a living one. Classification, as we have looked at in the previous chapters, often cannot be used alone to solve real-life problems with images. Typically, recognizing that you have many instances of an object in an image involves finding their position in an image and being able to distinguish between them. To do that, we need to be able to find the positions of each instance in the image and their borders. This is one of the most interesting (and more difficult) tasks in image recognition techniques that can be solved with CNNs.\n\n196\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nTypically, with object localization we want to determine the location\n\nof an object (for example, a person or a car) in an image and draw a rectangular bounding box around it.\n\nNote With object localization, we want to determine the location of one or more objects (for example, people or cars) in an image and draw a rectangular bounding box around it.\n\nSometimes in the literature researchers use the term localization when\n\nthe image contains only one instance of an object (for example, only one person or only one car) and the term detection when an image contains several instances of an object.\n\nNote Localization typically refers to when an image contains only one instance of an object, while detection is when there are several instances of an object in an image.\n\nTo summarize and clarify the terminology, here is an overview of all the words and terms used (a visual explanation is shown in Figure 6-1):\n\nClassification: Give a label to an image, or in other\n\nwords, “understand” what is in an image. For example, an image of a cat may have the label “cat” (we have seen several cases of this in the previous chapters).\n\nClassification and localization: Give a label to\n\nan image and determine the borders of the object contained in it (and typically draw a rectangle around the object).\n\n197\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nObject detection: This term is used when you have\n\nmultiple instances of an object in an image. In object detection, you want to determine all the instances of several objects (for example, people, cars, signs, etc.) and draw bounding boxes around them.\n\n\n\nInstance segmentation: You want to label each pixel of the image with a specific class for each separate instance, to be able to find the exact limits of the object instance.\n\nSemantic segmentation: You want to label each\n\npixel of the image with a specific class. The difference with instance segmentation is that you don’t care if you have several instances of a car as examples. All pixels belonging to the cars will be labelled as “car”. In instance segmentation, you will still be able to tell how many instances of a car you have and where they are exactly. To understand the difference, see Figure 6-1.\n\nFigure 6-1. A visual explanation of the different terms describing the general task of locating one or more objects in an image\n\nSegmentation is typically the most difficult task of all of them, and in particular instance segmentation is particularly difficult. Many advanced techniques come together to solve those problems. One of the things to remember is that getting enough training data is not easy. Keep in mind\n\n198\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nthat this is much more difficult than simple classification, since someone will need to mark where the objects are. With segmentation, someone needs to classify each pixel in the image, which means training data is very expensive and difficult to collect.\n\nMost Important Available Datasets\n\nA well-known dataset that can be used to work on these problems is the Microsoft COCO dataset at http://cocodataset.org. The dataset contains 91 object types with a total of 2.5 million labelled instances in 328,000 images.1 To give you an idea of the kind of labeling used, Figure 6-2 shows some examples from the dataset. You can see how specific instances of objects (like people and cats) are classified at the pixel level.\n\nFigure 6-2. Examples of the images in the COCO dataset\n\n1 The original paper describing the dataset is: Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Dollár, Microsoft COCO: Common Objects in Context, https://arxiv.org/abs/1405.0312\n\n199\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nA quick note about sizes: the 2017 training images are roughly 118,000\n\nand require 18GB2 of hard disk space, so keep that in mind. Training a network with such a large amount of data is not trivial and will require time and lots of computing power. There is an API to download the COCO images that you can use and that is also available in Python. More information can be found on the main web page or on the API GitHub repository at https://github.com/cocodataset/cocoapi. The images have five annotation types: object detection, keypoint detection, stuff segmentation, panoptic segmentation, and image captioning. More information can be found at http://cocodataset.org/#format-data. Another dataset that you may encounter is the Pascal VOC dataset. Unfortunately, the website is not that stable, and therefore mirrors exist where you can find the files. One mirror is https://pjreddie.com/ projects/pascal-voc-dataset-mirror/. Note that this is a much smaller dataset than the COCO dataset.\n\nIn this and the next chapter, we will concentrate mainly on object classification and localization. We will assume that in the images we have only one instance of a specific object, and the task is to determine what kind of object it is and draw a bounding box (a rectangle) around it. These present enough challenge for now! We will look briefly at how segmentation works, but we will not go into many details about it, since its problems are extremely difficult to solve. I will provide references that you may check and study on your own.\n\nIntersect Over Union (IoU)\n\nLet’s consider the task of classifying an image and then drawing a bounding box around the object in it. In Figure 6-3, you can see an example of the output we expect (where the class would be cat).\n\n2 http://cocodataset.org/#download\n\n200",
      "page_number": 204
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 212-219)",
      "start_page": 212,
      "end_page": 219,
      "detection_method": "topic_boundary",
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFigure 6-3. An example of object classification and localization3\n\nThis is a fully supervised task. This means that we will need to learn where the bounding boxes are and compare them to some given ground truth. We need a metric to quantify how good the overlap is between the predicted bounding boxes and the ground truth. This is typically done with the IOU (Intersect Over Union) . In Figure 6-4, you can see a visual explanation of it. As a formula, we could write\n\nIOU\n\n=\n\nArea of overlap Area of union\n\nFigure 6-4. A visual explanation of the IOU metric\n\n3 Image source: http://www.cbsr.ia.ac.cn/users/ynyu/detection.html\n\n201\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nIn the ideal case of perfect overlap, we have IOU = 1, while if there is no\n\noverlap at all, we have IOU = 0. You will find this term in blogs and books, so it’s a good idea to know how to measure bounding boxes using the ground truth.\n\nA Naïve Approach to Solving Object Localization (Sliding Window Approach)\n\nA naïve way of solving the problem of localization is the following (spoiler: this is a bad idea but it’s instructive to see why):\n\n1. You cut a small portion of your input image\n\nstarting from the top-left corner. Let’s suppose your image has dimensions x, y, and your portion has dimensions wx, wy, with wx < x and wy < y.\n\n2. You use a pre-trained network (how you train it or\n\nhow you get it is not relevant here) and you let it classify the image portion that you cut.\n\n3. You shift this window by an amount we call stride\n\nand indicate with s toward the right and then below. You use the network to classify this second portion.\n\n4. Once the sliding window has covered the entire\n\nimage, you choose the position of the window that gives you the highest classification probability. This position will give you the bounding box of your object (remember your window has dimensions wx, wy).\n\nIn Figure 6-5, you can see a graphical illustration of the algorithm (we\n\nassumed wx = wy = s).\n\n202\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFigure 6-5. A graphical illustration of the sliding window approach to solve the problem of object localization\n\nAs you can see in Figure 6-5, we start from the top left and slide the window toward the right. As soon as we reach the right border of the image and we don’t have any space to shift the window further to the right, we get back on the left border but we shift it s pixel down. We continue in this fashion until we reach the lower-right corner of the image.\n\nYou might immediately see some problems with this method:\n\nDepending on the choice of wx, wy, and s, we may\n\nnot be able to cover the entire image. (Do you see in Figure 6-5 the small portion of the image on the right of window 4 that remains not analyzed?)\n\nHow do you choose wx, wy, and s? This is a rather nasty problem, since the bounding box of our object will have exactly the dimensions wx, wy. What if the object is larger or smaller? We typically don’t know in advance its dimensions and that is a huge problem if we want to have precise bounding boxes.\n\n203\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nWhat if our object flows across two windows? In\n\nFigure 6-5, you can imagine that the object is half in window 2 and half in window 3. Then your bounding box would not be correct if you follow the algorithm as described.\n\nWe could solve the third problem by using s = 1 to be sure that we cover all possible cases, but the first two problems are not so easy to solve. To address the window size problem, we should try all possible sizes and all possible proportions. Do you see any problem here? The number of evulations that you will need to do with your network is getting out of control and will become quickly computationally infeasible.\n\nProblems and Limitations the with Sliding Window Approach\n\nIn the book’s GitHub repository, within the Chapter 6 folder, you can find an implementation of the sliding window algorithm. To make things easier, I decided to use the MNIST dataset since you should know it very well at this point and it’s an easy dataset to use. As a first step, I built a CNN trained on the MNIST dataset that reached 99.3% accuracy. I then proceeded to save the model and the weights on disk. The CNN I used has the following structure:\n\n204\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== conv2d_1 (Conv2D) (None, 26, 26, 32) 320 _______________________________________________________________ conv2d_2 (Conv2D) (None, 24, 24, 64) 18496 _______________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64) 0 _______________________________________________________________ dropout_1 (Dropout) (None, 12, 12, 64) 0 _______________________________________________________________ flatten_1 (Flatten) (None, 9216) 0 _______________________________________________________________ dense_1 (Dense) (None, 128) 1179776 _______________________________________________________________ dropout_2 (Dropout) (None, 128) 0 _______________________________________________________________ dense_2 (Dense) (None, 10) 1290 =============================================================== Total params: 1,199,882 Trainable params: 1,199,882 Non-trainable params: 0 _______________________________________________________________\n\nI then saved the model and weights using this code (we already\n\ndiscussed how to do this):\n\nmodel_json = model.to_json() with open(\"model_mnist.json\", \"w\") as json_file: json_file.write(model_json) model.save_weights(\"model_mnist.h5\")\n\nYou can see in Figure 6-6 how the network training and accuracy\n\nchanges with the number of epochs.\n\n205\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFigure 6-6. Loss function value and accuracy for the training (continuous line) and for the validation (dashed line) dataset versus the number of epochs\n\nThe weights and model can be found in the GitHub repository. I did that to avoid having to re-train the CNN every time. I can reuse the model every time by reloading it. You can do it with this code (after you mount your Google drive if you want to run the code in Google Colab as I did):\n\nmodel_path = '/content/drive/My Drive/pretrained-models/model_ mnist.json' weights_path = '/content/drive/My Drive/pretrained-models/ model_mnist.h5'\n\njson_file = open(model_path, 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights(weights_path)\n\nTo make things easier. I decided to create a larger image with one digit\n\nin the middle and see how efficiently I can put a bounding box around it. To create the image, I used the following code:\n\n206\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nfrom PIL import Image, ImageOps src_img = Image.fromarray(x_test[5].reshape(28,28)) newimg = ImageOps.expand(src_img,border=56,fill='black')\n\nThe resulting image is 140x140 pixel. You can see it in Figure 6-7.\n\nFigure 6-7. The new image created by adding a white border of 56 pixels around one of the digits in the MNIST dataset\n\nNow let’s start with a sliding window that’s 28x28 pixels. We can write\n\na function that will try to localize the digit and that will get the image as input, the stride s, and the values wx and wy:\n\ndef localize_digit(bigimg, stride, wx, wy): slidx, slidy = wx, wy\n\ndigit_found = -1 max_prob = -1 bbx = -1 # Bounding box x upper left bby = -1 # Bounding box y upper left max_prob_ = 0.0 bbx_ = -1 bby_ = -1 most_prob_digit = -1\n\n207\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nmaxloopx = (bigimg.shape[0] -wx) // stride maxloopy = (bigimg.shape[1] -wy) // stride print((maxloopx, maxloopy))\n\nfor slicey in range (0, maxloopx*stride, stride): for slicex in range (0, maxloopy*stride, stride): slice_ = bigimg[slicex:slicex+wx, slicey:slicey+wx] img_ = Image. fromarray(slice_).resize((28, 28), Image.\n\nNEAREST)\n\nprobs = loaded_model.predict(np.array(img_).\n\nreshape(1,28,28,1)) if (np.max(probs > 0.2)): most_prob_digit = np.argmax(probs) max_prob_ = np.max(probs) bbx_ = slicex bby_ = slicey\n\nif (max_prob_ > max_prob): max_prob = max_prob_ bbx = bbx_ bby = bby_ digit_found = most_prob_digit\n\nprint(\"Digit \"+str(digit_found)+ \" found, with probability \"+str(max_prob)+\" at coordinates \"+str(bbx)+\" \"+str(bby))\n\nreturn (max_prob, bbx, bby, digit_found)\n\nRunning on our image as so:\n\nlocalize_digit(np.array(newimg), 28, 28, 28)\n\nReturns this code:\n\nDigit 1 found, with probability 1.0 at coordinates 56 56 (1.0, 56, 56, 1)\n\n208",
      "page_number": 212
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 220-231)",
      "start_page": 220,
      "end_page": 231,
      "detection_method": "topic_boundary",
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nThe resulting bounding boxes can be seen in Figure 6-8.\n\nFigure 6-8. The bounding box found by the sliding window method with wx = 28, wy = 28, and stride s = 28\n\nSo that works quite well. But you may have noticed that we used values\n\nfor wx, wy, and s that are exactly 28, which is the size of our images. What happens if we change that? For example, consider the cases depicted in Figure 6-9. You can clearly see how this method stops working as soon as the size and proportions of the window change to different values than 28.\n\n209\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFigure 6-9. Results of the sliding window algorithm with different values of wx, wy, and s\n\nCheck the confidence of the classification in Figure 6-9, in the lower- left box. It is quite low. For example, for a window 40x40 and a stride of 10, the classification of the digit is correct (a 1) but is done with a probability of 21%. That’s a low value! In the lower-right box, the classification is completely wrong. Keep in mind that you need to resize the small portion you cut from your image, and therefore it may look different from the training data you used.\n\nIn this case, it may seem easy to choose the right window size and proportions, since you know what the images looks like, but in general you have no idea what value will work. You would have to test different\n\n210\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nproportions and sizes and get several possible bounding boxes and classifications and then decide which one is the best. You can easily see how this becomes computationally infeasible with real images that may contain several objects with different dimensions and proportions.\n\nClassification and Localization\n\nWe have seen that the sliding window approach is a bad idea. A better approach is to use multi-task learning. The idea is that we can build a network that will learn at the same time the class and the position of the bounding box. We can achieve that by adding two dense layers after the last one of a CNN. One with (for example) Nc neurons (to classify Nc classes) that will predict the class with a cross-entropy loss function (that we will indicate with Jclassification), and one with four neurons that will learn the bounding boxes with a ℓ2 loss function (that we will indicate with JBB). You can see a diagram of the network in Figure 6-10.\n\nFigure 6-10. A diagram that depicts a network that can predict the class and the bounding box position at the same time\n\n211\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nSince this will be a multi task learning problem, we will need to\n\nminimize a linear combination of the two loss functions:\n\nJ\n\nclassification\n\n+a J\n\nBB\n\nOf course, α is an additional hyper-parameter that needs to be tuned.\n\nJust as a reference a ℓ2 loss is proportional to the MSE\n\n2\n\nLoss Function\n\n=\n\nm\n\nå\n\n(\n\ny\n\n( ) i true\n\n\n\ny\n\n( ) i predicted\n\n)\n\n2\n\ni\n\n= 1\n\nWhere we have, as usual, indicated with m the number of observations we have at our disposal. This same idea is used very successfully in human pose estimation, which finds specific points of the human body (like for example, the joints), as can be seen in Figure 6-11.\n\nFigure 6-11. An example of human pose estimation. A CNN can be trained to find important points of the human body, such as the joints.\n\n212\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nThere is a lot of research going on in this field, and in the next sections\n\nwe will look at how those methods work. The implementation becomes quite complex and time consuming. If you want to work with these algorithms, the best way is to look at the original papers and study them. Unfortunately, there is no plug-and-play library that you can use for those tasks, although you may find a GitHub repository that will help you. In this chapter, we will look at the most common variations of CNNs to do object localization—R-CNN, fast R-CNN, and faster R-CNN. In the next chapter, we will look at the YOLO (You Only Look Once) algorithm. The next sections should serve only as pointers to the relevant papers and will give you a basic understanding of the building blocks of the networks. This is by no means an exhaustive analysis of these implementations, as that would require a massive amount of space.\n\nRegion-Based CNN (R-CNN)\n\nThe basic idea of region-based CNNs (also known as R-CNNs) is quite simple (but implementing it is not). As we discussed, the main problem with naïve approaches is that you need to test a huge number of windows to be able to find the best matching bounding boxes. Searching every possible location is computationally infeasible, as it is testing all possible aspect ratios and window sizes.\n\nSo Girshick et al.4 proposed a method where they used an algorithm called selective search5 to first propose 2000 regions from the image (called the region proposals) and then, instead of classifying a huge number of regions, they classified just those 2000 regions.\n\n4 https://arxiv.org/pdf/1311.2524.pdf 5 Jasper R. R. Uijlings, Koen E. A. van de Sande, Theo Gevers, Arnold W. M. Smeulders International Journal of Computer Vision, Volume 104 (2), page 154-171, 2013 [http://toe.lt/b]\n\n213\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nSelective search has nothing to do with machine learning and uses a classical approach to determine which regions may contain an object. The first step in the algorithm is to segment an image, using pixel intensities and graph-based methods (for example, the one by Felzenszwalb and Huttenlocher6). You can see in Figure 6-12 the result of this segmentation.\n\nFigure 6-12. An example of segmentation applied to an image (image source: http://cs.brown.edu/people/pfelzens/segment/)\n\nAfter this step, adjacent regions are grouped together based on\n\nsimilarities of the following features:\n\nColor similarity\n\nTexture similarity\n\n6 P. Felzenszwalb, D. Huttenlocher, Efficient Graph-Based Image Segmentation, International Journal of Computer Vision, Vol. 59, No. 2, September 2004\n\n214\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nSize similarity\n\nShape compatibility\n\nThe exact details of how this is done go beyond the scope of this book, since those techniques are typically used in image-processing algorithms.\n\nIn the OpenCV7 library, there is an implementation of the algorithm that you can try. In Figure 6-13, you can see an example. I applied the algorithm to a picture I took and I asked the algorithm to propose 40 regions.\n\nFigure 6-13. An example of the output of the selective search algorithm as implemented in the OpenCV library\n\n7 https://opencv.org\n\n215\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nThe Python code that I used can be found on the following website:\n\nhttps://www.learnopencv.com/selective-search-for-object- detection-cpp-python/. The main idea of R-CNN is to use a CNN to label the regions that this algorithm proposed and then use support vector machines for the final classification.\n\nIn Figure 6-9, you can see for example that the laptop has not been identified as an object. But that is why one uses 2000 regions in R-CNN, to make sure that enough regions are proposed. Checking many regions manually cannot be done visually by a person. The number of regions and their overlap is so big that the task is not feasible anymore. If you try the OpenCV implementation of the algorithm, you will notice that it is quite slow. This is one of the main reasons that additional methods have been developed. The manual approach is, for example, not suitable for real-time object detection (for example, in a self driving car).\n\nR-CNN can be summarized in the following steps (the steps have been\n\ntaken from http://toe.lt/d):\n\n1. Take a pre-trained imagenet CNN (such as Alexnet).\n\n2. Re-train the last fully connected layer with the\n\nobjects that need to be detected and the “no-object” class.\n\n3. Get all proposals (around 2000 region proposals for each image) from selective search and resize them to match the CNN input.\n\n216\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\n4. Train SVM to classify each region between object and background (one binary SVM for each class).\n\n5. Use bounding box regression. Train a linear\n\nregression classifier that will output some correction factor for the bounding boxes.\n\nFast R-CNN\n\nGirshick improved on its algorithm and created what are known as “fast R-CNNs”.8 The main idea behind this algorithm is the following\n\n1. The image goes through the CNN and feature maps are extracted (the output of the convolutional layers).\n\n2. Regions are proposed, not based on the initial image, but based on the feature maps.\n\n3. Then the same feature maps and the proposed\n\nregions are used passed to a classifier that decides which object is in which region.\n\nA diagram explaining these steps is shown in Figure 6-14.\n\n8 https://arxiv.org/pdf/1504.08083.pdf\n\n217\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFigure 6-14. A diagram depicting the main steps of the algorithm for fast R-CNN\n\nThe reason this algorithm is faster than R-CNN is because you don’t have to feed 2000 region proposals to the convolutional neural network every time9—you do it only once.\n\n9 http://toe.lt/c\n\n218\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFaster R-CNN\n\nNote that R-CNN and fast R-CNN both use selective search to propose regions, and therefore are relatively slow. Even fast R-CNN needs around two seconds for each image, making this variation not suitable for real- time object detection. R-CNN needs around 50 seconds, and fast R-CNN around two seconds. But it turns out we can do even better, by removing the need to use selective search, since this turns out to be the bottleneck of both algorithms.\n\nRen et al.10 developed a new idea: to use a neural network to learn regions from labelled data, removing the slow selective search algorithm altogether. Faster R-CNN requires around 0.2 seconds, making them a fast algorithm for object detection. There is a very nice diagram depicting the main steps of a faster R-CNN that can be found at http://toe.lt/e.11 We report it in Figure 6-15 for you since I think it really helps in intuitively understanding the main building blocks of a faster R-CNN. The details tend to be quite complicated and therefore an intuitive and superficial description will not serve you. To understand the steps and the subtleties, you need more time and experience.\n\n10 https://arxiv.org/pdf/1506.01497.pdf 11 Part of the image appears in the original paper by Ren, but additional labels and information have been added by Leonardo Araujo dos Santos (https://legacy. gitbook.com/@leonardoaraujosantos).\n\n219\n\nChapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFigure 6-15. A diagram depicting the main parts of faster R-CNN. Image source: http://toe.lt/e.\n\nIn the next chapter, we will look at another algorithm (YOLO) and see\n\nhow you can use those techniques in your own projects.\n\n220",
      "page_number": 220
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 232-241)",
      "start_page": 232,
      "end_page": 241,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 7\n\nObject Localization: An Implementation in Python\n\nIn this chapter, we will look at the YOLO (You Only Look Once) method for object detection. The chapter is split in two parts: in the first section we learn how the algorithm works, and in the second section, I will give an example of how you can use it in your own Python projects.\n\nKeep in mind that YOLO is quite complicated, so for 99% of you, a pre-trained model is the best choice for doing object detection. For the 1% at the forefront of the research, you probably don’t need this book anyway and you should know how to do object detection starting from scratch.\n\nThis chapter (as the previous one) should serve to point you in the right direction, give you the fundamentals you need to understand the algorithm, and give you your first experiences with object detection. You will notice quite soon that those methods are slow, difficult to implement, and have many limitations. This is a very active research field that is also very young. The paper describing YOLO version 3 (that we will use later in the chapter in the Python code) was published just in April, 2018. At the time of this writing, it’s less than two years old!\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_7\n\n221\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nThose algorithms are difficult to implement, difficult to understand, and very difficult to train. I hope that by the end of this chapter, you will understand the basics of it, and you can perform your first tests with the models.\n\nNote Those algorithms are difficult to implement, difficult to understand, and very difficult to train.\n\nThe You Only Look Once (YOLO) Method\n\nIn the last chapter, we looked at several methods for object detection. I also showed you why using a sliding window is a bad idea and where the difficulties are. In 2015, Redmon J. et al. proposed a new method to do object detection: they called it YOLO (You Only Look Once). They developed a network that can perform all the necessary tasks (detect where the objects are, classify multiple objects, etc.) in one pass. This is one of the reasons that this method is fast and is used often in real-time applications.\n\nIn the literature, you will find three versions of the algorithm: YOLOv1, YOLOv2, and YOLOv3. v2 and v3 are improvements over v1 (more on that later). The original network has been developed and trained with darknet, a neural network framework developed by the author of the original algorithm, Redmon J. You will not find an easy-to-download, pre-trained model that you can use with Keras. More on that later when I give you an example of how you can use it in your projects.\n\nIt is very instructive to read the original paper on YOLO, which can be\n\nfound at https://arxiv.org/abs/1506.02640.\n\n222\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nNote The main idea of the method is to reframe the detection problem as one single regression problem, from the pixels of the image as inputs, to the bounding box coordinates and class probabilities1.\n\nLet’s see how it works in detail.\n\nHow YOLO Works\n\nTo understand how YOLO works, it’s best to go through the algorithm step by step.\n\nDividing the Image Into Cells\n\nThe first step is to divide the image into S × S cells. For each cell, we predict what (and if an) object is in the cell. Only one object will be predicted for each cell, so one cell cannot predict multiple objects. Then for each cell, a certain number (B) of bounding boxes that should contain the object are predicted. In Figure 7-1, you can see the grid and the bounding boxes that the network might predict (as an example). In the original paper, the image was divided into a 7 × 7 grid, but for the sake of clarity in Figure 7-1, I divided the image into a 5x5 grid.\n\n1 Redmon J. et al., “You Only Look Once: Unified, Real-Time Object Detection,” https://arxiv.org/abs/1506.02640.\n\n223\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nFigure 7-1. Image divided into a 5 × 5 grid. For cell D3, we will predict the mouse and will predict bounding boxes (the yellow boxes). For cell B2, we will predict a bottle and its bounding boxes (the red rectangles).\n\nLet’s take as an example cell D3 in Figure 7-1. This cell will predict the presence of a mouse and then it will predict a certain number B of bounding boxes (the yellow rectangles). Similarly, cell B2 will predict the presence of the bottle and B bounding boxes (the red rectangles in Figure 7-1) all at the same time. Additionally, the model predicts a class confidence (a number) for each bounding box. To be precise, the model output for each cell is as follows:\n\nFor each bounding box (B in total), there are four\n\nvalues: x, y, w, h. These are the position of the center, its width, and its height. Note that the position of the center is given with relationship to the cell position, not as an absolute value.\n\n224\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nFor each bounding box (B in total), there is a\n\nconfidence score, which is a number that reflects how likely the box contains the object. In particular, at training time, if we indicate the probability of the cell containing the object as Pr(Object), the confidence is calculated as follows:\n\n(\n\nPr Object\n\n)´\n\nIOU\n\nWhere IOU indicates the Intersection Over Union, which is calculated using the training data (see the previous chapter for an explanation of the term and how to calculate it). This number encodes at the same time the probability that a specific object is in a box and how good the bounding box fits the object.\n\nTherefore, supposing we have S = 5, B = 2 and supposing the network can classify Nc = 80 classes, the network will have an output of size of the following:\n\n(\n\n´ ´ S S B\n\n´ + 5\n\nNc\n\n)= ´ ´ ´ + (\n\n5 5\n\n2 5 80\n\n)=\n\n2250\n\nIn the original paper, the authors used S = 7, B = 2 and used the VOC dataset2 with 20 labelled classes. Therefore, the output of the network was as follows:\n\n(\n\n´ ´ S S B\n\n´ + 5\n\nNc\n\n)= ´ ´ ´ + (\n\n7 7\n\n2 5 20\n\n)=\n\n1470\n\nThe network structure is quite easy. It’s simply a set of several\n\nconvolutional layers (with some maxpool thrown in there) and a big dense layer at the end to predict the necessary values (remember the problem is\n\n2 http://host.robots.ox.ac.uk/pascal/VOC/\n\n225\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nframed as a regression problem). In the original paper, the authors were inspired by the GoogLeNet model. The network has 24 layers followed by two dense layers (the last one having 1470 neurons; do you see why?). Training took, as the authors mentioned, one entire week. They used a few tricks for the training, and if you are interested, I strongly suggest you read the original paper. It’s quite instructive (for example, they also used learning rate decay in an unusual way, increasing the value of the learning rate at the beginning and then lowering it later). They also used dropout and extensive data augmentation. Training those models is not a trivial undertaking.\n\nYOLOv2 (Also Known As YOLO9000)\n\nThe original YOLO version had some shortcomings. For example, it was not very good at detecting objects that were too close. In the second version,3 the authors introduced some optimizations, the most important one being anchor boxes. The network gives pre-determined sets of boxes, and instead of predicting bounding boxes completely from scratch, it simply predicted deviations from the set of anchor boxes. The anchor boxes can be chosen depending on the type of objects that you want to predict, making the network better at certain specific tasks (for example, small or big objects).\n\nIn this version, they also changed the network structure, using 19 layers and then 11 more layers specifically designed for object detection, for a total of 30 layers. This version also struggled with small objects (also when using anchor boxes). This was because the layers downsampled the image and, during the forward pass-through, the network information was lost, making detecting small things difficult.\n\n3 Redmon J., Farhadi A., “YOLO9000: Better, Faster, Stronger,” https://arxiv.org/ abs/1612.08242\n\n226\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nYOLOv3\n\nThe last version4 introduces a few new concepts that makes the model quite powerful. Here are the main improvements:\n\nPredicting boxes at different scales: The model predicts boxes with different dimensions, so to say (is a bit more complicated than that, but that should give you an intuitive understanding of what is going on).\n\nThe network is much bigger: A total of 53 layers.\n\nThe network uses skip connections. Basically, this\n\nmeans that the output of a layer will be fed not only to the very next layer but also to a layer coming later in the network. This way, the information not yet downsampled will be used later to make detecting small objects easier. Skip connections are used in ResNets (not discussed in this book), and you can find a good introduction at http://toe.lt/w.\n\nThis version uses nine anchor boxes, three for\n\neach scale.\n\nThis version predicts more bounding boxes for\n\neach cell.\n\nAll those improvements make YOLOv3 quite good, but also quite slow,\n\ndue to the increased computational power needed to process all those numbers.\n\n4 Redmon J., Farhadi A., “YOLOv3: An Incremental Improvement,” https://arxiv. org/pdf/1804.02767.pdf\n\n227\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nNon-Maxima Suppression\n\nOnce you have all the predicted bounding boxes, you need to choose the best one. Remember that for each cell and object, the model predicts several bounding boxes (regardless of which version you use). Basically, you choose the best bounding box by following this procedure (called non- maxima suppression):\n\n1.\n\nIt first discards all cells in which the probability of an object being present is less than a given threshold (typically 0.6).\n\n2.\n\nIt takes all the cells with the highest probability of having an object inside.\n\n3.\n\nIt takes the bounding boxes that have the highest score and removes all other bounding boxes that have an IOU greater than a certain threshold (typically 0.5) with each other. That means that it removes all bounding boxes that are very similar to the chosen one.\n\nLoss Function\n\nNote that the networks mentioned previously have a large number of outputs, so you should not expect a simple loss function to work. Also note that different parts of the final layer have very different meanings. One part is bounding box positions, one part is class probabilities, and so on. The loss function has three parts:\n\nClassification loss\n\nLocalization loss (the error between predicted bounding boxes and the expected results)\n\nConfidence loss (whether there is an object in the box)\n\nLet’s take a closer look at these three aspects of loss.\n\n228\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nClassification Loss\n\nThe classification loss used is determined by\n\n2\n\nS\n\n( å å ( )- ( ) ˆ p c i\n\nobj i\n\np c i\n\n\n\n)\n\n2\n\ni\n\n=\n\n0\n\nÎ c classes\n\nWhere\n\nobjis1 if an object is in cell i, or 0 otherwise. i ˆp ci( ) denotes the probability of having class c in cell i.\n\nLocalization Loss\n\nThis loss measures the error of the predicted bounding boxes with respect to the expected ones.\n\nl\n\ncoord\n\nS\n\n2\n\nB\n\nåå \n\ni\n\n=\n\n0\n\n=\n\n0\n\nobj i\n\né ë\n\n( x i\n\nˆ x\n\ni\n\n2\n\n) +\n\n( y i\n\nˆ y\n\ni\n\n)\n\n2\n\nù û\n\n+\n\nl\n\ncoord\n\n2\n\nS\n\nå\n\ni\n\n=\n\n0\n\nB\n\nå \n\n=\n\n0\n\nobj i\n\né ê ë\n\n(\n\nw\n\ni\n\n\n\nˆ w\n\n2\n\n) +\n\n(\n\nh i\n\n\n\nˆ h i\n\n)\n\n2\n\nù ú û\n\nConfidence Loss\n\nThe confidence loss measures the error when deciding if an object is in the box or not.\n\n2\n\nS\n\nB\n\nåå \n\nobj ij\n\n(\n\nˆ - C Ci\n\n)\n\n2\n\ni\n\n=\n\n0\n\nj\n\n=\n\n0\n\nWhere ˆCi is the confidence of the box j in cell i. objis1 if the jth bounding box in cell i is responsible for detecting the ij object.\n\n229\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nSince most cells does not contain an object, we must be careful. The network could learn that the background is important. We need to add a term to the cost function to remedy this. This is done with the additional term:\n\nlnoobj\n\nS\n\n2\n\nB\n\nåå \n\nnoobj ij\n\n(\n\nˆ - C Ci\n\ni\n\n)\n\n2\n\n=\n\n0\n\nj\n\n=\n\n0\n\nWhere ij\n\nnoobj is the opposite of ij\n\nobj .\n\nTotal Loss Function\n\nThe total loss function is simply the sum of all the terms:\n\nL\n\n=\n\nS\n\n2\n\nå å\n\nobj i\n\n\n\ni\n\n=\n\n0\n\nÎ c classes\n\n(\n\n( )- ( ) ˆ p c i\n\np c i\n\n2\n\n) +\n\nl\n\ncoord\n\nS\n\n2\n\nB\n\nåå \n\ni\n\n=\n\n0\n\n=\n\n0\n\noobj i\n\né ë\n\n( x i\n\nˆ x\n\ni\n\n2\n\n) +\n\n( y i\n\nˆ y\n\nl\n\ncoord\n\nS\n\n2\n\nB\n\nåå \n\ni\n\n=\n\n0\n\n=\n\n0\n\nobj i\n\né ê ë\n\n(\n\n2\n\nS\n\nB\n\nåå \n\nobj ij\n\n(\n\nˆ - C Ci\n\ni\n\nw\n\ni\n\n2\n\n) +\n\n\n\nˆ w\n\n)\n\n22\n\nl\n\nnoobj\n\n2\n\nS\n\nå\n\n+\n\n(\n\nB\n\nå \n\nh i\n\nnoobj ij\n\n\n\nˆ h i\n\n)\n\n(\n\nˆ - C Ci\n\n2\n\ni\n\nù ú+ û )\n\n2\n\ni\n\n=\n\n0\n\nj\n\n=\n\n0\n\ni\n\n=\n\n0\n\njj\n\n=\n\n0\n\nAs you can see, it’s a complicated formula to implement. This is one of the reasons that the easiest way to do object detection is to download and use a pre-trained model. Starting from scratch will require some time and effort. Believe me.\n\nIn the next sections, we will look at how you can use YOLO algorithms\n\n(in particular, YOLOv3) in your own Python projects.\n\n230\n\n)\n\n2\n\nù û\n\n+",
      "page_number": 232
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 242-249)",
      "start_page": 242,
      "end_page": 249,
      "detection_method": "topic_boundary",
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nYOLO Implementation in Python and OpenCV Darknet Implementation of YOLO\n\nIf you followed the previous sections, you understand that developing your own models for YOLO from scratch is not feasible for a beginner (and for almost all practitioners), so, as we have done in previous chapters, we need to use pre-trained models to use object detection in your projects. The web page where you can find all the pre-trained models you could ever want is https://pjreddie.com. This is the home page of Joseph C. Redmon, the maintainer of Darknet.\n\nNote Darknet is an open source neural network framework written in C and CUDa. it is fast, easy to install, and supports CpU and GpU computation.\n\nOn a subpage (https://pjreddie.com/darknet/yolo/), you will find all the information you need about the YOLO algorithm. You can download from this page the weights of several pre-trained models. For each model, you will always need two files:\n\nA .cfg file, which basically contains the structure of the\n\nnetwork.\n\nA .weights file, which contains the weights obtained\n\nafter training.\n\nTo give you an idea of the content of the files, the .cfg file contains, among other things, information on all the layers used. An example follows:\n\n[convolutional] batch_normalize=1 filters=64 size=3\n\n231\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nstride=1 pad=1 activation=leaky\n\nThis tells you how that particular convolutional layer is structured. The\n\nmost important information contained in the file is about:\n\nNetwork architecture\n\nAnchor boxes\n\nNumber of classes\n\nLearning rate and other parameters used\n\nBatch size\n\nThe other file (.weights) contains the pre-trained weights that you need in order to perform inference. Note that they are not saved in a Keras compatible format (like the .h5 files we have used so far), so they cannot be loaded in a Keras model unless you convert them first.\n\nThere is no standard tool or utility to convert those files, since the format is not constant (it has changed for example between YOLOv2 and YOLOv3). If you are interested in using YOLO up to v2, you can use the YAD2K library (Yet Another Darknet 2 Keras), which can be found at https://github.com/allanzelener/YAD2K.\n\nNote that this does not work on YOLOv3 .cfg files. Believe me, I have\n\ntried. But if you are happy with YOLOv2, you can use the code in this repository to convert the .weight files into a more Keras-friendly format. I also want to point out another GitHub repository that implemented a converter for YOLOv3 at https://github.com/qqwweee/keras-yolo3. It has some limitations (for example, you must use standard anchors), but it may be a good starting point to convert the files. However, there is an easier way to use the pre-trained models and that is using OpenCV, as we will see later in the chapter.\n\n232\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nTesting Object Detection with Darknet\n\nIf you simply want to perform some classification on an image, the easiest way to do that is to follow the instructions on the darknet website. Let’s look at how that works here. Note that the instructions work if you are on a Linux or MacOS X system. On Windows, you need to have make, gcc, and several other tools installed. As described on the website, the installation needs only a few lines:\n\ngit clone https://github.com/pjreddie/darknet cd darknet make wget https://pjreddie.com/media/files/yolov3.weights\n\nAt this point, you can simply perform your object detection with this:5\n\n./darknet detect cfg/yolov3.cfg yolov3.weights table.jpg\n\nNote that the .weight file is very big (around 237MB). Keep that in mind when downloading it. On a CPU this is quite slow; it took a very modern MacBook Pro from 2018 18 seconds to download. You can see the result in Figure 7-2.\n\n5 You can find the image used for testing in the GitHub repository within Chapter 7.\n\n233\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nFigure 7-2. YOLOv3 used with darknet on a test image\n\nBy default, a threshold of 0.25 is used. But you can specify a different\n\none using the -thresh XYZ parameter. You must change XYZ to the threshold value you want to use.\n\nThis method is nice for playing with object detection, but it’s difficult to use in your Python projects. To do that, you will need to be able to use the pre-trained models in your code. There are several ways to do that, but the easiest way is to use the opencv library. If you are working with images, chances are that you are already working with this library. If you have never heard of it, I strongly suggest you check it out, since it’s a great library for working with images. You can find the official web page at https:// opencv.org.\n\nYou can, as usual, find the entire code in the GitHub repository, within the Chapter 7 folder of this book. We will discuss only the most important parts for brevity.\n\n234\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nYou will need to have the newest opencv library installed. The code we\n\ndiscuss here has been developed with version 4.1.0. To determine which version you have, use this:\n\nimport cv2 print (cv2.__version__)\n\nTo try the code we discuss here, you need three files from the https://\n\npjreddie.com website:\n\n\n\ncoco.names\n\n\n\nyolov3.cfg\n\n\n\nyolov3.weights\n\ncoco.names contains the labels of the classes that the pre-trained model can classify. The yolov3.cfg and yolov3.weights files contain the model configuration parameters (as we have discussed) and the weights we need to use. For your convenience, since the yolov3.weights is about 240MB and cannot be uploaded to GitHub, you can download a ZIP file of all three at http://toe.lt/r. In the code, we need to specify where the files are. For example, you can use the following code:\n\nweightsPath = \"yolo-coco/yolov3.weights\" configPath = \"yolo-coco/yolov3.cfg\"\n\nYou need to change the location to where you saved the files on your\n\nsystem. OpenCV provides a function to load the weights without the need to convert them:\n\nnet = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n\nThis is quite confortable, since you don’t need to analyze or write your\n\nown loading function. It returns a model object that we will use later for inference. If you remember from the discussion about the method at the\n\n235\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nbeginning of the chapter, we need to get the output layers, in order to get all the information we need, like bounding boxes or predicted classes. We can do that easily with the following code:\n\nln = net.getLayerNames() ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n\nThe getUnconnectedOutLayers() function returns indexes of layers with unconnected outputs, which is exactly what we are looking for. The ln variable will contain the following layers:\n\n['yolo_82', 'yolo_94', 'yolo_106']\n\nThen we need to resize the image in a square 416x416 image and\n\nnormalize it by dividing the pixel values by 255.0:\n\nblob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n\nThen we need to use it as input to our model saved in the net model:\n\nnet.setInput(blob)\n\nAnd then we can use the forward() call to do a forward pass-through\n\nof the pre-trained model:\n\nlayerOutputs = net.forward(ln)\n\nWe are not yet done, so don’t relax. We need to extract the bounding boxes, which we will save in the boxes list, then the confidences, saved in the confidences list, and then the predicted classes, saved in the classIDs list.\n\nWe first initialize the lists as follows:\n\nboxes = [] confidences = [] classIDs = []\n\n236\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nThen we loop over the layers and extract the information we need. We\n\ncan perform the loops as follows:\n\nfor output in layerOutputs: for detection in output:\n\nNow the scores are saved in the elements starting from the fifth in the detection variable, and we can extract the predicted class with np. argmax(scores):\n\nscores = detection[5:] classID = np.argmax(scores)\n\nThe confidence is of course the score of the predicted class:\n\nconfidence = scores[classID]\n\nWe want to keep predictions with a confidence bigger than zero. In the\n\ncode used here, we chose a limit of 0.15. The predicted bounding box is contained in the first four values of the detection variable:\n\nbox = detection[0:4] * np.array([W, H, W, H]) (centerX, centerY, width, height) = box.astype(\"int\")\n\nAnd if you remember, YOLO predicts the center of the bounding box,\n\nso we need to extract the upper-left corner position:\n\nx = int(centerX - (width / 2)) y = int(centerY - (height / 2))\n\nAnd then we can simply append the found values to the lists:\n\nboxes.append([x, y, int(width), int(height)]) confidences.append(float(confidence)) classIDs.append(classID)\n\n237\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nThen we need to use non-maxima suppression (as discussed in the\n\nprevious sections). OpenCV provides also a function6 for it:\n\nidxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.6,0.2)\n\nThe function needs the following parameters:\n\nA set of bounding boxes (saved in the boxes variable)\n\nA set of confidences (saved in the confidences\n\nvariable)\n\nA threshold used to filter boxes by score (0.6 in the\n\nprevious code)\n\nThe threshold used in non-maximum suppression (the\n\n0.2 in the previous code)\n\nThen we can obtain the right coordinates with this simple code:\n\nfor i in idxs.flatten(): # extract the bounding box coordinates (x, y) = (boxes[i][0], boxes[i][1]) (w, h) = (boxes[i][2], boxes[i][3])\n\nYou can see in Figure 7-3 the results of this code.\n\n6 You can find the official documentation at http://toe.lt/t.\n\n238",
      "page_number": 242
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 250-257)",
      "start_page": 250,
      "end_page": 257,
      "detection_method": "topic_boundary",
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nFigure 7-3. YOLOv3 results obtained with OpenCV\n\nThat is exactly as it should be—the same results as in Figure 7-2. In addition, we have the probability of the prediction on the box. You can see how easy this is. You simply add those few lines of code to your project. Keep in mind that the model we built using the pre-trained weights will only detect the objects that are contained in the image dataset that the pre-trained model has been trained with. If you need to use the model on different objects, you need to fine-tune the models, or train it from scratch for your objects. Describing how to train the model completely from scratch is beyond the scope of the book, but in the next section, I provide some pointers in case you need to do it.\n\n239\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nTraining a Model for YOLO for Your Specific Images\n\nI will not describe the different procedures you need to train your own YOLO models, since that would take a few chapters on its own, but I hope I can point you in the right direction. Let’s suppose you want to train a model specifically for your images. As a first step, you need the training data. Supposing you have enough images, you first need to label them. Remember that you need to mark the right bounding boxes for each image. Doing that manually is an almost impossible task, so I suggest two projects that will help you label your training data.\n\nBBox-Label-Tool by Darkflow Annotations: This tool can be found at https://github.com/enriqueav/ BBox-Label-Tool. The tool saves the annotations in the right format as expected by Darkflow (a Python wrapper that can use darknet weight files, https:// github.com/thtrieu/darkflow).\n\n\n\nlabelImg: This tool can be found at https://github. com/tzutalin/labelImg. This tool can be used with several Python installations (including Anaconda, for example) and on several operating systems (including Windows).\n\nCheck them out in case you want to try to train your YOLO model on your data. Since describing the entire procedure would go well beyond the scope of the book, I suggest you read the following medium post, which does quite a good job at describing how to do that: http://toe.lt/v. Remember that you need to modify a cfg file so that you can specify the right number of classes that you are trying to identify. For example, in the yolov3.cfg file, you will find this line (at line 610):\n\nclasses=80\n\n240\n\nChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nIt tells you how many classes you can identify with the models. You will\n\nneed to modify this line to reflect the number of classes you have in your problem.\n\nOn the official YOLO website, there is a detailed description of how to\n\ndo that: https://pjreddie.com/darknet/yolo/. Scroll down until you find the sections on training the models with your own datasets. Do not underestimate the complexity of this task. Lots of reading and testing will be required.\n\nConcluding Remarks\n\nAs you might have noticed, using these advanced techniques is quite complicated and not simply a matter of copying a few lines of code. You need to make sure you understand how the algorithms work to be able to use them in your own projects. Depending on the object you need to detect, you may need to spend quite some time building a custom model suited for your problem. That will require lots of testing and coding. It will not be easy. My goal with this chapter was to give you enough tools to help you and point you in the right direction.\n\nAfter the previous chapters, you have now enough understanding of advanced techniques to be able to re-implement even complicated algorithms as YOLO on your own, although this will require time and effort. You will suffer a lot, but if you don’t give up, you will be rewarded with success. I am sure of it.\n\nIn the next chapter, we look at a complete example that uses CNNs on real data, where we use all the techniques that we have learned so far. Consider Chapter 8 as an exercise. Try to play with the data and reproduce the results described there. I hope you have fun!\n\n241\n\nCHAPTER 8\n\nHistology Tissue Classification\n\nNow it’s time to put all we have learned together and see how the techniques we have learned so far can be used on a real dataset. We will use a dataset that I have used successfully as my end project in my university course on deep learning: the “collection of textures in colorectal cancer histology”.1 This dataset can be found on several websites:\n\n\n\nhttp://toe.lt/f: On zenodo.org\n\n\n\nhttp://toe.lt/g: On Kaggle (this dataset was prepared originally by Kevin Mäder2 and me for the purpose of the university course we held during the Autumn semester of 2018 at the Zürich University of Applied Science)\n\n\n\nhttp://toe.lt/h: Since TensorFlow 2.0, this is also available as a pre-read dataset (the link points to the TensorFlow GitHub repository for the dataset’s API)\n\n1 Kather JN, Weis CA, Bianconi F, Melchers SM, Schad LR, Gaiser T, Marx A, Zollner F: Multi-class texture analysis in colorectal cancer histology (2016), Scientific Reports (in press) 2 https://www.linkedin.com/in/kevinmader/\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_8\n\n243\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nDon’t download the data yet. I prepared a pickle (more on that later) file for you with all the data ready to be used. You will find all the information in the next section.\n\nThe thing we will use in this chapter is the Kather_texture_2016_\n\nimage_tiles_5000 folder and it contains 5000 histological images of 150x150px each (74x74μm). Each image belongs to exactly one of eight tissue categories (specified by the folder name from the Zenodo website). In the code, I assume that, within the folder where you have your Jupyter Notebooks, you have a data folder and under that data folder, you have the Kather_texture_2016_image_tiles_5000 folder.\n\nIn the GitHub repository for this book, the folder for Chapter 8 contains the complete code that you can use. In this chapter, we will look only at the parts that are relevant to our discussions. If you want to try this, please use the GitHub repository. The code is complete and directly usable. The goal of this project is to build a classifier that can classify the different images into one of eight classes. We will look at them in the next sections and see where the difficulties are. Let’s start, as usual, with the data.\n\nMost of the code was developed by Fabien Tarrade (https://www. linkedin.com/in/fabientarrade/) for my university course, and he was nice enough to give me permission to use it. I have updated it quite a bit to make it usable in this example. Note that everything that works is thanks to Fabien, and all the bugs are my fault.\n\nData Analysis and Preparation\n\nThe code for this section is contained in the notebook called 01- Data exploration and preparation.ipynb, which is in the book’s GitHub repository in the Chapter 8 folder. Feel free to follow this discussion with a window open on your computer to try the code. Since we have the images in different folders, we need to load them in a pandas dataframe and automatically generate a label from the folder name. For example, the image\n\n244\n\nChapter 8\n\nhistology tissue ClassifiCation\n\n1A11_CRC-Prim-HE-07_022.tif_Row_601_Col_151.tif is contained in the folder 01_TUMOR and therefore must have \"TUMOR\" as its label.\n\nWe can automate that process in a very simple way. We start with this\n\ncode (for all the imports, please check the code in GitHub):\n\ndf = pd.DataFrame({'path': glob(os.path.join(base_dir, '*', '*.tif'))})\n\nThis generates a dataframe with just one column, 'path'. This column\n\ncontains the path to each image we want to load. The base_dir variable contains the path to the Kather_texture_2016_image_tiles_5000 folder. For example, I am running the code in Google Colab and my base_dir looks like this:\n\nbase_dir = '/content/drive/My Drive/Book2-ch8/data/Kather_ texture_2016_image_tiles_5000'\n\nThe first five records of my dataframe look like this:\n\n/content/drive/My Drive/Book2-ch8/data/Kather_texture_2016_ image_tiles_5000/05_DEBRIS/5434_CRC-Prim-HE-04_002.tif_Row_451_ Col_1351.tif /content/drive/My Drive/Book2-ch8/data/Kather_texture_2016_ image_tiles_5000/05_DEBRIS/626A_CRC-Prim-HE-08_024.tif_Row_451_ Col_1.tif /content/drive/My Drive/Book2-ch8/data/Kather_texture_2016_ image_tiles_5000/05_DEBRIS/148A7_CRC-Prim-HE-04_004.tif_ Row_151_Col_901.tif /content/drive/My Drive/Book2-ch8/data/Kather_texture_2016_ image_tiles_5000/05_DEBRIS/6B37_CRC-Prim-HE-08_024.tif_ Row_1501_Col_301.tif /content/drive/My Drive/Book2-ch8/data/Kather_texture_2016_ image_tiles_5000/05_DEBRIS/6B44_CRC-Prim-HE-03_010.tif_Row_301_ Col_451.tif\n\n245\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nNow we can use the .map() function to extract all the information we\n\nneed and create new columns.\n\ndf['file_id'] = df['path'].map(lambda x: os.path.splitext(os. path.basename(x))[0]) df['cell_type'] = df['path'].map(lambda x: os.path.basename(os. path.dirname(x))) df['cell_type_idx'] = df['cell_type'].map(lambda x: int(x. split('_')[0])) df['cell_type'] = df['cell_type'].map(lambda x: x.split('_')[1]) df['full_image_name'] = df['file_id'].map(lambda x: x.split('_ Row')[0]) df['full_image_row'] = df['file_id'].map(lambda x: int(x. split('_')[-3])) df['full_image_col'] = df['file_id'].map(lambda x: int(x. split('_')[-1]))\n\nYou can easily check what each call is doing. The column name should\n\ntell you what you will have in each column. In Figure 8-1, you can see the first two records of the dataframe so far.\n\nFigure 8-1. The first two records of the dataframe df before loading the images\n\nAt this point, we must read the images with imread(). To do this, we\n\ncan simply use\n\ndf['image'] = df['path'].map(imread)\n\nKeep in mind that this can take some time (depending on where you are running it). This will create a new column called image that will contain\n\n246\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nthe images. For your convenience, I used the to_pickle() pandas call to save the dataframe to disk. Pickling is the process whereby a Python object hierarchy is converted into a byte stream3 and then can be saved on-disk. The file is called dataframe_Kather_texture_2016_image_tiles_5000. pkl. You can load it with:\n\ndf=pd.read_pickle('/content/drive/My Drive/Book2-ch8/data/ dataframe_Kather_texture_2016_image_tiles_5000.pkl')\n\nThis way, you can save yourself lots of time. You don’t even need to download the data since you can simply use the pickle I prepared for you. Note that the pickles are too big for GitHub, so I saved them on a server where you can download them. You will find the links in GitHub and at the end of this section. First things first: what classes do we have in this dataset? We can check the labels we have with this code:\n\ndf['cell_type'].unique()\n\nThis will give us the following:\n\narray(['DEBRIS', 'ADIPOSE', 'LYMPHO', 'EMPTY', 'STROMA', 'TUMOR', 'MUCOSA', 'COMPLEX'], dtype=object)\n\nSo here are our eight classes. We have 5000 images, which we can\n\ncheck using this:\n\ndf.shape\n\nIt gives us this:\n\n(5000, 8)\n\n3 From the official Python documentation: https://docs.python.org/2/library/ pickle.html\n\n247",
      "page_number": 250
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 258-265)",
      "start_page": 258,
      "end_page": 265,
      "detection_method": "topic_boundary",
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nThe next step is to check if we have a balanced class distribution. We\n\ncan count how many images we have for each class:\n\ndf['cell_type'].value_counts()\n\nLuckily, we have exactly 625 images for each class.\n\nEMPTY 625 ADIPOSE 625 STROMA 625 COMPLEX 625 LYMPHO 625 DEBRIS 625 TUMOR 625 MUCOSA 625 Name: cell_type, dtype: int64\n\nStrangely enough, there are five duplicate images. You can check that\n\nwith this code:\n\ndf['full_image_name'][df.duplicated('full_image_name')]\n\nThis will report the names of the images that appear twice. You can see them in Figure 8-2. Since there are only five, we will simply ignore this problem.\n\nFigure 8-2. Five images appear twice in the dataset\n\n248\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nIn Figure 8-3, you can see a few examples of each class.\n\nFigure 8-3. Examples of the images in each class\n\n249\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nAs expected, each image has a size of (150, 150, 3):\n\ndf['image'][0].shape (150, 150, 3)\n\nNote how the classes are ordered, which is due to how we loaded the data.\n\nThe DEBRIS class comes first, then ADIPOSE, and so on. This can be checked using a plot of the class label versus the index, as you can see in Figure 8-4.\n\nFigure 8-4. A plot showing how the images in the dataframe are ordered\n\nNow we can randomly shuffle the elements:\n\nimport random rows = df.index.values random.shuffle(rows) print(rows)\n\nThat will give you\n\narray([1115, 4839, 3684, ..., 187, 1497, 2375])\n\nYou can see that the indexes are now randomly shuffled. The last step\n\nwe need to take is to modify the actual dataframe:\n\ndf=df.reindex(rows) df.sort_index(inplace=True)\n\n250\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nAt this point, the elements are shuffled. Now we need to one-hot- encode the labels. Pandas provide a very useful and easy-to-use method for this process:\n\ndf_label = pd.get_dummies(df['cell_type'])\n\nIt will give you one-hot-encoded labels, as you can see from Figure 8-5.\n\nFigure 8-5. The result of using the get_dummies() pandas function to one-hot-encode labels\n\nThere are a few steps that we need to use the data with Keras. One is\n\nthat we need to transform the dataframe to a numpy array:\n\ndata=np.array(df['image'].tolist())\n\nThen, as usual, we need to create a training, test, and development\n\ndataset to make all the usual checks:\n\nx, x_test, y, y_test = train_test_split(data, label, test_ size=0.2,train_size=0.8) x_train, x_val, y_train, y_val = train_test_split(x, y, test_ size = 0.25,train_size =0.75)\n\nYou can check the dimensions of the three datasets easily with this code:\n\nprint('1- Training set:', x_train.shape, y_train.shape) print('2- Validation set:', x_val.shape, y_val.shape) print('3- Testing set:', x_test.shape, y_test.shape)\n\n251\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nThat should give you the following:\n\n1- Training set: (3000, 150, 150, 3) (3000, 8) 2- Validation set: (1000, 150, 150, 3) (1000, 8) 3- Testing set: (1000, 150, 150, 3) (1000, 8)\n\nNow you will see that the data is of type integer. We need to cast them\n\nto floating point numbers since we want to normalize them later. To do that, we use this code:\n\nx_train = np.array(x_train, dtype=np.float32) x_test = np.array(x_test, dtype=np.float32) x_val = np.array( x_val, dtype=np.float32)\n\nThen we can normalize the datasets (remember that each pixel will\n\nhave a maximum value of 255):\n\nx_train /= 255.0 x_test /= 255.0 x_val /= 255.0\n\nFor your convenience, I saved all the prepared datasets as pickles. If\n\nyou want to follow from here and play with the data, you need to load the pickles using the following commands (you will need to change the folder name where the files are saved):\n\nx_train=pickle.load(open('/content/drive/My Drive/Book2-ch8/ data/x_train.pkl', 'rb')) x_test=pickle.load(open('/content/drive/My Drive/Book2-ch8/ data/x_test.pkl', 'rb')) x_val=pickle.load(open('/content/drive/My Drive/Book2-ch8/ data/x_val.pkl', 'rb')) y_train=pickle.load(open('/content/drive/My Drive/Book2-ch8/ data/y_train.pkl', 'rb')) y_test=pickle.load(open('/content/drive/My Drive/Book2-ch8/ data/y_test.pkl', 'rb')) y_val=pickle.load(open('/content/drive/My Drive/Book2-ch8/ data/y_val.pkl', 'rb'))\n\n252\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nYou will then have everything ready. Keep in mind that the files\n\ncontaining the data (x_train, x_test, and x_val) are big files, with x_train being 800MB unzipped. Keep that in mind if you plan to download the files or upload them on your Google drive. Of course, you will need to change the folder to where your data is saved. This will save you time. Pickles are usually saved, since you don’t want to rerun the entire data preparation each time you experiment with the data. In the 01- Data exploration and preparation. ipynb files, you will also find some histogram analysis and data augmentation examples. For space reasons and to keep this chapter compact, we will not look at histogram analysis, but we will talk about data augmentation later in the chapter, as it’s a very effective way of fighting overfitting.\n\nThe files were too big for GitHub, so I put them on a server where you can download them. In the GitHub repository (the Chapter 8 folder), you will find all the information. If you don’t have access to GitHub and you still want to download the files, here are the links:\n\n\n\ndataframe_Kather_texture_201_image_tiles_5000. pkl (340MB unzipped): http://toe.lt/j\n\n\n\nx_test.pkl (270MB unzipped): http://toe.lt/k\n\n\n\nx_train.pkl (810MB unzipped): http://toe.lt/m\n\n\n\nx_val.pkl (270MB unzipped): http://toe.lt/n\n\n\n\ny_train, y_test, and y_val (all zipped together) (about 50KB unzipped): http://toe.lt/p\n\nModel Building\n\nIt is time to build some models. You will find all the code in the book’s GitHub repository (Chapter 8 folder, in the 02_Model_building.ipynb notebook), so we will not look at all the details here. The best way to follow along is to keep the notebook open and try the code while you are reading this. As mentioned, we first need to load the pickle files. We can do that with the following code:\n\n253\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nx_train=pickle.load(open(base_dir+'x_train.pkl', 'rb')) x_test=pickle.load(open(base_dir+'x_test.pkl', 'rb')) x_val=pickle.load(open(base_dir+'x_val.pkl', 'rb')) y_train=pickle.load(open(base_dir+'y_train.pkl', 'rb')) y_test=pickle.load(open(base_dir+'y_test.pkl', 'rb')) y_val=pickle.load(open(base_dir+'y_val.pkl', 'rb'))\n\nThen we need to define the input_shape variable that we will need for\n\nour CNNs. In the code we always define functions that return the Keras models. For example, our first try looks like this:\n\ndef model_cnn_v1():\n\n# must define the input shape in the first layer of the\n\nneural network\n\nmodel = tf.keras.models.Sequential() model.add(tf.keras.layers.Conv2D(32, 3, 3, input_\n\nshape=input_shape))\n\nmodel.add(tf.keras.layers.Activation('relu')) model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(tf.keras.layers.Conv2D(64, 3, 3)) model.add(tf.keras.layers.Activation('relu')) model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(64)) model.add(tf.keras.layers.Activation('relu')) model.add(tf.keras.layers.Dropout(0.5)) model.add(tf.keras.layers.Dense(8)) model.add(tf.keras.layers.Activation('sigmoid'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return model\n\n254\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nThis is a simple network, as you can check with the summary() function: _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 50, 50, 32) 896 _________________________________________________________________ activation (Activation) (None, 50, 50, 32) 0 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 25, 25, 32) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 8, 8, 64) 18496 _________________________________________________________________ activation_1 (Activation) (None, 8, 8, 64) 0 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64) 0 _________________________________________________________________ flatten (Flatten) (None, 1024) 0 _________________________________________________________________ dense (Dense) (None, 64) 65600 _________________________________________________________________ activation_2 (Activation) (None, 64) 0 _________________________________________________________________ dropout (Dropout) (None, 64) 0 _________________________________________________________________ dense_1 (Dense) (None, 8) 520 _________________________________________________________________ activation_3 (Activation) (None, 8) 0 ================================================================= Total params: 85,512 Trainable params: 85,512 Non-trainable params: 0 _________________________________________________________________\n\n255",
      "page_number": 258
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 266-273)",
      "start_page": 266,
      "end_page": 273,
      "detection_method": "topic_boundary",
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nTo make sure that the session is reset, we always use:\n\ntf.keras.backend.clear_session()\n\nThen we create an instance of the model, as follows:\n\nmodel_cnn_v1=model_cnn_v1()\n\nThen we also save the initial weights to make sure, if we do runs later,\n\nthat we start from these same weights:\n\ninitial_weights = model_cnn_v1.get_weights()\n\nThen we train the model with this:\n\nmodel_cnn_v1.set_weights(initial_weights)\n\n# define path to save the mnodel path_model=base_dir+'model_cnn_v1.weights.best.hdf5' shutil.rmtree(path_model, ignore_errors=True)\n\ncheckpointer = ModelCheckpoint(filepath=path_model, verbose = 1, save_best_only=True) EPOCHS=200 BATCH_SIZE=256\n\nhistory=model_cnn_v1.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_test, y_test), callbacks=[checkpointer])\n\n256\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nNote a few points:\n\nWe create a custom CallBack class ModelCheckpoint, which will save the weights of the network during training every time the loss functions diminishes.\n\nWe train the network with the fit() call and save its\n\noutput in a history variable, to be able to plot loss and metrics later.\n\nNote training such networks may be very slow if you do it on your laptop or desktop, depending on the hardware you have. i strongly suggest you do that on google Colab, since this will speed up your testing. all the notebooks in the book’s github repository have been tested on google Colab and can be opened in google Colab directly from github.\n\nOn Google Colab, training the previous network will take roughly three\n\nminutes. It will reach the following accuracies:\n\nAccuracy on the training dataset: 85%\n\nAccuracy on the validation dataset: 82.7%\n\nThese results are not bad, and we don’t have much overfitting (you can\n\nsee in Figure 8-6 how accuracy and loss change with the epochs).\n\nFigure 8-6. Accuracy and loss function for the first network described in the text\n\n257\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nLet’s move to a different model, which we will call v2. This one has a lot\n\nmore parameters than the previous one: _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 150, 150, 128) 9728 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 75, 75, 128) 0 _________________________________________________________________ dropout (Dropout) (None, 75, 75, 128) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 75, 75, 64) 73792 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 37, 37, 64) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 37, 37, 64) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 37, 37, 64) 36928 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 18, 18, 64) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 18, 18, 64) 0 _________________________________________________________________ flatten (Flatten) (None, 20736) 0 _________________________________________________________________ dense (Dense) (None, 256) 5308672 _________________________________________________________________ dense_1 (Dense) (None, 64) 16448 _________________________________________________________________ dense_2 (Dense) (None, 32) 2080 _________________________________________________________________ dense_3 (Dense) (None, 8) 264 ================================================================= Total params: 5,447,912 Trainable params: 5,447,912 Non-trainable params: 0 _________________________________________________________________\n\n258\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nAgain, you can find all the code in the GitHub repository. We will train\n\nit again, but this time, for time reasons, for 50 epochs and with a slightly smaller batch size of 64.\n\nEPOCHS=50 BATCH_SIZE=64\n\nhistory=model_cnn_v2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_test, y_test), callbacks=[checkpointer])\n\nOtherwise, everything remains the same. This time, due to the sheer number of parameters, you will notice that we get an evident overfitting. In fact, we get the following accuracies:\n\nAccuracy on the training dataset: 99.5%\n\nAccuracy on the validation dataset: 74%\n\nYou can clearly see the overfitting in Figure 8-7, looking at the plot of\n\nthe accuracies versus the number of epochs.\n\nFigure 8-7. Accuracies and loss functions versus the number of epochs for the v2 network\n\n259\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nWe need to work a bit more to get some more reasonable results. Now let’s use a network with fewer parameters (in particular, with fewer kernels):\n\n_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 150, 150, 16) 448 _________________________________________________________________ conv2d_1 (Conv2D) (None, 150, 150, 16) 2320 _________________________________________________________________ conv2d_2 (Conv2D) (None, 150, 150, 16) 2320 _________________________________________________________________ dropout (Dropout) (None, 150, 150, 16) 0 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 50, 50, 16) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 50, 50, 32) 4640 _________________________________________________________________ conv2d_4 (Conv2D) (None, 50, 50, 32) 9248 _________________________________________________________________ conv2d_5 (Conv2D) (None, 50, 50, 32) 9248 _________________________________________________________________ dropout_1 (Dropout) (None, 50, 50, 32) 0 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_6 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ conv2d_7 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ conv2d_8 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ dropout_2 (Dropout) (None, 16, 16, 64) 0 _________________________________________________________________\n\n260\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nmax_pooling2d_2 (MaxPooling2 (None, 5, 5, 64) 0 _________________________________________________________________ conv2d_9 (Conv2D) (None, 5, 5, 128) 73856 _________________________________________________________________ conv2d_10 (Conv2D) (None, 5, 5, 128) 147584 _________________________________________________________________ conv2d_11 (Conv2D) (None, 5, 5, 256) 295168 _________________________________________________________________ dropout_3 (Dropout) (None, 5, 5, 256) 0 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256) 0 _________________________________________________________________ global_max_pooling2d (Global (None, 256) 0 _________________________________________________________________ dense (Dense) (None, 8) 2056 ================================================================= Total params: 639,240 Trainable params: 639,240 Non-trainable params: 0\n\nWe will call this network v3. This time, the situation is not much better,\n\nas you can see in Figure 8-8.\n\nFigure 8-8. Accuracies and loss functions versus. the number of epochs for the v3 network.\n\n261\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nWhy don’t we use what we have learned so far? Let’s use transfer learning and see if we can use a pre-trained network. Let’s download the VGG16 network and retrain the last layers with our data. To do that, we need to use the following code (we will call this network vgg-v4):\n\ndef model_vgg16_v4():\n\n# load the VGG model vgg_conv = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape = input_shape)\n\n# freeze the layers except the last 4 layers for layer in vgg_conv.layers[:-4]: layer.trainable = False\n\n# Check the trainable status of the individual layers for layer in vgg_conv.layers: print(layer, layer.trainable)\n\n# create the model model = tf.keras.models.Sequential()\n\n# add the vgg convolutional base model model.add(vgg_conv)\n\n# add new layers model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(1024, activation='relu')) model.add(tf.keras.layers.Dropout(0.5)) model.add(tf.keras.layers.Dense(8, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nreturn model\n\n262\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nNote how we downloaded the pre-trained network (as we have seen in\n\nprevious chapters) with this code:\n\nvgg_conv = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape = input_shape)\n\nWe used the include_top=False parameter, since we want to remove the final dense layers and put our own in their place. We add a layer with 1024 neurons at the end:\n\nmodel.add(tf.keras.layers.Dense(1024, activation='relu'))\n\nThen we add an output layer with 8 as the softmax activation function\n\nfor classification:\n\nmodel.add(tf.keras.layers.Dense(8, activation='softmax'))\n\nThe summary() call will give you this overview:\n\n_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= vgg16 (Model) (None, 4, 4, 512) 14714688 _________________________________________________________________ flatten (Flatten) (None, 8192) 0 _________________________________________________________________ dense (Dense) (None, 1024) 8389632 _________________________________________________________________ dropout (Dropout) (None, 1024) 0 _________________________________________________________________ dense_1 (Dense) (None, 8) 8200 ================================================================= Total params: 23,112,520 Trainable params: 15,477,256 Non-trainable params: 7,635,264 _________________________________________________________________\n\n263",
      "page_number": 266
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 274-281)",
      "start_page": 274,
      "end_page": 281,
      "detection_method": "topic_boundary",
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nThe entire vgg16 network is condensed into one line (vgg16 (Model)).\n\nIn this network, we have 15’477’256 trainable parameters. Quite a few. In fact, training this network for 30 epochs will require around 11 minutes on Google Colab. You can see in Figure 8-9 how accuracy and loss change with the number of epochs.\n\nFigure 8-9. Accuracies and loss functions versus the number of epochs for the vgg-v4 network\n\nAs you can see, the situation is better, but we still get overfitting. It’s not\n\nas dramatic as before, but still quite noticeable. The only strategy we have to fight this is data augmentation. In the next sections, we’ll see how easy it is to do data augmentation in Keras and the effects it has.\n\nData Augmentation\n\nOne obvious strategy to fight overfitting (although one that is rarely doable in real life) is to get more training data. In our case here, this is not possible. The images given are the only ones available. But we can still do something in this case: data augmentation. What do we mean by that exactly? Typically, data augmentation consists of generating new images from existing ones by applying some kind of transformation to them and using them as additional training data.\n\n264\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nNote Data augmentation consists of generating new images from existing ones by applying some kind of transformation to them and using them as additional training data.\n\nThe most common transformations are as follows:\n\nShifting the image by a certain number of pixels\n\nhorizontally or vertically\n\nRotating the image\n\nChanging its brightness\n\nChanging the zoom\n\nChanging the contrast\n\nShearing the image4\n\nLet’s see how to do data augmentation in Keras, and let’s look\n\nat a few examples in our dataset. The function we need to use is ImageDataGenerator. To start, you need to import it from keras_ preprocessing.image:\n\nfrom keras_preprocessing.image import ImageDataGenerator\n\nNote that this function will not generate new images and save them\n\nto disk, but will create augmented image data for you just-in-time during the training in random fashion (later it will become clear how to use it). This will not require much additional memory, but will add\n\n4 In plane geometry, a shear mapping is a linear map that displaces each point in a fixed direction, by an amount proportional to its signed distance from the line that is parallel to that direction and goes through the origin. See https:// en.wikipedia.org/wiki/Shear_mapping.\n\n265\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nsome additional time during model training. The function can do lots of transformations and the best way to discover them all is to look at the official documentation at https://keras.io/preprocessing/image/. We will look at the most important ones with examples.\n\nHorizontal and Vertical Shifts\n\nTo shift images horizontally and vertically, you use the following code:\n\ndatagen = ImageDataGenerator(width_shift_range=.2, height_shift_range=.2, fill_mode='nearest')\n\n# fit parameters from data datagen.fit(x_train)\n\nThe result is shown in a few random images in Figure 8-10.\n\n266\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nFigure 8-10. The result of shifting images horizontally and vertically\n\nIf you check the images, you will notice how strange features appear at\n\nthe borders. Since we are shifting the image, we need to tell Keras how to fill the part of the image that remains empty. Consider Figure 8-11, where we shift an image horizontally. As you may notice, the part marked in the image with the A remains empty, and we can tell Keras how to fill that part using the fill_mode parameter.\n\n267\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nFigure 8-11. An example of shifting an image in the horizontal direction. The A marks the part of the resulting image that will remain empty.\n\nThe best way to understand the different possibilities for fill_mode is to consider a one-dimensional case. The explanation has been taken from the official documentation of the function. Let’s suppose we have a set of four pixels that will have some values that we indicate with a, b, c, and d. And let’s suppose we have boundaries that we need to fill. The parts that need to be filled are marked with o. Figure 8-12 shows a graphical explanation of the four possibilities: constant, nearest, reflect, and wrap.\n\nFigure 8-12. Possible values for the fill_mode parameter and a graphical explanation of the possibilities\n\n268\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nThe images in Figure 8-11 have been generated using the nearest fill\n\nmode. Although this transformation introduces artificial features, using those additional images for training increases the accuracy of the model and fights overfitting extremely effectively, as we will see later in the chapter. The most common method to fill the empty parts is nearest.\n\nFlipping Images Vertically\n\nTo flip images vertically, the following code can be used:\n\ndatagen = ImageDataGenerator(vertical_flip=True)\n\n# fit parameters from data datagen.fit(x_train)\n\nRandomly Rotating Images\n\nYou can randomly rotate images with this code:\n\ndatagen = ImageDataGenerator(rotation_range=40, fill_mode = 'constant')\n\n# fit parameters from data datagen.fit(x_train)\n\nAnd, as with the shifting transformation, you can choose different ways\n\nof filling the empty areas. You can see the effect of this code in Figure 8-13.\n\n269\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nFigure 8-13. The effect of rotating images in a random direction up to 40 degrees (the amount of rotation is chosen randomly up to 40 degrees). The parts of the images left empty by the rotation have been filled with a constant value.\n\nIn Figure 8-14, you can see the effect of the rotation when it’s filled with fill_mode = 'nearest'. Typically, this is the preferred way to fill the images to avoid giving black (or solid color) parts of images to the network.\n\n270\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nFigure 8-14. The effect of rotating images in a random direction up to 40 degrees. The parts of the images left empty by the rotation have been filled with the nearest mode.\n\n271",
      "page_number": 274
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 282-289)",
      "start_page": 282,
      "end_page": 289,
      "detection_method": "topic_boundary",
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nZooming in Images\n\nYou should now understand how these image transformations work. Zooming is as easy as the previous transformation:\n\ndatagen = ImageDataGenerator(zoom_range=0.2)\n\n# fit parameters from data datagen.fit(x_train)\n\nPutting All Together\n\nOne of the great things about Keras is that you don’t need to perform each transformation, one at a time. You can do everything in one shot. For example, consider this code:\n\ndatagen = ImageDataGenerator(rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\"nearest\")\n\nThis will enhance your dataset greatly, with several transformations\n\ndone at the same time:\n\nRotation\n\nShift\n\nShear\n\nZoom\n\nFlip\n\nLet’s put everything together and see how effective this technique is.\n\n272\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nVGG16 with Data Augmentation\n\nNow it’s time to train our vgg16 network with transfer learning and image augmentation. The only modification to the code we looked at before is in how we feed the data to train the model. Now we will need to use this code:\n\nhistory=model_vgg16_v4.fit_generator(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE), validation_data=(x_test,\n\ny_test),\n\nepochs=EPOCHS, callbacks=[checkpointer])\n\nInstead of the classical fit() call, we need to use fit_generator(). A small digression is necessary to explain the main differences between the two functions. Keras includes not two, but three functions that can be used to train a model:\n\n\n\nfit()\n\n\n\nfit_generator()\n\n\n\ntrain_on_batch()\n\nThe fit( ) Function\n\nUp to now, we used the fit() function when training our Keras models. The main implicit assumption when using this method is that the dataset that you feed to the model will fit completely in memory. We don’t need to move batches to and from memory. That is a pretty big assumption, especially if you are working on big datasets and your laptop or desktop doesn’t have a lot of memory available. Additionally, the assumption is that there is no need to do real-time data augmentation (as we want to do here).\n\n273\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nNote the fit() function is good for small datasets that can fit in your system memory and do not require real-time data augmentation.\n\nThe fit_generator( ) Function\n\nWhen the data does not fit in memory anymore, we need a smarter function that can help us deal with it. Note that the ImageDataGenerator we created before will generate, in a random fashion, batches that need to be fed to the model. The fit_generator() function assumes that there is a function that generates the data for it. When using fit_generator(), Keras follows this process:\n\n1. Keras calls the function that generates the batches.\n\nIn our code, that’s datagen.flow().\n\n2. This generator function returns a batch whose size is specified by the batch_size=BATCH_SIZE parameter.\n\n3. The fit.generator() function then performs backpropagation and updates the weights.\n\n4. This is repeated until we reach the number of\n\nepochs wanted.\n\nNote the fit_generator() function is meant to be used for bigger datasets that do not fit in memory and when you need to do data augmentation.\n\nNote that there is an important parameter that we have not used in our code: steps_per_epoch. The datagen.flow() function will generate a batch of images each time, but Keras needs to know how many such batches we want for each epoch, since the datagen.flow() can continue\n\n274\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nto generate as many batches as we want (remember that they are generated in a random fashion). We need to decide how many batches we want before declaring each epoch finished. You can decide with the steps_per_epoch parameter, but if you don’t specify it, Keras will use len(generator)5 as the number of steps.\n\nThe train_on_batch( ) Function\n\nIf you need to fine-tune your training, the train_on_batch() function is the one to use.\n\nNote the train_on_batch() function accepts a single batch of data, performs backpropagation, and then updates the model parameters.\n\nThe batch of data can be arbitrarily sized and can be theoretically in any format you need. You need this function when you need, for example, to perform custom data augmentation that cannot be done by the standard Keras functions.\n\nNote as they say—if you don’t know whether you need the train_on_batch() function, you probably don’t.\n\nYou can find more information from the official documentation at\n\nhttps://keras.io/models/sequential/.\n\n5 https://keras.io/models/sequential/\n\n275\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nTraining the Network\n\nWe can finally train our network and see how it performs. Training it for 50 epochs and with a batch size of 128 gives the following accuracies:\n\nAccuracy on the training dataset: 93.3%\n\nAccuracy on the validation dataset: 91%\n\nThat is a great result. Practically no overfitting and great accuracy. This\n\nnetwork took roughly 15 minutes on Google Colab, which is quite fast. Figure 8-15 shows the accuracies and loss versus the number of epochs.\n\nFigure 8-15. Accuracy and loss function versus the number of epochs for the VGG16 network with transfer learning and data augmentation\n\nTo summarize, we started with a simple CNN that was not too bad, but\n\nwe immediately realized that going deeper (more layers) and increasing the complexity (more kernels) led to overfitting quite dramatically. Adding dropout was not really helping, so the only solution was to use data augmentation.\n\nNote that we did not show the first networks described in this chapter with data augmentation for space reasons, but you should do that. If you try, you will realize that you fight overfitting quite efficiently, but the accuracy goes down. Using a pre-trained network gives us a very good starting point and allows us to go into the 90% accuracy regime in a few epochs.\n\n276\n\nChapter 8\n\nhistology tissue ClassifiCation\n\nAnd Now Have Fun…\n\nIn this book, you have learned powerful techniques that will allow you to read research papers, understand them, and start implementing more advanced networks that go beyond the easy CNNs that you find in blogs and websites. I hope you enjoyed the book and that it will help you in your journey toward deep learning mastery. Deep learning is really fun and an incredibly creative research field. I hope you now have a glimpse of the possibilities of the algorithms and the creativity involved. I love feedback and would love to hear from you. Don’t hesitate to get in touch and tell me how (and especially if) this book has helped you learn those algorithms.\n\n—Umberto Michelucci, Dübendorf, June 2019\n\n277\n\nIndex\n\nA Adam optimizer, 33, 162 Anaconda\n\nbenefits/drawbacks, 17–18 download, 9 install, 11–14 screen, 10\n\nB BBox-Label-Tool, 240 Building blocks, CNN convolutional\n\nlayer, 105–107\n\npooling layer, 108 stacking layers, 108, 109\n\nC call() function, 186 Chessboard image\n\nblurring kernel IB, 95, 96 creation, 91 horizontal edges, 93 kernel, IH, 92, 93 kernel, IL, 94, 95 kernel IV, 94, 97 transition, values, 98\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5\n\nClassification loss, 229 Confidence loss, 229 Content loss function, 179 Convolution\n\ndefinition, 85 example, chessboard (see\n\nChessboard image)\n\nkernel IH, 86, 87 kernels, 82 matrix, 82, 83 matrix 3 × 3, 88, 90 multiple channels, 125–128 Python, 90 size, 85 stride, 85, 87 tensors, 81, 82 visual explanation, 86 works, 86\n\nConvolutional neural network (CNN)\n\nbuilding blocks (see Building\n\nblocks, CNN)\n\nvisualization (see Visualization\n\nof CNN)\n\nweights\n\nconvolutional layer, 109 dense layer, 110 pooling layer, 110\n\n279\n\nCost function\n\nDataset abstraction, 68–71\n\ncross-entropy (see\n\nCross-entropy)\n\nmathematical notation, 165, 166 MSE (see Mean Square Error (MSE))\n\niterator, 71 MNIST dataset, simple batching, 73, 74, 76\n\nsimple batching, 72 tf.data.Dataset, eager execution\n\nCross-entropy\n\nbinary classification\n\nproblem, 173–175\n\ndistributions of probabilities, 171 probability mass\n\nmode, 76, 77 Dataset.map() function, 69 Deep learning models, 141, 176 Digression, 139–141 Docker image, 18\n\nfunctions, 172, 173\n\nself-information, 169, 170 suprisal associated with\n\nEvent X, 171\n\nsuprisal of an event, 169, 170\n\nE Eager execution, TensorFlow\n\nadvantages, 28 enabling, 29, 30 MNIST dataset, 34\n\nD Data analysis, 244–253 Data augmentation\n\nflip images vertically, 269 horizontal and vertical shifts, 266–269\n\nrandomly rotate\n\nimages, 269–271\n\nVGG16\n\naccuracy and loss function vs. number of epochs, 276\n\nfit() function, 273, 274 fit_generator()\n\nfeed-forward neural\n\nnetwork, 36\n\nimplementation, 34 keras.datasets.mnist\n\npackage, 35 learning rate, 38 loss_value, 37 nested loops, 36 plot image, 38 tf.data.Dataset object, 35\n\npolynomial fitting, 30 adam optimizer, 33 loss function vs. iteration\n\nfunction, 274, 275\n\nnumber, 32, 33\n\ntrain_on_batch() function, 275 zooming in images, 272\n\nminimize loss function, 32 MSE, 31 numpy arrays, 31\n\n280",
      "page_number": 282
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 290-294)",
      "start_page": 290,
      "end_page": 294,
      "detection_method": "topic_boundary",
      "content": "F Fast R-CNNs, 217–220 Filters, 79 fit() function, 273, 274 fit_generator() function, 274, 275\n\nG get_dummies() pandas\n\nfunction, 251\n\nget_next() method, 71 GitHub repository, 3 Google Colab, 5, 8, 184, 206 Google drive, 184, 206 GoogLeNet network, 135 Gramian matrices, 179\n\nH Hardware acceleration, TensorFlow\n\nchecking availability of\n\nGPU, 40, 41 device name, 41, 42 effect on MNIST, 45, 47 explicit device placement, 42 matrix multiplication, 43, 44\n\nI, J ImageDataGenerator, 265 Image filters, 177 Imagenet dataset, 177 imread() function, 246 Inception module\n\nclassical CNNs, 130\n\nIndex\n\ncomputational budget, 129 convolutional layers, 130 dimension reduction, 133, 134 functional APIs, Keras, 136–138 max-pooling operations, 132 MNIST dataset, 132 Naïve version, 131 number of parameters, 132\n\nInception networks, 123 input_shape variable, 254 Instance segmentation, 198 Intersect Over Union (IoU), 200–202\n\nit_init_op operation, 74\n\nK Keras, 116 keras.backend.function() CNTK backend, 116 computational graph, 115 dataset, 117 function(), 116 TensorFlow backend, 116 Theano backend, 116 Keras callback functions, 54 custom class, 55, 56 CC1 variable, 58 logs dictionary, 58–60 log variable, 58 on_epoch_end() function, 60 output, 61 Sequential model, 57\n\nKernels/filters, 79–81 effects, 118–121\n\n281\n\nL LabelEncoder, 151 labelImg tool, 240 LeNet-5 network, 109 Linear regression, 164–165 load_img() function, 184, 185 Localization loss, 229 Loss function, 162, 188\n\nmnist_model.variables, 37 Model, building\n\naccuracies and loss functions vs. number of epochs\n\nv2 network, 259 v3 network, 261 vgg-v4 network, 264 accuracy and loss function\n\nclassification loss, 229 confidence loss, 229, 230 localization loss, 229 total, 230 value, 206\n\nM make_one_shot_iterator(), 71, 72 .map() function, 246 Masking, 192–193 Mathematical notation, 165–166 max_dim variable, 185 Mean Square Error (MSE), 31 intuitive explanation, 167 moment-generating\n\nv1 network, 257 GitHub repository, 259 fit() call, 257 include_top=False parameter, 263 input_shape variable, 254 Keras models, 254 ModelCheckpoint, 257 model_cnn_v1=model_cnn_\n\nv1(), 256\n\noverfitting, 259 softmax activation function, classification, 263 summary() function, 255, 263 tf.keras.backend.clear_ session(), 256\n\nfunction, 167–169 Microsoft COCO dataset, 199, 200 MNIST dataset, 126, 204\n\nMultiple cost functions, 134, 135 Multi-task learning (MTL), 141\n\nconvolutional layers, 112 fit() method, 114 import packages, 110 Keras model, 111, 112 load data, 112 load images, 111 model.summary(), 113\n\nN Naïve approach, object\n\nlocalization, 202–204\n\nNeural network models\n\ncomponents, 161, 162 linear regression, 164, 165\n\n282\n\noptimization problem,\n\ntraining, 162, 163\n\noptimizer, 162\n\nNeural style transfer (NST) deep learning, 176 digital images, 176 in Keras, 183–190 masking, 192, 193 mathematics, 178–183 pre-trained network, 177 robust CNN, 177 Silhouettes, 190–192 Van Gogh painting, 177, 178\n\nnext_batch operation, 74 Non-central moments, 169 Non-maxima suppression, 228 numpy() method, 39\n\nO Object classification and\n\nlocalization, 197, 200, 201, 211, 213\n\nObject detection, 198, 200, 216,\n\n219, 221, 226, 231, 233–239\n\nObject localization\n\ndetection, 197, 233–239 instance segmentation, 198 IOU, 200–202 location of an object, 197 Microsoft COCO dataset, 199, 200 Naïve Approach, 202–204 Pascal VOC dataset, 200 self-driving car, 196\n\nIndex\n\nsemantic segmentation, 198 sliding window approach (see Sliding window approach)\n\non_epoch_end(self, epoch, logs) method, 55, 56, 60, 65, 66\n\nOptimizer, 2, 32, 33, 36, 37, 47, 68,\n\n162, 166 OutOfRangeError, 75\n\nP, Q Pascal VOC dataset, 200 Pickling, 247 Pooling\n\nconvolution, 102 max pooling, 100 padding, 104, 105 stride, 101 visualization, 102 Pre-trained CNNs, 176 Pre-trained networks, 276\n\napplications, 141 decode_predictions() function, 144 GitHub repository, 143 imagenet dataset, 143 Jupyter Notebook, 143 keras.applications package, 142\n\nProbability mass function (PMF), 170–173, 175 Python development environment\n\nAnaconda\n\nbenefits, 17 drawbacks, 18\n\n283\n\nPython development\n\nSliding window approach, 203\n\nenvironment (cont.) installation, 9, 10 installing TensorFlow, 11–13 Jupyter Notebook, 14, 16 Python packages, 10, 11\n\nbounding box, 209 object localization, solving, 203 problems and\n\nlimitations, 204–211 size and proportions, window\n\nDocker image benefits, 24 correct-v option, 23 drawbacks, 24 install, 18–20 Jupyter instance, 21 pyhton, 22 Google Colab benefits, 8 create notebook, 6 definition, 5 drawbacks, 9 popup, 7 possibilities, 4\n\nchange, 209\n\nsoftmax activation functions, 174 StyleContentModel class, 186, 188\n\nT, U tape.gradient function, 180 TensorFlow, 13\n\ndataset abstraction (see Dataset\n\nabstraction)\n\neager execution (see Eager\n\nexecution, TensorFlow)\n\nhardware acceleration (see\n\nR Region-based CNN\n\nHardware acceleration, TensorFlow) numpy compatibility, 39 removing layers, 52–54 save model\n\n(R-CNN), 213–217\n\nS Segmentation, 198, 214 Selective search\n\nalgorithm, 215, 219\n\nSemantic segmentation, 198 Shannon entropy, 171 Silhouettes, 190–192\n\ncallback function, 63, 65 checkpoint_path, 65 Dense layer, 62 entire model, 68 latest variable, 67 MNIST dataset, 61, 62 validation dataset, 64 weights manually, 67, 68\n\ntraining specific layers, 47\n\nfeed-forward network, 48\n\n284\n\nfrozen_model.summary(), 50 layer.trainable property, 51 model.summary()\n\nfunction, 49 parameters, 49 trainable_model. summary(), 51\n\nTensorFlow Variable, 180 Tensor Processing Unit\n\n(TPU), 8, 189 tf.GradientTape, 189 tf.train.AdamOptimizer\n\nTensorFlow, 38 to_pickle() pandas, 247 Total loss function, 135, 230 train_on_batch() function, 275 Transfer learning\n\nbase network, 146 classical approach\n\nbinary classification,\n\n155–157\n\nCNN model, 150 dense layer, 155 LabelEncoder, 151 model, 151, 153 pooling layer, 155 training_data, 150 validation_data, 150 VGG16 pre-trained\n\nnetwork, 155\n\ndefinition, 145 dog vs. cat dataset, 149\n\nIndex\n\nexperimentation\n\ndataset preparation, 158, 159 flexible, 157 frozen layers, 157 target subnetwork, 157, 159 validation dataset, 160\n\nfeatures, 147 image recognition problems, 147 schematic representation, 148 target dataset, 146\n\nV, W, X Visualization of CNN\n\nkeras.backend.function(), 115,\n\n117, 118\n\nkernels effect, 118, 119, 121 max pooling effect, 121, 123\n\nY, Z You Only Look Once (YOLO)\n\nmethod\n\ndarknet detection, 233–237 darknet implementation, 231, 232 detection variable, 237 division of image, 223, 224 model output, 224 non-maxima suppression, 238 OpenCV, 239 versions, 222 YOLOv3, 227\n\n285",
      "page_number": 290
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "Advanced Applied Deep Learning\n\nConvolutional Neural Networks and Object Detection — Umberto Michelucci",
      "content_length": 103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 2,
      "content": "Advanced Applied Deep Learning\n\nConvolutional Neural Networks and Object Detection\n\nUmberto Michelucci",
      "content_length": 102,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Advanced Applied Deep Learning: Convolutional Neural Networks and Object Detection\n\nUmberto Michelucci TOELT LLC, Dübendorf, Switzerland\n\nISBN-13 (pbk): 978-1-4842-4975-8 https://doi.org/10.1007/978-1-4842-4976-5\n\nISBN-13 (electronic): 978-1-4842-4976-5\n\nCopyright © 2019 by Umberto Michelucci\n\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.\n\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every occurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion and to the benefit of the trademark owner, with no intention of infringement of the trademark.\n\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.\n\nWhile the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.\n\nManaging Director, Apress Media LLC: Welmoed Spahr Acquisitions Editor: Celestin Suresh John Development Editor: Matthew Moodie Coordinating Editor: Aditee Mirashi\n\nCover designed by eStudioCalamar\n\nCover image designed by Freepik (www.freepik.com)\n\nDistributed to the book trade worldwide by Springer Science+Business Media New York, 233 Spring Street, 6th Floor, New York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail orders-ny@springer-sbm.com, or visit www.springeronline.com. Apress Media, LLC is a California LLC and the sole member (owner) is Springer Science + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware corporation.\n\nFor information on translations, please e-mail rights@apress.com, or visit http://www.apress. com/rights-permissions.\n\nApress titles may be purchased in bulk for academic, corporate, or promotional use. eBook versions and licenses are also available for most titles. For more information, reference our Print and eBook Bulk Sales web page at http://www.apress.com/bulk-sales.\n\nAny source code or other supplementary material referenced by the author in this book is available to readers on GitHub via the book’s product page, located at www.apress.com/978-1-4842-4975-8. For more detailed information, please visit http://www.apress.com/source-code.\n\nPrinted on acid-free paper",
      "content_length": 3017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "This book is dedicated to my wife Francesca and daughter Caterina, who always show me how important it is to have dreams and to follow them.",
      "content_length": 140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Table of Contents\n\nAbout the Author ���������������������������������������������������������������������������������xi\n\nAbout the Technical Reviewer �����������������������������������������������������������xiii\n\nAcknowledgments ������������������������������������������������������������������������������xv\n\nIntroduction ��������������������������������������������������������������������������������������xvii\n\nChapter 1: Introduction and Development Environment Setup�������������1\n\nGitHub Repository and Companion Website ����������������������������������������������������������3\n\nMathematical Level Required �������������������������������������������������������������������������������3\n\nPython Development Environment ������������������������������������������������������������������������4\n\nGoogle Colab ����������������������������������������������������������������������������������������������������5\n\nAnaconda ���������������������������������������������������������������������������������������������������������9\n\nDocker Image ������������������������������������������������������������������������������������������������18\n\nWhich Option Should You Choose?����������������������������������������������������������������������25\n\nChapter 2: TensorFlow: Advanced Topics �������������������������������������������27\n\nTensorflow Eager Execution ��������������������������������������������������������������������������������28\n\nEnabling Eager Execution ������������������������������������������������������������������������������29\n\nPolynomial Fitting with Eager Execution �������������������������������������������������������30\n\nMNIST Classification with Eager Execution ���������������������������������������������������34\n\nTensorFlow and Numpy Compatibility �����������������������������������������������������������������39\n\nHardware Acceleration ����������������������������������������������������������������������������������������40\n\nChecking the Availability of the GPU ��������������������������������������������������������������40\n\nDevice Names ������������������������������������������������������������������������������������������������41\n\nv",
      "content_length": 2194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Table of ConTenTs Table of ConTenTs\n\nExplicit Device Placement �����������������������������������������������������������������������������42\n\nGPU Acceleration Demonstration: Matrix Multiplication ��������������������������������43\n\nEffect of GPU Acceleration on the MNIST Example ����������������������������������������45\n\nTraining Only Specific Layers ������������������������������������������������������������������������������47\n\nTraining Only Specific Layers: An Example ����������������������������������������������������48\n\nRemoving Layers �������������������������������������������������������������������������������������������52\n\nKeras Callback Functions ������������������������������������������������������������������������������������54\n\nCustom Callback Class ����������������������������������������������������������������������������������55\n\nExample of a Custom Callback Class ������������������������������������������������������������57\n\nSave and Load Models ����������������������������������������������������������������������������������������61\n\nSave Your Weights Manually ��������������������������������������������������������������������������67\n\nSaving the Entire Model ��������������������������������������������������������������������������������68\n\nDataset Abstraction ���������������������������������������������������������������������������������������������68\n\nIterating Over a Dataset ���������������������������������������������������������������������������������71\n\nSimple Batching ��������������������������������������������������������������������������������������������72\n\nSimple Batching with the MNIST Dataset ������������������������������������������������������73\n\nUsing tf�data�Dataset in Eager Execution Mode ���������������������������������������������76\n\nConclusions ���������������������������������������������������������������������������������������������������������77\n\nChapter 3: Fundamentals of Convolutional Neural Networks �������������79\n\nKernels and Filters ����������������������������������������������������������������������������������������������79\n\nConvolution ���������������������������������������������������������������������������������������������������������81\n\nExamples of Convolution �������������������������������������������������������������������������������������91\n\nPooling ����������������������������������������������������������������������������������������������������������������99\n\nPadding �������������������������������������������������������������������������������������������������������104\n\nBuilding Blocks of a CNN ����������������������������������������������������������������������������������105\n\nConvolutional Layers �����������������������������������������������������������������������������������105\n\nPooling Layers ���������������������������������������������������������������������������������������������108\n\nStacking Layers Together ����������������������������������������������������������������������������108\n\nvi",
      "content_length": 3053,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Table of ConTenTs Table of ConTenTs\n\nNumber of Weights in a CNN �����������������������������������������������������������������������������109\n\nConvolutional Layer �������������������������������������������������������������������������������������109\n\nPooling Layer �����������������������������������������������������������������������������������������������110\n\nDense Layer �������������������������������������������������������������������������������������������������110\n\nExample of a CNN: MNIST Dataset ��������������������������������������������������������������������110\n\nVisualization of CNN Learning ���������������������������������������������������������������������������115\n\nBrief Digression: keras�backend�function( ) �������������������������������������������������115\n\nEffect of Kernels ������������������������������������������������������������������������������������������118\n\nEffect of Max-Pooling ����������������������������������������������������������������������������������121\n\nChapter 4: Advanced CNNs and Transfer Learning ���������������������������125\n\nConvolution with Multiple Channels ������������������������������������������������������������������125\n\nHistory and Basics of Inception Networks ��������������������������������������������������������129\n\nInception Module: Naïve Version �����������������������������������������������������������������131\n\nNumber of Parameters in the Naïve Inception Module ��������������������������������132\n\nInception Module with Dimension Reduction ����������������������������������������������133\n\nMultiple Cost Functions: GoogLeNet �����������������������������������������������������������������134\n\nExample of Inception Modules in Keras ������������������������������������������������������������136\n\nDigression: Custom Losses in Keras �����������������������������������������������������������������139\n\nHow To Use Pre-Trained Networks ��������������������������������������������������������������������141\n\nTransfer Learning: An Introduction ��������������������������������������������������������������������145\n\nA Dog and Cat Problem �������������������������������������������������������������������������������������149\n\nClassical Approach to Transfer Learning �����������������������������������������������������150\n\nExperimentation with Transfer Learning ������������������������������������������������������157\n\nChapter 5: Cost Functions and Style Transfer �����������������������������������161\n\nComponents of a Neural Network Model ����������������������������������������������������������161\n\nTraining Seen as an Optimization Problem ��������������������������������������������������162\n\nA Concrete Example: Linear Regression ������������������������������������������������������164\n\nvii",
      "content_length": 2804,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Table of ConTenTs Table of ConTenTs\n\nThe Cost Function ���������������������������������������������������������������������������������������������165\n\nMathematical Notation ��������������������������������������������������������������������������������165\n\nTypical Cost Functions ���������������������������������������������������������������������������������166\n\nNeural Style Transfer �����������������������������������������������������������������������������������������176\n\nThe Mathematics Behind NST ���������������������������������������������������������������������178\n\nAn Example of Style Transfer in Keras ���������������������������������������������������������183\n\nNST with Silhouettes �����������������������������������������������������������������������������������190\n\nMasking �������������������������������������������������������������������������������������������������������192\n\nChapter 6: Object Classification: An Introduction �����������������������������195\n\nWhat Is Object Localization? �����������������������������������������������������������������������������196\n\nMost Important Available Datasets ��������������������������������������������������������������199\n\nIntersect Over Union (IoU) ����������������������������������������������������������������������������200\n\nA Naïve Approach to Solving Object Localization (Sliding Window Approach) ��202\n\nProblems and Limitations the with Sliding Window Approach ��������������������204\n\nClassification and Localization ��������������������������������������������������������������������������211\n\nRegion-Based CNN (R-CNN) ������������������������������������������������������������������������������213\n\nFast R-CNN �������������������������������������������������������������������������������������������������������217\n\nFaster R-CNN ����������������������������������������������������������������������������������������������������219\n\nChapter 7: Object Localization: An Implementation in Python ���������221\n\nThe You Only Look Once (YOLO) Method �����������������������������������������������������������222\n\nHow YOLO Works �����������������������������������������������������������������������������������������223\n\nYOLOv2 (Also Known As YOLO9000)�������������������������������������������������������������226\n\nYOLOv3 ��������������������������������������������������������������������������������������������������������227\n\nNon-Maxima Suppression ���������������������������������������������������������������������������228\n\nLoss Function ����������������������������������������������������������������������������������������������228\n\nviii",
      "content_length": 2652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Table of ConTenTs Table of ConTenTs\n\nYOLO Implementation in Python and OpenCV ���������������������������������������������������231\n\nDarknet Implementation of YOLO �����������������������������������������������������������������231\n\nTesting Object Detection with Darknet ��������������������������������������������������������233\n\nTraining a Model for YOLO for Your Specific Images �����������������������������������������240\n\nConcluding Remarks �����������������������������������������������������������������������������������������241\n\nChapter 8: Histology Tissue Classification ���������������������������������������243\n\nData Analysis and Preparation ��������������������������������������������������������������������������244\n\nModel Building ��������������������������������������������������������������������������������������������������253\n\nData Augmentation �������������������������������������������������������������������������������������������264\n\nHorizontal and Vertical Shifts ����������������������������������������������������������������������266\n\nFlipping Images Vertically ����������������������������������������������������������������������������269\n\nRandomly Rotating Images ��������������������������������������������������������������������������269\n\nZooming in Images ��������������������������������������������������������������������������������������272\n\nPutting All Together��������������������������������������������������������������������������������������272\n\nVGG16 with Data Augmentation ������������������������������������������������������������������������273\n\nThe fit( ) Function �����������������������������������������������������������������������������������������273\n\nThe fit_generator( ) Function �����������������������������������������������������������������������274\n\nThe train_on_batch( ) Function ��������������������������������������������������������������������275\n\nTraining the Network �����������������������������������������������������������������������������������276\n\nAnd Now Have Fun… ����������������������������������������������������������������������������������������277\n\nIndex �������������������������������������������������������������������������������������������������279\n\nix",
      "content_length": 2270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "About the Author\n\nUmberto Michelucci studied physics and mathematics. He is an expert in numerical simulation, statistics, data science, and machine learning. Over the years, he has continuously expanded his expertise in post-graduate courses and research projects. In addition to several years of research experience at George Washington University (USA) and the University of Augsburg (DE), he has 15 years of practical experience in data warehouse, data science, and machine learning. He is currently responsible for Deep Learning, New Technologies, and Research Cooperation at Helsana Versicherung AG. In 2014, he completed a postgraduate certificate in professional studies in education in England to broaden his knowledge of teaching and pedagogy. He is the author of Applied Deep Learning: A Case-Based Approach to Understanding Deep Neural Networks, published by Springer in 2018. He regularly publishes his research results in leading journals and gives lectures at international conferences. He is also a founder of TOELT llc, a company focusing on research in AI in science.\n\nxi",
      "content_length": 1089,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "About the Technical Reviewer\n\nJojo Moolayil is an artificial intelligence professional. He has authored three books on machine learning, deep learning, and IoT. He is currently working with Amazon Web Services as a Research Scientist – A.I. in their Vancouver, BC office.\n\nHe was born and raised in Pune, India and\n\ngraduated from the University of Pune with a major in Information Technology Engineering. His passion for problem- solving and data- driven decision led him to start a career with Mu Sigma Inc., the world’s largest pure-play analytics provider. He was responsible for developing machine learning and decision science solutions to large complex problems for Healthcare and Telecom giants. He later worked with Flutura (an IoT analytics startup) and General Electric, with a focus on Industrial A.I in Bangalore, India.\n\nIn his current role with AWS, he works on researching and developing large-scale A.I. solutions for combating fraud and enriching the customer payment experience in the cloud. He is also actively involved as a tech reviewer and AI consultant with leading publishers and has reviewed over a dozen books on machine learning, deep learning, and business analytics.\n\nYou can reach out to Jojo at\n\n\n\nhttps://www.jojomoolayil.com/\n\n\n\nhttps://www.linkedin.com/in/jojo62000\n\n\n\nhttps://twitter.com/jojo62000\n\nxiii",
      "content_length": 1339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Acknowledgments\n\nWriting this second book on more advanced topics has been a challenge. Finding the right level—finding the right parts to discuss and the right ones to leave out—caused me a few sleepless nights. This would not have been possible without several people who gave me feedback and discussed the chapters with me. I need to thank my editors, from Aditee to Matt, and especially my technical editor Jojo, who read and tried all the code. What patience. The team at Apress has been great. Thanks go to Celestin John, the acquisitions editor, who believed in me. Thanks to everyone for everything; you are great.\n\nOf course, a big thank you goes to my family, who put up with me spending time at the computer writing, testing code, writing more, testing more, and so on and so forth. Thanks to my wife Francesca for her endless patience. I don’t know how she put up with me. I really don’t. To my daughter Caterina goes a special thank you. She shows me everyday how great it is to love something and do it just for fun, and how important it is not too take yourself too seriously.\n\nA special thanks to all the readers who invested part of their lives reading what I wrote. I am really flattered that you are doing that. I would love to hear from you, so get in touch. You can get in touch with me at umberto.michelucci@toelt.ai, or you use the GitHub repository to get in touch with me by opening an issue. Really, do that. I look forward to hearing from you.\n\nxv",
      "content_length": 1474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Introduction\n\nThis is the second book I have written, and it covers advanced topics in deep learning. It will require some knowledge to be understood. It’s not for beginners. If you are one, I suggest you check out my first book published by Apress (Applied Deep Learning: A Case-Based Approach, ISBN 978-1-4842-3790-8). To understand this book, you should have some intermediate to advanced experience in Python and some intermediate to advanced deep learning experience (and experience with neural networks in general). This book assumes you know things like regularization, hyperparameter tuning, mini-batch gradient descent, which optimizers are more efficient (does the name Adam tell you something?), and so on. I also use heavily Keras (from TensorFlow), so I suggest you get some experience with that too. It will help you work through the examples of the book.\n\nI tried to tackle advanced topics, like transfer learning or multi-loss function networks, with a practical approach. That means I explain the concepts and then show you how to implement those things in Keras. I invested quite some time in preparing code for you and it’s available on the GitHub repository for the book, so get the code and use it while you’re reading the book. The code for the advanced topics we deal with, is too long to discuss completely, so I only dissect the most important parts. In GitHub, you have all of it.\n\nThis book touches on several research fields, but was not written for very experienced researchers. It has been written for practitioners who want to start doing research; therefore, its goal is to bridge the gap between the beginner and the researcher. Very advanced topics, like in object detection, are not explained in much technical detail, since otherwise the book would turn into a collection of research papers.\n\nxvii",
      "content_length": 1833,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "InTroduCTIon InTroduCTIon\n\nKeep in mind that many of the things I describe in the book, like the YOLO object detection algorithm, are only a few years old. For advanced topics, the only way to understand an algorithm is to read the original paper. You should get used to doing that without any book. Here, I try to give you the tools and explain the language that you need to read the research papers. From there, you are on your own.\n\nIf you want to proceed further in your deep learning adventure, you\n\nshould get used to reading research papers. They are not easy to read and will require time. But this book should give you many tools and tips for a head start. Reading this book and understanding all of it will put you at the start of your research career. From there on, start reading research papers. Try to repeat what they did if possible (mostly is not, given the infrastructure needed for deep learning, but you can always try). Understanding algorithms and research papers will give you enough knowledge to evaluate libraries and see what others have done, if you are searching for a way to use a specific algorithm in your projects.\n\nI hope you enjoy the book, that you learn something from it, and that it\n\nhelps you, but more important than anything—I hope you have fun!\n\n—Umberto Michelucci, Zürich, 3rd of July 2019\n\nxviii",
      "content_length": 1340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "CHAPTER 1\n\nIntroduction and Development Environment Setup\n\nThis book assumes that you have some basic know-how in machine learning, neural networks, and TensorFlow.1 It follows my first book, Applied Deep Learning: A Case-Based Approach (ISBN 978-1-4842-3790-8), published by Apress in 2018, and assumes you know and understand what is explained in there. The first volume’s goal is to explain the basic concepts of neural networks and to give you a sound basis in deep learning, and this book’s goal is to explain more advanced topics, like convolutional and recurrent neural networks. To be able to profit from this book, you should have at least a basic knowledge of the following topics:\n\nHow a single neuron and its components work (activation functions, inputs, weights, and bias)\n\nHow to develop a simple neural network with several\n\nlayers in Python with TensorFlow or Keras\n\n1 TensorFlow, the TensorFlow logo, and any related marks are trademarks of Google Inc.\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_1\n\n1",
      "content_length": 1091,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nWhat an optimizer is and how it works (at least you\n\nshould know how gradient descent works)\n\nWhich advanced optimizers are available and how they\n\nwork (at least RMSProp, Momentum, and Adam)\n\nWhat regularization is and what the most common\n\nmethods are (ℓ1, ℓ2, and dropout)\n\nWhat hyperparameters are\n\nHow to train a network and which hyper-parameters\n\nplay an essential role (for example, the learning rate or the number of epochs)\n\nWhat hyperparameter tuning is and how to do it\n\nIn the next chapters, we switch freely between low-level TensorFlow\n\nAPIs and Keras (introduced in the next chapter) where needed, to be able to concentrate on the more advanced concepts and not on implementation details. We will not discuss why a specific optimizer works better or how neurons work. If any of that is unclear, you should keep my first book close and use it as a reference.\n\nAdditionally, not all the Python code in the book is discussed as extensively as in my first book. You should already understand Python code well. However, all the new concepts are explained. If you have a sound basis, you will understand very well what is going on (and why). This book is not for beginners of deep learning. If you are one, I suggest buying my first book and studying it before starting this one.\n\nI hope that the book will be enjoyable and that you will learn a lot from\n\nit. But most of all, I hope it will be fun.\n\n2",
      "content_length": 1471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nGitHub Repository and Companion Website\n\nThe Jupyter Notebooks related to the code I discuss in this book are found on GitHub.2 To find the link to them, go to the Apress web page for this book. Near the cover of the book, a button with the text “Download Code” can be found. It points to the GitHub repository. The notebooks contain specific topics discussed in the book, including exercises of additional material that did not fit in the book. It is even possible to leave feedback directly on GitHub using “Issues” (see https://goo.gl/294qg4 to learn how). It would be great to hear from you. The GitHub repository acts as a companion to the book, meaning it contains more code than is printed in the book. If you are a teacher, I hope you can use these notebooks for your students. The notebooks are the same ones I use in my university courses, and much work has gone into making them useful for teaching.\n\nThe best way to learn is to try. Don’t merely read the book: try, play\n\nwith the code, change it, and apply it to concrete problems.\n\nA companion website is also available, where news about\n\nthe book and additional useful material is found. Its URL is www.applieddeeplearningbook.com.\n\nMathematical Level Required\n\nThere are a few sections that are more mathematically advanced. You should understand most of these concepts without the mathematical details. However, it is essential to know what a matrix is, how to multiply matrices, what a transpose is, and so on. You basically need a sound grasp of linear algebra. If that is not the case, I suggest reviewing a linear algebra book before reading this book. A basic understanding of calculus is also\n\n2 In case you don’t know what GitHub is, you can learn the basics with this guide at https://guides.github.com/activities/hello-world/\n\n3",
      "content_length": 1863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nbeneficial. It is important not to skip the mathematical parts. They can help you understand why we do things in specific ways. You should also not be scared by more complex mathematical notations. The goal of this book is not to give you a mathematical foundation; I assume you have that already. Deep learning and neural networks (in general, machine learning) are complex and whoever tries to convince you otherwise is lying or doesn’t understand them.\n\nWe will not spend time justifying or deriving algorithms or equations.\n\nAdditionally, we will not discuss the applicability of specific equations. For example, we will not discuss the problem of differentiability of functions when we calculate derivatives. Just assume we can apply the formulas you find here. Many years of practical implementations have shown the deep learning community that those methods and equations work as expected. These kinds of advanced discussions would require a separate book.\n\nPython Development Environment\n\nIn this book, we work exclusively with TensorFlow and Keras from Google, and we develop our code exclusively with Jupyter Notebooks, so it is crucial to know how to deal with them. There are three main possibilities when working with the code in the book, and in general when working with Python and TensorFlow:\n\nUse Google Colab, a cloud-based Python development\n\nenvironment.\n\n\n\nInstall a Python development environment locally on a laptop or desktop.\n\nUse a Docker image provided by Google, with\n\nTensorFlow installed.\n\nLet’s look at the different options in order to decide which one is the\n\nbest for you.\n\n4",
      "content_length": 1668,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nGoogle Colab\n\nAs mentioned, Google Colab is a cloud-based environment. That means nothing has to be installed locally. A Google account and a web browser (preferably Google Chrome) are the only things you need. The URL of the service is https://colab.research.google.com/.\n\nJust log in with a Google account or create one if you don’t have one. You will then get a window where you can open existing notebooks, if\n\nyou have some already in the cloud, or create new ones. The window looks like Figure 1-1.\n\nFigure 1-1. The first screen you see when you log in to Google Colab. In this screenshot, the Recent tab is open. Sometimes the Recent tab is opened the first time you log in.\n\n5",
      "content_length": 743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nIn the lower right, you can see the NEW PYTHON 3 NOTEBOOK link (typically in blue). If you click on the small downward triangle, you have the option of creating a Python 2 notebook. In this book, we use Python 3 exclusively. If you click the link, you get an empty Jupyter Notebook, like the one shown in Figure 1-2.\n\nFigure 1-2. The empty Jupyter Notebook you see when you create a new notebook in Google Colab\n\nThe notebook works precisely like a locally installed Jupyter Notebook,\n\nwith the exception that keyboard shortcuts (referred to here as simply shortcuts) are not the same as the ones in a local installation. For example, pressing X to delete a cell does not work here (but works in a local installation). In case you are stuck, and you don’t find the shortcut you want, you can press Ctrl+Shift+P to get a popup where you can search through the shortcuts. Figure 1-3 shows this popup.\n\n6",
      "content_length": 960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nFigure 1-3. The popup to search keyboard shortcuts when pressing Ctrl+Shift+P. Note that you can type a command name to search for it. You don’t need to scroll through them.\n\nFor example, typing DELETE in the popup tells you that, to delete a cell, you need to type Ctrl+M and then D. An exceptional place to start learning what is possible in Google Colab is from this Google notebook:\n\nhttps://Colab.research.Google.com/notebooks/basic_features_\n\noverview.ipynb (https://goo.gl/h9Co1f).\n\n7",
      "content_length": 550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nNote Google Colab has a great feature: it allows you to use Gpu (Graphical processing unit) and tpu (tensor processing unit)3 hardware acceleration for your experimentation. I will explain what difference this makes and how to use this when the time comes, but it will not be necessary to try the code and examples in this book.\n\nBenefits and Drawbacks to Google Colab\n\nGoogle Colab is a great development environment, but it has positive and negative aspects. Here is an overview.\n\nPositives:\n\nYou don’t have to install anything on your laptop/\n\ndesktop.\n\nYou can use GPU and TPU acceleration without buying\n\nexpensive hardware.\n\n\n\nIt has excellent sharing possibilities.\n\nMultiple people can collaboratively edit the same\n\nnotebook at the same time. Like Google Docs, you can set collaborators both within the document (top right, left of the comments button) and within a cell (right of the cell).4\n\n3 In deep learning, most of the calculations are done between tensors (multi- dimensional arrays). GPUs and TPUs are chips that are highly optimized to perform such calculations (like matrix multiplications) between very big tensors (up to a million of elements). When developing networks, it is possible to let GPUs and TPUs perform such expensive calculation in Google Colab, speeding up the training of networks. 4 Google Colab documentation is found at https://goo.gl/bKNWy8\n\n8",
      "content_length": 1443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nNegatives:\n\nYou need to be online to use and work with it. If you want to study this book on a train while commuting, you may not be able to do so.\n\n\n\nIf you have sensitive data and you are not allowed to upload it to a cloud service, you cannot work with it.\n\nThis system is designed for research and\n\nexperimentation, so you should not use it as a substitute productive environment.\n\nAnaconda\n\nThe second way of using and testing the code in this book is to have a local installation of Python and TensorFlow on your laptop or desktop. The easiest way to do that is using Anaconda. Here I describe in quite some detail how to do that.\n\nTo set it up, first download and install Anaconda for your system (I used Anaconda on Windows 10, but the code is not dependent on it, so feel free to use a Mac or Linux version if you prefer). You can get the Anaconda from https://anaconda.org/.\n\nOn the right side of the web page (see Figure 1-4), you’ll find a\n\nDownload Anaconda link.\n\nFigure 1-4. On the top-right side of the Anaconda website, you’ll find a link to download the software\n\n9",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nJust follow the instructions to install it. When you start it after the\n\ninstallation, you should see the screen shown in Figure 1-5.\n\nFigure 1-5. The screen you see when you start Anaconda\n\nPython packages (like numpy) are updated regularly and very often. A new version of a package may make your code stop working. Functions are deprecated and removed and new ones are added. To solve this problem, in Anaconda you can create what is called an environment. That is a container that contains a specific Python version and specific versions of the packages you decide to install. This way, you can have a container for Python 2.7 and numpy 1.10 and another with Python 3.6 and numpy 1.13, for example. You may have to work with code that exists already, and that is based on Python 2.7, and therefore you need a container with the right Python version. However, at the same time, it may be that for your projects you need Python 3.6. With containers, you can do all this at the same time. Sometimes different packages conflict, so you must be careful,\n\n10",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nand you should avoid installing all the packages you find interesting in your environment, primarily if you use it for developing under a deadline. There’s nothing worse than discovering that your code is not working anymore, and you don’t know why.\n\nNote When you define an environment, try to install only the packages you need and pay attention when you update them to make sure that the upgrade does not break your code (remember that functions are deprecated, removed, added, or changed very often). Check the updates documentation before upgrading and do it only if you need the updated features.\n\nIn the first book of the series (https://goo.gl/ytiQ1k), I explained how to create an environment with the graphical interface, so you can check that to learn how, or you can read the following page on the Anaconda documentation to understand how to work with environments in detail:\n\nhttps://conda.io/docs/user-guide/tasks/manage-environments.html\n\nIn the next section, we will create an environment and install\n\nTensorFlow in one shot, with one command only.\n\nInstalling TensorFlow the Anaconda Way\n\nInstalling TensorFlow is not complicated and has gotten a lot easier in the last year since my last book. To start (we describe the procedure for Windows here), go into the Start menu in Windows and type Anaconda. You should see the Anaconda Prompt under Apps. (You should see something similar to what is shown in Figure 1-6.)\n\n11",
      "content_length": 1496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nFigure 1-6. If you type Anaconda in the Start menu search field in Windows 10 you should see at least two entries: the Anaconda Navigator and the Anaconda Prompt.\n\nStart the Anaconda Prompt (see Figure 1-7). A command-line interface should start. The difference between this and the simple cmd.exe command prompt is that, here, all the Anaconda commands are recognized without having to set up Windows environment variables.\n\nFigure 1-7. This is what you should see when you start the Anaconda Prompt. Note that the username will be different. You will not see “umber” (my username), but your username.\n\n12",
      "content_length": 665,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nThen just type the following commands:\n\nconda create -n tensorflow tensorflow conda activate tensorflow\n\nThe first line creates an environment called tensorflow with TensorFlow\n\nalready installed, and the second line activates the environment. Then you only need to install the following packages with this code:\n\nconda install Jupyter conda install matplotlib conda install scikit-learn\n\nNote that sometimes you may get some warnings simply by importing\n\nTensorFlow with this command:\n\nimport tensorflow as tf\n\nThe warnings are due, most probably, by an outdated hdf5 version. To solve this issue (if it happens to you), try to update it using this code (if you don’t get any warning you can skip this step):\n\nconda update hdf5\n\nYou should be all set up. If you have a compatible GPU graphic card installed locally, you can simply install the GPU version of TensorFlow by using this command:\n\nconda create -n tensorflow_gpuenv tensorflow-gpu\n\nThis will create an environment with the GPU version of TensorFlow\n\ninstalled. If you do this, remember to activate the environment and then install all the additional packages as we have done here, in this new environment. Note that to use a GPU, you need additional libraries installed on your system. You can find all the necessary information for the\n\n13",
      "content_length": 1361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\ndifferent operating systems (Windows, Mac, and Linux) at https://www. tensorflow.org/install/gpu. Note that the TensorFlow website suggests using a Docker image (discussed later in the chapter) if you’re using a GPU for hardware acceleration.\n\nLocal Jupyter Notebooks\n\nThe last step to be able to type code and let it run is to use a Jupyter Notebook from a local installation. The Jupyter Notebook can be described (according to the official website) as follows:\n\nThe Jupyter Notebook is an open source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. Uses include data cleaning and transformation, numerical simula- tion, statistical modeling, data visualization, machine learn- ing, and much more.\n\nIt is widely used in the machine learning community and is a good idea\n\nto learn how to use it. Check out the Jupyter project website at http:// Jupyter.org/. It is very instructive and includes many examples of what is possible.\n\nAll the code you find in this book has been developed and tested using\n\nJupyter Notebooks. I assume that you have some experience with this web-based development environment. If you need a refresher, I suggest you check out the documentation. You can find it on the Jupyter project website at this address: http://Jupyter.org/documentation.html. To start a notebook in your new environment, you must go back to Anaconda Navigator and click on the triangle to the right of your tensorflow environment (if you used a different name, you have to click on the triangle to the right of your new environment), as shown in Figure 1-8. Then click on the Open with Jupyter Notebook option.\n\n14",
      "content_length": 1760,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nFigure 1-8. To start a Jupyter Notebook in your new environment, click on the triangle to the right of the TensorFlow environment name and choose Open with Jupyter Notebook\n\nYour browser starts with a list of the folders in your user folder. (If you\n\nare using Windows, this is usually located in c:\\Users\\<YOUR USER NAME>, where you substitute <YOUR USER NAME> with your username.) From there, you should navigate to a folder where you want to save your notebook files. You can create a new one by clicking on the New button, as illustrated in Figure 1-9.\n\nFigure 1-9. To create a new notebook, click on the New button located on the top-right part of the page and choose Python 3\n\n15",
      "content_length": 744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nA new page that should look like the one in Figure 1-10 will open.\n\nFigure 1-10. An empty Jupyter Notebook as it appears immediately after creation\n\nFor example, you can type the following code in the first “cell” (the\n\nrectangular space where you can type).\n\na=1 b=2 print(a+b)\n\nTo evaluate the code press Shift+Enter and you should see the result (3)\n\nimmediately, as shown in Figure 1-11.\n\nFigure 1-11. After typing some code in the cell, pressing Shift+Enter evaluates the code in the cell\n\nThe result of a+b is 3 (as shown in Figure 1-11). A new empty cell is\n\nautomatically created after the result for you to type in.\n\nFor more information on how to add comments, equations, inline plots, and much more, I suggest you visit the Jupyter website and check out their documentation.\n\n16",
      "content_length": 848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nNote check the url of the page. For example, in my case, I have http:// localhost:8888/notebooks/Documents/Data%20Science/ Projects/Applied%20advanced%20deep%20learning%20 (book)/chapter%201/AADL%20-%20Chapter%201%20-%20 Introduction.ipynb. note that the url is merely a concatenation of the folders showing where the notebook is located, separated by forward slashes. a %20 character indicates a space. In this case, my notebook is in the Documents/Data Science/Projects/... folder. I often work with several notebooks at the same time and it’s useful to know where each notebook is located, in case you forget (as I often do).\n\nIn case you forget which folder your notebook is in, you can\n\nBenefits and Drawbacks to Anaconda\n\nLet’s take a look at the positive and the negative sides of Anaconda now.\n\nPositives:\n\nThe system does not require an active Internet\n\nconnection (except when installing), so you can work with it everywhere (on the train, for example).\n\n\n\nIf you are working on sensitive data that you cannot upload to a cloud service, this is the solution for you, since you can work with data locally.\n\nYou can keep close control over which packages you\n\ninstall and on which environment you create.\n\n17",
      "content_length": 1275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nNegatives:\n\n\n\nIt is quite annoying to get the TensorFlow GPU version to work (you need additional libraries for it to work) with this method. The TensorFlow website suggests using a Docker image (see the next section) for it.\n\n\n\nIt is complicated to share your work with other people directly. If sharing is essential, you should consider Google Colab.\n\n\n\nIf you are using a corporate laptop that must work behind a firewall or a proxy, it’s challenging to work with Jupyter Notebooks, since sometimes, the notebooks may need to connect to the Internet and, if you are behind a firewall, this may not be possible. Installing packages may also be complicated in this case.\n\nThe performance of your code depends on the power\n\nand memory of your laptop or desktop. If you are using a slow or old machine, your code may be very slow. In this case, Google Colab may be the better option.\n\nDocker Image\n\nThe third option you have is to use a Docker image with TensorFlow installed. Docker (https://www.docker.com/) in a way is a bit like a virtual machine. However, unlike a virtual machine, rather than creating a whole virtual operating system, it merely adds components that are not present on the host machine.5 First, you need to download Docker for your system. A good starting point to learn about it and download it is at https://docs. docker.com/install/.\n\n5 https://opensource.com/resources/what-docker [Last accessed: 19/12/2018]\n\n18",
      "content_length": 1497,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nFirst, install Docker on your system. Once you have done so, you can\n\naccess all different types of TensorFlow versions by using the following command. You must type this command into a command-line interface (for example, cmd in Windows, Terminal on the Mac, or a shell under Linux):\n\ndocker pull TensorFlow/TensorFlow:<TAG>\n\nYou should substitute <TAG> with the right text (called a tag as you may\n\nimagine), like latest-py3, if you want the latest stable CPU-based build from Python 3.5. You can find an updated list of all tags at https://hub. docker.com/r/TensorFlow/TensorFlow/tags/. In this example, you would need to type:\n\ndocker pull tensorflow/tensorflow:latest-py3\n\nThis command downloads the right image automatically. Docker is efficient, and you can ask it to run the image immediately. If it does not find it locally, it downloads it. You can use the following command to start the image:\n\ndocker run -it -p 8888:8888 tensorflow/tensorflow:latest-py3\n\nIf you haven’t already downloaded it, this command downloads the\n\nlatest TensorFlow version based on Python 3 and starts it. You should see output like the following if everything goes well:\n\nC:\\Users\\umber>docker run -it -p 8888:8888 tensorflow/ tensorflow:latest-py3 Unable to find image 'TensorFlow/TensorFlow:latest-py3' locally latest-py3: Pulling from TensorFlow/TensorFlow 18d680d61657: Already exists 0addb6fece63: Already exists 78e58219b215: Already exists eb6959a66df2: Already exists 3b57572cd8ae: Pull complete\n\n19",
      "content_length": 1554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\n56ffb7bbb1f1: Pull complete 1766f64e236d: Pull complete 983abc49e91e: Pull complete a6f427d2463d: Pull complete 1d2078adb47a: Pull complete f644ce975673: Pull complete a4eaf7b16108: Pull complete 8f591b09babe: Pull complete Digest: sha256:1658b00f06cdf8316cd8a905391235dad4bf25a488f1ea98 9a98a9fe9ec0386e Status: Downloaded newer image for TensorFlow/TensorFlow:latest-py3 [I 08:53:35.084 NotebookApp] Writing notebook server cookie secret to /root/.local/share/Jupyter/runtime/notebook_cookie_ secret [I 08:53:35.112 NotebookApp] Serving notebooks from local directory: /notebooks [I 08:53:35.112 NotebookApp] The Jupyter Notebook is running at: [I 08:53:35.112 NotebookApp] http://(9a30b4f7646e or 127.0.0.1):8888/?token=f2ff836cccb1d688f4d9ad8c7ac3af80011f11ea 77edc425 [I 08:53:35.112 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 08:53:35.113 NotebookApp]\n\nCopy/paste this URL into your browser when you connect for\n\nthe first time, to login with a token:\n\nhttp://(9a30b4f7646e or 127.0.0.1):8888/?token=f2ff836c\n\nccb1d688f4d9ad8c7ac3af80011f11ea77edc425\n\nAt this point, you can simply connect to a Jupyter server running from\n\nthe Docker image.\n\n20",
      "content_length": 1273,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nAt the end of all previous messages, you’ll find the URL you should type in the browser to use Jupyter Notebooks. When you copy the URL, simply substitute cbc82bb4e78c or 127.0.0.1 with 127.0.0.1. Copy it into the URL field of your browser. The page should look like the one shown in Figure 1-12.\n\nFigure 1-12. The navigation window you see when using a Docker image Jupyter instance\n\nIt’s important to note that if you use the notebook out of the box, all\n\nfiles and notebooks that you create will disappear the next time you start the Docker image.\n\nIf you use the Jupyter notebook server as it is, and you create Note new notebooks and files, they will all disappear the next time you start the server. You need to mount a local directory that resides on your machine so that you can save your files locally and not in the image itself.\n\n21",
      "content_length": 902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nLet’s suppose you are using a Windows machine and that your notebooks reside locally at c:\\python. To see and use them while using Jupyter Notebooks from the Docker image, you need to start the Docker instance using the -v option in the following way:\n\ndocker run -it -v c:/python:/notebooks/python -p 8888:8888 TensorFlow/TensorFlow:latest-py3\n\nThis way, you can see all your files that are under c:\\python in a folder called python in the Docker image. You specify the local folder (where the files are local) and the Docker folder name (where you want to see the files while using Jupyter Notebooks from the Docker image) with the -v option:\n\nv <LOCAL FOLDER>:/notebooks/<DOCKER FOLDER>\n\nIn our example, <LOCAL FOLDER> is c:/python (the local folder you want to use for your locally saved notebooks) and <DOCKER FOLDER> is python (where you want Docker to mount the folder with your notebooks). Once you run the code, you should see output like the following:\n\n[I 09:23:49.182 NotebookApp] Writing notebook server cookie secret to /root/.local/share/Jupyter/runtime/notebook_cookie_ secret [I 09:23:49.203 NotebookApp] Serving notebooks from local directory: /notebooks [I 09:23:49.203 NotebookApp] The Jupyter Notebook is running at: [I 09:23:49.203 NotebookApp] http://(93d95a95358a or 127.0.0.1):8888/?token=d564b4b1e806c62560ef9e477bfad99245bf9670 52bebf68 [I 09:23:49.203 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 09:23:49.204 NotebookApp]\n\n22",
      "content_length": 1574,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nCopy/paste this URL into your browser when you connect for\n\nthe first time, to log in with a token:\n\nhttp://(93d95a95358a or 127.0.0.1):8888/?token=d564b4b1\n\ne806c62560ef9e477bfad99245bf967052bebf68\n\nNow, when you start your browser with the URL given at the end of the last message (where you must substitute 93d95a95358a or 127.0.0.1 with 127.0.0.1), you should see a Python folder named python, as shown in the one circled in Figure 1-13.\n\nFigure 1-13. The folder that you should see when starting the Docker image with the correct -v option. In the folder, you can now see all the files that are saved locally in the c:\\python folder.\n\nYou can now see all your locally saved notebooks, and if you save a notebook in the folder, you will find it again when you restart your Docker image.\n\n23",
      "content_length": 853,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nOn a final note, if you have a compatible GPU at your disposal,6 you can directly download the latest GPU TensorFlow version, for example, using the tag, latest-gpu. You can find more information at https://www.TensorFlow.org/install/gpu.\n\nBenefits and Drawbacks to a Docker Image\n\nLet’s take a look at the positive and the negative aspects of this option.\n\nPositives:\n\nYou don’t need to install anything locally, except\n\nDocker.\n\nThe installation process is straightforward.\n\nYou get the latest version of TensorFlow automatically.\n\n\n\nIt is the preferred option to choose if you want to use the GPU version of TensorFlow.\n\nNegatives:\n\nYou cannot develop with this method in several environments and with several versions of the packages.\n\n\n\nInstalling specific package versions is complicated.\n\nSharing notebooks is more complicated than with other\n\noptions.\n\nThe performance of your code is limited by the\n\nhardware on which you are running the Docker image.\n\n6 You can find a list of all compatible GPUs at https://developer.nvidia.com/ cuda-gpus and TensorFlow information at https://www.TensorFlow.org/ install/gpu.\n\n24",
      "content_length": 1183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nWhich Option Should You Choose?\n\nYou can quickly start with any of the options described and later continue with another one. Your code will continue to work. The only thing you need to be aware of is that, if you develop extensive amounts of code with GPU support and then try to run this on a system without GPU support, you may need to modify the code extensively. To decide which option is the best one for you, I provided the following questions and answers.\n\nDo you need to work on sensitive data?\n\nIf you need to work on sensitive data (for example, medical data) that you cannot upload on a cloud service, you should choose a local installation with Anaconda or Docker. You cannot use Google Colab.\n\nDo you often work in an environment without an\n\nInternet connection?\n\nIf you want to write code and train your models without an active Internet connection (for example, while commuting), you should choose a local installation of Anaconda or Docker, since Google Colab requires an active Internet connection.\n\nDo you need to work on the same notebook in\n\nparallel with other people?\n\nIf you want to share your work with others and work on it at the same time as others, the best solution is to use Google Colab, since it offers a great sharing experience, one that is missing from the local installation options.\n\n25",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Chapter 1\n\nIntroduCtIon and development envIronment Setup\n\nYou don’t want to (or can’t) install anything on your\n\nlaptop/desktop?\n\nIf you don’t want to or can’t install anything on your laptop or desktop (maybe it’s a corporate laptop ), you should use Google Colab. You only need an Internet connection and a browser. Keep in mind that some features work only with Google Chrome and not Internet Explorer.\n\nNote the easiest way to get up and running and start developing models with tensorFlow is probably to use Google Colab since it does not require any installation. directly go the website, log in, and start writing code. If you need to work locally, the docker option is probably the easiest solution. It is straightforward to get it up and running and you get to work with the latest version of tensorFlow. If you need the flexibility of many environments and precise control over which version of each package you’re using, your only solution is to perform a complete local installation of a python development environment, like anaconda.\n\n26",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "CHAPTER 2\n\nTensorFlow: Advanced Topics\n\nThe TensorFlow library has come a long way from its first appearance. Especially in the last year, many more features have become available that can make the life of researchers a lot easier. Things like eager execution and Keras allow scientists to test and experiment much faster and debug models in ways that were not possible before. It is essential for any researcher to know those methods and know when it makes sense to use them. In this chapter, we will look at few of them: eager execution, GPU acceleration, Keras, how to freeze parts of a network and train only specific parts (used very often, especially in transfer learning and image recognition), and finally how to save and restore models already trained. Those technical skills will be very useful, not only to study this book, but in real-life research projects.\n\nThe goal of this chapter is not to teach you how to use Keras from the ground up, or to teach you all the intricacies of the methods, but to show you some advanced techniques to solve some specific problems. Consider the different sections as hints. Remember that is always a good idea to study the official documentation, since methods and functions change very often. In this chapter, I will avoid copying the official documentation, and instead give you few advanced examples of techniques that are very useful and are used very often. To go deeper (pun intended), you should study the official TensorFlow documentation at https://www.tensorflow.org/.\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_2\n\n27",
      "content_length": 1648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nTo study and understand advanced topics, a good basis in Tensorflow and Keras is required. A very good resource to get up to speed with Keras is the book Learn Keras for Deep Neural Networks - A Fast-Track Approach to Modern Deep Learning with Python from Jojo John Moolayil (https:// goo.gl/mW4Ubg). If you don’t have much experience, I suggest you get this book and study it before starting this one.\n\nTensorflow Eager Execution\n\nTensorFlow’s eager execution is an imperative programming environment.1 That, loosely explained, means that the commands are evaluated immediately. That also means that a computational graph is built in the background without you noticing it. Operations return concrete values immediately instead of having first open a session, and then run it. This makes it very easy to start with TensorFlow, since it resembles classical Python programming. Eager execution provides the following advantages:\n\nEasier debugging: You can debug your models with\n\nclassical Python debugging tools for immediate checks\n\n\n\nIntuitive interface: You can structure your code naturally, as you would do in a classical Python program\n\nSupport for GPU acceleration is available\n\nTo be able to use this execution mode, you will need the latest version\n\nof TensorFlow. If you have not yet installed it, see Chapter 1 to learn how to do it.\n\n1 https://www.tensorflow.org/guide/eager (accessed 17th January, 2019)\n\n28",
      "content_length": 1460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nEnabling Eager Execution\n\nTo enable eager execution, you can use the following code:\n\nimport tensorflow as tf tf.enable_eager_execution()\n\nRemember that you need to do that right at the beginning, after the\n\nimports and before any other command. Otherwise, you will get an error message. If that is the case, you can simply restart the kernel of the notebook.\n\nFor example, you can easily add two tensors\n\nprint(tf.add(1, 2))\n\nand get immediately this result\n\ntf.Tensor(3, shape=(), dtype=int32)\n\nIf you don’t enable eager execution and try the print command again,\n\nyou will get this result\n\nTensor(\"Add:0\", shape=(), dtype=int32)\n\nSince TensorFlow has not yet evaluated the node. You would need the\n\nfollowing code to get the result:\n\nsess = tf.Session() print(sess.run(tf.add(1,2))) sess.close()\n\nThe result will be, of course, 3. This second version of the code creates\n\na graph, then opens a session, and then evaluates it. With eager you get the result immediately. You can easily check if you have enabled eager execution with this:\n\ntf.executing_eagerly()\n\n29",
      "content_length": 1107,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nIt should return True or False, depending on if you have enabled it or\n\nnot.\n\nPolynomial Fitting with Eager Execution\n\nLet’s check how eager execution works in a practical example.2\n\nKeep in mind you need the following imports:\n\nimport tensorflow as tf import numpy as np import matplotlib.pyplot as plt import tensorflow.contrib.eager as tfe tf.enable_eager_execution()\n\nLet’s generate some fake data for this function\n\ny\n\n=\n\nx\n\n3\n\n\n\n24 x\n\n\n\n2\n\nx\n\n+\n\n2\n\nwith the code\n\nx = np.arange(0, 5, 0.1) y = x**3 - 4*x**2 - 2*x + 2 y_noise = y + np.random.normal(0, 1.5, size=(len(x),))\n\nWe have created two numpy arrays: y, which contains the function evaluated over the array x, and y_noise, which contains y with some noise added. You can see how the data looks in Figure 2-1.\n\n2 You can find the notebook with the code in the book repository. To find it, go to the Apress book website and click on the Download Code button. The link points to the GitHub repository. The notebook is in the Chapter2 folder.\n\n30",
      "content_length": 1044,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nFigure 2-1. The plot shows the two numpy arrays y (ground truth) and y_noise (ground truth + noise)\n\nNow we need to define a model that we want to fit and define our loss function (the one we want to minimize with TensorFlow). Remember we are facing a regression problem, so we will use the Mean Squared Error (MSE) as our loss function. The functions we need are as follows:\n\nclass Model(object): def __init__(self): self.w = tfe.Variable(tf.random_normal([4])) # The 4\n\nparameters\n\ndef f(self, x): return self.w[0] * x ** 3 + self.w[1] * x ** 2 +\n\nself.w[2] * x + self.w[3]\n\nand\n\ndef loss(model, x, y): err = model.f(x) - y return tf.reduce_mean(tf.square(err))\n\n31",
      "content_length": 707,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nNow is easy to minimize the loss function. First let’s define some\n\nvariables we will need:\n\nmodel = Model() grad = tfe.implicit_gradients(loss) optimizer = tf.train.AdamOptimizer()\n\nThen let’s, with a for loop, minimize the loss function:\n\niters = 20000 for i in range(iters): optimizer.apply_gradients(grad(model, x, y)) if i % 1000 == 0: print(\"Iteration {}, loss: {}\".format(i+1, loss(model,\n\nx, y).numpy()))\n\nThis code will produce some outputs showing you the value for the loss function each 1,000 iterations. Note that we are feeding all the data in one batch to the optimizer (since we have only 50 data points, we don’t really need to use mini-batches).\n\nYou should see several output lines like this one:\n\nIteration 20000, loss: 0.004939439240843058\n\nThe loss function plot versus the number of the iterations can be seen\n\nin Figure 2-2 and is decreasing constantly, as expected.\n\n32",
      "content_length": 934,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nFigure 2-2. The loss function (MSE) vs. the iteration number is decreasing as expected. That shows clearly that the optimizer is doing a good job finding the best weights to minimize the loss function.\n\nIn Figure 2-3, you can see the function the optimizer was able to find,\n\nby minimizing the weights.\n\nFigure 2-3. The red dashed line is the function obtained by minimizing the loss function with the Adam optimizer. The method worked perfectly and found the right function efficiently.\n\n33",
      "content_length": 531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nWhat you should note is that we did not create a computational graph explicitly and then evaluate it in a session. We simply used the commands as we would with any Python code. For example, in the code\n\nfor i in range(iters): optimizer.apply_gradients(grad(model, x, y))\n\nwe simply call a TensorFlow operation in a loop without the need of a session. With eager execution, it’s easy to start using TensorFlow operations quickly without too much overhead.\n\nMNIST Classification with Eager Execution\n\nTo give another example of how you can build a model with eager execution, let’s build a classifier for the famous MNIST dataset. This is a dataset containing 60000 images of handwritten digits (from 0 to 9), each with a dimension of 28x28 in gray levels (each pixel has a value ranging from 0 to 255). If you have not seen the MNIST dataset, I suggest you check out the original website at https://goo.gl/yF0yH, where you will find all the information. We will implement the following steps:\n\nLoad the dataset.\n\nNormalize the features and one-hot encode the labels.\n\nConvert the data in a tf.data.Dataset object.\n\nBuild a Keras model with two layers, each with 1024\n\nneurons.\n\nDefine the optimizer and the loss function.\n\nMinimize the loss function using the gradients and the\n\noptimizer directly.\n\n34",
      "content_length": 1341,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nLet’s start. While following the code, note how we implement each piece as we would do with plain numpy, meaning without the need of creating a graph or opening a TensorFlow session.\n\nSo first let’s load the MNIST dataset using the keras.datasets.mnist\n\npackage, reshape it, and one-hot encode the labels.\n\nimport tensorflow as tf import tensorflow.keras as keras\n\nnum_classes = 10\n\nmnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimage_vector_size = 28*28 x_train = x_train.reshape(x_train.shape[0], image_vector_size) x_test = x_test.reshape(x_test.shape[0], image_vector_size)\n\ny_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes)\n\nThen let’s convert the arrays in a tf.data.Dataset object. In case you\n\ndon’t understand what this is, don’t worry, we will look at this more later in this chapter. For the moment, it suffices to know that it is a convenient way to use mini-batches while you train your network.\n\ndataset = tf.data.Dataset.from_tensor_slices( (tf.cast(x_train/255.0, tf.float32), tf.cast(y_train,tf.int64)))\n\ndataset = dataset.shuffle(60000).batch(64)\n\n35",
      "content_length": 1218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nNow let’s build the model using a feed-forward neural network with\n\ntwo layers, each with 1024 neurons:\n\nmnist_model = tf.keras.Sequential([ tf.keras.layers.Dense(1024, input_shape=(784,)), tf.keras.layers.Dense(1024), tf.keras.layers.Dense(10) ])\n\nUp to now we have not done anything particularly new, so you should\n\nbe able to follow what we did quite easily. The next step is to define the optimizer (we will use Adam) and the list that will contain the loss function history:\n\noptimizer = tf.train.AdamOptimizer() loss_history = []\n\nAt this point we can start with the actual training. We will have two\n\nnested loops—the first is for the epochs, the second for the batches.\n\nfor i in range(10): # Epochs print (\"\\nEpoch:\", i) for (batch, (images, labels)) in enumerate(dataset.\n\ntake(60000)):\n\nif batch % 100 == 0: print('.', end=\") with tf.GradientTape() as tape: logits = mnist_model(images, training=True) # Prediction\n\nof the model\n\nloss_value = tf.losses.sparse_softmax_cross_entropy(tf.\n\nargmax(labels, axis = 1), logits)\n\nloss_history.append(loss_value.numpy()) grads = tape.gradient(loss_value, mnist_model.variables)\n\n# Evaluation of gradients\n\n36",
      "content_length": 1200,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\noptimizer.apply_gradients(zip(grads, mnist_model.\n\nvariables),\n\nglobal_step=tf.train.get_or_\n\ncreate_global_step())\n\nThe part of the code that is probably new to you is the part that\n\ncontains these two lines:\n\ngrads = tape.gradient(loss_value, mnist_model.variables) optimizer.apply_gradients(zip(grads, mnist_model.variables), global_step=tf.train.get_or_\n\ncreate_global_step())\n\nThe first line calculates the gradients of the loss_value TensorFlow\n\noperation with respect to the mnist_model.variables (the weights basically), and the second line uses the gradients to let the optimizer update the weights. To understand how Keras evaluates gradients automatically, I suggest you check the official documentation at https:// goo.gl/s9Uqjc. Running the code will finally train the network. As the training progress, you should see output like this for each epoch:\n\nEpoch: 0 ..........\n\nNow to check the accuracy, you can simply run the following two lines\n\n(that should be self-explanatory):\n\nprobs = tf.nn.softmax(mnist_model(x_train)) print(tf.reduce_mean(tf.cast(tf.equal(tf.argmax(probs, axis=1), tf.argmax(y_train, axis = 1)), tf.float32)))\n\nThis will give you as a result a tensor that will contain the accuracy\n\nreached by the model:\n\ntf.Tensor(0.8980333, shape=(), dtype=float32)\n\n37",
      "content_length": 1332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nIn this example, we reached 89.8% accuracy, a relatively good result for such a simple network. Of course, you could try to train the model for more epochs or try to change the learning rate, for example. In case you are wondering where we defined the learning rate, we did not. When we define the optimizer as tf.train.AdamOptimizer, TensorFlow will use, if not specified differently, the standard value of 10−3. You can check this by looking at the documentation at https://goo.gl/pU7yrB.\n\nWe could check one prediction easily. Let’s get one image from our\n\ndataset:\n\nimage = x_train[4:5,:] label = y_train[4]\n\nIf we plot the image, we will see the number nine (see Figure 2-4).\n\nFigure 2-4. One image from the MNIST dataset. This happens to be a 9.\n\nWe can easily check what the model predicts:\n\nprint(tf.argmax(tf.nn.softmax(mnist_model(image)), axis = 1))\n\n38",
      "content_length": 904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nThis returns the following, as we expected:\n\ntf.Tensor([9], shape=(1,), dtype=int64)\n\nYou should note how we wrote the code. We did not create a graph explicitly, but we simply used functions and operations as we would have done with numpy. There is no need to think in graphs and sessions. This is how eager execution works.\n\nTensorFlow and Numpy Compatibility\n\nTensorFlow makes switching to and from numpy arrays very easy:\n\nTensorFlow converts numpy arrays to tensors\n\nNumpy converts tensors to numpy arrays\n\nConverting a tensor to a numpy array is very easy and is enough to invoke the .numpy() method. This operation is fast and cheap since the numpy array and the tensor share the memory, so no shifting around in memory is happening. Now this is not possible if you are using GPU hardware acceleration, since numpy arrays cannot be stored in GPU memory and tensors can. Converting will involve copying data from the GPU memory to the CPU memory. Simply something to keep in mind.\n\nNote typically, tensorFlow tensors and numpy arrays share the same memory. Converting one to another is a very cheap operation. But if you use GpU accelerations, tensors may be held in the GpU memory, and numpy arrays cannot, so copying data will be required. this may be more expensive in terms of running time.\n\n39",
      "content_length": 1344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nHardware Acceleration Checking the Availability of the GPU\n\nIt is worth it to show briefly how to use GPUs and what difference it may make, just to give you a feeling for it. If you have never seen it, it’s quite impressive. The easiest way to test GPU acceleration is to use Google Colab. Create a new notebook in Google Colab, activate GPU3 acceleration, and import TensorFlow as usual:\n\nimport tensorflow as tf\n\nThen we need to test if we have a GPU at our disposal. This can be\n\neasily done with this code:\n\nprint(tf.test.is_gpu_available())\n\nThis will return True or False depending on if a GPU is available. In a\n\nslightly more sophisticated way, it can be done in this way:\n\ndevice_name = tf.test.gpu_device_name() if device_name != '/device:GPU:0': raise SystemError('GPU device not found.') print('Found GPU at: {}'.format(device_name))\n\nIf you run the code, you may get this error:\n\nSystemErrorTraceback (most recent call last) <ipython-input-1-d1680108c58e> in <module>() 2 device_name = tf.test.gpu_device_name() 3 if device_name != '/device:GPU:0':\n\n3 You can find this article at https://goo.gl/hXKNnf to learn how to do it.\n\n40",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\n----> 4 raise SystemError('GPU device not found') 5 print('Found GPU at: {}'.format(device_name)) SystemError: GPU device not found\n\nThe reason is that you may have not yet configured the notebook (if\n\nyou are in Google Colab) to use a GPU. Or, if you are working on a laptop or desktop, you may have not installed the right TensorFlow version or you may not have a compatible GPU available.\n\nTo enable the GPU hardware acceleration in Google Colab, choose the Edit ➤ Notebook Settings menu option. You are then presented with a window where you can set up the hardware accelerator. By default, it is set to None. If you set it to GPU and run the previous code again, you should get this message:\n\nFound GPU at: /device:GPU:0\n\nDevice Names\n\nNote how the device name, in our case /device:GPU:0, encodes lots of information. This name ends with GPU:<NUMBER>, where <NUMBER> is an integer that can be as big as the number of GPUs you have at your disposal. You can get a list of all the devices you have at your disposal with this code:\n\nlocal_device_protos = device_lib.list_local_devices() print(local_device_protos)\n\nYou will get a list of all the devices. Each list entry will resemble this\n\none (this example refers to a GPU device):\n\nname: \"/device:XLA_GPU:0\" device_type: \"XLA_GPU\" memory_limit: 17179869184 locality { }\n\n41",
      "content_length": 1368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nincarnation: 16797530695469281809 physical_device_desc: \"device: XLA_GPU device\"\n\nWith a function like this one:\n\ndef get_available_gpus(): local_device_protos = device_lib.list_local_devices() return [x.name for x in local_device_protos if x.device_\n\ntype.endswith('GPU')]\n\nYou will get an easier-to-read result like this one4:\n\n['/device:XLA_GPU:0', '/device:GPU:0']\n\nExplicit Device Placement\n\nIt is very easy to place an operation on a specific device. That can be achieved using the tf.device context. For example, to place an operation on a CPU, you can use the following code:\n\nwith tf.device(\"/cpu:0\"): # SOME OPERATION\n\nOr to place an operation on a GPU, you can use the code:\n\nwith tf.device('/gpu:0'): # SOME OPERATION\n\nNote Unless explicitly declared, tensorFlow automatically decides on which device each operation must run. don’t assume that if you don’t specify the device explicitly that your code will run on a CpU.\n\n4 The result was obtained when calling the function in a Google Colab notebook.\n\n42",
      "content_length": 1057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nGPU Acceleration Demonstration: Matrix Multiplication\n\nIt is interesting to see what effect hardware acceleration may have. To learn more about using GPUs, it is instructive to read the official documentation, which can be found at https://www.TensorFlow.org/guide/using_gpu.\n\nStart with the following code5:\n\nconfig = tf.ConfigProto() config.gpu_options.allow_growth = True sess = tf.Session(config=config)\n\nThe second line is needed since TensorFlow starts to allocate a little\n\nGPU memory. As the session is started and the processes run, more GPU memory is then allocated as needed. Then a session is created. Let’s try to multiply two matrices of dimensions 10000x10000 filled with random values and see if using a GPU makes a difference. The following code will run the multiplication on a GPU:\n\n%%time with tf.device('/gpu:0'): tensor1 = tf.random_normal((10000, 10000)) tensor2 = tf.random_normal((10000, 10000)) prod = tf.linalg.matmul(tensor1, tensor2) prod_sum = tf.reduce_sum(prod)\n\nsess.run(prod_sum)\n\nAnd the following runs it on a CPU:\n\n%%time with tf.device('/cpu:0'):\n\n5 The code has been inspired by the Google code in the Google Colab documentation.\n\n43",
      "content_length": 1212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\ntensor1 = tf.random_normal((10000, 10000)) tensor2 = tf.random_normal((10000, 10000)) prod = tf.linalg.matmul(tensor1, tensor2) prod_sum = tf.reduce_sum(prod)\n\nsess.run(prod_sum)\n\nWhen I ran the code, I got 1.86 sec total time on a GPU and 1min 4sec on a CPU: a factor 32 times faster. You can imagine then, when doing such calculations over and over (as is often the case in deep learning), that you’ll get quite a performance boost in your evaluations. Using TPUs is slightly more complicated and goes beyond the scope of this book, so we will skip that.\n\nNote Using a GpU does not always give you a performance boost. when the tensors involved are small, you will not see a huge difference between using a GpU and a CpU. the real difference will become evident when the dimensions of the tensors start to grow.\n\nIf you try to run the same code on smaller tensors, for example 100x100, you will not see any difference at all between using a GPU and a CPU. The tensors are small enough that a CPU will get the result as fast as a GPU. For two 100x100 matrices, GPU and CPU both give a result in roughly 20ms. Typically, practitioners let CPUs do all the preprocessing (for example, normalization, loading of data, etc.) and then let GPUs perform all the big tensor operations during training.\n\nNote typically, you should evaluate only expensive tensor operations (like matrix multiplications or convolution) on GpUs and do all preprocessing (like data loading, cleaning, etc.) on a CpU.\n\n44",
      "content_length": 1531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nWe will see later in the book (where applicable) how to do that. But\n\ndon’t be afraid. You will be able to use the code and follow the examples without a GPU at your disposal.\n\nEffect of GPU Acceleration on the MNIST Example\n\nIt is instructive to see the effect of hardware acceleration on the MNIST example. To run the training of the model completely on the CPU we need to force TensorFlow to do it, since otherwise it will try to place expensive operations on a GPU when available. To do that, you can use this code:\n\nwith tf.device('/cpu:0'): for i in range(10): # Loop for the Epochs print (\"\\nEpoch:\", i) for (batch, (images, labels)) in enumerate(dataset.\n\ntake(60000)): # Loop for the mini-batches\n\nif batch % 100 == 0: print('.', end=\") with tf.GradientTape() as tape: logits = mnist_model(images, training=True) loss_value = tf.losses.sparse_softmax_cross_entropy(tf.\n\nargmax(labels, axis = 1), logits)\n\nloss_history.append(loss_value.numpy()) grads = tape.gradient(loss_value, mnist_model.\n\nvariables)\n\noptimizer.apply_gradients(zip(grads, mnist_model.\n\nvariables),\n\nglobal_step=tf.train.get_or_\n\ncreate_global_step())\n\n45",
      "content_length": 1173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nThis code, on Google Colab, runs in roughly 8 minutes and 41 seconds.\n\nIf we put all the possible operations on a GPU, using this code:\n\nfor i in range(10): # Loop for the Epochs print (\"\\nEpoch:\", i)\n\nfor (batch, (images, labels)) in enumerate(dataset.\n\ntake(60000)): # Loop for the mini-batches\n\nif batch % 100 == 0: print('.', end=\") labels = tf.cast(labels, dtype = tf.int64)\n\nwith tf.GradientTape() as tape:\n\nwith tf.device('/gpu:0'): logits = mnist_model(images, training=True)\n\nwith tf.device('/cpu:0'): tgmax = tf.argmax(labels, axis = 1, output_type=tf.\n\nint64)\n\nwith tf.device('/gpu:0'): loss_value = tf.losses.sparse_softmax_cross_\n\nentropy(tgmax, logits)\n\nloss_history.append(loss_value.numpy()) grads = tape.gradient(loss_value, mnist_model.\n\nvariables)\n\noptimizer.apply_gradients(zip(grads, mnist_model.\n\nvariables),\n\nglobal_step=tf.train.get_ or_create_global_step())\n\n46",
      "content_length": 926,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nIt will run in 1 minute and 24 seconds. The reason that the tf.\n\nargmax() has been placed on a CPU is that at the time of writing the GPU implementation of tf.argmax has a bug and does not work as intended.\n\nYou can clearly see the dramatic effect that GPU acceleration has, even\n\non a simple network like the one we used.\n\nTraining Only Specific Layers\n\nYou should now know that Keras works with layers. When you define one, let’s say a Dense layer, as follows:\n\nlayer1 = Dense(32)\n\nYou can pass a trainable argument (that is Boolean) to a layer\n\nconstructor. This will stop the optimizer to update its weights\n\nlayer1 = dense(32, trainable = False)\n\nBut this would not be very useful. What is needed is the possibility of changing this property after instantiation. This is easy to do. For example, you can use the following code\n\nlayer = Dense(32) # something useful happens here layer.trainable = False\n\nNote For the trainable property’s change to take effect, you need to call the compile() method on your model. otherwise, the change will not have any effect while using the fit() method.\n\n47",
      "content_length": 1138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nTraining Only Specific Layers: An Example\n\nTo understand better how this all works, let’s look at an example. Let’s again consider a feed-forward network with two layers:\n\nmodel = Sequential() model.add(Dense(32, activation='relu', input_dim=784, name = 'input')) model.add(Dense(32, activation='relu', name = 'hidden1'))\n\nNote how we created a model with two Dense layers with a name property. One is called input and the second is called hidden1. Now you can check the network structure with model.summary(). In this simple example, you will get the following output:\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== input (Dense) (None, 32) 25120 _______________________________________________________________ hidden1 (Dense) (None, 32) 1056 =============================================================== Total params: 26,176 Trainable params: 26,176 Non-trainable params: 0 _______________________________________________________________\n\nNote how all the parameters are trainable and how you can find the layer name in the first column. Please take note, since assigning each layer a name will be useful in the future. To freeze the layer called hidden1, you simply need to find the layer with the name and change its trainable property as follows:\n\nmodel.get_layer('hidden1').trainable = False\n\n48",
      "content_length": 1459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nNow, if you check the model summary again, you will see a different\n\nnumber of trainable parameters:\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== input (Dense) (None, 32) 25120 _______________________________________________________________ hidden1 (Dense) (None, 32) 1056 =============================================================== Total params: 26,176 Trainable params: 25,120 Non-trainable params: 1,056 _______________________________________________________________\n\nAs you can see, the 1056 parameters contained in the hidden1 layer are no longer trainable. The layer is now frozen. If you have not assigned names to the layers and you want to find out what the layers are called, you can use the model.summary() function or you can simply loop through the layers in the model with this:\n\nfor layer in model.layers: print (layer.name)\n\nThis code will give you the following output:\n\ninput hidden1\n\nNote that model.layers is simply a list with layers as elements. As such, you can use the classical way of accessing elements from a list. For example, to access the last layer, you can use:\n\nmodel.layers[-1]\n\n49",
      "content_length": 1278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nOr to access the first layer, use:\n\nmodel.layers[0]\n\nTo freeze the last layer, for example, you can simply use:\n\nmodel.layers[-1].trainable = False\n\nNote when you change a property of a layer in Keras, like the trainable property, remember to recompile the model with the compile() function. otherwise, the change will not take effect during the training.\n\nTo summarize, consider the following code6:\n\nx = Input(shape=(4,)) layer = Dense(8) layer.trainable = False y = layer(x) frozen_model = Model(x, y)\n\nNow, if we run the following code:\n\nfrozen_model.compile(optimizer='Adam', loss='mse') frozen_model.fit(data, labels)\n\nIt will not modify the weights of layer. In fact, calling frozen_model.\n\nsummary() gives us this:\n\n6 Check the official documentation for the example at https://keras.io/ getting-started/faq/#how-can-i-freeze-keras-layers.\n\n50",
      "content_length": 891,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== input_1 (InputLayer) (None, 4) 0 _______________________________________________________________ dense_6 (Dense) (None, 8) 40 =============================================================== Total params: 40 Trainable params: 0 Non-trainable params: 40 _______________________________________________________________\n\nAs expected, there are no trainable parameters. We can simply modify\n\nthe layer.trainable property:\n\nlayer.trainable = True trainable_model = Model(x, y)\n\nNow we compile and fit the model:\n\ntrainable_model.compile(optimizer='Adam', loss='mse') trainable_model.fit(data, labels)\n\nThis time the weights of layer will be updated. We can check on that\n\nwith trainable_model.summary():\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== input_1 (InputLayer) (None, 4) 0 _______________________________________________________________ dense_6 (Dense) (None, 8) 40 ===============================================================\n\n51",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nTotal params: 40 Trainable params: 40 Non-trainable params: 0 _______________________________________________________________\n\nNow all the parameters are trainable, as we wanted.\n\nRemoving Layers\n\nIt’s very useful to remove one or more of the last layers in a model and add different ones to fine-tune it. The idea is used very often in transfer learning, when you train a network and want to fine- tune its behavior by training only the last few layers. Let’s consider the following model:\n\nmodel = Sequential() model.add(Dense(32, activation='relu', input_dim=784, name = 'input')) model.add(Dense(32, activation='relu', name = 'hidden1')) model.add(Dense(32, activation='relu', name = 'hidden2'))\n\nThe summary() call will give this output:\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== input (Dense) (None, 32) 25120 _______________________________________________________________ hidden1 (Dense) (None, 32) 1056 _______________________________________________________________ hidden2 (Dense) (None, 32) 1056 ===============================================================\n\n52",
      "content_length": 1236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nTotal params: 27,232 Trainable params: 27,232 Non-trainable params: 0 _______________________________________________________________\n\nSay you want to build a second model, keeping your trained weights in the input and hidden1 layers, but you want to substitute the hidden2 layer with a different layer (let’s say one with 16 neurons). You can easily do that in the following way:\n\nmodel2 = Sequential() for layer in model.layers[:-1]: model2.add(layer)\n\nThis gives you:\n\nLayer (type) Output Shape Param # =============================================================== input (Dense) (None, 32) 25120 _______________________________________________________________ hidden1 (Dense) (None, 32) 1056 =============================================================== Total params: 26,176 Trainable params: 26,176 Non-trainable params: 0 _______________________________________________________________\n\nAt this point, you can simply add a new layer with the following:\n\nmodel2.add(Dense(16, activation='relu', name = 'hidden3'))\n\n53",
      "content_length": 1065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nIt has the following structure:\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== input (Dense) (None, 32) 25120 _______________________________________________________________ hidden1 (Dense) (None, 32) 1056 _______________________________________________________________ hidden3 (Dense) (None, 16) 528 =============================================================== Total params: 26,704 Trainable params: 26,704 Non-trainable params: 0 _______________________________________________________________\n\nAfter that, remember to compile your model. For example, for a\n\nregression problem, your code may look like this:\n\nmodel.compile(loss='mse', optimizer='Adam', metrics=['mse'])\n\nKeras Callback Functions\n\nIt is instructive to understand a bit better what Keras callback functions are since they are used quite often while developing models. This is from the official documentation7:\n\nA callback is a set of functions to be applied at given stages of the training procedure.\n\nThe idea is that you can pass a list of callback functions to the .fit()\n\nmethod of the Sequential or Model classes. Relevant methods of the\n\n7 https://keras.io/callbacks/\n\n54",
      "content_length": 1304,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\ncallbacks will then be called at each stage of training [https://keras.io/ callbacks/, Accessed 01/02/2019]. Keunwoo Choi has written a nice overview on how to write a callback class that you can find at https:// goo.gl/hL37wq. We summarize it here and expand it with some practical examples.\n\nCustom Callback Class\n\nThe abstract base class, called Callback, can be found at the time of this writing at\n\ntensorflow/python/keras/callbacks.py (https://goo.gl/uMrMbH). To start, you need to define a custom class. The main methods you\n\nwant to redefine are typically the following\n\n\n\non_train_begin: Called at the beginning of training\n\n\n\non_train_end: Called at the end of training\n\n\n\non_epoch_begin: Called at the start of an epoch\n\n\n\non_epoch_end: Called at the end of an epoch\n\n\n\non_batch_begin: Called right before processing a batch\n\n\n\non_batch_end: Called at the end of a batch\n\nThis can be done with the following code:\n\nimport keras class My_Callback(keras.callbacks.Callback): def on_train_begin(self, logs={}): return\n\ndef on_train_end(self, logs={}): return\n\ndef on_epoch_begin(self, epoch, logs={}): return\n\n55",
      "content_length": 1160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\ndef on_epoch_end(self, epoch, logs={}): return\n\ndef on_batch_begin(self, batch, logs={}): return\n\ndef on_batch_end(self, batch, logs={}): self.losses.append(logs.get('loss')) return\n\nEach of the methods has slightly different inputs that you may use in\n\nyour class. Let’s look at them briefly (you can find them in the original Python code at https://goo.gl/uMrMbH).\n\non_epoch_begin, on_epoch_end\n\nArguments:\n\nepoch: integer, index of epoch.\n\nlogs: dictionary of logs.\n\non_train_begin, on_train_end\n\nArguments:\n\nlogs: dictionary of logs.\n\non_batch_begin, on_batch_end\n\nArguments:\n\nbatch: integer, index of batch within the current\n\nepoch.\n\nlogs: dictionary of logs.\n\nLet’s see with an example of how we can use this class.\n\n56",
      "content_length": 766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nExample of a Custom Callback Class\n\nLet’s again consider the MNIST example. It’s the same code you have seen by now:\n\nimport tensorflow as tf from tensorflow import keras (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n\ntrain_labels = train_labels[:5000] test_labels = test_labels[:5000]\n\ntrain_images = train_images[:5000].reshape(-1, 28 * 28) / 255.0 test_images = test_images[:5000].reshape(-1, 28 * 28) / 255.0\n\nLet’s define a Sequential model for our example:\n\nmodel = tf.keras.models.Sequential([ keras.layers.Dense(512, activation=tf.keras.activations.\n\nrelu, input_shape=(784,)), keras.layers.Dropout(0.2), \\keras.layers.Dense(10, activation=tf.keras.activations.\n\nsoftmax)\n\n])\n\nmodel.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_\n\ncrossentropy,\n\nmetrics=['accuracy'])\n\n57",
      "content_length": 888,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nNow let’s write a custom callback class, redefining only one of the methods to see the inputs. For example, let’s see what the logs variable contains at the beginning of the training:\n\nclass CustomCallback1(keras.callbacks.Callback): def on_train_begin(self, logs={}): print (logs) return\n\nYou can then use it with:\n\nCC1 = CustomCallback1() model.fit(train_images, train_labels, epochs = 2, validation_data = (test_images,test_labels), callbacks = [CC1]) # pass callback to training\n\nRemember to always instantiate the class and pass the CC1 variable,\n\nand not the class itself. You will get the following:\n\nTrain on 5000 samples, validate on 5000 samples {} Epoch 1/2 5000/5000 [==============================] - 1s 274us/step - loss: 0.0976 - acc: 0.9746 - val_loss: 0.2690 - val_acc: 0.9172 Epoch 2/2 5000/5000 [==============================] - 1s 275us/step - loss: 0.0650 - acc: 0.9852 - val_loss: 0.2925 - val_acc: 0.9114 {} <tensorflow.python.keras.callbacks.History at 0x7f795d750208>\n\nThe logs dictionary is empty, as you can see from the {}. Let’s expand\n\nour class a bit:\n\n58",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nclass CustomCallback2(keras.callbacks.Callback): def on_train_begin(self, logs={}): print (logs) return\n\ndef on_epoch_end(self, epoch, logs={}): print (\"Just finished epoch\", epoch) print (logs) return\n\nNow we train the network with this:\n\nCC2 = CustomCallback2() model.fit(train_images, train_labels, epochs = 2, validation_data = (test_images,test_labels), callbacks = [CC2]) # pass callback to training\n\nThis will give the following output (reported here for just one epoch for\n\nbrevity):\n\nTrain on 5000 samples, validate on 5000 samples {} Epoch 1/2 4864/5000 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9879 Just finished epoch 0 {'val_loss': 0.2545496598124504, 'val_acc': 0.9244, 'loss': 0.05098680723309517, 'acc': 0.9878}\n\nNow things are starting to get interesting. The logs dictionary now\n\ncontains a lot more information that we can access and use. In the dictionary, we have val_loss, val_acc, and acc. So let’s customize our output a bit. Let’s set verbose = 0 in the fit() call to suppress the standard output and then generate our own.\n\n59",
      "content_length": 1118,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nOur new class will be:\n\nclass CustomCallback3(keras.callbacks.Callback): def on_train_begin(self, logs={}): print (logs) return\n\ndef on_epoch_end(self, epoch, logs={}): print (\"Just finished epoch\", epoch) print ('Loss evaluated on the validation dataset\n\n=',logs.get('val_loss'))\n\nprint ('Accuracy reached is', logs.get('acc')) return\n\nWe can train our network with:\n\nCC3 = CustomCallback3() model.fit(train_images, train_labels, epochs = 2, validation_data = (test_images,test_labels), callbacks = [CC3], verbose = 0) # pass callback to\n\ntraining\n\nWe will get this:\n\n{} Just finished epoch 0 Loss evaluated on the validation dataset = 0.2546206972360611\n\nThe empty {} simply indicates the empty logs dictionary that on_train_begin received. Of course, you can print information every few epochs. For example, by modifying the on_epoch_end() function as follows:\n\ndef on_epoch_end(self, epoch, logs={}): if (epoch % 10 == 0): print (\"Just finished epoch\", epoch)\n\n60",
      "content_length": 1007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nprint ('Loss evaluated on the validation dataset\n\n=',logs.get('val_loss'))\n\nprint ('Accuracy reached is', logs.get('acc')) return\n\nYou will get the following output if you train your network for 30\n\nepochs:\n\n{} Just finished epoch 0 Loss evaluated on the validation dataset = 0.3692033936366439 Accuracy reached is 0.9932 Just finished epoch 10 Loss evaluated on the validation dataset = 0.3073081444747746 Accuracy reached is 1.0 Just finished epoch 20 Loss evaluated on the validation dataset = 0.31566708440929653 Accuracy reached is 0.9992 <tensorflow.python.keras.callbacks.History at 0x7f796083c4e0>\n\nNow you should start to get an idea as to how you can perform several\n\nthings during the training. A typical use of callbacks that we will look at in the next section is saving your model every few epochs. But you can, for example, save accuracy values in lists to be able to plot them later, or simply plot metrics to see how your training is going.\n\nSave and Load Models\n\nIt is often useful to save a model on disk, in order to be able to continue the training at a later stage, or to reuse a previously trained model. To see how you can do this, let’s consider the MNIST dataset again for the sake\n\n61",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nof giving a concrete example.8 The entire code is available in a dedicated notebook in the book’s GitHub repository in the chapter 2 folder.\n\nYou will need the following imports:\n\nimport os import tensorflow as tf from tensorflow import keras\n\nAgain, let’s load the MNIST dataset and take the first 5000\n\nobservations.\n\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data() train_labels = train_labels[:5000] test_labels = test_labels[:5000] train_images = train_images[:5000].reshape(-1, 28 * 28) / 255.0 test_images = test_images[:5000].reshape(-1, 28 * 28) / 255.0\n\nThen let’s build a simple Keras model using a Dense layer with 512 neurons, a bit of dropout, and the classical 10 neuron output layer for classification (remember the MNIST dataset has 10 classes).\n\nmodel = tf.keras.models.Sequential([ keras.layers.Dense(512, activation=tf.keras.activations.\n\nrelu, input_shape=(784,)), keras.layers.Dropout(0.2), keras.layers.Dense(10, activation=tf.keras.activations.\n\nsoftmax)\n\n])\n\nmodel.compile(optimizer='adam',\n\n8 The example was inspired by the official Keras documentation at https://www. tensorflow.org/tutorials/keras/save_and_restore_models.\n\n62",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nloss=tf.keras.losses.sparse_categorical_\n\ncrossentropy,\n\nmetrics=['accuracy'])\n\nWe added a bit of dropout, since this model has 407’050 trainable parameters. You can check this number simply by using model.summary(). What we need to do is define where we want to save the model on the\n\ndisk. And we can do that (for example) in this way:\n\ncheckpoint_path = \"training/cp.ckpt\" checkpoint_dir = os.path.dirname(checkpoint_path)\n\nAfter that, we need to define a callback (remember what we did in the\n\nlast section) that will save the weights:\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_\n\nonly=True, verbose=1)\n\nNote that now we don’t need to define a class as we did in the previous\n\nsection, since ModelCheckpoint inherits from the Callback class.\n\nThen we can simply train the model, specifying the correct callback\n\nfunction:\n\nmodel.fit(train_images, train_labels, epochs = 10, validation_data = (test_images,test_labels), callbacks = [cp_callback])\n\nIf you run a !ls command, you should see at least three files:\n\n\n\ncp.ckpt.data-00000-of-00001: Contains the weights (in case the number of weights is large, you will get many files like this one)\n\n63",
      "content_length": 1227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\n\n\ncp.ckpt.index: This file indicates which weights are in which files\n\n\n\ncheckpoint: This text file contains information about the checkpoint itself\n\nWe can now test our method. The previous code will give you a model\n\nthat will reach an accuracy on the validation dataset of roughly 92%. Now if we define a second model as so:\n\nmodel2 = tf.keras.models.Sequential([ keras.layers.Dense(512, activation=tf.keras.activations.\n\nrelu, input_shape=(784,)), keras.layers.Dropout(0.2), keras.layers.Dense(10, activation=tf.keras.activations.\n\nsoftmax)\n\n])\n\nmodel2.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_\n\ncrossentropy,\n\nmetrics=['accuracy'])\n\nAnd we check its accuracy on the validation dataset with this:\n\nloss, acc = model2.evaluate(test_images, test_labels) print(\"Untrained model, accuracy: {:5.2f}%\".format(100*acc))\n\nWe will get an accuracy of roughly 8.6%. That was expected, since this model has not been trained yet. But now we can load the saved weights in this model and try again.\n\nmodel2.load_weights(checkpoint_path) loss,acc = model2.evaluate(test_images, test_labels) print(\"Second model, accuracy: {:5.2f}%\".format(100*acc))\n\n64",
      "content_length": 1205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nWe should get this result:\n\n5000/5000 [==============================] - 0s 50us/step Restored model, accuracy: 92.06%\n\nThat makes again sense, since the new model is now using the weights on the old trained model. Keep in mind that, to load pre-trained weights in a new model, the new model needs to have the exact same architecture as the original one.\n\nNote to use saved weights with a new model, the new model must have the same architecture as the one used to save the weights. Using pre-trained weights can save you a lot of time, since you don't need to waste time training the network again.\n\nAs we will see again and again, the basic idea is to use callbacks and define a custom one that will save our weights. Of course, we can customize our callback function. For example, if want to save the weights every 100 epochs, each time with a different filename so that we can restore a specific checkpoint if needed, we must first define the filename in a dynamic way:\n\ncheckpoint_path = \"training/cp-{epoch:04d}.ckpt\" checkpoint_dir = os.path.dirname(checkpoint_path)\n\nWe should also use the following callback:\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint( checkpoint_path, verbose=1, save_weights_only=True, period=1)\n\nNote that checkpoint_path can contain named formatting options (in the name we have {epoch:04d}), which will be filled by the values of epoch and keys in logs (passed in on_epoch_end, which we saw in the previous\n\n65",
      "content_length": 1488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nsection).9 You can check the original code for tf.keras.callbacks. ModelCheckpoint and you will find that the formatting is done in the on_ epoch_end(self, epoch, logs) method:\n\nfilepath = self.filepath.format(epoch=epoch + 1, **logs)\n\nYou can define your filename with the epoch number and the values\n\ncontained in the logs dictionary.\n\nLet’s get back to our example. Let’s start by saving the first version of\n\nthe model:\n\nmodel.save_weights(checkpoint_path.format(epoch=0))\n\nThen we can fit the model as usual:\n\nmodel.fit(train_images, train_labels, epochs = 10, callbacks = [cp_callback], validation_data = (test_images,test_labels), verbose=0)\n\nBe careful since this will save lots of files. In our example, one file every epoch. So, for example, your directory content (obtainable with !ls training) may look like this:\n\ncheckpoint cp-0006.ckpt.data-00000-of-00001 cp-0000.ckpt.data-00000-of-00001 cp-0006.ckpt.index cp-0000.ckpt.index cp-0007.ckpt.data-00000-of-00001 cp-0001.ckpt.data-00000-of-00001 cp-0007.ckpt.index cp-0001.ckpt.index cp-0008.ckpt.data-00000-of-00001 cp-0002.ckpt.data-00000-of-00001 cp-0008.ckpt.index cp-0002.ckpt.index cp-0009.ckpt.data-00000-of-00001 cp-0003.ckpt.data-00000-of-00001 cp-0009.ckpt.index cp-0003.ckpt.index cp-0010.ckpt.data-00000-of-00001\n\n9 Check the official documentation at https://goo.gl/SnKgyQ.\n\n66",
      "content_length": 1392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\ncp-0004.ckpt.data-00000-of-00001 cp-0010.ckpt.index cp-0004.ckpt.index cp.ckpt.data-00000-of-00001 cp-0005.ckpt.data-00000-of-00001 cp.ckpt.index cp-0005.ckpt.index\n\nA last tip before moving on is how to get the latest checkpoint, without bothering to search its filename. This can be done easily with the following code:\n\nlatest = tf.train.latest_checkpoint('training') model.load_weights(latest)\n\nThis will load the weights saved in the latest checkpoint automatically.\n\nThe latest variable is simply a string and contains the last checkpoint filename. In our example, that is training/cp- 0010.ckpt.\n\nNote the checkpoint files are binary files that contain the weights of your model. so you will not be able to read them directly, and you should not need to.\n\nSave Your Weights Manually\n\nOf course, you can simply save your model weights manually when you are done training, without defining a callback function:\n\nmodel.save_weights('./checkpoints/my_checkpoint')\n\nThis command will generate three files, all starting with the string you\n\ngave as a name. In this case, it’s my_checkpoint. Running the previous code will generate the three files we described above:\n\ncheckpoint my_checkpoint.data-00000-of-00001 my_checkpoint.index\n\n67",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nReloading the weights in a new model is as simple as this:\n\nmodel.load_weights('./checkpoints/my_checkpoint')\n\nKeep in mind that, to be able to reload saved weights in a new model, the old model must have the same architecture as the new one. It must be exactly the same.\n\nSaving the Entire Model\n\nKeras also allows you to save the entire model on disk: weights, the architecture, and the optimizer. In this way, you can recreate the same model by moving some files. For example, we could use the following code\n\nmodel.save('my_model.h5')\n\nThis will save in one file, called my_model.h5, the entire model. You can simply move the file to a different computer and recreate the same trained model with this code:\n\nnew_model = keras.models.load_model('my_model.h5')\n\nNote that this model will have the same trained weights of your original\n\nmodel, so it’s ready to use. This may be helpful if you want to stop training your model and continue the training on a different machine, for example. Or maybe you must stop the training for a while and continue at a later time.\n\nDataset Abstraction\n\nThe tf.data.Dataset10 is a new abstraction in TensorFlow that is very useful for building data pipelines. It’s also very useful when you are dealing with datasets that do not fit in memory. We will see how to use it in more\n\n10 https://www.tensorflow.org/guide/datasets\n\n68",
      "content_length": 1403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\ndetail later in the book. In the next sections, I give you some hints on a couple of ways in which you can use it in your projects. To learn how to use it, a good starting point is to study the official documentation at https://www.tensorflow.org/guide/datasets. Remember: Always start there when you want to learn more about a specific method or feature of TensorFlow.\n\nBasically, a Dataset it is simply a sequence of elements, in which each\n\nelement contains one or more tensors. Typically, each element will be one training example or a batch of them. The basic idea is that first you create a Dataset with some data, and then you chain method calls on it. For example, you apply the Dataset.map() to apply a function to each element. Note that a dataset is made up of elements, each with the same structure.\n\nAs usual, let’s consider an example to understand how this works and\n\nhow to use it. Let’s suppose we have as input a matrix of 10 rows and 10 columns, defined by the following:\n\ninp = tf.random_uniform([10, 10])\n\nWe can simply create a dataset with the following:\n\ndataset = tf.data.Dataset.from_tensor_slices(inp)\n\nUsing print(dataset), will get you this output:\n\n<TensorSliceDataset shapes: (10,), types: tf.float32>\n\nThat tells you that each element in the dataset is a tensor with 10 elements (the rows in the inp tensor). A nice possibility is to apply specific functions to each element in a dataset. For example, we could multiply all elements by two:\n\ndataset2 = dataset.map(lambda x: x*2)\n\n69",
      "content_length": 1555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nIn order to check what happened, we could print the first element in\n\neach dataset. This can be easily done (more on that later) with:\n\ndataset.make_one_shot_iterator().get_next()\n\nand\n\ndataset2.make_one_shot_iterator().get_next()\n\nFrom the first line, you will get (your number will be different since we\n\nare dealing with random numbers here):\n\n<tf.Tensor: id=62, shape=(10,), dtype=float32, numpy= array([0.2215631 , 0.32099664, 0.04410303, 0.8502971 , 0.2472974 , 0.25522232, 0.94817066, 0.7719344 , 0.60333145, 0.75336015], dtype=float32)>\n\nAnd from the second line, you get:\n\n<tf.Tensor: id=71, shape=(10,), dtype=float32, numpy= array([0.4431262 , 0.6419933 , 0.08820605, 1.7005942 , 0.4945948 , 0.51044464, 1.8963413 , 1.5438688 , 1.2066629 , 1.5067203 ], dtype=float32)>\n\nAs expected, the second output contains all numbers of the first\n\nmultiplied by two.\n\nNote tf.data.dataset is designed to build data processing pipelines. For example, in image recognition you could do data augmentation, preparation, normalization, and so on in this way.\n\n70",
      "content_length": 1096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nI strongly suggest you check the official documentation to get more\n\ninformation on different ways of applying a function to each element. For example, you may need to apply transformation to the data and then flatten the result (see flat_map(), for example).\n\nIterating Over a Dataset\n\nOnce you have your dataset, you probably want to process the elements one by one, or in batches. To do that, you need an iterator. For example, to process the elements that you defined before one by one, you can instantiate a so-called make_one_shot_iterator() as follows:\n\niterator = dataset.make_one_shot_iterator()\n\nThen you can iterate over the elements using the get_next() method:\n\nfor i in range(10): value = print(iterator.get_next())\n\nThis will give you all the elements in the dataset. They will look like this\n\none (note that your number will be different):\n\ntf.Tensor( [0.2215631 0.32099664 0.04410303 0.8502971 0.2472974 0.25522232 0.94817066 0.7719344 0.60333145 0.75336015], shape=(10,), dtype=float32)\n\nNote that once you reach the end of the dataset, using the method\n\nget_next() will raise a tf.errors.OutOfRangeError.\n\n71",
      "content_length": 1167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nSimple Batching\n\nThe most fundamental way to batch consists of stacking n consecutive elements of a dataset in a single group. This will be very useful when we train our networks with mini-batches. This can be done using the batch() method. Let’s get back to our example. Remember that our dataset has 10 elements. Suppose we want to create batches, each having two elements. This can be done with this code:\n\nbatched_dataset = dataset.batch(2)\n\nNow let’s define an iterator again with:\n\niterator = batched_dataset.make_one_shot_iterator()\n\nNow let’s check what get_next() will return with this:\n\nprint(iterator.get_next())\n\nThe output will be:\n\ntf.Tensor( [[0.2215631 0.32099664 0.04410303 0.8502971 0.2472974 0.25522232 0.94817066 0.7719344 0.60333145 0.75336015] [0.28381765 0.3738917 0.8146689 0.20919728 0.5753969 0.9356725 0.7362906 0.76200795 0.01308048 0.14003313]], shape=(2, 10),\n\ndtype=float32)\n\nThat is two elements of our dataset.\n\nNote Batching with the batch() method is really useful when we train a neural network with mini-batches. we don’t have to bother creating the batches ourselves, as tf.data.dataset will do it for us.\n\n72",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nSimple Batching with the MNIST Dataset\n\nTo try the following code, you may want to restart the kernel you are using to avoid to conflicts with eager execution from the previous examples. Once you have done that, load the data (as before):\n\nnum_classes = 10\n\nmnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimage_vector_size = 28*28 x_train = x_train.reshape(x_train.shape[0], image_vector_size) x_test = x_test.reshape(x_test.shape[0], image_vector_size)\n\ny_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes)\n\nThen create the training Dataset:\n\nmnist_ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n\nNow build the Keras model using a simple feed-forward network with\n\ntwo layers:\n\nimg = tf.placeholder(tf.float32, shape=(None, 784)) x = Dense(128, activation='relu')(img) # fully-connected layer with 128 units and ReLU activation x = Dense(128, activation='relu')(x) preds = Dense(10, activation='softmax')(x) labels = tf.placeholder(tf.float32, shape=(None, 10)) loss = tf.reduce_mean(categorical_crossentropy(labels, preds))\n\ncorrect_prediction = tf.equal(tf.argmax(preds,1), tf.argmax(labels,1))\n\n73",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\ntrain_step = tf.train.AdamOptimizer(0.001).minimize(loss) init_op = tf.global_variables_initializer()\n\nNow we need to define the batch size:\n\ntrain_batched = mnist_ds_train.batch(1000)\n\nAnd now let’s define the iterator:\n\ntrain_iterator = train_batched.make_initializable_iterator() # So we can restart from the beginning next_batch = train_iterator.get_next() it_init_op = train_iterator.initializer\n\nThe it_init_op operation will be used to reset the iterator and will start from the beginning of each epoch. Note that the next_batch operation has the following structure:\n\n(<tf.Tensor 'IteratorGetNext_6:0' shape=(?, 784) dtype=uint8>, <tf.Tensor 'IteratorGetNext_6:1' shape=(?, 10) dtype=float32>)\n\nSince it contains the images and the labels. During our training, we will\n\nneed to get the batches in this form:\n\ntrain_batch_x, train_batch_y = sess.run(next_batch)\n\nFinally, let’s train our network:\n\nwith tf.Session() as sess: sess.run(init_op)\n\nfor epoch in range(50): sess.run(it_init_op) try:\n\n74",
      "content_length": 1112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nwhile True: train_batch_x, train_batch_y = sess.run(next_batch) sess.run(train_step,feed_dict={img: train_batch_x,\n\nlabels: train_batch_y})\n\nexcept tf.errors.OutOfRangeError: pass\n\nif (epoch % 10 == 0 ): print('epoch',epoch) print(sess.run(accuracy,feed_dict={img: x_train, labels: y_train}))\n\nNow, I have used a few tricks here that are good to know. In particular,\n\nsince you don’t know how many batches you have, you can use the following construct to avoid getting error messages:\n\ntry: while True: # Do something except tf.errors.OutOfRangeError: pass\n\nThis way, when you get an OutOfRangeError when you run out of batches, the exception will simply go on without interrupting your code. Note how, for each epoch, we call this code to reset the iterator:\n\nsess.run(it_init_op)\n\nOtherwise, we would get an OutOfRangeError immediately. Running\n\nthis code will get you to roughly 99% accuracy very fast. You should see output like this one (I show the output for epoch 40 only, for brevity):\n\nepoch 40 0.98903334\n\n75",
      "content_length": 1058,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nThis quick overview of the dataset is not exhaustive by any means, but\n\nshould give you an idea of its power. If you want to learn more, the best place to do so is, as usual, the official documentation.\n\nNote tf.data.Dataset is an extremely convenient way of building pipelines for data, beginning from loading, to manipulating, normalizing, augmenting, and so on. especially in image-recognition problems, this can be very useful. remember that using it means adding nodes to your computational graph. so no data is processed until the session evaluates the graph.\n\nUsing tf.data.Dataset in Eager Execution Mode\n\nThis chapter ends with one final hint. If you are working in eager execution mode, your life with datasets is even easier. For example, to iterate over a batched dataset, you can simply do as you would do with classical Python (for x in ...). To understand what I mean, let’s look at an easy example. First, you need to enable eager execution:\n\nimport tensorflow as tf from tensorflow import keras import tensorflow.contrib.eager as tfe\n\ntf.enable_eager_execution()\n\nThen you can simply do this:\n\ndataset = tf.data.Dataset.from_tensor_slices(tf.random_ uniform([4, 2])) dataset = dataset.batch(2) for batch in dataset: print(batch)\n\n76",
      "content_length": 1289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Chapter 2\n\ntensorFlow: advanCed topiCs\n\nThis can be very useful when you need to iterate over a dataset batch by batch. The output would be as follows (your numbers will be different, due to the tf.random.uniform() call):\n\ntf.Tensor( [[0.07181489 0.46992648] [0.00652897 0.9028846 ]], shape=(2, 2), dtype=float32) tf.Tensor( [[0.9167508 0.8379569 ] [0.33501422 0.3299384 ]], shape=(2, 2), dtype=float32)\n\nConclusions\n\nThis chapter had the goal of showing you a few techniques that we will use in this book and that will be very helpful to your projects. The goal was not to explain those methods in detail, as that would require a separate book. But the chapter should point you in the right direction when trying to do specific things, such as saving the weights of your model regularly. In the next chapters, we will use some of these techniques. If you want to learn a bit more about them, remember to always check the official documentation.\n\n77",
      "content_length": 949,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "CHAPTER 3\n\nFundamentals of Convolutional Neural Networks\n\nIn this chapter, we will look at the main components of a convolutional neural network (CNN): kernels and pooling layers. We will then look at how a typical network looks. We will then try to solve a classification problem with a simple convolutional network and try to visualize the convolutional operation. The purpose of this is to try to understand, at least intuitively, how the learning works.\n\nKernels and Filters\n\nOne of the main components of CNNs are filters, which are square matrices that have dimensions nK × nK, where nK is an integer and is usually a small number, like 3 or 5. Sometimes filters are also called kernels. Using kernels comes from classical image processing techniques. If you have used Photoshop or similar software, you are used to do operations like\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_3\n\n79",
      "content_length": 962,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nsharpening, blurring, embossing, and so on.1 All those operations are done with kernels. We will see in this section what exactly kernels are and how they work. Note that in this book we will use both terms (kernels and filters) interchangeably. Let’s define four different filters and let’s check later in the chapter their effect when used in convolution operations. For those examples, we will work with 3 × 3 filters. For the moment, just take the following definitions as a reference and we will see how to use them later in the chapter.\n\nThe following kernel will allow the detection of\n\nhorizontal edges\n\nIH =\n\næ ç ç ç è\n\n1\n\n0 - 1\n\n1\n\n0 - 1\n\n1\n\n0 - 1\n\nö ÷ ÷ ÷ ø\n\nThe following kernel will allow the detection of vertical\n\nedges\n\nIV =\n\næ ç ç ç è\n\n1 0\n\n1 0\n\n1 0\n\n1 - 1 - 1\n\nö ÷ ÷ ÷ ø\n\nThe following kernel will allow the detection of edges\n\nwhen luminosity changes drastically\n\nIL =\n\næ ç ç ç è\n\n1 - 1 - 1\n\n1\n\n8 - 1\n\n1 - 1 - 1\n\nö ÷ ÷ ÷ ø\n\n1 You can find a nice overview on Wikipedia at https://en.wikipedia.org/wiki/ Kernel_(image_processing).\n\n80",
      "content_length": 1109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nThe following kernel will blur edges in an image\n\nIB = -\n\n1 9\n\næ ç ç ç è\n\n1 1 1\n\n1 1 1\n\n1 1 1\n\nö ÷ ÷ ÷ ø\n\nIn the next sections, we will apply convolution to a test image with the filters, to see what their effect is.\n\nConvolution\n\nThe first step to understanding CNNs is to understand convolution. The easiest way is to see it in action with a few simple cases. First, in the context of neural networks, convolution is done between tensors. The operation gets two tensors as input and produces a tensor as output. The operation is usually indicated with the operator *.\n\nLet’s see how it works. Consider two tensors, both with dimensions\n\n3 × 3. The convolution operation is done by applying the following formula:\n\næ ç ç ç è\n\na 1\n\na\n\n4\n\na\n\n7\n\na\n\na\n\na\n\n2\n\n5\n\n8\n\na\n\na\n\na\n\n3\n\n6\n\n9\n\nö ÷ ÷ ÷ ø\n\n\n\næ ç ç ç è\n\nk 1\n\nk\n\n4\n\nk 7\n\nk\n\nk\n\nk\n\n2\n\n5\n\n8\n\nk\n\n3\n\nk 6 k\n\n9\n\nö ÷ ÷ ÷ ø\n\n=\n\n9\n\nå\n\nii\n\n= 1\n\nia k i\n\nIn this case, the result is merely the sum of each element, ai, multiplied\n\nby the respective element, ki. In more typical matrix formalism, this formula could be written with a double sum as\n\næ ç ç ç è\n\na\n\na\n\na\n\n11\n\n21\n\n31\n\na\n\n12\n\na\n\n22\n\na\n\n32\n\na\n\n13\n\na\n\n23\n\na\n\n33\n\nö ÷ ÷ ÷ ø\n\n\n\næ ç ç ç è\n\nk 11 k\n\n21\n\nk\n\n3 11\n\nk 12 k k\n\n22\n\n32\n\nk 13 k k\n\n23\n\n33\n\nö ÷ ÷ ÷ ø\n\n=\n\n3\n\n3\n\nåå\n\na k ij ij = 1\n\ni\n\n= 1\n\nj\n\n81",
      "content_length": 1349,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nHowever, the first version has the advantage of making the\n\nfundamental idea very clear: each element from one tensor is multiplied by the correspondent element (the element in the same position) of the second tensor, and then all the values are summed to get the result.\n\nIn the previous section, we talked about kernels, and the reason is that\n\nconvolution is usually done between a tensor, that we may indicate here with A, and a kernel. Typically, kernels are small, 3 × 3 or 5 × 5, while the input tensors A are normally bigger. In image recognition for example, the input tensors A are the images that may have dimensions as high as 1024 × 1024 × 3, where 1024 × 1024 is the resolution and the last dimension (3) is the number of the color channels, the RGB values.\n\nIn advanced applications, the images may even have higher resolution.\n\nTo understand how to apply convolution when we have matrices with different dimensions, let’s consider a matrix A that is 4 × 4\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1\n\na\n\n5\n\na\n\n9\n\na\n\n13\n\na\n\n2\n\na\n\n6\n\na\n\n10\n\na\n\n14\n\na\n\n3\n\na\n\n7\n\na\n\n11\n\na\n\n15\n\na\n\n4\n\na\n\n8\n\na\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\nAnd a Kernel K that we will take for this example to be 3 × 3\n\nK\n\n=\n\næ ç ç ç è\n\nk 1\n\nk\n\n4\n\nk 7\n\nk\n\nk\n\nk\n\n2\n\n5\n\n8\n\nk\n\n3\n\nk 6 k\n\n9\n\nö ÷ ÷ ÷ ø\n\nThe idea is to start in the top-left corner of the matrix A and select a\n\n3 × 3 region. In the example that would be\n\nA 1\n\n=\n\næ ç ç ç è\n\na 1 a\n\n5\n\na\n\n9\n\na a\n\n2\n\n6\n\na\n\n10\n\na a\n\n3\n\n7\n\na\n\n11\n\nö ÷ ÷ ÷ ø\n\n82",
      "content_length": 1508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nAlternatively, the elements marked in boldface here:\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1 a 5\n\na a\n\n9\n\n13\n\na\n\n2\n\na\n\n6\n\na a\n\n10\n\n14\n\na\n\n3\n\na\n\n7\n\na a\n\n11\n\n15\n\na\n\n4\n\na\n\n8\n\na\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\nThen we perform the convolution, as explained at the beginning between this smaller matrix A1 and K, getting (we will indicate the result with B1):\n\nB 1\n\n=\n\n\n\n=\n\nA K a k 1 1\n\n1\n\n+\n\na k 2 2\n\n+\n\na k 3 3\n\n+\n\nk a 4\n\n5\n\n+\n\nk a 5\n\n5\n\n+\n\nk a 7 6\n\n+\n\nk a 7\n\n9\n\n+\n\nk a\n\n8 10\n\n+\n\nk a\n\n9 11\n\nThen we need to shift the selected 3 × 3 region in matrix A of one\n\ncolumn to the right and select the elements marked in bold here:\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1\n\na\n\n5\n\na\n\n9\n\na\n\n13\n\na\n\n2\n\na\n\n6\n\na a\n\n10\n\n14\n\na\n\n3\n\na\n\n7\n\na a\n\n11\n\n15\n\na\n\n4\n\na\n\n8\n\na a\n\n12\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\nThis will give us the second sub-matrix A2:\n\nA 2\n\n=\n\næ ç ç ç è\n\na\n\n2\n\na\n\n6\n\na\n\n10\n\na\n\n3\n\na\n\n7\n\na\n\n11\n\na\n\n4\n\na\n\n8\n\na\n\n12\n\nö ÷ ÷ ÷ ø\n\nWe then perform the convolution between this smaller matrix A2\n\nand K again:\n\nB 2\n\n=\n\nA K a k 2 1\n\n\n\n=\n\n2\n\n+\n\na k 3 2\n\n+\n\na k 4 3\n\n+\n\na k 6 4\n\n+\n\na k 7 5\n\n+\n\na k 8 6\n\n+\n\na k 10 7\n\n+\n\na k 11 8\n\n+\n\na k 12 9\n\n83",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nWe cannot shift our 3 × 3 region anymore to the right, since we have reached the end of the matrix A, so what we do is shift it one row down and start again from the left side. The next selected region would be\n\nA 3\n\n=\n\næ ç ç ç è\n\na\n\n5\n\na\n\n9\n\na\n\n13\n\na\n\n6\n\na\n\n10\n\na\n\n14\n\na\n\n7\n\na\n\n11\n\na\n\n15\n\nö ÷ ÷ ÷ ø\n\nAgain, we perform convolution of A3 with K\n\nB\n\n3\n\n=\n\n\n\n=\n\nA K a k 5 1\n\n3\n\n+\n\na k 6 2\n\n+\n\na k 7 3\n\n+\n\na k 9 4\n\n+\n\na k 10 5\n\n+\n\na k 11 6\n\n+\n\na k 13 7\n\n+\n\na k 14 8\n\n+\n\na k 15 9\n\nAs you may have guessed at this point, the last step is to shift our 3 × 3 selected region to the right of one column and perform convolution again. Our selected region will now be\n\nA 4\n\n=\n\næ ç ç ç è\n\na\n\n6\n\na\n\n10\n\na\n\n14\n\na\n\n7\n\na\n\n11\n\na\n\n15\n\na\n\n8\n\na\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ø\n\nMoreover, the convolution will give this result:\n\nB\n\n4\n\n=\n\nA K a k 6 1\n\n\n\n=\n\n4\n\n+\n\na k 7 2\n\n+\n\na k 8 3\n\n+\n\na k 10 4\n\n+\n\na k 11 5\n\n+\n\na k 12 6\n\n+\n\na k 14 7\n\n+\n\na k 15 8\n\n+\n\na k 16 9\n\nNow we cannot shift our 3 × 3 region anymore, neither right nor down. We have calculated four values: B1, B2, B3, and B4. Those elements will form the resulting tensor of the convolution operation giving us the tensor B:\n\nB\n\n=\n\næ ç è\n\nB 1 B 3\n\nB 2 B\n\n4\n\nö ÷ ø\n\n84",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nThe same process can be applied when tensor A is bigger. You will simply get a bigger resulting B tensor, but the algorithm to get the elements Bi is the same. Before moving on, there is still a small detail that we need to discuss, and that is the concept of stride. In the previous process, we moved our 3 × 3 region always one column to the right and one row down. The number of rows and columns, in this example 1, is called the stride and is often indicated with s. Stride s = 2 means simply that we shift our 3 × 3 region two columns to the right and two rows down at each step. Something else that we need to discuss is the size of the selected region in the input matrix A. The dimensions of the selected region that we shifted around in the process must be the same as of the kernel used. If you use a 5 × 5 kernel, you will need to select a 5 × 5 region in A. In general, given a nK × nK kernel, you select a nK × nK region in A.\n\nIn a more formal definition, convolution with stride s in the neural network context is a process that takes a tensor A of dimensions nA × nA and a kernel K of dimensions nK × nK and gives as output a matrix B of dimensions nB × nB with\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\ns\n\nn\n\nK\n\n+\n\nú ûú1\n\nWhere we have indicated with ⌊x⌋ the integer part of x (in the programming world, this is often called the floor of x). A proof of this formula would take too long to discuss, but it’s easy to see why it is true (try to derive it). To make things a bit easier, suppose that nK is odd. You will see soon why this is important (although not fundamental). Let’s start explaining formally the case with a stride s = 1. The algorithm generates a new tensor B from an input tensor A and a kernel K according to the formula\n\nB ij\n\n=\n\n( A K\n\n) =\n\nij\n\nn\n\n1\n\nn\n\n1 A i 0\n\n1 A i 0\n\nK\n\nK\n\nå å\n\nf\n\n=\n\n0\n\nh\n\n+\n\nf\n\n+ j h\n\nK\n\ni\n\n+\n\n,\n\n+ j h\n\n85",
      "content_length": 1904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nThe formula is cryptic and is very difficult to understand. Let’s study some more examples to grasp the meaning better. In Figure 3-1, you can see a visual explanation of how convolution works. Suppose to have a 3 × 3 filter. Then in the Figure 3-1, you can see that the top left nine elements of the matrix A, marked by a square drawn with a black continuous line, are the one used to generate the first element of the matrix B1 according to this formula. The elements marked by the square drawn with a dotted line are the ones used to generate the second element B2 and so on.\n\nFigure 3-1. A visual explanation of convolution\n\nTo reiterate what we discuss in the example at the beginning, the basic\n\nidea is that each element of the 3 × 3 square from matrix A is multiplied by the corresponding element of the kernel K and all the numbers are summed. The sum is then the element of the new matrix B. After having calculated the value for B1, you shift the region you are considering in the original matrix of one column to the right (the square indicated in Figure 3-1 with a dotted line) and repeat the operation. You continue to shift your region to the right until you reach the border and then you move one element down and start again from the left. You continue in this fashion until the lower right angle of the matrix. The same kernel is used for all the regions in the original matrix.\n\nGiven the kernel IH for example, you can see in Figure 3-2 which element of A are multiplied by which elements in IH and the result for the element B1, that is nothing else as the sum of all the multiplications\n\n= ´ + ´ + ´ + ´ + ´ + ´ + ´ -( 1\n\nB11 1 1 2 1 3 1 1 0 2 0 3 0 4\n\n)+ ´ -( 3 1\n\n)+ ´ -( 2 1\n\n)= - 3\n\n86",
      "content_length": 1769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-2. A visualization of convolution with the kernel IH\n\nIn Figure 3-3, you can see an example of convolution with stride s = 2.\n\nFigure 3-3. A visual explanation of convolution with stride s = 2\n\nThe reason that the dimension of the output matrix takes only the floor\n\n(the integer part) of\n\nn\n\nA\n\nK- n s\n\n+1\n\nCan be seen intuitively in Figure 3-4. If s > 1, what can happen, depending on the dimensions of A, is that at a certain point you cannot shift your window on matrix A (the black square you can see in Figure 3-3 for example) anymore, and you cannot cover all of matrix A completely. In Figure 3-4, you can see how you would need an additional column to the right of matrix A (marked by many X) to be able to perform the convolution\n\n87",
      "content_length": 810,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\noperation. In Figure 3-4, we chose s = 3, and since we have nA = 5 and nK = 3, B will be a scalar as a result.\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\ns\n\nn\n\nK\n\n+\n\nú ûú= 1\n\nê ëê\n\n5 3 3\n\n+\n\nú ûú= 1\n\nê ëê\n\n5 3\n\nú ûú=\n\n1\n\nFigure 3-4. A visual explanation of why the floor function is needed when evaluating the resulting matrix B dimensions\n\nYou can easily see from Figure 3-4, how with a 3 × 3 region, one can only cover the top-left region of A, since with stride s = 3 you would end up outside A and therefore can consider one region only for the convolution operation. Therefore, you end up with a scalar for the resulting tensor B.\n\nLet’s now look at a few additional examples to make this formula even\n\nmore transparent. Let’s start with a small matrix 3 × 3\n\nA =\n\næ ç ç ç è\n\n1 2 3\n\n4 5 6\n\n7 8 9\n\nö ÷ ÷ ÷ ø\n\nMoreover, let’s consider the kernel\n\nK\n\n=\n\næ ç ç ç è\n\nk 1 k\n\n4\n\nk 7\n\nk k\n\nk\n\n2\n\n5\n\n8\n\nk k 6 k\n\n3\n\n9\n\nö ÷ ÷ ÷ ø\n\n88",
      "content_length": 964,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nwith stride s = 1. The convolution will be given by\n\nB A K\n\n= * = × + × 2 1\n\nk 1\n\nk\n\n2\n\n+ × 3\n\nk\n\n3\n\n+ × 4\n\nk\n\n4\n\n+ × 5\n\nk\n\n5\n\n+ × 6\n\nk 6\n\n+ × 7\n\nk 7\n\n+ × 8\n\nk\n\n8\n\n+ × 9\n\nk\n\n9\n\nMoreover, the result B will be a scalar, since nA = 3, nK = 3.\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\ns\n\nn\n\nK\n\n+\n\nú ûú= 1\n\nê ëê\n\n3 3 1\n\n+\n\nú ûú= 1\n\n1\n\nIf you consider a matrix A with dimensions 4 × 4, or nA = 4, nK = 3 and\n\ns = 1, you will get as output a matrix B with dimensions 2 × 2, since\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\ns\n\nn\n\nK\n\n+\n\nú ûú= 1\n\nê ëê\n\n4 3 1\n\n+\n\nú ûú= 1\n\n2\n\nFor example, you can verify that given\n\nA =\n\næ ç ç ç ç è\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10 11 12\n\n13 14 15 16\n\nö ÷ ÷ ÷ ÷ ø\n\nAnd\n\nK =\n\næ ç ç ç è\n\n1 2 3\n\n4 5 6\n\n7 8 9\n\nö ÷ ÷ ÷ ø\n\nWe have with stride s = 1\n\n= * B A K\n\n=\n\næ ç è\n\n348 393 528 573\n\nö ÷ ø\n\n89",
      "content_length": 831,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nWe’ll verify one of the elements: B11 with the formula I gave you. We have\n\nB\n\n11\n\n=\n\n2\n\n2\n\nå å A + 1\n\n+ , 1\n\nh\n\nK\n\n+ 1\n\nf\n\n+ , 1\n\nh\n\n=\n\n2\n\nå\n\n(\n\nA\n\n+ 1\n\nf\n\n,\n\n1\n\nK\n\n+ 1\n\nf\n\n, 1\n\n+\n\nA\n\n+ 1\n\nf\n\n,\n\n2\n\nK\n\n+ 1\n\n2 ,\n\n+\n\nA\n\n+ 1\n\nf\n\n3\n\nfK + 1\n\n3 ,\n\n)\n\n=\n\n0\n\nh\n\n=\n\n0\n\nf\n\n=\n\n0\n\n=\n\n( +\n\nA ( (\n\n+\n\n1 1 ,\n\nK\n\n1 1 ,\n\n+\n\nA\n\n1 2 ,\n\nK\n\n1 2 ,\n\nA\n\n, 3 1\n\nK\n\n, 3 1\n\n+\n\nA\n\n, 3 2\n\nK\n\n× +\n\n× +\n\n×\n\n9 7 10 8 11 9\n\n( K A AA )= × + × + 1 1 2 2\n\n)+ K\n\n+\n\n+\n\nK\n\nA\n\n1 3 ,\n\n2 1 ,\n\n1 3 ,\n\n2 1 ,\n\n(\n\nA\n\n+ )=\n\n, 3 3\n\n, 3 2\n\n, 3 3\n\n+\n\n+\n\n=\n\n14 92 242 348\n\n, 2 2\n\nK\n\n, 2 2\n\n+\n\n× 33 3\n\n)+\n\n(\n\n) × + × + × 5 4 6 5 7 6\n\nA\n\nK\n\n, 2 3\n\n, 2 3\n\nNote that the formula I gave you for the convolution works only for\n\nstride s = 1, but can be easily generalized for other values of s.\n\nThis calculation is very easy to implement in Python. The following\n\nfunction can evaluate the convolution of two matrices easily enough for s = 1 (you can do it in Python with existing functions, but I think it’s instructive to see how to do it from scratch):\n\nimport numpy as np def conv_2d(A, kernel): output = np.zeros([A.shape[0]-(kernel.shape[0]-1),\n\nA.shape[1]-(kernel.shape[0]-1)])\n\nfor row in range(1,A.shape[0]-1): for column in range(1, A.shape[1]-1): output[row-1, column-1] = np.tensordot(A[row-\n\n1:row+2, column-1:column+2], kernel)\n\nreturn output\n\nNote that the input matrix A does not even need to a square one, but it is assumed that the kernel is and that its dimension nK is odd. The previous example can be evaluated with the following code:\n\nA = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]]) K = np.array([[1,2,3],[4,5,6],[7,8,9]]) print(conv_2d(A,K))\n\n90\n\n)",
      "content_length": 1690,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nThis gives the result:\n\n[[ 348. 393.] [ 528. 573.]]\n\nExamples of Convolution\n\nNow let’s try to apply the kernels we defined at the beginning to a test image and see the results. As a test image, let’s create a chessboard of dimensions 160 × 160 pixels with the code:\n\nchessboard = np.zeros([8*20, 8*20]) for row in range(0, 8): for column in range (0, 8): if ((column+8*row) % 2 == 1) and (row % 2 == 0): chessboard[row*20:row*20+20,\n\ncolumn*20:column*20+20] = 1\n\nelif ((column+8*row) % 2 == 0) and (row % 2 == 1): chessboard[row*20:row*20+20,\n\ncolumn*20:column*20+20] = 1\n\n91",
      "content_length": 634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nIn Figure 3-5, you can see how the chessboard looks.\n\nFigure 3-5. The chessboard image generated with code\n\nNow let’s apply convolution to this image with the different kernels\n\nwith stride s = 1.\n\nUsing the kernel, IH will detect the horizontal edges. This can be\n\napplied with the code\n\nedgeh = np.matrix('1 1 1; 0 0 0; -1 -1 -1') outputh = conv_2d (chessboard, edgeh)\n\nIn Figure 3-6, you can see how the output looks. The image can be\n\neasily generated with this code:\n\nImport matplotlib.pyplot as plt plt.imshow(outputh)\n\n92",
      "content_length": 586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-6. The result of performing a convolution between the kernel IH and the chessboard image\n\nNow you can understand why this kernel detects horizontal edges. Additionally, this kernel detects when you go from light to dark or vice versa. Note this image is only 158 × 158 pixels, as expected, since\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\ns\n\nn\n\nK\n\n+\n\nú ûú= 1\n\nê ëê\n\n160 3 1\n\n+\n\nú ûú= 1\n\nê ëê\n\n157 1\n\n+\n\nú ûú= êë 1\n\n158\n\núû=\n\n158\n\nNow let’s apply IV using this code:\n\nedgev = np.matrix('1 0 -1; 1 0 -1; 1 0 -1') outputv = conv_2d (chessboard, edgev)\n\n93",
      "content_length": 598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nThis gives the result shown in Figure 3-7.\n\nFigure 3-7. The result of performing a convolution between the kernel IV and the chessboard image\n\nNow we can use kernel IL :\n\nedgel = np.matrix ('-1 -1 -1; -1 8 -1; -1 -1 -1') outputl = conv_2d (chessboard, edgel)\n\nThat gives the result shown in Figure 3-8.\n\n94",
      "content_length": 364,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-8. The result of performing a convolution between the kernel IL and the chessboard image\n\nMoreover, we can apply the blurring kernel IB :\n\nedge_blur = -1.0/9.0*np.matrix('1 1 1; 1 1 1; 1 1 1') output_blur = conv_2d (chessboard, edge_blur)\n\nIn Figure 3-9, you can see two plots—on the left the blurred image and\n\non the right the original one. The images show only a small region of the original chessboard to make the blurring clearer.\n\n95",
      "content_length": 506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-9. The effect of the blurring kernel IB. On the left is the blurred image and on the right is the original one.\n\nTo finish this section, let’s try to understand better how the edges can\n\nbe detected. Consider the following matrix with a sharp vertical transition, since the left part is full of 10 and the right part full of 0.\n\nex_mat = np.matrix('10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0; 10 10 10 10 0 0 0 0')\n\nThis looks like this\n\nmatrix([[10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0], [10, 10, 10, 10, 0, 0, 0, 0]])\n\n96",
      "content_length": 860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nLet’s consider the kernel IV . We can perform the convolution with\n\nthis code:\n\nex_out = conv_2d (ex_mat, edgev)\n\nThe result is as follows:\n\narray([[ 0., 0., 30., 30., 0., 0.], [ 0., 0., 30., 30., 0., 0.], [ 0., 0., 30., 30., 0., 0.], [ 0., 0., 30., 30., 0., 0.], [ 0., 0., 30., 30., 0., 0.], [ 0., 0., 30., 30., 0., 0.]])\n\nIn Figure 3-10, you can see the original matrix (on the left) and the output of the convolution on the right. The convolution with the kernel IV has clearly detected the sharp transition in the original matrix, marking with a vertical black line where the transition from black to white happens. For example, consider B11 = 0\n\nB\n\n=\n\n10 10 10\n\n1 - 1 - 1\n\n10 10 10\n\n1 0\n\næ ç ç çç è\n\nö ÷ ÷ ÷ ø ´ +\n\næ ç ç ç è ´- + 1 10 1 10 0 100\n\nö ÷ ÷ ÷ ø ´ +\n\næ ç ç ç è ´ +\n\nö ÷ ÷ ÷ ø ´- + 10 1 10 0 10\n\n\n\n=\n\n=\n\n\n\nI V\n\n10 10 10\n\n1 0\n\n10 10 10\n\n11\n\n10 10 10\n\n1 0\n\n10 10 10\n\n´ +\n\n´ +\n\n´ +\n\n1 10 1 10 0 10\n\n´- =\n\n1 0\n\nNote that in the input matrix\n\næ ç ç ç è\n\n10 10 10\n\n10 10 10\n\n10 10 10\n\nö ÷ ÷ ÷ ø\n\n97",
      "content_length": 1065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nthere is no transition, as all the values are the same. On the contrary, if you consider B13 you need to consider this region of the input matrix\n\næ ç ç ç è\n\n10 10 0\n\n10 10 0\n\n10 10 0\n\nö ÷ ÷ ÷ ø\n\nwhere there is a clear transition since the right-most column is made of zeros and the rest of 10. You get now a different result\n\n1 - 1 - 1\n\n1 0\n\n10 10 0\n\n10 10 0\n\næ ç ç ç è\n\nö ÷ ÷ ÷ ø ´\n\nö ÷ ÷ ÷ ø ´ + ´- + 10 1 10 0 0\n\næ ç ç ç è ´ +\n\nö ÷ ÷ ÷ ø\n\næ ç ç ç è ´ + ´- + 1 10 1 10 0 0\n\n1 0\n\n**\n\n10 10 0\n\nB\n\n=\n\nI V\n\n\n\n10 10 0\n\n=\n\n11\n\n10 10 0\n\n10 10 0\n\n1 0\n\n´ +\n\n+ 1 10 11 10 0 0\n\n=\n\n´ + ´- =\n\n1 30\n\nMoreover, this is precisely how, as soon as there is a significant change\n\nin values along the horizontal direction, the convolution returns a high value since the values multiplied by the column with 1 in the kernel will be more significant. When there is a transition from small to high values along the horizontal axis, the elements multiplied by -1 will give a result that is bigger in absolute value. Therefore the final result will be negative and big in absolute value. This is the reason that this kernel can also detect if you pass from a light color to a darker color and vice versa. If you consider the opposite transition (from 0 to 10) in a different hypothetical matrix A, you would have\n\n1 - 1 - 1\n\n1 0\n\n0 10 10\n\n0 10 10\n\nö ÷ ÷ ÷ ø ´ +\n\næ ç ç ç è\n\nö ÷ ÷ ÷ ø 1 0 1++\n\næ ç ç ç è ´- + ´ 1 0 1 10 0 10\n\nö ÷ ÷ ÷ ø ´- + ´ + 0 1 10 0 10\n\næ ç ç ç è\n\n\n\n=\n\n=\n\n**\n\nB\n\n1 0\n\n0 10 10\n\n0 10 10\n\nI V\n\n11\n\n0 10 10\n\n1 0\n\n0 10 10\n\n´ +\n\n= ´ +\n\n´ + 10 0 10\n\n´- = - 1\n\n30\n\nWe move from 0 to 10 along the horizontal direction.\n\n98",
      "content_length": 1669,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-10. The result of the convolution of the matrix ex_mat with the kernel IV as described in the text\n\nNote how, as expected, the output matrix has dimensions 5 × 5 since\n\nthe original matrix has dimensions 7 × 7 and the kernel is 3 × 3.\n\nPooling\n\nPooling is the second operation that is fundamental in CNNs. This operation is much easier to understand than convolution. To understand it, let’s look at a concrete example and consider what is called max pooling. Consider the 4 × 4 matrix we discussed during our convolution discussion again:\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1\n\na\n\n5\n\na\n\n9\n\na\n\n13\n\na\n\n2\n\na\n\n6\n\na\n\n10\n\na\n\n14\n\na\n\n3\n\na\n\n7\n\na\n\n11\n\na\n\n15\n\na\n\n4\n\na\n\n8\n\na\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\n99",
      "content_length": 744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nTo perform max pooling, we need to define a region of size nK × nK, analogous to what we did for convolution. Let’s consider nK = 2. What we need to do is start on the top-left corner of our matrix A and select a nK × nK region, in our case 2 × 2 from A. Here we would select\n\næ ç è\n\na 1\n\na\n\n5\n\na\n\na\n\n2\n\n6\n\nö ÷ ø\n\nAlternatively, the elements marked in boldface in the matrix A here:\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1 a 5 a\n\n9\n\na\n\n13\n\na\n\n2\n\na a\n\n6\n\n10\n\na\n\n14\n\na\n\n3\n\na\n\n7\n\na\n\n11\n\na\n\n15\n\na\n\n4\n\na\n\n8\n\na\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\nFrom the elements selected, a1, a2, a5 and a6, the max pooling operation selects the maximum value. The result is indicated with B1\n\nB 1\n\n=\n\ni\n\nmax = , 1 2 5 6 , ,\n\na i\n\nWe then need to shift our 2 × 2 window two columns to the right, typically the same number of columns the selected region has, and select the elements marked in bold:\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1\n\na\n\n5\n\na\n\n9\n\na\n\n13\n\na\n\n2\n\na\n\n6\n\na\n\n10\n\na\n\n14\n\na\n\n3\n\na a\n\n7\n\n11\n\na\n\n15\n\na\n\n4\n\na a\n\n8\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\n100",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nOr, in other words, the smaller matrix\n\næ ç è\n\na\n\na\n\n3\n\n7\n\na\n\na\n\n4\n\n8\n\nö ÷ ø\n\nThe max-pooling algorithm will then select the maximum of the values\n\nand give a result that we will indicate with B2\n\nB 2\n\n=\n\ni\n\nmax = , , 3 4 7 8 ,\n\na i\n\nAt this point we cannot shift the 2 × 2 region to the right anymore, so we shift it two rows down and start the process again from the left side of A, selecting the elements marked in bold and getting the maximum and calling it B3.\n\nA\n\n=\n\næ ç ç ç ç è\n\na 1\n\na\n\n5\n\na\n\n9\n\na\n\n13\n\na\n\n2\n\na\n\n6\n\na\n\n10\n\na\n\n14\n\na\n\n3\n\na\n\n7\n\na\n\n11\n\na\n\n15\n\na\n\n4\n\na\n\n8\n\na\n\n12\n\na\n\n16\n\nö ÷ ÷ ÷ ÷ ø\n\nThe stride s in this context has the same meaning we have already discussed in convolution. It’s simply the number of rows or columns you move your region when selecting the elements. Finally, we select the last region 2 × 2 in the bottom-lower part of A, selecting the elements a11, a12, a15, and a16. We then get the maximum and call it B4. With the values we obtain in this process, in the example the four values B1, B2, B3 and B4, we will build an output tensor:\n\nB\n\n=\n\næ ç è\n\nB 1 B 3\n\nB 2 B\n\n4\n\nö ÷ ø\n\n101",
      "content_length": 1170,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nIn the example, we have s = 2. Basically, this operation takes as input\n\na matrix A, a stride s, and a kernel size nK (the dimension of the region we selected in the example before) and returns a new matrix B with dimensions given by the same formula we discussed for convolution:\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\ns\n\nn\n\nK\n\n+\n\nú ûú1\n\nTo reiterate this idea, start from the top-left of matrix A, take a region of dimensions nK × nK, apply the max function to the selected elements, then shift the region of s elements toward the right, select a new region again of dimensions nK × nK, apply the function to its values, and so on. In Figure 3-11 you can see how you would select the elements from matrix A with stride s = 2.\n\nFigure 3-11. A visualization of pooling with stride s = 2\n\nFor example, applying max-pooling to the input A\n\nA =\n\næ ç ç ç ç è\n\n1\n\n3\n\n4\n\n5\n\n4\n\n1\n\n13 15\n\n5\n\n7\n\n11 3\n\n21 6 2 1\n\nö ÷ ÷ ÷ ÷ ø\n\n102",
      "content_length": 960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nWill get you this result (it’s very easy to verify it):\n\nB =\n\næ ç è\n\n4\n\n11\n\n15 21\n\nö ÷ ø\n\nSince four is the maximum of the values marked in bold.\n\nA =\n\næ ç ç ç ç è\n\n1\n\n3\n\n4 4\n\n5 1\n\n13 15\n\n5\n\n7\n\n11 3\n\n21 6\n\n1\n\n2\n\nö ÷ ÷ ÷ ÷ ø\n\nEleven is the maximum of the values marked in bold here:\n\nA =\n\næ ç ç ç ç è\n\n1\n\n3\n\n4\n\n5\n\n4\n\n1\n\n13 15\n\n5\n\n7\n\n11 3 21 6 2 1\n\nö ÷ ÷ ÷ ÷ ø\n\nAnd so on. It’s worth mentioning another way of doing pooling,\n\nalthough it’s not as widely used as max-pooling: average pooling. Instead of returning the maximum of the selected values, it returns the average.\n\nNote the most commonly used pooling operation is max pooling. average pooling is not as widely used but can be found in specific network architectures.\n\n103",
      "content_length": 786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nPadding\n\nSomething that’s worth mentioning here is padding. Sometimes, when dealing with images, it is not optimal to get a result from a convolution operation that has dimensions that are different from the original image. This is when padding is necessary. The idea is straightforward: you add rows of pixels on the top and bottom and columns of pixels on the right and left of the final images so the resulting matrices are the same size as the original. Some strategies fill the added pixels with zeros, with the values of the closest pixels and so on. For example, in our example, our ex_out matrix with zero padding would like like this\n\narray([[ 0., 0., 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 30., 30., 0., 0., 0.], [ 0., 0., 0., 30., 30., 0., 0., 0.], [ 0., 0., 0., 30., 30., 0., 0., 0.], [ 0., 0., 0., 30., 30., 0., 0., 0.], [ 0., 0., 0., 30., 30., 0., 0., 0.], [ 0., 0., 0., 30., 30., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0., 0., 0.]])\n\nOnly as a reference, in case you use padding p (the width of the rows\n\nand columns you use as padding), the final dimensions of the matrix B, in case of both convolution and pooling, is given by\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\n+\n\n2 p n s K\n\n+\n\nú 1 ûú\n\n104",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nNote when dealing with real images, you always have color images, coded in three channels: rGB. that means that convolution and pooling must be done in three dimensions: width, height, and color channel. this will add a layer of complexity to the algorithms.\n\nBuilding Blocks of a CNN\n\nConvolution and pooling operations are used to build the layers used in CNNs. In CNNs typically you can find the following layers\n\nConvolutional layers\n\nPooling layers\n\nFully connected layers\n\nFully connected layers are precisely what we have seen in all the previous chapters: a layer where neurons are connected to all neurons of previous and subsequent layers. You know them already. The other two require some additional explanation.\n\nConvolutional Layers\n\nA convolutional layer takes as input a tensor (which can be three- dimensional, due to the three color channels), for example an image, applies a certain number of kernels, typically 10, 16, or even more, adds a bias, applies ReLu activation functions (for example) to introduce non- linearity to the result of the convolution, and produces an output matrix B.\n\n105",
      "content_length": 1170,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nNow in the previous sections, I showed you some examples of applying\n\nconvolutions with just one kernel. How can you apply several kernels at the same time? Well, the answer is straightforward. The final tensor (I use now the word tensor since it will not be a simple matrix anymore) B will have not two dimensions but three. Let’s indicate the number of kernels you want to apply with nc (the c is used since sometimes people talk about channels). You simply apply each filter to the input independently and stack the results. So instead of a single matrix B with dimensions nB × nB you get a final tensor B of dimensions nB × nB × nc. That means that this\n\nB\n\n, i j\n\n1 ,\n\n\" Î[ , n 1 , i j\n\nB\n\n]\n\nWill be the output of convolution of the input image with the first\n\nkernel, and\n\nB\n\n, i j\n\n,\n\n2\n\n\" Î[ , n 1 , i j\n\nB\n\n]\n\nWill be the output of convolution with the second kernel, and so on. The convolution layer simply transforms the input into an output tensor. However, what are the weights in this layer? The weights, or the parameters that the network learns during the training phase, are the elements of the kernel themselves. We discussed that we have nc kernels, each of nK × nK dimensions. That means that we have n nK c layer.\n\n2\n\nparameter in a convolutional\n\nNote the number of parameters that you have in a convolutional layer, n nK c in reducing overfitting, especially when dealing with large input images.\n\n, is independent from the input image size. this fact helps\n\n2\n\n106",
      "content_length": 1550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nSometimes this layer is indicated with the word POOL and\n\nthen a number. In our case, we could indicate this layer with POOL1. In Figure 3-12, you can see a representation of a convolutional layer. The input image is transformed by applying convolution with nc kernels in a tensor of dimensions nA × nA × nc.\n\nFigure 3-12. A representation of a convolutional layer2\n\nOf course, a convolutional layer must not necessarily be placed immediately after the inputs. A convolutional layer may get as input the output of any other layer of course. Keep in mind that usually, the input image will have dimensions nA × nA × 3, since an image in color has three channels: Red, Green, and Blue. A complete analysis of the tensors involved in a CNN when considering color images is beyond the scope of this book. Very often in diagrams, the layer is simply indicated as a cube or a square.\n\n2 Cat image source: https://www.shutterstock.com/\n\n107",
      "content_length": 991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nPooling Layers\n\nA pooling layer is usually indicated with POOL and a number: for example, POOL1. It takes as input a tensor and gives as output another tensor after applying pooling to the input.\n\nNote a pooling layer has no parameter to learn, but it introduces additional hyperparameters: nK and stride v. typically, in pooling layers, you don't use any padding, since one of the reasons to use pooling is often to reduce the dimensionality of the tensors.\n\nStacking Layers Together\n\nIn CNNs you usually stack convolutional and pooling layer together. One after the other. In Figure 3-13, you can see a convolutional and a pooling layer stack. A convolutional layer is always followed by a pooling layer. Sometimes the two together are called a layer. The reason is that a pooling layer has no learnable weights and therefore it is merely seen as a simple operation that is associated with the convolutional layer. So be aware when you read papers or blogs and check what they intend.\n\nFigure 3-13. A representation of how to stack convolutional and pooling layers\n\n108",
      "content_length": 1129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nTo conclude this part of CNN in Figure 3-14, you can see an example of a CNN. In Figure 3-14, you see an example like the very famous LeNet-5 network, about which you can read more here: https://goo.gl/hM1kAL. You have the inputs, then two times convolution- pooling layer, then three fully connected layers, and then an output layers, with a softmax activation function to perform multiclass classification. I put some indicative numbers in the figure to give you an idea of the size of the different layers.\n\nFigure 3-14. A representation of a CNN similar to the famous LeNet-5 network\n\nNumber of Weights in a CNN\n\nIt is important to point out where the weights in a CNN are in the different layers.\n\nConvolutional Layer\n\nIn a convolutional layer, the parameters that are learned are the filters themselves. For example, if you have 32 filters, each of dimension 5x5, you will get 32x5x5=832 learnable parameters, since for each filter there is also a bias term that you will need to add. Note that this number is not dependent on the input image size. In a typical feed-forward neural network, the number of weights in the first layer is dependent on the input size, but not here.\n\n109",
      "content_length": 1246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nChapter 3\n\nFundamentals oF Convolutional neural networks\n\nThe number of weights in a convolutional layer is, in general terms,\n\ngiven by the following:\n\nn n n K\n\n×\n\n×\n\nC\n\nK\n\n+\n\nn C\n\nPooling Layer\n\nThe pooling layer has no learnable parameters, and as mentioned, this is the reason it’s typically associated with the convolutional layer. In this layer (operation), there are no learnable weights.\n\nDense Layer\n\nIn this layer, the weights are the ones you know from traditional feed- forward networks. So the number depends on the number of neurons and the number of neurons in the preceding and subsequent layers.\n\nNote the only layers in a Cnn that have learnable parameters are the convolutional and dense layers.\n\nExample of a CNN: MNIST Dataset\n\nLet’s start with some coding. We will develop a very simple CNN and try to do classification on the MNIST dataset. You should know the dataset very well by now, from Chapter 2.\n\nWe start, as usual, by importing the necessary packages:\n\nfrom keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D from keras.utils import np_utils\n\n110",
      "content_length": 1217,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nimport numpy as np import matplotlib.pyplot as plt\n\nWe need an additional step before we can start loading the data:\n\nfrom keras import backend as K K.set_image_dim_ordering('th')\n\nThe reason is the following. When you load images for your model, you\n\nwill need to convert them to tensors, each with three dimensions:\n\nNumber of pixels along the x-axis\n\nNumber of pixels along the y-axis\n\nNumber of color channels (in a gray image, this\n\nnumber is; if you have color images, this number is 3, one for each of the RGB channels)\n\nWhen doing convolution, Keras must know on which axis it finds the information. In particular, it is relevant to define if the index of the color channel’s dimension is the first or the last. To achieve this, we can define the ordering of the data with keras.backend.set_image_dim_ordering(). This function accepts as input a string that can assume two possible values:\n\n\n\n'th' (for the convention used by the library Theano): Theano expects the channel dimensions to be the second one (the first one will be the observation index).\n\n\n\n'tf' (for the convention used by TensorFlow): TensorFlow expects the channel dimension to be the last one.\n\nYou can use both, but simply pay attention when preparing the data to use the right convention. Otherwise, you will get error messages about tensor dimensions. In what follows, we will convert images in tensors having the color channel dimensions as the second one, as you can see later.\n\n111",
      "content_length": 1522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nNow we are ready to load the MNIST data with this code:\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nThe code will deliver the images “flattened,” meaning each image will be a one-dimensional vector of 784 elements (28x28). We need to reshape them as proper images, since our convolutional layers want images as inputs. After that, we need to normalize the data (remember the images are in a grayscale, and each pixel can have a value from 0 to 255).\n\nX_train = X_train.reshape(X_train.shape[0], 1, 28, 28). astype('float32') X_test = X_test.reshape(X_test.shape[0], 1, 28, 28). astype('float32') X_train = X_train / 255.0 X_test = X_test / 255.0\n\nNote how, since we have defined the ordering as 'th', the number of\n\nchannels (in this case 1) is the second element of the X arrays. As a next step, we need to one-hot-encode the labels:\n\ny_train = np_utils.to_categorical(y_train) y_test = np_utils.to_categorical(y_test)\n\nWe know we have 10 classes so we can simply define them:\n\nnum_classes = 10\n\nNow let’s define a function to create and compile our Keras model:\n\ndef baseline_model(): # create model model = Sequential() model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28),\n\nactivation='relu'))\n\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n112",
      "content_length": 1315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nmodel.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # Compile model model.compile(loss='categorical_crossentropy',\n\noptimizer='adam', metrics=['accuracy'])\n\nreturn model\n\nYou can see a diagram of this CNN in Figure 3-15.\n\nFigure 3-15. A diagram depicting the CNN we used in the text. The numbers are the dimensions of the tensors produced by each layer.\n\nTo determine what kind of model we have, we simply use the model.summary() call. Let’s first create a model and then check it:\n\nmodel = baseline_model() model.summary()\n\n113",
      "content_length": 672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nThe output (check the diagram form in Figure 3-15) is as follows:\n\n_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 24, 24) 832 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 32, 12, 12) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 32, 12, 12) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 4608) 0 _________________________________________________________________ dense_1 (Dense) (None, 128) 589952 _________________________________________________________________ dense_2 (Dense) (None, 10) 1290 ================================================================= Total params: 592,074 Trainable params: 592,074 Non-trainable params: 0\n\nIn case you are wondering why the max-pooling layer produces tensors\n\nof dimensions 12x12, the reason is that since we haven’t specified the stride, Keras will take as a standard value the dimension of the filter, which in our case is 2x2. Having input tensors that are 24x24 with stride 2 you will get tensors that are 12x12.\n\nThis network is quite simple. In the model we defined just one\n\nconvolutional and pooling layer, we added a bit of dropout, then we added a dense layer with 128 neurons and then an output layer for the softmax with 10 neurons. Now we can simply train it with the fit() method:\n\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=200, verbose=1)\n\n114",
      "content_length": 1703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nThis will train the network for only one epoch and should give you\n\noutput similar to this (your numbers may vary a bit):\n\nTrain on 60000 samples, validate on 10000 samples Epoch 1/1 60000/60000 [==============================] - 151s 3ms/step - loss: 0.0735 - acc: 0.9779 - val_loss: 0.0454 - val_acc: 0.9853\n\nWe have already reached good accuracy, without having any overfitting.\n\nNote when you pass to the compile() method the optimizer parameter, keras will use its standard parameters. if you want to change them, you need to define an optimizer separately. For example, to specify an adam optimizer with a starting learning rate of 0.001 you can use AdamOpt = adam(lr=0.001) and then pass it to the compile method with model. compile(optimizer=AdamOpt, loss='categorical_ crossentropy', metrics=['accuracy']).\n\nVisualization of CNN Learning Brief Digression: keras.backend.function( )\n\nSometime it’s useful to get intermediate results from a computational graph. For example, you may be interested in the output of a specific layer for debugging purposes. In low-level TensorFlow, you can simply evaluate in the session the relevant node in the graph, but it’s not so easy to understand how to do it in Keras. To find out, we need to consider what\n\n115",
      "content_length": 1316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nthe Keras backend is. The best way of explaining it is to cite the official documentation (https://keras.io/backend/):\n\nKeras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle low-level operations such as tensor products, convolutions, and so on itself. Instead, it relies on a specialized, well opti- mized tensor manipulation library to do so, serving as the “backend engine” of Keras.\n\nTo be complete, it is important to note that Keras uses (at the time of writing) three backends: the TensorFlow backend, the Theano backend, and the CNTK backend. When you want to write your own specific function, you should use the abstract Keras backend API that can be loaded with this code:\n\nfrom keras import backend as K\n\nUnderstanding how to use the Keras backend goes beyond the scope of this book (remember the focus of this book is not Keras), but I suggest you spend some time getting to know it. It may be very useful. For example, to reset a session when using Keras you can use this:\n\nkeras.backend.clear_session()\n\nWhat we are really interested in this chapter is a specific method\n\navailable in the backed: function(). Its arguments are as follows:\n\n\n\ninputs: List of placeholder tensors\n\noutputs: List of output tensors\n\nupdates: List of update ops\n\n\n\n**kwargs: Passed to tf.Session.run\n\n116",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nWe will use only the first two in this chapter. To understand how to use them, let’s consider for example the model we created in the previous sections:\n\nmodel = Sequential() model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu')) model.add(MaxPool2D(pool_size=(2, 2))) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(num_classes, activation='softmax'))\n\nHow do we get, for example, the output of the first convolutional layer?\n\nWe can do this easily by creating a function:\n\nget_1st_layer_output = K.function([model.layers[0]. input],[model.layers[0].output])\n\nThis will use the following arguments\n\n\n\ninputs: model.layers[0].input, which is the input of our network\n\noutputs: model.layers[0].output, which is the output\n\nof the first layer (with index 0)\n\nYou simply ask Keras to evaluate specific nodes in your computational\n\ngraph, given a specific set of inputs. Note that up to now we have only defined a function. Now we need to apply it to a specific dataset. For example, if we want to apply it to one single image, we could do this:\n\nlayer_conv_output = get_1st_layer_output([np.expand_dims(X_ test[21], axis=0)])[0]\n\n117",
      "content_length": 1261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nThis multidimensional array will have dimensions (1, 32, 24, 24) as expected: one image, 32 filters, 24x24 output. In the next section, we will use this function to see the effect of the learned filter in the network.\n\nEffect of Kernels\n\nIt is interesting to see what the effect of the learned kernels is on the input image. For this purpose, let’s take an image from the test dataset (if you shuffled your dataset, you may get a different digit at index 21).\n\ntst = X_test[21]\n\nNote how this array has dimensions (1,28,28). This is a six, as you can\n\nsee in Figure 3-16.\n\nFigure 3-16. The first image in the test dataset\n\n118",
      "content_length": 684,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nTo get the effect of the first layer (the convolutional one), we can use\n\nthe following code (explained in the previous section)\n\nget_1st_layer_output = K.function([model.layers[0]. input],[model.layers[0].output]) layer_conv_output = get_1st_layer_output([tst])[0]\n\nNote how the layer_conv_output is a multidimensional array, and it\n\nwill contain the convolution of the input image with each filter, stacked on top of each other. Its dimensions are (1,32,24,24). The first number is 1 since we applied the layer only to one single image, the second, 32, is the number of filters we have, and the second is 24 since, as we have discussed, the output tensor dimensions of a conv layer are given by\n\nn\n\nB\n\n=\n\nê ëê\n\nn\n\nA\n\n+\n\n2 p n s K\n\n+\n\nú 1 ûú\n\nMoreover, in our case\n\nnB =\n\nê ëê\n\n28 5 1\n\n+\n\nú ûú= 1\n\n24\n\n119",
      "content_length": 864,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-17. The test image (a 6) convoluted with the first 12 filters learned by the network\n\n120",
      "content_length": 156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nSince in our network, we have nA = 28, p = 0, nK = 5, and stride s = 1. In Figure 3-17, you can see our test image convoluted with the first 12 filters (32 was too many for a figure).\n\nFrom Figure 3-17 you can see how different filters learn to detect\n\ndifferent features. For example, the third filter (as can be seen in Figure 3-18) learned to detect diagonal lines.\n\nFigure 3-18. The test image convoluted with the third filter. It learned to detect diagonal lines.\n\nOther filters learn to detect horizontal lines or other features.\n\nEffect of Max-Pooling\n\nThe subsequent step is to apply max pooling to the output of the convolutional layer. As we discussed, this will reduce the dimensions of the tensors and will try to (intuitively) condense the relevant information.\n\nYou can see the output on the tensor coming from the first 12 filters in\n\nFigure 3-19.\n\n121",
      "content_length": 925,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nFigure 3-19. The output of the pooling layer when applied to the first 12 tensors coming from the convolutional layer\n\n122",
      "content_length": 180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "Chapter 3\n\nFundamentals oF Convolutional neural networks\n\nLet’s see how our test image is transformed from one convolutional\n\nand the pooling layer by one of the filters (consider the third, just for illustration purposes). You can easily see the effect in Figure 3-20.\n\nFigure 3-20. The original test image as in the dataset (in panel a); the image convoluted with the third learned filter (panel b); the image convoluted with the third filter after the max pooling layer (panel c)\n\nNote how the resolution of the image is changing, since we are not using any padding. In the next chapter, we will look at more complicated architectures, called inception networks, that are known to work better than traditional CNN (what we have described in this chapter) when dealing with images. In fact, simply adding more and more convolutional layers will not easily increase the accuracy of the predictions, and more complex architecture are known to be much more effective.\n\nNow that we have seen the very basic building blocks of a CNN, we are ready to move to some more advanced topics. In the next chapter, we will look at many exciting topics as inception networks, multiple loss functions, custom loss functions, and transfer learning.\n\n123",
      "content_length": 1238,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "CHAPTER 4\n\nAdvanced CNNs and Transfer Learning\n\nIn this chapter, we look at more advanced techniques typically used when developing CNNs. In particular, we will look at a very successful new convolutional network called the inception network that is based on the idea of several convolutional operations done in parallel instead of sequentially. We will then look at how to use the multiple cost function, in a similar fashion as what is done in multi-task learning. The next sections will show you how to use the pre-trained network that Keras makes available, and how to use transfer learning to tune those pre-trained networks for your specific problem. At the end of the chapter, we will look at a technique to implement transfer learning that is very efficient when dealing with big datasets.\n\nConvolution with Multiple Channels\n\nIn the previous chapter, you learned how convolution works. In the examples we have explicitly described how to perform it when the input is a bi-dimensional matrix. But this is not what happens in reality. For example, the input tensors may represent color images, and therefore will have three dimensions: the number of pixels in the x direction (resolution along the x axis), the number of pixels in the y direction (resolution along\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_4\n\n125",
      "content_length": 1394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nthe y axis), and the number of color channels, that is three when dealing with RGB images (one channel for the reds, one for the greens, and one for the blues). It can be even worse. A convolutional layer with 32 kernels, each 5 × 5, when expecting an input of images each 28 × 28 (see the MNIST example in the previous chapter) will have an output of dimensions (m, 32, 24, 24), where m is the number of training images. That means that our convolutions will have to be done with tensors with dimensions of 32 × 24 × 24. So how we can perform the convolutional operation on three-dimensional tensors? Well, it is actually quite easy. Mathematically speaking, if kernel K has dimensions nK × nK × nc, and the input tensors A have dimensions nx × ny × nc, the result of our convolution operation will be:\n\nn\n\nx\n\nn\n\ny\n\nn c\n\nååå\n\nK A ijk ijk\n\n= 1\n\nj\n\n= 1\n\nk\n\n= 1\n\nMeaning that we will sum over the channel dimension. In Keras, when\n\nyou define a convolutional layer in 2D, you use the following code:\n\nConv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu')\n\nWhere the first number (32) is the number of filters and (5,5) defines\n\nthe dimensions of the kernels. What Keras does not tell you is that it automatically takes kernels of nc × 5 × 5 where nc is the number of channels of the input tensors. This is why you need to give the first layer the input_ shape parameter. The number of channels is included in this information. But which of the three numbers is the correct one? How can Keras know that the right one is 1 in this case and not a 28?\n\nLet’s look at the concrete example we looked in the previous chapter\n\nmore in depth. Let’s suppose we import the MNIST dataset with this code:\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n126",
      "content_length": 1809,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nIn the previous chapter, we reshaped the input tensors with\n\nX_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n\nAs you will notice, we added a dimension of 1 before the x and y dimensions of 28. The 1 is the number of channels in the image: since it’s a grayscale image, it has only one channel. But we could have added the number of channels also after the x and y dimensions of 28. That was our choice. We can tell Keras which dimension to take with the code that we discussed in Chapter 3:\n\nK.set_image_dim_ordering('th')\n\nThis line is important, since Keras needs to know which one is the channel dimension in order to be able to extract the right channel dimension for the convolutional operation. Remember that for the kernels we specify only x and y dimensions, so Keras needs to find the third dimension by itself: in this case a 1. You will remember that a value of 'th' will expect the channel dimension to come before the x, y dimensions, while a value of 'tf' will expect the channel dimension to be the last one. So, it is just a matter of being consistent. You tell Keras with the code above, where the channel dimension is and then reshape your data accordingly. Let’s consider a few additional examples to make the concept even clearer.\n\nLet’s suppose we consider the following network with set_image_\n\ndim_ordering('th') (we will neglect the dimension for the number of observations m) when using MNIST images as the input:\n\nInput tensors shape: 1×28×28 Convolutional Layer 1 with 32 kernels, each 5×5: output shape 32×24×24 Convolutional Layer 2 with 16 kernels, each 3×3: output shape 16×22×22\n\n127",
      "content_length": 1688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nThe kernels in the second convolutional layer will have dimensions of 32 × 3 × 3. The number of channels coming from the first convolutional layer (32) do not play a role in determining the dimensions of the output of the second convolutional layer, since we sum over that dimension. In fact, if we change the number of kernels in the first layer to 128, we get the following dimensions:\n\nInput tensors shape: 1×28×28 Convolutional Layer 1 with 32 kernels, each 5×5: output shape 128×24×24 Convolutional Layer 2 with 16 kernels, each 3×3: output shape 16×22×22\n\nAs you can see, the output dimensions of the second layer have not\n\nchanged at all.\n\nNote Keras infers automatically the channel dimensions when creating the filters, so you need to tell Keras which one is the right dimension with set_image_dim_ordering() and then reshape your data accordingly.\n\nWHY A 1 × 1 CONVOLUTION REDUCES DIMENSIONALITY\n\nin this chapter we will look at inception networks, and we will use 1 × 1 kernels, with the reasonsing that those reduce dimensionality. at first it seems counter-intuitive, but you need to remember from the previous section discussion, that a filter always has a third dimension. Consider the following set of layers:\n\nInput tensors shape: 1 × 28 × 28\n\nConvolutional Layer 1 with 32 kernels, each 5 × 5: output shape\n\n128 × 24 × 24\n\n128",
      "content_length": 1392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nConvolutional Layer 2 with 16 kernels, each 1 × 1: output shape\n\n16 × 24 × 24\n\nnote how the layer with 1 × 1 kernels reduces the dimensions of the previous layer. it changes the dimensions from 128 × 24 × 24 to 16 × 24 × 24. a 1 × 1 kernel will not change the x, y dimensions of the tensors but it will change the channel dimension. this is the reason why, if you read blogs or books on inception networks, you will read that those kernels are used to reduce dimensions of the tensors used.\n\nKernels 1 × 1 does not change the x, y dimensions of tensors, but will change the channel dimension. this is why they are often used to reduce dimensionality of the tensors flowing through a network.\n\nHistory and Basics of Inception Networks\n\nInception networks were first proposed in a famous paper by Szegedy et al. titled Going Deeper with Convolutions.1 This new architecture that we will discuss in detail is the result of the efforts to get better results in image recognition tasks without increasing the computational budget.2 Adding more and more layers will create models with more and more parameters that will be increasingly difficult and slow to train. Additionally the authors wanted to find methods that could be used on machines that may not be as powerful as the ones used in big data centers. As they state in the paper, their models were designed to keep a “computational budget of 1.5 billion multiply-adds at inference time”. It is important that inference is cheap, because then it can be done on devices that are not that powerful; for example, on mobile phones.\n\n1 The original paper can be accessed on the arXiv archive at this link: http://toe.lt/4. 2 With computational budget we determine the time and hardware resources needed to perform a specific computation (for example, training a network).\n\n129",
      "content_length": 1870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nNote that the goal of this chapter is not to analyze the entire original paper on inception networks, but to explain the new building blocks and techniques that have been used and show you how to use them in your projects. To develop inception networks, we will need to start using the functional Keras APIs, work with multiple loss functions, and perform operations on the dataset with layers that are evaluated in parallel and not sequentially. We will also not look at all variants of the architecture, because that would simply require us to list the results of a few papers and will not bring any additional value to the reader (that is better served by reading the original papers). If you are interested, the best advice I can give you is to download it and study the original paper. You will find lots of interesting information in there. But at the end of this chapter, you will have the tools to really understand those new networks and will be able to develop one with Keras.\n\nLet’s go back to “classical” CNNs. Typically, those have a standard structure: stacked convolutional layers (with pooling of course) followed by a set of dense layers. It is very tempting to just increase the number of layers or the number of kernels or their size to try to get a better result. This leads to overfitting issues and therefore requires heavy use of regularization techniques (like dropout) to try to counter this problem. Bigger sizes (both in the number of layers and kernel size and numbers) mean of course a larger number of parameters and therefore the need of increasingly high computational resources. To summarize, some of the main problems of “classical” CNNs are as follows:\n\n\n\nIt is very difficult to get the right kernel size. Each image is different. Typically, larger kernels are good for more globally distributed information, and smaller ones for locally distributed information.\n\nDeep CNNs are prone to overfitting.\n\nTraining and inference of networks with many parameters is computationally intensive.\n\n130",
      "content_length": 2075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nInception Module: Naïve Version\n\nTo overcome these difficulties, the main idea of Szegedy and the co- authors of the paper is to perform convolution with multiple-size kernels in parallel, to be able to detect features at different sizes at the same time, instead of adding convolutional layer after layer sequentially. Those kinds of networks are said to be going wider instead of deeper.\n\nFor example, we may do convolution with 1 × 1, 3 × 3 and 5 × 5 kernels,\n\nand even max pooling at the same time in parallel, instead of adding several convolutional layers, one after the other. In Figure 4-1, you can see how the different convolutions can be done in parallel in what is called the naïve inception module.\n\nFigure 4-1. Different convolutions with different kernel sizes done in parallel. This is the basic module used in inception networks called the inception module.\n\nIn the example in Figure 4-1, the 1 × 1 kernel will look at very localized information, while the 5 × 5 will be able to spot more global features. In the next section, we will look at how we can develop exactly that with Keras.\n\n131",
      "content_length": 1156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nNumber of Parameters in the Naïve Inception Module\n\nLet’s look at the difference in number of parameters between inception and classical CNNs. Let’s suppose we consider the example in Figure 4- 1. Let’s suppose the “previous layer” is the input layer with the MNIST dataset. For the sake of this comparison, we will use 32 kernels for all layers or convolutional operations. The number of parameters for each convolution operation in the naïve inception module is\n\n\n\n1 × 1 convolutions: 64 parameters3\n\n\n\n3 × 3 convolutions: 320 parameters\n\n\n\n5 × 5 convolutions: 832 parameters\n\nRemember that the max-pooling operations have no learnable parameters. In total, we have 1216 learnable parameters. Now let’s suppose we create a network with the three convolutional layers, one after the other. The first one with 32 1 × 1 kernels, then one with 32 3 × 3 kernels, and finally one with 32 5 × 5 kernels. Now the total number of parameters in the layers will be (remember that, for example, the convolutional layer with the 32 3 × 3 kernels will have as input the output of the convolutional layer with the 32 1 × 1 kernels):\n\nLayer with 1 × 1 convolutions: 64 parameters\n\nLayer with 3 × 3 convolutions: 9248 parameters\n\nLayer with 5 × 5 convolutions: 25632 parameters\n\nFor a total of 34944 learnable parameters. Roughly 30 times the\n\nnumber of the parameters as the inception version. You can easily see how such parallel processing reduces drastically the number of parameters that the model must learn.\n\n3 Remember in this case we have one weight and one bias.\n\n132",
      "content_length": 1610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nInception Module with Dimension Reduction\n\nIn the naïve inception module, we get a smaller number of learnable parameters with respect to classical CNNs, but we can actually do even better. We can use 1 × 1 convolutions at the right places (mainly before the higher dimension convolutions) to reduce dimensions. This allows us to use an increasing number of such modules without blowing up the computational budget. In Figure 4-2, you can see how such a module could look.\n\nFigure 4-2. Inception module example with dimension reduction\n\nIt is instructive to see how many learnable parameters we have in this module. To see where the dimensionality reduction really helps, let’s suppose that the previous layer is the output of a previous operation and that its output has the dimensions of 256, 28, 28. Now let’s compare the naïve module and the one with dimension reduction pictured in Figure 4- 2.\n\nNaïve module:\n\n\n\n1 × 1 convolutions with 8 kernels: 2056 parameters4\n\n\n\n3 × 3 convolutions with 8 kernels: 18440 parameters\n\n\n\n5 × 5 convolutions with 8 kernels: 51208 parameters\n\n4 Remember in this case we have one weight and one bias.\n\n133",
      "content_length": 1190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nFor a total of 71704 learnable parameters. Module with dimension reduction:\n\n\n\n1 × 1 convolutions with 8 kernels: 2056 parameters\n\n\n\n1 × 1 followed by the 3 × 3 convolutions: 2640 parameters\n\n\n\n1 × 1 followed by the 5 × 5 convolutions: 3664 parameters\n\n\n\n3 × 3 max pooling followed by the 1 × 1 convolutions: 2056 parameters\n\nFor a total of 10416 learnable parameters. Comparing the number of learnable parameters, you can see why this module is said to reduce dimensions. Thanks to a smart placement of 1 × 1 convolutions, we can prevent the number of learnable parameters from blowing up without control.\n\nAn inception network is simply built by stacking lots of those modules\n\none after the other.\n\nMultiple Cost Functions: GoogLeNet\n\nIn Figure 4-3, you can see the main structure of the GoogLeNet network that won the imagenet challenge. This network, as described in the paper referenced at the beginning, stacks several inception models one after the other. The problem is, as the authors of the original paper quickly discovered, the middle layers tend to “die”. Meaning they tend to stop playing any role in the learning. To keep them from “dying,” the authors introduced classifiers along the network, as depicted in Figure 4-3.\n\nEach part of the network (PART 1, PART 2, and PART 3 in Figure 4-3) will be trained as a stand-alone classifier. The training of the three parts does not happen independently, but at the same time, in a very similar way to what happens in multi-task learning (MTL).\n\n134",
      "content_length": 1557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nFigure 4-3. The high-level architecture of the GoogLeNet network\n\nTo prevent the middle part of the network from not being so effective and effectively dying out, the authors introduced two classifiers along the network, indicated in Figure 4-3 with the yellow boxes. They introduced two intermediate loss functions and then computed the total loss function as a weighted sum of the auxiliary losses, effectively using a total loss evaluated with this formula:\n\nTotal Loss = Cost Function 1 + 0.3 * (Cost Function 2) + 0.3 * (Cost Function 3)\n\nWhere Cost Function 1 is the cost function evaluated with PART 1, Cost Function 2 is evaluated with PART 2, and Cost Function 3 with PART 3. Testing has shown that this is quite effective and you get a much better result than simply training the entire network as a single classifier. Of course, the auxiliary losses are used only in training and not during inference.\n\nThe authors have developed several versions of inception networks, with increasingly complex modules. If you are interested, you should read the original papers as they are very instructive. A second paper with more a complex architecture by the authors can be found at https://arxiv. org/pdf/1512.00567v3.pdf.\n\n135",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nExample of Inception Modules in Keras\n\nUsing the functional APIs of Keras makes building an inception module extremely easy. Let’s look at the necessary code. For space reasons, we will not build a complete model with a dataset, because that would take up too much space and would distract from the main learning goal, which is to see how to use Keras to build a network with layers that are evaluated in parallel instead of sequentially.\n\nLet’s suppose for the sake of this example that our training dataset is the CIFAR10.5 This is made of images, all 32 × 32 with three channels (the images are in color). So first we need to define the input layer of our network:\n\nfrom keras.layers import Input input_img = Input(shape = (32, 32, 3))\n\nThen we simply define one layer after the other:\n\nfrom keras.layers import Conv2D, MaxPooling2D tower_1 = Conv2D(64, (1,1), padding='same', activation='relu') (input_img) tower_1 = Conv2D(64, (3,3), padding='same', activation='relu') (tower_1) tower_2 = Conv2D(64, (1,1), padding='same', activation='relu') (input_img) tower_2 = Conv2D(64, (5,5), padding='same', activation='relu') (tower_2) tower_3 = MaxPooling2D((3,3), strides=(1,1), padding='same') (input_img) tower_3 = Conv2D(64, (1,1), padding='same', activation='relu') (tower_3)\n\n5 You can find all information on the dataset at https://www.cs.toronto. edu/~kriz/cifar.html.\n\n136",
      "content_length": 1426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nThis code will build the module depicted in Figure 4-4. The Keras\n\nfunctional APIs are easy to use: you define the layers as functions of another layer. Each function returns a tensor of the appropriate dimensions. The nice thing is that you don’t have to worry about dimensions; you can simply define layer after layer. Just take care to use the right one for the input. For example, with this line:\n\ntower_1 = Conv2D(64, (1,1), padding='same', activation='relu') (input_img)\n\nYou define a tensor, named tower_1, that is evaluated after a\n\nconvolutional operation with the input_img tensor and 64 1 × 1 kernels. Then this line:\n\ntower_1 = Conv2D(64, (3,3), padding='same', activation='relu') (tower_1)\n\nDefines a new tensor that is obtained by the convolution of 64 3 × 3\n\nkernels with the output of the previous line. We have taken the input tensor, performed convolution with 64 1 × 1 kernels, and then performed convolution with 64 3 × 3 kernels again.\n\nFigure 4-4. The inception module built from the given code\n\n137",
      "content_length": 1069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nThe concatenation of the layers is easy:\n\nfrom keras.layers import concatenate from tensorflow.keras import optimizers output = concatenate([tower_1, tower_2, tower_3], axis = 3)\n\nNow let’s add the couple of necessary dense layers:\n\nfrom keras.layers import Flatten, Dense output = Flatten()(output) out = Dense(10, activation='softmax')(output)\n\nThen we finally create the model:\n\nfrom keras.models import Model model = Model(inputs = input_img, outputs = out)\n\nThis model can then be compiled and trained as usual. An example of\n\nusage could be\n\nepochs = 50 model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(), metrics=['accuracy']) model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n\nSupposing the training dataset is composed by the arrays (X_train\n\nand y_train) and the validation dataset by (X_test, y_test).\n\nNote have to use the padding='same' option, since all the outputs of the convolutional operations must have the same dimensions.\n\nin all convolutional operations in the inception module, you\n\nThis section gave you a brief introduction to how to develop more complex network architectures using the functional APIs of Keras. You should now have a basic understanding of how inception networks work and their basic building blocks.\n\n138",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nDigression: Custom Losses in Keras\n\nSometimes it is useful to be able to develop custom losses in Keras. From the official Keras documentation (https://keras.io/losses/):\n\nYou can either pass the name of an existing loss function or pass a TensorFlow/Theano symbolic function that returns a scalar for each data point and takes the following two arguments:\n\ny_true: True labels. TensorFlow/Theano tensor.\n\ny_pred: Predictions. TensorFlow/Theano tensor of the same shape as y_true.\n\nLet’s suppose we want to define a loss that calculates the average of the\n\npredictions. We would need to write this\n\nimport keras.backend as K def mean_predictions(y_true, y_pred): return K.mean(y_pred)\n\nAnd then we can simply use it in the compile call as follows:\n\nmodel.compile(optimizer='rmsprop', loss=mean_predictions, metrics=['accuracy'])\n\nAlthough this would not make so much sense as a loss. Now this starts\n\nto get interesting the moment where the loss function can be evaluated only using intermediate results from specific layers. But to do that, we need to use a small trick. Since, as per official documentation, the function can only accept as input true labels and predictions. To do this we need to create a function that return a function that accepts only the true labels and the predictions. Seems convoluted? Let’s look at an example to understand it. Let’s suppose we have this model:\n\n139",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\ninputs = Input(shape=(512,)) x1 = Dense(128, activation=sigmoid)(inputs) x2 = Dense(64, activation=sigmoid)(x1) predictions = Dense(10, activation='softmax')(x2) model = Model(inputs=inputs, outputs=predictions)\n\nWe can define a loss function that depends on x1 with this code6 (what\n\nthe loss is doing is not relevant):\n\ndef custom_loss(layer): def loss(y_true,y_pred): return K.mean(K.square(y_pred - y_true) +\n\nK.square(layer), axis=-1)\n\nreturn loss\n\nThen we can simply use the loss function as before:\n\nmodel.compile(optimizer='adam', loss=custom_loss(x1), metrics=['accuracy'])\n\nThis is an easy way to develop and use custom losses. It is also sometimes useful to be able to train a model with multiple losses, as described in the inception networks. Keras is ready for this. Once you define the loss functions you can use the following syntax\n\nmodel.compile(loss = [loss1,loss2], loss_weights = [l1,l2], ...)\n\nand Keras will then use as loss function\n\nl1*loss1+l2*loss2\n\n6 The code was inspired by http://toe.lt/7.\n\n140",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nConsider that each loss will only affect the weights that are on the path between the inputs and the loss functions. In Figure 4-5, you can see a network divided in different parts: A, B, and C. loss1 is calculated using the output of B, and loss2 of C. Therefore, loss1 will only affect the weights in A and B, while loss2 will affect weights in A, B and C, as you can see in Figure 4-5.\n\nFigure 4-5. A schematic representation of the influence of multiple loss functions on different network parts\n\nAs a side note, this technique is heavily used in what is called multi-\n\ntask learning (MTL).7\n\nHow To Use Pre-Trained Networks\n\nKeras makes pre-trained deep learning models available for you to use. The models, called applications, can be used for predictions on new data. The models have already been trained on big datasets, so there is no need for big datasets or long training sessions. You can find all applications information on the official documentation at https://keras.io/ applications/. At the moment of writing there are 20 models available, each a variation of one of the following:\n\nXception\n\nVGG16\n\n7 You can find more information at https://en.wikipedia.org/wiki/ Multi-task_learning\n\n141",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nVGG19\n\nResNet\n\nResNetV2\n\nResNeXt\n\n\n\nInceptionV3\n\n\n\nInceptionResNetV2\n\nMobileNet\n\nMobileNetV2\n\nDenseNEt\n\nNASNet\n\nLet’s look at one example, and while doing so, let’s discuss the different\n\nparameters used in the functions. The pre-ready models are all in the keras.applications package. Each model has its own package. For example, ResNet50 is in the keras.applications.resnet50. Let’s suppose we have one image we want to classify. We may use the VGG16 network, a well known network that is very successful in image recognition. We can start with the following code\n\nimport tensorflow as tf from tensorflow.keras.applications.vgg16 import VGG16 from tensorflow.keras.preprocessing import image from tensorflow.keras.applications.vgg16 import preprocess_ input , decode_predictions\n\nimport numpy as np\n\nThen we can simply load the model with a simple line\n\nmodel = VGG16(weights='imagenet')\n\n142",
      "content_length": 942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nThe weights parameter is very important. If weights is None the weights are randomly initialized. That means that you get the VGG16 architecture and you can train it yourself. But be aware, it has roughly 138 million parameters, so you will need a really big training dataset and lots of patience (and a really powerful hardware). If you use the value imagenet, the weights are the ones obtained by training the network with the imagenet dataset.8 If you want a pre-trained network, you should use weights = 'imagenet'.\n\nIf you get an error message about certificates and you are on a Mac, there is an easy solution. The command above will try to download the weights over SSL and, if you just installed Python from python.org, the installed certificates will not work on your machine. Simply open a Finder window, navigate to the Applications/Python 3.7 (or the Python version you have installed), and double-click Install Certificates.command. A Terminal window will open, and a script will run. After that, the VGG16() call will work without an error message.\n\nAfter that, we need to tell Keras where the image is (let’s suppose you\n\nhave it in the folder where the Jupyter Notebook is) and load it:\n\nimg_path = 'elephant.jpg' img = image.load_img(img_path, target_size = (224, 224))\n\nYou can find the image in the GitHub repository in the folder for\n\nChapter 4. After that we need\n\nx = image.img_to_array(img) x = np.expand_dims(x, axis=0) x = preprocess_input(x)\n\nFirst, you convert the image to an array, then you need to expand its dimensions. What is meant is the following: the model works with batches of images, meaning it will expect as input a tensor with four axes (index\n\n8 http://www.image-net.org\n\n143",
      "content_length": 1766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nin the batch of images, resolution along x, resolution along y, number of channels). But our image has only three dimensions, the horizontal and vertical resolutions and the number of channels (in our example three, for the RGB channels). We need to add one dimension for the samples dimension. To be more concrete, our image has dimensions (224,244,3), but the model expects a tensor of dimensions (1,224,224,3), so we need to add the first dimension.\n\nThis can be done with the numpy function expand_dims(), which simply inserts a new axis in the tensor.9 As a last step, you need to pre- process the input image, since each model expects something slightly different (normalized between +1 and -1, or between 0 and 1, and so on) with the preprocess_input(x) call.\n\nNow we are ready to let the model predict the class of the image with\n\nthe following:\n\npreds = model.predict(x)\n\nTo get the top three classes of the prediction, we can use the decode_\n\npredictions() function.\n\nprint('Predicted:', decode_predictions(preds, top=3)[0])\n\nIt will produce (with our image) the following predictions:\n\nPredicted: [('n02504013', 'Indian_elephant', 0.7278206), ('n02504458', 'African_elephant', 0.14308284), ('n01871265', 'tusker', 0.12798567)]\n\nThe decode_predictions() returns tuples in the form (class_name, class_description, score). The first cryptic string is the internal class name, the second is the description (what we are interested in), and the last one is the probability. It seems our image, according to the VGG16 network, is with 72.8% probability an Indian elephant. I am not an expert\n\n9 You can check the official documentation for the function at http://toe.lt/5.\n\n144",
      "content_length": 1730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\non elephants, but I will trust the model. To use a different pre-trained network (for example ResNet50), you need to change the following imports:\n\nfrom keras.applications.resnet50 import ResNet50 from keras.applications.resnet50 import preprocess_input, decode_predictions\n\nAnd the way you define the model:\n\nmodel = ResNet50(weights='imagenet')\n\nThe rest of the code remains the same.\n\nTransfer Learning: An Introduction\n\nTransfer learning is a technique where a model trained to solve a specific problem is re-purposed10 for a new challenge related to the first problem. Let’s suppose we have a network with many layers. In image recognition typically, the first layers will learn to detect generic features, and the last layers will be able to detect more specific ones.11 Remember that in a classification problem the last layer will have N softmax neurons (assuming we are classifiying N classes), and therefore must learn to be very specific to your problem. You can intuitively understand transfer learning with the following steps, where we introduce some notation we will use in the next sections and chapters. Let’s suppose we have a network with nL layers.\n\n1. We train a base network (or get a pre-trained model) on a big dataset (called a base dataset) related to our\n\n10 The term has been used by Yosinki in https://arxiv.org/abs/1411.1792. 11 You can find a very interesting paper on the subject by Yosinki et al. at https://\n\narxiv.org/abs/1411.1792.\n\n145",
      "content_length": 1520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nproblem. For example, if we want to classify dogs’ images, we may train a model in this step on the imagenet dataset (since we basically want to classify images). It is important that, at this step, the dataset has enough data and that the task is related to the problem we want to solve. Getting a network trained for speech recognition will not be good at dog images classification. This network will probably not be that good for your specific problem.\n\n2. We get a new dataset that we call a target dataset (for\n\nexample, dogs’ breeds images) that will be our new training dataset. Typically, this dataset will be much smaller than the one used in Step 1.\n\n3. You then train a new network, called target network,\n\non the target dataset. The target network will typically have the same first nk (with nk < nL) layers of our base network. The learnable parameters of the first layers (let’s say 1 to nk, with nk < nL) are inherited from the base network trained in Step 1, and are not changed during the training of the target network. Only the last and new layers (in our example from layer nK to nL) are trained. The idea is that layers from 1 to nk (from the base network) will learn enough features in Step 1 to distinguish dogs from other animals, and the layers nk to nL (in the target network) will learn the features needed to distinguish different breeds. Sometimes you can even train your entire target network using the weights inherited from the base network as the initial values of the weights, although this requires much more powerful hardware.\n\n146",
      "content_length": 1615,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nNote layers inherited from the base network frozen, since otherwise it’s very easy to overfit the small dataset.\n\nif the target dataset is small, the best strategy is to keep the\n\nThe idea behind this is that you hope that in Step 1, the base network\n\nhas learned to extract generic features from images well enough and therefore you want to use this learned knowledge and avoid the need to learn it again. But to make predictions better, you want to fine-tune the predictions of your network for your specific case, optimizing how your target network extracts specific features (that typically happens in the last layers of a network) that are related to your problem.\n\nIn other words, you can think it this way. To recognize dog breeds, you\n\nimplicitly follow these steps:\n\n1. You look at an image and decide if it’s a dog or not.\n\n2.\n\nIf you are looking at a dog, you classify it into broad classes (for example, terrier).\n\n3. After that, you classify into sub-classes (for example,\n\na Welsh terrier or Tibetan terrier).\n\nTransfer learning is based on the idea that Steps 1 and possibly 2 can\n\nbe learned from a lot of generic images (for example from the imagenet dataset) from a base network, and that Step 3 can be learned by a much smaller dataset with the help of what has been learned in Step 1 and 2. When the target dataset is much smaller than the base dataset, this is a very powerful tool that will help avoiding overfitting of your training dataset.\n\nThis method is very useful when used with pre-trained models. For\n\nexample, using a VGG16 network trained on imagenet, and then re- training just the last layers is typically an extremely efficient way to solve specific image recognition problems. You get lots of features detection\n\n147",
      "content_length": 1801,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\ncapabilities for free. Keep in mind that training such networks on the imagenet networks costs several thousands of GPU hours. It’s typically not doable for researchers without the needed hardware and know-how. In the next sections, we will look at how to do exactly that. With Keras, it’s really easy and it will allow you to solve image classification problems with an accuracy that would not otherwise be possible. In Figure 4-6, you can see a schematic representation of the transfer learning process.\n\nFigure 4-6. A schematic representation of the transfer learning process\n\n148",
      "content_length": 631,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nA Dog and Cat Problem\n\nThe best way to understand how transfer learning works in practice is to try it in practice. Our goal is to be able to classify images of dogs and cats as best as we can, with the least effort (in computational resources) as possible. To do that, we will use the dataset with dog and cat images that you can find on Kaggle at https://www.kaggle.com/c/dogs-vs-cats. Warning: The download is almost 800MB. In Figure 4-7, you can see some of the images we will have to classify.\n\nFigure 4-7. Random samples of the images contained in the dog versus cat dataset\n\n149",
      "content_length": 633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nClassical Approach to Transfer Learning\n\nThe naïve way of solving this problem is to create a CNN model and train it with the images. First of all, we need to load the images and resize them to make sure they all have the same resolution. If you check the images in the dataset, you will notice that each has a different resolution. To do that let’s resize all the images to (150, 150) pixels. In Python, we would use this:\n\nimport glob import numpy as np import os\n\nimg_res = (150, 150)\n\ntrain_files = glob.glob('training_data/*') train_imgs = [img_to_array(load_img(img, target_size=img_res)) for img in train_files] train_imgs = np.array(train_imgs) train_labels = [fn.split('/')[1].split('.')[0].strip() for fn in train_files]\n\nvalidation_files = glob.glob('validation_data/*') validation_imgs = [img_to_array(load_img(img, target_size=img_ res)) for img in validation_files] validation_imgs = np.array(validation_imgs) validation_labels = [fn.split('/')[1].split('.')[0].strip() for fn in validation_files]\n\nSupposing we have 3000 training images in a folder called training_ data and 1000 validation images in a folder called validation_data, the shapes of the train_imgs and validation_imgs will be as follows:\n\n(3000, 150, 150, 3) (1000, 150, 150, 3)\n\n150",
      "content_length": 1311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nAs usual we will need to normalize the images. Each pixel now has a value between 0 and 255 and is an integer. So first we convert the numbers to floating point, and then we normalize them by dividing by 255, so that each value is now between 0 and 1.\n\ntrain_imgs_scaled = train_imgs.astype('float32') validation_imgs_scaled = validation_imgs.astype('float32') train_imgs_scaled /= 255 validation_imgs_scaled /= 255\n\nIf you check the train_labels you will see that they are strings: 'dog'\n\nor 'cat'. We need to transform the labels to integers, in particular into 0 and 1. To do that, we can use the Keras function called LabelEncoder.\n\nfrom sklearn.preprocessing import LabelEncoder le = LabelEncoder() le.fit(train_labels) train_labels_enc = le.transform(train_labels) validation_labels_enc = le.transform(validation_labels)\n\nWe can check the labels with this code:\n\nprint(train_labels[10:15], train_labels_enc[10:15])\n\nWhich will give this:\n\n['cat', 'dog', 'cat', 'cat', 'dog'] [0 1 0 0 1]\n\nNow we are ready to build our model. We can do this easily with the\n\nfollowing code:\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout from tensorflow.keras.models import Sequential from tensorflow.keras import optimizers\n\n151",
      "content_length": 1301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nmodel = Sequential()\n\nmodel.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) model.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten()) model.add(Dense(512, activation='relu')) model.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(), metrics=['accuracy'])\n\n152",
      "content_length": 613,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nThis is a small network that has this structure:\n\nLayer (type) Output Shape Param # ============================================================== conv2d_3 (Conv2D) (None, 148, 148, 16) 448 ______________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 74, 74, 16) 0 ______________________________________________________________ conv2d_4 (Conv2D) (None, 72, 72, 64) 9280 ______________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 36, 36, 64) 0 ______________________________________________________________ conv2d_5 (Conv2D) (None, 34, 34, 128) 73856 ______________________________________________________________ max_pooling2d_5 (MaxPooling2 (None, 17, 17, 128) 0 ______________________________________________________________ flatten_1 (Flatten) (None, 36992) 0 ______________________________________________________________ dense_2 (Dense) (None, 512) 18940416 ______________________________________________________________ dense_3 (Dense) (None, 1) 513 ============================================================== Total params: 19,024,513 Trainable params: 19,024,513 Non-trainable params: 0 ______________________________________________________________\n\nIn Figure 4-8, you can see a schematic representation of the network to\n\ngive you an idea of the layer sequence.\n\n153",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nFigure 4-8. A schematic representation of the network to give you an idea of the layer sequence\n\nAt this point we can train the network with the following:\n\nbatch_size = 30 num_classes = 2 epochs = 2 input_shape = (150, 150, 3) model.fit(x=train_imgs_scaled, y=train_labels_enc, validation_data=(validation_imgs_scaled, validation_labels_enc), batch_size=batch_size, epochs=epochs, verbose=1)\n\nWith two epochs, we get to about 69% validation accuracy and 70% training accuracy. Not really a good result. Let’s see if we can do better than this in just two epochs. The reason to do this in two epochs is merely a way of checking quickly different possibilities. Training such networks for many epochs can take easily few hours. Note that this model overfit the training data. That becomes clearly visible when training for more epochs, but the main goal here is not to get the best model, but to see how you can use pre-trained model to get better results, so we will ignore this problem.\n\n154",
      "content_length": 1040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nNow let’s import the VGG16 pre-trained network.\n\nfrom tensorflow.keras.applications import vgg16 from tensorflow.keras.models import Model import tensorflow.keras as keras\n\nbase_model=vgg16.VGG16(include_top=False, weights='imagenet')\n\nNote that the include_top=False parameter removes the last three fully connected layers of the network. In this way, we can append our own layers to the base network with the code:\n\nfrom tensorflow.keras.layers import Dense,GlobalAveragePooling2D x=base_model.output x=GlobalAveragePooling2D()(x) x=Dense(1024,activation='relu')(x) preds=Dense(1,activation='softmax')(x) model=Model(inputs=base_model.input,outputs=preds)\n\nWe added a pooling layer, then a Dense layer with 1024 neurons, and then an output layer with one neuron with a softmax activation function to do binary classification. We can check the structure with the following:\n\nmodel.summary()\n\nThe output is quite long, but at the end you will find this:\n\nTotal params: 15,242,050 Trainable params: 15,242,050 Non-trainable params: 0\n\n155",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nAll the 22 layers are trainable at the moment. To be able to really do transfer learning, we need to freeze all layers of the VGG16 base network. To do that we can do the following:\n\nfor layer in model.layers[:20]: layer.trainable=False for layer in model.layers[20:]: layer.trainable=True\n\nThis code will set the first 20 layers to a non trainable status, and the last two to a trainable status. Then we can compile our model as follows:\n\nmodel.compile(optimizer='Adam',loss='sparse_categorical_crossen tropy',metrics=['accuracy'])\n\nNote that we used loss='sparse_categorical_crossentropy' to be able to use the labels as they are, without having to hot-encode them. As we have done before, we can now train the network:\n\nmodel.fit(x=train_imgs_scaled, y=train_labels_enc, validation_data=(validation_imgs_scaled, validation_labels_enc), batch_size=batch_size, epochs=epochs, verbose=1)\n\nNote that although we are training only a portion of the network, this will require much more time than the simple network we tried before. The result will be an astounding 88% in two epochs. An incredibly better result than before! Your output should look something like this:\n\nTrain on 3000 samples, validate on 1000 samples Epoch 1/2 3000/3000 [==============================] - 283s 94ms/sample - loss: 0.3563 - acc: 0.8353 - val_loss: 0.2892 - val_acc: 0.8740\n\n156",
      "content_length": 1406,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nEpoch 2/2 3000/3000 [==============================] - 276s 92ms/sample - loss: 0.2913 - acc: 0.8730 - val_loss: 0.2699 - val_acc: 0.8820\n\nThis was thanks to the pre-trained first layers, which saved us a lot of work.\n\nExperimentation with Transfer Learning\n\nWhat if we want to try different architectures for the target networks, and we want to add a few more layers and try again? The previous approach has a slight downside: we need to train the entire network each time event though only the last layers should be trained. As you see from the section above, one epoch took roughly 4.5 minutes. Can we be more efficient? Turns out we can.\n\nConsider the configuration depicted in Figure 4-9.\n\nFigure 4-9. A schematic representation of a more flexible way of doing transfer learning in practice\n\nThe idea is to generate a new dataset that we will call the feature dataset, with the frozen layers. Since they will not be changed by training, those layers will always generate the same output. We can use this feature dataset as new input for a much smaller network (that we will call the target subnetwork), made by only the new layers we added to the base\n\n157",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nlayer in the previous section. We will need to train only a few layers, and that will be much faster. The generation of the feature dataset will take some time, but this must be done only once. At this point you can test different architecture for the target subnetwork and find the best configuration for your problem. Let’s see how we can do that in Keras. The base dataset preparation is the same as before, so we will not do it again.\n\nLet’s import the VGG16 pre-trained network as before:\n\nfrom tensorflow.keras.applications import vgg16 from tensorflow.keras.models import Model import tensorflow.keras as keras\n\nvgg = vgg16.VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n\noutput = vgg.layers[-1].output output = keras.layers.Flatten()(output) vgg_model = Model(vgg.input, output)\n\nvgg_model.trainable = False for layer in vgg_model.layers: layer.trainable = False\n\nwhere input_shape is (150, 150, 3).\n\nWe can simply generate the features dataset with a few lines (using\n\nthe predict functionality):\n\ndef get_ features(model, input_imgs): features = model.predict(input_imgs, verbose=0) return features\n\ntrain_features_vgg = get_features(vgg_model, train_imgs_scaled) validation_features_vgg = get_features(vgg_model, validation_ imgs_scaled)\n\n158",
      "content_length": 1324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\nNote that this will take a few minutes on a modern laptop. On a modern MacBook Pro, this will take 40 CPU minutes, meaning that if you have more cores/threads it will take a fraction of it. On my laptop, it takes effectively four minutes. Remember that since we used the parameter include_top = False, the three dense layers at the end of the network have been removed. The train_features_vgg will contain just the output of the last layer of the base network without the last three dense layers. At this point we can simply build our target subnetwork:\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer from tensorflow.keras.models import Sequential from tensorflow.keras import optimizers\n\ninput_shape = vgg_model.output_shape[1]\n\nmodel = Sequential() model.add(InputLayer(input_shape=(input_shape,))) model.add(Dense(512, activation='relu', input_dim=input_shape)) model.add(Dropout(0.3)) model.add(Dense(512, activation='relu')) model.add(Dropout(0.3)) model.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr =1e-4), metrics=['accuracy'])\n\nmodel.summary()\n\nTraining this network will be much faster than before. You will get in the range of 90% accuracy in a few seconds’ time (remember that you have created a new training dataset this time). But now you can change this network and it will be much faster to test different architectures. This time,\n\n159",
      "content_length": 1510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "Chapter 4\n\nadvanCed Cnns and transfer Learning\n\none epoch takes only six seconds, in comparison to the 4.5 minutes in the previous example. This method is much more efficient than the previous one. We split the training in two phases:\n\n1. Creation of the feature dataset. Done only once. (In our example, this needs about four minutes.)\n\n2. Train the new layers as a stand-alone network, using\n\nthe feature dataset as input. (This takes six seconds for each epoch.)\n\nIf we want to train our network for 100 epochs, with this method we would need 14 minutes. With the method described in the previous section, we would need 7.5 hours! The downside is that you need to create the new feature dataset for each dataset you want to use. In our example, we needed to do it for the training and for the validation dataset.\n\n160",
      "content_length": 820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "CHAPTER 5\n\nCost Functions and Style Transfer\n\nIn this chapter we will look in more depth at the role of the cost function in neural network models. In particular, we will discuss the MSE (mean square error) and the cross-entropy and discuss their origin and their interpretation. We will look at why we can use them to solve problems and how the MSE can be interpreted in a statistical sense, as well as how cross- entropy is related to information theory. Then, to give you an example of a much more advanced use of special loss functions, we will learn how to do neural style transfer, where we will discuss a neural network to paint in the style of famous painters.\n\nComponents of a Neural Network Model\n\nAt this point you have seen and developed several models that try to solve different types of problems. You should know by now that in all the neural network models, there are (at least) three main building blocks:\n\nNetwork architecture (number of layers, type of layers,\n\nactivation functions, etc.)\n\nLoss function (MSE, cross-entropy, etc.)\n\nThe optimizer\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_5\n\n161",
      "content_length": 1188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nThe optimizer is not typically problem specific. For example, to solve\n\na regression or classification problem, you need to choose a different architecture and loss function, but you can use in both cases the same optimizer. In regression, you may use a feed-forward network and the MSE for the loss function. In classification, you may choose a convolutional neural network and the cross-entropy loss function. But in both you can use the Adam optimizer. The component that plays the biggest role in deciding what a network can learn is the loss function. Change it and you will change what your network will be able to predict and learn.\n\nTraining Seen as an Optimization Problem\n\nLet’s try to understand why this is the case in more detail. From a purely theoretical point of view, training a network means nothing more than solving a really complex optimization problem. The standard formulation of continuous optimization problem is to find the minimum of a given function\n\nmin x\n\nf x(\n\n)\n\nSubject to two constraint types\n\n( g x i ( p x j\n\n)£ )=\n\n0\n\n,\n\n0\n\n,\n\ni\n\n= ¼ , m , 1 = ¼ , , 1\n\nj\n\nn\n\nwhere f : ℝn → ℝ is the continuous function we want to minimize, gi(x) ≤ 0 refers to the inequalities constraints, pj(x) = 0 refers to the equality constraints, and m, n ∈ ℕ+. And of course, is possible to have a problem without constraints. But how does this relate to neural networks? Well, the following parallels can be drawn:\n\n162",
      "content_length": 1477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nThe function f (x) is the loss function that we have chosen when building the neural network model.\n\nThe input x ∈ ℝn are the weights (the learnable parameters) of our network. Remember that any loss function that we may choose is always a function of the output of the network (that we indicate with ˆy), and the output is always a function of the weights W (the learnable parameters of the network).\n\nWhen we are training a network, we are actually solving an\n\noptimization problem, one where we want to minimize the loss function with respect to the weights. We implicitly have constraints, although we normally don’t declare them explicitly. For example, we may have the constraint that we want the inference time needed for one observation to be less than 10ms. In this case we would have n = 0 (no equality constraints), m = 1 (one inequality constraint) with g1 being the inference running time. To cite Wikipedia1:\n\nA loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some “cost” associated with the event\n\nTypically, a loss function measures how bad your model understands your data. Let’s look at a few simple examples so that you can understand in a concrete case this formulation of the training of a network.\n\n1 https://en.wikipedia.org/wiki/Loss_function\n\n163",
      "content_length": 1412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nA Concrete Example: Linear Regression\n\nAs you know, you can perform linear regression with a network with just one neuron if you choose as its activation function the identity function2. We indicate the set of observations with x[i] ∈ ℝn with i = 1, …, m where m is the number of observations we have at our disposal. The neuron (and therefore the network) will have the output\n\nˆy\n\n[ ] i\n\n=\n\nn\n\nå\n\nw x k = 1\n\nk\n\n[ ] i k\n\n+\n\nb\n\nwhere we have indicated the weights with w = (w1, …wn). We can choose the loss function as the mean square error (MSE):\n\n( J w b\n\n,\n\n)=\n\nm\n\nå1 m\n\n= 1\n\nk\n\n(\n\nˆ y\n\n[ ] i\n\n\n\ny\n\n[ ] i\n\n)\n\n2\n\nWhere y[i] is the target variable that we want to predict for the ith\n\nobservation. It’s easy to see how the loss function that we have defined is a function of the weights and the bias. In fact, we have\n\n( J w b\n\n,\n\n)=\n\n1 m\n\nm\n\nå\n\ni\n\n= 1\n\n(\n\nˆ y\n\n[ ] i\n\n\n\ny\n\n[ ] i\n\n) =\n\n2\n\n1 m\n\næ å å ç è\n\nm\n\nn\n\nw x k = 1\n\ni\n\n= 1\n\nk\n\n[ ] i k\n\n+ -\n\nb y\n\n[ ] i\n\nö ÷ øø\n\n2\n\nTraining this network as we typically do with (for example) a gradient\n\ndescent algorithm is nothing more than solving an unconstrained\n\n2 This example is discussed in detail in Michelucci, Umberto, 2018. Applied Deep Learning: A Case-Based Approach To Understanding Deep Neural Networks. 1. Auflage. New York: Apress. ISBN 978-1-4842-3789-2. Available from: https:// doi.org/10.1007/978-1-4842-3790-8\n\n164",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\noptimization problem where we have (using the notation we have used at the beginning):\n\nf\n\n:=\n\nJ\n\nThe Cost Function Mathematical Notation\n\nLet’s define some notation that we will use in the next sections. We will use\n\ni[ ]Î is the output of the network for the ith observation. ˆy ˆY Î ´ m k\n\nk\n\n is the tensor containing the output of the network for all\n\nobservations.3 [ ] ´ x i\n\nn n n c y\n\n´\n\nÎ\n\nx\n\nrepresents the ith observation input features (in general,\n\nfor images we would have nc channels, and a resolution of nx × ny). is the tensor containing all input observations.\n\nÎ ´ \n\n´ m n n n c x\n\n´\n\nX W is the set of all learnable parameters that are used in the network\n\ny\n\n(including the biases).\n\nm is the number of observations. nc is the number of image channels (for RGB images it would be 3). nx is the horizontal resolution of the input images. ny is the vertical resolution of the input images. J is the cost function. In general, we will define the so-called cost (or loss) function J\n\ngenerically as follows:\n\n( J X Y, Wˆ(\n\n)\n\n)\n\n3 Remember that the order of the dimensions depends on how you structure your network and you may need to change it. The dimensions here are for illustrative purposes only.\n\n165",
      "content_length": 1274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nThis function, in addition to the network architecture, will define what kind of problem our neural network model will be able to solve. Note how this function\n\nDepends on the network architecture, since it depends on the network output ˆY (and therefore from the learnable parameters, W)\n\nDepends on the input dataset, since it depends on the\n\ninput X\n\nThis is the function that will be used when finding the best weights. In\n\nalmost all optimizers, the weights are updated using Ñ some form.\n\nW\n\n( J X Yˆ\n\n( , W\n\n)\n\nTypical Cost Functions\n\nThere are several cost functions that you may use when training neural networks, as we have seen in the previous chapters. In the next sections, we will look at two of the most used in detail and try to understand their meaning and origin.\n\nMean Square Error\n\nThe mean square error function\n\n( J w b\n\n,\n\n)=\n\nm\n\nå1 m\n\n= 1\n\nk\n\n(\n\nˆ y\n\n[ ] i\n\n\n\ny\n\n[ ] i\n\n)\n\n2\n\nis probably the most used cost function used when developing models for regression. There are several interpretations of this cost function, but the following two should help you in get an intuitive and a more formal understanding of it.\n\n166\n\n)\n\nin",
      "content_length": 1195,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nIntuitive Explanation\n\nJ is nothing more than the average of the squared difference between the predictions and the measured values. So basically, it measures how far the predictions are from the expected values. A perfect model that would predict the data perfectly ( ˆy = for all i = 1, …, m) would have J = 0. In general, it holds the smallest J the better the predictions are.\n\n[ ] i\n\n[ ] i\n\ny\n\nNote predictions are (and therefore, the better the model is).\n\nin general, it holds that the smaller the Mse, the better the\n\nMinimizing the MSE means finding the parameters so that our\n\nnetwork will give output as close as possible to our training data. Note that you could achieve a similar result by using, for example, the MAE (Mean Absolute Error) given by\n\nMAE\n\n=\n\nå1 m\n\nm\n\n= 1\n\nk\n\nˆ y\n\n[ ] i\n\n\n\ny\n\n[ ] i\n\nAlthough this is not usually done.\n\nMSE as the Second Moment of a Moment-Generating Function\n\nThere is a more formal way of interpreting the MSE. Let’s define the quantity\n\nDY\n\n[ ] i\n\n=\n\n[ ] i -ˆ y\n\ny\n\n[ ] i\n\nLet’s define the moment-generating function\n\nD ( ) = éë : E e\n\nM t Y\n\nD t Y\n\nùû\n\n167",
      "content_length": 1151,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nWhere we have t ∈ ℝ and we have indicated with E[·] the expected value of the variable over all observations. We will skip the discussion about the existence of the expected value, depending on the characteristics of ΔY, since this goes beyond the scope of this book. We can expand etΔY with a Taylor series expansion4 (we will assume we can do that):\n\ne\n\ntDY = +\n\n1\n\nD t Y\n\n+\n\n2 D t Y ! 2\n\n2\n\n+¼\n\nTherefore\n\nD ( ) = éë : E e\n\nM t Y\n\nt\n\nD Y\n\nùû = + [ 1\n\nD tE Y\n\n]+\n\n2 éë D t E Y 2\n\n!\n\n2\n\nùû +¼\n\nE[ΔYn] is called the nth moment of the function MΔY(t). You can see that\n\nthe moments can be easily interpreted (at least the first):\n\nE[ΔY]: First moment of MΔY(t) - Average of ΔY\n\nE[ΔY2]: Second moment of MΔY(t) - is what we defined as the MSE function\n\nE[ΔY3]: Third moment of MΔY(t) - Skeweness5\n\nE[ΔY4]: Fourth moment of MΔY(t) - Kurtosis6\n\nWe can simply write the second moment as the average over the\n\nobservations\n\nD 2 éë E Y\n\nùû\n\n:=\n\n1 m\n\nm\n\nå D\n\nk\n\n= 1\n\nY\n\n[ ] i\n\n2\n\n=\n\n1 m\n\nm\n\nå\n\nk\n\n= 1\n\n(\n\nˆ y\n\n[ ] i\n\n\n\ny\n\n[ ] i\n\n)\n\n2\n\n4 https://en.wikipedia.org/wiki/Taylor_series 5 https://en.m.wikipedia.org/wiki/Skewness. In the case of E[ΔY] = 0. 6 https://en.m.wikipedia.org/wiki/Kurtosis. In the case of E[ΔY] = 0.\n\n168",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nIf we assume that our model predict data with E[ΔY] = 0, then the E[ΔY2] (and therefore the MSE) is nothing more than the variance of the distribution of our data points ΔY[i]. In this case, it simply measures how broad our points are spread around the average (that is zero): the perfect prediction. Remember that, if for an observation, we have ΔY[i] = 0, it means we have ˆy , meaning the prediciton is perfect. Just to give the correct terminology, if E[ΔY] is not zero, then the moments are sometimes called the non-central moments. If you are dealing with non-central moments, you cannot interpret them directly as a statistical quantity (as the variance) anymore.\n\n[ ] i\n\n[ ] i\n\n=\n\ny\n\nNote if you are dealing with non-central moments, you cannot interpret them directly as a statistical quantity (as the variance) anymore. if the average of ΔY [i] is zero, then the Mse is simply the variance of the distributions of our predictions. and of course, the smaller the value, the better the predictions are.\n\nCross-Entropy\n\nThere are several ways to understand the cross-entropy loss function, but I think the most fascinating way is obtained by starting the discussion from information theory. In this section, we will discuss some of the fundamental concepts on a more intuitive basis to give you enough information and understanding to get a very powerful understanding of cross-entropy.\n\nSelf-Information or Suprisal of an Event\n\nWe need to start with the concept of self-information, or suprisal of an event. To get an intuitive understanding of it, consider the following: when an unlikely outcome of an event occurs, we associate it with a high level\n\n169",
      "content_length": 1711,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nof information. When an outcome happens all the time, typically it does not have much information associated with it. In other words, we are more surprised when an unlikely event occurs; therefore, it’s also called suprisal of an outcome. How can we formulate this in a mathematical form? Let’s consider a random variable X with n possible outcomes x1, x2, …, xn and probability mass function7 P(X). Let’s indicate the probability of event xi to occur with pi = P(xi). Any monotonically decreasing function I(pi) between 0 and 1 could be used to represent the suprisal (or self-information) of the random variable X. But there is an important property that this function must have: if the events are independent, I should satisfy\n\n( I p p i\n\nj\n\n)= ( I p i\n\n)+ ( I p\n\n)\n\nIf the outcomes i and j are independent. There is immediately a function that comes to mind that has this property: the logarithm. In fact, it’s true that\n\nln\n\n(\n\np p i\n\nj\n\n)=\n\nlog\n\np i\n\n+\n\nlog\n\np\n\nTo have it monotonically decreasing, we can choose the following\n\nformula:\n\n( I p i\n\n)= -log\n\np i\n\n7 In probability and statistics, a probability mass function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value [Stewart, William J. (2011). Probability, Markov Chains, Queues, and Simulation: The Mathematical Basis of Performance Modeling. Princeton University Press. p. 105. ISBN 978-1-4008-3281-1.]\n\n170",
      "content_length": 1478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nSuprisal Associated with an Event X\n\nIn general, how much information do we have related to a specific event X? This is measured by the expected value over all possible outcomes for X (we will indicate this set with P). Mathematically, we can write this as\n\n( H X\n\n)=\n\n(\n\néë E I X\n\nP\n\n)\n\nùû =\n\nn\n\nå\n\n) ( P x I x i\n\n(\n\ni\n\n)= -\n\nn\n\nå\n\n( P x\n\ni\n\n)\n\nlog\n\nb\n\n( P x\n\ni\n\n)\n\ni\n\n= 1\n\ni\n\n= 1\n\nH(X) is called the Shannon entropy, and b is the basis of the algorithm\n\nand typically is chosen as 2, 10, or e.\n\nCross-Entropy\n\nNow let’s suppose we want to compare two distributions of probabilities for our event X. Let’s analyze what we do when we train a neural network for classification. Consider the following points:\n\nOur examples give us the “real” or expected\n\ndistributions of our events (the true labels). Their distributions will be our P. For example, our observations may contain cat classes (let’s suppose this is class 1) with a certain probability P(x1), where x1 is the outcome “this image has a cat in it”. We have a given probability mass function, P.\n\nThe network we have trained will give us a different probability mass function, Q, since the predictions will not be identical to the training data. Outcome x1 (“the image has a cat in it”) will occur with a different probability, Q(x1). You will remember that when building a network for classification, we use a softmax activation function for the output layer to interpret the output as probabilities. Do you see how everything seems to make suddenly more sense? 171",
      "content_length": 1572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nWe want to have a prediction that reflects as best as possible the given labels, meaning that we want to have a probability mass function Q that is as similar as possible to P.\n\nTo compare the two probability mass functions (what we are interested\n\nin), we can simply calculate the expected value of the self-information obtained by our network with the distribution obtained by the examples. In a more mathematical form\n\n( H Q P\n\n,\n\n)=\n\n(\n\néë E I Q\n\nP\n\n)\n\nùû =\n\nE\n\nP\n\n[\n\nlog\n\nb\n\nQ\n\n]= -\n\nn\n\nå\n\n( P x\n\ni\n\n)\n\nlog\n\nb\n\n( Q x\n\n)\n\ni\n\n= 1\n\nIf you have any experience in information theory, H(Q, P) will give a measure of the similary of the two probability mass functions, Q and P. To understand why, let’s consider a practical example. X will be the toss of a fair coin. X will have two possible outcomes: x1 will be the head and x2 will be the tail of the coint. The “true” probability mass function is of course a constant one with P(x1) = 0.5 and P(x2) = 0.5. Now let’s consider alternative probability mass functions Qi with (we will consider only nine possible values for illustrative purposes):\n\n\n\ni = 1 → Q1(x1) = 0.1, Q1(x2) = 0.9\n\n\n\ni = 2 → Q2(x1) = 0.2, Q2(x2) = 0.8\n\n\n\ni = 3 → Q3(x1) = 0.3, Q3(x2) = 0.7\n\n\n\ni = 4 → Q4(x1) = 0.4, Q4(x2) = 0.6\n\n\n\ni = 5 → Q5(x1) = 0.5, Q5(x2) = 0.5\n\n\n\ni = 6 → Q6(x1) = 0.6, Q6(x2) = 0.4\n\n\n\ni = 7 → Q7(x1) = 0.7, Q7(x2) = 0.3\n\n\n\ni = 8 → Q8(x1) = 0.8, Q8(x2) = 0.2\n\n\n\ni = 9 → Q9(x1) = 0.9, Q9(x2) = 0.1\n\n172",
      "content_length": 1488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nLet’s calculate H(Qi, P) for i = 1, …5. We don’t need to calculate H for i = 6, . . , 9 since the function is symmetric, meaning for example that H(Q4, P) = H(Q6, P). In Figure 5-1, you can see the plot of H(Qi, P). You can see how the maxium is reached for i = 5, exactly when the two probability mass functions are the same.\n\nFigure 5-1. H(Qi, P) for i = 1, …5. The minimum is obtained for i = 5, when the two probability mass functions are exactly the same.\n\nNote Cross-entropy H(Q, P) is a measure of how similar the two mass probability functions, Q and P, are.\n\nCross-Entropy for Binary Classification\n\nNow let’s consider a binary classification problem and let’s see how cross- entropy works. Let’s suppose our event X is the classification of a given image in two classes. The possible outcomes are only two: class 1 or class 2. Let’s suppose for illustrative purposes that our image belongs to class 1. Our “true” probability mass function for the image will have P(x1) = 1.0, P(x2) = 0. In other words, our probability mass function P can only be 0 or 1 since we know the true value.\n\n173",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nYou will remember that in a binary classification problem we used the\n\nfollowing\n\n(  ˆ y\n\n( ) j\n\n, y\n\n( ) j\n\n)= -\n\n(\n\ny\n\n( ) j\n\nlog\n\nˆ y\n\n( ) j\n\n+ -( 1\n\ny\n\n( ) j\n\n)\n\nlog\n\n( 1\n\nˆ y\n\n( ) j\n\n)\n\n)\n\nWhere y(j) represents the true labels (0 for class 1 and 1 for class 2) and ˆy j( ) is the probability of the image j of being of class 2, or in other words, of the output of the network assuming the value 1. The cost function we will minimize is given by a sum over all observations (or examples)\n\nJ\n\n(\n\nw,\n\nb\n\n)=\n\nm\n\nå1 m\n\nj\n\n= 1\n\n(  ˆ y\n\n( ) j\n\n,\n\ny\n\n( ) j\n\n)\n\nUsing the notation of the previous section, we can write for image j\n\n( p x 1 j\n\n)= - ( ) 1\n\nj\n\ny\n\n2( p x j\n\n)= ( ) y\n\nj\n\nRemember that y(j) can be only 0 or 1; therefore, we have only two\n\npossibilities: pj(x1) = 1, pj(x2) = 0 or pj(x1) = 0, pj(x2) = 1. And we can also write for the prediction of the network\n\n( q x 1 j\n\n)= - ( )ˆ 1\n\nj\n\ny\n\n2( q x j\n\n)= ( )ˆ y\n\nj\n\nRemember: This result is determined by how we built our network (since we used the softmax activation functions in the output layer to have probabilities) and by how we coded our labels (to be 0 and 1, so that they could be interpreted as probabilities). Let’s now write cross-entropy\n\n174",
      "content_length": 1261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nas defined in the previous section using our neural network notation but summing over all examples (remember that we want to have the entire cross-entropy for all the events, in other words for all images):\n\nm\n\n2\n\n( H Q P,\n\n)= -\n\nåå\n\n( p x j\n\ni\n\n)\n\nlog\n\nb\n\n( q x j\n\ni\n\n)\n\n=\n\n1\n\ni\n\n=\n\n1\n\n=-\n\nm\n\nå\n\n(\n\ny\n\n( ) j\n\nlog\n\nb\n\nˆ y\n\n( ) j\n\n( + - 1\n\ny\n\n( ) jj\n\n)\n\n( ( ) jy log 1 ˆ b )\n\n=\n\n1\n\n( So basically  ˆy\n\n( ) yi ,\n\n( ) i\n\n)\n\nis nothing more than the cross-entropy as it is\n\nderived in information theory.\n\nNote intuitively when we minimize the cross-entropy in a binary classification problem, we minimize the surprise that we may have when our predictions are different from what we expect.\n\nH(Q, P) measures how good our predictions probability mass function\n\n(Q) matches our training examples probability mass function (P).\n\nNote When we design a network for classification using the cross- entropy and we use the softmax activation function in the final layer to interpret the output as probabilities, we simply build a complex classification system that is based on information theory. We should thank shannon8 for classification with neural networks.\n\n8 https://en.wikipedia.org/wiki/Claude_Shannon\n\n175",
      "content_length": 1252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nCost Functions: A Final Word\n\nIt should be clear now that the cost function determines what a neural network can learn. Change it and the network will learn completely different things. It should come as no surprise that, to achieve special results, like art for example, it’s simply a matter of choosing the right architecture and the right cost function. In the next part of this chapter, we will look at neural style transfer and it will become immediately clear how choosing the right cost function (multiple ones in this example as we will see) is the key to achieving extraordinary results.\n\nNeural Style Transfer\n\nAt this point you have all the tools to start using networks for more advanced techniques: using pre-trained CNNs, extracting information from the hidden layers, and using custom cost functions. This is starting to be advanced material, so you need to understand all the basics we discussed in the previous chapters very well. If something seems unclear, go back and study the material again.\n\nAn interesting and fun application of CNNs is to make art. Neural style\n\ntransfer (NST) refers to a technique that manipulates digital images, to adopt the appearance or style of another image9. A fun application is to take an image and let the network manipulate it to adopt it to the style of a famous painter, like Van Gogh. NST using deep learning appeared first in a paper by Gatys et al. in 201510. It’s a new technique. The method developed by Gatys used pre-trained deep CNNs to separate the content of an image from the style.\n\n9 https://en.wikipedia.org/wiki/Neural_Style_Transfer 10 Gatys, Leon A.; Ecker, Alexander S.; Bethge, Matthias (26 August 2015). “A Neural\n\nAlgorithm of Artistic Style”. https://arxiv.org/abs/1508.06576\n\n176",
      "content_length": 1805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nThe idea is that an image is fed into a pre-trained VGG-1911 CNN trained on the imagenet dataset. The author assumed that the content of an image can be found in the network intermediate layers output (the image passed through the learned filters in each layers), while the style lies in the correlations of the different layers output (coded in a Gramian matrix). The pre-trained network can identify the content of images quite well, and therefore the features learned by each layer must relate strongly to the content of the image, and not to the style. In fact, a robust CNN that is good at identifying images does not care much about style. Intuitively, style is contained in how the different filter responses over the space of an image are related. A painter may use brush strokes that are wide or narrow, may use many colors close to each other or just a few, and so on. Remember in a CNN, each layer is simply a collection of image filters; therefore, the output of a given layer is simply a collection of differently filtered versions of the input image10.\n\nAnother way of seeing that is that content is found when you look at\n\nan image from afar (you don’t care much about the details), while style is found when looking at the image at a much closer scale and depends on how different parts of the image relate to each other. Gatys et al. have, in a smart way, simply implemented these ideas mathematically. To give you an idea, look at Figure 5-2. A network has manipulated the original image (upper left) into the style of the Van Gogh painting in the upper right, to obtain the image on the bottom.\n\n11 ”Very Deep CNNS for Large-Scale Visual Recognition”. Robots.ox.ac.uk. 2014. Retrieved 13 February 2019, http://www.robots.ox.ac.uk/~vgg/research/ very_deep/\n\n177",
      "content_length": 1825,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nFigure 5-2. An example of NST. The method has manipulated the original image (upper left) into the style of the Van Gogh painting in the upper right, to obtain the image on the bottom.\n\nThe Mathematics Behind NST\n\nThe original paper used the VGG19 network, which Keras makes available for us to download and use. An input image that we will indicate here with x (I will try to use the original notation as much as possible) is encoded\n\n178",
      "content_length": 485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nin each layer of the CNN. A layer with Nl filters (or kernels as they are sometimes called) will have Nl feature maps as output. In the algorithm those outputs will be flattened out in a one-dimensional vector of dimension Ml, where Ml is the height times the width of the output of each filter when applied to the input image. The response of a layer l can then be encoded in a tensor F l Î . Let’s pause a second here and try to understand with a concrete example what we mean.\n\n´ N Ml\n\nl\n\n\n\nLet’s suppose we use as input images in color, each with dimensions\n\n32 × 32. Let’s consider the first convolutional layer in a CNN that has been created with the code:\n\nConv2D(32, (3, 3), padding='same', activation='relu', input_ shape=input_shape))\n\nWhere of course input_shape = (32,32,3). The output of the layer\n\nwill have these dimensions\n\n(None, 32, 32, 32)\n\nWhere of course the None will assume the value of the number of observations used. This is because we used the parameter padding = 'same'. In this case, the output of layer l = 1, are 32 feature maps (or the result of the input image convoluted with the 32 filters) each with dimensions 32 × 32. In this case, we will have Nl = 1 = 32 and Ml = 1 = 32 × 32 = 1024. Each of the 32 × 32 feature maps will be flattened out before calculating the Gramian matrices. You will see clearly how this is done later in the code.\n\nLet’s call the original image p. This is the image we want to change. The image that is generated as output is called x. We will indicate with Pl and Fl their respective features maps obtained from layer l. We define the squre error loss, called the content loss function, as follows:\n\ncontent\n\n(\n\n, p x l ,\n\n)=\n\nå1 2\n\ni j ,\n\n(\n\nl F ij\n\n\n\nl P ij\n\n)\n\n2\n\n179",
      "content_length": 1782,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nIn Keras, we will implement this with the following code:\n\ncontent_loss = tf.add_n([tf.reduce_mean((content_outputs[name]- content_targets[name])**2) for name in content_outputs.\n\nkeys()])\n\nwhere the content_outputs[] and content_targets[] will contain the output of specific layers of VGG19 when applied to the input (content_ outputs) and the generated image (content_targets), respectively (already flattened). Later we will discuss it in more detail; don’t worry for the moment if you don’t understand it completely. You may wonder why ) will we don’t have the factor 1/2 but we don’t need it, since content p x l , , be multiplied by another factor, which will make the 1/2 useless.\n\n(\n\nWe need to calculate the gradient of the loss function with respect to\n\nthe image. This is quite an important point. What this means is that the parameters we want to learn are the pixel values of the image we want to change. The parameters of the network are fixed, and we don’t need to change them. With Keras, we will need to use the tape.gradient function in this form:\n\ntape.gradient(loss, image)\n\nWe will need to define the image as a TensorFlow Variable (more on that later). If you are not familiar with how tape.gradient works, I suggest you check out the official documentation at https://www.tensorflow. org/tutorials/eager/automatic_differentiation.\n\nNote the parameters we want to learn are the pixel values of the image we want to change, not the weights of the network.\n\nNow we need to take care of the style. To do this, we need to define a loss function for the style. To do this, we need to define the Gramian matrix\n\n180",
      "content_length": 1678,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nGl, which is the inner product between the flattened feature maps i and j in layer l. In other words\n\nl G ij\n\n=å\n\nl l F F kj ik\n\nk\n\nWith this newly defined quantity, we will define a Style loss function ( style a x,\n\n), where a is the image from which we want to use the style as\n\n5\n\nstyle\n\n(\n\n, a x\n\n)=\n\nå\n\nw E l = 1\n\nl\n\nl\n\nWhere\n\nE\n\n=\n\nå1 4 2 N M l\n\n2 l\n\ni j ,\n\n(\n\nl - G A ij\n\nl ij\n\n)\n\n2\n\nWhere wl are weights that in the original papers were chosen and equal 1/5. In Keras, we will implement this loss with the code (we will look at the details later):\n\ntf.add_n([tf.reduce_mean((style_outputs[name]-style_ targets[name])**2) for name in style_outputs.keys()])\n\nThe style_outputs and style_targets variables will contain the output of five of the layers of the VGG19 network. In the original paper, the following five layers were used:\n\nl=1 - block1_conv1 l=2 - block2_conv1 l=3 - block3_conv1 l=4 - block4_conv1 l=5 - block5_conv1\n\n181",
      "content_length": 987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nThose are the first layers in each block in the VGG19 network.\n\nRemember that you can get the layer names from the VGG19 simply with this code:\n\nvgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n\nprint() for layer in vgg.layers: print(layer.name)\n\nThat would give you this result:\n\ninput_1 block1_conv1 block1_conv2 block1_pool block2_conv1 block2_conv2 block2_pool block3_conv1 block3_conv2 block3_conv3 block3_conv4 block3_pool block4_conv1 block4_conv2 block4_conv3 block4_conv4 block4_pool block5_conv1 block5_conv2 block5_conv3\n\n182",
      "content_length": 606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nblock5_conv4 block5_pool\n\nNote that we have no dense layers, since we used include_top=False.\n\nFinally, we will minimize the following loss function\n\n5\n\n\n\ntotal\n\n(\n\n, p x a ,\n\n)=\n\na\n\n\n\nstyle\n\n(\n\n, a x\n\n)+\n\nå b \n\ncontent\n\n(\n\n, p x l ,\n\n)\n\n= 1\n\nWith gradient descent (for example), with respect to the image we want\n\nto change. The constants α and β can be choosen to give more weight to style or content. For the result in Figure 5-1, I chose α = 1.0, β = 104. Other typical values are α = 10−2, β = 104.\n\nAn Example of Style Transfer in Keras\n\nThe code that we will discuss here has been taken from the original TensorFlow NST tutorial and is greatly simplified for this discussion. We will discuss only part of the code to simplify the discussion, since in its entirety the code is relatively long. You can find the entire simplified version in the book’s GitHub repository in the Chapter 5 folder. I suggest you run the code in Google Colab with GPU enabled, since it is computationally quite intensive. To give you an idea, one epoch on my laptop takes roughly 13 seconds, while on Google Colab, it takes 0.5 seconds to work with 512 × 512 pixel images.\n\nTo make sure that you have the latest TensorFlow version installed, you\n\nshould run the following code at the beginning of your notebook:\n\nfrom __future__ import absolute_import, division, print_ function, unicode_literals !pip install tensorflow-gpu==2.0.0-alpha0 import tensorflow as tf\n\n183",
      "content_length": 1500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nIf you run the code on Google Colab, you need to save the images you\n\nwant to work with on your Google drive and mount it. To do that, you need to upload two images on your drive:\n\nA style image: For example, a famous painting. This is\n\nthe image you want to get the style from.\n\nA content image: For example, a landscape or a photo\n\nyou took. This is the image you want to modify.\n\nI assume here that you uploaded your images into a folder called data into the root directory of your Google drive. What you need to do now is to mount your Google drive in Google Colab to be able to access the images. To do that, you need the following code:\n\nfrom google.colab import drive drive.mount('/content/drive')\n\nIf you run this code, you need to go to a specific URL (that will be given\n\nto you by Google Colab) where you will receive the code that you need to paste in your notebook. A nice overview on how to do this can be found at http://toe.lt/a. Once mounted, you’ll get a list of the files in the directory with this:\n\n!ls \"/content/drive/My Drive/data\"\n\nWe can define the filenames of the images we will use with\n\ncontent_path = '/content/drive/My Drive/data/landscape.jpg' style_path = '/content/drive/My Drive/data/vangogh_landscape.jpg'\n\nYou need to change the filenames to yours, of course. But you will find the images I used for this example on the GitHub repository if you want to try the exercise with them. You need to create the data directory if you don’t have it and copy the images in there. The images will be loaded with the load_img() function. Note that in the function at the beginning we resize the images to have their maximum dimension equal\n\n184",
      "content_length": 1715,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nto 512 (the complete code for the load_img() function can be found on GitHub). This is a size that is manageable, but if you want to generate better-looking images, you need to increase this value. The image in Figure 5-1 was generated with max_dim = 1024. The function begins with\n\ndef load_img(path_to_img): max_dim = 512 img = tf.io.read_file(path_to_img)\n\nSo, you change the value of the max_dim variable to work with bigger\n\nimages. Now we need to select only the output of some layers, as we described in the previous section. To do that, we put the names of the layers we want to use into two lists:\n\n# Content layer where will pull our feature maps content_layers = ['block5_conv2']\n\n# Style layer we are interested in style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n\nThis way we can select the right layers using the names. What we need is a model that gets input and returns all the feature maps from each layer. To do that, we use the following code\n\ndef vgg_layers(layer_names): vgg = tf.keras.applications.VGG19(include_top=False,\n\nweights='imagenet') vgg.trainable = False\n\noutputs = [vgg.get_layer(name).output for name in layer_names]\n\n185",
      "content_length": 1249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nmodel = tf.keras.Model([vgg.input], outputs) return model\n\nThis function gets a list as input with the layer names and selects the\n\nnetwork layer output of the given layers with this line:\n\noutputs = [vgg.get_layer(name).output for name in layer_names]\n\nNote that there are no checks, so if you have a wrong layer names you\n\nwill not get the result you expect. But since the layers we need are fixed, you don’t need to check if the names exist in the network. This line\n\nmodel = tf.keras.Model([vgg.input], outputs)\n\ncreates a model with one input (vgg.input) and one or more outputs, depending on the number of layers in the layer_names input list.\n\nTo calculate Gij\n\nl (the Gramian matrix), we use this function\n\ndef gram_matrix(input_tensor): result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor,\n\ninput_tensor)\n\ninput_shape = tf.shape(input_tensor) num_locations = tf.cast(input_shape[1]*input_shape[2],\n\ntf.float32)\n\nreturn result/(num_locations)\n\nwhere the variable num_locations is simply Ml. Now comes the interesting part: the definition of the loss functions. We need to define a class called StyleContentModel that will take our model and return the output of the different layers at each iteration. The class has an __init__ part that we will skip here (you can find the code in the Jupyter Notebook). The interesting part is the call() function:\n\ndef call(self, inputs): inputs = inputs*255.0\n\n186",
      "content_length": 1460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\npreprocessed_input = tf.keras.applications.vgg19.\n\npreprocess_input(inputs)\n\noutputs = self.vgg(preprocessed_input) style_outputs, content_outputs = (outputs[:self.num_style_\n\nlayers],\n\noutputs[self.num_style_\n\nlayers:])\n\nstyle_outputs = [gram_matrix(style_output) for style_output in style_outputs]\n\ncontent_dict = {content_name:value for content_name, value in zip(self.content_layers, content_\n\noutputs)}\n\nstyle_dict = {style_name:value for style_name, value in zip(self.style_layers, style_outputs)}\n\nreturn {'content':content_dict, 'style':style_dict}\n\nThis function will return a dictionary with two elements—content_ dict contains the content layers and their output and style_dict contains the style layers and their outputs. You use this function:\n\nextractor = StyleContentModel(style_layers, content_layers)\n\nAnd then:\n\nstyle_targets = extractor(style_image)['style'] content_targets = extractor(content_image)['content']\n\nThis way, we can get the output of the different layers when applied\n\nto different images. Remember we need the output of the style layers when applied to our Van Gogh painting, but we need the content layer\n\n187",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\noutput when applied to the landscape (or your image) image. Let’s save the content image (the landscape or your image) in a variable and define a function (it will be useful later9) that will clip the values of an array between 0 and 1:\n\nimage = tf.Variable(content_image) def clip_0_1(image): return tf.clip_by_value(image, clip_value_min=0.0,\n\nclip_value_max=1.0)\n\nThen we can define the two variables α, β as follows:\n\nstyle_weight=1e-2 content_weight=1e4\n\nNow we have everything we need to define the loss function:\n\ndef style_content_loss(outputs): style_outputs = outputs['style'] content_outputs = outputs['content'] style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-\n\nstyle_targets[name])**2)\n\nfor name in style_outputs.keys()]) style_loss *= style_weight / num_style_layers\n\ncontent_loss = tf.add_n([tf.reduce_mean((content_\n\noutputs[name]-content_targets[name])**2)\n\nfor name in content_outputs.\n\nkeys()])\n\ncontent_loss *= content_weight / num_content_layers loss = style_loss + content_loss return loss\n\nThis code is rather self-explanatory, as we have discussed its parts already. This function expects as input the dictionary that we obtain using the StyleContentModel class.\n\n188",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nNow let’s create the function that will update the weights:\n\n@tf.function() def train_step(image): with tf.GradientTape() as tape: outputs = extractor(image) loss = style_content_loss(outputs)\n\ngrad = tape.gradient(loss, image) opt.apply_gradients([(grad, image)]) image.assign(clip_0_1(image))\n\nWe use tf.GradientTape to update the image. Note that when you annotate a function with @tf.function, you can still call it like any other function. But it will be compiled into a graph, which means you get the benefits of faster execution, running on GPU or TPU, or exporting to SavedModel (see https://www.tensorflow.org/alpha/guide/ autograph). Remember that the variable extractor has been obtained with this code:\n\nextractor = StyleContentModel(style_layers, content_layers)\n\nAnd is the dictionary with the output of the different layers. Now this code is rather advanced and complicated to understand\n\nat the beginning, so take your time and read the pages with the Jupyter Notebook open at the same time, to be able to follow the code and the explanation. Don’t be discouraged if at the beginning you don’t understand everything. The line:\n\ngrad = tape.gradient(loss, image)\n\nwill calculate the gradients of the loss function with respect to the variable image that we have defined. Each update step can be done with a simple line of code:\n\ntrain_step(image)\n\n189",
      "content_length": 1412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nNow we can do the final loop easily:\n\nepochs = 20 steps_per_epoch = 100\n\nstep = 0 for n in range(epochs): for m in range(steps_per_epoch): step += 1 train_step(image) print(\".\", end=\") display.clear_output(wait=True) imshow(image.read_value()) plt.title(\"Train step: {}\".format(step)) plt.show()\n\nWhile it’s running, you will see the image change every epoch and you\n\ncan witness how it is changing.\n\nNST with Silhouettes\n\nThere is a fun application that you can do with NST, and that has to do with silhouettes12. A silhouette is an image of something represented as a solid shape of a single color. In Figure 5-3, you can see an example; if you are a fan of Star Wars, you know who it is (hint: Darth Vader13).\n\n12 This part of the chapter has been inspired by the Medium post https://\n\nbecominghuman.ai/creating-intricate-art-with-neural-style-transfer- e5fee5f89481.\n\n13 https://en.wikipedia.org/wiki/Darth_Vader\n\n190",
      "content_length": 967,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nFigure 5-3. A silhouette of the Star Wars character Darth Vader\n\nYou should search the Internet14 for images that are similar to a mosaic\n\nor stained glass, like the one shown in Figure 5-4.\n\nFigure 5-4. A mosaic-like image\n\n14 Note that all the images used in this chapter were images free of copyright and free to use. If you use images for your papers or block, ensure that you can use them freely or you’ll need to pay royalties.\n\n191",
      "content_length": 484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nThe goal is to obtain an image like the one shown in Figure 5-5.\n\nFigure 5-5. NST done on a silhouette after applying masking (more on this later)\n\nMasking\n\nMasking has several meanings, depending on the field you are using it in. Here I refer to masking as the process of changing parts of an image to absolute white according to a silhouette. The idea is graphically illustrated in Figure 5-6. You can think of it this way: you put a silhouette over your image (they should have the same resolution) and keep only the parts where the silhouette is black.\n\nFigure 5-6. Masking applied to the mosaic image in Figure 5-4\n\n192",
      "content_length": 670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "Chapter 5\n\nCost FunCtions and style transFer\n\nThis is okay, but a bit unsatisfying, since for example you don’t have\n\nedges in the result. The mosaic shapes are simply cut in the middle. Visually this is not so satisfying. But we can use NST to make the end image much nicer. The process is the following:\n\nYou use the mosaic-like image as the style image.\n\nYou use your silhouette image as the content image.\n\nAt the end you apply masking to your end result using\n\nyour silhouette image.\n\nYou can see the result (using the same code) in Figure 5-5. You can see\n\nthat you get nice edges and the mosaic tiles are not cut in half.\n\nYou can find the entire code in the book’s GitHub repository in\n\nChapter 5. But as a reference, let’s suppose that you have your image saved as a numpy array. Let’s suppose that the silhouette is saved in an array called mask and that your image is saved in an array called result. The assumption (and you should check that) is that the mask array will contain only 0 or 255 values (black and white). Then masking is done simply with this:\n\nresult[mask] = 255\n\nThat simply makes white in the result image where there is white in the\n\nsilhouette and leaves the rest untouched.\n\n193",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "CHAPTER 6\n\nObject Classification: An Introduction\n\nIn this chapter, we will look at more advanced tasks in image processing that can be achieved with neural networks. We will look at semantic segmentation, localization, detection, and instance segmentation. The goal of this chapter is not to make you an expert, since one could easily read many books on the subject, but to give you enough information to be able to understand the algorithms and read the original papers. I hope that, by the end of this chapter, you will understand the difference between the methods, and you will have an intuitive understanding of the building blocks of these methods.\n\nThese algorithms need many advanced techniques that we have looked at in the previous chapters, like multiple loss functions and multi- task learning. We will look at a few more in this chapter. Keep in mind that the original papers on the methods are in some cases just a couple of years old, so to master the subject, you need to get your hands dirty and read the original papers.\n\nTraining and using the networks described in the papers is not doable\n\non a simple laptop and therefore you will find in this chapter (and in the next) less code and examples. I try to point you in the right direction and tell you what pre-trained libraries and networks are available at the time of this writing, in case you want to use those techniques in your own\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_6\n\n195",
      "content_length": 1530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nprojects. That will be the subject of the next chapter. Where relevant, I try to point out the differences, advantages, and disadvantages of the different methods. We will look at the most advanced methods in a very superficial way, since the details are so complex that only studying the original papers can give you all the information you need to implement those algorithms yourself.\n\nWhat Is Object Localization?\n\nLet’s start with an intuitive understanding of what object localization is. We have already seen image classification in many forms: it tells us what the content of an image is. That may sound easy, but there are many cases when this is difficult, and not because of the algorithms. For example, consider the case when you have a dog and a cat in an image at the same time. What is the class of the image: cat or dog? And what is the content of the image: A cat or a dog? Of course, both are in there, but classification algorithms give you one class only, so they are unable to tell you that you have two animals in the image. And what if you have many cats and many dogs? What if you have several objects? You get the idea.\n\nIt may be interesting to know where the cat and the dog are in the image. Consider the problem of a self-driving car: it is important to know where a person is, since that could mean the difference between a dead passerby and a living one. Classification, as we have looked at in the previous chapters, often cannot be used alone to solve real-life problems with images. Typically, recognizing that you have many instances of an object in an image involves finding their position in an image and being able to distinguish between them. To do that, we need to be able to find the positions of each instance in the image and their borders. This is one of the most interesting (and more difficult) tasks in image recognition techniques that can be solved with CNNs.\n\n196",
      "content_length": 1963,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nTypically, with object localization we want to determine the location\n\nof an object (for example, a person or a car) in an image and draw a rectangular bounding box around it.\n\nNote With object localization, we want to determine the location of one or more objects (for example, people or cars) in an image and draw a rectangular bounding box around it.\n\nSometimes in the literature researchers use the term localization when\n\nthe image contains only one instance of an object (for example, only one person or only one car) and the term detection when an image contains several instances of an object.\n\nNote Localization typically refers to when an image contains only one instance of an object, while detection is when there are several instances of an object in an image.\n\nTo summarize and clarify the terminology, here is an overview of all the words and terms used (a visual explanation is shown in Figure 6-1):\n\nClassification: Give a label to an image, or in other\n\nwords, “understand” what is in an image. For example, an image of a cat may have the label “cat” (we have seen several cases of this in the previous chapters).\n\nClassification and localization: Give a label to\n\nan image and determine the borders of the object contained in it (and typically draw a rectangle around the object).\n\n197",
      "content_length": 1355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nObject detection: This term is used when you have\n\nmultiple instances of an object in an image. In object detection, you want to determine all the instances of several objects (for example, people, cars, signs, etc.) and draw bounding boxes around them.\n\n\n\nInstance segmentation: You want to label each pixel of the image with a specific class for each separate instance, to be able to find the exact limits of the object instance.\n\nSemantic segmentation: You want to label each\n\npixel of the image with a specific class. The difference with instance segmentation is that you don’t care if you have several instances of a car as examples. All pixels belonging to the cars will be labelled as “car”. In instance segmentation, you will still be able to tell how many instances of a car you have and where they are exactly. To understand the difference, see Figure 6-1.\n\nFigure 6-1. A visual explanation of the different terms describing the general task of locating one or more objects in an image\n\nSegmentation is typically the most difficult task of all of them, and in particular instance segmentation is particularly difficult. Many advanced techniques come together to solve those problems. One of the things to remember is that getting enough training data is not easy. Keep in mind\n\n198",
      "content_length": 1342,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nthat this is much more difficult than simple classification, since someone will need to mark where the objects are. With segmentation, someone needs to classify each pixel in the image, which means training data is very expensive and difficult to collect.\n\nMost Important Available Datasets\n\nA well-known dataset that can be used to work on these problems is the Microsoft COCO dataset at http://cocodataset.org. The dataset contains 91 object types with a total of 2.5 million labelled instances in 328,000 images.1 To give you an idea of the kind of labeling used, Figure 6-2 shows some examples from the dataset. You can see how specific instances of objects (like people and cats) are classified at the pixel level.\n\nFigure 6-2. Examples of the images in the COCO dataset\n\n1 The original paper describing the dataset is: Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Dollár, Microsoft COCO: Common Objects in Context, https://arxiv.org/abs/1405.0312\n\n199",
      "content_length": 1108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nA quick note about sizes: the 2017 training images are roughly 118,000\n\nand require 18GB2 of hard disk space, so keep that in mind. Training a network with such a large amount of data is not trivial and will require time and lots of computing power. There is an API to download the COCO images that you can use and that is also available in Python. More information can be found on the main web page or on the API GitHub repository at https://github.com/cocodataset/cocoapi. The images have five annotation types: object detection, keypoint detection, stuff segmentation, panoptic segmentation, and image captioning. More information can be found at http://cocodataset.org/#format-data. Another dataset that you may encounter is the Pascal VOC dataset. Unfortunately, the website is not that stable, and therefore mirrors exist where you can find the files. One mirror is https://pjreddie.com/ projects/pascal-voc-dataset-mirror/. Note that this is a much smaller dataset than the COCO dataset.\n\nIn this and the next chapter, we will concentrate mainly on object classification and localization. We will assume that in the images we have only one instance of a specific object, and the task is to determine what kind of object it is and draw a bounding box (a rectangle) around it. These present enough challenge for now! We will look briefly at how segmentation works, but we will not go into many details about it, since its problems are extremely difficult to solve. I will provide references that you may check and study on your own.\n\nIntersect Over Union (IoU)\n\nLet’s consider the task of classifying an image and then drawing a bounding box around the object in it. In Figure 6-3, you can see an example of the output we expect (where the class would be cat).\n\n2 http://cocodataset.org/#download\n\n200",
      "content_length": 1857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFigure 6-3. An example of object classification and localization3\n\nThis is a fully supervised task. This means that we will need to learn where the bounding boxes are and compare them to some given ground truth. We need a metric to quantify how good the overlap is between the predicted bounding boxes and the ground truth. This is typically done with the IOU (Intersect Over Union) . In Figure 6-4, you can see a visual explanation of it. As a formula, we could write\n\nIOU\n\n=\n\nArea of overlap Area of union\n\nFigure 6-4. A visual explanation of the IOU metric\n\n3 Image source: http://www.cbsr.ia.ac.cn/users/ynyu/detection.html\n\n201",
      "content_length": 683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nIn the ideal case of perfect overlap, we have IOU = 1, while if there is no\n\noverlap at all, we have IOU = 0. You will find this term in blogs and books, so it’s a good idea to know how to measure bounding boxes using the ground truth.\n\nA Naïve Approach to Solving Object Localization (Sliding Window Approach)\n\nA naïve way of solving the problem of localization is the following (spoiler: this is a bad idea but it’s instructive to see why):\n\n1. You cut a small portion of your input image\n\nstarting from the top-left corner. Let’s suppose your image has dimensions x, y, and your portion has dimensions wx, wy, with wx < x and wy < y.\n\n2. You use a pre-trained network (how you train it or\n\nhow you get it is not relevant here) and you let it classify the image portion that you cut.\n\n3. You shift this window by an amount we call stride\n\nand indicate with s toward the right and then below. You use the network to classify this second portion.\n\n4. Once the sliding window has covered the entire\n\nimage, you choose the position of the window that gives you the highest classification probability. This position will give you the bounding box of your object (remember your window has dimensions wx, wy).\n\nIn Figure 6-5, you can see a graphical illustration of the algorithm (we\n\nassumed wx = wy = s).\n\n202",
      "content_length": 1357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFigure 6-5. A graphical illustration of the sliding window approach to solve the problem of object localization\n\nAs you can see in Figure 6-5, we start from the top left and slide the window toward the right. As soon as we reach the right border of the image and we don’t have any space to shift the window further to the right, we get back on the left border but we shift it s pixel down. We continue in this fashion until we reach the lower-right corner of the image.\n\nYou might immediately see some problems with this method:\n\nDepending on the choice of wx, wy, and s, we may\n\nnot be able to cover the entire image. (Do you see in Figure 6-5 the small portion of the image on the right of window 4 that remains not analyzed?)\n\nHow do you choose wx, wy, and s? This is a rather nasty problem, since the bounding box of our object will have exactly the dimensions wx, wy. What if the object is larger or smaller? We typically don’t know in advance its dimensions and that is a huge problem if we want to have precise bounding boxes.\n\n203",
      "content_length": 1089,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nWhat if our object flows across two windows? In\n\nFigure 6-5, you can imagine that the object is half in window 2 and half in window 3. Then your bounding box would not be correct if you follow the algorithm as described.\n\nWe could solve the third problem by using s = 1 to be sure that we cover all possible cases, but the first two problems are not so easy to solve. To address the window size problem, we should try all possible sizes and all possible proportions. Do you see any problem here? The number of evulations that you will need to do with your network is getting out of control and will become quickly computationally infeasible.\n\nProblems and Limitations the with Sliding Window Approach\n\nIn the book’s GitHub repository, within the Chapter 6 folder, you can find an implementation of the sliding window algorithm. To make things easier, I decided to use the MNIST dataset since you should know it very well at this point and it’s an easy dataset to use. As a first step, I built a CNN trained on the MNIST dataset that reached 99.3% accuracy. I then proceeded to save the model and the weights on disk. The CNN I used has the following structure:\n\n204",
      "content_length": 1216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\n_______________________________________________________________ Layer (type) Output Shape Param # =============================================================== conv2d_1 (Conv2D) (None, 26, 26, 32) 320 _______________________________________________________________ conv2d_2 (Conv2D) (None, 24, 24, 64) 18496 _______________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64) 0 _______________________________________________________________ dropout_1 (Dropout) (None, 12, 12, 64) 0 _______________________________________________________________ flatten_1 (Flatten) (None, 9216) 0 _______________________________________________________________ dense_1 (Dense) (None, 128) 1179776 _______________________________________________________________ dropout_2 (Dropout) (None, 128) 0 _______________________________________________________________ dense_2 (Dense) (None, 10) 1290 =============================================================== Total params: 1,199,882 Trainable params: 1,199,882 Non-trainable params: 0 _______________________________________________________________\n\nI then saved the model and weights using this code (we already\n\ndiscussed how to do this):\n\nmodel_json = model.to_json() with open(\"model_mnist.json\", \"w\") as json_file: json_file.write(model_json) model.save_weights(\"model_mnist.h5\")\n\nYou can see in Figure 6-6 how the network training and accuracy\n\nchanges with the number of epochs.\n\n205",
      "content_length": 1518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFigure 6-6. Loss function value and accuracy for the training (continuous line) and for the validation (dashed line) dataset versus the number of epochs\n\nThe weights and model can be found in the GitHub repository. I did that to avoid having to re-train the CNN every time. I can reuse the model every time by reloading it. You can do it with this code (after you mount your Google drive if you want to run the code in Google Colab as I did):\n\nmodel_path = '/content/drive/My Drive/pretrained-models/model_ mnist.json' weights_path = '/content/drive/My Drive/pretrained-models/ model_mnist.h5'\n\njson_file = open(model_path, 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights(weights_path)\n\nTo make things easier. I decided to create a larger image with one digit\n\nin the middle and see how efficiently I can put a bounding box around it. To create the image, I used the following code:\n\n206",
      "content_length": 1026,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nfrom PIL import Image, ImageOps src_img = Image.fromarray(x_test[5].reshape(28,28)) newimg = ImageOps.expand(src_img,border=56,fill='black')\n\nThe resulting image is 140x140 pixel. You can see it in Figure 6-7.\n\nFigure 6-7. The new image created by adding a white border of 56 pixels around one of the digits in the MNIST dataset\n\nNow let’s start with a sliding window that’s 28x28 pixels. We can write\n\na function that will try to localize the digit and that will get the image as input, the stride s, and the values wx and wy:\n\ndef localize_digit(bigimg, stride, wx, wy): slidx, slidy = wx, wy\n\ndigit_found = -1 max_prob = -1 bbx = -1 # Bounding box x upper left bby = -1 # Bounding box y upper left max_prob_ = 0.0 bbx_ = -1 bby_ = -1 most_prob_digit = -1\n\n207",
      "content_length": 813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nmaxloopx = (bigimg.shape[0] -wx) // stride maxloopy = (bigimg.shape[1] -wy) // stride print((maxloopx, maxloopy))\n\nfor slicey in range (0, maxloopx*stride, stride): for slicex in range (0, maxloopy*stride, stride): slice_ = bigimg[slicex:slicex+wx, slicey:slicey+wx] img_ = Image. fromarray(slice_).resize((28, 28), Image.\n\nNEAREST)\n\nprobs = loaded_model.predict(np.array(img_).\n\nreshape(1,28,28,1)) if (np.max(probs > 0.2)): most_prob_digit = np.argmax(probs) max_prob_ = np.max(probs) bbx_ = slicex bby_ = slicey\n\nif (max_prob_ > max_prob): max_prob = max_prob_ bbx = bbx_ bby = bby_ digit_found = most_prob_digit\n\nprint(\"Digit \"+str(digit_found)+ \" found, with probability \"+str(max_prob)+\" at coordinates \"+str(bbx)+\" \"+str(bby))\n\nreturn (max_prob, bbx, bby, digit_found)\n\nRunning on our image as so:\n\nlocalize_digit(np.array(newimg), 28, 28, 28)\n\nReturns this code:\n\nDigit 1 found, with probability 1.0 at coordinates 56 56 (1.0, 56, 56, 1)\n\n208",
      "content_length": 1001,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nThe resulting bounding boxes can be seen in Figure 6-8.\n\nFigure 6-8. The bounding box found by the sliding window method with wx = 28, wy = 28, and stride s = 28\n\nSo that works quite well. But you may have noticed that we used values\n\nfor wx, wy, and s that are exactly 28, which is the size of our images. What happens if we change that? For example, consider the cases depicted in Figure 6-9. You can clearly see how this method stops working as soon as the size and proportions of the window change to different values than 28.\n\n209",
      "content_length": 586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFigure 6-9. Results of the sliding window algorithm with different values of wx, wy, and s\n\nCheck the confidence of the classification in Figure 6-9, in the lower- left box. It is quite low. For example, for a window 40x40 and a stride of 10, the classification of the digit is correct (a 1) but is done with a probability of 21%. That’s a low value! In the lower-right box, the classification is completely wrong. Keep in mind that you need to resize the small portion you cut from your image, and therefore it may look different from the training data you used.\n\nIn this case, it may seem easy to choose the right window size and proportions, since you know what the images looks like, but in general you have no idea what value will work. You would have to test different\n\n210",
      "content_length": 830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nproportions and sizes and get several possible bounding boxes and classifications and then decide which one is the best. You can easily see how this becomes computationally infeasible with real images that may contain several objects with different dimensions and proportions.\n\nClassification and Localization\n\nWe have seen that the sliding window approach is a bad idea. A better approach is to use multi-task learning. The idea is that we can build a network that will learn at the same time the class and the position of the bounding box. We can achieve that by adding two dense layers after the last one of a CNN. One with (for example) Nc neurons (to classify Nc classes) that will predict the class with a cross-entropy loss function (that we will indicate with Jclassification), and one with four neurons that will learn the bounding boxes with a ℓ2 loss function (that we will indicate with JBB). You can see a diagram of the network in Figure 6-10.\n\nFigure 6-10. A diagram that depicts a network that can predict the class and the bounding box position at the same time\n\n211",
      "content_length": 1134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nSince this will be a multi task learning problem, we will need to\n\nminimize a linear combination of the two loss functions:\n\nJ\n\nclassification\n\n+a J\n\nBB\n\nOf course, α is an additional hyper-parameter that needs to be tuned.\n\nJust as a reference a ℓ2 loss is proportional to the MSE\n\n2\n\nLoss Function\n\n=\n\nm\n\nå\n\n(\n\ny\n\n( ) i true\n\n\n\ny\n\n( ) i predicted\n\n)\n\n2\n\ni\n\n= 1\n\nWhere we have, as usual, indicated with m the number of observations we have at our disposal. This same idea is used very successfully in human pose estimation, which finds specific points of the human body (like for example, the joints), as can be seen in Figure 6-11.\n\nFigure 6-11. An example of human pose estimation. A CNN can be trained to find important points of the human body, such as the joints.\n\n212",
      "content_length": 826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nThere is a lot of research going on in this field, and in the next sections\n\nwe will look at how those methods work. The implementation becomes quite complex and time consuming. If you want to work with these algorithms, the best way is to look at the original papers and study them. Unfortunately, there is no plug-and-play library that you can use for those tasks, although you may find a GitHub repository that will help you. In this chapter, we will look at the most common variations of CNNs to do object localization—R-CNN, fast R-CNN, and faster R-CNN. In the next chapter, we will look at the YOLO (You Only Look Once) algorithm. The next sections should serve only as pointers to the relevant papers and will give you a basic understanding of the building blocks of the networks. This is by no means an exhaustive analysis of these implementations, as that would require a massive amount of space.\n\nRegion-Based CNN (R-CNN)\n\nThe basic idea of region-based CNNs (also known as R-CNNs) is quite simple (but implementing it is not). As we discussed, the main problem with naïve approaches is that you need to test a huge number of windows to be able to find the best matching bounding boxes. Searching every possible location is computationally infeasible, as it is testing all possible aspect ratios and window sizes.\n\nSo Girshick et al.4 proposed a method where they used an algorithm called selective search5 to first propose 2000 regions from the image (called the region proposals) and then, instead of classifying a huge number of regions, they classified just those 2000 regions.\n\n4 https://arxiv.org/pdf/1311.2524.pdf 5 Jasper R. R. Uijlings, Koen E. A. van de Sande, Theo Gevers, Arnold W. M. Smeulders International Journal of Computer Vision, Volume 104 (2), page 154-171, 2013 [http://toe.lt/b]\n\n213",
      "content_length": 1868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nSelective search has nothing to do with machine learning and uses a classical approach to determine which regions may contain an object. The first step in the algorithm is to segment an image, using pixel intensities and graph-based methods (for example, the one by Felzenszwalb and Huttenlocher6). You can see in Figure 6-12 the result of this segmentation.\n\nFigure 6-12. An example of segmentation applied to an image (image source: http://cs.brown.edu/people/pfelzens/segment/)\n\nAfter this step, adjacent regions are grouped together based on\n\nsimilarities of the following features:\n\nColor similarity\n\nTexture similarity\n\n6 P. Felzenszwalb, D. Huttenlocher, Efficient Graph-Based Image Segmentation, International Journal of Computer Vision, Vol. 59, No. 2, September 2004\n\n214",
      "content_length": 832,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nSize similarity\n\nShape compatibility\n\nThe exact details of how this is done go beyond the scope of this book, since those techniques are typically used in image-processing algorithms.\n\nIn the OpenCV7 library, there is an implementation of the algorithm that you can try. In Figure 6-13, you can see an example. I applied the algorithm to a picture I took and I asked the algorithm to propose 40 regions.\n\nFigure 6-13. An example of the output of the selective search algorithm as implemented in the OpenCV library\n\n7 https://opencv.org\n\n215",
      "content_length": 591,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nThe Python code that I used can be found on the following website:\n\nhttps://www.learnopencv.com/selective-search-for-object- detection-cpp-python/. The main idea of R-CNN is to use a CNN to label the regions that this algorithm proposed and then use support vector machines for the final classification.\n\nIn Figure 6-9, you can see for example that the laptop has not been identified as an object. But that is why one uses 2000 regions in R-CNN, to make sure that enough regions are proposed. Checking many regions manually cannot be done visually by a person. The number of regions and their overlap is so big that the task is not feasible anymore. If you try the OpenCV implementation of the algorithm, you will notice that it is quite slow. This is one of the main reasons that additional methods have been developed. The manual approach is, for example, not suitable for real-time object detection (for example, in a self driving car).\n\nR-CNN can be summarized in the following steps (the steps have been\n\ntaken from http://toe.lt/d):\n\n1. Take a pre-trained imagenet CNN (such as Alexnet).\n\n2. Re-train the last fully connected layer with the\n\nobjects that need to be detected and the “no-object” class.\n\n3. Get all proposals (around 2000 region proposals for each image) from selective search and resize them to match the CNN input.\n\n216",
      "content_length": 1393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\n4. Train SVM to classify each region between object and background (one binary SVM for each class).\n\n5. Use bounding box regression. Train a linear\n\nregression classifier that will output some correction factor for the bounding boxes.\n\nFast R-CNN\n\nGirshick improved on its algorithm and created what are known as “fast R-CNNs”.8 The main idea behind this algorithm is the following\n\n1. The image goes through the CNN and feature maps are extracted (the output of the convolutional layers).\n\n2. Regions are proposed, not based on the initial image, but based on the feature maps.\n\n3. Then the same feature maps and the proposed\n\nregions are used passed to a classifier that decides which object is in which region.\n\nA diagram explaining these steps is shown in Figure 6-14.\n\n8 https://arxiv.org/pdf/1504.08083.pdf\n\n217",
      "content_length": 868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFigure 6-14. A diagram depicting the main steps of the algorithm for fast R-CNN\n\nThe reason this algorithm is faster than R-CNN is because you don’t have to feed 2000 region proposals to the convolutional neural network every time9—you do it only once.\n\n9 http://toe.lt/c\n\n218",
      "content_length": 327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFaster R-CNN\n\nNote that R-CNN and fast R-CNN both use selective search to propose regions, and therefore are relatively slow. Even fast R-CNN needs around two seconds for each image, making this variation not suitable for real- time object detection. R-CNN needs around 50 seconds, and fast R-CNN around two seconds. But it turns out we can do even better, by removing the need to use selective search, since this turns out to be the bottleneck of both algorithms.\n\nRen et al.10 developed a new idea: to use a neural network to learn regions from labelled data, removing the slow selective search algorithm altogether. Faster R-CNN requires around 0.2 seconds, making them a fast algorithm for object detection. There is a very nice diagram depicting the main steps of a faster R-CNN that can be found at http://toe.lt/e.11 We report it in Figure 6-15 for you since I think it really helps in intuitively understanding the main building blocks of a faster R-CNN. The details tend to be quite complicated and therefore an intuitive and superficial description will not serve you. To understand the steps and the subtleties, you need more time and experience.\n\n10 https://arxiv.org/pdf/1506.01497.pdf 11 Part of the image appears in the original paper by Ren, but additional labels and information have been added by Leonardo Araujo dos Santos (https://legacy. gitbook.com/@leonardoaraujosantos).\n\n219",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "Chapter 6\n\nObjeCt ClassifiCatiOn: an intrOduCtiOn\n\nFigure 6-15. A diagram depicting the main parts of faster R-CNN. Image source: http://toe.lt/e.\n\nIn the next chapter, we will look at another algorithm (YOLO) and see\n\nhow you can use those techniques in your own projects.\n\n220",
      "content_length": 278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "CHAPTER 7\n\nObject Localization: An Implementation in Python\n\nIn this chapter, we will look at the YOLO (You Only Look Once) method for object detection. The chapter is split in two parts: in the first section we learn how the algorithm works, and in the second section, I will give an example of how you can use it in your own Python projects.\n\nKeep in mind that YOLO is quite complicated, so for 99% of you, a pre-trained model is the best choice for doing object detection. For the 1% at the forefront of the research, you probably don’t need this book anyway and you should know how to do object detection starting from scratch.\n\nThis chapter (as the previous one) should serve to point you in the right direction, give you the fundamentals you need to understand the algorithm, and give you your first experiences with object detection. You will notice quite soon that those methods are slow, difficult to implement, and have many limitations. This is a very active research field that is also very young. The paper describing YOLO version 3 (that we will use later in the chapter in the Python code) was published just in April, 2018. At the time of this writing, it’s less than two years old!\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_7\n\n221",
      "content_length": 1321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nThose algorithms are difficult to implement, difficult to understand, and very difficult to train. I hope that by the end of this chapter, you will understand the basics of it, and you can perform your first tests with the models.\n\nNote Those algorithms are difficult to implement, difficult to understand, and very difficult to train.\n\nThe You Only Look Once (YOLO) Method\n\nIn the last chapter, we looked at several methods for object detection. I also showed you why using a sliding window is a bad idea and where the difficulties are. In 2015, Redmon J. et al. proposed a new method to do object detection: they called it YOLO (You Only Look Once). They developed a network that can perform all the necessary tasks (detect where the objects are, classify multiple objects, etc.) in one pass. This is one of the reasons that this method is fast and is used often in real-time applications.\n\nIn the literature, you will find three versions of the algorithm: YOLOv1, YOLOv2, and YOLOv3. v2 and v3 are improvements over v1 (more on that later). The original network has been developed and trained with darknet, a neural network framework developed by the author of the original algorithm, Redmon J. You will not find an easy-to-download, pre-trained model that you can use with Keras. More on that later when I give you an example of how you can use it in your projects.\n\nIt is very instructive to read the original paper on YOLO, which can be\n\nfound at https://arxiv.org/abs/1506.02640.\n\n222",
      "content_length": 1552,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nNote The main idea of the method is to reframe the detection problem as one single regression problem, from the pixels of the image as inputs, to the bounding box coordinates and class probabilities1.\n\nLet’s see how it works in detail.\n\nHow YOLO Works\n\nTo understand how YOLO works, it’s best to go through the algorithm step by step.\n\nDividing the Image Into Cells\n\nThe first step is to divide the image into S × S cells. For each cell, we predict what (and if an) object is in the cell. Only one object will be predicted for each cell, so one cell cannot predict multiple objects. Then for each cell, a certain number (B) of bounding boxes that should contain the object are predicted. In Figure 7-1, you can see the grid and the bounding boxes that the network might predict (as an example). In the original paper, the image was divided into a 7 × 7 grid, but for the sake of clarity in Figure 7-1, I divided the image into a 5x5 grid.\n\n1 Redmon J. et al., “You Only Look Once: Unified, Real-Time Object Detection,” https://arxiv.org/abs/1506.02640.\n\n223",
      "content_length": 1118,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nFigure 7-1. Image divided into a 5 × 5 grid. For cell D3, we will predict the mouse and will predict bounding boxes (the yellow boxes). For cell B2, we will predict a bottle and its bounding boxes (the red rectangles).\n\nLet’s take as an example cell D3 in Figure 7-1. This cell will predict the presence of a mouse and then it will predict a certain number B of bounding boxes (the yellow rectangles). Similarly, cell B2 will predict the presence of the bottle and B bounding boxes (the red rectangles in Figure 7-1) all at the same time. Additionally, the model predicts a class confidence (a number) for each bounding box. To be precise, the model output for each cell is as follows:\n\nFor each bounding box (B in total), there are four\n\nvalues: x, y, w, h. These are the position of the center, its width, and its height. Note that the position of the center is given with relationship to the cell position, not as an absolute value.\n\n224",
      "content_length": 1001,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nFor each bounding box (B in total), there is a\n\nconfidence score, which is a number that reflects how likely the box contains the object. In particular, at training time, if we indicate the probability of the cell containing the object as Pr(Object), the confidence is calculated as follows:\n\n(\n\nPr Object\n\n)´\n\nIOU\n\nWhere IOU indicates the Intersection Over Union, which is calculated using the training data (see the previous chapter for an explanation of the term and how to calculate it). This number encodes at the same time the probability that a specific object is in a box and how good the bounding box fits the object.\n\nTherefore, supposing we have S = 5, B = 2 and supposing the network can classify Nc = 80 classes, the network will have an output of size of the following:\n\n(\n\n´ ´ S S B\n\n´ + 5\n\nNc\n\n)= ´ ´ ´ + (\n\n5 5\n\n2 5 80\n\n)=\n\n2250\n\nIn the original paper, the authors used S = 7, B = 2 and used the VOC dataset2 with 20 labelled classes. Therefore, the output of the network was as follows:\n\n(\n\n´ ´ S S B\n\n´ + 5\n\nNc\n\n)= ´ ´ ´ + (\n\n7 7\n\n2 5 20\n\n)=\n\n1470\n\nThe network structure is quite easy. It’s simply a set of several\n\nconvolutional layers (with some maxpool thrown in there) and a big dense layer at the end to predict the necessary values (remember the problem is\n\n2 http://host.robots.ox.ac.uk/pascal/VOC/\n\n225",
      "content_length": 1390,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nframed as a regression problem). In the original paper, the authors were inspired by the GoogLeNet model. The network has 24 layers followed by two dense layers (the last one having 1470 neurons; do you see why?). Training took, as the authors mentioned, one entire week. They used a few tricks for the training, and if you are interested, I strongly suggest you read the original paper. It’s quite instructive (for example, they also used learning rate decay in an unusual way, increasing the value of the learning rate at the beginning and then lowering it later). They also used dropout and extensive data augmentation. Training those models is not a trivial undertaking.\n\nYOLOv2 (Also Known As YOLO9000)\n\nThe original YOLO version had some shortcomings. For example, it was not very good at detecting objects that were too close. In the second version,3 the authors introduced some optimizations, the most important one being anchor boxes. The network gives pre-determined sets of boxes, and instead of predicting bounding boxes completely from scratch, it simply predicted deviations from the set of anchor boxes. The anchor boxes can be chosen depending on the type of objects that you want to predict, making the network better at certain specific tasks (for example, small or big objects).\n\nIn this version, they also changed the network structure, using 19 layers and then 11 more layers specifically designed for object detection, for a total of 30 layers. This version also struggled with small objects (also when using anchor boxes). This was because the layers downsampled the image and, during the forward pass-through, the network information was lost, making detecting small things difficult.\n\n3 Redmon J., Farhadi A., “YOLO9000: Better, Faster, Stronger,” https://arxiv.org/ abs/1612.08242\n\n226",
      "content_length": 1872,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nYOLOv3\n\nThe last version4 introduces a few new concepts that makes the model quite powerful. Here are the main improvements:\n\nPredicting boxes at different scales: The model predicts boxes with different dimensions, so to say (is a bit more complicated than that, but that should give you an intuitive understanding of what is going on).\n\nThe network is much bigger: A total of 53 layers.\n\nThe network uses skip connections. Basically, this\n\nmeans that the output of a layer will be fed not only to the very next layer but also to a layer coming later in the network. This way, the information not yet downsampled will be used later to make detecting small objects easier. Skip connections are used in ResNets (not discussed in this book), and you can find a good introduction at http://toe.lt/w.\n\nThis version uses nine anchor boxes, three for\n\neach scale.\n\nThis version predicts more bounding boxes for\n\neach cell.\n\nAll those improvements make YOLOv3 quite good, but also quite slow,\n\ndue to the increased computational power needed to process all those numbers.\n\n4 Redmon J., Farhadi A., “YOLOv3: An Incremental Improvement,” https://arxiv. org/pdf/1804.02767.pdf\n\n227",
      "content_length": 1232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nNon-Maxima Suppression\n\nOnce you have all the predicted bounding boxes, you need to choose the best one. Remember that for each cell and object, the model predicts several bounding boxes (regardless of which version you use). Basically, you choose the best bounding box by following this procedure (called non- maxima suppression):\n\n1.\n\nIt first discards all cells in which the probability of an object being present is less than a given threshold (typically 0.6).\n\n2.\n\nIt takes all the cells with the highest probability of having an object inside.\n\n3.\n\nIt takes the bounding boxes that have the highest score and removes all other bounding boxes that have an IOU greater than a certain threshold (typically 0.5) with each other. That means that it removes all bounding boxes that are very similar to the chosen one.\n\nLoss Function\n\nNote that the networks mentioned previously have a large number of outputs, so you should not expect a simple loss function to work. Also note that different parts of the final layer have very different meanings. One part is bounding box positions, one part is class probabilities, and so on. The loss function has three parts:\n\nClassification loss\n\nLocalization loss (the error between predicted bounding boxes and the expected results)\n\nConfidence loss (whether there is an object in the box)\n\nLet’s take a closer look at these three aspects of loss.\n\n228",
      "content_length": 1452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nClassification Loss\n\nThe classification loss used is determined by\n\n2\n\nS\n\n( å å ( )- ( ) ˆ p c i\n\nobj i\n\np c i\n\n\n\n)\n\n2\n\ni\n\n=\n\n0\n\nÎ c classes\n\nWhere\n\nobjis1 if an object is in cell i, or 0 otherwise. i ˆp ci( ) denotes the probability of having class c in cell i.\n\nLocalization Loss\n\nThis loss measures the error of the predicted bounding boxes with respect to the expected ones.\n\nl\n\ncoord\n\nS\n\n2\n\nB\n\nåå \n\ni\n\n=\n\n0\n\n=\n\n0\n\nobj i\n\né ë\n\n( x i\n\nˆ x\n\ni\n\n2\n\n) +\n\n( y i\n\nˆ y\n\ni\n\n)\n\n2\n\nù û\n\n+\n\nl\n\ncoord\n\n2\n\nS\n\nå\n\ni\n\n=\n\n0\n\nB\n\nå \n\n=\n\n0\n\nobj i\n\né ê ë\n\n(\n\nw\n\ni\n\n\n\nˆ w\n\n2\n\n) +\n\n(\n\nh i\n\n\n\nˆ h i\n\n)\n\n2\n\nù ú û\n\nConfidence Loss\n\nThe confidence loss measures the error when deciding if an object is in the box or not.\n\n2\n\nS\n\nB\n\nåå \n\nobj ij\n\n(\n\nˆ - C Ci\n\n)\n\n2\n\ni\n\n=\n\n0\n\nj\n\n=\n\n0\n\nWhere ˆCi is the confidence of the box j in cell i. objis1 if the jth bounding box in cell i is responsible for detecting the ij object.\n\n229",
      "content_length": 965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nSince most cells does not contain an object, we must be careful. The network could learn that the background is important. We need to add a term to the cost function to remedy this. This is done with the additional term:\n\nlnoobj\n\nS\n\n2\n\nB\n\nåå \n\nnoobj ij\n\n(\n\nˆ - C Ci\n\ni\n\n)\n\n2\n\n=\n\n0\n\nj\n\n=\n\n0\n\nWhere ij\n\nnoobj is the opposite of ij\n\nobj .\n\nTotal Loss Function\n\nThe total loss function is simply the sum of all the terms:\n\nL\n\n=\n\nS\n\n2\n\nå å\n\nobj i\n\n\n\ni\n\n=\n\n0\n\nÎ c classes\n\n(\n\n( )- ( ) ˆ p c i\n\np c i\n\n2\n\n) +\n\nl\n\ncoord\n\nS\n\n2\n\nB\n\nåå \n\ni\n\n=\n\n0\n\n=\n\n0\n\noobj i\n\né ë\n\n( x i\n\nˆ x\n\ni\n\n2\n\n) +\n\n( y i\n\nˆ y\n\nl\n\ncoord\n\nS\n\n2\n\nB\n\nåå \n\ni\n\n=\n\n0\n\n=\n\n0\n\nobj i\n\né ê ë\n\n(\n\n2\n\nS\n\nB\n\nåå \n\nobj ij\n\n(\n\nˆ - C Ci\n\ni\n\nw\n\ni\n\n2\n\n) +\n\n\n\nˆ w\n\n)\n\n22\n\nl\n\nnoobj\n\n2\n\nS\n\nå\n\n+\n\n(\n\nB\n\nå \n\nh i\n\nnoobj ij\n\n\n\nˆ h i\n\n)\n\n(\n\nˆ - C Ci\n\n2\n\ni\n\nù ú+ û )\n\n2\n\ni\n\n=\n\n0\n\nj\n\n=\n\n0\n\ni\n\n=\n\n0\n\njj\n\n=\n\n0\n\nAs you can see, it’s a complicated formula to implement. This is one of the reasons that the easiest way to do object detection is to download and use a pre-trained model. Starting from scratch will require some time and effort. Believe me.\n\nIn the next sections, we will look at how you can use YOLO algorithms\n\n(in particular, YOLOv3) in your own Python projects.\n\n230\n\n)\n\n2\n\nù û\n\n+",
      "content_length": 1293,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nYOLO Implementation in Python and OpenCV Darknet Implementation of YOLO\n\nIf you followed the previous sections, you understand that developing your own models for YOLO from scratch is not feasible for a beginner (and for almost all practitioners), so, as we have done in previous chapters, we need to use pre-trained models to use object detection in your projects. The web page where you can find all the pre-trained models you could ever want is https://pjreddie.com. This is the home page of Joseph C. Redmon, the maintainer of Darknet.\n\nNote Darknet is an open source neural network framework written in C and CUDa. it is fast, easy to install, and supports CpU and GpU computation.\n\nOn a subpage (https://pjreddie.com/darknet/yolo/), you will find all the information you need about the YOLO algorithm. You can download from this page the weights of several pre-trained models. For each model, you will always need two files:\n\nA .cfg file, which basically contains the structure of the\n\nnetwork.\n\nA .weights file, which contains the weights obtained\n\nafter training.\n\nTo give you an idea of the content of the files, the .cfg file contains, among other things, information on all the layers used. An example follows:\n\n[convolutional] batch_normalize=1 filters=64 size=3\n\n231",
      "content_length": 1340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nstride=1 pad=1 activation=leaky\n\nThis tells you how that particular convolutional layer is structured. The\n\nmost important information contained in the file is about:\n\nNetwork architecture\n\nAnchor boxes\n\nNumber of classes\n\nLearning rate and other parameters used\n\nBatch size\n\nThe other file (.weights) contains the pre-trained weights that you need in order to perform inference. Note that they are not saved in a Keras compatible format (like the .h5 files we have used so far), so they cannot be loaded in a Keras model unless you convert them first.\n\nThere is no standard tool or utility to convert those files, since the format is not constant (it has changed for example between YOLOv2 and YOLOv3). If you are interested in using YOLO up to v2, you can use the YAD2K library (Yet Another Darknet 2 Keras), which can be found at https://github.com/allanzelener/YAD2K.\n\nNote that this does not work on YOLOv3 .cfg files. Believe me, I have\n\ntried. But if you are happy with YOLOv2, you can use the code in this repository to convert the .weight files into a more Keras-friendly format. I also want to point out another GitHub repository that implemented a converter for YOLOv3 at https://github.com/qqwweee/keras-yolo3. It has some limitations (for example, you must use standard anchors), but it may be a good starting point to convert the files. However, there is an easier way to use the pre-trained models and that is using OpenCV, as we will see later in the chapter.\n\n232",
      "content_length": 1541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nTesting Object Detection with Darknet\n\nIf you simply want to perform some classification on an image, the easiest way to do that is to follow the instructions on the darknet website. Let’s look at how that works here. Note that the instructions work if you are on a Linux or MacOS X system. On Windows, you need to have make, gcc, and several other tools installed. As described on the website, the installation needs only a few lines:\n\ngit clone https://github.com/pjreddie/darknet cd darknet make wget https://pjreddie.com/media/files/yolov3.weights\n\nAt this point, you can simply perform your object detection with this:5\n\n./darknet detect cfg/yolov3.cfg yolov3.weights table.jpg\n\nNote that the .weight file is very big (around 237MB). Keep that in mind when downloading it. On a CPU this is quite slow; it took a very modern MacBook Pro from 2018 18 seconds to download. You can see the result in Figure 7-2.\n\n5 You can find the image used for testing in the GitHub repository within Chapter 7.\n\n233",
      "content_length": 1064,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nFigure 7-2. YOLOv3 used with darknet on a test image\n\nBy default, a threshold of 0.25 is used. But you can specify a different\n\none using the -thresh XYZ parameter. You must change XYZ to the threshold value you want to use.\n\nThis method is nice for playing with object detection, but it’s difficult to use in your Python projects. To do that, you will need to be able to use the pre-trained models in your code. There are several ways to do that, but the easiest way is to use the opencv library. If you are working with images, chances are that you are already working with this library. If you have never heard of it, I strongly suggest you check it out, since it’s a great library for working with images. You can find the official web page at https:// opencv.org.\n\nYou can, as usual, find the entire code in the GitHub repository, within the Chapter 7 folder of this book. We will discuss only the most important parts for brevity.\n\n234",
      "content_length": 1002,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nYou will need to have the newest opencv library installed. The code we\n\ndiscuss here has been developed with version 4.1.0. To determine which version you have, use this:\n\nimport cv2 print (cv2.__version__)\n\nTo try the code we discuss here, you need three files from the https://\n\npjreddie.com website:\n\n\n\ncoco.names\n\n\n\nyolov3.cfg\n\n\n\nyolov3.weights\n\ncoco.names contains the labels of the classes that the pre-trained model can classify. The yolov3.cfg and yolov3.weights files contain the model configuration parameters (as we have discussed) and the weights we need to use. For your convenience, since the yolov3.weights is about 240MB and cannot be uploaded to GitHub, you can download a ZIP file of all three at http://toe.lt/r. In the code, we need to specify where the files are. For example, you can use the following code:\n\nweightsPath = \"yolo-coco/yolov3.weights\" configPath = \"yolo-coco/yolov3.cfg\"\n\nYou need to change the location to where you saved the files on your\n\nsystem. OpenCV provides a function to load the weights without the need to convert them:\n\nnet = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n\nThis is quite confortable, since you don’t need to analyze or write your\n\nown loading function. It returns a model object that we will use later for inference. If you remember from the discussion about the method at the\n\n235",
      "content_length": 1413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nbeginning of the chapter, we need to get the output layers, in order to get all the information we need, like bounding boxes or predicted classes. We can do that easily with the following code:\n\nln = net.getLayerNames() ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n\nThe getUnconnectedOutLayers() function returns indexes of layers with unconnected outputs, which is exactly what we are looking for. The ln variable will contain the following layers:\n\n['yolo_82', 'yolo_94', 'yolo_106']\n\nThen we need to resize the image in a square 416x416 image and\n\nnormalize it by dividing the pixel values by 255.0:\n\nblob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n\nThen we need to use it as input to our model saved in the net model:\n\nnet.setInput(blob)\n\nAnd then we can use the forward() call to do a forward pass-through\n\nof the pre-trained model:\n\nlayerOutputs = net.forward(ln)\n\nWe are not yet done, so don’t relax. We need to extract the bounding boxes, which we will save in the boxes list, then the confidences, saved in the confidences list, and then the predicted classes, saved in the classIDs list.\n\nWe first initialize the lists as follows:\n\nboxes = [] confidences = [] classIDs = []\n\n236",
      "content_length": 1299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nThen we loop over the layers and extract the information we need. We\n\ncan perform the loops as follows:\n\nfor output in layerOutputs: for detection in output:\n\nNow the scores are saved in the elements starting from the fifth in the detection variable, and we can extract the predicted class with np. argmax(scores):\n\nscores = detection[5:] classID = np.argmax(scores)\n\nThe confidence is of course the score of the predicted class:\n\nconfidence = scores[classID]\n\nWe want to keep predictions with a confidence bigger than zero. In the\n\ncode used here, we chose a limit of 0.15. The predicted bounding box is contained in the first four values of the detection variable:\n\nbox = detection[0:4] * np.array([W, H, W, H]) (centerX, centerY, width, height) = box.astype(\"int\")\n\nAnd if you remember, YOLO predicts the center of the bounding box,\n\nso we need to extract the upper-left corner position:\n\nx = int(centerX - (width / 2)) y = int(centerY - (height / 2))\n\nAnd then we can simply append the found values to the lists:\n\nboxes.append([x, y, int(width), int(height)]) confidences.append(float(confidence)) classIDs.append(classID)\n\n237",
      "content_length": 1192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nThen we need to use non-maxima suppression (as discussed in the\n\nprevious sections). OpenCV provides also a function6 for it:\n\nidxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.6,0.2)\n\nThe function needs the following parameters:\n\nA set of bounding boxes (saved in the boxes variable)\n\nA set of confidences (saved in the confidences\n\nvariable)\n\nA threshold used to filter boxes by score (0.6 in the\n\nprevious code)\n\nThe threshold used in non-maximum suppression (the\n\n0.2 in the previous code)\n\nThen we can obtain the right coordinates with this simple code:\n\nfor i in idxs.flatten(): # extract the bounding box coordinates (x, y) = (boxes[i][0], boxes[i][1]) (w, h) = (boxes[i][2], boxes[i][3])\n\nYou can see in Figure 7-3 the results of this code.\n\n6 You can find the official documentation at http://toe.lt/t.\n\n238",
      "content_length": 873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nFigure 7-3. YOLOv3 results obtained with OpenCV\n\nThat is exactly as it should be—the same results as in Figure 7-2. In addition, we have the probability of the prediction on the box. You can see how easy this is. You simply add those few lines of code to your project. Keep in mind that the model we built using the pre-trained weights will only detect the objects that are contained in the image dataset that the pre-trained model has been trained with. If you need to use the model on different objects, you need to fine-tune the models, or train it from scratch for your objects. Describing how to train the model completely from scratch is beyond the scope of the book, but in the next section, I provide some pointers in case you need to do it.\n\n239",
      "content_length": 815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nTraining a Model for YOLO for Your Specific Images\n\nI will not describe the different procedures you need to train your own YOLO models, since that would take a few chapters on its own, but I hope I can point you in the right direction. Let’s suppose you want to train a model specifically for your images. As a first step, you need the training data. Supposing you have enough images, you first need to label them. Remember that you need to mark the right bounding boxes for each image. Doing that manually is an almost impossible task, so I suggest two projects that will help you label your training data.\n\nBBox-Label-Tool by Darkflow Annotations: This tool can be found at https://github.com/enriqueav/ BBox-Label-Tool. The tool saves the annotations in the right format as expected by Darkflow (a Python wrapper that can use darknet weight files, https:// github.com/thtrieu/darkflow).\n\n\n\nlabelImg: This tool can be found at https://github. com/tzutalin/labelImg. This tool can be used with several Python installations (including Anaconda, for example) and on several operating systems (including Windows).\n\nCheck them out in case you want to try to train your YOLO model on your data. Since describing the entire procedure would go well beyond the scope of the book, I suggest you read the following medium post, which does quite a good job at describing how to do that: http://toe.lt/v. Remember that you need to modify a cfg file so that you can specify the right number of classes that you are trying to identify. For example, in the yolov3.cfg file, you will find this line (at line 610):\n\nclasses=80\n\n240",
      "content_length": 1677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "ChapTer 7\n\nObjeCT LOCaLizaTiOn: an impLemenTaTiOn in pyThOn\n\nIt tells you how many classes you can identify with the models. You will\n\nneed to modify this line to reflect the number of classes you have in your problem.\n\nOn the official YOLO website, there is a detailed description of how to\n\ndo that: https://pjreddie.com/darknet/yolo/. Scroll down until you find the sections on training the models with your own datasets. Do not underestimate the complexity of this task. Lots of reading and testing will be required.\n\nConcluding Remarks\n\nAs you might have noticed, using these advanced techniques is quite complicated and not simply a matter of copying a few lines of code. You need to make sure you understand how the algorithms work to be able to use them in your own projects. Depending on the object you need to detect, you may need to spend quite some time building a custom model suited for your problem. That will require lots of testing and coding. It will not be easy. My goal with this chapter was to give you enough tools to help you and point you in the right direction.\n\nAfter the previous chapters, you have now enough understanding of advanced techniques to be able to re-implement even complicated algorithms as YOLO on your own, although this will require time and effort. You will suffer a lot, but if you don’t give up, you will be rewarded with success. I am sure of it.\n\nIn the next chapter, we look at a complete example that uses CNNs on real data, where we use all the techniques that we have learned so far. Consider Chapter 8 as an exercise. Try to play with the data and reproduce the results described there. I hope you have fun!\n\n241",
      "content_length": 1666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "CHAPTER 8\n\nHistology Tissue Classification\n\nNow it’s time to put all we have learned together and see how the techniques we have learned so far can be used on a real dataset. We will use a dataset that I have used successfully as my end project in my university course on deep learning: the “collection of textures in colorectal cancer histology”.1 This dataset can be found on several websites:\n\n\n\nhttp://toe.lt/f: On zenodo.org\n\n\n\nhttp://toe.lt/g: On Kaggle (this dataset was prepared originally by Kevin Mäder2 and me for the purpose of the university course we held during the Autumn semester of 2018 at the Zürich University of Applied Science)\n\n\n\nhttp://toe.lt/h: Since TensorFlow 2.0, this is also available as a pre-read dataset (the link points to the TensorFlow GitHub repository for the dataset’s API)\n\n1 Kather JN, Weis CA, Bianconi F, Melchers SM, Schad LR, Gaiser T, Marx A, Zollner F: Multi-class texture analysis in colorectal cancer histology (2016), Scientific Reports (in press) 2 https://www.linkedin.com/in/kevinmader/\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5_8\n\n243",
      "content_length": 1162,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nDon’t download the data yet. I prepared a pickle (more on that later) file for you with all the data ready to be used. You will find all the information in the next section.\n\nThe thing we will use in this chapter is the Kather_texture_2016_\n\nimage_tiles_5000 folder and it contains 5000 histological images of 150x150px each (74x74μm). Each image belongs to exactly one of eight tissue categories (specified by the folder name from the Zenodo website). In the code, I assume that, within the folder where you have your Jupyter Notebooks, you have a data folder and under that data folder, you have the Kather_texture_2016_image_tiles_5000 folder.\n\nIn the GitHub repository for this book, the folder for Chapter 8 contains the complete code that you can use. In this chapter, we will look only at the parts that are relevant to our discussions. If you want to try this, please use the GitHub repository. The code is complete and directly usable. The goal of this project is to build a classifier that can classify the different images into one of eight classes. We will look at them in the next sections and see where the difficulties are. Let’s start, as usual, with the data.\n\nMost of the code was developed by Fabien Tarrade (https://www. linkedin.com/in/fabientarrade/) for my university course, and he was nice enough to give me permission to use it. I have updated it quite a bit to make it usable in this example. Note that everything that works is thanks to Fabien, and all the bugs are my fault.\n\nData Analysis and Preparation\n\nThe code for this section is contained in the notebook called 01- Data exploration and preparation.ipynb, which is in the book’s GitHub repository in the Chapter 8 folder. Feel free to follow this discussion with a window open on your computer to try the code. Since we have the images in different folders, we need to load them in a pandas dataframe and automatically generate a label from the folder name. For example, the image\n\n244",
      "content_length": 2015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\n1A11_CRC-Prim-HE-07_022.tif_Row_601_Col_151.tif is contained in the folder 01_TUMOR and therefore must have \"TUMOR\" as its label.\n\nWe can automate that process in a very simple way. We start with this\n\ncode (for all the imports, please check the code in GitHub):\n\ndf = pd.DataFrame({'path': glob(os.path.join(base_dir, '*', '*.tif'))})\n\nThis generates a dataframe with just one column, 'path'. This column\n\ncontains the path to each image we want to load. The base_dir variable contains the path to the Kather_texture_2016_image_tiles_5000 folder. For example, I am running the code in Google Colab and my base_dir looks like this:\n\nbase_dir = '/content/drive/My Drive/Book2-ch8/data/Kather_ texture_2016_image_tiles_5000'\n\nThe first five records of my dataframe look like this:\n\n/content/drive/My Drive/Book2-ch8/data/Kather_texture_2016_ image_tiles_5000/05_DEBRIS/5434_CRC-Prim-HE-04_002.tif_Row_451_ Col_1351.tif /content/drive/My Drive/Book2-ch8/data/Kather_texture_2016_ image_tiles_5000/05_DEBRIS/626A_CRC-Prim-HE-08_024.tif_Row_451_ Col_1.tif /content/drive/My Drive/Book2-ch8/data/Kather_texture_2016_ image_tiles_5000/05_DEBRIS/148A7_CRC-Prim-HE-04_004.tif_ Row_151_Col_901.tif /content/drive/My Drive/Book2-ch8/data/Kather_texture_2016_ image_tiles_5000/05_DEBRIS/6B37_CRC-Prim-HE-08_024.tif_ Row_1501_Col_301.tif /content/drive/My Drive/Book2-ch8/data/Kather_texture_2016_ image_tiles_5000/05_DEBRIS/6B44_CRC-Prim-HE-03_010.tif_Row_301_ Col_451.tif\n\n245",
      "content_length": 1509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nNow we can use the .map() function to extract all the information we\n\nneed and create new columns.\n\ndf['file_id'] = df['path'].map(lambda x: os.path.splitext(os. path.basename(x))[0]) df['cell_type'] = df['path'].map(lambda x: os.path.basename(os. path.dirname(x))) df['cell_type_idx'] = df['cell_type'].map(lambda x: int(x. split('_')[0])) df['cell_type'] = df['cell_type'].map(lambda x: x.split('_')[1]) df['full_image_name'] = df['file_id'].map(lambda x: x.split('_ Row')[0]) df['full_image_row'] = df['file_id'].map(lambda x: int(x. split('_')[-3])) df['full_image_col'] = df['file_id'].map(lambda x: int(x. split('_')[-1]))\n\nYou can easily check what each call is doing. The column name should\n\ntell you what you will have in each column. In Figure 8-1, you can see the first two records of the dataframe so far.\n\nFigure 8-1. The first two records of the dataframe df before loading the images\n\nAt this point, we must read the images with imread(). To do this, we\n\ncan simply use\n\ndf['image'] = df['path'].map(imread)\n\nKeep in mind that this can take some time (depending on where you are running it). This will create a new column called image that will contain\n\n246",
      "content_length": 1216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nthe images. For your convenience, I used the to_pickle() pandas call to save the dataframe to disk. Pickling is the process whereby a Python object hierarchy is converted into a byte stream3 and then can be saved on-disk. The file is called dataframe_Kather_texture_2016_image_tiles_5000. pkl. You can load it with:\n\ndf=pd.read_pickle('/content/drive/My Drive/Book2-ch8/data/ dataframe_Kather_texture_2016_image_tiles_5000.pkl')\n\nThis way, you can save yourself lots of time. You don’t even need to download the data since you can simply use the pickle I prepared for you. Note that the pickles are too big for GitHub, so I saved them on a server where you can download them. You will find the links in GitHub and at the end of this section. First things first: what classes do we have in this dataset? We can check the labels we have with this code:\n\ndf['cell_type'].unique()\n\nThis will give us the following:\n\narray(['DEBRIS', 'ADIPOSE', 'LYMPHO', 'EMPTY', 'STROMA', 'TUMOR', 'MUCOSA', 'COMPLEX'], dtype=object)\n\nSo here are our eight classes. We have 5000 images, which we can\n\ncheck using this:\n\ndf.shape\n\nIt gives us this:\n\n(5000, 8)\n\n3 From the official Python documentation: https://docs.python.org/2/library/ pickle.html\n\n247",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nThe next step is to check if we have a balanced class distribution. We\n\ncan count how many images we have for each class:\n\ndf['cell_type'].value_counts()\n\nLuckily, we have exactly 625 images for each class.\n\nEMPTY 625 ADIPOSE 625 STROMA 625 COMPLEX 625 LYMPHO 625 DEBRIS 625 TUMOR 625 MUCOSA 625 Name: cell_type, dtype: int64\n\nStrangely enough, there are five duplicate images. You can check that\n\nwith this code:\n\ndf['full_image_name'][df.duplicated('full_image_name')]\n\nThis will report the names of the images that appear twice. You can see them in Figure 8-2. Since there are only five, we will simply ignore this problem.\n\nFigure 8-2. Five images appear twice in the dataset\n\n248",
      "content_length": 728,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nIn Figure 8-3, you can see a few examples of each class.\n\nFigure 8-3. Examples of the images in each class\n\n249",
      "content_length": 155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nAs expected, each image has a size of (150, 150, 3):\n\ndf['image'][0].shape (150, 150, 3)\n\nNote how the classes are ordered, which is due to how we loaded the data.\n\nThe DEBRIS class comes first, then ADIPOSE, and so on. This can be checked using a plot of the class label versus the index, as you can see in Figure 8-4.\n\nFigure 8-4. A plot showing how the images in the dataframe are ordered\n\nNow we can randomly shuffle the elements:\n\nimport random rows = df.index.values random.shuffle(rows) print(rows)\n\nThat will give you\n\narray([1115, 4839, 3684, ..., 187, 1497, 2375])\n\nYou can see that the indexes are now randomly shuffled. The last step\n\nwe need to take is to modify the actual dataframe:\n\ndf=df.reindex(rows) df.sort_index(inplace=True)\n\n250",
      "content_length": 795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nAt this point, the elements are shuffled. Now we need to one-hot- encode the labels. Pandas provide a very useful and easy-to-use method for this process:\n\ndf_label = pd.get_dummies(df['cell_type'])\n\nIt will give you one-hot-encoded labels, as you can see from Figure 8-5.\n\nFigure 8-5. The result of using the get_dummies() pandas function to one-hot-encode labels\n\nThere are a few steps that we need to use the data with Keras. One is\n\nthat we need to transform the dataframe to a numpy array:\n\ndata=np.array(df['image'].tolist())\n\nThen, as usual, we need to create a training, test, and development\n\ndataset to make all the usual checks:\n\nx, x_test, y, y_test = train_test_split(data, label, test_ size=0.2,train_size=0.8) x_train, x_val, y_train, y_val = train_test_split(x, y, test_ size = 0.25,train_size =0.75)\n\nYou can check the dimensions of the three datasets easily with this code:\n\nprint('1- Training set:', x_train.shape, y_train.shape) print('2- Validation set:', x_val.shape, y_val.shape) print('3- Testing set:', x_test.shape, y_test.shape)\n\n251",
      "content_length": 1104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nThat should give you the following:\n\n1- Training set: (3000, 150, 150, 3) (3000, 8) 2- Validation set: (1000, 150, 150, 3) (1000, 8) 3- Testing set: (1000, 150, 150, 3) (1000, 8)\n\nNow you will see that the data is of type integer. We need to cast them\n\nto floating point numbers since we want to normalize them later. To do that, we use this code:\n\nx_train = np.array(x_train, dtype=np.float32) x_test = np.array(x_test, dtype=np.float32) x_val = np.array( x_val, dtype=np.float32)\n\nThen we can normalize the datasets (remember that each pixel will\n\nhave a maximum value of 255):\n\nx_train /= 255.0 x_test /= 255.0 x_val /= 255.0\n\nFor your convenience, I saved all the prepared datasets as pickles. If\n\nyou want to follow from here and play with the data, you need to load the pickles using the following commands (you will need to change the folder name where the files are saved):\n\nx_train=pickle.load(open('/content/drive/My Drive/Book2-ch8/ data/x_train.pkl', 'rb')) x_test=pickle.load(open('/content/drive/My Drive/Book2-ch8/ data/x_test.pkl', 'rb')) x_val=pickle.load(open('/content/drive/My Drive/Book2-ch8/ data/x_val.pkl', 'rb')) y_train=pickle.load(open('/content/drive/My Drive/Book2-ch8/ data/y_train.pkl', 'rb')) y_test=pickle.load(open('/content/drive/My Drive/Book2-ch8/ data/y_test.pkl', 'rb')) y_val=pickle.load(open('/content/drive/My Drive/Book2-ch8/ data/y_val.pkl', 'rb'))\n\n252",
      "content_length": 1441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nYou will then have everything ready. Keep in mind that the files\n\ncontaining the data (x_train, x_test, and x_val) are big files, with x_train being 800MB unzipped. Keep that in mind if you plan to download the files or upload them on your Google drive. Of course, you will need to change the folder to where your data is saved. This will save you time. Pickles are usually saved, since you don’t want to rerun the entire data preparation each time you experiment with the data. In the 01- Data exploration and preparation. ipynb files, you will also find some histogram analysis and data augmentation examples. For space reasons and to keep this chapter compact, we will not look at histogram analysis, but we will talk about data augmentation later in the chapter, as it’s a very effective way of fighting overfitting.\n\nThe files were too big for GitHub, so I put them on a server where you can download them. In the GitHub repository (the Chapter 8 folder), you will find all the information. If you don’t have access to GitHub and you still want to download the files, here are the links:\n\n\n\ndataframe_Kather_texture_201_image_tiles_5000. pkl (340MB unzipped): http://toe.lt/j\n\n\n\nx_test.pkl (270MB unzipped): http://toe.lt/k\n\n\n\nx_train.pkl (810MB unzipped): http://toe.lt/m\n\n\n\nx_val.pkl (270MB unzipped): http://toe.lt/n\n\n\n\ny_train, y_test, and y_val (all zipped together) (about 50KB unzipped): http://toe.lt/p\n\nModel Building\n\nIt is time to build some models. You will find all the code in the book’s GitHub repository (Chapter 8 folder, in the 02_Model_building.ipynb notebook), so we will not look at all the details here. The best way to follow along is to keep the notebook open and try the code while you are reading this. As mentioned, we first need to load the pickle files. We can do that with the following code:\n\n253",
      "content_length": 1876,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nx_train=pickle.load(open(base_dir+'x_train.pkl', 'rb')) x_test=pickle.load(open(base_dir+'x_test.pkl', 'rb')) x_val=pickle.load(open(base_dir+'x_val.pkl', 'rb')) y_train=pickle.load(open(base_dir+'y_train.pkl', 'rb')) y_test=pickle.load(open(base_dir+'y_test.pkl', 'rb')) y_val=pickle.load(open(base_dir+'y_val.pkl', 'rb'))\n\nThen we need to define the input_shape variable that we will need for\n\nour CNNs. In the code we always define functions that return the Keras models. For example, our first try looks like this:\n\ndef model_cnn_v1():\n\n# must define the input shape in the first layer of the\n\nneural network\n\nmodel = tf.keras.models.Sequential() model.add(tf.keras.layers.Conv2D(32, 3, 3, input_\n\nshape=input_shape))\n\nmodel.add(tf.keras.layers.Activation('relu')) model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(tf.keras.layers.Conv2D(64, 3, 3)) model.add(tf.keras.layers.Activation('relu')) model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(64)) model.add(tf.keras.layers.Activation('relu')) model.add(tf.keras.layers.Dropout(0.5)) model.add(tf.keras.layers.Dense(8)) model.add(tf.keras.layers.Activation('sigmoid'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return model\n\n254",
      "content_length": 1371,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nThis is a simple network, as you can check with the summary() function: _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 50, 50, 32) 896 _________________________________________________________________ activation (Activation) (None, 50, 50, 32) 0 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 25, 25, 32) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 8, 8, 64) 18496 _________________________________________________________________ activation_1 (Activation) (None, 8, 8, 64) 0 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64) 0 _________________________________________________________________ flatten (Flatten) (None, 1024) 0 _________________________________________________________________ dense (Dense) (None, 64) 65600 _________________________________________________________________ activation_2 (Activation) (None, 64) 0 _________________________________________________________________ dropout (Dropout) (None, 64) 0 _________________________________________________________________ dense_1 (Dense) (None, 8) 520 _________________________________________________________________ activation_3 (Activation) (None, 8) 0 ================================================================= Total params: 85,512 Trainable params: 85,512 Non-trainable params: 0 _________________________________________________________________\n\n255",
      "content_length": 1684,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nTo make sure that the session is reset, we always use:\n\ntf.keras.backend.clear_session()\n\nThen we create an instance of the model, as follows:\n\nmodel_cnn_v1=model_cnn_v1()\n\nThen we also save the initial weights to make sure, if we do runs later,\n\nthat we start from these same weights:\n\ninitial_weights = model_cnn_v1.get_weights()\n\nThen we train the model with this:\n\nmodel_cnn_v1.set_weights(initial_weights)\n\n# define path to save the mnodel path_model=base_dir+'model_cnn_v1.weights.best.hdf5' shutil.rmtree(path_model, ignore_errors=True)\n\ncheckpointer = ModelCheckpoint(filepath=path_model, verbose = 1, save_best_only=True) EPOCHS=200 BATCH_SIZE=256\n\nhistory=model_cnn_v1.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_test, y_test), callbacks=[checkpointer])\n\n256",
      "content_length": 847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nNote a few points:\n\nWe create a custom CallBack class ModelCheckpoint, which will save the weights of the network during training every time the loss functions diminishes.\n\nWe train the network with the fit() call and save its\n\noutput in a history variable, to be able to plot loss and metrics later.\n\nNote training such networks may be very slow if you do it on your laptop or desktop, depending on the hardware you have. i strongly suggest you do that on google Colab, since this will speed up your testing. all the notebooks in the book’s github repository have been tested on google Colab and can be opened in google Colab directly from github.\n\nOn Google Colab, training the previous network will take roughly three\n\nminutes. It will reach the following accuracies:\n\nAccuracy on the training dataset: 85%\n\nAccuracy on the validation dataset: 82.7%\n\nThese results are not bad, and we don’t have much overfitting (you can\n\nsee in Figure 8-6 how accuracy and loss change with the epochs).\n\nFigure 8-6. Accuracy and loss function for the first network described in the text\n\n257",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nLet’s move to a different model, which we will call v2. This one has a lot\n\nmore parameters than the previous one: _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 150, 150, 128) 9728 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 75, 75, 128) 0 _________________________________________________________________ dropout (Dropout) (None, 75, 75, 128) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 75, 75, 64) 73792 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 37, 37, 64) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 37, 37, 64) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 37, 37, 64) 36928 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 18, 18, 64) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 18, 18, 64) 0 _________________________________________________________________ flatten (Flatten) (None, 20736) 0 _________________________________________________________________ dense (Dense) (None, 256) 5308672 _________________________________________________________________ dense_1 (Dense) (None, 64) 16448 _________________________________________________________________ dense_2 (Dense) (None, 32) 2080 _________________________________________________________________ dense_3 (Dense) (None, 8) 264 ================================================================= Total params: 5,447,912 Trainable params: 5,447,912 Non-trainable params: 0 _________________________________________________________________\n\n258",
      "content_length": 1960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nAgain, you can find all the code in the GitHub repository. We will train\n\nit again, but this time, for time reasons, for 50 epochs and with a slightly smaller batch size of 64.\n\nEPOCHS=50 BATCH_SIZE=64\n\nhistory=model_cnn_v2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_test, y_test), callbacks=[checkpointer])\n\nOtherwise, everything remains the same. This time, due to the sheer number of parameters, you will notice that we get an evident overfitting. In fact, we get the following accuracies:\n\nAccuracy on the training dataset: 99.5%\n\nAccuracy on the validation dataset: 74%\n\nYou can clearly see the overfitting in Figure 8-7, looking at the plot of\n\nthe accuracies versus the number of epochs.\n\nFigure 8-7. Accuracies and loss functions versus the number of epochs for the v2 network\n\n259",
      "content_length": 869,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nWe need to work a bit more to get some more reasonable results. Now let’s use a network with fewer parameters (in particular, with fewer kernels):\n\n_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 150, 150, 16) 448 _________________________________________________________________ conv2d_1 (Conv2D) (None, 150, 150, 16) 2320 _________________________________________________________________ conv2d_2 (Conv2D) (None, 150, 150, 16) 2320 _________________________________________________________________ dropout (Dropout) (None, 150, 150, 16) 0 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 50, 50, 16) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 50, 50, 32) 4640 _________________________________________________________________ conv2d_4 (Conv2D) (None, 50, 50, 32) 9248 _________________________________________________________________ conv2d_5 (Conv2D) (None, 50, 50, 32) 9248 _________________________________________________________________ dropout_1 (Dropout) (None, 50, 50, 32) 0 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_6 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ conv2d_7 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ conv2d_8 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ dropout_2 (Dropout) (None, 16, 16, 64) 0 _________________________________________________________________\n\n260",
      "content_length": 1893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nmax_pooling2d_2 (MaxPooling2 (None, 5, 5, 64) 0 _________________________________________________________________ conv2d_9 (Conv2D) (None, 5, 5, 128) 73856 _________________________________________________________________ conv2d_10 (Conv2D) (None, 5, 5, 128) 147584 _________________________________________________________________ conv2d_11 (Conv2D) (None, 5, 5, 256) 295168 _________________________________________________________________ dropout_3 (Dropout) (None, 5, 5, 256) 0 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256) 0 _________________________________________________________________ global_max_pooling2d (Global (None, 256) 0 _________________________________________________________________ dense (Dense) (None, 8) 2056 ================================================================= Total params: 639,240 Trainable params: 639,240 Non-trainable params: 0\n\nWe will call this network v3. This time, the situation is not much better,\n\nas you can see in Figure 8-8.\n\nFigure 8-8. Accuracies and loss functions versus. the number of epochs for the v3 network.\n\n261",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nWhy don’t we use what we have learned so far? Let’s use transfer learning and see if we can use a pre-trained network. Let’s download the VGG16 network and retrain the last layers with our data. To do that, we need to use the following code (we will call this network vgg-v4):\n\ndef model_vgg16_v4():\n\n# load the VGG model vgg_conv = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape = input_shape)\n\n# freeze the layers except the last 4 layers for layer in vgg_conv.layers[:-4]: layer.trainable = False\n\n# Check the trainable status of the individual layers for layer in vgg_conv.layers: print(layer, layer.trainable)\n\n# create the model model = tf.keras.models.Sequential()\n\n# add the vgg convolutional base model model.add(vgg_conv)\n\n# add new layers model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(1024, activation='relu')) model.add(tf.keras.layers.Dropout(0.5)) model.add(tf.keras.layers.Dense(8, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nreturn model\n\n262",
      "content_length": 1125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nNote how we downloaded the pre-trained network (as we have seen in\n\nprevious chapters) with this code:\n\nvgg_conv = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape = input_shape)\n\nWe used the include_top=False parameter, since we want to remove the final dense layers and put our own in their place. We add a layer with 1024 neurons at the end:\n\nmodel.add(tf.keras.layers.Dense(1024, activation='relu'))\n\nThen we add an output layer with 8 as the softmax activation function\n\nfor classification:\n\nmodel.add(tf.keras.layers.Dense(8, activation='softmax'))\n\nThe summary() call will give you this overview:\n\n_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= vgg16 (Model) (None, 4, 4, 512) 14714688 _________________________________________________________________ flatten (Flatten) (None, 8192) 0 _________________________________________________________________ dense (Dense) (None, 1024) 8389632 _________________________________________________________________ dropout (Dropout) (None, 1024) 0 _________________________________________________________________ dense_1 (Dense) (None, 8) 8200 ================================================================= Total params: 23,112,520 Trainable params: 15,477,256 Non-trainable params: 7,635,264 _________________________________________________________________\n\n263",
      "content_length": 1504,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nThe entire vgg16 network is condensed into one line (vgg16 (Model)).\n\nIn this network, we have 15’477’256 trainable parameters. Quite a few. In fact, training this network for 30 epochs will require around 11 minutes on Google Colab. You can see in Figure 8-9 how accuracy and loss change with the number of epochs.\n\nFigure 8-9. Accuracies and loss functions versus the number of epochs for the vgg-v4 network\n\nAs you can see, the situation is better, but we still get overfitting. It’s not\n\nas dramatic as before, but still quite noticeable. The only strategy we have to fight this is data augmentation. In the next sections, we’ll see how easy it is to do data augmentation in Keras and the effects it has.\n\nData Augmentation\n\nOne obvious strategy to fight overfitting (although one that is rarely doable in real life) is to get more training data. In our case here, this is not possible. The images given are the only ones available. But we can still do something in this case: data augmentation. What do we mean by that exactly? Typically, data augmentation consists of generating new images from existing ones by applying some kind of transformation to them and using them as additional training data.\n\n264",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nNote Data augmentation consists of generating new images from existing ones by applying some kind of transformation to them and using them as additional training data.\n\nThe most common transformations are as follows:\n\nShifting the image by a certain number of pixels\n\nhorizontally or vertically\n\nRotating the image\n\nChanging its brightness\n\nChanging the zoom\n\nChanging the contrast\n\nShearing the image4\n\nLet’s see how to do data augmentation in Keras, and let’s look\n\nat a few examples in our dataset. The function we need to use is ImageDataGenerator. To start, you need to import it from keras_ preprocessing.image:\n\nfrom keras_preprocessing.image import ImageDataGenerator\n\nNote that this function will not generate new images and save them\n\nto disk, but will create augmented image data for you just-in-time during the training in random fashion (later it will become clear how to use it). This will not require much additional memory, but will add\n\n4 In plane geometry, a shear mapping is a linear map that displaces each point in a fixed direction, by an amount proportional to its signed distance from the line that is parallel to that direction and goes through the origin. See https:// en.wikipedia.org/wiki/Shear_mapping.\n\n265",
      "content_length": 1280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nsome additional time during model training. The function can do lots of transformations and the best way to discover them all is to look at the official documentation at https://keras.io/preprocessing/image/. We will look at the most important ones with examples.\n\nHorizontal and Vertical Shifts\n\nTo shift images horizontally and vertically, you use the following code:\n\ndatagen = ImageDataGenerator(width_shift_range=.2, height_shift_range=.2, fill_mode='nearest')\n\n# fit parameters from data datagen.fit(x_train)\n\nThe result is shown in a few random images in Figure 8-10.\n\n266",
      "content_length": 623,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nFigure 8-10. The result of shifting images horizontally and vertically\n\nIf you check the images, you will notice how strange features appear at\n\nthe borders. Since we are shifting the image, we need to tell Keras how to fill the part of the image that remains empty. Consider Figure 8-11, where we shift an image horizontally. As you may notice, the part marked in the image with the A remains empty, and we can tell Keras how to fill that part using the fill_mode parameter.\n\n267",
      "content_length": 524,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nFigure 8-11. An example of shifting an image in the horizontal direction. The A marks the part of the resulting image that will remain empty.\n\nThe best way to understand the different possibilities for fill_mode is to consider a one-dimensional case. The explanation has been taken from the official documentation of the function. Let’s suppose we have a set of four pixels that will have some values that we indicate with a, b, c, and d. And let’s suppose we have boundaries that we need to fill. The parts that need to be filled are marked with o. Figure 8-12 shows a graphical explanation of the four possibilities: constant, nearest, reflect, and wrap.\n\nFigure 8-12. Possible values for the fill_mode parameter and a graphical explanation of the possibilities\n\n268",
      "content_length": 812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nThe images in Figure 8-11 have been generated using the nearest fill\n\nmode. Although this transformation introduces artificial features, using those additional images for training increases the accuracy of the model and fights overfitting extremely effectively, as we will see later in the chapter. The most common method to fill the empty parts is nearest.\n\nFlipping Images Vertically\n\nTo flip images vertically, the following code can be used:\n\ndatagen = ImageDataGenerator(vertical_flip=True)\n\n# fit parameters from data datagen.fit(x_train)\n\nRandomly Rotating Images\n\nYou can randomly rotate images with this code:\n\ndatagen = ImageDataGenerator(rotation_range=40, fill_mode = 'constant')\n\n# fit parameters from data datagen.fit(x_train)\n\nAnd, as with the shifting transformation, you can choose different ways\n\nof filling the empty areas. You can see the effect of this code in Figure 8-13.\n\n269",
      "content_length": 943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nFigure 8-13. The effect of rotating images in a random direction up to 40 degrees (the amount of rotation is chosen randomly up to 40 degrees). The parts of the images left empty by the rotation have been filled with a constant value.\n\nIn Figure 8-14, you can see the effect of the rotation when it’s filled with fill_mode = 'nearest'. Typically, this is the preferred way to fill the images to avoid giving black (or solid color) parts of images to the network.\n\n270",
      "content_length": 511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nFigure 8-14. The effect of rotating images in a random direction up to 40 degrees. The parts of the images left empty by the rotation have been filled with the nearest mode.\n\n271",
      "content_length": 222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nZooming in Images\n\nYou should now understand how these image transformations work. Zooming is as easy as the previous transformation:\n\ndatagen = ImageDataGenerator(zoom_range=0.2)\n\n# fit parameters from data datagen.fit(x_train)\n\nPutting All Together\n\nOne of the great things about Keras is that you don’t need to perform each transformation, one at a time. You can do everything in one shot. For example, consider this code:\n\ndatagen = ImageDataGenerator(rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=\"nearest\")\n\nThis will enhance your dataset greatly, with several transformations\n\ndone at the same time:\n\nRotation\n\nShift\n\nShear\n\nZoom\n\nFlip\n\nLet’s put everything together and see how effective this technique is.\n\n272",
      "content_length": 848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nVGG16 with Data Augmentation\n\nNow it’s time to train our vgg16 network with transfer learning and image augmentation. The only modification to the code we looked at before is in how we feed the data to train the model. Now we will need to use this code:\n\nhistory=model_vgg16_v4.fit_generator(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE), validation_data=(x_test,\n\ny_test),\n\nepochs=EPOCHS, callbacks=[checkpointer])\n\nInstead of the classical fit() call, we need to use fit_generator(). A small digression is necessary to explain the main differences between the two functions. Keras includes not two, but three functions that can be used to train a model:\n\n\n\nfit()\n\n\n\nfit_generator()\n\n\n\ntrain_on_batch()\n\nThe fit( ) Function\n\nUp to now, we used the fit() function when training our Keras models. The main implicit assumption when using this method is that the dataset that you feed to the model will fit completely in memory. We don’t need to move batches to and from memory. That is a pretty big assumption, especially if you are working on big datasets and your laptop or desktop doesn’t have a lot of memory available. Additionally, the assumption is that there is no need to do real-time data augmentation (as we want to do here).\n\n273",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nNote the fit() function is good for small datasets that can fit in your system memory and do not require real-time data augmentation.\n\nThe fit_generator( ) Function\n\nWhen the data does not fit in memory anymore, we need a smarter function that can help us deal with it. Note that the ImageDataGenerator we created before will generate, in a random fashion, batches that need to be fed to the model. The fit_generator() function assumes that there is a function that generates the data for it. When using fit_generator(), Keras follows this process:\n\n1. Keras calls the function that generates the batches.\n\nIn our code, that’s datagen.flow().\n\n2. This generator function returns a batch whose size is specified by the batch_size=BATCH_SIZE parameter.\n\n3. The fit.generator() function then performs backpropagation and updates the weights.\n\n4. This is repeated until we reach the number of\n\nepochs wanted.\n\nNote the fit_generator() function is meant to be used for bigger datasets that do not fit in memory and when you need to do data augmentation.\n\nNote that there is an important parameter that we have not used in our code: steps_per_epoch. The datagen.flow() function will generate a batch of images each time, but Keras needs to know how many such batches we want for each epoch, since the datagen.flow() can continue\n\n274",
      "content_length": 1371,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nto generate as many batches as we want (remember that they are generated in a random fashion). We need to decide how many batches we want before declaring each epoch finished. You can decide with the steps_per_epoch parameter, but if you don’t specify it, Keras will use len(generator)5 as the number of steps.\n\nThe train_on_batch( ) Function\n\nIf you need to fine-tune your training, the train_on_batch() function is the one to use.\n\nNote the train_on_batch() function accepts a single batch of data, performs backpropagation, and then updates the model parameters.\n\nThe batch of data can be arbitrarily sized and can be theoretically in any format you need. You need this function when you need, for example, to perform custom data augmentation that cannot be done by the standard Keras functions.\n\nNote as they say—if you don’t know whether you need the train_on_batch() function, you probably don’t.\n\nYou can find more information from the official documentation at\n\nhttps://keras.io/models/sequential/.\n\n5 https://keras.io/models/sequential/\n\n275",
      "content_length": 1094,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nTraining the Network\n\nWe can finally train our network and see how it performs. Training it for 50 epochs and with a batch size of 128 gives the following accuracies:\n\nAccuracy on the training dataset: 93.3%\n\nAccuracy on the validation dataset: 91%\n\nThat is a great result. Practically no overfitting and great accuracy. This\n\nnetwork took roughly 15 minutes on Google Colab, which is quite fast. Figure 8-15 shows the accuracies and loss versus the number of epochs.\n\nFigure 8-15. Accuracy and loss function versus the number of epochs for the VGG16 network with transfer learning and data augmentation\n\nTo summarize, we started with a simple CNN that was not too bad, but\n\nwe immediately realized that going deeper (more layers) and increasing the complexity (more kernels) led to overfitting quite dramatically. Adding dropout was not really helping, so the only solution was to use data augmentation.\n\nNote that we did not show the first networks described in this chapter with data augmentation for space reasons, but you should do that. If you try, you will realize that you fight overfitting quite efficiently, but the accuracy goes down. Using a pre-trained network gives us a very good starting point and allows us to go into the 90% accuracy regime in a few epochs.\n\n276",
      "content_length": 1324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "Chapter 8\n\nhistology tissue ClassifiCation\n\nAnd Now Have Fun…\n\nIn this book, you have learned powerful techniques that will allow you to read research papers, understand them, and start implementing more advanced networks that go beyond the easy CNNs that you find in blogs and websites. I hope you enjoyed the book and that it will help you in your journey toward deep learning mastery. Deep learning is really fun and an incredibly creative research field. I hope you now have a glimpse of the possibilities of the algorithms and the creativity involved. I love feedback and would love to hear from you. Don’t hesitate to get in touch and tell me how (and especially if) this book has helped you learn those algorithms.\n\n—Umberto Michelucci, Dübendorf, June 2019\n\n277",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "Index\n\nA Adam optimizer, 33, 162 Anaconda\n\nbenefits/drawbacks, 17–18 download, 9 install, 11–14 screen, 10\n\nB BBox-Label-Tool, 240 Building blocks, CNN convolutional\n\nlayer, 105–107\n\npooling layer, 108 stacking layers, 108, 109\n\nC call() function, 186 Chessboard image\n\nblurring kernel IB, 95, 96 creation, 91 horizontal edges, 93 kernel, IH, 92, 93 kernel, IL, 94, 95 kernel IV, 94, 97 transition, values, 98\n\n© Umberto Michelucci 2019 U. Michelucci, Advanced Applied Deep Learning, https://doi.org/10.1007/978-1-4842-4976-5\n\nClassification loss, 229 Confidence loss, 229 Content loss function, 179 Convolution\n\ndefinition, 85 example, chessboard (see\n\nChessboard image)\n\nkernel IH, 86, 87 kernels, 82 matrix, 82, 83 matrix 3 × 3, 88, 90 multiple channels, 125–128 Python, 90 size, 85 stride, 85, 87 tensors, 81, 82 visual explanation, 86 works, 86\n\nConvolutional neural network (CNN)\n\nbuilding blocks (see Building\n\nblocks, CNN)\n\nvisualization (see Visualization\n\nof CNN)\n\nweights\n\nconvolutional layer, 109 dense layer, 110 pooling layer, 110\n\n279",
      "content_length": 1049,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "Cost function\n\nDataset abstraction, 68–71\n\ncross-entropy (see\n\nCross-entropy)\n\nmathematical notation, 165, 166 MSE (see Mean Square Error (MSE))\n\niterator, 71 MNIST dataset, simple batching, 73, 74, 76\n\nsimple batching, 72 tf.data.Dataset, eager execution\n\nCross-entropy\n\nbinary classification\n\nproblem, 173–175\n\ndistributions of probabilities, 171 probability mass\n\nmode, 76, 77 Dataset.map() function, 69 Deep learning models, 141, 176 Digression, 139–141 Docker image, 18\n\nfunctions, 172, 173\n\nself-information, 169, 170 suprisal associated with\n\nEvent X, 171\n\nsuprisal of an event, 169, 170\n\nE Eager execution, TensorFlow\n\nadvantages, 28 enabling, 29, 30 MNIST dataset, 34\n\nD Data analysis, 244–253 Data augmentation\n\nflip images vertically, 269 horizontal and vertical shifts, 266–269\n\nrandomly rotate\n\nimages, 269–271\n\nVGG16\n\naccuracy and loss function vs. number of epochs, 276\n\nfit() function, 273, 274 fit_generator()\n\nfeed-forward neural\n\nnetwork, 36\n\nimplementation, 34 keras.datasets.mnist\n\npackage, 35 learning rate, 38 loss_value, 37 nested loops, 36 plot image, 38 tf.data.Dataset object, 35\n\npolynomial fitting, 30 adam optimizer, 33 loss function vs. iteration\n\nfunction, 274, 275\n\nnumber, 32, 33\n\ntrain_on_batch() function, 275 zooming in images, 272\n\nminimize loss function, 32 MSE, 31 numpy arrays, 31\n\n280",
      "content_length": 1326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "F Fast R-CNNs, 217–220 Filters, 79 fit() function, 273, 274 fit_generator() function, 274, 275\n\nG get_dummies() pandas\n\nfunction, 251\n\nget_next() method, 71 GitHub repository, 3 Google Colab, 5, 8, 184, 206 Google drive, 184, 206 GoogLeNet network, 135 Gramian matrices, 179\n\nH Hardware acceleration, TensorFlow\n\nchecking availability of\n\nGPU, 40, 41 device name, 41, 42 effect on MNIST, 45, 47 explicit device placement, 42 matrix multiplication, 43, 44\n\nI, J ImageDataGenerator, 265 Image filters, 177 Imagenet dataset, 177 imread() function, 246 Inception module\n\nclassical CNNs, 130\n\nIndex\n\ncomputational budget, 129 convolutional layers, 130 dimension reduction, 133, 134 functional APIs, Keras, 136–138 max-pooling operations, 132 MNIST dataset, 132 Naïve version, 131 number of parameters, 132\n\nInception networks, 123 input_shape variable, 254 Instance segmentation, 198 Intersect Over Union (IoU), 200–202\n\nit_init_op operation, 74\n\nK Keras, 116 keras.backend.function() CNTK backend, 116 computational graph, 115 dataset, 117 function(), 116 TensorFlow backend, 116 Theano backend, 116 Keras callback functions, 54 custom class, 55, 56 CC1 variable, 58 logs dictionary, 58–60 log variable, 58 on_epoch_end() function, 60 output, 61 Sequential model, 57\n\nKernels/filters, 79–81 effects, 118–121\n\n281",
      "content_length": 1308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "L LabelEncoder, 151 labelImg tool, 240 LeNet-5 network, 109 Linear regression, 164–165 load_img() function, 184, 185 Localization loss, 229 Loss function, 162, 188\n\nmnist_model.variables, 37 Model, building\n\naccuracies and loss functions vs. number of epochs\n\nv2 network, 259 v3 network, 261 vgg-v4 network, 264 accuracy and loss function\n\nclassification loss, 229 confidence loss, 229, 230 localization loss, 229 total, 230 value, 206\n\nM make_one_shot_iterator(), 71, 72 .map() function, 246 Masking, 192–193 Mathematical notation, 165–166 max_dim variable, 185 Mean Square Error (MSE), 31 intuitive explanation, 167 moment-generating\n\nv1 network, 257 GitHub repository, 259 fit() call, 257 include_top=False parameter, 263 input_shape variable, 254 Keras models, 254 ModelCheckpoint, 257 model_cnn_v1=model_cnn_\n\nv1(), 256\n\noverfitting, 259 softmax activation function, classification, 263 summary() function, 255, 263 tf.keras.backend.clear_ session(), 256\n\nfunction, 167–169 Microsoft COCO dataset, 199, 200 MNIST dataset, 126, 204\n\nMultiple cost functions, 134, 135 Multi-task learning (MTL), 141\n\nconvolutional layers, 112 fit() method, 114 import packages, 110 Keras model, 111, 112 load data, 112 load images, 111 model.summary(), 113\n\nN Naïve approach, object\n\nlocalization, 202–204\n\nNeural network models\n\ncomponents, 161, 162 linear regression, 164, 165\n\n282",
      "content_length": 1369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "optimization problem,\n\ntraining, 162, 163\n\noptimizer, 162\n\nNeural style transfer (NST) deep learning, 176 digital images, 176 in Keras, 183–190 masking, 192, 193 mathematics, 178–183 pre-trained network, 177 robust CNN, 177 Silhouettes, 190–192 Van Gogh painting, 177, 178\n\nnext_batch operation, 74 Non-central moments, 169 Non-maxima suppression, 228 numpy() method, 39\n\nO Object classification and\n\nlocalization, 197, 200, 201, 211, 213\n\nObject detection, 198, 200, 216,\n\n219, 221, 226, 231, 233–239\n\nObject localization\n\ndetection, 197, 233–239 instance segmentation, 198 IOU, 200–202 location of an object, 197 Microsoft COCO dataset, 199, 200 Naïve Approach, 202–204 Pascal VOC dataset, 200 self-driving car, 196\n\nIndex\n\nsemantic segmentation, 198 sliding window approach (see Sliding window approach)\n\non_epoch_end(self, epoch, logs) method, 55, 56, 60, 65, 66\n\nOptimizer, 2, 32, 33, 36, 37, 47, 68,\n\n162, 166 OutOfRangeError, 75\n\nP, Q Pascal VOC dataset, 200 Pickling, 247 Pooling\n\nconvolution, 102 max pooling, 100 padding, 104, 105 stride, 101 visualization, 102 Pre-trained CNNs, 176 Pre-trained networks, 276\n\napplications, 141 decode_predictions() function, 144 GitHub repository, 143 imagenet dataset, 143 Jupyter Notebook, 143 keras.applications package, 142\n\nProbability mass function (PMF), 170–173, 175 Python development environment\n\nAnaconda\n\nbenefits, 17 drawbacks, 18\n\n283",
      "content_length": 1393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "Python development\n\nSliding window approach, 203\n\nenvironment (cont.) installation, 9, 10 installing TensorFlow, 11–13 Jupyter Notebook, 14, 16 Python packages, 10, 11\n\nbounding box, 209 object localization, solving, 203 problems and\n\nlimitations, 204–211 size and proportions, window\n\nDocker image benefits, 24 correct-v option, 23 drawbacks, 24 install, 18–20 Jupyter instance, 21 pyhton, 22 Google Colab benefits, 8 create notebook, 6 definition, 5 drawbacks, 9 popup, 7 possibilities, 4\n\nchange, 209\n\nsoftmax activation functions, 174 StyleContentModel class, 186, 188\n\nT, U tape.gradient function, 180 TensorFlow, 13\n\ndataset abstraction (see Dataset\n\nabstraction)\n\neager execution (see Eager\n\nexecution, TensorFlow)\n\nhardware acceleration (see\n\nR Region-based CNN\n\nHardware acceleration, TensorFlow) numpy compatibility, 39 removing layers, 52–54 save model\n\n(R-CNN), 213–217\n\nS Segmentation, 198, 214 Selective search\n\nalgorithm, 215, 219\n\nSemantic segmentation, 198 Shannon entropy, 171 Silhouettes, 190–192\n\ncallback function, 63, 65 checkpoint_path, 65 Dense layer, 62 entire model, 68 latest variable, 67 MNIST dataset, 61, 62 validation dataset, 64 weights manually, 67, 68\n\ntraining specific layers, 47\n\nfeed-forward network, 48\n\n284",
      "content_length": 1246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "frozen_model.summary(), 50 layer.trainable property, 51 model.summary()\n\nfunction, 49 parameters, 49 trainable_model. summary(), 51\n\nTensorFlow Variable, 180 Tensor Processing Unit\n\n(TPU), 8, 189 tf.GradientTape, 189 tf.train.AdamOptimizer\n\nTensorFlow, 38 to_pickle() pandas, 247 Total loss function, 135, 230 train_on_batch() function, 275 Transfer learning\n\nbase network, 146 classical approach\n\nbinary classification,\n\n155–157\n\nCNN model, 150 dense layer, 155 LabelEncoder, 151 model, 151, 153 pooling layer, 155 training_data, 150 validation_data, 150 VGG16 pre-trained\n\nnetwork, 155\n\ndefinition, 145 dog vs. cat dataset, 149\n\nIndex\n\nexperimentation\n\ndataset preparation, 158, 159 flexible, 157 frozen layers, 157 target subnetwork, 157, 159 validation dataset, 160\n\nfeatures, 147 image recognition problems, 147 schematic representation, 148 target dataset, 146\n\nV, W, X Visualization of CNN\n\nkeras.backend.function(), 115,\n\n117, 118\n\nkernels effect, 118, 119, 121 max pooling effect, 121, 123\n\nY, Z You Only Look Once (YOLO)\n\nmethod\n\ndarknet detection, 233–237 darknet implementation, 231, 232 detection variable, 237 division of image, 223, 224 model output, 224 non-maxima suppression, 238 OpenCV, 239 versions, 222 YOLOv3, 227\n\n285",
      "content_length": 1240,
      "extraction_method": "Unstructured"
    }
  ]
}