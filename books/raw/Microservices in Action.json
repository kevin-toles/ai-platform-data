{
  "metadata": {
    "title": "Microservices in Action",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 394,
    "conversion_date": "2025-12-19T17:38:56.336046",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Microservices in Action.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-11)",
      "start_page": 1,
      "end_page": 11,
      "detection_method": "topic_boundary",
      "content": "M A N N I N G\nMorgan Bruce\nPaulo A. Pereira\n\n\nA microservice production environment\nControl pane\nRuntime\nmanagement\nManages\nNetwork and routing\nConnects\nObservability\nObserves\nDeployment pipeline\nMonitors\nEngineers\nWrites\nCode\nProduction\nA microservice production environment has several components: a deployment target,  \na deployment pipeline, runtime management, networking features, and support for  \nobservability. In this book, we'll teach you about these components and how you can use  \nthem to build a stable, modern microservice application.\n \n\n\nMicroservices in Action\n \n\n\nMicroservices in Action\nMORGAN BRUCE \nPAULO A. PEREIRA\nM A N N I N G\nShelter Island\n \n\n\nFor online information and ordering of this and other Manning books, please visit www.manning.com. \nThe publisher offers discounts on this book when ordered in quantity.\nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2019 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form \nor by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the \npublisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed \nas trademarks. Where those designations appear in the book, and Manning Publications was aware of a \ntrademark claim, the designations have been printed in initial caps or all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have the books \nwe publish printed on acid-­free paper, and we exert our best efforts to that end. Recognizing also our \nresponsibility to conserve the resources of our planet, Manning books are printed on paper that is at \nleast 15 percent recycled and processed without the use of elemental chlorine.\n∞\n\t\nManning Publications Co. \n20 Baldwin Road\nPO Box 761 \nShelter Island, NY 11964\n\t\nAcquisitions editor:\t\nMichael Stephens\n\t\nDevelopment editor:\t\nKaren Miller\n\tTechnical development editor:\t\nKarsten Strøbæk\n\t\nReview editor:\t\nAleksander Dragosavljevic´\n\t\nProject editor:\t\nAnthony Calcara\n\t\nCopy editor:\t\nCarl Quesnel\n\t\nProofreader:\t\nKeri Hales\n\t\nTechnical proofreader:\t\nJohn Guthrie\n\t\nTypesetter:\t\nHappenstance Type-O-Rama\n\t\nCover designer:\t\nMarija Tudor\nISBN 9781617294457\nPrinted in the United States of America\n1 2 3 4 5 6 7 8 9 10 — DP — 23 22 21 20 19 18\n \n\n\nv\nbrief contents\nPart 1\t\nThe lay of the land.................................................. 1\n1\t■\t Designing and running microservices  3\n2\t■\t Microservices at SimpleBank  28\nPart 2\t\nDesign........................................................................49\n3\t■\t Architecture of a microservice application  51\n4\t■\t Designing new features  75\n5\t■\t Transactions and queries in microservices   105\n6\t■\t Designing reliable services  129\n7\t■\t Building a reusable microservice framework  159\nPart 3\t\nDeployment............................................................185\n8\t■\t Deploying microservices  187\n9\t■\t Deployment with containers and schedulers  214\n10\t■\t Building a delivery pipeline for microservices  243\nPart 4\t\nObservability and ownership..............................267\n11\t■\t Building a monitoring system  269\n12\t■\t Using logs and traces to understand behavior  296\n13\t■\t Building microservice teams  325\n \n\n\nvii\ncontents\npreface xv\nacknowledgments xvii\nabout this book xix\nabout the authors xxii\nabout the cover illustration xxiii\nPart 1\t The lay of the land.......................................1\n1\nDesigning and running microservices  3\n\t1.1\t\nWhat is a microservice application?  4\nScaling through decomposition  6  ■  Key principles  7  ■  Who uses \nmicroservices?    ■  Why are microservices a good choice?  12\n\t1.2\t\nWhat makes microservices challenging?  14\nDesign challenges    ■  Operational challenges  17\n\t1.3\t\nMicroservice development lifecycle  18\nDesigning microservices    ■  Deploying  \nmicroservices  ■  Observing microservices  24\n\t1.4\t\nResponsible and operationally aware engineering \nculture  26\n \n\n\nviii\nviii\n﻿ CONTENTS\n\t\n2\t\nMicroservices at SimpleBank  28\n\t2.1\t\nWhat does SimpleBank do?  29\n\t2.2\t\nAre microservices the right choice?  30\nRisk and inertia in financial software  31  ■  Reducing friction and \ndelivering sustainable value  31\n\t2.3\t\nBuilding a new feature  32\nIdentifying microservices by modeling the domain    ■  Service \ncollaboration  3  ■  Service choreography  37\n\t2.4\t\nExposing services to the world  39\n\t2.5\t\nTaking your feature to production  40\nQuality-controlled and automated deployment  42 \nResilience  43Transparency  43\n\t2.6\t\nScaling up microservice development  45\nTechnical divergence  45  ■  Isolation  46\n\t2.7\t\nWhat’s next?  47\nPart 2\t Design...........................................................49\n\t\n3\t\nArchitecture of a microservice application  51\n\t3.1\t\nArchitecture as a whole  52\nFrom monolith to microservices  52  ■  The role of an \narchitect  54  ■  Architectural principles  54  ■  The four tiers  \nof a microservice application  55\n\t3.2\t\nA microservice platform  56\nMapping your runtime platform  57\n\t3.3\t\nServices  58\nCapabilities  58  ■  Aggregation and higher order \nservices  59  ■  Critical and noncritical paths  60\n\t3.4\t\nCommunication  60\nWhen to use synchronous messages  61  ■  When to use \nasynchronous messages  62  ■  Asynchronous communication \npatterns  63  ■  Locating other services  65\n3.5\tThe application boundary  66\nAPI gateways  68  ■  Backends for frontends  69  ■  Consumer-\ndriven gateways  70\n\t3.6\t\nClients  71\nFrontend monoliths  71  ■  Micro-frontends  72\n \n\n\n\t\nix\n\t\nix\n﻿ CONTENTS\n\t\n4\t\nDesigning new features  75\n\t4.1\t\nA new feature for SimpleBank  76\n\t4.2\t\nScoping by business capabilities  78\nCapabilities and domain modeling  78  ■  Creating investment \nstrategies   79  ■  Nested contexts and services  86  ■  Challenges \nand limitations  87\n\t4.3\t\nScoping by use case  87\nPlacing investment strategy orders   88  ■  Actions and stores  92   \nOrchestration and choreography  94\n\t4.4\t\nScoping by volatility  94\n\t4.5\t\nTechnical capabilities  96\nSending notifications  96  ■  When to use technical capabilities  98\n\t4.6\t\nDealing with ambiguity  99\nStart with coarse-grained services  99  ■  Prepare for further \ndecomposition  100  ■  Retirement and migration  100\n\t4.7\t\nService ownership in organizations  103\n\t\n5\t\nTransactions and queries in microservices   105\n\t5.1\t\nConsistent transactions in distributed applications  106\nWhy can’t you use distributed transactions?  107\n\t5.2\t\nEvent-based communication  108\nEvents and choreography  109\n\t5.3\t\nSagas  111\nChoreographed sagas  112  ■  Orchestrated sagas  115   \nInterwoven sagas  117  ■  Consistency patterns  118  ■  Event \nsourcing  119\n\t5.4\t\nQueries in a distributed world  120\nStoring copies of data  122  ■  Separating queries and \ncommands  123  ■  CQRS challenges  125  ■  Analytics  \nand reporting  127\n\t5.5\t\nFurther reading  128\n \n\n\nx\n﻿ CONTENTS\n\t\n6\t\nDesigning reliable services  129\n\t6.1\t\nDefining reliability  130\n\t6.2\t\nWhat could go wrong?  132\nSources of failure  133  ■  Cascading failures  136\n\t6.3\t\nDesigning reliable communication  139\nRetries  140  ■  Fallbacks  143  ■  Timeouts  145  ■  Circuit \nbreakers  146 \n■  Asynchronous communication  149\n\t6.4\t\nMaximizing service reliability  150\nLoad balancing and service health  150  ■  Rate limits  152   \nValidating reliability and fault tolerance  152\n\t6.5\t\nSafety by default  156\nFrameworks  156  ■  Service mesh  157\n\t\n7\t\nBuilding a reusable microservice framework  159\n\t7.1\t\nA microservice chassis  160\n\t7.2\t\nWhat’s the purpose of a microservice chassis?  163\nReduced risk  164  ■  Faster bootstrapping  164\n\t7.3\t\nDesigning a chassis  165\nService discovery  167  ■  Observability  171  ■  Balancing and \nlimiting  177\n\t7.4\t\nExploring the feature implemented using the \nchassis  180\n\t7.5\t\nWasn’t heterogeneity one of the promises of \nmicroservices?  182\nPart 3\t Deployment................................................185\n\t\n8\t\nDeploying microservices  187\n\t8.1\t\nWhy is deployment important?  188\nStability and availability  189\n\t8.2\t\nA microservice production environment  189\nFeatures of a microservice production environment  190   \nAutomation and speed  191\n \n\n\n\t\nxi\n\t\nxi\n﻿ CONTENTS\n\t8.3\t\nDeploying a service, the quick way  191\nService startup  192  ■  Provisioning a virtual machine  192   \nRun multiple instances of your service  194  ■  Adding a load \nbalancer  196  ■  What have you learned?  198\n\t8.4\t\nBuilding service artifacts  199\nWhat’s in an artifact?  200  ■  Immutability  201  ■  Types of \nservice artifacts  202  ■  Configuration  206\n\t8.5\t\nService to host models  207\nSingle service to host  207  ■  Multiple static services per \nhost  208  ■  Multiple scheduled services per host  209\n\t8.6\t\nDeploying services without downtime  210\nCanaries and rolling deploys on GCE  211\n\t\n9\t\nDeployment with containers and schedulers  214\n\t9.1\t\nContainerizing a service  215\nWorking with images  216  ■  Building your image  218   \nRunning containers  220  ■  Storing an image  223\n\t9.2\t\nDeploying to a cluster  224\nDesigning and running pods  226  ■  Load balancing  228   \nA quick look under the hood  230  ■  Health checks  233   \nDeploying a new version  235  ■  Rolling back  241   \nConnecting multiple services  241\n\t 10\t\nBuilding a delivery pipeline for microservices  243\n\t10.1\t Making deploys boring  244\nA deployment pipeline  244\n\t10.2\t Building a pipeline with Jenkins  246\nConfiguring a build pipeline  247  ■  Building your image  251   \nRunning tests  252  ■  Publishing artifacts  254  ■  Deploying \nto staging  255  ■  Staging environments  258  ■  Deploying to \nproduction  259\n\t10.3\t Building reusable pipeline steps  262\nProcedural versus declarative build pipelines  263\n\t10.4\t Techniques for low-impact deployment and feature \nrelease  264\nDark launches  264  ■  Feature flags  265\n \n",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 12-21)",
      "start_page": 12,
      "end_page": 21,
      "detection_method": "topic_boundary",
      "content": "xii\nxii\n﻿ CONTENTS\nPart 4\t Observability and ownership..................267\n\t 11\t\nBuilding a monitoring system  269\n\t11.1\t A robust monitoring stack  270\nGood monitoring is layered  270  ■  Golden signals  272   \nTypes of metrics  273  ■  Recommended practices  274\n\t11.2\t Monitoring SimpleBank with Prometheus and \nGrafana  275\nSetting up your metric collection infrastructure  276  ■  Collecting \ninfrastructure metrics — RabbitMQ  282  ■  Instrumenting \nSimpleBank’s place order  285  ■  Setting up alerts  287\n\t11.3\t Raising sensible and actionable alerts  291\nWho needs to know when something is wrong?  292  ■  Symptoms, \nnot causes   292\n\t11.4\t Observing the whole application  293\n\t 12\t\nUsing logs and traces to understand behavior  296\n\t12.1\t Understanding behavior across services  297\n\t12.2\t Generating consistent, structured,  \nhuman-readable logs  300\nUseful information to include in log entries  300  ■  Structure  \nand readability  301\n\t12.3\t Setting up a logging infrastructure for SimpleBank  303\nELK- and Fluentd-based solution  304  ■  Setting up your logging \nsolution  306  ■  Configure what logs to collect  308  ■  Finding a \nneedle in the haystack  311  ■  Logging the right information  313\n\t12.4\t Tracing interactions between services  313\nCorrelating requests: traces and spans  314  ■  Setting up tracing in \nyour services  315\n\t12.5\t Visualizing traces  320\n\t 13\t\nBuilding microservice teams  325\n\t13.1\t Building effective teams  326\nConway’s Law  327  ■  Principles for effective teams  328\n\t13.2\t Team models  330\nGrouping by function  330  ■  Grouping across functions  332   \nSetting team boundaries  334  ■  Infrastructure, platform, and \nproduct  335  ■  Who’s on-call?  337  ■  Sharing knowledge  338\n \n\n\n\t\nxiii\n\t\nxiii\n﻿ CONTENTS\n13.3\t Recommended practices for microservice teams  340\nDrivers of change in microservices  340  ■  The role \nof architecture  341  ■  Homogeneity versus technical \nflexibility  343  ■  Open source model  343  ■  Design \nreview  345  ■  Living documentation  346  ■  Answering \nquestions about your application  347\n13.4\t Further reading  347\n\t\nappendix\t\nInstalling Jenkins on Minikube  349\n\t\nindex  357\n \n\n\nxv\npreface\nOver the past five years, the microservice architectural style — structuring applications \nas fine-grained, loosely coupled, and independently deployable services — has become \nincreasingly popular and increasingly feasible for engineering teams, regardless of \ncompany size.\nFor us, working on microservice projects at Onfido was a revelation, and this book \nrecords many of the things we learned along the way. By breaking apart our product, \nwe could ship faster and with less friction, instead of tripping over each other’s toes in \na large, monolithic codebase. A microservice approach helps engineers build applica-\ntions that can evolve over time, even as product complexity and team size grow.\nOriginally, we set out to write a book about our real-world experience running micro-\nservice applications. As we scoped the book, that mission evolved, and we decided to \ndistill our experience of the full application lifecycle — designing, deploying, and oper-\nating microservices — into a broad and practical review. We’ve picked tools to illustrate \nthese techniques — such as Kubernetes and Docker — that are popular and go hand in \nhand with microservice best practice, but we hope that you can apply the lessons within \nregardless of which language and tools you ultimately use to build applications.\nWe sincerely hope you find this book a valuable reference and guide — and that the \nknowledge, advice, and examples within help you build great products and applications \nwith microservices.\n \n\n\nxvii\nacknowledgments\nIn its evolution over the past year and a half, this book has grown from an idea to \nwrite a small book on deploying services to a substantial work covering a wide swath of \nmicroservice development topics — from design to communication to deployment and \noperation. It has been our privilege to work with so many talented people in delivering \na book that we truly hope will be useful, both to those who are starting to adopt this \ntype of architecture and to those who already are using it.\nI would like to thank my family, in particular Rosa and Beatriz, my wife and daughter, \nwho put up with the absences of a husband and father. I would also like to thank Mor-\ngan, my coauthor and colleague. He has been crucial in providing guidance and clarity \nfrom day one. Thank you!\n—Paulo\nThis book wouldn’t have been possible to write without the patience and support of \nmy family, who gracefully tolerated far too many weekends, evenings, and holidays with \nme sitting in front of a laptop. I’d also like to thank my parents, Heather and Allan, \nwho taught me a love of reading, without which I wouldn’t be writing this today. And \nlastly, thanks Paulo! You encouraged me to start this project with you, and although the \nway was sometimes challenging, I’ve learned so much from the journey.\n—Morgan\nTogether, we would like to thank:\n¡ Karen and Dan, our development editors, who were tireless, week after week, in \nproviding support and advice to help us write the best possible book\n¡ Karsten Strøbæk, our technical development editor, for his critical eye and gen-\nerous feedback\n \n\n\nxviii\nxviii\n﻿ ACKNOWLEDGMENTS\n¡ Michael Stephens, for his faith in us, and Marjan Bace, for his help in shaping \nour book into something compelling for Manning’s readers\n¡ The many other people we’ve worked with at Manning, who formed such a pro-\nfessional and talented team, without whom this book would have never been \npossible\n¡ Lastly, our reviewers, whose feedback and help improving our book we deeply \nappreciated, including Akshat Paul, Al Krinker, Andrew Miles, Andy Miles, Anto-\nnio Pessolano, Bachir Chihani, Christian Bach, Christian Thoudahl, Vittal Dam-\naraju, Deepak Bhaskaran, Evangelos Bardis, John Guthrie, Lorenzo De Leon, \nŁukasz Witczak, Maciej Jurkowski, Mike Jensen, Shobha Iyer, Srihari Sridharan, \nSteven Parr, Thorsten Weber, and Tiago Boldt Sousa\n \n\n\nxix\nabout this book\nMicroservices in Action is a practical book about building and deploying microservice-based \napplications. Written for developers and architects with a solid grasp of service-oriented \ndevelopment, it tackles the challenge of putting microservices into production. You'll \nbegin with an in-depth overview of microservice design principles, building on your \nknowledge of traditional systems. Then you'll start creating a reliable road to production. \nYou'll explore examples using Kubernetes, Docker, and Google Container Engine as you \nlearn to build clusters and maintain them after deployment.\nThe techniques in this book should apply to developing microservices in most pop-\nular programming languages. We decided to use Python as the primary language for \nthis book because its low-ceremony style and terse syntax lend themselves to clear and \nexplicit examples. Don’t worry if you’re not too familiar with Python — we’ll guide you \nthrough running the examples.\nHow this book is organized: a roadmap\nPart 1 of this book gives a brief introduction to microservices, exploring the properties \nand benefits of microservice-based systems and the challenges you may face in their \ndevelopment.\nChapter 1 introduces the microservice architecture. We examine the benefits and \ndrawbacks of the microservice approach and explain the key principles of microservice \ndevelopment. Lastly, we introduce the design and deployment challenges we’ll cover \nthroughout this book.\nChapter 2 applies the microservice approach to an example domain — SimpleBank. \nWe design a new feature with microservices and examine how to make that feature \nready for production.\n \n\n\nxx\nxx\n﻿ ABOUT THIS BOOK\nIn part 2, we explore the architecture and design of microservice applications.\nChapter 3 walks through the architecture of a microservice application, covering \nfour layers: platform, service, boundary, and client. The goal of this chapter is to give \nthe reader a big-picture model that they can use when working to understand any \nmicroservice system.\nChapter 4 covers one of the hardest parts of microservice design: how to decide on \nservice responsibilities. This chapter lays out four approaches to modeling — business \ncapabilities, use cases, technical capabilities, and volatility — and, using examples from \nSimpleBank, explores how to make good design decisions, even when boundaries are \nambiguous.\nChapter 5 explores how to write business logic in distributed systems, where trans-\nactional guarantees no longer apply. We introduce the reader to different transaction \npatterns, such as sagas, and query patterns, such as API composition and CQRS.\nChapter 6 covers reliability. Distributed systems can be more fragile than monolithic \napplications, and communication between microservices requires careful consider-\nation to avoid availability issues, downtime, and cascading failures. Using examples in \nPython, we explore common techniques for maximizing application resiliency, such as \nrate limits, circuit breakers, health checks, and retries.\nIn chapter 7, you’ll learn how to design a reusable microservice framework. Consis-\ntent practices across microservices improve overall application quality and reliability \nand reduce time to development for new services. We provide working examples in \nPython.\nIn part 3, we look at deployment best practices for microservices.\nChapter 8 emphasizes the importance of automated continuous delivery in microser-\nvice applications. Within this chapter, we take a single service to production — on Google \nCompute Engine — and from that example learn about the importance of immutable \nartifacts and the pros and cons of different microservice deployment models.\nChapter 9 introduces Kubernetes, a container scheduling platform. Containers, \ncombined with a scheduler like Kubernetes, are a natural and elegant fit for running \nmicroservices at scale. Using Minikube, you’ll learn how to package a microservice and \ndeploy it seamlessly to Kubernetes.\nIn chapter 10, you’ll build on the example in the previous chapter to construct an \nend-to-end delivery pipeline using Jenkins. You’ll script a pipeline with Jenkins and \nGroovy that takes new commits to production rapidly and reliably. You’ll also learn how \nto apply consistent deployment practices to a microservice fleet.\nIn this book’s final part, we explore observability and the human side of microservices.\nChapter 11 will walk you through the development of a monitoring system for micro-\nservices, using StatsD, Prometheus, and Grafana to collect and aggregate metrics to \nproduce dashboards and alerts. We’ll also discuss good practices for alert management \nand avoiding alert fatigue.\nChapter 12 builds on the work in the previous chapter to include logs and traces. \nGetting rich, real-time, and searchable information from our microservices helps us \n \n\n\n\t\nxxi\n\t\nxxi\n﻿ ABOUT THIS BOOK\nunderstand them, diagnose issues, and improve them in the future. Examples in this \nchapter use Elasticsearch, Kibana, and Jaeger.\nLastly, chapter 13 takes a slight left turn to explore the people side of microservice \ndevelopment. People implement software: building great software is about effective col-\nlaboration as much as implementation choices. We’ll examine the principles that make \nmicroservice teams effective and explore the psychological and practical implications \nof the microservice architectural approach on good engineering practices.\nAbout the code\nThis book contains many examples of source code, both in numbered listings and \ninline with normal text. In both cases, source code is formatted in a fixed-width font \nlike this to separate it from ordinary text. Sometimes code is also in bold, either \nto highlight specific lines or to differentiate entered commands from the resulting \noutput.\nIn many cases, we’ve reformatted the original source code; we’ve added line breaks \nand reworked indentation to accommodate the available page space in the book. In \nrare cases, even this was not enough, and listings include line-continuation markers \n(➥). Additionally, we’ve often removed comments in the source code from the listings \nwhen we’ve described the code in the text. Code annotations accompany many of the \nlistings, highlighting important concepts.\nThe source code within this book is available on the book’s website at https://www \n.manning.com/books/microservices-in-action and on the Github repository at https://\ngithub.com/morganjbruce/microservices-in-action.\nYou can find instructions on running examples throughout the book. We typically \nuse Docker and/or Docker Compose to simplify running examples. The appendix cov-\ners configuring Jenkins, used in chapter 10, to run smoothly on a local deployment of \nKubernetes.\nBook forum\nPurchase of Microservices in Action includes free access to a private web forum run by \nManning Publications where you can make comments about the book, ask techni-\ncal questions, and receive help from the authors and from other users. To access the \nforum, go to https://forums.manning.com/forums/microservices-in-action. You can \nalso learn more about Manning's forums and the rules of conduct at https://forums. \nmanning.com/forums/about.\nManning’s commitment to our readers is to provide a venue where a meaningful \ndialogue between individual readers and between readers and the authors can take \nplace. It is not a commitment to any specific amount of participation on the part of the \nauthors, whose contribution to the forum remains voluntary (and unpaid). We suggest \nyou try asking the authors some challenging questions, lest their interest stray! The \nforum and the archives of previous discussions will be accessible from the publisher’s \nwebsite as long as the book is in print.\n \n\n\nxxii\nabout the authors\nMorgan Bruce has significant experience in building complex \napplications, with particular expertise in the finance and identity \nverification industries, where accuracy, resilience, and security are \ncrucial. As an engineering leader, he has worked on large-scale \nrefactoring and rearchitecture efforts. He also has firsthand expe-\nrience leading the evolution from a monolithic application to a \nrobust microservice architecture.\nPaulo A. Pereira is currently leading a team involved in reshaping \na monolith into microservices, while dealing with the constraints \nof evolving a system where security and accuracy are of the utmost \nimportance. Enthusiastic about choosing the right tool for the job \nand combining different languages and paradigms, he is currently \nexploring functional programming, mainly via Elixir. Paulo also \nauthored Elixir Cookbook and was one of the technical reviewers for \nLearning Elixir and for Mastering Elixir.\n \n\n\nxxiii\nabout the cover illustration\nThe figure on the cover of Microservices in Action is captioned “Habit of a Lady of China \nin 1700.” The illustration is taken from Thomas Jefferys’ A Collection of the Dresses of \nDifferent Nations, Ancient and Modern (four volumes), London, published between 1757 \nand 1772. The title page states that these are hand-colored copperplate engravings, \nheightened with gum arabic.\nThomas Jefferys (1719–1771) was called “Geographer to King George III.” He was \nan English cartographer who was the leading map supplier of his day. He engraved and \nprinted maps for government and other official bodies and produced a wide range of \ncommercial maps and atlases, especially of North America. His work as a mapmaker \nsparked an interest in local dress customs of the lands he surveyed and mapped, which \nare brilliantly displayed in this collection. Fascination with faraway lands and travel for \npleasure were relatively new phenomena in the late 18th century, and collections such \nas this one were popular, introducing both the tourist and the armchair traveler to the \ninhabitants of other countries.\nThe diversity of the drawings in Jefferys’ volumes speaks vividly of the uniqueness and \nindividuality of the world’s nations some 200 years ago. Dress codes have changed since \nthen, and much of the diversity by region and country, so rich at the time, has faded \naway. It’s now often hard to tell the inhabitants of one continent from another. Perhaps, \ntrying to view it optimistically, we’ve traded a cultural and visual diversity for a more var-\nied personal life — or a more varied and interesting intellectual and technical life.\nAt a time when it’s difficult to tell one computer book from another, Manning cele-\nbrates the inventiveness and initiative of the computer business with book covers based \non the rich diversity of regional life of two centuries ago, brought back to life by Jeffreys’ \npictures.\n \n",
      "page_number": 12
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 22-31)",
      "start_page": 22,
      "end_page": 31,
      "detection_method": "topic_boundary",
      "content": "Part 1\nThe lay of the land\nThis part introduces microservice architecture, explores the properties and \nbenefits of microservice applications, and presents some of the challenges you’ll \nface in developing microservice applications. We’ll also introduce SimpleBank, a \nfictional company whose attempts to build a microservice application will be the \ncommon thread in many examples used in this book.\n \n\n\n3\n1\nDesigning and \nrunning microservices\nThis chapter covers\n¡ Defining a microservice application\n¡ The challenges of a microservices approach\n¡ Approaches to designing a microservice \napplication\n¡ Approaches to running microservices \nsuccessfully\nSoftware developers strive to craft effective and timely solutions to complex prob-\nlems. The first problem you usually try to solve is: What does your customer want? If \nyou’re skilled (or lucky), you get that right. But your efforts rarely stop there. Your \nsuccessful application continues to grow: you debug issues; you build new features; \nyou keep it available and running smoothly.\nEven the most disciplined teams can struggle to sustain their early pace and agility \nin the face of a growing application. At worst, your once simple and stable product \nbecomes both intractable and delicate. Instead of sustainably delivering more value to \nyour customers, you’re fatigued from outages, anxious about releasing, and too slow to \ndeliver new features or fixes. Neither your customers nor your developers are happy.\nMicroservices promise a better way to sustainably deliver business impact. Rather \nthan a single monolithic unit, applications built using microservices are made up of \n \n\n\n4\nChapter 1  Designing and running microservices \nloosely coupled, autonomous services. By building services that do one thing well, you \ncan avoid the inertia and entropy of large applications. Even in existing applications, \nyou can progressively extract functionality into independent services to make your \nwhole system more maintainable.\nWhen we started working with microservices, we quickly realized that building smaller \nand more self-contained services was only one part of running a stable and business-critical \napplication. After all, any successful application will spend much more of its life in pro-\nduction than in a code editor. To deliver value with microservices, our team couldn’t be \nfocused on build alone. We needed to be skilled at operations: deployment, observation, \nand diagnosis.\n1.1\t\nWhat is a microservice application?\nA microservice application is a collection of autonomous services, each of which does \none thing well, that work together to perform more intricate operations. Instead of a \nsingle complex system, you build and manage a suite of relatively simple services that \nmight interact in complex ways. These services collaborate with each other through \ntechnology-agnostic messaging protocols, either point-to-point or asynchronously.\nThis might seem like a simple idea, but it has striking implications for reducing friction in \nthe development of complex systems. Classical software engineering practice advocates high \ncohesion and loose coupling as desirable properties of a well-engineered system. A system that \nhas these properties will be easier to maintain and more malleable in the face of change.\nCohesion is the degree to which elements of a certain module belong together, whereas \ncoupling is the degree to which one element knows about the inner workings of another. \nRobert C. Martin’s Single Responsibility Principle is a useful way to consider the former:\nGather together the things that change for the same reasons. Separate those things that \nchange for different reasons.\nIn a monolithic application, you try to design for these properties at a class, module, \nor library level. In a microservice application, you aim instead to attain these proper-\nties at the level of independently deployable units of functionality. A single microservice \nshould be highly cohesive: it should be responsible for some single capability within an \napplication. Likewise, the less that each service knows about the inner workings of other \nservices, the easier it is to make changes to one service — or capability — without forcing \nchanges to others.\nTo get a better picture of how a microservice application fits together, let’s start by \nconsidering some of the features of an online investment tool:\n¡ Opening an account\n¡ Depositing and withdrawing money\n¡ Placing orders to buy or sell positions in financial products (for example, shares)\n¡ Modeling risk and making financial predictions\nLet’s explore the process of selling shares:\n1\t A user creates an order to sell some shares of a stock from their account.\n2\t This position is reserved on their account, so it can’t be sold multiple times.\n \n\n\n\t\n5\nWhat is a microservice application?\n3\t It costs money to place an order on the market — the account is charged a fee.\n4\t The system needs to communicate that order to the appropriate stock market.\nFigure  1.1 shows how placing that sell order might look as part of a microservice \napplication.\nYou can observe three key characteristics of microservices in figure 1.1:\n¡ Each microservice is responsible for a single capability. This might be business related or \nrepresent a shared technical capability, such as integration with a third party (for example, \nthe stock exchange).\n¡ A microservice owns its data store, if it has one. This reduces coupling between \nservices because other services can only access data they don’t own through the \ninterface that a service provides.\n¡ Microservices themselves, not the messaging mechanism that connects them nor \nanother piece of software, are responsible for choreography and collaboration  — the \nsequencing of messages and actions to perform some useful activity.\nTransaction\ndatabase\nUser\n1. Places order to sell\n100 units of Stock A\nfrom account ABC\n2. Records order\ndetails in database\n3. Requests reservation\nof 100 units of Stock A\nagainst account ABC\n4. Records reserved\nstock position against\naccount ABC\n5. Requests calculation\nof fee\n6. Requests placement\nof order to market\n7. Places order onto\nstock exchange\nAccount\ntransactions\nservice\nOrders service\nFees service\nFee rules\ndatabase\nMarket service\nStock exchange\nOrder\ndatabase\nFigure 1.1    The flow of communication through microservices in an application that allows users to sell \npositions in financial shares\n \n\n\n6\nChapter 1  Designing and running microservices \nIn addition to these three characteristics, you can identify two more fundamental attri-\nbutes of microservices:\n¡ Each microservice can be deployed independently. Without this, a microservice \napplication would still be monolithic at the point of deployment.\n¡ A microservice is replaceable. Having a single capability places natural bounds on \nsize; likewise, it makes the individual responsibility, or role, of a service easy to \ncomprehend.\nThe idea that microservices are responsible for coordinating actions in a system is the \ncrucial difference between this approach and traditional service-oriented architec-\ntures (SOAs). Those types of systems often used enterprise service buses (ESBs) or \nmore complex orchestration standards to externalize messaging and process orches-\ntration from applications themselves. In that model, services often lacked cohesion, \nas business logic was increasingly added to the service bus, rather than the services \nthemselves.\nIt’s interesting to think about how decoupling functionality in the online investment \nsystem helps you be more flexible in the face of changing requirements. Imagine that \nyou need to change how fees are calculated. You could make and release those changes \nto the fees service without any change to its upstream or downstream services. Or imag-\nine an entirely new requirement: when an order is placed, you need to alert your risk \nteam if it doesn’t match normal trading patterns. It’d be easy to build a new microser-\nvice to perform that operation based on an event raised by the orders service without \nchanging the rest of the system.\n1.1.1\t\nScaling through decomposition\nYou also can consider how microservices allow you to scale an application. In The Art \nof Scalability, Abbott and Fisher define three dimensions of scale as the scale cube (fig-\nure 1.2).\nMonolithic applications typically scale through horizontal duplication: deploying \nmultiple, identical instances of the application. This is also known as cookie-cutter, \nor X-axis, scaling. Conversely, microservice applications are an example of Y-axis scal-\ning, where you decompose a system to address the unique scaling needs of different \nfunctionality.\nNOTE     The Z axis refers to horizontal data partitions: sharding. You can apply \nsharding to either approach — microservices or monolithic applications — but \nwe won’t be exploring that topic in this book.\nLet’s revisit the investment tool as an example, with the following characteristics:\n¡ Financial predictions might be computationally onerous and are rarely done.\n¡ Complex regulatory and business rules may govern investment accounts.\n¡ Market trading may happen in extremely large volumes, while also relying on \nminimizing latency.\n \n\n\n\t\n7\nWhat is a microservice application?\nZ axis–data\npartitioning\nScale by splitting\nsimilar things\nY axis–functional\ndecomposition\nScale by splitting\ndifferent things\nx axis–horizontal duplication\nScale by cloning\nFigure 1.2    The three dimensions of scaling an application\nIf you build features as microservices that meet the requirements of these characteristics, \nyou can choose the ideal technical tools to solve each problem, rather than trying to fit \nsquare pegs into round holes. Likewise, autonomy and independent deployment mean \nyou can manage the microservices’ underlying resource needs separately. Interestingly, \nthis also implies a natural way to limit failure: if your financial prediction service fails, \nthat failure is unlikely to cascade to the market trading or investment account services. \nMicroservice applications have some interesting technical properties:\n¡ Building services along the lines of single capabilities places natural bounds on \nsize and responsibility.\n¡ Autonomy allows you to develop, deploy, and scale services independently.\n1.1.2\t\nKey principles\nFive cultural and architectural principles underpin microservices development:\n¡ Autonomy\n¡ Resilience\n¡ Transparency\n¡ Automation\n¡ Alignment\n \n\n\n8\nChapter 1  Designing and running microservices \nThese principles should drive your technical and organizational decisions when you’re \nbuilding and running a microservice application. Let’s explore each of them.\nAutonomy\nWe’ve established that microservices are autonomous  — each service operates and \nchanges independently of others. To ensure that autonomy, you need to design your ser-\nvices so they are:\n¡ Loosely coupled  — By interacting through clearly defined interfaces, or through \npublished events, each microservice remains independent of the internal imple-\nmentation of its collaborators. For example, the orders service we introduced \nearlier shouldn’t be aware of the implementation of the account transactions \nservice. This is illustrated in figure 1.3.\n¡ Independently deployable  — Services will be developed in parallel, often by multi-\nple teams. Being forced to deploy them in lockstep or in an orchestrated forma-\ntion would result in risky and anxious deployments. Ideally, you want to use your \nsmaller services to enable rapid, frequent, and small releases.\nAutonomy is also cultural. It’s vital that you delegate accountability for and ownership \nof services to teams responsible for delivering business impact. As we’ve established, \norganizational design has an influence on system design. Clear service ownership \nallows teams to build iteratively and make decisions based on their local context and \ngoals. Likewise, this model is ideal for promoting end-to-end ownership, where a team \nis responsible for a service in both development and production. \nNOTE     In chapter 13, we’ll discuss developing responsible and autonomous \nengineering teams and why this is crucial when working with microservices.\nMessaging between\nservices should be language\nagnostic—for example\ngRPC, Thrift, JSON+HTTP.\nA service exposes a\ncontract. Messages are\nconstructed according to\nthis contract.\nInternal implementation is\nirrelevant to the caller, as long\nas it meets the contract.\nImplements\nUses\nOrders service\nMessaging\nAccount\ntransactions\nservice\nContract\nFigure 1.3    You can loosely couple services by having them communicate through defined contracts \nthat hide implementation details.\n \n\n\n\t\n9\nWhat is a microservice application?\nResilience\nMicroservices are a natural mechanism for isolating failure: if you deploy them inde-\npendently, application or infrastructure failure may only affect part of your system. \nLikewise, being able to deploy smaller bits of functionality should help you change \nyour system more gradually, rather than releasing a risky big bang of new functionality.\nConsider the investment tool again. If the market service is unavailable, it won’t be \nable to place the order to market. But a user can still request the order, and the service \ncan pick it up later when the downstream functionality becomes available.\nAlthough splitting your application into multiple services can isolate failure, it \nalso will multiply points of failure. In addition, you’ll need to account for what hap-\npens when failure does occur to prevent cascades. This involves both design — favor-\ning asynchronous interaction where possible and using circuit breakers and timeouts \nappropriately — and operations — using provable continuous delivery techniques and \nrobustly monitoring system activity.\nTransparency\nMost importantly, you need to know when a failure has occurred, and rather than one \nsystem, a microservice application depends on the interaction and behavior of mul-\ntiple services, possibly built by different teams. At any point, your system should be \ntransparent and observable to ensure that you both observe and diagnose problems.\n Every service in your application will produce business, operational, and infrastruc-\nture metrics; application logs; and request traces. As a result, you’ll need to make sense \nof a huge amount of data.\nAutomation\nIt might seem counterintuitive to alleviate the pain of a growing application by build-\ning a multitude of services. It’s true that microservices are a more complex architecture \nthan building a single application. By embracing automation and seeking consistency \nin the infrastructure between services, you can significantly reduce the cost of managing \nthis additional complexity. You need to use automation to ensure the correctness of \ndeployments and system operation.\nIt’s not a coincidence that the popularity of microservice architecture parallels both \nthe increasing mainstream adoption of DevOps techniques, especially infrastructure- \nas-code, and the rise of infrastructure environments that are fully programmable \nthrough APIs (such as AWS or Azure). These two trends have done a lot to make micro-\nservices feasible for smaller teams.\nAlignment\nLastly, it’s critical that you align your development efforts in the right way. You should \naim to structure your services, and therefore your teams, around business concepts. \nThis leads to higher cohesion.\nTo understand why this is important, consider the alternative. Many traditional SOAs \ndeployed the technical tiers of an application separately — UI, business logic, integra-\ntion, data. Figure 1.4 compares SOA and microservice architecture.\n \n\n\n10\nChapter 1  Designing and running microservices \nPresentation\nUI\nUI\nAPI\nService\nProcess\nService\nTransport\nOrchestration\nMessage\nStore\nStore\nStore\nBusiness services\nResponsibility split by tier\nOrchestration between\ncomponents governed by\na smart message bus\nServices contain\nfull vertical stack.\nServices own data.\nMicroservice architecture\nService-Oriented Architecture (SOA)\nIndependent\ncomponents\ncollaborate directly.\nService\nService\nService\nService\nStore\nLogic\nAPI\nData services\nEnterprise service bus\nFigure 1.4    SOA versus microservice architecture\nThis use of horizontal decomposition in SOA is problematic, because cohesive function-\nality becomes spread across multiple systems. New features may require coordinated \nreleases to multiple services and may become unacceptably coupled to others at the \nsame level of technical abstraction.\nA microservice architecture, on the other hand, should be biased toward vertical \ndecomposition; each service should align to a single business capability, encapsulating \nall relevant technical layers.\nNOTE     In rare instances, it might make sense to build a service that implements \na technical capability, such as integration with a third-party service, if multiple \nservices require it.\nYou also should be mindful of the consumers of your services. To ensure a stable \nsystem, you need to ensure you’re developing patiently and maintaining backwards \ncompatibility — whether explicitly or by running multiple versions of a service — to \nensure that you don’t force other teams to upgrade or break complex interactions \nbetween services.\nWorking with these five principles in mind will help you develop microservices well, \nleading to systems that are highly amenable to change, scalable, and stable.\n1.1.3\t\nWho uses microservices?\nMany organizations have successfully built and deployed microservices, across many \ndomains: in media (The Guardian); content distribution (SoundCloud, Netflix); \ntransport and logistics (Hailo, Uber); e-commerce (Amazon, Gilt, Zalando); banking \n(Monzo); and social media (Twitter).\n \n\n\n\t\n11\nWhat is a microservice application?\nMost of these companies took a monolith-first approach.1 They started by building \na single large application, then progressively moved to microservices in response to \ngrowth pressures they faced. These pressures are outlined in Table 1.1.\nTable 1.1    Pressures of growth on a software system\nPressure\nDescription\nVolume\nThe volume of activity that a system performs may outgrow the capacity of \noriginal technology choices.\nNew features\nNew features may not be cohesive with existing features, or different technol-\nogies may be better at solving problems.\nEngineering team growth\nAs a team grows larger, lines of communication increase. New developers \nspend more time comprehending the existing system and less time adding \nproduct value.\nTechnical debt\nIncreased complexity in a system — including debt from previous build  \ndecisions — increases the difficulty of making changes.\nInternational distribution\nInternational distribution may lead to data consistency, availability, and \nlatency challenges.\nFor example, Hailo wanted to expand internationally — which would’ve been chal-\nlenging with their original architecture — but also increase their pace of feature deliv-\nery.2 SoundCloud wanted to be more productive, as the complexity of their original \nmonolithic application was holding them back.3 Sometimes, the shift coincided with a \nchange in business priority: Netflix famously moved from physical DVD distribution to \ncontent streaming. Some of these companies completely decommissioned their origi-\nnal monolith. But for many, this is an ongoing process, with a monolith surrounded by \na constellation of smaller services.\nAs microservice architecture has been more widely popularized — and as early \nadopters have open sourced, blogged, and presented the practices that worked for \nthem — teams have increasingly begun greenfield projects using microservices, rather \nthan building a single application first. For example, Monzo started with microservices \nas part of its mission to build a better and more scalable bank.4 \n1\t Martin Fowler expands on this pattern: “MonolithFirst,” June 3, 2015,  http://martinfowler.com/\nbliki/MonolithFirst.html.\n2\t See Matt Heath, “A Long Journey into a Microservice World,” Medium, May 30, 2015, http://mng.bz/\nXAOG.\n3\t See Phil Calçado, “How we ended up with microservices,” September 8, 2015, http://mng.bz/Qzhi.\n4\t  See Matt Heath, “Building microservice architectures in Go,” June 18, 2015, http://mng.bz/9L83.\n \n",
      "page_number": 22
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 32-39)",
      "start_page": 32,
      "end_page": 39,
      "detection_method": "topic_boundary",
      "content": "12\nChapter 1  Designing and running microservices \n1.1.4\t\nWhy are microservices a good choice?\nPlenty of successful businesses are built on monolithic software — Basecamp,5 Stack-\nOverflow, and Etsy spring to mind. And in monolithic applications, a wealth of ortho-\ndox, long-established software development practice and knowledge exists. Why \nchoose microservices?\nTechnical heterogeneity leads to microservices\nIn some companies, technical heterogeneity makes microservices an obvious choice. \nAt Onfido, we started building microservices when we introduced a product driven by \nmachine learning — not a great fit for our original Ruby stack! Even if you’re not fully \ncommitted to a microservice approach, applying microservice principles gives you a \ngreater range of technical choices to solve business problems. Nevertheless, it’s not \nalways so clear-cut.\nDevelopment friction increases as complex systems grow\nIt comes down to the nature of complex systems. At the beginning of the chapter, we \nmentioned that software developers strive to craft effective and timely solutions to com-\nplex problems. But the software systems we build are inherently complex. No methodol-\nogy or architecture can eliminate the essential complexity at the heart of such a system.\nBut that’s no reason to get downhearted! You can ensure that the development \napproaches you take result in good complex systems, free from accidental complexity.\n Take a moment and consider what you’re trying to achieve as an enterprise software \ndeveloper. Dan North puts it well:\nThe goal of software development is to sustainably minimize lead time to positive business \nimpact.\nThe hard part in complex software systems is to deliver sustainable value in the face of \nchange: to continue to deliver with agility, pace, and safety even as the system becomes \nlarger and more complex. Therefore, we believe a good complex system is one where \ntwo factors are minimized throughout the system’s lifecycle: friction and risk.\nFriction and risk limit your velocity and agility, and therefore your ability to deliver \nbusiness impact. As a monolith grows, the following factors may lead to friction:\n¡ Change cycles are coupled together, leading to higher coordination barriers and \nhigher risk of regression.\n¡ Soft module and context boundaries invite chaos in undisciplined teams, lead-\ning to tight or unanticipated coupling between components.\n¡ Size alone can be painful: continuous integration jobs and releases — even local \napplication startup — become slower.\nThese qualities aren’t true for all monoliths, but unfortunately they’re true for most \nthat we’ve encountered. Likewise, these types of challenges are a common thread in \nthe stories of the companies we mentioned.\n5\t  David Heinemeier Hansson coined the term “Majestic Monolith” to describe how 37signals built \nBasecamp: Signal v. Noise, February 29, 2016, http://mng.bz/1p3I.\n \n\n\n\t\n13\nWhat is a microservice application?\nMicroservices reduce friction and risk\nMicroservices help reduce friction and risk in three ways:\n¡ Isolating and minimizing dependencies at build time\n¡ Allowing developers to reason about cohesive individual components, rather \nthan an entire system\n¡ Enabling the continuous delivery of small, independent changes\nIsolating and minimizing dependencies at build time — whether between teams or on \nexisting code — allows developers to move faster. Development can move in parallel, \nwith reduced long-term dependency on past decisions made in a monolithic applica-\ntion. Technical debt is naturally limited to service boundaries.\nMicroservices are individually easier to build and reason about than monolithic \napplications. This is beneficial for the productivity of development in a growing orga-\nnization. It also provides a compelling and flexible paradigm for coping with increased \nscale or smoothly introducing new technologies.\nSmall services are also a great enabler of continuous delivery. Deployments in large \napplications can be risky and involve lengthy regression and verification cycles. By \ndeploying smaller elements of functionality, you better isolate changes to your active \nsystem, reducing the potential risk of an individual deployment.\nAt this point, we can come to two conclusions:\n¡ Developing small, autonomous services can reduce friction in the development \nof long-running complex systems.\n¡ By delivering cohesive and independent pieces of functionality, you can build a \nsystem that’s malleable and resilient in the face of change, helping you to deliver \nsustainable business impact with reduced risk.\nThat doesn’t mean everyone should build microservices. It’d be wonderful if there \nwas an objective answer to the question “Do I need microservices?” but unfortunately \nyou can only say “It depends” — on your team, on your company, and on the nature of \nthe system you’re building. If the scope of your system is trivial, then it’s unlikely you’ll \ngain benefits that outweigh the added complexity of building and running this type of \nfine-grained application. But if you’ve faced any of the challenges we mentioned ear-\nlier in this section, then microservices are a compelling solution.\nA cautionary tale\nWe once heard a story about a microservice implementation gone wrong. The startup \nin question had begun to scale, and the CTO had decided that the only solution was \nto rebuild the application as microservices. If you’re not worried by that sentence, you \nshould be!\nThe engineering team set out to rebuild their application. This took them five months, \nduring which time they released zero new features, nor did they release any of their micro-\nservices to production. The team proceeded to launch their new microservice application \n \n\n\n14\nChapter 1  Designing and running microservices \nduring the busiest month for the business, causing absolute chaos and necessitating a \nrollback to the original monolith.\nThis type of migration gives microservices a bad name. Few businesses have the luxury \nof a feature freeze for several months nor can they indulge a big-bang launch of a new \narchitecture. Although the sample set is small, most successful microservice migrations \nthat we’ve observed have been piecemeal, balancing architectural vision with business \nneeds, priorities, and resource constraints. Although it’ll take longer and require more \nengineering effort, hopefully you’ll never recognize your team being mentioned in a cau-\ntionary tale!\n \n1.2\t\nWhat makes microservices challenging?\nLet’s dig a little deeper and explore the costs and complexity of designing and run-\nning microservices. Microservices aren’t the only architecture that have promised nir-\nvana through decomposition and distribution, but those past attempts, such as SOA,6 \nare widely considered unsuccessful. No technique is a silver bullet. For example, as \nwe’ve mentioned, microservices drastically increase the number of moving parts in a \nsystem. By distributing functionality and data ownership across multiple autonomous \nservices, you likewise distribute responsibility for stability and sane operation of your \napplication.\nYou’ll encounter many challenges when designing and running a microservice \napplication:\n¡ Scoping and identifying microservices requires substantial domain knowledge.\n¡ The right boundaries and contracts between services are difficult to identify and, \nonce you’ve established them, can be time-consuming to change.\n¡ Microservices are distributed systems and therefore require different assumptions \nto be made about state, consistency, and network reliability.\n¡ By distributing system components across networks, and increasing technical \nheterogeneity, microservices introduce new modes of failure.\n¡ It’s more challenging to understand and verify what should happen in normal \noperation.\n1.2.1\t\nDesign challenges\nHow do these challenges impact the design and runtime phases of microservice devel-\nopment? Earlier we introduced the five key principles underlying microservice develop-\nment. The first of those was autonomy. For your services to be autonomous, you need to \ndesign them such that, together, they’re loosely coupled, and, individually, they encap-\nsulate highly cohesive elements of functionality. This is an evolutionary process. The \n6\t SOA is a wooly term. Although many principles of SOA are similar to microservices, the definition \nof the former is inextricably associated with heavyweight, enterprise vendor tools, such as ESBs.\n(continued)\n \n\n\n\t\n15\nWhat makes microservices challenging?\nscope of your services may change over time, and you’ll often choose to carve out new \nfunctionality from — or even retire — existing services.\nMaking those choices is challenging, and even more so at the start of developing \nan application! The primary driver of loose coupling is the boundaries you establish \nbetween services; getting those wrong will lead to services that are resistant to change \nand, overall, a less malleable and flexible application.\nScoping microservices requires domain knowledge \nEach microservice is responsible for a single capability. Identifying these capabilities \nrequires knowledge of the business domain of your application. Early in an application’s \nlifetime, your domain knowledge might be at best incomplete, or at worst, incorrect.\nInadequate understanding of your problem domain can result in poor design \nchoices. In a microservice application, the increased rigidity of a service boundary  \nwhen compared to a module within a monolithic application means the downstream \ncost of poor scoping decisions is likely to be higher:\n¡ You may need to refactor across multiple distinct codebases.\n¡ You may need to migrate data from one service’s database to another.\n¡ You may not have identified implicit dependencies between services, which could \nlead to errors or incompatibility on deployment.\nThese activities are illustrated in figure 1.5.\nDatabase\nConsumers\nConsumers\nService A\nService A\nService B\nService B\nDatabase\nService A depends on closely related functionality\nin Service B.\nRefactoring this relationship requires the\ncoordination of multiple changes.\nData needs to be\nmigrated.\nFunctionality is moved\nto new services.\nConsumers need to\nmove to a new\nservice.\nDatabase\nDatabase\nFigure 1.5    Incorrect service scoping decisions may require complex and costly refactoring across \nservice boundaries.\n \n\n\n16\nChapter 1  Designing and running microservices \nBut making design decisions based on insufficient domain knowledge is hardly \nunique to microservices! The difference is in the impact of those decisions.\nNOTE     In chapters 2 and 4, we’ll discuss best practices for identifying and scop-\ning services, using an example application.\nMaintaining contracts between services\nEach microservice should be independent of the implementation of other services. \nThis enables technical heterogeneity and autonomy. For this to work, each microser-\nvice should expose a contract  — analogous to an interface in object-oriented design —  \ndefining the messages it expects to receive and respond with. A good contract should be\n¡ Complete  — Defines the full scope of an interaction\n¡ Succinct  — Takes in no more information than is necessary, so that consumers \ncan construct messages within reasonable bounds\n¡ Predictable  — Accurately reflects the real behavior of any implementation\nAnyone who’s designed an API might know how hard these properties are to achieve. \nContracts become the glue between services. Over time, contracts may need to evolve \nwhile also needing to maintain backwards compatibility for existing collaborators. \nThese twin tensions — between stability and change — are challenging to navigate.\nMicroservice applications are designed by teams\nIn larger organizations, it’s likely that multiple teams will build and run a microser-\nvice application, each taking responsibility for different microservices. Each team may \nhave its own goals, way of working, and delivery lifecycle. It can be difficult to design a \ncohesive system when you also need to reconcile the timelines and priorities of other \nindependent teams. Coordinating the development of any substantial microservice \napplication therefore will require the agreement and reconciliation of priorities and \npractices across multiple teams.\nMicroservice applications are distributed systems\nDesigning microservice applications means designing distributed systems. Many falla-\ncies occur in the design of distributed systems,7 including\n¡ The network is reliable.\n¡ Latency is zero.\n¡ Bandwidth is infinite.\n¡ Transport cost is zero.\nClearly, assumptions you might make in nondistributed systems — such as the speed \nand reliability of method calls — are no longer appropriate and can lead to poor, unsta-\nble implementation. You must consider latency, reliability, and the consistency of state \nacross your application.\n7\t See Arnon Rotem-Gal-Oz, “Fallacies of Distributed Computing Explained,” https://pages.cs.wisc \n.edu/~zuyu/files/fallacies.pdf.\n \n\n\n\t\n17\nWhat makes microservices challenging?\nOnce the application is distributed — where the application’s underlying state \ndata is spread across a multitude of places — consistency becomes challenging. You \nmay not have guarantees of the order of operations. It won’t be possible to maintain \nACID-like transactional guarantees when actions take place across multiple services. \nThis will affect design at the application level: you’ll need to consider how a service \nmight operate in an inconsistent state and how to roll back in the event of transaction \nfailure.\n1.2.2\t\nOperational challenges\nA microservice approach will inherently multiply the possible points of failure in a \nsystem. To illustrate this, let’s return to the investment tool we mentioned earlier. Fig-\nure 1.6 identifies possible points of failure in this application. You can see that some-\nthing could go wrong in multiple places, and that could affect the normal processing \nof an order.\nConsider the questions you might need to answer when this application is in \nproduction:\n¡ If something goes wrong and your user’s order isn’t placed, how would you deter-\nmine where the fault occurred?\n¡ How do you deploy a new version of a service without affecting order placement?\n¡ How do you know which services were meant to be called?\n¡ How do you test that this behavior is working correctly across multiple services?\n¡ What happens if a service is unavailable?\nRather than eliminating risk, microservices move that cost to later in the lifecycle of \nyour system: reducing friction in development but increasing the complexity of how \nyou deploy, verify, and observe your application in operation.\nUser\nOverload\nService instances become\nsaturated with requests and fail\nto respond or pass timeout limits.\nNetwork/routing failure\nNetwork issues cause request routing\nbetween users/services/dependencies to fail.\nHardward failure\nThe hardware running the\ndatabase or service instances\nfails.\nDownstream failure\nService dependencies may fail\nor respond slowly.\nThird party failure\nRequests to third party\ndependencies may fail.\nThird-party\nproviders\nOther\nmicroservices\nDatabase\nOrder service\nFigure 1.6    Possible points of failure when placing a sell order\n \n\n\n18\nChapter 1  Designing and running microservices \nA microservices approach suggests an evolutionary approach to system design: you \ncan add new features independently without changing existing services. This minimizes \nthe cost and risk of change.\nBut in a decoupled system that constantly changes, it can be extremely difficult to \nkeep track of the big picture, which makes issue diagnosis and support more challeng-\ning. When something goes wrong, you need to have some way of tracing how the system \ndid behave (what services it called, in which order, and what the outcome was), but you \nalso need some way of knowing how the system should have behaved.\nUltimately, you face two operational challenges in microservices: observability and \nmultiple points of failure. Let’s focus on each of those in turn.\nObservability is difficult to achieve\nWe touched on the importance of transparency back in section 1.1.2. But why is it \nharder in microservice applications? It’s harder because you need to understand the \nbig picture. You need to assemble that big picture from multiple jigsaw pieces, to cor-\nrelate and link together the data each service produces to ensure you understand what \neach service does within the wider context of delivering some business output. Indi-\nvidual service logs provide a partial view of system operation, which is helpful, but you \nneed to use both a microscope and a wide-angle lens to understand the system in full.\nLikewise, because you’re running multiple applications, depending on how you \nchoose to deploy them, a less obvious correlation may exist between underlying infra-\nstructural metrics — like memory and CPU usage — and the application. These metrics \nare still useful but are less of a focus than they might be in a monolithic system.\nMultiplying services multiplies points of failure\nWe’re probably not being too pessimistic if we say that everything that can fail will fail. \nIt’s important that you start with that mindset: if you assume weakness and fragility \nin the multiple services forming your system, that can better inform how you design, \ndeploy, and monitor that system — rather than getting too surprised when something \ndoes go wrong.\nYou need to consider how your system will continue operating despite the failures \nof individual components. This implies that, individually, services will need to become \nmore robust — considering error checking, failover, and recovery — but also that the \nwhole system should act reliably, even when individual components are never 100% \nreliable.\n1.3\t\nMicroservice development lifecycle\nAt an individual level, each microservice should look familiar to you — even if it’s a bit \nsmaller. To build a microservice, you’ll use many of the same frameworks and tech-\nniques that you’d normally apply in building an application: web application frame-\nworks, SQL databases, unit tests, libraries, and so on.\nAt a system level, choosing a microservice architecture will have a significant impact \non how you design and run your application. Throughout this book, we’ll focus on these \n \n\n\n\t\n19\nMicroservice development lifecycle\nthree key stages in the development lifecycle of a microservice application: designing \nservices, deploying them to production, and observing their behavior. This cycle is illus-\ntrated in figure 1.7.\nMaking well-reasoned decisions in each of these three stages will help you build \napplications that are resilient, even in the face of changing requirements and increas-\ning complexity. Let’s walk through each stage and consider the steps you’ll take to \ndeliver an application with microservices.\n1.3.1\t\nDesigning microservices\nYou’ll need to make several design decisions when building a microservice application \nthat you wouldn’t have encountered building monolithic apps. The latter often fol-\nlow well-known patterns or frameworks, such as three-tier architecture or model-view \ncontroller (MVC). But techniques for designing microservices are still in their relative \ninfancy. You’ll need to consider\n¡ Whether to start with a monolith or commit to microservices up front\n¡ The overall architecture of your application and the façade it presents to outside \nconsumers\n¡ How to identify and scope the boundaries of your services\n¡ How your services communicate with each other, whether synchronously or \nasynchronously\n¡ How to achieve resiliency in services\nThat’s quite a lot of ground to cover. For now, we’ll touch on each of these consid-\nerations so you can see why paying attention to all of them is vital to a well-designed \nmicroservice application.\nMonolith first?\nYou’ll find two opposing trends to starting with microservices: monolith first or micro-\nservices only. Advocates of the former reason that you should always start with a mono-\nlith, as you won’t understand the component boundaries in your system at an early \nstage, and the cost of getting these wrong is much higher in a microservice application. \nOn the other hand, the boundaries you choose in a monolith aren’t necessarily the \nsame ones you’d choose in a well-designed microservice application.\nDesign\nDeploy\nObserve\nFigure 1.7    The key iterative stages — design, deploy, and observe — in the microservice development \nlifecycle\n \n",
      "page_number": 32
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 40-47)",
      "start_page": 40,
      "end_page": 47,
      "detection_method": "topic_boundary",
      "content": "20\nChapter 1  Designing and running microservices \nAlthough the speed of development may be slower to begin with, microservices will \nreduce friction and risk in future development. Likewise, as tooling and frameworks \nmature, microservices best practice is becoming increasingly less daunting to pick \nup. Either way you want to go, the advice in this book should be useful, regardless of \nwhether you’re thinking of migrating away from your monolith or starting afresh.\nScoping services\nChoosing the right level of responsibility for each service — its scope — is one of the \nmost difficult challenges in designing a microservice application. You’ll need to model \nservices based on the business capabilities they provide to an organization.\nLet’s extend the example from the beginning of this chapter. How might your ser-\nvices change if you wanted to introduce a new, special type of order? You have three \noptions to solve this problem (figure 1.8):\n1\t Extend the existing service interface\n2\t Add a new service endpoint\n3\t Add a new service\nEach of these options has pros and cons that will impact the cohesiveness and coupling \nbetween services in your application.\nOrders service exposes an\noperation to create an order\nTo support a new type of order, we could...\nOrders service\nOrders service\nOrders service\nOrders service\nSpecial order\nservice\nAdd a new service for the new order type\nAlter the operation contract to\naccept new fields\nAdd a new operation to the\nexisting orders service\nFigure 1.8. To scope functionality, you need to make decisions about whether capabilities belong in \nexisting services or if you need to design new services.\n \n\n\n\t\n21\nMicroservice development lifecycle\nNOTE     In chapters 2 and 4, we’ll explore service scoping and how to make opti-\nmal decisions about service responsibility.\nCommunication\nCommunication between services may be asynchronous or synchronous. Although \nsynchronous systems are easier to reason through, asynchronous systems are highly \ndecoupled — reducing the risk of change — and potentially more resilient. But the \ncomplexity of such a system is high. In a microservice application, you need to balance \nsynchronous and asynchronous messaging to choreograph and coordinate the actions \nof multiple microservices effectively.\nResiliency\nIn a distributed system, a service can’t trust its collaborators, not necessarily because \nthey’re coded poorly or because of human error, but because you can’t safely assume \nthe network between or behavior of those services is reliable or predictable. Services \nneed to be resilient in the face of failure. To achieve this, you need to design your \nservices to work defensively by backing off in the event of errors, limiting request rates \nfrom poor collaborators, and dynamically finding healthy services.\n1.3.2\t\nDeploying microservices\nDevelopment and operations must be closely intertwined when building microservices. \nIt’s not going to work if you build something and throw it over the fence for someone \nelse to deploy and operate it. In a system composed of numerous, autonomous ser-\nvices, if you build it, you should run it. Understanding how your services run will in \nturn help you make better design decisions as your system grows. \nRemember, what’s special about your application is the business impact it deliv-\ners. That emerges from collaboration between multiple services. In fact, you could \nstandardize or abstract away anything outside of the unique capability each service \noffers — ensuring teams are focused on business value. Ultimately, you should reach \na stage where there’s no ceremony involved in deploying a new service. Without this, \nyou’ll invest all your energy in plumbing, rather than creating value for customers.\nIn this book, we’ll teach you how to construct a reliable road to production for exist-\ning and new services. The cost of deploying new services must be negligible to enable \nrapid innovation. Likewise, you should standardize this process to simplify system oper-\nation and ensure consistency across services. To achieve this, you’ll need to\n¡ Standardize microservice deployment artifacts\n¡ Implement continuous delivery pipelines\nWe’ve heard reliable deployment described as boring, not in the sense that it’s unex-\nciting, but that it’s incident-free. Unfortunately, we’ve seen too many teams where the \nopposite is true: deploying software is stressful and encourages unhealthy all-hands-on-\ndeck behavior. This is bad enough for one service — if you’re deploying any number \n \n\n\n22\nChapter 1  Designing and running microservices \nof services, the anxiety alone will drive you mad! Let’s look at how these steps lead to \nstable and reliable microservice deployments. \nStandardize microservice deployment artifacts\nIt often seems like every language and framework has its own deployment tool. Python \nhas Fabric, Ruby has Capistrano, Elixir has exrm, and so on. And then the deployment \nenvironment itself is complex: \n¡ What server does an application run on? \n¡ What are the application’s dependencies on other tools? \n¡ How do you start that application? \nAt runtime, an application’s dependencies (figure 1.9) are broad and might include \nlibraries, binaries and OS packages (such as ImageMagick or libc), and OS processes \n(such as cron or fluentd).\nTechnically, heterogeneity is a fantastic benefit of service autonomy. But it doesn’t \nmake life easy for deployment. Without consistency, you won’t be able to standardize \nyour approach to taking services to production, which increases the cost of managing \ndeployments and introducing new technology. At worst, each team reinvents the wheel, \ncoming up with different approaches for managing dependencies, packing builds, get-\nting them onto servers, and operating the application itself.\nAn application exposes an\noperational API.\nRestart\nStart\nApplication\nSupporting\nprocesses,\nfor example,\nlogging,\ncron\nOperating system\nApplication libraries\nBinary dependencies, for\nexample, ImageMagick\nStop\nAn application has multiple\npoints of explicit and implicit\ndependency.\nFigure 1.9    An application exposes an operational API and has many types of dependencies, including \nlibraries, binary dependencies, and supporting processes.\n \n\n\n\t\n23\nMicroservice development lifecycle\nOur experience suggests the best tools for this job are containers. A container is an \noperating system-level virtualization method that supports running isolated sys-\ntems on a host, each with its own network and process space, sharing the same ker-\nnel. A container is quicker to build and quicker to start up than a virtual machine \n(seconds, rather than minutes). You can run multiple containers on one machine, \nwhich simplifies local development and can help to optimize resource usage in cloud \nenvironments.\nContainers standardize the packaging of an application, and the runtime interface \nto it, and provide immutability of both operating environment and code. This makes \nthem powerful building blocks for higher level composition. By using them, you can \ndefine and isolate the full execution environment of any service.\nAlthough many implementations of containers are available (and the concept exists \noutside of Linux, such as jails in FreeBSD and zones in Solaris), the most mature and \napproachable tooling that we’ve used so far is Docker. We’ll use that tool later in this book.\nImplement continuous delivery pipelines\nContinuous delivery is a practice in which developers produce software that they \ncan reliably release to production at any time. Imagine a factory production line: \nto continuously deliver software, you build similar pipelines to take your code from \ncommit to live operation. Figure 1.10 illustrates a simple pipeline. Each stage of \nthe pipeline provides feedback to the development team on the correctness of \ntheir code.\nEarlier, we mentioned that microservices are an ideal enabler of continuous \ndelivery because their smaller size means you can develop them quickly and release \nthem independently. But continuous delivery doesn’t automatically follow from \ndeveloping microservices. To continuously deliver software, you need to focus on \ntwo goals:\n¡ Building a set of validations that your software has to pass through. At each \nstage of your deployment process, you should be able to prove the correctness \nof your code.\n¡ Automating the pipeline that delivers your code from commit to production. \nCode commit\nBuild\nUnit test\nPackage\nProduction\nQuality uncertain\nDeployment pipeline\nQuality proven\nIntegration\ntest\nAcceptance\ntest\nFigure 1.10    A high-level deployment pipeline for a microservice\n \n\n\n24\nChapter 1  Designing and running microservices \nBuilding a provably correct deployment pipeline will allow developers to work safely \nand at pace as they iteratively develop services. Such a pipeline is a repeatable and reli-\nable process for delivering new features. Ideally, you should be able to standardize the \nvalidations and steps in your pipeline and use them across multiple services, further \nreducing the cost of deploying new services. \nContinuous delivery also reduces risk, because the quality of the software produced \nand the team’s agility in delivering changes are both increased. From a product per-\nspective, this may mean you can work in a leaner fashion — rapidly validating your \nassumptions and iterating on them.\nNOTE     In part 3, we’ll build a continuous delivery pipeline using the Pipe-\nline feature of the freely available Jenkins continuous integration tool. We’ll \nalso explore different deployment patterns, such as canaries and blue-green \ndeployments.\n1.3.3\t\nObserving microservices\nWe’ve discussed transparency and observability throughout this chapter. In produc-\ntion, you need to know what’s going on. The importance of this is twofold: \n¡ You want to proactively identify and refactor fragile implementation in your \nsystem.\n¡ You need to understand how your system is behaving. \nThorough monitoring is significantly more difficult in a microservice application \nbecause single transactions may span multiple distinct services; technically heteroge-\nneous services might produce data in irreconcilable formats; and the total volume of \noperational data is likely to be much higher than that of a single monolithic applica-\ntion. But if you’re able to understand how your system operates — and observe that \nclosely — despite this complexity, you’ll be better placed to make effective changes to \nyour system.\nIdentify and refactor potentially fragile implementation\nSystems will fail, whether because of bugs introduced, runtime errors, network failures, \nor hardware problems.8 Over time, the cost of eliminating unknown bugs and errors \nbecomes higher than the cost of being able to react quickly and effectively when they \noccur.\nMonitoring and alerting systems allow you to diagnose problems and determine \nwhat causes failures. You may have automated mechanisms reacting to the alerts that’ll \nspawn new container instances in different data centers or react to load issues by \nincreasing the number of running instances of a service.\nTo minimize the consequences of those failures, and prevent them cascading through-\nout the system, you need to be able to architect dependencies between services in ways \n8\t You even have to watch out for squirrels: Rich Miller, “Surviving Electric Squirrels and UPS Fail-\nures,” DataCenter Knowledge, July 9, 2012, http://mng.bz/rmbF.\n \n\n\n\t\n25\nMicroservice development lifecycle\nthat’ll allow for partial degradation. One service going down shouldn’t bring down the \nwhole application. It’s important to think about the possible failure points of your appli-\ncations, recognize that failure will always happen, and prepare accordingly.\nUnderstand behavior across hundreds of services\nYou need to prioritize transparency in design and implementation to understand \nbehavior across your services. Collecting logs and metrics — and unifying them for ana-\nlytical and alerting purposes — allows you to build a single source of truth to resort to \nwhen monitoring and investigating the behavior of your system.\nAs we mentioned in section 1.3.2, you can standardize and abstract anything outside \nof the unique capability each service offers. You can think of each service as an onion. \nAt the center of that onion, you have the unique business capability offered by that \nservice. Surrounding that, you have layers of instrumentation — business metrics, appli-\ncation logs, operational metrics, and infrastructure metrics — that make that capability \nobservable. You can then trace each request to the system through these layers. You’d \nthen push the data you collected from these layers to an operational data store for ana-\nlytics and alerting. This is illustrated in figure 1.11.\nNOTE    In part 4 of this book, we’ll discuss how to build a monitoring system \nfor microservices, collect appropriate data, and use that data to produce a live \nmodel for a complex microservice application.\nRequests\nResponses\nInfrastructure metrics\nOperational metrics\nApplication logs\nBusiness metrics\nBusiness\ncapability\nOperational data store\n \nFigure 1.11    A business capability microservice surrounded by layers of instrumentation, through which \npass requests to the microservice and its responses, with data collected from the process going to an \noperational data store\n \n\n\n26\nChapter 1  Designing and running microservices \n1.4\t\nResponsible and operationally aware engineering \nculture\nIt’d be a mistake to examine the technical nature of microservices in isolation from \nhow an engineering team works to develop them. Building an application out of small, \nindependent services will drastically change how an organization approaches engi-\nneering, so guiding the culture and priorities of your team will be a significant factor in \nwhether you successfully deliver a microservice application.\nIt can be difficult to separate cause and effect in organizations that have successfully \nbuilt microservices. Was the development of fine-grained services a logical outcome of \ntheir organizational structure and the behavior of their teams? Or did that structure \nand behavior arise from their experiences building fine-grained services?\nThe answer is a bit of both. A long-running system isn’t only an accumulation of \nfeatures requested, designed, and built. It also reflects the preferences, opinions, and \nobjectives of its builders and operators. Conway’s Law expresses this to some degree:\norganizations which design systems ... are constrained to produce designs which are copies \nof the communication structures of these organizations.\n“Constrained” might suggest that these communication structures will limit and con-\nstrict the effective development of a system. In fact, microservices practice implies the \nopposite: that a powerful way to avoid friction and tension in building systems is to \ndesign an organization in the shape of the system you intend to build.\nDeliberate symbiosis with organizational structure is one example of common \nmicroservices practice. To be able to realize benefits from microservices and adequately \nmanage their complexity, you need to develop working principles and practices that are \neffective for that type of application, rather than using the same techniques that you \nused to build monoliths.\nSummary\n¡ Microservices are both an architectural style and a set of cultural practices, \nunderpinned by five key principles: autonomy, resilience, transparency, automa-\ntion, and alignment.\n¡ Microservices reduce friction in development, enabling autonomy, technical \nflexibility, and loose coupling.\n¡ Designing microservices can be challenging because of the need for adequate \ndomain knowledge and balancing priorities across teams.\n¡ Services expose contracts to other services. Good contracts are succinct, com-\nplete, and predictable. \n¡ Complexity in long-running software systems is unavoidable, but you can deliver \nvalue sustainably in these systems if you make choices that minimize friction and risk.\n¡ Reliably incident-free (“boring”) deployment reduces the risk of microservices \nby making releases automated and provable.\n \n\n\n\t\n27\nSummary\n¡ Containers abstract away differences between services at runtime, simplifying \nlarge-scale management of heterogeneous microservices.\n¡ Failure is inevitable: microservices need to be transparent and observable for \nteams to proactively manage, understand, and own service operation ... and the \nlack thereof.\n¡ Teams adopting microservices need to be operationally mature and focus on the \nentire lifecycle of a service, not only on the design and build stages.\n \n",
      "page_number": 40
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 48-55)",
      "start_page": 48,
      "end_page": 55,
      "detection_method": "topic_boundary",
      "content": "28\n2\nMicroservices at \nSimpleBank\nThis chapter covers\n¡ Introducing SimpleBank, a company adopting \nmicroservices\n¡ Designing a new feature with microservices\n¡ How to expose microservice-based features to \nthe world\n¡ Ensuring features are production ready\n¡ Challenges faced in scaling up microservice \ndevelopment\nIn Chapter 1, you learned about the key principles of microservices and why they’re \na compelling approach for sustainably delivering software value. We also introduced \nthe design and development practices that underpin microservices development. In \nthis chapter, we’ll explore how you can apply those principles and practices to devel-\noping new product features with microservices.\nOver the course of this chapter, we’ll introduce the fictitious company of \nSimpleBank. They’re a company with big plans to change the world of investment, \nand you’re working for them as an engineer. The engineering team at Simple-\nBank wants to be able to deliver new features rapidly while ensuring scalability and \n \n\n\n\t\n29\nWhat does SimpleBank do?\nstability — after all, they’re dealing with people’s money! Microservices might be exactly \nwhat they need.\nBuilding and running an application made up of independently deployable and \nautonomous services is a vastly different challenge from building that application as \na single monolithic unit. We’ll begin by considering why a microservice architecture \nmight be a good fit for SimpleBank and then walk you through the design of a new fea-\nture using microservices. Finally, we’ll identify the steps needed to develop that proof of \nconcept into a production-grade application. Let’s get started.\n2.1\t\nWhat does SimpleBank do?\nThe team at SimpleBank wants to make smart financial investment available to every-\none, no matter how much money they have. They believe that buying shares, selling \nfunds, or trading currency should be as simple as opening a savings account.\nThat’s a compelling mission, but not an easy one. Financial products have multiple \ndimensions of complexity: SimpleBank will need to make sense of market rules and \nintricate regulations, as well as integrate with existing industry systems, all while meet-\ning stringent accuracy requirements.\nIn the previous chapter, we identified some of the functionality that SimpleBank \ncould offer its customers: opening accounts, managing payments, placing orders, and \nmodeling risk. Let’s expand on those possibilities and look at how they might fit within \nthe wider domain of an investment tool. Figure 2.1 illustrates the different elements of \nthis domain.\nAs the figure shows, an investment tool will need to do more than offer customer- \nfacing features, like the ability to open accounts and manage a financial portfolio. It \nalso will need to manage custody, which is how the bank holds assets on behalf of cus-\ntomers and moves them in or out of their possession, and manufacture, which is the \ncreation of financial products appropriate to customer needs.\nAccount\nmanagement\nCustomer management\nManaging accounts, tax,\nregulatory requirements\nCustody\nUnderlying banking services and\ninteraction with other market entities,\nfor example, brokerages, other banks\nProduct manufacture\nThe development and maintenance\nof financial products, for example,\nfunds consisting of other funds\nPortfolio\nreporting\nFinancial\npredictions and\nadvice\nFees\nTax\nOrder placement\nTrade &\nmarket\nexecution\nAsset transfers\nPayment\nprocessing\nCorporate actions,\nfor example,\ndividends\nOwnership\n& custody\nMarket data,\nfor example,\nshare prices\nAggregated\ntrading\nTax\nBulk orders\nRisk analysis\nPricing\nOwnership\nFigure 2.1    A high-level (and by no means exhaustive) model of functionality that SimpleBank might build\n \n\n\n30\nChapter 2  Microservices at SimpleBank \nAs you can see, it’s not so simple! You can begin to see some of the business capabilities \nthat SimpleBank might implement: portfolio management, market data integrations, \norder management, fund manufacture, and portfolio analysis. Each of the business \nareas identified might consist of any number of services that collaborate with each \nother or services in other areas.\nThis type of high-level domain model is a useful first step when approaching any sys-\ntem, but it’s crucial when building microservices. Without understanding your domain, \nyou might make incorrect decisions about the boundaries of your services. You don’t \nwant to build services that are anemic — existing only to perform trivial create, read, update, \ndelete (CRUD) operations. These often become a source of tight coupling within an application. At \nthe same time, you want to avoid pushing too much responsibility into a single service. Less cohesive \nservices make software changes slower and riskier — exactly what you’re trying to avoid.\nLastly, without this perspective, you might fall prey to overengineering — choosing \nmicroservices where they’re not justified by the real complexity of your product or domain.\n2.2\t\nAre microservices the right choice?\nThe engineers at SimpleBank believe that microservices are the best choice to tackle \nthe complexity of their domain and be flexible in the face of complex and changing \nrequirements. They anticipate that as their business grows, microservices will reduce the \nrisk of individual software changes, leading to a better product and happier customers.\nAs an example, let’s say they need to process every buy or sell transaction to calculate \ntax implications. But tax rules work differently in every country — and those rules tend \nto change frequently. In a monolithic application, you’d need to make coordinated, \ntime-sensitive releases to the entire platform, even if you only wanted to make changes \nfor one country. In a microservice application, you could build autonomous tax-han-\ndling services (whether by country, type of tax, or type of account) and deploy changes \nto them independently.\nIs SimpleBank making the right choice? Architecting software always involves ten-\nsion between pragmatism and idealism — balancing product needs, the pressures of \ngrowth, and the capabilities of a team. Poor choices may not be immediately apparent, \nas the needs of a system vary over its lifetime. Table 2.1 expands on the factors to con-\nsider when choosing microservices.\nTable 2.1    Factors to consider when choosing a microservice architecture\nFactor\nImpact\nDomain complexity\nIt’s difficult to objectively evaluate the complexity of a domain, but microservices \ncan address complexity in systems driven by competing pressures, such as regu-\nlatory requirements and market breadth.\nTechnical requirements\nYou can build different components of a system using different programming \nlanguages (and associated technical ecosystems). Microservices enable hetero-\ngeneous technical choices.\n \n\n\n\t\n31\nAre microservices the right choice?\nFactor\nImpact\nOrganizational growth\nRapidly growing engineering organizations may benefit from microservices \nbecause lowering dependency on existing codebases enables rapid ramp-up \nand productivity for new engineers.\nTeam knowledge\nMany engineers lack experience in microservices and distributed systems. If the \nteam lacks confidence or knowledge, it may be appropriate to build a proof-of-\nconcept microservice before fully committing to implementation.\nUsing these factors, you can evaluate whether microservices will help you deliver sus-\ntainable value in the face of increasing application complexity.\n2.2.1\t\nRisk and inertia in financial software\nLet’s take a moment to look at how SimpleBank’s competitors build software. Most \nbanks aren’t ahead of the curve in terms of technological innovation. There’s an ele-\nment of inertia that’s typical of larger organizations, although that’s not unique to the \nfinance industry. Two primary factors limit innovation and flexibility:\n¡ Aversion to risk  — Financial companies are heavily regulated and tend to build \ntop-down systems of change control to avoid risk by limiting the frequency and \nimpact of software changes.\n¡ Reliance on complex legacy systems  — Most core banking systems were built pre-1970. \nIn addition, mergers, acquisitions, and outsourcing have led to software systems \nthat are poorly integrated and contain substantial technical debt.\nBut limiting change and relying on existing systems hasn’t prevented software prob-\nlems from leading to pain for customers or the finance companies themselves. The \nRoyal Bank of Scotland was fined £56 million in 2014 when an outage caused payments \nto fail for 6.5 million customers. That’s on top of the £250 million it was already spend-\ning every year on its IT systems.1\nThat approach also hasn’t led to better products. Financial technology startups, such \nas Monzo and Transferwise, are building features at a pace most banks can only dream of.\n2.2.2\t\nReducing friction and delivering sustainable value\nCan you do any better? By any measure, the banking industry is a complex and com-\npetitive domain. A bank needs to be both resilient and agile, even when the lifetime of \na banking system is measured in decades. The increasing size of a monolithic applica-\ntion is antithetical to this goal. If a bank wants to launch a new product, it shouldn’t be \n1\t See Sean Farrell and Carmen Fishwick, “RBS could take until weekend to make 600,000 missing \npayments after glitch,” The Guardian, June 17, 2015, http://mng.bz/kxQY, and Chad Bray, “Royal \nBank of Scotland Fined $88 Million Over Technology Failure,” Dealbook, The New York Times, No-\nvember 20, 2014, http://mng.bz/hn8D.\nTable 2.1    Factors to consider when choosing a microservice architecture  (continued)\n \n\n\n32\nChapter 2  Microservices at SimpleBank \nbogged down by the legacy of previous builds2 or require outsize effort and investment \nto prevent regression in existing functionality.\nA well-designed microservice architecture can solve these challenges. As we estab-\nlished earlier, this type of architecture avoids many of the characteristics that, in mono-\nlithic applications, slow velocity in development. Individual teams can move forward \nwith increased confidence as\n¡ Change cycles are decoupled from other teams.\n¡ Interaction between collaborating components is disciplined.\n¡ Continuous delivery of small, isolated changes limits the risk of breaking \nfunctionality.\nThese factors reduce friction in the development of a complex system but maintain \nresiliency. As such, they reduce risk without stifling innovation through bureaucracy.\nThis isn’t only a short-term solution. Microservices aid engineering teams in deliv-\nering sustainable value throughout the lifecycle of an application by placing natural \nbounds on the conceptual and implementation complexity of individual components.\n2.3\t\nBuilding a new feature\nNow that we’ve established that microservices are a good choice for SimpleBank, \nlet’s look at how it might use them to build new features. Building a minimum via-\nble product — an MVP — is a great first step to ensure that a team understands the \nconstraints and requirements of the microservices style. We’ll start by exploring one \nof the features that SimpleBank needs to build and the design choices the team will \nmake, working through the lifecycle we illustrated in chapter 1 (figure 2.2).\nIn chapter 1, we touched on how services might collaborate to place a sell order. An \noverview of this process is shown in figure 2.3.\nLet’s look at how you’d approach building this feature. You need to answer several \nquestions:\n¡ Which services do you need to build?\n¡ How do those services collaborate with each other?\n¡ How do you expose their functionality to the world?\nDesign\nDeploy\nObserve\nFigure 2.2    The key iterative stages — design, deploy, and observe — in the microservice development \nlifecycle\n2\t How bad can it get? I once encountered a financial software company that maintained over 10 \ndistinct monolithic codebases, each surpassing 2 million lines of code!\n \n\n\n\t\n33\nBuilding a new feature\nUser requests\nsale of stock\nIs stock\navailable?\nStock is\nreserved\nagainst\naccount\nWait to place\norder\nNo\nMarket open?\nYes\nPlace order to\nmarket\nFee is charged\nto account\nYes\nNo\nFigure 2.3    The process of placing an order to sell a financial position from an account at SimpleBank\nThese may be similar to the questions you might ask yourself when designing a feature \nin a monolithic application, but they have different implications. For example, the \neffort required to deploy a new service is inherently higher than creating a new mod-\nule. In scoping microservices, you need to ensure that the benefits of dividing up your \nsystem aren’t outweighed by added complexity.\nNOTE    As the application evolves, these questions will take on added dimen-\nsions. Later, we’ll also ask whether to add functionality to existing services or \ncarve those services up. We’ll explore this further in chapters 4 and 5.\nAs we discussed earlier, each service should be responsible for a single capability. Your \nfirst step will be to identify the distinct business capabilities you want to implement and \nthe relationship between those capabilities.\n2.3.1\t\nIdentifying microservices by modeling the domain\nTo identify the business capabilities you want, you need to develop your understand-\ning of the domain where you’re building software. This is normally the hard work of \nproduct discovery or business analysis: research; prototyping; and talking to customers, \ncolleagues, or other end users.\nLet’s start by exploring the order placement example from figure 2.3. What value are \nyou trying to deliver? At a high level, a customer wants to be able to place an order. So, \nan obvious business capability will be the ability to store and manage the state of those \norders. This is your first microservice candidate.\nContinuing our exploration of the example, you can identify other functionalities \nyour application needs to offer. To sell something, you need to own it, so you need some \nway of representing a customer’s current holdings resulting from the transactions that \nhave occurred against their account. Your system needs to send an order to a broker —  \nthe application needs to be able to interact with that third party. In fact, this one feature, \n \n\n\n34\nChapter 2  Microservices at SimpleBank \nplacing a sell order, will require SimpleBank’s application to support all of the follow-\ning functionality:\n¡ Record the status and history of sell orders\n¡ Charge fees to the customer for placing an order\n¡ Record transactions against the customer’s account\n¡ Place an order onto a market\n¡ Provide valuation of holdings and order to customer\nIt’s not a given that each function maps to a single microservice. You need to deter-\nmine which functions are cohesive — they belong together. For example, transactions \nresulting from orders will be similar to transactions resulting from other events, such \nas dividends being paid on a share. Together, a group of functions forms a capability \nthat one service may offer. \nLet’s map these functions to business capabilities — what the business does. You can \nsee this mapping in figure 2.4. Some functions cross multiple domains, such as fees.\nOrder management\nRecording status and\nhistory of orders\nFunction\nBusiness capabilities\nPlacing an order to\nmarket\nCharging a fee\nRecording transactions\nagainst customer\naccount\nValue positions held in\nan account\nMarket execution\nFees\nTransaction ledger\nMarket data\nFigure 2.4    The relationship between application functionality and capabilities within SimpleBank’s \nbusiness\n \n\n\n\t\n35\nBuilding a new feature\nYou can start by mapping these capabilities directly to microservices. Each service \nshould reflect a capability that the business offers — this results in a good balance of \nsize versus responsibility. You also should consider what would drive a microservice \nto change in the future — whether it truly has single responsibility. For example, you \ncould argue that market execution is a subset of order management and therefore \nshouldn’t be a separate service. But the drivers for change in that area are the behavior \nand scope of the markets you’re supporting, whereas order management relates more \nclosely to the types of product and the account being used to trade. These two areas \ndon’t change together. By separating them, you isolate areas of volatility and maximize \ncohesiveness (figure 2.5).\nSome microservice practitioners would argue that microservices should more closely \nreflect single functions, rather than single capabilities. Some have even suggested that \nmicroservices are “append only” and that it’s always better to write new services than to \nadd to existing ones.\nWe disagree. Decomposing too much can lead to services that lack cohesiveness and \ntight coupling between closely related collaborators. Likewise, deploying and monitor-\ning many services might be beyond the abilities of the engineering team in the early \ndays of a microservice implementation. A useful rule of thumb is to err on the side of \nlarger services; it’s often easier to carve out functionality later if it becomes more spe-\ncialized or more clearly belongs in an independent service.\nLastly, keep in mind that understanding your domain isn’t a one-off process! Over \ntime, you’ll continue to iterate on your understanding of the domain; your users’ needs \nwill change, and your product will continue to evolve. As this understanding changes, \nyour system itself will change to meet those needs. Luckily, as we discussed in chap-\nter 1, coping with changing needs and requirements is a strength of the microservices \napproach.\nOrder management\nRecording status and\nhistory of orders\nPlacing an order to\nmarket\nMarket gateway\n2. Separating areas of\nindependent change\npromotes loose coupling and\nincreases cohesiveness.\nNew types of order;\ndifferent account rules;\npromotions...\nNew types of order;\ndifferent market rules;\nnew markets...\n1. These areas are likely to\nchange independently.\nService\nFunction\nReasons to change\nFigure 2.5    Services should isolate reasons to change to promote loose coupling and single \nresponsibility.\n \n",
      "page_number": 48
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 56-63)",
      "start_page": 56,
      "end_page": 63,
      "detection_method": "topic_boundary",
      "content": "36\nChapter 2  Microservices at SimpleBank \n2.3.2\t\nService collaboration\nWe’ve identified several microservice candidates. These services need to collaborate \nwith each other to do something useful for SimpleBank’s customers.\nAs you may already know, service collaboration can be either point-to-point or event-\ndriven. Point-to-point communication is typically synchronous, whereas event-driven \ncommunication is asynchronous. Many microservice applications begin by using syn-\nchronous communication. The motivations for doing so are twofold:\n¡ Synchronous calls are typically simpler and more explicit to reason through than \nasynchronous interaction. That said, don’t fall into the trap of thinking they \nshare the same characteristics as local, in-process function calls — requests across \na network are significantly slower and more unreliable.\n¡ Most, if not all, programming ecosystems already support a simple, language- \nagnostic transport mechanism with wide developer mindshare: HTTP, which is \nmainly used for synchronous calls but you can also use asynchronously.\nConsider SimpleBank’s order placement process. The orders service is responsible for \nrecording and placing an order to market. To do this, it needs to interact with your mar-\nket, fees, and account transaction services. This collaboration is illustrated in figure 2.6.\nTransaction\ndatabase\nUser\n1. Places order to sell\n100 units of Stock A\nfrom account ABC\n2. Records order\ndetails in database\n3. Requests reservation\nof 100 units of Stock A\nagainst account ABC\n4. Records reserved\nstock position against\naccount ABC\n5. Requests calculation\nof fee\n6. Requests placement\nof order to market\n7. Places order onto\nstock exchange\nAccount\ntransactions\nservice\nOrders service\nFees service\nFee rules\ndatabase\nMarket service\nStock exchange\nOrder\ndatabase\nFigure 2.6    The orders service orchestrates the behavior of several other services to place an order to market.\n \n\n\n\t\n37\nBuilding a new feature\nEarlier, we pointed out that microservices should be autonomous, and to achieve that, ser-\nvices should be loosely coupled. You achieve this partly through the design of your services, \n“[gathering] together the things that change for the same reasons” to minimize the chance \nthat changes to one service require changes to its upstream or downstream collaborators. \nYou also need to consider service contracts and service responsibility.\nService contracts\nThe messages that each service accepts, and the responses it returns, form a contract \nbetween that service and the services that rely on it, which you can call upstream collabo-\nrators. Contracts allow each service to be treated as a black box by its collaborators: you \nsend a request and you get something back. If that happens without errors, the service \nis doing what it’s meant to do.\nAlthough the implementation of a service may change over time, maintaining \ncontract-level compatibility ensures two things:\n1\t Those changes are less likely to break consumers.\n2\t Dependencies between services are explicitly identifiable and manageable.\nIn our experience, contracts are often implicit in naïve or early microservice imple-\nmentations; they’re suggested by documentation and practice, rather than explicitly \ncodified. As the number of services grows, you can realize significant benefit from stan-\ndardizing the interfaces between them in a machine-readable format. For example, \nREST APIs may use Swagger/OpenAPI. As well as aiding the conformance testing of \nindividual services, publishing standardized contracts will help engineers within an \norganization understand how to use available services.\nService responsibility\nYou can see in figure 2.6 that the orders service has a lot of responsibility. It directly \norchestrates the actions of every other service involved in the process of placing an \norder. This is conceptually simple, but it has downsides. At worst, our other services \nbecome anemic, with many dumb services controlled by a small number of smart ser-\nvices, and those smart services grow larger\nThis approach can lead to tighter coupling. If you want to introduce a new part of this \nprocess — let’s say you want to notify a customer’s account manager when a large order \nis placed — you’re forced to deploy new changes to the orders service. This increases \nthe cost of change. In theory, if the orders service doesn’t need to synchronously con-\nfirm the result of an action — only that it’s received a request — then it shouldn’t need \nto have any knowledge of those downstream actions.\n2.3.3\t\nService choreography\nWithin a microservice application, services will naturally have differing levels of responsi-\nbility. But you should balance orchestration with choreography. In a choreographed system, \na service doesn’t need to directly command and trigger actions in other services. Instead, \neach service owns specific responsibilities, which it performs in reaction to other events.\n \n\n\n38\nChapter 2  Microservices at SimpleBank \nLet’s revisit the earlier design and make a few tweaks:\n1\t When someone creates an order, the market might not currently be open. There-\nfore, you need to record what status an order is in: created or placed. Placement \nof an order doesn’t need to be synchronous.\n2\t You’ll only charge a fee once an order is placed, so charging fees doesn’t need \nto be synchronous. In fact, it should happen in reaction to the market service, \nrather than being orchestrated by the orders service.\nFigure 2.7 illustrates the changed design. Adding events adds an architectural con-\ncern: you need some way of storing them and exposing them to other applications. \nWe’d recommend using a message queue for that purpose, such as RabbitMQ or SQS.\nIn this design, we’ve removed the following responsibility from the orders service:\n¡ Charging fees  — The orders service has no awareness that a fee is being charged \nonce an order is being placed to market.\n¡ Placing orders  — The orders service has no direct interaction with the market ser-\nvice. You could easily replace this with a different implementation, or even a service \nper market, without needing to change the orders service itself.\nTransaction\ndatabase\nUser\n1. Places order to sell\n100 units of Stock A\nfrom account ABC\n2. Records order\ndetails in database\n3. Requests reservation\nof 100 units of Stock A\nagainst account ABC\n8. Orders service\nsubscribes to\nOrderPlaced events\n6. Trigger event\nOrderPlaced\n4. Market order\nservice subscribes\nto OrderCreated\nevents\n7. Fees service\nsubscribes to\nOrderPlaced events\n3. Triggers event\nOrderCreated\n9. Orders service\nupdates status of\norder to placed\n4. Records reserved\nstock position against\naccount ABC\n5. Places order onto\nstock exchange\nAccount\ntransactions\nservice\nOrders service\nFees service\nFee rules\ndatabase\nMarket service\nEvent queue\nStock exchange\nOrder\ndatabase\nFigure 2.7    You choreograph the behavior of other services through events, reducing the coordinating \nrole of the orders service. Note that some actions, for example, the two actions numbered “3.,” happen \nconcurrently.\n \n\n\n\t\n39\nExposing services to the world\nThe orders service itself also reacts to the behavior of other services by subscribing to the \nOrderPlaced event emitted by the market service. You can easily extend this to further \nrequirements; for example, the orders service might subscribe to TradeExecuted events \nto record when the sale has been completed on the market or OrderExpired events if the \nsale can’t be made within a certain timeframe.\nThis setup is more complex than the original synchronous collaboration. But by \nfavoring choreography where possible, you’ll build services that are highly decoupled \nand therefore independently deployable and amenable to change. These benefits do \ncome at a cost: a message queue is another piece of infrastructure to manage and scale \nand itself can become a single point of failure.\nThe design we’ve come up with also has some benefit in terms of resiliency. For \nexample, failure in the market service is isolated from failure in the orders service. If \nplacing an order fails, you can replay that event3 later, once the service is available, or \nexpire it if too much time passes. On the other hand, it’s now more difficult to trace the \nfull activity of the system, which you’ll need to consider when you think about how to \nmonitor these services in production.\n2.4\t\nExposing services to the world\nSo far, we’ve explored how services collaborate to achieve some business goal. How do \nyou expose this functionality to a real user application?\nSimpleBank wants to build both web and mobile products. To do this, the engineering \nteam have decided to build an API gateway as a façade over these services. This abstracts \naway backend concerns from the consuming application, ensuring it doesn’t need to \nhave any awareness of underlying microservices, or how those services interact with each \nother to deliver functionality. An API gateway delegates requests to underlying services \nand transforms or combines their responses as appropriate to the needs of a public API.\nImagine the user interface of a place order screen. It has four key functions:\n¡ Displaying information about the current holdings within a customer’s account, \nincluding both quantity and value\n¡ Displaying market data showing prices and market movements for a holding\n¡ Inputting orders, including cost calculation\n¡ Requesting execution of those orders against the specified holdings\nFigure 2.8 illustrates how an API gateway serves that functionality, and how that gate-\nway collaborates with underlying services.\nThe API gateway pattern is elegant but has a few downsides. Because it acts as a single \ncomposition point for multiple services, it’ll become large and possibly unwieldy. It may \nbe a temptation to add business logic in the gateway, rather than treating it as a proxy \nalone. It can suffer from trying to be all things to all applications: whereas a mobile cus-\ntomer application may want a smaller, cut-down payload, but an internal administration \nweb application might require significantly more data. It can be hard to balance these \ncompeting forces while building a cohesive API.\n3\t Assuming the queue itself is persistent.\n \n\n\n40\nChapter 2  Microservices at SimpleBank \nOrders\nHoldings\nTransactions\nMarket data\nBackend services\nAPI gateway\nUser interface\nHoldings\nMarket data\nInput\nSubmit\nGET /holdings\nGET /market/{id}\nPOST /orders\nFigure 2.8    A user interface, such as a web page or mobile app, interacts with the REST API that an API \ngateway exposes. The gateway provides a façade over underlying microservices and proxies requests to \nappropriate backend services.\nNOTE    We’ll revisit the API gateway pattern and discuss alternative approaches \nin chapter 3.\n2.5\t\nTaking your feature to production\nYou’ve designed a feature for SimpleBank that involves the interaction of multiple ser-\nvices, an event queue, and an API gateway. Let’s say you’ve taken the next step: you’ve \nbuilt those services and now the CEO is pushing you to get them into production.\nIn public clouds like AWS, Azure, or GCE, the obvious solution is to deploy each ser-\nvice to a group of virtual machines. You could use load balancers to spread load evenly \nacross instances of each web-facing service, or you could use a managed event queue, \nsuch as AWS’s Simple Queue Service, to distribute events between services.\nNOTE    An in-depth discussion of effective infrastructure automation and man-\nagement is outside the scope of this book. Most cloud providers provide this \ncapability through custom tooling, such as AWS’s CloudFormation or Elastic \nBeanstalk. Alternatively, you could consider open source tools, such as Chef or \nTerraform.\nAnyway — you compiled that code, FTP’d it onto those VMs, got the databases up and \nrunning, and tried some test requests. This took a few days. Figure 2.9 shows your pro-\nduction infrastructure.\nFor a few weeks, that didn’t work too badly. You made a few changes and pushed out \nthe new code. But soon you started to run into trouble. It was hard to tell if the services \n \n\n\n\t\n41\nTaking your feature to production\nwere working as expected. Worse, you were the only person at SimpleBank who knew \nhow to release a new version. Even worse than that, the guy who wrote the transaction \nservice went on vacation for a few weeks, and no one knew how the service was deployed. \nThese services would have a bus factor of 1 — suggesting they wouldn’t survive the disap-\npearance of any team member.\nDEFINITION    bus factor is a measurement of the risk of knowledge not being \nshared between multiple team members, from the phrase “in case they get hit by \na bus.” It’s also known as truck factor. The lower bus factors are, the worse they are.\nSomething was definitely wrong. You remembered that in your last job at GiantBank, \nthe infrastructure team managed releases. You’d log a ticket, argue back and forth, \nand after a few weeks, you’d have what you needed…or sometimes not, so you’d log \nanother ticket. That doesn’t seem like the right approach either. In fact, you were glad \nthat using microservices allowed you to manage deployment.\nVM\nVM\nTransaction\ndatabase\nHTTP requests are\nload-balanced across\nservice instances.\nEach service is deployed on\nmultiple VMs.\nAccount\ntransactions\nOrders\nservice\nLoad\nbalancer\nLoad\nbalancer\nLoad\nbalancer\nFees\nservice\nEvent queue\nGateway\nUser\nVM\nVM\nFee rules\ndatabase\nMarket\nservice\nOrder\ndatabase\nFigure 2.9    In a simple microservices deployment, requests to each service are load balanced across \nmultiple instances, running across multiple virtual machines. Likewise, multiple instances of a service \nmay subscribe to a queue.\n \n\n\n42\nChapter 2  Microservices at SimpleBank \nIt’s safe to say that your services weren’t ready for production. Running microservices \nrequires a level of operational awareness and maturity from an engineering team \nbeyond what’s typical in a monolithic application. You can only say a service is produc-\ntion ready if you can confidently trust it to serve production workloads.\nHow can you be confident a service is trustworthy? Let’s start with a list of questions \nyou might need to consider to achieve production readiness:\n¡ Reliability  — Is your service available and error free? Can you rely on your \ndeployment process to push out new features without introducing instability \nor defects?\n¡ Scalability  — Do you understand the resource and capacity needs of a service? \nHow will you maintain responsiveness under load?\n¡ Transparency  — Can you observe a service in operation through logs and metrics? \nIf something goes wrong, is someone notified?\n¡ Fault tolerance  — Have you mitigated single points of failure? How do you cope \nwith the failure of other service dependencies?\nAt this early stage in the lifetime of a microservice application, you need to establish \nthree fundamentals:\n¡ Quality-controlled and automated deployments\n¡ Resilience\n¡ Transparency\nLet’s examine how these fundamentals will help you address the problems that Simple-\nBank has encountered.\n2.5.1\t\nQuality-controlled and automated deployment\nYou’ll lose the added development speed you gain from microservices if you can’t get \nthem to production rapidly and reliably. The pain of unstable deployments — such as \nintroducing a serious error — will eliminate those speed gains.\nTraditional organizations often seek stability by introducing (often bureaucratic) \nchange control and approval processes. They’re designed to manage and limit change. \nThis isn’t an unreasonable impulse: if changes introduce most bugs4 — costing the com-\npany thousands (or millions) of dollars of engineering effort and lost revenue — then \nyou should closely control those changes.\nIn a microservice architecture, this won’t work, because the system will be in a state \nof continuous evolution; it’s this freedom that gives rise to tangible innovation. But \nto ensure that freedom doesn’t lead to errors and outages, you need to be able to \ntrust your development process and deployment. Equally, to enable such freedom in \nthe first place, you also need to minimize the effort required to release a new service \n4\t \"SRE has found that roughly 70% of outages are due to changes in a live system.\" Benjamin Treynor \nSloss, Chapter 1, Site Reliability Engineering, 2017, O’Reilly Media, http://mng.bz/7Mm4.\n \n\n\n\t\n43\nTaking your feature to production\nor change an existing one. You can achieve stability through standardization and \nautomation:\n¡ You should standardize the development process. You should review code changes, write \nappropriate tests, and maintain version control of the source code. We hope this \ndoesn’t surprise anyone!\n¡ You should standardize and automate the deployment process. You should thoroughly \nvalidate the delivery of a code change to production, and it should require mini-\nmal intervention from an engineer. This is a deployment pipeline.\n2.5.2\t\nResilience\nEnsuring a software system is resilient in the face of failure is a complicated task. The \ninfrastructure underpinning your systems is inherently unreliable; even if your code is \nperfect, network calls will fail and servers will go down. As part of designing a service, \nyou need to consider how it and its dependencies may fail and proactively work to \navoid — or minimize the impact of — those failure scenarios.\nTable 2.2 examines the potential areas of risk in the system that SimpleBank has \ndeployed. You can see that even a relatively simple microservice application introduces \nseveral areas of potential risk and complexity.\nTable 2.2    Areas of risk in SimpleBank’s microservice application\nArea\nPossible failures\nHardware\nHosts, data center components, physical network\nCommunication between services\nNetwork, firewall, DNS errors\nDependencies\nTimeouts, external dependencies, internal failures, for example,  \nsupporting databases\nNOTE    Chapter 6 will investigate techniques for maximizing service resilience.\n2.5.3\t\nTransparency\nThe behavior and state of a microservice should be observable: at any time, you should \nbe able to determine whether the service is healthy and whether it’s processing its \nworkload in the way you expect. If something affects a key metric — say, orders are \ntaking too long to be placed to market — this should send an actionable alert to the \nengineering team.\nWe’ll illustrate this with an example. Last week, there was an outage at SimpleBank. A \ncustomer called and told you she was unable to submit orders. Quick investigation turned \nup that this was affecting every customer: requests made to the order creation service \nwere timing out. Figure 2.10 illustrates the possible points of failure within that service.\n \n",
      "page_number": 56
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 64-71)",
      "start_page": 64,
      "end_page": 71,
      "detection_method": "topic_boundary",
      "content": "44\nChapter 2  Microservices at SimpleBank \nUser\nGateway\nNetwork issues affect connectivity,\nfor example, between gateway\nand load balancers.\nService instances\nbecome saturated with\nrequests and fail to\nrespond or pass\ntimeout limits.\nThe database is overloaded by\nrequests or poor queries.\nDependencies\nService dependencies may\nrespond slowly or become\nunresponsive.\nOrder \ncreation\nservice\nLB\nDatabase\nFigure 2.10    A service timeout may be due to several underlying reasons: network issues, problems with \nservice-internal dependencies — such as databases — or unhealthy behavior from other services.\nIt was clear that you had a major operational problem: you lacked logging to determine \nexactly what went wrong and where things were falling apart. Through manual testing, \nyou managed to isolate the problem: the account transaction service was unresponsive. \nMeanwhile, your customers had been unable to place orders for several hours. They \nweren’t happy.\nTo avoid such problems in the future, you need to add thorough instrumentation to \nyour microservices. Collecting data about application activity — at all layers — is vital to \nunderstanding the present and past operational behavior of a microservice application.\nAs a first step, SimpleBank set up infrastructure to aggregate the basic logs that your \nservices produced, sending them to a service that allowed you to tag and search them.5 \nFigure 2.11 illustrates this approach. By doing this, the next time a service failed, the \nengineering team could use those logs to identify the point where the system began to \nfail and diagnose the issue precisely where it occurred.\nBut inadequate logging wasn’t the only problem. It was embarrassing that SimpleBank \nonly identified an issue once a customer called. The company should have had alerting in \nplace to ensure that each service was meeting its responsibilities and service goals.\nIn such cases, in its most simple form, you should have a recurring heartbeat check \nthat happening on each service to alert the team if a service becomes completely unre-\nsponsive. Beyond that, a team should commit to operational guarantees for each ser-\nvice. For example, for a critical service, you might aim for 95% of requests to return \nin under 100ms with 99.99% uptime. Failing to meet these thresholds should result in \nalerts being sent to the service owners.\n5\t Several managed services exist for log aggregation, including Loggly, Splunk, and Sumo Logic. \nYou also can run this function in-house using the well-known ELK (Elasticsearch, Logstash, Kibana) \ntool stack.\n \n\n\n\t\n45\nScaling up microservice development\nAn agent runs on each VM to\ncollect log data from running\nservices, such as requests made.\nTransactions\nlogs\nVM\nAgent\nLog store\nEach agent ships logs to a\ndedicated store.\nYou can be index and search\nlogs to investigate service\nissues, build reports, or\ntrigger alerts.\nSearch\nEngineers\nOrders\nlogs\nVM\nAgent\nFigure 2.11    You install a logging collection agent on each instance. This ships application log data to a \ncentral repository where you can index, search, and analyze it further.\nBuilding thorough monitoring for a microservice application is a complex task. The \ndepth of monitoring you apply will evolve as your system increases in complexity and \nnumber of services. As well as the operational metrics and logging we’ve described, \na mature microservice monitoring solution will address business metrics, interservice \ntracing, and infrastructure metrics. If you are to trust your services, you need to con-\nstantly work at making sense of that data.\nNOTE    In part 4 of this book, we’ll discuss monitoring in detail, and how to \nuse tools like Prometheus to trigger alerts and build health dashboards for \nmicroservices.\n2.6\t\nScaling up microservice development\nThe technical flexibility of microservices is a blessing for the speed of development \nand the effective scalability of a system. But that same flexibility also leads to organiza-\ntional challenges that change the nature of how an engineering team works at scale. \nYou’ll quickly encounter two challenges: technical divergence and isolation.\n2.6.1\t\nTechnical divergence\nImagine SimpleBank has built a large microservice system of say 1,000 services. A small \nteam of engineers owns each service, with each team using their preferred languages, \ntheir favorite tools, their own deployment scripts, their favored design principles, their \npreferred external libraries,6 and so on.\nTake a moment to recoil in terror at the sheer weight of effort involved in main-\ntaining and supporting so many different approaches. Although microservices make it \npossible to choose different languages and frameworks for different services, it’s easy to \n6\t Unfortunately, this isn’t only an issue in microservices, although it’s exacerbated by hard compo-\nnent boundaries and explicit service ownership. Earlier in my career, I encountered a single Ruby \nproject that used six different HTTP client libraries!\n \n\n\n46\nChapter 2  Microservices at SimpleBank \nsee that without choosing reasonable standards and limits, the system will become an \nunimaginable and fragile sprawl.\nIt’s easy to see this frustration emerge on a smaller scale. Consider two ser-\nvices — account transactions and orders — that two different teams own. The first ser-\nvice produces well-structured log output for every request, including helpful diagnostic \ninformation such as timings, a request ID, and the currently released revision ID:\nservice=api\ngit_commit=d670460b4b4aece5915caf5c68d12f560a9fe3e4\nrequest_id=55f10e07-ec6c\nrequest_ip=1.2.3.4\nrequest_path=/users\nresponse_status=500\nerror_id=a323-da321\nparameters={ id: 1 }\nuser_id=123\ntiming_total_ms=223\nThe second service produces anemic messages in a difficult to parse format:\nProcessed /users in 223ms with response 500\nYou can see that even in this simple example of log message format, consistency and \nstandardization would make it easier to adequately diagnose issues and trace requests \nacross multiple services. It’s crucial to agree on reasonable standards at all layers of \nyour microservice system to manage divergence and sprawl.\n2.6.2\t\nIsolation\nIn chapter 1, we mentioned Conway’s Law. In an organization that works with micro­\nservices, the inverse of this law is likely to be true: the structure of the company is deter-\nmined by the architecture of its product.\nThis suggests that development teams will increasingly reflect microservices: they’ll \nbe highly specialized to do one thing well. Each team will own and be accountable for \nseveral closely related microservices. Taken collectively, the developers will know every-\nthing there is to know about a system, but individually they’ll have a narrow area of \nspecialization. As SimpleBank’s customer base and product complexity grow, this spe-\ncialization will deepen.\nThis configuration can be immensely challenging. Microservices have limited \nvalue by themselves and don’t function in isolation. Therefore, these independent \nteams must collaborate closely to build an application that runs seamlessly, even \nthough their goals as a team likely relate to their own narrower area of ownership. \nLikewise, a narrow focus may tempt a team to optimize for their local problems and \npreferences, rather than the needs of the whole organization. At its worst, this could \nlead to conflict between teams, in turn leading to slower deployment and a less reli-\nable product.\n \n\n\n\t\n47\nSummary\n2.7\t\nWhat’s next?\nIn this chapter, we established that microservices were a good fit for SimpleBank, \ndesigned a new feature, and considered how you might make that feature production \nready. We hope this case study has shown that a microservice-driven approach to appli-\ncation development is both compelling and challenging!\nThroughout the rest of this book, we’ll teach you the techniques and tools you need \nto know to run a great microservice application. Although microservices can lead to \nboth flexible and highly productive development, running multiple distributed ser-\nvices is much more demanding than running a single application. To avoid instability, \nyou need be able to design and deploy services that are production ready: transparent, \nfault-tolerant, reliable, and scalable.\nIn part 2, we’ll focus on design. Effectively designing a system of distributed, inter-\ndependent services requires careful consideration of your system domain and how \nthose services interact. Being able to identify the right boundaries between responsibil-\nities — and therefore build highly cohesive and loosely coupled services — is one of the \nmost valuable skills for any microservice practitioner.\nSummary\n¡ Microservices are highly applicable in systems with multiple dimensions of com-\nplexity — for example, breadth of product offering, global deployment, and reg-\nulatory pressures.\n¡ It’s crucial to understand the product domain when designing microservices.\n¡ Service interactions may be orchestrated or choreographed. The latter adds \ncomplexity but can lead to a more loosely coupled system.\n¡ API gateways are a common pattern for abstracting away the complexity of a \nmicroservice architecture for front-end or external consumers.\n¡ You can say a service is production ready if you can trust it to serve production \nworkloads.\n¡ You can be more confident in a service if you can reliably deploy and monitor it.\n¡ Service monitoring should include log aggregation and service-level health \nchecks.\n¡ Microservices can fail because of problems with hardware, communication, and \ndependencies, not just defects in code.\n¡ Collecting business metrics, logs, and interservice traces is vital to understanding \nthe present and past operational behavior of a microservice application.\n¡ Technical divergence and isolation will become increasingly challenging for \nan engineering organization as the number of microservices (and supporting \nteams) increases.\n¡ Avoiding divergence and isolation requires standards and best practices to be \nsimilar across multiple teams, regardless of technical underpinnings.\n \n\n\nPart 2\nDesign\nIn this part of the book, we’ll explore the design of microservice applications. \nWe’ll start with a big-picture view — the architecture of an entire application — and \nthen drill down to explore how to scope services and connect them together. You’ll \nlearn how to design services that are reliable and a microservice framework that’s \nreusable.\n \n\n\n51\n3\nArchitecture of \na microservice application\nThis chapter covers\n¡ The big picture view of a microservice \napplication\n¡ The four tiers of microservice architecture: \nplatform, service, boundary, and client\n¡ Patterns for service communication\n¡ Designing API gateways and consumer-driven \nfaçades as application boundaries\nIn chapter 2, we designed a new feature for SimpleBank as a set of microservices and \ndiscovered that deep understanding of the application domain is one of the keys to \na successful implementation. In this chapter, we’ll look at the bigger picture and \nconsider the design and architecture of an entire application made up of microser-\nvices. We can’t give you a deep understanding of the domain your own application \nlives in, but we can show you how having such an understanding will help you build \na system that’s flexible enough to grow and evolve over time.\nYou’ll see how a microservice application is typically designed to have four \ntiers — platform, service, boundary, and client — and you’ll learn what they are and \nhow they combine to deliver customer-facing applications. We’ll also highlight the \nrole of an event backbone in building a large-scale microservice application and \n \n\n\n52\nChapter 3  Architecture of a microservice application \ndiscuss different patterns for building application boundaries, such as API gateways. \nLastly, we’ll touch on recent trends in building user interfaces for microservice applica-\ntions, such as micro-frontends and frontend composition.\n3.1\t\nArchitecture as a whole\nAs a software designer, you want to build software that’s amenable to change. Many \nforces put pressure on your software: new requirements, defects, market demands, new \ncustomers, growth, and so on. Ideally, you can respond to these pressures at a steady \npace and with confidence. For you to be able to do that, your development approach \nshould reduce friction and minimize risk.\nYour engineering organization will want to remove any roadblocks to development \nas time goes by and the system evolves. You want to be able to quickly and seamlessly \nreplace any system’s component that becomes obsolete. You want to have teams in place \nthat can become completely autonomous and responsible for portions of a larger sys-\ntem. And you want those teams to coexist without the need for constant synchroniza-\ntion and without blocking other teams. For that, you need to think about architecture: \nyour plan for building an application.\n3.1.1\t\nFrom monolith to microservices\nWith a monolithic application, your primary deliverable is a single application. That \napplication is split horizontally into different technical layers — in a typical three-tier \napplication, they’d be data, logic, and presentation (figure 3.1) — and vertically into dif-\nferent business domains. Patterns like MVC and frameworks like Rails and Django \nreflect the three-tier model. Each tier provides services to the tier above: the data tier \nprovides persistent state; the logic tier executes useful work; and the presentation layer \npresents the results back to the end user.\nAn individual microservice is similar to a monolith: it stores data, performs some busi-\nness logic, and returns data and outcomes to consumers through APIs. Each microser-\nvice owns a business or technical capability of the application and interacts with other \nmicroservices to execute work. Figure 3.2 illustrates the high-level architecture of an \nindividual service.\nNOTE    Chapter 4 discusses microservice scoping — how to define the boundar-\nies and responsibilities of a microservice — in detail.\nIn a monolithic application, your architecture is limited to the boundaries of the appli-\ncation itself. In a microservice application, you’re planning for something that’ll keep \nevolving both in size and breadth. Think of it like a city: building a monolith is like \nbuilding a skyscraper; whereas building a microservice application is like building a \nneighborhood: you need to build infrastructure (plumbing, roads, cables) and plan \nfor growth (zone for small businesses versus houses).\nThis analogy highlights the importance of considering not only the components them-\nselves, but also the way they connect, where they’re placed, and how you can build them \nconcurrently. You want your plan to encourage growth along good lines, rather than dic-\ntate or enforce a certain structure on your overall application.\n \n\n\n\t\n53\nArchitecture as a whole\nClients\nPresentation\nApplication\nLogic\nData\nData store\nFigure 3.1    The architecture of a typical three-tier monolithic application\nAPI\nUpstream services\nDownstream services\nThis service owns data\nand manages storage.\nThe presentation layer\nexposes API resources for\nother services.\nThe logic layer performs\nwork, potentially\ninteracting with other\nservices.\nRequest\nResponse\nRequests\nLogic\nData\nData store\nFigure 3.2    The high-level architecture of an individual microservice\n \n",
      "page_number": 64
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 72-86)",
      "start_page": 72,
      "end_page": 86,
      "detection_method": "topic_boundary",
      "content": "54\nChapter 3  Architecture of a microservice application \nMostly importantly, you don’t run microservices in isolation; each microservice lives \nin an environment that enables you to build, deploy, and run it, in concert with \nother microservices. Your application architecture should encompass that whole \nenvironment.\n3.1.2\t\nThe role of an architect\nWhere do software architects fit in? Many enterprises employ software architects, \nalthough the effectiveness of and the approach to this role varies wildly.\nMicroservice applications enable rapid change: they evolve over time as teams build \nnew services, decommission existing services, refactor existing functionality, and so on. \nAs an architect or technical lead, your job is to enable evolution, rather than dictate \ndesign. If the microservice application is a city, then you’re a planner for the city council.\nAn architect’s role is to make sure the technical foundations of the application sup-\nport a fast pace and fluidity. An architect should have a global perspective and make \nsure the global needs of the application are met, guiding its evolution so that\n¡ The application is aligned to the wider strategic goals of the organization.\n¡ Teams share a common set of technical values and expectations.\n¡ Cross-cutting concerns — such as observability, deployment, and interservice \ncommunication — meet the needs of multiple teams.\n¡ The whole application is flexible and malleable in the face of change.\nTo achieve these things, an architect should guide development in two ways:\n¡ Principles  — Guidelines that the team should follow to achieve higher level tech-\nnical or organizational goals\n¡ Conceptual models  — High-level models of system relationships and application- \nlevel patterns\n3.1.3\t\nArchitectural principles\nPrinciples are guidelines (or sometimes rules) that teams should follow to achieve \nhigher level goals. They inform team practice. Figure 3.3 illustrates this model. For \nexample, if your product goal is to sell to privacy- and security-sensitive enterprises, you \nmight set the following principles:\n¡ Development practices must comply with recognized external standards (for \nexample, ISO 27001).\n¡ All data must be portable and stored with retention limits in mind.\n¡ Personal information must be clearly tracked and traceable through the application.\nPrinciples are flexible. They can and should change to reflect the priorities of the busi-\nness and the technical evolution of your application. For example, early development \nmight prioritize validating product-market fit, whereas a more mature application \nmight require a focus on performance and scalability.\n \n\n\n\t\n55\nArchitecture as a whole\nCompany and product goals\nTechnical principles\nTeam practices and decisions\nInform\nAchieve\nFigure 3.3    An architectural approach based on technical principles\n3.1.4\t\nThe four tiers of a microservice application\nArchitecture should reflect a clear high-level conceptual model. A model is a useful \ntool for reasoning about an application’s technical structure. A multi-tiered model, \nlike the three-tier model outlined in figure 3.1, is a common approach to applica-\ntion structure, reflecting layers of abstraction and responsibility within an overall \nsystem.\nIn the rest of this chapter, we’ll explore a four-tier model for a microservice \napplication:\n¡ Platform  — A microservice platform provides tooling, infrastructure, and high-\nlevel primitives to support the rapid development, operation, and deployment \nof microservices. A mature platform layer enables engineers to focus on building \nfeatures, not plumbing.\n¡ Services  — In this tier, the services that you build interact with each other to pro-\nvide business and technical capabilities, supported by the underlying platform.\n¡ Boundary  — Clients will interact with your application through a defined bound-\nary that exposes underlying functionality to meet the needs of outside consumers.\n¡ Client  — Client applications, such as websites and mobile applications, interact \nwith your microservice backend.\nFigure 3.4 illustrates these architectural layers. You should be able to apply them to any \nmicroservice application, regardless of underlying technology choices.\n \n\n\n56\nChapter 3  Architecture of a microservice application \nClient user interfaces, devices, third parties\nBoundary entry point, aggregation\nServices business logic, orchestration, communication\nPlatform infrastructure, deployment, communication\nAccesses\nExposes\nRun on\nSupports\nFigure 3.4    A four-tiered model of microservice application architecture\nEach layer is built on the capabilities of the layers below; for example, individual ser-\nvices take advantage of deployment pipelines, infrastructure, and communication \nmechanisms that the underlying microservice platform provides. A well-designed \nmicroservice application requires sophistication and investment at all layers.\nGreat! So now you have a model you can work with. In the next five sections, we’ll \nwalk through each layer in this architectural model and discuss how it contributes to \nbuilding sustainable, flexible, and evolutionary microservice applications.\n3.2\t\nA microservice platform\nMicroservices don’t live in isolation. A microservice is supported by infrastructure:\n¡ A deployment target where services are run, including infrastructure primitives, \nsuch as load balancers and virtual machines\n¡ Logging and monitoring aggregation to observe service operation\n¡ Consistent and repeatable deployment pipelines to test and release new services \nand versions\n¡ Support for secure operation, such as network controls, secret management, and \napplication hardening\n¡ Communication channels and service discovery to support service interaction\nFigure 3.5 illustrates these capabilities and how they relate to the service layer of the \napplication. If each microservice is a house, then the platform provides roads, water, \nelectricity, and telephone cables.\n \n\n\n\t\n57\nA microservice platform\nObservability\nObservability\nService A\nService B\nCommunication /\nservice discovery\nRuntime platform\nSecurity\nDeployment\npipeline\nDeployment\npipeline\nLogs, metrics, alerts\nTests, packaging,\nrollout, rollback\nVirtual machines, load\nbalancers, and so on\nNetwork controls,\nsecrets, hardening\nFigure 3.5    The capabilities of a microservice platform\nA robust platform layer decreases overall implementation cost, increases overall sta-\nbility, and enables rapid service development. Without this platform, product devel-\nopers would need to repeatedly write plumbing code themselves, taking energy away \nfrom delivering new features and business impact. The average developer shouldn’t \nneed to be an expert in the intricacies of every layer of the application. Ultimately, a \nsemi-independent, specialist team can develop the platform layer to meet the needs \nof multiple teams working in the service layer of the application.\n3.2.1\t\nMapping your runtime platform\nA microservice platform will help you be confident that you can trust the services your \nteam writes to serve production workloads and be resilient, transparent, and scalable. \nFigure 3.6 maps out a runtime platform for a microservice.\nA runtime platform (or deployment target) — for example, a cloud environment like \nAWS or a platform as a service (PaaS) like Heroku — provides infrastructure primitives \nnecessary to run multiple service instances and route requests between them. In addition, \nit provides mechanisms for providing configuration — secrets and environment-specific \nvariables — to service instances.\nYou build the other elements of a microservice platform on top of this foundation. \nObservability tools collect and correlate data from services and underlying infrastruc-\nture. Deployment pipelines manage the upgrade (or rollback) of this stack.\n \n\n\n58\nChapter 3  Architecture of a microservice application \nA service may require external\nconfiguration, such as secrets\nand environment variables.\nMicroservices run on\nhosts, usually virtual.\nServices may require\nother resources, such\nas disk volumes.\nConsumes\nMicroservice\ninstance\nMicroservice\ninstance\nMachine\nMachine\nRuns on\nRuns on\nUses\nResources\nRequests need to be\nrouted to services.\nEach microservice runs\nas multiple instances for\nscale and redundancy.\nA deployment pipeline\ncontrols rolling updates\nand rollback of this stack.\nLB\nService group\nUses\nConfiguration\nFigure 3.6    A deployment configuration for a microservice running in a typical cloud environment\n3.3\t\nServices\nThe service layer has perhaps the most self-explanatory name — this is where your ser-\nvices live. At this tier, services interact to perform useful work, relying on the under-\nlying platform abstractions for reliable operation and communication and exposing \ntheir work through the boundary layer to application clients. We also consider compo-\nnents that are logically internal to a service, such as data stores, to be part of this tier.\nThe structure of your service tier will differ widely depending on the nature of your \nbusiness. In this section, we’ll discuss some of the common patterns you’ll encounter:\n¡ Business and technical capabilities\n¡ Aggregation and higher order services\n¡ Services on critical and noncritical paths\n3.3.1\t\nCapabilities\nThe services you write will implement different capabilities:\n¡ A business capability is something that an organization does to generate value and \nmeet business goals. Microservices that you scope to business capabilities directly \nreflect business goals.\n \n\n\n\t\n59\nServices\nBusiness capability:\nmanage order placement\nand execution\nTechnical capability:\ninteract with market third\nparties\nImplemented by\nImplemented by\nA technical capability provides a\nshared technical service to support\nbusiness capabilities.\nA business capability is something\nan organization does to create\nvalue or meet goals.\nSupports\nSettlement\nMarket\ndata\nOrders\nMarket\nSupports\nSupports\nFigure 3.7    Microservices implementing business or technical capabilities\n¡ A technical capability supports other services by implementing a shared technical \nfeature.\nFigure 3.7 compares these two types of capability. SimpleBank’s orders service exposes \na capability for managing order execution — this is a business capability. The market \nservice is a technical capability; it provides a gateway to a third party that other services \n(such as exposing market information or settling trades) can reuse.\nNOTE    We’ll explore when to use business and technical capabilities and how \nyou map them to individual services in the next chapter.\n3.3.2\t\nAggregation and higher order services\nIn the early days of a microservice application, your services are likely to be flat; each \nservice is likely to have a similar level of responsibility. For example, the services in \nchapter 2 — orders, fees, transactions, and accounts — are scoped at a roughly equiva-\nlent level of abstraction.\nAs the application grows, you’ll encounter two pressures on the growth of services:\n¡ Aggregating data from multiple services to serve client requests for denormal-\nized data (for example, returning orders and fees together)\n¡ Providing specialized business logic that takes advantage of underlying capabili-\nties (for example, placing a specific type of order)\nOver time, these two pressures will lead to a hierarchy of services. Services that are closer \nto the system boundary will interact with several services to aggregate their output —  \nlet’s call those aggregators (figure 3.8). In addition, specialized services may act as coordi-\nnators for the work of multiple lower order services.\n \n\n\n60\nChapter 3  Architecture of a microservice application \nMarket\ndata\nTransactions\nQuery\nQuery\nHoldings\nRequest\nRequest\nOrders\nCommand\nTransactions\nMarket\nAccounts\nCommand\nCommand\nAggregator\nCoordinator\nFigure 3.8    An aggregator serves queries by joining data from underlying services, and a coordinator \norchestrates behavior by issuing commands to downstream services.\nThe challenge you’ll face is to determine when new data requirements or new appli-\ncation behavior requires a new service, rather than changes to an existing service. Cre-\nating a new service increases overall complexity and may result in tight coupling, but \nadding functionality to an existing service may make it less cohesive and more difficult \nto replace. That would bend a fundamental microservice principle.\n3.3.3\t\nCritical and noncritical paths\nAs your system evolves, some functions will naturally become more critical to your \ncustomer needs — and the successful operation of your business — than others. For \nexample, at SimpleBank, the orders service is on the critical path for order placement. \nWithout this service operating correctly, you can’t execute customer orders. Con-\nversely, other services are less important; if the customer profile service is unavailable, \nit’s less likely to affect a critical, revenue-generating component of your offering. Fig-\nure 3.9 illustrates example paths at SimpleBank.\nThis is a double-edged sword. The more services on a critical path, the more likely \nfailure will occur. Because no service is 100% reliable, the cumulative reliability of a ser-\nvice is the product of the reliability of its dependencies.\nBut microservices allow you to clearly identify these paths and treat them inde-\npendently, investing more engineering effort to maximize the resiliency and scalability \nof these paths than you invest in less crucial system areas.\n3.4\t\nCommunication\nCommunication is a fundamental element of a microservice application. Microser-\nvices communicate with each other to perform useful work. Your chosen methods for \nmicroservices to instruct and request action from other microservices determine the \nshape of the application you build.\n \n\n\n\t\n61\nCommunication\nPlace an order\nImplemented\nby\nImplemented\nby\nImplemented\nby\nOrders\nHoldings\nCustomers\nFees\nTransactions\nMarket data\nMarket\nThe reliability of downstream services\nwill impact upstream services.\nServices will participate\nin multiple paths.\nRetrieve holdings\nShow customer\nprofile\nFigure 3.9    Chains of services serve capabilities. Many services will participate in multiple paths.\nTIP    Network communication is also a primary source of unreliability in a \nmicroservice application. In chapter 6, we’ll explore techniques for maximiz-\ning the reliability of service-to-service communication.\nCommunication isn’t an independent architectural layer, but we’ve pulled this out into \na separate section because it blurs the boundary between the service and platform lay-\ners. Some elements — such as communication brokers — are part of the platform layer. \nBut services themselves are responsible for constructing and sending messages. You \nwant to build smart endpoints but dumb pipes.\nIn this section, we’ll discuss common patterns for microservice communication \nand how they impact the flexibility and evolution of a microservice application. Most \nmature microservice applications will mix both synchronous and asynchronous inter-\naction styles.\n3.4.1\t\nWhen to use synchronous messages\nSynchronous messages are often the first design approach that comes to mind. They’re \nwell-suited to scenarios where an action’s results — or acknowledgement of success or \nfailure — are required before proceeding with another action.\nFigure 3.10 illustrates a request–response pattern for synchronous messages. The \nfirst service constructs an appropriate message to a collaborator, which the application \nsends using a transport mechanism, such as HTTP. The destination service receives this \nmessage and responds accordingly.\n \n\n\n62\nChapter 3  Architecture of a microservice application \nConstruct\nmessage\nTransform\nresponse\nReceive\nresponse\nSend\nrequest\nReceive\nrequest\nTransform\nrequest\nConstruct\nresponse\nBusiness\nlogic\nBusiness\nlogic\nSend\nresponse\nTransport\nTransport\nService B\nService A\nFigure 3.10    A synchronous request–response lifecycle between two communicating services\nChoosing a transport\nThe choice of transport — RESTful HTTP, an RPC library, or something else — will \nimpact the design of your services. Each transport has different properties of latency, \nlanguage support, and strictness. For example, gRPC provides generated client/server \nAPI contracts using Protobufs, whereas HTTP is agnostic to the context of messages. \nAcross your application, using a single method of synchronous transport has econo-\nmies of scale; it’s easier to reason through, monitor, and support with tooling.\nSeparation of concerns within microservices is also important. You should separate \nyour choice of transport mechanism from the business logic of your service, which \nshouldn’t need to know about HTTP status codes or gRPC response streams. Doing \nso makes it easier to swap out different mechanisms in the future if your application’s \nneeds evolve.\nDrawbacks\nSynchronous messages have limitations:\n¡ They create tighter coupling between services, as services must be aware of their \ncollaborators.\n¡ They don’t have a strong model for broadcast or publish-subscribe models, limit-\ning your capability to perform parallel work.\n¡ They block code execution while waiting on responses. In a thread- or process- \nbased server model, this can exhaust capacity and trigger cascading failures.\n¡ Overuse of synchronous messages can build deep dependency chains, which \nincreases the overall fragility of a call path.\n3.4.2\t\nWhen to use asynchronous messages\nAn asynchronous style of messaging is more flexible. By announcing events, you make \nit easy to extend the system to handle new requirements, because services no longer \nneed to have knowledge of their downstream consumers. New services can consume \nexisting events without changing existing services.\n \n\n\n\t\n63\nCommunication\nTIP    Events represent post-hoc state changes. OrderCreated, OrderPlaced, \nand OrderCanceled are examples of events that the SimpleBank orders service \nmight emit.\nThis style enables more fluid evolution and creates looser coupling between services. \nThis does come at a cost: asynchronous interactions are more difficult to reason \nthrough, because overall system behavior is no longer explicitly encoded into linear \nsequences. System behavior will become increasingly emergent — developing unpredict-\nably from interactions between services — requiring investment in monitoring to ade-\nquately trace what’s happening.\nNOTE    Events enable different styles of persistence and querying, such as event \nsourcing and command query responsibility segregation (CQRS). These aren’t \na prerequisite for microservices but have some synergies with a microservice \napproach. We’ll explore them in chapter 5.\nAsynchronous messaging typically requires a communication broker, an independent sys-\ntem component that receives events and distributes them to event consumers. This is \nsometimes called an event backbone, which indicates how central to your application this \ncomponent becomes (figure 3.11). Tools commonly used as brokers include Kafka, \nRabbitMQ, and Redis. The semantics of these tools differ: Kafka specializes in high-vol-\nume, replayable event storage, whereas RabbitMQ provides higher level messaging \nmiddleware (based on the AMQP protocol (https://www.amqp.org/)).\n3.4.3\t\nAsynchronous communication patterns\nLet’s look at the two most common event-based patterns: job queue and publish-subscribe. \nYou’ll encounter these patterns a lot when architecting microservices — most higher level \ninteraction patterns are built on one of these two primitives.\nJob queue\nIn this pattern, workers take jobs from a queue and execute them (figure 3.12). A job \nshould only be processed once, regardless of how many worker instances you operate. \nThis pattern is also known as winner takes all.\nService\nPublishes\nevents\nEvent producers are unaware\nof event consumers.\nEvents indicate something “interesting”\nhas happened, for example, state change.\nEvent\nEvent\nEvent\nEvent\nEvent broker\nListen for\nevents\nListen for\nevents\nService C\nService B\nConsumers are unaware of\nwhich service emits events.\nFigure 3.11    Event-driven asynchronous communication between services\n \n\n\n64\nChapter 3  Architecture of a microservice application \nOrder\nMarket\ngateway 1\nService instances\nshare work.\nMarket\ngateway 2\nPick\nPick\nOrder Created\n4\n3\n2\nEach job is processed once.\n1\n1\n3\n2\n4\nFigure 3.12    A job queue distributes work to 1 to n consumers\nYour market gateway could operate in this fashion. Each order that the orders service \ncreates will trigger an OrderCreated event, which will be queued for the market gate-\nway service to place it. This pattern is useful where\n¡ A 1:1 relationship exists between an event and work to be done in response to \nthat event.\n¡ The work that needs to be done is complex or time-consuming, so it should be \ndone out-of-band from the triggering event.\nBy default, this approach doesn’t require sophisticated event delivery. Many task queue \nlibraries are available that use commodity data stores, such as Redis (Resque, Celery, \nSidekiq) or SQL databases.\nPublish-subscribe\nIn publish-subscribe, services trigger events for arbitrary listeners. All listeners that \nreceive the event act on it appropriately. In some ways, this is the ideal microservice \npattern: a service can send arbitrary events out into the world without caring who acts \non them (figure 3.13).\nMarket\ngateway\nPublish \nOrder Placed\nEvent\nSubscribe\nSubscribe\nSubscribe\nStatistics\nService N\nYou can add arbitrary\nlisteners at any point.\nNotify\nEvent\nEvent broker\nFigure 3.13    How publish-subscribe sends events out to subscribers\n \n\n\n\t\n65\nCommunication\nFor example, imagine you need to trigger other downstream actions once an order has \nbeen placed. You might send a push notification to the customer or use it to feed your \norder statistics and recommendation feature. These features can all listen for the same \nevent.\n3.4.4\t\nLocating other services\nTo wrap up this section, let’s take a moment to examine service discovery. For services to \ncommunicate, they need to be able to discover each other. The platform layer should \noffer this capability.\nA rudimentary approach to service discovery is to use load balancers (figure 3.14). \nFor example, an elastic load balancer (ELB) on AWS is assigned a DNS name and man-\nages health checking of underlying nodes, based on their membership in a group of \nvirtual machines (an auto-scaling group on AWS).\nThis works but doesn’t handle more complex scenarios. What if you want to route \ntraffic to different versions of your code to enable canary deployments or dark launches, \nor if you want to route traffic across different data centers?\nA more sophisticated approach is to use a registry, such as Consul (https://www.consul.io). \nService instances announce themselves to a registry, which provides an API — either through \nDNS or a custom mechanism for resolving requests for those services. Figure 3.15 illustrates \nthis approach.\nYour service discovery needs will depend on the complexity of your deployed applica-\ntion’s topology. More complex deployments, such as geographical distribution, require \nmore robust service discovery architecture.1\nNOTE    When you deploy to Kubernetes in chapter 9, you’ll learn about services, \nthe mechanism that Kubernetes uses to provide discovery.\nOrders\nForwards request\nto service\ninstances\nOn creation, notifies name server\nName server\nReturns IP\n192.8.1.2\norders.simple-bank\n.internal\nRequests 192.8.1.2\nService A\nLooks up DNS for\norders.simple-bank.internal\nLB\nOrders\nFigure 3.14    Service discovery using load balancers and known DNS names\n1\t bit.ly/2o86ShQ is a great place to start if you’re interested in further exploring different types of \nproxies and load balancing.\n \n\n\n66\nChapter 3  Architecture of a microservice application \nService A\n2. Lookup for\norders\n3. Return IP/port\nfor orders\n4. Request to orders\nOrders\n1. Register\nRegistry\nFigure 3.15    Service discovery using a service registry as a source of truth\n3.5\t\nThe application boundary\nA boundary layer provides a façade over the complex interactions of your internal ser-\nvices. Clients, such as mobile apps, web-based user interfaces, or IoT devices, may inter-\nact with a microservice application. (You might build these clients yourself, or third \nparties consuming a public API to your application may build them.) For example, \nSimpleBank has internal admin tools, an investment website, iOS and Android apps, \nand a public API, as depicted in figure 3.16.\nThe boundary layer provides an abstraction over internal complexity and change (fig-\nure 3.17). For example, you might provide a consistent interface for a client to list all \nhistoric orders, but, over time, you might completely refactor the internal implementa-\ntion of that functionality. Without this layer, clients would require too much knowledge \nof individual services, becoming tightly coupled to your system implementation.\nMicroservice\napplication\nAdmin UI\nPublic API\nInvestment\nwebsite\niOS app\nAndroid app\nFigure 3.16    Client applications at SimpleBank\n \n\n\n\t\n67\nThe application boundary\nHTTP\nHTTP\nThe boundary encapsulates\nbackend complexity.\ngRPC\ngRPC\nBoundary\nResponses are in\nclient-appropriate formats.\nService\nClient\nFigure 3.17    A boundary provides a façade over the service layer to hide internal complexity from a \nconsumer.\nSecond, the boundary tier provides access to data and functionality using a transport \nand content type appropriate to the consumer. For example, whereas services might \ncommunicate between each other with gRPC, a façade can expose an HTTP API to \nexternal consumers, which is much more appropriate for external applications to \nconsume.\nCombining these roles allows your application to become a black box, performing \nwhatever (unknown to the client) operations to deliver functionality. You also can make \nchanges to the service layer with more confidence, because the client interfaces with it \nthrough a single point.\nThe boundary layer also may implement other client-facing capabilities:\n¡ Authentication and authorization  — To verify the identity and claims of an API client\n¡ Rate limiting  — To provide defense against client abuse\n¡ Caching  — To reduce overall load on the backend\n¡ Collect logs and metrics  — To allow analysis and monitoring of client requests\nPlacing these edge capabilities in the boundary layer provides clear separation of \nconcerns — without a boundary, backend services would need to individually \nimplement these concerns, increasing their complexity.\nYou might also use boundaries within your service tier to separate domains. For exam-\nple, an order placement process might consist of several services, but only one of those \nservices should expose an entry point that other domains can access (figure 3.18).\nNOTE    Internal service boundaries often reflect bounded contexts: cohesive, \nbounded subsets of the overall application domain. We’ll explore them more \nin the next chapter.\nThat provides an overview for how you can use boundaries. Let’s get more specific and \nexplore three different (albeit related) patterns for application boundaries: API gate-\nways, backends for frontends, and consumer-driven gateways.\n \n\n\n68\nChapter 3  Architecture of a microservice application \nOrder management\nDifferent subsystems might\nexpose their own internal\nfaçade within a microservice\napplication.\nBoundary\nFees\nFigure 3.18    Boundaries might be present between different contexts within a microservice application.\n3.5.1\t\nAPI gateways\nWe introduced the API gateway pattern in chapter 2. An API gateway provides a single \nclient-entry point over a service-oriented backend. It proxies requests to underlying \nservices and transforms their responses. An API gateway might handle other cross-cut-\nting client concerns, such as authentication and request signing.\nTIP    API gateways that are available include such open source options as \nMashape’s Kong, as well as commercial offerings, such as AWS API Gateway.\nFigure 3.19 illustrates an API gateway. The gateway authenticates a request, and if that \nsucceeds, it proxies the request to an appropriate backend service. It transforms the \nresults it receives so that when it returns them, they’re palatable for your consuming \nclients.\nAPI gateway\ngRPC\ngRPC\nThe gateway authenticates, routes,\nand transforms a client request.\nService\nAuthentication\nRouting\nTransformation\nTransformation\nHTTP\nHTTP\nClient\nFigure 3.19    An API gateway serving a client request\n \n",
      "page_number": 72
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 87-96)",
      "start_page": 87,
      "end_page": 96,
      "detection_method": "topic_boundary",
      "content": "\t\n69\nThe application boundary\nA gateway also allows you to minimize the exposed area of your system from a security \nperspective by deploying internal services in a private network and restricting ingress \nto all but the gateway.\nWARNING    Sometimes an API gateway might perform API composition: com-\nposing responses from multiple services into a single response. The line \nbetween this and service layer aggregation is fuzzy. It’s best to be cautious and \ntry to avoid business logic bleeding into the gateway itself, which can overly \nincrease coupling between the gateway and underlying services.\n3.5.2\t\nBackends for frontends\nThe backends for frontends (BFF) pattern is a variation on the API gateway approach. \nAlthough the API gateway approach is elegant, it has a few downsides. If the API gate-\nway acts as a composition point for multiple applications, it’ll begin to take on more \nresponsibility.\nFor example, imagine you serve both desktop and mobile applications. Mobile \ndevices have different needs, displaying less data with less available bandwidth, and dif-\nferent user features, such as location and context awareness. In practice, this means \ndesktop and mobile API needs diverge, which increases the breadth of functionality \nyou need to integrate into a gateway. Different needs, such as the amount of data (and \ntherefore payload size) returned for a given resource, may also conflict. It can be hard \nto balance these competing forces while building a cohesive and optimized API.\nIn a BFF approach, you use an API gateway for each consuming client type. To take \nthe earlier example from SimpleBank, each user service they offered would have a \nunique gateway (figure 3.20).\nAdmin UI\niOS app\nAndroid app\nAdmin gateway\nWebsite gateway\niOS gateway\nAndroid gateway\nEach client has a\nspecialized API\nbackend.\nInvestment\nwebsite\nMicroservice\napplication\nFigure 3.20    The backends for frontends pattern for SimpleBank’s client applications\n \n\n\n70\nChapter 3  Architecture of a microservice application \nDoing so allows the gateway to be highly specific and responsive to the needs of its \nconsumer without bloat or conflict. This results in smaller, simpler gateways and more \nfocused development.\n3.5.3\t\nConsumer-driven gateways\nIn both previous patterns, the API gateway determines the structure of the data it \nreturns to your consumer. To serve different clients, you might build unique backends. \nLet’s flip this around. What if you could build a gateway that allowed consumers to \nexpress exactly what data they needed from your service? Think of this like an evolu-\ntion of the BFF approach: rather than building multiple APIs, you can build a single \n“super-set” API that allows consumers to define the shape of response they require.\nYou can achieve this using GraphQL. GraphQL is a query language for APIs that \nallows consumers to specify which data fields they want and to multiplex different \nresources into a single request. For example, you might expose the following schema \nfor SimpleBank clients.\nListing 3.1    Basic GraphQL schema for SimpleBank\ntype Account { \n  id: ID! \n  name: String!\n  currentHoldings: [Holding]! \n  orders: [Order]!\n}\ntype Order {\n  id: ID!\n  status: String!\n  asset: Asset!\n  quantity: Float!\n}\ntype Holding {\n  asset: Asset!\n  quantity: Float!\n}\ntype Asset {\n  id: ID!\n  name: String!\n  type: String!\n  price: Float!\n}\ntype Root {\n  accounts: [Account]! \n  account(id: ID): Account \n}\nschema: {\n  query: Root \n}\nThe ! indicates the field is non-nullable.\nAn account contains lists of Holding and Order.\nReturns all accounts or an account by its ID\nThe schema has a single entry point for queries.\n \n\n\n\t\n71\nClients\nThis schema exposes a customer’s accounts, as well as orders and holdings against each \nof those accounts. Clients then execute queries against this schema. If a mobile app \nscreen shows holdings and outstanding orders for an account, you could retrieve that \ndata in a single request, as shown in the following listing.\nListing 3.2    Request body using GraphQL\n{\n  account(id: \"101\") { \n    orders \n    currentHoldings \n  }\n}\nIn the backend, your GraphQL server would act like an API gateway, proxying and \ncomposing that data from multiple backend services (in this case, orders and hold-\nings). We won’t drill into GraphQL in further detail in this book, but if you’re inter-\nested, the official documentation (http://graphql.org/) is a great place to start. We’ve \nalso had some success using Apollo (https://www.apollographql.com/) to provide a \nGraphQL API façade over RESTful backend services.\n3.6\t\nClients\nThe client tier, like the presentation layer in the three-tier architecture, presents to \nyour users an interface to your application. Separating this layer from those below it \nallows you to develop user interfaces in a granular fashion and to serve the needs of \ndifferent types of clients. This also means you can develop the frontend independently \nfrom backend features. As mentioned in the previous section, your application may \nneed to serve many different clients — mobile devices, websites, both internal and \nexternal — each with different technology choices and constraints.\nIt’s unusual for a single microservice to serve its own user interface. Typically, the \nfunctionality exposed to a given set of users is broader than the capabilities of a single \nservice. For example, administrative staff at SimpleBank might deal with order manage-\nment, account setup, reconciliation, tax, and so on. And this comes with cross-cutting \nconcerns — authentication, audit logging, user management — that are clearly not the \nresponsibility of an orders or account setup service.\n3.6.1\t\nFrontend monoliths\nYour backend is straightforward to split into independently deployable and maintain-\nable services — well, relatively, you still have another 10 chapters to go. But this can be \nchallenging to achieve on the frontend. A typical frontend over a microservice applica-\ntion might still be a monolith that’s deployed and changed as a single unit (figure 3.21). \nSpecialist frontends, particularly mobile applications, often demand dedicated teams, \nmaking end-to-end feature ownership difficult to practically achieve.\nNOTE    We’ll talk more about end-to-end ownership (and why it’s desirable and \nbeneficial when developing microservice applications) in chapter 13.\nFilters by account ID\nRequests specific member fields in response\n \n\n\n72\nChapter 3  Architecture of a microservice application \n...regardless of\nthe backend\ndecomposition.\nAPI gateway\nBackend services\nFrontend\nOrders\nFees\nAccounts\nCustomers\nTax\nAll functionality gets\nadded to the same\nfrontend application...\nFigure 3.21    A typical frontend client in a microservice application can become monolithic.\n3.6.2\t\nMicro-frontends\nAs frontend applications grow larger, they begin to encounter the same coordination \nand friction issues that plague large-scale backend development. It’d be great if you \ncould split frontend development in the same way you can split your backend services. \nAn emerging trend in web applications is micro-frontends — serving fragments of a \nUI as independently packaged and deployable components that you can compose \ntogether. Figure 3.22 illustrates this approach.\nThis would allow each microservice team to deliver functionality end to end. For \nexample, if you had an orders team, it could independently deliver both order manage-\nment microservices and the web interface required to place and manage orders.\nShared/generic\ncomponents, such as\nnavigation\nIndependent\ncomponents\nComponents may\ncommunicate with\neach other.\nFigure 3.22    A user interface composed from independent fragments\n \n\n\n\t\n73\nSummary\nAlthough promising, this approach has many challenges:\n¡ Visual and interaction consistency across different components requires \nnontrivial effort to build and maintain common components and design \nprinciples.\n¡ Bundle size (and therefore load time) can be difficult to manage when loading \nJavaScript code from multiple sources.\n¡ Interface reloads and redraws can cause overall performance to suffer.\nMicro-frontends aren’t yet commonplace, but people are using several different tech-\nnical approaches in the wild, including\n¡ Serving UI fragments as web components with a clear, event-driven API\n¡ Integrating fragments using client-side includes\n¡ Using iframes to serve micro-apps into separate screen sections\n¡ Integrating components at the cache layer using edge side includes (ESI)\nIf you’re interested in learning more, Micro Frontends (https://micro-frontends.org/) \nand Zalando’s Project Mosaic (https://www.mosaic9.org/) are great starting points.\nSummary\n¡ Individually, microservices are similar internally to monolithic applications.\n¡ A microservice application is like a neighborhood: its final shape isn’t prescribed \nbut instead guided by principles and a high-level conceptual model.\n¡ The principles that guide microservice architecture reflect organizational goals \nand inform team practices.\n¡ Your architectural plan should encourage growth along good lines, rather than \ndictate approaches for your overall application.\n¡ A microservice application consists of four layers: platform, service, boundary, \nand client.\n¡ The platform layer provides tooling, plumbing, and infrastructure to support the \ndevelopment of product-oriented microservices.\n¡ Synchronous communication is often the first choice in a microservice applica-\ntion and is best suited to command-type interactions, but it has drawbacks and \ncan increase coupling and fragility.\n¡ Asynchronous communication is more flexible and amenable to rapid system \nevolution, at the cost of added complexity.\n¡ Common asynchronous communication patterns include queues and \npublish-subscribe.\n¡ The boundary layer provides a façade over your microservice application that’s \nappropriate for external consumers.\n \n\n\n74\nChapter 3  Architecture of a microservice application \n¡ Common types of boundaries include API gateways and consumer-driven gate-\nways, such as GraphQL.\n¡ Client applications, such as websites and mobile applications, interact with your \nmobile backend through the boundary layer.\n¡ Clients risk becoming monolithic, but techniques are beginning to emerge for \napplying microservice principles to frontend applications.\n \n\n\n75\n4\nDesigning new features\nThis chapter covers\n¡ Scoping microservices based on business \ncapabilities and use cases\n¡ When to scope microservices to reflect \ntechnical capabilities\n¡ Making design choices when service \nboundaries are unclear\n¡ Scoping effectively when multiple teams own \nmicroservices\nDesigning a new feature in a microservice application requires careful and well-reasoned \nscoping of microservices. You need to decide when to build new services or extend \nexisting services, where boundaries lie between those services, and how those services \nshould collaborate.\nWell-designed services have three key characteristics: they’re responsible for a \nsingle capability, independently deployable, and replaceable. If your microservices \nhave the wrong boundaries, or are too small, they can become tightly coupled, mak-\ning them challenging to deploy independently or replace. Tight coupling increases \nthe impact, and therefore the risk, of change. If your services are too large — taking \n \n\n\n76\nChapter 4  Designing new features\non too much responsibility — they become less cohesive, increasing friction in ongoing \ndevelopment.\n Even if you get it right the first time, you need to keep in mind that the requirements \nand needs of most complex software applications will evolve over time, and approaches \nthat worked early in that application’s lifetime may not always remain suitable. No \ndesign is perfect forever. \nYou’ll face additional challenges in longer running applications (and larger engi-\nneering organizations). Your services may rely on a web of dependencies managed by \nmultiple teams — as an engineer in one team, you’ll need to design cohesive function-\nality while relying on services that won’t necessarily be under your control. And you’ll \nneed to know when to retire and migrate away from services that no longer meet the \nneeds of the wider system.\nIn this chapter, we’ll walk you through designing a new feature using microservices. \nWe’ll use that example to explore techniques and practices that you can use to guide \nthe design of maintainable microservices in both new and longer running microservice \napplications.\n4.1\t\nA new feature for SimpleBank\nRemember SimpleBank? The team is doing well — customers love their product! But \nSimpleBank has discovered that most of those customers don’t want to pick their own \ninvestments — they’d much rather have SimpleBank do the hard work for them. Let’s \ntake this problem and work out how to solve it with a microservice application. In the \nnext few sections, we’ll develop the design in four stages:\n1\t Understanding the business problem, use cases, and potential solution\n2\t Identifying the different entities and business capabilities your services should \nsupport\n3\t Scoping services that are responsible for those capabilities\n4\t Validating your design against current and potential future requirements\nThis will build on the small collection of services we explored in chapters 2 and 3: \norders, market gateway, account transactions, fees, market data, and holdings.\nFirst, let’s understand the business problem you’re trying to solve. In the real \nworld, you could carry out the discovery and analysis of business problems using sev-\neral techniques, such as market research, customer interviews, or impact mapping. \nAs well as understanding the problem, you’d need to decide whether it was one your \ncompany should solve. Luckily, this isn’t a book about product management — you \ncan skip that part.\nNOTE    We haven’t tried to extrapolate a general approach to understanding \nbusiness problems — that’s another book altogether. \nUltimately, SimpleBank’s customers want to invest money, either up front or on a reg-\nular basis, and see their wealth increase, either over a defined period or to meet a \n \n\n\n\t\n77\nA new feature for SimpleBank\nspecific goal, such as a deposit on a house. Currently, SimpleBank’s customers need to \nchoose how their money is invested — even if they don’t have a clue about investing. \nAn uninformed investor might choose an asset based on high predicted returns, with-\nout realizing that higher returns typically mean significantly higher risk. \nTo solve this problem, SimpleBank could make investment decisions on the custom-\ner’s behalf by allowing the customer to choose a premade investment strategy. An invest-\nment strategy consists of proportions of different asset types — bonds, shares, funds, \nand so on — designed for a certain level of risk and investment timeline. When a cus-\ntomer adds money to their account, SimpleBank will automatically invest that money in \nline with this strategy. This setup is summarized in figure 4.1.\nSimpleBank\nCustomer\nGenerate\nstrategy orders\nInvest money\nSelect strategy\nCreate account\n<<Include>>\n<<Include>>\n<<Include>>\nDefine\navailable\nstrategies\nInvestment strategies\nSelect assets\nFigure 4.1    Potential use cases to support defining and selecting investment strategies \n \n\n\n78\nChapter 4  Designing new features\nBased on figure 4.1, you can start to identify the use cases you need to satisfy to solve \nthis problem:\n¡ SimpleBank must be able to create and update available strategies.\n¡ A customer must be able to create an account and elect an appropriate invest-\nment strategy.\n¡ A customer must be able to invest money using a strategy, and investing in a strat-\negy generates appropriate orders.\nOver the next few sections, we’ll explore these use cases. When identifying use cases in \nyour own domain, you may prefer to use a more structured and exhaustive approach, \nsuch as behavior-driven development (BDD) scenarios. What’s important is that you \nstart to establish a concrete understanding of the problem, which you then can use to \nvalidate an acceptable solution.\n4.2\t\nScoping by business capabilities\nAfter you’ve identified your business requirements, your next step is to identify the \ntechnical solution: which features you need to build and how you’ll support them \nwith existing and new microservices. Choosing the right scope and purpose for each \nmicroservice is essential to building a successful and maintainable microservice \napplication.\nThis process is called service scoping. It’s also known as decomposition or partition-\ning. Breaking apart an application into services is challenging — as much art as science. \nIn the following sections, we’ll explore three strategies for scoping services:\n¡ By business capability or bounded context  — Services should correspond to relatively \ncoarse-grained, but cohesive, areas of business functionality.\n¡ By use case  — Services should be verbs that reflect actions that will occur in a \nsystem.\n¡ By volatility  — Services should encapsulate areas where change is likely to occur \nin the future.\nYou don't necessarily use these approaches in isolation; in many microservice appli-\ncations, you’ll combine scoping strategies to design services appropriate to different \nscenarios and requirements.\n4.2.1\t\nCapabilities and domain modeling\nA business capability is something that an organization does to generate value and \nmeet business goals. Microservices that are scoped to business capabilities directly \nreflect business goals. In commercial software development, these goals are usually the \nprimary drivers of change within a system; therefore, it’s natural to structure the system \nto encapsulate those areas of change. You’ve seen several business capabilities imple-\nmented in services so far: order management, transaction ledgers, charging fees, and \nplacing orders to market (figure 4.2).\n \n",
      "page_number": 87
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 97-104)",
      "start_page": 97,
      "end_page": 104,
      "detection_method": "topic_boundary",
      "content": "\t\n79\nScoping by business capabilities\nFunctions\nCapabilities and\nservices\n Place order to market\nCharge order fees to\naccounts\nRecord transactions\noccurring on account\nManage order status\nFees service\nMarket service\nOrders service\nAccount\ntransactions\nservice\nTransactions\nOrder management\nFees\nFigure 4.2    Functions that existing microservices provide and their relationship to business capabilities \nperformed by SimpleBank\nBusiness capabilities are closely related to a domain-driven design approach. Domain-\ndriven design (DDD) was popularized by Eric Evans’ book of the same name and focuses \non building systems that reflect a shared, evolving view, or model, of a real-world domain.1 \nOne of the most useful concepts that Evans introduced was the notion of a bounded con-\ntext. Any given solution within a domain might consist of multiple bounded contexts; the \nmodels inside each context are highly cohesive and have the same view of the real world. \nEach context has a strong and explicit boundary between it and other contexts.\nBounded contexts are cohesive units with a clear scope and an explicit external bound-\nary. This makes them a natural starting point for scoping services. Each context demar-\ncates the boundaries between different areas of your solution. This often has a close \ncorrespondence with organizational boundaries; for example, an e-commerce company \nwill have different needs — and different teams — for shipping versus customer payments.\nTo begin with, a context typically maps directly to a service and an area of business \ncapability. As the business grows and becomes more complex, you may end up breaking \na context down into multiple subcapabilities, many of which you’ll implement as inde-\npendent, collaborating services. From the perspective of a client, though, the context \nmay still appear as a single logical service.\nTIP    The API gateway pattern we discussed in chapter 3 can be useful for estab-\nlishing boundaries between different contexts (and underlying groups of ser-\nvices) within your application.\n4.2.2\t\nCreating investment strategies \nYou can design services to support creating investment strategies using a business capa-\nbility approach. You might want to get a sketch pad to work through this one. To help \nyou work through this example and give the use case more shape, we’ve wireframed \nwhat the UI for this feature might look like in figure 4.3.\n1\t Although many of the implementation patterns — repositories, aggregates, and factories — are quite \nspecific to object-oriented programming, many of Evans’ analysis techniques — such as ubiquitous \nlanguage — are useful in any programming paradigm.\n \n\n\n80\nChapter 4  Designing new features\nFigure 4.3    A user interface for an admin user to create new investment strategies\nTo design services by business capability, it’s best to start with a domain model: some \ndescription of the functions your business performs in your bounded context(s) and \nthe entities that are involved. From figure 4.3, you’ve probably identified these already. \nA simple investment strategy has two components: a name and a set of assets, each with \na percentage allocation. An administrative staff member at SimpleBank will create a \nstrategy. We’ve drafted those entities in figure 4.4.\nThe design of these entities helps you understand the data your services own and \npersist. Only three entities, and it already looks like you’ve identified (at least) two new \nservices: user management and asset information. The user and asset entities are both \npart of distinct bounded contexts:\n¡ User management  — This covers features like sign-up, authentication, and \nauthorization. In a banking environment, authorization for different \nresources and functionality is subject to strict controls for security, regulatory, \nand privacy reasons.\n \n\n\n\t\n81\nScoping by business capabilities\nCreates\nContains\nAllocated to\n0..*\n0..*\n1\n1\n1\n0..*\n+ Id:integer\n+ Code:string\n+ Sector:string\n+ Id:integer\n+ Name:string\n+ Email:string\n+ Asset:Asset\n+ Percentage:float\n+ Id:integer\n+ Name:string\n+ Allocations:List<AssetAllocation>\nInvestment Strategy entity\nAsset Allocation\nAsset information service\nUser management service\nUser entity\nAsset entity\nInvestment strategies service\nFigure 4.4    The first draft of a domain model made up of entities to support the creation of investment \nstrategies\n¡ Asset information  — This covers integration with third-party providers of market \ndata, such as asset prices, categories, classification, and financial performance. \nThis capability would include asset search, as required by your user interface \n(figure 4.3). \nInterestingly, these different domains reflect the organization of SimpleBank itself. \nA dedicated operational team manages asset data; likewise, user management. This \ncomparability is desirable, as it means your services will reflect real-world lines of cross-\nteam communication.\nMore on that later — let’s get back to investment strategies. You know that \nyou can associate them with customer accounts and use them to generate orders. \nAccounts and orders are both distinct bounded contexts, but investment strategies \ndon’t belong in either one. When strategies change, the change is unlikely to affect \naccounts or orders themselves. Conversely, adding investment strategies to either of \nthose existing services will hamper their replaceability, making them less cohesive \nand less amenable to change.\nThese factors indicate that investment strategies are a distinct business capability, \nrequiring a new service. Figure 4.5 illustrates the relationships between this context and \nyour existing capabilities.\n \n\n\n82\nChapter 4  Designing new features\nOrders\nHave\nAssets\nResult in\nInvestment\nstrategies\nUser management\nManage\nAdmin users\nInvest using\nConsist of\nBuy/sell\nHold positions in\nAccounts\nCustomers\nAsset information\nFigure 4.5    Relationships between your new business capability and other bounded contexts within the \nSimpleBank application\nYou can see that some contexts are aware of information that belongs to other con-\ntexts. Some entities within your context are shared: they’re conceptually the same but \ncarry unique associations or behavior within different contexts. For example, you use \nassets in multiple ways:\n¡ The strategy context records the allocation of assets to different strategies.\n¡ The orders context manages the purchase and sale of assets.\n¡ The asset context stores fundamental asset information for use by multiple con-\ntexts, such as pricing and categorization.\nThe model we’ve drawn out in figure 4.5 doesn’t tell you much about the behavior of \na service; it only tells you the business scope your services cover. Now that you have a \nfirmer idea of where your service boundaries lie, you can draft out the contract that \nyour service offers to other services or end users.\nNOTE    Don’t worry too much about what technology you’ll use for communica-\ntion at this stage. The examples in this chapter could easily apply to any point-\nto-point messaging approach.\nFirst, your investment strategies service needs to expose methods for creating and \nretrieving investment strategies. Other services or your UI can then access this data. \nLet’s draft out an endpoint that allows creating an investment strategy. The example \nshown in listing 4.1 uses the OpenAPI specification (formerly known as Swagger), \nwhich is a popular technique for designing and documenting REST API interfaces. If \nyou’re interested in learning more, the Github page for the OpenAPI specification2 is \na good place to start.\n2\t See https://github.com/OAI/OpenAPI-Specification.\n \n\n\n\t\n83\nScoping by business capabilities\nListing 4.1    API for the investment strategies service\nopenapi: \"3.0.0\"\ninfo: \n  title: Investment Strategies \nservers: \n  - url: https://investment-strategies.simplebank.internal \npaths:\n  /strategies: \n    post: \n      summary: Create an investment strategy\n      operationId: createInvestmentStrategy\n      requestBody: \n        description: New strategy to create\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/NewInvestmentStrategy' \n      responses:\n        '201':\n          description: Created strategy\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/InvestmentStrategy' \ncomponents: \n  schemas:\n    NewInvestmentStrategy: \n      required:\n        - name\n        - assets\n      properties:\n        name:\n          type: string\n        assets: \n          type: array \n          items: \n            $ref: '#/components/schemas/AssetAllocation' \n    AssetAllocation:\n      required:\n        - assetId\n        - percentage\n      properties:\n        assetId:\n          type: string\n        percentage:\n          type: number\n          format: float\n    InvestmentStrategy:\n      allOf:\nStarts with some metadata about your API\nDefines a “POST /strategies” path\nThe body of this request should be \nthe new investment strategy.\nRefers to a location elsewhere in the \ndocument: the components key\nDefines the response type \nin the components section\nDefines reusable data types\nA new investment strategy type\nContains a list of assets \nof type AssetAllocation\n \n\n\n84\nChapter 4  Designing new features\n        - $ref: '#/components/schemas/NewInvestmentStrategy' \n        - required:\n          - id\n          - createdByUserId\n          - createdAt\n          properties:\n            id:\n              type: integer\n              format: int64\n            createdByUserId:\n              type: integer\n              format: int64\n            createdAt:\n              type: string\n              format: date-time\nIf you’re going to use strategies again later — and you are — you’ll need to retrieve them. \nImmediately under your paths: element in listing 4.1, add the code in the following listing.\nListing 4.2    API for retrieving strategies from the investment strategies service\n  /strategies/{id}: \n    get:\n      description: Returns an investment strategy by ID\n      operationId: findInvestmentStrategy\n      parameters: \n        - name: id \n          in: path \n          description: ID of strategy to fetch \n          required: true \n          schema: \n            type: integer \n            format: int64 \n      responses:\n        '200':\n          description: investment strategy\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/InvestmentStrategy' \nYou also should consider what events this service should emit. An event-based model \naids in decoupling services from each other, ensuring that you can choreograph long-\nterm interactions, rather than explicitly orchestrate them.\nWARNING    Anticipating future use cases of a service is one of the most difficult ele-\nments of service design. But building flexible APIs and integration points between \nservices reduces the need for future rework and coordination between teams.\nThe InvestmentStrategy type extends the \nNewInvestmentStrategy and adds fields, \nbased on your entity model.\nThe path for retrieving an investment strategy\nDefines the format of the ID\nReturns an investment strategy\n \n\n\n\t\n85\nScoping by business capabilities\nEndpoints\nEvents\nInvestment\nstrategies\nStrategy Created\nCreate Strategy\nGet Strategy\nFigure 4.6    The inbound and outbound contract of your investment strategies microservice\nFor example, imagine that creating a strategy will trigger email notifications to poten-\ntially interested customers. This is separate from the scope of the investment strate-\ngies service itself; it has no knowledge of customers (or their preferences). This is an \nideal use case for events. If a POST to /strategies post-hoc triggers an event — let’s \ncall it StrategyCreated — then arbitrary microservices can listen for that event and act \nappropriately. Figure 4.6 illustrates the full scope of your service’s API.\nGreat work — you’ve identified all the capabilities that you require to support this \nuse case. To see how this fits together, you can map the investment strategies service and \nthe other capabilities you’ve identified to the wireframe (figure 4.7).\nLet’s summarize what you’ve done so far:\n1\t For a sample problem, you’ve identified functions the business performs to gen-\nerate value and the natural seams between different areas of SimpleBank’s busi-\nness domain.\n2\t You’ve used that knowledge to identify boundaries within your microservice appli-\ncation, identifying entities and responsibility for different capabilities.\n3\t You’ve scoped your system into services that reflect those domain boundaries.\nCreate strategy\nGet strategy\nInvestment Strategies\nUser management\nWe've scoped the boundaries\nof this service.\nWe’ve identified capabilities\nthat need to be implemented\nin services.\nSearch for assets\nAsset information\nGet asset\nGet user\nFigure 4.7    Identified capabilities and services mapped to how they’d support functionality in the create \ninvestment strategy user interface\n \n\n\n86\nChapter 4  Designing new features\nThis approach results in services that are relatively stable, cohesive, oriented to busi-\nness value, and loosely coupled.\n4.2.3\t\nNested contexts and services\nEach bounded context provides an API to other contexts, while encapsulating internal \noperation. Let’s take asset information as an example (figure 4.8):\n¡ It exposes methods that other contexts can use, such as searching for and retriev-\ning assets.\n¡ Third-party integrations or specialist teams within SimpleBank populate asset \ndata.\nThe private/public divide provides a useful mechanism for service evolution. Early in a \nsystem’s lifecycle, you might choose to build coarser services, representing a high-level \nboundary. Over time, you might decompose services further, exposing behavior from \nnested contexts. Doing this maintains replaceability and high cohesion, even as busi-\nness logic increases in complexity.\nOther contexts\nInteract with external interface\nPublic\nPrivate\nGet asset\nSearch for asset\nPricing\nClassification\nInvestment research\nNested contexts are\npotential service partitions.\nAsset information\nFigure 4.8    A context exposes an external interface and may itself contain nested contexts\n \n",
      "page_number": 97
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 105-112)",
      "start_page": 105,
      "end_page": 112,
      "detection_method": "topic_boundary",
      "content": "\t\n87\nScoping by use case\n4.2.4\t\nChallenges and limitations\nIn the previous sections, you identified the natural seams within the organization’s \nbusiness domain and applied them to partition your services. This approach is effec-\ntive because it maps services to the functional structure of a business — directly reflect-\ning the domain in which an organization operates. But it’s not perfect.\nRequires substantial business knowledge \nPartitioning by business capabilities requires having significant understanding of the \nbusiness or problem domain. This can be difficult. If you don’t have enough infor-\nmation — or you’ve made the wrong assumptions — you can’t be completely certain \nyou’re making the right design decisions. Understanding the needs of any business \nproblem is a complex, time-consuming and iterative process.\nThis problem isn’t unique to microservices, but misunderstanding the business \nscope — and reflecting it incorrectly in your services — can incur higher refactoring \ncosts in this architecture, because both data and behavior can require time-consuming \nmigration between services. \nCoarse-grained services keep growing\nSimilarly, a business capability approach is biased toward the initial development of \ncoarse-grained services that cover a large business boundary — for example, orders, \naccounts, or assets. New requirements increase the breadth and depth of that area, \nincreasing the scope of the service’s responsibility. These new reasons to change can \nviolate the single responsibility principle. It’ll be necessary to partition that service fur-\nther to maintain an acceptable level of cohesion and replaceability.\nWARNING    Service teams sometimes add functionality to existing microservices \nbecause it’s easy — a deployable unit already exists — rather than investing \nmore time to create a new service or repartition the existing service appropri-\nately. Although teams sometimes need to make pragmatic decisions, they need \nto exercise discipline to minimize this source of technical debt.\n4.3\t\nScoping by use case\nSo far, your services have been nouns, oriented around objects and things that exist \nwithin the business domain. An alternative approach to scoping is to identify verbs, or \nuse cases within your application, and build services to match those responsibilities. \nFor example, an e-commerce site might implement a complex sign-up flow as a micro-\nservice that interacts with other services, such as user profile, welcome notifications, \nand special offers.\nThis approach can be useful when\n¡ A capability doesn’t clearly belong in one domain or interacts with multiple \ndomains.\n¡ The use case being implemented is complex, and placing it in another service \nwould violate single responsibility.\n \n\n\n88\nChapter 4  Designing new features\nLet’s apply this approach to SimpleBank to understand how it differs from noun-ori-\nented decomposition. Get your pencil and paper ready!\n4.3.1\t\nPlacing investment strategy orders \nA customer can invest money into an investment strategy. This will generate appropri-\nate orders; for example, if the customer invests $1,000, and the strategy specifies 20% \nshould be invested in Stock ABC, an order will be generated to purchase $200 of ABC.\nThis raises several questions:\n1\t How does SimpleBank accept money for investment? Let’s assume a customer \ncan make an investment by external payment (for example, a credit card or bank \ntransfer).\n2\t Which service is responsible for generating orders against a strategy? How does \nthis relate to your existing orders and investment strategies services?\n3\t How do you keep track of orders made against strategies?\nYou could build this capability into your existing investment strategies service. \nBut placing orders might unnecessarily widen the scope of responsibility that \nthe service encapsulates. Likewise, the capability doesn’t make sense to add to the \norders service. Coupling all possible sources of orders to that service would give it \ntoo many reasons to change.\nYou can draft out an independent service for this use case as a starting point — call it \nPlaceStrategyOrders. Figure 4.9 sketches out how you’d expect this service to behave. \nConsider the input to this service. For orders to be placed, this service needs three \nthings: the account placing them, the strategy to use, and the amount to invest. You can \nformalize that input, as shown in the following listing.\nOrders\nPlace Strategy\nOrders\nInput\nAmount invested\nDestination\naccount\nInvestment\nstrategy\nResults in\nRequest\nFigure 4.9    Expected behavior of a proposed PlaceStrategyOrders service\n \n\n\n\t\n89\nScoping by use case\nListing 4.3    Draft input for PlaceStrategyOrders\npaths:\n  /strategies/{id}/orders: \n    post:\n      summary: Place strategy orders\n      operationId: PlaceStrategyOrders\n      parameters:\n        - name: id\n          in: path\n          description: ID of strategy to order against\n          required: true\n          schema:\n            type: integer\n            format: int64\n      requestBody:\n        description: Details of order\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/StrategyOrder'\ncomponents:\n  schemas:\n    StrategyOrder:\n      required:\n        - destinationAccountId \n        - amount \n      properties:\n        destinationAccountId:\n          type: integer\n          format: int64\n        amount:\n          type: number\n          format: decimal\nThis is elegant but a little too simple. If you assume your payment is coming from an \nexternal source, you can’t execute orders until those funds are available. It doesn’t \nmake sense for PlaceStrategyOrders to handle receipt of funds — this is clearly a dis-\ntinct business capability. Instead, you can link placing strategy orders to a payment, as \nfollows.\nListing 4.4    Using payment ID for PlaceStrategyOrders\ncomponents:\n  schemas:\n    StrategyOrder:\n      required:\n        - destinationAccountId\n        - amount\n        - paymentId \n      properties:\n        destinationAccountId:\nExecuting an order is a subresource  \nof an investment strategy.\nAn order requires a destination account \nand an investment amount.\nYour new required field: paymentId\n \n\n\n90\nChapter 4  Designing new features\n          type: integer\n          format: int64\n        amount:\n          type: number\n          format: decimal\n\t\n  paymentId:\n          type: integer\n          format: int64\nThis anticipates the existence of a new service capability: payments. This capability \nshould support\n¡ Initiating payments by users\n¡ Processing those payments by interacting with third-party payment systems\n¡ Updating account positions at SimpleBank\nBecause you know that payments aren’t instantaneous, you’d expect this service to trig-\nger asynchronous events that other services can listen for, such as PaymentCompleted. \nFigure 4.10 illustrates this payments capability.\nFrom the perspective of PlaceStrategyOrders, it doesn’t matter how you implement \nthe payments capability, as long as something implements the interface the consumer \nexpects. It might be a single service — Payments — or a collection of action-oriented \nservices, for example, CompleteBankTransfer.\nYou can summarize what you’ve designed so far in a sequence diagram (figure 4.11).\nThere’s one missing element in this diagram: getting these orders to market. As men-\ntioned, although this service generates orders, this capability clearly doesn’t belong \nwithin your existing orders service. The orders service exposes behavior that multiple \nconsumers can use, including this new service (figure 4.12); although the source of \norders differs, the process of placing them remains the same.\nIf the service meets this interface,\nconsumers don't need to be aware\nof implementation details.\nThe payments capability should\nimplement a clear interface.\nPayments\nPayment\ncompleted\nCreate payment\nProcess payments\nUpdate positions\nInterface\nEvents\nFigure 4.10    The interface that your proposed payments capability expects\n \n\n\n\t\n91\nScoping by use case\nProcesses\npayment\nUI\nPlace Strategy Orders\nCreates payment\nSaves request\nConfirmation\nPayment details\nPayment completed\nGets strategy details\nStrategy details\nGenerates orders\nRequests investment\nPayments\nStrategies\nFigure 4.11    The process of creating a payment and making an investment using the proposed \nPlaceStrategyOrders service\nLastly, you need to persist the link between these orders and the strategy and investment \nthat created them. PlaceStrategyOrders should be responsible for storing any request it \nreceives — it clearly owns this data. Therefore, you should record any order IDs within \nthe strategy order service to preserve that foreign key relationship. You could also record \nthe order source ID — the ID of this investment strategy investment request — within \nthe orders service itself, although it seems less likely you’d query data in that direction.\nThis service manages the\nlifecycle of orders.\nPlace Order\nOther services\nPlace Strategy\nOrders\nOrders can come from\nmultiple sources.\nEnd users\nOrders\nFigure 4.12    Your orders service provides an API that multiple other services within your system  \ncan consume.\n \n\n\n92\nChapter 4  Designing new features\nThe orders service emits OrderCompleted events when an order has been completed. \nYour strategy orders service can listen for these events to reflect that status against the \noverall investment request.\nYou can add the orders service and tie this all together as shown in figure 4.13.\nGreat! You’ve designed another new service. Unlike the previous section, you \ndesigned a service that closely represented a specific complex use case, rather than a \nbroad capability.\nThis resulted in a service that was responsible for a single capability, replaceable, and \nindependently deployable, meeting your desired characteristics for well-scoped micro-\nservices. In contrast, unlike if you’d focused on business capabilities, the tight focus of \nthis service on a single use case limits potential for reuse in other use cases in the future. \nThis inflexibility suggests that fine-grained use case services are best used in tandem \nwith coarser grained services, rather than alone.\n4.3.2\t\nActions and stores\nWe’ve identified an interesting pattern in the above examples: multiple higher level \nmicroservices access a coarse-grained underlying business capability. This is especially \nprevalent in a verb-oriented approach, as the data needs of different actions often \noverlap.\nProcesses\npayment\nUI\nPlace Strategy Orders\nCreates payment\nSaves request\nConfirmation\nPayment details\nOrder details\nPlace orders\nOrder completed\nPayment completed\nGets strategy details\nStrategy details\nGenerates orders\nRecords order IDs\nUpdates request\nstatus\nRequests investment\nPayments\nStrategies\nOrders\nFigure 4.13    The full process of creating an investment strategy order using your new \nPlaceStrategyOrders service\n \n\n\n\t\n93\nScoping by use case\nFor example, imagine you have two actions: update order and cancel order. Both oper-\nations operate against the same underlying order state, so neither can exclusively own \nthat state itself, and you need to reconcile that conflict somewhere. In the previous \nexamples, the orders service took care of the problem. This service is the ultimate \nowner of that subset of your application’s persistent state.\nThis pattern is similar3 to Bob Martin’s clean architecture4 or Alistair Cockburn’s hex-\nagonal architecture. In those models, the core of an application consists of two layers:\n¡ Entities  — Enterprisewide business objects and rules\n¡ Use cases  — Application-specific operations that direct entities to achieve the \ngoals of the use case\nAround those layers, you use interface adapters to connect these business-logic con-\ncerns to application-level implementation concerns, such as particular web frame-\nworks or database libraries. Similarly, at an intraservice level, your use cases (or \nactions) interact with underlying entities (or stores) to generate some useful outcome. \nYou then wrap them in a façade, such as an API gateway, to map from your underlying \nservice-to-service representations to an output friendly to an external consumer (for \nexample, a RESTful API). Figure 4.14 sketches out that arrangement.\nIndependent UI /\napplication delivery\nUI/consumers\nGateway\nService\nService\nService\nService\nService\nAdaptation of\nunderlying formats to\npresentation & output\nUse case-specific\nbusiness logic\nBusiness objects and\nenterprisewide\nbusiness rules\nClean architecture\nMicroservice application\nFrameworks\nAdapters\nUse cases\nEntities\nFigure 4.14    The architecture of a microservice application compared to Bob Martin’s clean architecture\n3\t But not quite the same: Martin’s architecture is concerned with implementation detail indepen-\ndence within object-oriented applications (for example, keeping business logic independent of \nthe data storage solution), which isn’t particularly relevant at the intraservice level.\n4\t For a more detailed explanation of Martin’s clean architecture, see Uncle Bob, “The Clean Archi-\ntecture,” August 13, 2012, http://mng.bz/LJB4.\n \n\n\n94\nChapter 4  Designing new features\nThis architecture is conceptually elegant, but you need to apply it judiciously in a \nmicroservice system. Treating underlying capabilities as, first and foremost, stores of \npersistent state can lead to anemic, “dumb” services. These services fail to be truly \nautonomous because they can’t take any action without being mediated by another, \nhigher level service. This architecture also increases the number of remote calls and \nthe length of the service chain you need to perform any useful action.\nThis approach also risks tight coupling between actions and underlying stores, ham-\npering your ability to deploy services independently. To avoid these pitfalls, we recom-\nmend you design microservices from the inside out, building useful coarse-grained \ncapabilities before building fine-grained action-oriented services.\n4.3.3\t\nOrchestration and choreography\nIn chapter 2, we discussed the difference between orchestration and choreography in \nservice interaction. A bias toward choreography tends to result in more flexible, auton-\nomous, and maintainable services. Figure 4.15 illustrates the difference between these \napproaches.\nIf you scope services by use case, you might find yourself writing services that explic-\nitly orchestrate the behavior of several other services. This isn’t always ideal:\n¡ Orchestration can increase coupling between services and increase the risk of \ndependent deployments.\n¡ Underlying services can become anemic and lack purpose, as the orchestrating \nservice takes on more and more responsibility for useful business output.\nWhen designing services that reflect use cases, it’s important to consider their place \nwithin a broader chain of responsibilities. For example, the PlaceStrategyOrders ser-\nvice you designed earlier both orchestrates behavior (placing orders) and reacts to \nother events (payment processing). Taking a balanced approach to choosing orches-\ntration or choreography reduces the risk of building services that lack autonomy.\n4.4\t\nScoping by volatility\nIn an ideal world, you could build any feature by combining existing microservices. \nThis might sound impractical, but it’s interesting to consider how you maximize the \nreusability — and therefore long-term utility — of the services you build.\nServices react to events,\nchoreographing a process.\nRequest\nService A\nService D\nService B\nService C\nService C\nRequest\nService A\nService D\nService B\nEvent\nEvent\nEvent\nRequest\nRequest\nRequest\nService A calls other\nservices, orchestrating a\nprocess.\nFigure 4.15    Orchestration versus choreography in service interaction\n \n",
      "page_number": 105
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 113-120)",
      "start_page": 113,
      "end_page": 120,
      "detection_method": "topic_boundary",
      "content": "\t\n95\nScoping by volatility\nSo far, we’ve taken a predominantly functional approach to decomposing services. This \napproach is effective but has limitations. Functional decomposition is biased toward \nthe present needs of an application and doesn’t explicitly consider how that applica-\ntion might evolve. A purely functional approach can constrain the future growth of a \nsystem by resulting in services that are inflexible in the face of new or evolving require-\nments, thereby increasing the risk of change.\nTherefore, as well as considering the functionality of your system, you should con-\nsider where that application is likely to change in the future. This is known as volatility. \nBy encapsulating areas that are likely to change, you help to ensure that uncertainty \nin one area doesn’t negatively impact other areas of the application. You can find an \nanalogy to this in the stable dependencies principle in object-oriented programming: \n“a package should only depend on packages that are more stable than it is.”\nSimpleBank’s business domain has multiple axes of volatility. For example, placing \nan order to market is volatile: different orders need to go to different markets; Simple-\nBank might have different APIs to each market (for example, through a broker, direct \nto an exchange); and those markets might change as SimpleBank broadens its offering \nof financial assets. \nTightly coupling market interaction as part of the orders service would lead to a high \ndegree of instability. Instead, you’d split the market service and ultimately build multi-\nple services to meet the needs of each market. Figure 4.16 illustrates this approach.\nMarket service\nFixML API\nCSV over FTP\nCustom API\nMarket state\nOther provider\nOther provider\nThe market service encapsulates potential change in placing orders to market.\nOver time, we might decompose further to encapsulate subareas of volatility.\nFund manufacturer\nUS stock broker\nMarket service\nFixML API\nCSV over FTP\nCustom API\nMarket state\nMarket state\nMarket state\nFund market\nservice\nUS stock market\nservice\nFund manufacturer\nUS stock broker\nFigure 4.16    The market service encapsulates change in how SimpleBank communicates with different \nfinancial market providers. Over time, this might evolve into multiple services.\n \n\n\n96\nChapter 4  Designing new features\nInvestment\nstrategies\n1. Notified when\nnew strategy\nis created\n4. Updates investment allocation\nStrategy\noptimizer\n3. Evaluates strategy\nagainst goals\n2. Regularly queries\nasset performance\nMarket data\nFigure 4.17    Partitioning a distinct area of system volatility — investment strategy optimization — as a \nseparate service\nLet’s take one more example: imagine you have more than one type of investment \nstrategy. Perhaps you have strategies that are optimized by deep learning: the perfor-\nmance of assets on the market should drive adjustments to future strategy allocations.\nAdding this complex behavior to your InvestmentStrategies service would signifi-\ncantly broaden its reasons to change — reducing cohesiveness. Instead, you should add \nnew services with responsibility for that behavior — as you can see in figure 4.17. By \ndoing this, you can develop and release these services independently without unneces-\nsary coupling between different features or rates of change.\nUltimately, good architecture strikes a balance between the current and future needs \nof an application. If microservices are too narrowly scoped, you might find the cost of \nchange becomes higher in the future as you become increasingly constrained by earlier \nassumptions about the limits of your system. On the flipside, you should always be care-\nful to keep YAGNI — “you aren’t gonna need it” — in mind. You may not always have the \nluxury of time (or money) to anticipate and meet every possible future permutation of \nyour application.\n4.5\t\nTechnical capabilities\nThe services you’ve designed so far have reflected actions or entities that map closely \nto your business capabilities, such as placing orders. These business-oriented services \nare the primary type you’ll build in any microservice application.\nYou can also design services that reflect technical capabilities. A technical capability \nindirectly contributes to a business outcome by supporting other microservices. Com-\nmon examples of technical capabilities include integration with third-party systems and \ncross-cutting technical concerns, such as sending notifications.\n4.5.1\t\nSending notifications\nLet’s work through an example. Imagine that SimpleBank would like to notify a \ncustomer — perhaps through email — whenever a payment has been completed. Your \n \n\n\n\t\n97\nTechnical capabilities\nfirst instinct might be to build that code within your payments service (or services). \nBut that approach has three problems:\n1\t The payments service has no awareness of customer contact details or prefer-\nences. You’d need to extend its interface to include customer contact data (push-\ning that obligation on to service consumers) or query another service.\n2\t Other parts of your application might send notifications as well. You can easily \npicture other features — orders, account setup, marketing — that might trigger \nemails.\n3\t Customers might not even want to receive emails: they might prefer SMS or push \nnotifications…or even physical mail.\nThe first and second points suggest that this should be a separate service; the third \npoint suggests you might need multiple services — one to handle each type of notifica-\ntion. Figure 4.18 sketches that out. Your notification services can listen to the Payment-\nCompleted event that your payments service emits.\nYou can configure your group of notification services to listen to any events — from \nany service — that should result in a notification. Each service will need to be aware of \na customer’s contact preferences and details to send notifications. You could store that \ninformation in a separate service, such as a customers service, or have each service own \nit. This area has hidden dimensions of complexity; for example, many customers might \nown the payment’s destination account, triggering multiple notifications.\nYou may have realized that the notification services are also responsible for generat-\ning appropriate message content based on each event, which suggests they could grow \nsignificantly in the future, in line with the potential number of notifications. It eventu-\nally might be necessary to split message content from message delivery to reduce this \ncomplexity.\nPayments\nEmits\nevent\nPayment\nCompleted\nListens\nListens\nNotifies\nNotifies\nNotifies\nCustomer\nListens\nSMS\nnotifications\nEmail\nnotifications\nPush\nnotifications\nFigure 4.18    Supporting technical microservices for notifications\n \n\n\n98\nChapter 4  Designing new features\nThis example illustrates that implementing technical capabilities maximizes reusabil-\nity while simplifying your business services, decoupling them from nontrivial technical \nconcerns.\n4.5.2\t\nWhen to use technical capabilities\nYou should use a technical capability to support and simplify other microservices, limit-\ning the size and complexity of your business capabilities. Partitioning these capabilities \nis desirable when\n¡ Including the capability within a business-oriented service will make that service \nunreasonably complex, complicating any future replacement.\n¡ A technical capability is required by multiple services — for example, sending \nemail notifications.\n¡ A technical capability changes independently of the business capability — for \nexample, a nontrivial third-party integration.\nEncapsulating these capabilities in separate services captures axes of volatility — areas \nthat are likely to change independently — and maximizes service reusability.\nIn certain scenarios, it’s unwise to partition a technical capability. In some situations, \nextracting a capability will reduce the cohesiveness of a service. For example, in classic \nSOA, systems were often decomposed horizontally, in the belief that splitting data stor-\nage from business functionality would maximize reusability. Figure 4.19 illustrates how \nrequests would be serviced in this approach.\nUnfortunately, the intended reusability came at a high cost. Splitting those layers \nof an application led to tight coupling between different deployable units, as deliver-\ning individual features required simultaneous change across multiple applications (fig-\nure 4.20). When you have to coordinate changes to distinct components, this leads to \nerror-prone, lock-step deployments — a distributed monolith.\nIf you focus on business capabilities first, you’ll avoid these pitfalls. But you should \ncarefully scope any technical capability to ensure it’s truly autonomous and indepen-\ndent from other services.\nRequests\nCreate\norder\nCreate\norder\nCreate\norder\nOrder data\naccess\nOrder store\nOrder\nbusiness\nservice\nFigure 4.19    Lifecycle of a create order request in a horizontally partitioned service application \n \n\n\n\t\n99\nDealing with ambiguity\nChange 2\nChange 3\nChange 1\nNew features require coordinated changes across multiple deployable units.\nNew features lead to change in a single\ndeployable unit.\nOrder UI\nOrder\nbusiness\nservice\nOrder data\naccess\nOrder store\nOrder store\nOrders\nCreate\norder\nCreate\norder\nCreate\norder\nCreate\norder\nHorizontal partitioning \nNew feature\nNew feature\nBusiness capability partitioning\nFigure 4.20 The impact of change in a horizontally partitioned service versus a service scoped to \nbusiness capability\n4.6\t\nDealing with ambiguity\nScoping microservices is as much art as science. A large part of software design is find-\ning effective ways to achieve the best solution when faced with ambiguity:\n¡ Your understanding of the problem domain might be incomplete or incorrect. \nUnderstanding the needs of any business problem is a complex, time-consuming, \nand iterative process.\n¡ You need to anticipate how you might need to use a service in the future, rather \nthan only right now. But you’ll often run into tension between short-term feature \nneeds and long-term service malleability.\nSuboptimal service partitioning in microservices can be costly: it adds friction to devel-\nopment and extra effort to refactoring.\nNOTE    Understanding a business domain is hardly unique to microservices — or \neven the engineering process itself. Most modern product engineering meth-\nodologies aim to maintain flexibility and agility when facing an evolving \nunderstanding of requirements. For that reason, we strongly recommend fol-\nlowing an iterative and lean development process when building a microservice \napplication.\n4.6.1\t\nStart with coarse-grained services\nIn this section, we’ll explore a few approaches you can use to make practical service \ndecisions when the right solution isn’t obvious. To start, we’ve talked a lot about the \nimportance of keeping the responsibility of a service focused, cohesive, and limited, so \nwhat I’m about to say might sound a little counterintuitive. Sometimes, when in doubt \nabout service boundaries, it’s better to build larger services.\n \n\n\n100\nChapter 4  Designing new features\nIf you err on the side of building services that are too small, it can lead to tight cou-\npling between different services that should be combined in one service. This indicates \nyou’ve decomposed a business capability too far, making responsibility unclear and \nmaking it more difficult — and costly — to refactor this element of functionality.\nIf instead you combine that functionality into a larger service, you reduce the cost of \nfuture refactoring, as well as avoiding intractable cross-service dependencies. Likewise, \none of the most expensive costs you’ll incur in a microservice application is changing a \npublic interface; reducing the breadth of interfaces between components aids in main-\ntaining flexibility, especially in early stages of development.\nUnderstand that making a service larger also incurs a cost, because larger services \nbecome more resistant to change and difficult to replace. But at the beginning of its \nlife, a service will be small. The costs associated with a service being too large are less \nthan the costs of complexity that decomposing too far introduces. You need to care-\nfully observe both service size and complexity to ensure you’re not building more \nmonoliths.\nHere, it’s useful to apply a key principle of lean software development: decide as late \nas possible. Because building a service incurs cost in both implementation and opera-\ntion, avoiding premature decomposition when faced with uncertainty can give you time \nto develop your understanding of the problem space. It also will ensure you’re making \nwell-informed decisions about the shape of the application as it grows.\n4.6.2\t\nPrepare for further decomposition\nThe modeling and scoping techniques from earlier in this chapter will help you identify \nwhen a service has become too large. Often, you’ll be able to identify possible seams \nquite early in the lifetime of a service. If so, you should endeavor to design your service \ninternals to reflect them, whether through class and namespace design or as a separate \nlibrary. \nMaintaining disciplined internal module boundaries, with a clear public API, is gen-\nerally sound software design. In a microservice, it reduces the cost of future refactoring \nby reducing the chance that code becomes highly coupled and difficult to untangle. \nThat said, be careful — an API that’s well-designed in the context of a code library may \nnot always be ideal as the interface to a microservice. \n4.6.3\t\nRetirement and migration\nWe’ve talked about planning for future decomposition, but we also should talk about \nservice retirement. Microservice development requires a certain degree of ruthless-\nness. It’s important to remember that it’s what your application does that matters, not \nthe code. Over time — and especially if you start with larger services — you’ll find it \nnecessary to either carve out new microservices from existing services or retire micro­\nservices altogether.\n \n\n\n\t\n101\nDealing with ambiguity\nThis process can be difficult. Most importantly, you need to ensure that consum-\ning services don’t get broken and that they migrate in a timely way to any replacement \nservice.\nTo carve out new services, you should apply the expand-migrate-contract pattern. \nImagine you’re carving out a new service from your orders service. When you first built \nthe orders service, you were confident that it’d fit the needs of all order types, so you \nbuilt it as a single service. But one order type has turned out to be different from the \nothers, and supporting it has bloated your original service.\nFirst, you need to expand — pulling the target functionality into a new service (fig-\nure 4.21). Next, you need to migrate consumers of your old service to the new service \n(figure 4.22). If access is through an API gateway, you can redirect appropriate requests \nto your new service.\nBut if other services call the orders service, you need to migrate those usages. Telling \nother teams to migrate doesn’t always work (competing priorities, release cycles, and \nrisk). Instead, you need to either make sure your new service is compelling — make \npeople want to invest effort in migration — or do that migration for them.\nTo complete the process, you have one last step. Finally, you can contract the original \nservice, removing the now obsolete code (figure 4.23).\nPlace order\nAPI gateway\nAPI gateway\nWe expand by pulling that\ncode into a new service.\nPlace order\nPlace order B\nOur service contains\ncode we want to split.\nOrder data\nOrder data\nOrders\nOrders\nOrders B\nOrder data\nFigure 4.21    Expanding functionality from one service into a new service\n \n\n\n102\nChapter 4  Designing new features\nOrders B\nPlace order B\nOrders\nMigrate calls from\nother services.\nMigrate calls at the\ngateway layer.\nAPI gateway\nExisting data may need\nmigration.\nOrder data\nOrder data\nOther services\nFigure 4.22    Migrating existing consumers to the new service\nAPI gateway\nPlace order B\nOrders B\nOrders\nOrder data\nWe contract our original service, removing\nthe functionality that now resides in our new\nservice.\nOrder data\nPlace order\nFigure 4.23    In the final state of your service migration, you’ve contracted your service to remove \nfunctionality that now resides in the new service\n \n",
      "page_number": 113
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 121-129)",
      "start_page": 121,
      "end_page": 129,
      "detection_method": "topic_boundary",
      "content": "\t\n103\nService ownership in organizations\nGreat, you made it! This measured, multistep process systematically retires or migrates \nfunctionality while reducing the risk of breaking existing service consumers.\n4.7\t\nService ownership in organizations\nThe examples so far have mostly assumed that a single team is responsible for building \nand changing microservices. In a large organization, different teams will own different \nmicroservices. This isn’t a bad thing — it’s an important part of scaling as an engineer-\ning team.\nAs we pointed out earlier, bounded contexts themselves are an effective way of \nsplitting application ownership across different teams in an organization. Forming \nteams that own services in specific bounded contexts takes advantage of the inverse \nversion of Conway’s Law: if systems reflect the organizational structure that produces \nthem, you can attain a desirable system architecture by first shaping the structure and \nresponsibilities of your organization. Figure 4.24 illustrates how SimpleBank might \norganize its engineering teams around the services and bounded contexts you’ve \nidentified so far.\nSplitting ownership and delivery of services across teams has three implications:\n¡ Limited control  — You might not have full control over the interface or perfor-\nmance of your service dependencies. For example, payments are vital to placing \ninvestment strategy orders, but the team model in figure 4.24 means that another \nteam is responsible for the behavior of that dependency.\n¡ Design constraints  — The needs of consuming services will constrain your service \ncontracts; you need to ensure service changes don’t leave consumers behind. \nLikewise, the possibilities that other existing services offer will constrain your \npotential designs.\n¡ Multispeed development  — Services that different teams own will evolve and change \nat different rates, depending on that team’s size, efficiency, and priorities. A fea-\nture request from the investment team to the customers team may not make it to \nthe top of the customers team’s priority list.\nCustomers\nCustomers\nAccounts\nStrategies\nPlaceStrategyOrders\nInvestment\nstrategies\nAsset information\nPayments\nOrders\nOrders\nMarket gateway\nInvestment\nMarket &\nresearch\nCash\nResponsible\nteam\nContexts and\nservices\nFigure 4.24    A possible model of service and capability ownership by different engineering teams as the \nsize of SimpleBank’s engineering organization grows\n \n\n\n104\nChapter 4  Designing new features\nThese implications can present an immense challenge, but applying a few tactics can \nhelp:\n¡ Openness  — Ensuring that all engineers can view and change all code reduces \nprotectiveness, helps different teams understand each other’s work, and can \nreduce blockers.\n¡ Explicit interfaces  — Providing explicit, documented interfaces for services reduces \ncommunication overhead and improves overall quality.\n¡ Worry less about DRY  — A microservice approach is biased toward delivery pace, \nrather than efficiency. Although engineers want to practice DRY (don’t repeat \nyourself), you should expect some duplication of work in a microservice approach. \n¡ Clear expectations  — Teams should set clear expectations about the expected per-\nformance, availability, and characteristics of their production services.\nThese sorts of tactics touch on the people side of microservices. This is a substantial \ntopic by itself, which we’ll explore in depth in the final chapter of this book.\nSummary\n¡ You scope services through a process of understanding the business problem, \nidentifying entities and use cases, and partitioning service responsibility.\n¡ You can partition services in several ways: by business capability, use case, or vola-\ntility. And you can combine these approaches.\n¡ Good scoping decisions result in services that meet three key microservice char-\nacteristics: responsible for a single capability, replaceable, and independently \ndeployable.\n¡ Bounded contexts often align with service boundaries and provide a useful way \nof considering future service evolution.\n¡ By considering areas of volatility, you can encapsulate areas that change together \nand increase future amenability to change.\n¡ Poor scoping decisions can become costly to rectify, as the effort involved in \nrefactoring is higher across multiple codebases.\n¡ Services may also encapsulate technical capabilities, which simplify and support \nbusiness capabilities and maximize reusability.\n¡ If service boundaries are ambiguous, you should err on the side of coarse-grained \nservices but use internal modules to actively prepare for future decomposition.\n¡ Retiring services is challenging, but you’ll need to do it as a microservice applica-\ntion evolves.\n¡ Splitting ownership across teams is necessary in larger organizations but causes \nnew problems: limited control, design constraints, and multispeed development.\n¡ Code openness, explicit interfaces, continual communication, and a relaxed \napproach to the DRY principle can alleviate tension between teams.\n \n\n\n105\n5\nTransactions and \nqueries in microservices \nThis chapter covers\n¡ The challenges of consistency in a distributed \napplication\n¡ Synchronous and asynchronous \ncommunication\n¡ Using sagas to develop business logic across \nmultiple services\n¡ API composition and CQRS for microservice \nqueries\nMany monolithic applications rely on transactions to guarantee consistency and \nisolation when changing application state. Obtaining these properties is straight-\nforward: an application typically interacts with a single database, with strong consis-\ntency guarantees, using frameworks that provide support for starting, committing, \nor rolling back transactional operations. Each logical transaction might involve sev-\neral distinct entities; for example, placing an order will update transactions, reserve \nstock positions, and charge fees.\nYou’re not so lucky in a microservice application. As you learned earlier, each inde-\npendent service is responsible for a specific capability. Data ownership is decentralized, \n \n\n\n106\nChapter 5  Transactions and queries in microservices  \nensuring a single owner for each “source of truth.” This level of decoupling helps you gain \nautonomy, but you sacrifice some of the safety you were previously afforded, making con-\nsistency an application-level problem. Decentralized data ownership also makes retrieving \ndata more complex. Queries that previously used database-level joins now require calls to \nmultiple services. This is acceptable for some use cases but painful for large data sets.\nAvailability also impacts your application design. Interactions between services might \nfail, causing business processes to halt, leaving your system in an inconsistent state. \nIn this chapter, you’ll learn how to use sagas to coordinate complex transactions \nacross multiple services and explore best practices for efficiently querying data. Along \nthe way, we’ll examine different types of event-based architectures, such as event sourc-\ning, and their applicability to microservice applications.\n5.1\t\nConsistent transactions in distributed applications\nImagine you’re a customer at SimpleBank and you want to sell some stock. If you recall \nchapter 2, this involves several operations (figure 5.1):\n1\t You create an order.\n2\t The application validates and reserves the stock position.\n3\t The application charges you a fee.\n4\t The application places the order to the market.\nFrom your perspective as a customer, this operation appears to be atomic: charging a \nfee, reserving stock, and creating an order happen at the same time, and you can’t sell \nstock that you don’t have or sell a stock you do have more than once.\nIn many monolithic applications,1 those requirements are easy to meet: you can wrap \nyour database operations in an ACID transaction and rest easy in the knowledge that \nerrors will cause an invalid state to be rolled back.\nUser requests\nsale of stock\nIs stock \navailable?\nNo\nNo\nYes\nYes\nStock is reserved\nagainst account\nFee is charged\nto account\nMarket open?\nWait to place\norder\nPlace order to\nmarket\nFigure 5.1    Placing a sell order\n1\t At least, those with a typical three-tier architecture and a single persistent data store.\n \n\n\n\t\n107\nConsistent transactions in distributed applications\nUser\nOrders\nFees\nreserve stock\nconfirm reservation\ncharge fee\nReserve\nposition\nSave pending\norder\ncreate order\nAccount\nTransactions\nFigure 5.2    Failure occurs when charging a fee in your cross-service order placement process\nBy contrast, in your microservice application, each of the actions in figure 5.1 is per-\nformed by a distinct service responsible for a subset of application state. Decentral-\nized data ownership helps ensure services are independent and loosely coupled, but it \nforces you to build application-level mechanisms to maintain overall data consistency.\nLet’s say an orders service is responsible for coordinating the process of selling a \nstock. It calls account transactions to reserve stock and then the fees service to charge \nthe customer. But that transaction fails. (See figure 5.2.)\nAt this stage, your system is in an inconsistent state: stock is reserved, an order is cre-\nated, but you haven’t charged the customer. You can’t leave it like this — so the implemen-\ntation of orders needs to initiate corrective action, instructing the account transactions \nservice to compensate and remove the stock reservation. This might look simple, but \nit becomes increasingly complex when many services are involved, transactions are \nlong-running, or an action triggers further interleaved downstream transactions.\n5.1.1\t\nWhy can’t you use distributed transactions?\nFaced with this problem, your first impulse might be to design a system that achieves \ntransactional guarantees across multiple services. A common approach is to use the two-\nphase commit (2PC) protocol.2 In this approach, you use a transaction manager to split \noperations across multiple resources into two phases: prepare and commit (figure 5.3).\n2\t See https://en.wikipedia.org/wiki/Two-phase_commit_protocol for more information.\n \n\n\n108\nChapter 5  Transactions and queries in microservices  \nTransaction\nManager\nResource A\nResource B\nTransaction\nManager\nResource A\nResource B\nprepare\nprepare\nsuccess/failure\nsuccess/failure\ncommit\ncommit\nIn the prepare phase, the transaction manager\ninstructs resources to prepare their relevant action.\nIn the commit phase, the transaction manager instructs\nresources to commit or abort the prepared action.\ninitiate\naction\nFigure 5.3    The prepare and commit phases of a 2PC protocol\nThis sounds great — like what you’re used to. Unfortunately, this approach is flawed. \nFirst, 2PC implies synchronicity of communication between the transaction manager \nand resources. If a resource is unavailable, the transaction can’t be committed and \nmust roll back. This in turn increases the volume of retries and decreases the avail-\nability of the overall system. To support asynchronous service interactions, you would \nneed to support 2PC with services and the messaging layer between them, limiting your \ntechnical choices.\nNOTE    In a microservice application, availability is the product of all microser-\nvices involved in processing a given action. Because no service is 100% reliable, \ninvolving more services lessens overall reliability, increasing the probability of \nfailure. We’ll explore this in detail in the next chapter.\nHanding off significant orchestration responsibility to a transaction manager also vio-\nlates one of the core principles of microservices: service autonomy. At worst, you’d end \nup with dumb services representing CRUD operations against data, with transaction \nmanagers wholly encapsulating the interesting behavior of your system.\nFinally, a distributed transaction places a lock on the resources under transaction \nto ensure isolation. This makes it inappropriate for long-running operations, as it \nincreases the risk of contention and deadlock. What should you do instead?\n5.2\t\nEvent-based communication\nEarlier in this book, we discussed using events emitted by services as a communica-\ntion mechanism. Asynchronous events aid in decoupling services from each other and \nincrease overall system availability, but they also encourage service authors to think in \nterms of eventual consistency. In an eventually consistent system, you design complex \noutcomes to result from several independent local transactions over time, which leads \nyou to explicitly design underlying resources to represent tentative states. From the \nperspective of Eric Brewer’s CAP theorem,3 this design approach prioritizes the avail-\nability of underlying data.\n3\t Consistency, availability, partition tolerance—see Eric Brewer, “CAP Twelve Years Later: How the \n“Rules” Have Changed,” InfoQ, May 30, 2012, http://mng.bz/HGA3, for more information.\n \n\n\n\t\n109\nEvent-based communication\nUser\nOrders\ncreate order\nreserve stocks\nconfirm reservation\nconfirm charge\norder placed\nconfirm placement\nplace to market\ncharge fee\nAccount\nTransactions\nFees\nMarket\nFigure 5.4    The synchronous process of placing a sell order \nTo illustrate the difference between a synchronous and an asynchronous approach, \nlet’s return to the sell order example. In a synchronous approach (figure 5.4), the \norders service orchestrates the behavior of other services, invoking a sequence of steps \nuntil the order is placed to the market. If any steps fail, the orders service is responsible \nfor initiating rollback action with other services, such as reversing the charge.\nIn this approach, the orders service takes on substantial responsibility:\n¡ It knows which services it needs to call, as well as their order.\n¡ It needs to know what to do in case any downstream service produces an error or \ncan’t proceed due to business rules.\nAlthough this type of interaction is easy to reason through — as the call graph is log-\nical and sequential — this level of responsibility tightly couples the orders service to \nother services, limiting its independence and increasing the difficulty of making future \nchanges. \n5.2.1\t\nEvents and choreography\nYou can redesign this scenario to use events (figure 5.5). Each service subscribes to \nevents that interest it to know when it must perform some work:\n1\t When the user issues a sell request via the UI, the application publishes an \nOrderRequested event.\n2\t The orders service picks up this event, processes it, and publishes back to the \nevent queue an OrderCreated event. \n \n\n\n110\nChapter 5  Transactions and queries in microservices  \n3\t Both the transaction and fees services then pick up this event. Each one of them \nperforms its work and publishes back events to notify about the completion. \n4\t The market service in turn is waiting for a pair of events notifying it of the \ncharging of fees and the reservation of stocks. When both arrive, it knows it can \nplace the order against the stock exchange. Once that’s finished, the market ser-\nvice publishes a final event back to the queue.\nEvents allow you to take an optimistic approach to availability. For example, if the fees \nservice were down, the orders service would still be able to create orders. When the \nfees service came back online, it could continue processing a backlog of events. You \ncan extend this to rollback: if the fees service fails to charge because of insufficient \nfunds, it could emit a ChargeFailed event, which other services would then consume \nto cancel order placement.\nThis interaction is choreographed: each service reacts to events, acting independently \nwithout knowledge of the overall outcome of the process. These services are like danc-\ners: they know the steps and what to do in each section of a musical piece, and they react \naccordingly without you needing to explicitly invoke or command them. In turn, this \ndesign decouples services from each other, increasing their independence and making \nit easier to deploy changes independently.\nOrder Created\nMarket\nFees\nAccount\nTransactions\nOrders\nStock Reserved\nOrder Created\nemits\nconsumes\nemits\nconsumes\nemits\nconsumes\nemits\nconsumes\nOrder Requested\nFee Charged\nOrder Placed\nFigure 5.5    Services consuming and emitting events for order placement\n \n\n\n\t\n111\nSagas\nEvents and the monolith\nAn event-oriented approach to service communication shines when migrating a mono-\nlithic application to microservices. By emitting events from the monolith, you consume \nthem in microservices that you’re developing in parallel. This way, you can build new fea-\ntures without tightly coupling your monolith to your new services.\nThink about it: you emit an event, and that’s the only change you need to implement on \nthe monolith to make an external system work alongside the current one, lowering risk \nand enabling safer experimentation on new services.\n \n5.3\t\nSagas\nThe choreographed approach is a basic example of the saga pattern. A saga is a coordi-\nnated series of local transactions; a previous step triggers each step in the saga.\nThe concept itself significantly predates the microservice approach. Hector Garcia- \nMolina and Kenneth Salem originally described sagas in a 1987 paper4 as an approach \ntoward long-lived transactions in database systems. As with distributed transactions, lock-\ning in long-lived transactions reduces availability — a saga solves this as a sequence of \ninterleaved, individual transactions. \nAs each local transaction is atomic — but not the saga as a whole — a developer must write \ntheir code to ensure that the system ultimately reaches a consistent state, even if individual \ntransactions fail. Pat Helland’s famous paper, “Life Beyond Distributed Transactions,”5 sug-\ngests that you can think of this as uncertainty — an interaction across multiple services may \nnot have a guaranteed outcome. In a distributed transaction, you manage uncertainty using \nlocks on data; without transactions, you manage uncertainty through semantically appro-\npriate workflows that confirm, cancel, or compensate for actions as they occur.\nBefore we talk about sell orders and services, let’s look at a simple real-world saga: \npurchasing a cup of coffee.6 Typically, this might involve four steps: ordering, payment, \npreparation, and delivery (figure 5.6). In the normal outcome, the customer pays for \nand receives the coffee they ordered.\nOrder coffee\nPay for coffee\nPrepare coffee\nDeliver coffee\nFigure 5.6    The process of purchasing a cup of coffee\n4\t See Hector Garcia-Molina and Kenneth Salem, “Sagas,” http://mng.bz/Qdot, for the original \npaper.\n5\t See Pat Helland, “Life Beyond Distributed Transactions,” acmqueue, December 12, 2016, http://\nqueue.acm.org/detail.cfm?id=3025012.\n6\t Adapted from Gregor Hohpe, “Compensating Action,” Enterprise Integration Patterns, http://mng \n.bz/5FcG.\n \n",
      "page_number": 121
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 130-137)",
      "start_page": 130,
      "end_page": 137,
      "detection_method": "topic_boundary",
      "content": "112\nChapter 5  Transactions and queries in microservices  \nPay for coffee\nOrder\ncoffee\npayment failed\nsuccess\nPrepare coffee\nbad coffee\ncan’t make coffee\nRefund customer\nDeliver coffee\nwrong coffee/wrong person\nMmm,\ncoffee\nNo\ncoffee\nFigure 5.7    Purchasing a cup of coffee with compensating actions\nThis can go wrong! The coffee shop machine might break; the barista might make \na cappuccino, but I wanted a flat white; they might give my coffee to the wrong cus-\ntomer; and so on. If one of these events occurs, the barista will naturally compensate: \nthey might make my coffee again or refund my payment (figure 5.7). In most cases, I’ll \neventually get my coffee.\nYou use compensating actions in sagas to undo previous operations and return \nyour system to a more consistent state. The system isn’t guaranteed to be returned to \nthe original state; the appropriate actions depend on business semantics. This design \napproach makes writing business logic more complex — because you need to consider \na wide range of potential scenarios — but is a great tool for building reliable interac-\ntions between distributed services.\n5.3.1\t\nChoreographed sagas\nLet’s return to the earlier example — sell orders — to better understand how you \ncan apply the saga pattern to your microservices. The actions in this saga are choreo-\ngraphed: each action, TX, is performed in response to another, but without an overall \nconductor or orchestrator. You can break this task into five subtasks:\n¡ T1 — Create the order. \n¡ T2 — Reserve the stock position, which the account transaction service \nimplements.\n¡ T3 — Calculate and charge the fee, which the fees service implements.\n¡ T4 — Place the purchase order to the market, which the market service implements.\n¡ T5 — Update the status of the order to be placed.\nFigure 5.8 illustrates the optimistic — most likely — path of this interaction.\n \n\n\n\t\n113\nSagas\nOrders\nOrder Created\nOrder Placed\nMarket\nAccount\nTransactions\nFees\nFee Charged\nStock Reserved\nT1\nT2\nT5\nT4\nT3\nFigure 5.8    A saga for processing a sell order\nLet’s explain the five steps of this process:\n1\t The orders service performs T1 and emits an OrderCreated event.\n2\t The fees, account transactions, and market services consume this event.\n3\t The fees and account transactions services perform appropriate actions (T2 and \nT3) and emit events, and the market service consumes them.\n4\t When the prerequisites for the order are met, the market service places the order \n(T4) to the market and emits an OrderPlaced event.\n5\t Lastly, the orders service consumes that event and updates the status of the \norder (T5).\nEach of these tasks might fail — in which case, your application should roll back to a \nsane, consistent state. Each of your tasks has a compensating action:\n¡ C1 — Cancel the order that the customer created.\n¡ C2 — Reverse the reservation of stock positions.\n¡ C3 — Revert the fee charge, refunding the customer.\n¡ C4 — Cancel the order placed to market.\n¡ C5 — Reverse the state of the order.\nWhat triggers these actions? You guessed it — events! For example, imagine that plac-\ning the order to market fails. The market service will cancel the order by emitting an \nevent — OrderFailed — that each other service involved in this saga consumes. When \nreceiving the event, each service will act appropriately: the orders service will cancel \nthe customer’s order; the transaction service will cancel the stock reservation; and the \nfees service will reverse the fee charged, executing actions C1, C2, and C3, respectively. \nThis is shown in figure 5.9.\n \n\n\n114\nChapter 5  Transactions and queries in microservices  \nMarket\nOrder Failed\nOrders\nAccount\nTransactions\nFees\nCancels customer\norder\nReverses stock\nreservation\nReverses fee\ncharged\nFigure 5.9    The market service emits a failure event is to initiate a rollback process across multiple \nservices.\nThis form of rollback is intended to make the system semantically, not mathematically \nconsistent. Your system on rollback of an operation may not be able to return to the \nexact same initial state. Imagine one of the tasks executed on calculating the fees was \nsending out an email. You can’t unsend an email, so you’d instead send another one \nacknowledging the error and saying the amount that the fees service had charged was \ndeposited back to the account.\nEvery action involved in a process might have one or more appropriate compensat-\ning actions. This approach adds to system complexity — both in anticipating scenar-\nios and in coding for them and testing them — especially because the more services \ninvolved in an interaction, the greater the possible intricacy of rolling back.\n Anticipating failure scenarios is a crucial part of building services that reflect real-world \ncircumstance, rather than operating in isolation. When designing microservices, you need \nto take compensation into account to ensure that the wider application is resilient.\nAdvantages and drawbacks\nThe choreographed style of interaction is helpful because participating services don’t \nneed to explicitly know about each other, which ensures they’re loosely coupled. In \nturn, this increases the autonomy of each service. Unfortunately, it’s not perfect. \nNo single piece of your code knows how to execute a sell order. This can make valida-\ntion challenging, spreading those rules across multiple distinct services. It also increases \nthe complexity of state management: each service needs to reflect distinct states in the \nprocessing of an order. For example, the orders service must track whether an order \nhas been created, placed, canceled, rejected, and so on. This additional complexity \nincreases the difficulty of reasoning about your system.\nChoreography also introduces cyclic dependencies between services: the orders ser-\nvice emits events that the market service consumes, but, in turn, it also consumes events \nthat the market service emits. These types of dependencies can lead to release time \ncoupling between services.\nGenerally, when opting for an asynchronous communication style, you must invest \nin monitoring and tracing to be able to follow the execution flow of your system. In case \n \n\n\n\t\n115\nSagas\nof an error, or if you need to debug a distributed system, the monitoring and tracing \ncapabilities act as a flight recorder. You should have all that happens stored there so you \ncan later investigate every single event to make sense of what happened in a multitude \nof systems. This capability is crucial for choreographed interactions.\nNOTE    Chapters 11 and 12 will explore how to achieve observability through \nlogging, tracing, and monitoring in microservice applications.\nA choreographed approach makes it difficult to know how far along a process is. Like-\nwise, the order of rollback might be important; this isn’t guaranteed by choreography, \nwhich has looser time guarantees than an orchestrated or synchronous approach. For \nsimple, near-instant workflows, knowing where you’re at is often irrelevant, but many \nbusiness processes aren’t instant — they might take multiple days and involve disparate \nsystems, people, and organizations.\n5.3.2\t\nOrchestrated sagas\nInstead of choreography, you can use orchestration to implement sagas. In an orches-\ntrated saga, a service takes on the role of orchestrator (or coordinator): a process that \nexecutes and tracks the outcome of a saga across multiple services. An orchestrator \nmight be an independent service — recall the verb-oriented services from chapter \n4 — or a capability of an existing service.\nThe sole responsibility of the orchestrator is to manage the execution of the saga. It \nmay interact with participants in the saga via asynchronous events or request/response \nmessages. Most importantly, it should track the state of execution for each stage in the \nprocess; this is sometimes called the saga log.\nLet’s make the orders service a saga coordinator. Figure 5.10 illustrates the happy \npath where a customer places an order successfully.\nSaga\nlog\nOrders\nT0: order requested\nT1: fee charged\nT2: stock reserved\nT3: order ready\nT4: order placed\nFee Charged\nOrder Created\nStock Reserved\nOrder Prepared\nMarket\nAccount\nTransactions\nFees\nOrder Placed\nFigure 5.10    An orchestrated saga for placing an order\n \n\n\n116\nChapter 5  Transactions and queries in microservices  \nYou’ll quickly see the key difference between this and the choreographed example \nfrom figure 5.8: the orders service tracks the execution of each substep in the process \nof placing an order. It’s useful to think of the coordinator as a state machine: a series of \nstates and transitions between those states. Each response from a collaborator triggers \na state change, moving the orchestrator toward the saga outcome.\nAs you know, a saga won’t always be successful. In an orchestrated saga, the coordina-\ntor is responsible for initiating appropriate reconciliation actions to return the entities \naffected by the failed transaction to a valid, consistent state.\nLike you did earlier, imagine the market service can’t place the order to market. The \norchestrating service will initiate compensating actions:\n1\t It’ll issue a request to the account transaction service to reverse the lock placed \non the holdings to be sold.\n2\t It’ll issue a request to cancel the fee that was charged to the customer.\n3\t It may change the state of the order to reflect the outcome of the saga — for \nexample, to rejected or failed. This depends on the business logic (and whether \nfailed orders should be shown to the customer or retried).\nIn turn, the orchestrator also could track the outcome of actions 1 and 2. Figure 5.11 \nillustrates this failure scenario.\nOrders\nT0: order requested\nT1: fee charged\nT2: stock reserved\nT3: order ready\nT4: order failed\nT5: rollback lock\nT6: rollback fee\nT7: update order\nMarket\nAccount\nTransactions\nFees\nAccount\nTransactions\nFees\nFee Charged\nOrder Created\nStock Reserved\nOrder Prepared\nOrder Failed\nOrder Cancelled\nTriggers compensating actions\nto be performed by saga\nparticipants\nFailure occurs in market\nplacement\nFee Cancelled\nLock Cancelled\nABORT!\n3\n1\n2\nFigure 5.11    In this unsuccessful saga, a failure by the market service results in the orchestrator \ntriggering compensating actions.\n \n\n\n\t\n117\nSagas\nTIP    Don’t forget that compensating actions might not all happen instanta-\nneously or at the same time. For example, if the fee was charged to a customer’s \ndebit card, it might take a week for their bank to reverse the charge. \nBut if the desired actions you want to happen can fail, the compensating actions — or \nthe orchestrator itself — also could fail. You should design compensating actions to \nbe safe to retry without unintentional side effects (for example, double refunds). At \nworst, repeated failure during rollback might require manual intervention. Thorough \nerror monitoring should catch these scenarios.\nAdvantages and drawbacks\nCentralizing the saga’s sequencing logic in a single service makes it significantly \neasier to reason about the outcome and progress of that saga, as well as change \nthe sequencing in one place. In turn, this can simplify individual services, reduc-\ning the complexity of states they need to manage, because that logic moves to the \ncoordinator.\nThis approach does run the risk of moving too much logic to the coordinator. At \nworst, this makes the other services anemic wrappers for data storage, rather than \nautonomous and independently responsible business capabilities.\nMany microservice practitioners advocate peer-to-peer choreography over orches-\ntration, as they see this approach to reflect the “smart endpoints, dumb pipes” aim of \nmicroservice architecture, in contrast to the heavy workflow tools (such as WS-BPEL) \npeople often used in enterprise SOA. But orchestrated approaches are becoming \nincreasingly popular in the community, especially for building long-running inter-\nactions, as seen by the popularity of projects like Netflix Conductor and AWS Step \nWorkflows.\n5.3.3\t\nInterwoven sagas\nUnlike ACID transactions, sagas aren’t isolated. The result of each local transaction is \nimmediately visible to other transactions affecting that entity. This visibility means that \na given entity might get simultaneously involved in multiple, concurrent sagas. As such, \nyou need to design your business logic to expect and handle intermediate states. The \ncomplexity of the interleaving required primarily depends on the nature of the under-\nlying business logic.\nFor now, imagine that a customer placed an order by accident and wanted to cancel \nit. If they issued their request before the order was placed to market, the order place-\nment saga would still be in progress, and this new instruction would potentially need to \ninterrupt it  (figure 5.12).\nThree common strategies for handling interwoven sagas are available: short-circuiting, \nlocking, and interruption.\n \n\n\n118\nChapter 5  Transactions and queries in microservices  \nCreate order\nReserve stock\nCharge fee\nPlace to market\nCancel order\ncancels\nrequests\nrequests\nActions\nchange\nchange\nApplication state\nOther\nprocesses\nOther processes may need\nto tolerate inconsistent\napplication state\nSagas can be interrupted\nby other sagas\nCustomer\nFigure 5.12    Steps in sagas may be interwoven\nShort-circuiting\nYou could prevent the new saga from being initiated while the order is still within \nanother saga. For example, the customer couldn't cancel the order until after the mar-\nket service attempted to place it to the market. This isn’t great for a user but is probably \nthe easiest strategy!\nLocking\nYou could use locks to control access to an entity. Different sagas that want to change \nthe state of the entity would wait to obtain the lock. You’ve already seen an example of \nthis in action: you place a reservation — or lock — on a stock balance to ensure that a \ncustomer can’t sell a holding twice if it’s involved in an active order. \nThis can lead to deadlocks if multiple sagas block each other trying to access the \nlock, requiring you to implement deadlock monitoring and timeouts to make sure the \nsystem doesn’t grind to a halt.\nInterruption\nLastly, you could choose to interrupt the actions taking place. For example, you could \nupdate the order status to “failed.” When receiving a message to send an order to mar-\nket, the market gateway could revalidate the latest order status to ensure the order was \nstill valid to send, and in this case it would see a “failed” status. This approach increases \nthe complexity of business logic but avoids the risk of deadlocks.\n5.3.4\t\nConsistency patterns\nAlthough sagas rely heavily on compensating actions, they’re not the only approach \nyou might use to achieve consistency in service interactions. So far, we’ve encountered \ntwo patterns for dealing with failure: compensating actions (refund my coffee pay-\nment) and retries (try to make the coffee again). Table 5.1 outlines other strategies.\n \n\n\n\t\n119\nSagas\nTable 5.1    Consistency strategies in microservice applications\n#\nName\nStrategy\n1\nCompensating action\nPerform an action that undoes prior action(s)\n2\nRetry\nRetry until success or timeout\n3\nIgnore\nDo nothing in the event of errors\n4\nRestart\nReset to the original state and start again\n5\nTentative operation\nPerform a tentative operation and confirm (or cancel) later\nThe use of these strategies will depend on the business semantics of your service \ninteraction. For example, when processing a large data set, it might make sense to \nignore individual failures (applying strategy #3), because the cost of processing the \noverall data set is large. When interacting with a warehouse — for example, to fulfill \norders — it’d be reasonable to place a tentative hold (strategy #5) on a stock item in a \ncustomer’s basket to reduce the possibility of overselling.\n5.3.5\t\nEvent sourcing\nSo far, we’ve assumed that entity state and events are distinct: the former is stored in an \nappropriate transactional store, whereas the latter are published independently (fig-\nure 5.13).\nAn alternative to this approach is the event sourcing pattern: rather than publishing \nevents about entity state, you represent state entirely as a sequence of events that have \nhappened to an object. To get the state of an entity at a specific time, you aggregate \nevents before that date. For example, imagine your orders service:\n¡ In the traditional persistence approaches we’ve assumed so far, a database would \nstore the latest state of the order.\n¡ In event sourcing, you’d store the events that happened to change the state of \nthe order. You could materialize the current state of the order by replaying those \nevents.\nService\npublishes\nEvents\nupdates\nstate\nqueries\nstate\nStore\nFigure 5.13    A service storing state in a data store and publishing events, in two distinct actions\n \n",
      "page_number": 130
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 138-145)",
      "start_page": 138,
      "end_page": 145,
      "detection_method": "topic_boundary",
      "content": "120\nChapter 5  Transactions and queries in microservices  \ntime\noccured:12:58\norderId: 102\nstatus: CREATED\nquantity: 1000\noccured:12:59\norderId: 102\nstatus: CHARGED\noccured:01:00\norderId: 102\nstatus: RESERVED\noccured: 01:01\norderId: 102\nstatus: PLACED\norderId: 102\nstatus: PLACED\nupdated: 01:01\nquantity: 1000\nEvents are\naggregated to\nquery state at a\ngiven time\nEvents are stored\npersistently\norderId: 102\nstatus: CHARGED\nupdated: 12:59\nquantity: 1000\nOrder @12:59\nOrder @01:01\nOrder Event\nOrder Event\nOrder Event\nOrder Event\nFigure 5.14    An order, stored as a sequence of events\nFigure 5.14 illustrates the event sourcing approach for tracking an order’s history.\nThis architecture solves a common problem in enterprise applications: understand-\ning how you reached your current state. It removes the division between state and \nevents; you don’t need to stick events on top of your business logic, because your busi-\nness logic inherently generates and manipulates events. On the other hand, it makes \ncomplex queries more difficult: you’d need to materialize views to perform joins or \nfilter by field values, as your event storage format would only support retrieving entities \nby their primary key. \nEvent sourcing isn’t a requirement for a microservice application, but using events \nto store application state can be a particularly elegant tool, especially for applications \ninvolving complex sagas where tracking the history of state transitions is vital. If you’re \ninterested in learning more about event sourcing, Nick Chamberlain’s awesome-ddd \nlist (https://github.com/heynickc/awesome-ddd) has a great collection of resources \nand further reading. \n5.4\t\nQueries in a distributed world\nDecentralized data ownership also makes retrieving data more challenging, as it’s no \nlonger possible to aggregate related data at, or close to, the database level — for exam-\nple, through joins. Presenting data from disparate services is often necessary at the UI \nlayer of an application.\nFor example, imagine you’re building an administrative UI that shows a list of cus-\ntomers, together with their current open orders. In a SQL database, you’d join these \ntwo tables in a single query, returning one dataset. In a microservice application, this \ncomposition would typically take place at the API level: a service or an API gateway could \n \n\n\n\t\n121\nQueries in a distributed world\nperform this (figure 5.15). Correlation IDs  — roughly analogous to foreign keys in a rela-\ntional database — identify relationships between data that each service owns; for exam-\nple, each order would record the associated customer ID.\nThe two-step approach in figure 5.15 works well for single entities or small datasets \nbut will scale poorly for bulk requests. If the first query returns N customers, then the \nsecond query will be performed N times, which could quickly get out of hand. If we \nwere querying a SQL database, this would be trivial to solve with a join, but because our \ndata is spread across multiple data stores, an easy solution like using a join isn’t possible.\nWe could improve this query by introducing bulk request endpoints and paging, as \nin listing 5.1. Rather than getting every customer, you’d get the first page; rather than \nretrieving customer orders one-by-one, you could retrieve them with a list of IDs. You \nshould note, though, that if each customer had thousands of orders, having to page \nthose as well would add substantial overhead.\nListing 5.1    Different endpoints for data retrieval\n/customers?page=1&size=20 \n/orders?customerIds=4,5,10,20 \nAPI composition is simple and intuitive, and for many use cases, such as individual \naggregates or small enumerables, the performance of this approach will be acceptable. \nFor others, such as the following, performance will be inefficient and far from ideal:\n¡ Queries that return and join substantial data, such as reporting  — “I want all customer \norders from the last year.”\n¡ Queries that aggregate or perform analytics across multiple services  — “I want to know the \naverage order value of emerging market stocks purchased by customers over 35.”\n¡ Queries that aren’t optimally supported by the service’s own database  — For example, \ncomplex search patterns are often difficult to optimize in relational databases.\nCustomers\nOrders\nAPI gateway\nuser interface\nconsuming service\n1. get all customers\n2. for each customer,\nget all orders\nqueries\nqueries\nEach order stores the customer_id as a correlation id\nFigure 5.15    Data composition at the API level\nYou should page large datasets.\nYou should retrieve children using “IN” \nsemantics rather than individually.\n \n\n\n122\nChapter 5  Transactions and queries in microservices  \nLastly, API composition is impacted by availability. Composition requires synchronous \ncalls to underlying services, so the total availability of a query path is the product of \nthe availability of all services involved in that path. For example, if the two services and \nthe API gateway in figure 5.15 each have an availability of 99%, their availability when \ncalled together would be 99%^3: 97.02%. Over the next three sections, we’ll discuss \nhow you also can use events to build efficient queries in microservice applications.\nNOTE    We’ll discuss service availability and reliability, and techniques for maxi-\nmizing those properties in the following chapter.\n5.4.1\t\nStoring copies of data\nYou can elect to have services store or cache data that they receive from other services \nvia events. For example, in figure 5.16, when the fees service receives an OrderCreated \nmessage, it might elect to store additional detail about the order, beyond the correla-\ntion ID. This service can now handle queries like “What was the value of this order?” \nwithout needing to retrieve that data with an additional call to the orders service.\nThis technique can be quite useful but risky:\n¡ Maintaining multiple copies of data increases overall application and service \ncomplexity (and possibly, overall storage cost).\n¡ Breaking schema changes in events is extremely tricky to manage, as services \nbecome increasingly coupled to event content.\n¡ Cache invalidation is notoriously hard.7\nOrder Created\npublishes\nconsumes\nFees\nprocesses fee\nFee Id\n1\nValue\n£3.04\nOrder Id\n45\nOrder Value\n£521.01\nA correlation ID can be\nderived from the event.\nAdditional data can be stored\nor cached from the event.\nstores\nDatabase\nOrders\nFigure 5.16    You can use events to share, and therefore replicate, state across multiple services\n7\t See Martin Fowler, “TwoHardThings,” July 14, 2009, https://martinfowler.com/bliki \n/TwoHardThings.html, and Mark Heath, “Troubleshooting Caching Problems,” SoundCode, \nJanuary 23, 2018, http://mng.bz/M2J7.\n \n\n\n\t\n123\nQueries in a distributed world\nBy maintaining canonical data in multiple locations — updated via asynchronous \nevents, which could be delayed, or fail, or be delivered multiple times — you have to \ncope with eventual consistency and the chance that the copies of data you retrieve have \nbecome stale.\nWhether it’s fine for data to be stale sometimes is down to the business semantics of \nthe particular feature. But it’s a hard tradeoff. The CAP theorem8 says that you can’t \nhave things both ways: you need to choose between availability — returning a successful \nresult, without a guarantee that data is fresh — and consistency — returning the most \nrecent state, or an error. \nGuaranteeing consistency tends to result in increased coordination between \nsystems — such as distributed locks — which hampers transaction speed. In contrast, \na system that maximizes availability ultimately relies on compensating actions and \nretries — a lot like sagas. From an architectural perspective, availability is usually \neasier to achieve and, because of the reduced coordination cost, more amenable to \nbuilding scalable applications.\nPrioritizing availability\nBuilding systems that prioritize availability might require you to avoid the instinctual, con-\nsistency-oriented solution to a problem. Even systems that seem like they should priori-\ntize consistency often make availability tradeoffs to maximize successful use.\nA great example is an automated teller machine (ATM) — prioritizing availability increases \nbank revenue. If an ATM can’t connect to the bank backend, or the wider ATM network, \nit’ll still allow withdrawals, but cap them, ensuring risk of overdraft is limited. If a with-\ndrawal does place a customer in overdraft, the bank can recoup that with a fee.\nA recent article from Eric Brewer — http://mng.bz/HGA3 — has a great overview of this \nscenario.\n \n5.4.2\t\nSeparating queries and commands\nYou can generalize the previous approach — using events to build views — further. \nIn many systems, queries are substantially different from writes: whereas writes affect \nsingular, highly normalized entities, queries often retrieve denormalized data from a \nrange of sources. Some query patterns might benefit from completely different data \nstores than writes; for example, you might use PostgreSQL as a persistent transactional \nstore but Elasticsearch for indexing search queries. The command-query responsibility \nsegregation pattern (CQRS) is a general model for managing these scenarios by explic-\nitly separating reads (queries) from writes (commands) within your system.9\n8\t This is a fantastic, “plain English” explanation of the CAP theorem: ksat.me/a-plain-english \n-introduction-to-cap-theorem/, by Kaushik Sathupadi.\n9\t You need to use CQRS if you implement an event-sourcing architecture.\n \n\n\n124\nChapter 5  Transactions and queries in microservices  \nNOTE    We won’t go into specific technical detail about implementing CQRS, \nbut you can explore frameworks in many languages, such as Commanded \n(Elixir), CQRS.net (.NET), Lagom (Java and Scala), and Broadway (PHP).\nCQRS architecture\nLet’s sketch out this architecture. In figure 5.17, you can see that CQRS partitions com-\nmands and queries:\n¡ The command side of an application performs updates to a system — creates, \nupdates and deletes. Commands emit events, either in-band or to a distinct event \nbus, such as RabbitMQ or Kafka.\n¡ Event handlers consume events to build appropriate query or read models.\n¡ A separate data store may support each side of the system.\nYou can apply this pattern both within services and across your whole applica-\ntion — using events to build dedicated query services that own and maintain complex \nviews of application data. For example, imagine you wanted to aggregate order fees \nacross your entire customer base, potentially slicing them by multiple attributes (for \nexample, type of order, asset categories, payment method). This wouldn’t be possible \nat a service level, because neither the fees, orders, nor customers service has all the \ndata needed to filter those attributes. \nInstead, as figure 5.18 illustrates, you could build a query service, CustomerOrders, \nto construct appropriate views. A query service is a good way to handle views that don’t \nclearly belong to any other services, ensuring a reasonable separation of concerns.\nread\ncreate\nupdate\ndelete\ncommands\nevent handlers\nqueries\nService\nevents\nreads\nupdates\nupdates\nCommand\nstore\nQuery store\nFigure 5.17    CQRS partitions a service into command and query sides, each accessing separate data \nstores.\n \n\n\n\t\n125\nQueries in a distributed world\nOrders\nFees\nCustomers\nCustomer...\nFee Charged\nOrder Created\nevent handlers\nqueries\nupdates\nreads\nQuery store\nCustomer Orders\nget\nFigure 5.18    Query services can construct complex views from events that multiple services emit.\nTIP    You don’t need to use only CQRS within your application. Using different \nquery styles in different scenarios can help achieve a good balance of complex-\nity, implementation speed, and customer value.\nSo far, this all sounds great! In a microservices application, CQRS offers two key benefits:\n¡ You can optimize the query model for specific queries to improve their perfor-\nmance and remove the need for cross-service joins.\n¡ It aids in separation of concerns, both within services and at an application level.\nBut it’s not without drawbacks. Let’s explore those now.\n5.4.3\t\nCQRS challenges\nLike the data caching example, CQRS requires you to consider eventual consistency \nbecause of replication lag : inherently, the command state of a service will be updated \nbefore the query state. Because events update query models, someone querying that \ndata might receive an out of date view. This might be a frustrating user experience (fig-\nure 5.19). Imagine you update the value of an order, but on clicking Confirm, you see \nthe details of the original order! Web UIs that use a POST/redirect/GET10 pattern will \noften suffer from this problem.\n10\t See https://en.wikipedia.org/wiki/Post/Redirect/Get for more information.\n \n\n\n126\nChapter 5  Transactions and queries in microservices  \nUI\nOrders\nQueries made before\nthe view is updated\ncan’t find the new order.\nUpdating the query\nview takes time to\nperform.\ncreate order\nconfirm\nget order\nno order found!\norder created\nupdate view\nOrder Query\nFigure 5.19    Lag in updating a query view leads to inconsistent results when making a request.\nIn some systems, this might not matter. For example, delayed updates are common for \nactivity feeds11 — if I post an update on Twitter, it doesn’t matter if my followers don’t \nall receive it at the same time. And in fact, attempting to achieve greater consistency \ncan lead to substantial scalability challenges that might not be worth it.\nIn other systems, it’ll be important to ensure you don’t query invalid state. You can \napply three strategies (figure 5.20) in these scenarios: optimistic updates, polling, or \npublish-subscribe.\nOptimistic updates\nYou could update the UI optimistically, based on the expected result of a command. If \nthe command fails, you can roll back the UI state. For example, imagine you like a post \non Instagram. The app will show a red heart before the Instagram backend saves that \nchange. If that save fails, Instagram will roll back the optimistic UI change, and you’ll \nhave to like it again for it to show a red heart.\nThis approach relies on having — or being able to derive — all the information you \nneed to update the UI from the command input, so it works best when working with \nsimple entities.\n11\t If you’re interested in the architecture behind activity streams, https://github.com/tschellenbach/ \nStream-Framework is a good place to start.\n \n\n\n\t\n127\nQueries in a distributed world\n1. request\n3. confirm or rollback\n2. update to reflect\n1. request\nversion X\n2. poll until\nversion X\n2. request\n1. subscribe\n3. publish\nCommand\nUI\nUI\nUI\nCommand\nQuery\nQuery\nCommand\nOptimistic update\nPolling\nPublish-subscribe\nFigure 5.20    Three strategies for dealing with query-side replication lag in CQRS\nPolling\nThe UI could poll the query API until an expected change has occurred. When ini-\ntiating a command, the client would set a version, such as a timestamp. For subse-\nquent queries, the client would continue to poll until the version number was equal \nor greater to the version number specified, indicating that the query model had been \nupdated to reflect the new state.\nPublish-subscribe\nInstead of polling for changes, a UI could subscribe to events on a query model — for \nexample, over a web socket channel. In this case, the UI would only update when the \nread model published an “updated” event.\nAs you can see, it’s challenging to reason through CQRS, and it requires a different \nmindset from what you’d have when dealing with normal CRUD APIs. But it can be \nuseful in a microservice application. Done right, CQRS helps to ensure performance \nand availability in queries, even as you distribute data and responsibility across multiple \ndistinct services and data stores. \n5.4.4\t\nAnalytics and reporting\nYou can generalize the CQRS technique to other use cases, such as analytics and \nreporting. You can transform a stream of microservice events and store them in a data \nwarehouse, such as Amazon Redshift or Google BigQuery (figure 5.21). A transforma-\ntion stage may involve mapping events to the semantics and data model of the target \nwarehouse or combining events with data from other microservices. If you don’t yet \nknow how you want to treat or query events, you can store them in commodity stor-\nage, such as Amazon S3, for later querying or reprocessing with big data tools such as \nApache Spark or Presto.\n \n",
      "page_number": 138
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 146-154)",
      "start_page": 146,
      "end_page": 154,
      "detection_method": "topic_boundary",
      "content": "128\nChapter 5  Transactions and queries in microservices  \nevent stream\nTransform\nStorage\nAnalytic tools e.g.\nSpark\nData\nwarehouse\nevents\nevents\nFigure 5.21    You can use microservice events to populate data warehouses or other analytic stores.\t\n5.5\t\nFurther reading\nWe’ve covered a lot of ground in this chapter, but some topics, like sagas, event sourc-\ning, and CQRS, can each fill entire books. In case you’re interested in knowing more \nabout those topics, we recommend the following books:\n¡ Reactive Application Development, by Duncan K. DeVore, Sean Walsh, and Brian \nHanafee, https://www.manning.com/books/reactive-application-development \n(ISBN 9781617292460)\n¡ Microservices Patterns, by Chris Richardson, https://www.manning.com/books/\nmicroservices-patterns (ISBN 9781617294549)\n¡ Event Streams in Action, by Alexander Dean, https://www.manning.com/books/\nevent-streams-in-action (ISBN 9781617292347)\nSummary\n¡ ACID properties are difficult to achieve in interactions across multiple services; \nmicroservices require different approaches to achieve consistency.\n¡ Coordination approaches, such as two-phase commit, introduce locking and \ndon’t scale well.\n¡ An event-based architecture decouples independent components and provides a \nfoundation for scalable business logic and queries in a microservice application.\n¡ Biasing towards availability, rather than consistency, tends to lead to a more scal-\nable architecture.\n¡ Sagas are global actions composed from message-driven, independent local \ntransactions. They achieve consistency by using compensating actions to roll \nback incorrect state.\n¡ Anticipating failure scenarios is a crucial element of building services that reflect \nreal-world circumstance, rather than operating in isolation.\n¡ You typically implement queries across microservices by composing results from \nmultiple APIs.\n¡ Efficient complex queries should use the CQRS pattern to materialize read mod-\nels, especially where those query patterns require alternative data stores.\n \n\n\n129\n6\nDesigning reliable services\nThis chapter covers\n¡ The impact of service availability on application \nreliability\n¡ Designing microservices that defend against \nfaults in their dependencies\n¡ Applying retries, rate limits, circuit breakers, \nhealth checks, and caching to mitigate \ninterservice communication issues\n¡ Applying safe communication standards across \nmany services\nNo microservice is an island; each one plays a small part in a much larger system. Most ser-\nvices that you build will have other services that rely on them — upstream collaborators —  \nand in turn themselves will depend on other services — downstream collaborators — to \nperform useful functions. For a service to reliably and consistently perform its job, it \nneeds to be able to trust these collaborators.\nThis is easier said than done. Failures are inevitable in any complex system. An indi-\nvidual microservice might fail for a variety of reasons. Bugs can be introduced into code. \nDeployments can be unstable. Underlying infrastructure might let you down: resources \n \n\n\n130\nChapter 6  Designing reliable services\nmight be saturated by load; underlying nodes might become unhealthy; even entire data \ncenters can fail. As we discussed in chapter 5, you can’t even trust that the network between \nyour services is reliable — believing otherwise is a well-known fallacy of distributed comput-\ning.1 Lastly, human error can lead to major failures. For example, I’m writing this chapter a \nweek after an engineer’s mistake in running a maintenance script led to a severe outage in \nAmazon S3, affecting thousands of well-known websites.\nIt’s impossible to eliminate failure in microservice applications — the cost of that \nwould be infinite! Instead, your focus needs to be on designing microservices that are \ntolerant of dependency failures and able to gracefully recover from them or mitigate \nthe impact of those failures on their own responsibilities.\nIn this chapter, we’ll introduce the concept of service availability, discuss the impact of \nfailure in microservice applications, and explore approaches to designing reliable com-\nmunication between services. We’ll also discuss two different tactics — frameworks and \nproxies — for ensuring all microservices in an application interact safely. Using these \ntechniques will help you maximize the reliability of your microservice application — and \nkeep your users happy.\n6.1\t\nDefining reliability\nLet’s start by figuring out how to measure the reliability of a microservice. Consider a simple \nmicroservice system: a service, holdings, calls two dependencies, transactions and market-data. \nThose services in turn call further dependencies. Figure 6.1 illustrates this relationship.\nFor any of those services, you can assume that they spent some time performing work \nsuccessfully. This is known as uptime. Likewise, you can safely assume — because failure \nis inevitable — that they spent some time failing to complete work. This is known as \ndowntime. You can use uptime and downtime to calculate availability: the percentage of \noperational time during which the service was working correctly. A service’s availability \nis a measure of how reliable you can expect it to be.\nRequests\nHoldings\nD\nE\nC\nMarket-data\nTransactions\nFigure 6.1    A simple microservice system, illustrating dependencies between collaborating services\n1\t Peter Deutsch originally posited the eight fallacies of distributed computing in 1994. A good over-\nview is available here: http://mng.bz/9T5F.\n \n\n\n\t\n131\nDefining reliability\nA typical shorthand for high availability is “nines:” for example, two nines is 99%, \nwhereas five nines is 99.999%. It’d be highly unusual for critical production-facing ser-\nvices to be less reliable than this.\nTo illustrate how availability works, imagine that calls from holdings to market-data \nare successful 99.9% of the time. This might sound quite reliable, but downtime of \n0.1% quickly becomes pronounced as volumes increase: only one failure per 1,000 \nrequests, but 1,000 failures per million. These failures will directly affect your calling \nservice unless you can design that service to mitigate the impact of dependency failure.\nMicroservice dependency chains can quickly become complex. If those dependen-\ncies can fail, what’s the probability of failure within your whole system? You can treat \nyour availability figure as the probability of a request being successful — by multiplying \ntogether the availability figures for the parts of the chain, you can estimate the failure \nrate across your entire system.\nSay you expand the previous example to specify that you have six services with the \nsame success rate for calls. For any request to your system, you can expect one of four \noutcomes: all services work correctly, one service fails, multiple services fail, or all ser-\nvices fail.\nBecause calls to each microservice are successful 99.9% of the time, combined reli-\nability of the system will be 0.9996 = 0.994 = 99.4%. Although this is a simple model, \nyou can see that the whole application will always be less reliable than its independent \ncomponents; the maximum availability you can achieve is a product of the availability of \na service’s dependencies.\nTo illustrate, imagine that service D’s availability is degraded to 95%. Although this \nwon’t affect transactions — because it’s not part of that call hierarchy — it will reduce \nthe reliability of both market-data and holdings. Figure 6.2 illustrates this impact.\nHoldings\nD\nE\nC\nMarket-data\nTransactions\nRequests\nThe reliability of holdings is\nthe combined reliability of\nitself and its collaborators.\nThe reliability of market-data\nis the combined reliability of\nitself, D, and E.\nA reduction in D’s reliability\naffects all upstream services.\n99.9%\n99.9%\n99.9%\n95%\n99.9% * 95% * 99.9% =\n94.8%\n99.9% * 94.8% * 99.9% *\n99.9% = 94.5%\nFigure 6.2    The impact of service dependency availability on reliability in a microservice application\n \n\n\n132\nChapter 6  Designing reliable services\nIt’s crucial to maximize service availability — or isolate the impact of unreliability — to \nensure the availability of your entire application. Measuring availability won’t tell you \nhow to make your services reliable, but it gives you a target to aim for or, more specifi-\ncally, a goal to guide both the development of services and the expectations of consum-\ning services and engineers.\nNOTE    How do you monitor availability? We’ll explore approaches to monitor-\ning service availability in a microservice application in part 4 of this book.\nIf you can’t trust the network, your hardware, other services, or even your own services \nto be 100% reliable, how can you maximize availability? You need to design defensively \nto meet three goals:\n¡ Reduce the incidence of avoidable failures\n¡ Limit the cascading and system-wide impact of unpredictable failures\n¡ Recover quickly — and ideally automatically — when failures do occur\nAchieving these goals will ultimately maximize the uptime and availability of your \nservices.\n6.2\t\nWhat could go wrong?\nAs we’ve stated, failure is inevitable in a complex system. Over the lifetime of an appli-\ncation, it’s incredibly likely that any catastrophe that could happen, will happen. Con-\nsequently, you need to understand the different types of failures that your application \nmight be susceptible to. Understanding the nature of these risks and their likelihood \nis fundamental to both architecting appropriate mitigation strategies and reacting rap-\nidly when incidents do occur.\nBalancing risk and cost\nIt’s important to be pragmatic: you can neither anticipate nor eliminate every possible \ncause of failure. When you’re designing for resilience, you need to balance the risk of a \nfailure against what you can reasonably defend against given time and cost constraints:\n¡ The cost to design, build, deploy, and operate a defensive solution\n¡ The nature of your business and expectations of your customers\nTo put that in perspective, consider the S3 outage I mentioned earlier. You could defend \nagainst that error by replicating data across multiple regions in AWS or across multiple \nclouds. But given that S3 failures of that magnitude are exceptionally rare, that solution \nwouldn’t make economic sense for many organizations because it would significantly \nincrease operational costs and complexity.\n \nAs a responsible service designer, you need to identify possible types of failure within \nyour microservice application, rank them by anticipated frequency and impact, and \ndecide how you’ll mitigate their impact. In this section, we’ll walk you through some \n \n\n\n\t\n133\nWhat could go wrong?\ncommon failure scenarios in microservice applications and how they arise. We’ll also \nexplore cascading failures — a common catastrophic scenario in a distributed system.\n6.2.1\t\nSources of failure\nLet’s examine a microservice to understand where failure might arise, using one of Sim-\npleBank’s services as an example. You can assume a few things about the market-data \nservice:\n¡ The service will run on hardware — likely a virtualized host — that ultimately \ndepends on a physical data center.\n¡ Other upstream services depend on the capabilities of this service.\n¡ This service stores data in some store — for example, a SQL database.\n¡ It retrieves data from third-party data sources through APIs and file uploads.\n¡ It may call other downstream SimpleBank microservices.\nFigure 6.3 illustrates the service and its relationship to other components.\nData store\nStores &\nretrieves\ndata\nHost\nThird-party\ndependencies\nMakes requests\nMake requests\nMarket-data\nUpstream collaborators\nDownstream collaborators\nMakes requests\nFigure 6.3    Relationships between the market-data microservice and other components of the \napplication\n \n\n\n134\nChapter 6  Designing reliable services\nEvery point of interaction between your service and another component indicates a \npossible area of failure. Failures could occur in four major areas:\n¡ Hardware  — The underlying physical and virtual infrastructure on which a ser-\nvice operates\n¡ Communication  — Collaboration between different services and/or third parties\n¡ Dependencies  — Failure within dependencies of a service\n¡ Internal  — Errors within the code of the service itself, such as defects introduced \nby engineers\nLet’s explore each category in turn.\nHardware\nRegardless of whether you run your services in a public cloud, on-premise, or using \na PaaS, the reliability of the services will ultimately depend on the physical and vir-\ntual infrastructure that underpins them, whether that’s server racks, virtual machines, \noperating systems, or physical networks. Table 6.1 illustrates some of the causes of fail-\nure within the hardware layer of a microservice application.\nTable 6.1    Sources of failure within the hardware layer of a microservice application\nSource of failure \nFrequency\nDescription\nHost \nOften\nIndividual hosts (physical or virtual) may fail.\nData center\nRare\nData centers or components within them may fail.\nHost configuration \nOccasionally\nHosts may be misconfigured —  for example, through errors in \nprovisioning tools.\nPhysical network \nRare \nPhysical networking (within or between data centers) may fail.\nOperating system and \nresource isolation \nOccasionally\nThe OS or the isolation system — for example, Docker — may fail \nto operate correctly.\nThe range of possible failures at this layer of your application are diverse and unfortu-\nnately, often the most catastrophic because hardware component failure may affect the \noperation of multiple services within an organization.\nTypically, you can mitigate the impact of most hardware failures by designing appro-\npriate levels of redundancy into a system. For example, if you’re deploying an applica-\ntion in a public cloud, such as AWS, you’d typically spread replicas of a service across \nmultiple zones — geographically distinct data centers within a wider region — to reduce \nthe impact of failure within a single center.\nIt’s important to note that hardware redundancy can incur additional operational \ncost. Some solutions may be complex to architect and run — or just plain expensive. \nChoosing the right level of redundancy for an application requires careful consider-\nation of the frequency and impact of failure versus the cost of mitigating against poten-\ntially rare events.\n \n\n\n\t\n135\nWhat could go wrong?\nCommunication\nCommunication between services can fail: network, DNS, messaging, and firewalls are \nall possible sources of failure. Table 6.2 details possible communication failures.\nTable 6.2    Sources of communication failure within a microservice application\nSource of failure \nDescription\nNetwork\nNetwork connectivity may not be possible.\nFirewall\nConfiguration management can set security rules inappropriately.\nDNS errors\nHostnames may not be correctly propagated or resolved across an application.\nMessaging\nMessaging systems — for example, RPC — can fail.\nInadequate health \nchecks\nHealth checks may not adequately represent instance state, causing requests to be \nrouted to broken instances.\nCommunication failures can affect both internal and external network calls. For exam-\nple, connectivity between the market-data service and the external APIs it relies on \ncould degrade, leading to failure.\nNetwork and DNS failures are reasonably common, whether caused by changes in \nfirewall rules, IP address assignment, or DNS hostname propagation in a system. Net-\nwork issues can be challenging to mitigate, but because they’re often caused by human \nintervention (whether through service releases or configuration changes), the best way \nto avoid many of them is to ensure that you test configuration changes robustly, and \nthat they’re easy to roll back if issues occur.\nDependencies\nFailure can occur in other services that a microservice depends on, or within that \nmicroservice’s internal dependencies (such as databases). For example, the database \nthat market-data relies on to save and retrieve data might fail because of underlying \nhardware failure or hitting scalability limits — it wouldn’t be unheard of for a database \nto run out of disk space!\nAs we outlined earlier, such failures have a drastic effect on overall system availability. \nTable 6.3 outlines possible sources of failure.\nTable 6.3    Sources of dependency-related failure\nSource of failure \nDescription\nTimeouts\nRequests to services may time out, resulting in erroneous behavior.\nDecommissioned or nonbackwards- \ncompatible functionality\nDesign doesn’t take service dependencies into account, unexpect-\nedly changing or removing functionality.\nInternal component failures\nProblems with databases or caches prevent services from working \ncorrectly.\nExternal dependencies\nServices may have dependencies outside of the application that \ndon’t work correctly or as expected — for example, third-party APIs.\n \n\n\n136\nChapter 6  Designing reliable services\nIn addition to operational sources of failure, such as timeouts and service outages, depen-\ndencies are prone to errors caused by design and build failures. For example, a service \nmay rely on an endpoint in another service that’s changed in a nonbackwards-compatible \nway or, even worse, removed completely without appropriate decommissioning.\nService practices\nLastly, inadequate or limited engineering practices when developing and deploying ser-\nvices may lead to failure in production. Services may be poorly designed, inadequately \ntested, or deployed incorrectly. You may not catch errors in testing, or a team may not \nadequately monitor the behavior of their service in production. A service might scale \nineffectively: hitting memory, disk, or CPU limits on its provisioned hardware such that \nperformance is degraded — or the service becomes completely unresponsive.\nBecause each service contributes to the effectiveness of the whole system, one poor \nquality service can have a detrimental effect on the availability of swathes of function-\nality. Hopefully the practices we outline throughout this book will help you avoid \nthis — unfortunately common — source of failure!\n6.2.2\t\nCascading failures\nYou should now understand how failure in different areas can affect a single micro-\nservice. But the impact of failure doesn’t stop there. Because your applications are \ncomposed of multiple microservices that continually interact with each other, failure \nin one service can spread across an entire system.\nCascading failures are a common mode of failure in distributed applications. A cas-\ncading failure is an example of positive feedback: an event disturbs a system, leading to \nsome effect, which in turn increases the magnitude of the initial disturbance. In this \ncase, positive means that the effect increases — not that the outcome is beneficial.\nYou can observe this phenomenon in several real-world domains, such as financial \nmarkets, biological processes, or nuclear power stations. Consider a stampede in a herd \nof animals: panic causes an animal to run, which in turn spreads panic to other animals, \nwhich causes them to run, and so on. In a microservice application, overload can cause \na domino effect: failure in one service increases failure in upstream services, and in \nturn their upstream services. At worst, the result is widespread unavailability.\nLet’s work through an example to illustrate how overload can result in a cascading \nfailure. Imagine that SimpleBank built a UI to show a user their current financial hold-\nings (or positions) in an account. That might look something like figure 6.4.\nEach financial position is the sum of the transactions — purchases and sales of a \nstock — made to date, multiplied by the current price. Retrieving these values relies on \ncollaboration between three services:\n¡ Market-data  — A service responsible for retrieving and processing price and mar-\nket information for financial instruments, such as stocks\n¡ Transactions  — A service responsible for representing transactions occurring \nwithin an account\n¡ Holdings  — A service responsible for aggregating transactions and market-data to \nreport financial positions\n \n",
      "page_number": 146
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 155-163)",
      "start_page": 155,
      "end_page": 163,
      "detection_method": "topic_boundary",
      "content": "\t\n137\nWhat could go wrong?\nFigure 6.4    A user interface that reports financial holdings in an account\nFigure 6.5 outlines the production configuration of these services. For each service, \nload is balanced across multiple replicas.\nSuppose that holdings are being retrieved 1,000 times per second (QPS). If you have \ntwo replicas of your holdings service, each replica will receive 500 QPS (figure 6.6).\nUI\n1. Retrieves\nholdings\nLB\nLB\nLB\nData store\nData store\nQueries\nQueries\nThe transactions\nand market-data\nservices “own”\ndata within their\ndomains.\nHoldings\nLoad is balanced\nacross multiple\nreplicas of each\nmicroservice.\n2. Retrieves transactions\n3. Retrieves prices\nMarket-data\nTransactions\nFigure 6.5    Production configuration and collaboration between services to populate the “current \nfinancial holdings” user interface\nUI\n1000 QPS\n500 QPS\nLB\n500 QPS\nHoldings\nHoldings\nFigure 6.6    Queries made to a service are split across multiple replicas.\n \n\n\n138\nChapter 6  Designing reliable services\nYour holdings service subsequently queries transactions and market-data to construct \nthe response. Each call to holdings will generate two calls: one to transactions and one \nto market-data.\nNow, let’s say a failure occurs that takes down one of your transactions replicas. Your \nload balancer reroutes that load to the remaining replica, which now needs to service \n1,000 QPS (figure 6.7).\nBut that reduced capacity is unable to handle the level of demand to your service. \nDepending on how you’ve deployed your service — the characteristics of your web \nserver — the change in load might first lead to increased latency as requests are queued. \nIn turn, increased latency might start exceeding the maximum wait time that the hold-\nings service expects for that query. Alternatively, the transactions service may begin \ndropping requests.\nIt’s not unreasonable for a service to retry a request to a collaborator when it fails. \nNow, imagine that the holdings service will retry any request to transactions that times \nout or fails. This will further increase the load on your remaining transactions resource, \nwhich now needs to handle both the regular request volume and the heightened retry \nvolume (figure 6.8). In turn, the holdings service takes longer to respond while it waits \non its collaborator.\nLB\nHoldings\nHoldings\nTransactions\nTransactions\n1000 QPS\n500 QPS\n500 QPS\nFigure 6.7    One replica of a collaborating service fails, sending all load to the remaining instance.\nRequests to\nholdings begin to\ntake longer.\nCollaborators\nHoldings\nHoldings\nTransactions\nRequests\nRetries\n100 QPS\n500 QPS\n100 QPS\n500 QPS\nLB\n1200 QPS\nFailed requests to\ntransactions\ntrigger retries.\nRetries increase\nrequests to\ntransactions.\nFigure 6.8    Overload on transactions causes some requests to fail, in turn causing holdings to retry \nthose requests, which starts to degrade holdings’ response time.\n \n\n\n\t\n139\nDesigning reliable communication\nHoldings\nC\nB\nTransactions\nF\nE\nD\nB\nC\nTransactions\nHoldings\nIncreased failure in upstream dependencies leads to retries,\nrepeating the cycle of failure.\nUpstream dependencies are unable to service requests\nthat rely on transactions.\nFigure 6.9    Overload in a service leads to complete failure. Unhealthy retry behavior is repeated across \ndependency chains as service performance progressively degrades, leading to further overloads.\nThis feedback loop — failed requests lead to a higher volume of subsequent requests, \nleading to a higher rate of failure — continues to escalate. Your whole system is unable to \ncomplete work, as other services that rely on transactions or holdings begin to fail. Your \ninitial failure in a single service causes a domino effect, worsening response times and \navailability across several services. At worst, the cumulative impact on the transactions ser-\nvice causes it to fail completely. Figure 6.9 illustrates this final stage of a cascading failure.\nCascading failures aren’t only caused by overload — although this is one of the most \ncommon root causes. In general, increased error rates or slower response times can \nlead to unhealthy service behavior, increasing the chance of failure across multiple ser-\nvices that depend on each other.\nYou can use several approaches to limit the occurrence of cascading failures in micro­\nservice applications: circuit breakers; fallbacks; load testing and capacity planning; back-off \nand retries; and appropriate timeouts. We’ll explore these approaches in the next section.\n6.3\t\nDesigning reliable communication\nEarlier, we emphasized the importance of collaboration in a microservice application. \nDependency chains of multiple microservices will achieve most useful capabilities in \nan application. When one microservice fails, how does that impact its collaborators \nand ultimately, the application’s end customers?\nIf failure is inevitable, you need to design and build your services to maximize avail-\nability, correct operation, and rapid recovery when failure does occur. This is funda-\nmental to achieving resiliency. In this section, we’ll explore several techniques for \nensuring that services behave appropriately — maximizing correct operation — when \ntheir collaborators are unavailable:\n¡ Retries\n¡ Fallbacks, caching, and graceful degradation\n¡ Timeouts and deadlines\n \n\n\n140\nChapter 6  Designing reliable services\n¡ Circuit breakers\n¡ Communication brokers\nBefore we start, let’s get a simple service running that we can use to illustrate the concepts \nin this section. You can find these examples in the book’s Github repository (http://\nmng.bz/7eN9). Clone the repository to your computer and open the chapter-6 direc-\ntory. This directory contains some basic services — holdings and market-data — which \nyou’ll run inside Docker containers (figure 6.10). The holdings service exposes a GET \n/holdings endpoint, which makes a JSON API request to retrieve price information \nfrom market-data.\nTo run these, you’ll need docker-compose installed (directions online: https://docs \n.docker.com/compose/install/). If you’re ready to go, type the following at the com-\nmand line:\n$ docker-compose up\nThis will build Docker images for each service and start them as isolated containers on \nyour machine. Now let’s dive in!\n6.3.1\t\nRetries\nIn this section, we’ll explore how to use retries when failed requests occur. To under-\nstand these techniques, let’s start by examining communication from the perspective \nof your upstream service, holdings.\nImagine that a call from the holdings service to retrieve prices fails, returning an \nerror. From the perspective of the calling service, it’s not clear yet whether this failure \nis isolated — repeating that call is likely to succeed, or systemic — the next call has a \nhigh likelihood of failing. You expect calls to retrieve data to be idempotent  — to have no \neffect on the state of the target system and therefore be repeatable.2\nAs a result, your first instinct might be to retry the request. In Python, you can use \nan open source library — tenacity — to decorate the appropriate method of your \nAPI client (the MarketDataClient class in holdings/clients.py) and perform retries if \nthe method throws an exception. The following listing shows the class with retry code \nadded.\nHoldings\nHoldings\nRequests\nMarket-data\nMarket-data\nFigure 6.10    Docker containers for working with microservice requests\n2\t Requests that effect some system change aren’t typically idempotent. One strategy for guaranteeing \n“exactly once” semantics is to implement idempotency keys. See Brandur Leach, “Designing robust \nand predictable APIs with idempotency,” February 22, 2017, https://stripe.com/blog/idempotency.\n \n\n\n\t\n141\nDesigning reliable communication\nListing 6.1    Adding a retry to a service call\nimport requests\nimport logging\nfrom tenacity import retry, stop, before \nclass MarketDataClient(object):\n    logger = logging.getLogger(__name__)\n    base_url = 'http://market-data:8000'\n    def _make_request(self, url):\n        response = requests.get(f\"{self.base_url}/{url}\", \n                                headers={'content-type': 'application/json'})\n        return response.json()\n    @retry(stop=stop_after_attempt(3), \n           before=before_log(logger, logging.DEBUG)) \n    def all_prices(self):\n        return self._make_request(\"prices\")\nLet’s call the holdings service to see how it behaves. In another terminal window, make \nthe following request:\ncurl -I http://{DOCKER_HOST}/holdings\nThis will return a 500 error, but if you follow the logs from the market-data service, you \ncan see a request being made to GET /prices three times, before the holdings service \ngives up.\nIf you read the previous section, you should be wary at this point. Failure might be \nisolated or persistent, but the holdings service can’t know which one is the case based \non one call.\nIf the failure is isolated and transient, then a retry is a reasonable option. This helps \nto minimize direct impact to end users — and explicit intervention from operational \nstaff — when abnormal behavior occurs. It’s important to consider your budget for \nretries: if each retry takes a certain number of milliseconds, then the consuming service \ncan only absorb so many retries before it surpasses a reasonable response time.\nBut if the failure is persistent — for example, if the capacity of market-data is \nreduced — then subsequent calls may worsen the issue and further destabilize the \nsystem. Suppose you retry each failed request to market-data five times. Every failed \nrequest you make to this service potentially results in another five requests; the volume \nof retries continues to grow. The entire service is doing less useful work as it attempts \nto service a high volume of retries. At worst, retries suffocate your market-data service, \nmagnifying your original failure. Figure 6.11 illustrates this growth of requests.\nImports relevant functions \nfrom the library\nRetries the query up to three times\nLogs each retry  \nbefore execution\n \n\n\n142\nChapter 6  Designing reliable services\n# of requests\n3000\n0\n6000\n9000\n12000\nRetries\nRequests\nRequests vs Retries\nFigure 6.11    Growth of load on your unstable market-data service resulting from failed requests being \nretried\nHow can you use retries to improve your resiliency in the face of intermittent failures \nwithout contributing to wider system failure if persistent failures occur? First, you \ncould use a variable time between successive retries to try to spread them out evenly \nand reduce the frequency of retry-based load. This is known as an exponential back-off \nstrategy and is intended to give a system under load time to recover. You can change \nthe retry strategy you used earlier, as shown in the following listing. Afterwards, by curl-\ning the /holdings endpoint, you can observe the retry behavior of the service.\nListing 6.2    Changing your retry strategy to exponential back-off\n@retry(wait=wait_exponential(multiplier=1, max=5), \n       stop=stop_after_delay(5)) \ndef all_prices(self):\n    return self._make_request(\"prices\")\nUnfortunately, exponential back-off can lead to another instance of curious emergent \nbehavior. Imagine that a momentary failure interrupts several calls to market-data, \nleading to retries. Exponential back-off can cause the service to schedule those retries \ntogether so they further amplify themselves, like the ripples from throwing a stone in \na pond.\nInstead, back-off should include a random element — jitter — to spread out retries to \na more constant rate and avoid thundering herds of synchronized retries.3 The follow-\ning listing shows how to adjust your strategy again.\nWaits 2^x * 1 second \nbetween each retry\nStops after five seconds\n3\t A great article by Marc Brooker about exponential back-off and the importance of jitter is avail-\nable on the AWS Architecture Blog, March 4, 2015, http://mng.bz/TRk5.\n \n\n\n\t\n143\nDesigning reliable communication\nListing 6.3    Adding jitter to an exponential back-off\n@retry(wait=wait_exponential(multiplier=1, max=5) + wait_random(0, 1), \n       stop=stop_after_delay(5)) \ndef all_prices(self):\n    return self._make_request(\"prices\")\nThis strategy will ensure that retries are less likely to happen in synchronization across \nmultiple waiting clients.\nRetries are an effective strategy for tolerating intermittent dependency faults, but \nyou need to use them carefully to avoid exacerbating the underlying issue or consum-\ning unnecessary resources:\n¡ Always limit the total number of retries.\n¡ Use exponential back-off with jitter to smoothly distribute retry requests and \navoid compounding load.\n¡ Consider which error conditions should trigger a retry and, therefore, which \nretries are unlikely to, or will never, succeed.\nWhen your service meets retry limits or can’t retry a request, you can either accept \nfailure or find an alternative way to serve the request. In the next section, we’ll explore \nfallbacks.\n6.3.2\t\nFallbacks\nIf a service’s dependencies fail, you can explore four fallback options:\n¡ Graceful degradation\n¡ Caching\n¡ Functional redundancy\n¡ Stubbed data\nGraceful Degradation\nLet’s return to the problem with the holdings service: if market-data fails, the applica-\ntion may not be able to provide valuations to end customers. To resolve this issue, you \nmight be able to design an acceptable degradation of service. For example, you could \nshow holding quantities without valuations. This limits the richness of your UI but is \nbetter than showing nothing — or an error. You can see techniques like this in other \ndomains. For example, an e-commerce site could still allow purchases to be made, \neven if the site’s order dispatch isn’t functioning correctly.\nCaching\nAlternatively, you could cache the results of past queries for prices, reducing the need \nto query the market-data service at all. Say a price is valid for five minutes. If so, the \nholdings service could cache pricing data for up to five minutes, either locally or in \nExponentially backs off, adding a random \nwait between zero and one second\nStops after five seconds\n \n\n\n144\nChapter 6  Designing reliable services\na dedicated cache (for example, Memcached or Redis). This solution would both \nimprove performance and provide contingency in the event of a temporary outage.\nLet’s try out this technique. You’ll use a library called cachetools, which provides an \nimplementation of a time-to-live cache. As you did earlier with retries, you’ll decorate \nyour client method, as shown in the following listing.\nListing 6.4    Adding in-process caching to a client call\nimport requests\nimport logging\nfrom cachetools import cached, TTLCache\nclass MarketDataClient(object):\n    logger = logging.getLogger(__name__)\n    cache = TTLCache(maxsize=10, ttl=5*60) \n    base_url = 'http://market-data:8000'\n    def _make_request(self, url):\n        response = requests.get(f\"{self.base_url}/{url}\", \n                                headers={'content-type': 'application/json'})\n        return response.json()\n    @cached(cache) \n    def all_prices(self):\n        logger.debug(\"Making request to get all_prices\")\n        return self._make_request(\"prices\")\nSubsequent calls made to GET /holdings should retrieve price information from \nthe cache, rather than by making calls to market-data. If you used an external cache \ninstead, multiple instances could use the cache, further reducing load on market-data \nand providing greater resiliency for all holdings replicas, albeit at the cost of maintain-\ning an additional infrastructural component.\nFunctional redundancy\nSimilarly, you might be able to fall back to other services to achieve the same func-\ntionality. Imagine that you could purchase market data from multiple sources, each \ncovering a different set of instruments at a different cost. If source A failed, you could \ninstead make requests to source B (figure 6.12).\nFunctional redundancy within a system has many drivers: external integrations; algo-\nrithms for producing similar results with varying performance characteristics; and even \nolder features that remain operational but have been superseded. In a globally distrib-\nuted deployment, you could even fall back on services hosted in another region.4\nOnly some failure scenarios would allow the use of an alternative service. If the cause \nof failure was a code defect or resource overload in your original service, then rerouting \nto another service would make sense. But a general network failure could affect multi-\nple services, including ones you might try rerouting to.\nInstantiates a cache\nDecorates your method to  \nstore results using your cache\n4\t At the ultimate end of this scale, Netflix can serve a given customer from any of their global data \ncenters, conveying an impressive degree of resilience.\n \n\n\n\t\n145\nDesigning reliable communication\nFalls back to querying\nmarket-data-B\nRetrieves prices\nHoldings\nmarket-data-A\nHoldings\nmarket-data-A\nmarket-data-B\nRequests to market-data-A fail\nNormal operation\nFigure 6.12    If service failure occurs, you may be able to serve the same capability with other services.\nStubbed data\nLastly, although it wouldn’t be appropriate in this specific scenario, you could use \nstubbed data for fallbacks. Picture the “recommended to you” section on Amazon: if \nthe backend was unable for some reason to retrieve those personalized recommenda-\ntions, it’d be more graceful to fall back to a nonpersonalized data set than to show a \nblank section on the UI.\n6.3.3\t\nTimeouts\nWhen the holdings service sends a request to market-data, that service consumes \nresources waiting for a reply. Setting an appropriate deadline for that interaction limits \nthe time those resources are consumed.\nYou can set a timeout within your HTTP request function. For HTTP calls, you want \nto timeout if you haven’t received any response, but not if the response itself is slow to \ndownload. Try the following listing to add a timeout.\nListing 6.5    Adding a timeout to an HTTP call\ndef _make_request(self, url):\n    response = requests.get(f\"{self.base_url}/{url}\",\n                           headers={'content-type': 'application/json'},\n                           timeout=5) \n    return response.json()\nIn a computational sense, network communication is slow, so the speed of failures is \nimportant. In a distributed system, some errors might happen almost instantly. For \nexample, a dependent service may rapidly fail in the event of an internal bug. But many \nfailures are slow. For example, a service that’s overloaded by requests may respond \nsluggishly, in turn consuming the resources of the calling service while it waits for a \nresponse that may never come.\nSets a timeout of five seconds before \nreceiving data from market-data\n \n",
      "page_number": 155
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 164-171)",
      "start_page": 164,
      "end_page": 171,
      "detection_method": "topic_boundary",
      "content": "146\nChapter 6  Designing reliable services\nSlow failures illustrate the importance of setting appropriate deadlines — timing out \nin a reasonable timeframe — for communication between microservices. If you don’t \nset upper bounds, it’s easy for unresponsiveness to spread through entire microservice \ndependency chains. In fact, lack of deadlines can extend the impact of issues because a \nserver consumes resources while it waits forever for an issue to be resolved.\nPicking a deadline can be difficult. If they’re too long, they can consume unnecessary \nresources for a calling service if a service is unresponsive. If they’re too short, they can cause \nhigher levels of failure for expensive requests. Figure 6.13 illustrates these constraints.\nFor many microservice applications, you set deadlines at the level of individual inter-\nactions; for example, a call from holdings to market-data may always have a deadline of \n10 seconds. A more elegant approach is to apply an absolute deadline across an entire \noperation and propagate the remaining time across collaborators.\nWithout propagating deadlines, it can be difficult to make them consistent across a \nrequest. For example, holdings could waste resources waiting for market-data far beyond \nthe overall deadline imposed by a higher level of the stack, such as an API gateway.\nImagine a chain of dependencies between multiple services. Each service takes a cer-\ntain amount of time to do its work and expects its collaborators to take some time. If any \nof those times vary, static expectations may no longer be correct (figure 6.14).\nIf your service interactions are over HTTP, you could propagate deadlines using a \ncustom HTTP header, such as X-Deadline: 1000, passing that value to set read timeout \nvalues on subsequent HTTP client calls. Many RPC frameworks, such as gRPC, explic-\nitly implement mechanisms for propagating deadlines within a request context.\n6.3.4\t\nCircuit breakers\nYou can combine some of the techniques we've discussed so far. You can consider an \ninteraction between holdings and market-data as analogous to an electrical circuit. In \nelectrical wiring, circuit breakers perform a protective role — preventing spikes in cur-\nrent from damaging a wider system. Similarly, a circuit breaker is a pattern for pausing \nrequests made to a failing service to prevent cascading failures.\nUnresponsive\nTypical response\nLengthy deadlines lead\nto wasted resource\nconsumption when\nfailure occurs.\nShort deadlines\ncontribute to failure if\nresponses often take\nlonger.\nShort deadline\nLong deadline\nTime\nFigure 6.13    Choosing the right deadline requires balancing time constraints to maximize the window of \nsuccessful requests.\n \n\n\n\t\n147\nDesigning reliable communication\nThere is no synchronization between\ndeadlines; service B will work beyond\nthe deadline that service A sets.\nIn normal operation, response\ntimes are within those\ndeadlines.\nDeadline\nAvg response\ntime\nDownstream services are\nunaware of upstream limits.\nServices set wait times for calls\nto their dependencies.\nD\nC\nB\nA\n5s\n4s\n10s\n2s\n1s\n10s\nFigure 6.14    Services may set expectations about how long they expect calls to collaborators to take; \nvarying widely because of failure or latency can exacerbate the impact of those failures.\nHow does it work? Two principles, both of which we touched on in the previous sec-\ntion, inform the design of a circuit breaker:\n1\t Remote communication should fail quickly in the event of an issue, rather than \nwasting resources waiting for a response that might never come.\n2\t If a dependency is failing consistently, it’s better to stop making further requests \nuntil that dependency recovers.\nWhen making a request to a service, you can track the number of times that request \nsucceeds or fails. You might track this number within each running instance of a ser-\nvice or share that state (using an external cache) across multiple services. In this nor-\nmal operation, we consider the circuit to be closed.\nIf the number of failures seen or the rate of failures within a certain time window \npasses a threshold, then the circuit is opened. Rather than attempting to send requests \nto your collaborating service, you should short-circuit requests and, where possible, \nperform an appropriate fallback — returning a stubbed message, routing to a different \nservice, or returning a cached response. Figure 6.15 illustrates the lifecycle of a request \nusing a circuit breaker.\nSetting the time window and threshold requires careful consideration of both \nthe expected reliability of the target service and the volume of interactions between \nservices. If requests are relatively sparse, then a circuit breaker may not be effective, \nbecause a large time window might be required to obtain a representative sample of \nrequests. For service interactions with contrasting busy and quiet periods, you may want \nto introduce a minimum throughput to ensure a circuit only reacts when load is statis-\ntically significant.\n \n\n\n148\nChapter 6  Designing reliable services\nRequest\nResponse\nFallback\nOpen\nCircuit open?\nOpen circuit\nYes\nYes\nNo\nMake request\nto service\nClosed\nSuccess?\nReport metrics\nThreshold\nexceeded?\nFigure 6.15    A circuit breaker controls the flow of requests between two services and opens when the \nnumber of failed requests surpasses a threshold.\nNOTE    You should monitor when circuits are opened and closed, as well as \npotentially alerting the team responsible, especially if the circuit is frequently \nopened. We’ll discuss this further in part 4.\nOnce the circuit has opened, you probably don’t want to leave it that way. When avail-\nability returns to normal, the circuit should be closed. The circuit breaker needs to \nsend a trial request to determine whether the connection has returned to a healthy \nstate. In this trial state, the circuit breaker is half open: if the call succeeds, the circuit \nwill be closed; otherwise, it will remain open. As with other retries, you should sched-\nule these attempts with an exponential back-off with jitter. Figure 6.16 shows the three \ndistinct states of a circuit breaker.\nSeveral libraries are available that provide implementations of the circuit breaker \npattern in different languages, such as Hystrix (Java), CB2 (Ruby), or Polly (.NET).\nTIP    Don’t forget that closed is the good state for a circuit breaker! The use of \nopen and closed to represent, respectively, negative and positive states may seem \ncounterintuitive but reflects the real-world behavior of an electrical circuit.\n \n\n\n\t\n149\nDesigning reliable communication\nSuccessful requests\nIn this state, requests fail quickly.\n1. If failure thresholds aren’t\nmet, the circuit stays closed.\nOpen\nHalf open\nClosed\n2. If the failure threshold\nis exceeded, the circuit\nis opened.\n3. After a delay,\nthe circuit attemps to close.\n5. If closing is successful,\nthe circuit returns to closed.\n4. If this fails,\nthe circuit returns to open.\nFigure 6.16    A circuit breaker transitions between three stages: open, closed, and half open.\n6.3.5\t\nAsynchronous communication\nSo far, we’ve focused on failure in synchronous, point-to-point communication \nbetween services. As we outlined in the first section, the more services in a chain, the \nlower overall availability you can guarantee for that path.\nDesigning asynchronous service interactions, using a communication broker like a \nmessage queue, is another technique you can use to maximize reliability. Figure 6.17 \nillustrates this approach.\nWhere you don’t need immediate, consistent responses, you can use this tech-\nnique to reduce the number of direct service interactions, in turn increasing overall \navailability — albeit at the expense of making business logic more complex. As we \nmentioned elsewhere in this book, a communication broker becomes a single point \nof failure that will require careful attention for you to scale, monitor, and operate \neffectively.\n \n\n\n150\nChapter 6  Designing reliable services\nEmits\nmessage\nConsumes\nmessage\nConsumes\nmessage\nMessage\nqueue\nService\nService\nService\nFigure 6.17    Using a message queue to decouple services from direct interaction\n6.4\t\nMaximizing service reliability\nIn the previous sections, we explored techniques to ensure a service can tolerate faults \nin interactions with its collaborators. Now, let’s consider how you can maximize avail-\nability and fault tolerance within an individual service. In this section, we’ll explore \ntwo techniques — health checks and rate limits — as well as methods for validating the \nresilience of services.\n6.4.1\t\nLoad balancing and service health\nIn production, you deploy multiple instances of your market-data service to ensure \nredundancy and horizontal scalability. A load balancer will distribute requests from \nother services between these instances. In this scenario, the load balancer plays two roles:\n1\t Identifying which underlying instances are healthy and able to serve requests\n2\t Routing requests to different underlying instances of the service\nA load balancer is responsible for executing or consuming the results of health checks. \nIn the previous section, you could ascertain the health of a dependency at the point \nof interaction — when requests were being made. But that’s not entirely adequate. You \nshould have some way of understanding the application’s readiness to serve requests at \nany time, rather than when it’s actively being queried.\nEvery service you design and deploy should implement an appropriate health check. If \na service instance becomes unhealthy, that instance should no longer receive traffic from \nother services. For synchronous RPC-facing services, a load balancer will typically query \neach instance’s health check endpoint on an interval basis. Similarly, asynchronous services \nmay use a heartbeat mechanism to test connectivity between the queue and consumers.\nTIP    It’s often desirable for repeated or systematic instance failures, as detected \nby health checks, to trigger alerts to an operations team — a little human inter-\nvention can be helpful. We’ll explore that further in part 4 of this book.\n \n\n\n\t\n151\nMaximizing service reliability\nYou can classify health checks based on two criteria: liveness and readiness. A live-\nness check is typically a simple check that the application has started and is running \ncorrectly. For example, an HTTP service should expose an endpoint — commonly \n/health, /ping, or /heartbeat — that returns a 200 OK response once the service is \nrunning (figure 6.18). If an instance is unresponsive, or returns an error message, the \nload balancer will no longer deliver requests there.\nIn contrast, a readiness check indicates whether a service is ready to serve traffic, \nbecause being alive may still not indicate that requests will be successful. A service might \nhave many dependencies — databases, third-party services, configuration, caches — so \nyou can use a readiness check to see if these constituent components are in the correct \nstate to serve requests. Both of the example services implement a simple HTTP liveness \ncheck, as shown in the following listing.\nListing 6.6    Flask handler for an HTTP liveness check\n@app.route('/ping', methods=[\"GET\"])\ndef ping():\n    return 'OK'\nHealth checks are binary: either an instance is available or it isn’t. This works well with \ntypical round-robin load balancing, where requests are distributed to each replica in \nturn. But in some circumstances the functioning of a service may be degraded and \nexhibit increased latency or error rates without a health check reflecting this status. \nAs such, it can be beneficial to use load balancers that are aware of latency and able \nto route requests to instances that are performing better, or those that are under less \nload, to achieve more consistent service behavior. This is a typical feature of a microser-\nvice proxy, which we’ll touch on later in this chapter.\nLB\n4xx/5xx\n200\n200\nHoldings\nHoldings\nLB\n200\nHoldings\nHoldings\nFigure 6.18    Load balancers continuously query service instances to check their health. If an instance \nis unhealthy, the load balancer will no longer route requests to that instance until it recovers.\n \n\n\n152\nChapter 6  Designing reliable services\n6.4.2\t\nRate limits\nUnhealthy service usage patterns can sometimes arise in large microservice appli-\ncations. Upstream dependencies might make several calls, where a single batch call \nwould be more appropriate, or available resources may not be distributed equitably \namong all callers. Similarly, a service with third-party dependencies could be limited by \nrestrictions that those dependencies impose.\nAn appropriate solution is to explicitly limit the rate of requests or total requests \navailable to collaborating services in a timeframe. This helps to ensure that a service —  \nparticularly when it has many collaborators — isn’t overloaded. The limiting might be \nindiscriminate (drop all requests above a certain volume) or more sophisticated (drop \nrequests from infrequent service clients, prioritize requests for critical endpoints, and \ndrop low-priority requests). Table 6.4 outlines different rate-limiting strategies.\nTable 6.4    Common rate-limiting strategies\nStrategy\nDescription\nDrop requests above volume\nDrop consumer requests above a specified volume\nPrioritize critical traffic\nDrop requests to low-priority endpoints to prioritize resources for critical \ntraffic\nDrop uncommon clients\nPrioritize frequent consumers of the service over infrequent users\nLimit concurrent requests\nLimit the overall number of requests an upstream service can make over \na time period\nRate limits can be shared with a service’s clients at design time or, better, at runtime. A \nservice might return a header to a consumer that indicates the remaining volume of \nrequests available. On receipt, the upstream collaborator should take this into account \nand adjust its rate of outbound requests. This technique is known as back pressure.\n6.4.3\t\nValidating reliability and fault tolerance\nApplying the tactics and patterns we’ve covered will put you on a good path toward \nmaximizing availability. But it’s not enough to plan and design for resiliency: you need \nto validate that your services can tolerate faults and recover gracefully.\nThorough testing provides assurance that your chosen design is effective when \nboth predicted and unpredictable failures occur. Testing requires the application of \nload testing and chaos testing. Although it’s likely you’re familiar with code testing — such \nas unit and acceptance testing to validate implementation, usually in a controlled \nenviron­ment — you might not know that load and chaos testing are intended to val-\nidate service limits by closely replicating the turbulence of production operation. \nAlthough testing isn’t the primary focus of this book, it’s useful to understand how \nthese different testing techniques can help you build a robust microservice application.\n \n\n\n\t\n153\nMaximizing service reliability\nLoad testing\nAs a service developer, you can usually be confident that the number of requests made \nto your service will increase over time. When developing a service, you should\n1\t Model the expected growth and shape of service traffic to ensure that you under-\nstand the likely usage of your service\n2\t Estimate the capacity required to service that traffic\n3\t Validate the deployed capacity of the service by load testing against those limits\n4\t Use business and service metrics as appropriate to re-estimate capacity\nImagine you’re considering how much capacity the market-data service requires. First, \nwhat do you know about the service’s usage patterns? You know that holdings queries \nthe service, but it may be called from elsewhere too — pricing data is used throughout \nSimpleBank’s product.\nLet’s assume that queries to market-data grow roughly in line with the number of \nactive users on the platform, but you may experience spikes (for example, when the \nmarket opens in the morning). You can plan capacity based on predictions of your busi-\nness growth. Table 6.5 outlines a simple estimation of the QPS that you can expect this \nservice to receive over a three-month period.\nTable 6.5    Estimate of calls to a service per second based on growth in average active users over a three-\nmonth period\nJun\nJul\nAug\nTotal Users\n4000\n5600\n7840\nExpected Growth\n40%\n40%\n40%\nActive Users\nAverage\n20%\n800\n1120\n1568\nPeak\n70%\n2800\n3920\n5488\nService Calls\nAverage\nPer User/Minute\n30\n24000\n33600\n47040\nPer User/Second\n0.5\n400\n560\n784\nPeak\nPer User/Minute\n30\n84000\n117600\n164640\nPer User/Second\n0.5\n1400\n1960\n2744\nIdentifying the qualitative factors that drive growth in service utilization is vital to good \ndesign and optimizing capacity. Once you’ve done that, you can determine how much \n \n",
      "page_number": 164
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 172-179)",
      "start_page": 172,
      "end_page": 179,
      "detection_method": "topic_boundary",
      "content": "154\nChapter 6  Designing reliable services\ncapacity to deploy. For example, the table suggests you need to be able to service 400 \nrequests per second in normal operation, growing by 40% month on month, with \nspikes in peak usage to 1,400 requests per second.\nTIP    An in-depth review of capacity and scale planning techniques is outside the \nscope of this book, but a great overview is available in Abbott and Fisher’s The \nArt of Scalability (Addison-Wesley Professional, 2015) (ISBN 978-0134032801).\nOnce you’ve established a baseline capacity for your service, you can then iteratively \ntest that capacity against expected traffic patterns. Along with validating the traffic lim-\nits of a microservice configuration, load testing can identify potential bottlenecks or \ndesign flaws that aren’t apparent at lower levels of load. Load testing can provide you \nwith highly effective insight into the limitations of your services.\nAt the level of individual services, you should automate the load testing of each ser-\nvice as part of its delivery pipeline — something we’ll explore in part 3 of this book. \nAlong with this systematic load testing, you should perform exploratory load testing to \nidentify limits and test your assumptions about the load that services can handle.\nYou also should load test services together. This can aid in identifying unusual load \npatterns and bottlenecks based on service interaction. For example, you could write a \nload test that exercises all the services in the GET /holdings example.\nChaos testing\nMany failures in a microservice application don’t arise from within the microservices \nthemselves. Networks fail, virtual machines fail, databases become unresponsive — failure \nis everywhere! To test for these types of failure scenarios, you need to apply chaos testing.\nChaos testing pushes your microservice application to fail in production. By intro-\nducing instability and failure, it accurately mimics real system failures, as well as train-\ning an engineering team to be able to react to those failures. This should ultimately \nbuild your confidence in the system’s capability to withstand real chaos because you’ll \nbe gradually improving the resiliency of your system and reducing the possible number \nof events that would cause operational impact.\nAs explained on the “Principles of Chaos Engineering” website (https://principlesof \nchaos.org/), you can think of chaos testing as “the facilitation of experiments to \nuncover systemic weaknesses.” The website lays out this approach:\n1\t Define a measurable steady state of normal system operation.\n2\t Hypothesize that behavior in an experimental and control group will remain \nsteady; the system will be resilient to the failure introduced.\n3\t Introduce variables that reflect real-world failure events — for example, removing \nservers, severing network connections, or introducing higher levels of latency.\n4\t Attempt to disprove the hypothesis you defined in (2).\n \n\n\n\t\n155\nMaximizing service reliability\nRecall how the holdings, transactions, and market-data services were deployed in fig-\nure 6.5. In this case, you expect steady operation to return holdings data within a rea-\nsonable response time. A chaos test could introduce several variables:\n1\t Killing nodes running market-data or transactions, either partially or completely\n2\t Reducing capacity by killing holdings instances at random\n3\t Severing the network connection — for example, between holdings and down-\nstream services or between services and their data stores\nFigure 6.19 illustrates these options.\nCompanies with mature chaos testing practices might even perform testing on both \na systematic and random basis against live production environments. This might sound \nterrifying; real outages can be stressful enough, let alone actively working to make them \nhappen. But without taking this approach, it’s incredibly difficult to know that your \nsystem is truly resilient in the ways that you expect. In any organization, you should \nstart small, by introducing a limited set of possible failures, or only running scheduled, \nrather than random, tests. Although you can also perform chaos tests in a staging envi-\nronment, you’ll need to carefully consider whether that environment is truly represen-\ntative of or equivalent to your production configuration.\nTIP    Chaos Toolkit (http://chaostoolkit.org/) is a great tool to start with if \nyou’d like to practice chaos engineering techniques.\nUltimately, by regularly and systematically validating your system against chaotic events \nand resolving the issues you encounter, you and your team will be able to achieve a sig-\nnificant level of confidence in your application’s resilience to failure.\nUI\nLB\nLB\nSever or throttle\nnetwork\nconnections\nMarket-data\nTransactions\nRemove\ninstances to\nreduce capacity\nData store\nDisable\ncomponents\nData store\nLB\nHoldings\nFigure 6.19    Potential variables to introduce in a chaos test to reflect real-world failure events\n \n\n\n156\nChapter 6  Designing reliable services\n6.5\t\nSafety by default\nCritical paths in your microservice application will only be as resilient and available as \ntheir weakest link. Given the impact that individual services can have overall availabil-\nity, it’s imperative to avoid emergencies where introducing new services or changes in \na service dependency chain significantly degrade that measure. Likewise, you don’t \nwant to find out that crucial functionality can’t tolerate faults when that fault happens.\nWhen applications are technically heterogeneous, or distinct teams deliver underly-\ning services, it can be exceptionally difficult to maintain consistent approaches to reli-\nable interaction. We touched on this back in chapter 2 when we discussed isolation and \ntechnical divergence. Teams are under different delivery pressures and different ser-\nvices have different needs — at worst, developers might forget to follow good resiliency \npractices.\nAny change in service topology can have a negative impact. Figure 6.20 illustrates \ntwo examples: adding a new collaborator downstream from market-data might decrease \nmarket-data’s availability, whereas adding a new consumer might reduce the overall \ncapacity of the market-data service, reducing service for existing consumers.\nFrameworks and proxies are two different technical approaches to applying com-\nmunication standards across multiple services that make it easy for engineers to fall \ninto doing the right thing by ensuring services communicate resiliently and safely by \ndefault.\n6.5.1\t\nFrameworks\nA common approach for ensuring services always communicate appropriately is to \nmandate the use of specific libraries implementing common interaction patterns like \ncircuit breakers, retries, and fallbacks. Standardizing these interactions across all ser-\nvices using a library has the following advantages:\n1\t Increases the overall reliability of your application by avoiding roll-your-own \napproaches to service interaction\n2\t Simplifies the process of rolling out improvements or optimizations to communi-\ncation across any number of services\n3\t Clearly and consistently distinguishes network calls from local calls within code\n4\t Can be extended to provide supporting functionality, such as collecting metrics \non service interactions\nThis approach tends to be more effective when a company uses one language (or few \nlanguages) for writing code; for example, Hystrix, which we mentioned earlier, was \nintended to provide a standardized way — across all Java-based services in Netflix’s \norganization — of controlling interactions between distributed services.\nNOTE    Standardizing communication is a crucial element of building a \nmicroservice chassis, which we’ll explore in the next chapter.\n \n\n\n\t\n157\nSafety by default\nHoldings\nMarket-data\nNew\nconsumer\nIncreased load might\nnegatively impact existing\nservice availability.\nA new dependency can\nnegatively impact availability.\nNew\ncollaborator\nFigure 6.20    Availability impact of new services in a dependency chain\n6.5.2\t\nService mesh\nAlternatively, you could introduce a service mesh, such as Linkerd (https://linkerd.io) \nor Envoy (www.envoyproxy.io), between your services to control retries, fallbacks, and \ncircuit breakers, rather than making this behavior part of each individual service. A \nservice mesh acts as a proxy. Figure 6.21 illustrates how a service mesh handles commu-\nnication between services.\nInstead of services communicating directly with other services, service communica-\ntion passes through the service mesh application, typically deployed as a separate pro-\ncess on the same host as the service. You then can configure the proxy to manage that \ntraffic appropriately — retrying requests, managing timeouts, or balancing load across \ndifferent services. From the caller’s perspective, the mesh doesn’t exist — it makes \nHTTP or RPC calls to another service as normal.\nHost\nProxy\nA\nHost\nLogical communication\nProxied communication\nProxy\nService proxies\nroute the actual\nrequest; they abstract\nthis detail away\nfrom the services.\nService A makes a request\nto service B.\nB\nFigure 6.21    Communication between services using a service mesh\n \n\n\n158\nChapter 6  Designing reliable services\nAlthough this may make the treatment of service interaction less explicit to an engi-\nneer working on a service, it can simplify defensive communication in applications \nthat are heterogeneous. Otherwise, consistent communication can require significant \ntime investment to achieve across different languages, because ecosystems and librar-\nies may have unequal capabilities or support for resiliency features.\nSummary\n¡ Failure is inevitable in complex distributed systems — you have to consider fault \ntolerance when you’re designing them.\n¡ The availability of individual services affects the availability of the wider \napplication.\n¡ Choosing the right level of risk mitigation for an application requires careful \nconsideration of the frequency and impact of failure versus the cost of mitigating \nagainst potentially rare events.\n¡ Most failures occur in one of four areas: hardware, communication, dependen-\ncies, or internally.\n¡ Cascading failures result from positive feedback and are a common failure mode \nin a microservice application. They’re most commonly caused by server overload.\n¡ You can use retries and deadlines to mitigate against faults in service interactions. \nYou need to apply retries carefully to avoid exacerbating failure in other services.\n¡ You can use fallbacks — such as caching, alternative services, and default \nresults — to return successful responses, even when service dependencies fail.\n¡ You should propagate deadlines between services to ensure they’re consistent \nacross a system and to minimize wasted work.\n¡ Circuit breakers between services protect against cascading failures by failing \nquickly when a high threshold of errors is encountered.\n¡ Services can use rate limits to protect themselves from spikes in load beyond their \ncapacity to service.\n¡ Individual services should expose health checks for load balancers and monitor-\ning to be able to use.\n¡ You can effectively validate resiliency by practicing both load and chaos testing.\n¡ You can apply standards — whether through proxies or frameworks — to help \nengineers “fall into the pit of success” and build services that tolerate faults by \ndefault.\n \n\n\n159\n7\n Building a reusable \nmicroservice framework\nThis chapter covers\n¡ Building a microservice chassis\n¡ Advantages of enforcing uniform practices \nacross teams\n¡ Abstracting common concerns in a reusable \nframework\nOnce an organization fully embraces microservices and teams grow in number, it’s \nquite likely that each of those teams will start specializing in a given set of program-\nming languages and tools. Sometimes, even when using the same programming lan-\nguage, different teams will choose a different combination of tools to achieve the \nsame purpose. Although nothing is wrong with this, it may lead to an increased \nchallenge for engineers moving between different teams. The ritual to set up new \nservices, as well as the code structure, may be quite different. Even if teams eventu-\nally end up solving the same challenges in different ways, we believe this potential \nduplication is better than having to add a synchronization layer.\n \n\n\n160\nChapter 7   Building a reusable microservice framework \nHaving strict rules on the tools and languages that teams can use and enforcing a \ncanonical way of setting up new services across all teams may harm speed and inno-\nvation and will eventually lead to the use of the same tools for every problem. Fortu-\nnately, you can enforce some common practices while keeping things rather free for \nteams to choose the programming language for specific services. You can encapsu-\nlate a set of tools for each adopted language while making sure that engineers have \naccess to resources that’ll make it easy to abide by the practices across all teams. If \nteam A decides to go with Elixir to create a service for managing notifications and \nteam B decides to use Python for an image analysis service, they should both have the \ntools that allow those two services to emit metrics to the common metrics collection \ninfrastructure.\nYou should centralize logs in the same place and with the same format, and things \nlike circuit breakers, feature flags, or the ability to share the same event bus should be \navailable. That way, teams can make choices but also have the tools to become aligned \nwith the infrastructure available to run their services. These tools form a chassis, a foun-\ndation, that you can build new services on without much up-front investigation and \nceremony. Let's consider how to build a chassis for your services—one that abstracts \ncommon concerns and architectural choices while at the same time enables teams to \nquickly bootstrap new services.\n7.1\t\nA microservice chassis\nImagine an organization has eight different engineering teams and four engineers on \neach team. Now imagine one engineer on each team is responsible for bootstrapping a \nnew service in Python, Java, or C#. Those languages, like most mainstream languages, \nhave a lot of options in the form of available libraries. From http clients to logging \nlibraries, the choice is plentiful. What would be the odds of two teams selecting the \nsame language ending up with the same combination of libraries? I’d say pretty nar-\nrow! This issue isn’t exclusive to microservice applications; for a monolithic applica-\ntion I worked on, different programmers were using three distinct http client libraries!\nIn figures 7.1 to 7.3, you can see the choices a team may face while choosing compo-\nnents to use in a new project.\nAs you can see in figures 7.1–7.3, the choice isn’t easy! No matter which language \nyou choose, options are plentiful, so the time you take to select components can \nincrease, along with the risk of picking up a less than ideal library. An organization \nmost likely will settle with two or three languages as the widely adopted ones, depend-\ning on the problems they need to solve. As a result, teams using the same language will \ncoexist. Once one team gains some experience with a set of libraries, why wouldn’t you \nuse that experience to the benefit of other teams? You can provide a set of libraries and \ntools already used in production that people bootstrapping new projects could choose \nfrom without the burden of having to dig deeply into each library to weigh the pros \nand cons.\n \n\n\n\t\n161\nA microservice chassis\n \nFigure 7.1    Search results for object-relational mapping (ORM) libraries for the .NET ecosystem\nTo make the job easier for your teams to create new services, it’s worth your while to \nprovide basic structure and a set of vetted tools for each of the languages your orga-\nnization uses to build and operate services. You also should make sure that structure \nabides with your standards regarding observability and the abstraction of infrastruc-\nture-related code and it reflects your architectural choices regarding communication \nbetween services. An example of this, if the organization favors asynchronous commu-\nnication between services, would be providing the needed libraries for using an event \nbus infrastructure that’s already in place.\nNot only would you be able to soft-enforce some practices, you also could make it eas-\nier to spawn new services quickly and allow fast prototyping. After all, it wouldn't make \nsense to take longer to bootstrap a service than to write the business logic that powers it.\n \n",
      "page_number": 172
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 180-188)",
      "start_page": 180,
      "end_page": 188,
      "detection_method": "topic_boundary",
      "content": "162\nChapter 7   Building a reusable microservice framework \nFigure 7.2    Search for Advanced Message Queuing Protocol (AMQP) libraries for the Java ecosystem\nFigure 7.3. Search for circuit breaker libraries for Python\n \n\n\n\t\n163\nWhat’s the purpose of a microservice chassis?\nThe chassis structure allows teams to select a tech stack (language + libraries) and \nquickly set up a service. You might ask yourself: how hard is it to bootstrap a service \nwithout this so-called chassis? It can be easy, if you don’t have concerns like\n¡ Enabling deployments in the container scheduler from day one (CI/CD)\n¡ Setting up log aggregation\n¡ Collecting metrics\n¡ Having a mechanism for synchronous and asynchronous communication\n¡ Error reporting\nAt SimpleBank, no matter what programming language or tech stack a team chooses, \nservices should be providing all the functionality described in the list above. This type \nof setup isn’t trivial to achieve, and, depending on the stack you chose, it can take more \nthan a day to set up. Also, the combination of libraries two teams would choose for the \nsame purpose could be quite different. You mitigate any issues related to that differ-\nence by providing a microservice chassis, so each team can focus on delivering features \nthat SimpleBank customers will be using.\n7.2\t\nWhat’s the purpose of a microservice chassis?\nThe purpose of a microservice chassis is to allow you to make services easier to create \nwhile ensuring you have a set of standards that all services abide by, no matter which \nteam owns a service. Let’s look into some of the advantages of having a microservices \nchassis in place:\n¡ Making it easier to onboard team members\n¡ Getting a good understanding of the code structure and concerns regarding the \ntech stack that an engineering team uses\n¡ Limiting the scope of experimentation for production systems as the team builds \ncommon knowledge, even if not always in the same tech stack\n¡ Helping to adhere to best practices\nHaving a predictable code structure and commonly used libraries will make it easier \nfor team members to quickly understand a service’s implementation. They’ll only \nneed to bother with the business logic implementation, because any other code will be \npretty much common throughout all services. For example, common code will include \ncode to deal with or configure\n¡ Logging\n¡ Configuration fetching\n¡ Metrics collection\n¡ Data store setup\n¡ Health checks\n¡ Service registry and discovery\n¡ The chosen transport-related boilerplate (AMQP, HTTP)\n \n\n\n164\nChapter 7   Building a reusable microservice framework \nIf common code has already taken care of those concerns when someone is creating a \nnew service, the need to write boilerplate is reduced or eliminated, and developers will \nless likely have to reinvent the wheel. Good practices within the organization will also \nbe easier to enforce.\nFrom a knowledge sharing perspective, having a microservice chassis will also enable \neasy code reviews by members of different teams. If they’re using the same chassis, \nthey’ll be familiar with the code structure and how things are done. This will increase \nvisibility and allow you to gather opinions from engineers from other teams. It’s always \ndesirable to have a different view on the problems a specific team is working on solving.\n7.2.1\t\nReduced risk\nBy providing a microservice chassis, you reduce the risk you face, because you’ll have \nless of a chance of picking a combination of language and libraries that won’t work \nfor a particular need. Imagine a service you’re creating needs to fully communicate \nasynchronously with other services using an already existing event bus. If your chassis \nalready covers that use case, you’re not likely to end up with a setup that you need to \ntweak and eventually won’t work well. You can cover that asynchronous communica-\ntion use case as well as the synchronous one so you don’t need to expend further effort \nto find a working solution.\nThe chassis can be constantly evolving to incorporate the findings of different teams, \nallowing you to be always up to date with the organization’s practices and experience \ndealing with multiple use cases. All in all, there will be less chance for a team to face a \nchallenge that other teams haven’t solved before. And in case no one has solved that \ntype of challenge yet, only one team needs to solve it; then you can incorporate the solu-\ntion into the chassis, reducing the risks other teams have to take in the future.\nHaving a microservice chassis that already selects a set of libraries for use will limit the \nmanagement of dependencies an engineering team will have to deal with. Referring to \nfigures 7.1 to 7.3, if you have available one ORM, one AMQP, and one circuit breaker \nlibrary, those will eventually be well known across multiple teams, and if someone finds \na vulnerability in any of those libraries, you’ll be able to update them with ease.\n7.2.2\t\nFaster bootstrapping\nIt makes little sense to spend one or two days bootstrapping a service when it could \ntake far less time to implement the business logic. Also, wiring the needed components \nthat form a service is a repetitive task that can be error prone. Why make people have \nto go and set up components all over again every time they create a new service? Using, \nmaintaining, and updating a microservice chassis will lead to a setup that’s sound, \ntested, and reusable. This will allow for faster service bootstrapping. Then you could \nuse the extra time you gained by not having to write boilerplate code to develop, test, \nand deploy your features.\n \n\n\n\t\n165\nDesigning a chassis\nHaving a sound foundation that teams use widely and know well allows you to exper-\niment a lot more without worrying too much about the initial effort. If you can quickly \nturn a concept into a deployable service, you can easily validate it and decide to proceed \nwith it or abandon it altogether. The key notion here is to be fast and to have it as easy \nas possible to create new functionality. Having a chassis in place also can significantly \nlower the entry barrier for new team members, because it’ll be quicker for them to \njump into any project once they learn the structure that’s common to all services in \neach language.\n7.3\t\nDesigning a chassis\nAt SimpleBank, the team responsible for implementing the purchasing and selling of \nstocks decided to create a chassis for the wider engineering team to use—they had \nfaced a couple of challenges and want to share their experiences. We described a fea-\nture for selling stocks in chapter 2, figure 2.7. Let’s look at a flow diagram to better \nunderstand it (figure 7.4).\nTo sell stocks, a user issues a request via the web or a mobile application. An API gate-\nway will pick up the request and will act as the interface between the user-facing applica-\ntion and all internal services that’ll collaborate to provide the functionality.\nUser\nGateway\nPlace sell order\n(http)\nPlace sell order\n(rpc)\nPlaceOrder\n(http external)\nOrderCreated\n(trigger event)\n1\n2\n3\n4\n7\n7\n6\n5\n5\nConsume OrderPlaced event\nConsume OrderCreated event\nConsume OrderPlaced event\nOrderPlaced\n(trigger event)\nRespond to sell order\n(http)\nRespond to sell order\n(rpc)\nRequest reservation\n(rpc)\nRecord order details\nUpdate order to “placed”\nRecord reservation\nCharge fee\nOrders service\nEvent queue\nAccounts service\nMarket service\nFees service\nStock exchange\nFigure 7.4    The flow for selling stocks involves both synchronous and asynchronous communication \nbetween the intervening services.\n \n\n\n166\nChapter 7   Building a reusable microservice framework \nGiven that it can take a while to place the order to the stock exchange, most operations \nwill be asynchronous, and you’ll return a message to the client indicating their request \nwill be processed as soon as possible. Let’s look into the interactions between services \nand the type of communication:\n1\t The gateway passes the user request to the orders service.\n2\t The orders service sends an OrderCreated event to the event queue.\n3\t The orders service requests the reservation of a stock position to the account \ntransaction service.\n4\t The orders service replies to the initial call from the gateway, then the gateway \ninforms the user that the order is being processed.\n5\t The market service consumes the OrderCreated event and places the order to \nthe stock exchange.\n6\t The market service emits an OrderPlaced event to the event queue.\n7\t Both the fees service and the orders service consume the OrderPlaced event; \nthey then charge the fees for the operation and update the status of the order to \n“placed,” respectively.\nFor this feature, you have four internal services collaborating, interactions with an \nexternal entity (stock exchange), and communication that’s a mix of synchronous and \nasynchronous. The use of the event queue allows other systems to react to changes; \nfor instance, a service responsible for emailing or real-time notifications to clients can \neasily consume the OrderPlaced event, allowing it to send notifications of the placed \norder.\nGiven that the team owning this feature was comfortable with using Python, they cre-\nated the initial prototype using the nameko framework (https://github.com/nameko/\nnameko). This framework offers, out of the box, a few things:\n¡ AMQP RPC and events (pub-sub)\n¡ HTTP GET, POST, and websockets\n¡ CLI for easy and rapid development\n¡ Utilities for unit and integration testing\nBut a few things were missing, like circuit breakers, error reporting, feature flags, and \nemitting metrics, so the team decided to create a code repository with libraries to take \ncare of those concerns. They also created a Dockerfile and Docker compose file to \nallow building and running the feature with minimum effort and to offer a base for \nother teams to use when developing in Python. The code for the initial Python chassis \n(http://mng.bz/s4B2) and for the described feature (http://mng.bz/D19l) is avail-\nable at the book code repository.\nWe’ll now look with more detail at how the built chassis deals with service discovery, \nobservability, transport, and balancing and limiting.\n \n\n\n\t\n167\nDesigning a chassis\n7.3.1\t\nService discovery\nService discovery for the Python chassis that emerged from implementing the feature \nwe previously described is quite simple. The communication between the services \ninvolved occurs either synchronously via RPC calls or asynchronously by publishing \nevents. SimpleBank uses RabbitMQ (www.rabbitmq.com) as the message broker, so this \nindirectly provides a way of registering services for both the asynchronous and synchro-\nnous use case. RabbitMQ allows the use of synchronous request/response communica-\ntion implementing RPC over queues, and it’ll also load balance the consumers using \na round-robin algorithm (https://en.wikipedia.org/wiki/Round-robin_scheduling) by \ndefault. This allows you to use the messaging infrastructure to register services as well \nas to automatically distribute load between multiple instances of the same service. Fig-\nure 7.5 shows the RPC exchange your different services connect to.\nFigure 7.5    Services communicating via RPC register in an exchange. Multiple instances for a given \nservice use the same routing key, and RabbitMQ will route the incoming requests between those \ninstances.\n \n\n\n168\nChapter 7   Building a reusable microservice framework \nAll running services register themselves in this exchange. This will allow for them to \ncommunicate seamlessly without the need for each one to know explicitly where any \nservice is located. This is also the case for RPC communication over the AMQP proto-\ncol, which allows you to have the same request/response behavior you’d get by using \nHTTP.\nLet’s take a look on how easy it is to have this feature available to you by using the \ncapacities that the chassis provides, in this case by using the nameko framework, as \nshown in the following listing.\nListing 7.1    microservices-in-action/chapter-7/chassis/rpc_demo.py\nfrom nameko.rpc import rpc, RpcProxy\nclass RpcResponderDemoService:\n    name = \"rpc_responder_demo_service\" \n    @rpc \n    def hello(self, name):\n        return \"Hello, {}!\".format(name)\nclass RpcCallerDemoService:\n    name = \"rpc_caller_demo_service”\n    remote = RpcProxy(\"rpc_responder_demo_service\") \n    @rpc\n    def remote_hello(self, value=\"John Doe\"):\n        res = u\"{}\".format(value)\n        return self.remote.hello(res)\nIn this example, we’ve defined two classes, a responder and a caller. In each class, we \nalso defined a name variable that holds the identifier for the service. Use of the @rpc \nannotation will decorate the function. This decoration will allow you to transform what \nseems an ordinary function into something that’ll make use of the underlying AMQP \ninfrastructure (that RabbitMQ offers) to invoke a method in a service running else-\nwhere. Calling the remote_hello method from the RpcCallerDemoService class will \nresult in invoking the hello function in the RpcResponderDemoService, because that \nservice is registered as remote via a RpcProxy that the framework provides.\nOnce you run this example code, RabbitMQ will display something like figure 7.6.\nIn Figure  7.6, you can observe that once you boot the services that rpc_demo.py \ndefines, each one registers in a queue scoped to the service name: rpc-rpc_caller_\ndemo_service and rpc-rpc_responder_demo_service. Two other queues—rpc.\nreply-rpc_caller_demo_service* and rpc.reply-standalone_rpc_proxy*—also \nappear, and they’ll relay back the responses to the caller service. This is a way of imple-\nmenting blocking synchronous communication in RabbitMQ (http://mng.bz/4blSh).\nAssigns the service name a variable—This \nis the name that a particular service \nregisters to allow others to call it.\nAllows nameko to set up the RabbitMQ queues \nnecessary to perform a request/response type of \ncall—The rpc call will behave synchronously.\nCreates an RPC Proxy for service \nthat’ll be invoked via RPC—You pass \nthe name of the remote service.\nCalls the remote service via the RpcProxy—This will execute  \nthe hello function on the RpcResponderDemoService class.\n \n\n\n\t\n169\nDesigning a chassis\nFigure 7.6    Caller and responder demo services registered in RabbitMQ queues\nYour chassis makes it super easy to access this functionality so you can use the same \ninfrastructure for both synchronous and asynchronous communication between ser-\nvices. This setup brings you huge speed gains while prototyping solutions, because the \nteam can spend its time developing new features instead of having to build all the \nfunctionality from scratch. If you opt for an orchestrated behavior, with blocking calls \nbetween services, a choreographed behavior where all communication is asynchro-\nnous, or a mix between the two, you can use the same infrastructure and library.\nThe following listing shows an example on how to use full asynchronous communi-\ncation between services by using the functionality of the chassis.\nListing 7.2    microservices-in-action/chapter-7/chassis/events_demo.py\nfrom nameko.events import EventDispatcher, event_handler\nfrom nameko.rpc import rpc\nfrom nameko.timer import timer\nclass EventPublisherService:\n    name = \"publisher_service\" \n    dispatch = EventDispatcher()\n    @rpc\n    def publish(self, event_type, payload):\n        self.dispatch(event_type, payload)\nclass AnEventListenerService:\nRegisters the service name, which allows \nyou to refer to it on other services\nAllows this service to create events \nthat’ll be routed to a queue in RabbitMQ\n \n\n\n170\nChapter 7   Building a reusable microservice framework \n    name = \"an_event_listener_service\" \n    @event_handler(\"publisher_service\", \"an_event\") \n    def consume_an_event(self, payload):\n        print(\"service {} received:\".format(self.name), payload)\nclass AnotherEventListenerService:\n    name = \"another_event_listener_service\"\n    @event_handler(\"publisher_service\", \"another_event\")\n    def consume_another_event(self, payload):\n        print(\"service {} received:\".format(self.name), payload)\nclass ListenBothEventsService:\n    name = \"listen_both_events_service\" \n    @event_handler(\"publisher_service\", \"an_event\") \n    def consume_an_event(self, payload):\n        print(\"service {} received:\".format(self.name), payload)\n    @event_handler(\"publisher_service\", \"another_event\") \n    def consume_another_event(self, payload):\n        print(\"service {} received:\".format(self.name), payload)\nAs with the previous code example, each service a Python class implements declares a name \nvariable that the framework will use to set up the underlying queues that allow communi-\ncation. When running the services that each class in this file defines, RabbitMQ will create \nfour queues, one for each service. As you can see in figure 7.7, the publisher service reg-\nisters an RPC queue, without reply queue setup, contrary to the previous example that \nfigure 7.6 illustrated. The other listener services register a queue per consumed event.\nFigure 7.7    The queues that RabbitMQ creates when you run the services defined in events_demo.py\nRegisters the service name, which allows \nyou to refer to it on other services\nBy using this annotation, ListenBothEventsService will execute \nthe function when the publisher service issues an event. The \nfirst argument of the annotation is the name of the service \nwhose events will be listened to, and the second argument of \nthe annotation is the name of the event.\n \n",
      "page_number": 180
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 189-197)",
      "start_page": 189,
      "end_page": 197,
      "detection_method": "topic_boundary",
      "content": "\t\n171\nDesigning a chassis\nThe team chose nameko to be part of the microservice chassis because it makes it easy \nto abstract from the details of implementing and setting up these two types of commu-\nnication over the existing message broker. In section 7.3.3, we’ll also look into another \nadvantage that comes out of the box, because the message broker also takes care of \nload balancing.\n7.3.2\t\nObservability\nTo operate and maintain services, you need to be aware of what’s going on in produc-\ntion at all times. As a result, you’ll want the services to emit metrics to reflect the way \nthey’re operating, report errors, and aggregate logs in a usable format. In part 4 of the \nbook, we’ll focus on all these topics in more detail. But for now, let’s keep in mind that \nservices should address these concerns from day one. Operating and maintaining ser-\nvices is as important as writing them in the first place, and, in most cases, they’ll spend \na lot more time running than being developed.\nYour microservice chassis has the dependencies shown in the following listing.\nListing 7.3    microservices-in-action/chapter-7/chassis/setup.py\n(…)\n    keywords='microservices chassis development',\n    packages=find_packages(exclude=['contrib', 'docs', 'tests']),\n    install_requires=[\n        'nameko>=2.6.0',\n        'statsd>=3.2.1', \n        'nameko-sentry>=0.0.5', \n        'logstash_formatter>=0.5.16', \n        'circuitbreaker>=1.0.1',\n        'gutter>=0.5.0',\n        'request-id>=0.2.1',\n    ],\n(…)\nFrom the seven declared dependencies, you use three of them for observability pur-\nposes. These libraries will allow you to collect metrics, report errors, and gather some \ncontextual information around them and to adapt your logging to the format you use \nin all services deployed at SimpleBank.\nLibrary to emit metrics in StatsD format\nLibrary to Integrate with Sentry error reporting\nLibrary to format the logs in logstash format\n \n\n\n172\nChapter 7   Building a reusable microservice framework \nMetrics\nLet’s start with metrics collection and the use of StatsD.1 Etsy originally developed \nStatsD as a way to aggregate application metrics. It quickly became so popular that \nit’s now the de facto protocol to collect application metrics with clients in multiple pro-\ngramming languages. To be able to use StatsD, you need to instrument your code to \ncapture all metrics you find relevant. Then a client library, in your case statsd for \nPython, will collect those metrics and send them to an agent that listens to UDP traffic \nfrom client libraries, aggregates the data, and periodically sends it to a monitoring \nsystem. Both commercial and open source solutions are available for the monitoring \nsystems.\nIn the code repository, you’ll be able to find a simple agent that’ll be running in its \nown Docker container to simulate metrics collection. It’s a trivial ruby script that listens \nto port 8125 over UDP and outputs to the console, as follows.\nListing 7.4    microservices-in-action/chapter-7/feature/statsd-agent/statsd-agent.rb\n#!/usr/bin/env ruby\n#\n# This script was originally found  in a post by Lee Hambley\n# (http://lee.hambley.name)\n#\nrequire 'socket'\nrequire 'term/ansicolor'\ninclude Term::ANSIColor\n$stdout.sync = true\nc = Term::ANSIColor\ns = UDPSocket.new\ns.bind(\"0.0.0.0\", 8125)\nwhile blob = s.recvfrom(1024)\n  metric, value = blob.first.split(':')\n  puts \"StatsD Metric: #{c.blue(metric)} #{c.green(value)}\"\nend\nThis simple script allows you to simulate metrics collection while developing your \nservices. Figure 7.8 shows metrics collection for services running when placing a sell \norder, the feature we use as an example for this chapter.\nUsing an annotation in the code for each service, you enable them to send metrics \nfor some operations. Even though this is a simple example, because they’re only emit-\nting timing metrics, it serves the purpose of showing how you can instrument your code \nto collect data you find relevant. Let’s look into one of the services to see how this is \ndone. Consider the listing 7.5.\n1\t See Ian Malpass, “Measure Anything, Measure Everything,” Code as Craft, Etsy, http://mng \n.bz/9Tqo.\n \n\n\n\t\n173\nDesigning a chassis\nFigure 7.8    StatsD agent collecting metrics that services collaborating in a place sell order operation \nhave emitted\nListing 7.5    microservices-in-action/chapter-7/feature/fees/app.py\nimport json\nimport datetime\nfrom nameko.events import EventDispatcher, event_handler\nfrom statsd import StatsClient \nclass FeesService:\n    name = \"fees_service\"\n    statsd = StatsClient('statsd-agent', 8125,\n                         prefix='simplebank-demo.fees') \n    @event_handler(\"market_service\", \"order_placed\")\n    @statsd.timer('charge_fee') \n    def charge_fee(self, payload):\n        print(\"[{}] {} received order_placed event ... charging fee\".format(\n            payload, self.name))\nImports the StatsD client so \nyou can use it in the module\nConfigures the StatsD client \nby passing the host, the port, \nand the prefix you’ll use for \nall the emitted merics\nUsing this annotation enables you to collect the time it takes \nfor the 'charge_fee' function to run. The StatsD library uses the \nvalue passed as an argument for the annotation as the metric \nname. In this case, the charge_fee function will emit the metric \nnamed 'simplebank-demo.fees.charge_fee'; the prefix you \nconfigured before will be prepended to the metric name passed \nto the annotation.\n \n\n\n174\nChapter 7   Building a reusable microservice framework \nTo collect metrics using the StatsD client library, you need to initialize the client by \npassing the hostname, in this case statsd-agent, the port, and an optional prefix for \nmetrics collected in this service scope. If you annotate the charge_fee method with \n@statsd.timer('charge_fee'), the library will wrap the execution of that method \nin a timer and will collect the value from the timer and send it to the agent. You can \ncollect these metrics and feed them to monitoring systems that’ll allow you to observe \nyour system behavior and set up alerts or even autoscale your services.\nFor example, imagine the fees service becomes too busy, and the execution time \nthat StatsD reports increases over a threshold you set. You can automatically be alerted \nabout that and immediately investigate to understand if the service is throwing errors or \nif you need to increase its capacity by adding more instances. Figure 7.9 shows an exam-\nple of a dashboard displaying metrics that StatsD collected.\nError reporting\nMetrics allow you to observe how the system is behaving on an ongoing basis, but, \nunfortunately, they aren’t the only thing you need to care about. Sometimes errors \nhappen, and you need to be alerted about them and, if possible, gather some infor-\nmation about the context in which the error occurred. For example, you might get a \nstack trace so you can diagnose and try to replicate and solve the error. Several services \nprovide alerting and aggregation of errors. It’s easy to integrate error reporting in your \nservices, as shown in the following listing.\nListing 7.6    microservices-in-action/chapter-7/chassis/http_demo.py\nimport json\nfrom nameko.web.handlers import http\nfrom werkzeug.wrappers import Response\nfrom nameko_sentry import SentryReporter \nclass HttpDemoService:\n    name = \"http_demo_service\"\n    sentry = SentryReporter() \n    @http(\"GET\", \"/broken\")\n    def broken(self, request):\n        raise ConnectionRefusedError() \n    @http('GET', '/books/<string:uuid>')\n    def demo_get(self, request, uuid):\n        data = {'id': uuid, 'title': 'The unbearable lightness of being',\n                'author': 'Milan Kundera'}\n        return Response(json.dumps({'book': data}),\n                        mimetype='application/json')\n    @http('POST', '/books')\n    def demo_post(self, request):\n        return Response(json.dumps({'book': request.data.decode()}),\n                        mimetype='application/json')\nImports the error reporting module\nInitializes the error reporting service\nRaises an exception so you can \ntest the error reporting service\n \n\n\n\t\n175\nDesigning a chassis\nFigure 7.9    Example of a dashboard displaying metrics that StatsD collected from an application\nSetting up error reporting in the chassis you assembled is simple. You initialize the \nerror reporter, and it’ll take care of capturing any exceptions and sending them over \nto the error reporting service backend. It’s common for the error reporter to send \nalong some context with the errors, like a stack trace. Figure 7.10 shows the dashboard \nwith the error you get if you access the /broken endpoint in the demo service.\nFigure 7.10    Dashboard for an error reporting service (Sentry) after accessing the /broken endpoint\n \n\n\n176\nChapter 7   Building a reusable microservice framework \nLogging\nYour services output information either to log files or to the standard output. These \nfiles can record a given interaction, such as the result and timing of an http call or any \nother information developers find useful to record. Having multiple services running \nthis recording means you potentially have multiple services logging information across \nthe organization. In a microservice architecture, where interactions happen between \nmultiple services, you need to make sure you can trace those interactions and have \naccess to them in a consistent way.\nLogging is a concern for all teams and plays an important role in any organization. \nThis is the case either for compliance reasons, when you may need to keep track of \nspecific operations, or for allowing you to understand the flow of execution between \ndifferent systems. The importance of logging is a sound reason for making sure that \nteams, no matter what language they’re using to develop their services, keep logs in a \nconsistent way and, preferably, aggregate them in a common place.\nAt SimpleBank, the log aggregation system allows complex searches in logs, so you \nagree to send logs to the same place and in the same format. You use logstash format for \nlogging, so the Python chassis includes a library to emit logs in logstash format.\nLogstash is an open source data processing pipeline that allows ingestion of data \nfrom multiple sources. The logstash format became quite popular and is widely used \nbecause it’s a json message with some default fields, such as the ones you can find in the \nfollowing listing.\nListing 7.7    Logstash json formatted message\n{\n  \"message\"    => \"hello world\",\n  \"@version\"   => \"1\",\n  \"@timestamp\" => \"2017-08-01T23:03:14.111Z\",\n  \"type\"       => \"stdin\",\n  \"host\"       => \"hello.local\"\n}\nFigure 7.11 shows the log output that the gateway service generates when receiving a \nplace sell order request from a client. In such cases, it generates two messages. They \nboth contain a wealth of information, like the filename, module, and line executing \ncode, as well as the time it took for the operation to complete. The only information \nyou passed explicitly to the logger was what appears in the message fields. The library \nyou’re using inserts all the other information.\nBy sending this information to a log aggregation tool, you can correlate data in many \ninteresting ways. In this case, here are some example queries:\n¡ Group by module and function name\n¡ Select all entries for operations that took longer than x miliseconds\n¡ Group by host\n \n\n\n\t\n177\nDesigning a chassis\nFigure 7.11    Logstash formatted log messages that the gateway service generated\nThe most interesting thing is that the host, type, version, and timestamp fields will \nappear in all the messages that the services using the chassis generate, so you can cor-\nrelate messages from different services.\nIn your Python chassis, the following listing shows the code responsible for generat-\ning the log entries you can see in figure 7.11.\nListing 7.8    Logstash logger configuration in the Python chassis\nimport logging\nfrom logstash_formatter import LogstashFormatterV1\nlogger = logging.getLogger(__name__)\nhandler = logging.StreamHandler()\nformatter = LogstashFormatterV1()\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n(…)\n# to log a message …\nlogger.info(“this is a sample message”)\nThis code is responsible for initializing the logging and adding the handler that’ll for-\nmat the output in the logstash json format.\nBy using the microservice chassis, you create a standard way of accessing the tools to \nachieve the goal of running observable services. By choosing certain libraries, you’re \nable to enforce having all teams use the same underlying infrastructure without forcing \nany team to choose a particular language.\n7.3.3\t\nBalancing and limiting\nWe mentioned in section 7.3.1 on service discovery that the message broker provided \nnot only a way for services to discover each other implicitly but also a load balancing \ncapability.\nWhile benchmarking the place sell order feature, say you realize you have a bottleneck \nin your processing. The market service has to interact with an external actor, the stock \nexchange, and will only do that after a successful response creates the OrderPlaced event \n \n\n\n178\nChapter 7   Building a reusable microservice framework \nthat both the fees service and the orders service will consume. Requests are accumulat-\ning because the HTTP call to the external service is slower than the rest of the processing \nin the system. For this reason, you decide to increase the number of instances running \nthe market service. You deploy three instances to compensate for the extra time that the \norder placement onto the stock exchange takes. This change is seamless, because once \nyou add the new instances, they’re registered with the rpc-market_service queue in \nRabbitMQ. Figure 7.12 shows the three instances of the service connected.\nAs you can see, three instances are connected to the queue, each of them set to prefetch \n10 messages from the queue as soon as they arrive. Now that you have multiple instances \nconsuming from the same queue, you need to make sure only one of those instances pro-\ncesses each request. Once again, RabbitMQ makes your life easier because it deals with \nload balancing. By default, it’ll use a round-robin algorithm to schedule the delivery of \nmessages between the service instances. This means it’ll deliver the first 10 messages to \ninstance 1, then the next 10 to instance 2, and finally 10 to instance 3. It’ll keep repeating \nthis over and over. This is a naïve approach to scheduling work, because one instance may \ntake longer than another one, but it generally works quite well and is easy to understand.\nFigure 7.12    Multiple instances of the market service registered in the RPC queue\n \n\n\n\t\n179\nDesigning a chassis\nThe only thing you need to be careful about is checking if the connected instances \nare healthy so they don’t start accumulating messages. You can do so by making use \nof metrics, using StatsD, to monitor the number of messages that each instance is \nprocessing and if they’re accumulating. In your code, you also can implement health \nchecks so that any instance not responding to those health check requests can be \nflagged and restarted. RabbitMQ also will work as a limiting buffer, storing messages \nuntil the service instances can process them. According to the configuration shown in \nfigure 7.12, each instance will receive ten messages to process at a time and will only \nbe assigned new messages after it has finished processing previous ones.\nIt’s worth mentioning that in the particular case of the market service as it interacts \nwith a third-party system, you also implement a circuit breaking mechanism. Let’s look \nat the service code where the call to the stock exchange is implemented, as follows.\nListing 7.9    microservices-in-action/chapter-7/feature/market/app.py\nimport json\nimport requests\n(…)\nfrom statsd import StatsClient\nfrom circuitbreaker import circuit \nclass MarketService:\n    name = \"market_service\"\n    statsd = StatsClient('statsd-agent', 8125,\n                         prefix='simplebank-demo.market')\n    (…)\n    @statsd.timer('place_order_stock_exchange')\n    @circuit(failure_threshold=5, expected_exception=\n➥ConnectionError) \n    def __place_order_exchange(self, request):\n        print(\"[{}] {} placing order to stock exchange\".format(\n            request, self.name))\n        response = requests.get('https://jsonplaceholder.typicode.com/\nposts/1')\n        return json.dumps({'code': response.status_code, 'body': response.\ntext})\nYou make use of the circuit breaker library to configure the number of consecutive fail-\nures to connect that you’ll tolerate. In the example shown, if you have five consecutive \nfailing calls with the ConnectionError exception, you’ll open the circuit, and no call \nwill be made for 30 seconds. After those 30 seconds, you’ll enter the recovery stage, \nallowing one test call. If the call is successful, it’ll close the circuit again, resuming nor-\nmal operation and allowing calls to the external service; otherwise, it’ll prevent calls \nfor another 30 seconds.\nImports the circuit breaker \nfunctionality to use in the module\nAllows you to configure how many exceptions you tolerate before \nopening the circuit and the type of exceptions that’ll count as a failure\n \n",
      "page_number": 189
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 198-205)",
      "start_page": 198,
      "end_page": 205,
      "detection_method": "topic_boundary",
      "content": "180\nChapter 7   Building a reusable microservice framework \nNOTE    Because 30 seconds is the default value the circuit breaker library sets \nfor the recovery_timeout parameter, you don’t see it in listing 7.9. If you want \nto adjust this value, you can do so by passing it explicitly.\nYou could use this technique not only for external calls but also for calls between inter-\nnal components, because it will allow you to degrade the service. In the case of the \nmarket service, using this technique would mean messages that services retrieved from \nthe queue wouldn’t be acknowledged and would accumulate in the broker. Once the \nexternal service connectivity was resumed, you’d be able to start processing messages \nfrom the queue. You could complete the call to the stock exchange and create the \nOrderPlaced event that allows both the fees service and the orders service to complete \nthe execution of a place sell order request.\n7.4\t\nExploring the feature implemented using the chassis\nIn the previous section, you saw code examples for the implementation of the place \nsell order feature. Let’s briefly look into the resulting feature prototype that you’d \nimplement using the chassis. Based on the chassis code that you can find in the code \nrepository under chapter7/chassis, say you’ve created five services:\n¡ Gateway\n¡ Orders service\n¡ Market service\n¡ Account transactions service\n¡ Fees service\nFigure 7.13 shows the project structure and a Docker Compose file that allows you to locally \nstart the five components and the StatsD agent we mentioned previously. The Docker \nCompose file will allow booting the services as well as the needed infrastructure compo-\nnents: RabbitMQ, Redis, and the local StatsD agent, which will simulate metrics collection.\nWe won’t go deep on Docker or Docker Compose right now, because we’ll cover it in \nthe upcoming chapters. But if you do have Docker and Docker Compose available, you \ncan boot the services by entering the feature directory and running docker-compose \nup –build. This will build a Docker container for each service and boot everything up.\nFigure 7.14 shows all services running and processing a POST request to the shares/sell \ngateway endpoint.\nEven though the feature makes use of both synchronous and asynchronous commu-\nnication between the different components, the chassis you have in place allows you to \nquickly prototype it and run initial benchmarks using a tool that allows you to simulate \nconcurrent requests, with results such as the following: (Please note that these bench-\nmarks ran locally on a development machine and are merely indicative.)\n$ siege -c20 -t300S -H 'Content-Type: application/json' \n'http://192.168.64.3:5001/shares/sell POST'\n    (benchmark running for 5 minutes …)\nLifting the server siege...\n \n\n\n\t\n181\nExploring the feature implemented using the chassis\nTransactions:\t \t\n      \t 12663 hits\nAvailability:\t \t\n     \t\n100.00 %\nElapsed time:\t \t\n     \t\n299.78 secs\nData transferred:\t\n       \t0.77 MB\nResponse time:\t\t\n      \t 0.21 secs\nTransaction rate:\t\n      \t \t\n42.24 trans/sec\nThroughput:\t\n\t\n       \t0.00 MB/sec\nConcurrency:\t\n\t\n       \t9.04\nSuccessful transactions:       \t\n12663\nFailed transactions:\t\n     \t\n0\nLongest transaction:\t\n      \t 0.52\nShortest transaction:\t       \t 0.08\nThese numbers look good, but it’s worth mentioning that once the benchmark \nstopped, the market service still needed to consume 3000 messages—almost a quar-\nter of the total requests that the gateway processed. This benchmark allows you to \nidentify the bottleneck happening in the market service that we mentioned in section \n7.3.3. Referring to figure 7.4, you can see that the gateway receives a response from the \norders service, but asynchronous processing still happens after that.\nFigure 7.13    Project structure for the place sell order feature and the Docker Compose file that allows \nbooting the services and the needed infrastructure components\n \n\n\n182\nChapter 7   Building a reusable microservice framework \nFigure 7.14    Services used in the place sell order running locally\nThe engineering team at SimpleBank certainly will continue to improve the Python \nchassis so it reflects continuous team learnings. For now though, it’s already usable to \nimplement nontrivial functionality.\n7.5\t\nWasn’t heterogeneity one of the promises of \nmicroservices?\nIn the previous sections, we covered building and using a chassis for Python applica-\ntions at SimpleBank. You can apply the principles to any language used within your \norganization though. At SimpleBank, teams also use Java, Ruby, and Elixir for building \nservices. Would you go and build a chassis for each of these languages and stacks? If \nthe language is widely adopted within the organization and different teams bootstrap \nmore than a couple of services, I’d say sure! But it’s not imperative that you create a \nchassis. The only thing to keep in mind is that with or without a chassis, you need to \nmaintain principles like observability.\nOne of the advantages of a microservice architecture is enabling heterogeneity of lan-\nguages, paradigms, and tooling. In the end, it’ll enable teams to choose the right tool for \nthe job. Although in theory the choices are limitless, the fact is, teams will specialize in a \ncouple of technology stacks for their day-to-day development. They’ll naturally develop \na deeper knowledge around one or two different languages and their supporting eco-\nsystems. A supporting ecosystem is also important. Independent teams, such as the ones \nyou need to have in place to successfully run a microservice architecture, will also focus \n \n\n\n\t\n183\nSummary\non operations and will know about the platforms running their apps. Some examples are \nthe Java virtual machine (JVM) or the Erlang virtual machine (BEAM). Knowing about \nthe infrastructure will help with delivering better and more efficient apps.\nNetflix is a good example because they have a deep knowledge of the JVM. This \nenables them to be a proficient contributor of open source tools, allowing the commu-\nnity to benefit from the same tools they use to run their service. The fact that they have \nso many tools written targeting the JVM will make that ecosystem the first choice for \ntheir engineering teams. In some sense, it feels like: “You’re free to choose whatever \nyou want, as long as it abides with our given set of rules and implements some inter-\nfaces..., or you can use this chassis that takes care of all of that!”\nHaving existing chassis for some of the languages and stacks an organization has \nadopted may help direct teams’ choices toward those languages and stacks. Not only \nwill services be easier and faster to bootstrap, they’ll also become more maintainable \nfrom a risk standpoint. A chassis is a great way to indirectly enforce key concerns and \npractices of an engineering team.\nTIP    DRY (don’t repeat yourself) isn’t mandatory. A chassis shouldn’t be a sort \nof shared library or dependency to be included in services and updated in a \ncentralized way. You should use the chassis to bootstrap new services, but not \nnecessarily to update all running services with a given feature. It’s preferable \nto repeat yourself a little than to bring in shared libraries that increase cou-\npling. Do repeat yourself if that results in keeping systems decoupled and inde-\npendently maintained and managed.\nSummary\n¡ A microservice chassis allows for quick bootstrapping of new services, enabling \ngreater experimentation and reducing risk.\n¡ The use of a chassis allows you to abstract the implementation of certain infra-\nstructure-related code.\n¡ Service discovery, observability, and different communication protocols are con-\ncerns of a microservice chassis, and it should provide them.\n¡ You can quickly prototype a complex feature like the place sell order example, if \nthe proper tooling exists.\n¡ Although the microservice architecture is often associated with the possibility \nof building systems in any language, those systems, when in production, need to \noffer some guarantees and have mechanisms to allow their operation and main-\ntenance to be manageable.\n¡ A microservice chassis is a way to provide those guarantees while allowing fast \nbootstrap and quick development for you to test ideas and, if proven, deploy \nthem to production.\n \n\n\nPart 3\nDeployment\nA n application is only useful if you can deploy it to your users. This part \nof the book will introduce you to deployment practices for microservices. We’ll \nexplore deployment techniques, such as continuous delivery and packaging, \nand deployment platforms, including Google Cloud Platform and Kubernetes. \nThroughout the next few chapters, you’ll learn how to build a deployment pipe-\nline to take microservice code changes safely and rapidly to production.\n \n\n\n187\nThis chapter covers\n¡ Why it’s crucial to get deployment right in a \nmicroservice application\n¡ The fundamental components of a microservice \nproduction environment\n¡ Deploying a service to a public cloud\n¡ Packaging a service as an immutable artifact\nMature deployment practices are crucial to building reliable and stable microservices. \nUnlike a monolithic application, where you can optimize deployment for a single use \ncase, microservice deployment practices need to scale to multiple services, written in \ndifferent languages, each with their own dependencies. You need to be able to trust \nyour deployment process to push out new features — and new services — without \nharming overall availability or introducing critical defects.\nAs a microservice application evolves at the level of deployable units, the cost of \ndeploying new services must be negligible to enable engineers to rapidly innovate and \ndeliver value to users. The added development speed you gain from microservices will \nbe wasted if you can’t get them to production rapidly and reliably. Automated deploy-\nments are essential to developing microservices at scale.\n8\nDeploying microservices\n \n\n\n188\nChapter 8  Deploying microservices\nIn this chapter, we’ll explore the components of a microservice production environ-\nment. Following that, we’ll look at some deployment building blocks — such as artifacts \nand rolling updates — and how they apply to microservices. Throughout the chapter, \nwe’ll work with a simple service — market-data — to try out different approaches to \npackaging and deployment using a well-known cloud service, Google Cloud Platform. \nYou can find a starting point for this service in the book’s repository on Github (https://\ngithub.com/morganjbruce/microservices-in-action).\n8.1\t\nWhy is deployment important?\nDeployment is the riskiest moment in the lifecycle of a software system. The closest \nreal-world equivalent would be changing a tire — except the car is still moving at 100 \nmiles an hour. No company is immune to this risk: for example, Google’s site reliabil-\nity team identified that roughly 70% of outages are due to changes in a live system \n(https://landing.google.com/sre/book/chapters/introduction.html).\nMicroservices drastically increase the number of moving parts in a system, which \nincreases the complexity of deployment. You’ll face four challenges when deploying \nmicroservices (figure 8.1):\n¡ Maintaining stability when facing a high volume of releases and component \nchanges\n¡ Avoiding tight coupling between components leading to build- or release-time \ndependencies\n¡ Releasing breaking changes to the API of a service, which may negatively impact \nthat service’s clients\n¡ Retiring services\nWhen you do them well, deployments are based on simplicity and predictability. A con-\nsistent build pipeline produces predictable artifacts, which you can apply atomically to \na production environment.\nX\nService code\nSome changes\nmight cause errors.\nServices are\ninterdependent.\nServices may\nbecome obsolete.\nService code\nProduction\nFigure 8.1    A high-level view of production deployment\n \n\n\n\t\n189\nA microservice production environment\n8.1.1\t\nStability and availability\nIn an ideal world, deployment is “boring:” not unexciting, but incident-free. We’ve \nseen too many teams — both monolithic and microservice — that experience deploy-\ning software as incredibly stressful. But if working with microservices means you’re \nreleasing more components more frequently, doesn’t that mean you’re introducing \nmore risk and instability into a system?\nManual change management is costly\nTraditional change management methodologies attempt to reduce deployment risk by \nintroducing governance and ceremony. Changes must go through numerous quality \ngates and formal approvals, usually human-driven. Although this is intended to ensure \nthat only working code reaches production, this approach is costly to apply and doesn’t \nscale well to multiple services.\nSmall releases reduce risk and increase predictability\nThe larger a release, the higher the risk of introducing defects. Naturally, microservice \nreleases are smaller because the codebases are smaller. And that’s the trick — by releas-\ning smaller changes more often, you reduce the total impact of any single change. \nRather than stopping everything for a deployment, you can design your services and \ndeployment approaches with the expectation that they’ll face continuous change. \nReducing the surface area of possible change leads to releases that are quicker, easier \nto monitor, and less disruptive to the smooth functioning of an application.\nAutomation drives deployment pace and consistency\nEven if your releases are smaller, you still need to make sure your change sets are as \nfree from defects as possible. You can achieve this by automating the process of com-\nmit validation — unit tests, integration tests, linting, and so on — and the process of \nrollout — applying those changes in the production environment. This helps you to \nbuild systematic confidence in the code changes you’re making and apply consistent \npractices across multiple services.\nTIP    Building for anti-fragility, or resilience during failure, is also an important \nelement of overall application stability — don’t forget to read chapter 6!\n8.2\t\nA microservice production environment\nDeployment is a combination of process and architecture:\n¡ The process of taking code, making it work, and keeping it working\n¡ The architecture of the environment in which the software is operated\nProduction environments for running microservices vary widely, as do monolith pro-\nduction environments. What’s appropriate for your application may depend on your \norganization’s existing infrastructure, technical capabilities, and attitude toward risk, \nas well as regulatory requirements.\n \n",
      "page_number": 198
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 206-216)",
      "start_page": 206,
      "end_page": 216,
      "detection_method": "topic_boundary",
      "content": "190\nChapter 8  Deploying microservices\n8.2.1\t\nFeatures of a microservice production environment\nThe production environment for a microservice application needs to provide several \ncapabilities to support the smooth operation of multiple services. Figure 8.2 gives a \nhigh-level view of the capabilities of the production environment.\nA microservice production environment has six fundamental capabilities:\n1\t A deployment target, or runtime platform, where services are run, such as virtual \nmachines (Ideally, engineers can use an API to configure, deploy, and update \nservice configuration. You also could call this API the control pane, as shown in \nthe figure.)\n2\t Runtime management, such as autohealing and autoscaling, that allows the ser-\nvice environment to respond dynamically to failure or changes in load without \nhuman intervention (For example, if a service instance fails, it should automati-\ncally be replaced.)\n3\t Logging and monitoring to observe service operation and provide insight for engi-\nneers into how services are behaving\n4\t Support for secure operation, such as network controls, secret management, and \napplication hardening\n5\t Load balancers, DNS, and other routing components to route requests from users \nand between microservices\n6\t A deployment pipeline that delivers services from code, safely into operational usage \nin the production environment\nThese components are part of the platform layer of the microservice architecture stack.\nControl pane\nRuntime\nmanagement\nManages\nNetwork and routing\nConnects\nObservability\nObserves\nDeployment pipeline\nMonitors\nEngineers\nWrites\nCode\nProduction\nFigure 8.2    A microservice production environment\n \n\n\n\t\n191\nDeploying a service, the quick way\n8.2.2\t\nAutomation and speed\nAlong with the six fundamental features, two factors are key in assessing the suitability \nof a deployment platform for a microservice application:\n¡ Automation  — The bulk of infrastructural management and configuration, such \nas spinning up a new host, should be highly amenable to automation, ideally by \nthe team developing services themselves.\n¡ Speed  — If a significant cost is associated with every new deploy — whether obtain-\ning infrastructure resources or setting up a new deployment — then a microservice \napproach will be significantly hampered.\nAlthough you may not always have the luxury of choosing your deployment environ-\nment, it’s important to appreciate how different platforms might affect these char-\nacteristics and how you develop your microservice application. I once worked for a \ncompany that took six weeks to provision each new server. Suffice it to say that taking \nnew services into production was an exhausting endeavor!\nIt’s not coincidental that the popularity of microservice architecture coincides with \nthe wider adoption of DevOps practices, such as infrastructure as code, and the increasing \nuse of cloud providers to run applications. These practices enable rapid iteration and \ndeployment of services, which in turn makes a microservice architecture a scalable and \nfeasible approach.\nWhen possible you should aim to use a public infrastructure as a service (IaaS) cloud, \nsuch as Google Cloud Platform (GCP), AWS, or Microsoft Azure, for deploying any \nnontrivial microservice application. These cloud services offer a wide range of features \nand tools that ease the development of a robust microservice platform at a lower level \nof abstraction than a higher level deployment solution (such as Heroku). As such, they \noffer more flexibility. In the next section, we’ll show you how to use GCP to deploy, \naccess, and scale a microservice.\n8.3\t\nDeploying a service, the quick way\nIt’s time to get your hands dirty and deploy a service. You need to take your code, get \nit running on a virtual machine, and make it accessible from the outside world — as \nfigure 8.3 illustrates.\nVirtual machine\nOutside world\nRequests\nService\nDeploy as\nCode\nFigure 8.3    A simple microservice deployment\n \n\n\n192\nChapter 8  Deploying microservices\nYou’ll use Google Compute Engine (GCE) as a production environment. This is a ser-\nvice on GCP that you can use to run virtual machines. You can sign up for a free trial \nGCP subscription, which will have enough credit for this chapter’s examples. Although \nthe operations you’ll perform are specific to this platform, all major cloud providers, \nsuch as AWS and Azure, provide similar abstractions.\nWARNING    This example isn’t a robust production deployment solution!\nTo interact with GCE, you’ll use the gcloud command-line tool. This tool interacts with \nthe GCE API to perform operations on your cloud account. You can find install instruc-\ntions in the GCP documentation (https://cloud.google.com/sdk/docs/quickstarts). It’s \nnot the only option — you could use third-party tools like Ansible or Terraform instead.\nAssuming you’ve followed the install instructions and logged in with gcloud init, \nyou can create a new project:\ngcloud projects create <project-id> --set-as\n➥-default --enable-cloud-apis               \nThis project will contain the resources that’ll run your service.\nTIP    Don’t forget to tear down your project when you’re done. Running gcloud \nprojects delete <project-id> will do the trick.\n8.3.1\t\nService startup\nTo run your service, you’ll use a startup script, which will be executed at startup time \nwhen Google Cloud provisions your machine. We’ve written this for you already — you \ncan find it at chapter-8/market-data/startup-script.sh.\nTake your time to read through the script, which performs four key tasks:\n¡ Installs binary dependencies required to run a Python application\n¡ Downloads your service code from Github\n¡ Installs that code’s dependencies, such as the flask library\n¡ Configures a supervisor to run the Python service using the Gunicorn web server\nNow, let’s try it out.\n8.3.2\t\nProvisioning a virtual machine\nYou can provision a virtual machine from the command line. Change to the chapter-8/\nmarket-data directory and run the following command:\ngcloud compute instances create market-data-service \\ \n  --image-family=debian-9 \\ \n  --image-project=debian-cloud \\ \nReplace <project-id> with the name of your choice.\nThe name of your machine\nThe base image you’ll use for the machine\n \n\n\n\t\n193\nDeploying a service, the quick way\n  --machine-type=g1-small \\ \n  --scopes userinfo-email,cloud-platform \\\n  --metadata-from-file startup-script=startup\n➥-script.sh \\ \n  --tags api-service \\ \n  --zone=europe-west1-b \nThis will create a machine and return the machine’s external IP address — something \nlike figure 8.4.\nThis approach to startup does take a while. If you want to watch the progress of the \nstartup process, you can tail the output of the virtual machine’s serial port:\ngcloud compute instances tail-serial-port-output market-data-service\nOnce the startup process has completed, you should see a message in the log, similar \nto this example:\nMar 16 12:17:14 market-data-service-1 systemd[1]: Startup finished in\n➥ 1.880s (kernel) + 1min 52.486s (userspace) = 1min 54.367s.\nGreat! You’ve got a running service — although you can’t call it yet. You’ll need to open \nthe firewall to make an external call to this service. Running the following command \nwill open up public access to port 8080 for all services with the tag api-service:\ngcloud compute firewall-rules create default-allow-http-8080 \\\n  --allow tcp:8080 \\ \n  --source-ranges 0.0.0.0/0 \\ \n  --target-tags api-service \\ \n  --description \"Allow port 8080 access to api-service\"\nYou can test your service by curling the external IP of the virtual machine. The external \nIP was returned when you created the instance (figure 8.4). If you didn’t note it, you can \nretrieve all instances by running gcloud compute instances list. Here’s the curl:\ncurl -R http://<EXTERNAL-IP>:8080/ping \nIf all is going well, the response you get will be the name of the virtual machine —  \nmarket-data-service.\nFigure 8.4    Information about a newly created virtual machine\nThe size of the machine to provision\nStarts up using your startup script\nIdentifies this machine’s workload\nThe compute zone — or data center — where this service should start\nAllows tcp queries to port 8080\nFrom any IP address\nTo machines with the api-service tag\nReplace EXTERNAL-IP with the \nIP address of your service.\n \n\n\n194\nChapter 8  Deploying microservices\n8.3.3\t\nRun multiple instances of your service\nIt’s unlikely you’ll ever run a single instance of a microservice:\n¡ You’ll want to scale horizontally (the X-axis of scalability) by deploying multiple \nclones of the same service, each handling a proportion of requests. Although you \ncould serve more requests with progressively larger machines, it’s ultimately possi-\nble to scale further using more machines.\n¡ It’s important to deploy with redundancy to ensure that failures are isolated. A \nsingle instance of a service won’t maximize resiliency when failures occur.\nFigure 8.5 illustrates a service group. Requests made to the logical service, market-data, \nare load balanced to underlying market-data instances. This is a typical production \nconfiguration for a stateless microservice.\nNOTE    Services that consume from an event queue or message bus are also hor-\nizontally scalable — you distribute message load by running multiple message \nconsumers.\nYou can try this out. On GCE, a group of virtual machines is called an instance group \n(or on AWS, it’s an auto-scaling group). To create a group, you first need to create an \ninstance template:\ngcloud compute instance-templates create market-data-service-template \\\n  --machine-type g1-small \\\n  --image-family debian-9 \\\n  --image-project debian-cloud \\\n  --metadata-from-file startup-script=startup-script.sh \\\n  --tags api-service \\\n  --scopes userinfo-email,cloud-platform\nOutside world\nRequests\nLoad\nbalancer\nRequests\nRequests\nRequests\nVirtual machine\nService\nVirtual machine\nService\nVirtual machine\nService\nDeploy as\nDeploy as\nDeploy as\nCode\nFigure 8.5    A service group and load balancer\n \n\n\n\t\n195\nDeploying a service, the quick way\nRunning this code will create a template to build multiple market-data-service instances \nlike the one you built earlier. Once the template has been set up, create a group:\ngcloud compute instance-groups managed create market-data-service-group \\\n  --base-instance-name market-data-service \\ \n  --size 3 \\ \n  --template market-data-service-template \\ \n  --region europe-west1 \nThis will spin up three instances of your market-data service. If you open the Google \nCloud console and navigate to Compute Engine > Instance Groups, you should see a \nlist like the one in figure 8.6.\nUsing an instance template to build a group gives you some interesting capabilities \nout of the box: failure zones and self-healing. These two features are crucial to operat-\ning a resilient microservice.\nFailure zones\nFirst, note the zone column in figure 8.6. It lists three distinct values: europe-west1-d, \neurope-west1-c, and europe-west1-b. Each of these zones represents a distinct data \ncenter. If one of those data centers fails, that failure will be isolated and will only affect \n33% of your service capacity.\nSelf-healing\nIf you select one of those instances, you’ll see the option to delete that instance (figure 8.7). \nGive it a shot!\nFigure 8.6    Instances within an instance group\nFigure 8.7    Deleting a VM instance\nThe name prefix of each new instance\nThe number of instances in the group\nThe template to use\nThe region to start these instances in\n \n\n\n196\nChapter 8  Deploying microservices\nDeleting an instance will cause the instance group to spin up a replacement instance, \nensuring that capacity is maintained. If you look at the operation history of the project \n(Compute Engine > Operations), you’ll see that the delete operation results in GCE \nautomatically recreating the instance (figure 8.8).\nThe instance group will attempt to self-heal in response to any event that results in an \ninstance falling out of service, such as underlying machine failure. You can improve this \nby adding a health check that also targets your application:\ngcloud compute health-checks create http api-health-check \\\n  --port=8080 \\ \n  --request-path=\"/ping\" \ngcloud beta compute instance-groups managed set\n➥-autohealing \\ \n  market-data-service-group \\ \n  --region=europe-west1 \\ \n  --http-health-check=api-health-check \nNow, with the addition of the health check, whenever the application fails to reply to it, \nthe virtual machine will be recycled.\nAdding capacity\nAs your service is now deployed from a template, it’s trivial to add more capacity. You \ncan resize the group from the command line:\ngcloud compute instance-groups managed resize market-data-service-group \\\n--size=6 \\ \n--region=europe-west1\nYou also can add autoscaling rules to automatically add more capacity if metrics you \nobserve from your group, such as average CPU utilization, pass a given threshold.\n8.3.4\t\nAdding a load balancer\nIn all that excitement, you forgot to expose your service group to the wild! In this \ncase, GCE will provide your load balancer, which consists of a few interconnected com-\nponents, as outlined in figure 8.9. The load balancer uses these routing rules, prox-\nies, and maps to forward requests from the outside world to a set of healthy service \ninstances.\nFigure 8.8    Deleting an instance in a group results in the instance being recreated to maintain target \ncapacity (from bottom to top)\nThe health check will make \nHTTP calls to :8080/ping.\nAssociates the check with \nyour existing instance group\nYour new target group size\n \n\n\n\t\n197\nDeploying a service, the quick way\nInternet\nRequests\nForwarding rule\nForwards traffic\nfrom a single IP\nto the proxy\nProxies requests to\ncorrect backend,\nbased on path\nTarget proxy\nChecks\nDefines backends\nfor given URL paths\nURL map\nBackend service\nBalances requests\nacross healthy\nbackend instances\nHealth check\nRequests\nHealth checks\nService\nVirtual machine\nFigure 8.9    Request lifecycle for GCE load balancing\nNOTE    Managed load balancers are a key feature for all major cloud providers. \nOutside of these environments, you may come across other software load bal-\nancers, such as HAProxy.\nFirst, you’ll want to add a backend service, which is the most important component of \nyour load balancer because it’s responsible for directing traffic optimally to underlying \ninstances:\ngcloud compute instance-groups managed set-named-ports \\\n  market-data-service-group \\\n  --named-ports http:8080 \\\n  --region europe-west1\ngcloud compute backend-services create \\\n➥market-data-service \\ \n  --protocol HTTP \\\n  --health-checks api-health-check \\ \n  --global\nThis code creates two entities: a named part, identifying the port your service exposes, \nand a backend service, which uses the http health check you created earlier to test the \nhealth of your service.\nNext, you need a URL map and a proxy:\ngcloud compute url-maps create api-map \\\n  --default-service market-data-service \ngcloud compute target-http-proxies create api-proxy \\\n  --url-map api-map \nIf you had more than one service, you could use the map to route different subdo-\nmains to different backends. In this case, the URL map will direct all requests, regard-\nless of URL, to the market-data-service you created earlier.\nThe name of your backend service\nThe health check you created earlier\nCreates a URL map for your backend service\nCreates a proxy that uses the new URL map\n \n\n\n198\nChapter 8  Deploying microservices\nFinally, you need to create a static IP address for your service and a forwarding rule \nthat connects that IP to the HTTP proxy you’ve created:\ngcloud compute addresses create market-data-service-ip \\\n  --ip-version=IPV4 \\\n  --global\nexport IP=`gcloud compute addresses describe market\n➥-data-service-ip --global --format json | jq –raw\n➥-output '.address'` \ngcloud compute forwarding-rules create \\\n➥api-forwarding-rule \\ \n  --address $IP \\ \n  --global \\ \n  --target-http-proxy api-proxy \\ \n  --ports 80 \nprintenv IP \nThis code creates a public IP address and configures requests to that IP to be forwarded \nto your HTTP proxy and on to your backend service. Once run, these rules take sev-\neral minutes to propagate. After a wait, try to curl the service — curl \"http://$IP/\nping?[1-100]\". That will start you with 100 requests. If you see the names of differ-\nent market-data nodes being output to your terminal — terrific — you’ve deployed a \nload-balanced microservice!\nNOTE    In the real world, you’d be unlikely to expose microservices directly to \nthe outside world. You’re only doing it here because it makes testing much \neasier. GCE also supports internal load balancing (https://cloud.google.com/\nload-balancing/docs/internal/) and Cloud Endpoints, a managed API gate-\nway (https://cloud.google.com/endpoints/).\n8.3.5\t\nWhat have you learned?\nIn these examples, you’ve built some of the key elements of a microservice deployment \nprocess:\n¡ Using an instance template established a primitive deployment operation, mak-\ning it simple to add and remove capacity for a given service.\n¡ Combining instance groups, load balancers, and health checks allowed you to \nautoscale and autoheal your microservice deployment.\n¡ Deploying into independent zones helped you build bulwarks to limit the impact \nof failures.\nBut a few things are missing. Your releases weren’t predictable, because you pulled \nyour latest code and compiled it on the machine. A new code commit could cause dif-\nferent service instances to be running inconsistent versions of the code (figure 8.10). \nWithout any explicit versioning or packaging, there would be no easy way to roll your \ncode forward or back.\nRetrieves the IP address\nAdds a forwarding rule that forwards \nfrom the IP address to the HTTP proxy\nOutputs the IP address you created\n \n\n\n\t\n199\nBuilding service artifacts\nThe latest instances\npick up 6ae881.\nTime\nInstance 2\nInstance 1\nInstance 3\nThese instances start\nwith commit 5a43ab.\n5a43ab\nGit repository\n6ae881\nNew code change\nFigure 8.10    Releasing without packaged versions results in deploying inconsistent code.\nThe process of starting machines was slow because you made pulling dependencies \npart of startup, rather than baking them into your instance template. This arrange-\nment also meant that the dependencies could become inconsistent across different \ninstances.\nLastly, you didn’t automate anything. Not only will a manual process not scale to \nmultiple microservices, but it’s likely to be error prone. Over the next few sections and \nchapters, you can make this much better.\n8.4\t\nBuilding service artifacts\nIn the earlier deployment example, you didn’t package your code for deployment. The \nstartup script that you ran on each node pulled code from a Git repository, installed \nsome dependencies, and started your application. That worked, but it was flawed:\n¡ Starting up the application was slow, as each node performed the same pull and \nbuild steps in parallel.\n¡ There was no guarantee that each node was running the same version of your \nservice.\nThis made your deployment unpredictable — and fragile. To get the benefits you want, \nyou need to build a service artifact. A service artifact is an immutable and deterministic \npackage for your service. If you run the build process again for the same commit, it \nshould result in an equivalent artifact.\nMost technology stacks offer some sort of deployment artifact (for example, JAR files \nin Java, DLLs in .NET, gems in Ruby, and packages in Python). The runtime character-\nistics of these artifacts might differ. For instance, you need to run .NET web services \nusing an IIS server whereas JARs may be self-executable, embedding a server process \nlike Tomcat.\n \n\n\n200\nChapter 8  Deploying microservices\nService code\nBuild automation\ntool\nPushes to\nProduces\nArtifact\n repository\nDeployment target\nPulls from\nArtifact\nIs ingested by\nFigure 8.11    An artifact repository stores service artifacts that a build automation tool constructs and \nyou can pull later for deployment.\nFigure 8.11 illustrates the artifact construction, storage, and deployment process. Typi-\ncally, a build automation tool (such as Jenkins or CircleCI) builds a service artifact and \npushes it to an artifact repository. An artifact repository might be a dedicated tool — for \nexample, Docker provides a registry for storing images — or a generic file storage tool, \nsuch as Amazon S3.\n8.4.1\t\nWhat’s in an artifact?\nA microservice isn’t only code; it’ll have many constituent parts:\n¡ Your application code, compiled or not (depending on programming language)\n¡ Application libraries\n¡ Binary dependencies (for example, ImageMagick or libssl) that are installed on \nthe operating system\n¡ Supporting processes, such as logging or cron\n¡ External dependencies, such as data stores, load balancers, or other services\nSome of these dependencies, such as application libraries, are explicitly defined. Oth-\ners may be implicit; for example, language-specific package managers are often igno-\nrant of binary dependencies. Figure 8.12 illustrates these different parts.\nAn ideal deployment artifact for a microservice would allow you to package up \na specific version of your compiled code, specifying any binary dependencies, and \nprovide a standard operational abstraction for starting and stopping that service. This \nshould be environment-agnostic: you should be able to run the same artifact locally, \nin test, and in production. By abstracting out differences between languages at run-\ntime, you both reduce cognitive load and provide common abstractions for managing \nthose services.\n \n",
      "page_number": 206
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 217-225)",
      "start_page": 217,
      "end_page": 225,
      "detection_method": "topic_boundary",
      "content": "\t\n201\nBuilding service artifacts\nSupporting\nprocesses,\nfor example,\nlogging,\ncron\nApplication\nOperating system\nBinary dependencies, for\nexample, ImageMagick\nApplication libraries\nThird-party\nservices\nDatabases\nand storage\nQueues/\nmessaging\nLoad balancers/\nrequest stack\nService\nFigure 8.12    A service with internal and external dependencies\n8.4.2\t\nImmutability\nWe’ve touched on immutability a few times so far — let’s take a moment to look at why \nit matters. An immutable artifact, encapsulating as many dependencies of your ser-\nvice as feasible, gives you the highest possible confidence that the package you tested \nthroughout your deployment pipeline will be the same as what is deployed in produc-\ntion. Immutability also allows you to treat your service instances as disposable — if a \nservice develops a problem, you can easily replace it with a new instance of the last \nknown good state. On GCE, this autohealing process was automated by the instance \ngroup you created.\nIf a build of the same code can result in a different artifact being created — for exam-\nple, pulling different versions of dependencies — you increase the risk in deployment \nand the fragility of your code because unintentional changes can be included in a \nrelease. Immutability increases the predictability of your system, as it’s easier to reason \nthrough a system’s state and recreate a historic state of your application — crucial for \nrollback.\nImmutability and server management\nImmutability isn’t only for service artifacts: it’s also an important principle for effective \nvirtual server management.\nOne approach to managing the state of hosts is to apply cumulative changes over \ntime — installing patches, upgrading software, changing configuration. This often means \nthat the ideal current state of a server isn’t defined anywhere — there’s no known good \nstate that you can use to build new servers. This approach also encourages applying live \n \n\n\n202\nChapter 8  Deploying microservices\nfixes to servers which, counterintuitively, increases the risk of failure. These servers suf-\nfer from configuration drift.\nThis approach might make sense if individual hosts are a scarce resource. In a cloud \nenvironment, where individual hosts are cheap to run and replace, immutability is a bet-\nter option. Instead of managing hosts, you should build them using a base template that \nitself is version controlled. Rather than updating older hosts, you replace them with hosts \nyou build from a new version of a base template.\n \n8.4.3\t\nTypes of service artifacts\nMany languages have their own packaging mechanism, and this heterogeneity makes \ndeployment more complex when working with services written in different languages. \nYour deployment tools need to treat differently the interface that each deployment \npackage provides to get it running on a server (or to stop it).\nBetter tooling can reduce these differences, but technology-specific artifacts tend \nto work at too low an abstraction level. They primarily focus on packaging code, rather \nthan the broader nature of application requirements:\n¡ They lack a runtime environment. As you saw earlier, you needed to separately \ninstall other dependencies to run your service.\n¡ They don’t provide any form of resource management or isolation, which makes \nit challenging to adequately run multiple services on a single host.\nLuckily, you’ve got a few options: operating system packages, server images, or contain-\ners (figure 8.13).\nVirtual machine\nOS Package\nService\nBins/libs\nUnit of\npackaging\nGuest OS\nHypervisor\nHost OS\nServer\nOS packages\nVirtual\nmachine\nService\nBins/libs\nUnit of\npackaging\nUnit of\npackaging\nGuest OS\nVirtual\nmachine\nService\nBins/libs\nGuest OS\nHypervisor\nHost OS\nServer\nVirtual machine images\nContainer\nService\nBins/libs\nContainer\nService\nBins/libs\nContainer runtime\nHost OS\nServer\nContainers\nFigure 8.13    The structure of different service artifact types\n(continued)\n \n\n\n\t\n203\nBuilding service artifacts\nOperating system packages\nYou could use the packaging format of your target operating system, such as apt or \nyum in Linux. This approach standardizes the installation of an artifact, regardless of \ncontents, as you can use standard operating system tools to automate the installation \nprocess. When you start a new host, you can pull the appropriate version of your ser-\nvice package. In addition, packages can specify dependencies on other packages — for \nexample, a Rails application might specify dependencies on common Linux packages, \nsuch as libxml, libmagic, or libssl.\nNOTE    If you’re interested in exploring this approach further, you could try \nto build a deb package using py2deb (github.com/paylogic/py2deb) and the \nexample service.\nThe OS package approach has three weaknesses:\n¡ It adds a different infrastructure requirement: you’ll need to host and manage a \npackage repository.\n¡ These packages are often tightly coupled to a particular operating system, reduc-\ning your flexibility in using different deployment targets.\n¡ The packages aren’t at quite the right level of abstraction, as you still need to exe-\ncute them in a host environment.\nServer images\nIn typical virtualized environments, each server you run is built from an image, or tem-\nplate. The instance template you built in section 8.3 is an example of a server image.\nYou can use this image itself as a deployment artifact. Rather than pulling a package \nonto a generic machine, you could instead bake a new image for each version of your \nservice that you want to deploy. A typical bake process has four steps:\n1\t Select a template image as the basis for the new image.\n2\t Start a VM based on the template image.\n3\t Provision the new VM to the desired state.\n4\t Take a snapshot of the new VM and save it as a new image template.\nYou can try that out using Packer.\nTIP    You’ll need to set up Packer to authenticate with GCE. You can find direc-\ntions for that in the Packer documentation: https://www.packer.io/docs/\nbuilders/googlecompute.html.\nFirst, save the following configuration file as instance-template.json.\nListing 8.1    The instance-template.json file\n{\n  \"variables\": { \n    \"commit\": \"{{env `COMMIT`}}\"\nUser-provided variables\n \n\n\n204\nChapter 8  Deploying microservices\n  },\n  \"builders\": \n  [\n    {\n      \"type\": \"googlecompute\",\n      \"project_id\": \"market-data-1\",\n      \"source_image_family\": \"debian-9\",\n      \"zone\": \"europe-west1-b\",\n      \"image_name\": \"market-data-service-{{user `commit`}}\",\n      \"image_description\": \"image built for market-data\n➥-service {{user `commit`}}\",\n      \"instance_name\": \"market-data-service-{{uuid}}\",\n      \"machine_type\": \"n1-standard-1\",\n      \"disk_type\": \"pd-ssd\",\n      \"ssh_username\": \"debian\",\n      \"startup_script_file\": \"startup-script.sh\"\n    }\n  ]\n}\nNow, run the packer build command from within the chapter-8/market-data \ndirectory:\npacker build \\\n-var \"commit=`git rev-parse head`\" \\ \ninstance-template.json \nIf you watch the console output, it’ll reflect the four steps I outlined above: using the \nGCE API, Packer will start an instance, run the startup script, and save the instance as a \nnew template image, tagged with the source Git commit. You can use the Git commit to \nexplicitly distinguish different versions of your code.\nNOTE    In this case, you still pulled code directly from Git to your machine \nimage. In complied languages such as Java, compilation into an executable \nshould be a separate step that a build automation tool executes.\nThis approach builds an immutable, predictable, and self-contained artifact. This \nimmutable server pattern, combined with a configuration tool like Packer, allows you \nto store a reproducible base state as code.\nIt has a few limitations:\n¡ Images are locked to one cloud provider, making them nontransferable to other \nproviders as well as to developers who want to recreate the deployed artifact on \ntheir machines.\n¡ Image builds are often slow because of the lengthy time it takes to spin up a \nmachine and take a snapshot.\n¡ It’s not easy for you to use for a multiple service-per-host model.\nDefines how an image will be built\nGets the latest commit hash\nUses the instance template defined in listing 8.1\n \n\n\n\t\n205\nBuilding service artifacts\nContainers\nInstead of distributing entire machines, containerization tools, such as Docker or rkt, \nprovide a more lightweight approach to encapsulating an application and its depen-\ndencies. You can run multiple containers on one machine, isolated from each other \nbut with lower resource overhead than a virtual machine because they share the kernel \nof one operating system. They avoid the overhead of virtualizing the disk and guest \noperating system of each virtual machine.\nTry a quick example using Docker. (You can find instructions for installing Docker \non the Docker website: https://docs.docker.com/install/.)  You build a Docker image \nfrom a Dockerfile. Add the following file to the chapter-8/market-data folder.\nListing 8.2    Dockerfile for market-data service\nFROM python:3.6 \nADD . /app \nWORKDIR /app\nRUN pip install -r requirements.txt \nCMD [\"gunicorn\", \"-c\", \"config.py\", \"app:app\", \"--bind\"\n➥, \"0.0.0.0:8080\"] \nEXPOSE 8000 \nThen, use the docker command-line tool to build the container:\n$ docker build -t market-data:`git rev-parse head` .\nSending build context to Docker daemon 71.17 kB\nStep 1/3 : FROM python:3.6\n ---> 74145628c331\nStep 2/3 : ADD . /app\n ---> bb3608d5143f\nRemoving intermediate container 74c250f83f8c\nStep 3/3 : WORKDIR /app\n ---> 7a595179cc39\nRemoving intermediate container 19d3bffa4d2a\nSuccessfully built 7a595179cc39\nThis will build a container image and tag it with the name market-data:<commit ID>.\nNow that you’ve built an image for the application, you can run it locally. Try it out:\n$ docker run -d -p 8080:8080 market-data:`git rev-parse head`\nYou’ll see startup logs from gunicorn in your terminal. If you like, try to curl the service \non port 8000. You probably noticed that startup and build time for the container was \nsignificantly faster than the virtual machines on GCE. This is one of the key benefits of \nusing containers.\nStarts from a public base \nimage for Python apps\nAdds your application code to the container\nInstalls the service’s requirements\nSets a startup command for the service\nExposes the service’s \nport from the container\n \n\n\n206\nChapter 8  Deploying microservices\nIn a few short steps, you can run this container image on GCE. First, you need to \npush the image to a container registry. Luckily, GCE already provides one:\nTAG=\"market-data:$(git rev-parse head)\"\nPROJECT_ID=<your-project-id> \ndocker tag $TAG eu.gcr.io/$PROJECT_ID/$TAG \ngcloud docker -- push eu.gcr.io/$PROJECT_ID/$TAG \nThis registry acts as an artifact repository where you can store your Docker images for \nlater use. After the push has completed, start an instance running this container:\ngcloud beta compute instances create-with-container \\\n  market-data-service-c \\\n  --container-image eu.gcr.io/$PROJECT_ID/$TAG \n  --tags api-service\nSuccess! You’ve deployed a container, and you’ve seen firsthand that it provides a more \nflexible — and easy-to-use — abstraction than a VM image.\nAs well as acting as a packaging mechanism, a container provides a runtime environ-\nment that isolates execution, effectively easing the operation of diverse containers on a \nsingle machine. This is compelling because it provides sane abstractions above individ-\nual hosts.\nUnlike virtual machine images, container images are portable; you can run the same \ncontainer on any infrastructure that supports the container runtime. This eases deploy-\nment in scenarios where multiple deployment targets are required, such as companies \nthat run workloads in both cloud and on-premise environments. It also simplifies local \ndevelopment; running multiple containers on a typical developer machine is much \nmore manageable than building and managing multiple virtual machines.\n8.4.4\t\nConfiguration\nThe service’s configuration is likely to differ based on deployment environment (stag-\ning, dev, production, and so on). For that and other reasons, you can’t represent all \nelements of a service within an artifact: \n¡ You can’t distribute secrets or sensitive configuration data, such as database pass-\nwords, in clear text or source control. You may want to retain the ability to change \nthem independently of a service deployment (for example, as part of automated \ncredential rotation, or worse, in the event of a security breach).\n¡ Environment-specific configuration data, such as database URLs, log levels, or \nthird-party service endpoints, will vary.\nReplace with your GCE project ID\nRenames the Docker image you created\nPushes the Docker image to GCE\nYou set the project ID and tag \nvariables in the previous example.\n \n\n\n\t\n207\nService to host models\nCode\nService artifact\nDeployed to\nDeployed to\nProduction\nStaging\nUses\nconfig\nUses\nconfig\nIs built into\nFigure 8.14    Service configuration that differ by environment\nThe third principle of The Twelve-Factor App manifesto (12factor.net) states that you \nshould strictly separate deployment configuration from code and provide it as environ-\nment variables (figure 8.14). In practice, the deployment mechanism you choose will \ndefine how you store and provide environment-specific configuration. We recommend \nstoring configuration in two places:\n¡ In source control, version-controlled alongside the service, for nonsensitive con-\nfiguration (These are commonly stored in .env files.)\n¡ A separate, access-restricted “vault” for secret information (such as HashiCorp’s \nperfectly named www.vaultproject.io)\nThe process that starts a service artifact should pull this configuration and inject it into \nthe application’s environment.\nUnfortunately, managing configuration separately can increase risk, as people may \nmake changes to production outside of your immutable artifacts, affecting the pre-\ndictability of your deployments. You should err on the side of restraint and attempt to \ninclude as much configuration as possible within your artifacts and rely on the speed \nand robustness of your deployment pipeline for rapidly changing configuration.\n8.5\t\nService to host models\nIn this section, we’ll review three common models for deploying services to underlying \nhosts: single service to host, multiple services to host, and container scheduling.\n8.5.1\t\nSingle service to host\nIn earlier examples, we’ve used a one-to-one relationship between service and underly-\ning host. This approach is easy to understand and provides a clear and explicit isolation \nbetween the resource needs and runtime of multiple services. Figure 8.15 illustrates this \napproach. Although the analogy is somewhat cruel, using this model lets you treat serv-\ners as cattle: indistinguishable units that you can start, stop, and destroy on command.\nThis model isn’t perfect. Sizing virtual instances appropriately for the needs of each \nservice requires ongoing effort and evaluation. If you’re not running in the cloud, you \nmay run into the limits of your data center or virtualization solution. And as we touched on \nearlier, virtual machine startup time is comparatively slow, often taking several minutes.\n \n\n\n208\nChapter 8  Deploying microservices\nOrders\nVM\nHoldings\nVM\nMarket data\nVM\nAccounts\nVM\nTransactions\nVM\nOrders\nVM\nHoldings\nVM\nMarket data\nVM\nAccounts\nVM\nTransactions\nVM\nFigure 8.15    A single service to host model\n8.5.2\t\nMultiple static services per host\nIt’s possible to run multiple services per host (figure 8.16). In the static variant of this \nmodel, the allocation of services to hosts is manual and static; the service owner makes \na conscious choice, predeployment, about where each service should be run.\nAt first glance, this approach might seem desirable. If obtaining new hosts is costly \nor hosts are scarce, then the easiest route to production would be to maximize usage of \nyour existing, limited number of hosts.\nBut this approach has several weaknesses. It increases coupling between services: \ndeploying multiple services to a host leads to coupling between services, eliminating \nyour desire to release services independently. It also increases the complexity of depen-\ndency management: if one service needs package v1.1, but another needs v2.0, the \ndifference is difficult to reconcile. It becomes unclear which service owns the deploy-\nment environment — and therefore which team has responsibility for managing that \nconfiguration.\nOrders\nVM\nHoldings\nOrders\nVM\nHoldings\nMarket data\nAccounts\nVM\nTransactions\nMarket data\nAccounts\nVM\nTransactions\n\t\nFigure 8.16    A single virtual machine can potentially run multiple services.\n \n\n\n\t\n209\nService to host models\nThis approach also leads to challenges in monitoring and scaling services inde-\npendently. One noisy service on a box might adversely impact other services, and it can \nbe difficult to monitor the resource usage (CPU, memory) of services independently.\n8.5.3\t\nMultiple scheduled services per host\nIt’d be even simpler if you could avoid thinking about the underlying hosts that run \nyour services altogether and focus entirely on the unique runtime environment of \neach application. This was the initial promise of platform as a service (Paas) solutions, \nsuch as Heroku. A PaaS provides tools for deploying and running services with min-\nimal operational configuration or exposure to underlying infrastructural resources. \nAlthough these platforms are easy to use, they often strike a difficult balance between \nautomation and control — simplifying deployment but removing customization from \nthe developer’s hands — as well as being highly vendor specific.\nContainers provide a more elegant abstraction:\n¡ An engineer can define and distribute a holistic application artifact.\n¡ A virtual machine can run multiple individual containers, isolating them from \neach other.\n¡ Containers provide an operational API that you can automate using higher level \ntooling.\nThese three facets enable scheduling, or orchestration, of containers. A container sched-\nuler is a software tool that abstracts away from underlying hosts by managing the execu-\ntion of atomic, containerized applications across a shared pool of resources. Typically, a \nscheduler consists of a master node that distributes application workloads to a cluster of \nworker nodes. Developers, or a deployment automation tool, send instructions to this \nmaster node to perform container deployments. Figure 8.17 illustrates this setup.\nDeployment tool\nDeploy two instances of\nservice A, version 100\nScheduler master\nPerforms deployment\nCluster\nService A\nService D\nService B\nService A\nService D\nService B\nService C\nService D\nPulls from\nContainer\nregistry\nFigure 8.17    A container scheduler executes containers across a cluster of nodes, balancing the \nresource needs of those nodes.\n \n",
      "page_number": 217
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 226-233)",
      "start_page": 226,
      "end_page": 233,
      "detection_method": "topic_boundary",
      "content": "210\nChapter 8  Deploying microservices\nAdvantages of a scheduling model\nUnlike the multiple static services per host model, the allocation of services in a sched-\nuler model is dynamic and depends on the resources (CPU, disk, or memory needs) \ndefined for each application. This avoids the pitfalls of the static model, as the sched-\nuler aims to continually optimize resource usage within the cluster of nodes, while the \ncontainer model preserves service independence.\nBy using a scheduler as a deployment platform, a service developer can focus on \nthe environment of their service in isolation from the underlying needs of machine \nconfiguration. Operations engineers can focus on running the underlying scheduler \nplatform and defining common operational standards for running services.\nContainer schedulers are complex\nContainer schedulers such as Kubernetes are complex pieces of software and require \nsignificant expertise to operate, especially because the tools themselves are relatively \nnew. We strongly recommend them as the ideal deployment platform for microservices, \nbut only if you can use a managed scheduler (such as Google’s Kubernetes Engine) \nor have the operational resources to run it in-house. If not, the single service per host \nmodel, combined with container artifacts, is a great and flexible fallback.\n8.6\t\nDeploying services without downtime\nSo far, you’ve only deployed market-data once. But in a real application, you’ll be \ndeploying services often. You need to be able to deploy new versions without downtime \nto maintain overall application stability. Every service will rely on others to be up and \nrunning, so you also need to maximize the availability of every service.\nThree common deployment patterns are available for zero-downtime deployments:\n¡ Rolling deploy  — You progressively take old instances (version N) out of service \nwhile you bring up new instances (version N+1), ensuring that you maintain a \nminimum percentage of capacity during deployment.\n¡ Canaries  — You add a single new instance1 into service to test the reliability of \nversion N+1 before continuing with a full rollout. This pattern provides an added \nmeasure of safety beyond a normal rolling deploy.\n¡ Blue-green deploys  — You create a parallel group of services (the green set), run-\nning the new version of the code; you progressively shift requests away from the \nold version (the blue set). This can work better than canaries in scenarios where \nservice consumers are highly sensitive to error rates and can’t accept the risk of \nan unhealthy canary.\nAll of these patterns are built on a single primitive operation. You’re taking an instance, \nmoving it to a running state in an environment, and directing traffic toward it.\n1\t In larger service groups, for example, >50 instances, you may need more than one canary to get \nrepresentative feedback.\n1\t In larger service groups, for example, >50 instances, you may need more than one canary to get \nrepresentative feedback.\n \n\n\n\t\n211\nDeploying services without downtime\n8.6.1\t\nCanaries and rolling deploys on GCE\nIt’s always better when you can see things in action. You can deploy a new version of \nmarket-data to GCE. First, you’ll want to create a new instance template. You can use \nthe container you built and pushed in section 8.4.3:\ngcloud beta compute instance-templates create-with-container \\\n  market-data-service-template-2 \\\n  --container-image eu.gcr.io/$PROJECT_ID/$TAG\n  --tags=api-service\nThen, initiate a canary update:\ngcloud beta compute instance-groups managed rolling-action start-update \\\n  market-data-service-group \\\n  --version template=market-data-service-template \\\n  --canary-version template=market-data-service\n➥-template-2,target-size=1 \\ \n  --region europe-west1\nGCE will add the canary instance to the group and the backend service to begin receiv-\ning requests (figure 8.18). It’ll take a few minutes to come up. You also can see this on \nthe GCE console (figure 8.19; Compute Engine > Instance Groups).\nIf you’re happy, you can proceed with the rolling update:\ngcloud beta compute instance-groups managed rolling-action start-update \\\n  market-data-service-group \\\n  --version template=market-data-service-template-2 \\\n  --region europe-west1\nThe speed at which this update occurs depends on how much capacity you want to \nmaintain during the rollout. You also can elect to surge beyond your current capac-\nity during rollout to ensure the target number of instances is always maintained. Fig-\nure 8.20 illustrates the stages of a rollout across three instances.\nCanary is brought\ninto service\nService group\nv1\nv1\nv1\nv2\nFigure 8.18    You add a new canary to the group.\nRolls out one instance \nof your new template\n \n\n\n212\nChapter 8  Deploying microservices\nFigure 8.19    Your instance group contains your original instances plus a canary instance of a new version.\nIf you were unhappy, you could roll back the canary:\ngcloud beta compute instance-groups managed rolling-action start-update \\\n  market-data-service-group \\\n  --version template=market-data-service-template \\ \n  --region europe-west1\nThe command for a rollback is identical to a rollout, but it goes to a previous version. \nIn the real world, rollback may not be atomic. For example, the incorrect operation of \nnew instances may have left data in an inconsistent state, requiring manual interven-\ntion and reconciliation. Releasing small change sets and actively monitoring release \nbehavior will limit the occurrence and extent of these scenarios.\nOld instance is killed\nNew instance added\nRepeated until all instances are v2\nv2\nv2\nv1\nv1\nv2\nv2\nv2\nv1\nv2\nv1\nv1\nX\nv2\nv1\nv1\nCanary is brought\ninto service\nRollout approved\nFigure 8.20    Stages of a rolling deploy, beginning with a canary instance\nThe original version\n \n\n\n\t\n213\nSummary\nWe’ve covered a lot of ground in this chapter: you’ve deployed manually to a cloud \nprovider, packaged a service as a container and a virtual machine, and practiced safe \nrollout patterns. By building immutable service artifacts and performing safe, down-\ntime-free deployments, you’re well on your way to building a deployment process that \nworks reliably across multiple services. Ultimately, the more stable, reliable, and seam-\nless your deployment process, the easier it is to standardize services, release new ser-\nvices more rapidly, and deliver valuable new features without friction or risk.\nSummary\n¡ Deploying new applications and changes must be standardized and straightforward \nto avoid friction in microservice development.\n¡ Microservices can run anywhere, but ideal deployment platforms need to sup-\nport a range of features, including security, configuration management, service \ndiscovery, and redundancy.\n¡ You deploy a typical service as a group of identical instances, connected by a load \nbalancer.\n¡ Instance groups, load balancers, and health checks enable autohealing and auto-\nscaling of deployed services.\n¡ Service artifacts must be immutable and predictable to minimize risk, reduce \ncognitive load, and simplify deployment abstractions.\n¡ You can package services as language-specific packages, OS packages, virtual \nmachine templates, or container images.\n¡ Being able to add/remove a single instance of a microservice is a fundamental \nprimitive operation that you can use to compose higher level deployment.\n¡ You can use canaries or blue-green deployments to reduce the impact of unex-\npected defects on availability.\n \n\n\n214\n9\nDeployment with \ncontainers and schedulers\nThis chapter covers\n¡ Using containers to package a microservice \ninto a deployable artifact\n¡ How to run a microservice on Kubernetes, a \ncontainer scheduler\n¡ Core Kubernetes concepts, including pods, \nservices, and replica sets\n¡ Performing canary deployments and rollbacks \non Kubernetes\nContainers are an elegant abstraction for deploying and running microservices, \noffering consistent cross-language packaging, application-level isolation, and rapid \nstartup time.\nIn turn, container schedulers provide a higher level deployment platform for con-\ntainers by orchestrating and managing the execution of different workloads across a \npool of underlying infrastructure resources. Schedulers also provide (or tightly inte-\ngrate with) other tools — such as networking, service discovery, load balancing, and con-\nfiguration management — to deliver a holistic environment for running service-based \napplications.\n \n\n\n\t\n215\nContainerizing a service\nContainers aren’t a requirement for working with microservices. You can deploy ser-\nvices using many methods such as using the single service per VM model we outlined in \nthe previous chapter. But together with a scheduler, containers provide a particularly ele-\ngant and flexible approach that meets our two deployment goals: speed and automation.\nDocker is the most commonly used container tool, although other container runtimes \nare available, such as CoreOS’s rkt. An active group — the Open Container Initiative —  \nis also working to standardize container specifications.\nSome of the popular container schedulers available are Docker Swarm, Kubernetes, \nand Apache Mesos; different tools and distributions are built on top of those platforms. \nOf these, Kubernetes, Google’s open source container scheduler, has the widest mind-\nshare and has garnered significant implementation support from other organizations, \nsuch as Microsoft, and the open source community. Because of this popularity and the \nease of setting up a local installation, we’ll use Kubernetes in this book.\nWe significantly increased deployment velocity at our own company using Kuberne-\ntes. Whereas our previous approach could take several days to get a new service deploy-\nment working smoothly, with Kubernetes, any engineer can now deploy a new service \nin a few hours.\nIn this chapter, you’ll get your hands dirty with Docker and Kubernetes. You’ll use \nDocker to build, store, and run a container for a new service at SimpleBank. And you’ll \ntake that service to production using Kubernetes. Along with these examples, we’ll \nillustrate how a scheduler executes and manages different types of workloads and how \nfamiliar production concepts map to a scheduler platform. We’ll also examine the high-\nlevel architecture of Kubernetes.\n9.1\t\nContainerizing a service\nLet’s jump right in! Over the course of this chapter, your goal will be to take one of \nSimpleBank’s Python services — market-data — and get it running in production. You \ncan find a starting point for this service in the book’s repository on Github (http://\nmng.bz/7eN9). Figure 9.1 illustrates the process that will occur. Docker packages ser-\nvice code into a container image, which is stored in a repository. You'll use deploy \ninstructions to tell a scheduler to deploy and operate the packaged service on a cluster \nof underlying hosts.\nAs you know, a successful deployment is about more than running a single instance. \nFor each new version, you want to build an artifact that you can deploy multiple times \nfor redundancy, reliability, and horizontal scaling. In this section, you’ll learn how to do \nthe following:\n¡ Build an image for a service\n¡ Run multiple instances — or containers — of your image\n¡ Push your image to a shared repository, or registry\nFirst things first: if you’re going to ship this, you need to figure out how to put it in \na box. For this section, you’ll need to have Docker installed. You can find up-to-date \ninstructions online at https://docs.docker.com/install.\n \n\n\n216\nChapter 9  Deployment with containers and schedulers \nEngineer\nWrites\nWrites\nDeployment\ninstructions\nService\ncode\nIs packaged as\nInstructs\nScheduler\nHost 1\nHost N\nService\nHost 2\nCluster\nService\nContainer\nregistry\nPushes to\nPulls from\nDeploys\nContainer\nimage\nFigure 9.1    The process of deploying service code to a cluster scheduler\n9.1.1\t\nWorking with images\nTo package an application into a container, you need to build an image. The image will \ninclude the file system that your application needs to run — code and dependencies —  \nand other metadata, such as the command that starts your application. When you run \nyour application, you’ll start multiple instances of this image.\nMost powerfully, images can inherit from other images. That means your application \nimages can inherit from public, canonical images for different technology stacks, or \nyou can build your own base images to encapsulate standards and tools you use across \nmultiple services.\nTo get a feel for working with images, fire up the command line and try to pull a pub-\nlicly available Docker image:\n$ docker pull python:3.6\n3.6: Pulling from library/python\nef0380f84d05: Pull complete\n24c170465c65: Pull complete\n4f38f9d5c3c0: Pull complete\n4125326b53d8: Pull complete\n35de80d77198: Pull complete\nea2eeab506f8: Pull complete\n1c7da8f3172e: Pull complete\ne30a226be67a: Pull complete\nDigest: \nsha256:210d29a06581e5cd9da346e99ee53419910ec8071d166ad499a909c49705ba9b\nStatus: Downloaded newer image for python:3.6\nPulling an image downloads it to your local machine, ready for you to run. In this case, \nyou pulled a Python image from Docker Hub, the default public registry (or reposi-\ntory) for Docker images. Running the following command will start an instance of that \nimage, placing you at a Python interactive shell inside your new container:\n$ docker run --interactive --tty python:3.6\nPython 3.6.1 (default, Jun 17 2017, 06:29:46)\n \n\n\n\t\n217\nContainerizing a service\n[GCC 4.9.2] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\nYou should note a few things here. The –-interactive (or –i) flag indicates that the \ncontainer should be interactive, accepting input from STDIN, whereas the –-tty (or \n–t) flag connects a terminal for user input to the Docker container. When you started \nthe container, it executed the default command set within the image. You can check \nwhat that is by inspecting the image metadata:\n$ docker image inspect python:3.6 --format=\"{{.Config\n➥.Cmd}}\" \n[python3]\nYou can instruct Docker to execute other commands inside your container; for exam-\nple, to enter the container at an OS shell, rather than Python, you could suffix the \ncommand you used to start the image instance with bash.\nWhen you watched the output of your earlier pull command, you might’ve noticed \nthat Docker downloaded multiple items, each identified by a hash — these are layers. \nAn image is a union of multiple layers; when you build an image, each command you \nrun (apt-get update, pip install, apt-get install –y, and so on) creates a new \nlayer. You can list the commands that went into building the python:3.6 image:\n$ docker image history python:3.6\nEach line that this script returns represents a different command used to construct \nthe python:3.6 image. In turn, some of those layers were inherited from another \nbase image. Commands defined in a Dockerfile specify the layers in an image using \na lightweight domain-specific language (DSL). If you look at the Dockerfile for this \nimage — you can find it on Github (http://mng.bz/JxDj) — you’ll notice the first line:\nFROM buildpack-deps:jessie\nThis specifies that the image should inherit from the buildpack-deps:jessie image. \nIf you follow that thread on Docker Hub, you can see that your Python container has a \ndeep inheritance hierarchy that installs common binary dependencies and the under-\nlying Debian operating system. This is detailed in figure 9.2.\npython:3.6\nbuildpack-deps:jessie\nbuildpack-deps:jessie-scm\nbuildpack-deps:jessie-curl\ndebian:jessie\nInstalls base operating\nsystem\nInstalls basic HTTP tools,\nfor example, curl, wget\nInstalls common SCM\npackages, for example,\nGit, Mercurial\nInstalls common binaries,\nfor example, make, libmagick,\nlibc, libpq, gcc, libxml2\nInstalls and configures\npython and pip\nFigure 9.2    The inheritance hierarchy of images used to construct the python:3.6 container on Docker Hub\nThe Docker image configuration is output as JSON, \nwhich you can parse using Go text templates.\n \n",
      "page_number": 226
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 234-249)",
      "start_page": 234,
      "end_page": 249,
      "detection_method": "topic_boundary",
      "content": "218\nChapter 9  Deployment with containers and schedulers \nOther container ecosystems use different mechanisms — for example, rkt uses the \nacbuild command-line tool — but the end outcome is similar.\nAs well as enabling reusability, image layers optimize launch times for containers. If a \nparent layer is shared between two derivative images on one machine, you only need to \npull it from a registry once, not twice.\n9.1.2\t\nBuilding your image\nThis Python image is a good starting point for you to build your own application image. \nLet’s take a quick look at the dependencies of the market-data service:\n1\t It needs to run on an operating system — any distribution of Linux should do.\n2\t It relies on Python 3.6.x.\n3\t It installs several open source dependencies from PyPI using pip, a Python pack-\nage manager.\nIn fact, this list maps quite closely to the structure of the image that you’re going to \nbuild. Figure 9.3 illustrates the relationship between your image and the Python base \nimage you’ve worked with so far.\nTo build this image, first you need to create a Dockerfile in the root of the market-data \nservice directory. This should do the trick:\npython:3.6 image\nmarket-data image\nPython dependencies (installed by pip)\nApplication code and resources\nDebian\nCommon binaries\nPython 3.6\npip\nFigure 9.3    The structure of your market-data container image and its relationship to the python:3.6 \nbase image\n \n\n\n\t\n219\nContainerizing a service\nListing 9.1    Dockerfile for application container\nFROM python:3.6 \nADD . /app \nWORKDIR /app \nThat’s not quite the whole picture, but try building this image and see what it looks \nlike. You can use the docker build command to create an image from a Dockerfile:\n$ docker build -t market-data:first-build .\nSending build context to Docker daemon 71.17 kB\nStep 1/3 : FROM python:3.6\n ---> 74145628c331\nStep 2/3 : ADD . /app\n ---> bb3608d5143f\nRemoving intermediate container 74c250f83f8c\nStep 3/3 : WORKDIR /app\n ---> 7a595179cc39\nRemoving intermediate container 19d3bffa4d2a\nSuccessfully built 7a595179cc39\nThis builds an image with the name market-data and the tag first-build. We’ll make \nmore use of tagging later in this chapter. Check that you can start the container and it \ncontains the files you expect:\n$ docker run market-data:first-build bash -c 'ls'\nDockerfile\napp.py\nconfig.py\nrequirements.txt\nThe output of this command should match the contents of the market-data directory. \nIf it did for you, that’s great! You’ve built a new container and added some files — only \na few more steps until you have it running an application.\nAlthough you’ve added your application code, you still need to pull down dependen-\ncies and start the application up. First, you can use a RUN command within your Docker-\nfile to execute an arbitrary shell script:\nRUN pip install -r requirements.txt\nIf you recall, the pip tool itself was installed as part of the python base image. If you \nwere working with Ruby or Node, at this point you might call bundle install or npm \ninstall; if you were working with a compiled language, you might use a tool like make \nto produce compiled artifacts.\nInstructs Docker to build this image \nusing python:3.6 as a starting point\nCopies the current code directory to a \ndirectory /app inside the container image\nSets the working directory \nof the container to /app\n \n\n\n220\nChapter 9  Deployment with containers and schedulers \nNOTE    For more complex applications, especially for compiled languages, you \nmay want to use the builder pattern or multistage builds to separate your devel-\nopment and runtime Docker images: http://mng.bz/LMFr.\nNext, you need to set the command that’ll be used to start your application. Add \nanother line to your Dockerfile:\nCMD [\"gunicorn\", \"-c \", \"config.py\", \"app:app\"]\nAnd a final touch: you need to instruct Docker to expose a port to your app. In this \ncase, your Flask app expects traffic on port 8000. Putting that together, you get your \nfinal Dockerfile, as shown in the following listing. You should build the image again, \nthis time tagging it as latest.\nListing 9.2    Complete Dockerfile for the market-data service\nFROM python:3.6\nADD . /app\nWORKDIR /app\nRUN pip install -r requirements.txt\nCMD [\"gunicorn\", \"-c \", \"config.py\", \"app:app\"]\nEXPOSE 8000\nPublic images and security\nThe python:3.6 image we’ve used so far is derived from debian:jessie, which \nhas a reputation for being well maintained and rapidly releasing patches to disclosed \nvulnerabilities.\nBut, as when working with any software, it’s important to be aware that using public \nDocker images potentially increases your security risk. Many images, particularly those \nthat aren’t officially maintained, aren’t regularly patched or updated, which can increase \nthe threat surface of your system.\nIf in doubt, security scanning tools, such as Clair (https://github.com/coreos/clair), exist \nfor analyzing the security stance of Docker containers. You can use these on an ad-hoc \nbasis or integrate them into your continuous integration pipeline.\nMaintaining your own base images is also an option but does involve an extra time invest-\nment. Deciding to take this route requires careful consideration of your team’s capabili-\nties and security expertise.\n \n9.1.3\t\nRunning containers\nNow that you’ve built an image for the application, you can run it. Try it out:\n$ docker run -d -p 8000:8000 --name market-data market-data:latest\nThis command should return a long hash to the terminal. That’s the ID of your con-\ntainer — you’ve started it in detached mode, rather than in the foreground. You’ve also \nused the -p flag to map the container port so it’s accessible from the Docker host. If \n \n\n\n\t\n221\nContainerizing a service\nyou try and call the service — it has a health-check endpoint at /ping — you should get \na successful response:\n$ curl -I http://{DOCKER_HOST}:8000/ping \nHTTP/1.0 200 OK\nContent-Type: text/plain\nServer: Werkzeug/0.12.2 Python/3.6.1\nYou could easily run multiple instances and balance between them. Try a basic exam-\nple, using NGINX as a load balancer. Luckily you can pull an NGINX container from \nthe public registry — no hard work to get that running. Figure 9.4 illustrates the con-\ntainers you’re going to run.\nFirst, start up three instances of the market-data service. Run the code below in your \nterminal:\n$ docker network create market-data \n$ for i in {1..3} \n  do\n    docker run -d \\\n      --name market-data-$i \\\n      -–network market-data \\\n      market-data:latest\n  done\nIf you run docker ps -a, you’ll see three instances of the market-data service up and \nrunning.\nTIP    Instead of working on the command line, you could use Docker Compose \nto define a set of containers declaratively — in a YAML file — and run them. But \nin this case, it’s better to start at a lower level so you can see what’s happening.\nmarket-data-1\nPort\n5000\nmarket-data-1\nPort\n5000\nmarket-data-1\nPort\n5000\nRequests\nNGINX\nPort 80\nFigure 9.4    NGINX load-balances requests made to it between three market-data containers\nDOCKER_HOST will depend on how you’ve \ninstalled Docker in your environment.\nCreates a container network named market-data\nRuns three containers based on the \nmarket-data:latest image you created earlier\n \n\n\n222\nChapter 9  Deployment with containers and schedulers \nUnlike earlier, you didn’t map each container’s port to the host machine. That’s \nbecause you’ll only access these containers through NGINX. Instead, you created a \nnetwork. Running on the same network will allow the NGINX container to easily dis-\ncover your market-data instances, using the container name as a host name.\nNow, you can set up NGINX. Unlike before, you’re not going to build your own \nimage; instead, you’ll pull the official NGINX image from the public Docker registry. \nFirst, you’ll need to configure NGINX to load balance between three instances. Create \na file called nginx.conf using the following code.\nListing 9.3    nginx.conf\nupstream app {\n    server market-data-1:8000; \n    server market-data-2:8000;\n    server market-data-3:8000;\n}\nserver {\n    listen 80;\n    location / {\n        proxy_pass http://app; \n    }\n}\nThen you can start an NGINX container. You’ll use the volume flag (or –v) to \nmount your new nginx.conf file into the container, sharing it with the local filesys-\ntem. This is useful for sharing secrets and configurations that aren’t — or shouldn’t \nbe — built into a container image, such as encryption keys, SSL certificates, and \nenvironment-specific configuration files. In this case, you avoid having to build a \nseparate container to include a single new configuration file. Start the container by \nentering the following:\n$ docker run -d --name=nginx \\\n--network market-data \\ \n--volume `pwd`/nginx.conf:/etc/nginx/conf.d/\n➥default.conf \\ \n-p 80:80 \\ \nnginx\nAnd that should do the trick. Curling http://localhost/ping should return the \nhostname — by default, the container ID — of the container instance responding to \nthat request. NGINX will round-robin requests across the three nodes to (naively) \nbalance load across your instances.\nYou configure the upstream application \nusing the container name and port.\nThe NGINX server proxies requests received \non port 80 to the upstream application.\nRuns on the same network as \nyour market-data containers\nMounts your configuration into an \nappropriate location inside the container\nMaps the container port to \nport 80 on your host machine\n \n\n\n\t\n223\nContainerizing a service\n9.1.4\t\nStoring an image\nGood work so far — you’ve built an image and you’ve seen that it’s easy to run multi-\nple independent instances of an application. Unfortunately, that image isn’t much use \nin the long run if it’s only on your machine. When it comes to deploying this image, \nyou’ll pull it from a Docker registry. This might be Docker Hub, which you’ve already \nencountered; a managed registry, such as AWS ECR or Google Container Registry; or \nself-hosted — for example, using the Docker distribution open source project (https://\ngithub.com/docker/distribution). When you build a continuous delivery pipeline, \nthat pipeline will push to your registry on every valid commit.\nNOTE    It’s also possible to save Docker images as a tarball, using the docker \nsave command, although this isn’t commonly used in image distribution. In \ncontrast, rkt natively uses tarballs for container distribution. This means you \ncan store images in standard file stores, for example, S3, rather than using a \ncustom registry.\nFor now, you can push your image to https://hub.docker.com. First, you’ll need to cre-\nate an account and choose a Docker ID. This will be the namespace you’ll use to store \nyour containers. Once you’ve logged in, you’ll need to create a new repository — a \nstore for multiple versions of the same image — using the web UI (figure 9.5).\nTo push to this repository, you need to tag your market-data image with an appropri-\nate name. Docker image names follow the format <registry>/<repository>:<tag>. \nOnce that’s done, a simple docker push will upload your image to the registry. Try it out:\n$ docker tag market-data:latest <docker id>/market-data:latest\n$ docker login\n$ docker push <docker id>/market-data:latest\nFigure 9.5    Using the Create Repository page on Docker Hub to create a repository for market-data \nimages\n \n\n\n224\nChapter 9  Deployment with containers and schedulers \nFigure 9.6    The private repository page on Docker Hub shows a record of the tagged image you pushed.\nThat’s it! You’ve successfully pushed your image to a public repository. You can double- \ncheck that through the web UI (figure 9.6) by logging into https://hub.docker \n.com. Other engineers (if your repository is private, you’ll need to grant them access) \ncan pull your image using docker pull [image name].\nLet’s take stock for a moment:\n¡ You’ve learned how to package a simple application into a lightweight, \ncross-platform artifact — a container image.\n¡ We’ve explored how Docker images are built from multiple layers to support \ninheritance from common base containers and increase startup speed.\n¡ You’ve run multiple isolated instances of an application container.\n¡ You’ve pushed the image you built to a Docker registry.\nUsing these techniques in a build pipeline will ensure greater consistency and predict-\nability across a fleet of services, regardless of underlying programming language, as well \nas helping to simplify local development. Next, we’ll explore how a container sched-\nuler works by taking your containerized application and deploying it with Kubernetes.\n9.2\t\nDeploying to a cluster\nA container scheduler is a software tool that abstracts away from underlying hosts by \nmanaging the execution of atomic, containerized applications across a shared pool of \nresources. This is possible because containers provide strong isolation of resources and \na consistent API.\nUsing a scheduler is a compelling deployment platform for microservices because \nit eases the management of scaling, health checks, and releases across, in theory, any \nnumber of independent services. And it does so while ensuring efficient utilization of \nunderlying infrastructure. At a high level, a container scheduler workflow looks some-\nthing like this:\n¡ Developers write declarative instructions to specify which applications they want to \nrun. These workloads might vary: you might want to run a stateless, long-running \nservice; a one-off job; or a stateful application, like a database.\n¡ Those instructions go to a master node.\n \n\n\n\t\n225\nDeploying to a cluster\n¡ The master node executes those instructions, distributing the workloads to a \ncluster of underlying worker nodes.\n¡ Worker nodes pull containers from an appropriate registry and run those appli-\ncations as specified.\nFigure 9.7 illustrates this scheduler architecture. To an engineer, where and how an \napplication is executed is ultimately unimportant: the scheduler takes care of it. In \naddition to running containers, Kubernetes provides other functionality to support \nrunning applications, such as service discovery and secret management.\nEngineer\nInstructions\n2 x service A\nMaster node\nperforms\ndeployment\nWorker node\nWorker node\nWorker node\nWorker node\nContainer\nregistry\npulls from\nservice A\nD\nD\nB\nD\nB\nA\nC\nCluster\nFigure 9.7    High-level scheduler architecture and deployment process\nMany well-known cluster management tools are available, but in your case, you’re \ngoing to use Kubernetes, an open-source project that evolved from Google’s internal \nwork on Borg and Omega (https://research.google.com/pubs/pub41684.html). It’s \npossible to run Kubernetes pretty much anywhere — public cloud, private data center, \nor as a managed service (such as Google Kubernetes Engine (GKE)).\nIn the next few sections, we’re going to cover a lot of ground. You’ll do the following:\n¡ Learn about the unit of deployment used on Kubernetes — pods\n¡ Define and deploy multiple replicas of a pod for the market-data microservice\n¡ Route requests to your pods using services\n¡ Deploy a new version of the market-data microservice\n¡ Learn how to communicate between microservices on Kubernetes\nWe’ll start by using Minikube, which will run in a virtual machine on your local host. \nIn a real deployment environment, the master and worker nodes would be separate \nvirtual machines, but locally, the same machine will fulfill both roles. You can find an \n \n\n\n226\nChapter 9  Deployment with containers and schedulers \ninstallation guide for Minikube on the project’s Github page (https://github.com/\nkubernetes/minikube).\nTIP    If you used a private repository in section 9.1.4, you’ll need to configure \nMinikube so it can access that repository by running minikube addons con-\nfigure registry-creds and following the instructions onscreen.\n9.2.1\t\nDesigning and running pods\nThe basic building block in Kubernetes is a pod: a single container or a tightly coupled \ngroup of containers that are scheduled together on the same machine. A pod is the \nunit of deployment and represents a single instance of a service. Because it’s the unit of \ndeployment, it’s also the unit of horizontal scalability (or replication). When you scale \ncapacity up or down, you add or remove pods.\nTIP    Sometimes a service is deployed as more than one container — a composite \ncontainer. For example, Flask services running on a Gunicorn web server are \ntypically served behind NGINX. Using Kubernetes, a single pod would contain \nboth the service and the NGINX container. Other examples of composite con-\ntainer patterns are discussed on the Kubernetes blog at (http://mng.bz/tOyC).\nYou can define a set of pods for your market-data service. Create a file called market- \ndata-replica-set.yml in your app directory. Don’t worry if it doesn’t make much sense \nyet. Include the following code in your file.\nListing 9.4    market-data-replica-set.yml\n---\nkind: ReplicaSet \napiVersion: extensions/v1beta1\nmetadata:\n  name: market-data\nspec:\n  replicas: 3 \n  template: \n    metadata:\n      labels: \n        app: market-data \n        tier: backend \n        track: stable \n    spec:\n      containers:\n      - name: market-data\n        image: <docker id>/market-data:latest \n        ports:\n        - containerPort: 8000\nIn Kubernetes, you typically declare instructions to the scheduler in YAML files (or \nJSON, but YAML’s easier on the eyes). These instructions define Kubernetes objects, \nDefines a set of pods\nShould contain three replicas of your market-data pod\nCreates each pod using this template\nIdentifies pods within Kubernetes by label\nContains a single container, \npulled from your Docker registry\n \n\n\n\t\n227\nDeploying to a cluster\nand a pod is one kind of object. These configuration files represent the desired state \nof your cluster. When you apply this configuration to Kubernetes, the scheduler will \ncontinually work to maintain that ideal state. In this file, you’ve defined a ReplicaSet, \nwhich is a Kubernetes object that manages a group of pods.\nNOTE    We’ll occasionally use dot-notation to refer to paths within a *.yml file. \nFor example, in listing 9.4, the path to the market-data container definition \nwould be spec.template.spec.containers[0].\nTo apply this to your local cluster, you can use the kubectl command-line tool. When \nyou started Minikube, it should have automatically configured kubectl to operate on \nyour cluster. This tool interacts with an API exposed by the cluster’s master node. Give \nit a try:\n$ kubectl apply -f market-data-replica-set.yml\nreplicaset \"market-data\" configured\nKubernetes will asynchronously create the objects you’ve defined. You can observe the \nstatus of this operation using kubectl. Running kubectl get pods (or kubectl get \npods -l app=market-data) will show you the pods that your command has created \n(figure 9.8). They’ll take a few minutes to start up for the first time as the node down-\nloads your Docker image.\nYou saw earlier that you didn’t create individual pods. It’s unusual to create or destroy \npods directly; instead, pods are managed by controllers. A controller is responsible \nfor taking some desired state — say, always running three instances of the market-data \npod — and performing actions to reach that state. This observe-diff-act loop happens \ncontinually.\nYou’ve just encountered the most common type of controller: the ReplicaSet. If \nyou’ve ever encountered instance groups on AWS or GCP, you might find their behav-\nior similar. A replica set aims to ensure a specific number of pods are running at any \none time. For example, let’s say a pod dies — maybe a node in the cluster failed — the \nreplica set will observe that the state of the cluster no longer matches the desired state \nand will attempt to schedule a replacement elsewhere in the cluster.\nYou can see this in action. Delete one of the pods you’ve just created (pods are iden-\ntified by name):\n$ kubectl delete pod <pod name>\nThe replica set will schedule a new pod to replace the one you destroyed (figure 9.9).\nFigure 9.8    The results of the kubectl get pods command after creating a new replica set\n \n\n\n228\nChapter 9  Deployment with containers and schedulers \nFigure 9.9    The state of running pods after one member of the replica set is deleted\nThis matches the ideal we laid out in chapter 8: that deploying microservice instances \nshould be built on a single primitive operation. By combining controllers and \nimmutable containers, you can treat pods like cattle and rely on automation to main-\ntain capacity, even when the underlying infrastructure is unreliable.\nWARNING    A cluster alone isn’t a complete redundancy solution; your infra-\nstructure design also determines this. For example, if you run a cluster in a sin-\ngle data center — or one availability zone in AWS — you won’t have redundancy \nif that entire data center goes down. It’s important, where possible, to run your \ncluster(s) across multiple isolated zones of failure.\n9.2.2\t\nLoad balancing\nRight, so you’re running a microservice on Kubernetes. That was pretty quick. The bad \nnews is, you can’t access those pods yet. Like you did earlier with NGINX, you need to \nlink them to a load balancer to route requests and expose their capabilities to other \ncollaborators, either inside or outside your cluster.\nIn Kubernetes, a service defines a set of pods and provides a method for reaching \nthem, either by other applications in the cluster or from outside the cluster. The net-\nworking magic that achieves this feat is outside the scope of this book, but figure 9.10 \nillustrates how a service would connect to your existing pods.\nNow, you’re currently running a replica set containing three market-data pods. If \nyou recall from listing 9.4, your market-data pods have the labels app: market-data \nand tier: backend. That’s important, because a service forms a group of pods based \non their labels.\nTo create a service, you need another YAML file, as shown in the following listing. \nThis time, call it market-data-service.yml (great naming convention).\nListing 9.5    market-data-service.yml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: market-data\nspec:\n  type: NodePort\n \n\n\n\t\n229\nDeploying to a cluster\n  selector: \n    app: market-data \n    tier: backend \n  ports:\n    - protocol: TCP\n      port: 8000 \n      nodePort: 30623 \nApply this configuration using the same $ kubectl apply -f  command you used to \ncreate the replica set before, substituting the name of your new YAML file. This will \ncreate a service accessible on port 30623 of your cluster, which routes requests to your \nmarket-data pods on port 8000.\nYou should be able to curl your service and send requests to your pods. Doing so will \nreturn the name of each pod that serves the request:\n$ curl http://`minikube ip`:30623/ping \nRequests\nPort\nmarket-data\nservice\nmarket-data pods\napp: market-data\ntier: backend\ntrack: stable\napp: market-data\ntier: backend\n5000\napp: market-data\ntier: backend\ntrack: stable\n5000\napp: market-data\ntier: backend\ntrack: stable\n5000\nReplica set (market-data)\nFigure 9.10    Requests made to a service are forwarded to pods that match the label selector of the \nservice.\nDefines which pods this service will access\nThe service will route to this port on the specified pods.\nThe service will be exposed as a specified port \non the cluster. Excluding this line will assign a \nrandom port in range 30000-32767.\n`minikube ip` returns the IP \naddress of your local cluster.\n \n\n\n230\nChapter 9  Deployment with containers and schedulers \nSeveral types of services are available, and they’re outlined in table 9.1. In this case, \nyou used a NodePort service to map your service to an externally available port on \nyour cluster, but if only other cluster services access your microservice, it usually makes \nmore sense to use ClusterIP to keep access local to the cluster.\nTable 9.1    Types of service on Kubernetes\nService type\nBehavior\nClusterIP\nExposes the service on an IP address local to the cluster\nNodePort\nExposes the service on a static port accessible at the cluster’s IP address\nLoadBalancer\nExposes the service by provisioning an external cloud service load balancer (If you’re \nusing AWS, this creates an ELB.)\nThe service listens for events across the cluster and will be dynamically updated if the \ngroup of pods changes. For example, if you kill a pod, it will be removed from the \ngroup, and the service will route requests to any new pod created by the replica set.\n9.2.3\t\nA quick look under the hood\nSo far, this has been seamless: you send an instruction, and Kubernetes executes it! \nLet’s take a moment to learn how Kubernetes runs your pods.\nIf you drill down a level, you can see that the master and worker nodes on Kubernetes \nrun several specialized components. Figure 9.11 illustrates these components.\nComponents of the master node\nThe master node consists of four components:\n¡ The API server  — When you ran commands on kubectl, this is what it communi-\ncated with to perform operations. The API server exposes an API for both exter-\nnal users and other components within the cluster.\n¡ The scheduler  — This is responsible for selecting an appropriate node where a pod \nwill run, given priority, resource needs, and other constraints.\n¡ The controller manager  — This is responsible for executing control loops: the con-\ntinual observe-diff-act operation that underpins the operation of Kubernetes.\n¡ A distributed key-value data store, etcd  — This stores the underlying state of the clus-\nter and thereby makes sure it persists when nodes fail or restarts are required.\nTogether, these components act as a control plane for the cluster. Picture this as \nsomething like the cockpit of an airplane. Together, these components provide the \nAPI and backend required to orchestrate operations across a cluster of nodes.\n \n\n\n\t\n231\nDeploying to a cluster\netcd\nMaster\nWorker node(s)\nController\nmanager\nScheduler\nQueries & updates\nGets & updates state\nGets & updates state\nAPI server\nstate\nstate\nKubelet\nkube-proxy\nControls\nContainer runtime\nRuns\nContainers\nC\nC\nProxies\nFigure 9.11    Components of the master and worker nodes in a Kubernetes cluster\nComponents of a worker node\nEach worker node uses the following components to run and monitor applications:\n¡ A container runtime  — In your case, this is Docker.\n¡ The kubelet  — This interacts with the Kubernetes master to start, stop, and moni-\ntor containers on the node.\n¡ The kube-proxy  — This provides a network proxy to direct requests to and between \ndifferent pods across the cluster.\nThese components are relatively small and loosely coupled. A key design principle \nof Kubernetes is to separate concerns and ensure components can operate autono-\nmously — a little like microservices!\nWatches for state changes\nThe API server is responsible for recording the state of the cluster — and receiving \ninstructions from clients — but it doesn’t explicitly tell other components what to do. \nInstead, each component works independently to orchestrate cluster behavior when \nsome event or change occurs. To learn about state changes, each component watches \nthe API server: a component requests to be notified by the API server when something \ninteresting happens, so it can perform appropriate actions to attempt to match the \ndesired state.\nFor example, the scheduler needs to know when it should assign new pods to nodes. \nTherefore, it connects to the API server to receive a continuous stream of events that \nrelate to the pod resource. When it receives a notification about a newly created pod, it \nfinds an appropriate node for that pod. Figure 9.12 shows this process.\nIn turn, your kubelets watch the API server to learn when a pod has been assigned to \nits node and then they start the pod appropriately. Each component watches resources \nand events that interest it; for example, the controller manager watches replica sets and \nservices (among other things).\n \n\n\n232\nChapter 9  Deployment with containers and schedulers \nScheduler\n1. Watches\n2. Receives notification\nabout new pod\n3. Updates pod with assigned node\nAPI server\nPods\nFigure 9.12    The scheduler watches the API server for newly created pods and determines which node \nthey should run on.\nUnderstanding how pods are run\nWhat happens when you create a replica set? You saw earlier that this results in the \nexpected number of pods being run — from your perspective, it looked simple! But in \nreality, creating your replica set through kubectl triggers a complex chain of events \nacross multiple components. This chain is illustrated in figure 9.13.\nController\nmanager\nScheduler\n4. Assigns pods to node\n3. Notified\n3. Creates pods\n2. Notified\n1. Creates new replica set\nReplicaSet\nPod\nkubectl\nAPI server\n5. Notified\n6. run container\nKubelet\nContainer runtime\n7. runs containers\nContainers\nWorker node(s)\nFigure 9.13    The series of events from creating a replica set to running pods on Kubernetes\n \n\n\n\t\n233\nDeploying to a cluster\nLet’s walk through each step:\n1\t You instructed the API server to create a new replica set, using kubectl. The API \nserver stores this new resource in etcd.\n2\t The controller manager watches for creation and modification of replica sets. It \nreceives a notification about the new set you created.\n3\t The controller manager compares the current state of the cluster to the new \nstate, determining that it needs to create new pods. It creates these pod resources \nthrough the API server, based on the template you provided through kubectl.\n4\t The scheduler receives a notification about a new pod and assigns it an appro-\npriate node, again updating the pod’s definition through the API server. At this \npoint, you haven’t run any real application — the controllers and scheduler have \nonly updated the state that the API server is storing.\n5\t Once the pod is assigned to a node, the API server notifies the appropriate \nkubelet, and the kubelet instructs Docker to run containers. Images are down-\nloaded, containers are started, and the kubelet begins to monitor their opera-\ntion. At this point, your pods are running!\nAs you can see, each component acts independently, but together, they orchestrate a \ncomplex deployment action. Hopefully this has given you a useful glance under the \ncover.  Now, back to running your microservices.\n9.2.4\t\nHealth checks\nYou’re missing something. Unlike a typical cloud load balancer, a Kubernetes service \ndoesn’t itself execute health checks on your underlying application. Instead, the ser-\nvice checks the shared state of the cluster to determine if a pod is ready to receive \nrequests. But how do you know if a pod is ready?\nIn chapter 6, we introduced two types of health check:\n¡ Liveness  — Whether an application has started correctly\n¡ Readiness  — Whether an application is ready to serve requests\nThese health checks are crucial to the resiliency of your service. They ensure that traf-\nfic is routed to healthy instances of your microservice and away from instances that are \nperforming poorly (or not at all).\nBy default, Kubernetes executes lightweight, process-based liveness checks for every \npod you run. If one of your market-data containers fails a liveness check, Kubernetes \nwill attempt to restart that container (as long as the container’s restart policy isn’t set \nto Never). The kubelet process on each worker node carries out this health check. This \nprocess continually queries the container runtime (in your case, the Docker daemon) \nto establish whether it needs to restart a container.\n \n",
      "page_number": 234
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 250-261)",
      "start_page": 250,
      "end_page": 261,
      "detection_method": "topic_boundary",
      "content": "234\nChapter 9  Deployment with containers and schedulers \nTIP    Kubernetes performs restarts on an exponential back-off schedule; if a \npod isn’t live after five minutes, it’ll be marked for deletion. If a replica set man-\nages the pod, the controller will attempt to schedule a new pod to maintain the \ndesired service capacity.\nThis alone isn’t adequate, as your microservice may run into failure scenarios that \ndon’t cause the container itself to fail: whether deadlocks due to request saturation, \ntimeouts of underlying resources, or a plain old coding error. If the scheduler can’t \nidentify this scenario, performance can deteriorate as a service routes requests to unre-\nsponsive pods, potentially leading to cascading failures.\nTo avoid this situation, you need the scheduler to continually check the state of the \napplication inside your container, ensuring it’s both live and ready. With Kubernetes, \nyou can configure probes to achieve this, which you can define as part of your pod \ntemplate. Figure 9.14 illustrates how these checks, and the previous process check, will \nbe run.\nAdding probes is straightforward, although you do need to add some configuration \nsee the next listing to the container specification in market-data-replica-set.yml. Probes \ncan be HTTP GET requests, scripts executed inside a container, or TCP socket checks. In \nthis case, you’ll use a GET request, as shown in the following listing.\nKubelet\nUpdates\nPod status\nReady\nLive\nApplication ready?\nApplication live?\nContainer running?\nChecks status\nmarket-data\nservice\nRoutes requests\nmarket-data container\nApplication\nDocker\nruntime\nFigure 9.14    The kubelet process on each worker node runs health checks, or probes, in Kubernetes. \nReadiness probe results control routing by services.\n \n\n\n\t\n235\nDeploying to a cluster\nListing 9.6    Liveness probe in market-data-replica-set.yml \nlivenessProbe: \n  httpGet: \n    path: /ping \n    port: 8000 \n  initialDelaySeconds: 10 \n  timeoutSeconds: 15 \nreadinessProbe: \n    path: /ping \n    port: 8000 \n  initialDelaySeconds: 10 \n  timeoutSeconds: 15 \nReapply this configuration using kubectl to update the state of the replica set. Kuber-\nnetes will, to the best of its ability, use these probes to help ensure instances of your \nmicroservice are alive and kicking. In this example, both liveness and readiness check \nthe same endpoint, but if your microservice has external dependencies, such as a \nqueueing service, it makes sense to make readiness dependent on connectivity from \nyour application to those dependencies.\n9.2.5\t\nDeploying a new version\nYou should now understand how you use replica sets, pods, and services to run stateless \nmicroservices on Kubernetes. On top of these concepts, you can build a stable, seam-\nless deployment process for each of your microservices. In chapter 8, you learned about \ncanary deployments; in this section, you’ll try out the technique with Kubernetes.\nDeployments\nBefore we get started, we should quickly introduce deployments. Kubernetes provides \na higher level abstraction, the Deployment object, for orchestrating the deployment of \nnew replica sets. Each time you update a deployment, the scheduler will orchestrate a \nrolling update of instances in a replica set, ensuring they’re deployed seamlessly.\nYou can change the original approach to use a deployment instead. First, delete your \noriginal replica set:\n$ kubectl delete replicaset market-data\nAfter that, create a new file, market-data-deployment.yml. This should be similar to the \nreplica set you created earlier, except that the type of object should be Deployment, \nrather than ReplicaSet, as shown in the following listing.\nListing 9.7    market-data-deployment.yml\n---\napiVersion: extensions/v1beta1\nkind: Deployment \nmetadata:\n  name: market-data\nspec: \n  replicas: 3 \nConfigures a liveness probe to query /ping on port 8000\nConfigures a readiness probe to query /ping on port 8000\nDefines a Kubernetes deployment object\nThe desired number of pods to deploy\n \n\n\n236\nChapter 9  Deployment with containers and schedulers \n  template: \n    metadata:\n      labels:\n        app: market-data\n        tier: backend\n        track: stable\n    spec:\n      containers:\n      - name: market-data\n        image: <docker id>/market-data:latest\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 8000\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8000\n          initialDelaySeconds: 10\n          timeoutSeconds: 15\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8000\n          initialDelaySeconds: 10\n          timeoutSeconds: 15\nUse kubectl to apply this file to the cluster. This will create a deployment, which will \ncreate a replica set and three instances of the market-data pod.\nCanaries\nIn a canary deploy, you deploy a single instance of a microservice to ensure that a new \nbuild is stable when it faces real production traffic. This instance should run alongside \nexisting production instances. A canary release has four steps:\n1\t You release a single instance of a new version alongside the previous version.\n2\t You route some proportion of traffic to the new instance.\n3\t You assess the health of the new version by, for example, monitoring error rates \nor observing behavior.\n4\t If the new version is healthy, you commence a full rollout to replace other \ninstances. If not, you remove the canary instance, halting the release.\nOn Kubernetes, you can use labels to identify a canary pod. In the first example, you \nspecified a label track: stable on each pod in your replica set. To deploy a canary, \nyou’ll need to deploy a new pod that’s distinguished with track: canary. The service \nyou created earlier only selects on two labels (app and tier), so it’ll route requests to \nboth stable and canary pods. This is illustrated in figure 9.15.\nThe template to use for creating each pod\n \n\n\n\t\n237\nDeploying to a cluster\nRequests\nPort\nThe service routes requests\nto the new canary pod\nbased on the label selector.\napp: market-data\ntier: backend\nReplica set\n(market-data-canary)\napp: market-data\ntier: backend\ntrack: stable\n5000\napp: market-data\ntier: backend\ntrack: stable\n5000\napp: market-data\ntier: backend\ntrack: stable\n5000\napp: market-data\ntier: backend\ntrack: stable\n5000\nReplica set (market-data)\nFigure 9.15    The service forwards requests to your new canary pod based on the service’s label selector, \nwhich doesn’t restrict on track.\nFirst, you should build a new container for your new release. You’ll use tags to identify \nthe new version, and don’t forget to substitute your own Docker ID:\n$ docker build -t <docker id>/market-data:v2 .\n$ docker push <docker id>/market-data:v2\nThis version is tagged as v2, although in practice it may not be appropriate to apply a \nnumeric versioning scheme to your services. We’ve found tagging them with the com-\nmit ID also works well. (For Git repositories, we use git rev-parse --short HEAD.)\nOnce you’ve pushed that new image, create a yml file specifying your canary \ndeployment:\n¡ It should create one replica, instead of three.\n¡ It should release the v2 tag of the container, rather than latest.\n¡ It should look like the following listing.\nListing 9.8    market-data-canary-deployment.yml\n---\napiVersion: extensions/v1beta1\nkind: Deployment\n \n\n\n238\nChapter 9  Deployment with containers and schedulers \nmetadata:\n  name: market-data-canary\nspec:\n  replicas: 1 \n  template:\n    metadata:\n      labels:\n        app: market-data \n        tier: backend \n        track: canary \n    spec:\n      containers:\n      - name: market-data\n        image: <docker id>/market-data:v2 \n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 8000\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8000\n          initialDelaySeconds: 10\n          timeoutSeconds: 15\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8000\n          initialDelaySeconds: 10\n          timeoutSeconds: 15\nUse kubectl to apply this to your cluster. The deployment will create a new replica set \ncontaining a single canary pod for v2.\nLet’s take a closer look at the state of your cluster. If you run minikube dashboard on \nthe command line, it’ll open the dashboard for your cluster in a browser window (fig-\nure 9.16). In the dashboard — under Workloads — you should be able to see:\n¡ The canary deployment you’ve just created, as well as your original deployment\n¡ Four pods: the original three, plus a canary pod\n¡ Two replica sets: one each for the stable and canary tracks\nSo far so good! At this stage, for a real microservice, you might run some automated tests, \nor check the monitoring output of your service to ensure it’s processing work as expected. \nFor now, you can safely assume your canary is healthy and performing as expected, which \nmeans you can safely roll out the new version, replacing all your old instances.\nEdit the market-data-deployment.yml file and make two changes:\n¡ Change the container used to market-data:v2.\n¡ Add a strategy field to specify how pods will be updated.\nYou want to create one canary.\nThe canary deployment has \na distinct set of labels.\nThis deployment will release \nmarket-data:v2.\n \n\n\n\t\n239\nDeploying to a cluster\nFigure 9.16    The Kubernetes dashboard after multiple deploys — stable and canary — of the market-data \nmicroservice\nYour updated deployment file should look like the following listing.\nListing 9.9    Updated market-data-deployment.yml\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: market-data\nspec:\n  replicas: 3\n  strategy: \n    type: RollingUpdate \n    rollingUpdate: \n      maxUnavailable: 50% \n      maxSurge: 50% \n  template:\n    metadata:\n      labels:\n        app: market-data\n        tier: backend\n        track: stable\n    spec:\n      containers:\n      - name: market-data\n        image: morganjbruce/market-data:v2\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\nThe strategy field describes how Kubernetes \nwill execute the deployment of new pods.\n \n\n\n240\nChapter 9  Deployment with containers and schedulers \n        ports:\n        - containerPort: 8000\nApplying this configuration will create a new replica set, starting instances one by one \nwhile removing them from the original set.  This process is illustrated in figure 9.17.\nYou can also observe this in the event history of the controller by running kubectl \ndescribe deployment/market-data (figure 9.18).\nFrom this history, you can see how Kubernetes allows you to build higher level \ndeployment operations on top of simple operations. In this case, the scheduler used \nyour desired state of the world and a set of constraints to determine an appropriate \npath of deployment, but you could use replica sets and pods to build any deployment \npattern that was appropriate for your service.\nca\nDeployment\nCreates a new replica set\nv1\nv1\nv1\nReplica set A\nReplica set B\n1\nv1\nv1\nv1\nv2\nv2\nReplica set A\nReplica set B\n2\nv1\nv1\nv2\nv2\nv2\nReplica set A\nReplica set B\n4\nv1\nv1\nv2\nv2\nReplica set A\nReplica set B\n3\n5\nDeployment\nScales up number\nof new instances\nScales up number\nof new instances\nDeployment\nDeployment\nDeployment\nThe empty replica set A is\nmaintained to enable rollback.\nScales down\nold instances\nScales down\nold instances\nv2\nv2\nv2\nReplica set A\nReplica set B\nv2\nv2\nv2\nReplica set B\n6\nFigure 9.17    A new deployment creates a new replica set and progressively rolls instances between the \nold and new set.\nFigure 9.18    Events Kubernetes emitted during a rolling deployment\n \n\n\n\t\n241\nDeploying to a cluster\n9.2.6\t\nRolling back\nWell done! You’ve smoothly deployed a new version of your microservice.  If something \nwent wrong, you can also use the deployment object to undo all your hard work. First, \ncheck the rollout history:\n$ kubectl rollout history deployment/market-data\nThis should return two revisions: your original deployment and your v2 deployment. \nTo roll back, specify the target revision:\n$ kubectl rollout undo deployment/market-data --to-revision=1\nThis will perform the reverse of the previous rolling update to return the underlying \nreplica set to its original state.\n9.2.7\t\nConnecting multiple services\nLastly, your microservice isn’t going to be much use by itself, and several of Simple-\nBank’s services depend on the capabilities that the market-data service provides. It’d \nbe pretty much insane to hardcode a port number or an IP address into each service to \nrefer to the underlying endpoint of each collaborator; you shouldn’t tightly couple any \nservice to another’s internal network location. Instead, you need some way of accessing \na collaborator by a known name.\nKubernetes integrates a local DNS service to achieve this, and it runs as a pod on \nthe Kubernetes master. When new service is created, the DNS service assigns a name \nin the format {my-svc}.{my-namespace}.svc.cluster.local; for example, you \nshould be able to resolve your market-data service from any other pod using the name \nmarket-data.default.svc.cluster.local.\nGive it a shot. You can use kubectl to run an arbitrary container in your cluster — try \nbusybox, which is a great little image containing several common Linux utilities, such as \nnslookup. Run the following command to open a command prompt inside a container \nrunning on Minikube:\n$ kubectl run -i --tty lookup --image=busybox /bin/sh\nThen you can try an nslookup:\n/ # nslookup market-data.default.svc.cluster.local\nYou should get output that looks something like this:\nServer:    10.0.0.10\nAddress 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local\nName:      market-data.default.svc.cluster.local\nAddress 1: 10.0.0.156 market-data.default.svc.cluster.local\nThe IP address in the last entry should match the cluster IP assigned to your service. \n(If you don’t believe me, you can double-check by calling kubectl get services.) If \nso, success! You’ve covered a lot of ground: building and storing an image for a micro-\nservice, running it on Kubernetes, load-balancing multiple instances, deploying a new \nversion (and rolling back), and connecting microservices together. \n \n\n\n242\nChapter 9  Deployment with containers and schedulers \nSummary\n¡ Packaging microservices as immutable, executable artifacts allows you to \norchestrate deployment through a primitive operation — adding or removing a \ncontainer.\n¡ Schedulers and containers abstract away underlying machine management for \nservice development and deployment.\n¡ Schedulers work by trying to match the resource needs of an application to the \nresource usage of a cluster of machines, while health-checking running services \nto ensure they’re operating correctly.\n¡ Kubernetes provides ideal features of a microservice deployment platform, \nincluding secret management, service discovery, and horizontal scalability.\n¡ A Kubernetes user defines the desired state (or specification) of their cluster ser-\nvices, and Kubernetes figures out how to achieve that state, executing a continual \nloop of observe-diff-act.\n¡ The logical application unit on Kubernetes is a pod: one or more containers that \nexecute together.\n¡ Replica sets manage the lifecycle of groups of pods, starting new pods if existing \nones fail.\n¡ Deployments on Kubernetes are designed to maintain service availability by exe-\ncuting rolling updates of pods across replica sets.\n¡ You can use service objects to group underlying pods and make them available to \nother applications inside and outside of the cluster.\n \n\n\n243\n10\nBuilding a delivery \npipeline for microservices\nThis chapter covers\n¡ Designing a continuous delivery pipeline for a \nmicroservice\n¡ Using Jenkins and Kubernetes to automate \ndeployment tasks\n¡ Managing staging and production \nenvironments\n¡ Using feature flags and dark launches to \ndistinguish between deployment and release\nRapidly and reliably releasing new microservices and new features to production is \ncrucial to successfully maintaining a microservice application. Unlike a monolithic \napplication, where you can optimize deployment for a single use case, microservice \ndeployment practices need to scale to multiple services, written in different lan-\nguages, and each with their own dependencies. Investing in consistent and robust \ndeployment tooling and infrastructure will go a long way toward making a success of \nany microservice project.\nYou can achieve reliable microservice releases by applying the principles of con-\ntinuous delivery. The fundamental building block of continuous delivery is a deploy-\nment pipeline. Picture a factory production line: a conveyer belt takes your software \n \n\n\n244\nChapter 10  Building a delivery pipeline for microservices \nfrom code commits to deployable artifact to running software, while continually assess-\ning the quality of the output at each stage. Doing this leads to frequent, small deploy-\nments, rather than big-bang changes, to production.\nSo far, you’ve built and deployed a service using Docker, Kubernetes, and com-\nmand-line scripts. In this chapter, you’ll combine those steps into an end-to-end build \npipeline, using Jenkins, a widely used open source build automation tool. Along the \nway, we’ll examine how this approach minimizes risk and increases the stability of your \noverall application. After that, we’ll examine the difference between deploying new \ncode and releasing new features.\n10.1\t Making deploys boring\nDeploying software should be boring. You should be able to roll out changes and \nnew features without peering through your fingers or obsessively watching error \ndashboards.\nUnfortunately, as we mentioned in chapter 8, human error causes most issues in pro-\nduction, and microservice deployments leave plenty of room for that! Consider the big \npicture: teams are developing and deploying tens — if not hundreds — of independent \nservices on their own schedule, without explicit coordination or collaboration between \nteams. Any bad change to a service might have a wide-ranging impact on the perfor-\nmance of other services and the wider application.\nAn ideal microservice deployment process should meet two goals:\n¡ Safety at pace  — The faster you can deploy new services and changes, the quicker \nyou can iterate and deliver value to your end users. Deployment should maxi-\nmize safety: you should validate, as much as feasible, that a given change won’t \nnegatively impact the stability of a service.\n¡ Consistency  — Consistency of deployment process across different services, \nregardless of underlying tech stack, helps alleviate technical isolation and makes \noperations more predictable and scalable.\nIt’s not easy to maintain the fine balance between safety and pace. You could move \nquickly without safety by deploying code changes directly to production, but that’d \nbe crazy. Likewise, you could achieve stability by investing in a time-consuming \nchange-control and approval process, but that wouldn’t scale well to the high volume \nof change in a large, complex microservice application.\n10.1.1\t A deployment pipeline\nContinuous delivery strikes an ideal balance between reducing risk and increasing speed:\n¡ Releasing smaller sets of commits increases safety by reducing the amount of \nchange happening at any one time. Smaller changesets are also easier to reason \nthrough.\n¡ An automated pipeline of commit validation increases the probability that a \ngiven changeset is free from defects.\n \n\n\n\t\n245\nMaking deploys boring\nReleasing small changesets and systematically verifying their quality gives teams the \nconfidence to release features rapidly. Smaller, more atomic releases are less risky. \nThe continuous delivery approach empowers teams to ship services rapidly and \nindependently.\nOne of the weaknesses of monolithic development is that releases often become \nlarge, coupling together disparate features at release time. Likewise, even small \nchanges in a large application can have an unintentionally broad impact, particularly \nwhen made to cross-cutting concerns. At worst, commits in monolithic development \nbecome stale while waiting for a deployment; they’re no longer relevant to the needs of \nthe application or business by the time they reach customers.\nNOTE    Continuous delivery isn’t quite the same as continuous deployment. \nIn the latter, every validated change is automatically deployed to production; \nin the former, you can deploy every change to production, but whether you \ndeploy it or not is up to the engineering team and business needs.\nLet’s look at an example in figure 10.1. Most of the steps in this pipeline should look \nfamiliar:\n1\t First, an engineer commits some code to a microservice repository.\n2\t Next, a build automation server builds the code.\n3\t If the build is successful, the automation server runs unit tests to validate that \ncode.\n4\t If these tests pass, the automation server packages the service for deployment \nand stores this package in an artifact repository.\n5\t The automation server deploys code to a staging environment, where you can \ntest the service against other live collaborators.\n6\t If this is successful, the automation server will deploy the code to a production \nenvironment.\nEngineer\nCommits\nNotify\nService\ncode\nDeployment pipeline\nFailed build\nFailed tests\nUnit test\nBuild\nPackage\nDeploy to\nstaging\nTest\nFailed tests\nFailed push\nFailure at a stage produces feedback.\nArtifact\nrepository\nStaging\nenvironment\nProduction\nenvironment\nDeploys to\nDeploys to\nStores in\nDeploy to\nproduction\nFigure 10.1    An example deployment pipeline builds, validates, and deploys a commit to production, providing \nfeedback to engineers.\n \n",
      "page_number": 250
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 262-269)",
      "start_page": 262,
      "end_page": 269,
      "detection_method": "topic_boundary",
      "content": "246\nChapter 10  Building a delivery pipeline for microservices \nEach step in this pipeline provides feedback to the engineering team on the correct-\nness of their code. For example, if step 3 fails, you’ll receive a list of failed test asser-\ntions to correct.\nImplementing this pipeline should make the process of deployment highly visible \nand transparent — crucial for an audit trail, or if something goes wrong. Regardless of \nthe underlying language or technology, every service you deploy should be able to fol-\nlow a similar process.\n10.2\t Building a pipeline with Jenkins\nIn the previous chapter, you ran command-line scripts to perform steps in deploy-\nment: building containers, publishing artifacts, and deploying code. Now you’ll use \nJenkins — a build automation tool — to connect those steps together into a coher-\nent, reusable, and extensible deployment pipeline. We’ve picked Jenkins because \nit’s open source, is easy to get running, supports scriptable build jobs, and is widely \nused.\nUnfortunately, no perfect out-of-the-box solution for deployment is available: any \npipeline is usually a combination of multiple tools, depending on both the service’s \ntech stack and the target deployment platform. In your case, you’ll be using Jenkins to \nassemble tools you’ve (mostly) already used. Figure 10.2 illustrates the components of \nyour deployment pipeline.\nIn the next few sections, we’re going to cover a lot of ground:\n¡ Using Jenkins to script complex deployment pipelines\n¡ Building a pipeline that builds, tests, and deploys your service to different \nenvironments\n¡ Managing staging environments for microservices\n¡ Reusing your deployment pipeline across multiple services\nJenkins orchestrates the pipeline, combining multiple tools.\nPython-specific tools\nfor managing\ndependencies and\nrunning tests\nDocker\nPytest\nPypi\nBuild\nUnit test\nPackage\nJenkins\nDeploy to\nstaging\nTest\nDeploy to\nproduction\nManual\nKubernetes,\nkubectl\nKubernetes,\nkubectl\nYou deploy services to\nKubernetes using kubectl.\nYou use Docker to build and\npackage a deployable artifact.\nFigure 10.2    The deployment pipeline you’ll use, which combines multiple tools dependent on the tech stack \nand target deployment platform you’re using\n \n\n\n\t\n247\nBuilding a pipeline with Jenkins\nYou’ll need to have access to a running Jenkins instance to run the examples in this \nchapter. The appendix walks you through Jenkins setup on a local Minikube cluster —  \nwe’ll assume you’re using that approach in the following sections.\n10.2.1\t Configuring a build pipeline\nThe Jenkins application consists of a master node and, optionally, any number of \nagents. Running a Jenkins job executes scripts (using common tools, such as make \nor Maven) across these agent nodes to perform deployment activities. A job operates \nwithin a workspace  — a local copy of your code repository. Figure 10.3 illustrates this \narchitecture.\nTo write your build pipeline, you’re going to use a feature called Scripted Pipe-\nline. In Scripted Pipeline, you can express a build pipeline using a general-purpose \ndomain-specific language (DSL) written in Groovy. This DSL defines common methods \nfor writing build jobs, such as sh (for executing shell scripts) and stage (for identifying \ndifferent parts of a build pipeline). The Scripted Pipeline approach is more powerful \nthan you might think — by the end of the chapter, you’ll use it to build your own higher \nlevel, declarative DSL.\nNOTE    At the time of writing, Jenkins Pipeline only supports Groovy as a script-\ning language. Don’t worry — if you’re comfortable with Java, Python, or Ruby, \nunderstanding Groovy won’t be too taxing.\nJenkins will execute build jobs by executing a pipeline script defined in a Jenkinsfile. \nTry it yourself! First, copy the contents of chapter-10/market-data into a new directory \nand push that to a Git repo. It’s easiest if you push it to somewhere public, like GitHub. \nThis is the service you’ll be deploying in this chapter.\nWorkspace\nCloned from\nExecuted on\nJob {scripts}\nSource repo\nJenkins\nMaster\nAgent\nWorkspace\nAgent\nFigure 10.3    A Jenkins deployment consists of a master node, which manages execution, and agents \nthat perform build tasks within a workspace — a clone of the repository being built.\n \n\n\n248\nChapter 10  Building a delivery pipeline for microservices \nNext, you’ll want to create a Jenkinsfile in the root of your repository, and it should \nlook like the following listing.\nListing 10.1    A basic Jenkinsfile\nstage(\"Build Info\") { \n  node { \n    def commit = checkout scm \n    echo \"Latest commit id: ${commit.GIT_COMMIT}\"\n  }\n}\nWhen Jenkins runs this script, the script will check out a code repository as a workspace \nand write the latest commit ID to the console.\nYou can try this out by setting up a pipeline job for a service. Commit the Jenkinsfile \nyou just created, then push your changes to origin. Now, open the Jenkins UI. (Remem-\nber, you can do this with minikube service jenkins.) Follow these steps to create a \nmultibranch pipeline job:\n1\t Navigate to the Create New Jobs page.\n2\t Enter an item name, market-data; select Multibranch Pipeline as the job type; \nand click OK.\n3\t On the following page (see figure 10.4), select a Branch Source of Git and add \nyour repository’s clone URL to the Project Repository field. If you’re using a pri-\nvate Git repository, you’ll also need to configure your credentials.\n4\t Elect to periodically scan the pipeline every minute. This will trigger builds if \nchanges are detected.\n5\t Save your changes.\nOnce you’ve saved your changes, Jenkins will scan your repository for branches con-\ntaining a Jenkinsfile. The multibranch pipeline job type will generate a unique build \nfor each branch in your repository — later, this will let you treat feature branches differ-\nently from the master branch.\nTIP    Instead of clicking through the UI, you can use the Jenkins Job DSL to \ngenerate pipeline jobs. This is (another) Groovy DSL that generates jobs in \nJenkins’ underlying XML format. You can find examples in the project docu-\nmentation (https://github.com/jenkinsci/job-dsl-plugin/wiki).\nOnce the indexing is complete, Jenkins will run a build for your master branch. \nClicking on the name of the branch will take you the build history for that branch \n(figure 10.5).\nIdentifies a distinct phase of your pipeline\nTakes a closure (or function) as a parameter, instructing \nJenkins to execute this code on a build node\nChecks out some code \nfrom source control\n \n\n\n\t\n249\nBuilding a pipeline with Jenkins\nFigure 10.4    New project configuration screen, showing Branch Sources options\nClick the build number and then Console Output. This traces the output of the build. \nWithin that output, you should be able to see how your Jenkinsfile has been executed:\nAgent default-q3ccc is provisioned from template Kubernetes Pod Template\nAgent specification [Kubernetes Pod Template] (jenkins-jenkins-slave): \n* [jnlp] jenkins/jnlp-slave:3.10-1(resourceRequestCpu: 200m, resourceRequest\n➥Memory: 256Mi, resourceLimitCpu: 200m, resourceLimitMemory: 256Mi)\nRunning on default-q3ccc in /home/jenkins/workspace/market-data_master\n➥-27MDVADAYDBX5WJSRWQIFEL3T7GD4LWPU5CXCZNTJ4CKBDLP3LVA\n[Pipeline] {\n[Pipeline] checkout\nCloning the remote Git repository\nCloning with configured refspecs honoured and without tags\nCloning repository https://github.com/morganjbruce/market-data.git\n > git init /home/jenkins/workspace/market-data_master\n[CA}-27MDVADAYDBX5WJSRWQIFEL3T7GD4LWPU5CXCZNTJ4CKBDLP3LVA # timeout=10\nFetching upstream changes from https://github.com/morganjbruce/\n➥market-data.git\n > git --version # timeout=10\n > git fetch --no-tags --progress https://github.com/morganjbruce/\n➥market-data.git +refs/heads/*:refs/remotes/origin/*\n > git config remote.origin.url https://github.com/morganjbruce/\n➥market-data.git # timeout=10\n > git config --add remote.origin.fetch +refs/heads/*:refs/remotes/origin/\n➥* # timeout=10\n > git config remote.origin.url https://github.com/morganjbruce/\n➥market-data.git # timeout=10\nFetching without tags\n \n\n\n250\nChapter 10  Building a delivery pipeline for microservices \nFetching upstream changes from https://github.com/morganjbruce/\n➥market-data.git\n > git fetch --no-tags --progress https://github.com/morganjbruce/\n➥market-data.git +refs/heads/*:refs/remotes/origin/*\nChecking out Revision 80bfb7bdc4fa0b92dcf360393e5d49e0f348b43b (master)\n > git config core.sparsecheckout # timeout=10\n > git checkout -f 80bfb7bdc4fa0b92dcf360393e5d49e0f348b43b\nCommit message: \"working through ch10\"\nFirst time build. Skipping changelog.\n[Pipeline] echo\nLatest commit id: 80bfb7bdc4fa0b92dcf360393e5d49e0f348b43b\n[Pipeline] }\n[Pipeline] // node\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] End of Pipeline\nFinished: SUCCESS\nFigure 10.5    Build history for the master branch of your repository\n \n\n\n\t\n251\nBuilding a pipeline with Jenkins\nEach [Pipeline] step traces the execution of your code. Awesome — you’ve deployed \na build automation tool, configured it against a service repository, and run your first \nbuild pipeline! Next, let’s look at the first stage of your pipeline: build.\n10.2.2\t Building your image\nYou’re going to use Docker to build and package your images. First, let’s change your \nJenkinsfile, as shown in the following listing.\nListing 10.2    Jenkinsfile for build step\ndef withPod(body) { \n  podTemplate(label: 'pod', serviceAccount: 'jenkins', containers: [\n      containerTemplate(name: 'docker', image: 'docker', command: 'cat', \n➥ttyEnabled: true),\n      containerTemplate(name: 'kubectl', image: 'morganjbruce/kubectl', \n➥command: 'cat', ttyEnabled: true)\n    ],\n    volumes: [\n      hostPathVolume(mountPath: '/var/run/docker.sock', hostPath: \n➥'/var/run/docker.sock'),\n    ]\n ) { body() }\n}\nwithPod {\n  node('pod') { \n    def tag = \"${env.BRANCH_NAME}.${env.BUILD_NUMBER}\"\n    def service = \"market-data:${tag}\"\n    \n    checkout scm \n    container('docker') { \n      stage('Build') { \n        sh(\"docker build -t ${service} .\") \n      }\n    }\n  }\n}\nThis script will build your service and tag the resulting Docker container with the cur-\nrent build number. It’s definitely more complex than the earlier version, so let’s take a \nquick walk through what you’re doing:\n1\t You define a pod template for your build, which Jenkins will use to create pods \non Kubernetes for a build agent. This pod contains two containers — Docker and \nkubectl.\nDefines a pod template to use to run your job\nRequests an instance of your pod template\nChecks out the latest code from Git\nEnters the Docker container of your pod\nStarts a new pipeline stage\nRuns a docker command to build your service image\n \n\n\n252\nChapter 10  Building a delivery pipeline for microservices \n2\t Within that pod, you check out the latest version of your code from Git.\n3\t You then start a new pipeline stage, which you’ve called Build.\n4\t Within that stage, you enter the Docker container and run a docker command to \nbuild your service image.\nTIP    Jenkins also provides a Groovy DSL for Docker instead of the shell com-\nmands you’ve used. For example, you could use docker.build(imageName)in \nplace of the sh call in listing 10.5.\nCommit this new Jenkinsfile to your Git repo and navigate to the build job on Jenkins. \nWait for a rerun — or trigger the job manually — and in the console output, you should \nsee your container image being built successfully.\n10.2.3\t Running tests\nNext, you should run some tests. This should be like any other continuous integration \njob: if the tests are green, deployment can proceed; if not, you halt the pipeline. At this \nstage, you aim to provide rapid and accurate feedback on the quality of a changeset. \nFast test suites help engineers iterate quickly.\nBuilding your code and performing unit tests are only two of the possible activities \nyou might perform during this commit stage of the build pipeline. Table 10.1 outlines \nother possibilities.\nTable 10.1    Possible activities in the commit stage of a deployment pipeline\nActivity\nDescription\nUnit tests\nCode-level tests\nCompilation\nCompiling the artifact into an executable artifact\nDependency resolution\nResolving external dependencies — for example, open source packages\nStatic analysis\nEvaluating code against metrics\nLinting\nChecking syntax and stylistic principles of code\nFor now, you should get your unit tests running. Add a Test stage to your Jenkinsfile, \nimmediately after the Build stage as shown in the next listing.\nListing 10.3    Test stage\nstage('Test') {\n  sh(\"docker run --rm ${service} python setup.py test\")\n}\nCommit your Jenkinsfile and run the build. This will add a new stage to your pipeline, \nwhich executes the test cases defined in /tests (figure 10.6).\n \n\n\n\t\n253\nBuilding a pipeline with Jenkins\nFigure 10.6    Your pipeline so far with Build and Test stages\nUnfortunately, this code alone won’t make the results visible in the build. Only success \nor failure will do that. You can archive the XML results you’re generating by adding the \nfollowing to your Jenskinsfile.\nListing 10.4    Archiving results from test stage\nstage('Test') {\n  try {\n    sh(\"docker run -v `pwd`:/workspace --rm ${service} \n➥python setup.py test\") \n  } finally {\n    step([$class: 'JUnitResultArchiver', testResults: \n➥'results.xml']) \n  }\n} \nMounts the current \nworkspace as a volume\nArchives the results that the test job generates\n \n",
      "page_number": 262
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 270-277)",
      "start_page": 270,
      "end_page": 277,
      "detection_method": "topic_boundary",
      "content": "254\nChapter 10  Building a delivery pipeline for microservices \nThis code mounts the current workspace as a volume within the Docker container. The \npython test process will write output to that volume as /workspace/result.xml, and you \ncan access those results even after Docker has stopped and removed the service con-\ntainer. You use the try–finally statement to ensure you achieve results regardless of \npass or failure.\nTIP    A good build pipeline directs feedback to the responsible engineering \nteam. For example, our deployment pipelines at Onfido notify commit authors \nthrough Slack and email if pipeline stages fail. We also emit events from our \npipeline for monitoring tools such as PagerDuty to consume. For more on \nsending notifications, see the Jenkins documentation (http://mng.bz/C5X3).\nCommitting your changed Jenkinsfile and running a fresh build will store test results \nin Jenkins. You can view them on the build page. Great — you’ve validated your under-\nlying code. Now you’re almost ready to deploy.\n10.2.4\t Publishing artifacts\nYou need to publish an artifact — in this case, our Docker container image — to be able \nto deploy it. If you used a private Docker registry in chapter 9, you’ll need to configure \nyour Docker credentials within Jenkins:\n1\t Navigate to Credentials > System > Global Credentials > Add Credentials.\n2\t Add username and password credentials, using your credentials to https://hub \n.docker.com.\n3\t Set the ID as dockerhub and click OK to save these credentials.\nIf you’re using a public registry, you can skip this step. Either way, when you’re ready, \nadd a third step to your Jenkinsfile, as follows.\nListing 10.5    Publishing artifacts\ndef tagToDeploy = \"[your-account]/${service}\" \nstage('Publish') {\n  withDockerRegistry(registry: [credentialsId: \n➥'dockerhub']) { \n    sh(\"docker tag ${service} ${tagToDeploy}\") \n    sh(\"docker push ${tagToDeploy}\")\n  }\n}\nThe target public image tag — replace \nwith your account name\nLogs in to the Docker registry \nusing stored credentials\nTags the image with your Docker account name\n \n\n\n\t\n255\nBuilding a pipeline with Jenkins\nWhen you have that ready, commit and run your build. Jenkins will publish your con-\ntainer to the public Docker registry.\n10.2.5\t Deploying to staging\nAt this point, you’ve tested the service internally but in complete isolation; you haven’t \ninteracted with any of the service’s upstream or downstream collaborators. You could \ndeploy directly to production and hope for the best, but you probably shouldn’t. Instead, \nyou can deploy to a staging environment where you can run further automated and man-\nual tests against real collaborators.\nYou’re going to use Kubernetes namespaces to logically segregate your staging \nand production environments. To deploy your service, you’ll use kubectl, using an \napproach similar to the one you took in chapter 9. Rather than installing the tool on \nJenkins, you can use Docker to wrap this command-line tool. This is quite a useful \ntechnique.\nWARNING    Logical segregation isn’t always appropriate in a real-world environ-\nment. Compliance and security standards, such as PCI DSS, often mandate net-\nwork-level isolation between production and development workloads, which \nKubernetes namespaces wouldn’t currently satisfy. In addition, completely \nseparating staging and production infrastructure reduces the risk of a “noisy \nneighbor” in staging, such as a resource-hungry service, affecting production \nreliability.\nFirst, let’s look at your deployment and service definition. You should save the follow-\ning listing to deploy/staging/market-data.yml within your market-data repo.\nListing 10.6    Deployment specification for market-data\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: market-data\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 50%\n      maxSurge: 50%\n  template:\n    metadata:\n      labels:\n        app: market-data\n        tier: backend\n        track: stable\n    spec:\n      containers:\n      - name: market-data\n \n\n\n256\nChapter 10  Building a delivery pipeline for microservices \n        image: BUILD_TAG \n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 8000\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8000\n          initialDelaySeconds: 10\n          timeoutSeconds: 15\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8000\n          initialDelaySeconds: 10\n          timeoutSeconds: 15\nIf you saw this in chapter 9, you’ll notice one key difference: you don’t set a specific \nimage tag to deploy, only a placeholder of BUILD_TAG. You’ll replace this in your pipe-\nline with the version you’re deploying. This is a little unsophisticated — as you build \nmore complex deployments, you might want to explore higher level templating tools, \nsuch as ksonnet (https://ksonnet.io).\nYou’ll also want to add market-data-service.yml, as shown in the following listing, to \nthe same location.\nListing 10.7    market-data service definition\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: market-data\nspec:\n  type: NodePort\n  selector:\n    app: market-data\n    tier: backend\n  ports:\n    - protocol: TCP\n      port: 8000\n      nodePort: 30623\nBefore you deploy, create distinct namespaces to segregate your workloads, using \nkubectl:\nkubectl create namespace staging\nkubectl create namespace canary\nkubectl create namespace production\nA placeholder for the \nimage you want to deploy\n \n\n\n\t\n257\nBuilding a pipeline with Jenkins\nNow, add a deploy stage to your pipeline, as follows.\nListing 10.8    Deployment to staging (Jenkinsfile)\nstage('Deploy') {\n  sh(\"sed -i.bak 's#BUILD_TAG#${tagToDeploy}#' ./\n➥deploy/staging/*.yml\") \n  container('kubectl') {\n    sh(\"kubectl --namespace=staging apply -f deploy/\n➥staging/\") \n  }\n} \nAgain, commit and run the build. This time, a Kubernetes deploy should be triggered! \nYou can check the status of this deployment using kubectl rollout status:\n$ kubectl rollout status –n staging deployment/market-data\nWaiting for rollout to finish: 2 of 3 updated replicas are available... \ndeployment \"market-data\" successfully rolled out\nAs you can see, although your build was marked as complete, the deployment itself \ntakes some time to roll out. This is because kubectl apply works asynchronously and \ndoesn’t wait for the cluster to finish updating to reflect the new state. If you like, you \ncan add a call to the above kubectl rollout status method within the Jenkinsfile so \nthat Jenkins waits for rollouts to complete before proceeding.\nEither way, once the rollout is complete, you can access this service:\n$ curl `minikube service --namespace staging --url market-data`/ping\nHTTP/1.0 200 OK\nContent-Type: text/plain\nServer: Werkzeug/0.12.2 Python/3.6.1\nThis example service doesn’t do much. For your own services, you might trigger fur-\nther automated testing or perform further exploratory testing of the service and code \nchanges you’ve just deployed. Table 10.2 outlines some of the activities you might per-\nform at this stage of a deployment pipeline. For now, great work — you’ve automated \nyour first microservice deployment!\nTable 10.2    Possible activities to perform to validate a staging release of a microservice\nAcceptance testing\nAutomated tests\nRunning automated tests to check expectations, either \nregression or acceptance\nManual tests\nSome services may require manual validation or explor-\natory testing.\nNonfunctional testing\nSecurity tests\nTesting the security posture of the service\nLoad/capacity tests\nValidating expectations about capacity and load on a \nservice\nUses sed to replace BUILD_TAG with \nthe name of your new Docker image\nApplies all configuration files in deploy/staging to \nyour local cluster, using the staging namespace\n \n\n\n258\nChapter 10  Building a delivery pipeline for microservices \n10.2.6\t Staging environments\nLet’s take a break for a moment to discuss staging environments. You should make any \nnew release of a service to staging first. Microservices need to be tested together, and \nproduction isn’t the first place where that should happen.\nThe infrastructure configuration of your staging environment should be an exact \ncopy of production, albeit with less real traffic. It doesn’t need to run at the same scale. \nThe volume and type of testing you’ll use to put your services through their paces can \ndetermine the necessary size. As well as conducting various types of automated testing, \nyou might manually validate services in staging to ensure they meet acceptance criteria.\nAlong with shared staging environments, you might also run isolated staging envi-\nronments for individual or small sets of closely related services. Unlike full staging, \nthese environments might be ephemeral and spun up on-demand for the duration of \ntesting. This is useful for testing a feature in relative isolation, with tighter control of the \nstate of the environment. Figure 10.7 compares these different approaches to staging \nenvironments.\nAlthough staging environments are crucial, they can be hard to manage in a micro-\nservice application, as well as the source of significant contention between teams. A \nmicroservice might have many dependencies, all of which should be present and stable \nin full staging. Although a service in staging will have passed testing, code review, and \nother quality gates, it’s still possible that services in staging will be less stable than their \nproduction equivalents, and that can cause chaos. Any engineer deploying to a shared \nenvironment needs to act as a good neighbor to ensure that issues with services they \nown don’t substantially impact another team’s ability to smoothly test (and therefore \ndeliver) other services.\nTIP    To further reduce friction in staging, consider building your deployment \npipeline to allow any engineer to easily roll back the last deployment, regard-\nless of whether they own that service or not.\nC\nD\nB\nC\nD\nB\nA\nB\nA\nE\nIsolated staging environments may\ninclude a small subset of services.\nFull staging includes all services\nwithin an application.\nFigure 10.7    Isolated versus full staging environments\n \n\n\n\t\n259\nBuilding a pipeline with Jenkins\n10.2.7\t Deploying to production\nYou can use what you’ve learned so far to take this service to production. Table 10.3 \noutlines some of the different actions you might perform at this stage of your pipeline.\nTable 10.3    Possible activities to perform in deployment\nCode deployment\nDeploying code to a runtime environment\nRollback\nRolling back code to a previous version, if errors or unexpected behavior occurs\nSmoke tests\nValidating the behavior of a system using light-touch tests\n In this case, if a deployment to staging is successful, here’s what should happen next:\n1\t Your pipeline should wait for human approval to proceed to production.\n2\t When you have approval, you’ll release a canary instance first. This helps you vali-\ndate that your new build is stable when it faces real production traffic.\n3\t If you’re happy with the performance of the canary instance, the pipeline can \nproceed to deploy the remaining instances to production.\n4\t If you’re not happy, you can roll back your canary instance.\nFirst, you should add an approval stage. In continuous delivery — unlike continuous \ndeployment — you don’t necessarily want to push every commit immediately to pro-\nduction. Add the following to your Jenkinsfile.\nListing 10.9    Approving a production release\nstage('Approve release?') {\n  input message: \"Release ${tagToDeploy} to production?\"\n}\nRunning this code in Jenkins will show a dialog box in the build pipeline view, with two \noptions: Proceed or Abort. Clicking Abort will cancel the build; clicking Proceed will, \nfor now, cause the build to finish successfully — you haven’t added a deploy step!\nFirst, try a production deploy without a canary instance. Copy the YAML files you \ncreated earlier, from listings 10.6 and 10.7, to a new deploy/production directory. Feel \nfree to increase the number of replicas you’ll deploy.\nNext, add the code in the next listing to your Jenkinsfile, after the approval stage. \nThis is similar to the code you used in staging. Don’t worry about the code duplication \nfor now — you can work on that in a moment.\nListing 10.10    Production release stage\nstage('Deploy to production') {\n  sh(\"sed -i.bak 's#BUILD_TAG#${tagToDeploy}#' ./deploy/production/*.yml\")\n  container('kubectl') {\n \n\n\n260\nChapter 10  Building a delivery pipeline for microservices \n    sh(\"kubectl --namespace=production apply -f deploy/production/\")\n  }\n}\nAs always, commit and run the build in Jenkins. If successful, you’ve released to produc-\ntion! Let’s take this a few steps further and add some code to release a canary instance. \nBut before you add a new stage, let’s DRY up your code a little bit. You can move your \nrelease-related code into a separate file called deploy.groovy, as shown in the following \nlisting.\nListing 10.11    deploy.groovy\ndef toKubernetes(tagToDeploy, namespace, \n➥deploymentName) { \n  sh(\"sed -i.bak 's#BUILD_TAG#${tagToDeploy}#' ./deploy/${namespace}/*.yml\")\n  container('kubectl') {\n    kubectl(\"apply -f deploy/${namespace}/\")\n  }\n}\ndef kubectl(namespace, command) { \n  sh(\"kubectl --namespace=${namespace} ${command}\") \n} \ndef rollback(deploymentName) {\n  kubectl(\"rollout undo deployment/${deploymentName}\")\n}\nreturn this;\nThen you can load it in your Jenkinsfile, as shown in the following listing.\nListing 10.12    Using deploy.groovy in your Jenkinsfile\ndef deploy = load('deploy.groovy')\nstage('Deploy to staging') {\n  deploy.toKubernetes(tagToDeploy, 'staging', 'market-data')\n}\nstage('Approve release?') {\n  input \"Release ${tagToDeploy} to production?\"\n}\nstage('Deploy to production') {\n  deploy.toKubernetes(tagToDeploy, 'production', 'market-data')\n}\nThat’s much cleaner. This isn’t the only way to reuse pipeline code — we’ll discuss a \nbetter approach in section 10.3.\nWorks for any namespace and deployment\nPerforms any operations \non Kubernetes\n \n\n\n\t\n261\nBuilding a pipeline with Jenkins\nNext, create a canary deployment file. If you’ve read through chapter 9, you’ll \nremember that you use a distinct deployment with unique labels to identify this \ninstance. In deploy/canary, create a deployment YAML file like the one you used for \nproduction but with three changes:\n1\t Add a label track: canary to the pod specification.\n2\t Reduce the number of replicas to 1.\n3\t Change the name of the deployment to market-data-canary.\nAfter you’ve added that file, add a new stage to your deployment, as shown in the fol-\nlowing listing, before releasing to production.\nListing 10.13    Canary release stage (Jenkinsfile)\nstage('Deploy canary') {\n  deploy.toKubernetes(tagToDeploy, 'canary', 'market-data-canary')\n  try {\n    input message: \"Continue releasing ${tagToDeploy} to \n➥production?\" \n  } catch (Exception e) {\n    deploy.rollback('market-data-canary') \n  }\n}\nIn this example, we’re assuming human approval for moving from canary to produc-\ntion. In the real world, this might be an automated decision; for example, you could \nwrite code to monitor key metrics, such as error rate, for some time after a canary \ndeploy.\nOnce you’ve committed this code, you should be able to run the whole pipeline. Fig-\nure 10.8 illustrates the full journey of your code to production.\nLet’s take a breather so you can reflect on what you’ve learned:\n¡ You’ve automated the delivery of code from commit to production by using \nJenkins to build a structured deployment pipeline.\n¡ You’ve built different stages to validate the quality of that code and provide \nappropriate feedback to an engineering team.\n¡ You’ve learned about the importance of — and challenges in operating — a stag-\ning environment when developing microservices.\nThese techniques provide a consistent and reliable foundation for delivering code safely \nand rapidly to production. This helps ensure overall stability and robustness in a micro­\nservice application. But it’s far from ideal if every microservice copies and pastes the same \ndeployment code or reinvents the wheel for every new service. In the next section, we’ll \ndiscuss patterns for making deployment approaches reusable across a fleet of services.\nAsks for human input to proceed\nRolls back your canary \nif rollout is aborted\n \n",
      "page_number": 270
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 278-285)",
      "start_page": 278,
      "end_page": 285,
      "detection_method": "topic_boundary",
      "content": "262\nChapter 10  Building a delivery pipeline for microservices \nFigure 10.8    Successful deployment pipeline from commit to production release\n10.3\t Building reusable pipeline steps\nMicroservices enable independence and technical homogeneity, but these advantages \ncome at a cost:\n¡ It’s harder for developers to move between teams, as the tech stack can vastly \ndiffer.\n¡ It’s more complex for engineers to reason through the behavior of different \nservices.\n¡ You have to invest more time in different implementations of the same concerns, \nsuch as deployment, logging, and monitoring.\n¡ People may make technical decisions in isolation, risking local, rather than \nglobal, optimization.\nTo balance these risks while maintaining technical freedom and flexibility, you should \naggressively standardize the platform and tooling that services operate on. Doing so \nwill ensure that, even if the technology stack changes, common abstractions remain as \nclose as possible across different services. Figure 10.9 illustrates this approach.\nNOTE    We introduced the platform layer of microservice architecture in chapter 3.\nService A\nDeployment\npipeline\nDeployment platform\nStandardize these elements\nof your application\narchitecture.\nTransport/\ncommunication\nService B\nDeployment\npipeline\nObservability\nObservability\nUnique business logic and\nimplementation within\nservices can diverge.\nFigure 10.9    You can standardize many elements of a microservice application to reduce complexity, \nincrease reuse, and reduce ongoing operational cost.\n \n\n\n\t\n263\nBuilding reusable pipeline steps\nOver the past few chapters, you’ve applied this thinking in a few areas:\n¡ Using a microservice chassis to abstract common, nonbusiness logic functional-\nity, such as monitoring and service discovery\n¡ Using containers — with Docker — as a standardized service artifact for deployment\n¡ Using a container scheduler — Kubernetes — as a common deployment platform\nYou also can apply this approach to your deployment pipelines.\n10.3.1\t Procedural versus declarative build pipelines\nThe pipeline scripts you’ve written so far have three weaknesses:\n1\t Specific  — They’re tied to a single repository, and another repository can’t share \nthem.\n2\t Procedural  — They explicitly describe how you want the build to be carried out.\n3\t Don’t abstract internals  — They assume a lot of knowledge about Jenkins itself, \nsuch as how you start nodes, run commands, and use command-line tools.\nIdeally, a service deployment pipeline should be declarative: an engineer describes \nwhat they expect to happen (test my service, release it, and so on), and your frame-\nwork decides how to execute those steps. This approach also abstracts away changes \nto how those steps happen: if you want to tweak how a step works, you can change \nthe underlying framework implementation. Abstracting these implementation deci-\nsions away from individual services leads to greater consistency across the microservice \napplication.\nCompare the following script to the Jenkinsfile you wrote earlier in the chapter.\nListing 10.14    Example declarative build pipeline\nservice {\n  name('market-data')\n  stages {\n    build()\n    test(command: 'python setup.py test', results: 'results.xml')\n    publish()\n    deploy()\n  }\n}\nThis script defines some common configuration (service name) and a series of steps \n(build, test, publish, deploy) but hides the complexity of executing those steps from a \nservice developer. This allows any engineer to quickly follow best practice to reliably \nand rapidly take a new service to production.\nWith Jenkins Pipeline, you can implement declarative pipelines using shared librar-\nies. We won’t go into detail in this chapter — not enough pages left! — but this book’s \n \n\n\n264\nChapter 10  Building a delivery pipeline for microservices \nGithub repository includes an example pipeline library (http://mng.bz/P7hD). In \naddition, the Jenkins documentation (http://mng.bz/p3wz) provides a detailed refer-\nence on using shared libraries.\nNOTE    In other build tools, such as Travis CI or DroneCI, you declare build \nconfiguration using YAML files. These approaches are great, especially if your \nneeds are relatively straightforward. Conversely, building a DSL with a dynamic \nlanguage can offer an extra degree of flexibility and extensibility.\n10.4\t Techniques for low-impact deployment and feature \nrelease\nThroughout the past few chapters, we’ve used the terms deployment and release inter-\nchangeably. But in a microservice application, it’s important to distinguish between \nthe technical activity of deployment — updating the software version running in a pro-\nduction environment — and the decision to release a new feature to customers or con-\nsuming services.\nYou can use two techniques — dark launches and feature flags — to complement \nyour continuous delivery pipeline. These techniques will allow you to deploy new fea-\ntures without impacting customers and provide a flexible mechanism for rollback.\n10.4.1\t Dark launches\nDark launching is the practice of deploying a service to a production environment sig-\nnificantly prior to making it available to consumers. At our company, we practice this \nregularly and try to deploy within the first few days of building a new service, regardless \nof whether it’s feature-complete. Doing this allows us to perform exploratory testing \nfrom an early stage, which helps us understand how a service behaves and makes a new \nservice visible to our internal collaborators.\nIn addition, dark launching to a production environment allows you to test your \nservices against real production traffic. Let’s say that SimpleBank wants to offer a new \nfinancial prediction algorithm as a service. By passing production traffic in parallel with \nthe existing service, they can easily benchmark the new algorithm and understand how \nit performs in the real world, rather than against limited and artificial test scenarios \n(figure 10.10).\nWhether you validate this output manually or automatically depends on the nature of \nthe feature and the volume and distribution of requests required to adequately exhaust \npossible scenarios. The dark launch approach is also useful for testing that refactoring \ndoesn’t regress sensitive functionality.1\n1\t Similarly, the Ruby Scientist gem (https://github.com/github/scientist) was originally designed \nto help Github validate whether refactoring of user permissions caused authorization issues, such \nas users having incorrect access to repositories.\n \n\n\n\t\n265\nTechniques for low-impact deployment and feature release\nAPI gateway\nRequest\nRequest\nRequest\nClients\nBetter\nprediction\nPrediction\nClients access financial prediction\nthrough an API gateway.\nBy changing the gateway,\nyou can pass requests in parallel\nto “dark-launched” services.\nResponse\nAPI gateway\nRequest\nRequest\nClients\nPrediction\nResponse\nResponse\nPrediction\nlogs\nYou can compare the output of these\ndifferent services, for example, by\nexamining logs and analytics.\nFigure 10.10    Dark launches enable validation of new service behavior against real production traffic \nwithout exposing features to customers.\n10.4.2\t Feature flags\nFeature flags control the availability of features to customers. Unlike dark launches, \nyou can use them at any point in the lifecycle of a service, such as a feature release. A \nfeature flag (or toggle) wraps a feature in conditional logic, only enabling it for a cer-\ntain set of users. Many companies will use them to control rollout; for example, only \nreleasing a feature for internal staff first, or progressively increasing the number of \nusers who can access a feature over time.\nSeveral libraries are available to implement feature flags, such as Flipper (http://\ngithub.com/jnunemaker/flipper) or Togglz (http://github.com/togglz/togglz). These \nlibraries typically use a persistent backing store, like Redis, to maintain the state of feature \nflags for an application. In a larger microservice application, you may find it desirable to \nhave a single feature store to synchronize the rollout of features that involve the inter-\naction of multiple services, rather than independently managing features per service. \nFigure 10.11 illustrates these different approaches.\nManaging features per service is likely to be easier in a small microservice system \nthan a larger one. As your system becomes larger, centralizing feature configuration in \na single service reduces coordination overhead if you encounter situations where fea-\nture rollouts necessitate changes in multiple microservices.\n \n\n\n266\nChapter 10  Building a delivery pipeline for microservices \nApproach 1: Each service owns and maintains\na separate feature store.\nApproach 2: A features service owns all feature\nconfiguration, and other services call it.\nFeature store\nFeature store\nFeature store\nQueries\nQueries\nQueries\nQueries\nQueries\nA\nB\nA\nB\nFeatures\nFigure 10.11    You can store feature flags centrally — owned by one service — or maintain them in \nseparate applications.\nBy controlling which users see a change, feature flags can aid in minimizing the poten-\ntial impact of any change to a system, as you have partial control over code execution \nand feature availability. If errors occur, feature flags often allow for more rapid recov-\nery than typical rollback. For microservices, they can enable safer release of new func-\ntionality without adversely affecting service consumers.\nSummary\n¡ A microservice deployment process should meet two goals: safety at pace and \nconsistency.\n¡ The time it takes to deploy a new service is often a barrier in microservice applications.\n¡ Continuous delivery is an ideal deployment practice for microservices, reducing \nrisk through the rapid delivery of small, validated changesets.\n¡ A good continuous delivery pipeline ensures visibility, correctness, and rich feed-\nback to an engineering team.\n¡ Jenkins is a popular build automation tool that uses a scripting language to tie \nmultiple tools together into a delivery pipeline.\n¡ Staging environments are invaluable but can be challenging to maintain when \nthey face a high volume of independent change.\n¡ You can reuse declarative pipeline steps across multiple services; aggressive stan-\ndardization makes deployment predictable across teams.\n¡ To provide fine-grained control over rollout and rollback, you should manage \nthe technical activity of deployment separately from the business activity of \nreleasing a feature.\n \n\n\nPart 4\nObservability and ownership\nOnce you’ve deployed your services, you need to know what they’re actu-\nally doing. In this part, you’ll build a monitoring system — using metrics, tracing, \nand logging — to give you rich visibility into your microservice application. After \nthat, we’ll conclude our microservice journey by exploring how this architectural \napproach impacts how developers work together and discussing good day-to-day \npractices for developing microservice applications.\n \n\n\n269\n11\nBuilding a monitoring system\nThis chapter covers\n¡ Understanding what signals to gather from \nrunning applications\n¡ Building a monitoring system to collect metrics\n¡ Learning how to use the collected signals to set \nup alerts\n¡ Observing the behavior of individual services \nand their interactions as a system \nYou’ve now set up an infrastructure to run your services and have deployed multiple \ncomponents that you can combine to provide functionality to your users. In this \nchapter and the next, we’ll consider how you can make sure you’ll always be able to \nknow how those components are interacting and how the infrastructure is behav-\ning. It’s fundamental to know as early as possible when something isn’t behaving as \nexpected. In this chapter, we’ll focus on building a monitoring system so you can \ncollect relevant metrics, observe the system behavior, and set up relevant alerts to \nallow you to keep your systems running smoothly by taking actions preemptively. \n \n\n\n270\nChapter 11  Building a monitoring system\nWhen you can’t be preemptive, you’ll at least be able to quickly pinpoint the areas that \nneed your attention so you can address any issues. It’s also worth mentioning that you \nshould instrument as much as possible. The collected data you may not use today may \nturn out to be useful someday.\n11.1\t A robust monitoring stack\nA robust monitoring stack will allow you to start gathering metrics from your services \nand infrastructure and use those metrics to gather insights from the operation of a sys-\ntem. It should provide a way to collect data and store, display, and analyze it.\nYou should start by emitting metrics from your services, even if you have no monitor-\ning infrastructure in place. If you have those metrics stored, at any time you’ll be able to \naccess, display, and interpret them. Observability is a continuous effort, and monitoring \nis a key element in that effort. Monitoring allows you to know whether a system is work-\ning, whereas observability lets you ask why it’s not working.\nIn this chapter, we’ll be focusing on monitoring, metrics, and alerts. We’ll explain \nlogs and traces in chapter 12, and they’ll constitute the observability component.\nMonitoring doesn’t only allow you to anticipate or react to issues, you can also use \ncollected metrics from monitoring to predict system behavior or to provide data for \nbusiness analytic purposes.\nMultiple open source and commercial options are available for setting up a moni-\ntoring solution. Depending on the team size and resources available, you may find that \na commercial solution may be easier or more convenient to use. Nonetheless, in this \nchapter you’ll be using open source tools to build your own monitoring system. Your \nstack will be made up of a metrics collector and a display and alerting component. Logs \nand traces are also essential to achieve system observability. Figure 11.1 gives an over-\nview of all the components you need to be able to understand your system behavior and \nachieve observability.\nIn figure 11.1, we display the components of a monitoring stack:\n¡ Metrics\n¡ Logs\n¡ Traces\nEach of these components feeds into its own dashboards as an aggregation of data \nfrom multiple services. This allows you to set up automated alerts and look into all the \ncollected data to investigate any issues or better understand system behavior. Metrics \nwill enable monitoring, whereas logs and traces will enable observability.\n11.1.1\t Good monitoring is layered\nIn chapter 3, we discussed the architecture tiers: client, boundary, services, and plat-\nform. You should implement monitoring in all of these layers, because you can’t deter-\nmine the behavior of a given component in total isolation. A network issue will most \nlikely affect a service. If you collect metrics at the service level, the only thing you’ll be \n \n",
      "page_number": 278
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 286-293)",
      "start_page": 286,
      "end_page": 293,
      "detection_method": "topic_boundary",
      "content": "\t\n271\nA robust monitoring stack\nable to know is that the service itself isn’t serving requests. That alone tells you nothing \nabout the cause of the issue. If you also collect metrics at the infrastructure level, you \ncan understand problems that’ll most likely affect multiple other components.\nIn figure 11.2, you can see the services that work together to allow a client to place an \norder for selling or buying shares. Multiple services are involved. Some communication \nbetween services is synchronous, either via RPC or HTTP, and some is asynchronous, \nusing an event queue. To be able to understand how services are performing, you need \nto be able to collect multiple data points to monitor and either diagnose issues or pre-\nvent them before they even arise.\nMonitoring individual services will be of little to no use because services provide iso-\nlation but don’t exist isolated from the outside world. Services often depend on each \nother and on the underlying infrastructure (for example, the network, databases, cache \nstores, and event queues). You can get a lot of valuable information by monitoring ser-\nvices, but you need more. You need to understand what’s going on in all your layers. \n2018-02-04T13:55:19.263380548Z\n{\"@timestamp\": \"2018-02-04T13:55:19.262Z\",\n\"@version\": 1, \"source_host\": \"bb69db21f1eb\",\n\"name\": \"root\", \"args\": [], \"levelname\": \"INFO\",\n\"levelno\": 20, \"pathname\": \"./app.py\", \"filename\":\n\"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152,\nServices\nMetrics\nAlerts\nMetrics\nMetrics\nMetrics\nTraces\nLogs\nCentralized logging\nTraces\nTraces\nTraces\n2018-02-04T13:55:19.263380548Z\n{\"@timestamp\": \"2018-02-04T13:55:19.262Z\",\n\"@version\": 1, \"source_host\": \"bb69db21f1eb\",\n\"name\": \"root\", \"args\": [], \"levelname\": \"INFO\",\n\"levelno\": 20, \"pathname\": \"./app.py\", \"filename\":\n\"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152,\n2018-02-04T13:55:19.263380548Z\n{\"@timestamp\": \"2018-02-04T13:55:19.262Z\",\n\"@version\": 1, \"source_host\": \"bb69db21f1eb\",\n\"name\": \"root\", \"args\": [], \"levelname\": \"INFO\",\n\"levelno\": 20, \"pathname\": \"./app.py\", \"filename\":\n\"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152,\n2\nFigure 11.1    Components of a monitoring stack — metrics, traces, and logs — each aggregated in their \nown dashboards\n \n\n\n272\nChapter 11  Building a monitoring system\nGateway\nOrder\nservice\nMarket\nservice\nAMQP\n(async)\nFee\nservice\nRPC\n(sync)\nAMQP\n(async)\nAMQP\n(async)\nAMQP\n(async)\nAMQP\n(async)\nRPC\n(sync)\nAccount\ntransaction\nservice\nOrder\ncreated\nOrder\nplaced\nEvent queue\nFigure 11.2    Services involved in placing orders and their communication protocols\nYour monitoring solution should allow you to know what is broken or degrading and \nwhy. You’ll be able to quickly reveal any symptoms and use the available monitors to \ndetermine causes.\nReferring to figure 11.2, it’s worth mentioning that symptoms and causes vary \ndepending on the observation point. If the market service might be having issues com-\nmunicating with the stock exchange, you can diagnose that by measuring response \ntimes or HTTP status codes for that interaction. In that situation, you’ll be almost sure \nthat the place order feature won’t be working as expected.\nBut what if you have an issue with connectivity from services to the event queue? Ser-\nvices won’t be publishing messages, so downstream services won’t be consuming them. \nIn that situation, no service is failing because no service is performing any work. If you \nhave proper monitoring in place, it can alert you to the abnormal decrease in through-\nput. You can set your monitoring solution to send you automated notifications when \nthe number of messages in a given queue goes below a certain threshold.\nLack of messages isn’t the only thing that can indicate issues, though. What if you \nhave messages accumulating in a given queue? Such accumulation may indicate the \nservices that consume messages from the queue are either not working properly or are \nhaving trouble keeping up with increased demand. Monitoring allows you to identify \nissues or even predict load increases and act accordingly to maintain service quality. \nLet’s take some time for you to learn a bit more about the signals you should collect.\n11.1.2\t Golden signals\nYou should focus on four golden signals while collecting metrics from any user-facing \nsystem: latency, errors, traffic, and saturation.\nLatency\nLatency measures how much time passes between when you make a request to a given \nservice and when the service completes the request. You can determine a lot from this \nsignal. For example, you can infer that the service is degrading if it shows increasing \n \n\n\n\t\n273\nA robust monitoring stack\nlatency. You need to take extra care, though, in correlating this signal with errors. \nImagine you’re serving a request and the application responds quickly but with an \nerror? Latency has a low value in this case, but the outcome isn’t the desired one. It’s \nimportant to keep the latency of requests that result in errors out of this equation, \nbecause it can be misleading.\nErrors\nThis signal determines the number of requests that don’t result in a successful out-\ncome. The errors may be explicit or implicit — for example, having an HTTP 500 error \nversus having an HTTP 200 but with the wrong content. The latter isn’t trivial to mon-\nitor for because you can’t rely solely on the HTTP codes, and you may only be able to \ndetermine the error by finding wrong content in other components. You generally \ncatch these errors with end-to-end or contract tests.\nTraffic\nThis signal measures the demand placed on a system. It can vary depending on the type \nof system being observed, the number of requests per second, network I/O, and so on.\nSaturation\nAt a given point, this measures the capacity of the service. It mainly applies to resources \nthat tend to be more constrained, like CPU, memory, and network.\n11.1.3\t Types of metrics\nWhile collecting metrics, you need to determine the type that’s best suited for a given \nresource you’re aiming to monitor.\nCounters\nCounters are a cumulative metric representing a single numerical value that’ll always \nincrease. Examples of metrics using counters are:\n¡ Number of requests\n¡ Number of errors\n¡ Number of each HTTP code received\n¡ Bytes transmitted\nYou shouldn’t use a counter if the metric it represents can also decrease. For that, you \nshould use a gauge instead.\nGauges\nGauges are metrics representing single numerical arbitrary values that can go up or \ndown. Some examples of metrics using gauges are:\n¡ Number of connections to a database\n¡ Memory used\n¡ CPU used\n¡ Load average\n¡ Number of services operating abnormally\n \n\n\n274\nChapter 11  Building a monitoring system\nHistograms\nYou use histograms to sample observations and categorize them in configurable buck-\nets per type, time, and so on. Examples of metrics represented by histograms are:\n¡ Latency of a request\n¡ I/O latency\n¡ Bytes per response\n11.1.4\t Recommended practices\nAs we already mentioned, you should make sure you instrument as much as possible \nto collect as much data as you can about your services and infrastructure. You can use \nthe collected data at later stages once you devise new ways to correlate and expose it. \nYou can’t go back in time to collect data, but you can make data available that you pre-\nviously collected. \nKeep in mind that you should go about representing that data, showing it in dash-\nboards, and setting up alerts in a progression to avoid having too much information \nat once that will be hard to reason through. There is no point in throwing every single \ncollected metric for a service into one dashboard. You can create several dashboards \nper service with detailed views, but keep one top-level dashboard with the most import-\nant information. This dashboard should allow you, in a glance, to determine if a ser-\nvice is operating properly. It should give a high-level view of the service, and any more \nin-depth information should appear in more specialized dashboards.\nWhen representing metrics, you should focus on the most important ones, like \nresponse times, errors, and traffic. These will be the foundation of your observabil-\nity capabilities. You also should focus on the right percentiles for each use case: 99th, \n95th, 75th, and so on. For a given service, it may be good enough if only 95% of your \nrequests take less than x seconds, whereas on another service you may require 99% of \nthe requests to be below that time. There is no fixed rule for which percentile to focus \non — that generally depends on the business requirements.\nWhenever possible, you should use tags to provide context to your metrics. Examples \nof tags to associate with metrics are:\n¡ Environment: Production, Staging, QA\n¡ User ID\nBy tagging metrics, you can group them later on and perhaps come up with some more \ninsights. Take, for example, a response time you’ve tagged with the User ID; you can \ngroup the values by user and determine if all of the user base or only a particular group \nof users experiences an increase in response times. \nMake sure you always abide by some defined standards when you’re naming metrics. \nIt’s important that you maintain a naming scheme across services. One possible way of \n \n\n\n\t\n275\nMonitoring SimpleBank with Prometheus and Grafana\nnaming metrics is to use the service name, the method, and the type of metric you wish \nto collect. Here are some examples:\n¡ orders_service.sell_shares.count\n¡ orders_service.sell_shares.success\n¡ fees_service.charge_fee.failure\n¡ account_transactions_service. request_reservation.max\n¡ gateway.sell_shares.avg\n¡ market_service.place_order.95percentile\n11.2\t Monitoring SimpleBank with Prometheus and Grafana\nYou need to send the metrics you collect from your services and infrastructure to a \nsystem capable of aggregating and displaying them. The system will use those collected \nmetrics to provide alerting capabilities. For that purpose, you’ll be using Prometheus \nto collect metrics and Grafana to display them:\n¡ Prometheus (https://github.com/prometheus) is an open source systems mon-\nitoring and alerting toolkit originally built at SoundCloud. It’s now a standalone \nopen source project and is maintained independent of any company.\n¡ Grafana (https://grafana.com) is a tool that allows building dashboards on top \nof multiple metrics data sources, such as Graphite, InfluxDB, and Prometheus.\nYou’ll do all your setup using Docker. In chapter 7, you already added to your services \nthe ability to emit metrics via StatsD. You’ll keep those services unchanged and add \nsomething to your setup to convert metrics from StatsD format to the format that \nPrometheus uses. You’ll also add a RabbitMQ container that’s already set up to send \nmetrics to Prometheus. Figure 11.3 shows the components you’ll be adding to set up \nyour monitoring system.\nService\nPrometheus\nGrafana\nStatsD server\nMetrics\nMetrics\nMetrics\nMetrics\nStatsD exporter\nEvent queue\n \nFigure 11.3    The containers you need to build your monitoring system: StatsD server, StatsD exporter, \nPrometheus, and Grafana\n \n\n\n276\nChapter 11  Building a monitoring system\nYou’ll be using both Prometheus and StatsD metrics as a way to show how two types of \nmetrics collection protocols can coexist. StatsD is a push-based tool, whereas Prometheus \nis a pull-based tool. Systems using StatsD will be pushing data to a collector service, \nwhereas Prometheus will pull that data from the emitting systems.\n11.2.1\t Setting up your metric collection infrastructure\nYou’ll start by adding the services described in figure 11.2 to the Docker compose file, \nthen you’ll focus on configuring both the StatsD exporter and Prometheus. The last \nstep will be to create the dashboards in Grafana and start monitoring the services and \nthe event queue. All the code is available in the book’s code repository.\nAdding components to the Docker compose file\nThe Docker compose file (see the next listing) will allow you to boot all the services \nand infrastructure needed for the place order feature. For the sake of brevity, we’ll \nomit the individual services and will only list the infrastructure- and monitoring-re-\nlated containers.\nListing 11.1    docker-compose.yml file\n(…)\nrabbitmq: \n    container_name: simplebank-rabbitmq\n    image: deadtrickster/rabbitmq_prometheus\n    ports:\n      - \"5673:5672\"\n      - \"15673:15672\"\n  redis:\n    container_name: simplebank-redis\n    image: redis\n    ports:\n      - \"6380:6379\"\n  statsd_exporter: \n    image: prom/statsd-exporter\n    command: \"-statsd.mapping-config=/tmp/\n➥statsd_mapping.conf\" \n    ports:\n      - \"9102:9102\"\n      - \"9125:9125/udp\"\n    volumes:\n      - \"./metrics/statsd_mapping.conf:/tmp/statsd_mapping.conf\"\n  prometheus: \n    image: prom/prometheus\n    command: \"--config.file=/tmp/prometheus.yml \n➥--web.listen-address '0.0.0.0:9090'\" \n    ports:\nYou’ll use RabbitMQ as the event \nqueue. The image used here is already \nemitting metrics in the Prometheus \nformat, so you can connect it directly.\nFetches the metrics sent to the StatsD server \nand converts them to Prometheus format, so \nPrometheus can fetch them afterwards\nStarts statsd_exporter with a custom command \nthat’ll load the mapping configuration\nSets up the official Prometheus image\nAllows you to start Prometheus, \nbinding it to 0.0.0.0:9000 and \nreading a custom configuration \nfile that you’ll soon see in a bit \nmore detail\n \n\n\n\t\n277\nMonitoring SimpleBank with Prometheus and Grafana\n      - \"9090:9090\"\n    volumes:\n      - \"./metrics/prometheus.yml:/tmp/prometheus.yml\"\n  statsd: \n    image: dockerana/statsd\n    ports:\n      - \"8125:8125/udp\"\n      - \"8126:8126\"\n    volumes:\n      - \"./metrics/statsd_config.js:/src/statsd/\n➥config.js\" \n  grafana: \n    image: grafana/grafana\n    ports:\n      - \"3900:3000\"\nConfiguring StatsD exporter\nAs we mentioned before, the services involved in the place order feature emit metrics \nin the StatsD format. In table 11.1, we list all the services and the metrics each one \nemits. The services will all be emitting timer metrics.\nTable 11.1    Timer metrics emitted by the services involved in placing an order\nService\nMetrics\nAccount transactions \nrequest_reservation\nFees\ncharge_fee\nGateway\nhealth, sell_shares, \nMarket\nrequest_reservation, place_order_stock_exchange \nOrders\nsell_shares, request_reservation, place_order\nThe mapping config file allows you to configure each metric that StatsD collects and \nadd labels to it. The following listing provides the mapping you’ll create as a configura-\ntion file for the statsd-exporter container.\nListing 11.2    Configuration file to map StatsD metrics to Prometheus \nsimplebank-demo.account-transactions.request_reservation \nname=\"request_reservation\" \nSets the StatsD server that’ll collect \nmetrics that the services send\nUses a custom configuration to allow repeating the received metrics \nto the statsd-exporter container — As with the Prometheus and \nstatsd-exporter containers, the configuration files are located in the \nmetrics folder. This folder will be mounted as a volume so the \ncontainers can pick up these configurations at runtime.\nStarts Grafana, which will provide \na UI for the collected metrics\nAccount transactions service mapping\nSets the name of the metric in Prometheus\n \n\n\n278\nChapter 11  Building a monitoring system\napp=\"account-transactions\" \njob=\"simplebank-demo\" \nsimplebank-demo.fees.charge_fee \nname=\"charge_fee\"\napp=\"fees\"\njob=\"simplebank-demo\"\nsimplebank-demo.gateway.health \nname=\"health\"\napp=\"gateway\"\njob=\"simplebank-demo\"\nsimplebank-demo.gateway.sell_shares\nname=\"sell_shares\"\napp=\"gateway\"\njob=\"simplebank-demo\" \nsimplebank-demo.market.request_reservation \nname=\"request_reservation\"\napp=\"market\"\njob=\"simplebank-demo\"\nsimplebank-demo.market.place_order_stock_exchange\nname=\"place_order_stock_exchange\"\napp=\"market\"\njob=\"simplebank-demo\" \nsimplebank-demo.orders.sell_shares \nname=\"sell_shares\"\napp=\"orders\"\njob=\"simplebank-demo\"\nsimplebank-demo.orders.request_reservation\nname=\"request_reservation\"\napp=\"orders\"\njob=\"simplebank-demo\"\nsimplebank-demo.orders.place_order\nname=\"place_order\"\napp=\"orders\"\njob=\"simplebank-demo\" \nIf you don’t map the above metrics to Prometheus, they’ll still get collected, but the \nway they’ll be collected is less convenient. In the figure 11.4 example, you can see the \ndifference between mapped and unmapped metrics fetched from Prometheus from \nthe statsd_exporter service.\nAllows differentiating between metrics that have the same \nname (For example, request_reservation is used as a name \nfor a metric in both the orders and market services.)\nUsed to determine the \ncollector in statsd_exporter\nFees service mapping\nGateway mapping\nMarket service mapping\nOrders service mapping\n \n",
      "page_number": 286
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 294-307)",
      "start_page": 294,
      "end_page": 307,
      "detection_method": "topic_boundary",
      "content": "\t\n279\nMonitoring SimpleBank with Prometheus and Grafana\nFigure 11.4    Prometheus screenshot with collected SimpleBank metrics — The top two metrics aren’t \nmapped in the statsd_mapping.conf file, whereas the last one is.\nAs you can observe in figure 11.4, when the unmapped create_event metrics that \nboth the market and orders service emit reach Prometheus, they’re collected as:\n¡ simplebank_demo_market_create_event_timer\n¡ simplebank_demo_orders_create_event_timer\nFor the request_reservation_timer metric that the market, orders, and account \ntransactions services emit, there’s only one entry, the metric is the same, and the differ-\nentiation is in the metadata:\nrequest_reservation_timer{app=\"*\",exported_job=\"simplebank-demo\",e\nxporter=\"statsd\",instance=\"statsd-exporter:9102\",job=\"statsd_\nexporter\",quantile=\"0.5\"} \nrequest_reservation_timer{app=\"*\",exported_job=\"simplebank-demo\",e\nxporter=\"statsd\",instance=\"statsd-exporter:9102\",job=\"statsd_\nexporter\",quantile=\"0.9\"}\nMetric mapped in the statsd_exporter configuration file — The app label takes \nthe values off all apps generating the request_reservation_timer metric.\n \n\n\n280\nChapter 11  Building a monitoring system\nrequest_reservation_timer{app=\"*\",exported_job=\"simplebank-demo\",e\nxporter=\"statsd\",instance=\"statsd-exporter:9102\",job=\"statsd_\nexporter\",quantile=\"0.99\"}\nsimplebank_demo_market_create_event_timer{exporter=\"statsd\",instance=\"statsd-\nexporter:9102\",job=\"statsd_exporter\",quantile=\"0.5\"} \nConfiguring Prometheus\nNow that you’ve configured the StatsD exporter, it’s time to configure Prometheus for \nit to fetch data from both the StatsD exporter and RabbitMQ, as shown in the following \nlisting. Both of these sources will be available as targets for metrics data fetching.\nListing 11.3    Prometheus configuration file\nglobal:\n  scrape_interval:     5s \n  evaluation_interval: 10s\n  external_labels:\n      monitor: 'simplebank-demo'\nalerting:\n  alertmanagers:\n  - static_configs:\n    - targets:\nscrape_configs: \n  - job_name: 'statsd_exporter' \n    static_configs:\n      - targets: ['statsd-exporter:9102']\n        labels:\n          exporter: 'statsd'\n    metrics_path: '/metrics'\n  - job_name: 'rabbitmq'\n    static_configs:\n      - targets: ['rabbitmq:15672'] \n        labels:\n          exporter: 'rabbitmq'\n    metrics_path: '/api/metrics' \nSetting up Grafana\nTo receive metrics in Grafana, you need to set up a data source. First, you can boot your \napplications and infrastructure by using the Docker compose file. This will allow you to \naccess Grafana on port 3900, as follows.\nMetric not mapped in the statsd_exporter configuration \nfile — There’s no app and no exported_job labels.\nSets the interval at which Prometheus will \nscrape the configured targets for metrics\nScrapes config section where each target \nis configured\nWill be added as a label any time  \nseries scraped from the config\nThe target host and metrics path will be \nconcatenated to determine the URL to \ncollect metrics from. Given that in this case \nthe scheme defaults to http, the URL will be \n'http://rabbitmq:15672/api/metrics'.\n \n\n\n\t\n281\nMonitoring SimpleBank with Prometheus and Grafana\nListing 11.4    Grafana setup in the docker-compose.yml file\n  (...)\n  grafana:\n    image: grafana/grafana \n    ports:\n      - \"3900:3000\" \nTo start all applications and services using Docker compose, you need to get inside the \nfolder containing the compose file and issue the up command:\nchapter-11$ docker stop $(docker ps | grep simplebank | \n➥awk '{print $1}')\nchapter-11$ docker rm $(docker ps -a | grep simplebank | \n➥awk '{print $1}')\nchapter-11$ docker-compose up --build --remove-orphans  \nStarting simplebank-redis ...\nStarting chapter11_statsd-exporter_1 ...\nStarting chapter11_statsd_1 ...\nStarting simplebank-rabbitmq ...\nStarting chapter11_prometheus_1 ...\nStarting simplebank-rabbitmq ... done\nStarting simplebank-gateway ...\nStarting simplebank-fees ...\nStarting simplebank-orders ...\nStarting simplebank-market\nStarting simplebank-account-transactions ... done\nAttaching to chapter11_prometheus_1, simplebank-redis, chapter11_statsd_1, \nsimplebank-rabbitmq, chapter11_statsd-exporter_1, simplebank-gateway, \nsimplebank-fees, simplebank-orders, simplebank-market, simplebank-\naccount-transactions\n(…)\nThe output of the docker-compose up command will allow you to understand when \nall services and applications are ready. You can reach applications using the URL \nassigned to Docker or the IP address. By appending port 3900 as configured in the \ndocker-compose.yml file, you can access Grafana’s login screen as shown in figure 11.5. \nYou’ll be accessing Grafana using the default login credentials: username and pass-\nword are both admin.\nUses the official grafana Docker image with default settings\nGrafana uses port 3000 by default. The applications and services you start \nvia the compose file will be able to communicate using the default port. \nYou’re mapping it to port 3900 to access it from the host machine.\nStops all SimpleBank running containers\nRemoves all SimpleBank \ncontainers so you don’t \nhave any name clashes\nStarts the containers defined in the \ndocker-compose.yml file — The --build \noption builds images before starting the \ncontainers, and the --remove-orphans \noption removes any containers that \naren’t defined in the compose file.\n \n\n\n282\nChapter 11  Building a monitoring system\nFigure 11.5    Grafana login screen\nOnce you log in, you’ll have an Add Data Source option. Figure 11.6 shows the data \nsource configuration screen, Edit Data Source. To configure a Prometheus data source \nin Grafana, you need to select Prometheus as the type and insert the URL of the run-\nning Prometheus instance, in your case http://prometheus:9090, as configured in the \nDocker compose file.\nThe Save & Test button will give you instant feedback on the data source status. Once \nit’s working, you’re ready to use Grafana to build dashboards for your collected metrics. \nIn the next few sections, you’ll be using it to display metrics both for the services that \nenable the place orders functionality in SimpleBank and for monitoring a critical piece \nof the infrastructure, RabbitMQ, the event queue.\n11.2.2\t Collecting infrastructure metrics — RabbitMQ\nTo set up the dashboard to monitor RabbitMQ, you’ll be using a json configuration \nfile. This is a convenient and easy way to share dashboards. In the source code repos-\nitory, you’ll find a grafana folder. Inside that, a RabbitMQ Metrics.json file holds the \n \n\n\n\t\n283\nMonitoring SimpleBank with Prometheus and Grafana\nconfiguration for both the dashboard layout and the metrics you want to collect. You \ncan now import that file to have your RabbitMQ monitoring dashboard up in no time!\nFigure 11.6    Configuring a Prometheus data source in Grafana\nFigure 11.7 shows how you can access the import dashboard functionality in Grafana. \nBy clicking Grafana’s logo, you bring up a menu; if you hover over Dashboards, the \nImport option will be available.\nThe import option will bring up a dialog box that enables you to either paste the json \nin a text box or upload a file. Before you can use the imported dashboard, you need to \nconfigure the data source that’ll feed the dashboard. In this case, you’ll be using the \nSimpleBank data source you configured previously.\nThat’s all it takes to have your RabbitMQ dashboard up and running. In figure 11.8 \nyou can see how it looks.\n \n\n\n284\nChapter 11  Building a monitoring system\nFigure 11.7    Importing a dashboard from a json file\nFigure 11.8    RabbitMQ metrics collected via Prometheus and displayed in Grafana\n \n\n\n\t\n285\nMonitoring SimpleBank with Prometheus and Grafana\nYour RabbitMQ dashboard provides you an overview of the system by displaying a mon-\nitor for the server status that shows if it’s up or down, along with graphs for Exchanges, \nChannels, Consumers, Connections, Queues, Messages per Host, and Messages per \nQueue. You can hover over any graph to display details for metrics at a point in time. \nClicking the graph’s title will bring up a context menu that allows you to edit, view, \nduplicate, or delete it.\n11.2.3\t Instrumenting SimpleBank’s place order\nNow that you have services up and running, along with the monitoring infrastructure, \nPrometheus and Grafana, it’s time to collect the metrics described in table 11.1. You \ncan start by loading a dashboard exported as json that you can find in the source direc-\ntory under the grafana folder (Place Order.json). Follow the same instructions as the \nones in 11.2.2 for the RabbitMQ dashboard.\nFigure 11.9 displays the dashboard collecting metrics for the services involved in the \nplace order feature. By clicking on each of the panel titles, you can view, edit, duplicate, \nshare, and delete each of the panels.\nThis loaded dashboard collects the time metrics and displays the 0.5, 0.9, and 0.99 \nquantiles for each metric. In the top right corner, you find the manual refresh button \nas well as the period for displaying metrics. By clicking the Last 5 Minutes label, you \ncan select another period for displaying metrics, as shown in figure 11.10. You can \nselect one of the Quick Ranges values or create a custom one, and you can display \nstored metrics in any range you need.\nFigure 11.9    Place order dashboard accessible at Grafana’s /dashboard/db/place-order endpoint\n \n\n\n286\nChapter 11  Building a monitoring system\nFigure 11.10    Selecting the time range for which you want metrics to be displayed\nLet’s focus on the Market | Place Order Stock Exchange panel to see in detail how you can \nconfigure a specific metric display. To do so, click the panel title and then select the Edit \noption. Figure 11.11 shows the edit screen for the Market | Place Order Stock Exchange.\nThe edit screen has a set of tabs (1) you can select to configure different options. The \nhighlighted one is the Metrics tab, where you can add and edit metrics to be displayed. In \nthis particular case, you’re only collecting a metric (2), namely the place_order_stock_\nexchange_timer that gives you the time it took for the market service to place an order \ninto the stock exchange. The default display for a metric contains metadata like the app \nname, the exported job, and the quantile. To change the way the legend is presented, \nyou set a Legend Format (3). In this case, you set the name and use {{quantile}} block \nthat’ll be interpolated to display the quantile in both the graph legend and the hover-\ning window next to the vertical red line. (The red line acts as a cursor when you move \nyour mouse across the collected metrics.) In your dashboards, you’re displaying the min, \nmax, avg, and current values for each quantile (4).\nFigure 11.11    Panel edit screen for Market | Place Order Stock Exchange\n \n\n\n\t\n287\nMonitoring SimpleBank with Prometheus and Grafana\nFURTHER READING    To learn more about how to use Prometheus with Grafana \nand about the Prometheus data model, visit the following documentation \npages: http://mng.bz/ui3b and http://mng.bz/PZQ0.\nThe dashboard you’ve set up is quite simple, but it allows you to have an overview \nof how the system is behaving. You’re able to collect time-related metrics for several \nactions that services in your system perform.\n11.2.4\t Setting up alerts\nNow that you’re collecting metrics and storing them, you can set up alerts for when \nvalues deviate from what you consider as normal for a given metric. This can be an \nincrease in the time taken to process a given request, an increase in the percent of \nerrors, an abnormal variation in a counter, and so on.\nIn your case, you can consider the market service and set up an alert for knowing \nwhen the service needs to be scaled. Once you place a sell order via the gateway service, \na lot goes on. Multiple events are fired, and you know the bottleneck tends to be the \nmarket service processing the place order event. The good thing is you can set up an \nalert to send a message whenever messages in the market place order queue go above a \ncertain threshold. You can configure multiple channels for notifications: email, slack, \npagerduty, pingdom, webhooks, and so on.\nYou’ll be setting up a webhook notification to receive a message in your alert server \nevery time the number of messages goes above 100 in any message queue. For now, \nyou’ll only be receiving it in an alert service made with the purpose of illustrating the \nfeature. But you could easily change this service to trigger an increase in the number of \ninstances of a given service to increase the capacity to process messages from a queue.\nThe alert service is a simple app that also booted when you started all other apps and \nservices. It’ll be listening for incoming POST messages, so you can go ahead and con-\nfigure the alerts in Grafana. Figure 11.12 shows the activity for the market place order \nevent queue with indication of alerts, both when they were triggered and when the alert \ncondition ceased. When you set up alerts, Grafana will indicate as an overlay both the \nthreshold set for alerting (1) and the instants when alerts were triggered (2, 4, 6) and \nresolved (3, 5, 7).\nWith the current setup, the alert service sends an alert message as a webhook when \nthe number of messages in any queue goes over 100. The following shows you one of \nthose alert messages:\nalerts.alert.d26ab4ca-1642-445f-a04c-41adf84145fd: \n{\n  \"evalMatches\": [\n    {\n      \"value\":158.33333333333334, \n      \"metric\":\"evt-orders_service-order_created\n➥--market_service.place_order\", \n      \"tags\":{\n        \"__name__\":\"rabbitmq_queue_messages\",\n        \"exporter\":\"rabbitmq\",\nValue for the metric at the time of the alert\nThe name of the metric for \nwhich the alert was triggered\n \n\n\n288\nChapter 11  Building a monitoring system\n        \"instance\":\"rabbitmq:15672\",\n        \"job\":\"rabbitmq\",\n        \"queue\":\"evt-orders_service-order_created\n➥--market_service.place_order\", \n        \"vhost\":\"/\"\n      }\n    }\n  ],\n  \"message\":\"Messages accumulating in the queue\",\n  \"ruleId\":1,\n  \"ruleName\":\"High number of messages in a queue\", \n  \"ruleUrl\":\"http://localhost:3000/dashboard/db/rabbitmq-metrics?fullscreen\\\n➥u0026edit\\u0026tab=alert\\u0026panelId=2\\u0026orgId=1\",\n  \"state\":\"alerting\", \n  \"title\":\"[Alerting] High number of messages in a queue\"\n}\nFigure 11.12    The message queue’s status showing alert overlays\nShows information about the queue \nwhere the alert was triggered\nIdentifies what rule \nthis alert is related to\nIndicates the message type — In this case, \"alerting\" means \nan alert was triggered and the number of messages in the \nqueue is above the normal operating threshold.\n \n\n\n\t\n289\nMonitoring SimpleBank with Prometheus and Grafana\nLikewise, when the number of messages in a queue goes below the value defined as the \nthreshold for alerting, the service also issues a message to notify about it:\nalerts.alert.209f0d07-b36a-43f4-b97c-2663daa40410: \n{\n  \"evalMatches\":[],\n  \"message\":\"Messages accumulating in the queue\",\n  \"ruleId\":1,\n  \"ruleName\":\"High number of messages in a queue\", \n  \"ruleUrl\":\"http://localhost:3000/dashboard/db/rabbitmq-metrics?fullscreen\\\n➥u0026edit\\u0026tab=alert\\u0026panelId=2\\u0026orgId=1\",\n  \"state\":\"ok\",\n  \"title\":\"[OK] High number of messages in a queue\"\n}\nLet’s now see how you can set up this alert for the number of messages in queues. \nYou’ll also be using Grafana for setting up the alert, because it offers this capability and \nthe alerts will display on the panels they relate to. You’ll be able to both receive notifi-\ncations and check the panels for previous alerts.\nYou’ll start by adding a notification channel that’ll you’ll use to propagate alert \nevents. Figure 11.13 shows how to create a new notification channel\nTo set up a new notification channel in Grafana, follow these steps:\n1\t Click the Grafana icon on the top left of the screen.\n2\t Under the Alerting menu, select Notification Channels.\n \nFigure 11.13    Setting up a new notification channel in Grafana\nIdentifies the rule the \nalert is related to\nThe \"ok\" state means the number of messages in the queues are all back to \nbelow the set threshold; the conditions for a previous alert are no longer met.\n \n\n\n290\nChapter 11  Building a monitoring system\n3\t Enter the name for the channel and select the type as Webhook, then check the \nSend on All Alerts option.\n4\t Enter the URL for the service receiving the alerts. In your case, you’ll be using \nthe alerts service and listening for POST requests.\n5\t Click the Send Test button to verify all is working, and if so, click Save to save the \nchanges.\nNow that you have an alert channel set up, you can go ahead and create alerts on your \npanels. You’ll be setting an alert on the messages queue panel under the RabbitMQ \ndashboard you created previously. Clicking the Messages/Queue panel title will bring \nup a menu where you can select Edit. This allows you to create a new alert under the \nAlert tab. Figure 11.14 shows how to set up a new alert.\nUnder the Alert Config screen, start by adding the Name for the alert as well as the fre-\nquency at which you want the condition to be evaluated — in this case every 30 seconds. \nThe next step is to set the Conditions for the alert. You’ll be setting an alert to notify you \nwhenever the average of the values collected from query A is above 100 in the last minute.\nTIP    If you click the Metrics tab, you can see query A under it, which will be the \nrabbitmq_queue_messages metric. You also have the option to test the rules \nyou set by clicking the Test Rule button.\nUnder the Alert tab, you also can check the history of the configured alerts. Fig-\nure 11.15 shows the alert history for the number of messages in queues.\nFigure 11.14    Setting up alerts on the Messages/Queue graph on the RabbitMQ Dashboard\n \n\n\n\t\n291\nRaising sensible and actionable alerts\nFigure 11.15    Displaying the state history for a given alert\nThat’s it, you’re done! You’ve set up a monitoring infrastructure to collect both metrics \nthat your services already emitted and those that come from a key component that \nthose services use to communicate asynchronously: the event queue. You’ve also seen \nhow to create alerts to be notified whenever certain conditions in your system are met. \nLet’s now dig a bit deeper into alerts and how to use them.\n11.3\t Raising sensible and actionable alerts\nHaving a monitoring infrastructure in place means you can measure system perfor-\nmance and keep a historic record of those measures. It also means you can determine \nthresholds for your measures and automatically emit notifications when those thresh-\nolds are exceeded.\nOne thing you need to keep in mind, though, is that it’s easy to reach a stage \nwhere all this information can become overwhelming. Eventually, the overload of \ninformation can do more harm than good (for example, if it gets so bad that peo-\nple start ignoring recurring alerts). You need to make sure that the alerts you raise \nare actionable — and actioned — and that they’re targeting the correct people in the \norganization.\n \n\n\n292\nChapter 11  Building a monitoring system\nAlthough services may consume and take action on some of the alerts automatically; \nfor example, autoscaling a service if messages are accumulating in a queue, humans \nneed to consume and take action on some alerts. You want those alerts to reach the \ncorrect people and contain enough information so that diagnosing the cause becomes \nas easy as possible.\nYou also need to prioritize alerts, because most likely any issue with your services or \ninfrastructure will trigger multiple alerts. Whoever is dealing with those alerts needs \nto know immediately the urgency of each one. As a rule, you should direct alerts for \nservices to the teams owning those services. You should map the application into the \norganization, because this helps with determining the targets for alerts.\n11.3.1\t Who needs to know when something is wrong?\nIn day-to-day operation, alerts should target the team who owns the service and orig-\ninated it. This reflects the “you build it, you run it” mantra that should govern a \nmicroservices-oriented engineering team. As teams create and deploy services, it’s \nhard, if not impossible, for everyone to know about every service deployed. People \nwith the most knowledge about a service will be in the best position to interpret and \ntake action in response to alerts that the service generates.\nOrganizations also may have some on-call rotation or a dedicated team that’ll receive \nand monitor alerts and then escalate if necessary to specialized teams. When setting up \nalerts and notifications, it’s important to keep in mind that other people may consume \nthem, so you should keep those alerts as concise and informative as possible. It’s also \nimportant that each service have some sort of documentation on common issues and \ndiagnosing recipes so that on-call teams can, when they receive an alert, determine if \nthey can fix the issue or if they need to escalate it.\nYou also should categorize alerts by levels of urgency. Not every issue will need imme-\ndiate attention, but some are deal breakers that you need to address as soon as you \nknow about them.\nSevere issues should trigger a notification to ensure someone, either an engineer \nfrom the team that built the service or an on-call engineer, is notified. Issues that are \nof moderate severity should generate alerts as notifications in any channels deemed \nappropriate, so those monitoring them can pick them up. You can think of this type \nof alert as something generating a queue of tasks that you need to carry out as soon as \npossible but not immediately — they don’t need to interrupt someone’s flow or wake \nsomeone up in the middle of the night. The lowest priority alerts are those that only \ngenerate a record. These alerts aren’t strictly for human consumption, because services \ncan receive them and take some kind of action if needed (for example, autoscaling a \nservice when response times increase).\n11.3.2\t Symptoms, not causes \nSymptoms, not causes, should trigger alerts. An example of this is a user-facing error; if \nusers can no longer access a service, that inability should generate an alert. You shouldn’t \nbe tempted to trigger alerts for every single parameter that isn’t under the normal \n \n",
      "page_number": 294
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 308-317)",
      "start_page": 308,
      "end_page": 317,
      "detection_method": "topic_boundary",
      "content": "\t\n293\nObserving the whole application\nthreshold. With such partial information, you won’t be able to know what’s going on or \nwhat the problem is. In figure 11.2, we illustrated the flow for placing orders in the stock \nmarket. Four services cooperate with a gateway that works as the access point for the con-\nsumer of the feature. One or more of the services may be exhibiting erroneous behavior \nor be overloaded. Given the mainly asynchronous nature of the communication between \ncomponents, it may be hard to pinpoint why a given error may be happening.\nImagine you set an alert that relates the number of requests reaching the gateway \nand the number of issued notifications of orders placed. It’ll be simple to correlate \nthose two metrics over time and determine the ratio between the two. You’ll have a \nsymptom: the number of orders placed is greater than the ones completed. You can \nstart from there and then try to understand which component is failing (maybe even \nmultiple components). Is it the event queue or an infrastructure problem? Is the system \nunder high loads and can’t cope? The symptom will be the starting point for your inves-\ntigation, and from there you should follow the leads until you find the cause or causes.\nTIP    Avoid alert fatigue by keeping alert notifications to a minimum and keep-\ning them actionable. Generating an alert notification for every single deviation \nfrom the normal behavior of the system may quickly lead to alerts being disre-\ngarded or deemed unimportant. Such minimizing will eventually lead to some-\nthing important being overlooked.\n11.4\t Observing the whole application\nCorrelating metrics can be a precious tool to infer and understand more than a \nper-service state of the system. Monitoring can also help you understand and reason \nthrough the behavior of the system under different conditions, and this can help you \nto predict and adjust your capacity by using all the collected data. The good thing \nabout collecting per-service metrics is you can iteratively correlate them between dif-\nferent services and have an overall idea of the behavior of the whole application. In \nfigure 11.16, you can see a possible correlation of different service metrics.\nLet’s look into each of the suggested correlations:\n¡ A: Creating a new visualization comparing the rate of incoming requests to the gateway and \nthe orders service  — This allows you to understand if there are any issues in process-\ning the incoming requests from your users. You also can use the new correlation \nto set an alert every time that rate drops from 99%.\n¡ B: Correlating the number of user requests made to the gateway with the number of order- \ncreated messages in the queue  — Given that you know the order service is responsible \nfor publishing those messages, this will, similarly to A, allow you to understand if \nthe system is working correctly and customer requests are being processed.\n¡ C: Correlating the number of order-placed messages with the number of requests to the order \nservice  — This will allow you to infer if the fee service is working properly.\n \n\n\n294\nChapter 11  Building a monitoring system\nGateway dashboard\nResponse times\n# Status codes\n# Requests\nOrder service dashboard\nResponse times\nA\nB\nC\n# Status codes\n# Requests\nEvent queue dashboard\n# Order-placed messages\n# Order-created messages\nGateway\nOrder\nservice\nAMQP\n(async)\nRPC\n(sync)\nUser\nOrder\ncreated\nOrder\nplaced\nEvent queue\nFigure 11.16    Correlation of metrics between different services\nCombining different metrics into new dashboards and setting sensible alerts on them \nallows you to gain insights into the overall application. It’s then up to you to determine \nthe desired level of detail, from a high-level view to a detailed one. \nSo far, we’ve covered monitoring and alerting. You’ve set up a monitoring stack to \nbe able to understand how things happened. You’re now able to understand the status \nof services, observe the metrics they emit, and determine if they’re operating within \nexpected parameters. This is only part of the application observability effort. It’s a good \nstarting point, but you do need more!\nTo be able to fully understand what’s going on, you need to invest some more in \nlogging and tracing so you can have both a current view of what’s happening and a view \nof what happened before. In the next chapter, we’ll focus on logging and tracing as a \ncomplement to monitoring in your journey into observability. Doing so will help you to \nunderstand why things happened.\n \n\n\n\t\n295\nSummary\nSummary\n¡ A robust microservice monitoring stack consists of metrics, traces, and logs.\n¡ Collecting rich data from your microservices will help you identify issues, investi-\ngate problems, and understand your overall application behavior.\n¡ When collecting metrics, you should focus on four golden signals: latency, errors, \ntraffic (or throughput), and saturation.\n¡ Prometheus and StatsD are two common, language-independent tools for col-\nlecting metrics from microservices.\n¡ You can use Grafana to graph metric data, create human-readable dashboards, \nand trigger alerts.\n¡ Alerts based on metrics are more durable and maintainable if they indicate the \nsymptoms of incorrect system behavior, rather than the causes.\n¡ Well-defined alerts should have a clear priority, be escalated to the right people, \nbe actionable, and contain concise and useful information.\n¡ Collecting and aggregating data from multiple services will allow you to correlate \nand compare distinct metrics to gain a rich overall understanding of your system.\n \n\n\n296\n12\nUsing logs and traces \nto understand behavior\nThis chapter covers\n¡ Storing logs in a consistent and structured way \nin a machine-readable format\n¡ Setting up a logging infrastructure\n¡ Using traces and correlation IDs to understand \nsystem behavior\nIn the previous chapter, we focused on emitting metrics from your services and \nusing those metrics to create dashboards and alerts. Metrics and alerts are only one \npart of what you need to achieve observability in your microservice architecture. \nIn this chapter, we’ll focus on collecting logs and making sure you’re able to trace \nthe interactions between services. This will allow you to not only have an overview \nof how the system behaves but also go back in time and retrospectively follow each \nrequest. Doing so is important to debug errors and to identify bottlenecks. Logs give \nyou a sort of paper trail that documents the history of each request entering your \nsystem, whereas traces provide you a way to establish a timeline for each request, to \nunderstand how much time it spent in different services. \n \n\n\n\t\n297\nUnderstanding behavior across services\nBy the end of this chapter, you’ll have created a basic logging infrastructure and set \nup the tracing capability. You’ll be able to both monitor the operation of your applica-\ntion and have the tools to audit and investigate in case you need to do so for particular \nrequests. In addition, you’ll be able to identify performance issues by looking into trac-\ning data.\n12.1\t Understanding behavior across services\nIn a microservices-based architecture, multiple services will be involved in providing \nfunctionality to users. It gets hard to understand what goes on with every request when \nyou no longer have a central access point to data. Services are distributed across multi-\nple nodes, are ephemeral, and are continuously being deployed and scaled to meet the \nneeds of operation. Let’s revisit the sell order use case as you might have implemented \nit if you’d needed a single application running on a single machine (figure 12.1).\nHard disk\nAPI\n(gateway)\nOrders module\nAccounts module\nQueue\nFees module\nMarket module\nApplication log\nFigure 12.1    The sell order use case implemented in a single application\n \n\n\n298\nChapter 12  Using logs and traces to understand behavior \n In figure 12.1, we’ve represented each of the services that collaborate to allow a client \nto sell shares as modules in the same application. If you were to inspect a given request \nlifecycle in the system, you could log in to the machine and inspect log data stored on a \nhard drive. But you’d most likely have multiple machines running the application, for \nredundancy and availability, so things wouldn’t be as easy as logging in to one machine. \nOnce you identified the request you were interested in observing, you’d have to iden-\ntify which machine had run the request and then inspect it. Going through the logs \nfrom that machine would provide you needed insights.\nMaintaining logs in a single machine is by no means easy — a server can also crash \nand become unavailable. Our aim here isn’t to talk about minimizing the complexity of \nkeeping log data (or any data) safely persisted but to point out that having a single point \nfor storing all the data makes it easier and more convenient to consult.\nLet’s now compare the same scenario in a microservices application. Figure 12.2 \nillustrates the same use case with multiple services, each with multiple copies of itself, \nrunning independently. \nAs you can see below, you have five services running independently, and each of \nthose services has three instances running. This means potentially none of the pods \nare executing on the same physical machine. A request coming into the system will \nmost likely flow through multiple pods running on different physical machines, and \nyou have no easy way to track down that request by accessing logs. Could you even do \nit? What guarantee do you have that any of the pods are still running once you need to \naccess data?\nGateway\nOrder\nservice\nMarket\nservice\nAMQP\n(async)\nFee\nservice\nRPC\n(sync)\nAMQP\n(async)\nAMQP\n(async)\nAMQP\n(async)\nAMQP\n(async)\nRPC\n(sync)\nAccount\ntransaction\nservice\nOrder\ncreated\nOrder\nplaced\nEvent queue\nOrder service\nlog data\nGateway log\ndata\nMarket service\nlog data\nFee service\nlog data\nAccount\ntransaction\nservice log\ndata\nFigure 12.2    The sell order use case in SimpleBank with multiple services running, each with its own log data\n \n\n\n\t\n299\nUnderstanding behavior across services\nGateway\nOrders\nAccount\ntransaction\nFees\nMarket\nLog data\nLog data\nLog data\nLog data\nLog data\nGateway\nlogs\nAccount\nservice\nlogs\nFee\nservice\nlogs\nMarket\nservice\nlogs\nOrder\nservice\nlogs\nFigure 12.3    Accessing logs for each service in different running instances is challenging.\nIn figure 12.3, we illustrate the challenges you face when you try to gather data from a \ndistributed system. Even if you had some sort of persistency for the log data that would \nallow it to survive after a running pod is replaced, it’d be no easy task to track down \na request through your system. You need a better way to record what’s going on with \nyour system. To be able to fully understand behavior, you need to\n¡ Make sure you persist log data so it survives through service restarts and scaling\n¡ Aggregate all log data from multiple services and instances of those services in a \ncentral location\n¡ Make the stored data usable, allowing for easy searches and further processing\nOur objective by the end of this chapter will be to have an infrastructure that allows \nyou to collect log data from all your services, aggregating it and allowing you to per-\nform searches in it so you can, at any time, reason through the behavior of the system. \nYou’ll be able to use the available data to audit, debug, or even gather new insights \nby processing it further. One example of the processing you can do to augment the \n \n\n\n300\nChapter 12  Using logs and traces to understand behavior \navailable information is to collect the IP data stored in the logs and generate a visual-\nization showing the most common geographic areas of your users.\n To effectively store and make your log data searchable, you first need to agree on a \nformat the engineering team will use. A consistent format will help to guarantee that \nyou can store and process data effectively.\n12.2\t Generating consistent, structured,  \nhuman-readable logs\nTo be able to achieve observability, you have to collect data from multiple sources; \nnot only from running services but also from your infrastructure. Defining a common \nformat allows you to analyze data more easily and perform searches in that data using \nexisting tools with minimal effort. Examples of the data you may collect and use are:\n¡ Application logs\n¡ Database logs\n¡ Network logs\n¡ Performance data collected from the underlying operating system\nFor some components, you can’t control the format, so you have to cope with their \nspecificities and transform them somehow. But for now, let’s focus on what you can \ncontrol: your services. Making sure the whole engineering team abides by a format \nand a way of doing things pays off in the long run, because data collection will become \nsimpler and more effective. Let’s start by determining what you should store; then we \ncan look at how to store it.\n12.2.1\t Useful information to include in log entries\nFor your log data to be useful and effective in helping you to understand behavior in \nyour systems, you need to make sure it includes certain information that will allow you \nto communicate certain things. Let’s look into what you should include as part of each \nlog entry.\nTimestamps\nTo be able to correlate data and order it appropriately, you need to make sure you \nattach timestamps to your log entries. Timestamps should be as granular and verbose \nas possible; for example, use four-digit years and the best resolution available. Each \nservice should render its own timestamps, preferably in microseconds. Timestamps \nalso should include a time zone, and it’s advisable that you collect data as GMT/UTC \nwhenever possible.\nHaving these details allows you to avoid issues with correlating data from different \nservices with different time zones. Ordering data by time of occurrence will become \nmuch easier and require less context while analyzing. Getting timestamps right is \nthe first step in making sure you can understand the sequence in which events took \nplace.\n \n\n\n\t\n301\nGenerating consistent, structured, human-readable logs \nIdentifiers\nYou should use unique identifiers whenever, and as much as, possible in the data you \nintend to log. Request IDs, user IDs, and other unique identifiers are invaluable when \nyou’re cross-referencing data from multiple sources. They allow you to group data \nfrom different sources in an effective way.\nMost of the time, these IDs already exist in your system because you need to use them \nto identify resources. It’s likely they’re already being propagated through different ser-\nvices, so you should make the best use out of them. Unique identifiers used alongside \ntimestamps yield a powerful tool to understand the flow of events in a system.\nSource\nIdentifying the source of a given log entry allows easier debugging when needed. Typi-\ncal source data you can use includes:\n¡ Host\n¡ Class or module\n¡ Function\n¡ Filename\nWhen adding execution times on a given function call, the information you’ve col-\nlected for the source allows you to infer performance because you can extrapolate \nexecution times, even if not in real time. Although this isn’t a replacement for collect-\ning metrics, it can be effective in helping to identify bottlenecks and potential perfor-\nmance issues.\nLevel or category\nEach log entry should contain a category. The category can be either the type of data \nyou’re logging or the log level. Typically, the following values are used as log levels: \nERROR, DEBUG, INFO, WARN.\nThe category will allow you to group data. Some tools can parse log files searching \nfor messages with the ERROR level and communicate them to error reporting systems. \nThis is a perfect example of how you can use the log level or category to automate the \nprocess of error reporting without the need for explicit instructions.\n12.2.2\t Structure and readability\nYou want to generate log entries in a human-readable format, but at the same time \nthey need to be easily parseable by a machine. What we mean by human readable is \navoiding binary encoding of data or any type of encoding that your average human \ncan’t understand. An example of this would be storing the binary representation of an \nimage. You should probably use its ID, file size, and other associated data instead.\nYou also should avoid multiline logs because they can lead to fragmentation while \nparsing them in log aggregation tools. With such logs, it can be easy to lose some \nof the information associated with a particular log entry, like the ID, timestamp, or \nsource.\n \n\n\n302\nChapter 12  Using logs and traces to understand behavior \nFor the examples in this chapter, you’ll be using JSON to encode your log entries. \nDoing so allows you to provide human-readable and machine-parseable data, as well as \nto automatically include some of the data we mentioned in the previous section.\nIn chapter 7, when we were discussing a microservice chassis, we introduced a Python \nlibrary that provides log formatting: logstash-formatter. Logstash libraries are avail-\nable for different languages, so you can expect the format to be widespread and easily \nusable no matter what language you chose to code your services in.\nLogstash\nLogstash is a tool to collect, process, and forward events and log messages from multiple \nsources. It provides multiple plugins to configure data collecting.\nWe’re interested in the formatting conventions of Logstash, and you’ll be using its V1 for-\nmat specification in your SimpleBank services.\n \nLet’s now look into a log entry collected using the Logstash library for Python. This mes-\nsage is formatted using V1 of the logstash format, and the application generated it auto-\nmatically when it was booting, without the need for any specific code instruction to log it:\n{ \n    \"source_host\" : \"e7003378928a\", \n    \"pathname\" : \"usrlocallibpython3.6site-packagesnamekorunners.py\", \n➥\"relativeCreated\" : 386.46125793457031,  \n    \"levelno\" : 20,  \n    \"msecs\" : 118.99447441101074,  \n    \"process\" : 1,  \n    \"args\" : [    \"orders_service\"  ],\n    \"name\" : \"nameko.runners\",  \n    \"filename\" : \"runners.py\", \n    \"funcName\" : \"start\", \n    \"module\" : \"runners\", \n    \"lineno\" : 64,  \n    \"@timestamp\" : \"2018-02-02T18:42:09.119Z\", \n    \"@version\" : 1, \n    \"message\" : \"starting services: orders_service\", \n    \"levelname\" : \"INFO\", \n    \"stack_info\" : null,  \n    \"thread\" : 140612517945416,   \n    \"processName\" : \"MainProcess\",  \n    \"threadName\" : \"MainThread\",  \n    \"msg\" : \"starting services: %s\",  \n    \"created\" : 1520275329.1189945\n}\nInformation about the source: the host \nrunning the application\nTime taken to process the action\nFilename, function, module, and line \nnumber emitting the log\nTimestamp, with Z indicating \nthe UTC time zone\nVersion of the formatter (logstash-formatter v1)\nMessage indicating the \nstarting of the server\nLog level or category, in this case the INFO level\n \n",
      "page_number": 308
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 318-332)",
      "start_page": 318,
      "end_page": 332,
      "detection_method": "topic_boundary",
      "content": "\t\n303\nSetting up a logging infrastructure for SimpleBank\nAs you can see, the Logstash library inserts relevant information, taking that burden \nfrom the developer’s shoulders. In the following listing, you’ll see how an explicit log \ncall in code renders a log entry.\nListing 12.1    Logstash V1 formatted log message after an explicit log instruction\n# Python code for generating a log entry \nself.logger.info ({\"message\": \"Placing sell order\", \n➥\"uuid\": res})    \n{\n    \"@timestamp\": \"2018-02-02T18:43:08.221Z\",\n    \"@version\": 1,\n    \"source_host\": \"b0c90723c58f\",\n    \"name\": \"root\",\n    \"args\": [],\n    \"levelname\": \"INFO\", \n    \"levelno\": 20,\n    \"pathname\": \"./app.py\",\n    \"filename\": \"app.py\",\n    \"module\": \"app\",\n    \"stack_info\": null,\n    \"lineno\": 33,\n    \"funcName\": \"sell_shares\",\n    \"created\": 1520333830.3000789,\n    \"msecs\": 300.0788688659668,\n    \"relativeCreated\": 15495.944738388062,\n    \"thread\": 140456577504064,\n    \"threadName\": \"GreenThread-2\",\n    \"processName\": \"MainProcess\",\n    \"process\": 1,\n    \"message\": \"Placing sell order\", \n    \"uuid\": \"a95d17ac-f2b5-4f2c-8e8e-2a3f07c68cf2\" \n}\nIn your explicit call to the logger, you’ve only stated the desired level and the message \nto log in the form of key-value pairs containing a message and a UUID. Logstash auto-\nmatically collected and added all the other information present in the log entry with-\nout you having to declare it explicitly.\n12.3\t Setting up a logging infrastructure for SimpleBank\nNow that you’ve set a format for collecting and presenting info, you can move on to \ncreating a basic logging infrastructure. In this section, you’ll be setting up the infra-\nstructure that’ll allow you to collect logs from all the running services and aggregate \nthem. It also will provide you with search and correlation capabilities. The purpose is to \nhave a central access point to all the log data like you already have for metrics. In figure \n12.4, we illustrate what you want to achieve after setting up a logging infrastructure.\nThe log level, the message \nfield, and the uuid field\nThe log level determined by the call made \nto the logger module, in this case, INFO\nThe message field\nThe uuid field, which identifies \nand potentially allows correlation \nbetween this log entry and \nanother in different services\n \n\n\n304\nChapter 12  Using logs and traces to understand behavior \nOrder\nservice\nGateway\n2018-02-04T13:55:19.263380548Z {\"@timestamp\":\n\"2018-02-04T13:55:19.262Z\", \"@version\": 1,\n\"source_host\": \"bb69db21f1eb\", \"name\": \"root\", \"args\": [],\n\"levelname\": \"INFO\", \"levelno\": 20, \"pathname\": \"./app.py\",\n\"filename\": \"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152, \"msecs\": 261.5199089050293,\n\"relativeCreated\": 669360.3167533875, \"thread\":\n140130358674856, \"threadName\": \"GreenTread-13\",\n\"processName\": \"MainProcess\", \"process\": 1, \"message\":\n\"{'event': 'sell_order_received', 'uuid':\n'11fcdc94-c055-4f26-988b-886165655792'}\", \"event\":\n\"sell_order_received\", \"uuid\":\n\"11fcdc94-c055-4f26-988b-886165655792\"}\n2018-02-04T13:55:20.839781524Z {\"@timestamp\":\n\"2018-02-04T13:55:20.838Z\", \"@version\": 1,\n\"source_host\": \"e7003378928a\", \"name\": \"root\", \"args\": [],\n\"levelname\": \"INFO\", \"levelno\": 20, \"pathname\": \"./app.py\",\n\"filename\": \"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 45, \"funcName\": \"__create_event\", \"created\":\n1520275337.8386793, \"msecs\": 838.679313659668,\n\"relativeCreated\": 669360.3167533875, \"thread\":\n140612448880184, \"threadName\": \"GreenTread-2\",\n\"processName\": \"MainProcess\", \"process\": 1, \"message\":\n\"{'event': 'order_created', 'uuid':\n'11fcdc94-c055-4f26-988b-886165655792'}\", \"event\":\n\"order_created\", \"uuid\":\n\"11fcdc94-c055-4f26-988b-886165655792\"}\nLogs\nLogs\nMetrics\nMetrics\nOrder service dashboard\nResponse times\n# Status codes\n# Requests\nGateway dashboard\nMetrics dashboard\nCentralized logging\nResponse times\n# Status codes\n# Requests\nFigure 12.4    Services making use of centralized metrics and a centralized log for easy access to data\nOnce you set up the log aggregation capability, like you did for metrics in chapter 11, \nyou’ll have all services sending both metrics and logs to centralized systems that’ll allow \nyou to improve observability. You’ll be able to observe data about a running system and \ndig deeper to collect more information in case you need to audit or debug a particular \nrequest. You’ll set up a solution commonly called ELK (Elasticsearch, Logstash, and \nKibana) and will use a data collector called Fluentd.\n12.3.1\t ELK- and Fluentd-based solution\nYou’ll build the logging infrastructure we propose using Elasticsearch, Logstash, and \nKibana. Also, you’ll use Fluentd for pushing logs from the apps to your centralized log-\nging solution. Before we get into more details about these technologies, have a look at \nfigure 12.5 to get an overview of what we want to enable you to achieve.\nIn figure 12.5, you can see how you can collect the logs for multiple instances of the \ngateway service and forward them to your centralized logging system. We represent \nmultiple instances of the same service, but this will work for any of the services you have \nrunning. Services will redirect all the log information to STDOUT (standard output), and \nan agent running the Fluentd daemon will be responsible for pushing those logs into \nElasticsearch.\n \n\n\n\t\n305\nSetting up a logging infrastructure for SimpleBank\n2018-02-04T13:55:19.263380548Z {\"@timestamp\":\n\"2018-02-04T13:55:19.262Z\", \"@version\": 1,\n\"source_host\": \"bb69db21f1eb\", \"name\": \"root\", \"args\": [],\n\"levelname\": \"INFO\", \"levelno\": 20, \"pathname\": \"./app.py\",\n\"filename\": \"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152, \"msecs\": 261.5199089050293,\n\"relativeCreated\": 669360.3167533875, \"thread\":\n140130358674856, \"threadName\": \"GreenTread-13\",\n\"processName\": \"MainProcess\", \"process\": 1, \"message\":\n\"{'event': 'sell_order_received', 'uuid':\n'11fcdc94-c055-4f26-988b-886165655792'}\", \"event\":\n\"sell_order_received\", \"uuid\":\n\"11fcdc94-c055-4f26-988b-886165655792\"}\n2018-02-04T13:55:19.263380548Z {\"@timestamp\":\n\"2018-02-04T13:55:19.262Z\", \"@version\": 1,\n\"source_host\": \"bb69db21f1eb\", \"name\": \"root\", \"args\": [],\n\"levelname\": \"INFO\", \"levelno\": 20, \"pathname\": \"./app.py\",\n\"filename\": \"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152, \"msecs\": 261.5199089050293,\n\"relativeCreated\": 669360.3167533875, \"thread\":\n140130358674856, \"threadName\": \"GreenTread-13\",\n\"processName\": \"MainProcess\", \"process\": 1, \"message\":\n\"{'event': 'sell_order_received', 'uuid':\n'11fcdc94-c055-4f26-988b-886165655792'}\", \"event\":\n\"sell_order_received\", \"uuid\":\n\"11fcdc94-c055-4f26-988b-886165655792\"}\nGateway\n2018-02-04T13:55:19.263380548Z {\"@timestamp\":\n\"2018-02-04T13:55:19.262Z\", \"@version\": 1,\n\"source_host\": \"bb69db21f1eb\", \"name\": \"root\", \"args\": [],\n\"levelname\": \"INFO\", \"levelno\": 20, \"pathname\": \"./app.py\",\n\"filename\": \"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152, \"msecs\": 261.5199089050293,\n\"relativeCreated\": 669360.3167533875, \"thread\":\n140130358674856, \"threadName\": \"GreenTread-13\",\n\"processName\": \"MainProcess\", \"process\": 1, \"message\":\n\"{'event': 'sell_order_received', 'uuid':\n'11fcdc94-c055-4f26-988b-886165655792'}\", \"event\":\n\"sell_order_received\", \"uuid\":\n\"11fcdc94-c055-4f26-988b-886165655792\"}\nLogs\nGateway\nLogs\nGateway\nLogs\nCentralized logging\nSTDOUT\nSTDOUT\nFluentd\nSTDOUT\nGateway service deployed in Kubernetes\nFigure 12.5    Collecting logs from multiple service instances and forwarding them to a centralized location\nBy following this pattern when deploying any new services, you’ll make sure log data \ngets collected and indexed and becomes searchable. But before we move to imple-\nmentation, we’ll take a little time to introduce each of the technologies you’ll be \nusing.\nElasticsearch\nElasticsearch (www.elastic.co/products/elasticsearch) is a search and analytics engine \nthat stores data centrally. It indexes data, in your case log data, and allows you to per-\nform efficient search and aggregation operations on the data it has stored.\nLogstash\nLogstash (www.elastic.co/products/logstash) is a server-side processing pipeline that \nallows data ingestion from multiple sources and transforms that data prior to send-\ning it to Elasticsearch. In your case, you’ll be using the Logstash formatting and data \ncollection capabilities by taking advantage of client libraries. In this chapter’s earlier \nexamples, you already observed its ability to provide consistent data that you can send \nto Elasticsearch. But here you won’t be using Logstash to send data; you’ll be using \nFluentd instead.\nKibana\nKibana (www.elastic.co/products/kibana) is a UI for visualizing Elasticsearch data. It’s \na tool you can use to query data and explore its associations. In your use case, it’ll \noperate on log data. You can use Kibana to derive visualizations from gathered data, so \nit’s more than a search tool. Figure 12.6 shows an example of a dashboard powered by \nKibana.\n \n\n\n306\nChapter 12  Using logs and traces to understand behavior \nFigure 12.6    Kibana tutorial dashboard showing visualizations created from log data\nFluentd\nFluentd (www.fluentd.org) is an open source data collector that you’ll be using to push \ndata from your services to Elasticsearch. You’ll combine the data formatting and col-\nlecting capabilities of Logstash and use Fluentd to push that data. One of its advan-\ntages is the fact that you can use it as a logging provider for Dockerfiles if you declare it \nin Docker compose files.\n12.3.2\t Setting up your logging solution\nYou’ll set up your solution via the Docker compose file, like you already did in chap-\nter 11 to create the metrics collecting and alert infrastructure. You can find all the \ncode used in this chapter at Github (http://mng.bz/k191). There, you’ll find the \ndocker-compose.yml file, where you’ll be declaring the new dependencies. The fol-\nlowing listing shows the new components added to the compose file.\n Listing 12.2 Docker compose file with Elasticsearch, Kibana, and Fluentd containers\nversion: '2.1'\nservices:\n  gateway:\n    container_name: simplebank-gateway\n    restart: always\n    build: ./gateway\n    ports:\n \n\n\n\t\n307\nSetting up a logging infrastructure for SimpleBank\n      - 5001:5000\n    volumes:\n      - ./gateway:/usr/src/app\n    links:\n      - \"rabbitmq:simplebank-rabbitmq\"\n      - \"fluentd”\n    logging: \n      driver: \"fluentd\" \n      options:\n        fluentd-address: localhost:24224\n        tag: simplebank.gateway\n(…)\n  kibana:\n    image: kibana \n    links:\n      - \"elasticsearch\" \n    ports:\n      - \"5601:5601\"\n  elasticsearch:\n    image: elasticsearch \n    expose:\n      - 9200\n    ports:\n      - \"9200:9200\"\n  fluentd:\n    build: ./fluentd \n    volumes:\n      - ./fluentd/conf:/fluentd/etc \n    links:\n      - \"elasticsearch\" \n    ports:\n      - \"24224:24224\"\n      - \"24224:24224/udp\"\n(…)\nOnce you’ve added this content to the docker compose file, you’re almost ready to \nboot your logging infrastructure alongside your services. But first let’s cover the miss-\ning tweaks we mentioned previously that’ll allow you to configure Fluentd to your \nneeds. The Dockerfile you use for building Fluentd follows.\nListing 12.3    Fluentd Dockerfile (Fluentd/Dockerfile)\nFROM fluent/fluentd:v0.12-debian \nRUN [\"gem\", \"install\", \"fluent-plugin-elasticsearch\", \n➥\"--no-rdoc\", \"--no-ri\", \"--version\", \"1.9.2\"] \nNow all you need is to create a configuration file for Fluentd. The following listing \nshows the config file.\nAdds a logging directive to each service to \nforce Docker to push the output of each \ncontainer running a service to Fluentd, \nwhich in turn will make sure data gets \npushed to Elasticsearch\nFor Kibana, uses the default image in \nDocker Hub with the defaults set\nLinks Kibana to the Elasticsearch container, \nbecause it’ll consume data from it\nLike you did for Kibana, uses the default \nimage for Elasticsearch\nBuilds Fluentd from a custom Docker image\nInjects the configuration for Fluentd into \nthe built container, allowing you to \ntweak the default configuration\nLinks the Fluentd container to the Elasticsearch \ncontainer, because it’ll be pushing data into it\nPulls the Fluentd base image\nInstalls the Elasticsearch \nplugin for Fluentd\n \n\n\n308\nChapter 12  Using logs and traces to understand behavior \nListing 12.4    Fluentd configuration (fluentd/conf/fluent.conf)\n<source> \n  @type forward \n  port 24224\n  bind 0.0.0.0\n</source>\n<match *.**> \n  @type copy \n  <store>\n    @type elasticsearch \n    host elasticsearch\n    port 9200\n    logstash_format true\n    logstash_prefix fluentd\n    logstash_dateformat %Y%m%d\n    include_tag_key true\n    type_name access_log\n    tag_key @log_name\n    flush_interval 1s\n  </store>\n  <store>\n    @type stdout \n  </store>\n</match>\n The match section in the Fluentd configuration file contains all the needed configu-\nration for connecting with elasticsearch, port, and host, as well as the format used. \nYou’re using logstash format, as you may recall.\nWith all the needed setup done, you’re now ready to boot your services using the \nnew Docker compose file. Before doing so, let’s go through your services and change \ncode to enable you to send data to your centralized logging infrastructure. In the next \nsection, you’ll configure your services to make use of the Logstash logger. You’ll also set \nlog levels.\n12.3.3\t Configure what logs to collect\nIn your services, you can control the log level via environment variables, so you can \nhave different levels for development and production environments. Using different \nlog levels allows you to enable more verbose logs in production, in case you need to \ninvestigate any issue.\nLet’s look at your gateway service for its logging configuration and also at the service \ncode to understand how you can emit log messages. The logging configuration is shown \nin the following listing.\nConfigures the location data comes from: port \n24224 for both TCP and UDP, as you declared \nin the Docker compose file in listing 12.2\nPlugin to use for input; listens to a TCP socket \nand a UDP socket for heartbeats that work as \na way to monitor the health of Fluentd\nIndicates what Fluentd should do — In this section, \nyou configure two stores: one that processes json data \nand another that processes all other stdout data.\nOutput plugins used in the match section: \ncopy for copying events to multiple sources; \nelasticsearch to record data in Elasticsearch; \nand stdout to output all data entering Fluentd \n \n\n\n\t\n309\nSetting up a logging infrastructure for SimpleBank\nListing 12.5    Configuration file for the gateway service (gateway/config.yml)\nAMQP_URI: amqp://${RABBIT_USER:guest}:${RABBIT_PASSWORD:guest}@${RABBIT_HOST:\n➥localhost}:${RABBIT_PORT:5672}/\nWEB_SERVER_ADDRESS: '0.0.0.0:5000'\nRPC_EXCHANGE: 'simplebank-rpc'\nLOGGING: \n    version: 1\n    handlers:\n        console:\n            class: logging.StreamHandler \n    root:\n        level: ${LOG_LEVEL:INFO} \n        handlers: [console] \nThis configuration will allow setting the log level when the application boots. In the \nDocker compose file, you’ve set the environment variable LOG_LEVEL as INFO for all ser-\nvices except for the Gateway, which has a DEBUG value. Let’s now look into the gateway \ncode to set up logging, as shown in the following listing.\nListing 12.6    Enable logging in the gateway service (gateway/app.py)\nimport datetime\nimport json\nimport logging \nimport uuid\nfrom logstash_formatter import LogstashFormatterV1 \nfrom nameko.rpc import RpcProxy, rpc\nfrom nameko.web.handlers import http\nfrom statsd import StatsClient\nfrom werkzeug.wrappers import Request, Response\nclass Gateway:\n    name = \"gateway\"\n    orders = RpcProxy(\"orders_service\")\n    statsd = StatsClient('statsd', 8125,\n                         prefix='simplebank-demo.gateway')\n    logger = logging.getLogger() \n    handler = logging.StreamHandler() \n    formatter = LogstashFormatterV1() \n    handler.setFormatter(formatter) \n    logger.addHandler(handler) \n    @http('POST', '/shares/sell')\n    @statsd.timer('sell_shares')\nLogging configuration section\nDefines the handler class for console logging, \nwhich you’ll use as the handler in listing 12.6 \nReads the log level from an environment variable \n(LOG_LEVEL) and sets a default value of INFO in \ncase the environment variable isn’t defined \nRegisters only a `console` handler, because \nyou won’t be reading from log files\nImports Python’s logging facility (https://docs.python.org/3/ \nlibrary/logging.html)\nImports the Logstash \nFormatter so you can emit \nlogs in logstash format\nInitializes and configures logger\n \n\n\n310\nChapter 12  Using logs and traces to understand behavior \n    def sell_shares(self, request):\n        req_id = uuid.uuid4()\n        res = u\"{}\".format(req_id)\n        self.logger.debug(\n            \"this is a debug message from gateway\", \n➥extra={\"uuid\": res}) \n        self.logger.info(\"placing sell order\", extra=\n➥{\"uuid\": res}) \n        self.__sell_shares(res)\n        return Response(json.dumps(\n            {\"ok\": \"sell order {} placed\".format(req_id)}),\n            ➥mimetype='application/json')\n    @rpc\n    def __sell_shares(self, uuid):\n        self.logger.info(\"contacting orders service\", extra={\n            ➥\"uuid\": uuid}) \n        res = u\"{}\".format(uuid)\n        return self.orders.sell_shares(res)\n    @http('GET', '/health')\n    @statsd.timer('health')\n    def health(self, _request):\n        return json.dumps({'ok': datetime.datetime.utcnow().__str__()})\nIn listing 12.2, you saw how to enable the Fluentd driver for logging with Docker. This \nmeans you’re ready to send log data generated from your services to Elasticsearch \nusing Fluentd, and afterwards you’ll be able to explore that log data using Kibana. \nTo start all the services, metrics, and logging infrastructure from a console in the root \ndirectory, execute the following command:\ndocker-compose up --build --remove-orphans\nOnce all is ready, you need to complete one last step, which is configuring Kibana \nto use the logs collected via Fluentd and stored in Elasticsearch. To do so, access the \nKibana web dashboard (http://localhost:5601). On the first access, you’ll be redi-\nrected to the management page, where you’ll need to configure an index pattern. You \nneed to tell Kibana where to find your data. If you recall, in the Fluentd configuration, \nyou set an option for the logstash prefix with the value fluentd. This is what you need \nto enter in the index text box presented to you. Figure 12.7 shows the Kibana dash-\nboard management section and the value you need to input.\nAfter inserting fluentd-* as the index pattern and clicking the Create button, you’ll \nbe ready to explore all the log data that your multiple services create. Elasticsearch will \nforward all data to a central location, and you’ll be able to access it in a convenient way.\nExample of how to log a message with \nthe DEBUG level — This message will \nonly be sent if the application log level \nis set to DEBUG. If you set it to INFO, \nthis message won’t be sent.\nExamples of log messages \nemitted with the INFO log level\n \n\n\n\t\n311\nSetting up a logging infrastructure for SimpleBank\nFigure 12.7    The Kibana management section where you need to indicate the index pattern for it to be \nable to fetch data from Elasticsearch\nTo generate some log data, all you need to do is create a sell request to your gateway \nservice. To do so, you need to issue a POST request to the gateway. The following shows \npresenting a request via curl, but any tool capable of generating a POST request will do:\nchapter-12$ curl -X POST http://localhost:5001/shares/sell \\\n  -H 'cache-control: no-cache' \\\n  -H 'content-type: application/json' \nchapter-12$ {\"ok\": \"sell order e11f4713-8bd8-4882-b645\n➥-55f96d220e44 placed\"} \nNow that you have log data collected, you can explore it using Kibana. Clicking on \nthe Discover section on the left side of Kibana’s web dashboard will take you to a \npage where you can perform searches. In the search box, insert the request UUID \nyou received as the sell order response. In the case of this example, you’d be using \ne11f4713-8bd8-4882-b645-55f96d220e44 as your search parameter. In the next sec-\ntion, we’ll show you how to use Kibana to follow the execution of a sell request through \nthe different services involved.\n12.3.4\t Finding a needle in the haystack\nIn the code example for this chapter, you have five independent services collaborat-\ning to allow SimpleBank users to sell shares. All services are logging their operations \nand using the request UUID as a unique identifier to allow you to aggregate all log \ncurl command to the gateway service\nResponse from the service — the UUID you receive allows you to \nidentify your sell order, and you can use it as a search term on \nKibana. (The UUID shown is randomly generated, so please use \nthe one you receive as a response to the request you issued.)\n \n\n\n312\nChapter 12  Using logs and traces to understand behavior \nmessages referring to a sell order that’s processing. You can make use of Kibana to \nexplore your logs and track down the execution of a request. In figure 12.8, you use \nthe order ID that the gateway service returns to perform the search.\nWhen you use the request ID as the search parameter, Kibana filters the log data, and \nyou get 11 hits that allow you to follow the execution of a request through different ser-\nvices. Kibana allows you to use complex queries to be able to uncover insights. You get \nthe ability to filter per service, sort by time, and even use log data — for example, exe-\ncution times present in the log entries — to create dashboards to track performance. \nThis use is beyond the scope of this chapter, but do feel free to explore the possibilities \noffered to get new perspectives on the collected data.\nWe’ll now zoom in a bit and focus on some of the log entries that your query shows. \nFigure 12.9 shows some of those entries with a bit more detail.\nFigure 12.9    Detailed view of log messages in the Kibana search page\nFigure 12.8    Searching the log data using a request ID\n \n\n\n\t\n313\nTracing interactions between services\nIn figure 12.9, you’ll find messages from the market and gateway services. For the latter, \ngiven that the log level selected was DEBUG, you’ll find both info and debug messages. \nAs mentioned previously, by using the logstash-formatter library in your Python \ncode, you get more information for free. You can find data regarding execution times \nand scoping of the execution by module, function, line, process, and thread. All of this \ninfo can be useful if you need to diagnose any issues in the future.\n12.3.5\t Logging the right information\nNow that you’re able to collect logs and store them, you need to be careful about what \ninformation you send via logs. Things like passwords, credit card numbers, ID card \nnumbers, and potentially sensitive personal data that get sent will be stored and acces-\nsible by anyone who can use the logging infrastructure. In your case, you’re hosting \nand controlling the logging infrastructure, but if you were using a third-party provider, \nyou’d need to pay extra attention to these details. You have no easy way to delete only \nsome of the data already sent. In most cases, if you want to delete something specific, it \nmeans deleting all the log data for a given period of time.\nData privacy is a hot topic at the moment, and with the EU General Data Protection \nRegulation (GDPR) (www.eugdpr.org) now in effect, you need to take extra care when \nconsidering which data to log and how to log it. We won’t explore the needed steps \nin depth here, but both Fluentd and Elasticsearch allow you to apply filtering to data \nso that any sensitive fields get masked, encrypted, or removed from the data that they \nreceive and that Elasticsearch indexes. The general rule would be to log as little infor-\nmation as you can, avoid any personal data in logs, and take extra care with reviewing \nwhat gets logged before any changes make it into the production environment. Once \nyour services send data, it’s hard to erase it and doing so will have associated costs.\nThat said, you can and should use logs to communicate useful information to allow \nyou to understand system behavior. Sending IDs that allow you to correlate actions of \ndifferent systems and terse log messages indicating actions performed in or by systems \ncan help you keep track of what’s happened.\n12.4\t Tracing interactions between services\nWhen you were setting up your log infrastructure and making the code changes to emit \nlog messages, you already took care of propagating an ID field that allows you to follow \nthe execution path of a request through your system. With this set up, you can group log \nentries under the same context. You may even use log data to create visualizations that’ll \nallow you to understand how much time each component took to process a request. In \naddition, you can use it to help you identify bottlenecks and places where you can improve \nyour code to have extra performance. But logs aren’t the only tool available — you have \nanother method at your disposal for doing this that doesn’t rely on log data.\nYou can do better by reconstructing the journey of requests through your microservices. \nIn this section, you’ll be setting up distributed tracing to allow you to visualize the flow of \nexecution between services and at the same time, provide insights on how long each oper-\nation takes. This can be valuable, not only to understand the order in which a request flows \n \n\n\n314\nChapter 12  Using logs and traces to understand behavior \nthrough multiple services, but also to identify possible bottlenecks. For this, you’ll use Jae-\nger and libraries compatible with the OpenTracing API (http://opentracing.io). \nOpenTracing API\nThe OpenTracing API is a vendor-neutral open standard for distributed tracing. A lot of \ndistributed tracing systems (for example, Dapper, Zipkin, HTrace, X-Trace) provide tracing \ncapabilities but do so using incompatible APIs. Choosing one of those systems would \ngenerally mean tightly coupling systems potentially using different programming lan-\nguages to a single solution. The purpose of the OpenTracing initiative is to provide a set \nof conventions and a standard API for collection of traces. Libraries are available for mul-\ntiple languages and frameworks. You can find some of the supported tracer systems at \nhttp://mng.bz/Gvr3.\n \n12.4.1\t Correlating requests: traces and spans\nA trace is a direct acyclic graph (DAG) of one or more spans, where the edges of those \nspans are called references. Traces are used to aggregate and correlate execution flow \nthrough the system. To do so, some information needs to be propagated. A trace cap-\ntures the whole flow.\nLet’s look at figures 12.10 and 12.11. In these figures, we represent a trace made up \nof multiple spans, from both a dependency perspective and a temporal perspective.\nFigure 12.10    A trace made up of eight different spans from a dependency perspective\n \n\n\n\t\n315\nTracing interactions between services\nFigure 12.11    The temporal relationships in an eight-span trace\nIn figure 12.10, you can observe the dependency relationship between different spans. \nThese spans can be triggered either in the same application or in different ones. The \nonly requirement is for the parent span ID to be propagated, so when a new span is \ntriggered, it’ll hold a reference to its parent span.\nIn figure 12.11, you have a view of spans from a temporal perspective. By using tem-\nporal information contained in spans, you can organize them in a timeline. You can \nsee not only when each span happened relative to other spans but also how long each \noperation that a span encapsulates took to complete.\nEach span contains the following information:\n¡ An operation name\n¡ A start and a finish timestamp\n¡ Zero or more span tags (key value pairs)\n¡ Zero or more span logs (key value pairs with a timestamp)\n¡ A span context\n¡ References to zero or more spans (via the span context)\nThe span context contains the needed data to refer to different spans, either locally or \nacross service boundaries.\nLet’s now move on to setting up tracing between services. You’ll be using Jaeger \n(www.jaegertracing.io), a distributed tracing system, as well as a set of Python libraries \nthat are OpenTracing compatible.\n12.4.2\t Setting up tracing in your services\nTo be able to display tracing information and correlate requests between different ser-\nvices, you’ll need to set up a collector and a UI for traces, as well as including some \nlibraries and setting them up in your services. The services we’ll use as an example for \ndistributed tracing will be the SimpleBank profile and settings services. In figure 12.12, \nwe give an overview of the interactions you’ll be tracing.\n \n\n\n316\nChapter 12  Using logs and traces to understand behavior \nProfile\nservice\nSettings\nservice\nGET\nGET\nhttp://ip.jsontest.com\nFigure 12.12    Interactions of the profile service\nThe profile service will contact an external service, in this case jsontest.com, to retrieve \nits IP and will also be fetching user settings from the settings service. You’ll be setting \nup a tracing system (Jaeger) and making the code changes needed to display the trace \nand its spans and to be able to correlate those spans. Correlating them will allow you \nto understand in detail how long each operation took and how it contributed to the \noverall execution time of a call to the profile service. You’ll begin by setting up Jaeger, \nthe distributed tracing system collector and UI, by adding a Docker image into your \ndocker-compose.yml file (listing 12.7).\nJaeger\nInspired by Dapper and OpenZipkin, Jaeger is a distributed tracing system released as \nopen source by Uber Technologies. You use it for monitoring and troubleshooting micro­\nservice-based distributed systems.\n \nListing 12.7    Adding Jaeger to the docker-compose.yml file\n(…)\njaeger:\n    container_name: jaeger\n    image: jaegertracing/all-in-one:latest \n    ports:\n       - 5775:5775/udp \n       - 6831:6831/udp\n       - 6832:6832/udp\n       - 5778:5778\n       - 16686:16686 \n       - 14268:14268\n       - 9411:9411 \n    environment:\n      COLLECTOR_ZIPKIN_HTTP_PORT: \"9411\" \nYou’ll be using a jaeger image containing \nall the needed components because it’ll \nbe easier to set up. This all-in-one image \nhas in-memory-only storage for spans.\nPort for communicating spans\nPort for accessing the Jaeger UI\nPort used by Zipkin, another distributed tracing system — One of the advantages of \nthe OpenTracing initiative is the fact that you can use different systems without the \nneed to change all implementations or be locked to a particular one.\n \n\n\n\t\n317\nTracing interactions between services\nWith the Docker image added to your docker-compose file, once you boot all the \nSimpleBank infrastructure, you’ll have a distributed tracing system in place. Now you \nneed to make sure that the SimpleBank profile and settings services are able to create \ntraces and spans and communicate them to Jaeger.\nLet’s add the needed libraries to both the settings and profile services and initialize \nthe tracer. The following listing adds the tracing libraries.\nListing 12.8    Adding the tracing libraries to the services via a requirements.txt file\nFlask==0.12.0\nrequests==2.18.4\njaeger-client==3.7.1 \nopentracing>=1.2,<2 \nopentracing_instrumentation>=2.2,<3 \nBy adding these libraries, you’re now able to create traces and spans from both ser-\nvices. To make the process easier, you can also create a module to provide a convenient \nsetup function to initialize the tracer, as shown in the following listing.\nListing 12.9    Tracer initializer lib/tracing.py\nimport logging\nfrom jaeger_client import Config \ndef init_tracer(service): \n    logging.getLogger('').handlers = []\n    logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n    config = Config(\n        config={\n            'sampler': {\n                'type': 'const',\n                'param': 1,\n            },\n            'local_agent': { \n                'reporting_host': \"jaeger\",\n                'reporting_port': 5775,\n            },\n            'logging': True, \n            'reporter_batch_size': 1,\n        },\n        service_name=service, \n    )\n    return config.initialize_tracer()\nJaeger client library that connects the \nservice to the tracer system\nPython OpenTracing platform library\nCollection of instrumentation tools to simplify integration \nwith different frameworks and applications\nImports the Jaeger client that allows \nestablishing communication between \nthe app and the tracing collector system\nReceives the service name as an argument\nSets up both the host and the port where \ntraces and spans will be sent — In the \nDocker compose file, you have Jaeger \nrunning as “jaeger” and receiving \nmetrics via UDP on port 5775. This is \nnecessary because you’ll have one \ncollector agent running for all services.\nIn addition to collecting metrics in Jaeger, you’re \nalso emitting the trace events to the logs.\nSets the service name to the one  \nreceived as the init function argument\n \n",
      "page_number": 318
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 333-340)",
      "start_page": 333,
      "end_page": 340,
      "detection_method": "topic_boundary",
      "content": "318\nChapter 12  Using logs and traces to understand behavior \nThe SimpleBank profile and settings services will both be using the tracer initialization \nfunction shown in listing 12.9. This allows them to establish the connection to Jaeger. \nAs we showed in figure 12.12, the profile service contacts both an external service and \nthe settings internal service. You’ll be tracing the interaction of the profile service with \nboth of these collaborators. In the case of the interaction with the SimpleBank settings \nservice, you’ll need to pass along the context of the initial trace so you can visualize the \nfull cycle of a request.\nListing 12.10 shows the profile service code where you set up the spans for both the \nexternal http service and the settings service. For the former, you create a span, and for \nthe latter, you pass along the current span as a header so the settings service can make \nuse of it and create child spans.\nListing 12.10    Profile service code\nfrom urlparse import urljoin\nimport opentracing\nimport requests\nfrom flask import Flask, jsonify, request\nfrom opentracing.ext import tags \nfrom opentracing.propagation import Format \nfrom opentracing_instrumentation.request_context import \n➥get_current_span \nfrom opentracing_instrumentation.request_context import \n➥span_in_context \nfrom lib.tracing import init_tracer \napp = Flask(__name__)\ntracer = init_tracer('simplebank-profile') \n@app.route('/profile/<uuid:uuid>')\ndef profile(uuid):\n    with tracer.start_span('settings') as span: \n        span.set_tag('uuid', uuid)\n        with span_in_context(span):\n            ip = get_ip(uuid)\n            settings = get_user_settings(uuid)\n            return jsonify({'ip': ip, 'settings': settings})\ndef get_ip(uuid):\n    with tracer.start_span('get_ip', child_of=\n➥get_current_span()) as span: \n        span.set_tag('uuid', uuid) \n        with span_in_context(span): \n            jsontest_url = \"http://ip.jsontest.com/\"\n            r = requests.get(jsontest_url)\n            return r.json()\nImports the OpenTracing libraries to \nallow you to set up spans and tags\nImports the initializer function as defined in \nlisting 12.9 to set up the connection to Jaeger\nCalls the tracer initializer passing \nthe service name, which will \ncreate a tracer object you can use\nSets the initial span associated with \nthe tracer — The created span will \nbe the parent for spans in both the \ncall to the external service and the \ncall to the settings service.\nCreates a new span for the call to the \nexternal service, a child of the parent \nspan initialized above.\nWraps code execution under the newly created span\n \n\n\n\t\n319\nTracing interactions between services\ndef get_user_settings(uuid):\n    settings_url = urljoin(\"http://settings:5000/\n➥settings/\", \"{}\".format(uuid))\n    span = get_current_span() \n    span.set_tag(tags.HTTP_METHOD, 'GET') \n    span.set_tag(tags.HTTP_URL, settings_url) \n    span.set_tag(tags.SPAN_KIND, tags.SPAN_KIND_RPC\n➥_CLIENT) \n    span.set_tag('uuid', uuid) \n    headers = {}\n    tracer.inject(span, Format.HTTP_HEADERS, headers) \n    r = requests.get(settings_url, headers=headers)\n    return r.json()\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\nThe SimpleBank profile service initializes a trace that’ll be used to group different \nspans. It creates spans for the calls to \"http://ip.jsontest.com/\" and for the call to \nthe SimpleBank settings service. For the former, given that you don’t own the service, \nyou execute the call wrapped in a span. But for the latter, because you control it, you \ncan pass on span information that’ll be used to create child spans. This will allow you to \ngroup all related calls in Jaeger.\nLet’s now look into how you can make use of the injected span in the SimpleBank \nsettings service, as shown in the following listing.\nListing 12.11    Using a parent span in the settings service\nimport time\nfrom random import randint\nimport requests\nfrom flask import Flask, jsonify, request\nfrom opentracing.ext import tags\nfrom opentracing.propagation import Format\nfrom opentracing_instrumentation.request_context import get_current_span\nfrom opentracing_instrumentation.request_context import span_in_context\nfrom lib.tracing import init_tracer\napp = Flask(__name__)\ntracer = init_tracer('simplebank-settings') \n@app.route('/settings/<uuid:uuid>')\ndef settings(uuid):\n    span_ctx = tracer.extract(Format.HTTP_HEADERS, \n➥request.headers) \n    span_tags = {tags.SPAN_KIND: tags.SPAN_KIND_RPC\n➥_SERVER, 'uuid': uuid} \nSets tags for the span\nInjects the span context before \nthe call to the SimpleBank \nsettings service — The span \ncontext will be passed in the \nheaders, and the downstream \nservice will use it to initialize \nits own spans under the \nproper context.\nInitializes the tracer for the service\nExtracts the span context \nfrom the request headers\nSets up the tags for a new span\n \n\n\n320\nChapter 12  Using logs and traces to understand behavior \n    with tracer.start_span('settings', child_of=span\n➥_ctx, tags=span_tags): \n        time.sleep(randint(0, 2))\n        return jsonify({'settings': {'name': 'demo user', 'uuid': uuid}})\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\nBy extracting the span context from the request the settings service receives, you can \nthen make use of it as the parent to new spans. You can later visualize this new child \nspan independently. But you’ll also be able to take advantage of the fact that Jaeger will \nshow the span as both an independent span in the context of the SimpleBank settings \nservice and a child span in the context of the SimpleBank profile service.\n12.5\t Visualizing traces\nWith all the setup out of the way, all you need to do to start collecting traces is to issue \na request to the SimpleBank profile endpoint. You can use the command line or a \nbrowser. To access traces via the command line, you can use curl to issue the following \nrequest:\n$ curl http://localhost:5007/profile/26bc34c2-5959-4679-9d4d-491be0f3c0c0\n{\n  \"ip\": {\n    \"ip\": \"178.166.53.17\"\n  },\n  \"settings\": {\n    \"settings\": {\n      \"name\": \"demo user\",\n      \"uuid\": \"26bc34c2-5959-4679-9d4d-491be0f3c0c0\"\n    }\n  }\n}\nHere’s a brief recap of what’s going on when you hit the profile endpoint:\n¡ The profile service creates a span A.\n¡ The profile service contacts an external service to fetch the IP, wrapping it under \na new span B.\n¡ The profile service contacts the internal SimpleBank settings service to get user \ninfo under a new span C and passes the context of the parent span to the down-\nstream service.\n¡ Both services communicate spans to Jaeger.\nStarts a new span as a child of \nthe one propagated to the \nservice via the request headers\n \n\n\n\t\n321\nVisualizing traces\nFigure 12.13    Jaeger UI search page showing the services that have traces available\nTo visualize the traces, you need to access the Jaeger UI that’ll be running on port \n16686. Figure  12.13 shows the Jaeger UI and the list of services that have traces \navailable.\nIn the Service section, you see three services for which trace information is avail-\nable: two SimpleBank services and one called jaeger-query. The latter gathers Jaeger \ninternal traces and is of little use to you. You’re interested in the other two services \nlisted: simplebank-profile and simplebank-settings. If you recall, the profile service was \ncreating spans for the execution of an external call, as well as for the call to the settings \nservice. Go ahead and select simplebank-profile and click Find Traces at the bottom. \nFigure 12.14 shows the traces for the profile service.\nThe page lists six traces, and all of them have three spans across two services. This \nmeans you were able to collect information about the collaboration between two inter-\nnal services and to get timing information about the execution. Figure 12.15 shows a \ndetailed view of one of those traces.\n \n\n\n322\nChapter 12  Using logs and traces to understand behavior \nFigure 12.14    The simplebank-profile traces information\nFigure 12.15    The timing information and execution sequence of a call to the profile service\n \n\n\n\t\n323\nVisualizing traces\nIn figure 12.15, you can see a timeline for the different steps of execution in a call to \nthe profile service. You have information about the overall time of execution, as well \nas when each suboperation took place and for how long. The spans contain informa-\ntion about the operation, the component that generated them, and their execution \ntimes and relative positions, both in the timeline and in regards to dependencies with \nparent spans.\nThis information can be invaluable in order to know what’s going on in a distrib-\nuted system. You can now visualize the flow of requests through different services and \nknow how long each operation takes to complete. This simple setup allows you to both \nunderstand the flow of execution in a microservice architecture and identify potential \nbottlenecks that you can improve.\nYou also can use Jaeger to understand how different components in your system \nrelate to each other. The top navigation menu bar has a Dependencies link. By clicking \nit and then, in the page that comes up, selecting the DAG (direct acyclic graph) tab, you \nhave access to the view illustrated in figure 12.16.\nThe example we used was a simple one, but it allows you to understand the power of \ntracing in a microservice architecture. Along with logging and metrics, it allows you to \nhave an informed view of both the performance and the behavior of your system.\nFigure 12.16    The service dependency view in the Jaeger UI\n \n\n\n324\nChapter 12  Using logs and traces to understand behavior \nSummary\n¡ You can set up a logging infrastructure using Elasticsearch, Kibana, and Fluentd, \nand distributed tracing using Jaeger.\n¡ A logging infrastructure can generate, forward, and store indexed log data that \nallows searching and correlating requests. \n¡ Distributed tracing allows you to follow the journey of execution of requests \nthrough different microservices.\n¡ Alongside metrics collection, tracing allows you to better understand how the \nsystem is behaving, identify potential issues, and audit your system anytime.\n \n\n\n325\n13\nBuilding microservice teams\nThis chapter covers\n¡ How a microservice architecture affects your \nengineering culture and organization\n¡ Strategies and techniques for building effective \nmicroservice teams\n¡ Common pitfalls in microservice development\n¡ Governance and best practice in large \nmicroservice applications\nThroughout this book, we’ve focused on the technical side of microservices: how to \ndesign, deploy, and operate services. But it’d be a mistake to examine the technical \nnature of microservices alone. People implement software, and building great soft-\nware is as much about effective communication, alignment, and collaboration as \nimplementation choices.\nA microservice architecture is great for getting things done. It allows you to build \nnew services and capabilities rapidly and independently of existing functionality. \nConversely, it increases the scope and complexity of day-to-day tasks, such as oper-\nations, security, and on-call support. It can significantly change an organization’s \ntechnical strategy. It demands a strong culture of ownership and accountability from \n \n",
      "page_number": 333
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 341-350)",
      "start_page": 341,
      "end_page": 350,
      "detection_method": "topic_boundary",
      "content": "326\nChapter 13  Building microservice teams\nengineers. Achieving this culture, while minimizing friction and increasing pace, is vital \nto a successful microservice implementation.\nIn this chapter, we’ll begin by discussing team formation in software engineering \nand the principles that make teams effective. We’ll then examine different models for \nengineering team structure and how they apply to microservice development. Lastly, \nwe’ll explore recommended practices for governance and engineering culture within \nmicroservice teams. Throughout the chapter, we’ll touch on and explain how to miti-\ngate some common pitfalls of microservice development.\nAlthough you might not currently work as an engineering manager, a team lead, or a \ndirector, we think it’s essential to understand how these dynamics — and the choices you \nand your organization make — impact the pace and quality of microservice development.\n13.1\t Building effective teams\nSplitting engineers into independent teams is a natural outcome of organizational \ngrowth. Doing so is necessary to help an organization scale effectively, as limiting team \nsize has several benefits:\n¡ It ensures lines of communication remain manageable — figure 13.1 illustrates \nhow these grow — which aids team dynamism and collaboration while easing \nconflict resolution. Many heuristics exist for “right size,” such as Jeff Bezos’ two-\npizza rule or Michael Lopp’s 7 +/– 3 formula.\n¡ It clearly delineates responsibility and accountability while encouraging inde-\npendence and agility.\nSmall, independent teams can typically move faster than large teams. They also gel \nfaster and gain effectiveness more quickly. Contrastingly, distinct engineering teams \ncan also cause new problems:\n¡ Teams can become culturally isolated, following and accepting different prac-\ntices of quality or engineering values.\n¡ Teams may need to invest extra effort to align on competing priorities when they \ncollaborate with other teams.\n¡ Separate teams may isolate specialist knowledge to the detriment of global \nunderstanding or effectiveness.\n¡ Teams can duplicate work, leading to inefficiency.\nMicroservices can exacerbate these divisions. Different teams will likely no longer work \non the same shared body of code. Teams will have different, competing priorities —  \nand be less likely to have a global understanding of the application.\nBuilding an effective engineering organization beyond a small group of people —  \nand developing great software products — is a balancing act between these two tension \npoints: autonomy and collaboration. If boundaries between teams overlap and owner-\nship is unclear, tension can increase; conversely, independent teams still need to collab-\norate to deliver the whole application.\n \n\n\n\t\n327\nBuilding effective teams\n3 individuals\n3 connections\n5 individuals\n10 connections\n7 individuals\n21 connections\nFigure 13.1    Lines of communication by group size\n13.1.1\t Conway’s Law\nIt can be difficult to separate cause and effect in organizations that have successfully \nbuilt microservice applications. Was the development of fine-grained services a logical \noutcome of their organizational structure and the behavior of their teams? Or did that \nstructure and behavior arise from their experiences building fine-grained services?\nThe answer is: a bit of both! A long-running system isn’t only an accumulation of \nfeatures requested, designed, and built. It also reflects the preferences, opinions, and \nobjectives of its builders and operators. This indicates that structure — what teams work \non, what goals they set, and how they interact — will have a significant impact on how \nsuccessfully you build and run a microservice application.\n \n\n\n328\nChapter 13  Building microservice teams\nConway’s Law expresses this relationship between team and system:\n…organizations which design systems ... are constrained to produce designs which are \ncopies of the communication structures of these organizations…\n“Constrained” might suggest that these communication structures limit and constrict \nthe effective development of a system. But the inverse of the rule is also true: you can \ntake advantage of changes to team structure to produce a desired architecture. Team \nstructure and microservice architecture are symbiotic: both can and should influ-\nence each other. This is a powerful technique, which we’ll consider throughout this \nchapter.\n13.1.2\t Principles for effective teams\nAt a macro level, it’s best to think of teams as units of achievement and communication. \nThey’re how stuff gets done and how people relate to each other within an organiza-\ntion. To realize benefits from microservices and adequately manage their complexity, \nyour teams will need to adopt new working principles and practices, rather than using \nthe same techniques they used to build monoliths.\nThere’s no single right, perfect way to organize your teams. You’ll always suffer from \nconstraints: headcount, budget, personalities, skill sets, and priorities. Sometimes you \ncan hire to fill a gap; sometimes you can’t. The nature of your application and business \ndomain will demand different approaches and skills. Your organization may be lim-\nited in its capacity to change. The best approach we’ve found is to guide the formation \nof teams using a small set of shared principles: ownership, autonomy, and end-to-end \nresponsibility.\nNOTE    Making a move to microservices — or indeed, any large-scale architec-\ntural change — in many enterprises will be challenging and disruptive. You \nwon’t be successful in isolation: you’ll need to find sponsorship, build trust, \nand be prepared to argue your case — a lot! Richard Rodger’s book, The Tao of \nMicroservices (ISBN 9781617293146), goes into more (if slightly cynical) detail \non navigating these institutional politics.\nOwnership\nTeams with a strong sense of ownership have high intrinsic motivation and exercise a \nconsiderable degree of responsibility for the area they own. Because microservice appli-\ncations are typically long-lived, teams that have long-term ownership of an area sup-\nport the evolution of that code while developing deep understanding and knowledge.\nIn a monolithic application, ownership is typically n:1. Many teams own one service: \nthe monolith. This ownership is often split between different layers (such as frontend \nand backend) or between functional areas (such as orders and payments). In a micro­\nservice application, ownership is usually 1:n, meaning a team might own many services. \nFigure 13.2 depicts these ownership models.\n \n\n\n\t\n329\nBuilding effective teams\nTeam A\nTeam B\nApplication\nTeam C\nTeam A\nA\nB\nC\nI\nE\nD\nF\nG\nH\n1:n ownership\nn:1 ownership\nTeam B\nTeam C\nFigure 13.2    Team ownership in monolithic versus microservice codebases\nWARNING    In the 1:n ownership model, it’s usually bad practice for multiple \nteams to own one service. This can make accountability unclear and lead to \nconflict about technical choices and feature priority.\nAs an organization’s codebase grows and the makeup of the engineering team fluc-\ntuates, the risk of code that no one knows — or code that no one can fix when it \nbreaks — increases. Clear ownership helps you avoid this risk by placing natural, rea-\nsonable bounds on a team’s knowledge while ensuring that ownership is the responsi-\nbility of a group, not individual developers.\nAutonomy\nIt’s not coincidental that these three principles reflect some of the principles of micro-\nservices themselves. Teams that can work autonomously — with limited dependencies \non other teams — can work with less friction. These types of teams are highly aligned \nbut loosely coupled.\nAutonomy is important for scale. For an engineering manager, it’s exhausting to \ncontrol the work of multiple teams (not to mention, disempowering for the teams \nthemselves); instead, you can empower teams to self-manage.\nEnd-to-end responsibility\nA development team should own the full ideate-build-run loop of a product. With con-\ntrol over what’s being built, a team can make rational, local priority decisions; exper-\niment; and achieve a short cycle time between coming up with an idea and validating \nthat idea with real code and users.\nMost software spends significantly longer in operation than it ever spent being built. \nBut many software engineers focus on the build stage, throwing code over the fence for \na separate team to run it. This ultimately results in poorer quality and slower delivery. \nHow software operates — how you observe its behavior in the real world — should feed \nback into improving that software (figure 13.3). Without responsibility for operation, \nthis information is often lost. This tenet is also central to the DevOps movement.\n \n\n\n330\nChapter 13  Building microservice teams\nDesign\nDeploy\nObserve\nFigure 13.3    Software operation should continually inform future design and build.\nEnd-to-end responsibility correlates closely with autonomy and ownership:\n¡ The fewer cross-team dependencies in a team’s path to production, the more \nlikely it can control and optimize the pace of its delivery.\n¡ A wider scope of ownership enables the team to reasonably and productively take \non more responsibility for overall delivery.\n13.2\t Team models\nIn this section, we’ll explore two approaches for structuring teams — by function or \nacross function — and their benefits and disadvantages in developing microservices.\n¡ In a functional approach, you group employees by specialization, with a func-\ntional reporting line, and assign them to time-bound projects. Most organiza-\ntions fund projects for a specific scope and length of time. They measure success \nby the on-time delivery of that scope.\n¡ Teams that you build cross-functionally  — from a combination of different skill-\nsets — typically are aligned to long-term product goals or aspirational missions, \nwith freedom within that scope to prioritize projects and build features as needed \nto achieve those missions. You typically measure success through impact on busi-\nness key performance indicators (KPIs) and outcomes.\nThe latter approach is a natural fit with microservices development.\n13.2.1\t Grouping by function\nTraditionally, many engineering organizations have been grouped along horizontal, \nfunctional lines: backend engineers, frontend engineers, designers, testers, product \n(or project) management, and sysadmin/ops. Figure 13.4 illustrates this type of orga-\nnization. In other cases, teams or individuals may move between any number of time-\nbounded projects.\nThis approach optimizes for expertise:\n¡ It ensures that communication loops between specialists are short, so they share \nknowledge and solutions effectively and apply their skills consistently.\n¡ Similar work and approaches are grouped together, providing clear career \ngrowth and skill development.\n \n\n\n\t\n331\nTeam models\nProject A\nFE dev\nDev\nDesigner\nTester\nProject\nmanager\nSysadmin\nFE dev\nDev\nDesigner\nTester\nFE dev\nDev\nDev\nTester\nProject\nmanager\nSysadmin\nUI team\nBackend team\nDesign team\nTest team\nPMO\nOps\nFigure 13.4    Grouping into teams by function and project\nNow imagine you’re building a new feature. This functional approach almost looks \nlike a chain: the analyst team gathers requirements, engineers build backend services, \ntesting windows are scheduled with the QA team, and sysadmins deploy the service. \nYou can see that this approach involves a high coordination burden — delivering a fea-\nture relies on synchronization across several independent teams (figure 13.5).1 This \napproach fails to meet our three principles for effective organization.\nUnclear ownership\nNo team has clear ownership of business outcomes or value — they’re only cogs in the \nvalue chain. As such, ownership of individual services is unclear: once a project is fin-\nished, who maintains the services that were built? How are these iterated on, improved, \nor discarded? Work allocation based on projects tends to shortchange long-term think-\ning and encourages ownership of code by individual engineers, which you want to avoid.\nSpecs\nBuild\nBuild\nCoordinate\nTest\nDeploy\nFeature\nIdea\nAnalysts\nBackend\nFrontend\nPMO\nTest\nOps\nFigure 13.5    Functional teams contributing to the implementation of a feature\n1\t And lo, the organization invented project managers!\n \n\n\n332\nChapter 13  Building microservice teams\nLack of autonomy\nThese teams are tightly coupled, not autonomous. Their priorities are set elsewhere, and \nevery time work crosses a team boundary, the chance increases that a team will be blocked \nand development will be hampered. This leads to long lead times, rework, quality issues, \nand delays. Without alignment to the system architecture they’re building, the team will \nbe unable to evolve their application without being encumbered by other teams.\nNo long-term responsibility\nA project-oriented approach isn’t conducive to long-term responsibility for the code pro-\nduced or for the quality of a product. If the team is only together for a time-bound project, \nthey might hand off their code to another department to run the application, so the orig-\ninal team won’t be able to iterate on their original ideas and implementation. The organi-\nzation will also fail to realize benefits from knowledge retention in the original team.\nLastly, a new team requires time to normalize productive working behaviors — the \nlonger people work together, the better the team gels, and the more effective it becomes. \nA team that stays together longer will maintain a longer period of high performance.\nTIP    There’s also a risk that long-lived teams can become too comfortable or \nset in their ways. It’s important to balance long-term bonding and bringing \npeople with new perspectives and skills into the team.\nRisk of silos\nLastly, this approach also risks the formation of silos — teams diverge in goals and \nbecome incapable of effective, empathetic collaboration. Hopefully you’ve never \nworked someplace where the relationship between test and dev, or dev and ops, is \nalmost adversarial, but it’s been known to happen.\nUltimately, it’s unlikely that a functional, project-oriented organization will deliver \na microservice application without incurring significant friction and substantial cost.\n13.2.2\t Grouping across functions\nBy optimizing for expertise, the functional approach aims to eliminate duplicated work \nand skill-based inefficiencies, in turn reducing overall cost. But this can cause gridlock: \nincreasing friction and reducing your speed in achieving organizational goals. This isn’t \ngreat — your microservice architecture was meant to increase pace and reduce friction.\nLet’s look at an alternative. Instead of grouping by function, you can work cross- \nfunctionally. A cross-functional team is made up of people with different specialties and \nroles intended to achieve a specific business goal. You could call these teams market-driven: \nthey might aim toward a specific, long-term mission; build a product; or connect directly \nwith the needs of their end customer. Figure 13.6 depicts a typical cross-functional team.\nNOTE    We won’t cover team leadership or reporting lines in any detail in \nthis book. A product owner, an engineering lead, a technical lead, a project \nmanager, or a partnership between those roles might lead a cross-functional \nteam. For example, at Onfido, a product manager — who focuses on what the \nteam should do — and an engineering lead — who focuses on how to achieve \nit — lead our teams in partnership.\n \n\n\n\t\n333\nTeam models\nProduct\nmanager\nFE dev\nDev\nTest\nengineer\nDesigner\nDev\nTeam\nFigure 13.6    A typical cross-functional development team\nCompared to the functional approach, a cross-functional team can be more closely \naligned with the end goal of the team’s activity. The multidisciplinary nature of the team \nis conducive to ownership. By taking on end-to-end responsibility for specification, \ndeployment, and operation, the team can work autonomously to deliver features. The \nteam gains clear accountability by taking on a mission that has a meaningful impact on \nthe business’s success. Day-to-day partnership between different specialists eliminates \nsilos, as team members share ownership for the ultimate product of the team’s work.\nDesigning these teams to be long-lived (for example, at least six months) is also ben-\neficial. A long-lived team builds rapport, which increases their effectiveness, and shared \nknowledge, which increases their ability to optimize and improve the system under devel-\nopment. They also take long-term responsibility for the operation of the microservice \napplication, rather than handing it off to another team.\nThe cross-functional, end-to-end approach to structuring teams is advantageous to \nmicroservice development:\n¡ Aligning teams with business value will be reflected in the application developed; \nthe teams will build services that explicitly implement business capabilities.\n¡ Individual services will have clear ownership. \n¡ Service architecture will reflect low coupling and high cohesiveness of teams.\n¡ Functional specialists in different teams can collaborate informally to develop \nshared practices and ways of working.\nThis approach is common in modern web enterprises and is often cited as a reason \nfor their success. For example, Amazon’s CTO described the company’s approach to \narchitecture in 2006:\nIn the fine grained services approach that we use at Amazon, services do not only represent \na software structure but also the organizational structure. The services have a strong \nownership model, which combined with the small team size is intended to make it very easy \nto innovate. In some sense you can see these services as small startups within the walls of \na bigger company. Each of these services require a strong focus on who their customers are, \nregardless whether they are externally or internally.\n-—Werner Vogels\n \n\n\n334\nChapter 13  Building microservice teams\nPerhaps most importantly, a well-formed cross-functional team will be faster at deliver-\ning features than a group of functional teams, as lines of communication are shorter, \ncoordination is local, and team members are aligned. The cross-functional approach \nprioritizes pace — but not at the expense of quality!\n13.2.3\t Setting team boundaries\nA cross-functional team should have a mission. A mission is inspirational: it gives the \nteam something to strive toward but also sets the boundaries of a team’s responsibili-\nties. Determining what a team is (and isn’t) responsible for encourages autonomy and \nownership while helping other teams align with each other. A mission is usually a busi-\nness problem; for example, a growth team might aim to maximize recurring spend by \ncustomers, whereas a security team might aim to protect its codebase and data from \nknown and novel threats. Based on this mission, each team prioritizes its own roadmap \nin collaboration with relevant partners within the business. Cross-cutting initiatives are \ndriven by product or technical leadership.\nNOTE    This type of team organization is also known as product mode  — which \ndoesn’t mean each team is working on a self-contained product. Teams might \nown different vertical slices or different horizontal components of the same \nproduct. A particular component might be technically complex enough to \ndemand a dedicated team.2\nIf your company offers a range of small products — that a team of 7 +/– 3 can produc-\ntively work on — each team can be responsible for one product (figure 13.7). This isn’t \nthe case in many companies such as those that offer a large, complex product to mar-\nket, requiring the effort of multiple teams.\nFor larger scale scenarios, bounded contexts — covered in chapter 4 — are an effec-\ntive starting point for setting loose boundaries for different teams in an organization. \nThey also have the benefit of creating teams that map closely to business teams within \nthe enterprise; for example, a warehouse product team will interact closely with ware-\nhouse operations.3 Figure 13.8 illustrates a possible model for teams within SimpleBank.\nTeam A\nOwns\nServices\nProduct A\nTeam B\nOwns\nServices\nProduct B\nFigure 13.7    A team-per-product model\n2\t A recent ThoughtWorks article describes these product-mode teams: Sriram Narayan, “Products \nOver Projects,” February 20, 2018, http://mng.bz/r0v4.\n3\t Be careful about how you approach this: the organizational structure itself might be suboptimal!\n \n\n\n\t\n335\nTeam models\nCustomers\nAccounts\nCustomers\nStrategies\nPlaceStrategyOrder\nInvestment\nstrategies\nInvestment\nMarket &\nresearch\nCash\nAsset\ninformation\nPayments\nOrders\nMarket gateway\nOrders\nResponsible\nteam\nContexts and\nservices\nFigure 13.8    A possible model of service and capability ownership by different engineering teams for \nSimpleBank\nForming teams that own services in specific bounded contexts makes use of the inverse \nversion of Conway’s Law: if systems reflect the organizational structure that produces \nthem, then you can attain a desirable system architecture by first shaping the structure \nand responsibilities of your organization.\nAs with services themselves, the right boundaries between teams may not always be \nobvious. We keep two general rules in mind:\n¡ Watch the team size. If it approaches or surpasses nine people, it’s likely that a team \nis doing too much or beginning to suffer from communication overhead.\n¡ Consider coherence. Are the activities the team does cohesive and closely related? If not, \na natural split may exist within the team between different groups of coherent work.\n13.2.4\t Infrastructure, platform, and product\nAlthough we’ve advocated strongly for end-to-end ownership, it isn’t always practical. For \nexample, the underlying infrastructure — or microservice platform — of a large company \nis typically complex and requires a joined-up roadmap and dedicated effort, rather than \nloose collaboration between DevOps specialists spread across distinct teams.\nAs we outlined earlier in the book, building a microservice platform — deployment pro-\ncesses, chassis, tooling, and monitoring — is vital to sustainably and rapidly building a great \nmicroservice application. When you first start working with microservices, the team build-\ning the application will usually own the task of building the platform too (figure 13.9).\nOver time, this platform will need to serve the needs of multiple teams, at which stage \nyou might establish a platform team (figure 13.10).\nMicroservice team\nSupports\nBuilds\nBuilds\nApplication\nPlatform\nFigure 13.9    Early on, one team builds both the microservice application and the supporting platform.\n \n",
      "page_number": 341
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 351-365)",
      "start_page": 351,
      "end_page": 365,
      "detection_method": "topic_boundary",
      "content": "336\nChapter 13  Building microservice teams\nProduct teams\nA\nB\nC\nBuilds\nBuilds\nSupports\nPlatform\nservices\nApplication\nEnables\nPlatform team\nFigure 13.10    Establishing a platform team\nDepending on the needs of your company and your technical choices, you might split \nthis platform team further (figure 13.11) to distinguish core infrastructural concerns \n(such as cloud management and security) from specific microservice platform con-\ncerns (such as deployment and cluster operation). This is especially common in com-\npanies that operate their own infrastructure, rather than using a cloud provider.\nProduct tier\nApplication\nServices\nPlatform tier\nObservability\nDeployment\nCluster\nSecurity\nNetworking\nStorage\n...\n...\nA\nB\nC\nBuilds\nBuilds\nBuilds\nSupports\nSupports\nPlatform\nInfrastructure\nInfrastructure tier\nEnables\nEnables\nFigure 13.11    Establishing an infrastructure team as one tier in a three-tier model\n \n\n\n\t\n337\nTeam models\nIn an even larger engineering organization, these tiers might be separated further; for \nexample, different platform teams might focus on deployment tools, observability, or \ninter-service communication. This is also illustrated in figure 13.11.\nThe three-tier model shown in the figure provides economies of scale and special-\nization. This isn’t a service relationship, where teams log tickets to each other. Instead, \nthe output of each tier is a “product” that enables teams in the layer above to be more \neffective and productive.\n13.2.5\t Who’s on-call?\nThe DevOps movement has been a strong influence on microservice approaches. \nA DevOps mentality — breaking down the barriers between build and runtime — is \nvital for doing microservices well, as deploying and operating multiple applications \nincreases the cost and complexity of operational work. This movement encourages \na “you build it, you run it” mindset; a team that takes responsibility for the oper-\national lifetime of their services will build a better, more stable and more reliable \napplication. This includes being on-call — ready to answer alerts — for your produc-\ntion services.\nTIP    Chapter 11 covers best practices for triggering useful and actionable alerts \nfrom microservices.\nFor example, in the three-tier model:\n¡ Engineering teams would be on-call for alerts from their own services.\n¡ Platform and infrastructure teams would be on-call for issues in underlying infra-\nstructure or shared services, such as deployment.\n¡ An escalation path would exist between those two teams to support investigation.\nThis on-call model is illustrated in figure 13.12.\nProduct\nPlatform\nInfrastructure\nServices\nPlatform\nInfrastructure\nAlerts\nAlerts\nAlerts\nErrors, anomalous latency/\nthroughput, events from\napplication code\nErrors, saturation, anomalies\nfrom underlying cluster or tools,\nfor example, cluster, databases,\ndeployment\nErrors, saturation, anomalies\nfrom underlying infrastructure,\nfor example, network\nTeams\nApplication\nFigure 13.12    On-call model in a three-tier microservice team structure\n \n\n\n338\nChapter 13  Building microservice teams\nOf the many changes that microservices bring, this may the most difficult to roll out: \nengineers are likely to resist being on-call, even for their own code. A successful on-call \nrotation should be\n¡ Inclusive  — Everyone who can do it, should do it, including VPs and directors.\n¡ Fair  — On-call work should be remunerated in addition to normal working \nhours.\n¡ Sustainable  — Enough engineers should be in a rotation to avoid burnout and \navoid disruption to work-life balance or day-to-work in the office.\n¡ Reflective  — Your team should constantly review alerts and pages to ensure only \nalerts that matter wake someone up.\nIn this model, we split alerts across teams, because running software at scale is complex. \nOperational effort might be beyond the scope or knowledge of engineers within any \none team. Many operational tasks — such as operating an Elasticsearch cluster, deploy-\ning a Kafka instance, or tuning a database — require specific expertise that would be \nunreasonable to expect product engineers to gain uniformly. Operational work also \nruns at a cadence different from the pace of product delivery.\nWARNING    Historically, infrastructure operations teams have been responsible \nfor running applications in production: keeping them stable and waking up \nwhen they break. This leads to tension: operations teams resent developers \nthrowing unstable applications over the wall, whereas developers curse the lack \nof engineering skills in the operations team. This separate dev and ops model \nputs the onus for fixing production issues on the wrong team. Instead, if devel-\nopers are responsible for how their code operates, they’ll be better able to fix \nincidents and optimize that code in the long term.\nThe right choice for an on-call model that balances responsibility and expertise will \ndepend on the types of applications you build, the throughput of those applications, \nand the underlying architecture you choose. If you’re interested in learning more, \nIncrement recently published an in-depth review (https://increment.com/on-call/ \nwho-owns-on-call/) of on-call approaches used at Google, PagerDuty, Airbnb, and \nother organizations.\n13.2.6\t Sharing knowledge\nAlthough autonomous teams increase development pace, they have two downsides:\n¡ Different teams may solve the same problem multiple times in different ways.\n¡ Team members will have less engagement with their specialist peers on other \nteams.\n¡ Team members may make local decisions without considering the global context \nor the needs of the wider organization.\n \n\n\n\t\n339\nTeam models\nYou can mitigate these issues. We’ve had success applying Spotify’s model of chapters \nand guilds.4 These are communities of practice:\n¡ Chapters group people by functional specialties, for example, mobile \ndevelopment.5\n¡ A guild shares practice around a cross-cutting theme, for example, performance, \nsecurity.\nFigure 13.13 depicts this model.\nComparably, some organizations use matrix management to establish a formal iden-\ntity for functional units. This adds a line of management responsibility (head of QA, \nhead of design…) for functions, at the cost of building a more complicated manage-\nment structure.\nTIP    Most engineers have been taught to follow the DRY tenet — don’t repeat \nyourself. Within a service, this is still important — there’s no point in writing \nthe same code twice! Across multiple services, this is much less of an impera-\ntive because writing shared code that’s truly reusable is a costly endeavor, as is \ncoordinating the rollout of that code across multiple consumers. A degree of \nduplication is acceptable if it means you can deliver features more rapidly.\nFE dev\nDev\nDesigner\nTest\nPayments\nFE dev\nFE dev\nDev\nTest\nGrowth\nFE dev\nDev\nDev\nInternational\nFrontend\nchapter\nPerformance\nguild\nFigure 13.13    The chapters, guilds, and teams model\n4\t See Henrik Kniberg, “Scaling Agile @ Spotify with Tribes, Squads, Chapters & Guilds,” Crisp’s Blog, \nNovember 14, 2012, http://mng.bz/94Lv.\n5\t In larger organizations, a chapter may group by functional specialty within an engineering divi-\nsion. (Spotify calls this a tribe.)\n \n\n\n340\nChapter 13  Building microservice teams\nEither approach works well to disseminate knowledge and develop shared working \npractices. This helps to prevent the isolation that can arise in highly autonomous \nteams, ensuring teams remain aligned technically and culturally. Cross-pollination \nof ideas, solutions, and techniques also supports people moving between teams and \nreduces organization-level bus factor risks.\nIt’s also important to strike a balance between team lifetime and team fluidity. In the \nlong run, regularly rotating engineers between teams helps to share knowledge and \nskills and is a good complement for the chapter and guild model.\n13.3\t Recommended practices for microservice teams\nThe scale of change in a microservice application can be tremendous. It can be diffi-\ncult to keep up! It’s unreasonable to expect any engineer to have a deep understand-\ning of all services and how they interact, especially because the topography of those \nservices may change without warning. Likewise, grouping people into independent \nteams can be detrimental to forming a global perspective. These factors lead to some \ninteresting cultural implications:\n¡ Engineers will design solutions that are locally optimal — good for them or their \nteam — but not always right for the wider engineering organization or company.\n¡ It’s possible to build around problems rather than fixing them, or to deploy new \nservices instead of correcting issues with existing services.\n¡ Practices on teams might become highly local, making it difficult for engineers \nto move between teams.\n¡ It’s challenging for architects or engineering leads to gain visibility and make \neffective decisions across the entire application.\nGood engineering practices can help you avoid these problems. In this section, we’ll \nwalk through some of the practices that your teams should follow when building and \nmaintaining services.\n13.3.1\t Drivers of change in microservices\nTake a moment and consider the type of build items you might work on day to day. \nIf you’re on a product team, the items in your backlog are primarily functional addi-\ntions or changes. You want to launch a new feature; support a new request from a \ncustomer; enter a new market; and so on. As such, you build and change microservices \nin response to these new functional requirements. And, thankfully, microservices are \nintended to ensure your application is flexible in the face of change.\nBut functional requirements — changes from your business domain — aren’t the only \ndriver of change in services. Each microservice will change for many reasons (figure 13.14):\n¡ Underlying frameworks and dependencies (such as Rails, Spring, or Django) \nmay require upgrades for performance, security, or new features.\n¡ The service may no longer be fit for the purpose — for example, hitting natural \nscalability limits — and may require change or replacement.\n¡ You discover defects in the service or the service’s dependencies.\n \n\n\n\t\n341\nRecommended practices for microservice teams\nNew requirements\nDefects\nDependency\nchanges\nScalability\nissues\nSecurity issues,\nfor example, CVEs\nFramework\nchanges\nService\nFigure 13.14    Drivers of change to a microservice\nAll this change increases complexity. For example, instead of tracking security vul-\nnerabilities against a single monolithic application, you need to ensure your tooling \nsupports static analysis and alerting across several applications (and likely several \ndistinct programming languages and frameworks). Every new service generates \nmore work.\nAlternatively, some microservice practitioners have advocated immutable services —  \nonce a service is considered mature, put it under feature freeze, and add new services \nif change is required. There’s a tricky cost-benefit decision here: is the risk of breaking \na service through modification more than the cost of building a new service? It’s a diffi-\ncult question to answer definitively and will depend on both your business context and \nappetite for risk.\n13.3.2\t The role of architecture\nMicroservice applications evolve over time: teams build new services; decommission \nexisting services; refactor existing functionality; and so on. The faster pace and more \nfluid environment that microservices enable change the role of architects and techni-\ncal leads.\nArchitects have an important role to play in guiding the scope and overall shape of \nan application. But they need to perform that role without becoming a bottleneck. A \nprescriptive and centralized approach to major technical decisions doesn’t always work \nwell in a microservice application:\n¡ The microservice approach and the team model we’ve outlined should empower \nlocal teams to make rapid, context-aware decisions without layers of approval.\n¡ The fluidity of a microservice environment means that any overarching technical \nplan or desired model of the intended system will quickly pass its use-by date, as \nrequirements change, services evolve, and the business itself matures.\n¡ The volume of decisions increases with the number of services, which can over-\nwhelm an architect and make them a bottleneck.\n \n\n\n342\nChapter 13  Building microservice teams\nThat doesn’t mean that architecture isn’t useful or necessary. An architect should have \na global perspective and make sure the global needs of the application are met, guid-\ning its evolution so that\n¡ The application is aligned to the wider strategic goals of the organization.\n¡ Technical choices within one team don’t conflict with choices in another.\n¡ Teams share a common set of technical values and expectations.\n¡ Cross-cutting concerns — such as observability, deployment, and interservice \ncommunication — meet the needs of multiple teams.\n¡ The whole application is flexible and malleable in the face of change.\nThe best starting point for architecture is to set principles. Principles are guidelines (or \nsometimes rules) that teams should follow to achieve higher level goals. They inform \nteam practice. Figure 13.15 illustrates this model.\nFor example, if your product goal is to sell to privacy- and security-sensitive enter-\nprises, you might set principles of compliance with recognized external standards, data \nportability, and clear tracking of personal information. If your goal is to enter a new \nmarket, you might mandate flexibility around regional requirements, design for multi-\nple cloud regions, and out-of-the-box support for i18n (figure 13.16).\nPrinciples are flexible. They can and should change to reflect the priorities of the \nbusiness and the technical evolution of your application. For example, early develop-\nment might prioritize validating product-market fit, whereas a more mature applica-\ntion might require a focus on performance and scalability.\nCompany and product goals\nAchieve\nTechnical principles\nInform\nTeam practices and decisions\nFigure 13.15    An architectural approach based on technical principles\n \n\n\n\t\n343\nRecommended practices for microservice teams\nGoal\nEnter a new market\nPrinciples\n1. Support flexible regional\nrequirements\n2. Design for multiple cloud regions\n3. Support i18n\nPractices\n1. Regional rules are codified\n2. Use multimaster storage (for\nexample, Cassandra); services are\ndesigned using 12-factor principles\n3. Choose UI frameworks with good\ni18n support\nAchieve\nInform\nFigure 13.16    Principles and practices to support entering a new market\nSeveral day-to-day practices support this evolutionary approach to architecture, such \nas design review, an inner-source model, and living documentation. We’ll discuss them \nover the next few sections.\n13.3.3\t Homogeneity versus technical flexibility\nA tricky decision you’ll face is which languages to use to write microservices. Although \nmicroservices provide for technical freedom, using a wide range of languages and \nframeworks can increase risk:\n¡ Bus factor and key person dependencies may increase because of limited shared \nknowledge, making it difficult to maintain and support services.\n¡ Services in new languages may not meet production readiness standards.\nIn practice, you’ll always encounter scenarios where you need to pick a different lan-\nguage, such as specialist features or performance needs. For example, Java would be \nill-suited to writing systems infrastructure, just as Ruby doesn’t have the depth of scien-\ntific and machine learning libraries available to Python. In these scenarios, it’s import-\nant to share the development of services in new languages/frameworks across many \nteam members to reduce bus factor risk: rotate team members, have a pair program, \nwrite documentation, and mentor new engineers.\nPicking a single primary language, or a small set, allows you to better optimize prac-\ntices and approach for that language. The creation of service templates, chassis, and/\nor exemplars will naturally ease development in your favored language, leading more \ndevelopers to write services using it. Lowering friction this way creates a virtuous cir-\ncle. Even if you don’t explicitly choose a favored language, this can happen organically \n(although it’ll take longer).\nTIP    Microservices should be replaceable. If needed, you should be able to \nrewrite any service in a more favorable programming language.\n13.3.4\t Open source model\nApplying open source principles to microservice code can help to alleviate contention \nand technical isolation while improving knowledge sharing. As we mentioned earlier, \neach team in a microservice organization typically owns multiple services. But each \nservice you run in production must have a clear owner: a team that takes long-term \nresponsibility for that service’s functionality, maintenance, and stability.\n \n\n\n344\nChapter 13  Building microservice teams\nThat doesn’t mean those people must be the only contributors to that service. Other \nteams might need to tweak functionality to meet their needs or fix defects. If these \nchanges all needed the same group of people to make them, those people would be at \nthe mercy of their own priorities, which in turn would slow other teams down.\nInstead, an inner-source model — open source within your organization — balances \nownership and visibility:\n¡ Source code should be available internally for any service.6\n¡ Any engineer can submit pull requests to any service, as long as the service owner \nreviews them.\nThis model (figure 13.17) closely resembles most open source projects, where a core \ngroup of committers make most commits and key decisions, and others can submit \nchanges for approval. Imagine an engineer on Team A needs to make a change to a \nservice that Team B owns. They could argue for the priority of their change against \neverything else on Team A’s backlog, or they could pull the code, make the change \nthemselves, and submit a pull request for Team B to review.\nThis approach has three benefits:\n¡ Alleviates contention and priority negotiation between teams\n¡ Reduces the sense of technical isolation and possessiveness that can develop \nwhen service work is limited to a small number of people within an organization\n¡ Shares knowledge within an organization by helping engineers understand other \nteams’ services and better understand the needs of their internal consumers\nTeam A\nSubmits\nSubmits\nReviews\nMerged\nOwns\nService\ncodebase\nPull\nrequests\nTeam N\nTeam B\nFigure 13.17    Applying an open source model to service development\n6\t In some organizations, reasonable exceptions to this rule may apply, such as when code is highly \nsensitive.\n \n\n\n\t\n345\nRecommended practices for microservice teams\nNOTE    Contributing across multiple services is significantly easier when those \nservices follow common architectural and deployment conventions — like the \nones we’ve discussed throughout this book!\n13.3.5\t Design review\nEach new microservice is a blank slate. Each service will have different performance \ncharacteristics; might be written in a different language; might require new infrastruc-\nture; and so on. A new feature might be possible to write in several ways: as a new ser-\nvice, as many services, or within an existing service. This freedom is terrific, but a lack \nof oversight can result in\n¡ Inconsistency  — For example, a service might not log requests consistently, ham-\npering common operational tasks, such as investigating defects.\n¡ Suboptimal design decisions  — You might build multiple services, when a single ser-\nvice would be more maintainable and perform better.\nA few methods can help you get around this issue. In chapter 7, we discussed using \nservice chassis and service exemplars as best practice starting points. But that’s only a \npartial solution.\nIn our own company — comparable to practices at Uber and Criteo — we follow a \ndesign review process. For any new service or substantial new feature, the engineer \nresponsible produces a design document (we call this an RFC, or request for com-\nments) and asks for feedback from a group of reviewers, both in and outside of their \nown team. Table 13.1 outlines the sections in a typical design review document.\nTable 13.1    Sections in a design review document for a new microservice\nSection\nPurpose\nProblem & Context\nWhat technical and/or business problem does this feature solve? Why are \nwe doing this?\nSolution\nHow are you intending to solve this problem?\nDependencies & Integration\nHow does it interact with existing or planned services/functionality/\ncomponents?\nInterfaces\nWhat operations might this service expose?\nScale & Performance\nHow does the feature scale? What are the rough operational costs?\nReliability\nWhat level of reliability are you aiming for?\nRedundancy\nBackups, restores, deployment, fallbacks\nMonitoring & Instrumentation\nHow will you understand this service’s behavior?\nFailure Scenarios\nHow will you mitigate the impact of possible failures?\nSecurity\nThreat model, protection of data, and so on\nRollout\nHow will you launch this feature?\nRisks & Open Questions\nWhat risks have you identified? What don’t you know?\n \n\n\n346\nChapter 13  Building microservice teams\nThis process catches suboptimal design decisions early in the development cycle. \nAlthough writing a document may seem like extra effort, having a semiformal prompt \nto consider service design tends to result in faster overall development, as the team \nbrings to light the full range of considerations and tradeoffs before committing to an \nimplementation direction.\n13.3.6\t Living documentation\nAs we’ve mentioned, it’s difficult to keep a microservice architecture in your head. The \nscale of a microservice application demands that your team invest time in documenta-\ntion. For each service, we recommend a four-layered approach: overviews, contracts, \nrunbooks, and metadata. Table 13.2 details these four layers.\nTable 13.2    Recommended minimum layers for documenting microservices\nType\nSummary\nOverview\nAn overview of the service’s purpose, intended usage and overall architecture. Service over-\nviews should be an entry point for team members and service users.\nContract\nA service contract should describe the API that a service provides. Depending on transport \nmechanism, this can be machine-readable, for example, using Swagger (HTTP APIs) or proto-\ncol buffers (gRPC).\nRunbooks\nDocumented runbooks for production support detailing common operational and failure \nscenarios\nMetadata\nFacts about a service’s technical implementation, such as the programming language, major \nframework versions, links to supporting tools, and deployment URLs\nThis documentation should be discoverable in a registry  — a single website where \ndetails for all services are available. Good microservice documentation serves many \npurposes:\n¡ Developers can discover the capabilities of existing services, such as the contracts they \nexpose. This speeds up development and may reduce wasted or duplicated work.\n¡ On-call staff can use runbooks and service overviews to diagnose issues in pro-\nduction, as different services will vary operationally.\n¡ Teams can use metadata to track service infrastructure and answer questions, for \nexample, “How many services are running Ruby 2.2?”\nMany tools exist for writing project documentation, such as MkDocs (www.mkdocs.\norg). You could combine them with service metadata approaches, as described in table \n13.2, to build a microservice registry.\nTIP    Documentation is notoriously hard to keep up to date, even for a single \napplication. As much as feasible, you should aim to autogenerate documenta-\ntion from application state. For example, you can generate contract documen-\ntation from Swagger YML files using the swagger-ui library.\n \n\n\n\t\n347\nFurther reading\n13.3.7\t Answering questions about your application\nAs a service owner or an architect, you’ll often want to get an overarching view of the \nstate of your application to answer questions like\n¡ How many services are written in each language?\n¡ Which services have security vulnerabilities or outdated dependencies?\n¡ What upstream and downstream collaborators use Service A?\n¡ Which services are production-critical? Which are spikes and experiments, or \nless important to critical application paths?\nAt the time of this writing, few tools exist in the wild that combine this information \nto make it readily available. When it’s available, it’s typically spread across multiple \nlocations:\n¡ Language and framework choices require code analysis or repository tagging.\n¡ Dependency management tools (for example, Dependabot) scan for outdated \nlibraries.\n¡ Continuous integration jobs run arbitrary static analysis tasks.\n¡ Network metrics and code instrumentation surface relationships between \nservices.\nSimilar information might be kept in spreadsheets or architectural diagrams, which, \nsadly, are often out of date.\nA recent presentation from John Arthorne at Shopify7 proposed embedding a file, \nservice.yml, in each code repository and using that as a source of service metadata. This \nis a promising idea, but at the time of this writing, you’ll need to roll your own.\n13.4\t Further reading\nForming, growing, and improving engineering teams is a broad topic, and in this chap-\nter we’ve only scratched the surface. If you’re interested in learning more, we recom-\nmend the following books as good places to start:\n¡ Elastic Leadership, by Roy Osherove (ISBN 9781617293085)\n¡ Managing Humans, by Michael Lopp (ISBN 9781430243144)\n¡ Managing the Unmanageable, by Mickey W. Mantle and Ron Lichty (ISBN \n9780321822031)\n¡ PeopleWare, by Tom DeMarco and Timothy Lister (ISBN 9780932633439)\nWe’ve covered a lot of ground in this chapter. Choosing a microservice engineering \napproach is great for getting things done and empowering engineers, but changing \nyour technical foundation is only half the battle. Any system is deeply intertwined with \n7\t See John Arthorne, “Tracking Service Infrastructure at Scale,” SRECon San Francisco, March 13, \n2017, http://mng.bz/Z6d0.\n \n\n\n348\nChapter 13  Building microservice teams\nthe people building it — successful, sustainable development requires close collabora-\ntion, communication, and rigorous and responsible engineering practices.\nIn the end, people deliver software. Getting the best product out requires getting the \nbest out of your team.\nSummary\n¡ Building great software is as much about effective communication, alignment, \nand collaboration as implementation choices.\n¡ Application architecture and team structure have a symbiotic relationship. You \ncan use the latter to change the former.\n¡ If you want teams to be effective, you should organize them to maximize auton-\nomy, ownership, and end-to-end responsibility.\n¡ Cross-functional teams are faster and more efficient at delivering microservices \nthan a traditional, functional approach.\n¡ A larger engineering organization should develop a tiered model of infrastruc-\nture, platform, and product teams. Teams in lower tiers enable higher tier teams \nto work more effectively.\n¡ Communities of practice, such as guilds and chapters, can share functional \nknowledge.\n¡ A microservice application is difficult to fit in your head, which leads to chal-\nlenges for global decision making and on-call engineers.\n¡ Architects should guide and shape the evolution of an application, not dictate \ndirection and outcomes.\n¡ Inner-source models improve cross-team collaboration, weaken feelings of pos-\nsessiveness, and reduce bus factor risks.\n¡ Design reviews improve the quality, accessibility, and consistency of microservices.\n¡ Microservice documentation should include overviews, runbooks, metadata, \nand service contracts.\n \n\n\n349\nappendix \nInstalling Jenkins on Minikube\nThis appendix covers\n¡ Running Jenkins on Minikube\n¡ A short introduction to Helm\nThis appendix will walk you through the process of running Jenkins on your local \nMinikube cluster, which we use in the examples in chapter 10.\nRunning Jenkins on Kubernetes\nYou can run Jenkins as another service on the local Kubernetes cluster — Minikube —  \nyou set up in chapter 9. If you’re working from scratch, follow the installation \ninstructions on GitHub to get Minikube running (https://github.com/kubernetes/\nminikube). Once you have it installed, bring up the cluster by running minikube \nstart at your terminal.\nThe Jenkins application consists of a master node and, optionally, any number \nof agent nodes. Running a Jenkins job executes scripts (such as make) across agent \nnodes to perform deployment activities. A job operates within a workspace  — a local \ncopy of your code repository. Figure A.1 illustrates this architecture.\nYou’ll use Helm to install an “official” Kubernetes-ready configuration of Jenkins \non your Minikube cluster.\n \n\n\n350\nAppendix  Installing Jenkins on Minikube\nJenkins master\nJenkins agent\nWorkspace\nJob\nOrchestrates\nPulls \nfrom\nCode repo\nResources,\nfor example,\ninfrastructure\nInteracts with\nFigure A.1  A high-level Jenkins architecture\nSetting up Helm\nYou can think of Helm (https://helm.sh/) as a package manager for Kubernetes. \nHelm’s package format is a chart, which defines a set of Kubernetes object templates. \nCommunity-developed charts, like the one you’ll use for Jenkins, are stored on Github \n(https://github.com/helm/charts).\nHelm consists of two components:\n¡ A client, which you’ll use to interact with Helm charts\n¡ A server-side application (also known as Tiller), which performs installation of charts\nThis is illustrated in figure A.2.\nInstallation instructions for Helm are on Github (https://github.com/helm/helm). \nFollow them to get the Helm client running on your machine. Once you’ve installed \nHelm, you’ll need to set up Tiller on Minikube. Run helm init on the command line to \nset up this component.\nCreate a namespace and a volume\nBefore you install the Jenkins chart, you need to create two things:\n¡ A new namespace to logically segregate your Jenkins objects within the cluster\n¡ A persistent volume to store Jenkins configuration, even if you restart Minikube\nEngineer\nHelm client\nHelm install\nTiller\nKubernetes cluster\nDeploys\nApplication\nFigure A.2  Components of Helm, a package manager for Kubernetes\n \n",
      "page_number": 351
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 366-373)",
      "start_page": 366,
      "end_page": 373,
      "detection_method": "topic_boundary",
      "content": "\t\n351\nRunning Jenkins on Kubernetes\nTIP    You can find all the templates we use in this chapter in the book’s code \nrepo on Github: https://github.com/morganjbruce/microservices-in-action.\nTo create the namespace, apply the following template to your Minikube cluster, using \nkubectl apply -f <file_name>.\nListing A.1  jenkins-namespace.yml\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: jenkins\nAnd do the same again for the persistent volume, as follows.\nListing A.2  jenkins-volume.yml\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: jenkins-volume\n  namespace: jenkins\nspec:\n  storageClassName: jenkins-volume\n  accessModes:\n    - ReadWriteOnce\n  capacity:\n    storage: 10Gi\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /data/jenkins/\nInstalling Jenkins\nYou’ll install Jenkins with the community Helm chart. This chart is pretty complex; \nif you’re interested, you can explore it on Github: https://github.com/helm/charts/\ntree/master/stable/jenkins.\nFirst, create a values.yml file. Helm will interpolate the following code into the Jen-\nkins chart to set appropriate defaults for running on Minikube.\nListing A.3  values.yml\nMaster:\n  ServicePort: 8080\n  ServiceType: NodePort\n  NodePort: 32123\n  ScriptApproval:\n \n\n\n352\nAppendix  Installing Jenkins on Minikube\n    - \"method groovy.json.JsonSlurperClassic parseText java.lang.String\"\n    - \"new groovy.json.JsonSlurperClassic\"\n    - \"staticMethod org.codehaus.groovy.runtime.DefaultGroovyMethods \nleftShift java.util.Map java.util.Map\"\n    - \"staticMethod org.codehaus.groovy.runtime.DefaultGroovyMethods split \njava.lang.String\"\n  InstallPlugins:\n    - kubernetes:1.7.1 \n    - workflow-aggregator:2.5 \n    - workflow-job:2.21 \n    - credentials-binding:1.16 \n    - git:3.9.1 \nAgent:\n  volumes:\n    - type: HostPath\n      hostPath: /var/run/docker.sock\n      mountPath: /var/run/docker.sock\nPersistence:\n  Enabled: true\n  StorageClass: jenkins-volume \n  Size: 10Gi\nNetworkPolicy:\n  Enabled: false\n  ApiVersion: extensions/v1beta1\nrbac:\n  install: true\n  serviceAccountName: default\n  apiVersion: v1beta1\n  roleRef: cluster-admin\nNow, to install Jenkins, run the following helm command:\nhelm install\n  --name jenkins \n  --namespace jenkins\n  --values values.yml \n  stable/jenkins \nIf successful, this will output a list of created resources that looks like figure A.3.\nGive Jenkins a few minutes to start up. To access the server, you’ll need a password. \nYou can retrieve it using the following command:\nprintf $(kubectl get secret --namespace jenkins jenkins -o jsonpath=\"{.data.\njenkins-admin-password}\" | base64 --decode);echo\nThis default set of plugins will support \nrunning Jenkins Pipeline jobs.\nThe persistence settings refer  \nto your persistent volume.\nThe chart to install\n \n\n\n\t\n353\nRunning Jenkins on Kubernetes\nThen, navigate to the login page:\nminikube --namespace=jenkins service jenkins\nLog in with the username “admin” and the password you retrieved. Terrific — you’ve \nset up Jenkins!\nConfiguring RBAC\nMinikube uses RBAC — role-based access control — by default, which requires an addi-\ntional configuration step to ensure Jenkins can perform operations on the Kubernetes \ncluster.\nFigure A.3  Kubernetes objects that the stable/Jenkins Helm chart installed.\n \n\n\n354\nAppendix  Installing Jenkins on Minikube\nTo configure this appropriately on the Jenkins server:\n1\t Log in to the Jenkins dashboard.\n2\t Navigate to Credentials > System > Global Credentials > Add Credentials.\n3\t Add a Kubernetes Service Account credential, setting the value of the ID field to \njenkins.\n4\t Save and navigate to Jenkins > Manage Jenkins > System.\n5\t Under the Kubernetes section, configure the credentials to those you created in \nstep 3 (figure A.4) and click Save.\nFigure A.4  Kubernetes cloud credentials\nTesting it all works\nYou can run a simple build to make sure everything’s working. First, log in to your new \nJenkins dashboard and navigate to New Item in the left-hand column.\nCreate a new pipeline job named “test-job” per the configuration in figure A.5. Click \nOK to move to the next page and configure that job with the following script in the Pipe-\nline Script field.\nListing A.4  Test pipeline script\npodTemplate(label: 'build', containers: [\n    containerTemplate(name: 'docker', image: 'docker', command: 'cat', \nttyEnabled: true)\n  ],\n  volumes: [\n    hostPathVolume(mountPath: '/var/run/docker.sock', hostPath: '/var/run/\ndocker.sock'),\n \n\n\n\t\n355\nRunning Jenkins on Kubernetes\n  ]\n  ) {\n    node('build') {\n      container('docker') {\n        sh 'docker version'\n      }        \n    }  \n  }\nClick Save, then, on the following page, click Build Now. This will execute your job.\nTIP    The first build run may take some time!\nThe script you added will\n1\t Create a new pod, containing a Docker container\n2\t Execute the docker version command inside that container and output the \nresults to the console\nFigure A.5  New job page on Jenkins\n \n\n\n356\nAppendix  Installing Jenkins on Minikube\nOnce the build job has completed, navigate to the build’s console output \n(http://<insert Jenkins ip here>/job/test/1/console). You should see output similar \nto figure A.6, showing the output of the job script commands.\nIf your output looks like the figure, fantastic — everything’s in working order! If not, \nyour first point of call to diagnose any issues should be the Jenkins logs: http://<insert \nJenkins ip here>/log/all.\nFigure A.6  Console output from the test build job\n \n\n\n357\nindex\nSymbols\n2PC (two-phase commit) \nprotocol  107\nA\nabstraction  66\nacbuild command-line tool  218\nacceptance testing  257\naccidental complexity  12\nACID transaction  106\nactions  92–94\nactivity feeds  126\nAdvanced Message Queuing \nProtocol (AMQP)  63, 162\naggregating data  59\naggregation  59–60\nalerts\nraising  291–293\nsymptoms versus  \ncauses  292–293\nwho needs to know  292\nsetting up  287–291\nalignment  9–10\nambiguity  99–103\nmigration  100–103\npreparing for \ndecomposition  100\nretirement  100–103\nstarting with coarse-grained \nservices  99–100\nAMQP (Advanced Message \nQueuing Protocol)  63, 162\nanalytics  127\nanemic services  30\nAPI gateways  39, 68–69\nApollo  71\napplication boundaries  66–71\nAPI gateways  68–69\nBFF (backends for \nfrontends)  69–70\nconsumer-driven gateways  70–71\napplications\ndistributed  106–108\nobserving  293–295\narchitecture\nof CQRS pattern  124–125\nof microservices  52–56\narchitectural principles  54\nfour tiers of microservice \napplications  55–56\nrole of architect  54\nrole of  341–343\nartifacts  199–207\nconfiguring  206–207\ndefining  200\nimmutability of  201\npublishing  254–255\nstandardizing  22–23\ntypes of  202–206\ncontainers  205–206\noperating system \npackages  203\nserver images  203–204\nasset information  81\nasynchronous communication \npatterns  63–65\njob queues  63–64\noverview of  149\npublish-subscribe  64–65\nasynchronous messages  62–63\nautohealing  190\nautomated deployments  42–43\nautomated tests  257\nautomation\nof production environments  191\noverview of  9, 189\nautonomy\nlack of  332\noverview of  8, 329\nautoscaling  190, 194\navailability  189\nautomation  189\ncalculating  130\nmanual change \nmanagement  189\nprioritizing  123\nsmall releases  189\nB\nbackends for frontends. See BFF\nbalancing  177–180\nBDD (behavior-driven \ndevelopment)  78\nBEAM (Erlang virtual \nmachine)  183\nBFF (backends for frontends)  69–70\n \n\n\n358\nindex\nbinary dependencies  200\nblue-green deploys pattern  210\nbootstrapping  164–165\nbottlenecks  154\nboundaries. See application \nboundaries\nboundary layer  66\nbounded contexts  67, 79\nbuild pipelines\nconfiguring  247–251\nprocedural versus \ndeclarative  263–264\nbus factor  41\nbusiness capabilities, scoping \nby  78–87\ncapabilities  78–79\nchallenges and limitations  87\ncreating investment \nstrategies  79–86\ndomain modeling  78–79\nnested contexts and services  86\nbusybox  241\nC\ncachetools library  144\ncaching  67, 143–144\ncanaries\non GCE  211–213\noverview of  236–240\ncapacity  196\nCAP theorem  108, 123\ncascading failures  136–139\ncategories  301\nchaos testing  152, 154–155\nChaos Toolkit  155\nChargeFailed event  110\nchassis  160–163\ndesigning  165–180\nbalancing  177–180\nlimiting  177–180\nobservability  171–177\nservice discovery  167–171\nexploring features  180–182\npurpose of  163–165\nfaster bootstrapping  164–165\nreduced risk  164\nchoreographed interactions  110\nchoreographed sagas  112–115\nchoreography  37–39, 94, 109–110\ncircuit breakers  146–148\nClair  220\nclients  71–74\nfrontend monoliths  71\nmicro-frontends  72–74\nclusters, deploying to  224–242\ncomponents of master \nnodes  230\ncomponents of worker \nnodes  231\nconnecting multiple \nservices  241–242\ndeploying new versions  235–240\ndesigning pods  226–228\nhealth checks  233–235\nload balancing  228–230\noverview of  230–233\nrolling back  241\nrunning pods  228–232, 226–233\nstate changes  231\ncoarse-grained services\noverview of  87\nstarting with  99–100\ncodebases  329\ncode deployment  259\ncoherence  335\ncollaboration. See service \ncollaboration\ncollaborators  37\ncommand-query responsibility \nsegregation pattern. \nSee CQRS pattern\ncommands, separating queries \nfrom  123–125\ncommit phase, 2PC  108\ncommunication  60–65\nas sources of failures  135\nasynchronous \ncommunication  149\nasynchronous communication \npatterns  63–65\njob queues  63–64\npublish-subscribe  64–65\nasynchronous messages  62–63\nby group size  327\ndesigning  139–149\ncircuit breakers  146–148\nfallbacks  143–145\nretries  140–143\ntimeouts  145–146\nevent-based \ncommunication  108–110\nchoreography  109–110\nevents  109–110\nlocating services  65\noverview of  21\nstandardizing  156\nsynchronous messages\nchoosing transport  62\ndrawbacks of  62\nwhen to use  61–62\nusing service mesh  157\ncommunication broker  63\ncompensating actions  113\ncomponents\nadding to Docker compose \nfiles  276\nof master nodes  230\nof worker nodes  231\ncompose files  276\nconfiguration drift  202\nconfiguring\nbuild pipelines  247–251\nGrafana  280–282\nPrometheus  280\nservice artifacts  206–207\nStatsD exporter  277–279\nConnectionError exception  179\nconsistency patterns  118–119\nconstraints  103\nConsul  65\nconsumer-driven gateways  70–71\ncontainerizing services  215–224\nbuilding images  218–220\nrunning containers  220–222\nstoring images  223–224\nworking with images  216–218\ncontainers\noverview of  23, 205–206\nrunning  220–222\n \n",
      "page_number": 366
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 374-383)",
      "start_page": 374,
      "end_page": 383,
      "detection_method": "topic_boundary",
      "content": "\t\n359\nindex\ncontainer schedulers  210\ncontexts, nested  86\ncontinuous delivery  245\ncontinuous deployment  245\ncontracts\nbetween services  16\noverview of  37\ncontrol  103\nConway's Law  327\ncoordinators  59\ncorrelation IDs  121\nCQRS (command-query \nresponsibility segregation) \npattern\narchitecture of  124–125\nchallenges  125–127\noptimistic updates  126\noverview of  63\npolling  127\npublish-subscribe  127\ncritical paths  60\ncross-functionally  330\nCRUD (create, read, update, \ndelete)  30, 108\ncyclic dependencies  114\nD\nDAG (direct acyclic graph)  314\ndark launches  264\ndata\naggregating  59\nstoring copies of  122–123\nstubbed  145\ndata composition  121\ndata warehouses  127\nDDD (domain-driven design)  79\ndeadlines  146\ndecomposition\npreparing for  100\nscaling through  6–7\ndegradation  143\ndelivery pipelines, building  242, \n264–266\ndark launches  264\ndeployment pipeline  244–246\nfeature flags  23–24, 265–266\ndependencies  135–136\ndeploying\nmicroservices  21–24\nimplementing continuous \ndelivery pipelines  23–24\nstandardizing microservices \ndeployment artifacts   \n22–23\nservices  191–199, 187–213\nadding load balancers   \n196–198\nimportance of  188–189\nprovisioning virtual \nmachines  192–193\nrun multiple instances of \nservice  194–196\nservice startup  192\nto clusters  224–242\ncomponents of master \nnodes  230\ncomponents of worker \nnodes  231\nconnecting multiple \nservices  241–242\ndeploying new versions   \n235–240\ndesigning pods  226–228\nhealth checks  233–235\nload balancing  228–230\noverview of  230–233\nrolling back  241\nrunning pods  228–232, \n226–233\nstate changes  231\nto production  259–261\nto staging  255–257\nto underlying hosts  207–210\nmultiple scheduled services \nper host  209–210\nmultiple static services per \nhost  208–209\nsingle service to host  207\nwithout downtime  210–213\ncanaries on GCE  211–213\nrolling deploys on GCE   \n211–213\ndeployment pipelines  57, 190, \n244–246\ndeployment process  8, 43\ndeployments  235–236\nautomated  42–43\nquality-controlled  42–43\ndeployment target  190\ndescribed feature  166\ndesign reviews  345–346\ndevelopment process  43\ndirect acyclic graph (DAG)  314\ndistributed applications  106–108\ndistributed computing  14, 130\ndistributed systems  16–17\ndivergence. See technical \ndivergence\ndocker-compose, installing  140\ndocker-compose up command   \n180, 281\nDocker platform  276\ndocker version command  355\ndocumentation  346\ndomain-driven design (DDD)  79\ndomain modeling  78–79\ndomains. See modeling domains\ndomain-specific language \n(DSL)  217, 247\ndowntime  130\nDRY (don't repeat yourself)  104, \n183, 339\nDSL (domain-specific \nlanguage)  217, 247\nE\nedge capabilities  67\nedge side includes (ESI)  73\nelastic load balancer (ELB)  65\nElasticsearch engine  305\nElasticsearch, Logstash, and \nKibana. See ELK-based \nsolutions\nELB (elastic load balancer)  65\nELK (Elasticsearch, Logstash, and \nKibana)-based solutions   \n304–306\nElasticsearch  305\nKibana  305\nLogstash  305\nend-to-end responsibility  71, 329\nengineering culture  26–27\nenterprise service buses (ESBs)  6\n \n\n\n360\nindex\nenvironments, staging  258. See \nalso production \nenvironments\nErlang virtual machine (BEAM)  183\nERROR level  301\nerror reporting  174–175\nerrors  273\nESBs (enterprise service buses)  6\nESI (edge side includes)  73\nevent backbone  63\nevent-based communication   \n108–110\nchoreography  109–110\nevents  109–110\nevents  109–110\nevent sourcing  119–120\neventual consistency  108\nexpectations, setting  104\nexplicit interfaces  104\nexponential back-off strategy  142\nF\nfailures\ncascading  136–139\nsources of  133–136\ncommunication  135\ndependencies  135–136\nhardware  134\nservice practices  136\nfailure zones  195\nfallbacks  143–145\ncaching  143–144\nfunctional redundancy  144\ngraceful degradation  143\nstubbed data  145\nfault tolerance\noverview of  42\nvalidating  152–155\nchaos testing  154–155\nload testing  153–154\nfeature flags  265–266\nfeatures\nbuilding  32–39\nidentifying microservices by \nmodeling domains  33–35\nservice choreography  37–39\nservice collaboration  36–37\ndesigning  75–104\ntaking to production  40–45\nautomated deployments   \n42–43\nquality-controlled \ndeployments  42–43\nresilience  43\ntransparency  43–45\nfeedback loops  139\nflags  265–266\nflat services  59\nFlipper  265\nFluentd-based solutions  306\nFluentd daemon  304, 307\nframeworks\nbuilding reusable  159–183\noverview of  156\nfriction  13, 258\nfrontends\nBFF (backends for frontends)   \n69–70\nfrontend monoliths  71\nmicro-frontends  72–74\nfull staging environments  258\nfunctional redundancy  144\nfunctions\ngrouping teams across  332–333\ngrouping teams by  330–332\nlack of autonomy  332\nno long-term responsibility  332\nrisk of silos  332\nunclear ownership  331\nG\ngateways\nAPI gateways  39, 68–69\nconsumer-driven gateways  70–71\ngauges  273\nGCE (Google Kubernetes Engine)\ncanaries on  211–213\nrolling deploys on  211–213\nGCP (Google Cloud Platform)  191\nGDPR (General Data Protection \nRegulation)  313\nGKE (Google Kubernetes Engine). \nSee GCE\nGoogle Cloud Platform (GCP)  191\nGoogle Compute Engine (GCE)  192\nGoogle Kubernetes Engine \n(GKE)  225\ngraceful degradation  143\nGrafana tool\nconfiguring  280–282\nmonitoring systems with   \n275–291\nRabbitMQ  282–285\nsetting up alerts  287–291\nsetting up metric collection \ninfrastructure  276–283\nGraphQL  70\nGunicorn web server  226\nH\nhalf open circuit breaker  148\nHAProxy  197\nhardware as source of failure  134\nhealth checks  135, 150, 233–235\nHelm application package \nmanager  350\nheterogeneity\noverview of  182–183\ntechnical heterogeneity  12\nhigh cohesion  4\nhigher order services  59–60\nhistograms  274\nholdings  130, 136\nhomogeneity  343\nhorizontal decomposition  9\nhosts\nmultiple scheduled services \nper  209–210\nadvantages of scheduling \nmodel  210\ncontainer schedulers  210\nmultiple static services  \nper  208–209\nsingle service to  207\nunderlying, deploying services \nto  207–210\nHTTP liveness check  151\n \n\n\n\t\n361\nindex\nI\nIaaS (infrastructure as a \nservice)  191\nidempotent  140\nidentifiers  301\nimages\nbuilding  218–220, 251–252\noverview of  216–218\nstoring  223–224\nimmutability  201, 341\ninfrastructure as a service \n(IaaS)  191\ninfrastructures  9, 191, 335–337\ninner-source model  344\ninstalling Jenkins, on \nMinikube  349–355\ninstance group  194\ninteractions, choreographed  110\ninterfaces  104\ninternal load balancing  198\ninterruption  118\ninterwoven sagas  117–118\ninterruption  118\nlocking  118\nshort-circuiting  118\ninvestment strategies\ncreating  79–86\nplacing orders  88–92\nInvestmentStrategies service  96\nisolated staging environments  258\nisolation  46\nJ\nJaeger  315\nJava virtual machine ( JVM)  183\nJenkins Pipeline  263\nJenkins tool\nbuilding pipelines with  246–261\nbuilding images  251–252\nconfiguring build \npipelines  247–251\ndeploying to production   \n259–261\ndeploying to staging  255–257\npublishing artifacts  254–255\nrunning tests  252–254\nstaging environments  258\ninstalling on Minikube  349–355\nrunning on Kubernetes  349–355\nconfiguring RBAC  353–354\ncreating namespaces  350–351\ncreating volume  350–351\nsetting up Helm  350\ntesting  354–356\njitter  142, 148\njob queues  63–64\nJVM ( Java virtual machine)  183\nK\nKibana plugin  305\nKPIs (key performance \nindicators)  330\nksonnet  256\nkubectl command-line tool  227\nkube-proxy  231\nKubernetes platform, running \nJenkins on  349–355.  \nSee also GCE\nconfiguring RBAC  353–354\ncreating namespaces  350–351\ncreating volume  350–351\ninstalling Jenkins  351–352\nsetting up Helm  350\ntesting  354–356\nL\nlatency  272–273\nlaunches  264\nlevels  301\nlimiting  177–180\nLinkerd  157\nliveness health check  233\nliving documentation  346\nload balancers  56, 150–151, \n196–198, 228–230\nload/capacity tests  257\nload testing  152–154\nlocking  118\nlogging  176–177\ngenerating logs  300–303\nreadability  301–303\nstructure  301–303\nuseful information to include \nin log entries  300–301\ninformation  313\nsearching  311–313\nsetting up infrastructure   \n303–313\nconfigure which logs to \ncollect  308–311\nELK based solutions  304–306\nFluentd-based solutions  306\nsetting up solutions  306–308\nlogin screen, Grafana  282\nlogs  67, 305\nlogstash-formatter library  176, 313\nLogstash tool  302, 305\nloose coupling  4, 8\nM\nmanaged registries  223\nmanual tests  257\nmapping runtime platforms  57\nmarket-data  130, 136\nMarketDataClient class  140\nmaster nodes  230\nmessages. See also asynchronous \nmessages\nmetric collection infrastructure   \n276–283\nadding components to Docker \ncompose file  276\nconfiguring Prometheus  280\nconfiguring StatsD exporter   \n277–279\nsetting up Grafana  280–282\nmetrics\noverview of  67, 172–174\ntypes of  273–274\ngauges  273\nhistograms  274\nmicro-frontends  72–74\nmicroservice platforms  56–57\nmicroservices. See also services\nadvantages of  11–13, 30–32\ndelivering sustainable \nvalue  31–32\nreducing friction  31–32\nreducing friction and risk  13\nrisk and inertia in financial \nsoftware  31\ntechnical heterogeneity  12\n \n\n\n362\nindex\narchitecture of  52–56\narchitectural principles  54\nfour tiers of microservice \napplications  55–56\nrole of architect  54\nchallenges of  14–18\ndesign challenges  14–17\noperational challenges  17–18\ndeploying  21–24\nimplementing continuous \ndelivery pipelines  23–24\nstandardizing microservices \ndeployment artifacts  22–23\ndesigning  19–21\ncommunication  21\nmonoliths  19–20\nresiliency  21\nscoping services  20\ndevelopment lifecycle of  18–25\nidentifying by modeling \ndomains  33–35\nkey principles of  7–10\nalignment  9–10\nautomation  9\nautonomy  8\nresilience  9\ntransparency  9\nobserving  24–25\nbehaviors across hundreds of \nservices  25\nidentifying and refactoring \nfragile implementations   \n24–25\nscaling through \ndecomposition  6–7\nscaling up development \nof  45–46\nisolation  46\ntechnical divergence  45–46\nusers of  10–11\nmicroservice scoping  52\nmicroservice teams  325–348\nbuilding  326–329\nConway's Law  327\nprinciples for  328–330\nautonomy  329\nend-to-end responsibility  329\nownership  328–329\nrecommended practices \nfor  340–347\nanswering questions about \napplications  347\ndesign reviews  345–346\ndrivers of change in \nmicroservices  340–341\nhomogeneity versus technical \nflexibility  343\nliving documentation  346\nopen source models  343–344\nrole of architecture  341–343\nteam models  330–340\ngrouping across functions   \n332–333\ngrouping by function  330–332\ninfrastructures  335–337\non-call support  337–338\nplatforms  335–337\nproduct  335–337\nsetting team boundaries   \n334–336\nsharing knowledge  338–340\nmigration  100–103\nMinikube tool, installing Jenkins \non  349–355\nMkDocs  346\nmodeling domains  33–35\nmodel-view controller (MVC)  19\nmodes of failure  14\nmonitoring systems  269–295\nobserving whole application   \n293–295\nraising alerts  291–293\nsymptoms versus causes   \n292–293\nwho needs to know  292\nrobust monitoring stacks   \n270–275\ngolden signals  272–273\nlayered monitoring  270–272\nrecommended practices   \n274–275\nwith Grafana  275–291\nRabbitMQ  282–285\nsetting up alerts  287–291\nsetting up metric collection \ninfrastructure  276–283\nwith Prometheus  275–291\nRabbitMQ  282–285\nsetting up alerts  287–291\nsetting up metric collection \ninfrastructure  276–283\nmonoliths  19–20, 71, 106, 111\nmultispeed development  103\nMVC (model-view controller)  19\nN\nnameko framework  166\nnamespaces  350–351\nnested contexts  86\nnetwork communication  61\nnginx.conf file  222\nNGINX container  222\nNodePort service  230\nnonbackwards-compatible \nfunctionality  135\nnoncritical paths  60\nnonfunctional testing  257\nnotifications, sending  96–98\nnslookup  241\nO\nobject-oriented applications  93\nobject-relational mapping \n(ORM)  161\nobservability  18, 171–177\nerror reporting  174–175\nlogging  176–177\nmetrics  172–174\nobservable services  43\non-call support  337–338\nOpenAPI specification  82\nopen source models  343–344\nOpenTracing API  314\noperating system packages  203\noptimistic updates  126\norchestrated sagas  115–117\norchestration  94\nOrderCreated event  64, 109\n \n\n\n\t\n363\nindex\nOrderPlaced event  39\nOrderRequested event  109\norganizations, ownership  \nin  103–104\nORM (object-relational \nmapping)  161\nownership  103–104, 329–331\nP\nPaaS (platform as a service)  57, 209\npaths\ncritical  60\nnoncritical  60\npatterns, consistency patterns   \n118–119. See also CQRS \npattern\npipelines. See also delivery pipelines\nbuilding reusable  262–264\nbuilding with Jenkins  246–261\nbuilding images  251–252\nconfiguring build pipelines   \n247–251\ndeploying to production   \n259–261\ndeploying to staging  255–257\npublishing artifacts  254–255\nrunning tests  252–254\nstaging environments  258\noverview of  57, 246\nprocedural versus declarative \nbuild pipelines  263–264\npip tool  219\nPlaceStrategyOrders service  88, 91\nplatform as a service (PaaS)  57, 209\nplatforms, mapping runtime \nplatforms  57. See also  \nmicroservice platforms\npods\ndesigning  226–228\nrunning  228–232, 226–233\npoint-to-point communication  36\npolling  127\npositive feedback  136\nprepare phase, 2PC  108\nprinciples  342\nproduction\ndeploying to  259–261\ntaking features to  40–45\nautomated deployments   \n42–43\nquality-controlled deployments  \n42–43\nresilience  43\ntransparency  43–45\nproduction environments  189–190\nautomation  191\nfeatures of  190–191\nspeed  191\nproduct mode  334\nPrometheus ecosystem\nconfiguring  280\nmonitoring systems with   \n275–291\nRabbitMQ  282–285\nsetting up alerts  287–291\nsetting up metric collection \ninfrastructure  276–283\nprotocol buffers  346\nprovisioning virtual machines   \n192–193\npublic images, Docker  220\npublish-subscribe  64–65, 127\nPython chassis  166\nQ\nquality-controlled deployments   \n42–43\nqueries  120–127, 105–128\nanalytics  127\nCQRS pattern challenges   \n125–127\noptimistic updates  126\npolling  127\npublish-subscribe  127\nreporting  127\nseparating from commands   \n123–125\nCQRS pattern architecture  124\nstoring copies of data  122–123\nqueues. See job queues\nR\nRabbitMQ broker  167, 282–285\nrabbitmq_queue_messages \nmetric  290\nrate limits  67, 152\nRBAC (role-based access control)   \n353–354\nreadability of logs  301–303\nreadiness checks  151, 233\nrecovery_timeout parameter  180\nredundancy  144\nreleases  189\nreliability\ndefining  130–132\nmaximizing  150–155\nload balancing  150–151\nrate limits  152\nservice health  150–151\noverview of  42\nvalidating  152–155\nchaos testing  154–155\nload testing  153–154\nremote_hello method  168\nReplicaSet  227\nreplication lag  125\nreporting  127\nrequest for comments (RFC)  345\nresiliency  9–21, 43\nretirement  100–103\nretries  140–143\nRFC (request for comments)  345\nrisk, reducing  13, 164\nrobust monitoring stacks  270–275\ngolden signals  272–273\nerrors  273\nlatency  272–273\nsaturation  273\ntraffic  273\nlayered monitoring  270–272\nrecommended  \npractices  274–275\ntypes of metrics  273–274\ngauges  273\nhistograms  274\n \n\n\n364\nindex\nrole-based access control. See RBAC\nrolling back  114, 241, 259\nrolling deploys  211–213\nround-robin algorithm  167\nrouting components  190\nRPC-facing services  150\nrpc-market_service queue, \nRabbitMQ  178\nrun the packer build command  204\nruntime management  190\nruntime platforms  57, 190\nS\nsagas  111–120\nchoreographed  112–115\nconsistency patterns  118–119\nevent sourcing  119–120\ninterwoven  117–118\ninterruption  118\nlocking  118\nshort-circuiting  118\norchestrated  115–117\nsaturation  273\nscalability  42\nscaling up microservice \ndevelopment  45–46\nisolation  46\ntechnical divergence  45–46\nscanning tools  220\nscheduled services, multiple per \nhost  209–210\nadvantages of scheduling \nmodel  210\ncontainer schedulers  210\nscheduling models  210\nscoping  15–16\nby business capabilities  78–87\ncapabilities  78–79\nchallenges and limitations  87\ncreating investment \nstrategies  79–86\ndomain modeling  78–79\nnested contexts and services  86\nby use case  87–94\nactions  92–94\nchoreography  94\norchestration  94\nplacing investment strategy \norders  88–92\nstores  92–94\nby volatility  94–96\nservices  20\nScripted Pipeline  247\nsecure operation  190\nsecurity  220\nsecurity tests  257\nself-healing  195–196\nsending notifications  96–98\nserver images  203–204\nserver management  201\nservice artifact  199\nservice collaboration  36–37\nservice contracts  37\nservice responsibility  37\nservice discovery  65, 167–171\nservice health  150–151\nservice mesh  157–158\nservice migration  102\nservice-oriented architectures \n(SOAs)  6\nservice partitioning  99\nservice practices as source of \nfailures  136\nservice responsibility  37\nservices  58–60\naggregation  59–60\nAPI gateways  39\nbehavior across  297–300\nbuilding artifacts  199–207\nconfiguring  206–207\ndefining  200\nimmutability  201\ntypes of service artifacts   \n202–206\ncapabilities of  58–59\nchallenges of designing  132–139\ncascading failures  136–139\nsources of failure  133–136\ncoarse-grained\noverview of  87\nstarting with  99–100\nconnecting multiple  241–242\ncontainerizing  215–224\nbuilding images  218–220\nrunning containers  220–222\nstoring images  223–224\nworking with images  216–218\ncontracts between  16\ncritical paths  60\ndeploying  191–199, 187–213\nadding load balancers   \n196–198\nimportance of  188–189\nprovisioning virtual \nmachines  192–193\nservice startup  192\ndesigning  129–158\ndefining reliability  130–132\ndesigning reliable communication  \n139–149\nasynchronous communication  \n149\ncircuit breakers  146–148\nfallbacks  143–145\nretries  140–143\ntimeouts  145–146\nhigher order services  59–60\ninteractions between tracing \nand  313–320\nlocating  65\nmaximizing reliability  \nof  150–155\nload balancing  150–151\nrate limits  152\nservice health  150–151\nvalidating reliability and fault \ntolerance  152–155\nnested contexts and  86\nnoncritical paths  60\nownership in organizations   \n103–104\nrun multiple instances  \nof  194–196\nadding capacity  196\n \n\n\n\t\n365\nindex\nfailure zones  195\nself-healing  195–196\nsafety of  156–158\nframeworks  156\nservice mesh  157–158\nscoping  20\nsetting up tracing in  315–320\nto underlying hosts  207–210\nmultiple scheduled services \nper host  209–210\nmultiple static services per \nhost  208–209\nsingle service to host  207\nwithout downtime  210–213\ncanaries on GCE  211–213\nrolling deploys on  \nGCE  211–213\nservice teams  87\nservice to host models  207–210\nmultiple scheduled services per \nhost  209–210\nadvantages of scheduling \nmodel  210\ncontainer schedulers  210\nmultiple static services per \nhost  208–209\nsingle service to host  207\nsharding  6\nshort-circuiting  118\nsilos  332\nsmoke tests  259\nSOAs (service-oriented \narchitectures)  6\nsources  301\nspans  314–315\nstability  189\nautomation  189\nmanual change \nmanagement  189\nsmall releases  189\nstacks. See robust monitoring stacks\nstaging\ndeploying to  255–257\nenvironments  258\nstandardizing communication  156\nstartup logs  205\nstartups  192\nstate changes  231\nstatic services  208–209\nStatsD exporter  172–173, 277–279\nstatsd-exporter container  277\nSTDOUT (standard output)  304\nstores  92–94\nstoring\ncopies of data  122–123\nimages  223–224\nstubbed data  145\nsubtasks, sagas  112\nsupporting notifications  97\nsynchronous communication   \n36, 163\nsynchronous messages\nchoosing transport  62\ndrawbacks of  62\nwhen to use  61–62\nsynchronous requests  62\nT\ntarballs  223\nteam models  330–340\ngrouping across functions   \n332–333\ngrouping by function  330–332\nlack of autonomy  332\nno long-term responsibility  332\nrisk of silos  332\nunclear ownership  331\ninfrastructures  335–337\non-call support  337–338\nplatforms  335–337\nproduct  335–337\nsetting team boundaries   \n334–336\nsharing knowledge  338–340\nteams, challenges of  16. See \nalso microservice teams\ntechnical capabilities  96–98\noverview of  59\nsending notifications  96–98\nwhen to use  98\ntechnical divergence  45–46\ntechnical heterogeneity  12\ntenacity library  140\ntesting  252–254\nchaos testing  154–155\nJenkins on Minikube  354–356\nload testing  153–154\nthree-tier architecture  19, 52\ntimeouts  145–146\ntimestamps  300\nTogglz  265\ntraces, visualizing  320–324\ntracing\ninteractions between services \nand  313–320\nsetting up in services  315–320\nspans and  314–315\nTradeExecuted event  39\ntraffic  273\ntransactions  105–128\nconsistent  106–108\ndistributed  107–108\noverview of  130, 136\ntransparency  9, 42–45\ntransport  62\ntransport-related boilerplate  163\ntruck factor  41\ntry-finally statement  254\ntwo-phase commit (2PC) protocol   \n107\nU\nupdates, optimistic  126\nupstream collaborators  37\nuptime  130\nuse case, scoping by  87–94\nactions  92–94\nchoreography  94\norchestration  94\nplacing investment strategy \norders  88–92\nstores  92–94\nuser management  80\n \n\n\n366\nindex\nV\nvault  207\nVMs (virtual machines)  56, \n192–193\nvolatility  78, 94–96\nvolume  350–351\nW\nworker nodes, components of  231\nworkflow tools  117\nworkspaces  247\nZ\nzero-downtime deployments  210\n \n\n\nBruce  ●  Pereira\nI\nnvest your time in designing great applications, improving \ninfrastructure, and making the most out of your dev teams. \nMicroservices are easier to write, scale, and maintain than \ntraditional enterprise applications because they’re built as a \nsystem of independent components. Master a few important \nnew patterns and processes, and you’ll be ready to develop, \ndeploy, and run production-quality microservices.\nMicroservices in Action teaches you how to write and maintain \nmicroservice-based applications. Created with day-to-day \ndevelopment in mind, this informative guide immerses you \nin real-world use cases from design to deployment. You’ll \ndiscover how microservices enable an efﬁ cient continuous \ndelivery pipeline, and explore examples using Kubernetes, \nDocker, and Google Container Engine. \nWhat’s Inside\n● An overview of microservice architecture\n● Building a delivery pipeline\n● Best practices for designing multi-service transactions \n   and queries\n● Deploying with containers\n● Monitoring your microservices\nWritten for intermediate developers familiar with enterprise \narchitecture and cloud platforms like AWS and GCP.\nMorgan Bruce and Paulo A. Pereira are experienced engineering \nleaders. They work daily with microservices in a production \nenvironment, using the techniques detailed in this book.\nTo download their free eBook in PDF, ePub, and Kindle formats, owners \nof this book should visit manning.com/books/microservices-in-action\n$49.99 / Can $65.99  [INCLUDING eBOOK]\nMicroservices IN ACTION\nSOFTWARE DEVELOPMENT\nM A N N I N G\n“\nThe one [and only] book \non implementing micro-\nservices with a real-world, \ncover-to-cover example \nyou can relate to.”\n \n—Christian Bach, Swiss Re\n“\nA perfect ﬁ t for those who \nwant to move their majestic \nmonolith to a scalable \n  microservice architecture.”\n \n—Akshat Paul\nMcKinsey & Company\n“\nShows not only how to \nwrite microservices, but also \nhow to prepare your \nbusiness and infrastructure \n  for this change.”\n—Maciej Jurkowski, Grupa Pracuj \n“\nA deep dive into microser-\nvice development with many \nreal and useful examples.”\n \n—Antonio Pessolano\nConsoft Sistemi\nSee first page\n",
      "page_number": 374
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 384-392)",
      "start_page": 384,
      "end_page": 392,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 384
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 393-394)",
      "start_page": 393,
      "end_page": 394,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 393
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "M A N N I N G\nMorgan Bruce\nPaulo A. Pereira\n",
      "content_length": 44,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 2,
      "content": "A microservice production environment\nControl pane\nRuntime\nmanagement\nManages\nNetwork and routing\nConnects\nObservability\nObserves\nDeployment pipeline\nMonitors\nEngineers\nWrites\nCode\nProduction\nA microservice production environment has several components: a deployment target,  \na deployment pipeline, runtime management, networking features, and support for  \nobservability. In this book, we'll teach you about these components and how you can use  \nthem to build a stable, modern microservice application.\n \n",
      "content_length": 508,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "Microservices in Action\n \n",
      "content_length": 26,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "Microservices in Action\nMORGAN BRUCE \nPAULO A. PEREIRA\nM A N N I N G\nShelter Island\n \n",
      "content_length": 86,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "For online information and ordering of this and other Manning books, please visit www.manning.com. \nThe publisher offers discounts on this book when ordered in quantity.\nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2019 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form \nor by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the \npublisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed \nas trademarks. Where those designations appear in the book, and Manning Publications was aware of a \ntrademark claim, the designations have been printed in initial caps or all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have the books \nwe publish printed on acid-­free paper, and we exert our best efforts to that end. Recognizing also our \nresponsibility to conserve the resources of our planet, Manning books are printed on paper that is at \nleast 15 percent recycled and processed without the use of elemental chlorine.\n∞\n\t\nManning Publications Co. \n20 Baldwin Road\nPO Box 761 \nShelter Island, NY 11964\n\t\nAcquisitions editor:\t\nMichael Stephens\n\t\nDevelopment editor:\t\nKaren Miller\n\tTechnical development editor:\t\nKarsten Strøbæk\n\t\nReview editor:\t\nAleksander Dragosavljevic´\n\t\nProject editor:\t\nAnthony Calcara\n\t\nCopy editor:\t\nCarl Quesnel\n\t\nProofreader:\t\nKeri Hales\n\t\nTechnical proofreader:\t\nJohn Guthrie\n\t\nTypesetter:\t\nHappenstance Type-O-Rama\n\t\nCover designer:\t\nMarija Tudor\nISBN 9781617294457\nPrinted in the United States of America\n1 2 3 4 5 6 7 8 9 10 — DP — 23 22 21 20 19 18\n \n",
      "content_length": 1851,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "v\nbrief contents\nPart 1\t\nThe lay of the land.................................................. 1\n1\t■\t Designing and running microservices  3\n2\t■\t Microservices at SimpleBank  28\nPart 2\t\nDesign........................................................................49\n3\t■\t Architecture of a microservice application  51\n4\t■\t Designing new features  75\n5\t■\t Transactions and queries in microservices   105\n6\t■\t Designing reliable services  129\n7\t■\t Building a reusable microservice framework  159\nPart 3\t\nDeployment............................................................185\n8\t■\t Deploying microservices  187\n9\t■\t Deployment with containers and schedulers  214\n10\t■\t Building a delivery pipeline for microservices  243\nPart 4\t\nObservability and ownership..............................267\n11\t■\t Building a monitoring system  269\n12\t■\t Using logs and traces to understand behavior  296\n13\t■\t Building microservice teams  325\n \n",
      "content_length": 927,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "vii\ncontents\npreface xv\nacknowledgments xvii\nabout this book xix\nabout the authors xxii\nabout the cover illustration xxiii\nPart 1\t The lay of the land.......................................1\n1\nDesigning and running microservices  3\n\t1.1\t\nWhat is a microservice application?  4\nScaling through decomposition  6  ■  Key principles  7  ■  Who uses \nmicroservices?    ■  Why are microservices a good choice?  12\n\t1.2\t\nWhat makes microservices challenging?  14\nDesign challenges    ■  Operational challenges  17\n\t1.3\t\nMicroservice development lifecycle  18\nDesigning microservices    ■  Deploying  \nmicroservices  ■  Observing microservices  24\n\t1.4\t\nResponsible and operationally aware engineering \nculture  26\n \n",
      "content_length": 709,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "viii\nviii\n﻿ CONTENTS\n\t\n2\t\nMicroservices at SimpleBank  28\n\t2.1\t\nWhat does SimpleBank do?  29\n\t2.2\t\nAre microservices the right choice?  30\nRisk and inertia in financial software  31  ■  Reducing friction and \ndelivering sustainable value  31\n\t2.3\t\nBuilding a new feature  32\nIdentifying microservices by modeling the domain    ■  Service \ncollaboration  3  ■  Service choreography  37\n\t2.4\t\nExposing services to the world  39\n\t2.5\t\nTaking your feature to production  40\nQuality-controlled and automated deployment  42 \nResilience  43Transparency  43\n\t2.6\t\nScaling up microservice development  45\nTechnical divergence  45  ■  Isolation  46\n\t2.7\t\nWhat’s next?  47\nPart 2\t Design...........................................................49\n\t\n3\t\nArchitecture of a microservice application  51\n\t3.1\t\nArchitecture as a whole  52\nFrom monolith to microservices  52  ■  The role of an \narchitect  54  ■  Architectural principles  54  ■  The four tiers  \nof a microservice application  55\n\t3.2\t\nA microservice platform  56\nMapping your runtime platform  57\n\t3.3\t\nServices  58\nCapabilities  58  ■  Aggregation and higher order \nservices  59  ■  Critical and noncritical paths  60\n\t3.4\t\nCommunication  60\nWhen to use synchronous messages  61  ■  When to use \nasynchronous messages  62  ■  Asynchronous communication \npatterns  63  ■  Locating other services  65\n3.5\tThe application boundary  66\nAPI gateways  68  ■  Backends for frontends  69  ■  Consumer-\ndriven gateways  70\n\t3.6\t\nClients  71\nFrontend monoliths  71  ■  Micro-frontends  72\n \n",
      "content_length": 1534,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "\t\nix\n\t\nix\n﻿ CONTENTS\n\t\n4\t\nDesigning new features  75\n\t4.1\t\nA new feature for SimpleBank  76\n\t4.2\t\nScoping by business capabilities  78\nCapabilities and domain modeling  78  ■  Creating investment \nstrategies   79  ■  Nested contexts and services  86  ■  Challenges \nand limitations  87\n\t4.3\t\nScoping by use case  87\nPlacing investment strategy orders   88  ■  Actions and stores  92   \nOrchestration and choreography  94\n\t4.4\t\nScoping by volatility  94\n\t4.5\t\nTechnical capabilities  96\nSending notifications  96  ■  When to use technical capabilities  98\n\t4.6\t\nDealing with ambiguity  99\nStart with coarse-grained services  99  ■  Prepare for further \ndecomposition  100  ■  Retirement and migration  100\n\t4.7\t\nService ownership in organizations  103\n\t\n5\t\nTransactions and queries in microservices   105\n\t5.1\t\nConsistent transactions in distributed applications  106\nWhy can’t you use distributed transactions?  107\n\t5.2\t\nEvent-based communication  108\nEvents and choreography  109\n\t5.3\t\nSagas  111\nChoreographed sagas  112  ■  Orchestrated sagas  115   \nInterwoven sagas  117  ■  Consistency patterns  118  ■  Event \nsourcing  119\n\t5.4\t\nQueries in a distributed world  120\nStoring copies of data  122  ■  Separating queries and \ncommands  123  ■  CQRS challenges  125  ■  Analytics  \nand reporting  127\n\t5.5\t\nFurther reading  128\n \n",
      "content_length": 1333,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "x\n﻿ CONTENTS\n\t\n6\t\nDesigning reliable services  129\n\t6.1\t\nDefining reliability  130\n\t6.2\t\nWhat could go wrong?  132\nSources of failure  133  ■  Cascading failures  136\n\t6.3\t\nDesigning reliable communication  139\nRetries  140  ■  Fallbacks  143  ■  Timeouts  145  ■  Circuit \nbreakers  146 \n■  Asynchronous communication  149\n\t6.4\t\nMaximizing service reliability  150\nLoad balancing and service health  150  ■  Rate limits  152   \nValidating reliability and fault tolerance  152\n\t6.5\t\nSafety by default  156\nFrameworks  156  ■  Service mesh  157\n\t\n7\t\nBuilding a reusable microservice framework  159\n\t7.1\t\nA microservice chassis  160\n\t7.2\t\nWhat’s the purpose of a microservice chassis?  163\nReduced risk  164  ■  Faster bootstrapping  164\n\t7.3\t\nDesigning a chassis  165\nService discovery  167  ■  Observability  171  ■  Balancing and \nlimiting  177\n\t7.4\t\nExploring the feature implemented using the \nchassis  180\n\t7.5\t\nWasn’t heterogeneity one of the promises of \nmicroservices?  182\nPart 3\t Deployment................................................185\n\t\n8\t\nDeploying microservices  187\n\t8.1\t\nWhy is deployment important?  188\nStability and availability  189\n\t8.2\t\nA microservice production environment  189\nFeatures of a microservice production environment  190   \nAutomation and speed  191\n \n",
      "content_length": 1292,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "\t\nxi\n\t\nxi\n﻿ CONTENTS\n\t8.3\t\nDeploying a service, the quick way  191\nService startup  192  ■  Provisioning a virtual machine  192   \nRun multiple instances of your service  194  ■  Adding a load \nbalancer  196  ■  What have you learned?  198\n\t8.4\t\nBuilding service artifacts  199\nWhat’s in an artifact?  200  ■  Immutability  201  ■  Types of \nservice artifacts  202  ■  Configuration  206\n\t8.5\t\nService to host models  207\nSingle service to host  207  ■  Multiple static services per \nhost  208  ■  Multiple scheduled services per host  209\n\t8.6\t\nDeploying services without downtime  210\nCanaries and rolling deploys on GCE  211\n\t\n9\t\nDeployment with containers and schedulers  214\n\t9.1\t\nContainerizing a service  215\nWorking with images  216  ■  Building your image  218   \nRunning containers  220  ■  Storing an image  223\n\t9.2\t\nDeploying to a cluster  224\nDesigning and running pods  226  ■  Load balancing  228   \nA quick look under the hood  230  ■  Health checks  233   \nDeploying a new version  235  ■  Rolling back  241   \nConnecting multiple services  241\n\t 10\t\nBuilding a delivery pipeline for microservices  243\n\t10.1\t Making deploys boring  244\nA deployment pipeline  244\n\t10.2\t Building a pipeline with Jenkins  246\nConfiguring a build pipeline  247  ■  Building your image  251   \nRunning tests  252  ■  Publishing artifacts  254  ■  Deploying \nto staging  255  ■  Staging environments  258  ■  Deploying to \nproduction  259\n\t10.3\t Building reusable pipeline steps  262\nProcedural versus declarative build pipelines  263\n\t10.4\t Techniques for low-impact deployment and feature \nrelease  264\nDark launches  264  ■  Feature flags  265\n \n",
      "content_length": 1647,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "xii\nxii\n﻿ CONTENTS\nPart 4\t Observability and ownership..................267\n\t 11\t\nBuilding a monitoring system  269\n\t11.1\t A robust monitoring stack  270\nGood monitoring is layered  270  ■  Golden signals  272   \nTypes of metrics  273  ■  Recommended practices  274\n\t11.2\t Monitoring SimpleBank with Prometheus and \nGrafana  275\nSetting up your metric collection infrastructure  276  ■  Collecting \ninfrastructure metrics — RabbitMQ  282  ■  Instrumenting \nSimpleBank’s place order  285  ■  Setting up alerts  287\n\t11.3\t Raising sensible and actionable alerts  291\nWho needs to know when something is wrong?  292  ■  Symptoms, \nnot causes   292\n\t11.4\t Observing the whole application  293\n\t 12\t\nUsing logs and traces to understand behavior  296\n\t12.1\t Understanding behavior across services  297\n\t12.2\t Generating consistent, structured,  \nhuman-readable logs  300\nUseful information to include in log entries  300  ■  Structure  \nand readability  301\n\t12.3\t Setting up a logging infrastructure for SimpleBank  303\nELK- and Fluentd-based solution  304  ■  Setting up your logging \nsolution  306  ■  Configure what logs to collect  308  ■  Finding a \nneedle in the haystack  311  ■  Logging the right information  313\n\t12.4\t Tracing interactions between services  313\nCorrelating requests: traces and spans  314  ■  Setting up tracing in \nyour services  315\n\t12.5\t Visualizing traces  320\n\t 13\t\nBuilding microservice teams  325\n\t13.1\t Building effective teams  326\nConway’s Law  327  ■  Principles for effective teams  328\n\t13.2\t Team models  330\nGrouping by function  330  ■  Grouping across functions  332   \nSetting team boundaries  334  ■  Infrastructure, platform, and \nproduct  335  ■  Who’s on-call?  337  ■  Sharing knowledge  338\n \n",
      "content_length": 1740,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "\t\nxiii\n\t\nxiii\n﻿ CONTENTS\n13.3\t Recommended practices for microservice teams  340\nDrivers of change in microservices  340  ■  The role \nof architecture  341  ■  Homogeneity versus technical \nflexibility  343  ■  Open source model  343  ■  Design \nreview  345  ■  Living documentation  346  ■  Answering \nquestions about your application  347\n13.4\t Further reading  347\n\t\nappendix\t\nInstalling Jenkins on Minikube  349\n\t\nindex  357\n \n",
      "content_length": 431,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "xv\npreface\nOver the past five years, the microservice architectural style — structuring applications \nas fine-grained, loosely coupled, and independently deployable services — has become \nincreasingly popular and increasingly feasible for engineering teams, regardless of \ncompany size.\nFor us, working on microservice projects at Onfido was a revelation, and this book \nrecords many of the things we learned along the way. By breaking apart our product, \nwe could ship faster and with less friction, instead of tripping over each other’s toes in \na large, monolithic codebase. A microservice approach helps engineers build applica-\ntions that can evolve over time, even as product complexity and team size grow.\nOriginally, we set out to write a book about our real-world experience running micro-\nservice applications. As we scoped the book, that mission evolved, and we decided to \ndistill our experience of the full application lifecycle — designing, deploying, and oper-\nating microservices — into a broad and practical review. We’ve picked tools to illustrate \nthese techniques — such as Kubernetes and Docker — that are popular and go hand in \nhand with microservice best practice, but we hope that you can apply the lessons within \nregardless of which language and tools you ultimately use to build applications.\nWe sincerely hope you find this book a valuable reference and guide — and that the \nknowledge, advice, and examples within help you build great products and applications \nwith microservices.\n \n",
      "content_length": 1514,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "xvii\nacknowledgments\nIn its evolution over the past year and a half, this book has grown from an idea to \nwrite a small book on deploying services to a substantial work covering a wide swath of \nmicroservice development topics — from design to communication to deployment and \noperation. It has been our privilege to work with so many talented people in delivering \na book that we truly hope will be useful, both to those who are starting to adopt this \ntype of architecture and to those who already are using it.\nI would like to thank my family, in particular Rosa and Beatriz, my wife and daughter, \nwho put up with the absences of a husband and father. I would also like to thank Mor-\ngan, my coauthor and colleague. He has been crucial in providing guidance and clarity \nfrom day one. Thank you!\n—Paulo\nThis book wouldn’t have been possible to write without the patience and support of \nmy family, who gracefully tolerated far too many weekends, evenings, and holidays with \nme sitting in front of a laptop. I’d also like to thank my parents, Heather and Allan, \nwho taught me a love of reading, without which I wouldn’t be writing this today. And \nlastly, thanks Paulo! You encouraged me to start this project with you, and although the \nway was sometimes challenging, I’ve learned so much from the journey.\n—Morgan\nTogether, we would like to thank:\n¡ Karen and Dan, our development editors, who were tireless, week after week, in \nproviding support and advice to help us write the best possible book\n¡ Karsten Strøbæk, our technical development editor, for his critical eye and gen-\nerous feedback\n \n",
      "content_length": 1606,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "xviii\nxviii\n﻿ ACKNOWLEDGMENTS\n¡ Michael Stephens, for his faith in us, and Marjan Bace, for his help in shaping \nour book into something compelling for Manning’s readers\n¡ The many other people we’ve worked with at Manning, who formed such a pro-\nfessional and talented team, without whom this book would have never been \npossible\n¡ Lastly, our reviewers, whose feedback and help improving our book we deeply \nappreciated, including Akshat Paul, Al Krinker, Andrew Miles, Andy Miles, Anto-\nnio Pessolano, Bachir Chihani, Christian Bach, Christian Thoudahl, Vittal Dam-\naraju, Deepak Bhaskaran, Evangelos Bardis, John Guthrie, Lorenzo De Leon, \nŁukasz Witczak, Maciej Jurkowski, Mike Jensen, Shobha Iyer, Srihari Sridharan, \nSteven Parr, Thorsten Weber, and Tiago Boldt Sousa\n \n",
      "content_length": 777,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "xix\nabout this book\nMicroservices in Action is a practical book about building and deploying microservice-based \napplications. Written for developers and architects with a solid grasp of service-oriented \ndevelopment, it tackles the challenge of putting microservices into production. You'll \nbegin with an in-depth overview of microservice design principles, building on your \nknowledge of traditional systems. Then you'll start creating a reliable road to production. \nYou'll explore examples using Kubernetes, Docker, and Google Container Engine as you \nlearn to build clusters and maintain them after deployment.\nThe techniques in this book should apply to developing microservices in most pop-\nular programming languages. We decided to use Python as the primary language for \nthis book because its low-ceremony style and terse syntax lend themselves to clear and \nexplicit examples. Don’t worry if you’re not too familiar with Python — we’ll guide you \nthrough running the examples.\nHow this book is organized: a roadmap\nPart 1 of this book gives a brief introduction to microservices, exploring the properties \nand benefits of microservice-based systems and the challenges you may face in their \ndevelopment.\nChapter 1 introduces the microservice architecture. We examine the benefits and \ndrawbacks of the microservice approach and explain the key principles of microservice \ndevelopment. Lastly, we introduce the design and deployment challenges we’ll cover \nthroughout this book.\nChapter 2 applies the microservice approach to an example domain — SimpleBank. \nWe design a new feature with microservices and examine how to make that feature \nready for production.\n \n",
      "content_length": 1674,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "xx\nxx\n﻿ ABOUT THIS BOOK\nIn part 2, we explore the architecture and design of microservice applications.\nChapter 3 walks through the architecture of a microservice application, covering \nfour layers: platform, service, boundary, and client. The goal of this chapter is to give \nthe reader a big-picture model that they can use when working to understand any \nmicroservice system.\nChapter 4 covers one of the hardest parts of microservice design: how to decide on \nservice responsibilities. This chapter lays out four approaches to modeling — business \ncapabilities, use cases, technical capabilities, and volatility — and, using examples from \nSimpleBank, explores how to make good design decisions, even when boundaries are \nambiguous.\nChapter 5 explores how to write business logic in distributed systems, where trans-\nactional guarantees no longer apply. We introduce the reader to different transaction \npatterns, such as sagas, and query patterns, such as API composition and CQRS.\nChapter 6 covers reliability. Distributed systems can be more fragile than monolithic \napplications, and communication between microservices requires careful consider-\nation to avoid availability issues, downtime, and cascading failures. Using examples in \nPython, we explore common techniques for maximizing application resiliency, such as \nrate limits, circuit breakers, health checks, and retries.\nIn chapter 7, you’ll learn how to design a reusable microservice framework. Consis-\ntent practices across microservices improve overall application quality and reliability \nand reduce time to development for new services. We provide working examples in \nPython.\nIn part 3, we look at deployment best practices for microservices.\nChapter 8 emphasizes the importance of automated continuous delivery in microser-\nvice applications. Within this chapter, we take a single service to production — on Google \nCompute Engine — and from that example learn about the importance of immutable \nartifacts and the pros and cons of different microservice deployment models.\nChapter 9 introduces Kubernetes, a container scheduling platform. Containers, \ncombined with a scheduler like Kubernetes, are a natural and elegant fit for running \nmicroservices at scale. Using Minikube, you’ll learn how to package a microservice and \ndeploy it seamlessly to Kubernetes.\nIn chapter 10, you’ll build on the example in the previous chapter to construct an \nend-to-end delivery pipeline using Jenkins. You’ll script a pipeline with Jenkins and \nGroovy that takes new commits to production rapidly and reliably. You’ll also learn how \nto apply consistent deployment practices to a microservice fleet.\nIn this book’s final part, we explore observability and the human side of microservices.\nChapter 11 will walk you through the development of a monitoring system for micro-\nservices, using StatsD, Prometheus, and Grafana to collect and aggregate metrics to \nproduce dashboards and alerts. We’ll also discuss good practices for alert management \nand avoiding alert fatigue.\nChapter 12 builds on the work in the previous chapter to include logs and traces. \nGetting rich, real-time, and searchable information from our microservices helps us \n \n",
      "content_length": 3204,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "\t\nxxi\n\t\nxxi\n﻿ ABOUT THIS BOOK\nunderstand them, diagnose issues, and improve them in the future. Examples in this \nchapter use Elasticsearch, Kibana, and Jaeger.\nLastly, chapter 13 takes a slight left turn to explore the people side of microservice \ndevelopment. People implement software: building great software is about effective col-\nlaboration as much as implementation choices. We’ll examine the principles that make \nmicroservice teams effective and explore the psychological and practical implications \nof the microservice architectural approach on good engineering practices.\nAbout the code\nThis book contains many examples of source code, both in numbered listings and \ninline with normal text. In both cases, source code is formatted in a fixed-width font \nlike this to separate it from ordinary text. Sometimes code is also in bold, either \nto highlight specific lines or to differentiate entered commands from the resulting \noutput.\nIn many cases, we’ve reformatted the original source code; we’ve added line breaks \nand reworked indentation to accommodate the available page space in the book. In \nrare cases, even this was not enough, and listings include line-continuation markers \n(➥). Additionally, we’ve often removed comments in the source code from the listings \nwhen we’ve described the code in the text. Code annotations accompany many of the \nlistings, highlighting important concepts.\nThe source code within this book is available on the book’s website at https://www \n.manning.com/books/microservices-in-action and on the Github repository at https://\ngithub.com/morganjbruce/microservices-in-action.\nYou can find instructions on running examples throughout the book. We typically \nuse Docker and/or Docker Compose to simplify running examples. The appendix cov-\ners configuring Jenkins, used in chapter 10, to run smoothly on a local deployment of \nKubernetes.\nBook forum\nPurchase of Microservices in Action includes free access to a private web forum run by \nManning Publications where you can make comments about the book, ask techni-\ncal questions, and receive help from the authors and from other users. To access the \nforum, go to https://forums.manning.com/forums/microservices-in-action. You can \nalso learn more about Manning's forums and the rules of conduct at https://forums. \nmanning.com/forums/about.\nManning’s commitment to our readers is to provide a venue where a meaningful \ndialogue between individual readers and between readers and the authors can take \nplace. It is not a commitment to any specific amount of participation on the part of the \nauthors, whose contribution to the forum remains voluntary (and unpaid). We suggest \nyou try asking the authors some challenging questions, lest their interest stray! The \nforum and the archives of previous discussions will be accessible from the publisher’s \nwebsite as long as the book is in print.\n \n",
      "content_length": 2893,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "xxii\nabout the authors\nMorgan Bruce has significant experience in building complex \napplications, with particular expertise in the finance and identity \nverification industries, where accuracy, resilience, and security are \ncrucial. As an engineering leader, he has worked on large-scale \nrefactoring and rearchitecture efforts. He also has firsthand expe-\nrience leading the evolution from a monolithic application to a \nrobust microservice architecture.\nPaulo A. Pereira is currently leading a team involved in reshaping \na monolith into microservices, while dealing with the constraints \nof evolving a system where security and accuracy are of the utmost \nimportance. Enthusiastic about choosing the right tool for the job \nand combining different languages and paradigms, he is currently \nexploring functional programming, mainly via Elixir. Paulo also \nauthored Elixir Cookbook and was one of the technical reviewers for \nLearning Elixir and for Mastering Elixir.\n \n",
      "content_length": 971,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "xxiii\nabout the cover illustration\nThe figure on the cover of Microservices in Action is captioned “Habit of a Lady of China \nin 1700.” The illustration is taken from Thomas Jefferys’ A Collection of the Dresses of \nDifferent Nations, Ancient and Modern (four volumes), London, published between 1757 \nand 1772. The title page states that these are hand-colored copperplate engravings, \nheightened with gum arabic.\nThomas Jefferys (1719–1771) was called “Geographer to King George III.” He was \nan English cartographer who was the leading map supplier of his day. He engraved and \nprinted maps for government and other official bodies and produced a wide range of \ncommercial maps and atlases, especially of North America. His work as a mapmaker \nsparked an interest in local dress customs of the lands he surveyed and mapped, which \nare brilliantly displayed in this collection. Fascination with faraway lands and travel for \npleasure were relatively new phenomena in the late 18th century, and collections such \nas this one were popular, introducing both the tourist and the armchair traveler to the \ninhabitants of other countries.\nThe diversity of the drawings in Jefferys’ volumes speaks vividly of the uniqueness and \nindividuality of the world’s nations some 200 years ago. Dress codes have changed since \nthen, and much of the diversity by region and country, so rich at the time, has faded \naway. It’s now often hard to tell the inhabitants of one continent from another. Perhaps, \ntrying to view it optimistically, we’ve traded a cultural and visual diversity for a more var-\nied personal life — or a more varied and interesting intellectual and technical life.\nAt a time when it’s difficult to tell one computer book from another, Manning cele-\nbrates the inventiveness and initiative of the computer business with book covers based \non the rich diversity of regional life of two centuries ago, brought back to life by Jeffreys’ \npictures.\n \n",
      "content_length": 1953,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "Part 1\nThe lay of the land\nThis part introduces microservice architecture, explores the properties and \nbenefits of microservice applications, and presents some of the challenges you’ll \nface in developing microservice applications. We’ll also introduce SimpleBank, a \nfictional company whose attempts to build a microservice application will be the \ncommon thread in many examples used in this book.\n \n",
      "content_length": 403,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "3\n1\nDesigning and \nrunning microservices\nThis chapter covers\n¡ Defining a microservice application\n¡ The challenges of a microservices approach\n¡ Approaches to designing a microservice \napplication\n¡ Approaches to running microservices \nsuccessfully\nSoftware developers strive to craft effective and timely solutions to complex prob-\nlems. The first problem you usually try to solve is: What does your customer want? If \nyou’re skilled (or lucky), you get that right. But your efforts rarely stop there. Your \nsuccessful application continues to grow: you debug issues; you build new features; \nyou keep it available and running smoothly.\nEven the most disciplined teams can struggle to sustain their early pace and agility \nin the face of a growing application. At worst, your once simple and stable product \nbecomes both intractable and delicate. Instead of sustainably delivering more value to \nyour customers, you’re fatigued from outages, anxious about releasing, and too slow to \ndeliver new features or fixes. Neither your customers nor your developers are happy.\nMicroservices promise a better way to sustainably deliver business impact. Rather \nthan a single monolithic unit, applications built using microservices are made up of \n \n",
      "content_length": 1242,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "4\nChapter 1  Designing and running microservices \nloosely coupled, autonomous services. By building services that do one thing well, you \ncan avoid the inertia and entropy of large applications. Even in existing applications, \nyou can progressively extract functionality into independent services to make your \nwhole system more maintainable.\nWhen we started working with microservices, we quickly realized that building smaller \nand more self-contained services was only one part of running a stable and business-critical \napplication. After all, any successful application will spend much more of its life in pro-\nduction than in a code editor. To deliver value with microservices, our team couldn’t be \nfocused on build alone. We needed to be skilled at operations: deployment, observation, \nand diagnosis.\n1.1\t\nWhat is a microservice application?\nA microservice application is a collection of autonomous services, each of which does \none thing well, that work together to perform more intricate operations. Instead of a \nsingle complex system, you build and manage a suite of relatively simple services that \nmight interact in complex ways. These services collaborate with each other through \ntechnology-agnostic messaging protocols, either point-to-point or asynchronously.\nThis might seem like a simple idea, but it has striking implications for reducing friction in \nthe development of complex systems. Classical software engineering practice advocates high \ncohesion and loose coupling as desirable properties of a well-engineered system. A system that \nhas these properties will be easier to maintain and more malleable in the face of change.\nCohesion is the degree to which elements of a certain module belong together, whereas \ncoupling is the degree to which one element knows about the inner workings of another. \nRobert C. Martin’s Single Responsibility Principle is a useful way to consider the former:\nGather together the things that change for the same reasons. Separate those things that \nchange for different reasons.\nIn a monolithic application, you try to design for these properties at a class, module, \nor library level. In a microservice application, you aim instead to attain these proper-\nties at the level of independently deployable units of functionality. A single microservice \nshould be highly cohesive: it should be responsible for some single capability within an \napplication. Likewise, the less that each service knows about the inner workings of other \nservices, the easier it is to make changes to one service — or capability — without forcing \nchanges to others.\nTo get a better picture of how a microservice application fits together, let’s start by \nconsidering some of the features of an online investment tool:\n¡ Opening an account\n¡ Depositing and withdrawing money\n¡ Placing orders to buy or sell positions in financial products (for example, shares)\n¡ Modeling risk and making financial predictions\nLet’s explore the process of selling shares:\n1\t A user creates an order to sell some shares of a stock from their account.\n2\t This position is reserved on their account, so it can’t be sold multiple times.\n \n",
      "content_length": 3152,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "\t\n5\nWhat is a microservice application?\n3\t It costs money to place an order on the market — the account is charged a fee.\n4\t The system needs to communicate that order to the appropriate stock market.\nFigure  1.1 shows how placing that sell order might look as part of a microservice \napplication.\nYou can observe three key characteristics of microservices in figure 1.1:\n¡ Each microservice is responsible for a single capability. This might be business related or \nrepresent a shared technical capability, such as integration with a third party (for example, \nthe stock exchange).\n¡ A microservice owns its data store, if it has one. This reduces coupling between \nservices because other services can only access data they don’t own through the \ninterface that a service provides.\n¡ Microservices themselves, not the messaging mechanism that connects them nor \nanother piece of software, are responsible for choreography and collaboration  — the \nsequencing of messages and actions to perform some useful activity.\nTransaction\ndatabase\nUser\n1. Places order to sell\n100 units of Stock A\nfrom account ABC\n2. Records order\ndetails in database\n3. Requests reservation\nof 100 units of Stock A\nagainst account ABC\n4. Records reserved\nstock position against\naccount ABC\n5. Requests calculation\nof fee\n6. Requests placement\nof order to market\n7. Places order onto\nstock exchange\nAccount\ntransactions\nservice\nOrders service\nFees service\nFee rules\ndatabase\nMarket service\nStock exchange\nOrder\ndatabase\nFigure 1.1    The flow of communication through microservices in an application that allows users to sell \npositions in financial shares\n \n",
      "content_length": 1633,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "6\nChapter 1  Designing and running microservices \nIn addition to these three characteristics, you can identify two more fundamental attri-\nbutes of microservices:\n¡ Each microservice can be deployed independently. Without this, a microservice \napplication would still be monolithic at the point of deployment.\n¡ A microservice is replaceable. Having a single capability places natural bounds on \nsize; likewise, it makes the individual responsibility, or role, of a service easy to \ncomprehend.\nThe idea that microservices are responsible for coordinating actions in a system is the \ncrucial difference between this approach and traditional service-oriented architec-\ntures (SOAs). Those types of systems often used enterprise service buses (ESBs) or \nmore complex orchestration standards to externalize messaging and process orches-\ntration from applications themselves. In that model, services often lacked cohesion, \nas business logic was increasingly added to the service bus, rather than the services \nthemselves.\nIt’s interesting to think about how decoupling functionality in the online investment \nsystem helps you be more flexible in the face of changing requirements. Imagine that \nyou need to change how fees are calculated. You could make and release those changes \nto the fees service without any change to its upstream or downstream services. Or imag-\nine an entirely new requirement: when an order is placed, you need to alert your risk \nteam if it doesn’t match normal trading patterns. It’d be easy to build a new microser-\nvice to perform that operation based on an event raised by the orders service without \nchanging the rest of the system.\n1.1.1\t\nScaling through decomposition\nYou also can consider how microservices allow you to scale an application. In The Art \nof Scalability, Abbott and Fisher define three dimensions of scale as the scale cube (fig-\nure 1.2).\nMonolithic applications typically scale through horizontal duplication: deploying \nmultiple, identical instances of the application. This is also known as cookie-cutter, \nor X-axis, scaling. Conversely, microservice applications are an example of Y-axis scal-\ning, where you decompose a system to address the unique scaling needs of different \nfunctionality.\nNOTE     The Z axis refers to horizontal data partitions: sharding. You can apply \nsharding to either approach — microservices or monolithic applications — but \nwe won’t be exploring that topic in this book.\nLet’s revisit the investment tool as an example, with the following characteristics:\n¡ Financial predictions might be computationally onerous and are rarely done.\n¡ Complex regulatory and business rules may govern investment accounts.\n¡ Market trading may happen in extremely large volumes, while also relying on \nminimizing latency.\n \n",
      "content_length": 2789,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "\t\n7\nWhat is a microservice application?\nZ axis–data\npartitioning\nScale by splitting\nsimilar things\nY axis–functional\ndecomposition\nScale by splitting\ndifferent things\nx axis–horizontal duplication\nScale by cloning\nFigure 1.2    The three dimensions of scaling an application\nIf you build features as microservices that meet the requirements of these characteristics, \nyou can choose the ideal technical tools to solve each problem, rather than trying to fit \nsquare pegs into round holes. Likewise, autonomy and independent deployment mean \nyou can manage the microservices’ underlying resource needs separately. Interestingly, \nthis also implies a natural way to limit failure: if your financial prediction service fails, \nthat failure is unlikely to cascade to the market trading or investment account services. \nMicroservice applications have some interesting technical properties:\n¡ Building services along the lines of single capabilities places natural bounds on \nsize and responsibility.\n¡ Autonomy allows you to develop, deploy, and scale services independently.\n1.1.2\t\nKey principles\nFive cultural and architectural principles underpin microservices development:\n¡ Autonomy\n¡ Resilience\n¡ Transparency\n¡ Automation\n¡ Alignment\n \n",
      "content_length": 1238,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "8\nChapter 1  Designing and running microservices \nThese principles should drive your technical and organizational decisions when you’re \nbuilding and running a microservice application. Let’s explore each of them.\nAutonomy\nWe’ve established that microservices are autonomous  — each service operates and \nchanges independently of others. To ensure that autonomy, you need to design your ser-\nvices so they are:\n¡ Loosely coupled  — By interacting through clearly defined interfaces, or through \npublished events, each microservice remains independent of the internal imple-\nmentation of its collaborators. For example, the orders service we introduced \nearlier shouldn’t be aware of the implementation of the account transactions \nservice. This is illustrated in figure 1.3.\n¡ Independently deployable  — Services will be developed in parallel, often by multi-\nple teams. Being forced to deploy them in lockstep or in an orchestrated forma-\ntion would result in risky and anxious deployments. Ideally, you want to use your \nsmaller services to enable rapid, frequent, and small releases.\nAutonomy is also cultural. It’s vital that you delegate accountability for and ownership \nof services to teams responsible for delivering business impact. As we’ve established, \norganizational design has an influence on system design. Clear service ownership \nallows teams to build iteratively and make decisions based on their local context and \ngoals. Likewise, this model is ideal for promoting end-to-end ownership, where a team \nis responsible for a service in both development and production. \nNOTE     In chapter 13, we’ll discuss developing responsible and autonomous \nengineering teams and why this is crucial when working with microservices.\nMessaging between\nservices should be language\nagnostic—for example\ngRPC, Thrift, JSON+HTTP.\nA service exposes a\ncontract. Messages are\nconstructed according to\nthis contract.\nInternal implementation is\nirrelevant to the caller, as long\nas it meets the contract.\nImplements\nUses\nOrders service\nMessaging\nAccount\ntransactions\nservice\nContract\nFigure 1.3    You can loosely couple services by having them communicate through defined contracts \nthat hide implementation details.\n \n",
      "content_length": 2217,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "\t\n9\nWhat is a microservice application?\nResilience\nMicroservices are a natural mechanism for isolating failure: if you deploy them inde-\npendently, application or infrastructure failure may only affect part of your system. \nLikewise, being able to deploy smaller bits of functionality should help you change \nyour system more gradually, rather than releasing a risky big bang of new functionality.\nConsider the investment tool again. If the market service is unavailable, it won’t be \nable to place the order to market. But a user can still request the order, and the service \ncan pick it up later when the downstream functionality becomes available.\nAlthough splitting your application into multiple services can isolate failure, it \nalso will multiply points of failure. In addition, you’ll need to account for what hap-\npens when failure does occur to prevent cascades. This involves both design — favor-\ning asynchronous interaction where possible and using circuit breakers and timeouts \nappropriately — and operations — using provable continuous delivery techniques and \nrobustly monitoring system activity.\nTransparency\nMost importantly, you need to know when a failure has occurred, and rather than one \nsystem, a microservice application depends on the interaction and behavior of mul-\ntiple services, possibly built by different teams. At any point, your system should be \ntransparent and observable to ensure that you both observe and diagnose problems.\n Every service in your application will produce business, operational, and infrastruc-\nture metrics; application logs; and request traces. As a result, you’ll need to make sense \nof a huge amount of data.\nAutomation\nIt might seem counterintuitive to alleviate the pain of a growing application by build-\ning a multitude of services. It’s true that microservices are a more complex architecture \nthan building a single application. By embracing automation and seeking consistency \nin the infrastructure between services, you can significantly reduce the cost of managing \nthis additional complexity. You need to use automation to ensure the correctness of \ndeployments and system operation.\nIt’s not a coincidence that the popularity of microservice architecture parallels both \nthe increasing mainstream adoption of DevOps techniques, especially infrastructure- \nas-code, and the rise of infrastructure environments that are fully programmable \nthrough APIs (such as AWS or Azure). These two trends have done a lot to make micro-\nservices feasible for smaller teams.\nAlignment\nLastly, it’s critical that you align your development efforts in the right way. You should \naim to structure your services, and therefore your teams, around business concepts. \nThis leads to higher cohesion.\nTo understand why this is important, consider the alternative. Many traditional SOAs \ndeployed the technical tiers of an application separately — UI, business logic, integra-\ntion, data. Figure 1.4 compares SOA and microservice architecture.\n \n",
      "content_length": 2994,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "10\nChapter 1  Designing and running microservices \nPresentation\nUI\nUI\nAPI\nService\nProcess\nService\nTransport\nOrchestration\nMessage\nStore\nStore\nStore\nBusiness services\nResponsibility split by tier\nOrchestration between\ncomponents governed by\na smart message bus\nServices contain\nfull vertical stack.\nServices own data.\nMicroservice architecture\nService-Oriented Architecture (SOA)\nIndependent\ncomponents\ncollaborate directly.\nService\nService\nService\nService\nStore\nLogic\nAPI\nData services\nEnterprise service bus\nFigure 1.4    SOA versus microservice architecture\nThis use of horizontal decomposition in SOA is problematic, because cohesive function-\nality becomes spread across multiple systems. New features may require coordinated \nreleases to multiple services and may become unacceptably coupled to others at the \nsame level of technical abstraction.\nA microservice architecture, on the other hand, should be biased toward vertical \ndecomposition; each service should align to a single business capability, encapsulating \nall relevant technical layers.\nNOTE     In rare instances, it might make sense to build a service that implements \na technical capability, such as integration with a third-party service, if multiple \nservices require it.\nYou also should be mindful of the consumers of your services. To ensure a stable \nsystem, you need to ensure you’re developing patiently and maintaining backwards \ncompatibility — whether explicitly or by running multiple versions of a service — to \nensure that you don’t force other teams to upgrade or break complex interactions \nbetween services.\nWorking with these five principles in mind will help you develop microservices well, \nleading to systems that are highly amenable to change, scalable, and stable.\n1.1.3\t\nWho uses microservices?\nMany organizations have successfully built and deployed microservices, across many \ndomains: in media (The Guardian); content distribution (SoundCloud, Netflix); \ntransport and logistics (Hailo, Uber); e-commerce (Amazon, Gilt, Zalando); banking \n(Monzo); and social media (Twitter).\n \n",
      "content_length": 2074,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "\t\n11\nWhat is a microservice application?\nMost of these companies took a monolith-first approach.1 They started by building \na single large application, then progressively moved to microservices in response to \ngrowth pressures they faced. These pressures are outlined in Table 1.1.\nTable 1.1    Pressures of growth on a software system\nPressure\nDescription\nVolume\nThe volume of activity that a system performs may outgrow the capacity of \noriginal technology choices.\nNew features\nNew features may not be cohesive with existing features, or different technol-\nogies may be better at solving problems.\nEngineering team growth\nAs a team grows larger, lines of communication increase. New developers \nspend more time comprehending the existing system and less time adding \nproduct value.\nTechnical debt\nIncreased complexity in a system — including debt from previous build  \ndecisions — increases the difficulty of making changes.\nInternational distribution\nInternational distribution may lead to data consistency, availability, and \nlatency challenges.\nFor example, Hailo wanted to expand internationally — which would’ve been chal-\nlenging with their original architecture — but also increase their pace of feature deliv-\nery.2 SoundCloud wanted to be more productive, as the complexity of their original \nmonolithic application was holding them back.3 Sometimes, the shift coincided with a \nchange in business priority: Netflix famously moved from physical DVD distribution to \ncontent streaming. Some of these companies completely decommissioned their origi-\nnal monolith. But for many, this is an ongoing process, with a monolith surrounded by \na constellation of smaller services.\nAs microservice architecture has been more widely popularized — and as early \nadopters have open sourced, blogged, and presented the practices that worked for \nthem — teams have increasingly begun greenfield projects using microservices, rather \nthan building a single application first. For example, Monzo started with microservices \nas part of its mission to build a better and more scalable bank.4 \n1\t Martin Fowler expands on this pattern: “MonolithFirst,” June 3, 2015,  http://martinfowler.com/\nbliki/MonolithFirst.html.\n2\t See Matt Heath, “A Long Journey into a Microservice World,” Medium, May 30, 2015, http://mng.bz/\nXAOG.\n3\t See Phil Calçado, “How we ended up with microservices,” September 8, 2015, http://mng.bz/Qzhi.\n4\t  See Matt Heath, “Building microservice architectures in Go,” June 18, 2015, http://mng.bz/9L83.\n \n",
      "content_length": 2517,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "12\nChapter 1  Designing and running microservices \n1.1.4\t\nWhy are microservices a good choice?\nPlenty of successful businesses are built on monolithic software — Basecamp,5 Stack-\nOverflow, and Etsy spring to mind. And in monolithic applications, a wealth of ortho-\ndox, long-established software development practice and knowledge exists. Why \nchoose microservices?\nTechnical heterogeneity leads to microservices\nIn some companies, technical heterogeneity makes microservices an obvious choice. \nAt Onfido, we started building microservices when we introduced a product driven by \nmachine learning — not a great fit for our original Ruby stack! Even if you’re not fully \ncommitted to a microservice approach, applying microservice principles gives you a \ngreater range of technical choices to solve business problems. Nevertheless, it’s not \nalways so clear-cut.\nDevelopment friction increases as complex systems grow\nIt comes down to the nature of complex systems. At the beginning of the chapter, we \nmentioned that software developers strive to craft effective and timely solutions to com-\nplex problems. But the software systems we build are inherently complex. No methodol-\nogy or architecture can eliminate the essential complexity at the heart of such a system.\nBut that’s no reason to get downhearted! You can ensure that the development \napproaches you take result in good complex systems, free from accidental complexity.\n Take a moment and consider what you’re trying to achieve as an enterprise software \ndeveloper. Dan North puts it well:\nThe goal of software development is to sustainably minimize lead time to positive business \nimpact.\nThe hard part in complex software systems is to deliver sustainable value in the face of \nchange: to continue to deliver with agility, pace, and safety even as the system becomes \nlarger and more complex. Therefore, we believe a good complex system is one where \ntwo factors are minimized throughout the system’s lifecycle: friction and risk.\nFriction and risk limit your velocity and agility, and therefore your ability to deliver \nbusiness impact. As a monolith grows, the following factors may lead to friction:\n¡ Change cycles are coupled together, leading to higher coordination barriers and \nhigher risk of regression.\n¡ Soft module and context boundaries invite chaos in undisciplined teams, lead-\ning to tight or unanticipated coupling between components.\n¡ Size alone can be painful: continuous integration jobs and releases — even local \napplication startup — become slower.\nThese qualities aren’t true for all monoliths, but unfortunately they’re true for most \nthat we’ve encountered. Likewise, these types of challenges are a common thread in \nthe stories of the companies we mentioned.\n5\t  David Heinemeier Hansson coined the term “Majestic Monolith” to describe how 37signals built \nBasecamp: Signal v. Noise, February 29, 2016, http://mng.bz/1p3I.\n \n",
      "content_length": 2919,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "\t\n13\nWhat is a microservice application?\nMicroservices reduce friction and risk\nMicroservices help reduce friction and risk in three ways:\n¡ Isolating and minimizing dependencies at build time\n¡ Allowing developers to reason about cohesive individual components, rather \nthan an entire system\n¡ Enabling the continuous delivery of small, independent changes\nIsolating and minimizing dependencies at build time — whether between teams or on \nexisting code — allows developers to move faster. Development can move in parallel, \nwith reduced long-term dependency on past decisions made in a monolithic applica-\ntion. Technical debt is naturally limited to service boundaries.\nMicroservices are individually easier to build and reason about than monolithic \napplications. This is beneficial for the productivity of development in a growing orga-\nnization. It also provides a compelling and flexible paradigm for coping with increased \nscale or smoothly introducing new technologies.\nSmall services are also a great enabler of continuous delivery. Deployments in large \napplications can be risky and involve lengthy regression and verification cycles. By \ndeploying smaller elements of functionality, you better isolate changes to your active \nsystem, reducing the potential risk of an individual deployment.\nAt this point, we can come to two conclusions:\n¡ Developing small, autonomous services can reduce friction in the development \nof long-running complex systems.\n¡ By delivering cohesive and independent pieces of functionality, you can build a \nsystem that’s malleable and resilient in the face of change, helping you to deliver \nsustainable business impact with reduced risk.\nThat doesn’t mean everyone should build microservices. It’d be wonderful if there \nwas an objective answer to the question “Do I need microservices?” but unfortunately \nyou can only say “It depends” — on your team, on your company, and on the nature of \nthe system you’re building. If the scope of your system is trivial, then it’s unlikely you’ll \ngain benefits that outweigh the added complexity of building and running this type of \nfine-grained application. But if you’ve faced any of the challenges we mentioned ear-\nlier in this section, then microservices are a compelling solution.\nA cautionary tale\nWe once heard a story about a microservice implementation gone wrong. The startup \nin question had begun to scale, and the CTO had decided that the only solution was \nto rebuild the application as microservices. If you’re not worried by that sentence, you \nshould be!\nThe engineering team set out to rebuild their application. This took them five months, \nduring which time they released zero new features, nor did they release any of their micro-\nservices to production. The team proceeded to launch their new microservice application \n \n",
      "content_length": 2826,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "14\nChapter 1  Designing and running microservices \nduring the busiest month for the business, causing absolute chaos and necessitating a \nrollback to the original monolith.\nThis type of migration gives microservices a bad name. Few businesses have the luxury \nof a feature freeze for several months nor can they indulge a big-bang launch of a new \narchitecture. Although the sample set is small, most successful microservice migrations \nthat we’ve observed have been piecemeal, balancing architectural vision with business \nneeds, priorities, and resource constraints. Although it’ll take longer and require more \nengineering effort, hopefully you’ll never recognize your team being mentioned in a cau-\ntionary tale!\n \n1.2\t\nWhat makes microservices challenging?\nLet’s dig a little deeper and explore the costs and complexity of designing and run-\nning microservices. Microservices aren’t the only architecture that have promised nir-\nvana through decomposition and distribution, but those past attempts, such as SOA,6 \nare widely considered unsuccessful. No technique is a silver bullet. For example, as \nwe’ve mentioned, microservices drastically increase the number of moving parts in a \nsystem. By distributing functionality and data ownership across multiple autonomous \nservices, you likewise distribute responsibility for stability and sane operation of your \napplication.\nYou’ll encounter many challenges when designing and running a microservice \napplication:\n¡ Scoping and identifying microservices requires substantial domain knowledge.\n¡ The right boundaries and contracts between services are difficult to identify and, \nonce you’ve established them, can be time-consuming to change.\n¡ Microservices are distributed systems and therefore require different assumptions \nto be made about state, consistency, and network reliability.\n¡ By distributing system components across networks, and increasing technical \nheterogeneity, microservices introduce new modes of failure.\n¡ It’s more challenging to understand and verify what should happen in normal \noperation.\n1.2.1\t\nDesign challenges\nHow do these challenges impact the design and runtime phases of microservice devel-\nopment? Earlier we introduced the five key principles underlying microservice develop-\nment. The first of those was autonomy. For your services to be autonomous, you need to \ndesign them such that, together, they’re loosely coupled, and, individually, they encap-\nsulate highly cohesive elements of functionality. This is an evolutionary process. The \n6\t SOA is a wooly term. Although many principles of SOA are similar to microservices, the definition \nof the former is inextricably associated with heavyweight, enterprise vendor tools, such as ESBs.\n(continued)\n \n",
      "content_length": 2748,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "\t\n15\nWhat makes microservices challenging?\nscope of your services may change over time, and you’ll often choose to carve out new \nfunctionality from — or even retire — existing services.\nMaking those choices is challenging, and even more so at the start of developing \nan application! The primary driver of loose coupling is the boundaries you establish \nbetween services; getting those wrong will lead to services that are resistant to change \nand, overall, a less malleable and flexible application.\nScoping microservices requires domain knowledge \nEach microservice is responsible for a single capability. Identifying these capabilities \nrequires knowledge of the business domain of your application. Early in an application’s \nlifetime, your domain knowledge might be at best incomplete, or at worst, incorrect.\nInadequate understanding of your problem domain can result in poor design \nchoices. In a microservice application, the increased rigidity of a service boundary  \nwhen compared to a module within a monolithic application means the downstream \ncost of poor scoping decisions is likely to be higher:\n¡ You may need to refactor across multiple distinct codebases.\n¡ You may need to migrate data from one service’s database to another.\n¡ You may not have identified implicit dependencies between services, which could \nlead to errors or incompatibility on deployment.\nThese activities are illustrated in figure 1.5.\nDatabase\nConsumers\nConsumers\nService A\nService A\nService B\nService B\nDatabase\nService A depends on closely related functionality\nin Service B.\nRefactoring this relationship requires the\ncoordination of multiple changes.\nData needs to be\nmigrated.\nFunctionality is moved\nto new services.\nConsumers need to\nmove to a new\nservice.\nDatabase\nDatabase\nFigure 1.5    Incorrect service scoping decisions may require complex and costly refactoring across \nservice boundaries.\n \n",
      "content_length": 1896,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "16\nChapter 1  Designing and running microservices \nBut making design decisions based on insufficient domain knowledge is hardly \nunique to microservices! The difference is in the impact of those decisions.\nNOTE     In chapters 2 and 4, we’ll discuss best practices for identifying and scop-\ning services, using an example application.\nMaintaining contracts between services\nEach microservice should be independent of the implementation of other services. \nThis enables technical heterogeneity and autonomy. For this to work, each microser-\nvice should expose a contract  — analogous to an interface in object-oriented design —  \ndefining the messages it expects to receive and respond with. A good contract should be\n¡ Complete  — Defines the full scope of an interaction\n¡ Succinct  — Takes in no more information than is necessary, so that consumers \ncan construct messages within reasonable bounds\n¡ Predictable  — Accurately reflects the real behavior of any implementation\nAnyone who’s designed an API might know how hard these properties are to achieve. \nContracts become the glue between services. Over time, contracts may need to evolve \nwhile also needing to maintain backwards compatibility for existing collaborators. \nThese twin tensions — between stability and change — are challenging to navigate.\nMicroservice applications are designed by teams\nIn larger organizations, it’s likely that multiple teams will build and run a microser-\nvice application, each taking responsibility for different microservices. Each team may \nhave its own goals, way of working, and delivery lifecycle. It can be difficult to design a \ncohesive system when you also need to reconcile the timelines and priorities of other \nindependent teams. Coordinating the development of any substantial microservice \napplication therefore will require the agreement and reconciliation of priorities and \npractices across multiple teams.\nMicroservice applications are distributed systems\nDesigning microservice applications means designing distributed systems. Many falla-\ncies occur in the design of distributed systems,7 including\n¡ The network is reliable.\n¡ Latency is zero.\n¡ Bandwidth is infinite.\n¡ Transport cost is zero.\nClearly, assumptions you might make in nondistributed systems — such as the speed \nand reliability of method calls — are no longer appropriate and can lead to poor, unsta-\nble implementation. You must consider latency, reliability, and the consistency of state \nacross your application.\n7\t See Arnon Rotem-Gal-Oz, “Fallacies of Distributed Computing Explained,” https://pages.cs.wisc \n.edu/~zuyu/files/fallacies.pdf.\n \n",
      "content_length": 2629,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "\t\n17\nWhat makes microservices challenging?\nOnce the application is distributed — where the application’s underlying state \ndata is spread across a multitude of places — consistency becomes challenging. You \nmay not have guarantees of the order of operations. It won’t be possible to maintain \nACID-like transactional guarantees when actions take place across multiple services. \nThis will affect design at the application level: you’ll need to consider how a service \nmight operate in an inconsistent state and how to roll back in the event of transaction \nfailure.\n1.2.2\t\nOperational challenges\nA microservice approach will inherently multiply the possible points of failure in a \nsystem. To illustrate this, let’s return to the investment tool we mentioned earlier. Fig-\nure 1.6 identifies possible points of failure in this application. You can see that some-\nthing could go wrong in multiple places, and that could affect the normal processing \nof an order.\nConsider the questions you might need to answer when this application is in \nproduction:\n¡ If something goes wrong and your user’s order isn’t placed, how would you deter-\nmine where the fault occurred?\n¡ How do you deploy a new version of a service without affecting order placement?\n¡ How do you know which services were meant to be called?\n¡ How do you test that this behavior is working correctly across multiple services?\n¡ What happens if a service is unavailable?\nRather than eliminating risk, microservices move that cost to later in the lifecycle of \nyour system: reducing friction in development but increasing the complexity of how \nyou deploy, verify, and observe your application in operation.\nUser\nOverload\nService instances become\nsaturated with requests and fail\nto respond or pass timeout limits.\nNetwork/routing failure\nNetwork issues cause request routing\nbetween users/services/dependencies to fail.\nHardward failure\nThe hardware running the\ndatabase or service instances\nfails.\nDownstream failure\nService dependencies may fail\nor respond slowly.\nThird party failure\nRequests to third party\ndependencies may fail.\nThird-party\nproviders\nOther\nmicroservices\nDatabase\nOrder service\nFigure 1.6    Possible points of failure when placing a sell order\n \n",
      "content_length": 2230,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "18\nChapter 1  Designing and running microservices \nA microservices approach suggests an evolutionary approach to system design: you \ncan add new features independently without changing existing services. This minimizes \nthe cost and risk of change.\nBut in a decoupled system that constantly changes, it can be extremely difficult to \nkeep track of the big picture, which makes issue diagnosis and support more challeng-\ning. When something goes wrong, you need to have some way of tracing how the system \ndid behave (what services it called, in which order, and what the outcome was), but you \nalso need some way of knowing how the system should have behaved.\nUltimately, you face two operational challenges in microservices: observability and \nmultiple points of failure. Let’s focus on each of those in turn.\nObservability is difficult to achieve\nWe touched on the importance of transparency back in section 1.1.2. But why is it \nharder in microservice applications? It’s harder because you need to understand the \nbig picture. You need to assemble that big picture from multiple jigsaw pieces, to cor-\nrelate and link together the data each service produces to ensure you understand what \neach service does within the wider context of delivering some business output. Indi-\nvidual service logs provide a partial view of system operation, which is helpful, but you \nneed to use both a microscope and a wide-angle lens to understand the system in full.\nLikewise, because you’re running multiple applications, depending on how you \nchoose to deploy them, a less obvious correlation may exist between underlying infra-\nstructural metrics — like memory and CPU usage — and the application. These metrics \nare still useful but are less of a focus than they might be in a monolithic system.\nMultiplying services multiplies points of failure\nWe’re probably not being too pessimistic if we say that everything that can fail will fail. \nIt’s important that you start with that mindset: if you assume weakness and fragility \nin the multiple services forming your system, that can better inform how you design, \ndeploy, and monitor that system — rather than getting too surprised when something \ndoes go wrong.\nYou need to consider how your system will continue operating despite the failures \nof individual components. This implies that, individually, services will need to become \nmore robust — considering error checking, failover, and recovery — but also that the \nwhole system should act reliably, even when individual components are never 100% \nreliable.\n1.3\t\nMicroservice development lifecycle\nAt an individual level, each microservice should look familiar to you — even if it’s a bit \nsmaller. To build a microservice, you’ll use many of the same frameworks and tech-\nniques that you’d normally apply in building an application: web application frame-\nworks, SQL databases, unit tests, libraries, and so on.\nAt a system level, choosing a microservice architecture will have a significant impact \non how you design and run your application. Throughout this book, we’ll focus on these \n \n",
      "content_length": 3085,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "\t\n19\nMicroservice development lifecycle\nthree key stages in the development lifecycle of a microservice application: designing \nservices, deploying them to production, and observing their behavior. This cycle is illus-\ntrated in figure 1.7.\nMaking well-reasoned decisions in each of these three stages will help you build \napplications that are resilient, even in the face of changing requirements and increas-\ning complexity. Let’s walk through each stage and consider the steps you’ll take to \ndeliver an application with microservices.\n1.3.1\t\nDesigning microservices\nYou’ll need to make several design decisions when building a microservice application \nthat you wouldn’t have encountered building monolithic apps. The latter often fol-\nlow well-known patterns or frameworks, such as three-tier architecture or model-view \ncontroller (MVC). But techniques for designing microservices are still in their relative \ninfancy. You’ll need to consider\n¡ Whether to start with a monolith or commit to microservices up front\n¡ The overall architecture of your application and the façade it presents to outside \nconsumers\n¡ How to identify and scope the boundaries of your services\n¡ How your services communicate with each other, whether synchronously or \nasynchronously\n¡ How to achieve resiliency in services\nThat’s quite a lot of ground to cover. For now, we’ll touch on each of these consid-\nerations so you can see why paying attention to all of them is vital to a well-designed \nmicroservice application.\nMonolith first?\nYou’ll find two opposing trends to starting with microservices: monolith first or micro-\nservices only. Advocates of the former reason that you should always start with a mono-\nlith, as you won’t understand the component boundaries in your system at an early \nstage, and the cost of getting these wrong is much higher in a microservice application. \nOn the other hand, the boundaries you choose in a monolith aren’t necessarily the \nsame ones you’d choose in a well-designed microservice application.\nDesign\nDeploy\nObserve\nFigure 1.7    The key iterative stages — design, deploy, and observe — in the microservice development \nlifecycle\n \n",
      "content_length": 2161,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "20\nChapter 1  Designing and running microservices \nAlthough the speed of development may be slower to begin with, microservices will \nreduce friction and risk in future development. Likewise, as tooling and frameworks \nmature, microservices best practice is becoming increasingly less daunting to pick \nup. Either way you want to go, the advice in this book should be useful, regardless of \nwhether you’re thinking of migrating away from your monolith or starting afresh.\nScoping services\nChoosing the right level of responsibility for each service — its scope — is one of the \nmost difficult challenges in designing a microservice application. You’ll need to model \nservices based on the business capabilities they provide to an organization.\nLet’s extend the example from the beginning of this chapter. How might your ser-\nvices change if you wanted to introduce a new, special type of order? You have three \noptions to solve this problem (figure 1.8):\n1\t Extend the existing service interface\n2\t Add a new service endpoint\n3\t Add a new service\nEach of these options has pros and cons that will impact the cohesiveness and coupling \nbetween services in your application.\nOrders service exposes an\noperation to create an order\nTo support a new type of order, we could...\nOrders service\nOrders service\nOrders service\nOrders service\nSpecial order\nservice\nAdd a new service for the new order type\nAlter the operation contract to\naccept new fields\nAdd a new operation to the\nexisting orders service\nFigure 1.8. To scope functionality, you need to make decisions about whether capabilities belong in \nexisting services or if you need to design new services.\n \n",
      "content_length": 1656,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "\t\n21\nMicroservice development lifecycle\nNOTE     In chapters 2 and 4, we’ll explore service scoping and how to make opti-\nmal decisions about service responsibility.\nCommunication\nCommunication between services may be asynchronous or synchronous. Although \nsynchronous systems are easier to reason through, asynchronous systems are highly \ndecoupled — reducing the risk of change — and potentially more resilient. But the \ncomplexity of such a system is high. In a microservice application, you need to balance \nsynchronous and asynchronous messaging to choreograph and coordinate the actions \nof multiple microservices effectively.\nResiliency\nIn a distributed system, a service can’t trust its collaborators, not necessarily because \nthey’re coded poorly or because of human error, but because you can’t safely assume \nthe network between or behavior of those services is reliable or predictable. Services \nneed to be resilient in the face of failure. To achieve this, you need to design your \nservices to work defensively by backing off in the event of errors, limiting request rates \nfrom poor collaborators, and dynamically finding healthy services.\n1.3.2\t\nDeploying microservices\nDevelopment and operations must be closely intertwined when building microservices. \nIt’s not going to work if you build something and throw it over the fence for someone \nelse to deploy and operate it. In a system composed of numerous, autonomous ser-\nvices, if you build it, you should run it. Understanding how your services run will in \nturn help you make better design decisions as your system grows. \nRemember, what’s special about your application is the business impact it deliv-\ners. That emerges from collaboration between multiple services. In fact, you could \nstandardize or abstract away anything outside of the unique capability each service \noffers — ensuring teams are focused on business value. Ultimately, you should reach \na stage where there’s no ceremony involved in deploying a new service. Without this, \nyou’ll invest all your energy in plumbing, rather than creating value for customers.\nIn this book, we’ll teach you how to construct a reliable road to production for exist-\ning and new services. The cost of deploying new services must be negligible to enable \nrapid innovation. Likewise, you should standardize this process to simplify system oper-\nation and ensure consistency across services. To achieve this, you’ll need to\n¡ Standardize microservice deployment artifacts\n¡ Implement continuous delivery pipelines\nWe’ve heard reliable deployment described as boring, not in the sense that it’s unex-\nciting, but that it’s incident-free. Unfortunately, we’ve seen too many teams where the \nopposite is true: deploying software is stressful and encourages unhealthy all-hands-on-\ndeck behavior. This is bad enough for one service — if you’re deploying any number \n \n",
      "content_length": 2880,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "22\nChapter 1  Designing and running microservices \nof services, the anxiety alone will drive you mad! Let’s look at how these steps lead to \nstable and reliable microservice deployments. \nStandardize microservice deployment artifacts\nIt often seems like every language and framework has its own deployment tool. Python \nhas Fabric, Ruby has Capistrano, Elixir has exrm, and so on. And then the deployment \nenvironment itself is complex: \n¡ What server does an application run on? \n¡ What are the application’s dependencies on other tools? \n¡ How do you start that application? \nAt runtime, an application’s dependencies (figure 1.9) are broad and might include \nlibraries, binaries and OS packages (such as ImageMagick or libc), and OS processes \n(such as cron or fluentd).\nTechnically, heterogeneity is a fantastic benefit of service autonomy. But it doesn’t \nmake life easy for deployment. Without consistency, you won’t be able to standardize \nyour approach to taking services to production, which increases the cost of managing \ndeployments and introducing new technology. At worst, each team reinvents the wheel, \ncoming up with different approaches for managing dependencies, packing builds, get-\nting them onto servers, and operating the application itself.\nAn application exposes an\noperational API.\nRestart\nStart\nApplication\nSupporting\nprocesses,\nfor example,\nlogging,\ncron\nOperating system\nApplication libraries\nBinary dependencies, for\nexample, ImageMagick\nStop\nAn application has multiple\npoints of explicit and implicit\ndependency.\nFigure 1.9    An application exposes an operational API and has many types of dependencies, including \nlibraries, binary dependencies, and supporting processes.\n \n",
      "content_length": 1708,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "\t\n23\nMicroservice development lifecycle\nOur experience suggests the best tools for this job are containers. A container is an \noperating system-level virtualization method that supports running isolated sys-\ntems on a host, each with its own network and process space, sharing the same ker-\nnel. A container is quicker to build and quicker to start up than a virtual machine \n(seconds, rather than minutes). You can run multiple containers on one machine, \nwhich simplifies local development and can help to optimize resource usage in cloud \nenvironments.\nContainers standardize the packaging of an application, and the runtime interface \nto it, and provide immutability of both operating environment and code. This makes \nthem powerful building blocks for higher level composition. By using them, you can \ndefine and isolate the full execution environment of any service.\nAlthough many implementations of containers are available (and the concept exists \noutside of Linux, such as jails in FreeBSD and zones in Solaris), the most mature and \napproachable tooling that we’ve used so far is Docker. We’ll use that tool later in this book.\nImplement continuous delivery pipelines\nContinuous delivery is a practice in which developers produce software that they \ncan reliably release to production at any time. Imagine a factory production line: \nto continuously deliver software, you build similar pipelines to take your code from \ncommit to live operation. Figure 1.10 illustrates a simple pipeline. Each stage of \nthe pipeline provides feedback to the development team on the correctness of \ntheir code.\nEarlier, we mentioned that microservices are an ideal enabler of continuous \ndelivery because their smaller size means you can develop them quickly and release \nthem independently. But continuous delivery doesn’t automatically follow from \ndeveloping microservices. To continuously deliver software, you need to focus on \ntwo goals:\n¡ Building a set of validations that your software has to pass through. At each \nstage of your deployment process, you should be able to prove the correctness \nof your code.\n¡ Automating the pipeline that delivers your code from commit to production. \nCode commit\nBuild\nUnit test\nPackage\nProduction\nQuality uncertain\nDeployment pipeline\nQuality proven\nIntegration\ntest\nAcceptance\ntest\nFigure 1.10    A high-level deployment pipeline for a microservice\n \n",
      "content_length": 2391,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "24\nChapter 1  Designing and running microservices \nBuilding a provably correct deployment pipeline will allow developers to work safely \nand at pace as they iteratively develop services. Such a pipeline is a repeatable and reli-\nable process for delivering new features. Ideally, you should be able to standardize the \nvalidations and steps in your pipeline and use them across multiple services, further \nreducing the cost of deploying new services. \nContinuous delivery also reduces risk, because the quality of the software produced \nand the team’s agility in delivering changes are both increased. From a product per-\nspective, this may mean you can work in a leaner fashion — rapidly validating your \nassumptions and iterating on them.\nNOTE     In part 3, we’ll build a continuous delivery pipeline using the Pipe-\nline feature of the freely available Jenkins continuous integration tool. We’ll \nalso explore different deployment patterns, such as canaries and blue-green \ndeployments.\n1.3.3\t\nObserving microservices\nWe’ve discussed transparency and observability throughout this chapter. In produc-\ntion, you need to know what’s going on. The importance of this is twofold: \n¡ You want to proactively identify and refactor fragile implementation in your \nsystem.\n¡ You need to understand how your system is behaving. \nThorough monitoring is significantly more difficult in a microservice application \nbecause single transactions may span multiple distinct services; technically heteroge-\nneous services might produce data in irreconcilable formats; and the total volume of \noperational data is likely to be much higher than that of a single monolithic applica-\ntion. But if you’re able to understand how your system operates — and observe that \nclosely — despite this complexity, you’ll be better placed to make effective changes to \nyour system.\nIdentify and refactor potentially fragile implementation\nSystems will fail, whether because of bugs introduced, runtime errors, network failures, \nor hardware problems.8 Over time, the cost of eliminating unknown bugs and errors \nbecomes higher than the cost of being able to react quickly and effectively when they \noccur.\nMonitoring and alerting systems allow you to diagnose problems and determine \nwhat causes failures. You may have automated mechanisms reacting to the alerts that’ll \nspawn new container instances in different data centers or react to load issues by \nincreasing the number of running instances of a service.\nTo minimize the consequences of those failures, and prevent them cascading through-\nout the system, you need to be able to architect dependencies between services in ways \n8\t You even have to watch out for squirrels: Rich Miller, “Surviving Electric Squirrels and UPS Fail-\nures,” DataCenter Knowledge, July 9, 2012, http://mng.bz/rmbF.\n \n",
      "content_length": 2823,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "\t\n25\nMicroservice development lifecycle\nthat’ll allow for partial degradation. One service going down shouldn’t bring down the \nwhole application. It’s important to think about the possible failure points of your appli-\ncations, recognize that failure will always happen, and prepare accordingly.\nUnderstand behavior across hundreds of services\nYou need to prioritize transparency in design and implementation to understand \nbehavior across your services. Collecting logs and metrics — and unifying them for ana-\nlytical and alerting purposes — allows you to build a single source of truth to resort to \nwhen monitoring and investigating the behavior of your system.\nAs we mentioned in section 1.3.2, you can standardize and abstract anything outside \nof the unique capability each service offers. You can think of each service as an onion. \nAt the center of that onion, you have the unique business capability offered by that \nservice. Surrounding that, you have layers of instrumentation — business metrics, appli-\ncation logs, operational metrics, and infrastructure metrics — that make that capability \nobservable. You can then trace each request to the system through these layers. You’d \nthen push the data you collected from these layers to an operational data store for ana-\nlytics and alerting. This is illustrated in figure 1.11.\nNOTE    In part 4 of this book, we’ll discuss how to build a monitoring system \nfor microservices, collect appropriate data, and use that data to produce a live \nmodel for a complex microservice application.\nRequests\nResponses\nInfrastructure metrics\nOperational metrics\nApplication logs\nBusiness metrics\nBusiness\ncapability\nOperational data store\n \nFigure 1.11    A business capability microservice surrounded by layers of instrumentation, through which \npass requests to the microservice and its responses, with data collected from the process going to an \noperational data store\n \n",
      "content_length": 1923,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "26\nChapter 1  Designing and running microservices \n1.4\t\nResponsible and operationally aware engineering \nculture\nIt’d be a mistake to examine the technical nature of microservices in isolation from \nhow an engineering team works to develop them. Building an application out of small, \nindependent services will drastically change how an organization approaches engi-\nneering, so guiding the culture and priorities of your team will be a significant factor in \nwhether you successfully deliver a microservice application.\nIt can be difficult to separate cause and effect in organizations that have successfully \nbuilt microservices. Was the development of fine-grained services a logical outcome of \ntheir organizational structure and the behavior of their teams? Or did that structure \nand behavior arise from their experiences building fine-grained services?\nThe answer is a bit of both. A long-running system isn’t only an accumulation of \nfeatures requested, designed, and built. It also reflects the preferences, opinions, and \nobjectives of its builders and operators. Conway’s Law expresses this to some degree:\norganizations which design systems ... are constrained to produce designs which are copies \nof the communication structures of these organizations.\n“Constrained” might suggest that these communication structures will limit and con-\nstrict the effective development of a system. In fact, microservices practice implies the \nopposite: that a powerful way to avoid friction and tension in building systems is to \ndesign an organization in the shape of the system you intend to build.\nDeliberate symbiosis with organizational structure is one example of common \nmicroservices practice. To be able to realize benefits from microservices and adequately \nmanage their complexity, you need to develop working principles and practices that are \neffective for that type of application, rather than using the same techniques that you \nused to build monoliths.\nSummary\n¡ Microservices are both an architectural style and a set of cultural practices, \nunderpinned by five key principles: autonomy, resilience, transparency, automa-\ntion, and alignment.\n¡ Microservices reduce friction in development, enabling autonomy, technical \nflexibility, and loose coupling.\n¡ Designing microservices can be challenging because of the need for adequate \ndomain knowledge and balancing priorities across teams.\n¡ Services expose contracts to other services. Good contracts are succinct, com-\nplete, and predictable. \n¡ Complexity in long-running software systems is unavoidable, but you can deliver \nvalue sustainably in these systems if you make choices that minimize friction and risk.\n¡ Reliably incident-free (“boring”) deployment reduces the risk of microservices \nby making releases automated and provable.\n \n",
      "content_length": 2808,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "\t\n27\nSummary\n¡ Containers abstract away differences between services at runtime, simplifying \nlarge-scale management of heterogeneous microservices.\n¡ Failure is inevitable: microservices need to be transparent and observable for \nteams to proactively manage, understand, and own service operation ... and the \nlack thereof.\n¡ Teams adopting microservices need to be operationally mature and focus on the \nentire lifecycle of a service, not only on the design and build stages.\n \n",
      "content_length": 480,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "28\n2\nMicroservices at \nSimpleBank\nThis chapter covers\n¡ Introducing SimpleBank, a company adopting \nmicroservices\n¡ Designing a new feature with microservices\n¡ How to expose microservice-based features to \nthe world\n¡ Ensuring features are production ready\n¡ Challenges faced in scaling up microservice \ndevelopment\nIn Chapter 1, you learned about the key principles of microservices and why they’re \na compelling approach for sustainably delivering software value. We also introduced \nthe design and development practices that underpin microservices development. In \nthis chapter, we’ll explore how you can apply those principles and practices to devel-\noping new product features with microservices.\nOver the course of this chapter, we’ll introduce the fictitious company of \nSimpleBank. They’re a company with big plans to change the world of investment, \nand you’re working for them as an engineer. The engineering team at Simple-\nBank wants to be able to deliver new features rapidly while ensuring scalability and \n \n",
      "content_length": 1024,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "\t\n29\nWhat does SimpleBank do?\nstability — after all, they’re dealing with people’s money! Microservices might be exactly \nwhat they need.\nBuilding and running an application made up of independently deployable and \nautonomous services is a vastly different challenge from building that application as \na single monolithic unit. We’ll begin by considering why a microservice architecture \nmight be a good fit for SimpleBank and then walk you through the design of a new fea-\nture using microservices. Finally, we’ll identify the steps needed to develop that proof of \nconcept into a production-grade application. Let’s get started.\n2.1\t\nWhat does SimpleBank do?\nThe team at SimpleBank wants to make smart financial investment available to every-\none, no matter how much money they have. They believe that buying shares, selling \nfunds, or trading currency should be as simple as opening a savings account.\nThat’s a compelling mission, but not an easy one. Financial products have multiple \ndimensions of complexity: SimpleBank will need to make sense of market rules and \nintricate regulations, as well as integrate with existing industry systems, all while meet-\ning stringent accuracy requirements.\nIn the previous chapter, we identified some of the functionality that SimpleBank \ncould offer its customers: opening accounts, managing payments, placing orders, and \nmodeling risk. Let’s expand on those possibilities and look at how they might fit within \nthe wider domain of an investment tool. Figure 2.1 illustrates the different elements of \nthis domain.\nAs the figure shows, an investment tool will need to do more than offer customer- \nfacing features, like the ability to open accounts and manage a financial portfolio. It \nalso will need to manage custody, which is how the bank holds assets on behalf of cus-\ntomers and moves them in or out of their possession, and manufacture, which is the \ncreation of financial products appropriate to customer needs.\nAccount\nmanagement\nCustomer management\nManaging accounts, tax,\nregulatory requirements\nCustody\nUnderlying banking services and\ninteraction with other market entities,\nfor example, brokerages, other banks\nProduct manufacture\nThe development and maintenance\nof financial products, for example,\nfunds consisting of other funds\nPortfolio\nreporting\nFinancial\npredictions and\nadvice\nFees\nTax\nOrder placement\nTrade &\nmarket\nexecution\nAsset transfers\nPayment\nprocessing\nCorporate actions,\nfor example,\ndividends\nOwnership\n& custody\nMarket data,\nfor example,\nshare prices\nAggregated\ntrading\nTax\nBulk orders\nRisk analysis\nPricing\nOwnership\nFigure 2.1    A high-level (and by no means exhaustive) model of functionality that SimpleBank might build\n \n",
      "content_length": 2704,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "30\nChapter 2  Microservices at SimpleBank \nAs you can see, it’s not so simple! You can begin to see some of the business capabilities \nthat SimpleBank might implement: portfolio management, market data integrations, \norder management, fund manufacture, and portfolio analysis. Each of the business \nareas identified might consist of any number of services that collaborate with each \nother or services in other areas.\nThis type of high-level domain model is a useful first step when approaching any sys-\ntem, but it’s crucial when building microservices. Without understanding your domain, \nyou might make incorrect decisions about the boundaries of your services. You don’t \nwant to build services that are anemic — existing only to perform trivial create, read, update, \ndelete (CRUD) operations. These often become a source of tight coupling within an application. At \nthe same time, you want to avoid pushing too much responsibility into a single service. Less cohesive \nservices make software changes slower and riskier — exactly what you’re trying to avoid.\nLastly, without this perspective, you might fall prey to overengineering — choosing \nmicroservices where they’re not justified by the real complexity of your product or domain.\n2.2\t\nAre microservices the right choice?\nThe engineers at SimpleBank believe that microservices are the best choice to tackle \nthe complexity of their domain and be flexible in the face of complex and changing \nrequirements. They anticipate that as their business grows, microservices will reduce the \nrisk of individual software changes, leading to a better product and happier customers.\nAs an example, let’s say they need to process every buy or sell transaction to calculate \ntax implications. But tax rules work differently in every country — and those rules tend \nto change frequently. In a monolithic application, you’d need to make coordinated, \ntime-sensitive releases to the entire platform, even if you only wanted to make changes \nfor one country. In a microservice application, you could build autonomous tax-han-\ndling services (whether by country, type of tax, or type of account) and deploy changes \nto them independently.\nIs SimpleBank making the right choice? Architecting software always involves ten-\nsion between pragmatism and idealism — balancing product needs, the pressures of \ngrowth, and the capabilities of a team. Poor choices may not be immediately apparent, \nas the needs of a system vary over its lifetime. Table 2.1 expands on the factors to con-\nsider when choosing microservices.\nTable 2.1    Factors to consider when choosing a microservice architecture\nFactor\nImpact\nDomain complexity\nIt’s difficult to objectively evaluate the complexity of a domain, but microservices \ncan address complexity in systems driven by competing pressures, such as regu-\nlatory requirements and market breadth.\nTechnical requirements\nYou can build different components of a system using different programming \nlanguages (and associated technical ecosystems). Microservices enable hetero-\ngeneous technical choices.\n \n",
      "content_length": 3074,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "\t\n31\nAre microservices the right choice?\nFactor\nImpact\nOrganizational growth\nRapidly growing engineering organizations may benefit from microservices \nbecause lowering dependency on existing codebases enables rapid ramp-up \nand productivity for new engineers.\nTeam knowledge\nMany engineers lack experience in microservices and distributed systems. If the \nteam lacks confidence or knowledge, it may be appropriate to build a proof-of-\nconcept microservice before fully committing to implementation.\nUsing these factors, you can evaluate whether microservices will help you deliver sus-\ntainable value in the face of increasing application complexity.\n2.2.1\t\nRisk and inertia in financial software\nLet’s take a moment to look at how SimpleBank’s competitors build software. Most \nbanks aren’t ahead of the curve in terms of technological innovation. There’s an ele-\nment of inertia that’s typical of larger organizations, although that’s not unique to the \nfinance industry. Two primary factors limit innovation and flexibility:\n¡ Aversion to risk  — Financial companies are heavily regulated and tend to build \ntop-down systems of change control to avoid risk by limiting the frequency and \nimpact of software changes.\n¡ Reliance on complex legacy systems  — Most core banking systems were built pre-1970. \nIn addition, mergers, acquisitions, and outsourcing have led to software systems \nthat are poorly integrated and contain substantial technical debt.\nBut limiting change and relying on existing systems hasn’t prevented software prob-\nlems from leading to pain for customers or the finance companies themselves. The \nRoyal Bank of Scotland was fined £56 million in 2014 when an outage caused payments \nto fail for 6.5 million customers. That’s on top of the £250 million it was already spend-\ning every year on its IT systems.1\nThat approach also hasn’t led to better products. Financial technology startups, such \nas Monzo and Transferwise, are building features at a pace most banks can only dream of.\n2.2.2\t\nReducing friction and delivering sustainable value\nCan you do any better? By any measure, the banking industry is a complex and com-\npetitive domain. A bank needs to be both resilient and agile, even when the lifetime of \na banking system is measured in decades. The increasing size of a monolithic applica-\ntion is antithetical to this goal. If a bank wants to launch a new product, it shouldn’t be \n1\t See Sean Farrell and Carmen Fishwick, “RBS could take until weekend to make 600,000 missing \npayments after glitch,” The Guardian, June 17, 2015, http://mng.bz/kxQY, and Chad Bray, “Royal \nBank of Scotland Fined $88 Million Over Technology Failure,” Dealbook, The New York Times, No-\nvember 20, 2014, http://mng.bz/hn8D.\nTable 2.1    Factors to consider when choosing a microservice architecture  (continued)\n \n",
      "content_length": 2831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "32\nChapter 2  Microservices at SimpleBank \nbogged down by the legacy of previous builds2 or require outsize effort and investment \nto prevent regression in existing functionality.\nA well-designed microservice architecture can solve these challenges. As we estab-\nlished earlier, this type of architecture avoids many of the characteristics that, in mono-\nlithic applications, slow velocity in development. Individual teams can move forward \nwith increased confidence as\n¡ Change cycles are decoupled from other teams.\n¡ Interaction between collaborating components is disciplined.\n¡ Continuous delivery of small, isolated changes limits the risk of breaking \nfunctionality.\nThese factors reduce friction in the development of a complex system but maintain \nresiliency. As such, they reduce risk without stifling innovation through bureaucracy.\nThis isn’t only a short-term solution. Microservices aid engineering teams in deliv-\nering sustainable value throughout the lifecycle of an application by placing natural \nbounds on the conceptual and implementation complexity of individual components.\n2.3\t\nBuilding a new feature\nNow that we’ve established that microservices are a good choice for SimpleBank, \nlet’s look at how it might use them to build new features. Building a minimum via-\nble product — an MVP — is a great first step to ensure that a team understands the \nconstraints and requirements of the microservices style. We’ll start by exploring one \nof the features that SimpleBank needs to build and the design choices the team will \nmake, working through the lifecycle we illustrated in chapter 1 (figure 2.2).\nIn chapter 1, we touched on how services might collaborate to place a sell order. An \noverview of this process is shown in figure 2.3.\nLet’s look at how you’d approach building this feature. You need to answer several \nquestions:\n¡ Which services do you need to build?\n¡ How do those services collaborate with each other?\n¡ How do you expose their functionality to the world?\nDesign\nDeploy\nObserve\nFigure 2.2    The key iterative stages — design, deploy, and observe — in the microservice development \nlifecycle\n2\t How bad can it get? I once encountered a financial software company that maintained over 10 \ndistinct monolithic codebases, each surpassing 2 million lines of code!\n \n",
      "content_length": 2305,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "\t\n33\nBuilding a new feature\nUser requests\nsale of stock\nIs stock\navailable?\nStock is\nreserved\nagainst\naccount\nWait to place\norder\nNo\nMarket open?\nYes\nPlace order to\nmarket\nFee is charged\nto account\nYes\nNo\nFigure 2.3    The process of placing an order to sell a financial position from an account at SimpleBank\nThese may be similar to the questions you might ask yourself when designing a feature \nin a monolithic application, but they have different implications. For example, the \neffort required to deploy a new service is inherently higher than creating a new mod-\nule. In scoping microservices, you need to ensure that the benefits of dividing up your \nsystem aren’t outweighed by added complexity.\nNOTE    As the application evolves, these questions will take on added dimen-\nsions. Later, we’ll also ask whether to add functionality to existing services or \ncarve those services up. We’ll explore this further in chapters 4 and 5.\nAs we discussed earlier, each service should be responsible for a single capability. Your \nfirst step will be to identify the distinct business capabilities you want to implement and \nthe relationship between those capabilities.\n2.3.1\t\nIdentifying microservices by modeling the domain\nTo identify the business capabilities you want, you need to develop your understand-\ning of the domain where you’re building software. This is normally the hard work of \nproduct discovery or business analysis: research; prototyping; and talking to customers, \ncolleagues, or other end users.\nLet’s start by exploring the order placement example from figure 2.3. What value are \nyou trying to deliver? At a high level, a customer wants to be able to place an order. So, \nan obvious business capability will be the ability to store and manage the state of those \norders. This is your first microservice candidate.\nContinuing our exploration of the example, you can identify other functionalities \nyour application needs to offer. To sell something, you need to own it, so you need some \nway of representing a customer’s current holdings resulting from the transactions that \nhave occurred against their account. Your system needs to send an order to a broker —  \nthe application needs to be able to interact with that third party. In fact, this one feature, \n \n",
      "content_length": 2281,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "34\nChapter 2  Microservices at SimpleBank \nplacing a sell order, will require SimpleBank’s application to support all of the follow-\ning functionality:\n¡ Record the status and history of sell orders\n¡ Charge fees to the customer for placing an order\n¡ Record transactions against the customer’s account\n¡ Place an order onto a market\n¡ Provide valuation of holdings and order to customer\nIt’s not a given that each function maps to a single microservice. You need to deter-\nmine which functions are cohesive — they belong together. For example, transactions \nresulting from orders will be similar to transactions resulting from other events, such \nas dividends being paid on a share. Together, a group of functions forms a capability \nthat one service may offer. \nLet’s map these functions to business capabilities — what the business does. You can \nsee this mapping in figure 2.4. Some functions cross multiple domains, such as fees.\nOrder management\nRecording status and\nhistory of orders\nFunction\nBusiness capabilities\nPlacing an order to\nmarket\nCharging a fee\nRecording transactions\nagainst customer\naccount\nValue positions held in\nan account\nMarket execution\nFees\nTransaction ledger\nMarket data\nFigure 2.4    The relationship between application functionality and capabilities within SimpleBank’s \nbusiness\n \n",
      "content_length": 1314,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "\t\n35\nBuilding a new feature\nYou can start by mapping these capabilities directly to microservices. Each service \nshould reflect a capability that the business offers — this results in a good balance of \nsize versus responsibility. You also should consider what would drive a microservice \nto change in the future — whether it truly has single responsibility. For example, you \ncould argue that market execution is a subset of order management and therefore \nshouldn’t be a separate service. But the drivers for change in that area are the behavior \nand scope of the markets you’re supporting, whereas order management relates more \nclosely to the types of product and the account being used to trade. These two areas \ndon’t change together. By separating them, you isolate areas of volatility and maximize \ncohesiveness (figure 2.5).\nSome microservice practitioners would argue that microservices should more closely \nreflect single functions, rather than single capabilities. Some have even suggested that \nmicroservices are “append only” and that it’s always better to write new services than to \nadd to existing ones.\nWe disagree. Decomposing too much can lead to services that lack cohesiveness and \ntight coupling between closely related collaborators. Likewise, deploying and monitor-\ning many services might be beyond the abilities of the engineering team in the early \ndays of a microservice implementation. A useful rule of thumb is to err on the side of \nlarger services; it’s often easier to carve out functionality later if it becomes more spe-\ncialized or more clearly belongs in an independent service.\nLastly, keep in mind that understanding your domain isn’t a one-off process! Over \ntime, you’ll continue to iterate on your understanding of the domain; your users’ needs \nwill change, and your product will continue to evolve. As this understanding changes, \nyour system itself will change to meet those needs. Luckily, as we discussed in chap-\nter 1, coping with changing needs and requirements is a strength of the microservices \napproach.\nOrder management\nRecording status and\nhistory of orders\nPlacing an order to\nmarket\nMarket gateway\n2. Separating areas of\nindependent change\npromotes loose coupling and\nincreases cohesiveness.\nNew types of order;\ndifferent account rules;\npromotions...\nNew types of order;\ndifferent market rules;\nnew markets...\n1. These areas are likely to\nchange independently.\nService\nFunction\nReasons to change\nFigure 2.5    Services should isolate reasons to change to promote loose coupling and single \nresponsibility.\n \n",
      "content_length": 2567,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "36\nChapter 2  Microservices at SimpleBank \n2.3.2\t\nService collaboration\nWe’ve identified several microservice candidates. These services need to collaborate \nwith each other to do something useful for SimpleBank’s customers.\nAs you may already know, service collaboration can be either point-to-point or event-\ndriven. Point-to-point communication is typically synchronous, whereas event-driven \ncommunication is asynchronous. Many microservice applications begin by using syn-\nchronous communication. The motivations for doing so are twofold:\n¡ Synchronous calls are typically simpler and more explicit to reason through than \nasynchronous interaction. That said, don’t fall into the trap of thinking they \nshare the same characteristics as local, in-process function calls — requests across \na network are significantly slower and more unreliable.\n¡ Most, if not all, programming ecosystems already support a simple, language- \nagnostic transport mechanism with wide developer mindshare: HTTP, which is \nmainly used for synchronous calls but you can also use asynchronously.\nConsider SimpleBank’s order placement process. The orders service is responsible for \nrecording and placing an order to market. To do this, it needs to interact with your mar-\nket, fees, and account transaction services. This collaboration is illustrated in figure 2.6.\nTransaction\ndatabase\nUser\n1. Places order to sell\n100 units of Stock A\nfrom account ABC\n2. Records order\ndetails in database\n3. Requests reservation\nof 100 units of Stock A\nagainst account ABC\n4. Records reserved\nstock position against\naccount ABC\n5. Requests calculation\nof fee\n6. Requests placement\nof order to market\n7. Places order onto\nstock exchange\nAccount\ntransactions\nservice\nOrders service\nFees service\nFee rules\ndatabase\nMarket service\nStock exchange\nOrder\ndatabase\nFigure 2.6    The orders service orchestrates the behavior of several other services to place an order to market.\n \n",
      "content_length": 1940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "\t\n37\nBuilding a new feature\nEarlier, we pointed out that microservices should be autonomous, and to achieve that, ser-\nvices should be loosely coupled. You achieve this partly through the design of your services, \n“[gathering] together the things that change for the same reasons” to minimize the chance \nthat changes to one service require changes to its upstream or downstream collaborators. \nYou also need to consider service contracts and service responsibility.\nService contracts\nThe messages that each service accepts, and the responses it returns, form a contract \nbetween that service and the services that rely on it, which you can call upstream collabo-\nrators. Contracts allow each service to be treated as a black box by its collaborators: you \nsend a request and you get something back. If that happens without errors, the service \nis doing what it’s meant to do.\nAlthough the implementation of a service may change over time, maintaining \ncontract-level compatibility ensures two things:\n1\t Those changes are less likely to break consumers.\n2\t Dependencies between services are explicitly identifiable and manageable.\nIn our experience, contracts are often implicit in naïve or early microservice imple-\nmentations; they’re suggested by documentation and practice, rather than explicitly \ncodified. As the number of services grows, you can realize significant benefit from stan-\ndardizing the interfaces between them in a machine-readable format. For example, \nREST APIs may use Swagger/OpenAPI. As well as aiding the conformance testing of \nindividual services, publishing standardized contracts will help engineers within an \norganization understand how to use available services.\nService responsibility\nYou can see in figure 2.6 that the orders service has a lot of responsibility. It directly \norchestrates the actions of every other service involved in the process of placing an \norder. This is conceptually simple, but it has downsides. At worst, our other services \nbecome anemic, with many dumb services controlled by a small number of smart ser-\nvices, and those smart services grow larger\nThis approach can lead to tighter coupling. If you want to introduce a new part of this \nprocess — let’s say you want to notify a customer’s account manager when a large order \nis placed — you’re forced to deploy new changes to the orders service. This increases \nthe cost of change. In theory, if the orders service doesn’t need to synchronously con-\nfirm the result of an action — only that it’s received a request — then it shouldn’t need \nto have any knowledge of those downstream actions.\n2.3.3\t\nService choreography\nWithin a microservice application, services will naturally have differing levels of responsi-\nbility. But you should balance orchestration with choreography. In a choreographed system, \na service doesn’t need to directly command and trigger actions in other services. Instead, \neach service owns specific responsibilities, which it performs in reaction to other events.\n \n",
      "content_length": 3007,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "38\nChapter 2  Microservices at SimpleBank \nLet’s revisit the earlier design and make a few tweaks:\n1\t When someone creates an order, the market might not currently be open. There-\nfore, you need to record what status an order is in: created or placed. Placement \nof an order doesn’t need to be synchronous.\n2\t You’ll only charge a fee once an order is placed, so charging fees doesn’t need \nto be synchronous. In fact, it should happen in reaction to the market service, \nrather than being orchestrated by the orders service.\nFigure 2.7 illustrates the changed design. Adding events adds an architectural con-\ncern: you need some way of storing them and exposing them to other applications. \nWe’d recommend using a message queue for that purpose, such as RabbitMQ or SQS.\nIn this design, we’ve removed the following responsibility from the orders service:\n¡ Charging fees  — The orders service has no awareness that a fee is being charged \nonce an order is being placed to market.\n¡ Placing orders  — The orders service has no direct interaction with the market ser-\nvice. You could easily replace this with a different implementation, or even a service \nper market, without needing to change the orders service itself.\nTransaction\ndatabase\nUser\n1. Places order to sell\n100 units of Stock A\nfrom account ABC\n2. Records order\ndetails in database\n3. Requests reservation\nof 100 units of Stock A\nagainst account ABC\n8. Orders service\nsubscribes to\nOrderPlaced events\n6. Trigger event\nOrderPlaced\n4. Market order\nservice subscribes\nto OrderCreated\nevents\n7. Fees service\nsubscribes to\nOrderPlaced events\n3. Triggers event\nOrderCreated\n9. Orders service\nupdates status of\norder to placed\n4. Records reserved\nstock position against\naccount ABC\n5. Places order onto\nstock exchange\nAccount\ntransactions\nservice\nOrders service\nFees service\nFee rules\ndatabase\nMarket service\nEvent queue\nStock exchange\nOrder\ndatabase\nFigure 2.7    You choreograph the behavior of other services through events, reducing the coordinating \nrole of the orders service. Note that some actions, for example, the two actions numbered “3.,” happen \nconcurrently.\n \n",
      "content_length": 2131,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "\t\n39\nExposing services to the world\nThe orders service itself also reacts to the behavior of other services by subscribing to the \nOrderPlaced event emitted by the market service. You can easily extend this to further \nrequirements; for example, the orders service might subscribe to TradeExecuted events \nto record when the sale has been completed on the market or OrderExpired events if the \nsale can’t be made within a certain timeframe.\nThis setup is more complex than the original synchronous collaboration. But by \nfavoring choreography where possible, you’ll build services that are highly decoupled \nand therefore independently deployable and amenable to change. These benefits do \ncome at a cost: a message queue is another piece of infrastructure to manage and scale \nand itself can become a single point of failure.\nThe design we’ve come up with also has some benefit in terms of resiliency. For \nexample, failure in the market service is isolated from failure in the orders service. If \nplacing an order fails, you can replay that event3 later, once the service is available, or \nexpire it if too much time passes. On the other hand, it’s now more difficult to trace the \nfull activity of the system, which you’ll need to consider when you think about how to \nmonitor these services in production.\n2.4\t\nExposing services to the world\nSo far, we’ve explored how services collaborate to achieve some business goal. How do \nyou expose this functionality to a real user application?\nSimpleBank wants to build both web and mobile products. To do this, the engineering \nteam have decided to build an API gateway as a façade over these services. This abstracts \naway backend concerns from the consuming application, ensuring it doesn’t need to \nhave any awareness of underlying microservices, or how those services interact with each \nother to deliver functionality. An API gateway delegates requests to underlying services \nand transforms or combines their responses as appropriate to the needs of a public API.\nImagine the user interface of a place order screen. It has four key functions:\n¡ Displaying information about the current holdings within a customer’s account, \nincluding both quantity and value\n¡ Displaying market data showing prices and market movements for a holding\n¡ Inputting orders, including cost calculation\n¡ Requesting execution of those orders against the specified holdings\nFigure 2.8 illustrates how an API gateway serves that functionality, and how that gate-\nway collaborates with underlying services.\nThe API gateway pattern is elegant but has a few downsides. Because it acts as a single \ncomposition point for multiple services, it’ll become large and possibly unwieldy. It may \nbe a temptation to add business logic in the gateway, rather than treating it as a proxy \nalone. It can suffer from trying to be all things to all applications: whereas a mobile cus-\ntomer application may want a smaller, cut-down payload, but an internal administration \nweb application might require significantly more data. It can be hard to balance these \ncompeting forces while building a cohesive API.\n3\t Assuming the queue itself is persistent.\n \n",
      "content_length": 3169,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "40\nChapter 2  Microservices at SimpleBank \nOrders\nHoldings\nTransactions\nMarket data\nBackend services\nAPI gateway\nUser interface\nHoldings\nMarket data\nInput\nSubmit\nGET /holdings\nGET /market/{id}\nPOST /orders\nFigure 2.8    A user interface, such as a web page or mobile app, interacts with the REST API that an API \ngateway exposes. The gateway provides a façade over underlying microservices and proxies requests to \nappropriate backend services.\nNOTE    We’ll revisit the API gateway pattern and discuss alternative approaches \nin chapter 3.\n2.5\t\nTaking your feature to production\nYou’ve designed a feature for SimpleBank that involves the interaction of multiple ser-\nvices, an event queue, and an API gateway. Let’s say you’ve taken the next step: you’ve \nbuilt those services and now the CEO is pushing you to get them into production.\nIn public clouds like AWS, Azure, or GCE, the obvious solution is to deploy each ser-\nvice to a group of virtual machines. You could use load balancers to spread load evenly \nacross instances of each web-facing service, or you could use a managed event queue, \nsuch as AWS’s Simple Queue Service, to distribute events between services.\nNOTE    An in-depth discussion of effective infrastructure automation and man-\nagement is outside the scope of this book. Most cloud providers provide this \ncapability through custom tooling, such as AWS’s CloudFormation or Elastic \nBeanstalk. Alternatively, you could consider open source tools, such as Chef or \nTerraform.\nAnyway — you compiled that code, FTP’d it onto those VMs, got the databases up and \nrunning, and tried some test requests. This took a few days. Figure 2.9 shows your pro-\nduction infrastructure.\nFor a few weeks, that didn’t work too badly. You made a few changes and pushed out \nthe new code. But soon you started to run into trouble. It was hard to tell if the services \n \n",
      "content_length": 1874,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "\t\n41\nTaking your feature to production\nwere working as expected. Worse, you were the only person at SimpleBank who knew \nhow to release a new version. Even worse than that, the guy who wrote the transaction \nservice went on vacation for a few weeks, and no one knew how the service was deployed. \nThese services would have a bus factor of 1 — suggesting they wouldn’t survive the disap-\npearance of any team member.\nDEFINITION    bus factor is a measurement of the risk of knowledge not being \nshared between multiple team members, from the phrase “in case they get hit by \na bus.” It’s also known as truck factor. The lower bus factors are, the worse they are.\nSomething was definitely wrong. You remembered that in your last job at GiantBank, \nthe infrastructure team managed releases. You’d log a ticket, argue back and forth, \nand after a few weeks, you’d have what you needed…or sometimes not, so you’d log \nanother ticket. That doesn’t seem like the right approach either. In fact, you were glad \nthat using microservices allowed you to manage deployment.\nVM\nVM\nTransaction\ndatabase\nHTTP requests are\nload-balanced across\nservice instances.\nEach service is deployed on\nmultiple VMs.\nAccount\ntransactions\nOrders\nservice\nLoad\nbalancer\nLoad\nbalancer\nLoad\nbalancer\nFees\nservice\nEvent queue\nGateway\nUser\nVM\nVM\nFee rules\ndatabase\nMarket\nservice\nOrder\ndatabase\nFigure 2.9    In a simple microservices deployment, requests to each service are load balanced across \nmultiple instances, running across multiple virtual machines. Likewise, multiple instances of a service \nmay subscribe to a queue.\n \n",
      "content_length": 1596,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "42\nChapter 2  Microservices at SimpleBank \nIt’s safe to say that your services weren’t ready for production. Running microservices \nrequires a level of operational awareness and maturity from an engineering team \nbeyond what’s typical in a monolithic application. You can only say a service is produc-\ntion ready if you can confidently trust it to serve production workloads.\nHow can you be confident a service is trustworthy? Let’s start with a list of questions \nyou might need to consider to achieve production readiness:\n¡ Reliability  — Is your service available and error free? Can you rely on your \ndeployment process to push out new features without introducing instability \nor defects?\n¡ Scalability  — Do you understand the resource and capacity needs of a service? \nHow will you maintain responsiveness under load?\n¡ Transparency  — Can you observe a service in operation through logs and metrics? \nIf something goes wrong, is someone notified?\n¡ Fault tolerance  — Have you mitigated single points of failure? How do you cope \nwith the failure of other service dependencies?\nAt this early stage in the lifetime of a microservice application, you need to establish \nthree fundamentals:\n¡ Quality-controlled and automated deployments\n¡ Resilience\n¡ Transparency\nLet’s examine how these fundamentals will help you address the problems that Simple-\nBank has encountered.\n2.5.1\t\nQuality-controlled and automated deployment\nYou’ll lose the added development speed you gain from microservices if you can’t get \nthem to production rapidly and reliably. The pain of unstable deployments — such as \nintroducing a serious error — will eliminate those speed gains.\nTraditional organizations often seek stability by introducing (often bureaucratic) \nchange control and approval processes. They’re designed to manage and limit change. \nThis isn’t an unreasonable impulse: if changes introduce most bugs4 — costing the com-\npany thousands (or millions) of dollars of engineering effort and lost revenue — then \nyou should closely control those changes.\nIn a microservice architecture, this won’t work, because the system will be in a state \nof continuous evolution; it’s this freedom that gives rise to tangible innovation. But \nto ensure that freedom doesn’t lead to errors and outages, you need to be able to \ntrust your development process and deployment. Equally, to enable such freedom in \nthe first place, you also need to minimize the effort required to release a new service \n4\t \"SRE has found that roughly 70% of outages are due to changes in a live system.\" Benjamin Treynor \nSloss, Chapter 1, Site Reliability Engineering, 2017, O’Reilly Media, http://mng.bz/7Mm4.\n \n",
      "content_length": 2675,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "\t\n43\nTaking your feature to production\nor change an existing one. You can achieve stability through standardization and \nautomation:\n¡ You should standardize the development process. You should review code changes, write \nappropriate tests, and maintain version control of the source code. We hope this \ndoesn’t surprise anyone!\n¡ You should standardize and automate the deployment process. You should thoroughly \nvalidate the delivery of a code change to production, and it should require mini-\nmal intervention from an engineer. This is a deployment pipeline.\n2.5.2\t\nResilience\nEnsuring a software system is resilient in the face of failure is a complicated task. The \ninfrastructure underpinning your systems is inherently unreliable; even if your code is \nperfect, network calls will fail and servers will go down. As part of designing a service, \nyou need to consider how it and its dependencies may fail and proactively work to \navoid — or minimize the impact of — those failure scenarios.\nTable 2.2 examines the potential areas of risk in the system that SimpleBank has \ndeployed. You can see that even a relatively simple microservice application introduces \nseveral areas of potential risk and complexity.\nTable 2.2    Areas of risk in SimpleBank’s microservice application\nArea\nPossible failures\nHardware\nHosts, data center components, physical network\nCommunication between services\nNetwork, firewall, DNS errors\nDependencies\nTimeouts, external dependencies, internal failures, for example,  \nsupporting databases\nNOTE    Chapter 6 will investigate techniques for maximizing service resilience.\n2.5.3\t\nTransparency\nThe behavior and state of a microservice should be observable: at any time, you should \nbe able to determine whether the service is healthy and whether it’s processing its \nworkload in the way you expect. If something affects a key metric — say, orders are \ntaking too long to be placed to market — this should send an actionable alert to the \nengineering team.\nWe’ll illustrate this with an example. Last week, there was an outage at SimpleBank. A \ncustomer called and told you she was unable to submit orders. Quick investigation turned \nup that this was affecting every customer: requests made to the order creation service \nwere timing out. Figure 2.10 illustrates the possible points of failure within that service.\n \n",
      "content_length": 2349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "44\nChapter 2  Microservices at SimpleBank \nUser\nGateway\nNetwork issues affect connectivity,\nfor example, between gateway\nand load balancers.\nService instances\nbecome saturated with\nrequests and fail to\nrespond or pass\ntimeout limits.\nThe database is overloaded by\nrequests or poor queries.\nDependencies\nService dependencies may\nrespond slowly or become\nunresponsive.\nOrder \ncreation\nservice\nLB\nDatabase\nFigure 2.10    A service timeout may be due to several underlying reasons: network issues, problems with \nservice-internal dependencies — such as databases — or unhealthy behavior from other services.\nIt was clear that you had a major operational problem: you lacked logging to determine \nexactly what went wrong and where things were falling apart. Through manual testing, \nyou managed to isolate the problem: the account transaction service was unresponsive. \nMeanwhile, your customers had been unable to place orders for several hours. They \nweren’t happy.\nTo avoid such problems in the future, you need to add thorough instrumentation to \nyour microservices. Collecting data about application activity — at all layers — is vital to \nunderstanding the present and past operational behavior of a microservice application.\nAs a first step, SimpleBank set up infrastructure to aggregate the basic logs that your \nservices produced, sending them to a service that allowed you to tag and search them.5 \nFigure 2.11 illustrates this approach. By doing this, the next time a service failed, the \nengineering team could use those logs to identify the point where the system began to \nfail and diagnose the issue precisely where it occurred.\nBut inadequate logging wasn’t the only problem. It was embarrassing that SimpleBank \nonly identified an issue once a customer called. The company should have had alerting in \nplace to ensure that each service was meeting its responsibilities and service goals.\nIn such cases, in its most simple form, you should have a recurring heartbeat check \nthat happening on each service to alert the team if a service becomes completely unre-\nsponsive. Beyond that, a team should commit to operational guarantees for each ser-\nvice. For example, for a critical service, you might aim for 95% of requests to return \nin under 100ms with 99.99% uptime. Failing to meet these thresholds should result in \nalerts being sent to the service owners.\n5\t Several managed services exist for log aggregation, including Loggly, Splunk, and Sumo Logic. \nYou also can run this function in-house using the well-known ELK (Elasticsearch, Logstash, Kibana) \ntool stack.\n \n",
      "content_length": 2583,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "\t\n45\nScaling up microservice development\nAn agent runs on each VM to\ncollect log data from running\nservices, such as requests made.\nTransactions\nlogs\nVM\nAgent\nLog store\nEach agent ships logs to a\ndedicated store.\nYou can be index and search\nlogs to investigate service\nissues, build reports, or\ntrigger alerts.\nSearch\nEngineers\nOrders\nlogs\nVM\nAgent\nFigure 2.11    You install a logging collection agent on each instance. This ships application log data to a \ncentral repository where you can index, search, and analyze it further.\nBuilding thorough monitoring for a microservice application is a complex task. The \ndepth of monitoring you apply will evolve as your system increases in complexity and \nnumber of services. As well as the operational metrics and logging we’ve described, \na mature microservice monitoring solution will address business metrics, interservice \ntracing, and infrastructure metrics. If you are to trust your services, you need to con-\nstantly work at making sense of that data.\nNOTE    In part 4 of this book, we’ll discuss monitoring in detail, and how to \nuse tools like Prometheus to trigger alerts and build health dashboards for \nmicroservices.\n2.6\t\nScaling up microservice development\nThe technical flexibility of microservices is a blessing for the speed of development \nand the effective scalability of a system. But that same flexibility also leads to organiza-\ntional challenges that change the nature of how an engineering team works at scale. \nYou’ll quickly encounter two challenges: technical divergence and isolation.\n2.6.1\t\nTechnical divergence\nImagine SimpleBank has built a large microservice system of say 1,000 services. A small \nteam of engineers owns each service, with each team using their preferred languages, \ntheir favorite tools, their own deployment scripts, their favored design principles, their \npreferred external libraries,6 and so on.\nTake a moment to recoil in terror at the sheer weight of effort involved in main-\ntaining and supporting so many different approaches. Although microservices make it \npossible to choose different languages and frameworks for different services, it’s easy to \n6\t Unfortunately, this isn’t only an issue in microservices, although it’s exacerbated by hard compo-\nnent boundaries and explicit service ownership. Earlier in my career, I encountered a single Ruby \nproject that used six different HTTP client libraries!\n \n",
      "content_length": 2414,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "46\nChapter 2  Microservices at SimpleBank \nsee that without choosing reasonable standards and limits, the system will become an \nunimaginable and fragile sprawl.\nIt’s easy to see this frustration emerge on a smaller scale. Consider two ser-\nvices — account transactions and orders — that two different teams own. The first ser-\nvice produces well-structured log output for every request, including helpful diagnostic \ninformation such as timings, a request ID, and the currently released revision ID:\nservice=api\ngit_commit=d670460b4b4aece5915caf5c68d12f560a9fe3e4\nrequest_id=55f10e07-ec6c\nrequest_ip=1.2.3.4\nrequest_path=/users\nresponse_status=500\nerror_id=a323-da321\nparameters={ id: 1 }\nuser_id=123\ntiming_total_ms=223\nThe second service produces anemic messages in a difficult to parse format:\nProcessed /users in 223ms with response 500\nYou can see that even in this simple example of log message format, consistency and \nstandardization would make it easier to adequately diagnose issues and trace requests \nacross multiple services. It’s crucial to agree on reasonable standards at all layers of \nyour microservice system to manage divergence and sprawl.\n2.6.2\t\nIsolation\nIn chapter 1, we mentioned Conway’s Law. In an organization that works with micro­\nservices, the inverse of this law is likely to be true: the structure of the company is deter-\nmined by the architecture of its product.\nThis suggests that development teams will increasingly reflect microservices: they’ll \nbe highly specialized to do one thing well. Each team will own and be accountable for \nseveral closely related microservices. Taken collectively, the developers will know every-\nthing there is to know about a system, but individually they’ll have a narrow area of \nspecialization. As SimpleBank’s customer base and product complexity grow, this spe-\ncialization will deepen.\nThis configuration can be immensely challenging. Microservices have limited \nvalue by themselves and don’t function in isolation. Therefore, these independent \nteams must collaborate closely to build an application that runs seamlessly, even \nthough their goals as a team likely relate to their own narrower area of ownership. \nLikewise, a narrow focus may tempt a team to optimize for their local problems and \npreferences, rather than the needs of the whole organization. At its worst, this could \nlead to conflict between teams, in turn leading to slower deployment and a less reli-\nable product.\n \n",
      "content_length": 2463,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "\t\n47\nSummary\n2.7\t\nWhat’s next?\nIn this chapter, we established that microservices were a good fit for SimpleBank, \ndesigned a new feature, and considered how you might make that feature production \nready. We hope this case study has shown that a microservice-driven approach to appli-\ncation development is both compelling and challenging!\nThroughout the rest of this book, we’ll teach you the techniques and tools you need \nto know to run a great microservice application. Although microservices can lead to \nboth flexible and highly productive development, running multiple distributed ser-\nvices is much more demanding than running a single application. To avoid instability, \nyou need be able to design and deploy services that are production ready: transparent, \nfault-tolerant, reliable, and scalable.\nIn part 2, we’ll focus on design. Effectively designing a system of distributed, inter-\ndependent services requires careful consideration of your system domain and how \nthose services interact. Being able to identify the right boundaries between responsibil-\nities — and therefore build highly cohesive and loosely coupled services — is one of the \nmost valuable skills for any microservice practitioner.\nSummary\n¡ Microservices are highly applicable in systems with multiple dimensions of com-\nplexity — for example, breadth of product offering, global deployment, and reg-\nulatory pressures.\n¡ It’s crucial to understand the product domain when designing microservices.\n¡ Service interactions may be orchestrated or choreographed. The latter adds \ncomplexity but can lead to a more loosely coupled system.\n¡ API gateways are a common pattern for abstracting away the complexity of a \nmicroservice architecture for front-end or external consumers.\n¡ You can say a service is production ready if you can trust it to serve production \nworkloads.\n¡ You can be more confident in a service if you can reliably deploy and monitor it.\n¡ Service monitoring should include log aggregation and service-level health \nchecks.\n¡ Microservices can fail because of problems with hardware, communication, and \ndependencies, not just defects in code.\n¡ Collecting business metrics, logs, and interservice traces is vital to understanding \nthe present and past operational behavior of a microservice application.\n¡ Technical divergence and isolation will become increasingly challenging for \nan engineering organization as the number of microservices (and supporting \nteams) increases.\n¡ Avoiding divergence and isolation requires standards and best practices to be \nsimilar across multiple teams, regardless of technical underpinnings.\n \n",
      "content_length": 2630,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "Part 2\nDesign\nIn this part of the book, we’ll explore the design of microservice applications. \nWe’ll start with a big-picture view — the architecture of an entire application — and \nthen drill down to explore how to scope services and connect them together. You’ll \nlearn how to design services that are reliable and a microservice framework that’s \nreusable.\n \n",
      "content_length": 363,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "51\n3\nArchitecture of \na microservice application\nThis chapter covers\n¡ The big picture view of a microservice \napplication\n¡ The four tiers of microservice architecture: \nplatform, service, boundary, and client\n¡ Patterns for service communication\n¡ Designing API gateways and consumer-driven \nfaçades as application boundaries\nIn chapter 2, we designed a new feature for SimpleBank as a set of microservices and \ndiscovered that deep understanding of the application domain is one of the keys to \na successful implementation. In this chapter, we’ll look at the bigger picture and \nconsider the design and architecture of an entire application made up of microser-\nvices. We can’t give you a deep understanding of the domain your own application \nlives in, but we can show you how having such an understanding will help you build \na system that’s flexible enough to grow and evolve over time.\nYou’ll see how a microservice application is typically designed to have four \ntiers — platform, service, boundary, and client — and you’ll learn what they are and \nhow they combine to deliver customer-facing applications. We’ll also highlight the \nrole of an event backbone in building a large-scale microservice application and \n \n",
      "content_length": 1225,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "52\nChapter 3  Architecture of a microservice application \ndiscuss different patterns for building application boundaries, such as API gateways. \nLastly, we’ll touch on recent trends in building user interfaces for microservice applica-\ntions, such as micro-frontends and frontend composition.\n3.1\t\nArchitecture as a whole\nAs a software designer, you want to build software that’s amenable to change. Many \nforces put pressure on your software: new requirements, defects, market demands, new \ncustomers, growth, and so on. Ideally, you can respond to these pressures at a steady \npace and with confidence. For you to be able to do that, your development approach \nshould reduce friction and minimize risk.\nYour engineering organization will want to remove any roadblocks to development \nas time goes by and the system evolves. You want to be able to quickly and seamlessly \nreplace any system’s component that becomes obsolete. You want to have teams in place \nthat can become completely autonomous and responsible for portions of a larger sys-\ntem. And you want those teams to coexist without the need for constant synchroniza-\ntion and without blocking other teams. For that, you need to think about architecture: \nyour plan for building an application.\n3.1.1\t\nFrom monolith to microservices\nWith a monolithic application, your primary deliverable is a single application. That \napplication is split horizontally into different technical layers — in a typical three-tier \napplication, they’d be data, logic, and presentation (figure 3.1) — and vertically into dif-\nferent business domains. Patterns like MVC and frameworks like Rails and Django \nreflect the three-tier model. Each tier provides services to the tier above: the data tier \nprovides persistent state; the logic tier executes useful work; and the presentation layer \npresents the results back to the end user.\nAn individual microservice is similar to a monolith: it stores data, performs some busi-\nness logic, and returns data and outcomes to consumers through APIs. Each microser-\nvice owns a business or technical capability of the application and interacts with other \nmicroservices to execute work. Figure 3.2 illustrates the high-level architecture of an \nindividual service.\nNOTE    Chapter 4 discusses microservice scoping — how to define the boundar-\nies and responsibilities of a microservice — in detail.\nIn a monolithic application, your architecture is limited to the boundaries of the appli-\ncation itself. In a microservice application, you’re planning for something that’ll keep \nevolving both in size and breadth. Think of it like a city: building a monolith is like \nbuilding a skyscraper; whereas building a microservice application is like building a \nneighborhood: you need to build infrastructure (plumbing, roads, cables) and plan \nfor growth (zone for small businesses versus houses).\nThis analogy highlights the importance of considering not only the components them-\nselves, but also the way they connect, where they’re placed, and how you can build them \nconcurrently. You want your plan to encourage growth along good lines, rather than dic-\ntate or enforce a certain structure on your overall application.\n \n",
      "content_length": 3201,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "\t\n53\nArchitecture as a whole\nClients\nPresentation\nApplication\nLogic\nData\nData store\nFigure 3.1    The architecture of a typical three-tier monolithic application\nAPI\nUpstream services\nDownstream services\nThis service owns data\nand manages storage.\nThe presentation layer\nexposes API resources for\nother services.\nThe logic layer performs\nwork, potentially\ninteracting with other\nservices.\nRequest\nResponse\nRequests\nLogic\nData\nData store\nFigure 3.2    The high-level architecture of an individual microservice\n \n",
      "content_length": 511,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "54\nChapter 3  Architecture of a microservice application \nMostly importantly, you don’t run microservices in isolation; each microservice lives \nin an environment that enables you to build, deploy, and run it, in concert with \nother microservices. Your application architecture should encompass that whole \nenvironment.\n3.1.2\t\nThe role of an architect\nWhere do software architects fit in? Many enterprises employ software architects, \nalthough the effectiveness of and the approach to this role varies wildly.\nMicroservice applications enable rapid change: they evolve over time as teams build \nnew services, decommission existing services, refactor existing functionality, and so on. \nAs an architect or technical lead, your job is to enable evolution, rather than dictate \ndesign. If the microservice application is a city, then you’re a planner for the city council.\nAn architect’s role is to make sure the technical foundations of the application sup-\nport a fast pace and fluidity. An architect should have a global perspective and make \nsure the global needs of the application are met, guiding its evolution so that\n¡ The application is aligned to the wider strategic goals of the organization.\n¡ Teams share a common set of technical values and expectations.\n¡ Cross-cutting concerns — such as observability, deployment, and interservice \ncommunication — meet the needs of multiple teams.\n¡ The whole application is flexible and malleable in the face of change.\nTo achieve these things, an architect should guide development in two ways:\n¡ Principles  — Guidelines that the team should follow to achieve higher level tech-\nnical or organizational goals\n¡ Conceptual models  — High-level models of system relationships and application- \nlevel patterns\n3.1.3\t\nArchitectural principles\nPrinciples are guidelines (or sometimes rules) that teams should follow to achieve \nhigher level goals. They inform team practice. Figure 3.3 illustrates this model. For \nexample, if your product goal is to sell to privacy- and security-sensitive enterprises, you \nmight set the following principles:\n¡ Development practices must comply with recognized external standards (for \nexample, ISO 27001).\n¡ All data must be portable and stored with retention limits in mind.\n¡ Personal information must be clearly tracked and traceable through the application.\nPrinciples are flexible. They can and should change to reflect the priorities of the busi-\nness and the technical evolution of your application. For example, early development \nmight prioritize validating product-market fit, whereas a more mature application \nmight require a focus on performance and scalability.\n \n",
      "content_length": 2662,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "\t\n55\nArchitecture as a whole\nCompany and product goals\nTechnical principles\nTeam practices and decisions\nInform\nAchieve\nFigure 3.3    An architectural approach based on technical principles\n3.1.4\t\nThe four tiers of a microservice application\nArchitecture should reflect a clear high-level conceptual model. A model is a useful \ntool for reasoning about an application’s technical structure. A multi-tiered model, \nlike the three-tier model outlined in figure 3.1, is a common approach to applica-\ntion structure, reflecting layers of abstraction and responsibility within an overall \nsystem.\nIn the rest of this chapter, we’ll explore a four-tier model for a microservice \napplication:\n¡ Platform  — A microservice platform provides tooling, infrastructure, and high-\nlevel primitives to support the rapid development, operation, and deployment \nof microservices. A mature platform layer enables engineers to focus on building \nfeatures, not plumbing.\n¡ Services  — In this tier, the services that you build interact with each other to pro-\nvide business and technical capabilities, supported by the underlying platform.\n¡ Boundary  — Clients will interact with your application through a defined bound-\nary that exposes underlying functionality to meet the needs of outside consumers.\n¡ Client  — Client applications, such as websites and mobile applications, interact \nwith your microservice backend.\nFigure 3.4 illustrates these architectural layers. You should be able to apply them to any \nmicroservice application, regardless of underlying technology choices.\n \n",
      "content_length": 1568,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "56\nChapter 3  Architecture of a microservice application \nClient user interfaces, devices, third parties\nBoundary entry point, aggregation\nServices business logic, orchestration, communication\nPlatform infrastructure, deployment, communication\nAccesses\nExposes\nRun on\nSupports\nFigure 3.4    A four-tiered model of microservice application architecture\nEach layer is built on the capabilities of the layers below; for example, individual ser-\nvices take advantage of deployment pipelines, infrastructure, and communication \nmechanisms that the underlying microservice platform provides. A well-designed \nmicroservice application requires sophistication and investment at all layers.\nGreat! So now you have a model you can work with. In the next five sections, we’ll \nwalk through each layer in this architectural model and discuss how it contributes to \nbuilding sustainable, flexible, and evolutionary microservice applications.\n3.2\t\nA microservice platform\nMicroservices don’t live in isolation. A microservice is supported by infrastructure:\n¡ A deployment target where services are run, including infrastructure primitives, \nsuch as load balancers and virtual machines\n¡ Logging and monitoring aggregation to observe service operation\n¡ Consistent and repeatable deployment pipelines to test and release new services \nand versions\n¡ Support for secure operation, such as network controls, secret management, and \napplication hardening\n¡ Communication channels and service discovery to support service interaction\nFigure 3.5 illustrates these capabilities and how they relate to the service layer of the \napplication. If each microservice is a house, then the platform provides roads, water, \nelectricity, and telephone cables.\n \n",
      "content_length": 1732,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "\t\n57\nA microservice platform\nObservability\nObservability\nService A\nService B\nCommunication /\nservice discovery\nRuntime platform\nSecurity\nDeployment\npipeline\nDeployment\npipeline\nLogs, metrics, alerts\nTests, packaging,\nrollout, rollback\nVirtual machines, load\nbalancers, and so on\nNetwork controls,\nsecrets, hardening\nFigure 3.5    The capabilities of a microservice platform\nA robust platform layer decreases overall implementation cost, increases overall sta-\nbility, and enables rapid service development. Without this platform, product devel-\nopers would need to repeatedly write plumbing code themselves, taking energy away \nfrom delivering new features and business impact. The average developer shouldn’t \nneed to be an expert in the intricacies of every layer of the application. Ultimately, a \nsemi-independent, specialist team can develop the platform layer to meet the needs \nof multiple teams working in the service layer of the application.\n3.2.1\t\nMapping your runtime platform\nA microservice platform will help you be confident that you can trust the services your \nteam writes to serve production workloads and be resilient, transparent, and scalable. \nFigure 3.6 maps out a runtime platform for a microservice.\nA runtime platform (or deployment target) — for example, a cloud environment like \nAWS or a platform as a service (PaaS) like Heroku — provides infrastructure primitives \nnecessary to run multiple service instances and route requests between them. In addition, \nit provides mechanisms for providing configuration — secrets and environment-specific \nvariables — to service instances.\nYou build the other elements of a microservice platform on top of this foundation. \nObservability tools collect and correlate data from services and underlying infrastruc-\nture. Deployment pipelines manage the upgrade (or rollback) of this stack.\n \n",
      "content_length": 1857,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "58\nChapter 3  Architecture of a microservice application \nA service may require external\nconfiguration, such as secrets\nand environment variables.\nMicroservices run on\nhosts, usually virtual.\nServices may require\nother resources, such\nas disk volumes.\nConsumes\nMicroservice\ninstance\nMicroservice\ninstance\nMachine\nMachine\nRuns on\nRuns on\nUses\nResources\nRequests need to be\nrouted to services.\nEach microservice runs\nas multiple instances for\nscale and redundancy.\nA deployment pipeline\ncontrols rolling updates\nand rollback of this stack.\nLB\nService group\nUses\nConfiguration\nFigure 3.6    A deployment configuration for a microservice running in a typical cloud environment\n3.3\t\nServices\nThe service layer has perhaps the most self-explanatory name — this is where your ser-\nvices live. At this tier, services interact to perform useful work, relying on the under-\nlying platform abstractions for reliable operation and communication and exposing \ntheir work through the boundary layer to application clients. We also consider compo-\nnents that are logically internal to a service, such as data stores, to be part of this tier.\nThe structure of your service tier will differ widely depending on the nature of your \nbusiness. In this section, we’ll discuss some of the common patterns you’ll encounter:\n¡ Business and technical capabilities\n¡ Aggregation and higher order services\n¡ Services on critical and noncritical paths\n3.3.1\t\nCapabilities\nThe services you write will implement different capabilities:\n¡ A business capability is something that an organization does to generate value and \nmeet business goals. Microservices that you scope to business capabilities directly \nreflect business goals.\n \n",
      "content_length": 1703,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "\t\n59\nServices\nBusiness capability:\nmanage order placement\nand execution\nTechnical capability:\ninteract with market third\nparties\nImplemented by\nImplemented by\nA technical capability provides a\nshared technical service to support\nbusiness capabilities.\nA business capability is something\nan organization does to create\nvalue or meet goals.\nSupports\nSettlement\nMarket\ndata\nOrders\nMarket\nSupports\nSupports\nFigure 3.7    Microservices implementing business or technical capabilities\n¡ A technical capability supports other services by implementing a shared technical \nfeature.\nFigure 3.7 compares these two types of capability. SimpleBank’s orders service exposes \na capability for managing order execution — this is a business capability. The market \nservice is a technical capability; it provides a gateway to a third party that other services \n(such as exposing market information or settling trades) can reuse.\nNOTE    We’ll explore when to use business and technical capabilities and how \nyou map them to individual services in the next chapter.\n3.3.2\t\nAggregation and higher order services\nIn the early days of a microservice application, your services are likely to be flat; each \nservice is likely to have a similar level of responsibility. For example, the services in \nchapter 2 — orders, fees, transactions, and accounts — are scoped at a roughly equiva-\nlent level of abstraction.\nAs the application grows, you’ll encounter two pressures on the growth of services:\n¡ Aggregating data from multiple services to serve client requests for denormal-\nized data (for example, returning orders and fees together)\n¡ Providing specialized business logic that takes advantage of underlying capabili-\nties (for example, placing a specific type of order)\nOver time, these two pressures will lead to a hierarchy of services. Services that are closer \nto the system boundary will interact with several services to aggregate their output —  \nlet’s call those aggregators (figure 3.8). In addition, specialized services may act as coordi-\nnators for the work of multiple lower order services.\n \n",
      "content_length": 2087,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "60\nChapter 3  Architecture of a microservice application \nMarket\ndata\nTransactions\nQuery\nQuery\nHoldings\nRequest\nRequest\nOrders\nCommand\nTransactions\nMarket\nAccounts\nCommand\nCommand\nAggregator\nCoordinator\nFigure 3.8    An aggregator serves queries by joining data from underlying services, and a coordinator \norchestrates behavior by issuing commands to downstream services.\nThe challenge you’ll face is to determine when new data requirements or new appli-\ncation behavior requires a new service, rather than changes to an existing service. Cre-\nating a new service increases overall complexity and may result in tight coupling, but \nadding functionality to an existing service may make it less cohesive and more difficult \nto replace. That would bend a fundamental microservice principle.\n3.3.3\t\nCritical and noncritical paths\nAs your system evolves, some functions will naturally become more critical to your \ncustomer needs — and the successful operation of your business — than others. For \nexample, at SimpleBank, the orders service is on the critical path for order placement. \nWithout this service operating correctly, you can’t execute customer orders. Con-\nversely, other services are less important; if the customer profile service is unavailable, \nit’s less likely to affect a critical, revenue-generating component of your offering. Fig-\nure 3.9 illustrates example paths at SimpleBank.\nThis is a double-edged sword. The more services on a critical path, the more likely \nfailure will occur. Because no service is 100% reliable, the cumulative reliability of a ser-\nvice is the product of the reliability of its dependencies.\nBut microservices allow you to clearly identify these paths and treat them inde-\npendently, investing more engineering effort to maximize the resiliency and scalability \nof these paths than you invest in less crucial system areas.\n3.4\t\nCommunication\nCommunication is a fundamental element of a microservice application. Microser-\nvices communicate with each other to perform useful work. Your chosen methods for \nmicroservices to instruct and request action from other microservices determine the \nshape of the application you build.\n \n",
      "content_length": 2173,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "\t\n61\nCommunication\nPlace an order\nImplemented\nby\nImplemented\nby\nImplemented\nby\nOrders\nHoldings\nCustomers\nFees\nTransactions\nMarket data\nMarket\nThe reliability of downstream services\nwill impact upstream services.\nServices will participate\nin multiple paths.\nRetrieve holdings\nShow customer\nprofile\nFigure 3.9    Chains of services serve capabilities. Many services will participate in multiple paths.\nTIP    Network communication is also a primary source of unreliability in a \nmicroservice application. In chapter 6, we’ll explore techniques for maximiz-\ning the reliability of service-to-service communication.\nCommunication isn’t an independent architectural layer, but we’ve pulled this out into \na separate section because it blurs the boundary between the service and platform lay-\ners. Some elements — such as communication brokers — are part of the platform layer. \nBut services themselves are responsible for constructing and sending messages. You \nwant to build smart endpoints but dumb pipes.\nIn this section, we’ll discuss common patterns for microservice communication \nand how they impact the flexibility and evolution of a microservice application. Most \nmature microservice applications will mix both synchronous and asynchronous inter-\naction styles.\n3.4.1\t\nWhen to use synchronous messages\nSynchronous messages are often the first design approach that comes to mind. They’re \nwell-suited to scenarios where an action’s results — or acknowledgement of success or \nfailure — are required before proceeding with another action.\nFigure 3.10 illustrates a request–response pattern for synchronous messages. The \nfirst service constructs an appropriate message to a collaborator, which the application \nsends using a transport mechanism, such as HTTP. The destination service receives this \nmessage and responds accordingly.\n \n",
      "content_length": 1838,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "62\nChapter 3  Architecture of a microservice application \nConstruct\nmessage\nTransform\nresponse\nReceive\nresponse\nSend\nrequest\nReceive\nrequest\nTransform\nrequest\nConstruct\nresponse\nBusiness\nlogic\nBusiness\nlogic\nSend\nresponse\nTransport\nTransport\nService B\nService A\nFigure 3.10    A synchronous request–response lifecycle between two communicating services\nChoosing a transport\nThe choice of transport — RESTful HTTP, an RPC library, or something else — will \nimpact the design of your services. Each transport has different properties of latency, \nlanguage support, and strictness. For example, gRPC provides generated client/server \nAPI contracts using Protobufs, whereas HTTP is agnostic to the context of messages. \nAcross your application, using a single method of synchronous transport has econo-\nmies of scale; it’s easier to reason through, monitor, and support with tooling.\nSeparation of concerns within microservices is also important. You should separate \nyour choice of transport mechanism from the business logic of your service, which \nshouldn’t need to know about HTTP status codes or gRPC response streams. Doing \nso makes it easier to swap out different mechanisms in the future if your application’s \nneeds evolve.\nDrawbacks\nSynchronous messages have limitations:\n¡ They create tighter coupling between services, as services must be aware of their \ncollaborators.\n¡ They don’t have a strong model for broadcast or publish-subscribe models, limit-\ning your capability to perform parallel work.\n¡ They block code execution while waiting on responses. In a thread- or process- \nbased server model, this can exhaust capacity and trigger cascading failures.\n¡ Overuse of synchronous messages can build deep dependency chains, which \nincreases the overall fragility of a call path.\n3.4.2\t\nWhen to use asynchronous messages\nAn asynchronous style of messaging is more flexible. By announcing events, you make \nit easy to extend the system to handle new requirements, because services no longer \nneed to have knowledge of their downstream consumers. New services can consume \nexisting events without changing existing services.\n \n",
      "content_length": 2136,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "\t\n63\nCommunication\nTIP    Events represent post-hoc state changes. OrderCreated, OrderPlaced, \nand OrderCanceled are examples of events that the SimpleBank orders service \nmight emit.\nThis style enables more fluid evolution and creates looser coupling between services. \nThis does come at a cost: asynchronous interactions are more difficult to reason \nthrough, because overall system behavior is no longer explicitly encoded into linear \nsequences. System behavior will become increasingly emergent — developing unpredict-\nably from interactions between services — requiring investment in monitoring to ade-\nquately trace what’s happening.\nNOTE    Events enable different styles of persistence and querying, such as event \nsourcing and command query responsibility segregation (CQRS). These aren’t \na prerequisite for microservices but have some synergies with a microservice \napproach. We’ll explore them in chapter 5.\nAsynchronous messaging typically requires a communication broker, an independent sys-\ntem component that receives events and distributes them to event consumers. This is \nsometimes called an event backbone, which indicates how central to your application this \ncomponent becomes (figure 3.11). Tools commonly used as brokers include Kafka, \nRabbitMQ, and Redis. The semantics of these tools differ: Kafka specializes in high-vol-\nume, replayable event storage, whereas RabbitMQ provides higher level messaging \nmiddleware (based on the AMQP protocol (https://www.amqp.org/)).\n3.4.3\t\nAsynchronous communication patterns\nLet’s look at the two most common event-based patterns: job queue and publish-subscribe. \nYou’ll encounter these patterns a lot when architecting microservices — most higher level \ninteraction patterns are built on one of these two primitives.\nJob queue\nIn this pattern, workers take jobs from a queue and execute them (figure 3.12). A job \nshould only be processed once, regardless of how many worker instances you operate. \nThis pattern is also known as winner takes all.\nService\nPublishes\nevents\nEvent producers are unaware\nof event consumers.\nEvents indicate something “interesting”\nhas happened, for example, state change.\nEvent\nEvent\nEvent\nEvent\nEvent broker\nListen for\nevents\nListen for\nevents\nService C\nService B\nConsumers are unaware of\nwhich service emits events.\nFigure 3.11    Event-driven asynchronous communication between services\n \n",
      "content_length": 2388,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "64\nChapter 3  Architecture of a microservice application \nOrder\nMarket\ngateway 1\nService instances\nshare work.\nMarket\ngateway 2\nPick\nPick\nOrder Created\n4\n3\n2\nEach job is processed once.\n1\n1\n3\n2\n4\nFigure 3.12    A job queue distributes work to 1 to n consumers\nYour market gateway could operate in this fashion. Each order that the orders service \ncreates will trigger an OrderCreated event, which will be queued for the market gate-\nway service to place it. This pattern is useful where\n¡ A 1:1 relationship exists between an event and work to be done in response to \nthat event.\n¡ The work that needs to be done is complex or time-consuming, so it should be \ndone out-of-band from the triggering event.\nBy default, this approach doesn’t require sophisticated event delivery. Many task queue \nlibraries are available that use commodity data stores, such as Redis (Resque, Celery, \nSidekiq) or SQL databases.\nPublish-subscribe\nIn publish-subscribe, services trigger events for arbitrary listeners. All listeners that \nreceive the event act on it appropriately. In some ways, this is the ideal microservice \npattern: a service can send arbitrary events out into the world without caring who acts \non them (figure 3.13).\nMarket\ngateway\nPublish \nOrder Placed\nEvent\nSubscribe\nSubscribe\nSubscribe\nStatistics\nService N\nYou can add arbitrary\nlisteners at any point.\nNotify\nEvent\nEvent broker\nFigure 3.13    How publish-subscribe sends events out to subscribers\n \n",
      "content_length": 1455,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "\t\n65\nCommunication\nFor example, imagine you need to trigger other downstream actions once an order has \nbeen placed. You might send a push notification to the customer or use it to feed your \norder statistics and recommendation feature. These features can all listen for the same \nevent.\n3.4.4\t\nLocating other services\nTo wrap up this section, let’s take a moment to examine service discovery. For services to \ncommunicate, they need to be able to discover each other. The platform layer should \noffer this capability.\nA rudimentary approach to service discovery is to use load balancers (figure 3.14). \nFor example, an elastic load balancer (ELB) on AWS is assigned a DNS name and man-\nages health checking of underlying nodes, based on their membership in a group of \nvirtual machines (an auto-scaling group on AWS).\nThis works but doesn’t handle more complex scenarios. What if you want to route \ntraffic to different versions of your code to enable canary deployments or dark launches, \nor if you want to route traffic across different data centers?\nA more sophisticated approach is to use a registry, such as Consul (https://www.consul.io). \nService instances announce themselves to a registry, which provides an API — either through \nDNS or a custom mechanism for resolving requests for those services. Figure 3.15 illustrates \nthis approach.\nYour service discovery needs will depend on the complexity of your deployed applica-\ntion’s topology. More complex deployments, such as geographical distribution, require \nmore robust service discovery architecture.1\nNOTE    When you deploy to Kubernetes in chapter 9, you’ll learn about services, \nthe mechanism that Kubernetes uses to provide discovery.\nOrders\nForwards request\nto service\ninstances\nOn creation, notifies name server\nName server\nReturns IP\n192.8.1.2\norders.simple-bank\n.internal\nRequests 192.8.1.2\nService A\nLooks up DNS for\norders.simple-bank.internal\nLB\nOrders\nFigure 3.14    Service discovery using load balancers and known DNS names\n1\t bit.ly/2o86ShQ is a great place to start if you’re interested in further exploring different types of \nproxies and load balancing.\n \n",
      "content_length": 2140,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "66\nChapter 3  Architecture of a microservice application \nService A\n2. Lookup for\norders\n3. Return IP/port\nfor orders\n4. Request to orders\nOrders\n1. Register\nRegistry\nFigure 3.15    Service discovery using a service registry as a source of truth\n3.5\t\nThe application boundary\nA boundary layer provides a façade over the complex interactions of your internal ser-\nvices. Clients, such as mobile apps, web-based user interfaces, or IoT devices, may inter-\nact with a microservice application. (You might build these clients yourself, or third \nparties consuming a public API to your application may build them.) For example, \nSimpleBank has internal admin tools, an investment website, iOS and Android apps, \nand a public API, as depicted in figure 3.16.\nThe boundary layer provides an abstraction over internal complexity and change (fig-\nure 3.17). For example, you might provide a consistent interface for a client to list all \nhistoric orders, but, over time, you might completely refactor the internal implementa-\ntion of that functionality. Without this layer, clients would require too much knowledge \nof individual services, becoming tightly coupled to your system implementation.\nMicroservice\napplication\nAdmin UI\nPublic API\nInvestment\nwebsite\niOS app\nAndroid app\nFigure 3.16    Client applications at SimpleBank\n \n",
      "content_length": 1322,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "\t\n67\nThe application boundary\nHTTP\nHTTP\nThe boundary encapsulates\nbackend complexity.\ngRPC\ngRPC\nBoundary\nResponses are in\nclient-appropriate formats.\nService\nClient\nFigure 3.17    A boundary provides a façade over the service layer to hide internal complexity from a \nconsumer.\nSecond, the boundary tier provides access to data and functionality using a transport \nand content type appropriate to the consumer. For example, whereas services might \ncommunicate between each other with gRPC, a façade can expose an HTTP API to \nexternal consumers, which is much more appropriate for external applications to \nconsume.\nCombining these roles allows your application to become a black box, performing \nwhatever (unknown to the client) operations to deliver functionality. You also can make \nchanges to the service layer with more confidence, because the client interfaces with it \nthrough a single point.\nThe boundary layer also may implement other client-facing capabilities:\n¡ Authentication and authorization  — To verify the identity and claims of an API client\n¡ Rate limiting  — To provide defense against client abuse\n¡ Caching  — To reduce overall load on the backend\n¡ Collect logs and metrics  — To allow analysis and monitoring of client requests\nPlacing these edge capabilities in the boundary layer provides clear separation of \nconcerns — without a boundary, backend services would need to individually \nimplement these concerns, increasing their complexity.\nYou might also use boundaries within your service tier to separate domains. For exam-\nple, an order placement process might consist of several services, but only one of those \nservices should expose an entry point that other domains can access (figure 3.18).\nNOTE    Internal service boundaries often reflect bounded contexts: cohesive, \nbounded subsets of the overall application domain. We’ll explore them more \nin the next chapter.\nThat provides an overview for how you can use boundaries. Let’s get more specific and \nexplore three different (albeit related) patterns for application boundaries: API gate-\nways, backends for frontends, and consumer-driven gateways.\n \n",
      "content_length": 2140,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "68\nChapter 3  Architecture of a microservice application \nOrder management\nDifferent subsystems might\nexpose their own internal\nfaçade within a microservice\napplication.\nBoundary\nFees\nFigure 3.18    Boundaries might be present between different contexts within a microservice application.\n3.5.1\t\nAPI gateways\nWe introduced the API gateway pattern in chapter 2. An API gateway provides a single \nclient-entry point over a service-oriented backend. It proxies requests to underlying \nservices and transforms their responses. An API gateway might handle other cross-cut-\nting client concerns, such as authentication and request signing.\nTIP    API gateways that are available include such open source options as \nMashape’s Kong, as well as commercial offerings, such as AWS API Gateway.\nFigure 3.19 illustrates an API gateway. The gateway authenticates a request, and if that \nsucceeds, it proxies the request to an appropriate backend service. It transforms the \nresults it receives so that when it returns them, they’re palatable for your consuming \nclients.\nAPI gateway\ngRPC\ngRPC\nThe gateway authenticates, routes,\nand transforms a client request.\nService\nAuthentication\nRouting\nTransformation\nTransformation\nHTTP\nHTTP\nClient\nFigure 3.19    An API gateway serving a client request\n \n",
      "content_length": 1283,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "\t\n69\nThe application boundary\nA gateway also allows you to minimize the exposed area of your system from a security \nperspective by deploying internal services in a private network and restricting ingress \nto all but the gateway.\nWARNING    Sometimes an API gateway might perform API composition: com-\nposing responses from multiple services into a single response. The line \nbetween this and service layer aggregation is fuzzy. It’s best to be cautious and \ntry to avoid business logic bleeding into the gateway itself, which can overly \nincrease coupling between the gateway and underlying services.\n3.5.2\t\nBackends for frontends\nThe backends for frontends (BFF) pattern is a variation on the API gateway approach. \nAlthough the API gateway approach is elegant, it has a few downsides. If the API gate-\nway acts as a composition point for multiple applications, it’ll begin to take on more \nresponsibility.\nFor example, imagine you serve both desktop and mobile applications. Mobile \ndevices have different needs, displaying less data with less available bandwidth, and dif-\nferent user features, such as location and context awareness. In practice, this means \ndesktop and mobile API needs diverge, which increases the breadth of functionality \nyou need to integrate into a gateway. Different needs, such as the amount of data (and \ntherefore payload size) returned for a given resource, may also conflict. It can be hard \nto balance these competing forces while building a cohesive and optimized API.\nIn a BFF approach, you use an API gateway for each consuming client type. To take \nthe earlier example from SimpleBank, each user service they offered would have a \nunique gateway (figure 3.20).\nAdmin UI\niOS app\nAndroid app\nAdmin gateway\nWebsite gateway\niOS gateway\nAndroid gateway\nEach client has a\nspecialized API\nbackend.\nInvestment\nwebsite\nMicroservice\napplication\nFigure 3.20    The backends for frontends pattern for SimpleBank’s client applications\n \n",
      "content_length": 1963,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "70\nChapter 3  Architecture of a microservice application \nDoing so allows the gateway to be highly specific and responsive to the needs of its \nconsumer without bloat or conflict. This results in smaller, simpler gateways and more \nfocused development.\n3.5.3\t\nConsumer-driven gateways\nIn both previous patterns, the API gateway determines the structure of the data it \nreturns to your consumer. To serve different clients, you might build unique backends. \nLet’s flip this around. What if you could build a gateway that allowed consumers to \nexpress exactly what data they needed from your service? Think of this like an evolu-\ntion of the BFF approach: rather than building multiple APIs, you can build a single \n“super-set” API that allows consumers to define the shape of response they require.\nYou can achieve this using GraphQL. GraphQL is a query language for APIs that \nallows consumers to specify which data fields they want and to multiplex different \nresources into a single request. For example, you might expose the following schema \nfor SimpleBank clients.\nListing 3.1    Basic GraphQL schema for SimpleBank\ntype Account { \n  id: ID! \n  name: String!\n  currentHoldings: [Holding]! \n  orders: [Order]!\n}\ntype Order {\n  id: ID!\n  status: String!\n  asset: Asset!\n  quantity: Float!\n}\ntype Holding {\n  asset: Asset!\n  quantity: Float!\n}\ntype Asset {\n  id: ID!\n  name: String!\n  type: String!\n  price: Float!\n}\ntype Root {\n  accounts: [Account]! \n  account(id: ID): Account \n}\nschema: {\n  query: Root \n}\nThe ! indicates the field is non-nullable.\nAn account contains lists of Holding and Order.\nReturns all accounts or an account by its ID\nThe schema has a single entry point for queries.\n \n",
      "content_length": 1699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "\t\n71\nClients\nThis schema exposes a customer’s accounts, as well as orders and holdings against each \nof those accounts. Clients then execute queries against this schema. If a mobile app \nscreen shows holdings and outstanding orders for an account, you could retrieve that \ndata in a single request, as shown in the following listing.\nListing 3.2    Request body using GraphQL\n{\n  account(id: \"101\") { \n    orders \n    currentHoldings \n  }\n}\nIn the backend, your GraphQL server would act like an API gateway, proxying and \ncomposing that data from multiple backend services (in this case, orders and hold-\nings). We won’t drill into GraphQL in further detail in this book, but if you’re inter-\nested, the official documentation (http://graphql.org/) is a great place to start. We’ve \nalso had some success using Apollo (https://www.apollographql.com/) to provide a \nGraphQL API façade over RESTful backend services.\n3.6\t\nClients\nThe client tier, like the presentation layer in the three-tier architecture, presents to \nyour users an interface to your application. Separating this layer from those below it \nallows you to develop user interfaces in a granular fashion and to serve the needs of \ndifferent types of clients. This also means you can develop the frontend independently \nfrom backend features. As mentioned in the previous section, your application may \nneed to serve many different clients — mobile devices, websites, both internal and \nexternal — each with different technology choices and constraints.\nIt’s unusual for a single microservice to serve its own user interface. Typically, the \nfunctionality exposed to a given set of users is broader than the capabilities of a single \nservice. For example, administrative staff at SimpleBank might deal with order manage-\nment, account setup, reconciliation, tax, and so on. And this comes with cross-cutting \nconcerns — authentication, audit logging, user management — that are clearly not the \nresponsibility of an orders or account setup service.\n3.6.1\t\nFrontend monoliths\nYour backend is straightforward to split into independently deployable and maintain-\nable services — well, relatively, you still have another 10 chapters to go. But this can be \nchallenging to achieve on the frontend. A typical frontend over a microservice applica-\ntion might still be a monolith that’s deployed and changed as a single unit (figure 3.21). \nSpecialist frontends, particularly mobile applications, often demand dedicated teams, \nmaking end-to-end feature ownership difficult to practically achieve.\nNOTE    We’ll talk more about end-to-end ownership (and why it’s desirable and \nbeneficial when developing microservice applications) in chapter 13.\nFilters by account ID\nRequests specific member fields in response\n \n",
      "content_length": 2768,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "72\nChapter 3  Architecture of a microservice application \n...regardless of\nthe backend\ndecomposition.\nAPI gateway\nBackend services\nFrontend\nOrders\nFees\nAccounts\nCustomers\nTax\nAll functionality gets\nadded to the same\nfrontend application...\nFigure 3.21    A typical frontend client in a microservice application can become monolithic.\n3.6.2\t\nMicro-frontends\nAs frontend applications grow larger, they begin to encounter the same coordination \nand friction issues that plague large-scale backend development. It’d be great if you \ncould split frontend development in the same way you can split your backend services. \nAn emerging trend in web applications is micro-frontends — serving fragments of a \nUI as independently packaged and deployable components that you can compose \ntogether. Figure 3.22 illustrates this approach.\nThis would allow each microservice team to deliver functionality end to end. For \nexample, if you had an orders team, it could independently deliver both order manage-\nment microservices and the web interface required to place and manage orders.\nShared/generic\ncomponents, such as\nnavigation\nIndependent\ncomponents\nComponents may\ncommunicate with\neach other.\nFigure 3.22    A user interface composed from independent fragments\n \n",
      "content_length": 1254,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "\t\n73\nSummary\nAlthough promising, this approach has many challenges:\n¡ Visual and interaction consistency across different components requires \nnontrivial effort to build and maintain common components and design \nprinciples.\n¡ Bundle size (and therefore load time) can be difficult to manage when loading \nJavaScript code from multiple sources.\n¡ Interface reloads and redraws can cause overall performance to suffer.\nMicro-frontends aren’t yet commonplace, but people are using several different tech-\nnical approaches in the wild, including\n¡ Serving UI fragments as web components with a clear, event-driven API\n¡ Integrating fragments using client-side includes\n¡ Using iframes to serve micro-apps into separate screen sections\n¡ Integrating components at the cache layer using edge side includes (ESI)\nIf you’re interested in learning more, Micro Frontends (https://micro-frontends.org/) \nand Zalando’s Project Mosaic (https://www.mosaic9.org/) are great starting points.\nSummary\n¡ Individually, microservices are similar internally to monolithic applications.\n¡ A microservice application is like a neighborhood: its final shape isn’t prescribed \nbut instead guided by principles and a high-level conceptual model.\n¡ The principles that guide microservice architecture reflect organizational goals \nand inform team practices.\n¡ Your architectural plan should encourage growth along good lines, rather than \ndictate approaches for your overall application.\n¡ A microservice application consists of four layers: platform, service, boundary, \nand client.\n¡ The platform layer provides tooling, plumbing, and infrastructure to support the \ndevelopment of product-oriented microservices.\n¡ Synchronous communication is often the first choice in a microservice applica-\ntion and is best suited to command-type interactions, but it has drawbacks and \ncan increase coupling and fragility.\n¡ Asynchronous communication is more flexible and amenable to rapid system \nevolution, at the cost of added complexity.\n¡ Common asynchronous communication patterns include queues and \npublish-subscribe.\n¡ The boundary layer provides a façade over your microservice application that’s \nappropriate for external consumers.\n \n",
      "content_length": 2211,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "74\nChapter 3  Architecture of a microservice application \n¡ Common types of boundaries include API gateways and consumer-driven gate-\nways, such as GraphQL.\n¡ Client applications, such as websites and mobile applications, interact with your \nmobile backend through the boundary layer.\n¡ Clients risk becoming monolithic, but techniques are beginning to emerge for \napplying microservice principles to frontend applications.\n \n",
      "content_length": 426,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "75\n4\nDesigning new features\nThis chapter covers\n¡ Scoping microservices based on business \ncapabilities and use cases\n¡ When to scope microservices to reflect \ntechnical capabilities\n¡ Making design choices when service \nboundaries are unclear\n¡ Scoping effectively when multiple teams own \nmicroservices\nDesigning a new feature in a microservice application requires careful and well-reasoned \nscoping of microservices. You need to decide when to build new services or extend \nexisting services, where boundaries lie between those services, and how those services \nshould collaborate.\nWell-designed services have three key characteristics: they’re responsible for a \nsingle capability, independently deployable, and replaceable. If your microservices \nhave the wrong boundaries, or are too small, they can become tightly coupled, mak-\ning them challenging to deploy independently or replace. Tight coupling increases \nthe impact, and therefore the risk, of change. If your services are too large — taking \n \n",
      "content_length": 1009,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "76\nChapter 4  Designing new features\non too much responsibility — they become less cohesive, increasing friction in ongoing \ndevelopment.\n Even if you get it right the first time, you need to keep in mind that the requirements \nand needs of most complex software applications will evolve over time, and approaches \nthat worked early in that application’s lifetime may not always remain suitable. No \ndesign is perfect forever. \nYou’ll face additional challenges in longer running applications (and larger engi-\nneering organizations). Your services may rely on a web of dependencies managed by \nmultiple teams — as an engineer in one team, you’ll need to design cohesive function-\nality while relying on services that won’t necessarily be under your control. And you’ll \nneed to know when to retire and migrate away from services that no longer meet the \nneeds of the wider system.\nIn this chapter, we’ll walk you through designing a new feature using microservices. \nWe’ll use that example to explore techniques and practices that you can use to guide \nthe design of maintainable microservices in both new and longer running microservice \napplications.\n4.1\t\nA new feature for SimpleBank\nRemember SimpleBank? The team is doing well — customers love their product! But \nSimpleBank has discovered that most of those customers don’t want to pick their own \ninvestments — they’d much rather have SimpleBank do the hard work for them. Let’s \ntake this problem and work out how to solve it with a microservice application. In the \nnext few sections, we’ll develop the design in four stages:\n1\t Understanding the business problem, use cases, and potential solution\n2\t Identifying the different entities and business capabilities your services should \nsupport\n3\t Scoping services that are responsible for those capabilities\n4\t Validating your design against current and potential future requirements\nThis will build on the small collection of services we explored in chapters 2 and 3: \norders, market gateway, account transactions, fees, market data, and holdings.\nFirst, let’s understand the business problem you’re trying to solve. In the real \nworld, you could carry out the discovery and analysis of business problems using sev-\neral techniques, such as market research, customer interviews, or impact mapping. \nAs well as understanding the problem, you’d need to decide whether it was one your \ncompany should solve. Luckily, this isn’t a book about product management — you \ncan skip that part.\nNOTE    We haven’t tried to extrapolate a general approach to understanding \nbusiness problems — that’s another book altogether. \nUltimately, SimpleBank’s customers want to invest money, either up front or on a reg-\nular basis, and see their wealth increase, either over a defined period or to meet a \n \n",
      "content_length": 2797,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "\t\n77\nA new feature for SimpleBank\nspecific goal, such as a deposit on a house. Currently, SimpleBank’s customers need to \nchoose how their money is invested — even if they don’t have a clue about investing. \nAn uninformed investor might choose an asset based on high predicted returns, with-\nout realizing that higher returns typically mean significantly higher risk. \nTo solve this problem, SimpleBank could make investment decisions on the custom-\ner’s behalf by allowing the customer to choose a premade investment strategy. An invest-\nment strategy consists of proportions of different asset types — bonds, shares, funds, \nand so on — designed for a certain level of risk and investment timeline. When a cus-\ntomer adds money to their account, SimpleBank will automatically invest that money in \nline with this strategy. This setup is summarized in figure 4.1.\nSimpleBank\nCustomer\nGenerate\nstrategy orders\nInvest money\nSelect strategy\nCreate account\n<<Include>>\n<<Include>>\n<<Include>>\nDefine\navailable\nstrategies\nInvestment strategies\nSelect assets\nFigure 4.1    Potential use cases to support defining and selecting investment strategies \n \n",
      "content_length": 1147,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "78\nChapter 4  Designing new features\nBased on figure 4.1, you can start to identify the use cases you need to satisfy to solve \nthis problem:\n¡ SimpleBank must be able to create and update available strategies.\n¡ A customer must be able to create an account and elect an appropriate invest-\nment strategy.\n¡ A customer must be able to invest money using a strategy, and investing in a strat-\negy generates appropriate orders.\nOver the next few sections, we’ll explore these use cases. When identifying use cases in \nyour own domain, you may prefer to use a more structured and exhaustive approach, \nsuch as behavior-driven development (BDD) scenarios. What’s important is that you \nstart to establish a concrete understanding of the problem, which you then can use to \nvalidate an acceptable solution.\n4.2\t\nScoping by business capabilities\nAfter you’ve identified your business requirements, your next step is to identify the \ntechnical solution: which features you need to build and how you’ll support them \nwith existing and new microservices. Choosing the right scope and purpose for each \nmicroservice is essential to building a successful and maintainable microservice \napplication.\nThis process is called service scoping. It’s also known as decomposition or partition-\ning. Breaking apart an application into services is challenging — as much art as science. \nIn the following sections, we’ll explore three strategies for scoping services:\n¡ By business capability or bounded context  — Services should correspond to relatively \ncoarse-grained, but cohesive, areas of business functionality.\n¡ By use case  — Services should be verbs that reflect actions that will occur in a \nsystem.\n¡ By volatility  — Services should encapsulate areas where change is likely to occur \nin the future.\nYou don't necessarily use these approaches in isolation; in many microservice appli-\ncations, you’ll combine scoping strategies to design services appropriate to different \nscenarios and requirements.\n4.2.1\t\nCapabilities and domain modeling\nA business capability is something that an organization does to generate value and \nmeet business goals. Microservices that are scoped to business capabilities directly \nreflect business goals. In commercial software development, these goals are usually the \nprimary drivers of change within a system; therefore, it’s natural to structure the system \nto encapsulate those areas of change. You’ve seen several business capabilities imple-\nmented in services so far: order management, transaction ledgers, charging fees, and \nplacing orders to market (figure 4.2).\n \n",
      "content_length": 2598,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "\t\n79\nScoping by business capabilities\nFunctions\nCapabilities and\nservices\n Place order to market\nCharge order fees to\naccounts\nRecord transactions\noccurring on account\nManage order status\nFees service\nMarket service\nOrders service\nAccount\ntransactions\nservice\nTransactions\nOrder management\nFees\nFigure 4.2    Functions that existing microservices provide and their relationship to business capabilities \nperformed by SimpleBank\nBusiness capabilities are closely related to a domain-driven design approach. Domain-\ndriven design (DDD) was popularized by Eric Evans’ book of the same name and focuses \non building systems that reflect a shared, evolving view, or model, of a real-world domain.1 \nOne of the most useful concepts that Evans introduced was the notion of a bounded con-\ntext. Any given solution within a domain might consist of multiple bounded contexts; the \nmodels inside each context are highly cohesive and have the same view of the real world. \nEach context has a strong and explicit boundary between it and other contexts.\nBounded contexts are cohesive units with a clear scope and an explicit external bound-\nary. This makes them a natural starting point for scoping services. Each context demar-\ncates the boundaries between different areas of your solution. This often has a close \ncorrespondence with organizational boundaries; for example, an e-commerce company \nwill have different needs — and different teams — for shipping versus customer payments.\nTo begin with, a context typically maps directly to a service and an area of business \ncapability. As the business grows and becomes more complex, you may end up breaking \na context down into multiple subcapabilities, many of which you’ll implement as inde-\npendent, collaborating services. From the perspective of a client, though, the context \nmay still appear as a single logical service.\nTIP    The API gateway pattern we discussed in chapter 3 can be useful for estab-\nlishing boundaries between different contexts (and underlying groups of ser-\nvices) within your application.\n4.2.2\t\nCreating investment strategies \nYou can design services to support creating investment strategies using a business capa-\nbility approach. You might want to get a sketch pad to work through this one. To help \nyou work through this example and give the use case more shape, we’ve wireframed \nwhat the UI for this feature might look like in figure 4.3.\n1\t Although many of the implementation patterns — repositories, aggregates, and factories — are quite \nspecific to object-oriented programming, many of Evans’ analysis techniques — such as ubiquitous \nlanguage — are useful in any programming paradigm.\n \n",
      "content_length": 2668,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "80\nChapter 4  Designing new features\nFigure 4.3    A user interface for an admin user to create new investment strategies\nTo design services by business capability, it’s best to start with a domain model: some \ndescription of the functions your business performs in your bounded context(s) and \nthe entities that are involved. From figure 4.3, you’ve probably identified these already. \nA simple investment strategy has two components: a name and a set of assets, each with \na percentage allocation. An administrative staff member at SimpleBank will create a \nstrategy. We’ve drafted those entities in figure 4.4.\nThe design of these entities helps you understand the data your services own and \npersist. Only three entities, and it already looks like you’ve identified (at least) two new \nservices: user management and asset information. The user and asset entities are both \npart of distinct bounded contexts:\n¡ User management  — This covers features like sign-up, authentication, and \nauthorization. In a banking environment, authorization for different \nresources and functionality is subject to strict controls for security, regulatory, \nand privacy reasons.\n \n",
      "content_length": 1167,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "\t\n81\nScoping by business capabilities\nCreates\nContains\nAllocated to\n0..*\n0..*\n1\n1\n1\n0..*\n+ Id:integer\n+ Code:string\n+ Sector:string\n+ Id:integer\n+ Name:string\n+ Email:string\n+ Asset:Asset\n+ Percentage:float\n+ Id:integer\n+ Name:string\n+ Allocations:List<AssetAllocation>\nInvestment Strategy entity\nAsset Allocation\nAsset information service\nUser management service\nUser entity\nAsset entity\nInvestment strategies service\nFigure 4.4    The first draft of a domain model made up of entities to support the creation of investment \nstrategies\n¡ Asset information  — This covers integration with third-party providers of market \ndata, such as asset prices, categories, classification, and financial performance. \nThis capability would include asset search, as required by your user interface \n(figure 4.3). \nInterestingly, these different domains reflect the organization of SimpleBank itself. \nA dedicated operational team manages asset data; likewise, user management. This \ncomparability is desirable, as it means your services will reflect real-world lines of cross-\nteam communication.\nMore on that later — let’s get back to investment strategies. You know that \nyou can associate them with customer accounts and use them to generate orders. \nAccounts and orders are both distinct bounded contexts, but investment strategies \ndon’t belong in either one. When strategies change, the change is unlikely to affect \naccounts or orders themselves. Conversely, adding investment strategies to either of \nthose existing services will hamper their replaceability, making them less cohesive \nand less amenable to change.\nThese factors indicate that investment strategies are a distinct business capability, \nrequiring a new service. Figure 4.5 illustrates the relationships between this context and \nyour existing capabilities.\n \n",
      "content_length": 1819,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "82\nChapter 4  Designing new features\nOrders\nHave\nAssets\nResult in\nInvestment\nstrategies\nUser management\nManage\nAdmin users\nInvest using\nConsist of\nBuy/sell\nHold positions in\nAccounts\nCustomers\nAsset information\nFigure 4.5    Relationships between your new business capability and other bounded contexts within the \nSimpleBank application\nYou can see that some contexts are aware of information that belongs to other con-\ntexts. Some entities within your context are shared: they’re conceptually the same but \ncarry unique associations or behavior within different contexts. For example, you use \nassets in multiple ways:\n¡ The strategy context records the allocation of assets to different strategies.\n¡ The orders context manages the purchase and sale of assets.\n¡ The asset context stores fundamental asset information for use by multiple con-\ntexts, such as pricing and categorization.\nThe model we’ve drawn out in figure 4.5 doesn’t tell you much about the behavior of \na service; it only tells you the business scope your services cover. Now that you have a \nfirmer idea of where your service boundaries lie, you can draft out the contract that \nyour service offers to other services or end users.\nNOTE    Don’t worry too much about what technology you’ll use for communica-\ntion at this stage. The examples in this chapter could easily apply to any point-\nto-point messaging approach.\nFirst, your investment strategies service needs to expose methods for creating and \nretrieving investment strategies. Other services or your UI can then access this data. \nLet’s draft out an endpoint that allows creating an investment strategy. The example \nshown in listing 4.1 uses the OpenAPI specification (formerly known as Swagger), \nwhich is a popular technique for designing and documenting REST API interfaces. If \nyou’re interested in learning more, the Github page for the OpenAPI specification2 is \na good place to start.\n2\t See https://github.com/OAI/OpenAPI-Specification.\n \n",
      "content_length": 1980,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "\t\n83\nScoping by business capabilities\nListing 4.1    API for the investment strategies service\nopenapi: \"3.0.0\"\ninfo: \n  title: Investment Strategies \nservers: \n  - url: https://investment-strategies.simplebank.internal \npaths:\n  /strategies: \n    post: \n      summary: Create an investment strategy\n      operationId: createInvestmentStrategy\n      requestBody: \n        description: New strategy to create\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/NewInvestmentStrategy' \n      responses:\n        '201':\n          description: Created strategy\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/InvestmentStrategy' \ncomponents: \n  schemas:\n    NewInvestmentStrategy: \n      required:\n        - name\n        - assets\n      properties:\n        name:\n          type: string\n        assets: \n          type: array \n          items: \n            $ref: '#/components/schemas/AssetAllocation' \n    AssetAllocation:\n      required:\n        - assetId\n        - percentage\n      properties:\n        assetId:\n          type: string\n        percentage:\n          type: number\n          format: float\n    InvestmentStrategy:\n      allOf:\nStarts with some metadata about your API\nDefines a “POST /strategies” path\nThe body of this request should be \nthe new investment strategy.\nRefers to a location elsewhere in the \ndocument: the components key\nDefines the response type \nin the components section\nDefines reusable data types\nA new investment strategy type\nContains a list of assets \nof type AssetAllocation\n \n",
      "content_length": 1652,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "84\nChapter 4  Designing new features\n        - $ref: '#/components/schemas/NewInvestmentStrategy' \n        - required:\n          - id\n          - createdByUserId\n          - createdAt\n          properties:\n            id:\n              type: integer\n              format: int64\n            createdByUserId:\n              type: integer\n              format: int64\n            createdAt:\n              type: string\n              format: date-time\nIf you’re going to use strategies again later — and you are — you’ll need to retrieve them. \nImmediately under your paths: element in listing 4.1, add the code in the following listing.\nListing 4.2    API for retrieving strategies from the investment strategies service\n  /strategies/{id}: \n    get:\n      description: Returns an investment strategy by ID\n      operationId: findInvestmentStrategy\n      parameters: \n        - name: id \n          in: path \n          description: ID of strategy to fetch \n          required: true \n          schema: \n            type: integer \n            format: int64 \n      responses:\n        '200':\n          description: investment strategy\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/InvestmentStrategy' \nYou also should consider what events this service should emit. An event-based model \naids in decoupling services from each other, ensuring that you can choreograph long-\nterm interactions, rather than explicitly orchestrate them.\nWARNING    Anticipating future use cases of a service is one of the most difficult ele-\nments of service design. But building flexible APIs and integration points between \nservices reduces the need for future rework and coordination between teams.\nThe InvestmentStrategy type extends the \nNewInvestmentStrategy and adds fields, \nbased on your entity model.\nThe path for retrieving an investment strategy\nDefines the format of the ID\nReturns an investment strategy\n \n",
      "content_length": 1956,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "\t\n85\nScoping by business capabilities\nEndpoints\nEvents\nInvestment\nstrategies\nStrategy Created\nCreate Strategy\nGet Strategy\nFigure 4.6    The inbound and outbound contract of your investment strategies microservice\nFor example, imagine that creating a strategy will trigger email notifications to poten-\ntially interested customers. This is separate from the scope of the investment strate-\ngies service itself; it has no knowledge of customers (or their preferences). This is an \nideal use case for events. If a POST to /strategies post-hoc triggers an event — let’s \ncall it StrategyCreated — then arbitrary microservices can listen for that event and act \nappropriately. Figure 4.6 illustrates the full scope of your service’s API.\nGreat work — you’ve identified all the capabilities that you require to support this \nuse case. To see how this fits together, you can map the investment strategies service and \nthe other capabilities you’ve identified to the wireframe (figure 4.7).\nLet’s summarize what you’ve done so far:\n1\t For a sample problem, you’ve identified functions the business performs to gen-\nerate value and the natural seams between different areas of SimpleBank’s busi-\nness domain.\n2\t You’ve used that knowledge to identify boundaries within your microservice appli-\ncation, identifying entities and responsibility for different capabilities.\n3\t You’ve scoped your system into services that reflect those domain boundaries.\nCreate strategy\nGet strategy\nInvestment Strategies\nUser management\nWe've scoped the boundaries\nof this service.\nWe’ve identified capabilities\nthat need to be implemented\nin services.\nSearch for assets\nAsset information\nGet asset\nGet user\nFigure 4.7    Identified capabilities and services mapped to how they’d support functionality in the create \ninvestment strategy user interface\n \n",
      "content_length": 1827,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "86\nChapter 4  Designing new features\nThis approach results in services that are relatively stable, cohesive, oriented to busi-\nness value, and loosely coupled.\n4.2.3\t\nNested contexts and services\nEach bounded context provides an API to other contexts, while encapsulating internal \noperation. Let’s take asset information as an example (figure 4.8):\n¡ It exposes methods that other contexts can use, such as searching for and retriev-\ning assets.\n¡ Third-party integrations or specialist teams within SimpleBank populate asset \ndata.\nThe private/public divide provides a useful mechanism for service evolution. Early in a \nsystem’s lifecycle, you might choose to build coarser services, representing a high-level \nboundary. Over time, you might decompose services further, exposing behavior from \nnested contexts. Doing this maintains replaceability and high cohesion, even as busi-\nness logic increases in complexity.\nOther contexts\nInteract with external interface\nPublic\nPrivate\nGet asset\nSearch for asset\nPricing\nClassification\nInvestment research\nNested contexts are\npotential service partitions.\nAsset information\nFigure 4.8    A context exposes an external interface and may itself contain nested contexts\n \n",
      "content_length": 1215,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "\t\n87\nScoping by use case\n4.2.4\t\nChallenges and limitations\nIn the previous sections, you identified the natural seams within the organization’s \nbusiness domain and applied them to partition your services. This approach is effec-\ntive because it maps services to the functional structure of a business — directly reflect-\ning the domain in which an organization operates. But it’s not perfect.\nRequires substantial business knowledge \nPartitioning by business capabilities requires having significant understanding of the \nbusiness or problem domain. This can be difficult. If you don’t have enough infor-\nmation — or you’ve made the wrong assumptions — you can’t be completely certain \nyou’re making the right design decisions. Understanding the needs of any business \nproblem is a complex, time-consuming and iterative process.\nThis problem isn’t unique to microservices, but misunderstanding the business \nscope — and reflecting it incorrectly in your services — can incur higher refactoring \ncosts in this architecture, because both data and behavior can require time-consuming \nmigration between services. \nCoarse-grained services keep growing\nSimilarly, a business capability approach is biased toward the initial development of \ncoarse-grained services that cover a large business boundary — for example, orders, \naccounts, or assets. New requirements increase the breadth and depth of that area, \nincreasing the scope of the service’s responsibility. These new reasons to change can \nviolate the single responsibility principle. It’ll be necessary to partition that service fur-\nther to maintain an acceptable level of cohesion and replaceability.\nWARNING    Service teams sometimes add functionality to existing microservices \nbecause it’s easy — a deployable unit already exists — rather than investing \nmore time to create a new service or repartition the existing service appropri-\nately. Although teams sometimes need to make pragmatic decisions, they need \nto exercise discipline to minimize this source of technical debt.\n4.3\t\nScoping by use case\nSo far, your services have been nouns, oriented around objects and things that exist \nwithin the business domain. An alternative approach to scoping is to identify verbs, or \nuse cases within your application, and build services to match those responsibilities. \nFor example, an e-commerce site might implement a complex sign-up flow as a micro-\nservice that interacts with other services, such as user profile, welcome notifications, \nand special offers.\nThis approach can be useful when\n¡ A capability doesn’t clearly belong in one domain or interacts with multiple \ndomains.\n¡ The use case being implemented is complex, and placing it in another service \nwould violate single responsibility.\n \n",
      "content_length": 2759,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "88\nChapter 4  Designing new features\nLet’s apply this approach to SimpleBank to understand how it differs from noun-ori-\nented decomposition. Get your pencil and paper ready!\n4.3.1\t\nPlacing investment strategy orders \nA customer can invest money into an investment strategy. This will generate appropri-\nate orders; for example, if the customer invests $1,000, and the strategy specifies 20% \nshould be invested in Stock ABC, an order will be generated to purchase $200 of ABC.\nThis raises several questions:\n1\t How does SimpleBank accept money for investment? Let’s assume a customer \ncan make an investment by external payment (for example, a credit card or bank \ntransfer).\n2\t Which service is responsible for generating orders against a strategy? How does \nthis relate to your existing orders and investment strategies services?\n3\t How do you keep track of orders made against strategies?\nYou could build this capability into your existing investment strategies service. \nBut placing orders might unnecessarily widen the scope of responsibility that \nthe service encapsulates. Likewise, the capability doesn’t make sense to add to the \norders service. Coupling all possible sources of orders to that service would give it \ntoo many reasons to change.\nYou can draft out an independent service for this use case as a starting point — call it \nPlaceStrategyOrders. Figure 4.9 sketches out how you’d expect this service to behave. \nConsider the input to this service. For orders to be placed, this service needs three \nthings: the account placing them, the strategy to use, and the amount to invest. You can \nformalize that input, as shown in the following listing.\nOrders\nPlace Strategy\nOrders\nInput\nAmount invested\nDestination\naccount\nInvestment\nstrategy\nResults in\nRequest\nFigure 4.9    Expected behavior of a proposed PlaceStrategyOrders service\n \n",
      "content_length": 1852,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "\t\n89\nScoping by use case\nListing 4.3    Draft input for PlaceStrategyOrders\npaths:\n  /strategies/{id}/orders: \n    post:\n      summary: Place strategy orders\n      operationId: PlaceStrategyOrders\n      parameters:\n        - name: id\n          in: path\n          description: ID of strategy to order against\n          required: true\n          schema:\n            type: integer\n            format: int64\n      requestBody:\n        description: Details of order\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/StrategyOrder'\ncomponents:\n  schemas:\n    StrategyOrder:\n      required:\n        - destinationAccountId \n        - amount \n      properties:\n        destinationAccountId:\n          type: integer\n          format: int64\n        amount:\n          type: number\n          format: decimal\nThis is elegant but a little too simple. If you assume your payment is coming from an \nexternal source, you can’t execute orders until those funds are available. It doesn’t \nmake sense for PlaceStrategyOrders to handle receipt of funds — this is clearly a dis-\ntinct business capability. Instead, you can link placing strategy orders to a payment, as \nfollows.\nListing 4.4    Using payment ID for PlaceStrategyOrders\ncomponents:\n  schemas:\n    StrategyOrder:\n      required:\n        - destinationAccountId\n        - amount\n        - paymentId \n      properties:\n        destinationAccountId:\nExecuting an order is a subresource  \nof an investment strategy.\nAn order requires a destination account \nand an investment amount.\nYour new required field: paymentId\n \n",
      "content_length": 1635,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "90\nChapter 4  Designing new features\n          type: integer\n          format: int64\n        amount:\n          type: number\n          format: decimal\n\t\n  paymentId:\n          type: integer\n          format: int64\nThis anticipates the existence of a new service capability: payments. This capability \nshould support\n¡ Initiating payments by users\n¡ Processing those payments by interacting with third-party payment systems\n¡ Updating account positions at SimpleBank\nBecause you know that payments aren’t instantaneous, you’d expect this service to trig-\nger asynchronous events that other services can listen for, such as PaymentCompleted. \nFigure 4.10 illustrates this payments capability.\nFrom the perspective of PlaceStrategyOrders, it doesn’t matter how you implement \nthe payments capability, as long as something implements the interface the consumer \nexpects. It might be a single service — Payments — or a collection of action-oriented \nservices, for example, CompleteBankTransfer.\nYou can summarize what you’ve designed so far in a sequence diagram (figure 4.11).\nThere’s one missing element in this diagram: getting these orders to market. As men-\ntioned, although this service generates orders, this capability clearly doesn’t belong \nwithin your existing orders service. The orders service exposes behavior that multiple \nconsumers can use, including this new service (figure 4.12); although the source of \norders differs, the process of placing them remains the same.\nIf the service meets this interface,\nconsumers don't need to be aware\nof implementation details.\nThe payments capability should\nimplement a clear interface.\nPayments\nPayment\ncompleted\nCreate payment\nProcess payments\nUpdate positions\nInterface\nEvents\nFigure 4.10    The interface that your proposed payments capability expects\n \n",
      "content_length": 1808,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "\t\n91\nScoping by use case\nProcesses\npayment\nUI\nPlace Strategy Orders\nCreates payment\nSaves request\nConfirmation\nPayment details\nPayment completed\nGets strategy details\nStrategy details\nGenerates orders\nRequests investment\nPayments\nStrategies\nFigure 4.11    The process of creating a payment and making an investment using the proposed \nPlaceStrategyOrders service\nLastly, you need to persist the link between these orders and the strategy and investment \nthat created them. PlaceStrategyOrders should be responsible for storing any request it \nreceives — it clearly owns this data. Therefore, you should record any order IDs within \nthe strategy order service to preserve that foreign key relationship. You could also record \nthe order source ID — the ID of this investment strategy investment request — within \nthe orders service itself, although it seems less likely you’d query data in that direction.\nThis service manages the\nlifecycle of orders.\nPlace Order\nOther services\nPlace Strategy\nOrders\nOrders can come from\nmultiple sources.\nEnd users\nOrders\nFigure 4.12    Your orders service provides an API that multiple other services within your system  \ncan consume.\n \n",
      "content_length": 1171,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "92\nChapter 4  Designing new features\nThe orders service emits OrderCompleted events when an order has been completed. \nYour strategy orders service can listen for these events to reflect that status against the \noverall investment request.\nYou can add the orders service and tie this all together as shown in figure 4.13.\nGreat! You’ve designed another new service. Unlike the previous section, you \ndesigned a service that closely represented a specific complex use case, rather than a \nbroad capability.\nThis resulted in a service that was responsible for a single capability, replaceable, and \nindependently deployable, meeting your desired characteristics for well-scoped micro-\nservices. In contrast, unlike if you’d focused on business capabilities, the tight focus of \nthis service on a single use case limits potential for reuse in other use cases in the future. \nThis inflexibility suggests that fine-grained use case services are best used in tandem \nwith coarser grained services, rather than alone.\n4.3.2\t\nActions and stores\nWe’ve identified an interesting pattern in the above examples: multiple higher level \nmicroservices access a coarse-grained underlying business capability. This is especially \nprevalent in a verb-oriented approach, as the data needs of different actions often \noverlap.\nProcesses\npayment\nUI\nPlace Strategy Orders\nCreates payment\nSaves request\nConfirmation\nPayment details\nOrder details\nPlace orders\nOrder completed\nPayment completed\nGets strategy details\nStrategy details\nGenerates orders\nRecords order IDs\nUpdates request\nstatus\nRequests investment\nPayments\nStrategies\nOrders\nFigure 4.13    The full process of creating an investment strategy order using your new \nPlaceStrategyOrders service\n \n",
      "content_length": 1733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "\t\n93\nScoping by use case\nFor example, imagine you have two actions: update order and cancel order. Both oper-\nations operate against the same underlying order state, so neither can exclusively own \nthat state itself, and you need to reconcile that conflict somewhere. In the previous \nexamples, the orders service took care of the problem. This service is the ultimate \nowner of that subset of your application’s persistent state.\nThis pattern is similar3 to Bob Martin’s clean architecture4 or Alistair Cockburn’s hex-\nagonal architecture. In those models, the core of an application consists of two layers:\n¡ Entities  — Enterprisewide business objects and rules\n¡ Use cases  — Application-specific operations that direct entities to achieve the \ngoals of the use case\nAround those layers, you use interface adapters to connect these business-logic con-\ncerns to application-level implementation concerns, such as particular web frame-\nworks or database libraries. Similarly, at an intraservice level, your use cases (or \nactions) interact with underlying entities (or stores) to generate some useful outcome. \nYou then wrap them in a façade, such as an API gateway, to map from your underlying \nservice-to-service representations to an output friendly to an external consumer (for \nexample, a RESTful API). Figure 4.14 sketches out that arrangement.\nIndependent UI /\napplication delivery\nUI/consumers\nGateway\nService\nService\nService\nService\nService\nAdaptation of\nunderlying formats to\npresentation & output\nUse case-specific\nbusiness logic\nBusiness objects and\nenterprisewide\nbusiness rules\nClean architecture\nMicroservice application\nFrameworks\nAdapters\nUse cases\nEntities\nFigure 4.14    The architecture of a microservice application compared to Bob Martin’s clean architecture\n3\t But not quite the same: Martin’s architecture is concerned with implementation detail indepen-\ndence within object-oriented applications (for example, keeping business logic independent of \nthe data storage solution), which isn’t particularly relevant at the intraservice level.\n4\t For a more detailed explanation of Martin’s clean architecture, see Uncle Bob, “The Clean Archi-\ntecture,” August 13, 2012, http://mng.bz/LJB4.\n \n",
      "content_length": 2214,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "94\nChapter 4  Designing new features\nThis architecture is conceptually elegant, but you need to apply it judiciously in a \nmicroservice system. Treating underlying capabilities as, first and foremost, stores of \npersistent state can lead to anemic, “dumb” services. These services fail to be truly \nautonomous because they can’t take any action without being mediated by another, \nhigher level service. This architecture also increases the number of remote calls and \nthe length of the service chain you need to perform any useful action.\nThis approach also risks tight coupling between actions and underlying stores, ham-\npering your ability to deploy services independently. To avoid these pitfalls, we recom-\nmend you design microservices from the inside out, building useful coarse-grained \ncapabilities before building fine-grained action-oriented services.\n4.3.3\t\nOrchestration and choreography\nIn chapter 2, we discussed the difference between orchestration and choreography in \nservice interaction. A bias toward choreography tends to result in more flexible, auton-\nomous, and maintainable services. Figure 4.15 illustrates the difference between these \napproaches.\nIf you scope services by use case, you might find yourself writing services that explic-\nitly orchestrate the behavior of several other services. This isn’t always ideal:\n¡ Orchestration can increase coupling between services and increase the risk of \ndependent deployments.\n¡ Underlying services can become anemic and lack purpose, as the orchestrating \nservice takes on more and more responsibility for useful business output.\nWhen designing services that reflect use cases, it’s important to consider their place \nwithin a broader chain of responsibilities. For example, the PlaceStrategyOrders ser-\nvice you designed earlier both orchestrates behavior (placing orders) and reacts to \nother events (payment processing). Taking a balanced approach to choosing orches-\ntration or choreography reduces the risk of building services that lack autonomy.\n4.4\t\nScoping by volatility\nIn an ideal world, you could build any feature by combining existing microservices. \nThis might sound impractical, but it’s interesting to consider how you maximize the \nreusability — and therefore long-term utility — of the services you build.\nServices react to events,\nchoreographing a process.\nRequest\nService A\nService D\nService B\nService C\nService C\nRequest\nService A\nService D\nService B\nEvent\nEvent\nEvent\nRequest\nRequest\nRequest\nService A calls other\nservices, orchestrating a\nprocess.\nFigure 4.15    Orchestration versus choreography in service interaction\n \n",
      "content_length": 2620,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "\t\n95\nScoping by volatility\nSo far, we’ve taken a predominantly functional approach to decomposing services. This \napproach is effective but has limitations. Functional decomposition is biased toward \nthe present needs of an application and doesn’t explicitly consider how that applica-\ntion might evolve. A purely functional approach can constrain the future growth of a \nsystem by resulting in services that are inflexible in the face of new or evolving require-\nments, thereby increasing the risk of change.\nTherefore, as well as considering the functionality of your system, you should con-\nsider where that application is likely to change in the future. This is known as volatility. \nBy encapsulating areas that are likely to change, you help to ensure that uncertainty \nin one area doesn’t negatively impact other areas of the application. You can find an \nanalogy to this in the stable dependencies principle in object-oriented programming: \n“a package should only depend on packages that are more stable than it is.”\nSimpleBank’s business domain has multiple axes of volatility. For example, placing \nan order to market is volatile: different orders need to go to different markets; Simple-\nBank might have different APIs to each market (for example, through a broker, direct \nto an exchange); and those markets might change as SimpleBank broadens its offering \nof financial assets. \nTightly coupling market interaction as part of the orders service would lead to a high \ndegree of instability. Instead, you’d split the market service and ultimately build multi-\nple services to meet the needs of each market. Figure 4.16 illustrates this approach.\nMarket service\nFixML API\nCSV over FTP\nCustom API\nMarket state\nOther provider\nOther provider\nThe market service encapsulates potential change in placing orders to market.\nOver time, we might decompose further to encapsulate subareas of volatility.\nFund manufacturer\nUS stock broker\nMarket service\nFixML API\nCSV over FTP\nCustom API\nMarket state\nMarket state\nMarket state\nFund market\nservice\nUS stock market\nservice\nFund manufacturer\nUS stock broker\nFigure 4.16    The market service encapsulates change in how SimpleBank communicates with different \nfinancial market providers. Over time, this might evolve into multiple services.\n \n",
      "content_length": 2287,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "96\nChapter 4  Designing new features\nInvestment\nstrategies\n1. Notified when\nnew strategy\nis created\n4. Updates investment allocation\nStrategy\noptimizer\n3. Evaluates strategy\nagainst goals\n2. Regularly queries\nasset performance\nMarket data\nFigure 4.17    Partitioning a distinct area of system volatility — investment strategy optimization — as a \nseparate service\nLet’s take one more example: imagine you have more than one type of investment \nstrategy. Perhaps you have strategies that are optimized by deep learning: the perfor-\nmance of assets on the market should drive adjustments to future strategy allocations.\nAdding this complex behavior to your InvestmentStrategies service would signifi-\ncantly broaden its reasons to change — reducing cohesiveness. Instead, you should add \nnew services with responsibility for that behavior — as you can see in figure 4.17. By \ndoing this, you can develop and release these services independently without unneces-\nsary coupling between different features or rates of change.\nUltimately, good architecture strikes a balance between the current and future needs \nof an application. If microservices are too narrowly scoped, you might find the cost of \nchange becomes higher in the future as you become increasingly constrained by earlier \nassumptions about the limits of your system. On the flipside, you should always be care-\nful to keep YAGNI — “you aren’t gonna need it” — in mind. You may not always have the \nluxury of time (or money) to anticipate and meet every possible future permutation of \nyour application.\n4.5\t\nTechnical capabilities\nThe services you’ve designed so far have reflected actions or entities that map closely \nto your business capabilities, such as placing orders. These business-oriented services \nare the primary type you’ll build in any microservice application.\nYou can also design services that reflect technical capabilities. A technical capability \nindirectly contributes to a business outcome by supporting other microservices. Com-\nmon examples of technical capabilities include integration with third-party systems and \ncross-cutting technical concerns, such as sending notifications.\n4.5.1\t\nSending notifications\nLet’s work through an example. Imagine that SimpleBank would like to notify a \ncustomer — perhaps through email — whenever a payment has been completed. Your \n \n",
      "content_length": 2356,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "\t\n97\nTechnical capabilities\nfirst instinct might be to build that code within your payments service (or services). \nBut that approach has three problems:\n1\t The payments service has no awareness of customer contact details or prefer-\nences. You’d need to extend its interface to include customer contact data (push-\ning that obligation on to service consumers) or query another service.\n2\t Other parts of your application might send notifications as well. You can easily \npicture other features — orders, account setup, marketing — that might trigger \nemails.\n3\t Customers might not even want to receive emails: they might prefer SMS or push \nnotifications…or even physical mail.\nThe first and second points suggest that this should be a separate service; the third \npoint suggests you might need multiple services — one to handle each type of notifica-\ntion. Figure 4.18 sketches that out. Your notification services can listen to the Payment-\nCompleted event that your payments service emits.\nYou can configure your group of notification services to listen to any events — from \nany service — that should result in a notification. Each service will need to be aware of \na customer’s contact preferences and details to send notifications. You could store that \ninformation in a separate service, such as a customers service, or have each service own \nit. This area has hidden dimensions of complexity; for example, many customers might \nown the payment’s destination account, triggering multiple notifications.\nYou may have realized that the notification services are also responsible for generat-\ning appropriate message content based on each event, which suggests they could grow \nsignificantly in the future, in line with the potential number of notifications. It eventu-\nally might be necessary to split message content from message delivery to reduce this \ncomplexity.\nPayments\nEmits\nevent\nPayment\nCompleted\nListens\nListens\nNotifies\nNotifies\nNotifies\nCustomer\nListens\nSMS\nnotifications\nEmail\nnotifications\nPush\nnotifications\nFigure 4.18    Supporting technical microservices for notifications\n \n",
      "content_length": 2101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "98\nChapter 4  Designing new features\nThis example illustrates that implementing technical capabilities maximizes reusabil-\nity while simplifying your business services, decoupling them from nontrivial technical \nconcerns.\n4.5.2\t\nWhen to use technical capabilities\nYou should use a technical capability to support and simplify other microservices, limit-\ning the size and complexity of your business capabilities. Partitioning these capabilities \nis desirable when\n¡ Including the capability within a business-oriented service will make that service \nunreasonably complex, complicating any future replacement.\n¡ A technical capability is required by multiple services — for example, sending \nemail notifications.\n¡ A technical capability changes independently of the business capability — for \nexample, a nontrivial third-party integration.\nEncapsulating these capabilities in separate services captures axes of volatility — areas \nthat are likely to change independently — and maximizes service reusability.\nIn certain scenarios, it’s unwise to partition a technical capability. In some situations, \nextracting a capability will reduce the cohesiveness of a service. For example, in classic \nSOA, systems were often decomposed horizontally, in the belief that splitting data stor-\nage from business functionality would maximize reusability. Figure 4.19 illustrates how \nrequests would be serviced in this approach.\nUnfortunately, the intended reusability came at a high cost. Splitting those layers \nof an application led to tight coupling between different deployable units, as deliver-\ning individual features required simultaneous change across multiple applications (fig-\nure 4.20). When you have to coordinate changes to distinct components, this leads to \nerror-prone, lock-step deployments — a distributed monolith.\nIf you focus on business capabilities first, you’ll avoid these pitfalls. But you should \ncarefully scope any technical capability to ensure it’s truly autonomous and indepen-\ndent from other services.\nRequests\nCreate\norder\nCreate\norder\nCreate\norder\nOrder data\naccess\nOrder store\nOrder\nbusiness\nservice\nFigure 4.19    Lifecycle of a create order request in a horizontally partitioned service application \n \n",
      "content_length": 2230,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "\t\n99\nDealing with ambiguity\nChange 2\nChange 3\nChange 1\nNew features require coordinated changes across multiple deployable units.\nNew features lead to change in a single\ndeployable unit.\nOrder UI\nOrder\nbusiness\nservice\nOrder data\naccess\nOrder store\nOrder store\nOrders\nCreate\norder\nCreate\norder\nCreate\norder\nCreate\norder\nHorizontal partitioning \nNew feature\nNew feature\nBusiness capability partitioning\nFigure 4.20 The impact of change in a horizontally partitioned service versus a service scoped to \nbusiness capability\n4.6\t\nDealing with ambiguity\nScoping microservices is as much art as science. A large part of software design is find-\ning effective ways to achieve the best solution when faced with ambiguity:\n¡ Your understanding of the problem domain might be incomplete or incorrect. \nUnderstanding the needs of any business problem is a complex, time-consuming, \nand iterative process.\n¡ You need to anticipate how you might need to use a service in the future, rather \nthan only right now. But you’ll often run into tension between short-term feature \nneeds and long-term service malleability.\nSuboptimal service partitioning in microservices can be costly: it adds friction to devel-\nopment and extra effort to refactoring.\nNOTE    Understanding a business domain is hardly unique to microservices — or \neven the engineering process itself. Most modern product engineering meth-\nodologies aim to maintain flexibility and agility when facing an evolving \nunderstanding of requirements. For that reason, we strongly recommend fol-\nlowing an iterative and lean development process when building a microservice \napplication.\n4.6.1\t\nStart with coarse-grained services\nIn this section, we’ll explore a few approaches you can use to make practical service \ndecisions when the right solution isn’t obvious. To start, we’ve talked a lot about the \nimportance of keeping the responsibility of a service focused, cohesive, and limited, so \nwhat I’m about to say might sound a little counterintuitive. Sometimes, when in doubt \nabout service boundaries, it’s better to build larger services.\n \n",
      "content_length": 2092,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "100\nChapter 4  Designing new features\nIf you err on the side of building services that are too small, it can lead to tight cou-\npling between different services that should be combined in one service. This indicates \nyou’ve decomposed a business capability too far, making responsibility unclear and \nmaking it more difficult — and costly — to refactor this element of functionality.\nIf instead you combine that functionality into a larger service, you reduce the cost of \nfuture refactoring, as well as avoiding intractable cross-service dependencies. Likewise, \none of the most expensive costs you’ll incur in a microservice application is changing a \npublic interface; reducing the breadth of interfaces between components aids in main-\ntaining flexibility, especially in early stages of development.\nUnderstand that making a service larger also incurs a cost, because larger services \nbecome more resistant to change and difficult to replace. But at the beginning of its \nlife, a service will be small. The costs associated with a service being too large are less \nthan the costs of complexity that decomposing too far introduces. You need to care-\nfully observe both service size and complexity to ensure you’re not building more \nmonoliths.\nHere, it’s useful to apply a key principle of lean software development: decide as late \nas possible. Because building a service incurs cost in both implementation and opera-\ntion, avoiding premature decomposition when faced with uncertainty can give you time \nto develop your understanding of the problem space. It also will ensure you’re making \nwell-informed decisions about the shape of the application as it grows.\n4.6.2\t\nPrepare for further decomposition\nThe modeling and scoping techniques from earlier in this chapter will help you identify \nwhen a service has become too large. Often, you’ll be able to identify possible seams \nquite early in the lifetime of a service. If so, you should endeavor to design your service \ninternals to reflect them, whether through class and namespace design or as a separate \nlibrary. \nMaintaining disciplined internal module boundaries, with a clear public API, is gen-\nerally sound software design. In a microservice, it reduces the cost of future refactoring \nby reducing the chance that code becomes highly coupled and difficult to untangle. \nThat said, be careful — an API that’s well-designed in the context of a code library may \nnot always be ideal as the interface to a microservice. \n4.6.3\t\nRetirement and migration\nWe’ve talked about planning for future decomposition, but we also should talk about \nservice retirement. Microservice development requires a certain degree of ruthless-\nness. It’s important to remember that it’s what your application does that matters, not \nthe code. Over time — and especially if you start with larger services — you’ll find it \nnecessary to either carve out new microservices from existing services or retire micro­\nservices altogether.\n \n",
      "content_length": 2973,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "\t\n101\nDealing with ambiguity\nThis process can be difficult. Most importantly, you need to ensure that consum-\ning services don’t get broken and that they migrate in a timely way to any replacement \nservice.\nTo carve out new services, you should apply the expand-migrate-contract pattern. \nImagine you’re carving out a new service from your orders service. When you first built \nthe orders service, you were confident that it’d fit the needs of all order types, so you \nbuilt it as a single service. But one order type has turned out to be different from the \nothers, and supporting it has bloated your original service.\nFirst, you need to expand — pulling the target functionality into a new service (fig-\nure 4.21). Next, you need to migrate consumers of your old service to the new service \n(figure 4.22). If access is through an API gateway, you can redirect appropriate requests \nto your new service.\nBut if other services call the orders service, you need to migrate those usages. Telling \nother teams to migrate doesn’t always work (competing priorities, release cycles, and \nrisk). Instead, you need to either make sure your new service is compelling — make \npeople want to invest effort in migration — or do that migration for them.\nTo complete the process, you have one last step. Finally, you can contract the original \nservice, removing the now obsolete code (figure 4.23).\nPlace order\nAPI gateway\nAPI gateway\nWe expand by pulling that\ncode into a new service.\nPlace order\nPlace order B\nOur service contains\ncode we want to split.\nOrder data\nOrder data\nOrders\nOrders\nOrders B\nOrder data\nFigure 4.21    Expanding functionality from one service into a new service\n \n",
      "content_length": 1675,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "102\nChapter 4  Designing new features\nOrders B\nPlace order B\nOrders\nMigrate calls from\nother services.\nMigrate calls at the\ngateway layer.\nAPI gateway\nExisting data may need\nmigration.\nOrder data\nOrder data\nOther services\nFigure 4.22    Migrating existing consumers to the new service\nAPI gateway\nPlace order B\nOrders B\nOrders\nOrder data\nWe contract our original service, removing\nthe functionality that now resides in our new\nservice.\nOrder data\nPlace order\nFigure 4.23    In the final state of your service migration, you’ve contracted your service to remove \nfunctionality that now resides in the new service\n \n",
      "content_length": 614,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "\t\n103\nService ownership in organizations\nGreat, you made it! This measured, multistep process systematically retires or migrates \nfunctionality while reducing the risk of breaking existing service consumers.\n4.7\t\nService ownership in organizations\nThe examples so far have mostly assumed that a single team is responsible for building \nand changing microservices. In a large organization, different teams will own different \nmicroservices. This isn’t a bad thing — it’s an important part of scaling as an engineer-\ning team.\nAs we pointed out earlier, bounded contexts themselves are an effective way of \nsplitting application ownership across different teams in an organization. Forming \nteams that own services in specific bounded contexts takes advantage of the inverse \nversion of Conway’s Law: if systems reflect the organizational structure that produces \nthem, you can attain a desirable system architecture by first shaping the structure and \nresponsibilities of your organization. Figure 4.24 illustrates how SimpleBank might \norganize its engineering teams around the services and bounded contexts you’ve \nidentified so far.\nSplitting ownership and delivery of services across teams has three implications:\n¡ Limited control  — You might not have full control over the interface or perfor-\nmance of your service dependencies. For example, payments are vital to placing \ninvestment strategy orders, but the team model in figure 4.24 means that another \nteam is responsible for the behavior of that dependency.\n¡ Design constraints  — The needs of consuming services will constrain your service \ncontracts; you need to ensure service changes don’t leave consumers behind. \nLikewise, the possibilities that other existing services offer will constrain your \npotential designs.\n¡ Multispeed development  — Services that different teams own will evolve and change \nat different rates, depending on that team’s size, efficiency, and priorities. A fea-\nture request from the investment team to the customers team may not make it to \nthe top of the customers team’s priority list.\nCustomers\nCustomers\nAccounts\nStrategies\nPlaceStrategyOrders\nInvestment\nstrategies\nAsset information\nPayments\nOrders\nOrders\nMarket gateway\nInvestment\nMarket &\nresearch\nCash\nResponsible\nteam\nContexts and\nservices\nFigure 4.24    A possible model of service and capability ownership by different engineering teams as the \nsize of SimpleBank’s engineering organization grows\n \n",
      "content_length": 2455,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "104\nChapter 4  Designing new features\nThese implications can present an immense challenge, but applying a few tactics can \nhelp:\n¡ Openness  — Ensuring that all engineers can view and change all code reduces \nprotectiveness, helps different teams understand each other’s work, and can \nreduce blockers.\n¡ Explicit interfaces  — Providing explicit, documented interfaces for services reduces \ncommunication overhead and improves overall quality.\n¡ Worry less about DRY  — A microservice approach is biased toward delivery pace, \nrather than efficiency. Although engineers want to practice DRY (don’t repeat \nyourself), you should expect some duplication of work in a microservice approach. \n¡ Clear expectations  — Teams should set clear expectations about the expected per-\nformance, availability, and characteristics of their production services.\nThese sorts of tactics touch on the people side of microservices. This is a substantial \ntopic by itself, which we’ll explore in depth in the final chapter of this book.\nSummary\n¡ You scope services through a process of understanding the business problem, \nidentifying entities and use cases, and partitioning service responsibility.\n¡ You can partition services in several ways: by business capability, use case, or vola-\ntility. And you can combine these approaches.\n¡ Good scoping decisions result in services that meet three key microservice char-\nacteristics: responsible for a single capability, replaceable, and independently \ndeployable.\n¡ Bounded contexts often align with service boundaries and provide a useful way \nof considering future service evolution.\n¡ By considering areas of volatility, you can encapsulate areas that change together \nand increase future amenability to change.\n¡ Poor scoping decisions can become costly to rectify, as the effort involved in \nrefactoring is higher across multiple codebases.\n¡ Services may also encapsulate technical capabilities, which simplify and support \nbusiness capabilities and maximize reusability.\n¡ If service boundaries are ambiguous, you should err on the side of coarse-grained \nservices but use internal modules to actively prepare for future decomposition.\n¡ Retiring services is challenging, but you’ll need to do it as a microservice applica-\ntion evolves.\n¡ Splitting ownership across teams is necessary in larger organizations but causes \nnew problems: limited control, design constraints, and multispeed development.\n¡ Code openness, explicit interfaces, continual communication, and a relaxed \napproach to the DRY principle can alleviate tension between teams.\n \n",
      "content_length": 2585,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "105\n5\nTransactions and \nqueries in microservices \nThis chapter covers\n¡ The challenges of consistency in a distributed \napplication\n¡ Synchronous and asynchronous \ncommunication\n¡ Using sagas to develop business logic across \nmultiple services\n¡ API composition and CQRS for microservice \nqueries\nMany monolithic applications rely on transactions to guarantee consistency and \nisolation when changing application state. Obtaining these properties is straight-\nforward: an application typically interacts with a single database, with strong consis-\ntency guarantees, using frameworks that provide support for starting, committing, \nor rolling back transactional operations. Each logical transaction might involve sev-\neral distinct entities; for example, placing an order will update transactions, reserve \nstock positions, and charge fees.\nYou’re not so lucky in a microservice application. As you learned earlier, each inde-\npendent service is responsible for a specific capability. Data ownership is decentralized, \n \n",
      "content_length": 1020,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "106\nChapter 5  Transactions and queries in microservices  \nensuring a single owner for each “source of truth.” This level of decoupling helps you gain \nautonomy, but you sacrifice some of the safety you were previously afforded, making con-\nsistency an application-level problem. Decentralized data ownership also makes retrieving \ndata more complex. Queries that previously used database-level joins now require calls to \nmultiple services. This is acceptable for some use cases but painful for large data sets.\nAvailability also impacts your application design. Interactions between services might \nfail, causing business processes to halt, leaving your system in an inconsistent state. \nIn this chapter, you’ll learn how to use sagas to coordinate complex transactions \nacross multiple services and explore best practices for efficiently querying data. Along \nthe way, we’ll examine different types of event-based architectures, such as event sourc-\ning, and their applicability to microservice applications.\n5.1\t\nConsistent transactions in distributed applications\nImagine you’re a customer at SimpleBank and you want to sell some stock. If you recall \nchapter 2, this involves several operations (figure 5.1):\n1\t You create an order.\n2\t The application validates and reserves the stock position.\n3\t The application charges you a fee.\n4\t The application places the order to the market.\nFrom your perspective as a customer, this operation appears to be atomic: charging a \nfee, reserving stock, and creating an order happen at the same time, and you can’t sell \nstock that you don’t have or sell a stock you do have more than once.\nIn many monolithic applications,1 those requirements are easy to meet: you can wrap \nyour database operations in an ACID transaction and rest easy in the knowledge that \nerrors will cause an invalid state to be rolled back.\nUser requests\nsale of stock\nIs stock \navailable?\nNo\nNo\nYes\nYes\nStock is reserved\nagainst account\nFee is charged\nto account\nMarket open?\nWait to place\norder\nPlace order to\nmarket\nFigure 5.1    Placing a sell order\n1\t At least, those with a typical three-tier architecture and a single persistent data store.\n \n",
      "content_length": 2168,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "\t\n107\nConsistent transactions in distributed applications\nUser\nOrders\nFees\nreserve stock\nconfirm reservation\ncharge fee\nReserve\nposition\nSave pending\norder\ncreate order\nAccount\nTransactions\nFigure 5.2    Failure occurs when charging a fee in your cross-service order placement process\nBy contrast, in your microservice application, each of the actions in figure 5.1 is per-\nformed by a distinct service responsible for a subset of application state. Decentral-\nized data ownership helps ensure services are independent and loosely coupled, but it \nforces you to build application-level mechanisms to maintain overall data consistency.\nLet’s say an orders service is responsible for coordinating the process of selling a \nstock. It calls account transactions to reserve stock and then the fees service to charge \nthe customer. But that transaction fails. (See figure 5.2.)\nAt this stage, your system is in an inconsistent state: stock is reserved, an order is cre-\nated, but you haven’t charged the customer. You can’t leave it like this — so the implemen-\ntation of orders needs to initiate corrective action, instructing the account transactions \nservice to compensate and remove the stock reservation. This might look simple, but \nit becomes increasingly complex when many services are involved, transactions are \nlong-running, or an action triggers further interleaved downstream transactions.\n5.1.1\t\nWhy can’t you use distributed transactions?\nFaced with this problem, your first impulse might be to design a system that achieves \ntransactional guarantees across multiple services. A common approach is to use the two-\nphase commit (2PC) protocol.2 In this approach, you use a transaction manager to split \noperations across multiple resources into two phases: prepare and commit (figure 5.3).\n2\t See https://en.wikipedia.org/wiki/Two-phase_commit_protocol for more information.\n \n",
      "content_length": 1885,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "108\nChapter 5  Transactions and queries in microservices  \nTransaction\nManager\nResource A\nResource B\nTransaction\nManager\nResource A\nResource B\nprepare\nprepare\nsuccess/failure\nsuccess/failure\ncommit\ncommit\nIn the prepare phase, the transaction manager\ninstructs resources to prepare their relevant action.\nIn the commit phase, the transaction manager instructs\nresources to commit or abort the prepared action.\ninitiate\naction\nFigure 5.3    The prepare and commit phases of a 2PC protocol\nThis sounds great — like what you’re used to. Unfortunately, this approach is flawed. \nFirst, 2PC implies synchronicity of communication between the transaction manager \nand resources. If a resource is unavailable, the transaction can’t be committed and \nmust roll back. This in turn increases the volume of retries and decreases the avail-\nability of the overall system. To support asynchronous service interactions, you would \nneed to support 2PC with services and the messaging layer between them, limiting your \ntechnical choices.\nNOTE    In a microservice application, availability is the product of all microser-\nvices involved in processing a given action. Because no service is 100% reliable, \ninvolving more services lessens overall reliability, increasing the probability of \nfailure. We’ll explore this in detail in the next chapter.\nHanding off significant orchestration responsibility to a transaction manager also vio-\nlates one of the core principles of microservices: service autonomy. At worst, you’d end \nup with dumb services representing CRUD operations against data, with transaction \nmanagers wholly encapsulating the interesting behavior of your system.\nFinally, a distributed transaction places a lock on the resources under transaction \nto ensure isolation. This makes it inappropriate for long-running operations, as it \nincreases the risk of contention and deadlock. What should you do instead?\n5.2\t\nEvent-based communication\nEarlier in this book, we discussed using events emitted by services as a communica-\ntion mechanism. Asynchronous events aid in decoupling services from each other and \nincrease overall system availability, but they also encourage service authors to think in \nterms of eventual consistency. In an eventually consistent system, you design complex \noutcomes to result from several independent local transactions over time, which leads \nyou to explicitly design underlying resources to represent tentative states. From the \nperspective of Eric Brewer’s CAP theorem,3 this design approach prioritizes the avail-\nability of underlying data.\n3\t Consistency, availability, partition tolerance—see Eric Brewer, “CAP Twelve Years Later: How the \n“Rules” Have Changed,” InfoQ, May 30, 2012, http://mng.bz/HGA3, for more information.\n \n",
      "content_length": 2765,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "\t\n109\nEvent-based communication\nUser\nOrders\ncreate order\nreserve stocks\nconfirm reservation\nconfirm charge\norder placed\nconfirm placement\nplace to market\ncharge fee\nAccount\nTransactions\nFees\nMarket\nFigure 5.4    The synchronous process of placing a sell order \nTo illustrate the difference between a synchronous and an asynchronous approach, \nlet’s return to the sell order example. In a synchronous approach (figure 5.4), the \norders service orchestrates the behavior of other services, invoking a sequence of steps \nuntil the order is placed to the market. If any steps fail, the orders service is responsible \nfor initiating rollback action with other services, such as reversing the charge.\nIn this approach, the orders service takes on substantial responsibility:\n¡ It knows which services it needs to call, as well as their order.\n¡ It needs to know what to do in case any downstream service produces an error or \ncan’t proceed due to business rules.\nAlthough this type of interaction is easy to reason through — as the call graph is log-\nical and sequential — this level of responsibility tightly couples the orders service to \nother services, limiting its independence and increasing the difficulty of making future \nchanges. \n5.2.1\t\nEvents and choreography\nYou can redesign this scenario to use events (figure 5.5). Each service subscribes to \nevents that interest it to know when it must perform some work:\n1\t When the user issues a sell request via the UI, the application publishes an \nOrderRequested event.\n2\t The orders service picks up this event, processes it, and publishes back to the \nevent queue an OrderCreated event. \n \n",
      "content_length": 1642,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "110\nChapter 5  Transactions and queries in microservices  \n3\t Both the transaction and fees services then pick up this event. Each one of them \nperforms its work and publishes back events to notify about the completion. \n4\t The market service in turn is waiting for a pair of events notifying it of the \ncharging of fees and the reservation of stocks. When both arrive, it knows it can \nplace the order against the stock exchange. Once that’s finished, the market ser-\nvice publishes a final event back to the queue.\nEvents allow you to take an optimistic approach to availability. For example, if the fees \nservice were down, the orders service would still be able to create orders. When the \nfees service came back online, it could continue processing a backlog of events. You \ncan extend this to rollback: if the fees service fails to charge because of insufficient \nfunds, it could emit a ChargeFailed event, which other services would then consume \nto cancel order placement.\nThis interaction is choreographed: each service reacts to events, acting independently \nwithout knowledge of the overall outcome of the process. These services are like danc-\ners: they know the steps and what to do in each section of a musical piece, and they react \naccordingly without you needing to explicitly invoke or command them. In turn, this \ndesign decouples services from each other, increasing their independence and making \nit easier to deploy changes independently.\nOrder Created\nMarket\nFees\nAccount\nTransactions\nOrders\nStock Reserved\nOrder Created\nemits\nconsumes\nemits\nconsumes\nemits\nconsumes\nemits\nconsumes\nOrder Requested\nFee Charged\nOrder Placed\nFigure 5.5    Services consuming and emitting events for order placement\n \n",
      "content_length": 1720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "\t\n111\nSagas\nEvents and the monolith\nAn event-oriented approach to service communication shines when migrating a mono-\nlithic application to microservices. By emitting events from the monolith, you consume \nthem in microservices that you’re developing in parallel. This way, you can build new fea-\ntures without tightly coupling your monolith to your new services.\nThink about it: you emit an event, and that’s the only change you need to implement on \nthe monolith to make an external system work alongside the current one, lowering risk \nand enabling safer experimentation on new services.\n \n5.3\t\nSagas\nThe choreographed approach is a basic example of the saga pattern. A saga is a coordi-\nnated series of local transactions; a previous step triggers each step in the saga.\nThe concept itself significantly predates the microservice approach. Hector Garcia- \nMolina and Kenneth Salem originally described sagas in a 1987 paper4 as an approach \ntoward long-lived transactions in database systems. As with distributed transactions, lock-\ning in long-lived transactions reduces availability — a saga solves this as a sequence of \ninterleaved, individual transactions. \nAs each local transaction is atomic — but not the saga as a whole — a developer must write \ntheir code to ensure that the system ultimately reaches a consistent state, even if individual \ntransactions fail. Pat Helland’s famous paper, “Life Beyond Distributed Transactions,”5 sug-\ngests that you can think of this as uncertainty — an interaction across multiple services may \nnot have a guaranteed outcome. In a distributed transaction, you manage uncertainty using \nlocks on data; without transactions, you manage uncertainty through semantically appro-\npriate workflows that confirm, cancel, or compensate for actions as they occur.\nBefore we talk about sell orders and services, let’s look at a simple real-world saga: \npurchasing a cup of coffee.6 Typically, this might involve four steps: ordering, payment, \npreparation, and delivery (figure 5.6). In the normal outcome, the customer pays for \nand receives the coffee they ordered.\nOrder coffee\nPay for coffee\nPrepare coffee\nDeliver coffee\nFigure 5.6    The process of purchasing a cup of coffee\n4\t See Hector Garcia-Molina and Kenneth Salem, “Sagas,” http://mng.bz/Qdot, for the original \npaper.\n5\t See Pat Helland, “Life Beyond Distributed Transactions,” acmqueue, December 12, 2016, http://\nqueue.acm.org/detail.cfm?id=3025012.\n6\t Adapted from Gregor Hohpe, “Compensating Action,” Enterprise Integration Patterns, http://mng \n.bz/5FcG.\n \n",
      "content_length": 2564,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "112\nChapter 5  Transactions and queries in microservices  \nPay for coffee\nOrder\ncoffee\npayment failed\nsuccess\nPrepare coffee\nbad coffee\ncan’t make coffee\nRefund customer\nDeliver coffee\nwrong coffee/wrong person\nMmm,\ncoffee\nNo\ncoffee\nFigure 5.7    Purchasing a cup of coffee with compensating actions\nThis can go wrong! The coffee shop machine might break; the barista might make \na cappuccino, but I wanted a flat white; they might give my coffee to the wrong cus-\ntomer; and so on. If one of these events occurs, the barista will naturally compensate: \nthey might make my coffee again or refund my payment (figure 5.7). In most cases, I’ll \neventually get my coffee.\nYou use compensating actions in sagas to undo previous operations and return \nyour system to a more consistent state. The system isn’t guaranteed to be returned to \nthe original state; the appropriate actions depend on business semantics. This design \napproach makes writing business logic more complex — because you need to consider \na wide range of potential scenarios — but is a great tool for building reliable interac-\ntions between distributed services.\n5.3.1\t\nChoreographed sagas\nLet’s return to the earlier example — sell orders — to better understand how you \ncan apply the saga pattern to your microservices. The actions in this saga are choreo-\ngraphed: each action, TX, is performed in response to another, but without an overall \nconductor or orchestrator. You can break this task into five subtasks:\n¡ T1 — Create the order. \n¡ T2 — Reserve the stock position, which the account transaction service \nimplements.\n¡ T3 — Calculate and charge the fee, which the fees service implements.\n¡ T4 — Place the purchase order to the market, which the market service implements.\n¡ T5 — Update the status of the order to be placed.\nFigure 5.8 illustrates the optimistic — most likely — path of this interaction.\n \n",
      "content_length": 1884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "\t\n113\nSagas\nOrders\nOrder Created\nOrder Placed\nMarket\nAccount\nTransactions\nFees\nFee Charged\nStock Reserved\nT1\nT2\nT5\nT4\nT3\nFigure 5.8    A saga for processing a sell order\nLet’s explain the five steps of this process:\n1\t The orders service performs T1 and emits an OrderCreated event.\n2\t The fees, account transactions, and market services consume this event.\n3\t The fees and account transactions services perform appropriate actions (T2 and \nT3) and emit events, and the market service consumes them.\n4\t When the prerequisites for the order are met, the market service places the order \n(T4) to the market and emits an OrderPlaced event.\n5\t Lastly, the orders service consumes that event and updates the status of the \norder (T5).\nEach of these tasks might fail — in which case, your application should roll back to a \nsane, consistent state. Each of your tasks has a compensating action:\n¡ C1 — Cancel the order that the customer created.\n¡ C2 — Reverse the reservation of stock positions.\n¡ C3 — Revert the fee charge, refunding the customer.\n¡ C4 — Cancel the order placed to market.\n¡ C5 — Reverse the state of the order.\nWhat triggers these actions? You guessed it — events! For example, imagine that plac-\ning the order to market fails. The market service will cancel the order by emitting an \nevent — OrderFailed — that each other service involved in this saga consumes. When \nreceiving the event, each service will act appropriately: the orders service will cancel \nthe customer’s order; the transaction service will cancel the stock reservation; and the \nfees service will reverse the fee charged, executing actions C1, C2, and C3, respectively. \nThis is shown in figure 5.9.\n \n",
      "content_length": 1686,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "114\nChapter 5  Transactions and queries in microservices  \nMarket\nOrder Failed\nOrders\nAccount\nTransactions\nFees\nCancels customer\norder\nReverses stock\nreservation\nReverses fee\ncharged\nFigure 5.9    The market service emits a failure event is to initiate a rollback process across multiple \nservices.\nThis form of rollback is intended to make the system semantically, not mathematically \nconsistent. Your system on rollback of an operation may not be able to return to the \nexact same initial state. Imagine one of the tasks executed on calculating the fees was \nsending out an email. You can’t unsend an email, so you’d instead send another one \nacknowledging the error and saying the amount that the fees service had charged was \ndeposited back to the account.\nEvery action involved in a process might have one or more appropriate compensat-\ning actions. This approach adds to system complexity — both in anticipating scenar-\nios and in coding for them and testing them — especially because the more services \ninvolved in an interaction, the greater the possible intricacy of rolling back.\n Anticipating failure scenarios is a crucial part of building services that reflect real-world \ncircumstance, rather than operating in isolation. When designing microservices, you need \nto take compensation into account to ensure that the wider application is resilient.\nAdvantages and drawbacks\nThe choreographed style of interaction is helpful because participating services don’t \nneed to explicitly know about each other, which ensures they’re loosely coupled. In \nturn, this increases the autonomy of each service. Unfortunately, it’s not perfect. \nNo single piece of your code knows how to execute a sell order. This can make valida-\ntion challenging, spreading those rules across multiple distinct services. It also increases \nthe complexity of state management: each service needs to reflect distinct states in the \nprocessing of an order. For example, the orders service must track whether an order \nhas been created, placed, canceled, rejected, and so on. This additional complexity \nincreases the difficulty of reasoning about your system.\nChoreography also introduces cyclic dependencies between services: the orders ser-\nvice emits events that the market service consumes, but, in turn, it also consumes events \nthat the market service emits. These types of dependencies can lead to release time \ncoupling between services.\nGenerally, when opting for an asynchronous communication style, you must invest \nin monitoring and tracing to be able to follow the execution flow of your system. In case \n \n",
      "content_length": 2601,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "\t\n115\nSagas\nof an error, or if you need to debug a distributed system, the monitoring and tracing \ncapabilities act as a flight recorder. You should have all that happens stored there so you \ncan later investigate every single event to make sense of what happened in a multitude \nof systems. This capability is crucial for choreographed interactions.\nNOTE    Chapters 11 and 12 will explore how to achieve observability through \nlogging, tracing, and monitoring in microservice applications.\nA choreographed approach makes it difficult to know how far along a process is. Like-\nwise, the order of rollback might be important; this isn’t guaranteed by choreography, \nwhich has looser time guarantees than an orchestrated or synchronous approach. For \nsimple, near-instant workflows, knowing where you’re at is often irrelevant, but many \nbusiness processes aren’t instant — they might take multiple days and involve disparate \nsystems, people, and organizations.\n5.3.2\t\nOrchestrated sagas\nInstead of choreography, you can use orchestration to implement sagas. In an orches-\ntrated saga, a service takes on the role of orchestrator (or coordinator): a process that \nexecutes and tracks the outcome of a saga across multiple services. An orchestrator \nmight be an independent service — recall the verb-oriented services from chapter \n4 — or a capability of an existing service.\nThe sole responsibility of the orchestrator is to manage the execution of the saga. It \nmay interact with participants in the saga via asynchronous events or request/response \nmessages. Most importantly, it should track the state of execution for each stage in the \nprocess; this is sometimes called the saga log.\nLet’s make the orders service a saga coordinator. Figure 5.10 illustrates the happy \npath where a customer places an order successfully.\nSaga\nlog\nOrders\nT0: order requested\nT1: fee charged\nT2: stock reserved\nT3: order ready\nT4: order placed\nFee Charged\nOrder Created\nStock Reserved\nOrder Prepared\nMarket\nAccount\nTransactions\nFees\nOrder Placed\nFigure 5.10    An orchestrated saga for placing an order\n \n",
      "content_length": 2091,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "116\nChapter 5  Transactions and queries in microservices  \nYou’ll quickly see the key difference between this and the choreographed example \nfrom figure 5.8: the orders service tracks the execution of each substep in the process \nof placing an order. It’s useful to think of the coordinator as a state machine: a series of \nstates and transitions between those states. Each response from a collaborator triggers \na state change, moving the orchestrator toward the saga outcome.\nAs you know, a saga won’t always be successful. In an orchestrated saga, the coordina-\ntor is responsible for initiating appropriate reconciliation actions to return the entities \naffected by the failed transaction to a valid, consistent state.\nLike you did earlier, imagine the market service can’t place the order to market. The \norchestrating service will initiate compensating actions:\n1\t It’ll issue a request to the account transaction service to reverse the lock placed \non the holdings to be sold.\n2\t It’ll issue a request to cancel the fee that was charged to the customer.\n3\t It may change the state of the order to reflect the outcome of the saga — for \nexample, to rejected or failed. This depends on the business logic (and whether \nfailed orders should be shown to the customer or retried).\nIn turn, the orchestrator also could track the outcome of actions 1 and 2. Figure 5.11 \nillustrates this failure scenario.\nOrders\nT0: order requested\nT1: fee charged\nT2: stock reserved\nT3: order ready\nT4: order failed\nT5: rollback lock\nT6: rollback fee\nT7: update order\nMarket\nAccount\nTransactions\nFees\nAccount\nTransactions\nFees\nFee Charged\nOrder Created\nStock Reserved\nOrder Prepared\nOrder Failed\nOrder Cancelled\nTriggers compensating actions\nto be performed by saga\nparticipants\nFailure occurs in market\nplacement\nFee Cancelled\nLock Cancelled\nABORT!\n3\n1\n2\nFigure 5.11    In this unsuccessful saga, a failure by the market service results in the orchestrator \ntriggering compensating actions.\n \n",
      "content_length": 1979,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "\t\n117\nSagas\nTIP    Don’t forget that compensating actions might not all happen instanta-\nneously or at the same time. For example, if the fee was charged to a customer’s \ndebit card, it might take a week for their bank to reverse the charge. \nBut if the desired actions you want to happen can fail, the compensating actions — or \nthe orchestrator itself — also could fail. You should design compensating actions to \nbe safe to retry without unintentional side effects (for example, double refunds). At \nworst, repeated failure during rollback might require manual intervention. Thorough \nerror monitoring should catch these scenarios.\nAdvantages and drawbacks\nCentralizing the saga’s sequencing logic in a single service makes it significantly \neasier to reason about the outcome and progress of that saga, as well as change \nthe sequencing in one place. In turn, this can simplify individual services, reduc-\ning the complexity of states they need to manage, because that logic moves to the \ncoordinator.\nThis approach does run the risk of moving too much logic to the coordinator. At \nworst, this makes the other services anemic wrappers for data storage, rather than \nautonomous and independently responsible business capabilities.\nMany microservice practitioners advocate peer-to-peer choreography over orches-\ntration, as they see this approach to reflect the “smart endpoints, dumb pipes” aim of \nmicroservice architecture, in contrast to the heavy workflow tools (such as WS-BPEL) \npeople often used in enterprise SOA. But orchestrated approaches are becoming \nincreasingly popular in the community, especially for building long-running inter-\nactions, as seen by the popularity of projects like Netflix Conductor and AWS Step \nWorkflows.\n5.3.3\t\nInterwoven sagas\nUnlike ACID transactions, sagas aren’t isolated. The result of each local transaction is \nimmediately visible to other transactions affecting that entity. This visibility means that \na given entity might get simultaneously involved in multiple, concurrent sagas. As such, \nyou need to design your business logic to expect and handle intermediate states. The \ncomplexity of the interleaving required primarily depends on the nature of the under-\nlying business logic.\nFor now, imagine that a customer placed an order by accident and wanted to cancel \nit. If they issued their request before the order was placed to market, the order place-\nment saga would still be in progress, and this new instruction would potentially need to \ninterrupt it  (figure 5.12).\nThree common strategies for handling interwoven sagas are available: short-circuiting, \nlocking, and interruption.\n \n",
      "content_length": 2645,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "118\nChapter 5  Transactions and queries in microservices  \nCreate order\nReserve stock\nCharge fee\nPlace to market\nCancel order\ncancels\nrequests\nrequests\nActions\nchange\nchange\nApplication state\nOther\nprocesses\nOther processes may need\nto tolerate inconsistent\napplication state\nSagas can be interrupted\nby other sagas\nCustomer\nFigure 5.12    Steps in sagas may be interwoven\nShort-circuiting\nYou could prevent the new saga from being initiated while the order is still within \nanother saga. For example, the customer couldn't cancel the order until after the mar-\nket service attempted to place it to the market. This isn’t great for a user but is probably \nthe easiest strategy!\nLocking\nYou could use locks to control access to an entity. Different sagas that want to change \nthe state of the entity would wait to obtain the lock. You’ve already seen an example of \nthis in action: you place a reservation — or lock — on a stock balance to ensure that a \ncustomer can’t sell a holding twice if it’s involved in an active order. \nThis can lead to deadlocks if multiple sagas block each other trying to access the \nlock, requiring you to implement deadlock monitoring and timeouts to make sure the \nsystem doesn’t grind to a halt.\nInterruption\nLastly, you could choose to interrupt the actions taking place. For example, you could \nupdate the order status to “failed.” When receiving a message to send an order to mar-\nket, the market gateway could revalidate the latest order status to ensure the order was \nstill valid to send, and in this case it would see a “failed” status. This approach increases \nthe complexity of business logic but avoids the risk of deadlocks.\n5.3.4\t\nConsistency patterns\nAlthough sagas rely heavily on compensating actions, they’re not the only approach \nyou might use to achieve consistency in service interactions. So far, we’ve encountered \ntwo patterns for dealing with failure: compensating actions (refund my coffee pay-\nment) and retries (try to make the coffee again). Table 5.1 outlines other strategies.\n \n",
      "content_length": 2041,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "\t\n119\nSagas\nTable 5.1    Consistency strategies in microservice applications\n#\nName\nStrategy\n1\nCompensating action\nPerform an action that undoes prior action(s)\n2\nRetry\nRetry until success or timeout\n3\nIgnore\nDo nothing in the event of errors\n4\nRestart\nReset to the original state and start again\n5\nTentative operation\nPerform a tentative operation and confirm (or cancel) later\nThe use of these strategies will depend on the business semantics of your service \ninteraction. For example, when processing a large data set, it might make sense to \nignore individual failures (applying strategy #3), because the cost of processing the \noverall data set is large. When interacting with a warehouse — for example, to fulfill \norders — it’d be reasonable to place a tentative hold (strategy #5) on a stock item in a \ncustomer’s basket to reduce the possibility of overselling.\n5.3.5\t\nEvent sourcing\nSo far, we’ve assumed that entity state and events are distinct: the former is stored in an \nappropriate transactional store, whereas the latter are published independently (fig-\nure 5.13).\nAn alternative to this approach is the event sourcing pattern: rather than publishing \nevents about entity state, you represent state entirely as a sequence of events that have \nhappened to an object. To get the state of an entity at a specific time, you aggregate \nevents before that date. For example, imagine your orders service:\n¡ In the traditional persistence approaches we’ve assumed so far, a database would \nstore the latest state of the order.\n¡ In event sourcing, you’d store the events that happened to change the state of \nthe order. You could materialize the current state of the order by replaying those \nevents.\nService\npublishes\nEvents\nupdates\nstate\nqueries\nstate\nStore\nFigure 5.13    A service storing state in a data store and publishing events, in two distinct actions\n \n",
      "content_length": 1874,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "120\nChapter 5  Transactions and queries in microservices  \ntime\noccured:12:58\norderId: 102\nstatus: CREATED\nquantity: 1000\noccured:12:59\norderId: 102\nstatus: CHARGED\noccured:01:00\norderId: 102\nstatus: RESERVED\noccured: 01:01\norderId: 102\nstatus: PLACED\norderId: 102\nstatus: PLACED\nupdated: 01:01\nquantity: 1000\nEvents are\naggregated to\nquery state at a\ngiven time\nEvents are stored\npersistently\norderId: 102\nstatus: CHARGED\nupdated: 12:59\nquantity: 1000\nOrder @12:59\nOrder @01:01\nOrder Event\nOrder Event\nOrder Event\nOrder Event\nFigure 5.14    An order, stored as a sequence of events\nFigure 5.14 illustrates the event sourcing approach for tracking an order’s history.\nThis architecture solves a common problem in enterprise applications: understand-\ning how you reached your current state. It removes the division between state and \nevents; you don’t need to stick events on top of your business logic, because your busi-\nness logic inherently generates and manipulates events. On the other hand, it makes \ncomplex queries more difficult: you’d need to materialize views to perform joins or \nfilter by field values, as your event storage format would only support retrieving entities \nby their primary key. \nEvent sourcing isn’t a requirement for a microservice application, but using events \nto store application state can be a particularly elegant tool, especially for applications \ninvolving complex sagas where tracking the history of state transitions is vital. If you’re \ninterested in learning more about event sourcing, Nick Chamberlain’s awesome-ddd \nlist (https://github.com/heynickc/awesome-ddd) has a great collection of resources \nand further reading. \n5.4\t\nQueries in a distributed world\nDecentralized data ownership also makes retrieving data more challenging, as it’s no \nlonger possible to aggregate related data at, or close to, the database level — for exam-\nple, through joins. Presenting data from disparate services is often necessary at the UI \nlayer of an application.\nFor example, imagine you’re building an administrative UI that shows a list of cus-\ntomers, together with their current open orders. In a SQL database, you’d join these \ntwo tables in a single query, returning one dataset. In a microservice application, this \ncomposition would typically take place at the API level: a service or an API gateway could \n \n",
      "content_length": 2347,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "\t\n121\nQueries in a distributed world\nperform this (figure 5.15). Correlation IDs  — roughly analogous to foreign keys in a rela-\ntional database — identify relationships between data that each service owns; for exam-\nple, each order would record the associated customer ID.\nThe two-step approach in figure 5.15 works well for single entities or small datasets \nbut will scale poorly for bulk requests. If the first query returns N customers, then the \nsecond query will be performed N times, which could quickly get out of hand. If we \nwere querying a SQL database, this would be trivial to solve with a join, but because our \ndata is spread across multiple data stores, an easy solution like using a join isn’t possible.\nWe could improve this query by introducing bulk request endpoints and paging, as \nin listing 5.1. Rather than getting every customer, you’d get the first page; rather than \nretrieving customer orders one-by-one, you could retrieve them with a list of IDs. You \nshould note, though, that if each customer had thousands of orders, having to page \nthose as well would add substantial overhead.\nListing 5.1    Different endpoints for data retrieval\n/customers?page=1&size=20 \n/orders?customerIds=4,5,10,20 \nAPI composition is simple and intuitive, and for many use cases, such as individual \naggregates or small enumerables, the performance of this approach will be acceptable. \nFor others, such as the following, performance will be inefficient and far from ideal:\n¡ Queries that return and join substantial data, such as reporting  — “I want all customer \norders from the last year.”\n¡ Queries that aggregate or perform analytics across multiple services  — “I want to know the \naverage order value of emerging market stocks purchased by customers over 35.”\n¡ Queries that aren’t optimally supported by the service’s own database  — For example, \ncomplex search patterns are often difficult to optimize in relational databases.\nCustomers\nOrders\nAPI gateway\nuser interface\nconsuming service\n1. get all customers\n2. for each customer,\nget all orders\nqueries\nqueries\nEach order stores the customer_id as a correlation id\nFigure 5.15    Data composition at the API level\nYou should page large datasets.\nYou should retrieve children using “IN” \nsemantics rather than individually.\n \n",
      "content_length": 2298,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "122\nChapter 5  Transactions and queries in microservices  \nLastly, API composition is impacted by availability. Composition requires synchronous \ncalls to underlying services, so the total availability of a query path is the product of \nthe availability of all services involved in that path. For example, if the two services and \nthe API gateway in figure 5.15 each have an availability of 99%, their availability when \ncalled together would be 99%^3: 97.02%. Over the next three sections, we’ll discuss \nhow you also can use events to build efficient queries in microservice applications.\nNOTE    We’ll discuss service availability and reliability, and techniques for maxi-\nmizing those properties in the following chapter.\n5.4.1\t\nStoring copies of data\nYou can elect to have services store or cache data that they receive from other services \nvia events. For example, in figure 5.16, when the fees service receives an OrderCreated \nmessage, it might elect to store additional detail about the order, beyond the correla-\ntion ID. This service can now handle queries like “What was the value of this order?” \nwithout needing to retrieve that data with an additional call to the orders service.\nThis technique can be quite useful but risky:\n¡ Maintaining multiple copies of data increases overall application and service \ncomplexity (and possibly, overall storage cost).\n¡ Breaking schema changes in events is extremely tricky to manage, as services \nbecome increasingly coupled to event content.\n¡ Cache invalidation is notoriously hard.7\nOrder Created\npublishes\nconsumes\nFees\nprocesses fee\nFee Id\n1\nValue\n£3.04\nOrder Id\n45\nOrder Value\n£521.01\nA correlation ID can be\nderived from the event.\nAdditional data can be stored\nor cached from the event.\nstores\nDatabase\nOrders\nFigure 5.16    You can use events to share, and therefore replicate, state across multiple services\n7\t See Martin Fowler, “TwoHardThings,” July 14, 2009, https://martinfowler.com/bliki \n/TwoHardThings.html, and Mark Heath, “Troubleshooting Caching Problems,” SoundCode, \nJanuary 23, 2018, http://mng.bz/M2J7.\n \n",
      "content_length": 2083,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "\t\n123\nQueries in a distributed world\nBy maintaining canonical data in multiple locations — updated via asynchronous \nevents, which could be delayed, or fail, or be delivered multiple times — you have to \ncope with eventual consistency and the chance that the copies of data you retrieve have \nbecome stale.\nWhether it’s fine for data to be stale sometimes is down to the business semantics of \nthe particular feature. But it’s a hard tradeoff. The CAP theorem8 says that you can’t \nhave things both ways: you need to choose between availability — returning a successful \nresult, without a guarantee that data is fresh — and consistency — returning the most \nrecent state, or an error. \nGuaranteeing consistency tends to result in increased coordination between \nsystems — such as distributed locks — which hampers transaction speed. In contrast, \na system that maximizes availability ultimately relies on compensating actions and \nretries — a lot like sagas. From an architectural perspective, availability is usually \neasier to achieve and, because of the reduced coordination cost, more amenable to \nbuilding scalable applications.\nPrioritizing availability\nBuilding systems that prioritize availability might require you to avoid the instinctual, con-\nsistency-oriented solution to a problem. Even systems that seem like they should priori-\ntize consistency often make availability tradeoffs to maximize successful use.\nA great example is an automated teller machine (ATM) — prioritizing availability increases \nbank revenue. If an ATM can’t connect to the bank backend, or the wider ATM network, \nit’ll still allow withdrawals, but cap them, ensuring risk of overdraft is limited. If a with-\ndrawal does place a customer in overdraft, the bank can recoup that with a fee.\nA recent article from Eric Brewer — http://mng.bz/HGA3 — has a great overview of this \nscenario.\n \n5.4.2\t\nSeparating queries and commands\nYou can generalize the previous approach — using events to build views — further. \nIn many systems, queries are substantially different from writes: whereas writes affect \nsingular, highly normalized entities, queries often retrieve denormalized data from a \nrange of sources. Some query patterns might benefit from completely different data \nstores than writes; for example, you might use PostgreSQL as a persistent transactional \nstore but Elasticsearch for indexing search queries. The command-query responsibility \nsegregation pattern (CQRS) is a general model for managing these scenarios by explic-\nitly separating reads (queries) from writes (commands) within your system.9\n8\t This is a fantastic, “plain English” explanation of the CAP theorem: ksat.me/a-plain-english \n-introduction-to-cap-theorem/, by Kaushik Sathupadi.\n9\t You need to use CQRS if you implement an event-sourcing architecture.\n \n",
      "content_length": 2820,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "124\nChapter 5  Transactions and queries in microservices  \nNOTE    We won’t go into specific technical detail about implementing CQRS, \nbut you can explore frameworks in many languages, such as Commanded \n(Elixir), CQRS.net (.NET), Lagom (Java and Scala), and Broadway (PHP).\nCQRS architecture\nLet’s sketch out this architecture. In figure 5.17, you can see that CQRS partitions com-\nmands and queries:\n¡ The command side of an application performs updates to a system — creates, \nupdates and deletes. Commands emit events, either in-band or to a distinct event \nbus, such as RabbitMQ or Kafka.\n¡ Event handlers consume events to build appropriate query or read models.\n¡ A separate data store may support each side of the system.\nYou can apply this pattern both within services and across your whole applica-\ntion — using events to build dedicated query services that own and maintain complex \nviews of application data. For example, imagine you wanted to aggregate order fees \nacross your entire customer base, potentially slicing them by multiple attributes (for \nexample, type of order, asset categories, payment method). This wouldn’t be possible \nat a service level, because neither the fees, orders, nor customers service has all the \ndata needed to filter those attributes. \nInstead, as figure 5.18 illustrates, you could build a query service, CustomerOrders, \nto construct appropriate views. A query service is a good way to handle views that don’t \nclearly belong to any other services, ensuring a reasonable separation of concerns.\nread\ncreate\nupdate\ndelete\ncommands\nevent handlers\nqueries\nService\nevents\nreads\nupdates\nupdates\nCommand\nstore\nQuery store\nFigure 5.17    CQRS partitions a service into command and query sides, each accessing separate data \nstores.\n \n",
      "content_length": 1776,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "\t\n125\nQueries in a distributed world\nOrders\nFees\nCustomers\nCustomer...\nFee Charged\nOrder Created\nevent handlers\nqueries\nupdates\nreads\nQuery store\nCustomer Orders\nget\nFigure 5.18    Query services can construct complex views from events that multiple services emit.\nTIP    You don’t need to use only CQRS within your application. Using different \nquery styles in different scenarios can help achieve a good balance of complex-\nity, implementation speed, and customer value.\nSo far, this all sounds great! In a microservices application, CQRS offers two key benefits:\n¡ You can optimize the query model for specific queries to improve their perfor-\nmance and remove the need for cross-service joins.\n¡ It aids in separation of concerns, both within services and at an application level.\nBut it’s not without drawbacks. Let’s explore those now.\n5.4.3\t\nCQRS challenges\nLike the data caching example, CQRS requires you to consider eventual consistency \nbecause of replication lag : inherently, the command state of a service will be updated \nbefore the query state. Because events update query models, someone querying that \ndata might receive an out of date view. This might be a frustrating user experience (fig-\nure 5.19). Imagine you update the value of an order, but on clicking Confirm, you see \nthe details of the original order! Web UIs that use a POST/redirect/GET10 pattern will \noften suffer from this problem.\n10\t See https://en.wikipedia.org/wiki/Post/Redirect/Get for more information.\n \n",
      "content_length": 1497,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "126\nChapter 5  Transactions and queries in microservices  \nUI\nOrders\nQueries made before\nthe view is updated\ncan’t find the new order.\nUpdating the query\nview takes time to\nperform.\ncreate order\nconfirm\nget order\nno order found!\norder created\nupdate view\nOrder Query\nFigure 5.19    Lag in updating a query view leads to inconsistent results when making a request.\nIn some systems, this might not matter. For example, delayed updates are common for \nactivity feeds11 — if I post an update on Twitter, it doesn’t matter if my followers don’t \nall receive it at the same time. And in fact, attempting to achieve greater consistency \ncan lead to substantial scalability challenges that might not be worth it.\nIn other systems, it’ll be important to ensure you don’t query invalid state. You can \napply three strategies (figure 5.20) in these scenarios: optimistic updates, polling, or \npublish-subscribe.\nOptimistic updates\nYou could update the UI optimistically, based on the expected result of a command. If \nthe command fails, you can roll back the UI state. For example, imagine you like a post \non Instagram. The app will show a red heart before the Instagram backend saves that \nchange. If that save fails, Instagram will roll back the optimistic UI change, and you’ll \nhave to like it again for it to show a red heart.\nThis approach relies on having — or being able to derive — all the information you \nneed to update the UI from the command input, so it works best when working with \nsimple entities.\n11\t If you’re interested in the architecture behind activity streams, https://github.com/tschellenbach/ \nStream-Framework is a good place to start.\n \n",
      "content_length": 1655,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "\t\n127\nQueries in a distributed world\n1. request\n3. confirm or rollback\n2. update to reflect\n1. request\nversion X\n2. poll until\nversion X\n2. request\n1. subscribe\n3. publish\nCommand\nUI\nUI\nUI\nCommand\nQuery\nQuery\nCommand\nOptimistic update\nPolling\nPublish-subscribe\nFigure 5.20    Three strategies for dealing with query-side replication lag in CQRS\nPolling\nThe UI could poll the query API until an expected change has occurred. When ini-\ntiating a command, the client would set a version, such as a timestamp. For subse-\nquent queries, the client would continue to poll until the version number was equal \nor greater to the version number specified, indicating that the query model had been \nupdated to reflect the new state.\nPublish-subscribe\nInstead of polling for changes, a UI could subscribe to events on a query model — for \nexample, over a web socket channel. In this case, the UI would only update when the \nread model published an “updated” event.\nAs you can see, it’s challenging to reason through CQRS, and it requires a different \nmindset from what you’d have when dealing with normal CRUD APIs. But it can be \nuseful in a microservice application. Done right, CQRS helps to ensure performance \nand availability in queries, even as you distribute data and responsibility across multiple \ndistinct services and data stores. \n5.4.4\t\nAnalytics and reporting\nYou can generalize the CQRS technique to other use cases, such as analytics and \nreporting. You can transform a stream of microservice events and store them in a data \nwarehouse, such as Amazon Redshift or Google BigQuery (figure 5.21). A transforma-\ntion stage may involve mapping events to the semantics and data model of the target \nwarehouse or combining events with data from other microservices. If you don’t yet \nknow how you want to treat or query events, you can store them in commodity stor-\nage, such as Amazon S3, for later querying or reprocessing with big data tools such as \nApache Spark or Presto.\n \n",
      "content_length": 1979,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "128\nChapter 5  Transactions and queries in microservices  \nevent stream\nTransform\nStorage\nAnalytic tools e.g.\nSpark\nData\nwarehouse\nevents\nevents\nFigure 5.21    You can use microservice events to populate data warehouses or other analytic stores.\t\n5.5\t\nFurther reading\nWe’ve covered a lot of ground in this chapter, but some topics, like sagas, event sourc-\ning, and CQRS, can each fill entire books. In case you’re interested in knowing more \nabout those topics, we recommend the following books:\n¡ Reactive Application Development, by Duncan K. DeVore, Sean Walsh, and Brian \nHanafee, https://www.manning.com/books/reactive-application-development \n(ISBN 9781617292460)\n¡ Microservices Patterns, by Chris Richardson, https://www.manning.com/books/\nmicroservices-patterns (ISBN 9781617294549)\n¡ Event Streams in Action, by Alexander Dean, https://www.manning.com/books/\nevent-streams-in-action (ISBN 9781617292347)\nSummary\n¡ ACID properties are difficult to achieve in interactions across multiple services; \nmicroservices require different approaches to achieve consistency.\n¡ Coordination approaches, such as two-phase commit, introduce locking and \ndon’t scale well.\n¡ An event-based architecture decouples independent components and provides a \nfoundation for scalable business logic and queries in a microservice application.\n¡ Biasing towards availability, rather than consistency, tends to lead to a more scal-\nable architecture.\n¡ Sagas are global actions composed from message-driven, independent local \ntransactions. They achieve consistency by using compensating actions to roll \nback incorrect state.\n¡ Anticipating failure scenarios is a crucial element of building services that reflect \nreal-world circumstance, rather than operating in isolation.\n¡ You typically implement queries across microservices by composing results from \nmultiple APIs.\n¡ Efficient complex queries should use the CQRS pattern to materialize read mod-\nels, especially where those query patterns require alternative data stores.\n \n",
      "content_length": 2019,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "129\n6\nDesigning reliable services\nThis chapter covers\n¡ The impact of service availability on application \nreliability\n¡ Designing microservices that defend against \nfaults in their dependencies\n¡ Applying retries, rate limits, circuit breakers, \nhealth checks, and caching to mitigate \ninterservice communication issues\n¡ Applying safe communication standards across \nmany services\nNo microservice is an island; each one plays a small part in a much larger system. Most ser-\nvices that you build will have other services that rely on them — upstream collaborators —  \nand in turn themselves will depend on other services — downstream collaborators — to \nperform useful functions. For a service to reliably and consistently perform its job, it \nneeds to be able to trust these collaborators.\nThis is easier said than done. Failures are inevitable in any complex system. An indi-\nvidual microservice might fail for a variety of reasons. Bugs can be introduced into code. \nDeployments can be unstable. Underlying infrastructure might let you down: resources \n \n",
      "content_length": 1059,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "130\nChapter 6  Designing reliable services\nmight be saturated by load; underlying nodes might become unhealthy; even entire data \ncenters can fail. As we discussed in chapter 5, you can’t even trust that the network between \nyour services is reliable — believing otherwise is a well-known fallacy of distributed comput-\ning.1 Lastly, human error can lead to major failures. For example, I’m writing this chapter a \nweek after an engineer’s mistake in running a maintenance script led to a severe outage in \nAmazon S3, affecting thousands of well-known websites.\nIt’s impossible to eliminate failure in microservice applications — the cost of that \nwould be infinite! Instead, your focus needs to be on designing microservices that are \ntolerant of dependency failures and able to gracefully recover from them or mitigate \nthe impact of those failures on their own responsibilities.\nIn this chapter, we’ll introduce the concept of service availability, discuss the impact of \nfailure in microservice applications, and explore approaches to designing reliable com-\nmunication between services. We’ll also discuss two different tactics — frameworks and \nproxies — for ensuring all microservices in an application interact safely. Using these \ntechniques will help you maximize the reliability of your microservice application — and \nkeep your users happy.\n6.1\t\nDefining reliability\nLet’s start by figuring out how to measure the reliability of a microservice. Consider a simple \nmicroservice system: a service, holdings, calls two dependencies, transactions and market-data. \nThose services in turn call further dependencies. Figure 6.1 illustrates this relationship.\nFor any of those services, you can assume that they spent some time performing work \nsuccessfully. This is known as uptime. Likewise, you can safely assume — because failure \nis inevitable — that they spent some time failing to complete work. This is known as \ndowntime. You can use uptime and downtime to calculate availability: the percentage of \noperational time during which the service was working correctly. A service’s availability \nis a measure of how reliable you can expect it to be.\nRequests\nHoldings\nD\nE\nC\nMarket-data\nTransactions\nFigure 6.1    A simple microservice system, illustrating dependencies between collaborating services\n1\t Peter Deutsch originally posited the eight fallacies of distributed computing in 1994. A good over-\nview is available here: http://mng.bz/9T5F.\n \n",
      "content_length": 2458,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "\t\n131\nDefining reliability\nA typical shorthand for high availability is “nines:” for example, two nines is 99%, \nwhereas five nines is 99.999%. It’d be highly unusual for critical production-facing ser-\nvices to be less reliable than this.\nTo illustrate how availability works, imagine that calls from holdings to market-data \nare successful 99.9% of the time. This might sound quite reliable, but downtime of \n0.1% quickly becomes pronounced as volumes increase: only one failure per 1,000 \nrequests, but 1,000 failures per million. These failures will directly affect your calling \nservice unless you can design that service to mitigate the impact of dependency failure.\nMicroservice dependency chains can quickly become complex. If those dependen-\ncies can fail, what’s the probability of failure within your whole system? You can treat \nyour availability figure as the probability of a request being successful — by multiplying \ntogether the availability figures for the parts of the chain, you can estimate the failure \nrate across your entire system.\nSay you expand the previous example to specify that you have six services with the \nsame success rate for calls. For any request to your system, you can expect one of four \noutcomes: all services work correctly, one service fails, multiple services fail, or all ser-\nvices fail.\nBecause calls to each microservice are successful 99.9% of the time, combined reli-\nability of the system will be 0.9996 = 0.994 = 99.4%. Although this is a simple model, \nyou can see that the whole application will always be less reliable than its independent \ncomponents; the maximum availability you can achieve is a product of the availability of \na service’s dependencies.\nTo illustrate, imagine that service D’s availability is degraded to 95%. Although this \nwon’t affect transactions — because it’s not part of that call hierarchy — it will reduce \nthe reliability of both market-data and holdings. Figure 6.2 illustrates this impact.\nHoldings\nD\nE\nC\nMarket-data\nTransactions\nRequests\nThe reliability of holdings is\nthe combined reliability of\nitself and its collaborators.\nThe reliability of market-data\nis the combined reliability of\nitself, D, and E.\nA reduction in D’s reliability\naffects all upstream services.\n99.9%\n99.9%\n99.9%\n95%\n99.9% * 95% * 99.9% =\n94.8%\n99.9% * 94.8% * 99.9% *\n99.9% = 94.5%\nFigure 6.2    The impact of service dependency availability on reliability in a microservice application\n \n",
      "content_length": 2454,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "132\nChapter 6  Designing reliable services\nIt’s crucial to maximize service availability — or isolate the impact of unreliability — to \nensure the availability of your entire application. Measuring availability won’t tell you \nhow to make your services reliable, but it gives you a target to aim for or, more specifi-\ncally, a goal to guide both the development of services and the expectations of consum-\ning services and engineers.\nNOTE    How do you monitor availability? We’ll explore approaches to monitor-\ning service availability in a microservice application in part 4 of this book.\nIf you can’t trust the network, your hardware, other services, or even your own services \nto be 100% reliable, how can you maximize availability? You need to design defensively \nto meet three goals:\n¡ Reduce the incidence of avoidable failures\n¡ Limit the cascading and system-wide impact of unpredictable failures\n¡ Recover quickly — and ideally automatically — when failures do occur\nAchieving these goals will ultimately maximize the uptime and availability of your \nservices.\n6.2\t\nWhat could go wrong?\nAs we’ve stated, failure is inevitable in a complex system. Over the lifetime of an appli-\ncation, it’s incredibly likely that any catastrophe that could happen, will happen. Con-\nsequently, you need to understand the different types of failures that your application \nmight be susceptible to. Understanding the nature of these risks and their likelihood \nis fundamental to both architecting appropriate mitigation strategies and reacting rap-\nidly when incidents do occur.\nBalancing risk and cost\nIt’s important to be pragmatic: you can neither anticipate nor eliminate every possible \ncause of failure. When you’re designing for resilience, you need to balance the risk of a \nfailure against what you can reasonably defend against given time and cost constraints:\n¡ The cost to design, build, deploy, and operate a defensive solution\n¡ The nature of your business and expectations of your customers\nTo put that in perspective, consider the S3 outage I mentioned earlier. You could defend \nagainst that error by replicating data across multiple regions in AWS or across multiple \nclouds. But given that S3 failures of that magnitude are exceptionally rare, that solution \nwouldn’t make economic sense for many organizations because it would significantly \nincrease operational costs and complexity.\n \nAs a responsible service designer, you need to identify possible types of failure within \nyour microservice application, rank them by anticipated frequency and impact, and \ndecide how you’ll mitigate their impact. In this section, we’ll walk you through some \n \n",
      "content_length": 2661,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "\t\n133\nWhat could go wrong?\ncommon failure scenarios in microservice applications and how they arise. We’ll also \nexplore cascading failures — a common catastrophic scenario in a distributed system.\n6.2.1\t\nSources of failure\nLet’s examine a microservice to understand where failure might arise, using one of Sim-\npleBank’s services as an example. You can assume a few things about the market-data \nservice:\n¡ The service will run on hardware — likely a virtualized host — that ultimately \ndepends on a physical data center.\n¡ Other upstream services depend on the capabilities of this service.\n¡ This service stores data in some store — for example, a SQL database.\n¡ It retrieves data from third-party data sources through APIs and file uploads.\n¡ It may call other downstream SimpleBank microservices.\nFigure 6.3 illustrates the service and its relationship to other components.\nData store\nStores &\nretrieves\ndata\nHost\nThird-party\ndependencies\nMakes requests\nMake requests\nMarket-data\nUpstream collaborators\nDownstream collaborators\nMakes requests\nFigure 6.3    Relationships between the market-data microservice and other components of the \napplication\n \n",
      "content_length": 1157,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "134\nChapter 6  Designing reliable services\nEvery point of interaction between your service and another component indicates a \npossible area of failure. Failures could occur in four major areas:\n¡ Hardware  — The underlying physical and virtual infrastructure on which a ser-\nvice operates\n¡ Communication  — Collaboration between different services and/or third parties\n¡ Dependencies  — Failure within dependencies of a service\n¡ Internal  — Errors within the code of the service itself, such as defects introduced \nby engineers\nLet’s explore each category in turn.\nHardware\nRegardless of whether you run your services in a public cloud, on-premise, or using \na PaaS, the reliability of the services will ultimately depend on the physical and vir-\ntual infrastructure that underpins them, whether that’s server racks, virtual machines, \noperating systems, or physical networks. Table 6.1 illustrates some of the causes of fail-\nure within the hardware layer of a microservice application.\nTable 6.1    Sources of failure within the hardware layer of a microservice application\nSource of failure \nFrequency\nDescription\nHost \nOften\nIndividual hosts (physical or virtual) may fail.\nData center\nRare\nData centers or components within them may fail.\nHost configuration \nOccasionally\nHosts may be misconfigured —  for example, through errors in \nprovisioning tools.\nPhysical network \nRare \nPhysical networking (within or between data centers) may fail.\nOperating system and \nresource isolation \nOccasionally\nThe OS or the isolation system — for example, Docker — may fail \nto operate correctly.\nThe range of possible failures at this layer of your application are diverse and unfortu-\nnately, often the most catastrophic because hardware component failure may affect the \noperation of multiple services within an organization.\nTypically, you can mitigate the impact of most hardware failures by designing appro-\npriate levels of redundancy into a system. For example, if you’re deploying an applica-\ntion in a public cloud, such as AWS, you’d typically spread replicas of a service across \nmultiple zones — geographically distinct data centers within a wider region — to reduce \nthe impact of failure within a single center.\nIt’s important to note that hardware redundancy can incur additional operational \ncost. Some solutions may be complex to architect and run — or just plain expensive. \nChoosing the right level of redundancy for an application requires careful consider-\nation of the frequency and impact of failure versus the cost of mitigating against poten-\ntially rare events.\n \n",
      "content_length": 2584,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "\t\n135\nWhat could go wrong?\nCommunication\nCommunication between services can fail: network, DNS, messaging, and firewalls are \nall possible sources of failure. Table 6.2 details possible communication failures.\nTable 6.2    Sources of communication failure within a microservice application\nSource of failure \nDescription\nNetwork\nNetwork connectivity may not be possible.\nFirewall\nConfiguration management can set security rules inappropriately.\nDNS errors\nHostnames may not be correctly propagated or resolved across an application.\nMessaging\nMessaging systems — for example, RPC — can fail.\nInadequate health \nchecks\nHealth checks may not adequately represent instance state, causing requests to be \nrouted to broken instances.\nCommunication failures can affect both internal and external network calls. For exam-\nple, connectivity between the market-data service and the external APIs it relies on \ncould degrade, leading to failure.\nNetwork and DNS failures are reasonably common, whether caused by changes in \nfirewall rules, IP address assignment, or DNS hostname propagation in a system. Net-\nwork issues can be challenging to mitigate, but because they’re often caused by human \nintervention (whether through service releases or configuration changes), the best way \nto avoid many of them is to ensure that you test configuration changes robustly, and \nthat they’re easy to roll back if issues occur.\nDependencies\nFailure can occur in other services that a microservice depends on, or within that \nmicroservice’s internal dependencies (such as databases). For example, the database \nthat market-data relies on to save and retrieve data might fail because of underlying \nhardware failure or hitting scalability limits — it wouldn’t be unheard of for a database \nto run out of disk space!\nAs we outlined earlier, such failures have a drastic effect on overall system availability. \nTable 6.3 outlines possible sources of failure.\nTable 6.3    Sources of dependency-related failure\nSource of failure \nDescription\nTimeouts\nRequests to services may time out, resulting in erroneous behavior.\nDecommissioned or nonbackwards- \ncompatible functionality\nDesign doesn’t take service dependencies into account, unexpect-\nedly changing or removing functionality.\nInternal component failures\nProblems with databases or caches prevent services from working \ncorrectly.\nExternal dependencies\nServices may have dependencies outside of the application that \ndon’t work correctly or as expected — for example, third-party APIs.\n \n",
      "content_length": 2519,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "136\nChapter 6  Designing reliable services\nIn addition to operational sources of failure, such as timeouts and service outages, depen-\ndencies are prone to errors caused by design and build failures. For example, a service \nmay rely on an endpoint in another service that’s changed in a nonbackwards-compatible \nway or, even worse, removed completely without appropriate decommissioning.\nService practices\nLastly, inadequate or limited engineering practices when developing and deploying ser-\nvices may lead to failure in production. Services may be poorly designed, inadequately \ntested, or deployed incorrectly. You may not catch errors in testing, or a team may not \nadequately monitor the behavior of their service in production. A service might scale \nineffectively: hitting memory, disk, or CPU limits on its provisioned hardware such that \nperformance is degraded — or the service becomes completely unresponsive.\nBecause each service contributes to the effectiveness of the whole system, one poor \nquality service can have a detrimental effect on the availability of swathes of function-\nality. Hopefully the practices we outline throughout this book will help you avoid \nthis — unfortunately common — source of failure!\n6.2.2\t\nCascading failures\nYou should now understand how failure in different areas can affect a single micro-\nservice. But the impact of failure doesn’t stop there. Because your applications are \ncomposed of multiple microservices that continually interact with each other, failure \nin one service can spread across an entire system.\nCascading failures are a common mode of failure in distributed applications. A cas-\ncading failure is an example of positive feedback: an event disturbs a system, leading to \nsome effect, which in turn increases the magnitude of the initial disturbance. In this \ncase, positive means that the effect increases — not that the outcome is beneficial.\nYou can observe this phenomenon in several real-world domains, such as financial \nmarkets, biological processes, or nuclear power stations. Consider a stampede in a herd \nof animals: panic causes an animal to run, which in turn spreads panic to other animals, \nwhich causes them to run, and so on. In a microservice application, overload can cause \na domino effect: failure in one service increases failure in upstream services, and in \nturn their upstream services. At worst, the result is widespread unavailability.\nLet’s work through an example to illustrate how overload can result in a cascading \nfailure. Imagine that SimpleBank built a UI to show a user their current financial hold-\nings (or positions) in an account. That might look something like figure 6.4.\nEach financial position is the sum of the transactions — purchases and sales of a \nstock — made to date, multiplied by the current price. Retrieving these values relies on \ncollaboration between three services:\n¡ Market-data  — A service responsible for retrieving and processing price and mar-\nket information for financial instruments, such as stocks\n¡ Transactions  — A service responsible for representing transactions occurring \nwithin an account\n¡ Holdings  — A service responsible for aggregating transactions and market-data to \nreport financial positions\n \n",
      "content_length": 3246,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "\t\n137\nWhat could go wrong?\nFigure 6.4    A user interface that reports financial holdings in an account\nFigure 6.5 outlines the production configuration of these services. For each service, \nload is balanced across multiple replicas.\nSuppose that holdings are being retrieved 1,000 times per second (QPS). If you have \ntwo replicas of your holdings service, each replica will receive 500 QPS (figure 6.6).\nUI\n1. Retrieves\nholdings\nLB\nLB\nLB\nData store\nData store\nQueries\nQueries\nThe transactions\nand market-data\nservices “own”\ndata within their\ndomains.\nHoldings\nLoad is balanced\nacross multiple\nreplicas of each\nmicroservice.\n2. Retrieves transactions\n3. Retrieves prices\nMarket-data\nTransactions\nFigure 6.5    Production configuration and collaboration between services to populate the “current \nfinancial holdings” user interface\nUI\n1000 QPS\n500 QPS\nLB\n500 QPS\nHoldings\nHoldings\nFigure 6.6    Queries made to a service are split across multiple replicas.\n \n",
      "content_length": 959,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "138\nChapter 6  Designing reliable services\nYour holdings service subsequently queries transactions and market-data to construct \nthe response. Each call to holdings will generate two calls: one to transactions and one \nto market-data.\nNow, let’s say a failure occurs that takes down one of your transactions replicas. Your \nload balancer reroutes that load to the remaining replica, which now needs to service \n1,000 QPS (figure 6.7).\nBut that reduced capacity is unable to handle the level of demand to your service. \nDepending on how you’ve deployed your service — the characteristics of your web \nserver — the change in load might first lead to increased latency as requests are queued. \nIn turn, increased latency might start exceeding the maximum wait time that the hold-\nings service expects for that query. Alternatively, the transactions service may begin \ndropping requests.\nIt’s not unreasonable for a service to retry a request to a collaborator when it fails. \nNow, imagine that the holdings service will retry any request to transactions that times \nout or fails. This will further increase the load on your remaining transactions resource, \nwhich now needs to handle both the regular request volume and the heightened retry \nvolume (figure 6.8). In turn, the holdings service takes longer to respond while it waits \non its collaborator.\nLB\nHoldings\nHoldings\nTransactions\nTransactions\n1000 QPS\n500 QPS\n500 QPS\nFigure 6.7    One replica of a collaborating service fails, sending all load to the remaining instance.\nRequests to\nholdings begin to\ntake longer.\nCollaborators\nHoldings\nHoldings\nTransactions\nRequests\nRetries\n100 QPS\n500 QPS\n100 QPS\n500 QPS\nLB\n1200 QPS\nFailed requests to\ntransactions\ntrigger retries.\nRetries increase\nrequests to\ntransactions.\nFigure 6.8    Overload on transactions causes some requests to fail, in turn causing holdings to retry \nthose requests, which starts to degrade holdings’ response time.\n \n",
      "content_length": 1939,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "\t\n139\nDesigning reliable communication\nHoldings\nC\nB\nTransactions\nF\nE\nD\nB\nC\nTransactions\nHoldings\nIncreased failure in upstream dependencies leads to retries,\nrepeating the cycle of failure.\nUpstream dependencies are unable to service requests\nthat rely on transactions.\nFigure 6.9    Overload in a service leads to complete failure. Unhealthy retry behavior is repeated across \ndependency chains as service performance progressively degrades, leading to further overloads.\nThis feedback loop — failed requests lead to a higher volume of subsequent requests, \nleading to a higher rate of failure — continues to escalate. Your whole system is unable to \ncomplete work, as other services that rely on transactions or holdings begin to fail. Your \ninitial failure in a single service causes a domino effect, worsening response times and \navailability across several services. At worst, the cumulative impact on the transactions ser-\nvice causes it to fail completely. Figure 6.9 illustrates this final stage of a cascading failure.\nCascading failures aren’t only caused by overload — although this is one of the most \ncommon root causes. In general, increased error rates or slower response times can \nlead to unhealthy service behavior, increasing the chance of failure across multiple ser-\nvices that depend on each other.\nYou can use several approaches to limit the occurrence of cascading failures in micro­\nservice applications: circuit breakers; fallbacks; load testing and capacity planning; back-off \nand retries; and appropriate timeouts. We’ll explore these approaches in the next section.\n6.3\t\nDesigning reliable communication\nEarlier, we emphasized the importance of collaboration in a microservice application. \nDependency chains of multiple microservices will achieve most useful capabilities in \nan application. When one microservice fails, how does that impact its collaborators \nand ultimately, the application’s end customers?\nIf failure is inevitable, you need to design and build your services to maximize avail-\nability, correct operation, and rapid recovery when failure does occur. This is funda-\nmental to achieving resiliency. In this section, we’ll explore several techniques for \nensuring that services behave appropriately — maximizing correct operation — when \ntheir collaborators are unavailable:\n¡ Retries\n¡ Fallbacks, caching, and graceful degradation\n¡ Timeouts and deadlines\n \n",
      "content_length": 2407,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "140\nChapter 6  Designing reliable services\n¡ Circuit breakers\n¡ Communication brokers\nBefore we start, let’s get a simple service running that we can use to illustrate the concepts \nin this section. You can find these examples in the book’s Github repository (http://\nmng.bz/7eN9). Clone the repository to your computer and open the chapter-6 direc-\ntory. This directory contains some basic services — holdings and market-data — which \nyou’ll run inside Docker containers (figure 6.10). The holdings service exposes a GET \n/holdings endpoint, which makes a JSON API request to retrieve price information \nfrom market-data.\nTo run these, you’ll need docker-compose installed (directions online: https://docs \n.docker.com/compose/install/). If you’re ready to go, type the following at the com-\nmand line:\n$ docker-compose up\nThis will build Docker images for each service and start them as isolated containers on \nyour machine. Now let’s dive in!\n6.3.1\t\nRetries\nIn this section, we’ll explore how to use retries when failed requests occur. To under-\nstand these techniques, let’s start by examining communication from the perspective \nof your upstream service, holdings.\nImagine that a call from the holdings service to retrieve prices fails, returning an \nerror. From the perspective of the calling service, it’s not clear yet whether this failure \nis isolated — repeating that call is likely to succeed, or systemic — the next call has a \nhigh likelihood of failing. You expect calls to retrieve data to be idempotent  — to have no \neffect on the state of the target system and therefore be repeatable.2\nAs a result, your first instinct might be to retry the request. In Python, you can use \nan open source library — tenacity — to decorate the appropriate method of your \nAPI client (the MarketDataClient class in holdings/clients.py) and perform retries if \nthe method throws an exception. The following listing shows the class with retry code \nadded.\nHoldings\nHoldings\nRequests\nMarket-data\nMarket-data\nFigure 6.10    Docker containers for working with microservice requests\n2\t Requests that effect some system change aren’t typically idempotent. One strategy for guaranteeing \n“exactly once” semantics is to implement idempotency keys. See Brandur Leach, “Designing robust \nand predictable APIs with idempotency,” February 22, 2017, https://stripe.com/blog/idempotency.\n \n",
      "content_length": 2375,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "\t\n141\nDesigning reliable communication\nListing 6.1    Adding a retry to a service call\nimport requests\nimport logging\nfrom tenacity import retry, stop, before \nclass MarketDataClient(object):\n    logger = logging.getLogger(__name__)\n    base_url = 'http://market-data:8000'\n    def _make_request(self, url):\n        response = requests.get(f\"{self.base_url}/{url}\", \n                                headers={'content-type': 'application/json'})\n        return response.json()\n    @retry(stop=stop_after_attempt(3), \n           before=before_log(logger, logging.DEBUG)) \n    def all_prices(self):\n        return self._make_request(\"prices\")\nLet’s call the holdings service to see how it behaves. In another terminal window, make \nthe following request:\ncurl -I http://{DOCKER_HOST}/holdings\nThis will return a 500 error, but if you follow the logs from the market-data service, you \ncan see a request being made to GET /prices three times, before the holdings service \ngives up.\nIf you read the previous section, you should be wary at this point. Failure might be \nisolated or persistent, but the holdings service can’t know which one is the case based \non one call.\nIf the failure is isolated and transient, then a retry is a reasonable option. This helps \nto minimize direct impact to end users — and explicit intervention from operational \nstaff — when abnormal behavior occurs. It’s important to consider your budget for \nretries: if each retry takes a certain number of milliseconds, then the consuming service \ncan only absorb so many retries before it surpasses a reasonable response time.\nBut if the failure is persistent — for example, if the capacity of market-data is \nreduced — then subsequent calls may worsen the issue and further destabilize the \nsystem. Suppose you retry each failed request to market-data five times. Every failed \nrequest you make to this service potentially results in another five requests; the volume \nof retries continues to grow. The entire service is doing less useful work as it attempts \nto service a high volume of retries. At worst, retries suffocate your market-data service, \nmagnifying your original failure. Figure 6.11 illustrates this growth of requests.\nImports relevant functions \nfrom the library\nRetries the query up to three times\nLogs each retry  \nbefore execution\n \n",
      "content_length": 2323,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "142\nChapter 6  Designing reliable services\n# of requests\n3000\n0\n6000\n9000\n12000\nRetries\nRequests\nRequests vs Retries\nFigure 6.11    Growth of load on your unstable market-data service resulting from failed requests being \nretried\nHow can you use retries to improve your resiliency in the face of intermittent failures \nwithout contributing to wider system failure if persistent failures occur? First, you \ncould use a variable time between successive retries to try to spread them out evenly \nand reduce the frequency of retry-based load. This is known as an exponential back-off \nstrategy and is intended to give a system under load time to recover. You can change \nthe retry strategy you used earlier, as shown in the following listing. Afterwards, by curl-\ning the /holdings endpoint, you can observe the retry behavior of the service.\nListing 6.2    Changing your retry strategy to exponential back-off\n@retry(wait=wait_exponential(multiplier=1, max=5), \n       stop=stop_after_delay(5)) \ndef all_prices(self):\n    return self._make_request(\"prices\")\nUnfortunately, exponential back-off can lead to another instance of curious emergent \nbehavior. Imagine that a momentary failure interrupts several calls to market-data, \nleading to retries. Exponential back-off can cause the service to schedule those retries \ntogether so they further amplify themselves, like the ripples from throwing a stone in \na pond.\nInstead, back-off should include a random element — jitter — to spread out retries to \na more constant rate and avoid thundering herds of synchronized retries.3 The follow-\ning listing shows how to adjust your strategy again.\nWaits 2^x * 1 second \nbetween each retry\nStops after five seconds\n3\t A great article by Marc Brooker about exponential back-off and the importance of jitter is avail-\nable on the AWS Architecture Blog, March 4, 2015, http://mng.bz/TRk5.\n \n",
      "content_length": 1877,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "\t\n143\nDesigning reliable communication\nListing 6.3    Adding jitter to an exponential back-off\n@retry(wait=wait_exponential(multiplier=1, max=5) + wait_random(0, 1), \n       stop=stop_after_delay(5)) \ndef all_prices(self):\n    return self._make_request(\"prices\")\nThis strategy will ensure that retries are less likely to happen in synchronization across \nmultiple waiting clients.\nRetries are an effective strategy for tolerating intermittent dependency faults, but \nyou need to use them carefully to avoid exacerbating the underlying issue or consum-\ning unnecessary resources:\n¡ Always limit the total number of retries.\n¡ Use exponential back-off with jitter to smoothly distribute retry requests and \navoid compounding load.\n¡ Consider which error conditions should trigger a retry and, therefore, which \nretries are unlikely to, or will never, succeed.\nWhen your service meets retry limits or can’t retry a request, you can either accept \nfailure or find an alternative way to serve the request. In the next section, we’ll explore \nfallbacks.\n6.3.2\t\nFallbacks\nIf a service’s dependencies fail, you can explore four fallback options:\n¡ Graceful degradation\n¡ Caching\n¡ Functional redundancy\n¡ Stubbed data\nGraceful Degradation\nLet’s return to the problem with the holdings service: if market-data fails, the applica-\ntion may not be able to provide valuations to end customers. To resolve this issue, you \nmight be able to design an acceptable degradation of service. For example, you could \nshow holding quantities without valuations. This limits the richness of your UI but is \nbetter than showing nothing — or an error. You can see techniques like this in other \ndomains. For example, an e-commerce site could still allow purchases to be made, \neven if the site’s order dispatch isn’t functioning correctly.\nCaching\nAlternatively, you could cache the results of past queries for prices, reducing the need \nto query the market-data service at all. Say a price is valid for five minutes. If so, the \nholdings service could cache pricing data for up to five minutes, either locally or in \nExponentially backs off, adding a random \nwait between zero and one second\nStops after five seconds\n \n",
      "content_length": 2195,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "144\nChapter 6  Designing reliable services\na dedicated cache (for example, Memcached or Redis). This solution would both \nimprove performance and provide contingency in the event of a temporary outage.\nLet’s try out this technique. You’ll use a library called cachetools, which provides an \nimplementation of a time-to-live cache. As you did earlier with retries, you’ll decorate \nyour client method, as shown in the following listing.\nListing 6.4    Adding in-process caching to a client call\nimport requests\nimport logging\nfrom cachetools import cached, TTLCache\nclass MarketDataClient(object):\n    logger = logging.getLogger(__name__)\n    cache = TTLCache(maxsize=10, ttl=5*60) \n    base_url = 'http://market-data:8000'\n    def _make_request(self, url):\n        response = requests.get(f\"{self.base_url}/{url}\", \n                                headers={'content-type': 'application/json'})\n        return response.json()\n    @cached(cache) \n    def all_prices(self):\n        logger.debug(\"Making request to get all_prices\")\n        return self._make_request(\"prices\")\nSubsequent calls made to GET /holdings should retrieve price information from \nthe cache, rather than by making calls to market-data. If you used an external cache \ninstead, multiple instances could use the cache, further reducing load on market-data \nand providing greater resiliency for all holdings replicas, albeit at the cost of maintain-\ning an additional infrastructural component.\nFunctional redundancy\nSimilarly, you might be able to fall back to other services to achieve the same func-\ntionality. Imagine that you could purchase market data from multiple sources, each \ncovering a different set of instruments at a different cost. If source A failed, you could \ninstead make requests to source B (figure 6.12).\nFunctional redundancy within a system has many drivers: external integrations; algo-\nrithms for producing similar results with varying performance characteristics; and even \nolder features that remain operational but have been superseded. In a globally distrib-\nuted deployment, you could even fall back on services hosted in another region.4\nOnly some failure scenarios would allow the use of an alternative service. If the cause \nof failure was a code defect or resource overload in your original service, then rerouting \nto another service would make sense. But a general network failure could affect multi-\nple services, including ones you might try rerouting to.\nInstantiates a cache\nDecorates your method to  \nstore results using your cache\n4\t At the ultimate end of this scale, Netflix can serve a given customer from any of their global data \ncenters, conveying an impressive degree of resilience.\n \n",
      "content_length": 2702,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "\t\n145\nDesigning reliable communication\nFalls back to querying\nmarket-data-B\nRetrieves prices\nHoldings\nmarket-data-A\nHoldings\nmarket-data-A\nmarket-data-B\nRequests to market-data-A fail\nNormal operation\nFigure 6.12    If service failure occurs, you may be able to serve the same capability with other services.\nStubbed data\nLastly, although it wouldn’t be appropriate in this specific scenario, you could use \nstubbed data for fallbacks. Picture the “recommended to you” section on Amazon: if \nthe backend was unable for some reason to retrieve those personalized recommenda-\ntions, it’d be more graceful to fall back to a nonpersonalized data set than to show a \nblank section on the UI.\n6.3.3\t\nTimeouts\nWhen the holdings service sends a request to market-data, that service consumes \nresources waiting for a reply. Setting an appropriate deadline for that interaction limits \nthe time those resources are consumed.\nYou can set a timeout within your HTTP request function. For HTTP calls, you want \nto timeout if you haven’t received any response, but not if the response itself is slow to \ndownload. Try the following listing to add a timeout.\nListing 6.5    Adding a timeout to an HTTP call\ndef _make_request(self, url):\n    response = requests.get(f\"{self.base_url}/{url}\",\n                           headers={'content-type': 'application/json'},\n                           timeout=5) \n    return response.json()\nIn a computational sense, network communication is slow, so the speed of failures is \nimportant. In a distributed system, some errors might happen almost instantly. For \nexample, a dependent service may rapidly fail in the event of an internal bug. But many \nfailures are slow. For example, a service that’s overloaded by requests may respond \nsluggishly, in turn consuming the resources of the calling service while it waits for a \nresponse that may never come.\nSets a timeout of five seconds before \nreceiving data from market-data\n \n",
      "content_length": 1951,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "146\nChapter 6  Designing reliable services\nSlow failures illustrate the importance of setting appropriate deadlines — timing out \nin a reasonable timeframe — for communication between microservices. If you don’t \nset upper bounds, it’s easy for unresponsiveness to spread through entire microservice \ndependency chains. In fact, lack of deadlines can extend the impact of issues because a \nserver consumes resources while it waits forever for an issue to be resolved.\nPicking a deadline can be difficult. If they’re too long, they can consume unnecessary \nresources for a calling service if a service is unresponsive. If they’re too short, they can cause \nhigher levels of failure for expensive requests. Figure 6.13 illustrates these constraints.\nFor many microservice applications, you set deadlines at the level of individual inter-\nactions; for example, a call from holdings to market-data may always have a deadline of \n10 seconds. A more elegant approach is to apply an absolute deadline across an entire \noperation and propagate the remaining time across collaborators.\nWithout propagating deadlines, it can be difficult to make them consistent across a \nrequest. For example, holdings could waste resources waiting for market-data far beyond \nthe overall deadline imposed by a higher level of the stack, such as an API gateway.\nImagine a chain of dependencies between multiple services. Each service takes a cer-\ntain amount of time to do its work and expects its collaborators to take some time. If any \nof those times vary, static expectations may no longer be correct (figure 6.14).\nIf your service interactions are over HTTP, you could propagate deadlines using a \ncustom HTTP header, such as X-Deadline: 1000, passing that value to set read timeout \nvalues on subsequent HTTP client calls. Many RPC frameworks, such as gRPC, explic-\nitly implement mechanisms for propagating deadlines within a request context.\n6.3.4\t\nCircuit breakers\nYou can combine some of the techniques we've discussed so far. You can consider an \ninteraction between holdings and market-data as analogous to an electrical circuit. In \nelectrical wiring, circuit breakers perform a protective role — preventing spikes in cur-\nrent from damaging a wider system. Similarly, a circuit breaker is a pattern for pausing \nrequests made to a failing service to prevent cascading failures.\nUnresponsive\nTypical response\nLengthy deadlines lead\nto wasted resource\nconsumption when\nfailure occurs.\nShort deadlines\ncontribute to failure if\nresponses often take\nlonger.\nShort deadline\nLong deadline\nTime\nFigure 6.13    Choosing the right deadline requires balancing time constraints to maximize the window of \nsuccessful requests.\n \n",
      "content_length": 2704,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "\t\n147\nDesigning reliable communication\nThere is no synchronization between\ndeadlines; service B will work beyond\nthe deadline that service A sets.\nIn normal operation, response\ntimes are within those\ndeadlines.\nDeadline\nAvg response\ntime\nDownstream services are\nunaware of upstream limits.\nServices set wait times for calls\nto their dependencies.\nD\nC\nB\nA\n5s\n4s\n10s\n2s\n1s\n10s\nFigure 6.14    Services may set expectations about how long they expect calls to collaborators to take; \nvarying widely because of failure or latency can exacerbate the impact of those failures.\nHow does it work? Two principles, both of which we touched on in the previous sec-\ntion, inform the design of a circuit breaker:\n1\t Remote communication should fail quickly in the event of an issue, rather than \nwasting resources waiting for a response that might never come.\n2\t If a dependency is failing consistently, it’s better to stop making further requests \nuntil that dependency recovers.\nWhen making a request to a service, you can track the number of times that request \nsucceeds or fails. You might track this number within each running instance of a ser-\nvice or share that state (using an external cache) across multiple services. In this nor-\nmal operation, we consider the circuit to be closed.\nIf the number of failures seen or the rate of failures within a certain time window \npasses a threshold, then the circuit is opened. Rather than attempting to send requests \nto your collaborating service, you should short-circuit requests and, where possible, \nperform an appropriate fallback — returning a stubbed message, routing to a different \nservice, or returning a cached response. Figure 6.15 illustrates the lifecycle of a request \nusing a circuit breaker.\nSetting the time window and threshold requires careful consideration of both \nthe expected reliability of the target service and the volume of interactions between \nservices. If requests are relatively sparse, then a circuit breaker may not be effective, \nbecause a large time window might be required to obtain a representative sample of \nrequests. For service interactions with contrasting busy and quiet periods, you may want \nto introduce a minimum throughput to ensure a circuit only reacts when load is statis-\ntically significant.\n \n",
      "content_length": 2286,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "148\nChapter 6  Designing reliable services\nRequest\nResponse\nFallback\nOpen\nCircuit open?\nOpen circuit\nYes\nYes\nNo\nMake request\nto service\nClosed\nSuccess?\nReport metrics\nThreshold\nexceeded?\nFigure 6.15    A circuit breaker controls the flow of requests between two services and opens when the \nnumber of failed requests surpasses a threshold.\nNOTE    You should monitor when circuits are opened and closed, as well as \npotentially alerting the team responsible, especially if the circuit is frequently \nopened. We’ll discuss this further in part 4.\nOnce the circuit has opened, you probably don’t want to leave it that way. When avail-\nability returns to normal, the circuit should be closed. The circuit breaker needs to \nsend a trial request to determine whether the connection has returned to a healthy \nstate. In this trial state, the circuit breaker is half open: if the call succeeds, the circuit \nwill be closed; otherwise, it will remain open. As with other retries, you should sched-\nule these attempts with an exponential back-off with jitter. Figure 6.16 shows the three \ndistinct states of a circuit breaker.\nSeveral libraries are available that provide implementations of the circuit breaker \npattern in different languages, such as Hystrix (Java), CB2 (Ruby), or Polly (.NET).\nTIP    Don’t forget that closed is the good state for a circuit breaker! The use of \nopen and closed to represent, respectively, negative and positive states may seem \ncounterintuitive but reflects the real-world behavior of an electrical circuit.\n \n",
      "content_length": 1538,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "\t\n149\nDesigning reliable communication\nSuccessful requests\nIn this state, requests fail quickly.\n1. If failure thresholds aren’t\nmet, the circuit stays closed.\nOpen\nHalf open\nClosed\n2. If the failure threshold\nis exceeded, the circuit\nis opened.\n3. After a delay,\nthe circuit attemps to close.\n5. If closing is successful,\nthe circuit returns to closed.\n4. If this fails,\nthe circuit returns to open.\nFigure 6.16    A circuit breaker transitions between three stages: open, closed, and half open.\n6.3.5\t\nAsynchronous communication\nSo far, we’ve focused on failure in synchronous, point-to-point communication \nbetween services. As we outlined in the first section, the more services in a chain, the \nlower overall availability you can guarantee for that path.\nDesigning asynchronous service interactions, using a communication broker like a \nmessage queue, is another technique you can use to maximize reliability. Figure 6.17 \nillustrates this approach.\nWhere you don’t need immediate, consistent responses, you can use this tech-\nnique to reduce the number of direct service interactions, in turn increasing overall \navailability — albeit at the expense of making business logic more complex. As we \nmentioned elsewhere in this book, a communication broker becomes a single point \nof failure that will require careful attention for you to scale, monitor, and operate \neffectively.\n \n",
      "content_length": 1385,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "150\nChapter 6  Designing reliable services\nEmits\nmessage\nConsumes\nmessage\nConsumes\nmessage\nMessage\nqueue\nService\nService\nService\nFigure 6.17    Using a message queue to decouple services from direct interaction\n6.4\t\nMaximizing service reliability\nIn the previous sections, we explored techniques to ensure a service can tolerate faults \nin interactions with its collaborators. Now, let’s consider how you can maximize avail-\nability and fault tolerance within an individual service. In this section, we’ll explore \ntwo techniques — health checks and rate limits — as well as methods for validating the \nresilience of services.\n6.4.1\t\nLoad balancing and service health\nIn production, you deploy multiple instances of your market-data service to ensure \nredundancy and horizontal scalability. A load balancer will distribute requests from \nother services between these instances. In this scenario, the load balancer plays two roles:\n1\t Identifying which underlying instances are healthy and able to serve requests\n2\t Routing requests to different underlying instances of the service\nA load balancer is responsible for executing or consuming the results of health checks. \nIn the previous section, you could ascertain the health of a dependency at the point \nof interaction — when requests were being made. But that’s not entirely adequate. You \nshould have some way of understanding the application’s readiness to serve requests at \nany time, rather than when it’s actively being queried.\nEvery service you design and deploy should implement an appropriate health check. If \na service instance becomes unhealthy, that instance should no longer receive traffic from \nother services. For synchronous RPC-facing services, a load balancer will typically query \neach instance’s health check endpoint on an interval basis. Similarly, asynchronous services \nmay use a heartbeat mechanism to test connectivity between the queue and consumers.\nTIP    It’s often desirable for repeated or systematic instance failures, as detected \nby health checks, to trigger alerts to an operations team — a little human inter-\nvention can be helpful. We’ll explore that further in part 4 of this book.\n \n",
      "content_length": 2179,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "\t\n151\nMaximizing service reliability\nYou can classify health checks based on two criteria: liveness and readiness. A live-\nness check is typically a simple check that the application has started and is running \ncorrectly. For example, an HTTP service should expose an endpoint — commonly \n/health, /ping, or /heartbeat — that returns a 200 OK response once the service is \nrunning (figure 6.18). If an instance is unresponsive, or returns an error message, the \nload balancer will no longer deliver requests there.\nIn contrast, a readiness check indicates whether a service is ready to serve traffic, \nbecause being alive may still not indicate that requests will be successful. A service might \nhave many dependencies — databases, third-party services, configuration, caches — so \nyou can use a readiness check to see if these constituent components are in the correct \nstate to serve requests. Both of the example services implement a simple HTTP liveness \ncheck, as shown in the following listing.\nListing 6.6    Flask handler for an HTTP liveness check\n@app.route('/ping', methods=[\"GET\"])\ndef ping():\n    return 'OK'\nHealth checks are binary: either an instance is available or it isn’t. This works well with \ntypical round-robin load balancing, where requests are distributed to each replica in \nturn. But in some circumstances the functioning of a service may be degraded and \nexhibit increased latency or error rates without a health check reflecting this status. \nAs such, it can be beneficial to use load balancers that are aware of latency and able \nto route requests to instances that are performing better, or those that are under less \nload, to achieve more consistent service behavior. This is a typical feature of a microser-\nvice proxy, which we’ll touch on later in this chapter.\nLB\n4xx/5xx\n200\n200\nHoldings\nHoldings\nLB\n200\nHoldings\nHoldings\nFigure 6.18    Load balancers continuously query service instances to check their health. If an instance \nis unhealthy, the load balancer will no longer route requests to that instance until it recovers.\n \n",
      "content_length": 2066,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "152\nChapter 6  Designing reliable services\n6.4.2\t\nRate limits\nUnhealthy service usage patterns can sometimes arise in large microservice appli-\ncations. Upstream dependencies might make several calls, where a single batch call \nwould be more appropriate, or available resources may not be distributed equitably \namong all callers. Similarly, a service with third-party dependencies could be limited by \nrestrictions that those dependencies impose.\nAn appropriate solution is to explicitly limit the rate of requests or total requests \navailable to collaborating services in a timeframe. This helps to ensure that a service —  \nparticularly when it has many collaborators — isn’t overloaded. The limiting might be \nindiscriminate (drop all requests above a certain volume) or more sophisticated (drop \nrequests from infrequent service clients, prioritize requests for critical endpoints, and \ndrop low-priority requests). Table 6.4 outlines different rate-limiting strategies.\nTable 6.4    Common rate-limiting strategies\nStrategy\nDescription\nDrop requests above volume\nDrop consumer requests above a specified volume\nPrioritize critical traffic\nDrop requests to low-priority endpoints to prioritize resources for critical \ntraffic\nDrop uncommon clients\nPrioritize frequent consumers of the service over infrequent users\nLimit concurrent requests\nLimit the overall number of requests an upstream service can make over \na time period\nRate limits can be shared with a service’s clients at design time or, better, at runtime. A \nservice might return a header to a consumer that indicates the remaining volume of \nrequests available. On receipt, the upstream collaborator should take this into account \nand adjust its rate of outbound requests. This technique is known as back pressure.\n6.4.3\t\nValidating reliability and fault tolerance\nApplying the tactics and patterns we’ve covered will put you on a good path toward \nmaximizing availability. But it’s not enough to plan and design for resiliency: you need \nto validate that your services can tolerate faults and recover gracefully.\nThorough testing provides assurance that your chosen design is effective when \nboth predicted and unpredictable failures occur. Testing requires the application of \nload testing and chaos testing. Although it’s likely you’re familiar with code testing — such \nas unit and acceptance testing to validate implementation, usually in a controlled \nenviron­ment — you might not know that load and chaos testing are intended to val-\nidate service limits by closely replicating the turbulence of production operation. \nAlthough testing isn’t the primary focus of this book, it’s useful to understand how \nthese different testing techniques can help you build a robust microservice application.\n \n",
      "content_length": 2770,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "\t\n153\nMaximizing service reliability\nLoad testing\nAs a service developer, you can usually be confident that the number of requests made \nto your service will increase over time. When developing a service, you should\n1\t Model the expected growth and shape of service traffic to ensure that you under-\nstand the likely usage of your service\n2\t Estimate the capacity required to service that traffic\n3\t Validate the deployed capacity of the service by load testing against those limits\n4\t Use business and service metrics as appropriate to re-estimate capacity\nImagine you’re considering how much capacity the market-data service requires. First, \nwhat do you know about the service’s usage patterns? You know that holdings queries \nthe service, but it may be called from elsewhere too — pricing data is used throughout \nSimpleBank’s product.\nLet’s assume that queries to market-data grow roughly in line with the number of \nactive users on the platform, but you may experience spikes (for example, when the \nmarket opens in the morning). You can plan capacity based on predictions of your busi-\nness growth. Table 6.5 outlines a simple estimation of the QPS that you can expect this \nservice to receive over a three-month period.\nTable 6.5    Estimate of calls to a service per second based on growth in average active users over a three-\nmonth period\nJun\nJul\nAug\nTotal Users\n4000\n5600\n7840\nExpected Growth\n40%\n40%\n40%\nActive Users\nAverage\n20%\n800\n1120\n1568\nPeak\n70%\n2800\n3920\n5488\nService Calls\nAverage\nPer User/Minute\n30\n24000\n33600\n47040\nPer User/Second\n0.5\n400\n560\n784\nPeak\nPer User/Minute\n30\n84000\n117600\n164640\nPer User/Second\n0.5\n1400\n1960\n2744\nIdentifying the qualitative factors that drive growth in service utilization is vital to good \ndesign and optimizing capacity. Once you’ve done that, you can determine how much \n \n",
      "content_length": 1830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "154\nChapter 6  Designing reliable services\ncapacity to deploy. For example, the table suggests you need to be able to service 400 \nrequests per second in normal operation, growing by 40% month on month, with \nspikes in peak usage to 1,400 requests per second.\nTIP    An in-depth review of capacity and scale planning techniques is outside the \nscope of this book, but a great overview is available in Abbott and Fisher’s The \nArt of Scalability (Addison-Wesley Professional, 2015) (ISBN 978-0134032801).\nOnce you’ve established a baseline capacity for your service, you can then iteratively \ntest that capacity against expected traffic patterns. Along with validating the traffic lim-\nits of a microservice configuration, load testing can identify potential bottlenecks or \ndesign flaws that aren’t apparent at lower levels of load. Load testing can provide you \nwith highly effective insight into the limitations of your services.\nAt the level of individual services, you should automate the load testing of each ser-\nvice as part of its delivery pipeline — something we’ll explore in part 3 of this book. \nAlong with this systematic load testing, you should perform exploratory load testing to \nidentify limits and test your assumptions about the load that services can handle.\nYou also should load test services together. This can aid in identifying unusual load \npatterns and bottlenecks based on service interaction. For example, you could write a \nload test that exercises all the services in the GET /holdings example.\nChaos testing\nMany failures in a microservice application don’t arise from within the microservices \nthemselves. Networks fail, virtual machines fail, databases become unresponsive — failure \nis everywhere! To test for these types of failure scenarios, you need to apply chaos testing.\nChaos testing pushes your microservice application to fail in production. By intro-\nducing instability and failure, it accurately mimics real system failures, as well as train-\ning an engineering team to be able to react to those failures. This should ultimately \nbuild your confidence in the system’s capability to withstand real chaos because you’ll \nbe gradually improving the resiliency of your system and reducing the possible number \nof events that would cause operational impact.\nAs explained on the “Principles of Chaos Engineering” website (https://principlesof \nchaos.org/), you can think of chaos testing as “the facilitation of experiments to \nuncover systemic weaknesses.” The website lays out this approach:\n1\t Define a measurable steady state of normal system operation.\n2\t Hypothesize that behavior in an experimental and control group will remain \nsteady; the system will be resilient to the failure introduced.\n3\t Introduce variables that reflect real-world failure events — for example, removing \nservers, severing network connections, or introducing higher levels of latency.\n4\t Attempt to disprove the hypothesis you defined in (2).\n \n",
      "content_length": 2968,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "\t\n155\nMaximizing service reliability\nRecall how the holdings, transactions, and market-data services were deployed in fig-\nure 6.5. In this case, you expect steady operation to return holdings data within a rea-\nsonable response time. A chaos test could introduce several variables:\n1\t Killing nodes running market-data or transactions, either partially or completely\n2\t Reducing capacity by killing holdings instances at random\n3\t Severing the network connection — for example, between holdings and down-\nstream services or between services and their data stores\nFigure 6.19 illustrates these options.\nCompanies with mature chaos testing practices might even perform testing on both \na systematic and random basis against live production environments. This might sound \nterrifying; real outages can be stressful enough, let alone actively working to make them \nhappen. But without taking this approach, it’s incredibly difficult to know that your \nsystem is truly resilient in the ways that you expect. In any organization, you should \nstart small, by introducing a limited set of possible failures, or only running scheduled, \nrather than random, tests. Although you can also perform chaos tests in a staging envi-\nronment, you’ll need to carefully consider whether that environment is truly represen-\ntative of or equivalent to your production configuration.\nTIP    Chaos Toolkit (http://chaostoolkit.org/) is a great tool to start with if \nyou’d like to practice chaos engineering techniques.\nUltimately, by regularly and systematically validating your system against chaotic events \nand resolving the issues you encounter, you and your team will be able to achieve a sig-\nnificant level of confidence in your application’s resilience to failure.\nUI\nLB\nLB\nSever or throttle\nnetwork\nconnections\nMarket-data\nTransactions\nRemove\ninstances to\nreduce capacity\nData store\nDisable\ncomponents\nData store\nLB\nHoldings\nFigure 6.19    Potential variables to introduce in a chaos test to reflect real-world failure events\n \n",
      "content_length": 2015,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "156\nChapter 6  Designing reliable services\n6.5\t\nSafety by default\nCritical paths in your microservice application will only be as resilient and available as \ntheir weakest link. Given the impact that individual services can have overall availabil-\nity, it’s imperative to avoid emergencies where introducing new services or changes in \na service dependency chain significantly degrade that measure. Likewise, you don’t \nwant to find out that crucial functionality can’t tolerate faults when that fault happens.\nWhen applications are technically heterogeneous, or distinct teams deliver underly-\ning services, it can be exceptionally difficult to maintain consistent approaches to reli-\nable interaction. We touched on this back in chapter 2 when we discussed isolation and \ntechnical divergence. Teams are under different delivery pressures and different ser-\nvices have different needs — at worst, developers might forget to follow good resiliency \npractices.\nAny change in service topology can have a negative impact. Figure 6.20 illustrates \ntwo examples: adding a new collaborator downstream from market-data might decrease \nmarket-data’s availability, whereas adding a new consumer might reduce the overall \ncapacity of the market-data service, reducing service for existing consumers.\nFrameworks and proxies are two different technical approaches to applying com-\nmunication standards across multiple services that make it easy for engineers to fall \ninto doing the right thing by ensuring services communicate resiliently and safely by \ndefault.\n6.5.1\t\nFrameworks\nA common approach for ensuring services always communicate appropriately is to \nmandate the use of specific libraries implementing common interaction patterns like \ncircuit breakers, retries, and fallbacks. Standardizing these interactions across all ser-\nvices using a library has the following advantages:\n1\t Increases the overall reliability of your application by avoiding roll-your-own \napproaches to service interaction\n2\t Simplifies the process of rolling out improvements or optimizations to communi-\ncation across any number of services\n3\t Clearly and consistently distinguishes network calls from local calls within code\n4\t Can be extended to provide supporting functionality, such as collecting metrics \non service interactions\nThis approach tends to be more effective when a company uses one language (or few \nlanguages) for writing code; for example, Hystrix, which we mentioned earlier, was \nintended to provide a standardized way — across all Java-based services in Netflix’s \norganization — of controlling interactions between distributed services.\nNOTE    Standardizing communication is a crucial element of building a \nmicroservice chassis, which we’ll explore in the next chapter.\n \n",
      "content_length": 2773,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "\t\n157\nSafety by default\nHoldings\nMarket-data\nNew\nconsumer\nIncreased load might\nnegatively impact existing\nservice availability.\nA new dependency can\nnegatively impact availability.\nNew\ncollaborator\nFigure 6.20    Availability impact of new services in a dependency chain\n6.5.2\t\nService mesh\nAlternatively, you could introduce a service mesh, such as Linkerd (https://linkerd.io) \nor Envoy (www.envoyproxy.io), between your services to control retries, fallbacks, and \ncircuit breakers, rather than making this behavior part of each individual service. A \nservice mesh acts as a proxy. Figure 6.21 illustrates how a service mesh handles commu-\nnication between services.\nInstead of services communicating directly with other services, service communica-\ntion passes through the service mesh application, typically deployed as a separate pro-\ncess on the same host as the service. You then can configure the proxy to manage that \ntraffic appropriately — retrying requests, managing timeouts, or balancing load across \ndifferent services. From the caller’s perspective, the mesh doesn’t exist — it makes \nHTTP or RPC calls to another service as normal.\nHost\nProxy\nA\nHost\nLogical communication\nProxied communication\nProxy\nService proxies\nroute the actual\nrequest; they abstract\nthis detail away\nfrom the services.\nService A makes a request\nto service B.\nB\nFigure 6.21    Communication between services using a service mesh\n \n",
      "content_length": 1421,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "158\nChapter 6  Designing reliable services\nAlthough this may make the treatment of service interaction less explicit to an engi-\nneer working on a service, it can simplify defensive communication in applications \nthat are heterogeneous. Otherwise, consistent communication can require significant \ntime investment to achieve across different languages, because ecosystems and librar-\nies may have unequal capabilities or support for resiliency features.\nSummary\n¡ Failure is inevitable in complex distributed systems — you have to consider fault \ntolerance when you’re designing them.\n¡ The availability of individual services affects the availability of the wider \napplication.\n¡ Choosing the right level of risk mitigation for an application requires careful \nconsideration of the frequency and impact of failure versus the cost of mitigating \nagainst potentially rare events.\n¡ Most failures occur in one of four areas: hardware, communication, dependen-\ncies, or internally.\n¡ Cascading failures result from positive feedback and are a common failure mode \nin a microservice application. They’re most commonly caused by server overload.\n¡ You can use retries and deadlines to mitigate against faults in service interactions. \nYou need to apply retries carefully to avoid exacerbating failure in other services.\n¡ You can use fallbacks — such as caching, alternative services, and default \nresults — to return successful responses, even when service dependencies fail.\n¡ You should propagate deadlines between services to ensure they’re consistent \nacross a system and to minimize wasted work.\n¡ Circuit breakers between services protect against cascading failures by failing \nquickly when a high threshold of errors is encountered.\n¡ Services can use rate limits to protect themselves from spikes in load beyond their \ncapacity to service.\n¡ Individual services should expose health checks for load balancers and monitor-\ning to be able to use.\n¡ You can effectively validate resiliency by practicing both load and chaos testing.\n¡ You can apply standards — whether through proxies or frameworks — to help \nengineers “fall into the pit of success” and build services that tolerate faults by \ndefault.\n \n",
      "content_length": 2207,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "159\n7\n Building a reusable \nmicroservice framework\nThis chapter covers\n¡ Building a microservice chassis\n¡ Advantages of enforcing uniform practices \nacross teams\n¡ Abstracting common concerns in a reusable \nframework\nOnce an organization fully embraces microservices and teams grow in number, it’s \nquite likely that each of those teams will start specializing in a given set of program-\nming languages and tools. Sometimes, even when using the same programming lan-\nguage, different teams will choose a different combination of tools to achieve the \nsame purpose. Although nothing is wrong with this, it may lead to an increased \nchallenge for engineers moving between different teams. The ritual to set up new \nservices, as well as the code structure, may be quite different. Even if teams eventu-\nally end up solving the same challenges in different ways, we believe this potential \nduplication is better than having to add a synchronization layer.\n \n",
      "content_length": 955,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "160\nChapter 7   Building a reusable microservice framework \nHaving strict rules on the tools and languages that teams can use and enforcing a \ncanonical way of setting up new services across all teams may harm speed and inno-\nvation and will eventually lead to the use of the same tools for every problem. Fortu-\nnately, you can enforce some common practices while keeping things rather free for \nteams to choose the programming language for specific services. You can encapsu-\nlate a set of tools for each adopted language while making sure that engineers have \naccess to resources that’ll make it easy to abide by the practices across all teams. If \nteam A decides to go with Elixir to create a service for managing notifications and \nteam B decides to use Python for an image analysis service, they should both have the \ntools that allow those two services to emit metrics to the common metrics collection \ninfrastructure.\nYou should centralize logs in the same place and with the same format, and things \nlike circuit breakers, feature flags, or the ability to share the same event bus should be \navailable. That way, teams can make choices but also have the tools to become aligned \nwith the infrastructure available to run their services. These tools form a chassis, a foun-\ndation, that you can build new services on without much up-front investigation and \nceremony. Let's consider how to build a chassis for your services—one that abstracts \ncommon concerns and architectural choices while at the same time enables teams to \nquickly bootstrap new services.\n7.1\t\nA microservice chassis\nImagine an organization has eight different engineering teams and four engineers on \neach team. Now imagine one engineer on each team is responsible for bootstrapping a \nnew service in Python, Java, or C#. Those languages, like most mainstream languages, \nhave a lot of options in the form of available libraries. From http clients to logging \nlibraries, the choice is plentiful. What would be the odds of two teams selecting the \nsame language ending up with the same combination of libraries? I’d say pretty nar-\nrow! This issue isn’t exclusive to microservice applications; for a monolithic applica-\ntion I worked on, different programmers were using three distinct http client libraries!\nIn figures 7.1 to 7.3, you can see the choices a team may face while choosing compo-\nnents to use in a new project.\nAs you can see in figures 7.1–7.3, the choice isn’t easy! No matter which language \nyou choose, options are plentiful, so the time you take to select components can \nincrease, along with the risk of picking up a less than ideal library. An organization \nmost likely will settle with two or three languages as the widely adopted ones, depend-\ning on the problems they need to solve. As a result, teams using the same language will \ncoexist. Once one team gains some experience with a set of libraries, why wouldn’t you \nuse that experience to the benefit of other teams? You can provide a set of libraries and \ntools already used in production that people bootstrapping new projects could choose \nfrom without the burden of having to dig deeply into each library to weigh the pros \nand cons.\n \n",
      "content_length": 3195,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "\t\n161\nA microservice chassis\n \nFigure 7.1    Search results for object-relational mapping (ORM) libraries for the .NET ecosystem\nTo make the job easier for your teams to create new services, it’s worth your while to \nprovide basic structure and a set of vetted tools for each of the languages your orga-\nnization uses to build and operate services. You also should make sure that structure \nabides with your standards regarding observability and the abstraction of infrastruc-\nture-related code and it reflects your architectural choices regarding communication \nbetween services. An example of this, if the organization favors asynchronous commu-\nnication between services, would be providing the needed libraries for using an event \nbus infrastructure that’s already in place.\nNot only would you be able to soft-enforce some practices, you also could make it eas-\nier to spawn new services quickly and allow fast prototyping. After all, it wouldn't make \nsense to take longer to bootstrap a service than to write the business logic that powers it.\n \n",
      "content_length": 1052,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "162\nChapter 7   Building a reusable microservice framework \nFigure 7.2    Search for Advanced Message Queuing Protocol (AMQP) libraries for the Java ecosystem\nFigure 7.3. Search for circuit breaker libraries for Python\n \n",
      "content_length": 221,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "\t\n163\nWhat’s the purpose of a microservice chassis?\nThe chassis structure allows teams to select a tech stack (language + libraries) and \nquickly set up a service. You might ask yourself: how hard is it to bootstrap a service \nwithout this so-called chassis? It can be easy, if you don’t have concerns like\n¡ Enabling deployments in the container scheduler from day one (CI/CD)\n¡ Setting up log aggregation\n¡ Collecting metrics\n¡ Having a mechanism for synchronous and asynchronous communication\n¡ Error reporting\nAt SimpleBank, no matter what programming language or tech stack a team chooses, \nservices should be providing all the functionality described in the list above. This type \nof setup isn’t trivial to achieve, and, depending on the stack you chose, it can take more \nthan a day to set up. Also, the combination of libraries two teams would choose for the \nsame purpose could be quite different. You mitigate any issues related to that differ-\nence by providing a microservice chassis, so each team can focus on delivering features \nthat SimpleBank customers will be using.\n7.2\t\nWhat’s the purpose of a microservice chassis?\nThe purpose of a microservice chassis is to allow you to make services easier to create \nwhile ensuring you have a set of standards that all services abide by, no matter which \nteam owns a service. Let’s look into some of the advantages of having a microservices \nchassis in place:\n¡ Making it easier to onboard team members\n¡ Getting a good understanding of the code structure and concerns regarding the \ntech stack that an engineering team uses\n¡ Limiting the scope of experimentation for production systems as the team builds \ncommon knowledge, even if not always in the same tech stack\n¡ Helping to adhere to best practices\nHaving a predictable code structure and commonly used libraries will make it easier \nfor team members to quickly understand a service’s implementation. They’ll only \nneed to bother with the business logic implementation, because any other code will be \npretty much common throughout all services. For example, common code will include \ncode to deal with or configure\n¡ Logging\n¡ Configuration fetching\n¡ Metrics collection\n¡ Data store setup\n¡ Health checks\n¡ Service registry and discovery\n¡ The chosen transport-related boilerplate (AMQP, HTTP)\n \n",
      "content_length": 2313,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "164\nChapter 7   Building a reusable microservice framework \nIf common code has already taken care of those concerns when someone is creating a \nnew service, the need to write boilerplate is reduced or eliminated, and developers will \nless likely have to reinvent the wheel. Good practices within the organization will also \nbe easier to enforce.\nFrom a knowledge sharing perspective, having a microservice chassis will also enable \neasy code reviews by members of different teams. If they’re using the same chassis, \nthey’ll be familiar with the code structure and how things are done. This will increase \nvisibility and allow you to gather opinions from engineers from other teams. It’s always \ndesirable to have a different view on the problems a specific team is working on solving.\n7.2.1\t\nReduced risk\nBy providing a microservice chassis, you reduce the risk you face, because you’ll have \nless of a chance of picking a combination of language and libraries that won’t work \nfor a particular need. Imagine a service you’re creating needs to fully communicate \nasynchronously with other services using an already existing event bus. If your chassis \nalready covers that use case, you’re not likely to end up with a setup that you need to \ntweak and eventually won’t work well. You can cover that asynchronous communica-\ntion use case as well as the synchronous one so you don’t need to expend further effort \nto find a working solution.\nThe chassis can be constantly evolving to incorporate the findings of different teams, \nallowing you to be always up to date with the organization’s practices and experience \ndealing with multiple use cases. All in all, there will be less chance for a team to face a \nchallenge that other teams haven’t solved before. And in case no one has solved that \ntype of challenge yet, only one team needs to solve it; then you can incorporate the solu-\ntion into the chassis, reducing the risks other teams have to take in the future.\nHaving a microservice chassis that already selects a set of libraries for use will limit the \nmanagement of dependencies an engineering team will have to deal with. Referring to \nfigures 7.1 to 7.3, if you have available one ORM, one AMQP, and one circuit breaker \nlibrary, those will eventually be well known across multiple teams, and if someone finds \na vulnerability in any of those libraries, you’ll be able to update them with ease.\n7.2.2\t\nFaster bootstrapping\nIt makes little sense to spend one or two days bootstrapping a service when it could \ntake far less time to implement the business logic. Also, wiring the needed components \nthat form a service is a repetitive task that can be error prone. Why make people have \nto go and set up components all over again every time they create a new service? Using, \nmaintaining, and updating a microservice chassis will lead to a setup that’s sound, \ntested, and reusable. This will allow for faster service bootstrapping. Then you could \nuse the extra time you gained by not having to write boilerplate code to develop, test, \nand deploy your features.\n \n",
      "content_length": 3075,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "\t\n165\nDesigning a chassis\nHaving a sound foundation that teams use widely and know well allows you to exper-\niment a lot more without worrying too much about the initial effort. If you can quickly \nturn a concept into a deployable service, you can easily validate it and decide to proceed \nwith it or abandon it altogether. The key notion here is to be fast and to have it as easy \nas possible to create new functionality. Having a chassis in place also can significantly \nlower the entry barrier for new team members, because it’ll be quicker for them to \njump into any project once they learn the structure that’s common to all services in \neach language.\n7.3\t\nDesigning a chassis\nAt SimpleBank, the team responsible for implementing the purchasing and selling of \nstocks decided to create a chassis for the wider engineering team to use—they had \nfaced a couple of challenges and want to share their experiences. We described a fea-\nture for selling stocks in chapter 2, figure 2.7. Let’s look at a flow diagram to better \nunderstand it (figure 7.4).\nTo sell stocks, a user issues a request via the web or a mobile application. An API gate-\nway will pick up the request and will act as the interface between the user-facing applica-\ntion and all internal services that’ll collaborate to provide the functionality.\nUser\nGateway\nPlace sell order\n(http)\nPlace sell order\n(rpc)\nPlaceOrder\n(http external)\nOrderCreated\n(trigger event)\n1\n2\n3\n4\n7\n7\n6\n5\n5\nConsume OrderPlaced event\nConsume OrderCreated event\nConsume OrderPlaced event\nOrderPlaced\n(trigger event)\nRespond to sell order\n(http)\nRespond to sell order\n(rpc)\nRequest reservation\n(rpc)\nRecord order details\nUpdate order to “placed”\nRecord reservation\nCharge fee\nOrders service\nEvent queue\nAccounts service\nMarket service\nFees service\nStock exchange\nFigure 7.4    The flow for selling stocks involves both synchronous and asynchronous communication \nbetween the intervening services.\n \n",
      "content_length": 1940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "166\nChapter 7   Building a reusable microservice framework \nGiven that it can take a while to place the order to the stock exchange, most operations \nwill be asynchronous, and you’ll return a message to the client indicating their request \nwill be processed as soon as possible. Let’s look into the interactions between services \nand the type of communication:\n1\t The gateway passes the user request to the orders service.\n2\t The orders service sends an OrderCreated event to the event queue.\n3\t The orders service requests the reservation of a stock position to the account \ntransaction service.\n4\t The orders service replies to the initial call from the gateway, then the gateway \ninforms the user that the order is being processed.\n5\t The market service consumes the OrderCreated event and places the order to \nthe stock exchange.\n6\t The market service emits an OrderPlaced event to the event queue.\n7\t Both the fees service and the orders service consume the OrderPlaced event; \nthey then charge the fees for the operation and update the status of the order to \n“placed,” respectively.\nFor this feature, you have four internal services collaborating, interactions with an \nexternal entity (stock exchange), and communication that’s a mix of synchronous and \nasynchronous. The use of the event queue allows other systems to react to changes; \nfor instance, a service responsible for emailing or real-time notifications to clients can \neasily consume the OrderPlaced event, allowing it to send notifications of the placed \norder.\nGiven that the team owning this feature was comfortable with using Python, they cre-\nated the initial prototype using the nameko framework (https://github.com/nameko/\nnameko). This framework offers, out of the box, a few things:\n¡ AMQP RPC and events (pub-sub)\n¡ HTTP GET, POST, and websockets\n¡ CLI for easy and rapid development\n¡ Utilities for unit and integration testing\nBut a few things were missing, like circuit breakers, error reporting, feature flags, and \nemitting metrics, so the team decided to create a code repository with libraries to take \ncare of those concerns. They also created a Dockerfile and Docker compose file to \nallow building and running the feature with minimum effort and to offer a base for \nother teams to use when developing in Python. The code for the initial Python chassis \n(http://mng.bz/s4B2) and for the described feature (http://mng.bz/D19l) is avail-\nable at the book code repository.\nWe’ll now look with more detail at how the built chassis deals with service discovery, \nobservability, transport, and balancing and limiting.\n \n",
      "content_length": 2603,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "\t\n167\nDesigning a chassis\n7.3.1\t\nService discovery\nService discovery for the Python chassis that emerged from implementing the feature \nwe previously described is quite simple. The communication between the services \ninvolved occurs either synchronously via RPC calls or asynchronously by publishing \nevents. SimpleBank uses RabbitMQ (www.rabbitmq.com) as the message broker, so this \nindirectly provides a way of registering services for both the asynchronous and synchro-\nnous use case. RabbitMQ allows the use of synchronous request/response communica-\ntion implementing RPC over queues, and it’ll also load balance the consumers using \na round-robin algorithm (https://en.wikipedia.org/wiki/Round-robin_scheduling) by \ndefault. This allows you to use the messaging infrastructure to register services as well \nas to automatically distribute load between multiple instances of the same service. Fig-\nure 7.5 shows the RPC exchange your different services connect to.\nFigure 7.5    Services communicating via RPC register in an exchange. Multiple instances for a given \nservice use the same routing key, and RabbitMQ will route the incoming requests between those \ninstances.\n \n",
      "content_length": 1180,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "168\nChapter 7   Building a reusable microservice framework \nAll running services register themselves in this exchange. This will allow for them to \ncommunicate seamlessly without the need for each one to know explicitly where any \nservice is located. This is also the case for RPC communication over the AMQP proto-\ncol, which allows you to have the same request/response behavior you’d get by using \nHTTP.\nLet’s take a look on how easy it is to have this feature available to you by using the \ncapacities that the chassis provides, in this case by using the nameko framework, as \nshown in the following listing.\nListing 7.1    microservices-in-action/chapter-7/chassis/rpc_demo.py\nfrom nameko.rpc import rpc, RpcProxy\nclass RpcResponderDemoService:\n    name = \"rpc_responder_demo_service\" \n    @rpc \n    def hello(self, name):\n        return \"Hello, {}!\".format(name)\nclass RpcCallerDemoService:\n    name = \"rpc_caller_demo_service”\n    remote = RpcProxy(\"rpc_responder_demo_service\") \n    @rpc\n    def remote_hello(self, value=\"John Doe\"):\n        res = u\"{}\".format(value)\n        return self.remote.hello(res)\nIn this example, we’ve defined two classes, a responder and a caller. In each class, we \nalso defined a name variable that holds the identifier for the service. Use of the @rpc \nannotation will decorate the function. This decoration will allow you to transform what \nseems an ordinary function into something that’ll make use of the underlying AMQP \ninfrastructure (that RabbitMQ offers) to invoke a method in a service running else-\nwhere. Calling the remote_hello method from the RpcCallerDemoService class will \nresult in invoking the hello function in the RpcResponderDemoService, because that \nservice is registered as remote via a RpcProxy that the framework provides.\nOnce you run this example code, RabbitMQ will display something like figure 7.6.\nIn Figure  7.6, you can observe that once you boot the services that rpc_demo.py \ndefines, each one registers in a queue scoped to the service name: rpc-rpc_caller_\ndemo_service and rpc-rpc_responder_demo_service. Two other queues—rpc.\nreply-rpc_caller_demo_service* and rpc.reply-standalone_rpc_proxy*—also \nappear, and they’ll relay back the responses to the caller service. This is a way of imple-\nmenting blocking synchronous communication in RabbitMQ (http://mng.bz/4blSh).\nAssigns the service name a variable—This \nis the name that a particular service \nregisters to allow others to call it.\nAllows nameko to set up the RabbitMQ queues \nnecessary to perform a request/response type of \ncall—The rpc call will behave synchronously.\nCreates an RPC Proxy for service \nthat’ll be invoked via RPC—You pass \nthe name of the remote service.\nCalls the remote service via the RpcProxy—This will execute  \nthe hello function on the RpcResponderDemoService class.\n \n",
      "content_length": 2831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "\t\n169\nDesigning a chassis\nFigure 7.6    Caller and responder demo services registered in RabbitMQ queues\nYour chassis makes it super easy to access this functionality so you can use the same \ninfrastructure for both synchronous and asynchronous communication between ser-\nvices. This setup brings you huge speed gains while prototyping solutions, because the \nteam can spend its time developing new features instead of having to build all the \nfunctionality from scratch. If you opt for an orchestrated behavior, with blocking calls \nbetween services, a choreographed behavior where all communication is asynchro-\nnous, or a mix between the two, you can use the same infrastructure and library.\nThe following listing shows an example on how to use full asynchronous communi-\ncation between services by using the functionality of the chassis.\nListing 7.2    microservices-in-action/chapter-7/chassis/events_demo.py\nfrom nameko.events import EventDispatcher, event_handler\nfrom nameko.rpc import rpc\nfrom nameko.timer import timer\nclass EventPublisherService:\n    name = \"publisher_service\" \n    dispatch = EventDispatcher()\n    @rpc\n    def publish(self, event_type, payload):\n        self.dispatch(event_type, payload)\nclass AnEventListenerService:\nRegisters the service name, which allows \nyou to refer to it on other services\nAllows this service to create events \nthat’ll be routed to a queue in RabbitMQ\n \n",
      "content_length": 1409,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "170\nChapter 7   Building a reusable microservice framework \n    name = \"an_event_listener_service\" \n    @event_handler(\"publisher_service\", \"an_event\") \n    def consume_an_event(self, payload):\n        print(\"service {} received:\".format(self.name), payload)\nclass AnotherEventListenerService:\n    name = \"another_event_listener_service\"\n    @event_handler(\"publisher_service\", \"another_event\")\n    def consume_another_event(self, payload):\n        print(\"service {} received:\".format(self.name), payload)\nclass ListenBothEventsService:\n    name = \"listen_both_events_service\" \n    @event_handler(\"publisher_service\", \"an_event\") \n    def consume_an_event(self, payload):\n        print(\"service {} received:\".format(self.name), payload)\n    @event_handler(\"publisher_service\", \"another_event\") \n    def consume_another_event(self, payload):\n        print(\"service {} received:\".format(self.name), payload)\nAs with the previous code example, each service a Python class implements declares a name \nvariable that the framework will use to set up the underlying queues that allow communi-\ncation. When running the services that each class in this file defines, RabbitMQ will create \nfour queues, one for each service. As you can see in figure 7.7, the publisher service reg-\nisters an RPC queue, without reply queue setup, contrary to the previous example that \nfigure 7.6 illustrated. The other listener services register a queue per consumed event.\nFigure 7.7    The queues that RabbitMQ creates when you run the services defined in events_demo.py\nRegisters the service name, which allows \nyou to refer to it on other services\nBy using this annotation, ListenBothEventsService will execute \nthe function when the publisher service issues an event. The \nfirst argument of the annotation is the name of the service \nwhose events will be listened to, and the second argument of \nthe annotation is the name of the event.\n \n",
      "content_length": 1918,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "\t\n171\nDesigning a chassis\nThe team chose nameko to be part of the microservice chassis because it makes it easy \nto abstract from the details of implementing and setting up these two types of commu-\nnication over the existing message broker. In section 7.3.3, we’ll also look into another \nadvantage that comes out of the box, because the message broker also takes care of \nload balancing.\n7.3.2\t\nObservability\nTo operate and maintain services, you need to be aware of what’s going on in produc-\ntion at all times. As a result, you’ll want the services to emit metrics to reflect the way \nthey’re operating, report errors, and aggregate logs in a usable format. In part 4 of the \nbook, we’ll focus on all these topics in more detail. But for now, let’s keep in mind that \nservices should address these concerns from day one. Operating and maintaining ser-\nvices is as important as writing them in the first place, and, in most cases, they’ll spend \na lot more time running than being developed.\nYour microservice chassis has the dependencies shown in the following listing.\nListing 7.3    microservices-in-action/chapter-7/chassis/setup.py\n(…)\n    keywords='microservices chassis development',\n    packages=find_packages(exclude=['contrib', 'docs', 'tests']),\n    install_requires=[\n        'nameko>=2.6.0',\n        'statsd>=3.2.1', \n        'nameko-sentry>=0.0.5', \n        'logstash_formatter>=0.5.16', \n        'circuitbreaker>=1.0.1',\n        'gutter>=0.5.0',\n        'request-id>=0.2.1',\n    ],\n(…)\nFrom the seven declared dependencies, you use three of them for observability pur-\nposes. These libraries will allow you to collect metrics, report errors, and gather some \ncontextual information around them and to adapt your logging to the format you use \nin all services deployed at SimpleBank.\nLibrary to emit metrics in StatsD format\nLibrary to Integrate with Sentry error reporting\nLibrary to format the logs in logstash format\n \n",
      "content_length": 1939,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "172\nChapter 7   Building a reusable microservice framework \nMetrics\nLet’s start with metrics collection and the use of StatsD.1 Etsy originally developed \nStatsD as a way to aggregate application metrics. It quickly became so popular that \nit’s now the de facto protocol to collect application metrics with clients in multiple pro-\ngramming languages. To be able to use StatsD, you need to instrument your code to \ncapture all metrics you find relevant. Then a client library, in your case statsd for \nPython, will collect those metrics and send them to an agent that listens to UDP traffic \nfrom client libraries, aggregates the data, and periodically sends it to a monitoring \nsystem. Both commercial and open source solutions are available for the monitoring \nsystems.\nIn the code repository, you’ll be able to find a simple agent that’ll be running in its \nown Docker container to simulate metrics collection. It’s a trivial ruby script that listens \nto port 8125 over UDP and outputs to the console, as follows.\nListing 7.4    microservices-in-action/chapter-7/feature/statsd-agent/statsd-agent.rb\n#!/usr/bin/env ruby\n#\n# This script was originally found  in a post by Lee Hambley\n# (http://lee.hambley.name)\n#\nrequire 'socket'\nrequire 'term/ansicolor'\ninclude Term::ANSIColor\n$stdout.sync = true\nc = Term::ANSIColor\ns = UDPSocket.new\ns.bind(\"0.0.0.0\", 8125)\nwhile blob = s.recvfrom(1024)\n  metric, value = blob.first.split(':')\n  puts \"StatsD Metric: #{c.blue(metric)} #{c.green(value)}\"\nend\nThis simple script allows you to simulate metrics collection while developing your \nservices. Figure 7.8 shows metrics collection for services running when placing a sell \norder, the feature we use as an example for this chapter.\nUsing an annotation in the code for each service, you enable them to send metrics \nfor some operations. Even though this is a simple example, because they’re only emit-\nting timing metrics, it serves the purpose of showing how you can instrument your code \nto collect data you find relevant. Let’s look into one of the services to see how this is \ndone. Consider the listing 7.5.\n1\t See Ian Malpass, “Measure Anything, Measure Everything,” Code as Craft, Etsy, http://mng \n.bz/9Tqo.\n \n",
      "content_length": 2213,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "\t\n173\nDesigning a chassis\nFigure 7.8    StatsD agent collecting metrics that services collaborating in a place sell order operation \nhave emitted\nListing 7.5    microservices-in-action/chapter-7/feature/fees/app.py\nimport json\nimport datetime\nfrom nameko.events import EventDispatcher, event_handler\nfrom statsd import StatsClient \nclass FeesService:\n    name = \"fees_service\"\n    statsd = StatsClient('statsd-agent', 8125,\n                         prefix='simplebank-demo.fees') \n    @event_handler(\"market_service\", \"order_placed\")\n    @statsd.timer('charge_fee') \n    def charge_fee(self, payload):\n        print(\"[{}] {} received order_placed event ... charging fee\".format(\n            payload, self.name))\nImports the StatsD client so \nyou can use it in the module\nConfigures the StatsD client \nby passing the host, the port, \nand the prefix you’ll use for \nall the emitted merics\nUsing this annotation enables you to collect the time it takes \nfor the 'charge_fee' function to run. The StatsD library uses the \nvalue passed as an argument for the annotation as the metric \nname. In this case, the charge_fee function will emit the metric \nnamed 'simplebank-demo.fees.charge_fee'; the prefix you \nconfigured before will be prepended to the metric name passed \nto the annotation.\n \n",
      "content_length": 1287,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "174\nChapter 7   Building a reusable microservice framework \nTo collect metrics using the StatsD client library, you need to initialize the client by \npassing the hostname, in this case statsd-agent, the port, and an optional prefix for \nmetrics collected in this service scope. If you annotate the charge_fee method with \n@statsd.timer('charge_fee'), the library will wrap the execution of that method \nin a timer and will collect the value from the timer and send it to the agent. You can \ncollect these metrics and feed them to monitoring systems that’ll allow you to observe \nyour system behavior and set up alerts or even autoscale your services.\nFor example, imagine the fees service becomes too busy, and the execution time \nthat StatsD reports increases over a threshold you set. You can automatically be alerted \nabout that and immediately investigate to understand if the service is throwing errors or \nif you need to increase its capacity by adding more instances. Figure 7.9 shows an exam-\nple of a dashboard displaying metrics that StatsD collected.\nError reporting\nMetrics allow you to observe how the system is behaving on an ongoing basis, but, \nunfortunately, they aren’t the only thing you need to care about. Sometimes errors \nhappen, and you need to be alerted about them and, if possible, gather some infor-\nmation about the context in which the error occurred. For example, you might get a \nstack trace so you can diagnose and try to replicate and solve the error. Several services \nprovide alerting and aggregation of errors. It’s easy to integrate error reporting in your \nservices, as shown in the following listing.\nListing 7.6    microservices-in-action/chapter-7/chassis/http_demo.py\nimport json\nfrom nameko.web.handlers import http\nfrom werkzeug.wrappers import Response\nfrom nameko_sentry import SentryReporter \nclass HttpDemoService:\n    name = \"http_demo_service\"\n    sentry = SentryReporter() \n    @http(\"GET\", \"/broken\")\n    def broken(self, request):\n        raise ConnectionRefusedError() \n    @http('GET', '/books/<string:uuid>')\n    def demo_get(self, request, uuid):\n        data = {'id': uuid, 'title': 'The unbearable lightness of being',\n                'author': 'Milan Kundera'}\n        return Response(json.dumps({'book': data}),\n                        mimetype='application/json')\n    @http('POST', '/books')\n    def demo_post(self, request):\n        return Response(json.dumps({'book': request.data.decode()}),\n                        mimetype='application/json')\nImports the error reporting module\nInitializes the error reporting service\nRaises an exception so you can \ntest the error reporting service\n \n",
      "content_length": 2653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "\t\n175\nDesigning a chassis\nFigure 7.9    Example of a dashboard displaying metrics that StatsD collected from an application\nSetting up error reporting in the chassis you assembled is simple. You initialize the \nerror reporter, and it’ll take care of capturing any exceptions and sending them over \nto the error reporting service backend. It’s common for the error reporter to send \nalong some context with the errors, like a stack trace. Figure 7.10 shows the dashboard \nwith the error you get if you access the /broken endpoint in the demo service.\nFigure 7.10    Dashboard for an error reporting service (Sentry) after accessing the /broken endpoint\n \n",
      "content_length": 654,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "176\nChapter 7   Building a reusable microservice framework \nLogging\nYour services output information either to log files or to the standard output. These \nfiles can record a given interaction, such as the result and timing of an http call or any \nother information developers find useful to record. Having multiple services running \nthis recording means you potentially have multiple services logging information across \nthe organization. In a microservice architecture, where interactions happen between \nmultiple services, you need to make sure you can trace those interactions and have \naccess to them in a consistent way.\nLogging is a concern for all teams and plays an important role in any organization. \nThis is the case either for compliance reasons, when you may need to keep track of \nspecific operations, or for allowing you to understand the flow of execution between \ndifferent systems. The importance of logging is a sound reason for making sure that \nteams, no matter what language they’re using to develop their services, keep logs in a \nconsistent way and, preferably, aggregate them in a common place.\nAt SimpleBank, the log aggregation system allows complex searches in logs, so you \nagree to send logs to the same place and in the same format. You use logstash format for \nlogging, so the Python chassis includes a library to emit logs in logstash format.\nLogstash is an open source data processing pipeline that allows ingestion of data \nfrom multiple sources. The logstash format became quite popular and is widely used \nbecause it’s a json message with some default fields, such as the ones you can find in the \nfollowing listing.\nListing 7.7    Logstash json formatted message\n{\n  \"message\"    => \"hello world\",\n  \"@version\"   => \"1\",\n  \"@timestamp\" => \"2017-08-01T23:03:14.111Z\",\n  \"type\"       => \"stdin\",\n  \"host\"       => \"hello.local\"\n}\nFigure 7.11 shows the log output that the gateway service generates when receiving a \nplace sell order request from a client. In such cases, it generates two messages. They \nboth contain a wealth of information, like the filename, module, and line executing \ncode, as well as the time it took for the operation to complete. The only information \nyou passed explicitly to the logger was what appears in the message fields. The library \nyou’re using inserts all the other information.\nBy sending this information to a log aggregation tool, you can correlate data in many \ninteresting ways. In this case, here are some example queries:\n¡ Group by module and function name\n¡ Select all entries for operations that took longer than x miliseconds\n¡ Group by host\n \n",
      "content_length": 2625,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "\t\n177\nDesigning a chassis\nFigure 7.11    Logstash formatted log messages that the gateway service generated\nThe most interesting thing is that the host, type, version, and timestamp fields will \nappear in all the messages that the services using the chassis generate, so you can cor-\nrelate messages from different services.\nIn your Python chassis, the following listing shows the code responsible for generat-\ning the log entries you can see in figure 7.11.\nListing 7.8    Logstash logger configuration in the Python chassis\nimport logging\nfrom logstash_formatter import LogstashFormatterV1\nlogger = logging.getLogger(__name__)\nhandler = logging.StreamHandler()\nformatter = LogstashFormatterV1()\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n(…)\n# to log a message …\nlogger.info(“this is a sample message”)\nThis code is responsible for initializing the logging and adding the handler that’ll for-\nmat the output in the logstash json format.\nBy using the microservice chassis, you create a standard way of accessing the tools to \nachieve the goal of running observable services. By choosing certain libraries, you’re \nable to enforce having all teams use the same underlying infrastructure without forcing \nany team to choose a particular language.\n7.3.3\t\nBalancing and limiting\nWe mentioned in section 7.3.1 on service discovery that the message broker provided \nnot only a way for services to discover each other implicitly but also a load balancing \ncapability.\nWhile benchmarking the place sell order feature, say you realize you have a bottleneck \nin your processing. The market service has to interact with an external actor, the stock \nexchange, and will only do that after a successful response creates the OrderPlaced event \n \n",
      "content_length": 1749,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "178\nChapter 7   Building a reusable microservice framework \nthat both the fees service and the orders service will consume. Requests are accumulat-\ning because the HTTP call to the external service is slower than the rest of the processing \nin the system. For this reason, you decide to increase the number of instances running \nthe market service. You deploy three instances to compensate for the extra time that the \norder placement onto the stock exchange takes. This change is seamless, because once \nyou add the new instances, they’re registered with the rpc-market_service queue in \nRabbitMQ. Figure 7.12 shows the three instances of the service connected.\nAs you can see, three instances are connected to the queue, each of them set to prefetch \n10 messages from the queue as soon as they arrive. Now that you have multiple instances \nconsuming from the same queue, you need to make sure only one of those instances pro-\ncesses each request. Once again, RabbitMQ makes your life easier because it deals with \nload balancing. By default, it’ll use a round-robin algorithm to schedule the delivery of \nmessages between the service instances. This means it’ll deliver the first 10 messages to \ninstance 1, then the next 10 to instance 2, and finally 10 to instance 3. It’ll keep repeating \nthis over and over. This is a naïve approach to scheduling work, because one instance may \ntake longer than another one, but it generally works quite well and is easy to understand.\nFigure 7.12    Multiple instances of the market service registered in the RPC queue\n \n",
      "content_length": 1562,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "\t\n179\nDesigning a chassis\nThe only thing you need to be careful about is checking if the connected instances \nare healthy so they don’t start accumulating messages. You can do so by making use \nof metrics, using StatsD, to monitor the number of messages that each instance is \nprocessing and if they’re accumulating. In your code, you also can implement health \nchecks so that any instance not responding to those health check requests can be \nflagged and restarted. RabbitMQ also will work as a limiting buffer, storing messages \nuntil the service instances can process them. According to the configuration shown in \nfigure 7.12, each instance will receive ten messages to process at a time and will only \nbe assigned new messages after it has finished processing previous ones.\nIt’s worth mentioning that in the particular case of the market service as it interacts \nwith a third-party system, you also implement a circuit breaking mechanism. Let’s look \nat the service code where the call to the stock exchange is implemented, as follows.\nListing 7.9    microservices-in-action/chapter-7/feature/market/app.py\nimport json\nimport requests\n(…)\nfrom statsd import StatsClient\nfrom circuitbreaker import circuit \nclass MarketService:\n    name = \"market_service\"\n    statsd = StatsClient('statsd-agent', 8125,\n                         prefix='simplebank-demo.market')\n    (…)\n    @statsd.timer('place_order_stock_exchange')\n    @circuit(failure_threshold=5, expected_exception=\n➥ConnectionError) \n    def __place_order_exchange(self, request):\n        print(\"[{}] {} placing order to stock exchange\".format(\n            request, self.name))\n        response = requests.get('https://jsonplaceholder.typicode.com/\nposts/1')\n        return json.dumps({'code': response.status_code, 'body': response.\ntext})\nYou make use of the circuit breaker library to configure the number of consecutive fail-\nures to connect that you’ll tolerate. In the example shown, if you have five consecutive \nfailing calls with the ConnectionError exception, you’ll open the circuit, and no call \nwill be made for 30 seconds. After those 30 seconds, you’ll enter the recovery stage, \nallowing one test call. If the call is successful, it’ll close the circuit again, resuming nor-\nmal operation and allowing calls to the external service; otherwise, it’ll prevent calls \nfor another 30 seconds.\nImports the circuit breaker \nfunctionality to use in the module\nAllows you to configure how many exceptions you tolerate before \nopening the circuit and the type of exceptions that’ll count as a failure\n \n",
      "content_length": 2571,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "180\nChapter 7   Building a reusable microservice framework \nNOTE    Because 30 seconds is the default value the circuit breaker library sets \nfor the recovery_timeout parameter, you don’t see it in listing 7.9. If you want \nto adjust this value, you can do so by passing it explicitly.\nYou could use this technique not only for external calls but also for calls between inter-\nnal components, because it will allow you to degrade the service. In the case of the \nmarket service, using this technique would mean messages that services retrieved from \nthe queue wouldn’t be acknowledged and would accumulate in the broker. Once the \nexternal service connectivity was resumed, you’d be able to start processing messages \nfrom the queue. You could complete the call to the stock exchange and create the \nOrderPlaced event that allows both the fees service and the orders service to complete \nthe execution of a place sell order request.\n7.4\t\nExploring the feature implemented using the chassis\nIn the previous section, you saw code examples for the implementation of the place \nsell order feature. Let’s briefly look into the resulting feature prototype that you’d \nimplement using the chassis. Based on the chassis code that you can find in the code \nrepository under chapter7/chassis, say you’ve created five services:\n¡ Gateway\n¡ Orders service\n¡ Market service\n¡ Account transactions service\n¡ Fees service\nFigure 7.13 shows the project structure and a Docker Compose file that allows you to locally \nstart the five components and the StatsD agent we mentioned previously. The Docker \nCompose file will allow booting the services as well as the needed infrastructure compo-\nnents: RabbitMQ, Redis, and the local StatsD agent, which will simulate metrics collection.\nWe won’t go deep on Docker or Docker Compose right now, because we’ll cover it in \nthe upcoming chapters. But if you do have Docker and Docker Compose available, you \ncan boot the services by entering the feature directory and running docker-compose \nup –build. This will build a Docker container for each service and boot everything up.\nFigure 7.14 shows all services running and processing a POST request to the shares/sell \ngateway endpoint.\nEven though the feature makes use of both synchronous and asynchronous commu-\nnication between the different components, the chassis you have in place allows you to \nquickly prototype it and run initial benchmarks using a tool that allows you to simulate \nconcurrent requests, with results such as the following: (Please note that these bench-\nmarks ran locally on a development machine and are merely indicative.)\n$ siege -c20 -t300S -H 'Content-Type: application/json' \n'http://192.168.64.3:5001/shares/sell POST'\n    (benchmark running for 5 minutes …)\nLifting the server siege...\n \n",
      "content_length": 2797,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "\t\n181\nExploring the feature implemented using the chassis\nTransactions:\t \t\n      \t 12663 hits\nAvailability:\t \t\n     \t\n100.00 %\nElapsed time:\t \t\n     \t\n299.78 secs\nData transferred:\t\n       \t0.77 MB\nResponse time:\t\t\n      \t 0.21 secs\nTransaction rate:\t\n      \t \t\n42.24 trans/sec\nThroughput:\t\n\t\n       \t0.00 MB/sec\nConcurrency:\t\n\t\n       \t9.04\nSuccessful transactions:       \t\n12663\nFailed transactions:\t\n     \t\n0\nLongest transaction:\t\n      \t 0.52\nShortest transaction:\t       \t 0.08\nThese numbers look good, but it’s worth mentioning that once the benchmark \nstopped, the market service still needed to consume 3000 messages—almost a quar-\nter of the total requests that the gateway processed. This benchmark allows you to \nidentify the bottleneck happening in the market service that we mentioned in section \n7.3.3. Referring to figure 7.4, you can see that the gateway receives a response from the \norders service, but asynchronous processing still happens after that.\nFigure 7.13    Project structure for the place sell order feature and the Docker Compose file that allows \nbooting the services and the needed infrastructure components\n \n",
      "content_length": 1142,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "182\nChapter 7   Building a reusable microservice framework \nFigure 7.14    Services used in the place sell order running locally\nThe engineering team at SimpleBank certainly will continue to improve the Python \nchassis so it reflects continuous team learnings. For now though, it’s already usable to \nimplement nontrivial functionality.\n7.5\t\nWasn’t heterogeneity one of the promises of \nmicroservices?\nIn the previous sections, we covered building and using a chassis for Python applica-\ntions at SimpleBank. You can apply the principles to any language used within your \norganization though. At SimpleBank, teams also use Java, Ruby, and Elixir for building \nservices. Would you go and build a chassis for each of these languages and stacks? If \nthe language is widely adopted within the organization and different teams bootstrap \nmore than a couple of services, I’d say sure! But it’s not imperative that you create a \nchassis. The only thing to keep in mind is that with or without a chassis, you need to \nmaintain principles like observability.\nOne of the advantages of a microservice architecture is enabling heterogeneity of lan-\nguages, paradigms, and tooling. In the end, it’ll enable teams to choose the right tool for \nthe job. Although in theory the choices are limitless, the fact is, teams will specialize in a \ncouple of technology stacks for their day-to-day development. They’ll naturally develop \na deeper knowledge around one or two different languages and their supporting eco-\nsystems. A supporting ecosystem is also important. Independent teams, such as the ones \nyou need to have in place to successfully run a microservice architecture, will also focus \n \n",
      "content_length": 1680,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "\t\n183\nSummary\non operations and will know about the platforms running their apps. Some examples are \nthe Java virtual machine (JVM) or the Erlang virtual machine (BEAM). Knowing about \nthe infrastructure will help with delivering better and more efficient apps.\nNetflix is a good example because they have a deep knowledge of the JVM. This \nenables them to be a proficient contributor of open source tools, allowing the commu-\nnity to benefit from the same tools they use to run their service. The fact that they have \nso many tools written targeting the JVM will make that ecosystem the first choice for \ntheir engineering teams. In some sense, it feels like: “You’re free to choose whatever \nyou want, as long as it abides with our given set of rules and implements some inter-\nfaces..., or you can use this chassis that takes care of all of that!”\nHaving existing chassis for some of the languages and stacks an organization has \nadopted may help direct teams’ choices toward those languages and stacks. Not only \nwill services be easier and faster to bootstrap, they’ll also become more maintainable \nfrom a risk standpoint. A chassis is a great way to indirectly enforce key concerns and \npractices of an engineering team.\nTIP    DRY (don’t repeat yourself) isn’t mandatory. A chassis shouldn’t be a sort \nof shared library or dependency to be included in services and updated in a \ncentralized way. You should use the chassis to bootstrap new services, but not \nnecessarily to update all running services with a given feature. It’s preferable \nto repeat yourself a little than to bring in shared libraries that increase cou-\npling. Do repeat yourself if that results in keeping systems decoupled and inde-\npendently maintained and managed.\nSummary\n¡ A microservice chassis allows for quick bootstrapping of new services, enabling \ngreater experimentation and reducing risk.\n¡ The use of a chassis allows you to abstract the implementation of certain infra-\nstructure-related code.\n¡ Service discovery, observability, and different communication protocols are con-\ncerns of a microservice chassis, and it should provide them.\n¡ You can quickly prototype a complex feature like the place sell order example, if \nthe proper tooling exists.\n¡ Although the microservice architecture is often associated with the possibility \nof building systems in any language, those systems, when in production, need to \noffer some guarantees and have mechanisms to allow their operation and main-\ntenance to be manageable.\n¡ A microservice chassis is a way to provide those guarantees while allowing fast \nbootstrap and quick development for you to test ideas and, if proven, deploy \nthem to production.\n \n",
      "content_length": 2693,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "Part 3\nDeployment\nA n application is only useful if you can deploy it to your users. This part \nof the book will introduce you to deployment practices for microservices. We’ll \nexplore deployment techniques, such as continuous delivery and packaging, \nand deployment platforms, including Google Cloud Platform and Kubernetes. \nThroughout the next few chapters, you’ll learn how to build a deployment pipe-\nline to take microservice code changes safely and rapidly to production.\n \n",
      "content_length": 481,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "187\nThis chapter covers\n¡ Why it’s crucial to get deployment right in a \nmicroservice application\n¡ The fundamental components of a microservice \nproduction environment\n¡ Deploying a service to a public cloud\n¡ Packaging a service as an immutable artifact\nMature deployment practices are crucial to building reliable and stable microservices. \nUnlike a monolithic application, where you can optimize deployment for a single use \ncase, microservice deployment practices need to scale to multiple services, written in \ndifferent languages, each with their own dependencies. You need to be able to trust \nyour deployment process to push out new features — and new services — without \nharming overall availability or introducing critical defects.\nAs a microservice application evolves at the level of deployable units, the cost of \ndeploying new services must be negligible to enable engineers to rapidly innovate and \ndeliver value to users. The added development speed you gain from microservices will \nbe wasted if you can’t get them to production rapidly and reliably. Automated deploy-\nments are essential to developing microservices at scale.\n8\nDeploying microservices\n \n",
      "content_length": 1173,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "188\nChapter 8  Deploying microservices\nIn this chapter, we’ll explore the components of a microservice production environ-\nment. Following that, we’ll look at some deployment building blocks — such as artifacts \nand rolling updates — and how they apply to microservices. Throughout the chapter, \nwe’ll work with a simple service — market-data — to try out different approaches to \npackaging and deployment using a well-known cloud service, Google Cloud Platform. \nYou can find a starting point for this service in the book’s repository on Github (https://\ngithub.com/morganjbruce/microservices-in-action).\n8.1\t\nWhy is deployment important?\nDeployment is the riskiest moment in the lifecycle of a software system. The closest \nreal-world equivalent would be changing a tire — except the car is still moving at 100 \nmiles an hour. No company is immune to this risk: for example, Google’s site reliabil-\nity team identified that roughly 70% of outages are due to changes in a live system \n(https://landing.google.com/sre/book/chapters/introduction.html).\nMicroservices drastically increase the number of moving parts in a system, which \nincreases the complexity of deployment. You’ll face four challenges when deploying \nmicroservices (figure 8.1):\n¡ Maintaining stability when facing a high volume of releases and component \nchanges\n¡ Avoiding tight coupling between components leading to build- or release-time \ndependencies\n¡ Releasing breaking changes to the API of a service, which may negatively impact \nthat service’s clients\n¡ Retiring services\nWhen you do them well, deployments are based on simplicity and predictability. A con-\nsistent build pipeline produces predictable artifacts, which you can apply atomically to \na production environment.\nX\nService code\nSome changes\nmight cause errors.\nServices are\ninterdependent.\nServices may\nbecome obsolete.\nService code\nProduction\nFigure 8.1    A high-level view of production deployment\n \n",
      "content_length": 1942,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "\t\n189\nA microservice production environment\n8.1.1\t\nStability and availability\nIn an ideal world, deployment is “boring:” not unexciting, but incident-free. We’ve \nseen too many teams — both monolithic and microservice — that experience deploy-\ning software as incredibly stressful. But if working with microservices means you’re \nreleasing more components more frequently, doesn’t that mean you’re introducing \nmore risk and instability into a system?\nManual change management is costly\nTraditional change management methodologies attempt to reduce deployment risk by \nintroducing governance and ceremony. Changes must go through numerous quality \ngates and formal approvals, usually human-driven. Although this is intended to ensure \nthat only working code reaches production, this approach is costly to apply and doesn’t \nscale well to multiple services.\nSmall releases reduce risk and increase predictability\nThe larger a release, the higher the risk of introducing defects. Naturally, microservice \nreleases are smaller because the codebases are smaller. And that’s the trick — by releas-\ning smaller changes more often, you reduce the total impact of any single change. \nRather than stopping everything for a deployment, you can design your services and \ndeployment approaches with the expectation that they’ll face continuous change. \nReducing the surface area of possible change leads to releases that are quicker, easier \nto monitor, and less disruptive to the smooth functioning of an application.\nAutomation drives deployment pace and consistency\nEven if your releases are smaller, you still need to make sure your change sets are as \nfree from defects as possible. You can achieve this by automating the process of com-\nmit validation — unit tests, integration tests, linting, and so on — and the process of \nrollout — applying those changes in the production environment. This helps you to \nbuild systematic confidence in the code changes you’re making and apply consistent \npractices across multiple services.\nTIP    Building for anti-fragility, or resilience during failure, is also an important \nelement of overall application stability — don’t forget to read chapter 6!\n8.2\t\nA microservice production environment\nDeployment is a combination of process and architecture:\n¡ The process of taking code, making it work, and keeping it working\n¡ The architecture of the environment in which the software is operated\nProduction environments for running microservices vary widely, as do monolith pro-\nduction environments. What’s appropriate for your application may depend on your \norganization’s existing infrastructure, technical capabilities, and attitude toward risk, \nas well as regulatory requirements.\n \n",
      "content_length": 2721,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "190\nChapter 8  Deploying microservices\n8.2.1\t\nFeatures of a microservice production environment\nThe production environment for a microservice application needs to provide several \ncapabilities to support the smooth operation of multiple services. Figure 8.2 gives a \nhigh-level view of the capabilities of the production environment.\nA microservice production environment has six fundamental capabilities:\n1\t A deployment target, or runtime platform, where services are run, such as virtual \nmachines (Ideally, engineers can use an API to configure, deploy, and update \nservice configuration. You also could call this API the control pane, as shown in \nthe figure.)\n2\t Runtime management, such as autohealing and autoscaling, that allows the ser-\nvice environment to respond dynamically to failure or changes in load without \nhuman intervention (For example, if a service instance fails, it should automati-\ncally be replaced.)\n3\t Logging and monitoring to observe service operation and provide insight for engi-\nneers into how services are behaving\n4\t Support for secure operation, such as network controls, secret management, and \napplication hardening\n5\t Load balancers, DNS, and other routing components to route requests from users \nand between microservices\n6\t A deployment pipeline that delivers services from code, safely into operational usage \nin the production environment\nThese components are part of the platform layer of the microservice architecture stack.\nControl pane\nRuntime\nmanagement\nManages\nNetwork and routing\nConnects\nObservability\nObserves\nDeployment pipeline\nMonitors\nEngineers\nWrites\nCode\nProduction\nFigure 8.2    A microservice production environment\n \n",
      "content_length": 1680,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "\t\n191\nDeploying a service, the quick way\n8.2.2\t\nAutomation and speed\nAlong with the six fundamental features, two factors are key in assessing the suitability \nof a deployment platform for a microservice application:\n¡ Automation  — The bulk of infrastructural management and configuration, such \nas spinning up a new host, should be highly amenable to automation, ideally by \nthe team developing services themselves.\n¡ Speed  — If a significant cost is associated with every new deploy — whether obtain-\ning infrastructure resources or setting up a new deployment — then a microservice \napproach will be significantly hampered.\nAlthough you may not always have the luxury of choosing your deployment environ-\nment, it’s important to appreciate how different platforms might affect these char-\nacteristics and how you develop your microservice application. I once worked for a \ncompany that took six weeks to provision each new server. Suffice it to say that taking \nnew services into production was an exhausting endeavor!\nIt’s not coincidental that the popularity of microservice architecture coincides with \nthe wider adoption of DevOps practices, such as infrastructure as code, and the increasing \nuse of cloud providers to run applications. These practices enable rapid iteration and \ndeployment of services, which in turn makes a microservice architecture a scalable and \nfeasible approach.\nWhen possible you should aim to use a public infrastructure as a service (IaaS) cloud, \nsuch as Google Cloud Platform (GCP), AWS, or Microsoft Azure, for deploying any \nnontrivial microservice application. These cloud services offer a wide range of features \nand tools that ease the development of a robust microservice platform at a lower level \nof abstraction than a higher level deployment solution (such as Heroku). As such, they \noffer more flexibility. In the next section, we’ll show you how to use GCP to deploy, \naccess, and scale a microservice.\n8.3\t\nDeploying a service, the quick way\nIt’s time to get your hands dirty and deploy a service. You need to take your code, get \nit running on a virtual machine, and make it accessible from the outside world — as \nfigure 8.3 illustrates.\nVirtual machine\nOutside world\nRequests\nService\nDeploy as\nCode\nFigure 8.3    A simple microservice deployment\n \n",
      "content_length": 2303,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "192\nChapter 8  Deploying microservices\nYou’ll use Google Compute Engine (GCE) as a production environment. This is a ser-\nvice on GCP that you can use to run virtual machines. You can sign up for a free trial \nGCP subscription, which will have enough credit for this chapter’s examples. Although \nthe operations you’ll perform are specific to this platform, all major cloud providers, \nsuch as AWS and Azure, provide similar abstractions.\nWARNING    This example isn’t a robust production deployment solution!\nTo interact with GCE, you’ll use the gcloud command-line tool. This tool interacts with \nthe GCE API to perform operations on your cloud account. You can find install instruc-\ntions in the GCP documentation (https://cloud.google.com/sdk/docs/quickstarts). It’s \nnot the only option — you could use third-party tools like Ansible or Terraform instead.\nAssuming you’ve followed the install instructions and logged in with gcloud init, \nyou can create a new project:\ngcloud projects create <project-id> --set-as\n➥-default --enable-cloud-apis               \nThis project will contain the resources that’ll run your service.\nTIP    Don’t forget to tear down your project when you’re done. Running gcloud \nprojects delete <project-id> will do the trick.\n8.3.1\t\nService startup\nTo run your service, you’ll use a startup script, which will be executed at startup time \nwhen Google Cloud provisions your machine. We’ve written this for you already — you \ncan find it at chapter-8/market-data/startup-script.sh.\nTake your time to read through the script, which performs four key tasks:\n¡ Installs binary dependencies required to run a Python application\n¡ Downloads your service code from Github\n¡ Installs that code’s dependencies, such as the flask library\n¡ Configures a supervisor to run the Python service using the Gunicorn web server\nNow, let’s try it out.\n8.3.2\t\nProvisioning a virtual machine\nYou can provision a virtual machine from the command line. Change to the chapter-8/\nmarket-data directory and run the following command:\ngcloud compute instances create market-data-service \\ \n  --image-family=debian-9 \\ \n  --image-project=debian-cloud \\ \nReplace <project-id> with the name of your choice.\nThe name of your machine\nThe base image you’ll use for the machine\n \n",
      "content_length": 2277,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "\t\n193\nDeploying a service, the quick way\n  --machine-type=g1-small \\ \n  --scopes userinfo-email,cloud-platform \\\n  --metadata-from-file startup-script=startup\n➥-script.sh \\ \n  --tags api-service \\ \n  --zone=europe-west1-b \nThis will create a machine and return the machine’s external IP address — something \nlike figure 8.4.\nThis approach to startup does take a while. If you want to watch the progress of the \nstartup process, you can tail the output of the virtual machine’s serial port:\ngcloud compute instances tail-serial-port-output market-data-service\nOnce the startup process has completed, you should see a message in the log, similar \nto this example:\nMar 16 12:17:14 market-data-service-1 systemd[1]: Startup finished in\n➥ 1.880s (kernel) + 1min 52.486s (userspace) = 1min 54.367s.\nGreat! You’ve got a running service — although you can’t call it yet. You’ll need to open \nthe firewall to make an external call to this service. Running the following command \nwill open up public access to port 8080 for all services with the tag api-service:\ngcloud compute firewall-rules create default-allow-http-8080 \\\n  --allow tcp:8080 \\ \n  --source-ranges 0.0.0.0/0 \\ \n  --target-tags api-service \\ \n  --description \"Allow port 8080 access to api-service\"\nYou can test your service by curling the external IP of the virtual machine. The external \nIP was returned when you created the instance (figure 8.4). If you didn’t note it, you can \nretrieve all instances by running gcloud compute instances list. Here’s the curl:\ncurl -R http://<EXTERNAL-IP>:8080/ping \nIf all is going well, the response you get will be the name of the virtual machine —  \nmarket-data-service.\nFigure 8.4    Information about a newly created virtual machine\nThe size of the machine to provision\nStarts up using your startup script\nIdentifies this machine’s workload\nThe compute zone — or data center — where this service should start\nAllows tcp queries to port 8080\nFrom any IP address\nTo machines with the api-service tag\nReplace EXTERNAL-IP with the \nIP address of your service.\n \n",
      "content_length": 2058,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "194\nChapter 8  Deploying microservices\n8.3.3\t\nRun multiple instances of your service\nIt’s unlikely you’ll ever run a single instance of a microservice:\n¡ You’ll want to scale horizontally (the X-axis of scalability) by deploying multiple \nclones of the same service, each handling a proportion of requests. Although you \ncould serve more requests with progressively larger machines, it’s ultimately possi-\nble to scale further using more machines.\n¡ It’s important to deploy with redundancy to ensure that failures are isolated. A \nsingle instance of a service won’t maximize resiliency when failures occur.\nFigure 8.5 illustrates a service group. Requests made to the logical service, market-data, \nare load balanced to underlying market-data instances. This is a typical production \nconfiguration for a stateless microservice.\nNOTE    Services that consume from an event queue or message bus are also hor-\nizontally scalable — you distribute message load by running multiple message \nconsumers.\nYou can try this out. On GCE, a group of virtual machines is called an instance group \n(or on AWS, it’s an auto-scaling group). To create a group, you first need to create an \ninstance template:\ngcloud compute instance-templates create market-data-service-template \\\n  --machine-type g1-small \\\n  --image-family debian-9 \\\n  --image-project debian-cloud \\\n  --metadata-from-file startup-script=startup-script.sh \\\n  --tags api-service \\\n  --scopes userinfo-email,cloud-platform\nOutside world\nRequests\nLoad\nbalancer\nRequests\nRequests\nRequests\nVirtual machine\nService\nVirtual machine\nService\nVirtual machine\nService\nDeploy as\nDeploy as\nDeploy as\nCode\nFigure 8.5    A service group and load balancer\n \n",
      "content_length": 1696,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "\t\n195\nDeploying a service, the quick way\nRunning this code will create a template to build multiple market-data-service instances \nlike the one you built earlier. Once the template has been set up, create a group:\ngcloud compute instance-groups managed create market-data-service-group \\\n  --base-instance-name market-data-service \\ \n  --size 3 \\ \n  --template market-data-service-template \\ \n  --region europe-west1 \nThis will spin up three instances of your market-data service. If you open the Google \nCloud console and navigate to Compute Engine > Instance Groups, you should see a \nlist like the one in figure 8.6.\nUsing an instance template to build a group gives you some interesting capabilities \nout of the box: failure zones and self-healing. These two features are crucial to operat-\ning a resilient microservice.\nFailure zones\nFirst, note the zone column in figure 8.6. It lists three distinct values: europe-west1-d, \neurope-west1-c, and europe-west1-b. Each of these zones represents a distinct data \ncenter. If one of those data centers fails, that failure will be isolated and will only affect \n33% of your service capacity.\nSelf-healing\nIf you select one of those instances, you’ll see the option to delete that instance (figure 8.7). \nGive it a shot!\nFigure 8.6    Instances within an instance group\nFigure 8.7    Deleting a VM instance\nThe name prefix of each new instance\nThe number of instances in the group\nThe template to use\nThe region to start these instances in\n \n",
      "content_length": 1490,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "196\nChapter 8  Deploying microservices\nDeleting an instance will cause the instance group to spin up a replacement instance, \nensuring that capacity is maintained. If you look at the operation history of the project \n(Compute Engine > Operations), you’ll see that the delete operation results in GCE \nautomatically recreating the instance (figure 8.8).\nThe instance group will attempt to self-heal in response to any event that results in an \ninstance falling out of service, such as underlying machine failure. You can improve this \nby adding a health check that also targets your application:\ngcloud compute health-checks create http api-health-check \\\n  --port=8080 \\ \n  --request-path=\"/ping\" \ngcloud beta compute instance-groups managed set\n➥-autohealing \\ \n  market-data-service-group \\ \n  --region=europe-west1 \\ \n  --http-health-check=api-health-check \nNow, with the addition of the health check, whenever the application fails to reply to it, \nthe virtual machine will be recycled.\nAdding capacity\nAs your service is now deployed from a template, it’s trivial to add more capacity. You \ncan resize the group from the command line:\ngcloud compute instance-groups managed resize market-data-service-group \\\n--size=6 \\ \n--region=europe-west1\nYou also can add autoscaling rules to automatically add more capacity if metrics you \nobserve from your group, such as average CPU utilization, pass a given threshold.\n8.3.4\t\nAdding a load balancer\nIn all that excitement, you forgot to expose your service group to the wild! In this \ncase, GCE will provide your load balancer, which consists of a few interconnected com-\nponents, as outlined in figure 8.9. The load balancer uses these routing rules, prox-\nies, and maps to forward requests from the outside world to a set of healthy service \ninstances.\nFigure 8.8    Deleting an instance in a group results in the instance being recreated to maintain target \ncapacity (from bottom to top)\nThe health check will make \nHTTP calls to :8080/ping.\nAssociates the check with \nyour existing instance group\nYour new target group size\n \n",
      "content_length": 2077,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "\t\n197\nDeploying a service, the quick way\nInternet\nRequests\nForwarding rule\nForwards traffic\nfrom a single IP\nto the proxy\nProxies requests to\ncorrect backend,\nbased on path\nTarget proxy\nChecks\nDefines backends\nfor given URL paths\nURL map\nBackend service\nBalances requests\nacross healthy\nbackend instances\nHealth check\nRequests\nHealth checks\nService\nVirtual machine\nFigure 8.9    Request lifecycle for GCE load balancing\nNOTE    Managed load balancers are a key feature for all major cloud providers. \nOutside of these environments, you may come across other software load bal-\nancers, such as HAProxy.\nFirst, you’ll want to add a backend service, which is the most important component of \nyour load balancer because it’s responsible for directing traffic optimally to underlying \ninstances:\ngcloud compute instance-groups managed set-named-ports \\\n  market-data-service-group \\\n  --named-ports http:8080 \\\n  --region europe-west1\ngcloud compute backend-services create \\\n➥market-data-service \\ \n  --protocol HTTP \\\n  --health-checks api-health-check \\ \n  --global\nThis code creates two entities: a named part, identifying the port your service exposes, \nand a backend service, which uses the http health check you created earlier to test the \nhealth of your service.\nNext, you need a URL map and a proxy:\ngcloud compute url-maps create api-map \\\n  --default-service market-data-service \ngcloud compute target-http-proxies create api-proxy \\\n  --url-map api-map \nIf you had more than one service, you could use the map to route different subdo-\nmains to different backends. In this case, the URL map will direct all requests, regard-\nless of URL, to the market-data-service you created earlier.\nThe name of your backend service\nThe health check you created earlier\nCreates a URL map for your backend service\nCreates a proxy that uses the new URL map\n \n",
      "content_length": 1851,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "198\nChapter 8  Deploying microservices\nFinally, you need to create a static IP address for your service and a forwarding rule \nthat connects that IP to the HTTP proxy you’ve created:\ngcloud compute addresses create market-data-service-ip \\\n  --ip-version=IPV4 \\\n  --global\nexport IP=`gcloud compute addresses describe market\n➥-data-service-ip --global --format json | jq –raw\n➥-output '.address'` \ngcloud compute forwarding-rules create \\\n➥api-forwarding-rule \\ \n  --address $IP \\ \n  --global \\ \n  --target-http-proxy api-proxy \\ \n  --ports 80 \nprintenv IP \nThis code creates a public IP address and configures requests to that IP to be forwarded \nto your HTTP proxy and on to your backend service. Once run, these rules take sev-\neral minutes to propagate. After a wait, try to curl the service — curl \"http://$IP/\nping?[1-100]\". That will start you with 100 requests. If you see the names of differ-\nent market-data nodes being output to your terminal — terrific — you’ve deployed a \nload-balanced microservice!\nNOTE    In the real world, you’d be unlikely to expose microservices directly to \nthe outside world. You’re only doing it here because it makes testing much \neasier. GCE also supports internal load balancing (https://cloud.google.com/\nload-balancing/docs/internal/) and Cloud Endpoints, a managed API gate-\nway (https://cloud.google.com/endpoints/).\n8.3.5\t\nWhat have you learned?\nIn these examples, you’ve built some of the key elements of a microservice deployment \nprocess:\n¡ Using an instance template established a primitive deployment operation, mak-\ning it simple to add and remove capacity for a given service.\n¡ Combining instance groups, load balancers, and health checks allowed you to \nautoscale and autoheal your microservice deployment.\n¡ Deploying into independent zones helped you build bulwarks to limit the impact \nof failures.\nBut a few things are missing. Your releases weren’t predictable, because you pulled \nyour latest code and compiled it on the machine. A new code commit could cause dif-\nferent service instances to be running inconsistent versions of the code (figure 8.10). \nWithout any explicit versioning or packaging, there would be no easy way to roll your \ncode forward or back.\nRetrieves the IP address\nAdds a forwarding rule that forwards \nfrom the IP address to the HTTP proxy\nOutputs the IP address you created\n \n",
      "content_length": 2364,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "\t\n199\nBuilding service artifacts\nThe latest instances\npick up 6ae881.\nTime\nInstance 2\nInstance 1\nInstance 3\nThese instances start\nwith commit 5a43ab.\n5a43ab\nGit repository\n6ae881\nNew code change\nFigure 8.10    Releasing without packaged versions results in deploying inconsistent code.\nThe process of starting machines was slow because you made pulling dependencies \npart of startup, rather than baking them into your instance template. This arrange-\nment also meant that the dependencies could become inconsistent across different \ninstances.\nLastly, you didn’t automate anything. Not only will a manual process not scale to \nmultiple microservices, but it’s likely to be error prone. Over the next few sections and \nchapters, you can make this much better.\n8.4\t\nBuilding service artifacts\nIn the earlier deployment example, you didn’t package your code for deployment. The \nstartup script that you ran on each node pulled code from a Git repository, installed \nsome dependencies, and started your application. That worked, but it was flawed:\n¡ Starting up the application was slow, as each node performed the same pull and \nbuild steps in parallel.\n¡ There was no guarantee that each node was running the same version of your \nservice.\nThis made your deployment unpredictable — and fragile. To get the benefits you want, \nyou need to build a service artifact. A service artifact is an immutable and deterministic \npackage for your service. If you run the build process again for the same commit, it \nshould result in an equivalent artifact.\nMost technology stacks offer some sort of deployment artifact (for example, JAR files \nin Java, DLLs in .NET, gems in Ruby, and packages in Python). The runtime character-\nistics of these artifacts might differ. For instance, you need to run .NET web services \nusing an IIS server whereas JARs may be self-executable, embedding a server process \nlike Tomcat.\n \n",
      "content_length": 1904,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "200\nChapter 8  Deploying microservices\nService code\nBuild automation\ntool\nPushes to\nProduces\nArtifact\n repository\nDeployment target\nPulls from\nArtifact\nIs ingested by\nFigure 8.11    An artifact repository stores service artifacts that a build automation tool constructs and \nyou can pull later for deployment.\nFigure 8.11 illustrates the artifact construction, storage, and deployment process. Typi-\ncally, a build automation tool (such as Jenkins or CircleCI) builds a service artifact and \npushes it to an artifact repository. An artifact repository might be a dedicated tool — for \nexample, Docker provides a registry for storing images — or a generic file storage tool, \nsuch as Amazon S3.\n8.4.1\t\nWhat’s in an artifact?\nA microservice isn’t only code; it’ll have many constituent parts:\n¡ Your application code, compiled or not (depending on programming language)\n¡ Application libraries\n¡ Binary dependencies (for example, ImageMagick or libssl) that are installed on \nthe operating system\n¡ Supporting processes, such as logging or cron\n¡ External dependencies, such as data stores, load balancers, or other services\nSome of these dependencies, such as application libraries, are explicitly defined. Oth-\ners may be implicit; for example, language-specific package managers are often igno-\nrant of binary dependencies. Figure 8.12 illustrates these different parts.\nAn ideal deployment artifact for a microservice would allow you to package up \na specific version of your compiled code, specifying any binary dependencies, and \nprovide a standard operational abstraction for starting and stopping that service. This \nshould be environment-agnostic: you should be able to run the same artifact locally, \nin test, and in production. By abstracting out differences between languages at run-\ntime, you both reduce cognitive load and provide common abstractions for managing \nthose services.\n \n",
      "content_length": 1895,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "\t\n201\nBuilding service artifacts\nSupporting\nprocesses,\nfor example,\nlogging,\ncron\nApplication\nOperating system\nBinary dependencies, for\nexample, ImageMagick\nApplication libraries\nThird-party\nservices\nDatabases\nand storage\nQueues/\nmessaging\nLoad balancers/\nrequest stack\nService\nFigure 8.12    A service with internal and external dependencies\n8.4.2\t\nImmutability\nWe’ve touched on immutability a few times so far — let’s take a moment to look at why \nit matters. An immutable artifact, encapsulating as many dependencies of your ser-\nvice as feasible, gives you the highest possible confidence that the package you tested \nthroughout your deployment pipeline will be the same as what is deployed in produc-\ntion. Immutability also allows you to treat your service instances as disposable — if a \nservice develops a problem, you can easily replace it with a new instance of the last \nknown good state. On GCE, this autohealing process was automated by the instance \ngroup you created.\nIf a build of the same code can result in a different artifact being created — for exam-\nple, pulling different versions of dependencies — you increase the risk in deployment \nand the fragility of your code because unintentional changes can be included in a \nrelease. Immutability increases the predictability of your system, as it’s easier to reason \nthrough a system’s state and recreate a historic state of your application — crucial for \nrollback.\nImmutability and server management\nImmutability isn’t only for service artifacts: it’s also an important principle for effective \nvirtual server management.\nOne approach to managing the state of hosts is to apply cumulative changes over \ntime — installing patches, upgrading software, changing configuration. This often means \nthat the ideal current state of a server isn’t defined anywhere — there’s no known good \nstate that you can use to build new servers. This approach also encourages applying live \n \n",
      "content_length": 1943,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "202\nChapter 8  Deploying microservices\nfixes to servers which, counterintuitively, increases the risk of failure. These servers suf-\nfer from configuration drift.\nThis approach might make sense if individual hosts are a scarce resource. In a cloud \nenvironment, where individual hosts are cheap to run and replace, immutability is a bet-\nter option. Instead of managing hosts, you should build them using a base template that \nitself is version controlled. Rather than updating older hosts, you replace them with hosts \nyou build from a new version of a base template.\n \n8.4.3\t\nTypes of service artifacts\nMany languages have their own packaging mechanism, and this heterogeneity makes \ndeployment more complex when working with services written in different languages. \nYour deployment tools need to treat differently the interface that each deployment \npackage provides to get it running on a server (or to stop it).\nBetter tooling can reduce these differences, but technology-specific artifacts tend \nto work at too low an abstraction level. They primarily focus on packaging code, rather \nthan the broader nature of application requirements:\n¡ They lack a runtime environment. As you saw earlier, you needed to separately \ninstall other dependencies to run your service.\n¡ They don’t provide any form of resource management or isolation, which makes \nit challenging to adequately run multiple services on a single host.\nLuckily, you’ve got a few options: operating system packages, server images, or contain-\ners (figure 8.13).\nVirtual machine\nOS Package\nService\nBins/libs\nUnit of\npackaging\nGuest OS\nHypervisor\nHost OS\nServer\nOS packages\nVirtual\nmachine\nService\nBins/libs\nUnit of\npackaging\nUnit of\npackaging\nGuest OS\nVirtual\nmachine\nService\nBins/libs\nGuest OS\nHypervisor\nHost OS\nServer\nVirtual machine images\nContainer\nService\nBins/libs\nContainer\nService\nBins/libs\nContainer runtime\nHost OS\nServer\nContainers\nFigure 8.13    The structure of different service artifact types\n(continued)\n \n",
      "content_length": 1991,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "\t\n203\nBuilding service artifacts\nOperating system packages\nYou could use the packaging format of your target operating system, such as apt or \nyum in Linux. This approach standardizes the installation of an artifact, regardless of \ncontents, as you can use standard operating system tools to automate the installation \nprocess. When you start a new host, you can pull the appropriate version of your ser-\nvice package. In addition, packages can specify dependencies on other packages — for \nexample, a Rails application might specify dependencies on common Linux packages, \nsuch as libxml, libmagic, or libssl.\nNOTE    If you’re interested in exploring this approach further, you could try \nto build a deb package using py2deb (github.com/paylogic/py2deb) and the \nexample service.\nThe OS package approach has three weaknesses:\n¡ It adds a different infrastructure requirement: you’ll need to host and manage a \npackage repository.\n¡ These packages are often tightly coupled to a particular operating system, reduc-\ning your flexibility in using different deployment targets.\n¡ The packages aren’t at quite the right level of abstraction, as you still need to exe-\ncute them in a host environment.\nServer images\nIn typical virtualized environments, each server you run is built from an image, or tem-\nplate. The instance template you built in section 8.3 is an example of a server image.\nYou can use this image itself as a deployment artifact. Rather than pulling a package \nonto a generic machine, you could instead bake a new image for each version of your \nservice that you want to deploy. A typical bake process has four steps:\n1\t Select a template image as the basis for the new image.\n2\t Start a VM based on the template image.\n3\t Provision the new VM to the desired state.\n4\t Take a snapshot of the new VM and save it as a new image template.\nYou can try that out using Packer.\nTIP    You’ll need to set up Packer to authenticate with GCE. You can find direc-\ntions for that in the Packer documentation: https://www.packer.io/docs/\nbuilders/googlecompute.html.\nFirst, save the following configuration file as instance-template.json.\nListing 8.1    The instance-template.json file\n{\n  \"variables\": { \n    \"commit\": \"{{env `COMMIT`}}\"\nUser-provided variables\n \n",
      "content_length": 2266,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "204\nChapter 8  Deploying microservices\n  },\n  \"builders\": \n  [\n    {\n      \"type\": \"googlecompute\",\n      \"project_id\": \"market-data-1\",\n      \"source_image_family\": \"debian-9\",\n      \"zone\": \"europe-west1-b\",\n      \"image_name\": \"market-data-service-{{user `commit`}}\",\n      \"image_description\": \"image built for market-data\n➥-service {{user `commit`}}\",\n      \"instance_name\": \"market-data-service-{{uuid}}\",\n      \"machine_type\": \"n1-standard-1\",\n      \"disk_type\": \"pd-ssd\",\n      \"ssh_username\": \"debian\",\n      \"startup_script_file\": \"startup-script.sh\"\n    }\n  ]\n}\nNow, run the packer build command from within the chapter-8/market-data \ndirectory:\npacker build \\\n-var \"commit=`git rev-parse head`\" \\ \ninstance-template.json \nIf you watch the console output, it’ll reflect the four steps I outlined above: using the \nGCE API, Packer will start an instance, run the startup script, and save the instance as a \nnew template image, tagged with the source Git commit. You can use the Git commit to \nexplicitly distinguish different versions of your code.\nNOTE    In this case, you still pulled code directly from Git to your machine \nimage. In complied languages such as Java, compilation into an executable \nshould be a separate step that a build automation tool executes.\nThis approach builds an immutable, predictable, and self-contained artifact. This \nimmutable server pattern, combined with a configuration tool like Packer, allows you \nto store a reproducible base state as code.\nIt has a few limitations:\n¡ Images are locked to one cloud provider, making them nontransferable to other \nproviders as well as to developers who want to recreate the deployed artifact on \ntheir machines.\n¡ Image builds are often slow because of the lengthy time it takes to spin up a \nmachine and take a snapshot.\n¡ It’s not easy for you to use for a multiple service-per-host model.\nDefines how an image will be built\nGets the latest commit hash\nUses the instance template defined in listing 8.1\n \n",
      "content_length": 1991,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "\t\n205\nBuilding service artifacts\nContainers\nInstead of distributing entire machines, containerization tools, such as Docker or rkt, \nprovide a more lightweight approach to encapsulating an application and its depen-\ndencies. You can run multiple containers on one machine, isolated from each other \nbut with lower resource overhead than a virtual machine because they share the kernel \nof one operating system. They avoid the overhead of virtualizing the disk and guest \noperating system of each virtual machine.\nTry a quick example using Docker. (You can find instructions for installing Docker \non the Docker website: https://docs.docker.com/install/.)  You build a Docker image \nfrom a Dockerfile. Add the following file to the chapter-8/market-data folder.\nListing 8.2    Dockerfile for market-data service\nFROM python:3.6 \nADD . /app \nWORKDIR /app\nRUN pip install -r requirements.txt \nCMD [\"gunicorn\", \"-c\", \"config.py\", \"app:app\", \"--bind\"\n➥, \"0.0.0.0:8080\"] \nEXPOSE 8000 \nThen, use the docker command-line tool to build the container:\n$ docker build -t market-data:`git rev-parse head` .\nSending build context to Docker daemon 71.17 kB\nStep 1/3 : FROM python:3.6\n ---> 74145628c331\nStep 2/3 : ADD . /app\n ---> bb3608d5143f\nRemoving intermediate container 74c250f83f8c\nStep 3/3 : WORKDIR /app\n ---> 7a595179cc39\nRemoving intermediate container 19d3bffa4d2a\nSuccessfully built 7a595179cc39\nThis will build a container image and tag it with the name market-data:<commit ID>.\nNow that you’ve built an image for the application, you can run it locally. Try it out:\n$ docker run -d -p 8080:8080 market-data:`git rev-parse head`\nYou’ll see startup logs from gunicorn in your terminal. If you like, try to curl the service \non port 8000. You probably noticed that startup and build time for the container was \nsignificantly faster than the virtual machines on GCE. This is one of the key benefits of \nusing containers.\nStarts from a public base \nimage for Python apps\nAdds your application code to the container\nInstalls the service’s requirements\nSets a startup command for the service\nExposes the service’s \nport from the container\n \n",
      "content_length": 2135,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "206\nChapter 8  Deploying microservices\nIn a few short steps, you can run this container image on GCE. First, you need to \npush the image to a container registry. Luckily, GCE already provides one:\nTAG=\"market-data:$(git rev-parse head)\"\nPROJECT_ID=<your-project-id> \ndocker tag $TAG eu.gcr.io/$PROJECT_ID/$TAG \ngcloud docker -- push eu.gcr.io/$PROJECT_ID/$TAG \nThis registry acts as an artifact repository where you can store your Docker images for \nlater use. After the push has completed, start an instance running this container:\ngcloud beta compute instances create-with-container \\\n  market-data-service-c \\\n  --container-image eu.gcr.io/$PROJECT_ID/$TAG \n  --tags api-service\nSuccess! You’ve deployed a container, and you’ve seen firsthand that it provides a more \nflexible — and easy-to-use — abstraction than a VM image.\nAs well as acting as a packaging mechanism, a container provides a runtime environ-\nment that isolates execution, effectively easing the operation of diverse containers on a \nsingle machine. This is compelling because it provides sane abstractions above individ-\nual hosts.\nUnlike virtual machine images, container images are portable; you can run the same \ncontainer on any infrastructure that supports the container runtime. This eases deploy-\nment in scenarios where multiple deployment targets are required, such as companies \nthat run workloads in both cloud and on-premise environments. It also simplifies local \ndevelopment; running multiple containers on a typical developer machine is much \nmore manageable than building and managing multiple virtual machines.\n8.4.4\t\nConfiguration\nThe service’s configuration is likely to differ based on deployment environment (stag-\ning, dev, production, and so on). For that and other reasons, you can’t represent all \nelements of a service within an artifact: \n¡ You can’t distribute secrets or sensitive configuration data, such as database pass-\nwords, in clear text or source control. You may want to retain the ability to change \nthem independently of a service deployment (for example, as part of automated \ncredential rotation, or worse, in the event of a security breach).\n¡ Environment-specific configuration data, such as database URLs, log levels, or \nthird-party service endpoints, will vary.\nReplace with your GCE project ID\nRenames the Docker image you created\nPushes the Docker image to GCE\nYou set the project ID and tag \nvariables in the previous example.\n \n",
      "content_length": 2450,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "\t\n207\nService to host models\nCode\nService artifact\nDeployed to\nDeployed to\nProduction\nStaging\nUses\nconfig\nUses\nconfig\nIs built into\nFigure 8.14    Service configuration that differ by environment\nThe third principle of The Twelve-Factor App manifesto (12factor.net) states that you \nshould strictly separate deployment configuration from code and provide it as environ-\nment variables (figure 8.14). In practice, the deployment mechanism you choose will \ndefine how you store and provide environment-specific configuration. We recommend \nstoring configuration in two places:\n¡ In source control, version-controlled alongside the service, for nonsensitive con-\nfiguration (These are commonly stored in .env files.)\n¡ A separate, access-restricted “vault” for secret information (such as HashiCorp’s \nperfectly named www.vaultproject.io)\nThe process that starts a service artifact should pull this configuration and inject it into \nthe application’s environment.\nUnfortunately, managing configuration separately can increase risk, as people may \nmake changes to production outside of your immutable artifacts, affecting the pre-\ndictability of your deployments. You should err on the side of restraint and attempt to \ninclude as much configuration as possible within your artifacts and rely on the speed \nand robustness of your deployment pipeline for rapidly changing configuration.\n8.5\t\nService to host models\nIn this section, we’ll review three common models for deploying services to underlying \nhosts: single service to host, multiple services to host, and container scheduling.\n8.5.1\t\nSingle service to host\nIn earlier examples, we’ve used a one-to-one relationship between service and underly-\ning host. This approach is easy to understand and provides a clear and explicit isolation \nbetween the resource needs and runtime of multiple services. Figure 8.15 illustrates this \napproach. Although the analogy is somewhat cruel, using this model lets you treat serv-\ners as cattle: indistinguishable units that you can start, stop, and destroy on command.\nThis model isn’t perfect. Sizing virtual instances appropriately for the needs of each \nservice requires ongoing effort and evaluation. If you’re not running in the cloud, you \nmay run into the limits of your data center or virtualization solution. And as we touched on \nearlier, virtual machine startup time is comparatively slow, often taking several minutes.\n \n",
      "content_length": 2422,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "208\nChapter 8  Deploying microservices\nOrders\nVM\nHoldings\nVM\nMarket data\nVM\nAccounts\nVM\nTransactions\nVM\nOrders\nVM\nHoldings\nVM\nMarket data\nVM\nAccounts\nVM\nTransactions\nVM\nFigure 8.15    A single service to host model\n8.5.2\t\nMultiple static services per host\nIt’s possible to run multiple services per host (figure 8.16). In the static variant of this \nmodel, the allocation of services to hosts is manual and static; the service owner makes \na conscious choice, predeployment, about where each service should be run.\nAt first glance, this approach might seem desirable. If obtaining new hosts is costly \nor hosts are scarce, then the easiest route to production would be to maximize usage of \nyour existing, limited number of hosts.\nBut this approach has several weaknesses. It increases coupling between services: \ndeploying multiple services to a host leads to coupling between services, eliminating \nyour desire to release services independently. It also increases the complexity of depen-\ndency management: if one service needs package v1.1, but another needs v2.0, the \ndifference is difficult to reconcile. It becomes unclear which service owns the deploy-\nment environment — and therefore which team has responsibility for managing that \nconfiguration.\nOrders\nVM\nHoldings\nOrders\nVM\nHoldings\nMarket data\nAccounts\nVM\nTransactions\nMarket data\nAccounts\nVM\nTransactions\n\t\nFigure 8.16    A single virtual machine can potentially run multiple services.\n \n",
      "content_length": 1453,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "\t\n209\nService to host models\nThis approach also leads to challenges in monitoring and scaling services inde-\npendently. One noisy service on a box might adversely impact other services, and it can \nbe difficult to monitor the resource usage (CPU, memory) of services independently.\n8.5.3\t\nMultiple scheduled services per host\nIt’d be even simpler if you could avoid thinking about the underlying hosts that run \nyour services altogether and focus entirely on the unique runtime environment of \neach application. This was the initial promise of platform as a service (Paas) solutions, \nsuch as Heroku. A PaaS provides tools for deploying and running services with min-\nimal operational configuration or exposure to underlying infrastructural resources. \nAlthough these platforms are easy to use, they often strike a difficult balance between \nautomation and control — simplifying deployment but removing customization from \nthe developer’s hands — as well as being highly vendor specific.\nContainers provide a more elegant abstraction:\n¡ An engineer can define and distribute a holistic application artifact.\n¡ A virtual machine can run multiple individual containers, isolating them from \neach other.\n¡ Containers provide an operational API that you can automate using higher level \ntooling.\nThese three facets enable scheduling, or orchestration, of containers. A container sched-\nuler is a software tool that abstracts away from underlying hosts by managing the execu-\ntion of atomic, containerized applications across a shared pool of resources. Typically, a \nscheduler consists of a master node that distributes application workloads to a cluster of \nworker nodes. Developers, or a deployment automation tool, send instructions to this \nmaster node to perform container deployments. Figure 8.17 illustrates this setup.\nDeployment tool\nDeploy two instances of\nservice A, version 100\nScheduler master\nPerforms deployment\nCluster\nService A\nService D\nService B\nService A\nService D\nService B\nService C\nService D\nPulls from\nContainer\nregistry\nFigure 8.17    A container scheduler executes containers across a cluster of nodes, balancing the \nresource needs of those nodes.\n \n",
      "content_length": 2173,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "210\nChapter 8  Deploying microservices\nAdvantages of a scheduling model\nUnlike the multiple static services per host model, the allocation of services in a sched-\nuler model is dynamic and depends on the resources (CPU, disk, or memory needs) \ndefined for each application. This avoids the pitfalls of the static model, as the sched-\nuler aims to continually optimize resource usage within the cluster of nodes, while the \ncontainer model preserves service independence.\nBy using a scheduler as a deployment platform, a service developer can focus on \nthe environment of their service in isolation from the underlying needs of machine \nconfiguration. Operations engineers can focus on running the underlying scheduler \nplatform and defining common operational standards for running services.\nContainer schedulers are complex\nContainer schedulers such as Kubernetes are complex pieces of software and require \nsignificant expertise to operate, especially because the tools themselves are relatively \nnew. We strongly recommend them as the ideal deployment platform for microservices, \nbut only if you can use a managed scheduler (such as Google’s Kubernetes Engine) \nor have the operational resources to run it in-house. If not, the single service per host \nmodel, combined with container artifacts, is a great and flexible fallback.\n8.6\t\nDeploying services without downtime\nSo far, you’ve only deployed market-data once. But in a real application, you’ll be \ndeploying services often. You need to be able to deploy new versions without downtime \nto maintain overall application stability. Every service will rely on others to be up and \nrunning, so you also need to maximize the availability of every service.\nThree common deployment patterns are available for zero-downtime deployments:\n¡ Rolling deploy  — You progressively take old instances (version N) out of service \nwhile you bring up new instances (version N+1), ensuring that you maintain a \nminimum percentage of capacity during deployment.\n¡ Canaries  — You add a single new instance1 into service to test the reliability of \nversion N+1 before continuing with a full rollout. This pattern provides an added \nmeasure of safety beyond a normal rolling deploy.\n¡ Blue-green deploys  — You create a parallel group of services (the green set), run-\nning the new version of the code; you progressively shift requests away from the \nold version (the blue set). This can work better than canaries in scenarios where \nservice consumers are highly sensitive to error rates and can’t accept the risk of \nan unhealthy canary.\nAll of these patterns are built on a single primitive operation. You’re taking an instance, \nmoving it to a running state in an environment, and directing traffic toward it.\n1\t In larger service groups, for example, >50 instances, you may need more than one canary to get \nrepresentative feedback.\n1\t In larger service groups, for example, >50 instances, you may need more than one canary to get \nrepresentative feedback.\n \n",
      "content_length": 3000,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "\t\n211\nDeploying services without downtime\n8.6.1\t\nCanaries and rolling deploys on GCE\nIt’s always better when you can see things in action. You can deploy a new version of \nmarket-data to GCE. First, you’ll want to create a new instance template. You can use \nthe container you built and pushed in section 8.4.3:\ngcloud beta compute instance-templates create-with-container \\\n  market-data-service-template-2 \\\n  --container-image eu.gcr.io/$PROJECT_ID/$TAG\n  --tags=api-service\nThen, initiate a canary update:\ngcloud beta compute instance-groups managed rolling-action start-update \\\n  market-data-service-group \\\n  --version template=market-data-service-template \\\n  --canary-version template=market-data-service\n➥-template-2,target-size=1 \\ \n  --region europe-west1\nGCE will add the canary instance to the group and the backend service to begin receiv-\ning requests (figure 8.18). It’ll take a few minutes to come up. You also can see this on \nthe GCE console (figure 8.19; Compute Engine > Instance Groups).\nIf you’re happy, you can proceed with the rolling update:\ngcloud beta compute instance-groups managed rolling-action start-update \\\n  market-data-service-group \\\n  --version template=market-data-service-template-2 \\\n  --region europe-west1\nThe speed at which this update occurs depends on how much capacity you want to \nmaintain during the rollout. You also can elect to surge beyond your current capac-\nity during rollout to ensure the target number of instances is always maintained. Fig-\nure 8.20 illustrates the stages of a rollout across three instances.\nCanary is brought\ninto service\nService group\nv1\nv1\nv1\nv2\nFigure 8.18    You add a new canary to the group.\nRolls out one instance \nof your new template\n \n",
      "content_length": 1725,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "212\nChapter 8  Deploying microservices\nFigure 8.19    Your instance group contains your original instances plus a canary instance of a new version.\nIf you were unhappy, you could roll back the canary:\ngcloud beta compute instance-groups managed rolling-action start-update \\\n  market-data-service-group \\\n  --version template=market-data-service-template \\ \n  --region europe-west1\nThe command for a rollback is identical to a rollout, but it goes to a previous version. \nIn the real world, rollback may not be atomic. For example, the incorrect operation of \nnew instances may have left data in an inconsistent state, requiring manual interven-\ntion and reconciliation. Releasing small change sets and actively monitoring release \nbehavior will limit the occurrence and extent of these scenarios.\nOld instance is killed\nNew instance added\nRepeated until all instances are v2\nv2\nv2\nv1\nv1\nv2\nv2\nv2\nv1\nv2\nv1\nv1\nX\nv2\nv1\nv1\nCanary is brought\ninto service\nRollout approved\nFigure 8.20    Stages of a rolling deploy, beginning with a canary instance\nThe original version\n \n",
      "content_length": 1067,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "\t\n213\nSummary\nWe’ve covered a lot of ground in this chapter: you’ve deployed manually to a cloud \nprovider, packaged a service as a container and a virtual machine, and practiced safe \nrollout patterns. By building immutable service artifacts and performing safe, down-\ntime-free deployments, you’re well on your way to building a deployment process that \nworks reliably across multiple services. Ultimately, the more stable, reliable, and seam-\nless your deployment process, the easier it is to standardize services, release new ser-\nvices more rapidly, and deliver valuable new features without friction or risk.\nSummary\n¡ Deploying new applications and changes must be standardized and straightforward \nto avoid friction in microservice development.\n¡ Microservices can run anywhere, but ideal deployment platforms need to sup-\nport a range of features, including security, configuration management, service \ndiscovery, and redundancy.\n¡ You deploy a typical service as a group of identical instances, connected by a load \nbalancer.\n¡ Instance groups, load balancers, and health checks enable autohealing and auto-\nscaling of deployed services.\n¡ Service artifacts must be immutable and predictable to minimize risk, reduce \ncognitive load, and simplify deployment abstractions.\n¡ You can package services as language-specific packages, OS packages, virtual \nmachine templates, or container images.\n¡ Being able to add/remove a single instance of a microservice is a fundamental \nprimitive operation that you can use to compose higher level deployment.\n¡ You can use canaries or blue-green deployments to reduce the impact of unex-\npected defects on availability.\n \n",
      "content_length": 1669,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "214\n9\nDeployment with \ncontainers and schedulers\nThis chapter covers\n¡ Using containers to package a microservice \ninto a deployable artifact\n¡ How to run a microservice on Kubernetes, a \ncontainer scheduler\n¡ Core Kubernetes concepts, including pods, \nservices, and replica sets\n¡ Performing canary deployments and rollbacks \non Kubernetes\nContainers are an elegant abstraction for deploying and running microservices, \noffering consistent cross-language packaging, application-level isolation, and rapid \nstartup time.\nIn turn, container schedulers provide a higher level deployment platform for con-\ntainers by orchestrating and managing the execution of different workloads across a \npool of underlying infrastructure resources. Schedulers also provide (or tightly inte-\ngrate with) other tools — such as networking, service discovery, load balancing, and con-\nfiguration management — to deliver a holistic environment for running service-based \napplications.\n \n",
      "content_length": 966,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "\t\n215\nContainerizing a service\nContainers aren’t a requirement for working with microservices. You can deploy ser-\nvices using many methods such as using the single service per VM model we outlined in \nthe previous chapter. But together with a scheduler, containers provide a particularly ele-\ngant and flexible approach that meets our two deployment goals: speed and automation.\nDocker is the most commonly used container tool, although other container runtimes \nare available, such as CoreOS’s rkt. An active group — the Open Container Initiative —  \nis also working to standardize container specifications.\nSome of the popular container schedulers available are Docker Swarm, Kubernetes, \nand Apache Mesos; different tools and distributions are built on top of those platforms. \nOf these, Kubernetes, Google’s open source container scheduler, has the widest mind-\nshare and has garnered significant implementation support from other organizations, \nsuch as Microsoft, and the open source community. Because of this popularity and the \nease of setting up a local installation, we’ll use Kubernetes in this book.\nWe significantly increased deployment velocity at our own company using Kuberne-\ntes. Whereas our previous approach could take several days to get a new service deploy-\nment working smoothly, with Kubernetes, any engineer can now deploy a new service \nin a few hours.\nIn this chapter, you’ll get your hands dirty with Docker and Kubernetes. You’ll use \nDocker to build, store, and run a container for a new service at SimpleBank. And you’ll \ntake that service to production using Kubernetes. Along with these examples, we’ll \nillustrate how a scheduler executes and manages different types of workloads and how \nfamiliar production concepts map to a scheduler platform. We’ll also examine the high-\nlevel architecture of Kubernetes.\n9.1\t\nContainerizing a service\nLet’s jump right in! Over the course of this chapter, your goal will be to take one of \nSimpleBank’s Python services — market-data — and get it running in production. You \ncan find a starting point for this service in the book’s repository on Github (http://\nmng.bz/7eN9). Figure 9.1 illustrates the process that will occur. Docker packages ser-\nvice code into a container image, which is stored in a repository. You'll use deploy \ninstructions to tell a scheduler to deploy and operate the packaged service on a cluster \nof underlying hosts.\nAs you know, a successful deployment is about more than running a single instance. \nFor each new version, you want to build an artifact that you can deploy multiple times \nfor redundancy, reliability, and horizontal scaling. In this section, you’ll learn how to do \nthe following:\n¡ Build an image for a service\n¡ Run multiple instances — or containers — of your image\n¡ Push your image to a shared repository, or registry\nFirst things first: if you’re going to ship this, you need to figure out how to put it in \na box. For this section, you’ll need to have Docker installed. You can find up-to-date \ninstructions online at https://docs.docker.com/install.\n \n",
      "content_length": 3080,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "216\nChapter 9  Deployment with containers and schedulers \nEngineer\nWrites\nWrites\nDeployment\ninstructions\nService\ncode\nIs packaged as\nInstructs\nScheduler\nHost 1\nHost N\nService\nHost 2\nCluster\nService\nContainer\nregistry\nPushes to\nPulls from\nDeploys\nContainer\nimage\nFigure 9.1    The process of deploying service code to a cluster scheduler\n9.1.1\t\nWorking with images\nTo package an application into a container, you need to build an image. The image will \ninclude the file system that your application needs to run — code and dependencies —  \nand other metadata, such as the command that starts your application. When you run \nyour application, you’ll start multiple instances of this image.\nMost powerfully, images can inherit from other images. That means your application \nimages can inherit from public, canonical images for different technology stacks, or \nyou can build your own base images to encapsulate standards and tools you use across \nmultiple services.\nTo get a feel for working with images, fire up the command line and try to pull a pub-\nlicly available Docker image:\n$ docker pull python:3.6\n3.6: Pulling from library/python\nef0380f84d05: Pull complete\n24c170465c65: Pull complete\n4f38f9d5c3c0: Pull complete\n4125326b53d8: Pull complete\n35de80d77198: Pull complete\nea2eeab506f8: Pull complete\n1c7da8f3172e: Pull complete\ne30a226be67a: Pull complete\nDigest: \nsha256:210d29a06581e5cd9da346e99ee53419910ec8071d166ad499a909c49705ba9b\nStatus: Downloaded newer image for python:3.6\nPulling an image downloads it to your local machine, ready for you to run. In this case, \nyou pulled a Python image from Docker Hub, the default public registry (or reposi-\ntory) for Docker images. Running the following command will start an instance of that \nimage, placing you at a Python interactive shell inside your new container:\n$ docker run --interactive --tty python:3.6\nPython 3.6.1 (default, Jun 17 2017, 06:29:46)\n \n",
      "content_length": 1917,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "\t\n217\nContainerizing a service\n[GCC 4.9.2] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\nYou should note a few things here. The –-interactive (or –i) flag indicates that the \ncontainer should be interactive, accepting input from STDIN, whereas the –-tty (or \n–t) flag connects a terminal for user input to the Docker container. When you started \nthe container, it executed the default command set within the image. You can check \nwhat that is by inspecting the image metadata:\n$ docker image inspect python:3.6 --format=\"{{.Config\n➥.Cmd}}\" \n[python3]\nYou can instruct Docker to execute other commands inside your container; for exam-\nple, to enter the container at an OS shell, rather than Python, you could suffix the \ncommand you used to start the image instance with bash.\nWhen you watched the output of your earlier pull command, you might’ve noticed \nthat Docker downloaded multiple items, each identified by a hash — these are layers. \nAn image is a union of multiple layers; when you build an image, each command you \nrun (apt-get update, pip install, apt-get install –y, and so on) creates a new \nlayer. You can list the commands that went into building the python:3.6 image:\n$ docker image history python:3.6\nEach line that this script returns represents a different command used to construct \nthe python:3.6 image. In turn, some of those layers were inherited from another \nbase image. Commands defined in a Dockerfile specify the layers in an image using \na lightweight domain-specific language (DSL). If you look at the Dockerfile for this \nimage — you can find it on Github (http://mng.bz/JxDj) — you’ll notice the first line:\nFROM buildpack-deps:jessie\nThis specifies that the image should inherit from the buildpack-deps:jessie image. \nIf you follow that thread on Docker Hub, you can see that your Python container has a \ndeep inheritance hierarchy that installs common binary dependencies and the under-\nlying Debian operating system. This is detailed in figure 9.2.\npython:3.6\nbuildpack-deps:jessie\nbuildpack-deps:jessie-scm\nbuildpack-deps:jessie-curl\ndebian:jessie\nInstalls base operating\nsystem\nInstalls basic HTTP tools,\nfor example, curl, wget\nInstalls common SCM\npackages, for example,\nGit, Mercurial\nInstalls common binaries,\nfor example, make, libmagick,\nlibc, libpq, gcc, libxml2\nInstalls and configures\npython and pip\nFigure 9.2    The inheritance hierarchy of images used to construct the python:3.6 container on Docker Hub\nThe Docker image configuration is output as JSON, \nwhich you can parse using Go text templates.\n \n",
      "content_length": 2588,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "218\nChapter 9  Deployment with containers and schedulers \nOther container ecosystems use different mechanisms — for example, rkt uses the \nacbuild command-line tool — but the end outcome is similar.\nAs well as enabling reusability, image layers optimize launch times for containers. If a \nparent layer is shared between two derivative images on one machine, you only need to \npull it from a registry once, not twice.\n9.1.2\t\nBuilding your image\nThis Python image is a good starting point for you to build your own application image. \nLet’s take a quick look at the dependencies of the market-data service:\n1\t It needs to run on an operating system — any distribution of Linux should do.\n2\t It relies on Python 3.6.x.\n3\t It installs several open source dependencies from PyPI using pip, a Python pack-\nage manager.\nIn fact, this list maps quite closely to the structure of the image that you’re going to \nbuild. Figure 9.3 illustrates the relationship between your image and the Python base \nimage you’ve worked with so far.\nTo build this image, first you need to create a Dockerfile in the root of the market-data \nservice directory. This should do the trick:\npython:3.6 image\nmarket-data image\nPython dependencies (installed by pip)\nApplication code and resources\nDebian\nCommon binaries\nPython 3.6\npip\nFigure 9.3    The structure of your market-data container image and its relationship to the python:3.6 \nbase image\n \n",
      "content_length": 1419,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "\t\n219\nContainerizing a service\nListing 9.1    Dockerfile for application container\nFROM python:3.6 \nADD . /app \nWORKDIR /app \nThat’s not quite the whole picture, but try building this image and see what it looks \nlike. You can use the docker build command to create an image from a Dockerfile:\n$ docker build -t market-data:first-build .\nSending build context to Docker daemon 71.17 kB\nStep 1/3 : FROM python:3.6\n ---> 74145628c331\nStep 2/3 : ADD . /app\n ---> bb3608d5143f\nRemoving intermediate container 74c250f83f8c\nStep 3/3 : WORKDIR /app\n ---> 7a595179cc39\nRemoving intermediate container 19d3bffa4d2a\nSuccessfully built 7a595179cc39\nThis builds an image with the name market-data and the tag first-build. We’ll make \nmore use of tagging later in this chapter. Check that you can start the container and it \ncontains the files you expect:\n$ docker run market-data:first-build bash -c 'ls'\nDockerfile\napp.py\nconfig.py\nrequirements.txt\nThe output of this command should match the contents of the market-data directory. \nIf it did for you, that’s great! You’ve built a new container and added some files — only \na few more steps until you have it running an application.\nAlthough you’ve added your application code, you still need to pull down dependen-\ncies and start the application up. First, you can use a RUN command within your Docker-\nfile to execute an arbitrary shell script:\nRUN pip install -r requirements.txt\nIf you recall, the pip tool itself was installed as part of the python base image. If you \nwere working with Ruby or Node, at this point you might call bundle install or npm \ninstall; if you were working with a compiled language, you might use a tool like make \nto produce compiled artifacts.\nInstructs Docker to build this image \nusing python:3.6 as a starting point\nCopies the current code directory to a \ndirectory /app inside the container image\nSets the working directory \nof the container to /app\n \n",
      "content_length": 1927,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "220\nChapter 9  Deployment with containers and schedulers \nNOTE    For more complex applications, especially for compiled languages, you \nmay want to use the builder pattern or multistage builds to separate your devel-\nopment and runtime Docker images: http://mng.bz/LMFr.\nNext, you need to set the command that’ll be used to start your application. Add \nanother line to your Dockerfile:\nCMD [\"gunicorn\", \"-c \", \"config.py\", \"app:app\"]\nAnd a final touch: you need to instruct Docker to expose a port to your app. In this \ncase, your Flask app expects traffic on port 8000. Putting that together, you get your \nfinal Dockerfile, as shown in the following listing. You should build the image again, \nthis time tagging it as latest.\nListing 9.2    Complete Dockerfile for the market-data service\nFROM python:3.6\nADD . /app\nWORKDIR /app\nRUN pip install -r requirements.txt\nCMD [\"gunicorn\", \"-c \", \"config.py\", \"app:app\"]\nEXPOSE 8000\nPublic images and security\nThe python:3.6 image we’ve used so far is derived from debian:jessie, which \nhas a reputation for being well maintained and rapidly releasing patches to disclosed \nvulnerabilities.\nBut, as when working with any software, it’s important to be aware that using public \nDocker images potentially increases your security risk. Many images, particularly those \nthat aren’t officially maintained, aren’t regularly patched or updated, which can increase \nthe threat surface of your system.\nIf in doubt, security scanning tools, such as Clair (https://github.com/coreos/clair), exist \nfor analyzing the security stance of Docker containers. You can use these on an ad-hoc \nbasis or integrate them into your continuous integration pipeline.\nMaintaining your own base images is also an option but does involve an extra time invest-\nment. Deciding to take this route requires careful consideration of your team’s capabili-\nties and security expertise.\n \n9.1.3\t\nRunning containers\nNow that you’ve built an image for the application, you can run it. Try it out:\n$ docker run -d -p 8000:8000 --name market-data market-data:latest\nThis command should return a long hash to the terminal. That’s the ID of your con-\ntainer — you’ve started it in detached mode, rather than in the foreground. You’ve also \nused the -p flag to map the container port so it’s accessible from the Docker host. If \n \n",
      "content_length": 2333,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "\t\n221\nContainerizing a service\nyou try and call the service — it has a health-check endpoint at /ping — you should get \na successful response:\n$ curl -I http://{DOCKER_HOST}:8000/ping \nHTTP/1.0 200 OK\nContent-Type: text/plain\nServer: Werkzeug/0.12.2 Python/3.6.1\nYou could easily run multiple instances and balance between them. Try a basic exam-\nple, using NGINX as a load balancer. Luckily you can pull an NGINX container from \nthe public registry — no hard work to get that running. Figure 9.4 illustrates the con-\ntainers you’re going to run.\nFirst, start up three instances of the market-data service. Run the code below in your \nterminal:\n$ docker network create market-data \n$ for i in {1..3} \n  do\n    docker run -d \\\n      --name market-data-$i \\\n      -–network market-data \\\n      market-data:latest\n  done\nIf you run docker ps -a, you’ll see three instances of the market-data service up and \nrunning.\nTIP    Instead of working on the command line, you could use Docker Compose \nto define a set of containers declaratively — in a YAML file — and run them. But \nin this case, it’s better to start at a lower level so you can see what’s happening.\nmarket-data-1\nPort\n5000\nmarket-data-1\nPort\n5000\nmarket-data-1\nPort\n5000\nRequests\nNGINX\nPort 80\nFigure 9.4    NGINX load-balances requests made to it between three market-data containers\nDOCKER_HOST will depend on how you’ve \ninstalled Docker in your environment.\nCreates a container network named market-data\nRuns three containers based on the \nmarket-data:latest image you created earlier\n \n",
      "content_length": 1550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "222\nChapter 9  Deployment with containers and schedulers \nUnlike earlier, you didn’t map each container’s port to the host machine. That’s \nbecause you’ll only access these containers through NGINX. Instead, you created a \nnetwork. Running on the same network will allow the NGINX container to easily dis-\ncover your market-data instances, using the container name as a host name.\nNow, you can set up NGINX. Unlike before, you’re not going to build your own \nimage; instead, you’ll pull the official NGINX image from the public Docker registry. \nFirst, you’ll need to configure NGINX to load balance between three instances. Create \na file called nginx.conf using the following code.\nListing 9.3    nginx.conf\nupstream app {\n    server market-data-1:8000; \n    server market-data-2:8000;\n    server market-data-3:8000;\n}\nserver {\n    listen 80;\n    location / {\n        proxy_pass http://app; \n    }\n}\nThen you can start an NGINX container. You’ll use the volume flag (or –v) to \nmount your new nginx.conf file into the container, sharing it with the local filesys-\ntem. This is useful for sharing secrets and configurations that aren’t — or shouldn’t \nbe — built into a container image, such as encryption keys, SSL certificates, and \nenvironment-specific configuration files. In this case, you avoid having to build a \nseparate container to include a single new configuration file. Start the container by \nentering the following:\n$ docker run -d --name=nginx \\\n--network market-data \\ \n--volume `pwd`/nginx.conf:/etc/nginx/conf.d/\n➥default.conf \\ \n-p 80:80 \\ \nnginx\nAnd that should do the trick. Curling http://localhost/ping should return the \nhostname — by default, the container ID — of the container instance responding to \nthat request. NGINX will round-robin requests across the three nodes to (naively) \nbalance load across your instances.\nYou configure the upstream application \nusing the container name and port.\nThe NGINX server proxies requests received \non port 80 to the upstream application.\nRuns on the same network as \nyour market-data containers\nMounts your configuration into an \nappropriate location inside the container\nMaps the container port to \nport 80 on your host machine\n \n",
      "content_length": 2201,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "\t\n223\nContainerizing a service\n9.1.4\t\nStoring an image\nGood work so far — you’ve built an image and you’ve seen that it’s easy to run multi-\nple independent instances of an application. Unfortunately, that image isn’t much use \nin the long run if it’s only on your machine. When it comes to deploying this image, \nyou’ll pull it from a Docker registry. This might be Docker Hub, which you’ve already \nencountered; a managed registry, such as AWS ECR or Google Container Registry; or \nself-hosted — for example, using the Docker distribution open source project (https://\ngithub.com/docker/distribution). When you build a continuous delivery pipeline, \nthat pipeline will push to your registry on every valid commit.\nNOTE    It’s also possible to save Docker images as a tarball, using the docker \nsave command, although this isn’t commonly used in image distribution. In \ncontrast, rkt natively uses tarballs for container distribution. This means you \ncan store images in standard file stores, for example, S3, rather than using a \ncustom registry.\nFor now, you can push your image to https://hub.docker.com. First, you’ll need to cre-\nate an account and choose a Docker ID. This will be the namespace you’ll use to store \nyour containers. Once you’ve logged in, you’ll need to create a new repository — a \nstore for multiple versions of the same image — using the web UI (figure 9.5).\nTo push to this repository, you need to tag your market-data image with an appropri-\nate name. Docker image names follow the format <registry>/<repository>:<tag>. \nOnce that’s done, a simple docker push will upload your image to the registry. Try it out:\n$ docker tag market-data:latest <docker id>/market-data:latest\n$ docker login\n$ docker push <docker id>/market-data:latest\nFigure 9.5    Using the Create Repository page on Docker Hub to create a repository for market-data \nimages\n \n",
      "content_length": 1875,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "224\nChapter 9  Deployment with containers and schedulers \nFigure 9.6    The private repository page on Docker Hub shows a record of the tagged image you pushed.\nThat’s it! You’ve successfully pushed your image to a public repository. You can double- \ncheck that through the web UI (figure 9.6) by logging into https://hub.docker \n.com. Other engineers (if your repository is private, you’ll need to grant them access) \ncan pull your image using docker pull [image name].\nLet’s take stock for a moment:\n¡ You’ve learned how to package a simple application into a lightweight, \ncross-platform artifact — a container image.\n¡ We’ve explored how Docker images are built from multiple layers to support \ninheritance from common base containers and increase startup speed.\n¡ You’ve run multiple isolated instances of an application container.\n¡ You’ve pushed the image you built to a Docker registry.\nUsing these techniques in a build pipeline will ensure greater consistency and predict-\nability across a fleet of services, regardless of underlying programming language, as well \nas helping to simplify local development. Next, we’ll explore how a container sched-\nuler works by taking your containerized application and deploying it with Kubernetes.\n9.2\t\nDeploying to a cluster\nA container scheduler is a software tool that abstracts away from underlying hosts by \nmanaging the execution of atomic, containerized applications across a shared pool of \nresources. This is possible because containers provide strong isolation of resources and \na consistent API.\nUsing a scheduler is a compelling deployment platform for microservices because \nit eases the management of scaling, health checks, and releases across, in theory, any \nnumber of independent services. And it does so while ensuring efficient utilization of \nunderlying infrastructure. At a high level, a container scheduler workflow looks some-\nthing like this:\n¡ Developers write declarative instructions to specify which applications they want to \nrun. These workloads might vary: you might want to run a stateless, long-running \nservice; a one-off job; or a stateful application, like a database.\n¡ Those instructions go to a master node.\n \n",
      "content_length": 2198,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "\t\n225\nDeploying to a cluster\n¡ The master node executes those instructions, distributing the workloads to a \ncluster of underlying worker nodes.\n¡ Worker nodes pull containers from an appropriate registry and run those appli-\ncations as specified.\nFigure 9.7 illustrates this scheduler architecture. To an engineer, where and how an \napplication is executed is ultimately unimportant: the scheduler takes care of it. In \naddition to running containers, Kubernetes provides other functionality to support \nrunning applications, such as service discovery and secret management.\nEngineer\nInstructions\n2 x service A\nMaster node\nperforms\ndeployment\nWorker node\nWorker node\nWorker node\nWorker node\nContainer\nregistry\npulls from\nservice A\nD\nD\nB\nD\nB\nA\nC\nCluster\nFigure 9.7    High-level scheduler architecture and deployment process\nMany well-known cluster management tools are available, but in your case, you’re \ngoing to use Kubernetes, an open-source project that evolved from Google’s internal \nwork on Borg and Omega (https://research.google.com/pubs/pub41684.html). It’s \npossible to run Kubernetes pretty much anywhere — public cloud, private data center, \nor as a managed service (such as Google Kubernetes Engine (GKE)).\nIn the next few sections, we’re going to cover a lot of ground. You’ll do the following:\n¡ Learn about the unit of deployment used on Kubernetes — pods\n¡ Define and deploy multiple replicas of a pod for the market-data microservice\n¡ Route requests to your pods using services\n¡ Deploy a new version of the market-data microservice\n¡ Learn how to communicate between microservices on Kubernetes\nWe’ll start by using Minikube, which will run in a virtual machine on your local host. \nIn a real deployment environment, the master and worker nodes would be separate \nvirtual machines, but locally, the same machine will fulfill both roles. You can find an \n \n",
      "content_length": 1879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "226\nChapter 9  Deployment with containers and schedulers \ninstallation guide for Minikube on the project’s Github page (https://github.com/\nkubernetes/minikube).\nTIP    If you used a private repository in section 9.1.4, you’ll need to configure \nMinikube so it can access that repository by running minikube addons con-\nfigure registry-creds and following the instructions onscreen.\n9.2.1\t\nDesigning and running pods\nThe basic building block in Kubernetes is a pod: a single container or a tightly coupled \ngroup of containers that are scheduled together on the same machine. A pod is the \nunit of deployment and represents a single instance of a service. Because it’s the unit of \ndeployment, it’s also the unit of horizontal scalability (or replication). When you scale \ncapacity up or down, you add or remove pods.\nTIP    Sometimes a service is deployed as more than one container — a composite \ncontainer. For example, Flask services running on a Gunicorn web server are \ntypically served behind NGINX. Using Kubernetes, a single pod would contain \nboth the service and the NGINX container. Other examples of composite con-\ntainer patterns are discussed on the Kubernetes blog at (http://mng.bz/tOyC).\nYou can define a set of pods for your market-data service. Create a file called market- \ndata-replica-set.yml in your app directory. Don’t worry if it doesn’t make much sense \nyet. Include the following code in your file.\nListing 9.4    market-data-replica-set.yml\n---\nkind: ReplicaSet \napiVersion: extensions/v1beta1\nmetadata:\n  name: market-data\nspec:\n  replicas: 3 \n  template: \n    metadata:\n      labels: \n        app: market-data \n        tier: backend \n        track: stable \n    spec:\n      containers:\n      - name: market-data\n        image: <docker id>/market-data:latest \n        ports:\n        - containerPort: 8000\nIn Kubernetes, you typically declare instructions to the scheduler in YAML files (or \nJSON, but YAML’s easier on the eyes). These instructions define Kubernetes objects, \nDefines a set of pods\nShould contain three replicas of your market-data pod\nCreates each pod using this template\nIdentifies pods within Kubernetes by label\nContains a single container, \npulled from your Docker registry\n \n",
      "content_length": 2227,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "\t\n227\nDeploying to a cluster\nand a pod is one kind of object. These configuration files represent the desired state \nof your cluster. When you apply this configuration to Kubernetes, the scheduler will \ncontinually work to maintain that ideal state. In this file, you’ve defined a ReplicaSet, \nwhich is a Kubernetes object that manages a group of pods.\nNOTE    We’ll occasionally use dot-notation to refer to paths within a *.yml file. \nFor example, in listing 9.4, the path to the market-data container definition \nwould be spec.template.spec.containers[0].\nTo apply this to your local cluster, you can use the kubectl command-line tool. When \nyou started Minikube, it should have automatically configured kubectl to operate on \nyour cluster. This tool interacts with an API exposed by the cluster’s master node. Give \nit a try:\n$ kubectl apply -f market-data-replica-set.yml\nreplicaset \"market-data\" configured\nKubernetes will asynchronously create the objects you’ve defined. You can observe the \nstatus of this operation using kubectl. Running kubectl get pods (or kubectl get \npods -l app=market-data) will show you the pods that your command has created \n(figure 9.8). They’ll take a few minutes to start up for the first time as the node down-\nloads your Docker image.\nYou saw earlier that you didn’t create individual pods. It’s unusual to create or destroy \npods directly; instead, pods are managed by controllers. A controller is responsible \nfor taking some desired state — say, always running three instances of the market-data \npod — and performing actions to reach that state. This observe-diff-act loop happens \ncontinually.\nYou’ve just encountered the most common type of controller: the ReplicaSet. If \nyou’ve ever encountered instance groups on AWS or GCP, you might find their behav-\nior similar. A replica set aims to ensure a specific number of pods are running at any \none time. For example, let’s say a pod dies — maybe a node in the cluster failed — the \nreplica set will observe that the state of the cluster no longer matches the desired state \nand will attempt to schedule a replacement elsewhere in the cluster.\nYou can see this in action. Delete one of the pods you’ve just created (pods are iden-\ntified by name):\n$ kubectl delete pod <pod name>\nThe replica set will schedule a new pod to replace the one you destroyed (figure 9.9).\nFigure 9.8    The results of the kubectl get pods command after creating a new replica set\n \n",
      "content_length": 2456,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "228\nChapter 9  Deployment with containers and schedulers \nFigure 9.9    The state of running pods after one member of the replica set is deleted\nThis matches the ideal we laid out in chapter 8: that deploying microservice instances \nshould be built on a single primitive operation. By combining controllers and \nimmutable containers, you can treat pods like cattle and rely on automation to main-\ntain capacity, even when the underlying infrastructure is unreliable.\nWARNING    A cluster alone isn’t a complete redundancy solution; your infra-\nstructure design also determines this. For example, if you run a cluster in a sin-\ngle data center — or one availability zone in AWS — you won’t have redundancy \nif that entire data center goes down. It’s important, where possible, to run your \ncluster(s) across multiple isolated zones of failure.\n9.2.2\t\nLoad balancing\nRight, so you’re running a microservice on Kubernetes. That was pretty quick. The bad \nnews is, you can’t access those pods yet. Like you did earlier with NGINX, you need to \nlink them to a load balancer to route requests and expose their capabilities to other \ncollaborators, either inside or outside your cluster.\nIn Kubernetes, a service defines a set of pods and provides a method for reaching \nthem, either by other applications in the cluster or from outside the cluster. The net-\nworking magic that achieves this feat is outside the scope of this book, but figure 9.10 \nillustrates how a service would connect to your existing pods.\nNow, you’re currently running a replica set containing three market-data pods. If \nyou recall from listing 9.4, your market-data pods have the labels app: market-data \nand tier: backend. That’s important, because a service forms a group of pods based \non their labels.\nTo create a service, you need another YAML file, as shown in the following listing. \nThis time, call it market-data-service.yml (great naming convention).\nListing 9.5    market-data-service.yml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: market-data\nspec:\n  type: NodePort\n \n",
      "content_length": 2056,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "\t\n229\nDeploying to a cluster\n  selector: \n    app: market-data \n    tier: backend \n  ports:\n    - protocol: TCP\n      port: 8000 \n      nodePort: 30623 \nApply this configuration using the same $ kubectl apply -f  command you used to \ncreate the replica set before, substituting the name of your new YAML file. This will \ncreate a service accessible on port 30623 of your cluster, which routes requests to your \nmarket-data pods on port 8000.\nYou should be able to curl your service and send requests to your pods. Doing so will \nreturn the name of each pod that serves the request:\n$ curl http://`minikube ip`:30623/ping \nRequests\nPort\nmarket-data\nservice\nmarket-data pods\napp: market-data\ntier: backend\ntrack: stable\napp: market-data\ntier: backend\n5000\napp: market-data\ntier: backend\ntrack: stable\n5000\napp: market-data\ntier: backend\ntrack: stable\n5000\nReplica set (market-data)\nFigure 9.10    Requests made to a service are forwarded to pods that match the label selector of the \nservice.\nDefines which pods this service will access\nThe service will route to this port on the specified pods.\nThe service will be exposed as a specified port \non the cluster. Excluding this line will assign a \nrandom port in range 30000-32767.\n`minikube ip` returns the IP \naddress of your local cluster.\n \n",
      "content_length": 1291,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "230\nChapter 9  Deployment with containers and schedulers \nSeveral types of services are available, and they’re outlined in table 9.1. In this case, \nyou used a NodePort service to map your service to an externally available port on \nyour cluster, but if only other cluster services access your microservice, it usually makes \nmore sense to use ClusterIP to keep access local to the cluster.\nTable 9.1    Types of service on Kubernetes\nService type\nBehavior\nClusterIP\nExposes the service on an IP address local to the cluster\nNodePort\nExposes the service on a static port accessible at the cluster’s IP address\nLoadBalancer\nExposes the service by provisioning an external cloud service load balancer (If you’re \nusing AWS, this creates an ELB.)\nThe service listens for events across the cluster and will be dynamically updated if the \ngroup of pods changes. For example, if you kill a pod, it will be removed from the \ngroup, and the service will route requests to any new pod created by the replica set.\n9.2.3\t\nA quick look under the hood\nSo far, this has been seamless: you send an instruction, and Kubernetes executes it! \nLet’s take a moment to learn how Kubernetes runs your pods.\nIf you drill down a level, you can see that the master and worker nodes on Kubernetes \nrun several specialized components. Figure 9.11 illustrates these components.\nComponents of the master node\nThe master node consists of four components:\n¡ The API server  — When you ran commands on kubectl, this is what it communi-\ncated with to perform operations. The API server exposes an API for both exter-\nnal users and other components within the cluster.\n¡ The scheduler  — This is responsible for selecting an appropriate node where a pod \nwill run, given priority, resource needs, and other constraints.\n¡ The controller manager  — This is responsible for executing control loops: the con-\ntinual observe-diff-act operation that underpins the operation of Kubernetes.\n¡ A distributed key-value data store, etcd  — This stores the underlying state of the clus-\nter and thereby makes sure it persists when nodes fail or restarts are required.\nTogether, these components act as a control plane for the cluster. Picture this as \nsomething like the cockpit of an airplane. Together, these components provide the \nAPI and backend required to orchestrate operations across a cluster of nodes.\n \n",
      "content_length": 2370,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "\t\n231\nDeploying to a cluster\netcd\nMaster\nWorker node(s)\nController\nmanager\nScheduler\nQueries & updates\nGets & updates state\nGets & updates state\nAPI server\nstate\nstate\nKubelet\nkube-proxy\nControls\nContainer runtime\nRuns\nContainers\nC\nC\nProxies\nFigure 9.11    Components of the master and worker nodes in a Kubernetes cluster\nComponents of a worker node\nEach worker node uses the following components to run and monitor applications:\n¡ A container runtime  — In your case, this is Docker.\n¡ The kubelet  — This interacts with the Kubernetes master to start, stop, and moni-\ntor containers on the node.\n¡ The kube-proxy  — This provides a network proxy to direct requests to and between \ndifferent pods across the cluster.\nThese components are relatively small and loosely coupled. A key design principle \nof Kubernetes is to separate concerns and ensure components can operate autono-\nmously — a little like microservices!\nWatches for state changes\nThe API server is responsible for recording the state of the cluster — and receiving \ninstructions from clients — but it doesn’t explicitly tell other components what to do. \nInstead, each component works independently to orchestrate cluster behavior when \nsome event or change occurs. To learn about state changes, each component watches \nthe API server: a component requests to be notified by the API server when something \ninteresting happens, so it can perform appropriate actions to attempt to match the \ndesired state.\nFor example, the scheduler needs to know when it should assign new pods to nodes. \nTherefore, it connects to the API server to receive a continuous stream of events that \nrelate to the pod resource. When it receives a notification about a newly created pod, it \nfinds an appropriate node for that pod. Figure 9.12 shows this process.\nIn turn, your kubelets watch the API server to learn when a pod has been assigned to \nits node and then they start the pod appropriately. Each component watches resources \nand events that interest it; for example, the controller manager watches replica sets and \nservices (among other things).\n \n",
      "content_length": 2101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "232\nChapter 9  Deployment with containers and schedulers \nScheduler\n1. Watches\n2. Receives notification\nabout new pod\n3. Updates pod with assigned node\nAPI server\nPods\nFigure 9.12    The scheduler watches the API server for newly created pods and determines which node \nthey should run on.\nUnderstanding how pods are run\nWhat happens when you create a replica set? You saw earlier that this results in the \nexpected number of pods being run — from your perspective, it looked simple! But in \nreality, creating your replica set through kubectl triggers a complex chain of events \nacross multiple components. This chain is illustrated in figure 9.13.\nController\nmanager\nScheduler\n4. Assigns pods to node\n3. Notified\n3. Creates pods\n2. Notified\n1. Creates new replica set\nReplicaSet\nPod\nkubectl\nAPI server\n5. Notified\n6. run container\nKubelet\nContainer runtime\n7. runs containers\nContainers\nWorker node(s)\nFigure 9.13    The series of events from creating a replica set to running pods on Kubernetes\n \n",
      "content_length": 999,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "\t\n233\nDeploying to a cluster\nLet’s walk through each step:\n1\t You instructed the API server to create a new replica set, using kubectl. The API \nserver stores this new resource in etcd.\n2\t The controller manager watches for creation and modification of replica sets. It \nreceives a notification about the new set you created.\n3\t The controller manager compares the current state of the cluster to the new \nstate, determining that it needs to create new pods. It creates these pod resources \nthrough the API server, based on the template you provided through kubectl.\n4\t The scheduler receives a notification about a new pod and assigns it an appro-\npriate node, again updating the pod’s definition through the API server. At this \npoint, you haven’t run any real application — the controllers and scheduler have \nonly updated the state that the API server is storing.\n5\t Once the pod is assigned to a node, the API server notifies the appropriate \nkubelet, and the kubelet instructs Docker to run containers. Images are down-\nloaded, containers are started, and the kubelet begins to monitor their opera-\ntion. At this point, your pods are running!\nAs you can see, each component acts independently, but together, they orchestrate a \ncomplex deployment action. Hopefully this has given you a useful glance under the \ncover.  Now, back to running your microservices.\n9.2.4\t\nHealth checks\nYou’re missing something. Unlike a typical cloud load balancer, a Kubernetes service \ndoesn’t itself execute health checks on your underlying application. Instead, the ser-\nvice checks the shared state of the cluster to determine if a pod is ready to receive \nrequests. But how do you know if a pod is ready?\nIn chapter 6, we introduced two types of health check:\n¡ Liveness  — Whether an application has started correctly\n¡ Readiness  — Whether an application is ready to serve requests\nThese health checks are crucial to the resiliency of your service. They ensure that traf-\nfic is routed to healthy instances of your microservice and away from instances that are \nperforming poorly (or not at all).\nBy default, Kubernetes executes lightweight, process-based liveness checks for every \npod you run. If one of your market-data containers fails a liveness check, Kubernetes \nwill attempt to restart that container (as long as the container’s restart policy isn’t set \nto Never). The kubelet process on each worker node carries out this health check. This \nprocess continually queries the container runtime (in your case, the Docker daemon) \nto establish whether it needs to restart a container.\n \n",
      "content_length": 2585,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "234\nChapter 9  Deployment with containers and schedulers \nTIP    Kubernetes performs restarts on an exponential back-off schedule; if a \npod isn’t live after five minutes, it’ll be marked for deletion. If a replica set man-\nages the pod, the controller will attempt to schedule a new pod to maintain the \ndesired service capacity.\nThis alone isn’t adequate, as your microservice may run into failure scenarios that \ndon’t cause the container itself to fail: whether deadlocks due to request saturation, \ntimeouts of underlying resources, or a plain old coding error. If the scheduler can’t \nidentify this scenario, performance can deteriorate as a service routes requests to unre-\nsponsive pods, potentially leading to cascading failures.\nTo avoid this situation, you need the scheduler to continually check the state of the \napplication inside your container, ensuring it’s both live and ready. With Kubernetes, \nyou can configure probes to achieve this, which you can define as part of your pod \ntemplate. Figure 9.14 illustrates how these checks, and the previous process check, will \nbe run.\nAdding probes is straightforward, although you do need to add some configuration \nsee the next listing to the container specification in market-data-replica-set.yml. Probes \ncan be HTTP GET requests, scripts executed inside a container, or TCP socket checks. In \nthis case, you’ll use a GET request, as shown in the following listing.\nKubelet\nUpdates\nPod status\nReady\nLive\nApplication ready?\nApplication live?\nContainer running?\nChecks status\nmarket-data\nservice\nRoutes requests\nmarket-data container\nApplication\nDocker\nruntime\nFigure 9.14    The kubelet process on each worker node runs health checks, or probes, in Kubernetes. \nReadiness probe results control routing by services.\n \n",
      "content_length": 1781,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "\t\n235\nDeploying to a cluster\nListing 9.6    Liveness probe in market-data-replica-set.yml \nlivenessProbe: \n  httpGet: \n    path: /ping \n    port: 8000 \n  initialDelaySeconds: 10 \n  timeoutSeconds: 15 \nreadinessProbe: \n    path: /ping \n    port: 8000 \n  initialDelaySeconds: 10 \n  timeoutSeconds: 15 \nReapply this configuration using kubectl to update the state of the replica set. Kuber-\nnetes will, to the best of its ability, use these probes to help ensure instances of your \nmicroservice are alive and kicking. In this example, both liveness and readiness check \nthe same endpoint, but if your microservice has external dependencies, such as a \nqueueing service, it makes sense to make readiness dependent on connectivity from \nyour application to those dependencies.\n9.2.5\t\nDeploying a new version\nYou should now understand how you use replica sets, pods, and services to run stateless \nmicroservices on Kubernetes. On top of these concepts, you can build a stable, seam-\nless deployment process for each of your microservices. In chapter 8, you learned about \ncanary deployments; in this section, you’ll try out the technique with Kubernetes.\nDeployments\nBefore we get started, we should quickly introduce deployments. Kubernetes provides \na higher level abstraction, the Deployment object, for orchestrating the deployment of \nnew replica sets. Each time you update a deployment, the scheduler will orchestrate a \nrolling update of instances in a replica set, ensuring they’re deployed seamlessly.\nYou can change the original approach to use a deployment instead. First, delete your \noriginal replica set:\n$ kubectl delete replicaset market-data\nAfter that, create a new file, market-data-deployment.yml. This should be similar to the \nreplica set you created earlier, except that the type of object should be Deployment, \nrather than ReplicaSet, as shown in the following listing.\nListing 9.7    market-data-deployment.yml\n---\napiVersion: extensions/v1beta1\nkind: Deployment \nmetadata:\n  name: market-data\nspec: \n  replicas: 3 \nConfigures a liveness probe to query /ping on port 8000\nConfigures a readiness probe to query /ping on port 8000\nDefines a Kubernetes deployment object\nThe desired number of pods to deploy\n \n",
      "content_length": 2227,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "236\nChapter 9  Deployment with containers and schedulers \n  template: \n    metadata:\n      labels:\n        app: market-data\n        tier: backend\n        track: stable\n    spec:\n      containers:\n      - name: market-data\n        image: <docker id>/market-data:latest\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 8000\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8000\n          initialDelaySeconds: 10\n          timeoutSeconds: 15\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8000\n          initialDelaySeconds: 10\n          timeoutSeconds: 15\nUse kubectl to apply this file to the cluster. This will create a deployment, which will \ncreate a replica set and three instances of the market-data pod.\nCanaries\nIn a canary deploy, you deploy a single instance of a microservice to ensure that a new \nbuild is stable when it faces real production traffic. This instance should run alongside \nexisting production instances. A canary release has four steps:\n1\t You release a single instance of a new version alongside the previous version.\n2\t You route some proportion of traffic to the new instance.\n3\t You assess the health of the new version by, for example, monitoring error rates \nor observing behavior.\n4\t If the new version is healthy, you commence a full rollout to replace other \ninstances. If not, you remove the canary instance, halting the release.\nOn Kubernetes, you can use labels to identify a canary pod. In the first example, you \nspecified a label track: stable on each pod in your replica set. To deploy a canary, \nyou’ll need to deploy a new pod that’s distinguished with track: canary. The service \nyou created earlier only selects on two labels (app and tier), so it’ll route requests to \nboth stable and canary pods. This is illustrated in figure 9.15.\nThe template to use for creating each pod\n \n",
      "content_length": 1975,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "\t\n237\nDeploying to a cluster\nRequests\nPort\nThe service routes requests\nto the new canary pod\nbased on the label selector.\napp: market-data\ntier: backend\nReplica set\n(market-data-canary)\napp: market-data\ntier: backend\ntrack: stable\n5000\napp: market-data\ntier: backend\ntrack: stable\n5000\napp: market-data\ntier: backend\ntrack: stable\n5000\napp: market-data\ntier: backend\ntrack: stable\n5000\nReplica set (market-data)\nFigure 9.15    The service forwards requests to your new canary pod based on the service’s label selector, \nwhich doesn’t restrict on track.\nFirst, you should build a new container for your new release. You’ll use tags to identify \nthe new version, and don’t forget to substitute your own Docker ID:\n$ docker build -t <docker id>/market-data:v2 .\n$ docker push <docker id>/market-data:v2\nThis version is tagged as v2, although in practice it may not be appropriate to apply a \nnumeric versioning scheme to your services. We’ve found tagging them with the com-\nmit ID also works well. (For Git repositories, we use git rev-parse --short HEAD.)\nOnce you’ve pushed that new image, create a yml file specifying your canary \ndeployment:\n¡ It should create one replica, instead of three.\n¡ It should release the v2 tag of the container, rather than latest.\n¡ It should look like the following listing.\nListing 9.8    market-data-canary-deployment.yml\n---\napiVersion: extensions/v1beta1\nkind: Deployment\n \n",
      "content_length": 1411,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "238\nChapter 9  Deployment with containers and schedulers \nmetadata:\n  name: market-data-canary\nspec:\n  replicas: 1 \n  template:\n    metadata:\n      labels:\n        app: market-data \n        tier: backend \n        track: canary \n    spec:\n      containers:\n      - name: market-data\n        image: <docker id>/market-data:v2 \n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 8000\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8000\n          initialDelaySeconds: 10\n          timeoutSeconds: 15\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8000\n          initialDelaySeconds: 10\n          timeoutSeconds: 15\nUse kubectl to apply this to your cluster. The deployment will create a new replica set \ncontaining a single canary pod for v2.\nLet’s take a closer look at the state of your cluster. If you run minikube dashboard on \nthe command line, it’ll open the dashboard for your cluster in a browser window (fig-\nure 9.16). In the dashboard — under Workloads — you should be able to see:\n¡ The canary deployment you’ve just created, as well as your original deployment\n¡ Four pods: the original three, plus a canary pod\n¡ Two replica sets: one each for the stable and canary tracks\nSo far so good! At this stage, for a real microservice, you might run some automated tests, \nor check the monitoring output of your service to ensure it’s processing work as expected. \nFor now, you can safely assume your canary is healthy and performing as expected, which \nmeans you can safely roll out the new version, replacing all your old instances.\nEdit the market-data-deployment.yml file and make two changes:\n¡ Change the container used to market-data:v2.\n¡ Add a strategy field to specify how pods will be updated.\nYou want to create one canary.\nThe canary deployment has \na distinct set of labels.\nThis deployment will release \nmarket-data:v2.\n \n",
      "content_length": 1991,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "\t\n239\nDeploying to a cluster\nFigure 9.16    The Kubernetes dashboard after multiple deploys — stable and canary — of the market-data \nmicroservice\nYour updated deployment file should look like the following listing.\nListing 9.9    Updated market-data-deployment.yml\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: market-data\nspec:\n  replicas: 3\n  strategy: \n    type: RollingUpdate \n    rollingUpdate: \n      maxUnavailable: 50% \n      maxSurge: 50% \n  template:\n    metadata:\n      labels:\n        app: market-data\n        tier: backend\n        track: stable\n    spec:\n      containers:\n      - name: market-data\n        image: morganjbruce/market-data:v2\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\nThe strategy field describes how Kubernetes \nwill execute the deployment of new pods.\n \n",
      "content_length": 855,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "240\nChapter 9  Deployment with containers and schedulers \n        ports:\n        - containerPort: 8000\nApplying this configuration will create a new replica set, starting instances one by one \nwhile removing them from the original set.  This process is illustrated in figure 9.17.\nYou can also observe this in the event history of the controller by running kubectl \ndescribe deployment/market-data (figure 9.18).\nFrom this history, you can see how Kubernetes allows you to build higher level \ndeployment operations on top of simple operations. In this case, the scheduler used \nyour desired state of the world and a set of constraints to determine an appropriate \npath of deployment, but you could use replica sets and pods to build any deployment \npattern that was appropriate for your service.\nca\nDeployment\nCreates a new replica set\nv1\nv1\nv1\nReplica set A\nReplica set B\n1\nv1\nv1\nv1\nv2\nv2\nReplica set A\nReplica set B\n2\nv1\nv1\nv2\nv2\nv2\nReplica set A\nReplica set B\n4\nv1\nv1\nv2\nv2\nReplica set A\nReplica set B\n3\n5\nDeployment\nScales up number\nof new instances\nScales up number\nof new instances\nDeployment\nDeployment\nDeployment\nThe empty replica set A is\nmaintained to enable rollback.\nScales down\nold instances\nScales down\nold instances\nv2\nv2\nv2\nReplica set A\nReplica set B\nv2\nv2\nv2\nReplica set B\n6\nFigure 9.17    A new deployment creates a new replica set and progressively rolls instances between the \nold and new set.\nFigure 9.18    Events Kubernetes emitted during a rolling deployment\n \n",
      "content_length": 1486,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "\t\n241\nDeploying to a cluster\n9.2.6\t\nRolling back\nWell done! You’ve smoothly deployed a new version of your microservice.  If something \nwent wrong, you can also use the deployment object to undo all your hard work. First, \ncheck the rollout history:\n$ kubectl rollout history deployment/market-data\nThis should return two revisions: your original deployment and your v2 deployment. \nTo roll back, specify the target revision:\n$ kubectl rollout undo deployment/market-data --to-revision=1\nThis will perform the reverse of the previous rolling update to return the underlying \nreplica set to its original state.\n9.2.7\t\nConnecting multiple services\nLastly, your microservice isn’t going to be much use by itself, and several of Simple-\nBank’s services depend on the capabilities that the market-data service provides. It’d \nbe pretty much insane to hardcode a port number or an IP address into each service to \nrefer to the underlying endpoint of each collaborator; you shouldn’t tightly couple any \nservice to another’s internal network location. Instead, you need some way of accessing \na collaborator by a known name.\nKubernetes integrates a local DNS service to achieve this, and it runs as a pod on \nthe Kubernetes master. When new service is created, the DNS service assigns a name \nin the format {my-svc}.{my-namespace}.svc.cluster.local; for example, you \nshould be able to resolve your market-data service from any other pod using the name \nmarket-data.default.svc.cluster.local.\nGive it a shot. You can use kubectl to run an arbitrary container in your cluster — try \nbusybox, which is a great little image containing several common Linux utilities, such as \nnslookup. Run the following command to open a command prompt inside a container \nrunning on Minikube:\n$ kubectl run -i --tty lookup --image=busybox /bin/sh\nThen you can try an nslookup:\n/ # nslookup market-data.default.svc.cluster.local\nYou should get output that looks something like this:\nServer:    10.0.0.10\nAddress 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local\nName:      market-data.default.svc.cluster.local\nAddress 1: 10.0.0.156 market-data.default.svc.cluster.local\nThe IP address in the last entry should match the cluster IP assigned to your service. \n(If you don’t believe me, you can double-check by calling kubectl get services.) If \nso, success! You’ve covered a lot of ground: building and storing an image for a micro-\nservice, running it on Kubernetes, load-balancing multiple instances, deploying a new \nversion (and rolling back), and connecting microservices together. \n \n",
      "content_length": 2565,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "242\nChapter 9  Deployment with containers and schedulers \nSummary\n¡ Packaging microservices as immutable, executable artifacts allows you to \norchestrate deployment through a primitive operation — adding or removing a \ncontainer.\n¡ Schedulers and containers abstract away underlying machine management for \nservice development and deployment.\n¡ Schedulers work by trying to match the resource needs of an application to the \nresource usage of a cluster of machines, while health-checking running services \nto ensure they’re operating correctly.\n¡ Kubernetes provides ideal features of a microservice deployment platform, \nincluding secret management, service discovery, and horizontal scalability.\n¡ A Kubernetes user defines the desired state (or specification) of their cluster ser-\nvices, and Kubernetes figures out how to achieve that state, executing a continual \nloop of observe-diff-act.\n¡ The logical application unit on Kubernetes is a pod: one or more containers that \nexecute together.\n¡ Replica sets manage the lifecycle of groups of pods, starting new pods if existing \nones fail.\n¡ Deployments on Kubernetes are designed to maintain service availability by exe-\ncuting rolling updates of pods across replica sets.\n¡ You can use service objects to group underlying pods and make them available to \nother applications inside and outside of the cluster.\n \n",
      "content_length": 1367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "243\n10\nBuilding a delivery \npipeline for microservices\nThis chapter covers\n¡ Designing a continuous delivery pipeline for a \nmicroservice\n¡ Using Jenkins and Kubernetes to automate \ndeployment tasks\n¡ Managing staging and production \nenvironments\n¡ Using feature flags and dark launches to \ndistinguish between deployment and release\nRapidly and reliably releasing new microservices and new features to production is \ncrucial to successfully maintaining a microservice application. Unlike a monolithic \napplication, where you can optimize deployment for a single use case, microservice \ndeployment practices need to scale to multiple services, written in different lan-\nguages, and each with their own dependencies. Investing in consistent and robust \ndeployment tooling and infrastructure will go a long way toward making a success of \nany microservice project.\nYou can achieve reliable microservice releases by applying the principles of con-\ntinuous delivery. The fundamental building block of continuous delivery is a deploy-\nment pipeline. Picture a factory production line: a conveyer belt takes your software \n \n",
      "content_length": 1119,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "244\nChapter 10  Building a delivery pipeline for microservices \nfrom code commits to deployable artifact to running software, while continually assess-\ning the quality of the output at each stage. Doing this leads to frequent, small deploy-\nments, rather than big-bang changes, to production.\nSo far, you’ve built and deployed a service using Docker, Kubernetes, and com-\nmand-line scripts. In this chapter, you’ll combine those steps into an end-to-end build \npipeline, using Jenkins, a widely used open source build automation tool. Along the \nway, we’ll examine how this approach minimizes risk and increases the stability of your \noverall application. After that, we’ll examine the difference between deploying new \ncode and releasing new features.\n10.1\t Making deploys boring\nDeploying software should be boring. You should be able to roll out changes and \nnew features without peering through your fingers or obsessively watching error \ndashboards.\nUnfortunately, as we mentioned in chapter 8, human error causes most issues in pro-\nduction, and microservice deployments leave plenty of room for that! Consider the big \npicture: teams are developing and deploying tens — if not hundreds — of independent \nservices on their own schedule, without explicit coordination or collaboration between \nteams. Any bad change to a service might have a wide-ranging impact on the perfor-\nmance of other services and the wider application.\nAn ideal microservice deployment process should meet two goals:\n¡ Safety at pace  — The faster you can deploy new services and changes, the quicker \nyou can iterate and deliver value to your end users. Deployment should maxi-\nmize safety: you should validate, as much as feasible, that a given change won’t \nnegatively impact the stability of a service.\n¡ Consistency  — Consistency of deployment process across different services, \nregardless of underlying tech stack, helps alleviate technical isolation and makes \noperations more predictable and scalable.\nIt’s not easy to maintain the fine balance between safety and pace. You could move \nquickly without safety by deploying code changes directly to production, but that’d \nbe crazy. Likewise, you could achieve stability by investing in a time-consuming \nchange-control and approval process, but that wouldn’t scale well to the high volume \nof change in a large, complex microservice application.\n10.1.1\t A deployment pipeline\nContinuous delivery strikes an ideal balance between reducing risk and increasing speed:\n¡ Releasing smaller sets of commits increases safety by reducing the amount of \nchange happening at any one time. Smaller changesets are also easier to reason \nthrough.\n¡ An automated pipeline of commit validation increases the probability that a \ngiven changeset is free from defects.\n \n",
      "content_length": 2792,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "\t\n245\nMaking deploys boring\nReleasing small changesets and systematically verifying their quality gives teams the \nconfidence to release features rapidly. Smaller, more atomic releases are less risky. \nThe continuous delivery approach empowers teams to ship services rapidly and \nindependently.\nOne of the weaknesses of monolithic development is that releases often become \nlarge, coupling together disparate features at release time. Likewise, even small \nchanges in a large application can have an unintentionally broad impact, particularly \nwhen made to cross-cutting concerns. At worst, commits in monolithic development \nbecome stale while waiting for a deployment; they’re no longer relevant to the needs of \nthe application or business by the time they reach customers.\nNOTE    Continuous delivery isn’t quite the same as continuous deployment. \nIn the latter, every validated change is automatically deployed to production; \nin the former, you can deploy every change to production, but whether you \ndeploy it or not is up to the engineering team and business needs.\nLet’s look at an example in figure 10.1. Most of the steps in this pipeline should look \nfamiliar:\n1\t First, an engineer commits some code to a microservice repository.\n2\t Next, a build automation server builds the code.\n3\t If the build is successful, the automation server runs unit tests to validate that \ncode.\n4\t If these tests pass, the automation server packages the service for deployment \nand stores this package in an artifact repository.\n5\t The automation server deploys code to a staging environment, where you can \ntest the service against other live collaborators.\n6\t If this is successful, the automation server will deploy the code to a production \nenvironment.\nEngineer\nCommits\nNotify\nService\ncode\nDeployment pipeline\nFailed build\nFailed tests\nUnit test\nBuild\nPackage\nDeploy to\nstaging\nTest\nFailed tests\nFailed push\nFailure at a stage produces feedback.\nArtifact\nrepository\nStaging\nenvironment\nProduction\nenvironment\nDeploys to\nDeploys to\nStores in\nDeploy to\nproduction\nFigure 10.1    An example deployment pipeline builds, validates, and deploys a commit to production, providing \nfeedback to engineers.\n \n",
      "content_length": 2198,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "246\nChapter 10  Building a delivery pipeline for microservices \nEach step in this pipeline provides feedback to the engineering team on the correct-\nness of their code. For example, if step 3 fails, you’ll receive a list of failed test asser-\ntions to correct.\nImplementing this pipeline should make the process of deployment highly visible \nand transparent — crucial for an audit trail, or if something goes wrong. Regardless of \nthe underlying language or technology, every service you deploy should be able to fol-\nlow a similar process.\n10.2\t Building a pipeline with Jenkins\nIn the previous chapter, you ran command-line scripts to perform steps in deploy-\nment: building containers, publishing artifacts, and deploying code. Now you’ll use \nJenkins — a build automation tool — to connect those steps together into a coher-\nent, reusable, and extensible deployment pipeline. We’ve picked Jenkins because \nit’s open source, is easy to get running, supports scriptable build jobs, and is widely \nused.\nUnfortunately, no perfect out-of-the-box solution for deployment is available: any \npipeline is usually a combination of multiple tools, depending on both the service’s \ntech stack and the target deployment platform. In your case, you’ll be using Jenkins to \nassemble tools you’ve (mostly) already used. Figure 10.2 illustrates the components of \nyour deployment pipeline.\nIn the next few sections, we’re going to cover a lot of ground:\n¡ Using Jenkins to script complex deployment pipelines\n¡ Building a pipeline that builds, tests, and deploys your service to different \nenvironments\n¡ Managing staging environments for microservices\n¡ Reusing your deployment pipeline across multiple services\nJenkins orchestrates the pipeline, combining multiple tools.\nPython-specific tools\nfor managing\ndependencies and\nrunning tests\nDocker\nPytest\nPypi\nBuild\nUnit test\nPackage\nJenkins\nDeploy to\nstaging\nTest\nDeploy to\nproduction\nManual\nKubernetes,\nkubectl\nKubernetes,\nkubectl\nYou deploy services to\nKubernetes using kubectl.\nYou use Docker to build and\npackage a deployable artifact.\nFigure 10.2    The deployment pipeline you’ll use, which combines multiple tools dependent on the tech stack \nand target deployment platform you’re using\n \n",
      "content_length": 2234,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "\t\n247\nBuilding a pipeline with Jenkins\nYou’ll need to have access to a running Jenkins instance to run the examples in this \nchapter. The appendix walks you through Jenkins setup on a local Minikube cluster —  \nwe’ll assume you’re using that approach in the following sections.\n10.2.1\t Configuring a build pipeline\nThe Jenkins application consists of a master node and, optionally, any number of \nagents. Running a Jenkins job executes scripts (using common tools, such as make \nor Maven) across these agent nodes to perform deployment activities. A job operates \nwithin a workspace  — a local copy of your code repository. Figure 10.3 illustrates this \narchitecture.\nTo write your build pipeline, you’re going to use a feature called Scripted Pipe-\nline. In Scripted Pipeline, you can express a build pipeline using a general-purpose \ndomain-specific language (DSL) written in Groovy. This DSL defines common methods \nfor writing build jobs, such as sh (for executing shell scripts) and stage (for identifying \ndifferent parts of a build pipeline). The Scripted Pipeline approach is more powerful \nthan you might think — by the end of the chapter, you’ll use it to build your own higher \nlevel, declarative DSL.\nNOTE    At the time of writing, Jenkins Pipeline only supports Groovy as a script-\ning language. Don’t worry — if you’re comfortable with Java, Python, or Ruby, \nunderstanding Groovy won’t be too taxing.\nJenkins will execute build jobs by executing a pipeline script defined in a Jenkinsfile. \nTry it yourself! First, copy the contents of chapter-10/market-data into a new directory \nand push that to a Git repo. It’s easiest if you push it to somewhere public, like GitHub. \nThis is the service you’ll be deploying in this chapter.\nWorkspace\nCloned from\nExecuted on\nJob {scripts}\nSource repo\nJenkins\nMaster\nAgent\nWorkspace\nAgent\nFigure 10.3    A Jenkins deployment consists of a master node, which manages execution, and agents \nthat perform build tasks within a workspace — a clone of the repository being built.\n \n",
      "content_length": 2030,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "248\nChapter 10  Building a delivery pipeline for microservices \nNext, you’ll want to create a Jenkinsfile in the root of your repository, and it should \nlook like the following listing.\nListing 10.1    A basic Jenkinsfile\nstage(\"Build Info\") { \n  node { \n    def commit = checkout scm \n    echo \"Latest commit id: ${commit.GIT_COMMIT}\"\n  }\n}\nWhen Jenkins runs this script, the script will check out a code repository as a workspace \nand write the latest commit ID to the console.\nYou can try this out by setting up a pipeline job for a service. Commit the Jenkinsfile \nyou just created, then push your changes to origin. Now, open the Jenkins UI. (Remem-\nber, you can do this with minikube service jenkins.) Follow these steps to create a \nmultibranch pipeline job:\n1\t Navigate to the Create New Jobs page.\n2\t Enter an item name, market-data; select Multibranch Pipeline as the job type; \nand click OK.\n3\t On the following page (see figure 10.4), select a Branch Source of Git and add \nyour repository’s clone URL to the Project Repository field. If you’re using a pri-\nvate Git repository, you’ll also need to configure your credentials.\n4\t Elect to periodically scan the pipeline every minute. This will trigger builds if \nchanges are detected.\n5\t Save your changes.\nOnce you’ve saved your changes, Jenkins will scan your repository for branches con-\ntaining a Jenkinsfile. The multibranch pipeline job type will generate a unique build \nfor each branch in your repository — later, this will let you treat feature branches differ-\nently from the master branch.\nTIP    Instead of clicking through the UI, you can use the Jenkins Job DSL to \ngenerate pipeline jobs. This is (another) Groovy DSL that generates jobs in \nJenkins’ underlying XML format. You can find examples in the project docu-\nmentation (https://github.com/jenkinsci/job-dsl-plugin/wiki).\nOnce the indexing is complete, Jenkins will run a build for your master branch. \nClicking on the name of the branch will take you the build history for that branch \n(figure 10.5).\nIdentifies a distinct phase of your pipeline\nTakes a closure (or function) as a parameter, instructing \nJenkins to execute this code on a build node\nChecks out some code \nfrom source control\n \n",
      "content_length": 2229,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "\t\n249\nBuilding a pipeline with Jenkins\nFigure 10.4    New project configuration screen, showing Branch Sources options\nClick the build number and then Console Output. This traces the output of the build. \nWithin that output, you should be able to see how your Jenkinsfile has been executed:\nAgent default-q3ccc is provisioned from template Kubernetes Pod Template\nAgent specification [Kubernetes Pod Template] (jenkins-jenkins-slave): \n* [jnlp] jenkins/jnlp-slave:3.10-1(resourceRequestCpu: 200m, resourceRequest\n➥Memory: 256Mi, resourceLimitCpu: 200m, resourceLimitMemory: 256Mi)\nRunning on default-q3ccc in /home/jenkins/workspace/market-data_master\n➥-27MDVADAYDBX5WJSRWQIFEL3T7GD4LWPU5CXCZNTJ4CKBDLP3LVA\n[Pipeline] {\n[Pipeline] checkout\nCloning the remote Git repository\nCloning with configured refspecs honoured and without tags\nCloning repository https://github.com/morganjbruce/market-data.git\n > git init /home/jenkins/workspace/market-data_master\n[CA}-27MDVADAYDBX5WJSRWQIFEL3T7GD4LWPU5CXCZNTJ4CKBDLP3LVA # timeout=10\nFetching upstream changes from https://github.com/morganjbruce/\n➥market-data.git\n > git --version # timeout=10\n > git fetch --no-tags --progress https://github.com/morganjbruce/\n➥market-data.git +refs/heads/*:refs/remotes/origin/*\n > git config remote.origin.url https://github.com/morganjbruce/\n➥market-data.git # timeout=10\n > git config --add remote.origin.fetch +refs/heads/*:refs/remotes/origin/\n➥* # timeout=10\n > git config remote.origin.url https://github.com/morganjbruce/\n➥market-data.git # timeout=10\nFetching without tags\n \n",
      "content_length": 1562,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "250\nChapter 10  Building a delivery pipeline for microservices \nFetching upstream changes from https://github.com/morganjbruce/\n➥market-data.git\n > git fetch --no-tags --progress https://github.com/morganjbruce/\n➥market-data.git +refs/heads/*:refs/remotes/origin/*\nChecking out Revision 80bfb7bdc4fa0b92dcf360393e5d49e0f348b43b (master)\n > git config core.sparsecheckout # timeout=10\n > git checkout -f 80bfb7bdc4fa0b92dcf360393e5d49e0f348b43b\nCommit message: \"working through ch10\"\nFirst time build. Skipping changelog.\n[Pipeline] echo\nLatest commit id: 80bfb7bdc4fa0b92dcf360393e5d49e0f348b43b\n[Pipeline] }\n[Pipeline] // node\n[Pipeline] }\n[Pipeline] // stage\n[Pipeline] End of Pipeline\nFinished: SUCCESS\nFigure 10.5    Build history for the master branch of your repository\n \n",
      "content_length": 778,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "\t\n251\nBuilding a pipeline with Jenkins\nEach [Pipeline] step traces the execution of your code. Awesome — you’ve deployed \na build automation tool, configured it against a service repository, and run your first \nbuild pipeline! Next, let’s look at the first stage of your pipeline: build.\n10.2.2\t Building your image\nYou’re going to use Docker to build and package your images. First, let’s change your \nJenkinsfile, as shown in the following listing.\nListing 10.2    Jenkinsfile for build step\ndef withPod(body) { \n  podTemplate(label: 'pod', serviceAccount: 'jenkins', containers: [\n      containerTemplate(name: 'docker', image: 'docker', command: 'cat', \n➥ttyEnabled: true),\n      containerTemplate(name: 'kubectl', image: 'morganjbruce/kubectl', \n➥command: 'cat', ttyEnabled: true)\n    ],\n    volumes: [\n      hostPathVolume(mountPath: '/var/run/docker.sock', hostPath: \n➥'/var/run/docker.sock'),\n    ]\n ) { body() }\n}\nwithPod {\n  node('pod') { \n    def tag = \"${env.BRANCH_NAME}.${env.BUILD_NUMBER}\"\n    def service = \"market-data:${tag}\"\n    \n    checkout scm \n    container('docker') { \n      stage('Build') { \n        sh(\"docker build -t ${service} .\") \n      }\n    }\n  }\n}\nThis script will build your service and tag the resulting Docker container with the cur-\nrent build number. It’s definitely more complex than the earlier version, so let’s take a \nquick walk through what you’re doing:\n1\t You define a pod template for your build, which Jenkins will use to create pods \non Kubernetes for a build agent. This pod contains two containers — Docker and \nkubectl.\nDefines a pod template to use to run your job\nRequests an instance of your pod template\nChecks out the latest code from Git\nEnters the Docker container of your pod\nStarts a new pipeline stage\nRuns a docker command to build your service image\n \n",
      "content_length": 1817,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "252\nChapter 10  Building a delivery pipeline for microservices \n2\t Within that pod, you check out the latest version of your code from Git.\n3\t You then start a new pipeline stage, which you’ve called Build.\n4\t Within that stage, you enter the Docker container and run a docker command to \nbuild your service image.\nTIP    Jenkins also provides a Groovy DSL for Docker instead of the shell com-\nmands you’ve used. For example, you could use docker.build(imageName)in \nplace of the sh call in listing 10.5.\nCommit this new Jenkinsfile to your Git repo and navigate to the build job on Jenkins. \nWait for a rerun — or trigger the job manually — and in the console output, you should \nsee your container image being built successfully.\n10.2.3\t Running tests\nNext, you should run some tests. This should be like any other continuous integration \njob: if the tests are green, deployment can proceed; if not, you halt the pipeline. At this \nstage, you aim to provide rapid and accurate feedback on the quality of a changeset. \nFast test suites help engineers iterate quickly.\nBuilding your code and performing unit tests are only two of the possible activities \nyou might perform during this commit stage of the build pipeline. Table 10.1 outlines \nother possibilities.\nTable 10.1    Possible activities in the commit stage of a deployment pipeline\nActivity\nDescription\nUnit tests\nCode-level tests\nCompilation\nCompiling the artifact into an executable artifact\nDependency resolution\nResolving external dependencies — for example, open source packages\nStatic analysis\nEvaluating code against metrics\nLinting\nChecking syntax and stylistic principles of code\nFor now, you should get your unit tests running. Add a Test stage to your Jenkinsfile, \nimmediately after the Build stage as shown in the next listing.\nListing 10.3    Test stage\nstage('Test') {\n  sh(\"docker run --rm ${service} python setup.py test\")\n}\nCommit your Jenkinsfile and run the build. This will add a new stage to your pipeline, \nwhich executes the test cases defined in /tests (figure 10.6).\n \n",
      "content_length": 2055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "\t\n253\nBuilding a pipeline with Jenkins\nFigure 10.6    Your pipeline so far with Build and Test stages\nUnfortunately, this code alone won’t make the results visible in the build. Only success \nor failure will do that. You can archive the XML results you’re generating by adding the \nfollowing to your Jenskinsfile.\nListing 10.4    Archiving results from test stage\nstage('Test') {\n  try {\n    sh(\"docker run -v `pwd`:/workspace --rm ${service} \n➥python setup.py test\") \n  } finally {\n    step([$class: 'JUnitResultArchiver', testResults: \n➥'results.xml']) \n  }\n} \nMounts the current \nworkspace as a volume\nArchives the results that the test job generates\n \n",
      "content_length": 656,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "254\nChapter 10  Building a delivery pipeline for microservices \nThis code mounts the current workspace as a volume within the Docker container. The \npython test process will write output to that volume as /workspace/result.xml, and you \ncan access those results even after Docker has stopped and removed the service con-\ntainer. You use the try–finally statement to ensure you achieve results regardless of \npass or failure.\nTIP    A good build pipeline directs feedback to the responsible engineering \nteam. For example, our deployment pipelines at Onfido notify commit authors \nthrough Slack and email if pipeline stages fail. We also emit events from our \npipeline for monitoring tools such as PagerDuty to consume. For more on \nsending notifications, see the Jenkins documentation (http://mng.bz/C5X3).\nCommitting your changed Jenkinsfile and running a fresh build will store test results \nin Jenkins. You can view them on the build page. Great — you’ve validated your under-\nlying code. Now you’re almost ready to deploy.\n10.2.4\t Publishing artifacts\nYou need to publish an artifact — in this case, our Docker container image — to be able \nto deploy it. If you used a private Docker registry in chapter 9, you’ll need to configure \nyour Docker credentials within Jenkins:\n1\t Navigate to Credentials > System > Global Credentials > Add Credentials.\n2\t Add username and password credentials, using your credentials to https://hub \n.docker.com.\n3\t Set the ID as dockerhub and click OK to save these credentials.\nIf you’re using a public registry, you can skip this step. Either way, when you’re ready, \nadd a third step to your Jenkinsfile, as follows.\nListing 10.5    Publishing artifacts\ndef tagToDeploy = \"[your-account]/${service}\" \nstage('Publish') {\n  withDockerRegistry(registry: [credentialsId: \n➥'dockerhub']) { \n    sh(\"docker tag ${service} ${tagToDeploy}\") \n    sh(\"docker push ${tagToDeploy}\")\n  }\n}\nThe target public image tag — replace \nwith your account name\nLogs in to the Docker registry \nusing stored credentials\nTags the image with your Docker account name\n \n",
      "content_length": 2081,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "\t\n255\nBuilding a pipeline with Jenkins\nWhen you have that ready, commit and run your build. Jenkins will publish your con-\ntainer to the public Docker registry.\n10.2.5\t Deploying to staging\nAt this point, you’ve tested the service internally but in complete isolation; you haven’t \ninteracted with any of the service’s upstream or downstream collaborators. You could \ndeploy directly to production and hope for the best, but you probably shouldn’t. Instead, \nyou can deploy to a staging environment where you can run further automated and man-\nual tests against real collaborators.\nYou’re going to use Kubernetes namespaces to logically segregate your staging \nand production environments. To deploy your service, you’ll use kubectl, using an \napproach similar to the one you took in chapter 9. Rather than installing the tool on \nJenkins, you can use Docker to wrap this command-line tool. This is quite a useful \ntechnique.\nWARNING    Logical segregation isn’t always appropriate in a real-world environ-\nment. Compliance and security standards, such as PCI DSS, often mandate net-\nwork-level isolation between production and development workloads, which \nKubernetes namespaces wouldn’t currently satisfy. In addition, completely \nseparating staging and production infrastructure reduces the risk of a “noisy \nneighbor” in staging, such as a resource-hungry service, affecting production \nreliability.\nFirst, let’s look at your deployment and service definition. You should save the follow-\ning listing to deploy/staging/market-data.yml within your market-data repo.\nListing 10.6    Deployment specification for market-data\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: market-data\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 50%\n      maxSurge: 50%\n  template:\n    metadata:\n      labels:\n        app: market-data\n        tier: backend\n        track: stable\n    spec:\n      containers:\n      - name: market-data\n \n",
      "content_length": 1994,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "256\nChapter 10  Building a delivery pipeline for microservices \n        image: BUILD_TAG \n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 8000\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8000\n          initialDelaySeconds: 10\n          timeoutSeconds: 15\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8000\n          initialDelaySeconds: 10\n          timeoutSeconds: 15\nIf you saw this in chapter 9, you’ll notice one key difference: you don’t set a specific \nimage tag to deploy, only a placeholder of BUILD_TAG. You’ll replace this in your pipe-\nline with the version you’re deploying. This is a little unsophisticated — as you build \nmore complex deployments, you might want to explore higher level templating tools, \nsuch as ksonnet (https://ksonnet.io).\nYou’ll also want to add market-data-service.yml, as shown in the following listing, to \nthe same location.\nListing 10.7    market-data service definition\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: market-data\nspec:\n  type: NodePort\n  selector:\n    app: market-data\n    tier: backend\n  ports:\n    - protocol: TCP\n      port: 8000\n      nodePort: 30623\nBefore you deploy, create distinct namespaces to segregate your workloads, using \nkubectl:\nkubectl create namespace staging\nkubectl create namespace canary\nkubectl create namespace production\nA placeholder for the \nimage you want to deploy\n \n",
      "content_length": 1517,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "\t\n257\nBuilding a pipeline with Jenkins\nNow, add a deploy stage to your pipeline, as follows.\nListing 10.8    Deployment to staging (Jenkinsfile)\nstage('Deploy') {\n  sh(\"sed -i.bak 's#BUILD_TAG#${tagToDeploy}#' ./\n➥deploy/staging/*.yml\") \n  container('kubectl') {\n    sh(\"kubectl --namespace=staging apply -f deploy/\n➥staging/\") \n  }\n} \nAgain, commit and run the build. This time, a Kubernetes deploy should be triggered! \nYou can check the status of this deployment using kubectl rollout status:\n$ kubectl rollout status –n staging deployment/market-data\nWaiting for rollout to finish: 2 of 3 updated replicas are available... \ndeployment \"market-data\" successfully rolled out\nAs you can see, although your build was marked as complete, the deployment itself \ntakes some time to roll out. This is because kubectl apply works asynchronously and \ndoesn’t wait for the cluster to finish updating to reflect the new state. If you like, you \ncan add a call to the above kubectl rollout status method within the Jenkinsfile so \nthat Jenkins waits for rollouts to complete before proceeding.\nEither way, once the rollout is complete, you can access this service:\n$ curl `minikube service --namespace staging --url market-data`/ping\nHTTP/1.0 200 OK\nContent-Type: text/plain\nServer: Werkzeug/0.12.2 Python/3.6.1\nThis example service doesn’t do much. For your own services, you might trigger fur-\nther automated testing or perform further exploratory testing of the service and code \nchanges you’ve just deployed. Table 10.2 outlines some of the activities you might per-\nform at this stage of a deployment pipeline. For now, great work — you’ve automated \nyour first microservice deployment!\nTable 10.2    Possible activities to perform to validate a staging release of a microservice\nAcceptance testing\nAutomated tests\nRunning automated tests to check expectations, either \nregression or acceptance\nManual tests\nSome services may require manual validation or explor-\natory testing.\nNonfunctional testing\nSecurity tests\nTesting the security posture of the service\nLoad/capacity tests\nValidating expectations about capacity and load on a \nservice\nUses sed to replace BUILD_TAG with \nthe name of your new Docker image\nApplies all configuration files in deploy/staging to \nyour local cluster, using the staging namespace\n \n",
      "content_length": 2311,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "258\nChapter 10  Building a delivery pipeline for microservices \n10.2.6\t Staging environments\nLet’s take a break for a moment to discuss staging environments. You should make any \nnew release of a service to staging first. Microservices need to be tested together, and \nproduction isn’t the first place where that should happen.\nThe infrastructure configuration of your staging environment should be an exact \ncopy of production, albeit with less real traffic. It doesn’t need to run at the same scale. \nThe volume and type of testing you’ll use to put your services through their paces can \ndetermine the necessary size. As well as conducting various types of automated testing, \nyou might manually validate services in staging to ensure they meet acceptance criteria.\nAlong with shared staging environments, you might also run isolated staging envi-\nronments for individual or small sets of closely related services. Unlike full staging, \nthese environments might be ephemeral and spun up on-demand for the duration of \ntesting. This is useful for testing a feature in relative isolation, with tighter control of the \nstate of the environment. Figure 10.7 compares these different approaches to staging \nenvironments.\nAlthough staging environments are crucial, they can be hard to manage in a micro-\nservice application, as well as the source of significant contention between teams. A \nmicroservice might have many dependencies, all of which should be present and stable \nin full staging. Although a service in staging will have passed testing, code review, and \nother quality gates, it’s still possible that services in staging will be less stable than their \nproduction equivalents, and that can cause chaos. Any engineer deploying to a shared \nenvironment needs to act as a good neighbor to ensure that issues with services they \nown don’t substantially impact another team’s ability to smoothly test (and therefore \ndeliver) other services.\nTIP    To further reduce friction in staging, consider building your deployment \npipeline to allow any engineer to easily roll back the last deployment, regard-\nless of whether they own that service or not.\nC\nD\nB\nC\nD\nB\nA\nB\nA\nE\nIsolated staging environments may\ninclude a small subset of services.\nFull staging includes all services\nwithin an application.\nFigure 10.7    Isolated versus full staging environments\n \n",
      "content_length": 2361,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "\t\n259\nBuilding a pipeline with Jenkins\n10.2.7\t Deploying to production\nYou can use what you’ve learned so far to take this service to production. Table 10.3 \noutlines some of the different actions you might perform at this stage of your pipeline.\nTable 10.3    Possible activities to perform in deployment\nCode deployment\nDeploying code to a runtime environment\nRollback\nRolling back code to a previous version, if errors or unexpected behavior occurs\nSmoke tests\nValidating the behavior of a system using light-touch tests\n In this case, if a deployment to staging is successful, here’s what should happen next:\n1\t Your pipeline should wait for human approval to proceed to production.\n2\t When you have approval, you’ll release a canary instance first. This helps you vali-\ndate that your new build is stable when it faces real production traffic.\n3\t If you’re happy with the performance of the canary instance, the pipeline can \nproceed to deploy the remaining instances to production.\n4\t If you’re not happy, you can roll back your canary instance.\nFirst, you should add an approval stage. In continuous delivery — unlike continuous \ndeployment — you don’t necessarily want to push every commit immediately to pro-\nduction. Add the following to your Jenkinsfile.\nListing 10.9    Approving a production release\nstage('Approve release?') {\n  input message: \"Release ${tagToDeploy} to production?\"\n}\nRunning this code in Jenkins will show a dialog box in the build pipeline view, with two \noptions: Proceed or Abort. Clicking Abort will cancel the build; clicking Proceed will, \nfor now, cause the build to finish successfully — you haven’t added a deploy step!\nFirst, try a production deploy without a canary instance. Copy the YAML files you \ncreated earlier, from listings 10.6 and 10.7, to a new deploy/production directory. Feel \nfree to increase the number of replicas you’ll deploy.\nNext, add the code in the next listing to your Jenkinsfile, after the approval stage. \nThis is similar to the code you used in staging. Don’t worry about the code duplication \nfor now — you can work on that in a moment.\nListing 10.10    Production release stage\nstage('Deploy to production') {\n  sh(\"sed -i.bak 's#BUILD_TAG#${tagToDeploy}#' ./deploy/production/*.yml\")\n  container('kubectl') {\n \n",
      "content_length": 2286,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "260\nChapter 10  Building a delivery pipeline for microservices \n    sh(\"kubectl --namespace=production apply -f deploy/production/\")\n  }\n}\nAs always, commit and run the build in Jenkins. If successful, you’ve released to produc-\ntion! Let’s take this a few steps further and add some code to release a canary instance. \nBut before you add a new stage, let’s DRY up your code a little bit. You can move your \nrelease-related code into a separate file called deploy.groovy, as shown in the following \nlisting.\nListing 10.11    deploy.groovy\ndef toKubernetes(tagToDeploy, namespace, \n➥deploymentName) { \n  sh(\"sed -i.bak 's#BUILD_TAG#${tagToDeploy}#' ./deploy/${namespace}/*.yml\")\n  container('kubectl') {\n    kubectl(\"apply -f deploy/${namespace}/\")\n  }\n}\ndef kubectl(namespace, command) { \n  sh(\"kubectl --namespace=${namespace} ${command}\") \n} \ndef rollback(deploymentName) {\n  kubectl(\"rollout undo deployment/${deploymentName}\")\n}\nreturn this;\nThen you can load it in your Jenkinsfile, as shown in the following listing.\nListing 10.12    Using deploy.groovy in your Jenkinsfile\ndef deploy = load('deploy.groovy')\nstage('Deploy to staging') {\n  deploy.toKubernetes(tagToDeploy, 'staging', 'market-data')\n}\nstage('Approve release?') {\n  input \"Release ${tagToDeploy} to production?\"\n}\nstage('Deploy to production') {\n  deploy.toKubernetes(tagToDeploy, 'production', 'market-data')\n}\nThat’s much cleaner. This isn’t the only way to reuse pipeline code — we’ll discuss a \nbetter approach in section 10.3.\nWorks for any namespace and deployment\nPerforms any operations \non Kubernetes\n \n",
      "content_length": 1583,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "\t\n261\nBuilding a pipeline with Jenkins\nNext, create a canary deployment file. If you’ve read through chapter 9, you’ll \nremember that you use a distinct deployment with unique labels to identify this \ninstance. In deploy/canary, create a deployment YAML file like the one you used for \nproduction but with three changes:\n1\t Add a label track: canary to the pod specification.\n2\t Reduce the number of replicas to 1.\n3\t Change the name of the deployment to market-data-canary.\nAfter you’ve added that file, add a new stage to your deployment, as shown in the fol-\nlowing listing, before releasing to production.\nListing 10.13    Canary release stage (Jenkinsfile)\nstage('Deploy canary') {\n  deploy.toKubernetes(tagToDeploy, 'canary', 'market-data-canary')\n  try {\n    input message: \"Continue releasing ${tagToDeploy} to \n➥production?\" \n  } catch (Exception e) {\n    deploy.rollback('market-data-canary') \n  }\n}\nIn this example, we’re assuming human approval for moving from canary to produc-\ntion. In the real world, this might be an automated decision; for example, you could \nwrite code to monitor key metrics, such as error rate, for some time after a canary \ndeploy.\nOnce you’ve committed this code, you should be able to run the whole pipeline. Fig-\nure 10.8 illustrates the full journey of your code to production.\nLet’s take a breather so you can reflect on what you’ve learned:\n¡ You’ve automated the delivery of code from commit to production by using \nJenkins to build a structured deployment pipeline.\n¡ You’ve built different stages to validate the quality of that code and provide \nappropriate feedback to an engineering team.\n¡ You’ve learned about the importance of — and challenges in operating — a stag-\ning environment when developing microservices.\nThese techniques provide a consistent and reliable foundation for delivering code safely \nand rapidly to production. This helps ensure overall stability and robustness in a micro­\nservice application. But it’s far from ideal if every microservice copies and pastes the same \ndeployment code or reinvents the wheel for every new service. In the next section, we’ll \ndiscuss patterns for making deployment approaches reusable across a fleet of services.\nAsks for human input to proceed\nRolls back your canary \nif rollout is aborted\n \n",
      "content_length": 2299,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "262\nChapter 10  Building a delivery pipeline for microservices \nFigure 10.8    Successful deployment pipeline from commit to production release\n10.3\t Building reusable pipeline steps\nMicroservices enable independence and technical homogeneity, but these advantages \ncome at a cost:\n¡ It’s harder for developers to move between teams, as the tech stack can vastly \ndiffer.\n¡ It’s more complex for engineers to reason through the behavior of different \nservices.\n¡ You have to invest more time in different implementations of the same concerns, \nsuch as deployment, logging, and monitoring.\n¡ People may make technical decisions in isolation, risking local, rather than \nglobal, optimization.\nTo balance these risks while maintaining technical freedom and flexibility, you should \naggressively standardize the platform and tooling that services operate on. Doing so \nwill ensure that, even if the technology stack changes, common abstractions remain as \nclose as possible across different services. Figure 10.9 illustrates this approach.\nNOTE    We introduced the platform layer of microservice architecture in chapter 3.\nService A\nDeployment\npipeline\nDeployment platform\nStandardize these elements\nof your application\narchitecture.\nTransport/\ncommunication\nService B\nDeployment\npipeline\nObservability\nObservability\nUnique business logic and\nimplementation within\nservices can diverge.\nFigure 10.9    You can standardize many elements of a microservice application to reduce complexity, \nincrease reuse, and reduce ongoing operational cost.\n \n",
      "content_length": 1541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": "\t\n263\nBuilding reusable pipeline steps\nOver the past few chapters, you’ve applied this thinking in a few areas:\n¡ Using a microservice chassis to abstract common, nonbusiness logic functional-\nity, such as monitoring and service discovery\n¡ Using containers — with Docker — as a standardized service artifact for deployment\n¡ Using a container scheduler — Kubernetes — as a common deployment platform\nYou also can apply this approach to your deployment pipelines.\n10.3.1\t Procedural versus declarative build pipelines\nThe pipeline scripts you’ve written so far have three weaknesses:\n1\t Specific  — They’re tied to a single repository, and another repository can’t share \nthem.\n2\t Procedural  — They explicitly describe how you want the build to be carried out.\n3\t Don’t abstract internals  — They assume a lot of knowledge about Jenkins itself, \nsuch as how you start nodes, run commands, and use command-line tools.\nIdeally, a service deployment pipeline should be declarative: an engineer describes \nwhat they expect to happen (test my service, release it, and so on), and your frame-\nwork decides how to execute those steps. This approach also abstracts away changes \nto how those steps happen: if you want to tweak how a step works, you can change \nthe underlying framework implementation. Abstracting these implementation deci-\nsions away from individual services leads to greater consistency across the microservice \napplication.\nCompare the following script to the Jenkinsfile you wrote earlier in the chapter.\nListing 10.14    Example declarative build pipeline\nservice {\n  name('market-data')\n  stages {\n    build()\n    test(command: 'python setup.py test', results: 'results.xml')\n    publish()\n    deploy()\n  }\n}\nThis script defines some common configuration (service name) and a series of steps \n(build, test, publish, deploy) but hides the complexity of executing those steps from a \nservice developer. This allows any engineer to quickly follow best practice to reliably \nand rapidly take a new service to production.\nWith Jenkins Pipeline, you can implement declarative pipelines using shared librar-\nies. We won’t go into detail in this chapter — not enough pages left! — but this book’s \n \n",
      "content_length": 2208,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "264\nChapter 10  Building a delivery pipeline for microservices \nGithub repository includes an example pipeline library (http://mng.bz/P7hD). In \naddition, the Jenkins documentation (http://mng.bz/p3wz) provides a detailed refer-\nence on using shared libraries.\nNOTE    In other build tools, such as Travis CI or DroneCI, you declare build \nconfiguration using YAML files. These approaches are great, especially if your \nneeds are relatively straightforward. Conversely, building a DSL with a dynamic \nlanguage can offer an extra degree of flexibility and extensibility.\n10.4\t Techniques for low-impact deployment and feature \nrelease\nThroughout the past few chapters, we’ve used the terms deployment and release inter-\nchangeably. But in a microservice application, it’s important to distinguish between \nthe technical activity of deployment — updating the software version running in a pro-\nduction environment — and the decision to release a new feature to customers or con-\nsuming services.\nYou can use two techniques — dark launches and feature flags — to complement \nyour continuous delivery pipeline. These techniques will allow you to deploy new fea-\ntures without impacting customers and provide a flexible mechanism for rollback.\n10.4.1\t Dark launches\nDark launching is the practice of deploying a service to a production environment sig-\nnificantly prior to making it available to consumers. At our company, we practice this \nregularly and try to deploy within the first few days of building a new service, regardless \nof whether it’s feature-complete. Doing this allows us to perform exploratory testing \nfrom an early stage, which helps us understand how a service behaves and makes a new \nservice visible to our internal collaborators.\nIn addition, dark launching to a production environment allows you to test your \nservices against real production traffic. Let’s say that SimpleBank wants to offer a new \nfinancial prediction algorithm as a service. By passing production traffic in parallel with \nthe existing service, they can easily benchmark the new algorithm and understand how \nit performs in the real world, rather than against limited and artificial test scenarios \n(figure 10.10).\nWhether you validate this output manually or automatically depends on the nature of \nthe feature and the volume and distribution of requests required to adequately exhaust \npossible scenarios. The dark launch approach is also useful for testing that refactoring \ndoesn’t regress sensitive functionality.1\n1\t Similarly, the Ruby Scientist gem (https://github.com/github/scientist) was originally designed \nto help Github validate whether refactoring of user permissions caused authorization issues, such \nas users having incorrect access to repositories.\n \n",
      "content_length": 2761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "\t\n265\nTechniques for low-impact deployment and feature release\nAPI gateway\nRequest\nRequest\nRequest\nClients\nBetter\nprediction\nPrediction\nClients access financial prediction\nthrough an API gateway.\nBy changing the gateway,\nyou can pass requests in parallel\nto “dark-launched” services.\nResponse\nAPI gateway\nRequest\nRequest\nClients\nPrediction\nResponse\nResponse\nPrediction\nlogs\nYou can compare the output of these\ndifferent services, for example, by\nexamining logs and analytics.\nFigure 10.10    Dark launches enable validation of new service behavior against real production traffic \nwithout exposing features to customers.\n10.4.2\t Feature flags\nFeature flags control the availability of features to customers. Unlike dark launches, \nyou can use them at any point in the lifecycle of a service, such as a feature release. A \nfeature flag (or toggle) wraps a feature in conditional logic, only enabling it for a cer-\ntain set of users. Many companies will use them to control rollout; for example, only \nreleasing a feature for internal staff first, or progressively increasing the number of \nusers who can access a feature over time.\nSeveral libraries are available to implement feature flags, such as Flipper (http://\ngithub.com/jnunemaker/flipper) or Togglz (http://github.com/togglz/togglz). These \nlibraries typically use a persistent backing store, like Redis, to maintain the state of feature \nflags for an application. In a larger microservice application, you may find it desirable to \nhave a single feature store to synchronize the rollout of features that involve the inter-\naction of multiple services, rather than independently managing features per service. \nFigure 10.11 illustrates these different approaches.\nManaging features per service is likely to be easier in a small microservice system \nthan a larger one. As your system becomes larger, centralizing feature configuration in \na single service reduces coordination overhead if you encounter situations where fea-\nture rollouts necessitate changes in multiple microservices.\n \n",
      "content_length": 2045,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "266\nChapter 10  Building a delivery pipeline for microservices \nApproach 1: Each service owns and maintains\na separate feature store.\nApproach 2: A features service owns all feature\nconfiguration, and other services call it.\nFeature store\nFeature store\nFeature store\nQueries\nQueries\nQueries\nQueries\nQueries\nA\nB\nA\nB\nFeatures\nFigure 10.11    You can store feature flags centrally — owned by one service — or maintain them in \nseparate applications.\nBy controlling which users see a change, feature flags can aid in minimizing the poten-\ntial impact of any change to a system, as you have partial control over code execution \nand feature availability. If errors occur, feature flags often allow for more rapid recov-\nery than typical rollback. For microservices, they can enable safer release of new func-\ntionality without adversely affecting service consumers.\nSummary\n¡ A microservice deployment process should meet two goals: safety at pace and \nconsistency.\n¡ The time it takes to deploy a new service is often a barrier in microservice applications.\n¡ Continuous delivery is an ideal deployment practice for microservices, reducing \nrisk through the rapid delivery of small, validated changesets.\n¡ A good continuous delivery pipeline ensures visibility, correctness, and rich feed-\nback to an engineering team.\n¡ Jenkins is a popular build automation tool that uses a scripting language to tie \nmultiple tools together into a delivery pipeline.\n¡ Staging environments are invaluable but can be challenging to maintain when \nthey face a high volume of independent change.\n¡ You can reuse declarative pipeline steps across multiple services; aggressive stan-\ndardization makes deployment predictable across teams.\n¡ To provide fine-grained control over rollout and rollback, you should manage \nthe technical activity of deployment separately from the business activity of \nreleasing a feature.\n \n",
      "content_length": 1898,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "Part 4\nObservability and ownership\nOnce you’ve deployed your services, you need to know what they’re actu-\nally doing. In this part, you’ll build a monitoring system — using metrics, tracing, \nand logging — to give you rich visibility into your microservice application. After \nthat, we’ll conclude our microservice journey by exploring how this architectural \napproach impacts how developers work together and discussing good day-to-day \npractices for developing microservice applications.\n \n",
      "content_length": 493,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "269\n11\nBuilding a monitoring system\nThis chapter covers\n¡ Understanding what signals to gather from \nrunning applications\n¡ Building a monitoring system to collect metrics\n¡ Learning how to use the collected signals to set \nup alerts\n¡ Observing the behavior of individual services \nand their interactions as a system \nYou’ve now set up an infrastructure to run your services and have deployed multiple \ncomponents that you can combine to provide functionality to your users. In this \nchapter and the next, we’ll consider how you can make sure you’ll always be able to \nknow how those components are interacting and how the infrastructure is behav-\ning. It’s fundamental to know as early as possible when something isn’t behaving as \nexpected. In this chapter, we’ll focus on building a monitoring system so you can \ncollect relevant metrics, observe the system behavior, and set up relevant alerts to \nallow you to keep your systems running smoothly by taking actions preemptively. \n \n",
      "content_length": 986,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "270\nChapter 11  Building a monitoring system\nWhen you can’t be preemptive, you’ll at least be able to quickly pinpoint the areas that \nneed your attention so you can address any issues. It’s also worth mentioning that you \nshould instrument as much as possible. The collected data you may not use today may \nturn out to be useful someday.\n11.1\t A robust monitoring stack\nA robust monitoring stack will allow you to start gathering metrics from your services \nand infrastructure and use those metrics to gather insights from the operation of a sys-\ntem. It should provide a way to collect data and store, display, and analyze it.\nYou should start by emitting metrics from your services, even if you have no monitor-\ning infrastructure in place. If you have those metrics stored, at any time you’ll be able to \naccess, display, and interpret them. Observability is a continuous effort, and monitoring \nis a key element in that effort. Monitoring allows you to know whether a system is work-\ning, whereas observability lets you ask why it’s not working.\nIn this chapter, we’ll be focusing on monitoring, metrics, and alerts. We’ll explain \nlogs and traces in chapter 12, and they’ll constitute the observability component.\nMonitoring doesn’t only allow you to anticipate or react to issues, you can also use \ncollected metrics from monitoring to predict system behavior or to provide data for \nbusiness analytic purposes.\nMultiple open source and commercial options are available for setting up a moni-\ntoring solution. Depending on the team size and resources available, you may find that \na commercial solution may be easier or more convenient to use. Nonetheless, in this \nchapter you’ll be using open source tools to build your own monitoring system. Your \nstack will be made up of a metrics collector and a display and alerting component. Logs \nand traces are also essential to achieve system observability. Figure 11.1 gives an over-\nview of all the components you need to be able to understand your system behavior and \nachieve observability.\nIn figure 11.1, we display the components of a monitoring stack:\n¡ Metrics\n¡ Logs\n¡ Traces\nEach of these components feeds into its own dashboards as an aggregation of data \nfrom multiple services. This allows you to set up automated alerts and look into all the \ncollected data to investigate any issues or better understand system behavior. Metrics \nwill enable monitoring, whereas logs and traces will enable observability.\n11.1.1\t Good monitoring is layered\nIn chapter 3, we discussed the architecture tiers: client, boundary, services, and plat-\nform. You should implement monitoring in all of these layers, because you can’t deter-\nmine the behavior of a given component in total isolation. A network issue will most \nlikely affect a service. If you collect metrics at the service level, the only thing you’ll be \n \n",
      "content_length": 2868,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "\t\n271\nA robust monitoring stack\nable to know is that the service itself isn’t serving requests. That alone tells you nothing \nabout the cause of the issue. If you also collect metrics at the infrastructure level, you \ncan understand problems that’ll most likely affect multiple other components.\nIn figure 11.2, you can see the services that work together to allow a client to place an \norder for selling or buying shares. Multiple services are involved. Some communication \nbetween services is synchronous, either via RPC or HTTP, and some is asynchronous, \nusing an event queue. To be able to understand how services are performing, you need \nto be able to collect multiple data points to monitor and either diagnose issues or pre-\nvent them before they even arise.\nMonitoring individual services will be of little to no use because services provide iso-\nlation but don’t exist isolated from the outside world. Services often depend on each \nother and on the underlying infrastructure (for example, the network, databases, cache \nstores, and event queues). You can get a lot of valuable information by monitoring ser-\nvices, but you need more. You need to understand what’s going on in all your layers. \n2018-02-04T13:55:19.263380548Z\n{\"@timestamp\": \"2018-02-04T13:55:19.262Z\",\n\"@version\": 1, \"source_host\": \"bb69db21f1eb\",\n\"name\": \"root\", \"args\": [], \"levelname\": \"INFO\",\n\"levelno\": 20, \"pathname\": \"./app.py\", \"filename\":\n\"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152,\nServices\nMetrics\nAlerts\nMetrics\nMetrics\nMetrics\nTraces\nLogs\nCentralized logging\nTraces\nTraces\nTraces\n2018-02-04T13:55:19.263380548Z\n{\"@timestamp\": \"2018-02-04T13:55:19.262Z\",\n\"@version\": 1, \"source_host\": \"bb69db21f1eb\",\n\"name\": \"root\", \"args\": [], \"levelname\": \"INFO\",\n\"levelno\": 20, \"pathname\": \"./app.py\", \"filename\":\n\"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152,\n2018-02-04T13:55:19.263380548Z\n{\"@timestamp\": \"2018-02-04T13:55:19.262Z\",\n\"@version\": 1, \"source_host\": \"bb69db21f1eb\",\n\"name\": \"root\", \"args\": [], \"levelname\": \"INFO\",\n\"levelno\": 20, \"pathname\": \"./app.py\", \"filename\":\n\"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152,\n2\nFigure 11.1    Components of a monitoring stack — metrics, traces, and logs — each aggregated in their \nown dashboards\n \n",
      "content_length": 2441,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "272\nChapter 11  Building a monitoring system\nGateway\nOrder\nservice\nMarket\nservice\nAMQP\n(async)\nFee\nservice\nRPC\n(sync)\nAMQP\n(async)\nAMQP\n(async)\nAMQP\n(async)\nAMQP\n(async)\nRPC\n(sync)\nAccount\ntransaction\nservice\nOrder\ncreated\nOrder\nplaced\nEvent queue\nFigure 11.2    Services involved in placing orders and their communication protocols\nYour monitoring solution should allow you to know what is broken or degrading and \nwhy. You’ll be able to quickly reveal any symptoms and use the available monitors to \ndetermine causes.\nReferring to figure 11.2, it’s worth mentioning that symptoms and causes vary \ndepending on the observation point. If the market service might be having issues com-\nmunicating with the stock exchange, you can diagnose that by measuring response \ntimes or HTTP status codes for that interaction. In that situation, you’ll be almost sure \nthat the place order feature won’t be working as expected.\nBut what if you have an issue with connectivity from services to the event queue? Ser-\nvices won’t be publishing messages, so downstream services won’t be consuming them. \nIn that situation, no service is failing because no service is performing any work. If you \nhave proper monitoring in place, it can alert you to the abnormal decrease in through-\nput. You can set your monitoring solution to send you automated notifications when \nthe number of messages in a given queue goes below a certain threshold.\nLack of messages isn’t the only thing that can indicate issues, though. What if you \nhave messages accumulating in a given queue? Such accumulation may indicate the \nservices that consume messages from the queue are either not working properly or are \nhaving trouble keeping up with increased demand. Monitoring allows you to identify \nissues or even predict load increases and act accordingly to maintain service quality. \nLet’s take some time for you to learn a bit more about the signals you should collect.\n11.1.2\t Golden signals\nYou should focus on four golden signals while collecting metrics from any user-facing \nsystem: latency, errors, traffic, and saturation.\nLatency\nLatency measures how much time passes between when you make a request to a given \nservice and when the service completes the request. You can determine a lot from this \nsignal. For example, you can infer that the service is degrading if it shows increasing \n \n",
      "content_length": 2362,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "\t\n273\nA robust monitoring stack\nlatency. You need to take extra care, though, in correlating this signal with errors. \nImagine you’re serving a request and the application responds quickly but with an \nerror? Latency has a low value in this case, but the outcome isn’t the desired one. It’s \nimportant to keep the latency of requests that result in errors out of this equation, \nbecause it can be misleading.\nErrors\nThis signal determines the number of requests that don’t result in a successful out-\ncome. The errors may be explicit or implicit — for example, having an HTTP 500 error \nversus having an HTTP 200 but with the wrong content. The latter isn’t trivial to mon-\nitor for because you can’t rely solely on the HTTP codes, and you may only be able to \ndetermine the error by finding wrong content in other components. You generally \ncatch these errors with end-to-end or contract tests.\nTraffic\nThis signal measures the demand placed on a system. It can vary depending on the type \nof system being observed, the number of requests per second, network I/O, and so on.\nSaturation\nAt a given point, this measures the capacity of the service. It mainly applies to resources \nthat tend to be more constrained, like CPU, memory, and network.\n11.1.3\t Types of metrics\nWhile collecting metrics, you need to determine the type that’s best suited for a given \nresource you’re aiming to monitor.\nCounters\nCounters are a cumulative metric representing a single numerical value that’ll always \nincrease. Examples of metrics using counters are:\n¡ Number of requests\n¡ Number of errors\n¡ Number of each HTTP code received\n¡ Bytes transmitted\nYou shouldn’t use a counter if the metric it represents can also decrease. For that, you \nshould use a gauge instead.\nGauges\nGauges are metrics representing single numerical arbitrary values that can go up or \ndown. Some examples of metrics using gauges are:\n¡ Number of connections to a database\n¡ Memory used\n¡ CPU used\n¡ Load average\n¡ Number of services operating abnormally\n \n",
      "content_length": 2017,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "274\nChapter 11  Building a monitoring system\nHistograms\nYou use histograms to sample observations and categorize them in configurable buck-\nets per type, time, and so on. Examples of metrics represented by histograms are:\n¡ Latency of a request\n¡ I/O latency\n¡ Bytes per response\n11.1.4\t Recommended practices\nAs we already mentioned, you should make sure you instrument as much as possible \nto collect as much data as you can about your services and infrastructure. You can use \nthe collected data at later stages once you devise new ways to correlate and expose it. \nYou can’t go back in time to collect data, but you can make data available that you pre-\nviously collected. \nKeep in mind that you should go about representing that data, showing it in dash-\nboards, and setting up alerts in a progression to avoid having too much information \nat once that will be hard to reason through. There is no point in throwing every single \ncollected metric for a service into one dashboard. You can create several dashboards \nper service with detailed views, but keep one top-level dashboard with the most import-\nant information. This dashboard should allow you, in a glance, to determine if a ser-\nvice is operating properly. It should give a high-level view of the service, and any more \nin-depth information should appear in more specialized dashboards.\nWhen representing metrics, you should focus on the most important ones, like \nresponse times, errors, and traffic. These will be the foundation of your observabil-\nity capabilities. You also should focus on the right percentiles for each use case: 99th, \n95th, 75th, and so on. For a given service, it may be good enough if only 95% of your \nrequests take less than x seconds, whereas on another service you may require 99% of \nthe requests to be below that time. There is no fixed rule for which percentile to focus \non — that generally depends on the business requirements.\nWhenever possible, you should use tags to provide context to your metrics. Examples \nof tags to associate with metrics are:\n¡ Environment: Production, Staging, QA\n¡ User ID\nBy tagging metrics, you can group them later on and perhaps come up with some more \ninsights. Take, for example, a response time you’ve tagged with the User ID; you can \ngroup the values by user and determine if all of the user base or only a particular group \nof users experiences an increase in response times. \nMake sure you always abide by some defined standards when you’re naming metrics. \nIt’s important that you maintain a naming scheme across services. One possible way of \n \n",
      "content_length": 2586,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "\t\n275\nMonitoring SimpleBank with Prometheus and Grafana\nnaming metrics is to use the service name, the method, and the type of metric you wish \nto collect. Here are some examples:\n¡ orders_service.sell_shares.count\n¡ orders_service.sell_shares.success\n¡ fees_service.charge_fee.failure\n¡ account_transactions_service. request_reservation.max\n¡ gateway.sell_shares.avg\n¡ market_service.place_order.95percentile\n11.2\t Monitoring SimpleBank with Prometheus and Grafana\nYou need to send the metrics you collect from your services and infrastructure to a \nsystem capable of aggregating and displaying them. The system will use those collected \nmetrics to provide alerting capabilities. For that purpose, you’ll be using Prometheus \nto collect metrics and Grafana to display them:\n¡ Prometheus (https://github.com/prometheus) is an open source systems mon-\nitoring and alerting toolkit originally built at SoundCloud. It’s now a standalone \nopen source project and is maintained independent of any company.\n¡ Grafana (https://grafana.com) is a tool that allows building dashboards on top \nof multiple metrics data sources, such as Graphite, InfluxDB, and Prometheus.\nYou’ll do all your setup using Docker. In chapter 7, you already added to your services \nthe ability to emit metrics via StatsD. You’ll keep those services unchanged and add \nsomething to your setup to convert metrics from StatsD format to the format that \nPrometheus uses. You’ll also add a RabbitMQ container that’s already set up to send \nmetrics to Prometheus. Figure 11.3 shows the components you’ll be adding to set up \nyour monitoring system.\nService\nPrometheus\nGrafana\nStatsD server\nMetrics\nMetrics\nMetrics\nMetrics\nStatsD exporter\nEvent queue\n \nFigure 11.3    The containers you need to build your monitoring system: StatsD server, StatsD exporter, \nPrometheus, and Grafana\n \n",
      "content_length": 1845,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "276\nChapter 11  Building a monitoring system\nYou’ll be using both Prometheus and StatsD metrics as a way to show how two types of \nmetrics collection protocols can coexist. StatsD is a push-based tool, whereas Prometheus \nis a pull-based tool. Systems using StatsD will be pushing data to a collector service, \nwhereas Prometheus will pull that data from the emitting systems.\n11.2.1\t Setting up your metric collection infrastructure\nYou’ll start by adding the services described in figure 11.2 to the Docker compose file, \nthen you’ll focus on configuring both the StatsD exporter and Prometheus. The last \nstep will be to create the dashboards in Grafana and start monitoring the services and \nthe event queue. All the code is available in the book’s code repository.\nAdding components to the Docker compose file\nThe Docker compose file (see the next listing) will allow you to boot all the services \nand infrastructure needed for the place order feature. For the sake of brevity, we’ll \nomit the individual services and will only list the infrastructure- and monitoring-re-\nlated containers.\nListing 11.1    docker-compose.yml file\n(…)\nrabbitmq: \n    container_name: simplebank-rabbitmq\n    image: deadtrickster/rabbitmq_prometheus\n    ports:\n      - \"5673:5672\"\n      - \"15673:15672\"\n  redis:\n    container_name: simplebank-redis\n    image: redis\n    ports:\n      - \"6380:6379\"\n  statsd_exporter: \n    image: prom/statsd-exporter\n    command: \"-statsd.mapping-config=/tmp/\n➥statsd_mapping.conf\" \n    ports:\n      - \"9102:9102\"\n      - \"9125:9125/udp\"\n    volumes:\n      - \"./metrics/statsd_mapping.conf:/tmp/statsd_mapping.conf\"\n  prometheus: \n    image: prom/prometheus\n    command: \"--config.file=/tmp/prometheus.yml \n➥--web.listen-address '0.0.0.0:9090'\" \n    ports:\nYou’ll use RabbitMQ as the event \nqueue. The image used here is already \nemitting metrics in the Prometheus \nformat, so you can connect it directly.\nFetches the metrics sent to the StatsD server \nand converts them to Prometheus format, so \nPrometheus can fetch them afterwards\nStarts statsd_exporter with a custom command \nthat’ll load the mapping configuration\nSets up the official Prometheus image\nAllows you to start Prometheus, \nbinding it to 0.0.0.0:9000 and \nreading a custom configuration \nfile that you’ll soon see in a bit \nmore detail\n \n",
      "content_length": 2321,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "\t\n277\nMonitoring SimpleBank with Prometheus and Grafana\n      - \"9090:9090\"\n    volumes:\n      - \"./metrics/prometheus.yml:/tmp/prometheus.yml\"\n  statsd: \n    image: dockerana/statsd\n    ports:\n      - \"8125:8125/udp\"\n      - \"8126:8126\"\n    volumes:\n      - \"./metrics/statsd_config.js:/src/statsd/\n➥config.js\" \n  grafana: \n    image: grafana/grafana\n    ports:\n      - \"3900:3000\"\nConfiguring StatsD exporter\nAs we mentioned before, the services involved in the place order feature emit metrics \nin the StatsD format. In table 11.1, we list all the services and the metrics each one \nemits. The services will all be emitting timer metrics.\nTable 11.1    Timer metrics emitted by the services involved in placing an order\nService\nMetrics\nAccount transactions \nrequest_reservation\nFees\ncharge_fee\nGateway\nhealth, sell_shares, \nMarket\nrequest_reservation, place_order_stock_exchange \nOrders\nsell_shares, request_reservation, place_order\nThe mapping config file allows you to configure each metric that StatsD collects and \nadd labels to it. The following listing provides the mapping you’ll create as a configura-\ntion file for the statsd-exporter container.\nListing 11.2    Configuration file to map StatsD metrics to Prometheus \nsimplebank-demo.account-transactions.request_reservation \nname=\"request_reservation\" \nSets the StatsD server that’ll collect \nmetrics that the services send\nUses a custom configuration to allow repeating the received metrics \nto the statsd-exporter container — As with the Prometheus and \nstatsd-exporter containers, the configuration files are located in the \nmetrics folder. This folder will be mounted as a volume so the \ncontainers can pick up these configurations at runtime.\nStarts Grafana, which will provide \na UI for the collected metrics\nAccount transactions service mapping\nSets the name of the metric in Prometheus\n \n",
      "content_length": 1859,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "278\nChapter 11  Building a monitoring system\napp=\"account-transactions\" \njob=\"simplebank-demo\" \nsimplebank-demo.fees.charge_fee \nname=\"charge_fee\"\napp=\"fees\"\njob=\"simplebank-demo\"\nsimplebank-demo.gateway.health \nname=\"health\"\napp=\"gateway\"\njob=\"simplebank-demo\"\nsimplebank-demo.gateway.sell_shares\nname=\"sell_shares\"\napp=\"gateway\"\njob=\"simplebank-demo\" \nsimplebank-demo.market.request_reservation \nname=\"request_reservation\"\napp=\"market\"\njob=\"simplebank-demo\"\nsimplebank-demo.market.place_order_stock_exchange\nname=\"place_order_stock_exchange\"\napp=\"market\"\njob=\"simplebank-demo\" \nsimplebank-demo.orders.sell_shares \nname=\"sell_shares\"\napp=\"orders\"\njob=\"simplebank-demo\"\nsimplebank-demo.orders.request_reservation\nname=\"request_reservation\"\napp=\"orders\"\njob=\"simplebank-demo\"\nsimplebank-demo.orders.place_order\nname=\"place_order\"\napp=\"orders\"\njob=\"simplebank-demo\" \nIf you don’t map the above metrics to Prometheus, they’ll still get collected, but the \nway they’ll be collected is less convenient. In the figure 11.4 example, you can see the \ndifference between mapped and unmapped metrics fetched from Prometheus from \nthe statsd_exporter service.\nAllows differentiating between metrics that have the same \nname (For example, request_reservation is used as a name \nfor a metric in both the orders and market services.)\nUsed to determine the \ncollector in statsd_exporter\nFees service mapping\nGateway mapping\nMarket service mapping\nOrders service mapping\n \n",
      "content_length": 1457,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": "\t\n279\nMonitoring SimpleBank with Prometheus and Grafana\nFigure 11.4    Prometheus screenshot with collected SimpleBank metrics — The top two metrics aren’t \nmapped in the statsd_mapping.conf file, whereas the last one is.\nAs you can observe in figure 11.4, when the unmapped create_event metrics that \nboth the market and orders service emit reach Prometheus, they’re collected as:\n¡ simplebank_demo_market_create_event_timer\n¡ simplebank_demo_orders_create_event_timer\nFor the request_reservation_timer metric that the market, orders, and account \ntransactions services emit, there’s only one entry, the metric is the same, and the differ-\nentiation is in the metadata:\nrequest_reservation_timer{app=\"*\",exported_job=\"simplebank-demo\",e\nxporter=\"statsd\",instance=\"statsd-exporter:9102\",job=\"statsd_\nexporter\",quantile=\"0.5\"} \nrequest_reservation_timer{app=\"*\",exported_job=\"simplebank-demo\",e\nxporter=\"statsd\",instance=\"statsd-exporter:9102\",job=\"statsd_\nexporter\",quantile=\"0.9\"}\nMetric mapped in the statsd_exporter configuration file — The app label takes \nthe values off all apps generating the request_reservation_timer metric.\n \n",
      "content_length": 1136,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "280\nChapter 11  Building a monitoring system\nrequest_reservation_timer{app=\"*\",exported_job=\"simplebank-demo\",e\nxporter=\"statsd\",instance=\"statsd-exporter:9102\",job=\"statsd_\nexporter\",quantile=\"0.99\"}\nsimplebank_demo_market_create_event_timer{exporter=\"statsd\",instance=\"statsd-\nexporter:9102\",job=\"statsd_exporter\",quantile=\"0.5\"} \nConfiguring Prometheus\nNow that you’ve configured the StatsD exporter, it’s time to configure Prometheus for \nit to fetch data from both the StatsD exporter and RabbitMQ, as shown in the following \nlisting. Both of these sources will be available as targets for metrics data fetching.\nListing 11.3    Prometheus configuration file\nglobal:\n  scrape_interval:     5s \n  evaluation_interval: 10s\n  external_labels:\n      monitor: 'simplebank-demo'\nalerting:\n  alertmanagers:\n  - static_configs:\n    - targets:\nscrape_configs: \n  - job_name: 'statsd_exporter' \n    static_configs:\n      - targets: ['statsd-exporter:9102']\n        labels:\n          exporter: 'statsd'\n    metrics_path: '/metrics'\n  - job_name: 'rabbitmq'\n    static_configs:\n      - targets: ['rabbitmq:15672'] \n        labels:\n          exporter: 'rabbitmq'\n    metrics_path: '/api/metrics' \nSetting up Grafana\nTo receive metrics in Grafana, you need to set up a data source. First, you can boot your \napplications and infrastructure by using the Docker compose file. This will allow you to \naccess Grafana on port 3900, as follows.\nMetric not mapped in the statsd_exporter configuration \nfile — There’s no app and no exported_job labels.\nSets the interval at which Prometheus will \nscrape the configured targets for metrics\nScrapes config section where each target \nis configured\nWill be added as a label any time  \nseries scraped from the config\nThe target host and metrics path will be \nconcatenated to determine the URL to \ncollect metrics from. Given that in this case \nthe scheme defaults to http, the URL will be \n'http://rabbitmq:15672/api/metrics'.\n \n",
      "content_length": 1957,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": "\t\n281\nMonitoring SimpleBank with Prometheus and Grafana\nListing 11.4    Grafana setup in the docker-compose.yml file\n  (...)\n  grafana:\n    image: grafana/grafana \n    ports:\n      - \"3900:3000\" \nTo start all applications and services using Docker compose, you need to get inside the \nfolder containing the compose file and issue the up command:\nchapter-11$ docker stop $(docker ps | grep simplebank | \n➥awk '{print $1}')\nchapter-11$ docker rm $(docker ps -a | grep simplebank | \n➥awk '{print $1}')\nchapter-11$ docker-compose up --build --remove-orphans  \nStarting simplebank-redis ...\nStarting chapter11_statsd-exporter_1 ...\nStarting chapter11_statsd_1 ...\nStarting simplebank-rabbitmq ...\nStarting chapter11_prometheus_1 ...\nStarting simplebank-rabbitmq ... done\nStarting simplebank-gateway ...\nStarting simplebank-fees ...\nStarting simplebank-orders ...\nStarting simplebank-market\nStarting simplebank-account-transactions ... done\nAttaching to chapter11_prometheus_1, simplebank-redis, chapter11_statsd_1, \nsimplebank-rabbitmq, chapter11_statsd-exporter_1, simplebank-gateway, \nsimplebank-fees, simplebank-orders, simplebank-market, simplebank-\naccount-transactions\n(…)\nThe output of the docker-compose up command will allow you to understand when \nall services and applications are ready. You can reach applications using the URL \nassigned to Docker or the IP address. By appending port 3900 as configured in the \ndocker-compose.yml file, you can access Grafana’s login screen as shown in figure 11.5. \nYou’ll be accessing Grafana using the default login credentials: username and pass-\nword are both admin.\nUses the official grafana Docker image with default settings\nGrafana uses port 3000 by default. The applications and services you start \nvia the compose file will be able to communicate using the default port. \nYou’re mapping it to port 3900 to access it from the host machine.\nStops all SimpleBank running containers\nRemoves all SimpleBank \ncontainers so you don’t \nhave any name clashes\nStarts the containers defined in the \ndocker-compose.yml file — The --build \noption builds images before starting the \ncontainers, and the --remove-orphans \noption removes any containers that \naren’t defined in the compose file.\n \n",
      "content_length": 2233,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "282\nChapter 11  Building a monitoring system\nFigure 11.5    Grafana login screen\nOnce you log in, you’ll have an Add Data Source option. Figure 11.6 shows the data \nsource configuration screen, Edit Data Source. To configure a Prometheus data source \nin Grafana, you need to select Prometheus as the type and insert the URL of the run-\nning Prometheus instance, in your case http://prometheus:9090, as configured in the \nDocker compose file.\nThe Save & Test button will give you instant feedback on the data source status. Once \nit’s working, you’re ready to use Grafana to build dashboards for your collected metrics. \nIn the next few sections, you’ll be using it to display metrics both for the services that \nenable the place orders functionality in SimpleBank and for monitoring a critical piece \nof the infrastructure, RabbitMQ, the event queue.\n11.2.2\t Collecting infrastructure metrics — RabbitMQ\nTo set up the dashboard to monitor RabbitMQ, you’ll be using a json configuration \nfile. This is a convenient and easy way to share dashboards. In the source code repos-\nitory, you’ll find a grafana folder. Inside that, a RabbitMQ Metrics.json file holds the \n \n",
      "content_length": 1166,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "\t\n283\nMonitoring SimpleBank with Prometheus and Grafana\nconfiguration for both the dashboard layout and the metrics you want to collect. You \ncan now import that file to have your RabbitMQ monitoring dashboard up in no time!\nFigure 11.6    Configuring a Prometheus data source in Grafana\nFigure 11.7 shows how you can access the import dashboard functionality in Grafana. \nBy clicking Grafana’s logo, you bring up a menu; if you hover over Dashboards, the \nImport option will be available.\nThe import option will bring up a dialog box that enables you to either paste the json \nin a text box or upload a file. Before you can use the imported dashboard, you need to \nconfigure the data source that’ll feed the dashboard. In this case, you’ll be using the \nSimpleBank data source you configured previously.\nThat’s all it takes to have your RabbitMQ dashboard up and running. In figure 11.8 \nyou can see how it looks.\n \n",
      "content_length": 917,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "284\nChapter 11  Building a monitoring system\nFigure 11.7    Importing a dashboard from a json file\nFigure 11.8    RabbitMQ metrics collected via Prometheus and displayed in Grafana\n \n",
      "content_length": 183,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": "\t\n285\nMonitoring SimpleBank with Prometheus and Grafana\nYour RabbitMQ dashboard provides you an overview of the system by displaying a mon-\nitor for the server status that shows if it’s up or down, along with graphs for Exchanges, \nChannels, Consumers, Connections, Queues, Messages per Host, and Messages per \nQueue. You can hover over any graph to display details for metrics at a point in time. \nClicking the graph’s title will bring up a context menu that allows you to edit, view, \nduplicate, or delete it.\n11.2.3\t Instrumenting SimpleBank’s place order\nNow that you have services up and running, along with the monitoring infrastructure, \nPrometheus and Grafana, it’s time to collect the metrics described in table 11.1. You \ncan start by loading a dashboard exported as json that you can find in the source direc-\ntory under the grafana folder (Place Order.json). Follow the same instructions as the \nones in 11.2.2 for the RabbitMQ dashboard.\nFigure 11.9 displays the dashboard collecting metrics for the services involved in the \nplace order feature. By clicking on each of the panel titles, you can view, edit, duplicate, \nshare, and delete each of the panels.\nThis loaded dashboard collects the time metrics and displays the 0.5, 0.9, and 0.99 \nquantiles for each metric. In the top right corner, you find the manual refresh button \nas well as the period for displaying metrics. By clicking the Last 5 Minutes label, you \ncan select another period for displaying metrics, as shown in figure 11.10. You can \nselect one of the Quick Ranges values or create a custom one, and you can display \nstored metrics in any range you need.\nFigure 11.9    Place order dashboard accessible at Grafana’s /dashboard/db/place-order endpoint\n \n",
      "content_length": 1737,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "286\nChapter 11  Building a monitoring system\nFigure 11.10    Selecting the time range for which you want metrics to be displayed\nLet’s focus on the Market | Place Order Stock Exchange panel to see in detail how you can \nconfigure a specific metric display. To do so, click the panel title and then select the Edit \noption. Figure 11.11 shows the edit screen for the Market | Place Order Stock Exchange.\nThe edit screen has a set of tabs (1) you can select to configure different options. The \nhighlighted one is the Metrics tab, where you can add and edit metrics to be displayed. In \nthis particular case, you’re only collecting a metric (2), namely the place_order_stock_\nexchange_timer that gives you the time it took for the market service to place an order \ninto the stock exchange. The default display for a metric contains metadata like the app \nname, the exported job, and the quantile. To change the way the legend is presented, \nyou set a Legend Format (3). In this case, you set the name and use {{quantile}} block \nthat’ll be interpolated to display the quantile in both the graph legend and the hover-\ning window next to the vertical red line. (The red line acts as a cursor when you move \nyour mouse across the collected metrics.) In your dashboards, you’re displaying the min, \nmax, avg, and current values for each quantile (4).\nFigure 11.11    Panel edit screen for Market | Place Order Stock Exchange\n \n",
      "content_length": 1421,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "\t\n287\nMonitoring SimpleBank with Prometheus and Grafana\nFURTHER READING    To learn more about how to use Prometheus with Grafana \nand about the Prometheus data model, visit the following documentation \npages: http://mng.bz/ui3b and http://mng.bz/PZQ0.\nThe dashboard you’ve set up is quite simple, but it allows you to have an overview \nof how the system is behaving. You’re able to collect time-related metrics for several \nactions that services in your system perform.\n11.2.4\t Setting up alerts\nNow that you’re collecting metrics and storing them, you can set up alerts for when \nvalues deviate from what you consider as normal for a given metric. This can be an \nincrease in the time taken to process a given request, an increase in the percent of \nerrors, an abnormal variation in a counter, and so on.\nIn your case, you can consider the market service and set up an alert for knowing \nwhen the service needs to be scaled. Once you place a sell order via the gateway service, \na lot goes on. Multiple events are fired, and you know the bottleneck tends to be the \nmarket service processing the place order event. The good thing is you can set up an \nalert to send a message whenever messages in the market place order queue go above a \ncertain threshold. You can configure multiple channels for notifications: email, slack, \npagerduty, pingdom, webhooks, and so on.\nYou’ll be setting up a webhook notification to receive a message in your alert server \nevery time the number of messages goes above 100 in any message queue. For now, \nyou’ll only be receiving it in an alert service made with the purpose of illustrating the \nfeature. But you could easily change this service to trigger an increase in the number of \ninstances of a given service to increase the capacity to process messages from a queue.\nThe alert service is a simple app that also booted when you started all other apps and \nservices. It’ll be listening for incoming POST messages, so you can go ahead and con-\nfigure the alerts in Grafana. Figure 11.12 shows the activity for the market place order \nevent queue with indication of alerts, both when they were triggered and when the alert \ncondition ceased. When you set up alerts, Grafana will indicate as an overlay both the \nthreshold set for alerting (1) and the instants when alerts were triggered (2, 4, 6) and \nresolved (3, 5, 7).\nWith the current setup, the alert service sends an alert message as a webhook when \nthe number of messages in any queue goes over 100. The following shows you one of \nthose alert messages:\nalerts.alert.d26ab4ca-1642-445f-a04c-41adf84145fd: \n{\n  \"evalMatches\": [\n    {\n      \"value\":158.33333333333334, \n      \"metric\":\"evt-orders_service-order_created\n➥--market_service.place_order\", \n      \"tags\":{\n        \"__name__\":\"rabbitmq_queue_messages\",\n        \"exporter\":\"rabbitmq\",\nValue for the metric at the time of the alert\nThe name of the metric for \nwhich the alert was triggered\n \n",
      "content_length": 2942,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": "288\nChapter 11  Building a monitoring system\n        \"instance\":\"rabbitmq:15672\",\n        \"job\":\"rabbitmq\",\n        \"queue\":\"evt-orders_service-order_created\n➥--market_service.place_order\", \n        \"vhost\":\"/\"\n      }\n    }\n  ],\n  \"message\":\"Messages accumulating in the queue\",\n  \"ruleId\":1,\n  \"ruleName\":\"High number of messages in a queue\", \n  \"ruleUrl\":\"http://localhost:3000/dashboard/db/rabbitmq-metrics?fullscreen\\\n➥u0026edit\\u0026tab=alert\\u0026panelId=2\\u0026orgId=1\",\n  \"state\":\"alerting\", \n  \"title\":\"[Alerting] High number of messages in a queue\"\n}\nFigure 11.12    The message queue’s status showing alert overlays\nShows information about the queue \nwhere the alert was triggered\nIdentifies what rule \nthis alert is related to\nIndicates the message type — In this case, \"alerting\" means \nan alert was triggered and the number of messages in the \nqueue is above the normal operating threshold.\n \n",
      "content_length": 908,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 316,
      "content": "\t\n289\nMonitoring SimpleBank with Prometheus and Grafana\nLikewise, when the number of messages in a queue goes below the value defined as the \nthreshold for alerting, the service also issues a message to notify about it:\nalerts.alert.209f0d07-b36a-43f4-b97c-2663daa40410: \n{\n  \"evalMatches\":[],\n  \"message\":\"Messages accumulating in the queue\",\n  \"ruleId\":1,\n  \"ruleName\":\"High number of messages in a queue\", \n  \"ruleUrl\":\"http://localhost:3000/dashboard/db/rabbitmq-metrics?fullscreen\\\n➥u0026edit\\u0026tab=alert\\u0026panelId=2\\u0026orgId=1\",\n  \"state\":\"ok\",\n  \"title\":\"[OK] High number of messages in a queue\"\n}\nLet’s now see how you can set up this alert for the number of messages in queues. \nYou’ll also be using Grafana for setting up the alert, because it offers this capability and \nthe alerts will display on the panels they relate to. You’ll be able to both receive notifi-\ncations and check the panels for previous alerts.\nYou’ll start by adding a notification channel that’ll you’ll use to propagate alert \nevents. Figure 11.13 shows how to create a new notification channel\nTo set up a new notification channel in Grafana, follow these steps:\n1\t Click the Grafana icon on the top left of the screen.\n2\t Under the Alerting menu, select Notification Channels.\n \nFigure 11.13    Setting up a new notification channel in Grafana\nIdentifies the rule the \nalert is related to\nThe \"ok\" state means the number of messages in the queues are all back to \nbelow the set threshold; the conditions for a previous alert are no longer met.\n \n",
      "content_length": 1539,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "290\nChapter 11  Building a monitoring system\n3\t Enter the name for the channel and select the type as Webhook, then check the \nSend on All Alerts option.\n4\t Enter the URL for the service receiving the alerts. In your case, you’ll be using \nthe alerts service and listening for POST requests.\n5\t Click the Send Test button to verify all is working, and if so, click Save to save the \nchanges.\nNow that you have an alert channel set up, you can go ahead and create alerts on your \npanels. You’ll be setting an alert on the messages queue panel under the RabbitMQ \ndashboard you created previously. Clicking the Messages/Queue panel title will bring \nup a menu where you can select Edit. This allows you to create a new alert under the \nAlert tab. Figure 11.14 shows how to set up a new alert.\nUnder the Alert Config screen, start by adding the Name for the alert as well as the fre-\nquency at which you want the condition to be evaluated — in this case every 30 seconds. \nThe next step is to set the Conditions for the alert. You’ll be setting an alert to notify you \nwhenever the average of the values collected from query A is above 100 in the last minute.\nTIP    If you click the Metrics tab, you can see query A under it, which will be the \nrabbitmq_queue_messages metric. You also have the option to test the rules \nyou set by clicking the Test Rule button.\nUnder the Alert tab, you also can check the history of the configured alerts. Fig-\nure 11.15 shows the alert history for the number of messages in queues.\nFigure 11.14    Setting up alerts on the Messages/Queue graph on the RabbitMQ Dashboard\n \n",
      "content_length": 1606,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "\t\n291\nRaising sensible and actionable alerts\nFigure 11.15    Displaying the state history for a given alert\nThat’s it, you’re done! You’ve set up a monitoring infrastructure to collect both metrics \nthat your services already emitted and those that come from a key component that \nthose services use to communicate asynchronously: the event queue. You’ve also seen \nhow to create alerts to be notified whenever certain conditions in your system are met. \nLet’s now dig a bit deeper into alerts and how to use them.\n11.3\t Raising sensible and actionable alerts\nHaving a monitoring infrastructure in place means you can measure system perfor-\nmance and keep a historic record of those measures. It also means you can determine \nthresholds for your measures and automatically emit notifications when those thresh-\nolds are exceeded.\nOne thing you need to keep in mind, though, is that it’s easy to reach a stage \nwhere all this information can become overwhelming. Eventually, the overload of \ninformation can do more harm than good (for example, if it gets so bad that peo-\nple start ignoring recurring alerts). You need to make sure that the alerts you raise \nare actionable — and actioned — and that they’re targeting the correct people in the \norganization.\n \n",
      "content_length": 1261,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "292\nChapter 11  Building a monitoring system\nAlthough services may consume and take action on some of the alerts automatically; \nfor example, autoscaling a service if messages are accumulating in a queue, humans \nneed to consume and take action on some alerts. You want those alerts to reach the \ncorrect people and contain enough information so that diagnosing the cause becomes \nas easy as possible.\nYou also need to prioritize alerts, because most likely any issue with your services or \ninfrastructure will trigger multiple alerts. Whoever is dealing with those alerts needs \nto know immediately the urgency of each one. As a rule, you should direct alerts for \nservices to the teams owning those services. You should map the application into the \norganization, because this helps with determining the targets for alerts.\n11.3.1\t Who needs to know when something is wrong?\nIn day-to-day operation, alerts should target the team who owns the service and orig-\ninated it. This reflects the “you build it, you run it” mantra that should govern a \nmicroservices-oriented engineering team. As teams create and deploy services, it’s \nhard, if not impossible, for everyone to know about every service deployed. People \nwith the most knowledge about a service will be in the best position to interpret and \ntake action in response to alerts that the service generates.\nOrganizations also may have some on-call rotation or a dedicated team that’ll receive \nand monitor alerts and then escalate if necessary to specialized teams. When setting up \nalerts and notifications, it’s important to keep in mind that other people may consume \nthem, so you should keep those alerts as concise and informative as possible. It’s also \nimportant that each service have some sort of documentation on common issues and \ndiagnosing recipes so that on-call teams can, when they receive an alert, determine if \nthey can fix the issue or if they need to escalate it.\nYou also should categorize alerts by levels of urgency. Not every issue will need imme-\ndiate attention, but some are deal breakers that you need to address as soon as you \nknow about them.\nSevere issues should trigger a notification to ensure someone, either an engineer \nfrom the team that built the service or an on-call engineer, is notified. Issues that are \nof moderate severity should generate alerts as notifications in any channels deemed \nappropriate, so those monitoring them can pick them up. You can think of this type \nof alert as something generating a queue of tasks that you need to carry out as soon as \npossible but not immediately — they don’t need to interrupt someone’s flow or wake \nsomeone up in the middle of the night. The lowest priority alerts are those that only \ngenerate a record. These alerts aren’t strictly for human consumption, because services \ncan receive them and take some kind of action if needed (for example, autoscaling a \nservice when response times increase).\n11.3.2\t Symptoms, not causes \nSymptoms, not causes, should trigger alerts. An example of this is a user-facing error; if \nusers can no longer access a service, that inability should generate an alert. You shouldn’t \nbe tempted to trigger alerts for every single parameter that isn’t under the normal \n \n",
      "content_length": 3252,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "\t\n293\nObserving the whole application\nthreshold. With such partial information, you won’t be able to know what’s going on or \nwhat the problem is. In figure 11.2, we illustrated the flow for placing orders in the stock \nmarket. Four services cooperate with a gateway that works as the access point for the con-\nsumer of the feature. One or more of the services may be exhibiting erroneous behavior \nor be overloaded. Given the mainly asynchronous nature of the communication between \ncomponents, it may be hard to pinpoint why a given error may be happening.\nImagine you set an alert that relates the number of requests reaching the gateway \nand the number of issued notifications of orders placed. It’ll be simple to correlate \nthose two metrics over time and determine the ratio between the two. You’ll have a \nsymptom: the number of orders placed is greater than the ones completed. You can \nstart from there and then try to understand which component is failing (maybe even \nmultiple components). Is it the event queue or an infrastructure problem? Is the system \nunder high loads and can’t cope? The symptom will be the starting point for your inves-\ntigation, and from there you should follow the leads until you find the cause or causes.\nTIP    Avoid alert fatigue by keeping alert notifications to a minimum and keep-\ning them actionable. Generating an alert notification for every single deviation \nfrom the normal behavior of the system may quickly lead to alerts being disre-\ngarded or deemed unimportant. Such minimizing will eventually lead to some-\nthing important being overlooked.\n11.4\t Observing the whole application\nCorrelating metrics can be a precious tool to infer and understand more than a \nper-service state of the system. Monitoring can also help you understand and reason \nthrough the behavior of the system under different conditions, and this can help you \nto predict and adjust your capacity by using all the collected data. The good thing \nabout collecting per-service metrics is you can iteratively correlate them between dif-\nferent services and have an overall idea of the behavior of the whole application. In \nfigure 11.16, you can see a possible correlation of different service metrics.\nLet’s look into each of the suggested correlations:\n¡ A: Creating a new visualization comparing the rate of incoming requests to the gateway and \nthe orders service  — This allows you to understand if there are any issues in process-\ning the incoming requests from your users. You also can use the new correlation \nto set an alert every time that rate drops from 99%.\n¡ B: Correlating the number of user requests made to the gateway with the number of order- \ncreated messages in the queue  — Given that you know the order service is responsible \nfor publishing those messages, this will, similarly to A, allow you to understand if \nthe system is working correctly and customer requests are being processed.\n¡ C: Correlating the number of order-placed messages with the number of requests to the order \nservice  — This will allow you to infer if the fee service is working properly.\n \n",
      "content_length": 3110,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "294\nChapter 11  Building a monitoring system\nGateway dashboard\nResponse times\n# Status codes\n# Requests\nOrder service dashboard\nResponse times\nA\nB\nC\n# Status codes\n# Requests\nEvent queue dashboard\n# Order-placed messages\n# Order-created messages\nGateway\nOrder\nservice\nAMQP\n(async)\nRPC\n(sync)\nUser\nOrder\ncreated\nOrder\nplaced\nEvent queue\nFigure 11.16    Correlation of metrics between different services\nCombining different metrics into new dashboards and setting sensible alerts on them \nallows you to gain insights into the overall application. It’s then up to you to determine \nthe desired level of detail, from a high-level view to a detailed one. \nSo far, we’ve covered monitoring and alerting. You’ve set up a monitoring stack to \nbe able to understand how things happened. You’re now able to understand the status \nof services, observe the metrics they emit, and determine if they’re operating within \nexpected parameters. This is only part of the application observability effort. It’s a good \nstarting point, but you do need more!\nTo be able to fully understand what’s going on, you need to invest some more in \nlogging and tracing so you can have both a current view of what’s happening and a view \nof what happened before. In the next chapter, we’ll focus on logging and tracing as a \ncomplement to monitoring in your journey into observability. Doing so will help you to \nunderstand why things happened.\n \n",
      "content_length": 1416,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "\t\n295\nSummary\nSummary\n¡ A robust microservice monitoring stack consists of metrics, traces, and logs.\n¡ Collecting rich data from your microservices will help you identify issues, investi-\ngate problems, and understand your overall application behavior.\n¡ When collecting metrics, you should focus on four golden signals: latency, errors, \ntraffic (or throughput), and saturation.\n¡ Prometheus and StatsD are two common, language-independent tools for col-\nlecting metrics from microservices.\n¡ You can use Grafana to graph metric data, create human-readable dashboards, \nand trigger alerts.\n¡ Alerts based on metrics are more durable and maintainable if they indicate the \nsymptoms of incorrect system behavior, rather than the causes.\n¡ Well-defined alerts should have a clear priority, be escalated to the right people, \nbe actionable, and contain concise and useful information.\n¡ Collecting and aggregating data from multiple services will allow you to correlate \nand compare distinct metrics to gain a rich overall understanding of your system.\n \n",
      "content_length": 1053,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "296\n12\nUsing logs and traces \nto understand behavior\nThis chapter covers\n¡ Storing logs in a consistent and structured way \nin a machine-readable format\n¡ Setting up a logging infrastructure\n¡ Using traces and correlation IDs to understand \nsystem behavior\nIn the previous chapter, we focused on emitting metrics from your services and \nusing those metrics to create dashboards and alerts. Metrics and alerts are only one \npart of what you need to achieve observability in your microservice architecture. \nIn this chapter, we’ll focus on collecting logs and making sure you’re able to trace \nthe interactions between services. This will allow you to not only have an overview \nof how the system behaves but also go back in time and retrospectively follow each \nrequest. Doing so is important to debug errors and to identify bottlenecks. Logs give \nyou a sort of paper trail that documents the history of each request entering your \nsystem, whereas traces provide you a way to establish a timeline for each request, to \nunderstand how much time it spent in different services. \n \n",
      "content_length": 1079,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": "\t\n297\nUnderstanding behavior across services\nBy the end of this chapter, you’ll have created a basic logging infrastructure and set \nup the tracing capability. You’ll be able to both monitor the operation of your applica-\ntion and have the tools to audit and investigate in case you need to do so for particular \nrequests. In addition, you’ll be able to identify performance issues by looking into trac-\ning data.\n12.1\t Understanding behavior across services\nIn a microservices-based architecture, multiple services will be involved in providing \nfunctionality to users. It gets hard to understand what goes on with every request when \nyou no longer have a central access point to data. Services are distributed across multi-\nple nodes, are ephemeral, and are continuously being deployed and scaled to meet the \nneeds of operation. Let’s revisit the sell order use case as you might have implemented \nit if you’d needed a single application running on a single machine (figure 12.1).\nHard disk\nAPI\n(gateway)\nOrders module\nAccounts module\nQueue\nFees module\nMarket module\nApplication log\nFigure 12.1    The sell order use case implemented in a single application\n \n",
      "content_length": 1163,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "298\nChapter 12  Using logs and traces to understand behavior \n In figure 12.1, we’ve represented each of the services that collaborate to allow a client \nto sell shares as modules in the same application. If you were to inspect a given request \nlifecycle in the system, you could log in to the machine and inspect log data stored on a \nhard drive. But you’d most likely have multiple machines running the application, for \nredundancy and availability, so things wouldn’t be as easy as logging in to one machine. \nOnce you identified the request you were interested in observing, you’d have to iden-\ntify which machine had run the request and then inspect it. Going through the logs \nfrom that machine would provide you needed insights.\nMaintaining logs in a single machine is by no means easy — a server can also crash \nand become unavailable. Our aim here isn’t to talk about minimizing the complexity of \nkeeping log data (or any data) safely persisted but to point out that having a single point \nfor storing all the data makes it easier and more convenient to consult.\nLet’s now compare the same scenario in a microservices application. Figure 12.2 \nillustrates the same use case with multiple services, each with multiple copies of itself, \nrunning independently. \nAs you can see below, you have five services running independently, and each of \nthose services has three instances running. This means potentially none of the pods \nare executing on the same physical machine. A request coming into the system will \nmost likely flow through multiple pods running on different physical machines, and \nyou have no easy way to track down that request by accessing logs. Could you even do \nit? What guarantee do you have that any of the pods are still running once you need to \naccess data?\nGateway\nOrder\nservice\nMarket\nservice\nAMQP\n(async)\nFee\nservice\nRPC\n(sync)\nAMQP\n(async)\nAMQP\n(async)\nAMQP\n(async)\nAMQP\n(async)\nRPC\n(sync)\nAccount\ntransaction\nservice\nOrder\ncreated\nOrder\nplaced\nEvent queue\nOrder service\nlog data\nGateway log\ndata\nMarket service\nlog data\nFee service\nlog data\nAccount\ntransaction\nservice log\ndata\nFigure 12.2    The sell order use case in SimpleBank with multiple services running, each with its own log data\n \n",
      "content_length": 2229,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 326,
      "content": "\t\n299\nUnderstanding behavior across services\nGateway\nOrders\nAccount\ntransaction\nFees\nMarket\nLog data\nLog data\nLog data\nLog data\nLog data\nGateway\nlogs\nAccount\nservice\nlogs\nFee\nservice\nlogs\nMarket\nservice\nlogs\nOrder\nservice\nlogs\nFigure 12.3    Accessing logs for each service in different running instances is challenging.\nIn figure 12.3, we illustrate the challenges you face when you try to gather data from a \ndistributed system. Even if you had some sort of persistency for the log data that would \nallow it to survive after a running pod is replaced, it’d be no easy task to track down \na request through your system. You need a better way to record what’s going on with \nyour system. To be able to fully understand behavior, you need to\n¡ Make sure you persist log data so it survives through service restarts and scaling\n¡ Aggregate all log data from multiple services and instances of those services in a \ncentral location\n¡ Make the stored data usable, allowing for easy searches and further processing\nOur objective by the end of this chapter will be to have an infrastructure that allows \nyou to collect log data from all your services, aggregating it and allowing you to per-\nform searches in it so you can, at any time, reason through the behavior of the system. \nYou’ll be able to use the available data to audit, debug, or even gather new insights \nby processing it further. One example of the processing you can do to augment the \n \n",
      "content_length": 1447,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "300\nChapter 12  Using logs and traces to understand behavior \navailable information is to collect the IP data stored in the logs and generate a visual-\nization showing the most common geographic areas of your users.\n To effectively store and make your log data searchable, you first need to agree on a \nformat the engineering team will use. A consistent format will help to guarantee that \nyou can store and process data effectively.\n12.2\t Generating consistent, structured,  \nhuman-readable logs\nTo be able to achieve observability, you have to collect data from multiple sources; \nnot only from running services but also from your infrastructure. Defining a common \nformat allows you to analyze data more easily and perform searches in that data using \nexisting tools with minimal effort. Examples of the data you may collect and use are:\n¡ Application logs\n¡ Database logs\n¡ Network logs\n¡ Performance data collected from the underlying operating system\nFor some components, you can’t control the format, so you have to cope with their \nspecificities and transform them somehow. But for now, let’s focus on what you can \ncontrol: your services. Making sure the whole engineering team abides by a format \nand a way of doing things pays off in the long run, because data collection will become \nsimpler and more effective. Let’s start by determining what you should store; then we \ncan look at how to store it.\n12.2.1\t Useful information to include in log entries\nFor your log data to be useful and effective in helping you to understand behavior in \nyour systems, you need to make sure it includes certain information that will allow you \nto communicate certain things. Let’s look into what you should include as part of each \nlog entry.\nTimestamps\nTo be able to correlate data and order it appropriately, you need to make sure you \nattach timestamps to your log entries. Timestamps should be as granular and verbose \nas possible; for example, use four-digit years and the best resolution available. Each \nservice should render its own timestamps, preferably in microseconds. Timestamps \nalso should include a time zone, and it’s advisable that you collect data as GMT/UTC \nwhenever possible.\nHaving these details allows you to avoid issues with correlating data from different \nservices with different time zones. Ordering data by time of occurrence will become \nmuch easier and require less context while analyzing. Getting timestamps right is \nthe first step in making sure you can understand the sequence in which events took \nplace.\n \n",
      "content_length": 2542,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": "\t\n301\nGenerating consistent, structured, human-readable logs \nIdentifiers\nYou should use unique identifiers whenever, and as much as, possible in the data you \nintend to log. Request IDs, user IDs, and other unique identifiers are invaluable when \nyou’re cross-referencing data from multiple sources. They allow you to group data \nfrom different sources in an effective way.\nMost of the time, these IDs already exist in your system because you need to use them \nto identify resources. It’s likely they’re already being propagated through different ser-\nvices, so you should make the best use out of them. Unique identifiers used alongside \ntimestamps yield a powerful tool to understand the flow of events in a system.\nSource\nIdentifying the source of a given log entry allows easier debugging when needed. Typi-\ncal source data you can use includes:\n¡ Host\n¡ Class or module\n¡ Function\n¡ Filename\nWhen adding execution times on a given function call, the information you’ve col-\nlected for the source allows you to infer performance because you can extrapolate \nexecution times, even if not in real time. Although this isn’t a replacement for collect-\ning metrics, it can be effective in helping to identify bottlenecks and potential perfor-\nmance issues.\nLevel or category\nEach log entry should contain a category. The category can be either the type of data \nyou’re logging or the log level. Typically, the following values are used as log levels: \nERROR, DEBUG, INFO, WARN.\nThe category will allow you to group data. Some tools can parse log files searching \nfor messages with the ERROR level and communicate them to error reporting systems. \nThis is a perfect example of how you can use the log level or category to automate the \nprocess of error reporting without the need for explicit instructions.\n12.2.2\t Structure and readability\nYou want to generate log entries in a human-readable format, but at the same time \nthey need to be easily parseable by a machine. What we mean by human readable is \navoiding binary encoding of data or any type of encoding that your average human \ncan’t understand. An example of this would be storing the binary representation of an \nimage. You should probably use its ID, file size, and other associated data instead.\nYou also should avoid multiline logs because they can lead to fragmentation while \nparsing them in log aggregation tools. With such logs, it can be easy to lose some \nof the information associated with a particular log entry, like the ID, timestamp, or \nsource.\n \n",
      "content_length": 2523,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "302\nChapter 12  Using logs and traces to understand behavior \nFor the examples in this chapter, you’ll be using JSON to encode your log entries. \nDoing so allows you to provide human-readable and machine-parseable data, as well as \nto automatically include some of the data we mentioned in the previous section.\nIn chapter 7, when we were discussing a microservice chassis, we introduced a Python \nlibrary that provides log formatting: logstash-formatter. Logstash libraries are avail-\nable for different languages, so you can expect the format to be widespread and easily \nusable no matter what language you chose to code your services in.\nLogstash\nLogstash is a tool to collect, process, and forward events and log messages from multiple \nsources. It provides multiple plugins to configure data collecting.\nWe’re interested in the formatting conventions of Logstash, and you’ll be using its V1 for-\nmat specification in your SimpleBank services.\n \nLet’s now look into a log entry collected using the Logstash library for Python. This mes-\nsage is formatted using V1 of the logstash format, and the application generated it auto-\nmatically when it was booting, without the need for any specific code instruction to log it:\n{ \n    \"source_host\" : \"e7003378928a\", \n    \"pathname\" : \"usrlocallibpython3.6site-packagesnamekorunners.py\", \n➥\"relativeCreated\" : 386.46125793457031,  \n    \"levelno\" : 20,  \n    \"msecs\" : 118.99447441101074,  \n    \"process\" : 1,  \n    \"args\" : [    \"orders_service\"  ],\n    \"name\" : \"nameko.runners\",  \n    \"filename\" : \"runners.py\", \n    \"funcName\" : \"start\", \n    \"module\" : \"runners\", \n    \"lineno\" : 64,  \n    \"@timestamp\" : \"2018-02-02T18:42:09.119Z\", \n    \"@version\" : 1, \n    \"message\" : \"starting services: orders_service\", \n    \"levelname\" : \"INFO\", \n    \"stack_info\" : null,  \n    \"thread\" : 140612517945416,   \n    \"processName\" : \"MainProcess\",  \n    \"threadName\" : \"MainThread\",  \n    \"msg\" : \"starting services: %s\",  \n    \"created\" : 1520275329.1189945\n}\nInformation about the source: the host \nrunning the application\nTime taken to process the action\nFilename, function, module, and line \nnumber emitting the log\nTimestamp, with Z indicating \nthe UTC time zone\nVersion of the formatter (logstash-formatter v1)\nMessage indicating the \nstarting of the server\nLog level or category, in this case the INFO level\n \n",
      "content_length": 2352,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "\t\n303\nSetting up a logging infrastructure for SimpleBank\nAs you can see, the Logstash library inserts relevant information, taking that burden \nfrom the developer’s shoulders. In the following listing, you’ll see how an explicit log \ncall in code renders a log entry.\nListing 12.1    Logstash V1 formatted log message after an explicit log instruction\n# Python code for generating a log entry \nself.logger.info ({\"message\": \"Placing sell order\", \n➥\"uuid\": res})    \n{\n    \"@timestamp\": \"2018-02-02T18:43:08.221Z\",\n    \"@version\": 1,\n    \"source_host\": \"b0c90723c58f\",\n    \"name\": \"root\",\n    \"args\": [],\n    \"levelname\": \"INFO\", \n    \"levelno\": 20,\n    \"pathname\": \"./app.py\",\n    \"filename\": \"app.py\",\n    \"module\": \"app\",\n    \"stack_info\": null,\n    \"lineno\": 33,\n    \"funcName\": \"sell_shares\",\n    \"created\": 1520333830.3000789,\n    \"msecs\": 300.0788688659668,\n    \"relativeCreated\": 15495.944738388062,\n    \"thread\": 140456577504064,\n    \"threadName\": \"GreenThread-2\",\n    \"processName\": \"MainProcess\",\n    \"process\": 1,\n    \"message\": \"Placing sell order\", \n    \"uuid\": \"a95d17ac-f2b5-4f2c-8e8e-2a3f07c68cf2\" \n}\nIn your explicit call to the logger, you’ve only stated the desired level and the message \nto log in the form of key-value pairs containing a message and a UUID. Logstash auto-\nmatically collected and added all the other information present in the log entry with-\nout you having to declare it explicitly.\n12.3\t Setting up a logging infrastructure for SimpleBank\nNow that you’ve set a format for collecting and presenting info, you can move on to \ncreating a basic logging infrastructure. In this section, you’ll be setting up the infra-\nstructure that’ll allow you to collect logs from all the running services and aggregate \nthem. It also will provide you with search and correlation capabilities. The purpose is to \nhave a central access point to all the log data like you already have for metrics. In figure \n12.4, we illustrate what you want to achieve after setting up a logging infrastructure.\nThe log level, the message \nfield, and the uuid field\nThe log level determined by the call made \nto the logger module, in this case, INFO\nThe message field\nThe uuid field, which identifies \nand potentially allows correlation \nbetween this log entry and \nanother in different services\n \n",
      "content_length": 2303,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "304\nChapter 12  Using logs and traces to understand behavior \nOrder\nservice\nGateway\n2018-02-04T13:55:19.263380548Z {\"@timestamp\":\n\"2018-02-04T13:55:19.262Z\", \"@version\": 1,\n\"source_host\": \"bb69db21f1eb\", \"name\": \"root\", \"args\": [],\n\"levelname\": \"INFO\", \"levelno\": 20, \"pathname\": \"./app.py\",\n\"filename\": \"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152, \"msecs\": 261.5199089050293,\n\"relativeCreated\": 669360.3167533875, \"thread\":\n140130358674856, \"threadName\": \"GreenTread-13\",\n\"processName\": \"MainProcess\", \"process\": 1, \"message\":\n\"{'event': 'sell_order_received', 'uuid':\n'11fcdc94-c055-4f26-988b-886165655792'}\", \"event\":\n\"sell_order_received\", \"uuid\":\n\"11fcdc94-c055-4f26-988b-886165655792\"}\n2018-02-04T13:55:20.839781524Z {\"@timestamp\":\n\"2018-02-04T13:55:20.838Z\", \"@version\": 1,\n\"source_host\": \"e7003378928a\", \"name\": \"root\", \"args\": [],\n\"levelname\": \"INFO\", \"levelno\": 20, \"pathname\": \"./app.py\",\n\"filename\": \"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 45, \"funcName\": \"__create_event\", \"created\":\n1520275337.8386793, \"msecs\": 838.679313659668,\n\"relativeCreated\": 669360.3167533875, \"thread\":\n140612448880184, \"threadName\": \"GreenTread-2\",\n\"processName\": \"MainProcess\", \"process\": 1, \"message\":\n\"{'event': 'order_created', 'uuid':\n'11fcdc94-c055-4f26-988b-886165655792'}\", \"event\":\n\"order_created\", \"uuid\":\n\"11fcdc94-c055-4f26-988b-886165655792\"}\nLogs\nLogs\nMetrics\nMetrics\nOrder service dashboard\nResponse times\n# Status codes\n# Requests\nGateway dashboard\nMetrics dashboard\nCentralized logging\nResponse times\n# Status codes\n# Requests\nFigure 12.4    Services making use of centralized metrics and a centralized log for easy access to data\nOnce you set up the log aggregation capability, like you did for metrics in chapter 11, \nyou’ll have all services sending both metrics and logs to centralized systems that’ll allow \nyou to improve observability. You’ll be able to observe data about a running system and \ndig deeper to collect more information in case you need to audit or debug a particular \nrequest. You’ll set up a solution commonly called ELK (Elasticsearch, Logstash, and \nKibana) and will use a data collector called Fluentd.\n12.3.1\t ELK- and Fluentd-based solution\nYou’ll build the logging infrastructure we propose using Elasticsearch, Logstash, and \nKibana. Also, you’ll use Fluentd for pushing logs from the apps to your centralized log-\nging solution. Before we get into more details about these technologies, have a look at \nfigure 12.5 to get an overview of what we want to enable you to achieve.\nIn figure 12.5, you can see how you can collect the logs for multiple instances of the \ngateway service and forward them to your centralized logging system. We represent \nmultiple instances of the same service, but this will work for any of the services you have \nrunning. Services will redirect all the log information to STDOUT (standard output), and \nan agent running the Fluentd daemon will be responsible for pushing those logs into \nElasticsearch.\n \n",
      "content_length": 3059,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": "\t\n305\nSetting up a logging infrastructure for SimpleBank\n2018-02-04T13:55:19.263380548Z {\"@timestamp\":\n\"2018-02-04T13:55:19.262Z\", \"@version\": 1,\n\"source_host\": \"bb69db21f1eb\", \"name\": \"root\", \"args\": [],\n\"levelname\": \"INFO\", \"levelno\": 20, \"pathname\": \"./app.py\",\n\"filename\": \"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152, \"msecs\": 261.5199089050293,\n\"relativeCreated\": 669360.3167533875, \"thread\":\n140130358674856, \"threadName\": \"GreenTread-13\",\n\"processName\": \"MainProcess\", \"process\": 1, \"message\":\n\"{'event': 'sell_order_received', 'uuid':\n'11fcdc94-c055-4f26-988b-886165655792'}\", \"event\":\n\"sell_order_received\", \"uuid\":\n\"11fcdc94-c055-4f26-988b-886165655792\"}\n2018-02-04T13:55:19.263380548Z {\"@timestamp\":\n\"2018-02-04T13:55:19.262Z\", \"@version\": 1,\n\"source_host\": \"bb69db21f1eb\", \"name\": \"root\", \"args\": [],\n\"levelname\": \"INFO\", \"levelno\": 20, \"pathname\": \"./app.py\",\n\"filename\": \"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152, \"msecs\": 261.5199089050293,\n\"relativeCreated\": 669360.3167533875, \"thread\":\n140130358674856, \"threadName\": \"GreenTread-13\",\n\"processName\": \"MainProcess\", \"process\": 1, \"message\":\n\"{'event': 'sell_order_received', 'uuid':\n'11fcdc94-c055-4f26-988b-886165655792'}\", \"event\":\n\"sell_order_received\", \"uuid\":\n\"11fcdc94-c055-4f26-988b-886165655792\"}\nGateway\n2018-02-04T13:55:19.263380548Z {\"@timestamp\":\n\"2018-02-04T13:55:19.262Z\", \"@version\": 1,\n\"source_host\": \"bb69db21f1eb\", \"name\": \"root\", \"args\": [],\n\"levelname\": \"INFO\", \"levelno\": 20, \"pathname\": \"./app.py\",\n\"filename\": \"app.py\", \"module\": \"app\", \"stack_info\": null,\n\"lineno\": 33, \"funcName\": \"sell_shares\", \"created\":\n1520171719.26152, \"msecs\": 261.5199089050293,\n\"relativeCreated\": 669360.3167533875, \"thread\":\n140130358674856, \"threadName\": \"GreenTread-13\",\n\"processName\": \"MainProcess\", \"process\": 1, \"message\":\n\"{'event': 'sell_order_received', 'uuid':\n'11fcdc94-c055-4f26-988b-886165655792'}\", \"event\":\n\"sell_order_received\", \"uuid\":\n\"11fcdc94-c055-4f26-988b-886165655792\"}\nLogs\nGateway\nLogs\nGateway\nLogs\nCentralized logging\nSTDOUT\nSTDOUT\nFluentd\nSTDOUT\nGateway service deployed in Kubernetes\nFigure 12.5    Collecting logs from multiple service instances and forwarding them to a centralized location\nBy following this pattern when deploying any new services, you’ll make sure log data \ngets collected and indexed and becomes searchable. But before we move to imple-\nmentation, we’ll take a little time to introduce each of the technologies you’ll be \nusing.\nElasticsearch\nElasticsearch (www.elastic.co/products/elasticsearch) is a search and analytics engine \nthat stores data centrally. It indexes data, in your case log data, and allows you to per-\nform efficient search and aggregation operations on the data it has stored.\nLogstash\nLogstash (www.elastic.co/products/logstash) is a server-side processing pipeline that \nallows data ingestion from multiple sources and transforms that data prior to send-\ning it to Elasticsearch. In your case, you’ll be using the Logstash formatting and data \ncollection capabilities by taking advantage of client libraries. In this chapter’s earlier \nexamples, you already observed its ability to provide consistent data that you can send \nto Elasticsearch. But here you won’t be using Logstash to send data; you’ll be using \nFluentd instead.\nKibana\nKibana (www.elastic.co/products/kibana) is a UI for visualizing Elasticsearch data. It’s \na tool you can use to query data and explore its associations. In your use case, it’ll \noperate on log data. You can use Kibana to derive visualizations from gathered data, so \nit’s more than a search tool. Figure 12.6 shows an example of a dashboard powered by \nKibana.\n \n",
      "content_length": 3783,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "306\nChapter 12  Using logs and traces to understand behavior \nFigure 12.6    Kibana tutorial dashboard showing visualizations created from log data\nFluentd\nFluentd (www.fluentd.org) is an open source data collector that you’ll be using to push \ndata from your services to Elasticsearch. You’ll combine the data formatting and col-\nlecting capabilities of Logstash and use Fluentd to push that data. One of its advan-\ntages is the fact that you can use it as a logging provider for Dockerfiles if you declare it \nin Docker compose files.\n12.3.2\t Setting up your logging solution\nYou’ll set up your solution via the Docker compose file, like you already did in chap-\nter 11 to create the metrics collecting and alert infrastructure. You can find all the \ncode used in this chapter at Github (http://mng.bz/k191). There, you’ll find the \ndocker-compose.yml file, where you’ll be declaring the new dependencies. The fol-\nlowing listing shows the new components added to the compose file.\n Listing 12.2 Docker compose file with Elasticsearch, Kibana, and Fluentd containers\nversion: '2.1'\nservices:\n  gateway:\n    container_name: simplebank-gateway\n    restart: always\n    build: ./gateway\n    ports:\n \n",
      "content_length": 1198,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "\t\n307\nSetting up a logging infrastructure for SimpleBank\n      - 5001:5000\n    volumes:\n      - ./gateway:/usr/src/app\n    links:\n      - \"rabbitmq:simplebank-rabbitmq\"\n      - \"fluentd”\n    logging: \n      driver: \"fluentd\" \n      options:\n        fluentd-address: localhost:24224\n        tag: simplebank.gateway\n(…)\n  kibana:\n    image: kibana \n    links:\n      - \"elasticsearch\" \n    ports:\n      - \"5601:5601\"\n  elasticsearch:\n    image: elasticsearch \n    expose:\n      - 9200\n    ports:\n      - \"9200:9200\"\n  fluentd:\n    build: ./fluentd \n    volumes:\n      - ./fluentd/conf:/fluentd/etc \n    links:\n      - \"elasticsearch\" \n    ports:\n      - \"24224:24224\"\n      - \"24224:24224/udp\"\n(…)\nOnce you’ve added this content to the docker compose file, you’re almost ready to \nboot your logging infrastructure alongside your services. But first let’s cover the miss-\ning tweaks we mentioned previously that’ll allow you to configure Fluentd to your \nneeds. The Dockerfile you use for building Fluentd follows.\nListing 12.3    Fluentd Dockerfile (Fluentd/Dockerfile)\nFROM fluent/fluentd:v0.12-debian \nRUN [\"gem\", \"install\", \"fluent-plugin-elasticsearch\", \n➥\"--no-rdoc\", \"--no-ri\", \"--version\", \"1.9.2\"] \nNow all you need is to create a configuration file for Fluentd. The following listing \nshows the config file.\nAdds a logging directive to each service to \nforce Docker to push the output of each \ncontainer running a service to Fluentd, \nwhich in turn will make sure data gets \npushed to Elasticsearch\nFor Kibana, uses the default image in \nDocker Hub with the defaults set\nLinks Kibana to the Elasticsearch container, \nbecause it’ll consume data from it\nLike you did for Kibana, uses the default \nimage for Elasticsearch\nBuilds Fluentd from a custom Docker image\nInjects the configuration for Fluentd into \nthe built container, allowing you to \ntweak the default configuration\nLinks the Fluentd container to the Elasticsearch \ncontainer, because it’ll be pushing data into it\nPulls the Fluentd base image\nInstalls the Elasticsearch \nplugin for Fluentd\n \n",
      "content_length": 2058,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "308\nChapter 12  Using logs and traces to understand behavior \nListing 12.4    Fluentd configuration (fluentd/conf/fluent.conf)\n<source> \n  @type forward \n  port 24224\n  bind 0.0.0.0\n</source>\n<match *.**> \n  @type copy \n  <store>\n    @type elasticsearch \n    host elasticsearch\n    port 9200\n    logstash_format true\n    logstash_prefix fluentd\n    logstash_dateformat %Y%m%d\n    include_tag_key true\n    type_name access_log\n    tag_key @log_name\n    flush_interval 1s\n  </store>\n  <store>\n    @type stdout \n  </store>\n</match>\n The match section in the Fluentd configuration file contains all the needed configu-\nration for connecting with elasticsearch, port, and host, as well as the format used. \nYou’re using logstash format, as you may recall.\nWith all the needed setup done, you’re now ready to boot your services using the \nnew Docker compose file. Before doing so, let’s go through your services and change \ncode to enable you to send data to your centralized logging infrastructure. In the next \nsection, you’ll configure your services to make use of the Logstash logger. You’ll also set \nlog levels.\n12.3.3\t Configure what logs to collect\nIn your services, you can control the log level via environment variables, so you can \nhave different levels for development and production environments. Using different \nlog levels allows you to enable more verbose logs in production, in case you need to \ninvestigate any issue.\nLet’s look at your gateway service for its logging configuration and also at the service \ncode to understand how you can emit log messages. The logging configuration is shown \nin the following listing.\nConfigures the location data comes from: port \n24224 for both TCP and UDP, as you declared \nin the Docker compose file in listing 12.2\nPlugin to use for input; listens to a TCP socket \nand a UDP socket for heartbeats that work as \na way to monitor the health of Fluentd\nIndicates what Fluentd should do — In this section, \nyou configure two stores: one that processes json data \nand another that processes all other stdout data.\nOutput plugins used in the match section: \ncopy for copying events to multiple sources; \nelasticsearch to record data in Elasticsearch; \nand stdout to output all data entering Fluentd \n \n",
      "content_length": 2249,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": "\t\n309\nSetting up a logging infrastructure for SimpleBank\nListing 12.5    Configuration file for the gateway service (gateway/config.yml)\nAMQP_URI: amqp://${RABBIT_USER:guest}:${RABBIT_PASSWORD:guest}@${RABBIT_HOST:\n➥localhost}:${RABBIT_PORT:5672}/\nWEB_SERVER_ADDRESS: '0.0.0.0:5000'\nRPC_EXCHANGE: 'simplebank-rpc'\nLOGGING: \n    version: 1\n    handlers:\n        console:\n            class: logging.StreamHandler \n    root:\n        level: ${LOG_LEVEL:INFO} \n        handlers: [console] \nThis configuration will allow setting the log level when the application boots. In the \nDocker compose file, you’ve set the environment variable LOG_LEVEL as INFO for all ser-\nvices except for the Gateway, which has a DEBUG value. Let’s now look into the gateway \ncode to set up logging, as shown in the following listing.\nListing 12.6    Enable logging in the gateway service (gateway/app.py)\nimport datetime\nimport json\nimport logging \nimport uuid\nfrom logstash_formatter import LogstashFormatterV1 \nfrom nameko.rpc import RpcProxy, rpc\nfrom nameko.web.handlers import http\nfrom statsd import StatsClient\nfrom werkzeug.wrappers import Request, Response\nclass Gateway:\n    name = \"gateway\"\n    orders = RpcProxy(\"orders_service\")\n    statsd = StatsClient('statsd', 8125,\n                         prefix='simplebank-demo.gateway')\n    logger = logging.getLogger() \n    handler = logging.StreamHandler() \n    formatter = LogstashFormatterV1() \n    handler.setFormatter(formatter) \n    logger.addHandler(handler) \n    @http('POST', '/shares/sell')\n    @statsd.timer('sell_shares')\nLogging configuration section\nDefines the handler class for console logging, \nwhich you’ll use as the handler in listing 12.6 \nReads the log level from an environment variable \n(LOG_LEVEL) and sets a default value of INFO in \ncase the environment variable isn’t defined \nRegisters only a `console` handler, because \nyou won’t be reading from log files\nImports Python’s logging facility (https://docs.python.org/3/ \nlibrary/logging.html)\nImports the Logstash \nFormatter so you can emit \nlogs in logstash format\nInitializes and configures logger\n \n",
      "content_length": 2110,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "310\nChapter 12  Using logs and traces to understand behavior \n    def sell_shares(self, request):\n        req_id = uuid.uuid4()\n        res = u\"{}\".format(req_id)\n        self.logger.debug(\n            \"this is a debug message from gateway\", \n➥extra={\"uuid\": res}) \n        self.logger.info(\"placing sell order\", extra=\n➥{\"uuid\": res}) \n        self.__sell_shares(res)\n        return Response(json.dumps(\n            {\"ok\": \"sell order {} placed\".format(req_id)}),\n            ➥mimetype='application/json')\n    @rpc\n    def __sell_shares(self, uuid):\n        self.logger.info(\"contacting orders service\", extra={\n            ➥\"uuid\": uuid}) \n        res = u\"{}\".format(uuid)\n        return self.orders.sell_shares(res)\n    @http('GET', '/health')\n    @statsd.timer('health')\n    def health(self, _request):\n        return json.dumps({'ok': datetime.datetime.utcnow().__str__()})\nIn listing 12.2, you saw how to enable the Fluentd driver for logging with Docker. This \nmeans you’re ready to send log data generated from your services to Elasticsearch \nusing Fluentd, and afterwards you’ll be able to explore that log data using Kibana. \nTo start all the services, metrics, and logging infrastructure from a console in the root \ndirectory, execute the following command:\ndocker-compose up --build --remove-orphans\nOnce all is ready, you need to complete one last step, which is configuring Kibana \nto use the logs collected via Fluentd and stored in Elasticsearch. To do so, access the \nKibana web dashboard (http://localhost:5601). On the first access, you’ll be redi-\nrected to the management page, where you’ll need to configure an index pattern. You \nneed to tell Kibana where to find your data. If you recall, in the Fluentd configuration, \nyou set an option for the logstash prefix with the value fluentd. This is what you need \nto enter in the index text box presented to you. Figure 12.7 shows the Kibana dash-\nboard management section and the value you need to input.\nAfter inserting fluentd-* as the index pattern and clicking the Create button, you’ll \nbe ready to explore all the log data that your multiple services create. Elasticsearch will \nforward all data to a central location, and you’ll be able to access it in a convenient way.\nExample of how to log a message with \nthe DEBUG level — This message will \nonly be sent if the application log level \nis set to DEBUG. If you set it to INFO, \nthis message won’t be sent.\nExamples of log messages \nemitted with the INFO log level\n \n",
      "content_length": 2495,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 338,
      "content": "\t\n311\nSetting up a logging infrastructure for SimpleBank\nFigure 12.7    The Kibana management section where you need to indicate the index pattern for it to be \nable to fetch data from Elasticsearch\nTo generate some log data, all you need to do is create a sell request to your gateway \nservice. To do so, you need to issue a POST request to the gateway. The following shows \npresenting a request via curl, but any tool capable of generating a POST request will do:\nchapter-12$ curl -X POST http://localhost:5001/shares/sell \\\n  -H 'cache-control: no-cache' \\\n  -H 'content-type: application/json' \nchapter-12$ {\"ok\": \"sell order e11f4713-8bd8-4882-b645\n➥-55f96d220e44 placed\"} \nNow that you have log data collected, you can explore it using Kibana. Clicking on \nthe Discover section on the left side of Kibana’s web dashboard will take you to a \npage where you can perform searches. In the search box, insert the request UUID \nyou received as the sell order response. In the case of this example, you’d be using \ne11f4713-8bd8-4882-b645-55f96d220e44 as your search parameter. In the next sec-\ntion, we’ll show you how to use Kibana to follow the execution of a sell request through \nthe different services involved.\n12.3.4\t Finding a needle in the haystack\nIn the code example for this chapter, you have five independent services collaborat-\ning to allow SimpleBank users to sell shares. All services are logging their operations \nand using the request UUID as a unique identifier to allow you to aggregate all log \ncurl command to the gateway service\nResponse from the service — the UUID you receive allows you to \nidentify your sell order, and you can use it as a search term on \nKibana. (The UUID shown is randomly generated, so please use \nthe one you receive as a response to the request you issued.)\n \n",
      "content_length": 1809,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 339,
      "content": "312\nChapter 12  Using logs and traces to understand behavior \nmessages referring to a sell order that’s processing. You can make use of Kibana to \nexplore your logs and track down the execution of a request. In figure 12.8, you use \nthe order ID that the gateway service returns to perform the search.\nWhen you use the request ID as the search parameter, Kibana filters the log data, and \nyou get 11 hits that allow you to follow the execution of a request through different ser-\nvices. Kibana allows you to use complex queries to be able to uncover insights. You get \nthe ability to filter per service, sort by time, and even use log data — for example, exe-\ncution times present in the log entries — to create dashboards to track performance. \nThis use is beyond the scope of this chapter, but do feel free to explore the possibilities \noffered to get new perspectives on the collected data.\nWe’ll now zoom in a bit and focus on some of the log entries that your query shows. \nFigure 12.9 shows some of those entries with a bit more detail.\nFigure 12.9    Detailed view of log messages in the Kibana search page\nFigure 12.8    Searching the log data using a request ID\n \n",
      "content_length": 1173,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 340,
      "content": "\t\n313\nTracing interactions between services\nIn figure 12.9, you’ll find messages from the market and gateway services. For the latter, \ngiven that the log level selected was DEBUG, you’ll find both info and debug messages. \nAs mentioned previously, by using the logstash-formatter library in your Python \ncode, you get more information for free. You can find data regarding execution times \nand scoping of the execution by module, function, line, process, and thread. All of this \ninfo can be useful if you need to diagnose any issues in the future.\n12.3.5\t Logging the right information\nNow that you’re able to collect logs and store them, you need to be careful about what \ninformation you send via logs. Things like passwords, credit card numbers, ID card \nnumbers, and potentially sensitive personal data that get sent will be stored and acces-\nsible by anyone who can use the logging infrastructure. In your case, you’re hosting \nand controlling the logging infrastructure, but if you were using a third-party provider, \nyou’d need to pay extra attention to these details. You have no easy way to delete only \nsome of the data already sent. In most cases, if you want to delete something specific, it \nmeans deleting all the log data for a given period of time.\nData privacy is a hot topic at the moment, and with the EU General Data Protection \nRegulation (GDPR) (www.eugdpr.org) now in effect, you need to take extra care when \nconsidering which data to log and how to log it. We won’t explore the needed steps \nin depth here, but both Fluentd and Elasticsearch allow you to apply filtering to data \nso that any sensitive fields get masked, encrypted, or removed from the data that they \nreceive and that Elasticsearch indexes. The general rule would be to log as little infor-\nmation as you can, avoid any personal data in logs, and take extra care with reviewing \nwhat gets logged before any changes make it into the production environment. Once \nyour services send data, it’s hard to erase it and doing so will have associated costs.\nThat said, you can and should use logs to communicate useful information to allow \nyou to understand system behavior. Sending IDs that allow you to correlate actions of \ndifferent systems and terse log messages indicating actions performed in or by systems \ncan help you keep track of what’s happened.\n12.4\t Tracing interactions between services\nWhen you were setting up your log infrastructure and making the code changes to emit \nlog messages, you already took care of propagating an ID field that allows you to follow \nthe execution path of a request through your system. With this set up, you can group log \nentries under the same context. You may even use log data to create visualizations that’ll \nallow you to understand how much time each component took to process a request. In \naddition, you can use it to help you identify bottlenecks and places where you can improve \nyour code to have extra performance. But logs aren’t the only tool available — you have \nanother method at your disposal for doing this that doesn’t rely on log data.\nYou can do better by reconstructing the journey of requests through your microservices. \nIn this section, you’ll be setting up distributed tracing to allow you to visualize the flow of \nexecution between services and at the same time, provide insights on how long each oper-\nation takes. This can be valuable, not only to understand the order in which a request flows \n \n",
      "content_length": 3462,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 341,
      "content": "314\nChapter 12  Using logs and traces to understand behavior \nthrough multiple services, but also to identify possible bottlenecks. For this, you’ll use Jae-\nger and libraries compatible with the OpenTracing API (http://opentracing.io). \nOpenTracing API\nThe OpenTracing API is a vendor-neutral open standard for distributed tracing. A lot of \ndistributed tracing systems (for example, Dapper, Zipkin, HTrace, X-Trace) provide tracing \ncapabilities but do so using incompatible APIs. Choosing one of those systems would \ngenerally mean tightly coupling systems potentially using different programming lan-\nguages to a single solution. The purpose of the OpenTracing initiative is to provide a set \nof conventions and a standard API for collection of traces. Libraries are available for mul-\ntiple languages and frameworks. You can find some of the supported tracer systems at \nhttp://mng.bz/Gvr3.\n \n12.4.1\t Correlating requests: traces and spans\nA trace is a direct acyclic graph (DAG) of one or more spans, where the edges of those \nspans are called references. Traces are used to aggregate and correlate execution flow \nthrough the system. To do so, some information needs to be propagated. A trace cap-\ntures the whole flow.\nLet’s look at figures 12.10 and 12.11. In these figures, we represent a trace made up \nof multiple spans, from both a dependency perspective and a temporal perspective.\nFigure 12.10    A trace made up of eight different spans from a dependency perspective\n \n",
      "content_length": 1485,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 342,
      "content": "\t\n315\nTracing interactions between services\nFigure 12.11    The temporal relationships in an eight-span trace\nIn figure 12.10, you can observe the dependency relationship between different spans. \nThese spans can be triggered either in the same application or in different ones. The \nonly requirement is for the parent span ID to be propagated, so when a new span is \ntriggered, it’ll hold a reference to its parent span.\nIn figure 12.11, you have a view of spans from a temporal perspective. By using tem-\nporal information contained in spans, you can organize them in a timeline. You can \nsee not only when each span happened relative to other spans but also how long each \noperation that a span encapsulates took to complete.\nEach span contains the following information:\n¡ An operation name\n¡ A start and a finish timestamp\n¡ Zero or more span tags (key value pairs)\n¡ Zero or more span logs (key value pairs with a timestamp)\n¡ A span context\n¡ References to zero or more spans (via the span context)\nThe span context contains the needed data to refer to different spans, either locally or \nacross service boundaries.\nLet’s now move on to setting up tracing between services. You’ll be using Jaeger \n(www.jaegertracing.io), a distributed tracing system, as well as a set of Python libraries \nthat are OpenTracing compatible.\n12.4.2\t Setting up tracing in your services\nTo be able to display tracing information and correlate requests between different ser-\nvices, you’ll need to set up a collector and a UI for traces, as well as including some \nlibraries and setting them up in your services. The services we’ll use as an example for \ndistributed tracing will be the SimpleBank profile and settings services. In figure 12.12, \nwe give an overview of the interactions you’ll be tracing.\n \n",
      "content_length": 1794,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 343,
      "content": "316\nChapter 12  Using logs and traces to understand behavior \nProfile\nservice\nSettings\nservice\nGET\nGET\nhttp://ip.jsontest.com\nFigure 12.12    Interactions of the profile service\nThe profile service will contact an external service, in this case jsontest.com, to retrieve \nits IP and will also be fetching user settings from the settings service. You’ll be setting \nup a tracing system (Jaeger) and making the code changes needed to display the trace \nand its spans and to be able to correlate those spans. Correlating them will allow you \nto understand in detail how long each operation took and how it contributed to the \noverall execution time of a call to the profile service. You’ll begin by setting up Jaeger, \nthe distributed tracing system collector and UI, by adding a Docker image into your \ndocker-compose.yml file (listing 12.7).\nJaeger\nInspired by Dapper and OpenZipkin, Jaeger is a distributed tracing system released as \nopen source by Uber Technologies. You use it for monitoring and troubleshooting micro­\nservice-based distributed systems.\n \nListing 12.7    Adding Jaeger to the docker-compose.yml file\n(…)\njaeger:\n    container_name: jaeger\n    image: jaegertracing/all-in-one:latest \n    ports:\n       - 5775:5775/udp \n       - 6831:6831/udp\n       - 6832:6832/udp\n       - 5778:5778\n       - 16686:16686 \n       - 14268:14268\n       - 9411:9411 \n    environment:\n      COLLECTOR_ZIPKIN_HTTP_PORT: \"9411\" \nYou’ll be using a jaeger image containing \nall the needed components because it’ll \nbe easier to set up. This all-in-one image \nhas in-memory-only storage for spans.\nPort for communicating spans\nPort for accessing the Jaeger UI\nPort used by Zipkin, another distributed tracing system — One of the advantages of \nthe OpenTracing initiative is the fact that you can use different systems without the \nneed to change all implementations or be locked to a particular one.\n \n",
      "content_length": 1895,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 344,
      "content": "\t\n317\nTracing interactions between services\nWith the Docker image added to your docker-compose file, once you boot all the \nSimpleBank infrastructure, you’ll have a distributed tracing system in place. Now you \nneed to make sure that the SimpleBank profile and settings services are able to create \ntraces and spans and communicate them to Jaeger.\nLet’s add the needed libraries to both the settings and profile services and initialize \nthe tracer. The following listing adds the tracing libraries.\nListing 12.8    Adding the tracing libraries to the services via a requirements.txt file\nFlask==0.12.0\nrequests==2.18.4\njaeger-client==3.7.1 \nopentracing>=1.2,<2 \nopentracing_instrumentation>=2.2,<3 \nBy adding these libraries, you’re now able to create traces and spans from both ser-\nvices. To make the process easier, you can also create a module to provide a convenient \nsetup function to initialize the tracer, as shown in the following listing.\nListing 12.9    Tracer initializer lib/tracing.py\nimport logging\nfrom jaeger_client import Config \ndef init_tracer(service): \n    logging.getLogger('').handlers = []\n    logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n    config = Config(\n        config={\n            'sampler': {\n                'type': 'const',\n                'param': 1,\n            },\n            'local_agent': { \n                'reporting_host': \"jaeger\",\n                'reporting_port': 5775,\n            },\n            'logging': True, \n            'reporter_batch_size': 1,\n        },\n        service_name=service, \n    )\n    return config.initialize_tracer()\nJaeger client library that connects the \nservice to the tracer system\nPython OpenTracing platform library\nCollection of instrumentation tools to simplify integration \nwith different frameworks and applications\nImports the Jaeger client that allows \nestablishing communication between \nthe app and the tracing collector system\nReceives the service name as an argument\nSets up both the host and the port where \ntraces and spans will be sent — In the \nDocker compose file, you have Jaeger \nrunning as “jaeger” and receiving \nmetrics via UDP on port 5775. This is \nnecessary because you’ll have one \ncollector agent running for all services.\nIn addition to collecting metrics in Jaeger, you’re \nalso emitting the trace events to the logs.\nSets the service name to the one  \nreceived as the init function argument\n \n",
      "content_length": 2415,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 345,
      "content": "318\nChapter 12  Using logs and traces to understand behavior \nThe SimpleBank profile and settings services will both be using the tracer initialization \nfunction shown in listing 12.9. This allows them to establish the connection to Jaeger. \nAs we showed in figure 12.12, the profile service contacts both an external service and \nthe settings internal service. You’ll be tracing the interaction of the profile service with \nboth of these collaborators. In the case of the interaction with the SimpleBank settings \nservice, you’ll need to pass along the context of the initial trace so you can visualize the \nfull cycle of a request.\nListing 12.10 shows the profile service code where you set up the spans for both the \nexternal http service and the settings service. For the former, you create a span, and for \nthe latter, you pass along the current span as a header so the settings service can make \nuse of it and create child spans.\nListing 12.10    Profile service code\nfrom urlparse import urljoin\nimport opentracing\nimport requests\nfrom flask import Flask, jsonify, request\nfrom opentracing.ext import tags \nfrom opentracing.propagation import Format \nfrom opentracing_instrumentation.request_context import \n➥get_current_span \nfrom opentracing_instrumentation.request_context import \n➥span_in_context \nfrom lib.tracing import init_tracer \napp = Flask(__name__)\ntracer = init_tracer('simplebank-profile') \n@app.route('/profile/<uuid:uuid>')\ndef profile(uuid):\n    with tracer.start_span('settings') as span: \n        span.set_tag('uuid', uuid)\n        with span_in_context(span):\n            ip = get_ip(uuid)\n            settings = get_user_settings(uuid)\n            return jsonify({'ip': ip, 'settings': settings})\ndef get_ip(uuid):\n    with tracer.start_span('get_ip', child_of=\n➥get_current_span()) as span: \n        span.set_tag('uuid', uuid) \n        with span_in_context(span): \n            jsontest_url = \"http://ip.jsontest.com/\"\n            r = requests.get(jsontest_url)\n            return r.json()\nImports the OpenTracing libraries to \nallow you to set up spans and tags\nImports the initializer function as defined in \nlisting 12.9 to set up the connection to Jaeger\nCalls the tracer initializer passing \nthe service name, which will \ncreate a tracer object you can use\nSets the initial span associated with \nthe tracer — The created span will \nbe the parent for spans in both the \ncall to the external service and the \ncall to the settings service.\nCreates a new span for the call to the \nexternal service, a child of the parent \nspan initialized above.\nWraps code execution under the newly created span\n \n",
      "content_length": 2626,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 346,
      "content": "\t\n319\nTracing interactions between services\ndef get_user_settings(uuid):\n    settings_url = urljoin(\"http://settings:5000/\n➥settings/\", \"{}\".format(uuid))\n    span = get_current_span() \n    span.set_tag(tags.HTTP_METHOD, 'GET') \n    span.set_tag(tags.HTTP_URL, settings_url) \n    span.set_tag(tags.SPAN_KIND, tags.SPAN_KIND_RPC\n➥_CLIENT) \n    span.set_tag('uuid', uuid) \n    headers = {}\n    tracer.inject(span, Format.HTTP_HEADERS, headers) \n    r = requests.get(settings_url, headers=headers)\n    return r.json()\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\nThe SimpleBank profile service initializes a trace that’ll be used to group different \nspans. It creates spans for the calls to \"http://ip.jsontest.com/\" and for the call to \nthe SimpleBank settings service. For the former, given that you don’t own the service, \nyou execute the call wrapped in a span. But for the latter, because you control it, you \ncan pass on span information that’ll be used to create child spans. This will allow you to \ngroup all related calls in Jaeger.\nLet’s now look into how you can make use of the injected span in the SimpleBank \nsettings service, as shown in the following listing.\nListing 12.11    Using a parent span in the settings service\nimport time\nfrom random import randint\nimport requests\nfrom flask import Flask, jsonify, request\nfrom opentracing.ext import tags\nfrom opentracing.propagation import Format\nfrom opentracing_instrumentation.request_context import get_current_span\nfrom opentracing_instrumentation.request_context import span_in_context\nfrom lib.tracing import init_tracer\napp = Flask(__name__)\ntracer = init_tracer('simplebank-settings') \n@app.route('/settings/<uuid:uuid>')\ndef settings(uuid):\n    span_ctx = tracer.extract(Format.HTTP_HEADERS, \n➥request.headers) \n    span_tags = {tags.SPAN_KIND: tags.SPAN_KIND_RPC\n➥_SERVER, 'uuid': uuid} \nSets tags for the span\nInjects the span context before \nthe call to the SimpleBank \nsettings service — The span \ncontext will be passed in the \nheaders, and the downstream \nservice will use it to initialize \nits own spans under the \nproper context.\nInitializes the tracer for the service\nExtracts the span context \nfrom the request headers\nSets up the tags for a new span\n \n",
      "content_length": 2254,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 347,
      "content": "320\nChapter 12  Using logs and traces to understand behavior \n    with tracer.start_span('settings', child_of=span\n➥_ctx, tags=span_tags): \n        time.sleep(randint(0, 2))\n        return jsonify({'settings': {'name': 'demo user', 'uuid': uuid}})\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\nBy extracting the span context from the request the settings service receives, you can \nthen make use of it as the parent to new spans. You can later visualize this new child \nspan independently. But you’ll also be able to take advantage of the fact that Jaeger will \nshow the span as both an independent span in the context of the SimpleBank settings \nservice and a child span in the context of the SimpleBank profile service.\n12.5\t Visualizing traces\nWith all the setup out of the way, all you need to do to start collecting traces is to issue \na request to the SimpleBank profile endpoint. You can use the command line or a \nbrowser. To access traces via the command line, you can use curl to issue the following \nrequest:\n$ curl http://localhost:5007/profile/26bc34c2-5959-4679-9d4d-491be0f3c0c0\n{\n  \"ip\": {\n    \"ip\": \"178.166.53.17\"\n  },\n  \"settings\": {\n    \"settings\": {\n      \"name\": \"demo user\",\n      \"uuid\": \"26bc34c2-5959-4679-9d4d-491be0f3c0c0\"\n    }\n  }\n}\nHere’s a brief recap of what’s going on when you hit the profile endpoint:\n¡ The profile service creates a span A.\n¡ The profile service contacts an external service to fetch the IP, wrapping it under \na new span B.\n¡ The profile service contacts the internal SimpleBank settings service to get user \ninfo under a new span C and passes the context of the parent span to the down-\nstream service.\n¡ Both services communicate spans to Jaeger.\nStarts a new span as a child of \nthe one propagated to the \nservice via the request headers\n \n",
      "content_length": 1818,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 348,
      "content": "\t\n321\nVisualizing traces\nFigure 12.13    Jaeger UI search page showing the services that have traces available\nTo visualize the traces, you need to access the Jaeger UI that’ll be running on port \n16686. Figure  12.13 shows the Jaeger UI and the list of services that have traces \navailable.\nIn the Service section, you see three services for which trace information is avail-\nable: two SimpleBank services and one called jaeger-query. The latter gathers Jaeger \ninternal traces and is of little use to you. You’re interested in the other two services \nlisted: simplebank-profile and simplebank-settings. If you recall, the profile service was \ncreating spans for the execution of an external call, as well as for the call to the settings \nservice. Go ahead and select simplebank-profile and click Find Traces at the bottom. \nFigure 12.14 shows the traces for the profile service.\nThe page lists six traces, and all of them have three spans across two services. This \nmeans you were able to collect information about the collaboration between two inter-\nnal services and to get timing information about the execution. Figure 12.15 shows a \ndetailed view of one of those traces.\n \n",
      "content_length": 1180,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 349,
      "content": "322\nChapter 12  Using logs and traces to understand behavior \nFigure 12.14    The simplebank-profile traces information\nFigure 12.15    The timing information and execution sequence of a call to the profile service\n \n",
      "content_length": 217,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 350,
      "content": "\t\n323\nVisualizing traces\nIn figure 12.15, you can see a timeline for the different steps of execution in a call to \nthe profile service. You have information about the overall time of execution, as well \nas when each suboperation took place and for how long. The spans contain informa-\ntion about the operation, the component that generated them, and their execution \ntimes and relative positions, both in the timeline and in regards to dependencies with \nparent spans.\nThis information can be invaluable in order to know what’s going on in a distrib-\nuted system. You can now visualize the flow of requests through different services and \nknow how long each operation takes to complete. This simple setup allows you to both \nunderstand the flow of execution in a microservice architecture and identify potential \nbottlenecks that you can improve.\nYou also can use Jaeger to understand how different components in your system \nrelate to each other. The top navigation menu bar has a Dependencies link. By clicking \nit and then, in the page that comes up, selecting the DAG (direct acyclic graph) tab, you \nhave access to the view illustrated in figure 12.16.\nThe example we used was a simple one, but it allows you to understand the power of \ntracing in a microservice architecture. Along with logging and metrics, it allows you to \nhave an informed view of both the performance and the behavior of your system.\nFigure 12.16    The service dependency view in the Jaeger UI\n \n",
      "content_length": 1475,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 351,
      "content": "324\nChapter 12  Using logs and traces to understand behavior \nSummary\n¡ You can set up a logging infrastructure using Elasticsearch, Kibana, and Fluentd, \nand distributed tracing using Jaeger.\n¡ A logging infrastructure can generate, forward, and store indexed log data that \nallows searching and correlating requests. \n¡ Distributed tracing allows you to follow the journey of execution of requests \nthrough different microservices.\n¡ Alongside metrics collection, tracing allows you to better understand how the \nsystem is behaving, identify potential issues, and audit your system anytime.\n \n",
      "content_length": 595,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 352,
      "content": "325\n13\nBuilding microservice teams\nThis chapter covers\n¡ How a microservice architecture affects your \nengineering culture and organization\n¡ Strategies and techniques for building effective \nmicroservice teams\n¡ Common pitfalls in microservice development\n¡ Governance and best practice in large \nmicroservice applications\nThroughout this book, we’ve focused on the technical side of microservices: how to \ndesign, deploy, and operate services. But it’d be a mistake to examine the technical \nnature of microservices alone. People implement software, and building great soft-\nware is as much about effective communication, alignment, and collaboration as \nimplementation choices.\nA microservice architecture is great for getting things done. It allows you to build \nnew services and capabilities rapidly and independently of existing functionality. \nConversely, it increases the scope and complexity of day-to-day tasks, such as oper-\nations, security, and on-call support. It can significantly change an organization’s \ntechnical strategy. It demands a strong culture of ownership and accountability from \n \n",
      "content_length": 1110,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 353,
      "content": "326\nChapter 13  Building microservice teams\nengineers. Achieving this culture, while minimizing friction and increasing pace, is vital \nto a successful microservice implementation.\nIn this chapter, we’ll begin by discussing team formation in software engineering \nand the principles that make teams effective. We’ll then examine different models for \nengineering team structure and how they apply to microservice development. Lastly, \nwe’ll explore recommended practices for governance and engineering culture within \nmicroservice teams. Throughout the chapter, we’ll touch on and explain how to miti-\ngate some common pitfalls of microservice development.\nAlthough you might not currently work as an engineering manager, a team lead, or a \ndirector, we think it’s essential to understand how these dynamics — and the choices you \nand your organization make — impact the pace and quality of microservice development.\n13.1\t Building effective teams\nSplitting engineers into independent teams is a natural outcome of organizational \ngrowth. Doing so is necessary to help an organization scale effectively, as limiting team \nsize has several benefits:\n¡ It ensures lines of communication remain manageable — figure 13.1 illustrates \nhow these grow — which aids team dynamism and collaboration while easing \nconflict resolution. Many heuristics exist for “right size,” such as Jeff Bezos’ two-\npizza rule or Michael Lopp’s 7 +/– 3 formula.\n¡ It clearly delineates responsibility and accountability while encouraging inde-\npendence and agility.\nSmall, independent teams can typically move faster than large teams. They also gel \nfaster and gain effectiveness more quickly. Contrastingly, distinct engineering teams \ncan also cause new problems:\n¡ Teams can become culturally isolated, following and accepting different prac-\ntices of quality or engineering values.\n¡ Teams may need to invest extra effort to align on competing priorities when they \ncollaborate with other teams.\n¡ Separate teams may isolate specialist knowledge to the detriment of global \nunderstanding or effectiveness.\n¡ Teams can duplicate work, leading to inefficiency.\nMicroservices can exacerbate these divisions. Different teams will likely no longer work \non the same shared body of code. Teams will have different, competing priorities —  \nand be less likely to have a global understanding of the application.\nBuilding an effective engineering organization beyond a small group of people —  \nand developing great software products — is a balancing act between these two tension \npoints: autonomy and collaboration. If boundaries between teams overlap and owner-\nship is unclear, tension can increase; conversely, independent teams still need to collab-\norate to deliver the whole application.\n \n",
      "content_length": 2767,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 354,
      "content": "\t\n327\nBuilding effective teams\n3 individuals\n3 connections\n5 individuals\n10 connections\n7 individuals\n21 connections\nFigure 13.1    Lines of communication by group size\n13.1.1\t Conway’s Law\nIt can be difficult to separate cause and effect in organizations that have successfully \nbuilt microservice applications. Was the development of fine-grained services a logical \noutcome of their organizational structure and the behavior of their teams? Or did that \nstructure and behavior arise from their experiences building fine-grained services?\nThe answer is: a bit of both! A long-running system isn’t only an accumulation of \nfeatures requested, designed, and built. It also reflects the preferences, opinions, and \nobjectives of its builders and operators. This indicates that structure — what teams work \non, what goals they set, and how they interact — will have a significant impact on how \nsuccessfully you build and run a microservice application.\n \n",
      "content_length": 954,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 355,
      "content": "328\nChapter 13  Building microservice teams\nConway’s Law expresses this relationship between team and system:\n…organizations which design systems ... are constrained to produce designs which are \ncopies of the communication structures of these organizations…\n“Constrained” might suggest that these communication structures limit and constrict \nthe effective development of a system. But the inverse of the rule is also true: you can \ntake advantage of changes to team structure to produce a desired architecture. Team \nstructure and microservice architecture are symbiotic: both can and should influ-\nence each other. This is a powerful technique, which we’ll consider throughout this \nchapter.\n13.1.2\t Principles for effective teams\nAt a macro level, it’s best to think of teams as units of achievement and communication. \nThey’re how stuff gets done and how people relate to each other within an organiza-\ntion. To realize benefits from microservices and adequately manage their complexity, \nyour teams will need to adopt new working principles and practices, rather than using \nthe same techniques they used to build monoliths.\nThere’s no single right, perfect way to organize your teams. You’ll always suffer from \nconstraints: headcount, budget, personalities, skill sets, and priorities. Sometimes you \ncan hire to fill a gap; sometimes you can’t. The nature of your application and business \ndomain will demand different approaches and skills. Your organization may be lim-\nited in its capacity to change. The best approach we’ve found is to guide the formation \nof teams using a small set of shared principles: ownership, autonomy, and end-to-end \nresponsibility.\nNOTE    Making a move to microservices — or indeed, any large-scale architec-\ntural change — in many enterprises will be challenging and disruptive. You \nwon’t be successful in isolation: you’ll need to find sponsorship, build trust, \nand be prepared to argue your case — a lot! Richard Rodger’s book, The Tao of \nMicroservices (ISBN 9781617293146), goes into more (if slightly cynical) detail \non navigating these institutional politics.\nOwnership\nTeams with a strong sense of ownership have high intrinsic motivation and exercise a \nconsiderable degree of responsibility for the area they own. Because microservice appli-\ncations are typically long-lived, teams that have long-term ownership of an area sup-\nport the evolution of that code while developing deep understanding and knowledge.\nIn a monolithic application, ownership is typically n:1. Many teams own one service: \nthe monolith. This ownership is often split between different layers (such as frontend \nand backend) or between functional areas (such as orders and payments). In a micro­\nservice application, ownership is usually 1:n, meaning a team might own many services. \nFigure 13.2 depicts these ownership models.\n \n",
      "content_length": 2857,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 356,
      "content": "\t\n329\nBuilding effective teams\nTeam A\nTeam B\nApplication\nTeam C\nTeam A\nA\nB\nC\nI\nE\nD\nF\nG\nH\n1:n ownership\nn:1 ownership\nTeam B\nTeam C\nFigure 13.2    Team ownership in monolithic versus microservice codebases\nWARNING    In the 1:n ownership model, it’s usually bad practice for multiple \nteams to own one service. This can make accountability unclear and lead to \nconflict about technical choices and feature priority.\nAs an organization’s codebase grows and the makeup of the engineering team fluc-\ntuates, the risk of code that no one knows — or code that no one can fix when it \nbreaks — increases. Clear ownership helps you avoid this risk by placing natural, rea-\nsonable bounds on a team’s knowledge while ensuring that ownership is the responsi-\nbility of a group, not individual developers.\nAutonomy\nIt’s not coincidental that these three principles reflect some of the principles of micro-\nservices themselves. Teams that can work autonomously — with limited dependencies \non other teams — can work with less friction. These types of teams are highly aligned \nbut loosely coupled.\nAutonomy is important for scale. For an engineering manager, it’s exhausting to \ncontrol the work of multiple teams (not to mention, disempowering for the teams \nthemselves); instead, you can empower teams to self-manage.\nEnd-to-end responsibility\nA development team should own the full ideate-build-run loop of a product. With con-\ntrol over what’s being built, a team can make rational, local priority decisions; exper-\niment; and achieve a short cycle time between coming up with an idea and validating \nthat idea with real code and users.\nMost software spends significantly longer in operation than it ever spent being built. \nBut many software engineers focus on the build stage, throwing code over the fence for \na separate team to run it. This ultimately results in poorer quality and slower delivery. \nHow software operates — how you observe its behavior in the real world — should feed \nback into improving that software (figure 13.3). Without responsibility for operation, \nthis information is often lost. This tenet is also central to the DevOps movement.\n \n",
      "content_length": 2155,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 357,
      "content": "330\nChapter 13  Building microservice teams\nDesign\nDeploy\nObserve\nFigure 13.3    Software operation should continually inform future design and build.\nEnd-to-end responsibility correlates closely with autonomy and ownership:\n¡ The fewer cross-team dependencies in a team’s path to production, the more \nlikely it can control and optimize the pace of its delivery.\n¡ A wider scope of ownership enables the team to reasonably and productively take \non more responsibility for overall delivery.\n13.2\t Team models\nIn this section, we’ll explore two approaches for structuring teams — by function or \nacross function — and their benefits and disadvantages in developing microservices.\n¡ In a functional approach, you group employees by specialization, with a func-\ntional reporting line, and assign them to time-bound projects. Most organiza-\ntions fund projects for a specific scope and length of time. They measure success \nby the on-time delivery of that scope.\n¡ Teams that you build cross-functionally  — from a combination of different skill-\nsets — typically are aligned to long-term product goals or aspirational missions, \nwith freedom within that scope to prioritize projects and build features as needed \nto achieve those missions. You typically measure success through impact on busi-\nness key performance indicators (KPIs) and outcomes.\nThe latter approach is a natural fit with microservices development.\n13.2.1\t Grouping by function\nTraditionally, many engineering organizations have been grouped along horizontal, \nfunctional lines: backend engineers, frontend engineers, designers, testers, product \n(or project) management, and sysadmin/ops. Figure 13.4 illustrates this type of orga-\nnization. In other cases, teams or individuals may move between any number of time-\nbounded projects.\nThis approach optimizes for expertise:\n¡ It ensures that communication loops between specialists are short, so they share \nknowledge and solutions effectively and apply their skills consistently.\n¡ Similar work and approaches are grouped together, providing clear career \ngrowth and skill development.\n \n",
      "content_length": 2104,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 358,
      "content": "\t\n331\nTeam models\nProject A\nFE dev\nDev\nDesigner\nTester\nProject\nmanager\nSysadmin\nFE dev\nDev\nDesigner\nTester\nFE dev\nDev\nDev\nTester\nProject\nmanager\nSysadmin\nUI team\nBackend team\nDesign team\nTest team\nPMO\nOps\nFigure 13.4    Grouping into teams by function and project\nNow imagine you’re building a new feature. This functional approach almost looks \nlike a chain: the analyst team gathers requirements, engineers build backend services, \ntesting windows are scheduled with the QA team, and sysadmins deploy the service. \nYou can see that this approach involves a high coordination burden — delivering a fea-\nture relies on synchronization across several independent teams (figure 13.5).1 This \napproach fails to meet our three principles for effective organization.\nUnclear ownership\nNo team has clear ownership of business outcomes or value — they’re only cogs in the \nvalue chain. As such, ownership of individual services is unclear: once a project is fin-\nished, who maintains the services that were built? How are these iterated on, improved, \nor discarded? Work allocation based on projects tends to shortchange long-term think-\ning and encourages ownership of code by individual engineers, which you want to avoid.\nSpecs\nBuild\nBuild\nCoordinate\nTest\nDeploy\nFeature\nIdea\nAnalysts\nBackend\nFrontend\nPMO\nTest\nOps\nFigure 13.5    Functional teams contributing to the implementation of a feature\n1\t And lo, the organization invented project managers!\n \n",
      "content_length": 1448,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 359,
      "content": "332\nChapter 13  Building microservice teams\nLack of autonomy\nThese teams are tightly coupled, not autonomous. Their priorities are set elsewhere, and \nevery time work crosses a team boundary, the chance increases that a team will be blocked \nand development will be hampered. This leads to long lead times, rework, quality issues, \nand delays. Without alignment to the system architecture they’re building, the team will \nbe unable to evolve their application without being encumbered by other teams.\nNo long-term responsibility\nA project-oriented approach isn’t conducive to long-term responsibility for the code pro-\nduced or for the quality of a product. If the team is only together for a time-bound project, \nthey might hand off their code to another department to run the application, so the orig-\ninal team won’t be able to iterate on their original ideas and implementation. The organi-\nzation will also fail to realize benefits from knowledge retention in the original team.\nLastly, a new team requires time to normalize productive working behaviors — the \nlonger people work together, the better the team gels, and the more effective it becomes. \nA team that stays together longer will maintain a longer period of high performance.\nTIP    There’s also a risk that long-lived teams can become too comfortable or \nset in their ways. It’s important to balance long-term bonding and bringing \npeople with new perspectives and skills into the team.\nRisk of silos\nLastly, this approach also risks the formation of silos — teams diverge in goals and \nbecome incapable of effective, empathetic collaboration. Hopefully you’ve never \nworked someplace where the relationship between test and dev, or dev and ops, is \nalmost adversarial, but it’s been known to happen.\nUltimately, it’s unlikely that a functional, project-oriented organization will deliver \na microservice application without incurring significant friction and substantial cost.\n13.2.2\t Grouping across functions\nBy optimizing for expertise, the functional approach aims to eliminate duplicated work \nand skill-based inefficiencies, in turn reducing overall cost. But this can cause gridlock: \nincreasing friction and reducing your speed in achieving organizational goals. This isn’t \ngreat — your microservice architecture was meant to increase pace and reduce friction.\nLet’s look at an alternative. Instead of grouping by function, you can work cross- \nfunctionally. A cross-functional team is made up of people with different specialties and \nroles intended to achieve a specific business goal. You could call these teams market-driven: \nthey might aim toward a specific, long-term mission; build a product; or connect directly \nwith the needs of their end customer. Figure 13.6 depicts a typical cross-functional team.\nNOTE    We won’t cover team leadership or reporting lines in any detail in \nthis book. A product owner, an engineering lead, a technical lead, a project \nmanager, or a partnership between those roles might lead a cross-functional \nteam. For example, at Onfido, a product manager — who focuses on what the \nteam should do — and an engineering lead — who focuses on how to achieve \nit — lead our teams in partnership.\n \n",
      "content_length": 3208,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 360,
      "content": "\t\n333\nTeam models\nProduct\nmanager\nFE dev\nDev\nTest\nengineer\nDesigner\nDev\nTeam\nFigure 13.6    A typical cross-functional development team\nCompared to the functional approach, a cross-functional team can be more closely \naligned with the end goal of the team’s activity. The multidisciplinary nature of the team \nis conducive to ownership. By taking on end-to-end responsibility for specification, \ndeployment, and operation, the team can work autonomously to deliver features. The \nteam gains clear accountability by taking on a mission that has a meaningful impact on \nthe business’s success. Day-to-day partnership between different specialists eliminates \nsilos, as team members share ownership for the ultimate product of the team’s work.\nDesigning these teams to be long-lived (for example, at least six months) is also ben-\neficial. A long-lived team builds rapport, which increases their effectiveness, and shared \nknowledge, which increases their ability to optimize and improve the system under devel-\nopment. They also take long-term responsibility for the operation of the microservice \napplication, rather than handing it off to another team.\nThe cross-functional, end-to-end approach to structuring teams is advantageous to \nmicroservice development:\n¡ Aligning teams with business value will be reflected in the application developed; \nthe teams will build services that explicitly implement business capabilities.\n¡ Individual services will have clear ownership. \n¡ Service architecture will reflect low coupling and high cohesiveness of teams.\n¡ Functional specialists in different teams can collaborate informally to develop \nshared practices and ways of working.\nThis approach is common in modern web enterprises and is often cited as a reason \nfor their success. For example, Amazon’s CTO described the company’s approach to \narchitecture in 2006:\nIn the fine grained services approach that we use at Amazon, services do not only represent \na software structure but also the organizational structure. The services have a strong \nownership model, which combined with the small team size is intended to make it very easy \nto innovate. In some sense you can see these services as small startups within the walls of \na bigger company. Each of these services require a strong focus on who their customers are, \nregardless whether they are externally or internally.\n-—Werner Vogels\n \n",
      "content_length": 2395,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 361,
      "content": "334\nChapter 13  Building microservice teams\nPerhaps most importantly, a well-formed cross-functional team will be faster at deliver-\ning features than a group of functional teams, as lines of communication are shorter, \ncoordination is local, and team members are aligned. The cross-functional approach \nprioritizes pace — but not at the expense of quality!\n13.2.3\t Setting team boundaries\nA cross-functional team should have a mission. A mission is inspirational: it gives the \nteam something to strive toward but also sets the boundaries of a team’s responsibili-\nties. Determining what a team is (and isn’t) responsible for encourages autonomy and \nownership while helping other teams align with each other. A mission is usually a busi-\nness problem; for example, a growth team might aim to maximize recurring spend by \ncustomers, whereas a security team might aim to protect its codebase and data from \nknown and novel threats. Based on this mission, each team prioritizes its own roadmap \nin collaboration with relevant partners within the business. Cross-cutting initiatives are \ndriven by product or technical leadership.\nNOTE    This type of team organization is also known as product mode  — which \ndoesn’t mean each team is working on a self-contained product. Teams might \nown different vertical slices or different horizontal components of the same \nproduct. A particular component might be technically complex enough to \ndemand a dedicated team.2\nIf your company offers a range of small products — that a team of 7 +/– 3 can produc-\ntively work on — each team can be responsible for one product (figure 13.7). This isn’t \nthe case in many companies such as those that offer a large, complex product to mar-\nket, requiring the effort of multiple teams.\nFor larger scale scenarios, bounded contexts — covered in chapter 4 — are an effec-\ntive starting point for setting loose boundaries for different teams in an organization. \nThey also have the benefit of creating teams that map closely to business teams within \nthe enterprise; for example, a warehouse product team will interact closely with ware-\nhouse operations.3 Figure 13.8 illustrates a possible model for teams within SimpleBank.\nTeam A\nOwns\nServices\nProduct A\nTeam B\nOwns\nServices\nProduct B\nFigure 13.7    A team-per-product model\n2\t A recent ThoughtWorks article describes these product-mode teams: Sriram Narayan, “Products \nOver Projects,” February 20, 2018, http://mng.bz/r0v4.\n3\t Be careful about how you approach this: the organizational structure itself might be suboptimal!\n \n",
      "content_length": 2558,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 362,
      "content": "\t\n335\nTeam models\nCustomers\nAccounts\nCustomers\nStrategies\nPlaceStrategyOrder\nInvestment\nstrategies\nInvestment\nMarket &\nresearch\nCash\nAsset\ninformation\nPayments\nOrders\nMarket gateway\nOrders\nResponsible\nteam\nContexts and\nservices\nFigure 13.8    A possible model of service and capability ownership by different engineering teams for \nSimpleBank\nForming teams that own services in specific bounded contexts makes use of the inverse \nversion of Conway’s Law: if systems reflect the organizational structure that produces \nthem, then you can attain a desirable system architecture by first shaping the structure \nand responsibilities of your organization.\nAs with services themselves, the right boundaries between teams may not always be \nobvious. We keep two general rules in mind:\n¡ Watch the team size. If it approaches or surpasses nine people, it’s likely that a team \nis doing too much or beginning to suffer from communication overhead.\n¡ Consider coherence. Are the activities the team does cohesive and closely related? If not, \na natural split may exist within the team between different groups of coherent work.\n13.2.4\t Infrastructure, platform, and product\nAlthough we’ve advocated strongly for end-to-end ownership, it isn’t always practical. For \nexample, the underlying infrastructure — or microservice platform — of a large company \nis typically complex and requires a joined-up roadmap and dedicated effort, rather than \nloose collaboration between DevOps specialists spread across distinct teams.\nAs we outlined earlier in the book, building a microservice platform — deployment pro-\ncesses, chassis, tooling, and monitoring — is vital to sustainably and rapidly building a great \nmicroservice application. When you first start working with microservices, the team build-\ning the application will usually own the task of building the platform too (figure 13.9).\nOver time, this platform will need to serve the needs of multiple teams, at which stage \nyou might establish a platform team (figure 13.10).\nMicroservice team\nSupports\nBuilds\nBuilds\nApplication\nPlatform\nFigure 13.9    Early on, one team builds both the microservice application and the supporting platform.\n \n",
      "content_length": 2184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 363,
      "content": "336\nChapter 13  Building microservice teams\nProduct teams\nA\nB\nC\nBuilds\nBuilds\nSupports\nPlatform\nservices\nApplication\nEnables\nPlatform team\nFigure 13.10    Establishing a platform team\nDepending on the needs of your company and your technical choices, you might split \nthis platform team further (figure 13.11) to distinguish core infrastructural concerns \n(such as cloud management and security) from specific microservice platform con-\ncerns (such as deployment and cluster operation). This is especially common in com-\npanies that operate their own infrastructure, rather than using a cloud provider.\nProduct tier\nApplication\nServices\nPlatform tier\nObservability\nDeployment\nCluster\nSecurity\nNetworking\nStorage\n...\n...\nA\nB\nC\nBuilds\nBuilds\nBuilds\nSupports\nSupports\nPlatform\nInfrastructure\nInfrastructure tier\nEnables\nEnables\nFigure 13.11    Establishing an infrastructure team as one tier in a three-tier model\n \n",
      "content_length": 913,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 364,
      "content": "\t\n337\nTeam models\nIn an even larger engineering organization, these tiers might be separated further; for \nexample, different platform teams might focus on deployment tools, observability, or \ninter-service communication. This is also illustrated in figure 13.11.\nThe three-tier model shown in the figure provides economies of scale and special-\nization. This isn’t a service relationship, where teams log tickets to each other. Instead, \nthe output of each tier is a “product” that enables teams in the layer above to be more \neffective and productive.\n13.2.5\t Who’s on-call?\nThe DevOps movement has been a strong influence on microservice approaches. \nA DevOps mentality — breaking down the barriers between build and runtime — is \nvital for doing microservices well, as deploying and operating multiple applications \nincreases the cost and complexity of operational work. This movement encourages \na “you build it, you run it” mindset; a team that takes responsibility for the oper-\national lifetime of their services will build a better, more stable and more reliable \napplication. This includes being on-call — ready to answer alerts — for your produc-\ntion services.\nTIP    Chapter 11 covers best practices for triggering useful and actionable alerts \nfrom microservices.\nFor example, in the three-tier model:\n¡ Engineering teams would be on-call for alerts from their own services.\n¡ Platform and infrastructure teams would be on-call for issues in underlying infra-\nstructure or shared services, such as deployment.\n¡ An escalation path would exist between those two teams to support investigation.\nThis on-call model is illustrated in figure 13.12.\nProduct\nPlatform\nInfrastructure\nServices\nPlatform\nInfrastructure\nAlerts\nAlerts\nAlerts\nErrors, anomalous latency/\nthroughput, events from\napplication code\nErrors, saturation, anomalies\nfrom underlying cluster or tools,\nfor example, cluster, databases,\ndeployment\nErrors, saturation, anomalies\nfrom underlying infrastructure,\nfor example, network\nTeams\nApplication\nFigure 13.12    On-call model in a three-tier microservice team structure\n \n",
      "content_length": 2097,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 365,
      "content": "338\nChapter 13  Building microservice teams\nOf the many changes that microservices bring, this may the most difficult to roll out: \nengineers are likely to resist being on-call, even for their own code. A successful on-call \nrotation should be\n¡ Inclusive  — Everyone who can do it, should do it, including VPs and directors.\n¡ Fair  — On-call work should be remunerated in addition to normal working \nhours.\n¡ Sustainable  — Enough engineers should be in a rotation to avoid burnout and \navoid disruption to work-life balance or day-to-work in the office.\n¡ Reflective  — Your team should constantly review alerts and pages to ensure only \nalerts that matter wake someone up.\nIn this model, we split alerts across teams, because running software at scale is complex. \nOperational effort might be beyond the scope or knowledge of engineers within any \none team. Many operational tasks — such as operating an Elasticsearch cluster, deploy-\ning a Kafka instance, or tuning a database — require specific expertise that would be \nunreasonable to expect product engineers to gain uniformly. Operational work also \nruns at a cadence different from the pace of product delivery.\nWARNING    Historically, infrastructure operations teams have been responsible \nfor running applications in production: keeping them stable and waking up \nwhen they break. This leads to tension: operations teams resent developers \nthrowing unstable applications over the wall, whereas developers curse the lack \nof engineering skills in the operations team. This separate dev and ops model \nputs the onus for fixing production issues on the wrong team. Instead, if devel-\nopers are responsible for how their code operates, they’ll be better able to fix \nincidents and optimize that code in the long term.\nThe right choice for an on-call model that balances responsibility and expertise will \ndepend on the types of applications you build, the throughput of those applications, \nand the underlying architecture you choose. If you’re interested in learning more, \nIncrement recently published an in-depth review (https://increment.com/on-call/ \nwho-owns-on-call/) of on-call approaches used at Google, PagerDuty, Airbnb, and \nother organizations.\n13.2.6\t Sharing knowledge\nAlthough autonomous teams increase development pace, they have two downsides:\n¡ Different teams may solve the same problem multiple times in different ways.\n¡ Team members will have less engagement with their specialist peers on other \nteams.\n¡ Team members may make local decisions without considering the global context \nor the needs of the wider organization.\n \n",
      "content_length": 2608,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 366,
      "content": "\t\n339\nTeam models\nYou can mitigate these issues. We’ve had success applying Spotify’s model of chapters \nand guilds.4 These are communities of practice:\n¡ Chapters group people by functional specialties, for example, mobile \ndevelopment.5\n¡ A guild shares practice around a cross-cutting theme, for example, performance, \nsecurity.\nFigure 13.13 depicts this model.\nComparably, some organizations use matrix management to establish a formal iden-\ntity for functional units. This adds a line of management responsibility (head of QA, \nhead of design…) for functions, at the cost of building a more complicated manage-\nment structure.\nTIP    Most engineers have been taught to follow the DRY tenet — don’t repeat \nyourself. Within a service, this is still important — there’s no point in writing \nthe same code twice! Across multiple services, this is much less of an impera-\ntive because writing shared code that’s truly reusable is a costly endeavor, as is \ncoordinating the rollout of that code across multiple consumers. A degree of \nduplication is acceptable if it means you can deliver features more rapidly.\nFE dev\nDev\nDesigner\nTest\nPayments\nFE dev\nFE dev\nDev\nTest\nGrowth\nFE dev\nDev\nDev\nInternational\nFrontend\nchapter\nPerformance\nguild\nFigure 13.13    The chapters, guilds, and teams model\n4\t See Henrik Kniberg, “Scaling Agile @ Spotify with Tribes, Squads, Chapters & Guilds,” Crisp’s Blog, \nNovember 14, 2012, http://mng.bz/94Lv.\n5\t In larger organizations, a chapter may group by functional specialty within an engineering divi-\nsion. (Spotify calls this a tribe.)\n \n",
      "content_length": 1575,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 367,
      "content": "340\nChapter 13  Building microservice teams\nEither approach works well to disseminate knowledge and develop shared working \npractices. This helps to prevent the isolation that can arise in highly autonomous \nteams, ensuring teams remain aligned technically and culturally. Cross-pollination \nof ideas, solutions, and techniques also supports people moving between teams and \nreduces organization-level bus factor risks.\nIt’s also important to strike a balance between team lifetime and team fluidity. In the \nlong run, regularly rotating engineers between teams helps to share knowledge and \nskills and is a good complement for the chapter and guild model.\n13.3\t Recommended practices for microservice teams\nThe scale of change in a microservice application can be tremendous. It can be diffi-\ncult to keep up! It’s unreasonable to expect any engineer to have a deep understand-\ning of all services and how they interact, especially because the topography of those \nservices may change without warning. Likewise, grouping people into independent \nteams can be detrimental to forming a global perspective. These factors lead to some \ninteresting cultural implications:\n¡ Engineers will design solutions that are locally optimal — good for them or their \nteam — but not always right for the wider engineering organization or company.\n¡ It’s possible to build around problems rather than fixing them, or to deploy new \nservices instead of correcting issues with existing services.\n¡ Practices on teams might become highly local, making it difficult for engineers \nto move between teams.\n¡ It’s challenging for architects or engineering leads to gain visibility and make \neffective decisions across the entire application.\nGood engineering practices can help you avoid these problems. In this section, we’ll \nwalk through some of the practices that your teams should follow when building and \nmaintaining services.\n13.3.1\t Drivers of change in microservices\nTake a moment and consider the type of build items you might work on day to day. \nIf you’re on a product team, the items in your backlog are primarily functional addi-\ntions or changes. You want to launch a new feature; support a new request from a \ncustomer; enter a new market; and so on. As such, you build and change microservices \nin response to these new functional requirements. And, thankfully, microservices are \nintended to ensure your application is flexible in the face of change.\nBut functional requirements — changes from your business domain — aren’t the only \ndriver of change in services. Each microservice will change for many reasons (figure 13.14):\n¡ Underlying frameworks and dependencies (such as Rails, Spring, or Django) \nmay require upgrades for performance, security, or new features.\n¡ The service may no longer be fit for the purpose — for example, hitting natural \nscalability limits — and may require change or replacement.\n¡ You discover defects in the service or the service’s dependencies.\n \n",
      "content_length": 2979,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 368,
      "content": "\t\n341\nRecommended practices for microservice teams\nNew requirements\nDefects\nDependency\nchanges\nScalability\nissues\nSecurity issues,\nfor example, CVEs\nFramework\nchanges\nService\nFigure 13.14    Drivers of change to a microservice\nAll this change increases complexity. For example, instead of tracking security vul-\nnerabilities against a single monolithic application, you need to ensure your tooling \nsupports static analysis and alerting across several applications (and likely several \ndistinct programming languages and frameworks). Every new service generates \nmore work.\nAlternatively, some microservice practitioners have advocated immutable services —  \nonce a service is considered mature, put it under feature freeze, and add new services \nif change is required. There’s a tricky cost-benefit decision here: is the risk of breaking \na service through modification more than the cost of building a new service? It’s a diffi-\ncult question to answer definitively and will depend on both your business context and \nappetite for risk.\n13.3.2\t The role of architecture\nMicroservice applications evolve over time: teams build new services; decommission \nexisting services; refactor existing functionality; and so on. The faster pace and more \nfluid environment that microservices enable change the role of architects and techni-\ncal leads.\nArchitects have an important role to play in guiding the scope and overall shape of \nan application. But they need to perform that role without becoming a bottleneck. A \nprescriptive and centralized approach to major technical decisions doesn’t always work \nwell in a microservice application:\n¡ The microservice approach and the team model we’ve outlined should empower \nlocal teams to make rapid, context-aware decisions without layers of approval.\n¡ The fluidity of a microservice environment means that any overarching technical \nplan or desired model of the intended system will quickly pass its use-by date, as \nrequirements change, services evolve, and the business itself matures.\n¡ The volume of decisions increases with the number of services, which can over-\nwhelm an architect and make them a bottleneck.\n \n",
      "content_length": 2160,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 369,
      "content": "342\nChapter 13  Building microservice teams\nThat doesn’t mean that architecture isn’t useful or necessary. An architect should have \na global perspective and make sure the global needs of the application are met, guid-\ning its evolution so that\n¡ The application is aligned to the wider strategic goals of the organization.\n¡ Technical choices within one team don’t conflict with choices in another.\n¡ Teams share a common set of technical values and expectations.\n¡ Cross-cutting concerns — such as observability, deployment, and interservice \ncommunication — meet the needs of multiple teams.\n¡ The whole application is flexible and malleable in the face of change.\nThe best starting point for architecture is to set principles. Principles are guidelines (or \nsometimes rules) that teams should follow to achieve higher level goals. They inform \nteam practice. Figure 13.15 illustrates this model.\nFor example, if your product goal is to sell to privacy- and security-sensitive enter-\nprises, you might set principles of compliance with recognized external standards, data \nportability, and clear tracking of personal information. If your goal is to enter a new \nmarket, you might mandate flexibility around regional requirements, design for multi-\nple cloud regions, and out-of-the-box support for i18n (figure 13.16).\nPrinciples are flexible. They can and should change to reflect the priorities of the \nbusiness and the technical evolution of your application. For example, early develop-\nment might prioritize validating product-market fit, whereas a more mature applica-\ntion might require a focus on performance and scalability.\nCompany and product goals\nAchieve\nTechnical principles\nInform\nTeam practices and decisions\nFigure 13.15    An architectural approach based on technical principles\n \n",
      "content_length": 1802,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 370,
      "content": "\t\n343\nRecommended practices for microservice teams\nGoal\nEnter a new market\nPrinciples\n1. Support flexible regional\nrequirements\n2. Design for multiple cloud regions\n3. Support i18n\nPractices\n1. Regional rules are codified\n2. Use multimaster storage (for\nexample, Cassandra); services are\ndesigned using 12-factor principles\n3. Choose UI frameworks with good\ni18n support\nAchieve\nInform\nFigure 13.16    Principles and practices to support entering a new market\nSeveral day-to-day practices support this evolutionary approach to architecture, such \nas design review, an inner-source model, and living documentation. We’ll discuss them \nover the next few sections.\n13.3.3\t Homogeneity versus technical flexibility\nA tricky decision you’ll face is which languages to use to write microservices. Although \nmicroservices provide for technical freedom, using a wide range of languages and \nframeworks can increase risk:\n¡ Bus factor and key person dependencies may increase because of limited shared \nknowledge, making it difficult to maintain and support services.\n¡ Services in new languages may not meet production readiness standards.\nIn practice, you’ll always encounter scenarios where you need to pick a different lan-\nguage, such as specialist features or performance needs. For example, Java would be \nill-suited to writing systems infrastructure, just as Ruby doesn’t have the depth of scien-\ntific and machine learning libraries available to Python. In these scenarios, it’s import-\nant to share the development of services in new languages/frameworks across many \nteam members to reduce bus factor risk: rotate team members, have a pair program, \nwrite documentation, and mentor new engineers.\nPicking a single primary language, or a small set, allows you to better optimize prac-\ntices and approach for that language. The creation of service templates, chassis, and/\nor exemplars will naturally ease development in your favored language, leading more \ndevelopers to write services using it. Lowering friction this way creates a virtuous cir-\ncle. Even if you don’t explicitly choose a favored language, this can happen organically \n(although it’ll take longer).\nTIP    Microservices should be replaceable. If needed, you should be able to \nrewrite any service in a more favorable programming language.\n13.3.4\t Open source model\nApplying open source principles to microservice code can help to alleviate contention \nand technical isolation while improving knowledge sharing. As we mentioned earlier, \neach team in a microservice organization typically owns multiple services. But each \nservice you run in production must have a clear owner: a team that takes long-term \nresponsibility for that service’s functionality, maintenance, and stability.\n \n",
      "content_length": 2754,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 371,
      "content": "344\nChapter 13  Building microservice teams\nThat doesn’t mean those people must be the only contributors to that service. Other \nteams might need to tweak functionality to meet their needs or fix defects. If these \nchanges all needed the same group of people to make them, those people would be at \nthe mercy of their own priorities, which in turn would slow other teams down.\nInstead, an inner-source model — open source within your organization — balances \nownership and visibility:\n¡ Source code should be available internally for any service.6\n¡ Any engineer can submit pull requests to any service, as long as the service owner \nreviews them.\nThis model (figure 13.17) closely resembles most open source projects, where a core \ngroup of committers make most commits and key decisions, and others can submit \nchanges for approval. Imagine an engineer on Team A needs to make a change to a \nservice that Team B owns. They could argue for the priority of their change against \neverything else on Team A’s backlog, or they could pull the code, make the change \nthemselves, and submit a pull request for Team B to review.\nThis approach has three benefits:\n¡ Alleviates contention and priority negotiation between teams\n¡ Reduces the sense of technical isolation and possessiveness that can develop \nwhen service work is limited to a small number of people within an organization\n¡ Shares knowledge within an organization by helping engineers understand other \nteams’ services and better understand the needs of their internal consumers\nTeam A\nSubmits\nSubmits\nReviews\nMerged\nOwns\nService\ncodebase\nPull\nrequests\nTeam N\nTeam B\nFigure 13.17    Applying an open source model to service development\n6\t In some organizations, reasonable exceptions to this rule may apply, such as when code is highly \nsensitive.\n \n",
      "content_length": 1807,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 372,
      "content": "\t\n345\nRecommended practices for microservice teams\nNOTE    Contributing across multiple services is significantly easier when those \nservices follow common architectural and deployment conventions — like the \nones we’ve discussed throughout this book!\n13.3.5\t Design review\nEach new microservice is a blank slate. Each service will have different performance \ncharacteristics; might be written in a different language; might require new infrastruc-\nture; and so on. A new feature might be possible to write in several ways: as a new ser-\nvice, as many services, or within an existing service. This freedom is terrific, but a lack \nof oversight can result in\n¡ Inconsistency  — For example, a service might not log requests consistently, ham-\npering common operational tasks, such as investigating defects.\n¡ Suboptimal design decisions  — You might build multiple services, when a single ser-\nvice would be more maintainable and perform better.\nA few methods can help you get around this issue. In chapter 7, we discussed using \nservice chassis and service exemplars as best practice starting points. But that’s only a \npartial solution.\nIn our own company — comparable to practices at Uber and Criteo — we follow a \ndesign review process. For any new service or substantial new feature, the engineer \nresponsible produces a design document (we call this an RFC, or request for com-\nments) and asks for feedback from a group of reviewers, both in and outside of their \nown team. Table 13.1 outlines the sections in a typical design review document.\nTable 13.1    Sections in a design review document for a new microservice\nSection\nPurpose\nProblem & Context\nWhat technical and/or business problem does this feature solve? Why are \nwe doing this?\nSolution\nHow are you intending to solve this problem?\nDependencies & Integration\nHow does it interact with existing or planned services/functionality/\ncomponents?\nInterfaces\nWhat operations might this service expose?\nScale & Performance\nHow does the feature scale? What are the rough operational costs?\nReliability\nWhat level of reliability are you aiming for?\nRedundancy\nBackups, restores, deployment, fallbacks\nMonitoring & Instrumentation\nHow will you understand this service’s behavior?\nFailure Scenarios\nHow will you mitigate the impact of possible failures?\nSecurity\nThreat model, protection of data, and so on\nRollout\nHow will you launch this feature?\nRisks & Open Questions\nWhat risks have you identified? What don’t you know?\n \n",
      "content_length": 2482,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 373,
      "content": "346\nChapter 13  Building microservice teams\nThis process catches suboptimal design decisions early in the development cycle. \nAlthough writing a document may seem like extra effort, having a semiformal prompt \nto consider service design tends to result in faster overall development, as the team \nbrings to light the full range of considerations and tradeoffs before committing to an \nimplementation direction.\n13.3.6\t Living documentation\nAs we’ve mentioned, it’s difficult to keep a microservice architecture in your head. The \nscale of a microservice application demands that your team invest time in documenta-\ntion. For each service, we recommend a four-layered approach: overviews, contracts, \nrunbooks, and metadata. Table 13.2 details these four layers.\nTable 13.2    Recommended minimum layers for documenting microservices\nType\nSummary\nOverview\nAn overview of the service’s purpose, intended usage and overall architecture. Service over-\nviews should be an entry point for team members and service users.\nContract\nA service contract should describe the API that a service provides. Depending on transport \nmechanism, this can be machine-readable, for example, using Swagger (HTTP APIs) or proto-\ncol buffers (gRPC).\nRunbooks\nDocumented runbooks for production support detailing common operational and failure \nscenarios\nMetadata\nFacts about a service’s technical implementation, such as the programming language, major \nframework versions, links to supporting tools, and deployment URLs\nThis documentation should be discoverable in a registry  — a single website where \ndetails for all services are available. Good microservice documentation serves many \npurposes:\n¡ Developers can discover the capabilities of existing services, such as the contracts they \nexpose. This speeds up development and may reduce wasted or duplicated work.\n¡ On-call staff can use runbooks and service overviews to diagnose issues in pro-\nduction, as different services will vary operationally.\n¡ Teams can use metadata to track service infrastructure and answer questions, for \nexample, “How many services are running Ruby 2.2?”\nMany tools exist for writing project documentation, such as MkDocs (www.mkdocs.\norg). You could combine them with service metadata approaches, as described in table \n13.2, to build a microservice registry.\nTIP    Documentation is notoriously hard to keep up to date, even for a single \napplication. As much as feasible, you should aim to autogenerate documenta-\ntion from application state. For example, you can generate contract documen-\ntation from Swagger YML files using the swagger-ui library.\n \n",
      "content_length": 2619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 374,
      "content": "\t\n347\nFurther reading\n13.3.7\t Answering questions about your application\nAs a service owner or an architect, you’ll often want to get an overarching view of the \nstate of your application to answer questions like\n¡ How many services are written in each language?\n¡ Which services have security vulnerabilities or outdated dependencies?\n¡ What upstream and downstream collaborators use Service A?\n¡ Which services are production-critical? Which are spikes and experiments, or \nless important to critical application paths?\nAt the time of this writing, few tools exist in the wild that combine this information \nto make it readily available. When it’s available, it’s typically spread across multiple \nlocations:\n¡ Language and framework choices require code analysis or repository tagging.\n¡ Dependency management tools (for example, Dependabot) scan for outdated \nlibraries.\n¡ Continuous integration jobs run arbitrary static analysis tasks.\n¡ Network metrics and code instrumentation surface relationships between \nservices.\nSimilar information might be kept in spreadsheets or architectural diagrams, which, \nsadly, are often out of date.\nA recent presentation from John Arthorne at Shopify7 proposed embedding a file, \nservice.yml, in each code repository and using that as a source of service metadata. This \nis a promising idea, but at the time of this writing, you’ll need to roll your own.\n13.4\t Further reading\nForming, growing, and improving engineering teams is a broad topic, and in this chap-\nter we’ve only scratched the surface. If you’re interested in learning more, we recom-\nmend the following books as good places to start:\n¡ Elastic Leadership, by Roy Osherove (ISBN 9781617293085)\n¡ Managing Humans, by Michael Lopp (ISBN 9781430243144)\n¡ Managing the Unmanageable, by Mickey W. Mantle and Ron Lichty (ISBN \n9780321822031)\n¡ PeopleWare, by Tom DeMarco and Timothy Lister (ISBN 9780932633439)\nWe’ve covered a lot of ground in this chapter. Choosing a microservice engineering \napproach is great for getting things done and empowering engineers, but changing \nyour technical foundation is only half the battle. Any system is deeply intertwined with \n7\t See John Arthorne, “Tracking Service Infrastructure at Scale,” SRECon San Francisco, March 13, \n2017, http://mng.bz/Z6d0.\n \n",
      "content_length": 2295,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 375,
      "content": "348\nChapter 13  Building microservice teams\nthe people building it — successful, sustainable development requires close collabora-\ntion, communication, and rigorous and responsible engineering practices.\nIn the end, people deliver software. Getting the best product out requires getting the \nbest out of your team.\nSummary\n¡ Building great software is as much about effective communication, alignment, \nand collaboration as implementation choices.\n¡ Application architecture and team structure have a symbiotic relationship. You \ncan use the latter to change the former.\n¡ If you want teams to be effective, you should organize them to maximize auton-\nomy, ownership, and end-to-end responsibility.\n¡ Cross-functional teams are faster and more efficient at delivering microservices \nthan a traditional, functional approach.\n¡ A larger engineering organization should develop a tiered model of infrastruc-\nture, platform, and product teams. Teams in lower tiers enable higher tier teams \nto work more effectively.\n¡ Communities of practice, such as guilds and chapters, can share functional \nknowledge.\n¡ A microservice application is difficult to fit in your head, which leads to chal-\nlenges for global decision making and on-call engineers.\n¡ Architects should guide and shape the evolution of an application, not dictate \ndirection and outcomes.\n¡ Inner-source models improve cross-team collaboration, weaken feelings of pos-\nsessiveness, and reduce bus factor risks.\n¡ Design reviews improve the quality, accessibility, and consistency of microservices.\n¡ Microservice documentation should include overviews, runbooks, metadata, \nand service contracts.\n \n",
      "content_length": 1659,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 376,
      "content": "349\nappendix \nInstalling Jenkins on Minikube\nThis appendix covers\n¡ Running Jenkins on Minikube\n¡ A short introduction to Helm\nThis appendix will walk you through the process of running Jenkins on your local \nMinikube cluster, which we use in the examples in chapter 10.\nRunning Jenkins on Kubernetes\nYou can run Jenkins as another service on the local Kubernetes cluster — Minikube —  \nyou set up in chapter 9. If you’re working from scratch, follow the installation \ninstructions on GitHub to get Minikube running (https://github.com/kubernetes/\nminikube). Once you have it installed, bring up the cluster by running minikube \nstart at your terminal.\nThe Jenkins application consists of a master node and, optionally, any number \nof agent nodes. Running a Jenkins job executes scripts (such as make) across agent \nnodes to perform deployment activities. A job operates within a workspace  — a local \ncopy of your code repository. Figure A.1 illustrates this architecture.\nYou’ll use Helm to install an “official” Kubernetes-ready configuration of Jenkins \non your Minikube cluster.\n \n",
      "content_length": 1086,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 377,
      "content": "350\nAppendix  Installing Jenkins on Minikube\nJenkins master\nJenkins agent\nWorkspace\nJob\nOrchestrates\nPulls \nfrom\nCode repo\nResources,\nfor example,\ninfrastructure\nInteracts with\nFigure A.1  A high-level Jenkins architecture\nSetting up Helm\nYou can think of Helm (https://helm.sh/) as a package manager for Kubernetes. \nHelm’s package format is a chart, which defines a set of Kubernetes object templates. \nCommunity-developed charts, like the one you’ll use for Jenkins, are stored on Github \n(https://github.com/helm/charts).\nHelm consists of two components:\n¡ A client, which you’ll use to interact with Helm charts\n¡ A server-side application (also known as Tiller), which performs installation of charts\nThis is illustrated in figure A.2.\nInstallation instructions for Helm are on Github (https://github.com/helm/helm). \nFollow them to get the Helm client running on your machine. Once you’ve installed \nHelm, you’ll need to set up Tiller on Minikube. Run helm init on the command line to \nset up this component.\nCreate a namespace and a volume\nBefore you install the Jenkins chart, you need to create two things:\n¡ A new namespace to logically segregate your Jenkins objects within the cluster\n¡ A persistent volume to store Jenkins configuration, even if you restart Minikube\nEngineer\nHelm client\nHelm install\nTiller\nKubernetes cluster\nDeploys\nApplication\nFigure A.2  Components of Helm, a package manager for Kubernetes\n \n",
      "content_length": 1428,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 378,
      "content": "\t\n351\nRunning Jenkins on Kubernetes\nTIP    You can find all the templates we use in this chapter in the book’s code \nrepo on Github: https://github.com/morganjbruce/microservices-in-action.\nTo create the namespace, apply the following template to your Minikube cluster, using \nkubectl apply -f <file_name>.\nListing A.1  jenkins-namespace.yml\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: jenkins\nAnd do the same again for the persistent volume, as follows.\nListing A.2  jenkins-volume.yml\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: jenkins-volume\n  namespace: jenkins\nspec:\n  storageClassName: jenkins-volume\n  accessModes:\n    - ReadWriteOnce\n  capacity:\n    storage: 10Gi\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /data/jenkins/\nInstalling Jenkins\nYou’ll install Jenkins with the community Helm chart. This chart is pretty complex; \nif you’re interested, you can explore it on Github: https://github.com/helm/charts/\ntree/master/stable/jenkins.\nFirst, create a values.yml file. Helm will interpolate the following code into the Jen-\nkins chart to set appropriate defaults for running on Minikube.\nListing A.3  values.yml\nMaster:\n  ServicePort: 8080\n  ServiceType: NodePort\n  NodePort: 32123\n  ScriptApproval:\n \n",
      "content_length": 1257,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 379,
      "content": "352\nAppendix  Installing Jenkins on Minikube\n    - \"method groovy.json.JsonSlurperClassic parseText java.lang.String\"\n    - \"new groovy.json.JsonSlurperClassic\"\n    - \"staticMethod org.codehaus.groovy.runtime.DefaultGroovyMethods \nleftShift java.util.Map java.util.Map\"\n    - \"staticMethod org.codehaus.groovy.runtime.DefaultGroovyMethods split \njava.lang.String\"\n  InstallPlugins:\n    - kubernetes:1.7.1 \n    - workflow-aggregator:2.5 \n    - workflow-job:2.21 \n    - credentials-binding:1.16 \n    - git:3.9.1 \nAgent:\n  volumes:\n    - type: HostPath\n      hostPath: /var/run/docker.sock\n      mountPath: /var/run/docker.sock\nPersistence:\n  Enabled: true\n  StorageClass: jenkins-volume \n  Size: 10Gi\nNetworkPolicy:\n  Enabled: false\n  ApiVersion: extensions/v1beta1\nrbac:\n  install: true\n  serviceAccountName: default\n  apiVersion: v1beta1\n  roleRef: cluster-admin\nNow, to install Jenkins, run the following helm command:\nhelm install\n  --name jenkins \n  --namespace jenkins\n  --values values.yml \n  stable/jenkins \nIf successful, this will output a list of created resources that looks like figure A.3.\nGive Jenkins a few minutes to start up. To access the server, you’ll need a password. \nYou can retrieve it using the following command:\nprintf $(kubectl get secret --namespace jenkins jenkins -o jsonpath=\"{.data.\njenkins-admin-password}\" | base64 --decode);echo\nThis default set of plugins will support \nrunning Jenkins Pipeline jobs.\nThe persistence settings refer  \nto your persistent volume.\nThe chart to install\n \n",
      "content_length": 1520,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 380,
      "content": "\t\n353\nRunning Jenkins on Kubernetes\nThen, navigate to the login page:\nminikube --namespace=jenkins service jenkins\nLog in with the username “admin” and the password you retrieved. Terrific — you’ve \nset up Jenkins!\nConfiguring RBAC\nMinikube uses RBAC — role-based access control — by default, which requires an addi-\ntional configuration step to ensure Jenkins can perform operations on the Kubernetes \ncluster.\nFigure A.3  Kubernetes objects that the stable/Jenkins Helm chart installed.\n \n",
      "content_length": 491,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 381,
      "content": "354\nAppendix  Installing Jenkins on Minikube\nTo configure this appropriately on the Jenkins server:\n1\t Log in to the Jenkins dashboard.\n2\t Navigate to Credentials > System > Global Credentials > Add Credentials.\n3\t Add a Kubernetes Service Account credential, setting the value of the ID field to \njenkins.\n4\t Save and navigate to Jenkins > Manage Jenkins > System.\n5\t Under the Kubernetes section, configure the credentials to those you created in \nstep 3 (figure A.4) and click Save.\nFigure A.4  Kubernetes cloud credentials\nTesting it all works\nYou can run a simple build to make sure everything’s working. First, log in to your new \nJenkins dashboard and navigate to New Item in the left-hand column.\nCreate a new pipeline job named “test-job” per the configuration in figure A.5. Click \nOK to move to the next page and configure that job with the following script in the Pipe-\nline Script field.\nListing A.4  Test pipeline script\npodTemplate(label: 'build', containers: [\n    containerTemplate(name: 'docker', image: 'docker', command: 'cat', \nttyEnabled: true)\n  ],\n  volumes: [\n    hostPathVolume(mountPath: '/var/run/docker.sock', hostPath: '/var/run/\ndocker.sock'),\n \n",
      "content_length": 1177,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 382,
      "content": "\t\n355\nRunning Jenkins on Kubernetes\n  ]\n  ) {\n    node('build') {\n      container('docker') {\n        sh 'docker version'\n      }        \n    }  \n  }\nClick Save, then, on the following page, click Build Now. This will execute your job.\nTIP    The first build run may take some time!\nThe script you added will\n1\t Create a new pod, containing a Docker container\n2\t Execute the docker version command inside that container and output the \nresults to the console\nFigure A.5  New job page on Jenkins\n \n",
      "content_length": 497,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 383,
      "content": "356\nAppendix  Installing Jenkins on Minikube\nOnce the build job has completed, navigate to the build’s console output \n(http://<insert Jenkins ip here>/job/test/1/console). You should see output similar \nto figure A.6, showing the output of the job script commands.\nIf your output looks like the figure, fantastic — everything’s in working order! If not, \nyour first point of call to diagnose any issues should be the Jenkins logs: http://<insert \nJenkins ip here>/log/all.\nFigure A.6  Console output from the test build job\n \n",
      "content_length": 527,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 384,
      "content": "357\nindex\nSymbols\n2PC (two-phase commit) \nprotocol  107\nA\nabstraction  66\nacbuild command-line tool  218\nacceptance testing  257\naccidental complexity  12\nACID transaction  106\nactions  92–94\nactivity feeds  126\nAdvanced Message Queuing \nProtocol (AMQP)  63, 162\naggregating data  59\naggregation  59–60\nalerts\nraising  291–293\nsymptoms versus  \ncauses  292–293\nwho needs to know  292\nsetting up  287–291\nalignment  9–10\nambiguity  99–103\nmigration  100–103\npreparing for \ndecomposition  100\nretirement  100–103\nstarting with coarse-grained \nservices  99–100\nAMQP (Advanced Message \nQueuing Protocol)  63, 162\nanalytics  127\nanemic services  30\nAPI gateways  39, 68–69\nApollo  71\napplication boundaries  66–71\nAPI gateways  68–69\nBFF (backends for \nfrontends)  69–70\nconsumer-driven gateways  70–71\napplications\ndistributed  106–108\nobserving  293–295\narchitecture\nof CQRS pattern  124–125\nof microservices  52–56\narchitectural principles  54\nfour tiers of microservice \napplications  55–56\nrole of architect  54\nrole of  341–343\nartifacts  199–207\nconfiguring  206–207\ndefining  200\nimmutability of  201\npublishing  254–255\nstandardizing  22–23\ntypes of  202–206\ncontainers  205–206\noperating system \npackages  203\nserver images  203–204\nasset information  81\nasynchronous communication \npatterns  63–65\njob queues  63–64\noverview of  149\npublish-subscribe  64–65\nasynchronous messages  62–63\nautohealing  190\nautomated deployments  42–43\nautomated tests  257\nautomation\nof production environments  191\noverview of  9, 189\nautonomy\nlack of  332\noverview of  8, 329\nautoscaling  190, 194\navailability  189\nautomation  189\ncalculating  130\nmanual change \nmanagement  189\nprioritizing  123\nsmall releases  189\nB\nbackends for frontends. See BFF\nbalancing  177–180\nBDD (behavior-driven \ndevelopment)  78\nBEAM (Erlang virtual \nmachine)  183\nBFF (backends for frontends)  69–70\n \n",
      "content_length": 1873,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 385,
      "content": "358\nindex\nbinary dependencies  200\nblue-green deploys pattern  210\nbootstrapping  164–165\nbottlenecks  154\nboundaries. See application \nboundaries\nboundary layer  66\nbounded contexts  67, 79\nbuild pipelines\nconfiguring  247–251\nprocedural versus \ndeclarative  263–264\nbus factor  41\nbusiness capabilities, scoping \nby  78–87\ncapabilities  78–79\nchallenges and limitations  87\ncreating investment \nstrategies  79–86\ndomain modeling  78–79\nnested contexts and services  86\nbusybox  241\nC\ncachetools library  144\ncaching  67, 143–144\ncanaries\non GCE  211–213\noverview of  236–240\ncapacity  196\nCAP theorem  108, 123\ncascading failures  136–139\ncategories  301\nchaos testing  152, 154–155\nChaos Toolkit  155\nChargeFailed event  110\nchassis  160–163\ndesigning  165–180\nbalancing  177–180\nlimiting  177–180\nobservability  171–177\nservice discovery  167–171\nexploring features  180–182\npurpose of  163–165\nfaster bootstrapping  164–165\nreduced risk  164\nchoreographed interactions  110\nchoreographed sagas  112–115\nchoreography  37–39, 94, 109–110\ncircuit breakers  146–148\nClair  220\nclients  71–74\nfrontend monoliths  71\nmicro-frontends  72–74\nclusters, deploying to  224–242\ncomponents of master \nnodes  230\ncomponents of worker \nnodes  231\nconnecting multiple \nservices  241–242\ndeploying new versions  235–240\ndesigning pods  226–228\nhealth checks  233–235\nload balancing  228–230\noverview of  230–233\nrolling back  241\nrunning pods  228–232, 226–233\nstate changes  231\ncoarse-grained services\noverview of  87\nstarting with  99–100\ncodebases  329\ncode deployment  259\ncoherence  335\ncollaboration. See service \ncollaboration\ncollaborators  37\ncommand-query responsibility \nsegregation pattern. \nSee CQRS pattern\ncommands, separating queries \nfrom  123–125\ncommit phase, 2PC  108\ncommunication  60–65\nas sources of failures  135\nasynchronous \ncommunication  149\nasynchronous communication \npatterns  63–65\njob queues  63–64\npublish-subscribe  64–65\nasynchronous messages  62–63\nby group size  327\ndesigning  139–149\ncircuit breakers  146–148\nfallbacks  143–145\nretries  140–143\ntimeouts  145–146\nevent-based \ncommunication  108–110\nchoreography  109–110\nevents  109–110\nlocating services  65\noverview of  21\nstandardizing  156\nsynchronous messages\nchoosing transport  62\ndrawbacks of  62\nwhen to use  61–62\nusing service mesh  157\ncommunication broker  63\ncompensating actions  113\ncomponents\nadding to Docker compose \nfiles  276\nof master nodes  230\nof worker nodes  231\ncompose files  276\nconfiguration drift  202\nconfiguring\nbuild pipelines  247–251\nGrafana  280–282\nPrometheus  280\nservice artifacts  206–207\nStatsD exporter  277–279\nConnectionError exception  179\nconsistency patterns  118–119\nconstraints  103\nConsul  65\nconsumer-driven gateways  70–71\ncontainerizing services  215–224\nbuilding images  218–220\nrunning containers  220–222\nstoring images  223–224\nworking with images  216–218\ncontainers\noverview of  23, 205–206\nrunning  220–222\n \n",
      "content_length": 2950,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 386,
      "content": "\t\n359\nindex\ncontainer schedulers  210\ncontexts, nested  86\ncontinuous delivery  245\ncontinuous deployment  245\ncontracts\nbetween services  16\noverview of  37\ncontrol  103\nConway's Law  327\ncoordinators  59\ncorrelation IDs  121\nCQRS (command-query \nresponsibility segregation) \npattern\narchitecture of  124–125\nchallenges  125–127\noptimistic updates  126\noverview of  63\npolling  127\npublish-subscribe  127\ncritical paths  60\ncross-functionally  330\nCRUD (create, read, update, \ndelete)  30, 108\ncyclic dependencies  114\nD\nDAG (direct acyclic graph)  314\ndark launches  264\ndata\naggregating  59\nstoring copies of  122–123\nstubbed  145\ndata composition  121\ndata warehouses  127\nDDD (domain-driven design)  79\ndeadlines  146\ndecomposition\npreparing for  100\nscaling through  6–7\ndegradation  143\ndelivery pipelines, building  242, \n264–266\ndark launches  264\ndeployment pipeline  244–246\nfeature flags  23–24, 265–266\ndependencies  135–136\ndeploying\nmicroservices  21–24\nimplementing continuous \ndelivery pipelines  23–24\nstandardizing microservices \ndeployment artifacts   \n22–23\nservices  191–199, 187–213\nadding load balancers   \n196–198\nimportance of  188–189\nprovisioning virtual \nmachines  192–193\nrun multiple instances of \nservice  194–196\nservice startup  192\nto clusters  224–242\ncomponents of master \nnodes  230\ncomponents of worker \nnodes  231\nconnecting multiple \nservices  241–242\ndeploying new versions   \n235–240\ndesigning pods  226–228\nhealth checks  233–235\nload balancing  228–230\noverview of  230–233\nrolling back  241\nrunning pods  228–232, \n226–233\nstate changes  231\nto production  259–261\nto staging  255–257\nto underlying hosts  207–210\nmultiple scheduled services \nper host  209–210\nmultiple static services per \nhost  208–209\nsingle service to host  207\nwithout downtime  210–213\ncanaries on GCE  211–213\nrolling deploys on GCE   \n211–213\ndeployment pipelines  57, 190, \n244–246\ndeployment process  8, 43\ndeployments  235–236\nautomated  42–43\nquality-controlled  42–43\ndeployment target  190\ndescribed feature  166\ndesign reviews  345–346\ndevelopment process  43\ndirect acyclic graph (DAG)  314\ndistributed applications  106–108\ndistributed computing  14, 130\ndistributed systems  16–17\ndivergence. See technical \ndivergence\ndocker-compose, installing  140\ndocker-compose up command   \n180, 281\nDocker platform  276\ndocker version command  355\ndocumentation  346\ndomain-driven design (DDD)  79\ndomain modeling  78–79\ndomains. See modeling domains\ndomain-specific language \n(DSL)  217, 247\ndowntime  130\nDRY (don't repeat yourself)  104, \n183, 339\nDSL (domain-specific \nlanguage)  217, 247\nE\nedge capabilities  67\nedge side includes (ESI)  73\nelastic load balancer (ELB)  65\nElasticsearch engine  305\nElasticsearch, Logstash, and \nKibana. See ELK-based \nsolutions\nELB (elastic load balancer)  65\nELK (Elasticsearch, Logstash, and \nKibana)-based solutions   \n304–306\nElasticsearch  305\nKibana  305\nLogstash  305\nend-to-end responsibility  71, 329\nengineering culture  26–27\nenterprise service buses (ESBs)  6\n \n",
      "content_length": 3034,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 387,
      "content": "360\nindex\nenvironments, staging  258. See \nalso production \nenvironments\nErlang virtual machine (BEAM)  183\nERROR level  301\nerror reporting  174–175\nerrors  273\nESBs (enterprise service buses)  6\nESI (edge side includes)  73\nevent backbone  63\nevent-based communication   \n108–110\nchoreography  109–110\nevents  109–110\nevents  109–110\nevent sourcing  119–120\neventual consistency  108\nexpectations, setting  104\nexplicit interfaces  104\nexponential back-off strategy  142\nF\nfailures\ncascading  136–139\nsources of  133–136\ncommunication  135\ndependencies  135–136\nhardware  134\nservice practices  136\nfailure zones  195\nfallbacks  143–145\ncaching  143–144\nfunctional redundancy  144\ngraceful degradation  143\nstubbed data  145\nfault tolerance\noverview of  42\nvalidating  152–155\nchaos testing  154–155\nload testing  153–154\nfeature flags  265–266\nfeatures\nbuilding  32–39\nidentifying microservices by \nmodeling domains  33–35\nservice choreography  37–39\nservice collaboration  36–37\ndesigning  75–104\ntaking to production  40–45\nautomated deployments   \n42–43\nquality-controlled \ndeployments  42–43\nresilience  43\ntransparency  43–45\nfeedback loops  139\nflags  265–266\nflat services  59\nFlipper  265\nFluentd-based solutions  306\nFluentd daemon  304, 307\nframeworks\nbuilding reusable  159–183\noverview of  156\nfriction  13, 258\nfrontends\nBFF (backends for frontends)   \n69–70\nfrontend monoliths  71\nmicro-frontends  72–74\nfull staging environments  258\nfunctional redundancy  144\nfunctions\ngrouping teams across  332–333\ngrouping teams by  330–332\nlack of autonomy  332\nno long-term responsibility  332\nrisk of silos  332\nunclear ownership  331\nG\ngateways\nAPI gateways  39, 68–69\nconsumer-driven gateways  70–71\ngauges  273\nGCE (Google Kubernetes Engine)\ncanaries on  211–213\nrolling deploys on  211–213\nGCP (Google Cloud Platform)  191\nGDPR (General Data Protection \nRegulation)  313\nGKE (Google Kubernetes Engine). \nSee GCE\nGoogle Cloud Platform (GCP)  191\nGoogle Compute Engine (GCE)  192\nGoogle Kubernetes Engine \n(GKE)  225\ngraceful degradation  143\nGrafana tool\nconfiguring  280–282\nmonitoring systems with   \n275–291\nRabbitMQ  282–285\nsetting up alerts  287–291\nsetting up metric collection \ninfrastructure  276–283\nGraphQL  70\nGunicorn web server  226\nH\nhalf open circuit breaker  148\nHAProxy  197\nhardware as source of failure  134\nhealth checks  135, 150, 233–235\nHelm application package \nmanager  350\nheterogeneity\noverview of  182–183\ntechnical heterogeneity  12\nhigh cohesion  4\nhigher order services  59–60\nhistograms  274\nholdings  130, 136\nhomogeneity  343\nhorizontal decomposition  9\nhosts\nmultiple scheduled services \nper  209–210\nadvantages of scheduling \nmodel  210\ncontainer schedulers  210\nmultiple static services  \nper  208–209\nsingle service to  207\nunderlying, deploying services \nto  207–210\nHTTP liveness check  151\n \n",
      "content_length": 2846,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 388,
      "content": "\t\n361\nindex\nI\nIaaS (infrastructure as a \nservice)  191\nidempotent  140\nidentifiers  301\nimages\nbuilding  218–220, 251–252\noverview of  216–218\nstoring  223–224\nimmutability  201, 341\ninfrastructure as a service \n(IaaS)  191\ninfrastructures  9, 191, 335–337\ninner-source model  344\ninstalling Jenkins, on \nMinikube  349–355\ninstance group  194\ninteractions, choreographed  110\ninterfaces  104\ninternal load balancing  198\ninterruption  118\ninterwoven sagas  117–118\ninterruption  118\nlocking  118\nshort-circuiting  118\ninvestment strategies\ncreating  79–86\nplacing orders  88–92\nInvestmentStrategies service  96\nisolated staging environments  258\nisolation  46\nJ\nJaeger  315\nJava virtual machine ( JVM)  183\nJenkins Pipeline  263\nJenkins tool\nbuilding pipelines with  246–261\nbuilding images  251–252\nconfiguring build \npipelines  247–251\ndeploying to production   \n259–261\ndeploying to staging  255–257\npublishing artifacts  254–255\nrunning tests  252–254\nstaging environments  258\ninstalling on Minikube  349–355\nrunning on Kubernetes  349–355\nconfiguring RBAC  353–354\ncreating namespaces  350–351\ncreating volume  350–351\nsetting up Helm  350\ntesting  354–356\njitter  142, 148\njob queues  63–64\nJVM ( Java virtual machine)  183\nK\nKibana plugin  305\nKPIs (key performance \nindicators)  330\nksonnet  256\nkubectl command-line tool  227\nkube-proxy  231\nKubernetes platform, running \nJenkins on  349–355.  \nSee also GCE\nconfiguring RBAC  353–354\ncreating namespaces  350–351\ncreating volume  350–351\ninstalling Jenkins  351–352\nsetting up Helm  350\ntesting  354–356\nL\nlatency  272–273\nlaunches  264\nlevels  301\nlimiting  177–180\nLinkerd  157\nliveness health check  233\nliving documentation  346\nload balancers  56, 150–151, \n196–198, 228–230\nload/capacity tests  257\nload testing  152–154\nlocking  118\nlogging  176–177\ngenerating logs  300–303\nreadability  301–303\nstructure  301–303\nuseful information to include \nin log entries  300–301\ninformation  313\nsearching  311–313\nsetting up infrastructure   \n303–313\nconfigure which logs to \ncollect  308–311\nELK based solutions  304–306\nFluentd-based solutions  306\nsetting up solutions  306–308\nlogin screen, Grafana  282\nlogs  67, 305\nlogstash-formatter library  176, 313\nLogstash tool  302, 305\nloose coupling  4, 8\nM\nmanaged registries  223\nmanual tests  257\nmapping runtime platforms  57\nmarket-data  130, 136\nMarketDataClient class  140\nmaster nodes  230\nmessages. See also asynchronous \nmessages\nmetric collection infrastructure   \n276–283\nadding components to Docker \ncompose file  276\nconfiguring Prometheus  280\nconfiguring StatsD exporter   \n277–279\nsetting up Grafana  280–282\nmetrics\noverview of  67, 172–174\ntypes of  273–274\ngauges  273\nhistograms  274\nmicro-frontends  72–74\nmicroservice platforms  56–57\nmicroservices. See also services\nadvantages of  11–13, 30–32\ndelivering sustainable \nvalue  31–32\nreducing friction  31–32\nreducing friction and risk  13\nrisk and inertia in financial \nsoftware  31\ntechnical heterogeneity  12\n \n",
      "content_length": 2993,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 389,
      "content": "362\nindex\narchitecture of  52–56\narchitectural principles  54\nfour tiers of microservice \napplications  55–56\nrole of architect  54\nchallenges of  14–18\ndesign challenges  14–17\noperational challenges  17–18\ndeploying  21–24\nimplementing continuous \ndelivery pipelines  23–24\nstandardizing microservices \ndeployment artifacts  22–23\ndesigning  19–21\ncommunication  21\nmonoliths  19–20\nresiliency  21\nscoping services  20\ndevelopment lifecycle of  18–25\nidentifying by modeling \ndomains  33–35\nkey principles of  7–10\nalignment  9–10\nautomation  9\nautonomy  8\nresilience  9\ntransparency  9\nobserving  24–25\nbehaviors across hundreds of \nservices  25\nidentifying and refactoring \nfragile implementations   \n24–25\nscaling through \ndecomposition  6–7\nscaling up development \nof  45–46\nisolation  46\ntechnical divergence  45–46\nusers of  10–11\nmicroservice scoping  52\nmicroservice teams  325–348\nbuilding  326–329\nConway's Law  327\nprinciples for  328–330\nautonomy  329\nend-to-end responsibility  329\nownership  328–329\nrecommended practices \nfor  340–347\nanswering questions about \napplications  347\ndesign reviews  345–346\ndrivers of change in \nmicroservices  340–341\nhomogeneity versus technical \nflexibility  343\nliving documentation  346\nopen source models  343–344\nrole of architecture  341–343\nteam models  330–340\ngrouping across functions   \n332–333\ngrouping by function  330–332\ninfrastructures  335–337\non-call support  337–338\nplatforms  335–337\nproduct  335–337\nsetting team boundaries   \n334–336\nsharing knowledge  338–340\nmigration  100–103\nMinikube tool, installing Jenkins \non  349–355\nMkDocs  346\nmodeling domains  33–35\nmodel-view controller (MVC)  19\nmodes of failure  14\nmonitoring systems  269–295\nobserving whole application   \n293–295\nraising alerts  291–293\nsymptoms versus causes   \n292–293\nwho needs to know  292\nrobust monitoring stacks   \n270–275\ngolden signals  272–273\nlayered monitoring  270–272\nrecommended practices   \n274–275\nwith Grafana  275–291\nRabbitMQ  282–285\nsetting up alerts  287–291\nsetting up metric collection \ninfrastructure  276–283\nwith Prometheus  275–291\nRabbitMQ  282–285\nsetting up alerts  287–291\nsetting up metric collection \ninfrastructure  276–283\nmonoliths  19–20, 71, 106, 111\nmultispeed development  103\nMVC (model-view controller)  19\nN\nnameko framework  166\nnamespaces  350–351\nnested contexts  86\nnetwork communication  61\nnginx.conf file  222\nNGINX container  222\nNodePort service  230\nnonbackwards-compatible \nfunctionality  135\nnoncritical paths  60\nnonfunctional testing  257\nnotifications, sending  96–98\nnslookup  241\nO\nobject-oriented applications  93\nobject-relational mapping \n(ORM)  161\nobservability  18, 171–177\nerror reporting  174–175\nlogging  176–177\nmetrics  172–174\nobservable services  43\non-call support  337–338\nOpenAPI specification  82\nopen source models  343–344\nOpenTracing API  314\noperating system packages  203\noptimistic updates  126\norchestrated sagas  115–117\norchestration  94\nOrderCreated event  64, 109\n \n",
      "content_length": 2998,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 390,
      "content": "\t\n363\nindex\nOrderPlaced event  39\nOrderRequested event  109\norganizations, ownership  \nin  103–104\nORM (object-relational \nmapping)  161\nownership  103–104, 329–331\nP\nPaaS (platform as a service)  57, 209\npaths\ncritical  60\nnoncritical  60\npatterns, consistency patterns   \n118–119. See also CQRS \npattern\npipelines. See also delivery pipelines\nbuilding reusable  262–264\nbuilding with Jenkins  246–261\nbuilding images  251–252\nconfiguring build pipelines   \n247–251\ndeploying to production   \n259–261\ndeploying to staging  255–257\npublishing artifacts  254–255\nrunning tests  252–254\nstaging environments  258\noverview of  57, 246\nprocedural versus declarative \nbuild pipelines  263–264\npip tool  219\nPlaceStrategyOrders service  88, 91\nplatform as a service (PaaS)  57, 209\nplatforms, mapping runtime \nplatforms  57. See also  \nmicroservice platforms\npods\ndesigning  226–228\nrunning  228–232, 226–233\npoint-to-point communication  36\npolling  127\npositive feedback  136\nprepare phase, 2PC  108\nprinciples  342\nproduction\ndeploying to  259–261\ntaking features to  40–45\nautomated deployments   \n42–43\nquality-controlled deployments  \n42–43\nresilience  43\ntransparency  43–45\nproduction environments  189–190\nautomation  191\nfeatures of  190–191\nspeed  191\nproduct mode  334\nPrometheus ecosystem\nconfiguring  280\nmonitoring systems with   \n275–291\nRabbitMQ  282–285\nsetting up alerts  287–291\nsetting up metric collection \ninfrastructure  276–283\nprotocol buffers  346\nprovisioning virtual machines   \n192–193\npublic images, Docker  220\npublish-subscribe  64–65, 127\nPython chassis  166\nQ\nquality-controlled deployments   \n42–43\nqueries  120–127, 105–128\nanalytics  127\nCQRS pattern challenges   \n125–127\noptimistic updates  126\npolling  127\npublish-subscribe  127\nreporting  127\nseparating from commands   \n123–125\nCQRS pattern architecture  124\nstoring copies of data  122–123\nqueues. See job queues\nR\nRabbitMQ broker  167, 282–285\nrabbitmq_queue_messages \nmetric  290\nrate limits  67, 152\nRBAC (role-based access control)   \n353–354\nreadability of logs  301–303\nreadiness checks  151, 233\nrecovery_timeout parameter  180\nredundancy  144\nreleases  189\nreliability\ndefining  130–132\nmaximizing  150–155\nload balancing  150–151\nrate limits  152\nservice health  150–151\noverview of  42\nvalidating  152–155\nchaos testing  154–155\nload testing  153–154\nremote_hello method  168\nReplicaSet  227\nreplication lag  125\nreporting  127\nrequest for comments (RFC)  345\nresiliency  9–21, 43\nretirement  100–103\nretries  140–143\nRFC (request for comments)  345\nrisk, reducing  13, 164\nrobust monitoring stacks  270–275\ngolden signals  272–273\nerrors  273\nlatency  272–273\nsaturation  273\ntraffic  273\nlayered monitoring  270–272\nrecommended  \npractices  274–275\ntypes of metrics  273–274\ngauges  273\nhistograms  274\n \n",
      "content_length": 2806,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 391,
      "content": "364\nindex\nrole-based access control. See RBAC\nrolling back  114, 241, 259\nrolling deploys  211–213\nround-robin algorithm  167\nrouting components  190\nRPC-facing services  150\nrpc-market_service queue, \nRabbitMQ  178\nrun the packer build command  204\nruntime management  190\nruntime platforms  57, 190\nS\nsagas  111–120\nchoreographed  112–115\nconsistency patterns  118–119\nevent sourcing  119–120\ninterwoven  117–118\ninterruption  118\nlocking  118\nshort-circuiting  118\norchestrated  115–117\nsaturation  273\nscalability  42\nscaling up microservice \ndevelopment  45–46\nisolation  46\ntechnical divergence  45–46\nscanning tools  220\nscheduled services, multiple per \nhost  209–210\nadvantages of scheduling \nmodel  210\ncontainer schedulers  210\nscheduling models  210\nscoping  15–16\nby business capabilities  78–87\ncapabilities  78–79\nchallenges and limitations  87\ncreating investment \nstrategies  79–86\ndomain modeling  78–79\nnested contexts and services  86\nby use case  87–94\nactions  92–94\nchoreography  94\norchestration  94\nplacing investment strategy \norders  88–92\nstores  92–94\nby volatility  94–96\nservices  20\nScripted Pipeline  247\nsecure operation  190\nsecurity  220\nsecurity tests  257\nself-healing  195–196\nsending notifications  96–98\nserver images  203–204\nserver management  201\nservice artifact  199\nservice collaboration  36–37\nservice contracts  37\nservice responsibility  37\nservice discovery  65, 167–171\nservice health  150–151\nservice mesh  157–158\nservice migration  102\nservice-oriented architectures \n(SOAs)  6\nservice partitioning  99\nservice practices as source of \nfailures  136\nservice responsibility  37\nservices  58–60\naggregation  59–60\nAPI gateways  39\nbehavior across  297–300\nbuilding artifacts  199–207\nconfiguring  206–207\ndefining  200\nimmutability  201\ntypes of service artifacts   \n202–206\ncapabilities of  58–59\nchallenges of designing  132–139\ncascading failures  136–139\nsources of failure  133–136\ncoarse-grained\noverview of  87\nstarting with  99–100\nconnecting multiple  241–242\ncontainerizing  215–224\nbuilding images  218–220\nrunning containers  220–222\nstoring images  223–224\nworking with images  216–218\ncontracts between  16\ncritical paths  60\ndeploying  191–199, 187–213\nadding load balancers   \n196–198\nimportance of  188–189\nprovisioning virtual \nmachines  192–193\nservice startup  192\ndesigning  129–158\ndefining reliability  130–132\ndesigning reliable communication  \n139–149\nasynchronous communication  \n149\ncircuit breakers  146–148\nfallbacks  143–145\nretries  140–143\ntimeouts  145–146\nhigher order services  59–60\ninteractions between tracing \nand  313–320\nlocating  65\nmaximizing reliability  \nof  150–155\nload balancing  150–151\nrate limits  152\nservice health  150–151\nvalidating reliability and fault \ntolerance  152–155\nnested contexts and  86\nnoncritical paths  60\nownership in organizations   \n103–104\nrun multiple instances  \nof  194–196\nadding capacity  196\n \n",
      "content_length": 2926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 392,
      "content": "\t\n365\nindex\nfailure zones  195\nself-healing  195–196\nsafety of  156–158\nframeworks  156\nservice mesh  157–158\nscoping  20\nsetting up tracing in  315–320\nto underlying hosts  207–210\nmultiple scheduled services \nper host  209–210\nmultiple static services per \nhost  208–209\nsingle service to host  207\nwithout downtime  210–213\ncanaries on GCE  211–213\nrolling deploys on  \nGCE  211–213\nservice teams  87\nservice to host models  207–210\nmultiple scheduled services per \nhost  209–210\nadvantages of scheduling \nmodel  210\ncontainer schedulers  210\nmultiple static services per \nhost  208–209\nsingle service to host  207\nsharding  6\nshort-circuiting  118\nsilos  332\nsmoke tests  259\nSOAs (service-oriented \narchitectures)  6\nsources  301\nspans  314–315\nstability  189\nautomation  189\nmanual change \nmanagement  189\nsmall releases  189\nstacks. See robust monitoring stacks\nstaging\ndeploying to  255–257\nenvironments  258\nstandardizing communication  156\nstartup logs  205\nstartups  192\nstate changes  231\nstatic services  208–209\nStatsD exporter  172–173, 277–279\nstatsd-exporter container  277\nSTDOUT (standard output)  304\nstores  92–94\nstoring\ncopies of data  122–123\nimages  223–224\nstubbed data  145\nsubtasks, sagas  112\nsupporting notifications  97\nsynchronous communication   \n36, 163\nsynchronous messages\nchoosing transport  62\ndrawbacks of  62\nwhen to use  61–62\nsynchronous requests  62\nT\ntarballs  223\nteam models  330–340\ngrouping across functions   \n332–333\ngrouping by function  330–332\nlack of autonomy  332\nno long-term responsibility  332\nrisk of silos  332\nunclear ownership  331\ninfrastructures  335–337\non-call support  337–338\nplatforms  335–337\nproduct  335–337\nsetting team boundaries   \n334–336\nsharing knowledge  338–340\nteams, challenges of  16. See \nalso microservice teams\ntechnical capabilities  96–98\noverview of  59\nsending notifications  96–98\nwhen to use  98\ntechnical divergence  45–46\ntechnical heterogeneity  12\ntenacity library  140\ntesting  252–254\nchaos testing  154–155\nJenkins on Minikube  354–356\nload testing  153–154\nthree-tier architecture  19, 52\ntimeouts  145–146\ntimestamps  300\nTogglz  265\ntraces, visualizing  320–324\ntracing\ninteractions between services \nand  313–320\nsetting up in services  315–320\nspans and  314–315\nTradeExecuted event  39\ntraffic  273\ntransactions  105–128\nconsistent  106–108\ndistributed  107–108\noverview of  130, 136\ntransparency  9, 42–45\ntransport  62\ntransport-related boilerplate  163\ntruck factor  41\ntry-finally statement  254\ntwo-phase commit (2PC) protocol   \n107\nU\nupdates, optimistic  126\nupstream collaborators  37\nuptime  130\nuse case, scoping by  87–94\nactions  92–94\nchoreography  94\norchestration  94\nplacing investment strategy \norders  88–92\nstores  92–94\nuser management  80\n \n",
      "content_length": 2767,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 393,
      "content": "366\nindex\nV\nvault  207\nVMs (virtual machines)  56, \n192–193\nvolatility  78, 94–96\nvolume  350–351\nW\nworker nodes, components of  231\nworkflow tools  117\nworkspaces  247\nZ\nzero-downtime deployments  210\n \n",
      "content_length": 204,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 394,
      "content": "Bruce  ●  Pereira\nI\nnvest your time in designing great applications, improving \ninfrastructure, and making the most out of your dev teams. \nMicroservices are easier to write, scale, and maintain than \ntraditional enterprise applications because they’re built as a \nsystem of independent components. Master a few important \nnew patterns and processes, and you’ll be ready to develop, \ndeploy, and run production-quality microservices.\nMicroservices in Action teaches you how to write and maintain \nmicroservice-based applications. Created with day-to-day \ndevelopment in mind, this informative guide immerses you \nin real-world use cases from design to deployment. You’ll \ndiscover how microservices enable an efﬁ cient continuous \ndelivery pipeline, and explore examples using Kubernetes, \nDocker, and Google Container Engine. \nWhat’s Inside\n● An overview of microservice architecture\n● Building a delivery pipeline\n● Best practices for designing multi-service transactions \n   and queries\n● Deploying with containers\n● Monitoring your microservices\nWritten for intermediate developers familiar with enterprise \narchitecture and cloud platforms like AWS and GCP.\nMorgan Bruce and Paulo A. Pereira are experienced engineering \nleaders. They work daily with microservices in a production \nenvironment, using the techniques detailed in this book.\nTo download their free eBook in PDF, ePub, and Kindle formats, owners \nof this book should visit manning.com/books/microservices-in-action\n$49.99 / Can $65.99  [INCLUDING eBOOK]\nMicroservices IN ACTION\nSOFTWARE DEVELOPMENT\nM A N N I N G\n“\nThe one [and only] book \non implementing micro-\nservices with a real-world, \ncover-to-cover example \nyou can relate to.”\n \n—Christian Bach, Swiss Re\n“\nA perfect ﬁ t for those who \nwant to move their majestic \nmonolith to a scalable \n  microservice architecture.”\n \n—Akshat Paul\nMcKinsey & Company\n“\nShows not only how to \nwrite microservices, but also \nhow to prepare your \nbusiness and infrastructure \n  for this change.”\n—Maciej Jurkowski, Grupa Pracuj \n“\nA deep dive into microser-\nvice development with many \nreal and useful examples.”\n \n—Antonio Pessolano\nConsoft Sistemi\nSee first page\n",
      "content_length": 2175,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}