{
  "metadata": {
    "title": "Python Microservices Development",
    "author": "Tarek Ziadé",
    "publisher": "Packt Publishing",
    "edition": "1st Edition",
    "isbn": "978-1785881114",
    "total_pages": 334,
    "conversion_date": "2025-11-08T12:43:15.486733",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Python Microservices Dev.pdf"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary"
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "detection_method": "topic_boundary"
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 17-24)",
      "start_page": 17,
      "end_page": 24,
      "detection_method": "topic_boundary"
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 25-32)",
      "start_page": 25,
      "end_page": 32,
      "detection_method": "topic_boundary"
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 33-40)",
      "start_page": 33,
      "end_page": 40,
      "detection_method": "topic_boundary"
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 41-48)",
      "start_page": 41,
      "end_page": 48,
      "detection_method": "topic_boundary"
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 49-56)",
      "start_page": 49,
      "end_page": 56,
      "detection_method": "topic_boundary"
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 57-64)",
      "start_page": 57,
      "end_page": 64,
      "detection_method": "topic_boundary"
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 65-73)",
      "start_page": 65,
      "end_page": 73,
      "detection_method": "topic_boundary"
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 74-81)",
      "start_page": 74,
      "end_page": 81,
      "detection_method": "topic_boundary"
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 82-89)",
      "start_page": 82,
      "end_page": 89,
      "detection_method": "topic_boundary"
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 90-98)",
      "start_page": 90,
      "end_page": 98,
      "detection_method": "topic_boundary"
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 99-106)",
      "start_page": 99,
      "end_page": 106,
      "detection_method": "topic_boundary"
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 107-114)",
      "start_page": 107,
      "end_page": 114,
      "detection_method": "topic_boundary"
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 115-122)",
      "start_page": 115,
      "end_page": 122,
      "detection_method": "topic_boundary"
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 123-131)",
      "start_page": 123,
      "end_page": 131,
      "detection_method": "topic_boundary"
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 132-142)",
      "start_page": 132,
      "end_page": 142,
      "detection_method": "topic_boundary"
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 143-150)",
      "start_page": 143,
      "end_page": 150,
      "detection_method": "topic_boundary"
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 151-158)",
      "start_page": 151,
      "end_page": 158,
      "detection_method": "topic_boundary"
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 159-166)",
      "start_page": 159,
      "end_page": 166,
      "detection_method": "topic_boundary"
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 167-174)",
      "start_page": 167,
      "end_page": 174,
      "detection_method": "topic_boundary"
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 175-182)",
      "start_page": 175,
      "end_page": 182,
      "detection_method": "topic_boundary"
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 183-192)",
      "start_page": 183,
      "end_page": 192,
      "detection_method": "topic_boundary"
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 193-200)",
      "start_page": 193,
      "end_page": 200,
      "detection_method": "topic_boundary"
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 201-208)",
      "start_page": 201,
      "end_page": 208,
      "detection_method": "topic_boundary"
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 209-216)",
      "start_page": 209,
      "end_page": 216,
      "detection_method": "topic_boundary"
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 217-225)",
      "start_page": 217,
      "end_page": 225,
      "detection_method": "topic_boundary"
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 226-233)",
      "start_page": 226,
      "end_page": 233,
      "detection_method": "topic_boundary"
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 234-246)",
      "start_page": 234,
      "end_page": 246,
      "detection_method": "topic_boundary"
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 247-255)",
      "start_page": 247,
      "end_page": 255,
      "detection_method": "topic_boundary"
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 256-263)",
      "start_page": 256,
      "end_page": 263,
      "detection_method": "topic_boundary"
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 264-271)",
      "start_page": 264,
      "end_page": 271,
      "detection_method": "topic_boundary"
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 272-284)",
      "start_page": 272,
      "end_page": 284,
      "detection_method": "topic_boundary"
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 285-295)",
      "start_page": 285,
      "end_page": 295,
      "detection_method": "topic_boundary"
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 296-303)",
      "start_page": 296,
      "end_page": 303,
      "detection_method": "topic_boundary"
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 304-311)",
      "start_page": 304,
      "end_page": 311,
      "detection_method": "topic_boundary"
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 312-320)",
      "start_page": 312,
      "end_page": 320,
      "detection_method": "topic_boundary"
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 321-334)",
      "start_page": 321,
      "end_page": 334,
      "detection_method": "topic_boundary"
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "Tarek Ziadé\n\nPython Microservices\nDevelopment\n\nLUT Packt»",
      "content_length": 57,
      "extraction_method": "OCR"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "Python Microservices\nDevelopment\nBuild, test, deploy, and scale microservices in Python\nTarek Ziadé\nBIRMINGHAM - MUMBAI",
      "content_length": 119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "Python Microservices Development\nCopyright © 2017 Packt Publishing\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or\ntransmitted in any form or by any means, without the prior written permission of the\npublisher, except in the case of brief quotations embedded in critical articles or reviews.\nEvery effort has been made in the preparation of this book to ensure the accuracy of the\ninformation presented. However, the information contained in this book is sold without\nwarranty, either express or implied. Neither the author, nor Packt Publishing, and its\ndealers and distributors will be held liable for any damages caused or alleged to be caused\ndirectly or indirectly by this book. Packt Publishing has endeavored to provide trademark\ninformation about all of the companies and products mentioned in this book by the\nappropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of\nthis information.\nFirst published: July 2017\nProduction reference: 1210717\nPublished by Packt Publishing Ltd.\nLivery Place\n35 Livery Street\nBirmingham\nB3 2PB, UK.\nISBN 978-1-78588-111-4\nwww.packtpub.com",
      "content_length": 1155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "Credits\nAuthor\nTarek Ziadé\nCopy Editor\nSonia Mathur\nReviewer\nWilliam Kahn-Greene\nProject Coordinator\nVaidehi Sawant\nCommissioning Editor\nAaron Lazar\nProofreader\nSafis Editing\nAcquisition Editor\nChaitanya Nair\nIndexer\nMariammal Chettiyar\nContent Development Editor\nRohit Kumar Singh\nGraphics\nJason Monteiro\nTechnical Editor\nPavan Ramchandani\nProduction Coordinator\nNilesh Mohite",
      "content_length": 377,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "About the Author\nTarek Ziadé is a Python developer, located in the countryside near Dijon, France. He works\nat Mozilla in the services team. He founded a French Python user group called Afpy, and he\nhas written several books about Python in French and English. When he is not hacking on\nhis computer or hanging out with his family, he's spending time between his two passions,\nrunning and playing the trumpet.\nYou can visit his personal blog (Fetchez le Python) and follow him on Twitter\n(@tarek_ziade). You can also take a look at one of his books on Amazon, Expert Python\nProgramming, published by Packt.\nI would like to thank the Packt team for their help, and the following hackers who helped\nme: Stéfane Fermigier, William Kahn-Greene, Chris Kolosiwsky, Julien Vehent, and Ryan\nKelly.\nI would also like to thank Amina, Milo, Suki, and Freya for their love and patience.\nI hope you will enjoy this book as much as I’ve enjoyed writing it!",
      "content_length": 942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "About the Reviewer\nWilliam Kahn-Greene has been writing Python and building applications on the web since\nthe late 90s. He works in the crash-stats group on the crash ingestion pipeline at Mozilla\nand also maintains a variety of Python libraries, including bleach. When he’s waiting for CI\nto test his code changes, he’s building things with wood, tending to his tomato plant, and\ncooking for four.",
      "content_length": 398,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "www.PacktPub.com\nFor support files and downloads related to your book, please visit www.PacktPub.com.\nDid you know that Packt offers eBook versions of every book published, with PDF and\nePub files available? You can upgrade to the eBook version at www.PacktPub.com and as a\nprint book customer, you are entitled to a discount on the eBook copy. Get in touch with us\nat service@packtpub.com for more details.\nAt www.PacktPub.com, you can also read a collection of free technical articles, sign up for a\nrange of free newsletters and receive exclusive discounts and offers on Packt books and\neBook.\nh t t p s ://w w w . p a c k t p u b . c o m /m a p t\nGet the most in-demand software skills with Mapt. Mapt gives you full access to all Packt\nbooks and video courses, as well as industry-leading tools to help you plan your personal\ndevelopment and advance your career.\nWhy subscribe?\nFully searchable across every book published by Packt\nCopy and paste, print, and bookmark content\nOn demand and accessible via a web browser",
      "content_length": 1023,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "Customer Feedback\nThanks for purchasing this Packt book. At Packt, quality is at the heart of our editorial\nprocess. To help us improve, please leave us an honest review on this book's Amazon page\nat h t t p s ://w w w . a m a z o n . c o m /d p /1785881116.\nIf you'd like to join our team of regular reviewers, you can e-mail us at\ncustomerreviews@packtpub.com. We award our regular reviewers with free eBooks and\nvideos in exchange for their valuable feedback. Help us be relentless in improving our\nproducts!",
      "content_length": 511,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "Table of Contents\nPreface\n1\nChapter 1: Understanding Microservices\n8\nOrigins of Service-Oriented Architecture\n9\nThe monolithic approach\n10\nThe microservice approach\n14\nMicroservice benefits\n16\nSeparation of concerns\n16\nSmaller projects\n16\nScaling and deployment\n17\nMicroservices pitfalls\n18\nIllogical splitting\n18\nMore network interactions\n19\nData storing and sharing\n19\nCompatibility issues\n20\nTesting\n20\nImplementing microservices with Python\n21\nThe WSGI standard\n22\nGreenlet and Gevent\n23\nTwisted and Tornado\n25\nasyncio\n26\nLanguage performances\n29\nSummary\n31\nChapter 2: Discovering Flask\n33\nWhich Python?\n35\nHow Flask handles requests\n35\nRouting\n39\nVariables and converters\n40\nThe url_for function\n43\nRequest\n44\nResponse\n45\nFlask built-in features\n47\nThe session object\n47\nGlobals\n48\nSignals\n49",
      "content_length": 797,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "[ ii ]\nExtensions and middlewares\n51\nTemplates\n52\nConfiguration\n54\nBlueprints\n56\nError handling and debugging\n57\nCustom error handler\n58\nThe debug mode\n60\nA microservice skeleton\n61\nSummary\n64\nChapter 3: Coding, Testing, and Documenting - the Virtuous Cycle\n65\nDifferent kinds of tests\n67\nUnit tests\n68\nFunctional tests\n71\nIntegration tests\n73\nLoad tests\n74\nEnd-to-end tests\n77\nUsing WebTest\n79\nUsing pytest and Tox\n80\nDeveloper documentation\n83\nContinuous Integration\n88\nTravis-CI\n89\nReadTheDocs\n90\nCoveralls\n91\nSummary\n94\nChapter 4: Designing Runnerly\n95\nThe Runnerly application\n96\nUser stories\n96\nMonolithic design\n98\nModel\n98\nView and Template\n99\nBackground tasks\n103\nStrava token\n106\nAuthentication and authorization\n107\nPutting together the monolithic design\n111\nSplitting the monolith\n112\nData Service\n114\nUsing Open API 2.0\n115\nMore splitting\n118\nSummary\n119",
      "content_length": 867,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "[ iii ]\nChapter 5: Interacting with Other Services\n121\nSynchronous calls\n122\nUsing Session in a Flask app\n123\nConnection pooling\n127\nHTTP cache headers\n129\nImproving data transfer\n132\nGZIP compression\n132\nBinary payloads\n134\nPutting it together\n137\nAsynchronous calls\n137\nTask queues\n138\nTopic queues\n139\nPublish/subscribe\n144\nRPC over AMQP\n144\nPutting it together\n145\nTesting\n145\nMocking synchronous calls\n145\nMocking asynchronous calls\n147\nMocking Celery\n147\nMocking other asynchronous calls\n149\nSummary\n150\nChapter 6: Monitoring Your Services\n151\nCentralizing logs\n152\nSetting up Graylog\n154\nSending logs to Graylog\n157\nAdding extra fields\n159\nPerformance metrics\n161\nSystem metrics\n161\nCode metrics\n164\nWeb server metrics\n166\nSummary\n168\nChapter 7: Securing Your Services\n169\nThe OAuth2 protocol\n170\nToken-based authentication\n172\nThe JWT standard\n173\nPyJWT\n175\nX.509 certificate-based authentication\n176\nThe TokenDealer microservice\n179\nThe POST/oauth/token implementation\n180",
      "content_length": 981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "[ iv ]\nUsing TokenDealer\n184\nWeb application firewall\n186\nOpenResty - Lua and nginx\n188\nRate and concurrency limiting\n191\nOther OpenResty features\n193\nSecuring your code\n194\nAsserting incoming data\n194\nLimiting your application scope\n198\nUsing Bandit linter\n199\nSummary\n202\nChapter 8: Bringing It All Together\n204\nBuilding a ReactJS dashboard\n205\nThe JSX syntax\n206\nReact components\n207\nReactJS and Flask\n210\nUsing Bower, npm, and Babel\n211\nCross-origin resource sharing\n215\nAuthentication and authorization\n218\nInteracting with Data Service\n218\nGetting the Strava token\n219\nJavaScript authentication\n221\nSummary\n223\nChapter 9: Packaging and Running Runnerly\n225\nThe packaging toolchain\n226\nA few definitions\n227\nPackaging\n228\nThe setup.py file\n228\nThe requirements.txt file\n233\nThe MANIFEST.in file\n235\nVersioning\n236\nReleasing\n239\nDistributing\n241\nRunning all microservices\n244\nProcess management\n246\nSummary\n250\nChapter 10: Containerized Services\n251\nWhat is Docker?\n252\nDocker 101\n254\nRunning Flask in Docker\n256",
      "content_length": 1016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "[ v ]\nThe full stack - OpenResty, Circus and Flask\n258\nOpenResty\n259\nCircus\n261\nDocker-based deployments\n264\nDocker Compose\n265\nIntroduction to Clustering and Provisioning\n267\nSummary\n270\nChapter 11: Deploying on AWS\n271\nAWS overview\n272\nRouting - Route53, ELB, and AutoScaling\n274\nExecution - EC2 and Lambda\n274\nStorage - EBS, S3, RDS, ElasticCache, and CloudFront\n276\nMessaging - SES, SQS, and SNS\n277\nSimple Email Service (SES)\n277\nSimple Queue Service (SQS)\n278\nSimple Notification Service (SNS)\n278\nProvisioning and deployment - CloudFormation and ECS\n279\nDeploying on AWS - the basics\n280\nSetting up your AWS account\n280\nDeploying on EC2 with CoreOS\n284\nDeploying with ECS\n288\nRoute53\n294\nSummary\n296\nChapter 12: What Next?\n297\nIterators and generators\n298\nCoroutines\n301\nThe asyncio library\n303\nThe aiohttp framework\n304\nSanic\n305\nAsynchronous versus synchronous\n306\nSummary\n309\nIndex\n310",
      "content_length": 895,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "Preface\nIf we try to deploy our web applications into the cloud, it requires our code to interact with\nmany third-party services. Using microservice architectures, you can build applications that\nwill allow you to manage these interactions. However, this comes with its own set of\nchallenges, since each set has its own complexity, and getting their interaction right isn't\neasy. This easy-to-follow guide covers techniques to help you overcome these challenges.\nYou will learn how to best design, write, test, and deploy your microservices. The real-\nworld examples will help Python developers create their own Python microservices using\nthe most efficient methods. By the end of this book, you will have acquired the skills to craft\napplications that are built as small standard units, using all the proven best practices and\navoiding the usual traps. Also, this is a useful guide for the vast community of Python\ndevelopers who are shifting from monolithic design to the new microservice-based\ndevelopment paradigm.\nWhat this book covers\nChapter 1, Understanding Microservices, defines what microservices are, and their roles in\nmodern web applications. It also introduces Python and explains why it's great for building\nmicroservices.\nChapter 2, Discovering Flask, introduces Flask and goes through its main features. It\nshowcases the framework with a sample web application that will be the basis for building\nmicroservices.\nChapter 3, Coding, Testing, and Documenting - the Virtuous Cycle, describes the Test-Driven\nDevelopment and Continuous Integration approach, and how to use it in practice to build\nand package Flask applications.\nChapter 4, Designing Runnerly, takes you through the app features and user stories,\nexplains how it could be built as a monolithic app, then decomposes it into microservices\nand explains how they interact with the data. It will also introduce the Open API 2.0\nspecification (ex-Swagger), which can be used to describe HTTP APIs.\nChapter 5, Interacting with Other Services, explains how a service interacts with backend\nservices, how to deal with network splits and other interaction problems, and how to test\nthe service in isolation.",
      "content_length": 2176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "Preface\n[ 2 ]\nChapter 6, Securing Your Services, explains how to secure your microservices and how to\ndeal with user authentication, service-to-service authentication, as well as user\nmanagement. It will also introduce the reader to fraud and abuse, and how to mitigate it.\nChapter 7, Monitoring Your Services, explains how to add logging and metrics in your code,\nand how to make sure you have a clear global understanding of what's going on in your\napplication to track down issues and understand your services usage.\nChapter 8, Bringing It All Together, describes how to design and build a JavaScript\napplication that leverages and uses the microservices in an end-user interface.\nChapter 9, Packaging and Running Runnerly, describes how to package, build, and run the\nwhole Forrest application. As a developer, it's vital to be able to run all the parts that\ncompose your application into a single dev box.\nChapter 10, Containerized Services, explains what is virtualization, how to use Docker, and\nalso how to Dockerize your services.\nChapter 11, Deploying on AWS, introduces you to existing cloud service providers and then\nto the AWS world, and shows how to instantiate servers and use the major AWS services\nthat are useful to run a microservices-based application. It also introduces CoreOS, a Linux\ndistribution specifically created to deploy Docker containers in the cloud.\nChapter 12, What Next?, concludes the book by giving some hints on how your\nmicroservices can be built independently from specific cloud providers and virtualization\ntechnologies, to avoid the trap of putting all your eggs in the same basket. It emphasizes\nwhat you learned in Chapter 9, Packaging and Running Runnerly.\nWhat you need for this book\nTo execute the commands and applications in this book, you will need Python 3.x,\nVirtualenv 1.x, and Docker CE installed on your system. Detailed instructions are given in\nthe chapters where needed.\nWho this book is for\nIf you are a developer who has basic knowledge of Python, the command line, and HTTP-\nbased application principles, and who wants to learn how to build, test, scale, and manage\nPython 3 microservices, then this book is for you. No prior experience of writing\nmicroservices in Python is assumed.",
      "content_length": 2247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "Preface\n[ 3 ]\nConventions\nIn this book, you will find a number of text styles that distinguish between different kinds\nof information. Here are some examples of these styles and an explanation of their meaning.\nCode words in text, database table names, folder names, filenames, file extensions,\npathnames, dummy URLs, user input, and Twitter handles are shown as follows: \"The only\nhint we're using async is the async keyword, which marks the handle function as being a\ncoroutine.\"\nA block of code is set as follows:\n    import time\n    def application(environ, start_response):\n        headers = [('Content-type', 'application/json')]\n        start_response('200 OK', headers)\n    return bytes(json.dumps({'time': time.time()}), 'utf8')\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines\nor items are set in bold:\n    from greenlet import greenlet\n    def test1(x, y):\n        z = gr2.switch(x+y)\n        print(z)\nAny command-line input or output is written as follows:\ndocker-compose up\nNew terms and important words are shown in bold.\nWarnings or important notes appear like this.\nTips and tricks appear like this.",
      "content_length": 1159,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "Preface\n[ 4 ]\nReader feedback\nFeedback from our readers is always welcome. Let us know what you think about this\nbook-what you liked or disliked. Reader feedback is important for us as it helps us develop\ntitles that you will really get the most out of.\nTo send us general feedback, simply e-mail feedback@packtpub.com, and mention the\nbook's title in the subject of your message.\nIf there is a topic that you have expertise in and you are interested in either writing or\ncontributing to a book, see our author guide at www.packtpub.com/authors.\nCustomer support\nNow that you are the proud owner of a Packt book, we have a number of things to help you\nto get the most from your purchase.\nDownloading the example code\nYou can download the example code files for this book from your account at h t t p ://w w w . p\na c k t p u b . c o m . If you purchased this book elsewhere, you can visit h t t p ://w w w . p a c k t p u b . c\no m /s u p p o r t and register to have the files e-mailed directly to you.\nYou can download the code files by following these steps:\nLog in or register to our website using your e-mail address and password.\n1.\nHover the mouse pointer on the SUPPORT tab at the top.\n2.\nClick on Code Downloads & Errata.\n3.\nEnter the name of the book in the Search box.\n4.\nSelect the book for which you're looking to download the code files.\n5.\nChoose from the drop-down menu where you purchased this book from.\n6.\nClick on Code Download.\n7.",
      "content_length": 1451,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "Preface\n[ 5 ]\nOnce the file is downloaded, please make sure that you unzip or extract the folder using the\nlatest version of:\nWinRAR / 7-Zip for Windows\nZipeg / iZip / UnRarX for Mac\n7-Zip / PeaZip for Linux\nThe code bundle for the book is also hosted on GitHub at h t t p s ://g i t h u b . c o m /P a c k t P u b l\ni s h i n g /P y t h o n - M i c r o s e r v i c e s - D e v e l o p m e n t . We also have other code bundles from our\nrich catalog of books and videos available at h t t p s ://g i t h u b . c o m /P a c k t P u b l i s h i n g /.\nCheck them out!\nErrata\nAlthough we have taken every care to ensure the accuracy of our content, mistakes do\nhappen. If you find a mistake in one of our books-maybe a mistake in the text or the code-\nwe would be grateful if you could report this to us. By doing so, you can save other readers\nfrom frustration and help us improve subsequent versions of this book. If you find any\nerrata, please report them by visiting h t t p ://w w w . p a c k t p u b . c o m /s u b m i t - e r r a t a , selecting\nyour book, clicking on the Errata Submission Form link, and entering the details of your\nerrata. Once your errata are verified, your submission will be accepted and the errata will\nbe uploaded to our website or added to any list of existing errata under the Errata section of\nthat title.\nTo view the previously submitted errata, go to h t t p s ://w w w . p a c k t p u b . c o m /b o o k s /c o n t e n\nt /s u p p o r t and enter the name of the book in the search field. The required information will\nappear under the Errata section.\nPiracy\nPiracy of copyrighted material on the Internet is an ongoing problem across all media. At\nPackt, we take the protection of our copyright and licenses very seriously. If you come\nacross any illegal copies of our works in any form on the Internet, please provide us with\nthe location address or website name immediately so that we can pursue a remedy.\nPlease contact us at copyright@packtpub.com with a link to the suspected pirated\nmaterial.\nWe appreciate your help in protecting our authors and our ability to bring you valuable\ncontent.",
      "content_length": 2130,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "Preface\n[ 6 ]\nQuestions\nIf you have a problem with any aspect of this book, you can contact us at\nquestions@packtpub.com, and we will do our best to address the problem.",
      "content_length": 169,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "Introduction\nWhen I started to work at Mozilla 7 years ago, we began to write web services for some\nFirefox features. Some of them eventually became microservices. This change did not\nhappen over time, but gradually. The first driver of this shift was the fact that we moved all\nour services to a cloud provider and started to interact with some of their third-party\nservices. When you host your app in the cloud, a microservice architecture becomes a\nnatural fit. The other driver was the Firefox Account project. We wanted to offer a single\nidentity to our users to interact with our services from Firefox. By doing so, all our services\nhad to interact with the same identity provider, and some server-side pieces started to get\nredesigned as microservices to be more efficient in that context.\nI think a lot of web developers out there have been through a similar experience or are\ngoing through it right now. I also believe Python is one of the best languages to write small\nand efficient microservices; its ecosystem is vibrant and the latest Python 3 features make\nPython competitive in that field against Node.js, which has had a stellar growth in the last 5\nyears.\nThis is what is this book is all about; I wanted to share my experience of writing\nmicroservices in Python through a simple use case that I have created for this purpose--\nRunnerly, which is available on GitHub for you to study. You can interact with me there,\npoint mistakes if you see any, and we can continue to learn about writing excellent Python\napps together.",
      "content_length": 1539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "1\nUnderstanding Microservices\nWe're always trying to improve how we build software, and since the punched-card era, we\nhave improved a lot, to say the least.\nThe microservices trend is one improvement that has emerged in the last few years,\npartially based on companies' willingness to speed up their release cycles. They want to\nship new products and new features to their customers as fast as possible. They want to be\nagile by iterating often, and they want to ship, ship, and ship again.\nIf thousands, or even millions, of customers use your service, pushing in production an\nexperimental feature, and removing it if it does not work, is considered good practice rather\nthan baking it for months before you publish it.\nCompanies such as Netflix are promoting their continuous delivery techniques where small\nchanges are made very often into production, and tested on a subset of the user base.\nThey've developed tools such as Spinnaker (h t t p ://w w w . s p i n n a k e r . i o /) to automate as\nmany steps as possible to update production, and ship their features in the cloud as\nindependent microservices.\nBut if you read Hacker News or Reddit, it can be quite hard to detangle what's useful for\nyou and what's just buzzwords-compliant journalistic-style info.\n\"Write a paper promising salvation, make it a structured something or a virtual\nsomething, or abstract, distributed or higher-order or applicative and you can almost be\ncertain of having started a new cult.\n- Edsger W. Dijkstra",
      "content_length": 1496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "Understanding Microservices\n[ 9 ]\nThis chapter is going to help you understand what are microservices, and will then focus on\nthe various ways in which you can implement them using Python. It's composed of the\nfollowing few sections:\nA word on Service-Oriented Architecture\nMonolithic approach of building an application\nMicroservices approach of building applications\nBenefits of microservices\nPitfalls in microservices\nImplementing microservices with Python\nHopefully, once you've reached the end of the chapter, you will be able to dive into\nbuilding microservices with a good understanding of what they are and what they aren't--\nand how you can use Python.\nOrigins of Service-Oriented Architecture\nThere are many definitions out there, since there is no official standard for microservices.\nPeople often mention Service-Oriented Architecture (SOA) when they are trying to explain\nwhat microservices are.\nSOA predates microservices, and its core principle is the idea that you organize\napplications into a discrete unit of functionality that can be accessed remotely and acted\nupon and updated independently.\n- Wikipedia\nEach unit in this preceding definition is a self-contained service, which implements one\nfacet of a business, and provides its feature through some interface.\nWhile SOA clearly states that services should be standalone processes, it does not enforce\nwhat protocols should be used for those processes to interact with each other, and stays\nquite vague about how you deploy and organize your application.\nIf you read the SOA Manifesto (h t t p ://w w w . s o a - m a n i f e s t o . o r g ) that a handful of experts\npublished on the web circa 2009, they don't even mention if the services interact via the\nnetwork.",
      "content_length": 1738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "Understanding Microservices\n[ 10 ]\nSOA services could communicate via Inter-Process Communication (IPC) using sockets on\nthe same machine, through shared memory, through indirect message queues, or even with\nRemote Procedure Calls (RPC). The options are extensive, and at the end of the day, SOA\ncan be everything and anything as long as you are not running all your application code\ninto a single process.\nHowever, it is common to say that microservices are one specialization of SOA, which have\nstarted to emerge over the last few years, because they fulfill some of the SOA goals which\nare to build apps with standalone components that interact with each other.\nNow if we want to give a complete definition of what are microservices, the best way to do\nit is to first look at how most software are architectured.\nThe monolithic approach\nLet's take a very simple example of a traditional monolithic application: a hotel booking\nwebsite.\nBesides the static HTML content, the website has a booking feature that will let its users\nbook hotels in any city in the world. Users can search for hotels, then book them with their\ncredit cards.\nWhen a user performs a search on the hotel website, the application goes through the\nfollowing steps:\nIt runs a couple of SQL queries against its hotels' database.\n1.\nAn HTTP request to a partner's service is made to add more hotels to the list.\n2.\nAn HTML results page is generated using an HTML template engine.\n3.\nFrom there, once the user has found the perfect hotel and clicked on it to book it, the\napplication performs these steps:\nThe customer gets created in the database if needed, and has to authenticate.\n1.\nPayment is carried out by interacting with the bank web service.\n2.\nThe app saves the payment details in the database for legal reasons.\n3.\nA receipt is generated using a PDF generator.\n4.\nA recap email is sent to the user using the email service.\n5.\nA reservation email is forwarded to the third-party hotel using the email service.\n6.\nA database entry is added to keep track of the reservation.\n7.",
      "content_length": 2056,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "Understanding Microservices\n[ 11 ]\nThis process is a simplified model of course, but quite realistic.\nThe application interacts with a database that contains the hotel's information, the\nreservation details, the billing, the user information, and so on. It also interacts with\nexternal services for sending emails, making payments, and getting more hotels from\npartners.\nIn the good old LAMP (Linux-Apache-MySQL-Perl/PHP/Python) architecture, every \nincoming request generates a cascade of SQL queries on the database, and a few network\ncalls to external services, and then the server generates the HTML response using a\ntemplate engine.\nThe following diagram illustrates this centralized architecture:\nThis application is a typical monolith, and it has a lot of obvious benefits.\nThe biggest one is that the whole application is in a single code base, and when the project\ncoding starts, it makes everything simpler. Building a good test coverage is easy, and you\ncan organize your code in a clean and structured way inside the code base. Storing all the\ndata into a single database also simplifies the development of the application. You can\ntweak the data model, and how the code will query it.",
      "content_length": 1197,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "Understanding Microservices\n[ 12 ]\nThe deployment is also a no brainer: we can tag the code base, build a package, and run it\nsomewhere. To scale it, we can run several instances of the booking app, and run several\ndatabases with some replication mechanism in place.\nIf your application stays small, this model works well and is easy to maintain for a single\nteam.\nBut projects are usually growing, and they get bigger than what was first intended. And\nhaving the whole application in a single code base brings some nasty issues along the way.\nFor instance, if you need to make a sweeping change that is large in scope such as changing\nyour banking service or your database layer, the whole application gets into a very unstable\nstate. These changes are a big deal in the project's life, and they necessitate a lot of extra\ntesting to deploy a new version. And changes like this will happen in a project life.\nSmall changes can also generate collateral damage because different parts of the system\nhave different uptime and stability requirements. Putting the billing and reservation\nprocesses at risk because the function that creates the PDF crashes the server is a bit of a\nproblem.\nUncontrolled growth is another issue. The application is bound to get new features, and\nwith developers leaving and joining the project, the code organization might start to get\nmessy, the tests a bit slower. This growth usually ends up with a spaghetti code base that's\nhard to maintain, with a hairy database that needs complicated migration plans every time\nsome developer refactors the data model.\nBig software projects usually take a couple of years to mature, and then they slowly start to\nturn into an incomprehensible mess that's hard to maintain. And it does not happen\nbecause developers are bad. It happens because as the complexity grows, fewer people fully\nunderstand the implications of every small change they make. So they try to work in\nisolation in one corner of the code base, and when you take the 10,000-foot view of the\nproject, you can see the mess.\nWe've all been there.\nIt's not fun, and developers who work on such a project dream of building the application\nfrom scratch with the newest framework. And by doing so, they usually fall into the same\nissues again--the same story is repeated.\nThe following points summarize the pros and cons of the monolithic approach:\nStarting a project as a monolith is easy, and probably the best approach.\nA centralized database simplifies the design and organization of the data.\nDeploying one application is simple.",
      "content_length": 2564,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "Understanding Microservices\n[ 13 ]\nAny change in the code can impact unrelated features. When something breaks,\nthe whole application may break.\nSolutions to scale your application are limited: you can deploy several instances,\nbut if one particular feature inside the app takes all the resources, it impacts\neverything.\nAs the code base grows, it's hard to keep it clean and under control.\nThere are, of course, some ways to avoid some of the issues described here.\nThe obvious solution is to split the application into separate pieces, even if the resulting\ncode is still going to run in a single process. Developers do this by building their apps with\nexternal libraries and frameworks. Those tools can be in-house or from the Open Source\nSoftware (OSS) community.\nBuilding a web app in Python if you use a framework like Flask, lets you focus on the\nbusiness logic, and makes it very appealing to externalize some of your code into Flask\nextensions and small Python packages. And splitting your code into small packages is often\na good idea to control your application growth.\n\"Small is beautiful.\"\n- The UNIX Philosophy\nFor instance, the PDF generator described in the hotel booking app could be a separate\nPython package that uses Reportlab and some templates to do the work.\nChances are this package can be reused in some other apps, and maybe, even published to\nthe Python Package Index (PyPI) for the community.\nBut you're still building a single application and some problems remain, like the inability to\nscale parts differently, or any indirect issue introduced by a buggy dependency.\nYou'll even get new challenges, because you're now using dependencies. One problem you\ncan get is dependency hell. If one part of your application uses a library, but the PDF\ngenerator can only use a specific version of that library, there are good chances you will\neventually have to deal with it with some ugly workaround, or even fork the dependency to\nhave a custom fix there.\nOf course, all the problems described in this section do not appear on day 1 when the\nproject starts, but rather pile up over time.\nLet's now look at how the same application would look like if we were to use microservices\nto build it.",
      "content_length": 2213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "Understanding Microservices\n[ 14 ]\nThe microservice approach\nIf we were to build the same application using microservices, we would organize the code\ninto several separate components that run in separate processes. Instead of having a single\napplication in charge of everything, we would split it into many different microservices, as\nshown in the following diagram:\nDon't be afraid of the number of components displayed in this diagram. The internal\ninteractions of the monolithic application are just being made visible by separate pieces.\nWe've shifted some of the complexity and ended up with these seven standalone\ncomponents:\nBooking UI: A frontend service, which generates the web user interface, and\n1.\ninteracts with all the other microservices.\nPDF reporting service: A very simple service that would create PDFs for the\n2.\nreceipts or any other document given a template and some data.\nSearch: A service that can be queried to get a list of hotels given a city name. This\n3.\nservice has its own database.",
      "content_length": 1015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "Understanding Microservices\n[ 15 ]\nPayments: A service that interacts with the third-party bank service, and\n4.\nmanages a billing database. It also sends e-mails on successful payments.\nReservations: Stores reservations, and generates PDFs.\n5.\nUsers: Stores the user information, and interacts with users via emails.\n6.\nAuthentication: An OAuth 2-based service that returns authentication tokens,\n7.\nwhich each microservice can use to authenticate when calling others.\nThose microservices, along with the few external services like the email service, would\nprovide a feature set similar to the monolithic application. In this design, each component\ncommunicates using the HTTP protocol, and features are made available through RESTful\nweb services.\nThere's no centralized database, as each microservice deals internally with its own data\nstructures, and the data that gets in and out uses a language-agnostic format like JSON. It\ncould use XML or YAML as long as it can be produced and consumed by any language, and\ntravel through HTTP requests and responses.\nThe Booking UI service is a bit particular in that regard, since it generates the User Interface\n(UI). Depending on the frontend framework used to build the UI, the Booking UI output\ncould be a mix of HTML and JSON, or even plain JSON if the interface uses a static\nJavaScript-based client-side tool to generate the interface directly in the browser.\nBut besides this particular UI case, a web application designed with microservices is a\ncomposition of several microservices, which may interact with each other through HTTP to\nprovide the whole system.\nIn that context, microservices are logical units that focus on a very particular task. Here's a\nfull definition attempt:\nA microservice is a lightweight application, which provides a narrowed\nlist of features with a well-defined contract. It's a component with a single\nresponsibility, which can be developed and deployed independently.\nThis definition does not mention HTTP or JSON, because you could consider a small UDP-\nbased service that exchanges binary data as a microservice for example.\nBut in our case, and throughout the book, all our microservices are just simple web\napplications that use the HTTP protocol, and consume and produce JSON when it's not a\nUI.",
      "content_length": 2283,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "Understanding Microservices\n[ 16 ]\nMicroservice benefits\nWhile the microservices architecture looks more complicated than its monolithic\ncounterpart, its advantages are multiple. It offers the following:\nSeparation of concerns\nSmaller projects to deal with\nMore scaling and deployment options\nWe will discuss them in more detail in the following sections.\nSeparation of concerns\nFirst of all, each microservice can be developed independently by a separate team. For\ninstance, building a reservation service can be a full project on its own. The team in charge\ncan make it in whatever programming language and database, as long as it has a well-\ndocumented HTTP API.\nThat also means the evolution of the app is more under control than with monoliths. For\nexample, if the payment system changes its underlying interactions with the bank, the\nimpact is localized inside that service, and the rest of the application stays stable and is\nprobably unaffected.\nThis loose coupling improves the overall project velocity a lot, as we apply, at the service\nlevel, a philosophy similar to the single responsibility principle.\nThe single responsibility principle was defined by Robert Martin to explain that a class\nshould have only one reason to change; in other words, each class should provide a single,\nwell-defined feature. Applied to microservices, it means that we want to make sure that\neach microservice focuses on a single role.\nSmaller projects\nThe second benefit is breaking the complexity of the project. When you add a feature to an\napplication such as PDF reporting, even if you do it cleanly, you make the base code bigger,\nmore complicated, and sometimes, slower. Building that feature in a separate application\navoids this problem, and makes it easier to write it with whatever tools you want. You can\nrefactor it often, shorten your release cycles, and stay on top of things. The growth of the\napplication remains under your control.",
      "content_length": 1940,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "Understanding Microservices\n[ 17 ]\nDealing with a smaller project also reduces risks when improving the application: if a team\nwants to try out the latest programming language or framework, they can iterate quickly on\na prototype that implements the same microservice API, try it out, and decide whether or\nnot to stick with it.\nOne real-life example in mind is the Firefox Sync storage microservice. There are currently\nsome experiments to switch from the current Python + MySQL implementation to a Go-\nbased one, which stores users' data in standalone SQLite databases. That prototype is highly\nexperimental, but since we have isolated the storage feature in a microservice with a well-\ndefined HTTP API, it's easy enough to give it a try with a small subset of the user base.\nScaling and deployment\nFinally, having your application split into components makes it easier to scale depending on\nyour constraints. Let's say you start getting a lot of customers who book hotels daily, and\nthe PDF generation starts to heat up the CPUs. You can deploy that specific microservice in\nsome servers that have bigger CPUs.\nAnother typical example are RAM-consuming microservices like the ones that interact with\nmemory databases like Redis or Memcache. You could tweak your deployments,\nconsequently, by deploying them on servers with less CPU and a lot more RAM.\nWe can, thus, summarize the benefits of microservices as follows:\nA team can develop each microservice independently, and use whatever\ntechnological stack makes sense. They can define a custom release cycle. All they\nneed to define is a language-agnostic HTTP API.\nDevelopers break the application complexity into logical components. Each\nmicroservice focuses on doing one thing well.\nSince microservices are standalone applications, there's a finer control on\ndeployments, which makes scaling easier.\nThe microservices architecture is good at solving a lot of the problems that may arise once\nyour application starts to grow. However, we need to be aware of some of the new issues\nthey also bring in practice.",
      "content_length": 2066,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "Understanding Microservices\n[ 18 ]\nMicroservices pitfalls\nAs said earlier, building an application with microservices has a lot of benefits, but it's not a\nsilver bullet by all means.\nYou need to be aware of these main problems you might have to deal with when coding\nmicroservices:\nIllogical splitting\nMore network interactions\nData storing and sharing\nCompatibility issues\nTesting\nThese issues will be covered in detail in the following sections.\nIllogical splitting\nThe first issue of a microservice architecture is how it gets designed. There's no way a team\ncan come up with the perfect microservice architecture in the first shot. Some microservices\nlike the PDF generator are an obvious use case. But as soon as you deal with the business\nlogic, there are good chances that your code will move around before you get a good grasp\nof how to split things into the right set of microservices.\nThe design needs to mature with some try-and-fail cycles. And adding and removing\nmicroservices can be more painful than refactoring a monolithic application.\nYou can mitigate this problem by avoiding splitting your app in microservices if the split is\nnot evident.\nPremature splitting is the root of all evil.\nIf there's any doubt that the split makes sense, keeping the code in the same app is the safe\nbet. It's always easier to split apart some of the code into a new microservice later than to\nmerge back to two microservices in the same code base because the decision turned out to\nbe wrong.\nFor instance, if you always have to deploy two microservices together, or if one change in a\nmicroservice impacts the data model of another one, the odds are that you did not split the\napplication correctly, and that those two services should be reunited.",
      "content_length": 1749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "Understanding Microservices\n[ 19 ]\nMore network interactions\nThe second problem is the amount of network interactions added to build the same\napplication. In the monolithic version, even if the code gets messy, everything happens in\nthe same process, and you can send back the result without having to call too many\nbackend services to build the actual response.\nThat requires extra attention on how each backend service is called, and raises a lot of\nquestions like the following:\nWhat happens when the Booking UI cannot reach the PDF reporting service\nbecause of a network split or a laggy service?\nDoes the Booking UI call the other services synchronously or asynchronously?\nHow will that impact the response time?\nWe will need to have a solid strategy to be able to answer all those questions, and we will\naddress those in Chapter 5, Interacting with Other Services.\nData storing and sharing\nAnother problem is data storing and sharing. An effective microservice needs to be\nindependent of other microservices, and ideally, should not share a database. What does\nthis mean for our hotel booking app?\nAgain, that raises a lot of questions such as the following:\nDo we use the same users' IDs across all databases, or do we have independent\nIDs in each service and keep it as a hidden implementation detail?\nOnce a user is added to the system, do we replicate some of her information in\nother services databases via strategies like data pumping, or is that overkill?\nHow do we deal with data removal?\nThese are hard questions to answer, and there are many different ways to solve those\nproblems, as we'll learn throughout the book.\nAvoiding data duplication as much as possible while keeping\nmicroservices in isolation is one of the biggest challenges in designing\nmicroservices-based applications.",
      "content_length": 1800,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "Understanding Microservices\n[ 20 ]\nCompatibility issues\nAnother problem happens when a feature change impacts several microservices. If a change\naffects in a backward incompatible way the data that travels between services, you're in for\nsome trouble.\nCan you deploy your new service, and will it work with older versions of other services? Or\ndo you need to change and deploy several services at once? Does it mean you've just\nstumbled on some services that should probably be merged back together?\nA good versioning and API design hygiene help to mitigate those issues, as we will\ndiscover in the second part of the book when we'll build our application.\nTesting\nLast, when you want to do some end-to-end tests and deploy your whole app, you now\nhave to deal with many bricks. You need to have a robust and agile deployment process to\nbe efficient. You need to be able to play with your whole application when you develop it.\nYou can't fully test things out with just one piece of the puzzle.\nHopefully, there are now many tools to facilitate deployments of applications that are built\nwith several components, as we will learn about throughout this book. And all those tools\nprobably helped in the success and adoption of microservices and vice versa.\nMicroservices-style architecture boosts deployment tools innovation, and\ndeployment tools lower the bar for the approval of microservices-style\narchitecture.\nThe pitfalls of using microservices can be summarized as follows:\nPremature splitting of an application into microservices can lead to architectural\nproblems\nNetwork interactions between microservices add weaknesses spots and\nadditional overhead\nTesting and deploying microservices can be complex\nAnd the biggest challenge--data sharing between microservices is hard\nYou should not worry too much about all the pitfalls described in this section for now.",
      "content_length": 1867,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "Understanding Microservices\n[ 21 ]\nThey may seem overwhelming, and the traditional monolithic application may look like a\nsafer bet, but in the long term, splitting your project into microservices will make many of\nyour tasks, as a developer or as an Operation person (Ops), easier.\nImplementing microservices with Python\nPython is an amazingly versatile language.\nAs you probably already know, it's used to build many different kinds of applications--from\nsimple system scripts that perform tasks on a server to large object-oriented applications\nthat run services for millions of users.\nAccording to a study conducted by Philip Guo in 2014, published on the Association for\nComputing Machinery (ACM) website, Python has surpassed Java in top U.S. universities,\nand is the most popular language to learn computer science.\nThis trend is also true in the software industry. Python sits now in the top five languages in\nthe TIOBE index (h t t p ://w w w . t i o b e . c o m /t i o b e - i n d e x /), and it's probably even bigger in\nthe web development land, since languages like C are rarely used as main languages to\nbuild web applications.\nThis book makes the assumption that you are already familiar with the\nPython programming language. If you are not an experienced Python\ndeveloper, you can read the book Expert Python Programming, Second\nEdition, where you will learn advanced programming skills in Python.\nHowever, some developers criticize Python for being slow and unfit for building efficient\nweb services. Python is slow, and this is undeniable. But it still is a language of choice for\nbuilding microservices, and many major companies are happily using it.\nThis section will give you some background on the different ways you can write\nmicroservices using Python, some insights on asynchronous versus synchronous\nprogramming, and conclude with some details on Python performances.\nThis section is composed of five parts:\nThe WSGI standard\nGreenlet and Gevent\nTwisted and Tornado\nasyncio\nLanguage performances",
      "content_length": 2021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "Understanding Microservices\n[ 22 ]\nThe WSGI standard\nWhat strikes most web developers who start with Python is how easy it is to get a web\napplication up and running.\nThe Python web community has created a standard (inspired by the Common Gateway\nInterface or CGI) called Web Server Gateway Interface (WSGI). It simplifies a lot how you\ncan write a Python application in order to serve HTTP requests.\nWhen your code uses that standard, your project can be executed by standard web servers\nlike Apache or nginx, using WSGI extensions like uwsgi or mod_wsgi.\nYour application just has to deal with incoming requests and send back JSON responses,\nand Python includes all that goodness in its standard library.\nYou can create a fully functional microservice that returns the server's local time with a\nvanilla Python module of fewer than 10 lines. It is given as follows:\n    import json\n    import time\n    def application(environ, start_response):\n        headers = [('Content-type', 'application/json')]\n        start_response('200 OK', headers)\n        return [bytes(json.dumps({'time': time.time()}), 'utf8')]\nSince its introduction, the WSGI protocol became an essential standard, and the Python web\ncommunity widely adopted it. Developers wrote middlewares, which are functions you can\nhook before or after the WSGI application function itself, to do something within the\nenvironment.\nSome web frameworks, like Bottle (h t t p ://b o t t l e p y . o r g ), were created specifically\naround that standard, and soon enough, every framework out there could be used through\nWSGI in one way or another.\nThe biggest problem with WSGI though is its synchronous nature. The application function\nyou saw in the preceding code is called exactly once per incoming request, and when the\nfunction returns, it has to send back the response. That means that every time you call the\nfunction, it will block until the response is ready.\nAnd writing microservices means your code will have to wait for responses from various\nnetwork resources all the time. In other words, your application will be idle, and just block\nthe client until everything is ready.",
      "content_length": 2141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "Understanding Microservices\n[ 23 ]\nThat's an entirely okay behavior for HTTP APIs. We're not talking about building\nbidirectional applications like web socket-based ones. But what happens when you have\nseveral incoming requests that call your application at the same time?\nWSGI servers will let you run a pool of threads to serve several requests concurrently. But\nyou can't run thousands of them, and as soon as the pool is exhausted, the next request will\nblock the client's access even if your microservice is doing nothing but idling and waiting\nfor backend services' responses.\nThat's one of the reasons why non-WSGI frameworks like Twisted and Tornado, and in \nJavaScript land, Node.js, became very successful--it's fully async.\nWhen you're coding a Twisted application, you can use callbacks to pause and resume the\nwork done to build a response. That means that you can accept new requests and start to\ntreat them. That model dramatically reduces the idling time in your process. It can serve\nthousands of concurrent requests. Of course, that does not mean the application will return\neach single response faster. It just means one process can accept more concurrent requests,\nand juggle between them as the data is getting ready to be sent back.\nThere's no simple way with the WSGI standard to introduce something similar, and the\ncommunity has debated for years to come up with a consensus--and failed. The odds are\nthat the community will eventually drop the WSGI standard for something else.\nIn the meantime, building microservices with synchronous frameworks is still possible and\ncompletely fine if your deployments take into account the one request == one thread limitation\nof the WSGI standard.\nThere's, however, one trick to boost synchronous web applications--Greenlet, which is\nexplained in the following section.\nGreenlet and Gevent\nThe general principle of asynchronous programming is that the process deals with several\nconcurrent execution contexts to simulate parallelism.\nAsynchronous applications use an event loop that pauses and resumes execution contexts\nwhen an event is triggered--only one context is active, and they take turns. Explicit\ninstruction in the code will tell the event loop that this is where it can pause the execution.",
      "content_length": 2265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "Understanding Microservices\n[ 24 ]\nWhen that occurs, the process will look for some other pending work to resume. Eventually,\nthe process will come back to your function and continue it where it stopped. Moving from\nan execution context to another is called switching.\nThe Greenlet project (h t t p s ://g i t h u b . c o m /p y t h o n - g r e e n l e t /g r e e n l e t ) is a package \nbased on the Stackless project, a particular CPython implementation, and provides\ngreenlets.\nGreenlets are pseudo-threads that are very cheap to instantiate, unlike real threads, and that\ncan be used to call Python functions. Within those functions, you can switch, and give back\nthe control to another function. The switching is done with an event loop, and allows you to\nwrite an asynchronous application using a thread-like interface paradigm.\nHere's an example from the Greenlet documentation:\n    from greenlet import greenlet\n    def test1(x, y):\n        z = gr2.switch(x+y)\n        print(z)\n    def test2(u):\n        print (u)\n        gr1.switch(42)\n    gr1 = greenlet(test1)\n    gr2 = greenlet(test2)\n    gr1.switch(\"hello\", \" world\")\nThe two greenlets in the preceding example explicitly switch from one to the other.\nFor building microservices based on the WSGI standard, if the underlying code uses\ngreenlets, we could accept several concurrent requests, and just switch from one to another\nwhen we know a call is going to block the request--like I/O requests.\nHowever, switching from one greenlet to another has to be done explicitly, and the\nresulting code can quickly become messy and hard to understand. That's where Gevent can\nbecome very useful.\nThe Gevent project (h t t p ://w w w . g e v e n t . o r g /) is built on top of Greenlet, and offers an\nimplicit and automatic way of switching between greenlets, among many other things.",
      "content_length": 1839,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "Understanding Microservices\n[ 25 ]\nIt provides a cooperative version of the socket module, which uses greenlets to automatically\npause and resume the execution when some data is made available in the socket. There's\neven a monkey patch feature, which automatically replaces the standard library socket with\nGevent's version. That makes your standard synchronous code magically asynchronous\nevery time it uses sockets--with just one extra line:\n    from gevent import monkey; monkey.patch_all()\n    def application(environ, start_response):\n        headers = [('Content-type', 'application/json')]\n        start_response('200 OK', headers)\n        # ...do something with sockets here...\n        return result\nThis implicit magic comes at a price though. For Gevent to work well, all the underlying\ncode needs to be compatible with the patching that Gevent does. Some packages from the\ncommunity will continue to block or even have unexpected results because of this--in\nparticular, if they use C extensions, and bypass some of the features of the standard library\nGevent patched.\nBut it works well for most cases. Projects that play well with Gevent are dubbed green, and\nwhen a library is not functioning well, and the community asks its authors to make it green,\nit usually happens.\nThat's what was used to scale the Firefox Sync service at Mozilla, for instance.\nTwisted and Tornado\nIf you are building microservices where increasing the number of concurrent requests you\ncan hold is important, it's tempting to drop the WSGI standard, and just use an\nasynchronous framework like Tornado (h t t p ://w w w . t o r n a d o w e b . o r g /) or Twisted (h t t p s\n://t w i s t e d m a t r i x . c o m /t r a c /).\nTwisted has been around for ages. To implement the same microservices, you need to write\na slightly more verbose code like this:\n    import time\n    import json\n    from twisted.web import server, resource\n    from twisted.internet import reactor, endpoints\n    class Simple(resource.Resource):\n        isLeaf = True\n        def render_GET(self, request):",
      "content_length": 2068,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "Understanding Microservices\n[ 26 ]\n            request.responseHeaders.addRawHeader(b\"content-type\",\n                                                 b\"application/json\")\n            return bytes(json.dumps({'time': time.time()}), 'utf8')\n        site = server.Site(Simple())\n        endpoint = endpoints.TCP4ServerEndpoint(reactor, 8080)\n        endpoint.listen(site)\n        reactor.run()\nWhile Twisted is an extremely robust and efficient framework, it suffers from a few\nproblems when building HTTP microservices, which are as follows:\nYou need to implement each endpoint in your microservice with a class derived\nfrom a Resource class, and that implements each supported method. For a few\nsimple APIs, it adds a lot of boilerplate code.\nTwisted code can be hard to understand and debug due to its asynchronous\nnature.\nIt's easy to fall into callback hell when you chain too many functions that get\ntriggered successively one after the other--and the code can get messy.\nProperly testing your Twisted application is hard, and you have to use a Twisted-\nspecific unit testing model.\nTornado is based on a similar model, but does a better job in some areas. It has a lighter\nrouting system, and does everything possible to make the code closer to plain Python.\nTornado also uses a callback model, so debugging can be hard.\nBut both frameworks are working hard at bridging the gap to rely on the new async\nfeatures introduced in Python 3.\nasyncio\nWhen Guido van Rossum started to work on adding async features in Python 3, part of the\ncommunity pushed for a Gevent-like solution, because it made a lot of sense to write\napplications in a synchronous, sequential fashion rather than having to add explicit\ncallbacks like in Tornado or Twisted.\nBut Guido picked the explicit technique, and experimented in a project called Tulip\ninspired by Twisted. Eventually, the asyncio module was born out of that side project and\nadded into Python.",
      "content_length": 1936,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "Understanding Microservices\n[ 27 ]\nIn hindsight, implementing an explicit event loop mechanism in Python instead of going\nthe Gevent way makes a lot of sense. The way the Python core developers coded asyncio,\nand how they elegantly extended the language with the async and await keywords to\nimplement coroutines, made asynchronous applications built with vanilla Python 3.5+ code\nlook very elegant and close to synchronous programming.\nCoroutines are functions that can suspend and resume their execution.\nChapter 12, What Next?, explains in detail how they are implemented in\nPython and how to use them.\nBy doing this, Python did a great job at avoiding the callback syntax mess we sometimes see\nin Node.js or Twisted (Python 2) applications.\nAnd beyond coroutines, Python 3 has introduced a full set of features and helpers in the\nasyncio package to build asynchronous applications, refer to h t t p s ://d o c s . p y t h o n . o r g\n/3/l i b r a r y /a s y n c i o . h t m l .\nPython is now as expressive as languages like Lua to create coroutine-based applications,\nand there are now a few emerging frameworks that have embraced those features, and will\nonly work with Python 3.5+ to benefit from this.\nKeepSafe's aiohttp (h t t p ://a i o h t t p . r e a d t h e d o c s . i o ) is one of them, and building the\nsame microservice, fully asynchronous, with it would simply need these few elegant lines:\n    from aiohttp import web\n    import time\n    async def handle(request):\n        return web.json_response({'time': time.time()})\n    if __name__ == '__main__':\n        app = web.Application()\n        app.router.add_get('/', handle)\n        web.run_app(app)\nIn this small example, we're very close to how we would implement a synchronous app.\nThe only hint we're using async is the async keyword, which marks the handle function as\nbeing a coroutine.",
      "content_length": 1859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "Understanding Microservices\n[ 28 ]\nAnd that's what's going to be used at every level of an async Python app going forward.\nHere's another example using aiopg, a PostgreSQL library for asyncio from the project\ndocumentation:\n    import asyncio\n    import aiopg\n    dsn = 'dbname=aiopg user=aiopg password=passwd host=127.0.0.1'\n    async def go():\n        pool = await aiopg.create_pool(dsn)\n        async with pool.acquire() as conn:\n            async with conn.cursor() as cur:\n                await cur.execute(\"SELECT 1\")\n                ret = []\n                async for row in cur:\n                    ret.append(row)\n                assert ret == [(1,)]\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(go())\nWith a few async and await prefixes, the function that performs an SQL query and sends\nback the result looks a lot like a synchronous function.\nBut asynchronous frameworks and libraries based on Python 3 are still emerging, and if you\nare using asyncio or a framework like aiohttp, you will need to stick with particular\nasynchronous implementations for each feature you need.\nIf you need to use a library that is not asynchronous in your code, to use it from your\nasynchronous code means that you will need to go through some extra and challenging\nwork if you want to prevent blocking the event loop.\nIf your microservices deal with a limited number of resources, it could be manageable. But\nit's probably a safer bet at the time of this writing to stick with a synchronous framework\nthat's been around for a while rather than an asynchronous one. Let's enjoy the existing\necosystem of mature packages, and wait until the asyncio ecosystem gets more\nsophisticated.\nAnd there are many great synchronous frameworks to build microservices with Python,\nlike Bottle, Pyramid with Cornice, or Flask.",
      "content_length": 1825,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "Understanding Microservices\n[ 29 ]\nThere are good chances that the second edition of this book will use an\nasynchronous framework. But for this edition, we'll use the Flask\nframework throughout the book. It's been around for some time, and is\nvery robust and mature. However, keep in mind that whatever Python\nweb framework you use, you should be able to transpose all the examples\nin this book. This is because most of the coding involved when building\nmicroservices is very close to plain Python, and the framework is mostly\nto route the requests and offer a few helpers.\nLanguage performances\nIn the previous sections, we've been through the two different ways to write microservices:\nasynchronous versus synchronous, and whatever technique you use, the speed of Python\ndirectly impacts the performance of your microservice.\nOf course, everyone knows Python is slower than Java or Go, but execution speed is not\nalways the top priority. A microservice is often a thin layer of code that sits most of its life\nwaiting for some network responses from other services. Its core speed is usually less\nimportant than how fast your SQL queries will take to return from your Postgres server,\nbecause the latter will represent most of the time spent to build the response.\nBut wanting an application that's as fast as possible is legitimate.\nOne controversial topic in the Python community around speeding up the language is how\nthe Global Interpreter Lock (GIL) mutex can ruin performances, because multi-threaded\napplications cannot use several processes.\nThe GIL has good reasons to exist. It protects non-thread-safe parts of the CPython\ninterpreter, and exists in other languages like Ruby. And all attempts to remove it so far\nhave failed to produce a faster CPython implementation.\nLarry Hasting is working on a GIL-free CPython project called Gilectomy\n(h t t p s ://g i t h u b . c o m /l a r r y h a s t i n g s /g i l e c t o m y ). Its minimal goal is to\ncome up with a GIL-free implementation, which can run a single-threaded\napplication as fast as CPython. As of the time of this writing, this\nimplementation is still slower that CPython. But it's interesting to follow\nthis work, and see if it reaches speed parity one day. That would make a\nGIL-free CPython very appealing.",
      "content_length": 2283,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "Understanding Microservices\n[ 30 ]\nFor microservices, besides preventing the usage of multiple cores in the same process, the\nGIL will slightly degrade performances on high load because of the system calls overhead\nintroduced by the mutex.\nHowever, all the scrutiny around the GIL has been beneficial: work has been done in the\npast years to reduce GIL contention in the interpreter, and in some areas, Python’s\nperformance has improved a lot.\nBear in mind that even if the core team removes the GIL, Python is an interpreted and\ngarbage collected language and suffers performance penalties for those properties.\nPython provides the dis module if you are interested to see how the interpreter\ndecomposes a function. In the following example, the interpreter will decompose a simple\nfunction that yields incremented values from a sequence in no less than 29 steps:\n    >>> def myfunc(data):\n    ...     for value in data:\n    ...         yield value + 1\n    ...\n    >>> import dis\n    >>> dis.dis(myfunc)\n      2           0 SETUP_LOOP              23 (to 26)\n                  3 LOAD_FAST                0 (data)\n                  6 GET_ITER\n            >>    7 FOR_ITER                15 (to 25)\n                  10 STORE_FAST              1 (value)\n      3         13 LOAD_FAST                 1 (value)\n                16 LOAD_CONST                1 (1)\n                19 BINARY_ADD\n                20 YIELD_VALUE\n                21 POP_TOP\n                22 JUMP_ABSOLUTE        7\n          >>    25 POP_BLOCK\n          >>    26 LOAD_CONST                0 (None)\n                29 RETURN_VALUE\nA similar function written in a statically compiled language will dramatically reduce the\nnumber of operations required to produce the same result. There are ways to speed up\nPython execution, though.\nOne is to write a part of your code into compiled code by building C extensions, or using a\nstatic extension of the language like Cython (h t t p ://c y t h o n . o r g /), but that makes your\ncode more complicated.",
      "content_length": 2019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "Understanding Microservices\n[ 31 ]\nAnother solution, which is the most promising one, is by simply running your application\nusing the PyPy interpreter (h t t p ://p y p y . o r g /).\nPyPy implements a Just-In-Time (JIT) compiler. This compiler directly replaces, at runtime,\npieces of Python with machine code that can be directly used by the CPU. The whole trick\nfor the JIT is to detect in real time, ahead of the execution, when and how to do it.\nEven if PyPy is always a few Python versions behind CPython, it has reached a point where\nyou can use it in production, and its performances can be quite amazing. In one of our\nprojects at Mozilla that needs fast execution, the PyPy version was almost as fast as the Go\nversion, and we've decided to use Python there instead.\nThe Pypy Speed Center website is a great place to look at how PyPy\ncompares to CPython ( h t t p ://s p e e d . p y p y . o r g /).\nHowever, if your program uses C extensions, you will need to recompile them for PyPy,\nand that can be a problem. In particular, if other developers maintain some of the\nextensions you are using.\nBut if you build your microservice with a standard set of libraries, chances are that it will\nwork out of the box with the PyPy interpreter, so that's worth a try.\nIn any case, for most projects, the benefits of Python and its ecosystem largely surpass the\nperformance issues described in this section, because the overhead in a microservice is\nrarely a problem. And if performance is a problem, the microservice approach allows you to\nrewrite performance-critical components without affecting the rest of the system.\nSummary\nIn this chapter, we've compared the monolithic versus microservice approach to building\nweb applications, and it became apparent that it's not a binary world where you have to\npick one model on day one and stick with it.\nYou should see microservices as an improvement of an application that started its life as a\nmonolith. As the project matures, parts of the service logic should migrate into\nmicroservices. It is a useful approach as we've learned in this chapter, but it should be done\ncarefully to avoid falling into some common traps.",
      "content_length": 2168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "Understanding Microservices\n[ 32 ]\nAnother important lesson is that Python is considered to be one of the best languages to\nwrite web applications, and therefore, microservices--for the same reasons, it's a language of\nchoice in other areas, and also because it provides tons of mature frameworks and packages\nto do the work.\nWe've rapidly looked through the chapter at several frameworks, both synchronous and\nasynchronous, and for the rest of the book, we'll be using Flask.\nThe next chapter will introduce this fantastic framework, and if you are not familiar with it,\nyou will probably love it.\nLastly, Python is a slow language, and that can be a problem in very specific cases. But\nknowing what makes it slow, and the different solutions to avoid this issue will usually be\nenough to make that problem not relevant.",
      "content_length": 821,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "2\nDiscovering Flask\nFlask was started around 2010, leveraging the Werkzeug WSGI toolkit (h t t p ://w e r k z e u g .\np o c o o . o r g /), which provides the foundations for interacting with HTTP requests via the\nWSGI protocol, and various tools such as a routing system.\nWerkzeug is equivalent to Paste, which provided similar features. The Pylons project (h t t p\n://p y l o n s p r o j e c t . o r g ), which is the umbrella organization for projects like Pyramid --\nanother web framework-- integrated Paste and its various components at some point.\nTogether with Bottle (h t t p ://b o t t l e p y . o r g /) and a handful of other projects, they\ncomposed the Python microframeworks ecosystem.\nAll those projects have a similar goal--they want to offer to the Python community simple\ntools to build web applications faster.\nHowever, the term microframework can be a bit misleading. It does not mean you can only\ncreate micro applications. Using those tools, you can build any application--even a large\none. The prefix micro here means that the framework tries to take as few decisions as\npossible. It lets you freely organize your application code as you want, and use whatever\nlibraries you want.\nA microframework acts as the glue code that delivers requests to your\nsystem, and sends back responses. It does not enforce any particular\nparadigm on your project.\nA typical example of this philosophy is when you need to interact with an SQL database. A\nframework like Django is batteries-included, and provides everything you need to build your\nweb app including an Object-Relational Mapper (ORM) to bind objects with database\nquery results. The rest of the framework tightly integrates with the ORM.",
      "content_length": 1705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "Discovering Flask\n[ 34 ]\nIf you want to use an alternative ORM like SQLAlchemy (SA) in Django to benefit from\nsome of its great features, you'd not be taking the easiest path, because the whole idea of\nDjango is to provide an entire working system, and let the developer focus on building\noriginal features.\nFlask, on the other hand, does not care what library you use to interact with your data. The\nframework will only try to make sure it has enough hooks to be extended by external\nlibraries to provide all kinds of features. In other words, using SQLAlchemy in Flask, and\nmaking sure you're doing the right thing with SQL sessions and transactions, will mostly\nconsist of adding a package like Flask-SQLAlchemy in your project. And if you don't like\nhow that particular library integrates SLQAlchemy, you're free to use another one, or to\nbuild your integration.\nOf course, that's not a silver bullet. Being completely free in your choices also means it's\neasier to make poor decisions, and build an application that relies on defective libraries or\none that's not well designed.\nBut fear not! This chapter will make sure you know what Flask has to offer, and how to\norganize your code for building microservices.\nThis chapter covers the following topics:\nWhich Python?\nHow Flask handles requests\nFlask built-in features\nA microservice skeleton\nThe goal of this chapter is to give you all the information needed to build\nmicroservices with Flask. By doing so, it inevitably duplicates some of the\ninformation you can find in Flask's official documentation--but focuses on\nproviding interesting details and anything relevant when building\nmicroservices. Flask has a good online documentation. Make sure you take\na look at its user guide at h t t p ://f l a s k . p o c o o . o r g /d o c s , which should\nbe a great complement to this chapter. The code base in GitHub, located at\nh t t p s ://g i t h u b . c o m /p a l l e t s /f l a s k , is very well documented as well--\nand the source code is always the ultimate source of truth when you need\nto understand how something works.",
      "content_length": 2085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "Discovering Flask\n[ 35 ]\nWhich Python?\nBefore we start digging into Flask, there's one question we should answer. What Python\nversion should be used at this point with Flask, since it supports both?\nWe're now in 2017, and as we've seen in the previous chapter, Python 3 has made some\nincredible progress. Packages that don't support Python 3 are now less common. Unless\nyou're building something very specific, you should not have any problem with Python 3.\nAnd building microservices means each app will run in isolation, so it would be entirely\nimaginable to run some in Python 2 and some in Python 3 depending on your constraints.\nYou can even using PyPy.\nDespite the initial pushbacks the Flask creator had on some of the Python 3 language\ndecisions, the documentation explicitly says at this point that new projects should start\nusing Python 3; refer to h t t p ://f l a s k . p o c o o . o r g /d o c s /l a t e s t /p y t h o n 3/#p y t h o n 3- s u p p o\nr t .\nSince Flask is not using any new bleeding-edge Python 3 language features, your code will\nprobably be able to run in Python 2 and 3 anyway. In the worst case, you can use a tool like\nSix (h t t p ://p y t h o n h o s t e d . o r g /s i x /) to make your code compatible with both versions if\nyou need to.\nThe general advice is to use Python 3 unless you have some constraints that require Python\n2. Python 2 will not be supported anymore after 2020; see h t t p s ://p y t h o n c l o c k . o r g /.\nThis book uses the latest Python 3.5 stable release for all its code examples,\nbut they are likely to work on the last Python 3.x versions.\nAt this point, you should make sure you have a working Python 3\nenvironment with Virtualenv (h t t p s ://v i r t u a l e n v . p y p a . i o ) installed.\nEvery code example in the book runs in a terminal.\nHow Flask handles requests\nThe framework entry point is the Flask class in the flask.app module. Running a Flask\napplication means running one single instance of this class, which will take care of handling\nincoming Web Server Gateway Interface (WSGI) requests, dispatch them to the right code,\nand then return a response.",
      "content_length": 2137,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "Discovering Flask\n[ 36 ]\nWSGI is a specification that defines the interface between web servers and\nPython applications. The incoming request is described in a single\nmapping, and frameworks such as Flask take care of routing the call to the\nright callable.\nThe class offers a route method, which can decorate your functions. When you decorate a\nfunction with it, it becomes a view, and it's registered into Werkzeug's routing system. That\nsystem uses a small rule engine to match views with incoming requests, and will be\ndescribed later in this chapter.\nHere's a very basic example of a fully functional Flask application:\n    from flask import Flask, jsonify\n    app = Flask(__name__)\n    @app.route('/api')\n    def my_microservice():\n        return jsonify({'Hello': 'World!'})\n    if __name__ == '__main__':\n        app.run()\nThat app returns a JSON mapping when called on /api. Every other endpoint would\nreturn a 404 Error.\nThe __name__ variable, whose value will be __main__ when you run that single Python\nmodule, is the name of the application package. It's used by Flask to instantiate a new\nlogger with that name, and to find where the file is located on the disk. Flask will use the\ndirectory as the root for helpers like the config that's associated with your app, and to\ndetermine default locations for the static and templates directories.\nIf you run that module in a shell, the Flask app will run its web server, and start listen to\nincoming connections on the 5000 port:\n$ python flask_basic.py\n* Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\nCalling /api with the curl command will return a valid JSON response with the right\nheaders, thanks to the jsonify() function, which takes care of converting the Python dict\ninto a valid JSON response with the proper Content-Type header.",
      "content_length": 1809,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "Discovering Flask\n[ 37 ]\nThe curl command is going to be used a lot in this book. If you are under\nLinux or macOS, it should be pre-installed; refer to h t t p s ://c u r l . h a x x . s\ne /.\n$ curl -v http://127.0.0.1:5000/api\n*   Trying 127.0.0.1...\n...\n< HTTP/1.0 200 OK\n< Content-Type: application/json\n< Content-Length: 24\n< Server: Werkzeug/0.11.11 Python/3.5.2\n< Date: Thu, 22 Dec 2016 13:54:41 GMT\n<\n{\n  \"Hello\": \"World!\"\n}\nThe jsonify() function creates a Response object, and dumps the mapping in its body.\nWhile many web frameworks explicitly pass a request object to your code, Flask provides\nan implicit global request variable, which points to the current Request object it built\nwith the incoming call by parsing the HTTP call into a WSGI environment dictionary.\nThis design decision makes the simpler views code very concise: like in our example, if you\ndon't have to look at the request content to reply, there's no need to have it around. As long\nas your view returns what the client should get and Flask can serialize it, everything is\npretty much transparent.\nFor other views, they can just import that variable and use it.\nThe request variable is global, but unique, to each incoming request and\nis thread safe. Flask uses a mechanism called context locals, which we will\nexplain later.\nLet's add some print method calls here and there so that we can see what's happening\nunder the hood:\n    from flask import Flask, jsonify, request\n    app = Flask(__name__)\n    @app.route('/api')\n    def my_microservice():",
      "content_length": 1530,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "Discovering Flask\n[ 38 ]\n        print(request)\n        print(request.environ)\n        response = jsonify({'Hello': 'World!'})\n        print(response)\n        print(response.data)\n        return response\n    if __name__ == '__main__':\n        print(app.url_map)\n        app.run()\nRunning that new version and hitting it with the curl command in another shell, you get a\nlot of details, like the following:\n$ python flask_details.py\nMap([<Rule '/api' (GET, OPTIONS, HEAD) -> my_microservice>,\n     <Rule '/static/<filename>' (GET, OPTIONS, HEAD) -> static>])\n* Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n<Request 'http://127.0.0.1:5000/api' [GET]>\n{'wsgi.url_scheme': 'http', 'HTTP_ACCEPT': '*/*',\n 'wsgi.run_once': False, 'PATH_INFO': '/api', 'SCRIPT_NAME': '',\n 'wsgi.version': (1, 0), 'SERVER_SOFTWARE': 'Werkzeug/0.11.11',\n 'REMOTE_ADDR': '127.0.0.1',\n 'wsgi.input': <_io.BufferedReader name=5>,\n 'SERVER_NAME': '127.0.0.1', 'CONTENT_LENGTH': '',\n 'werkzeug.request': <Request 'http://127.0.0.1:5000/api' [GET]>,\n 'SERVER_PORT': '5000', 'HTTP_USER_AGENT': 'curl/7.51.0',\n 'wsgi.multiprocess': False, 'REQUEST_METHOD': 'GET',\n 'SERVER_PROTOCOL': 'HTTP/1.1', 'REMOTE_PORT': 22135,\n 'wsgi.multithread': False, 'werkzeug.server.shutdown': <function\n      WSGIRequestHandler.make_environ.<locals>.shutdown_server at\n      0x1034e12f0>,\n 'HTTP_HOST': '127.0.0.1:5000', 'QUERY_STRING': '',\n 'wsgi.errors': <_io.TextIOWrapper name='<stderr>' mode='w'\n     encoding='UTF-8'>, 'CONTENT_TYPE': ''}\n <Response 24 bytes [200 OK]>\n b'{n  \"Hello\": \"World!\"n}n'\n 127.0.0.1 - - [22/Dec/2016 15:07:01] \"GET /api HTTP/1.1\" 200",
      "content_length": 1624,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "Discovering Flask\n[ 39 ]\nLet's explore what's happening here on the call:\nRouting: Flask creates the Map class\nRequest: Flask passes a Request object to the view\nResponse: A Response object is sent back with the response content\nRouting\nThe routing happens in app.url_map, which is an instance of Werkzeug's Map class. That\nclass uses regular expressions to determine if a function decorated by @app.route matches\nthe incoming request. The routing only looks at the path you provided in the route call to\nsee if it matches the client's request.\nBy default, the mapper will only accept GET, OPTIONS, and HEAD calls on a declared route.\nCalling a valid endpoint with an unsupported method will return a 405 Method Not\nAllowed response together with the list of supported methods in the Allow header:\n$ curl -v -XDELETE localhost:5000/api\n* Connected to localhost (127.0.0.1) port 5000 (#0)\n> DELETE /api/person/1 HTTP/1.1\n> Host: localhost:5000\n> User-Agent: curl/7.51.0\n> Accept: */*\n>\n* HTTP 1.0, assume close after body\n< HTTP/1.0 405 METHOD NOT ALLOWED\n< Content-Type: text/html\n< Allow: GET, OPTIONS, HEAD\n< Content-Length: 178\n< Server: Werkzeug/0.11.11 Python/3.5.2\n< Date: Thu, 22 Dec 2016 21:35:01 GMT\n<\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>405 Method Not Allowed</title>\n<h1>Method Not Allowed</h1>\n<p>The method is not allowed for the requested URL.</p> *\n   Curl_http_done: called premature == 0\n   Closing connection 0",
      "content_length": 1455,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "Discovering Flask\n[ 40 ]\nIf you want to support specific methods, you can pass them to the route decorator with the\nmethods argument as follows:\n    @app.route('/api', methods=['POST', 'DELETE', 'GET'])\n    def my_microservice():\n        return jsonify({'Hello': 'World!'})\nNote that the OPTIONS and HEADS methods are implicitly added in all\nrules, since it is automatically managed by the request handler. You can\ndeactivate this behavior by setting a provide_automatic_options\nattribute to False to the function. This can be useful when you want to\nadd custom headers in the response when OPTIONS is called, like when\ndealing with CORS where you need to add several Access-Control-\nAllow-* headers.\nVariables and converters\nAnother feature provided by the routing system is variables.\nYou can use variables using the <VARIABLE_NAME> syntax. This notation is pretty standard\n(Bottle uses the same), and allows you to describe endpoints with dynamic values.\nFor example, if you want to create a function that handles all requests to /person/N, with N\nbeing the unique ID of a person, you could use /person/<person_id>.\nWhen Flask calls your function, it converts the value it finds in the URL section as the\nperson_id argument:\n    @app.route('/api/person/<person_id>')\n    def person(person_id):\n        response = jsonify({'Hello': person_id})\n        return response\n    $ curl localhost:5000/api/person/3\n    {\n      \"Hello\": \"3\"\n    }",
      "content_length": 1439,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "Discovering Flask\n[ 41 ]\nIf you have several routes that match the same URL, the mapper uses a\nparticular set of rules to determine which one it calls. This is the\nimplementation description taken from Werkzeug's routing module:\nRules without any arguments come first for performance. This is\n1.\nbecause we expect them to match faster and some common\nrules usually don't have any arguments (index pages, and so\non).\nThe more complex rules come first, so the second argument is\n2.\nthe negative length of the number of weights.\nLastly, we order by the actual weights.\n3.\nWerzeug's Rules have, therefore, weights that are used to sort them, and\nthis is not used or surfaced in Flask. So, it boils down to picking views\nwith more variables first, then the others --in order of appearance--when\nPython imports the different modules. The rule of thumb is to make sure\nthat every declared route in your app is unique, otherwise, tracking which\none gets picked will give you headaches.\nThere's also a basic converter that will convert the variable to a particular type. For instance,\nif you want an integer, you would use <int:VARIABLE_NAME>. In the person example, that\ntranslates to /person/<int:person_id>.\nIf a request matches a route, but a converter fails to change a value, Flask will return a 404\nError unless another route matches the same path.\nBuilt-in converters are string (the default, a Unicode string), int, float, path, any, and\nuuid.\nThe path converter is like the default converter, but includes slashes. It's similar to the\n[^/].*? regular expression.\nThe any converter allows you to combine several values. It's a bit too smart, and rarely\nused. The uuid converter matches the UUIDs strings.\nIt's quite easy to create your custom converter. For example, if you want to match users' IDs\nwith usernames, you could create a converter that looks up a database, and converts the\ninteger into a username.",
      "content_length": 1911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "Discovering Flask\n[ 42 ]\nTo do this, you need to create a class derived from the BaseConverter class, which\nimplements two methods: the to_python() method to convert the value to a Python object\nfor the view, and the to_url() method to go the other way (used by url_for() described\nin the next section uses to_url()):\n    from flask import Flask, jsonify, request\n    from werkzeug.routing import BaseConverter, ValidationError\n    _USERS = {'1': 'Tarek', '2': 'Freya'}\n    _IDS = {val: id for id, val in _USERS.items()}\n    class RegisteredUser(BaseConverter):\n        def to_python(self, value):\n            if value in _USERS:\n                return _USERS[value]\n            raise ValidationError()\n        def to_url(self, value):\n            return _IDS[value]\n    app = Flask(__name__)\n    app.url_map.converters['registered'] = RegisteredUser\n    @app.route('/api/person/<registered:name>')\n    def person(name):\n        response = jsonify({'Hello hey': name})\n        return response\n    if __name__ == '__main__':\n        app.run()\nThe ValidationError method is raised in case the conversion fails, and the mapper will\nconsider that the route simply does not match that request.\nLet's try a few calls to see how that works in practice:\n$ curl localhost:5000/api/person/1\n{\n  \"Hello hey\": \"Tarek\"\n}\n$ curl localhost:5000/api/person/2\n{\n  \"Hello hey\": \"Freya\"\n}\n$ curl localhost:5000/api/person/3",
      "content_length": 1404,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "Discovering Flask\n[ 43 ]\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>404 Not Found</title>\n<h1>Not Found</h1>\n<p> The requested URL was not found on the server.  If you entered\n    the URL manually please check your spelling and try again.</p>\nBut beware that this was just an example to demonstrate the power of converters. In real\napplications, we would need to be careful not to rely on too many converters, because it\nwould be painful to change all the routes when the code evolves.\nThe best practice for routing is to keep it as static and straightforward as\npossible, and see it as mere labels you put on your functions.\nThe url_for function\nThe last interesting feature of Flask's routing system is the url_for() function. Given any\nview, it will return its actual URL.\nHere's an example with the previous app:\n    >>> from flask_converter import app\n    >>> from flask import url_for\n    >>> with app.test_request_context():\n    ...     print(url_for('person', name='Tarek'))\n    ...\n    /api/person/1\nThe previous example uses the Read-Eval-Print Loop (REPL), which you\ncan get by running the Python executable directly.\nThis feature is quite useful in templates when you want to display the URLs of some views\ndepending on the execution context. Instead of hardcoding some links, you can just point\nthe function name to url_for to get it.",
      "content_length": 1367,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "Discovering Flask\n[ 44 ]\nRequest\nWhen a request comes in, Flask calls the view inside a thread-safe block, and uses\nWerzeug's local helper (h t t p ://w e r k z e u g . p o c o o . o r g /d o c s /l a t e s t /l o c a l /). This helper\ndoes a job similar to Python's threading.local\n(https://docs.python.org/3/library/threading.html#thread-local-data), and makes\nsure that each thread has an isolated environment, specific to that request.\nIn other words, when you access the global request object in your view, you are guaranteed\nthat it's unique to your thread, and will not leak data to another thread in a multi-threaded\nenvironment.\nAs we've seen earlier, Flask uses the incoming WSGI environment data to create the request\nobject. That object is a Request class instance, which merges several mixin classes in\ncharge of parsing specific headers from the incoming environment.\nCheck out the WSGI PEP (Python Environment Proposal) to get more\ndetails on what's in a WSGI environment at h t t p s ://w w w . p y t h o n . o r g /d e\nv /p e p s /p e p - 0333/#e n v i r o n - v a r i a b l e s .\nThe bottom line is that a view can introspect the incoming request through the request\nobject attributes without having to deal with some parsing. The work done by Flask is quite\nhigh level. For instance, the Authorization header is looked at and decomposed\nautomatically when possible.\nIn the following example, an HTTP Basic Auth that is sent by the client is always\nconverted to a base64 form when sent to the server. Flask will detect the Basic prefix, and\nwill parse it into username and password fields in the request.authorization\nattribute:\n    from flask import Flask, request\n    app = Flask(__name__)\n    @app.route(\"/\")\n    def auth():\n        print(\"The raw Authorization header\")\n        print(request.environ[\"HTTP_AUTHORIZATION\"])\n        print(\"Flask's Authorization header\")\n        print(request.authorization)\n        return \"\"\n    if __name__ == \"__main__\":\n        app.run()",
      "content_length": 1994,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "Discovering Flask\n[ 45 ]\n    $ curl http://localhost:5000/ -u tarek:password\n    $ bin/python flask_auth.py\n    * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n    The raw Authorization header\n    Basic dGFyZWs6cGFzc3dvcmQ=\n    Flask's Authorization header\n    {'username': 'tarek', 'password': 'password'}\n    127.0.0.1 - - [26/Dec/2016 11:33:04] \"GET / HTTP/1.1\" 200 -\nThis behavior makes it easy to implement a pluggable authentication system on top of the\nrequest object.\nOther common request elements like cookies, files, and so on are all accessible via other\nattributes, as we will discover throughout the book.\nResponse\nIn the previous examples, we've used the jsonify() function, which creates a Response\nobject from the mapping returned by the view.\nThe Response object is, technically, a standard WSGI application you could use directly.\nIt's wrapped by Flask, and called with the WSGI's environ, and the start_response\nfunction is received from the web server.\nWhen Flask picks a view via its URL mapper, it expects it to return a callable object that can\nreceive the environ and start_response arguments.\nThis design may seem a little awkward since the WSGI environ is already\nparsed into a Request object by the time the Response object is called\nwith the WSGI environ again. But, in practice, this is just an\nimplementation detail. When your code needs to interact with the request,\nit can use the global Request object, and ignore what's happening inside\nthe Response class.",
      "content_length": 1501,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "Discovering Flask\n[ 46 ]\nIn case the returned value is not a callable, Flask will try to convert it into a Response object\nif it's one of the following cases:\nstr: The data gets encoded as UTF-8 and used as the HTTP response body.\nbytes/bytesarray: Used as the body.\nA (response, status, headers) tuple: Where response can be a Response object or\none of the previous types. status is an integer value that overwrites the response\nstatus, and headers is a mapping that extends the response headers.\nA (response, status) tuple: Like the previous one, but without specific headers\nA (response, headers) tuple: Like the preceding one, but with just extra headers.\nAny other case will lead to an exception.\nIn most cases, when building microservices, we'll use the built-in jsonify() function, but\nin case you need your endpoints to produce another content type, creating a function that\nwill convert the generated data into a Response class is easy enough.\nHere's an example with YAML: the yamlify() function will return a (response, status,\nheaders) tuple, which will be converted by Flask into a proper Response object.\n    from flask import Flask\n    import yaml      # requires PyYAML\n    app = Flask(__name__)\n    def yamlify(data, status=200, headers=None):\n        _headers = {'Content-Type': 'application/x-yaml'}\n        if headers is not None:\n            _headers.update(headers)\n        return yaml.safe_dump(data), status, _headers\n    @app.route('/api')\n    def my_microservice():\n        return yamlify(['Hello', 'YAML', 'World!'])\n    if __name__ == '__main__':\n        app.run()\nThe way Flask handles requests can be summarized as follows:\nWhen the application starts, any function decorated with @app.route() is\n1.\nregistered as a view, and stored into the app.url_map.\nA call is dispatched to the right view depending on its endpoint and method.\n2.",
      "content_length": 1863,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "Discovering Flask\n[ 47 ]\nA Request object is created in a thread-safe thread-local execution context.\n3.\nA Response object wraps the content to send back.\n4.\nThese four steps are roughly all you need to know to start building apps using Flask. The\nnext section will summarize the most important built-in features that Flask offers alongside\nthis request-response mechanism.\nFlask built-in features\nThe previous section gave us a good understanding of how Flask processes a request, and\nthat's good enough to get you started.\nBut Flask comes with more helpers, which are quite useful. We'll discover the following\nmain ones in this section:\nThe session object: Cookie-based data\nGlobals: Storing data in the request context\nSignals: Sending and intercepting events\nExtensions and middlewares: Adding features\nTemplates: Building text-based content\nConfiguring: Grouping your running options in a config file\nBlueprints: Organizing your code in namespaces\nError handling and debugging: Dealing with errors in your app\nThe session object\nLike the request object, Flask creates a session object, which is unique to the request\ncontext.\nIt's a dict-like object, which Flask serializes into a cookie on the user side. The data contained\ninto the session mapping is dumped into a JSON mapping, then compressed using zlib\nwhen that makes it smaller, and finally encoded in base64.\nWhen the session gets serialized, the itsdangerous (h t t p s ://p y t h o n h o s t e d . o r g /i t s d a n g e\nr o u s /) library signs the content using the secret_key value defined at the application\nlevel. The signing uses HMAC (h t t p s ://e n . w i k i p e d i a . o r g /w i k i /H a s h - b a s e d _ m e s s a g e _ a\nu t h e n t i c a t i o n _ c o d e ) and SHA1.",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "Discovering Flask\n[ 48 ]\nThis signature, which is added as a suffix in the data, ensures that the client cannot tamper\nwith the data that is stored in a cookie unless they know the secret key to sign the data.\nNote that the data itself is not encrypted.\nFlask will let you customize the signing algorithm to use, but HMAC + SHA1 is good\nenough when you need to store data in cookies.\nHowever, when you're building microservices that are not producing HTML, you rarely\nrely on cookies since they are specific to web browsers. But the idea of keeping a volatile\nkey-value storage per user can be extremely useful to speed up some of the server-side\nwork. For instance, if you need to perform some database look-ups to get some information\nabout a user every time they connect, caching this information in a session-like object on the\nserver side makes a lot of sense.\nGlobals\nAs discussed earlier in this chapter, Flask provides a mechanism to store global variables\nthat are unique to a particular thread and request context. That's used for request and\nsession, but is also available to store any custom object.\nThe flask.g variable contains all globals, and you can set whatever attributes you want on\nit.\nIn Flask, the @app.before_request decorator can be used to point a function that the app\nwill call every time a request is made just before it dispatches the request to a view.\nIt's a typical pattern in Flask to use before_request to set values in the globals. That way,\nall the functions that are called within the request context can interact with g and get the\ndata.\nIn the following example, we copy the username provided when the client performs an\nHTTP basic authentication in the user attribute:\n    from flask import Flask, jsonify, g, request\n    app = Flask(__name__)\n    @app.before_request\n    def authenticate():\n        if request.authorization:\n            g.user = request.authorization['username']\n        else:\n            g.user = 'Anonymous'",
      "content_length": 1968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "Discovering Flask\n[ 49 ]\n    @app.route('/api')\n    def my_microservice():\n        return jsonify({'Hello': g.user})\n    if __name__ == '__main__':\n        app.run()\nWhen a client requests the /api view, the authenticate function will set g.user depending\non the provided headers:\n$ curl http://127.0.0.1:5000/api\n{\n  \"Hello\": \"Anonymous\"\n}\n$ curl http://127.0.0.1:5000/api --user tarek:pass\n{\n  \"Hello\": \"tarek\"\n}\nAny data you may think of that's specific to a request context, and could be shared\nthroughout your code, can be shared via flask.g.\nSignals\nFlask integrates with Blinker (h t t p s ://p y t h o n h o s t e d . o r g /b l i n k e r /), which is a signal\nlibrary that lets you subscribe a function to an event.\nEvents are instances of the blinker.signal class created with a unique label, and Flask\ninstantiates ten of them in 0.12. Flask triggers signals at critical moments during the \nprocessing of a request. Refer to h t t p ://f l a s k . p o c o o . o r g /d o c s /l a t e s t /a p i /#c o r e - s i g n\na l s - l i s t for the full list.\nRegistering to a particular event is done by calling the signal's connect method. Signals are\ntriggered when some code calls the signal's send method. The send method accepts extra\narguments to pass data to all the registered functions.\nIn the following example, we register the finished function to the request_finished\nsignal. That function will receive the response object:\n    from flask import Flask, jsonify, g, request_finished\n    from flask.signals import signals_available\n    if not signals_available:\n        raise RuntimeError(\"pip install blinker\")",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "Discovering Flask\n[ 50 ]\n    app = Flask(__name__)\n    def finished(sender, response, **extra):\n        print('About to send a Response')\n        print(response)\n    request_finished.connect(finished)\n    @app.route('/api')\n    def my_microservice():\n        return jsonify({'Hello': 'World'})\n    if __name__ == '__main__':\n        app.run()\nNotice that the signal feature will only work if you install Blinker, which is not installed by\ndefault as a dependency when you install Flask.\nSome signals implemented in Flask are not useful in microservices, such as the ones\noccurring when the framework renders a template. But there are some interesting signals\nthat Flask triggers throughout the request life, which can be used to log what's going on\nFor instance, the got_request_exception signal is triggered when an exception occurs\nbefore the framework does something with it. That's how Sentry's (h t t p s ://s e n t r y . i o )\nPython client (Raven) hooks itself onto Flask to log exceptions.\nIt can also be interesting to implement custom signals in your apps when you want to\ntrigger some of your features with events and decouple the code.\nFor example, if your microservice produces PDF reports, and you want to have the reports\ncryptographically signed, you could trigger a report_ready signal, and have a signer\nregister to that event.\nOne important aspect of the Blinker implementation is that all registered functions are\ncalled in no particular order and synchronously on the signal.send calls. So, if your\napplication starts to use a lot of signals, all the triggering could become an important part of\nthe time spent processing a request, and create bottlenecks.\nIf you need to do work that doesn't impact the response, consider using a queue like\nRabbitMQ (h t t p s ://w w w . r a b b i t m q . c o m /) to queue up the task and have a separate service\ndo that work.",
      "content_length": 1883,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "Discovering Flask\n[ 51 ]\nExtensions and middlewares\nFlask extensions are simply Python projects that, once installed, provide a package or a\nmodule named flask_something. In previous versions, it was flask.ext.something.\nThe project has to follow a few guidelines, as described at h t t p ://f l a s k . p o c o o . o r g /d o c s\n/l a t e s t /e x t e n s i o n d e v . These guidelines are more or less good practices that could apply\nto any Python project. Flask has a curated list of extensions maintained at h t t p ://f l a s k . p o\nc o o . o r g /e x t e n s i o n s /, which is a good first stop when you are looking for extra features.\nWhat's provided by the extension is up to the developers, and not much is enforced besides\nthe guidelines described in Flask documentation.\nThe other mechanism to extend Flask is to use WSGI middlewares. A WSGI middleware is\na pattern to extend WSGI apps by wrapping the calls made to the WSGI endpoint.\nIn the example that follows, the middleware fakes a X-Forwarded-For header, so the Flask\napplication thinks it's behind a proxy like nginx. This is a useful middleware in a testing\nenvironment when you want to make sure your application behaves properly when it tries\nto get the remote IP address, since the remote_addr attribute will get the IP of the proxy,\nnot the real client:\n    from flask import Flask, jsonify, request\n    import json\n    class XFFMiddleware(object):\n        def __init__(self, app, real_ip='10.1.1.1'):\n            self.app = app\n            self.real_ip = real_ip\n        def __call__(self, environ, start_response):\n            if 'HTTP_X_FORWARDED_FOR' not in environ:\n                values = '%s, 10.3.4.5, 127.0.0.1' % self.real_ip\n                environ['HTTP_X_FORWARDED_FOR'] = values\n            return self.app(environ, start_response)\n    app = Flask(__name__)\n    app.wsgi_app = XFFMiddleware(app.wsgi_app)\n    @app.route('/api')\n    def my_microservice():\n        if \"X-Forwarded-For\" in request.headers:\n            ips = [ip.strip() for ip in\n                   request.headers['X-Forwarded-For'].split(',')]\n            ip = ips[0]\n        else:\n            ip = request.remote_addr",
      "content_length": 2176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "Discovering Flask\n[ 52 ]\n        return jsonify({'Hello': ip})\n    if __name__ == '__main__':\n        app.run()\nNotice that we use app.wsgi_app here to wrap the WSGI app. In Flask,\nthe app object is not the WSGI application itself as we've seen earlier.\nTampering with the WSGI environ before your application gets it is fine, but if you want\nto implement anything that will impact the response, doing it inside a WSGI middleware is\ngoing to make your work very painful.\nThe WSGI protocol requires that the start_response function gets called with the\nresponse status code, and headers before the app sends back the actual body content.\nUnfortunately, a single function call on your application triggers this two-step mechanism.\nSo, changing the results on the fly from outside the app requires some callback magic to\nwork.\nA good example is when you want to modify the response body. That impacts the\nContent-Length header, so your middleware will need to intercept the headers sent by\nthe app, and rewrite them after the body has been modified.\nAnd this is just one problem of the WSGI protocol design; there are many other issues\naround it.\nUnless you want your functionality to work for other WSGI frameworks, there are no good\nreasons to extend your apps with WSGI middlewares. It's much better to write a Flask\nextension that will interact from within the Flask application.\nTemplates\nSending back JSON or YAML documents is easy enough, since we're just serializing data.\nAnd most microservices produce machine-parseable data. But in some cases, we might need\nto create documents with some layout--whether it's an HTML page, or a PDF report, or an\nemail.\nFor anything that's text-based, Flask integrates a template engine called Jinja (h t t p ://j i n j\na . p o c o o . o r g ). The main reason Flask incorporates Jinja is to produce HTML documents, so\nyou will find helpers like render_template, which generate responses by picking a Jinja\ntemplate, and provide the output given some data.",
      "content_length": 1997,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "Discovering Flask\n[ 53 ]\nBut Jinja is not unique to HTML or other tag-based documents. It can create any document\nas long as it's text-based.\nFor example, if your microservice sends emails, instead of relying on the standard library's\nemail package to produce the email content, which can be cumbersome, you could use\nJinja.\nThe following is an example of an email template:\nDate: {{date}}\nFrom: {{from}}\nSubject: {{subject}}\nTo: {{to}}\nContent-Type: text/plain\nHello {{name}},\nWe have received your payment!\nBelow is the list of items we will deliver for lunch:\n{% for item in items %}- {{item['name']}} ({{item['price']}} Euros)\n{% endfor %}\nThank you for your business!\n--\nTarek's Burger\nJinja uses double brackets for marking variables that will be replaced by a value. Variables\ncan be anything that's passed to Jinja at execution time.\nYou can also use Python's if and for blocks directly in your templates with the {% for x\nin y % }... {% endfor %} and {% if x %}...{% endif %} notations.\nThe following is a Python script that uses the email template to produce an entirely valid\nRFC 822 message, which you can send via SMTP:\n    from datetime import datetime\n    from jinja2 import Template\n    from email.utils import format_datetime\n    def render_email(**data):\n        with open('email_template.eml') as f:\n            template = Template(f.read())\n        return template.render(**data)",
      "content_length": 1399,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "Discovering Flask\n[ 54 ]\n    data = {'date': format_datetime(datetime.now()),\n            'to': 'bob@example.com',\n            'from': 'tarek@ziade.org',\n            'subject': \"Your Tarek's Burger order\",\n            'name': 'Bob',\n            'items': [{'name': 'Cheeseburger', 'price': 4.5},\n                      {'name': 'Fries', 'price': 2.},\n                      {'name': 'Root Beer', 'price': 3.}]}\n    print(render_email(**data))\nThe render_email function uses the Template class to generate the email using the\nprovided data.\nJinja is quite powerful, and comes with many features we won't describe\nhere, since it's out of the chapter's scope. But if you need to do some\ntemplating work in your microservices, it's a good choice, and it's present\nin Flask. Check out http://jinja.pocoo.org/docs for a full\ndocumentation on Jinja features.\nConfiguration\nWhen building applications, you will need to expose options to run them, like the\ninformation to connect to a database or any other variable that is specific to a deployment.\nFlask uses a mechanism similar to Django in its configuration approach. The Flask object\ncomes with an object called config, which contains some built-in variables, and which can\nbe updated when you start your Flask app via your configuration objects.\nFor example, you can define a Config class in a prod_settings.py file as follows:\n    class Config:\n        DEBUG = False\n        SQLURI = 'postgres://tarek:xxx@localhost/db'\nAnd then, load it from your app object using app.config.from_object :\n>>> from flask import Flask\n>>> app = Flask(__name__)\n>>> app.config.from_object('prod_settings.Config')\n>>> print(app.config)\n<Config {'SESSION_COOKIE_HTTPONLY': True, 'LOGGER_NAME': '__main__',\n         'APPLICATION_ROOT': None, 'MAX_CONTENT_LENGTH': None,\n         'PRESERVE_CONTEXT_ON_EXCEPTION': None,\n         'LOGGER_HANDLER_POLICY': 'always',",
      "content_length": 1885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "Discovering Flask\n[ 55 ]\n         'SESSION_COOKIE_DOMAIN': None, 'SECRET_KEY': None,\n         'EXPLAIN_TEMPLATE_LOADING': False,\n         'TRAP_BAD_REQUEST_ERRORS': False,\n         'SESSION_REFRESH_EACH_REQUEST': True,\n         'TEMPLATES_AUTO_RELOAD': None,\n         'JSONIFY_PRETTYPRINT_REGULAR': True,\n         'SESSION_COOKIE_PATH': None,\n         'SQLURI': 'postgres://tarek:xxx@localhost/db',\n         'JSON_SORT_KEYS': True, 'PROPAGATE_EXCEPTIONS': None,\n         'JSON_AS_ASCII': True, 'PREFERRED_URL_SCHEME': 'http',\n         'TESTING': False, 'TRAP_HTTP_EXCEPTIONS': False,\n         'SERVER_NAME': None, 'USE_X_SENDFILE': False,\n         'SESSION_COOKIE_NAME': 'session', 'DEBUG': False,\n         'JSONIFY_MIMETYPE': 'application/json',\n         'PERMANENT_SESSION_LIFETIME': datetime.timedelta(31),\n         'SESSION_COOKIE_SECURE': False,\n         'SEND_FILE_MAX_AGE_DEFAULT': datetime.timedelta(0, 43200)}>\nHowever, there are two significant drawbacks when using Python modules as\nconfiguration files.\nFirst, it can be tempting to add into those configuration modules some code that's more\ncomplex than simple flat classes; and by doing so, it means you will have to treat those\nmodules like the rest of the application code. That's usually not what happens when\napplications are deployed: the configuration files are managed separately from the code.\nSecondly, if another team is in charge of managing the configuration file of your\napplication, they will need to edit the Python code to do so. While this is usually fine, it\nmakes it easier to introduce some problems. For instance, it's harder to make Puppet\ntemplates out of Python modules rather than flat, static configuration files.\nSince Flask exposes its configuration via app.config, it's pretty simple to load additional\noptions from a YAML file, or any other text-based file.\nThe INI format is the most-used format in the Python community, because there's an INI\nparser included in the standard library, and because it's pretty universal.\nMany Flask extensions exist to load the configuration from an INI file, but using the\nstandard library ConfigParser is trivial. Although, there's one major caveat from using\nINI files: variables values are all strings, and your application needs to take care of\nconverting them to the right type.\nThe Konfig project (h t t p s ://g i t h u b . c o m /m o z i l l a - s e r v i c e s /k o n f i g ) is a small layer on\ntop of ConfigParser, which automates the conversion of simple types like integers and\nBooleans.",
      "content_length": 2527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "Discovering Flask\n[ 56 ]\nUsing it with Flask is straightforward:\n$ more settings.ini\n[flask]\nDEBUG = 0\nSQLURI = postgres://tarek:xxx@localhost/db\n$ python\n>>> from konfig import Config\n>>> from flask import Flask\n>>> c = Config('settings.ini')\n>>> app = Flask(__name__)\n>>> app.config.update(c.get_map('flask'))\n>>> app.config['SQLURI']\n'postgres://tarek:xxx@localhost/db\nBlueprints\nWhen you write microservices that have more than a single endpoint, you will end up with\na handful of decorated functions--maybe, a few per endpoint. The first logical step to\norganize your code is to have one module per endpoint, and when you create your app\ninstance, to make sure they get imported so that Flask registers the views.\nFor example, if your microservice manages a company employees database, you could have\none endpoint to interact with all employees, and one with teams. You could organize your\napplication in these three modules:\napp.py: To contain the Flask app object, and to run the app\nemployees.py: To provide all the views related to employees\nteams.py: To provide all the views related to teams\nFrom there, employee and teams can be seen as a subset of the app, and might have a few\nspecific utilities and configuration.\nBlueprints take that logic a step further by providing a way to group your views into\nnamespaces. You can create a Blueprint object which looks like a Flask app object, and\nthen use it to arrange some views. The initialization process can then register blueprints\nwith app.register_blueprint. That call will make sure that all the views defined in the\nblueprint are part of the app.",
      "content_length": 1611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "Discovering Flask\n[ 57 ]\nA possible implementation of the employee's blueprint could be as follows:\n    from flask import Blueprint, jsonify\n    teams = Blueprint('teams', __name__)\n    _DEVS = ['Tarek', 'Bob']\n    _OPS = ['Bill']\n    _TEAMS = {1: _DEVS, 2: _OPS}\n    @teams.route('/teams')\n    def get_all():\n        return jsonify(_TEAMS)\n    @teams.route('/teams/<int:team_id>')\n    def get_team(team_id):\n        return jsonify(_TEAMS[team_id])\nThe main module (app.py) can then import this file, and register its blueprint with\napp.register_blueprint(teams).\nThis mechanism is also interesting when you want to reuse a generic set of views in another\napplication, or several times in the same application.\nThe Flask-Restless (h t t p s ://f l a s k - r e s t l e s s . r e a d t h e d o c s . i o ) extension, for instance,\nwhich is a Create, Read, Update, and Delete (CRUD) tool that automatically exposes a\ndatabase through a REST API by introspecting SQLAlchemy models, generates one\nblueprint per SQLAlchemy model.\nThe following is from the Flask-Restless documentation (Person is SQLAlchemy model):\n    blueprint = manager.create_api_blueprint(Person, methods=['GET',\n    'POST'])\n    app.register_blueprint(blueprint)\nError handling and debugging\nWhen something goes wrong in your application, it's important to be able to control what\nresponses will the clients will receive. In HTML web apps, you usually get specific HTML\npages when you encounter a 404 or a 50x error, and that's how Flask works out of the box.\nBut when building microservices, you need to have more control on what should be sent\nback to the client--that's where custom error handlers come in handy.",
      "content_length": 1681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "Discovering Flask\n[ 58 ]\nThe other important feature is the ability to debug what's wrong with your code when an\nunexpected error occurs. And Flask comes with a built-in debugger we'll discover in this\nsection, which can be activated when your app runs in the debug mode.\nCustom error handler\nWhen your code does not handle an exception, Flask returns an HTTP 500 response without\nproviding any specific information, like the traceback. Producing a generic error is a safe\ndefault behavior to avoid leaking any private information to the users in the error body.\nThe default 500 response is a simple HTML page along with the right status code:\n$ curl http://localhost:5000/api\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your\nrequest.  Either the server is overloaded or there is an error in the\napplication.</p>\nWhen implementing microservices using JSON, it's a good practice to make sure that every\nresponse sent to the clients, including any exception, is JSON formatted. Consumers of your\nmicroservice will expect every response to be machine-parseable.\nFlask lets you customize the app error handling via a couple of functions. The first one is\nthe @app.errorhandler decorator, which works like @app.route. But instead of\nproviding an endpoint, the decorator links a function to a specific error code.\nIn the following example, we use it to connect a function that will return a JSON-formatted\nerror when Flask returns a 500 server response (any code exception):\n    from flask import Flask, jsonify\n    app = Flask(__name__)\n    @app.errorhandler(500)\n    def error_handling(error):\n        return jsonify({'Error': str(error)}, 500)\n    @app.route('/api')\n    def my_microservice():\n        raise TypeError(\"Some Exception\")\n    if __name__ == '__main__':\n        app.run()",
      "content_length": 1936,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "Discovering Flask\n[ 59 ]\nFlask will call this error view no matter what exception the code raises.\nHowever, in case your application issues an HTTP 404 or any other 4xx or 5xx response,\nyou will be back to the default HTML responses that Flask sends.\nTo make sure your app sends JSON for every 4xx and 50x, we need to register that function\nto each error code.\nOne place where you can find the list of errors is in the abort.mapping dict. In the\nfollowing code snippet, we register the error_handling function to every error using\napp.register_error_handler, which is similar to the @app.errorhandler decorator:\n    from flask import Flask, jsonify, abort\n    from werkzeug.exceptions import HTTPException, default_exceptions\n    def JsonApp(app):\n        def error_handling(error):\n            if isinstance(error, HTTPException):\n                result = {'code': error.code, 'description':\n                           error.description, 'message': str(error)}\n            else:\n                description = abort.mapping[500].description\n                result = {'code': 500, 'description': description,\n                          'message': str(error)}\n            resp = jsonify(result)\n            resp.status_code = result['code']\n            return resp\n        for code in default_exceptions.keys():\n            app.register_error_handler(code, error_handling)\n        return app\n    app = JsonApp(Flask(__name__))\n    @app.route('/api')\n    def my_microservice():\n        raise TypeError(\"Some Exception\")\n    if __name__ == '__main__':\n        app.run()\nThe JsonApp function wraps a Flask app instance, and sets up the custom JSON error\nhandler for every 4xx and 50x error that might occur.",
      "content_length": 1701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "Discovering Flask\n[ 60 ]\nThe debug mode\nThe Flask application run method has a debug option, which, when used, runs it in the\ndebug mode:\n    app.run(debug=True)\nThe debug mode is a special mode, where the built-in debugger takes precedence on any\nerror, and allows you to interact with the app from a browser:",
      "content_length": 310,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "Discovering Flask\n[ 61 ]\nThe console in the web-debugger will let you interact with the current app, and inspect\nvariables or execute any Python code that is in the current execution frame.\nFlask will even let you configure a third-party debugger. JetBrains's PyCharm (h t t p s ://w w\nw . j e t b r a i n s . c o m /p y c h a r m ), for example, is a commercial IDE for Python, which offers a\npowerful visual debugger that can be set up to run with Flask.\nSince the debug mode allows remote code execution, it's a security hazard\neven though you need to provide a PIN to access the console. In 2015, the\nPatreon online service got hacked via the Flask debugger. You need to be\nextremely cautious not to run the debug mode in production. The Bandit\nsecurity linter (h t t p s ://w i k i . o p e n s t a c k . o r g /w i k i /S e c u r i t y /P r o j e c t s\n/B a n d i t ) tracks Flask applications that are executed with a plain debug\nflag, and can be used to prevent deploying an application with that flag.\nThe plain old pdb module is also a good option when you are tracking down a problem by\ninserting a pdb.set_trace() call in your code.\nA microservice skeleton\nSo far in this chapter, we've looked at how Flask works, and at most of the built-in features\nit provides--and we will be using them throughout this book.\nOne topic we have not covered yet is how to organize the code in your projects, and how to\ninstantiate your Flask app. Every example so far used a single Python module and the\napp.run() call to run the service.\nHaving everything in a module is, of course, a terrible idea unless your code is just a few\nlines. And since we will want to release and deploy the code, it's better to have it inside a\nPython package so that we can use standard packaging tools like Pip and Setuptools.\nIt's also a good idea to organize views into blueprints, and have one module per blueprint.\nLastly, the run() call can be removed from the code, since Flask provides a generic runner\nthat looks for an app variable given a module pointed by the FLASK_APP environment\nvariable. Using that runner offers extra options like the ability to configure the host and port\nthat will be used to run the app.",
      "content_length": 2200,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "Discovering Flask\n[ 62 ]\nThe microservice project on GitHub (h t t p s ://g i t h u b . c o m /R u n n e r l y /m i c r o s e r v i c e ) was\ncreated for this book, and is a generic Flask project that you can use to start a microservice.\nIt implements a simple layout, which works well for building microservices.\nYou can install it and run it, then modify it.\nThis project uses Flakon (h t t p s ://g i t h u b . c o m /R u n n e r l y /f l a k o n ), which\nis a minimalistic helper that takes care of configuring and instantiating a\nFlask application with an INI file and a default JSON behavior.\nFlakon was also created for this book to let you focus on building\nmicroservices with the minimal amount of boilerplate code.\nFlakon is opinionated, so if its decisions do not suit you, you can just\nremove it from your project, and build your function that creates an app;\nor use one of the existing open source projects that provide this kind of\nfeature.\nThe microservice project skeleton contains the following structure:\nsetup.py: Distutils' setup file, which is used to install and release the project\nMakefile: A Makefile that contains a few useful targets to make, build, and run\nthe project\nsettings.ini: The application default settings in the INI file\nrequirements.txt: The project dependencies following the pip format\nmyservices/: The actual package\n__init__.py\napp.py: The app module, which contains the app itself\nviews/: A directory containing the views organized in blueprints\n__init__.py\nhome.py: The home blueprint, which serves the root\nendpoint\ntests: The directory containing all the tests\n__init__.py\ntest_home.py: Tests for the home blueprint views",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "Discovering Flask\n[ 63 ]\nIn the following code, the app.py file instantiates a Flask app using Flakon's create_app\nhelper; that takes a few options like a list of blueprints, which get registered:\n    import os\n    from flakon import create_app\n    from myservice.views import blueprints\n    _HERE = os.path.dirname(__file__)\n    _SETTINGS = os.path.join(_HERE, '..', 'settings.ini')\n    app = create_app(blueprints=blueprints, settings=_SETTINGS)\nThe home.py view uses Flakon's JsonBlueprint class, which implements the error\nhandling we've seen in the previous section. It also automatically calls jsonify() on the\nobject returned by the view if its a dictionary, like how the Bottle framework does:\n    from flakon import JsonBlueprint\n    home = JsonBlueprint('home', __name__)\n    @home.route('/')\n    def index():\n        \"\"\"Home view.\n        This view will return an empty JSON mapping.\n        \"\"\"\n        return {}\nThis example application can run via Flask's built-in command line, using the package\nname:\n$ FLASK_APP=myservice flask run\n * Serving Flask app \"myservice\"\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\nFrom there, building JSON views for your microservice consists of adding modules in\nmicroservice/views, and their corresponding tests.",
      "content_length": 1276,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "Discovering Flask\n[ 64 ]\nSummary\nThis chapter gave us a pretty detailed overview of the Flask framework, and how it can be\nused to build microservices.\nThe main takeaways are as follows:\nFlask wraps a simple request-response mechanism around the WSGI protocol,\nwhich lets you write your applications in almost vanilla Python.\nFlask is easy to extend, and it works with Python 3.\nFlask comes with nice built-in features: blueprints, globals, signals, a template\nengine, error handlers, and a debugger.\nThe microservice project is a Flask skeleton, which will be used to write\nmicroservices throughout this book. It's a simple app that uses an INI file for its\nconfiguration, and makes sure everything produced by the app is JSON.\nThe next chapter will focus on development methodology: how to continuously code, test,\nand, document your microservices.",
      "content_length": 850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "3\nCoding, Testing, and\nDocumenting - the Virtuous\nCycle\nEvery software project that's deployed suffers from bugs that are inevitable--and bugs are\ntime and money consuming.\nUsing a Test-Driven Development (TDD) approach, where you write tests alongside the\ncode you are creating, will not always improve the quality of your project, but it will make\nyour team more agile. This means that the developers who need to fix a bug, or refactor a\npart of an application, will be able to do a faster and better job when relying on a battery of\ntests. If they break a feature, the tests should warn them about it.",
      "content_length": 604,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 66 ]\nWriting tests is time-consuming at first, but in the long run, it's often the best approach to\nmake a project grow. Of course, it's always possible to write bad tests and end up with poor\nresults, or create a test suite that's horrible to maintain and takes too long to run. The best\ntools and processes in the world won't prevent a sloppy developer from producing bad\nsoftware:\nOriginal image credits: h t t p s ://x k c d . c o m /303/\nSoftware industry has long debated on the virtues of TDD. But in the last decade, most of\nthe research papers that tried to measure the benefits of TDD concluded that software built\nwith it costs less money in the long term, and is as good, or better, in terms of quality. This\npage links to a few research papers on this topic at h t t p ://b i b l i o . g d i n w i d d i e . c o m /b i b l i\no /S t u d i e s O f T e s t D r i v e n D e v e l o p m e n t .\nWriting tests is also a good way to get some perspective on your code. Does the API you've\ndesigned make sense? Do things fit well together? And when the team grows or changes,\ntests are the best source of information. Unlike documentation, they should reflect what the\ncurrent version of the code does.\nBut documentation is still an important part of a project even though it's hard and time-\nconsuming to maintain. It's the first stop for anyone using your software or joining the team\nto work on it. How is the application installed and configured? How to run tests or add\nfeatures? How is it designed the way it is, and why?",
      "content_length": 1587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 67 ]\nAfter a while, it's pretty rare to see a project's documentation fully up-to-date with what the\ncode has become unless some dedicated people work on it. And it can be an immense\nfrustration for developers to find out that the code examples in the documentation are\nbroken after some refactoring. But there are ways to mitigate these issues; for instance, code\nextracts in the documentation could be part of the test suite to make sure they work.\nIn any case, no matter how much energy you spend on tests and documentation, there's one\ngolden rule: testing, documenting, and coding your projects should be done continuously. In other\nwords, changes in the code should ideally be reflected in the tests and documentation as\nthey happen.\nAfter providing a few general tips on how to test in Python, this chapter focuses on what\ntesting and documentation tools can be used in the context of building microservices with\nFlask, and how to set up continuous integration with some popular online services.\nIt's organized into five parts:\nThe different kind of tests\nUsing WebTest against your microservice\nUsing pytest and Tox\nDeveloper documentation\nContinuous integration\nDifferent kinds of tests\nThere are many different kinds of tests, and it can be confusing sometimes to know what\nwe're talking about. For instance, when people refer to functional tests, they may refer to\ndifferent kinds of tests depending on the project's nature.\nIn the microservice land, we can classify tests into these five distinct goals:\nUnit tests: Make sure a class or a function works as expected in isolation\nFunctional tests: Verify that the microservice does what it says from the\nconsumer's point of view, and behaves correctly even on bad requests\nIntegration tests: Verify how a microservice integrates with all its network\ndependencies\nLoad tests: Measure the microservice performances\nEnd-to-end tests: Verify that the whole system works with an end-to-end test\nWe will see these in detail in the following sections.",
      "content_length": 2061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 68 ]\nUnit tests\nUnit tests are the simplest tests to add to a project, and the standard library comes with\neverything needed to write some. In a project based on Flask, there usually are, alongside\nthe views, some functions and classes, which can be unit-tested in isolation.\nHowever, the concept of separation is quite vague for a Python project, because we don't use\ncontracts or interfaces like in other languages, where the implementation of the class is\nseparated from its definition.\nTesting in isolation in Python usually means that you instantiate a class or call a function\nwith specific arguments, and verify that you get the expected result. When the class or\nfunction calls another piece of code that's not built in Python or its standard library, it's not\nin isolation anymore.\nIn some cases, it will be useful to mock those calls to achieve isolation. Mocking means \nreplacing a piece of code with a mock version, which takes specified input, yields specified\noutputs, and fakes the behavior in between. But mocking is often a dangerous exercise,\nbecause it's easy to implement a different behavior in your mocks and end up with some\ncode that works with your mocks but not the real thing. That problem often occurs when\nyou update your project's dependencies, and your mocks are not updated to reflect the new\nbehaviors, which might have been introduced in some library.\nSo, limiting the usage of mocks to the three following use cases is good practice:\nI/O operations: When the code performs calls to third-party services or a\nresource (socket, files, and so on), and you can't run them from within your tests\nCPU intensive operations: When the call computes something that would make\nthe test suite too slow\nSpecific behaviors to reproduce: When you want to write a test to try out your\ncode under specific behaviors (for example, a network error or changing the date\nor time by mocking the date time and time modules)\nConsider the following class, which can be used to query a bug list via the Bugzilla REST\nAPI, using the requests (h t t p ://d o c s . p y t h o n - r e q u e s t s . o r g ) library:\n    import requests\n    class MyBugzilla:\n        def __init__(self, account, server =\n                     'https://bugzilla.mozilla.org'):\n            self.account = account\n            self.server = server\n            self.session = requests.Session()",
      "content_length": 2431,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 69 ]\n        def bug_link(self, bug_id):\n            return '%s/show_bug.cgi?id=%s' % (self.server, bug_id)\n        def get_new_bugs(self):\n            call = self.server + '/rest/bug'\n            params = {'assigned_to': self.account,\n                      'status': 'NEW',\n                      'limit': 10}\n            try:\n                res = self.session.get(call, params=params).json()\n            except requests.exceptions.ConnectionError:\n                # oh well\n                res = {'bugs': []}\n            def _add_link(bug):\n                bug['link'] = self.bug_link(bug)\n                return bug\n            for bug in res['bugs']:\n                yield _add_link(bug)\nThis class has a bug_link() method, which we can test in isolation, and one\nget_new_bugs() method, which performs calls to the Bugzilla server. It would be too\ncomplicated to run our Bugzilla server when the test is executed, so we can mock the calls\nand provide our JSON values for the class to work in isolation.\nThis technique is used in the following example with request_mock (h t t p ://r e q u e s t s - m o\nc k . r e a d t h e d o c s . i o ), which is a handy library to mock request network calls:\n    import unittest\n    from unittest import mock\n    import requests\n    from requests.exceptions import ConnectionError\n    import requests_mock\n    from bugzilla import MyBugzilla\n    class TestBugzilla(unittest.TestCase):\n        def test_bug_id(self):\n            zilla = MyBugzilla('tarek@mozilla.com', server =\n                               ='http://example.com')\n            link = zilla.bug_link(23)\n            self.assertEqual(link, 'http://example.com/show_bug.cgi?id=23')\n        @requests_mock.mock()\n        def test_get_new_bugs(self, mocker):\n            # mocking the requests call and send back two bugs",
      "content_length": 1879,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 70 ]\n            bugs = [{'id': 1184528}, {'id': 1184524}]\n            mocker.get(requests_mock.ANY, json={'bugs': bugs})\n            zilla = MyBugzilla('tarek@mozilla.com',\n                               server ='http://example.com')\n            bugs = list(zilla.get_new_bugs())\n            self.assertEqual(bugs[0]['link'],\n                             'http://example.com/show_bug.cgi?id=1184528')\n        @mock.patch.object(requests.Session, 'get',\n                           side_effect=ConnectionError('No network'))\n        def test_network_error(self, mocked):\n            # faking a connection error in request if the web is down\n            zilla = MyBugzilla('tarek@mozilla.com',\n                                server='http://example.com')\n            bugs = list(zilla.get_new_bugs())\n            self.assertEqual(len(bugs), 0)\n    if __name__ == '__main__':\n        unittest.main()\nYou should keep an eye on all your mocks as the project grows, and make\nsure they are not the only kind of tests that cover a particular feature. For\ninstance, if the Bugzilla project comes up with a new structure for its REST\nAPI, and the server your project uses is updated, your tests will happily\npass with your broken code until the mocks reflect the new behavior.\nThe test_network_error() method is a second test which fakes a network error by\ntriggering requests' connection error, using Python's mock patch decorator. This test ensures\nthe class behaves as expected when there's no network.\nThis kind of unit test is usually enough to cover most of your classes' and functions'\nbehaviors.\nThis test class will probably cover more cases as the project grows and new situations occur.\nFor instance, what happens if the server sends back a malformed JSON body?",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 71 ]\nBut there's no need to have tests for all the failures you can come up with on day one. In a\nmicroservice project, unit tests are not a priority, and aiming at 100% test coverage (where\nevery line of your code is called somewhere in your tests) in your unit tests will add a lot of \nmaintenance work for little benefits.\nIt's better to focus on building a robust set of functional tests.\nFunctional tests\nFunctional tests for a microservice project are all the tests that interact with the published\nAPI by sending HTTP requests and asserting the HTTP responses.\nThis definition is broad enough to include any test that can call the app, from fuzzing tests\n(you send gibberish to your app and see what happens) to penetration tests (you try to break\nthe app security), and so on.\nAs developers, the two most important kinds of functional tests we should focus on are\nthese:\nTests that verify that the application does what it was built for\nTests that ensure an abnormal behavior that was fixed is not happening anymore\nThe way those scenarios are organized in the tests class is up to the developers, but the\ngeneral pattern is to create an instance of the application in the test class and then interact\nwith it.\nIn that context, the network layer is not used, and the application is called directly by the\ntests, but the same request-response cycle happens, so it's realistic enough. However, we\nwould still mock out any network calls happening within the application.\nFlask includes a FlaskClient class to build requests, which can be instantiated directly\nfrom the app object via its test_client() method.\nThe following is an example of a test against the first app we showed in this chapter, which\nsends back a JSON body on /api/:\n    import unittest\n    import json\n    from flask_basic import app as tested_app\n    class TestApp(unittest.TestCase):\n        def test_help(self):\n            # creating a FlaskClient instance to interact with the app",
      "content_length": 2016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 72 ]\n            app = tested_app.test_client()\n            # calling /api/ endpoint\n            hello = app.get('/api')\n            # asserting the body\n            body = json.loads(str(hello.data, 'utf8'))\n            self.assertEqual(body['Hello'], 'World!')\n    if __name__ == '__main__':\n        unittest.main()\nThe FlaskClient class has one method per HTTP verb, and sends back Response objects\nthat can be used to assert the results. In the preceding example, we used .get().\nThere's a testing flag in the Flask class, which you can use to propagate exceptions to the\ntest, but some prefer not to use it by default to get back from the app what a real client\nwould get--for instance, to make sure the body of 5xx or 4xx errors are converted to JSON\nfor API consistency.\nIn the following example, the /api/ call produces an exception, and we're making sure the\nclient gets a proper 500 with a structured JSON body in test_raise().\nThe test_proper_404() test method does the same tests on a non-existent path:\n    import unittest\n    import json\n    from flask_error import app as tested_app\n    _404 = ('The requested URL was not found on the server.  '\n            'If you entered the URL manually please check your '\n            'spelling and try again.')\n    class TestApp(unittest.TestCase):\n        def setUp(self):\n            # creating a client to interact with the app\n            self.app = tested_app.test_client()\n        def test_raise(self):\n            # this won't raise a Python exception but return a 500\n            hello = self.app.get('/api')\n            body = json.loads(str(hello.data, 'utf8'))\n            self.assertEqual(body['code'], 500)\n        def test_proper_404(self):\n            # calling a non existing endpoint\n            hello = self.app.get('/dwdwqqwdwqd')",
      "content_length": 1859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 73 ]\n            # yeah it's not there\n            self.assertEqual(hello.status_code, 404)\n            # but we still get a nice JSON body\n            body = json.loads(str(hello.data, 'utf8'))\n            self.assertEqual(body['code'], 404)\n            self.assertEqual(body['message'], '404: Not Found')\n            self.assertEqual(body['description'], _404)\n    if __name__ == '__main__':\n        unittest.main()\nAn alternative to the FlaskClient method is WebTest (h t t p ://w e b t e s t .\np y t h o n p a s t e . o r g ), which offers a few more features out of the box. It's\ncovered later in this chapter.\nIntegration tests\nUnit tests and functional tests focus on testing your service code without calling other\nnetwork resources, whether they are other microservices from your application or third-\nparty services like databases, queues, and so on. For the sake of speed, isolation, and\nsimplicity, network calls are mocked.\nIntegration tests are functional tests without any mocking, and should be able to run on a\nreal deployment of your application. For example, if your service interacts with Redis and\nRabbitMQ, they will be called by your service as normal when the integration tests are run.\nThe benefit is to avoid falling into the problems that were described earlier when mocking\nnetwork interactions. You will be sure that your application works in a production\nexecution context only if you try it for real.\nThe caveat is that running tests against an actual deployment makes it harder to set up tests\ndata, or to clean up whatever data was produced from within the service during the test.\nPatching the application behavior to reproduce a problem is also a difficult task.\nBut along with unit and functional tests, integration tests are an excellent complement to\nverify your application behavior.\nTypically, integration tests are executed on a development or staging deployment of your\nservice, but if it's easy to do, you can also have a dedicated testing deployment, which will\nbe used for that sole purpose.",
      "content_length": 2092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 74 ]\nYou can use whatever tool you want to write your integration test. A curl script, for\ninstance, might sometimes be enough on some microservices.\nBut it's nicer if integration tests can be written in Python, and can be part of your project's\ntests collection. To do this, a Python script that uses requests to call your microservice can\ndo the trick. Or better, in case you provide a client library for your microservice, it's a good\nway to test it.\nWhat differentiates integration tests from functional tests is mostly the fact\nthat it's a real server that gets called. What if we could write functional\ntests that can either be run on a local Flask application or against an actual\ndeployment? This is possible with WebTest, as we will find out later in\nthis chapter.\nLoad tests\nThe goal of a load test is to understand your service's bottlenecks under stress and to plan\nfor the future, not to do premature optimizations.\nMaybe the first version of your service will be fast enough for what it will be used for, but\nunderstanding its limits will help you determining how you want to deploy it and if its\ndesign is future-proof in case the load increases.\nIt's a common mistake to spend a lot of time on making each microservice as fast as\npossible, and end up with an application that relies on a single point of failure because your\ndesign makes it hard to deploy several instances of one particular microservice.\nWriting load tests can help you answer the following questions:\nHow many users can one instance of my service serve when I deploy it on this\nmachine?\nWhat's the average response time when there are 10, 100 or 1,000 concurrent\nrequests? Can I handle that much concurrency?\nWhen my service is under stress, is it running out of RAM or is it mainly CPU-\nbound?\nCan I add other instances of the same service and scale horizontally?\nIf my microservice calls other services, can I use pools of connectors, or do I have\nto serialize all the interactions through a single connection?\nCan my service run for multiple days at a time without degradation?\nIs my service working properly after a usage peak?",
      "content_length": 2172,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 75 ]\nDepending on the kind of load you want to achieve, there are many tools available, from\nsimple command-line tools to heavier distributed load systems.\nFor performing a simple load test that does not require any particular scenario, Boom (h t t p\ns ://g i t h u b . c o m /t a r e k z i a d e /b o o m ) is an Apache Bench (AB) equivalent written in\nPython, which can be used to hit your endpoints.\nIn the following example, Boom performs a 10-second load test against a Flask web server\non the /api/ endpoint, using 100 concurrent users--and ends up with 286 requests per\nsecond (RPS):\n$ boom http://127.0.0.1:5000/api -c 100 -d 10 -q\n-------- Results --------\nSuccessful calls           2866\nTotal time                 10.0177 s\nAverage                    0.3327 s\nFastest                    0.2038 s\nSlowest                    0.4782 s\nAmplitude                  0.2744 s\nStandard deviation         0.035476\nRPS                        286\nBSI                        Pretty good\n-------- Status codes --------\nCode 200                   2866 times.\n-------- Legend --------\nRPS: Request Per Second\nBSI: Boom Speed Index\nThese numbers don't mean much, as they will vary a lot depending on the deployment, and\nfrom where you run them. For instance, if your Flask application is served behind nginx\nwith several workers, it will handle better the stream of incoming connections.\nBut this small test alone can often catch problems early on, in particular when your code is\nopening socket connections itself. If something's wrong in the microservice design, it's quite\neasy to detect it with a tool like Boom.",
      "content_length": 1666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 76 ]\nIf you need to write interactive scenarios, another small command-line tool is Molotov (h t t\np s ://g i t h u b . c o m /t a r e k z i a d e /m o l o t o v ), which gives you the ability to write Python \nfunctions where you can query a microservice and check the responses.\nIn the following example, each function is a possible scenario that gets picked by Molotov\nto run against the server:\n    import json\n    from molotov import scenario\n    @scenario(5)\n    async def scenario_one(session):\n        res = await session.get('http://localhost:5000/api').json()\n        assert res['Hello'] == 'World!'\n    @scenario(30)\n    async def scenario_two(session):\n        somedata = json.dumps({'OK': 1})\n        res = await session.post('http://localhost:5000/api',\n                                 data=somedata)\n        assert res.status_code == 200\nBoth tools will give you some metrics, but they are not very accurate because of the network\nand client CPU variance on the box they are launched from. For instance, the test itself will\nstress the machine resources, and that will impact the metrics.\nWhen performing a load test, it's better to add some metrics on the server side. At the Flask\nlevel, you can use a small tool like flask-profiler (h t t p s ://g i t h u b . c o m /m u a t i k /f l a s k - p r o\nf i l e r ), which collects how long each request takes, and offers a dashboard that will let you \nbrowse the collected times--its overhead is minimal.\nYou can also send detail metrics via StatsD (h t t p s ://g i t h u b . c o m /e t s y /s\nt a t s d ), and use a dedicated dashboard application like Graphite (h t t p\n://g r a p h i t e . r e a d t h e d o c s . i o ). Metrics are covered in Chapter 6,\nMonitoring Your Services.",
      "content_length": 1803,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 77 ]\nIf you need to do a heavier load test, you will need to use a load testing framework, which\ncan distribute the tests across several agents. One of the possible tools is locust.io (h t t p ://d\no c s . l o c u s t . i o /).\nEnd-to-end tests\nAn end-to-end test will check that the whole system works as expected from the end-user\npoint of view. The test needs to behave like a real client, and call the system through the\nsame User Interface (UI).",
      "content_length": 506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 78 ]\nDepending on the type of application you are writing, a simple HTTP client might not be\nenough to simulate a real user. For instance, if the visible part of the system through which\nusers are interacting is a web application with HTML pages that gets rendered on a client-\nside, you will need to use a tool like Selenium (h t t p ://d o c s . s e l e n i u m h q . o r g /). It will\nautomate your browser in order to make sure that the client requests every CSS and\nJavaScript files and then renders correctly every page.\nJavaScript frameworks are now doing a lot of work on the client side to produce pages.\nSome of them have completely removed server-side rendering of templates, and just grab\ndata from the server to generate the HTML page by manipulating the Document Object\nModel (DOM) through the browser APIs. Calls to the server, in that case, consist of getting\nall the static JavaScript files needed for rendering a given URL, plus the data.\nWriting end-to-end tests is outside the scope of this book, but you can\nrefer to Selenium Testing Tools Cookbook from the same editor to learn how\nto write some.\nThe following points summarize what we've learned in this section:\nFunctional tests are the most important tests to write, and it's easy to do it in\nFlask by instantiating the app in the tests and interacting with it\nUnit tests are a good complement, but don't abuse mocks\nIntegration tests are like functional tests, but against a real deployment\nLoad tests are useful to learn about your microservice bottlenecks and plan for\nthe next steps\nEnd-to-end tests require using the same UI that the client would normally use\nKnowing when you will need to write integration, load, or end-to-end tests depends on\nhow your project is managed--but both unit and functional tests should be written every\ntime you change something. Ideally, each change you make in your code should include a\nnew test or modify an existing test.\nUnit tests can be written using vanilla Python, thanks to the excellent unittest package\nincluded in the standard library--and we will see later how the pytest (h t t p ://d o c s . p y t e s\nt . o r g ) library adds awesomeness on the top of it.\nFor functional tests, we'll look in the next section at WebTest.",
      "content_length": 2305,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 79 ]\nUsing WebTest\nWebTest (h t t p ://w e b t e s t . r e a d t h e d o c s . i o ) has been around for a long time. It was \nwritten by Ian Bicking back in the days of the Paste project, and is based on the WebOb (h t\nt p ://d o c s . w e b o b . o r g ) project, which provides a Request and Response class similar (but\nnot compatible) to Flask's.\nWebTest wraps call to a WSGI application like FlaskTest does, and lets you interact with\nit. WebTest is somewhat similar to FlaskTest, with a few extra helpers when dealing with\nJSON, and a neat feature to call non-WSGI applications.\nTo use it with Flask, you can install the flask-webtest package (h t t p s ://f l a s k - w e b t e s t\n. r e a d t h e d o c s . i o /), and you will get a similar integration level as Flask's native tool:\n    import unittest\n    from flask_basic import app as tested_app\n    from flask_webtest import TestApp\n    class TestMyApp(unittest.TestCase):\n        def test_help(self):\n            # creating a client to interact with the app\n            app = TestApp(tested_app)\n            # calling /api/ endpoint\n            hello = app.get('/api')\n            # asserting the body\n            self.assertEqual(hello.json['Hello'], 'World!')\n    if __name__ == '__main__':\n        unittest.main()\nWe've said earlier that integration tests were similar to functional tests except that they\ncalled a real server instead of instantiating a local WSGI app.\nWebTest leverages the WSGIProxy2 library (h t t p s ://p y p i . p y t h o n . o r g /p y p i /W S G I P r o x y\n2), which converts calls that are made to the Python application to HTTP requests made to a\nreal HTTP application.\nThe previous script can be slightly modified to become an integration test if you set an\nHTTP_SERVER variable in the environ function, as follows:\n    import unittest\n    import os\n    class TestMyApp(unittest.TestCase):",
      "content_length": 1940,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 80 ]\n        def setUp(self):\n            # if HTPP_SERVER is set, we use it as an endpoint\n            http_server = os.environ.get('HTTP_SERVER')\n            if http_server is not None:\n                from webtest import TestApp\n                self.app = TestApp(http_server)\n            else:\n                # fallbacks to the wsgi app\n                from flask_basic import app\n                from flask_webtest import TestApp\n                self.app = TestApp(app)\n        def test_help(self):\n            # calling /api/ endpoint\n            hello = self.app.get('/api')\n            # asserting the body\n            self.assertEqual(hello.json['Hello'], 'World!')\n    if __name__ == '__main__':\n        unittest.main()\nWhen this last test is executed with HTTP_SERVER=http://myservice/, it performs all its\ncalls to that service.\nThat trick is pretty handy to turn some of your functional tests into integration tests without\nhaving to write two distinct tests. As we said earlier, it has some limitations, since you can't\ninteract locally with the application instance. But it's extremely useful to validate that a\ndeployed service works as expected directly from your test suite, just by flipping an option.\nUsing pytest and Tox\nSo far, all the tests we have written used unittest.TestCase classes and\nunittest.main() to run them. As your project grows, you will have more and more tests\nmodules around.\nTo automatically discover and run all the tests in a project, the unittest package has\nintroduced a Test Discovery feature in Python 3.2, which finds and runs tests given a few\noptions. This feature has been around for a while in projects like Nose (h t t p s ://n o s e . r e a d\nt h e d o c s . i o ) and pytest, and that's what inspired the unittest package in the standard\nlibrary.",
      "content_length": 1859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 81 ]\nWhich runner to use is a matter of taste, and as long as you stick to writing your tests in\nTestCase classes, your tests will be compatible with all of them.\nThat said, the pytest project is very popular in the Python community, and since it offers\nextensions, people have started to write useful tools around it. Its runner is also quite\nefficient, as it starts to run the tests while they are still discovered in the background,\nmaking it a little faster than the others. Its output in the console is also beautiful and bright.\nTo use it in your project, you can simply install the pytest package with pip, and use the\nprovided pytest command line. In the following example, the pytest command runs all\nthe modules that start with test_:\n$ pytest test_*\n============= test session starts ================================\nplatform darwin -- Python 3.5.2, pytest-3.0.5, py-1.4.32, pluggy-0.4.0\nrootdir: /Users/tarek/Dev/github.com/microbook/code, inifile:\ncollected 7 items\ntest_app.py .\ntest_app_webtest.py .\ntest_bugzilla.py ...\ntest_error.py ..\n============= 7 passed in 0.55 seconds =============================\nThe pytest package comes with a lot of extensions, which are listed at h t t p ://p l u g i n c o m p\na t . h e r o k u a p p . c o m /.\nTwo useful extensions are pytest-cov and pytest-flake8. The first one uses the coverage tool\n(h t t p s ://c o v e r a g e . r e a d t h e d o c s . i o ) to display the test coverage of your project, and the\nsecond one runs the Flake8 (h t t p s ://g i t l a b . c o m /p y c q a /f l a k e 8) linter to make sure that\nyour code is following the PEP8 style, and has no unused imports.\nHere's an invocation example with some style issues left on purpose:\n$ pytest --cov=flask_basic --flake8 test_*\n============= test session starts ================================\nplatform darwin -- Python 3.5.2, pytest-3.0.5, py-1.4.32, pluggy-0.4.0\nrootdir: /Users/tarek/Dev/github.com/microbook/code, inifile:\nplugins: flake8-0.8.1, cov-2.4.0\ncollected 11 items\ntest_app.py F.\ntest_app_webtest.py F.\ntest_bugzilla.py F...",
      "content_length": 2124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 82 ]\n---------- coverage: platform darwin, python 3.5.2-final-0 -----------\nName             Stmts   Miss  Cover\n------------------------------------\nflask_basic.py       6      1    83%\n============= FAILURES =====================================\n______________ FLAKE8-check ___________________________________\ntest_app.py:18:1: E305 expected 2 blank lines after class or function\ndefinition, found 1\ntest_app.py:21:1: W391 blank line at end of file\n______________ FLAKE8-check ___________________________________\ntest_app_webtest.py:29:1: W391 blank line at end of file\n______________ FLAKE8-check ___________________________________\ntest_bugzilla.py:26:80: E501 line too long (80 > 79 characters)\ntest_bugzilla.py:28:80: E501 line too long (82 > 79 characters)\ntest_bugzilla.py:40:1: W391 blank line at end of file\n============= 3 failed, 7 passed, 0 skipped in 2.19 seconds =============\nAnother useful tool that can be used in conjunction with pytest is Tox (h t t p ://t o x . r e a d t h\ne d o c s . i o ).\nIf your projects need to run on several version of Python, or if you only want to make sure\nthat your code can work on the latest Python 2 and Python 3 versions, Tox can automate the\ncreation of separate environments to run your tests.\nTelling Tox to run your project on Python 2.7 and Python 3.5 is done by installing Tox\n(using the pip installs tox command), and then creating a tox.ini configuration file\nin the root of your project. Tox makes the assumption that your project is a Python package,\nand therefore, has a setup.py file in the root directory alongside the tox.ini file, but\nthat's the only requirement.\nThe tox.ini file contains the command lines to run the tests along with the Python\nversions it should be run against:\n    [tox]\n    envlist = py27,py35\n    [testenv]\n    deps = pytest\n       pytest-cov\n       pytest-flake8\n    commands =  pytest --cov=flask_basic --flake8 test_*",
      "content_length": 1968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 83 ]\nWhen Tox is executed by calling the tox command, it will create a separate environment for\neach Python version, deploy your package and its dependencies in it, and run the tests in it\nusing the pytest command.\nYou can run a single environment with tox -e, which is very handy when you want to run\nthe tests quickly. For instance, tox -e py35 will just run pytest under Python 3.5.\nEven if you support a single Python version, using Tox will ensure that your project can be\ninstalled in a current Python environment, and that you've correctly described all the\ndependencies.\nUsing this tool is highly recommended.\nChapter 9, Packaging Runnerly, covers in detail how to package\nmicroservices, and will use Tox to do so among other instruments.\nDeveloper documentation\nSo far, we've looked at the different kinds of tests a microservice can have, and we've\nmentioned that the documentation should evolve with the code.\nWe're talking here about developer documentation. This includes everything a developer\nshould know about your microservices project, things such as:\nHow it's designed\nHow to install it\nHow to run the tests\nWhat are the exposed APIs and what data comes in and out, and so on\nThe Sphinx tool (h t t p ://w w w . s p h i n x - d o c . o r g /), which was developed by Georg Brandl to\ndocument Python itself, became the standard in the Python community.\nSphinx treats documents like source code by separating the content from the layout. The\nusual way to use Sphinx is to have a docs directory in the project that contains the\ndocumentation content, and then call Sphinx's command-line utility to generate the\ndocumentation using an output format like HTML.",
      "content_length": 1730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 84 ]\nProducing an HTML output with Sphinx makes an excellent static website, which can be\npublished on the web, as the tool adds index pages, a small JavaScript-based search engine,\nand navigation features.\nThe content of the documentation must be written in reStructuredText (reST) (h t t p ://d o c u\nt i l s . s o u r c e f o r g e . n e t /r s t . h t m l ), which is the standard markup language in the Python\ncommunity. A reST file is a simple text file with a non-intrusive syntax to mark section\nheaders, links, text styles, and so on. Sphinx adds a few extensions and summarizes reST\nusage in this document, which should be your go-to page for learning how to write docs ( h\nt t p ://w w w . s p h i n x - d o c . o r g /e n /l a t e s t /r e s t . h t m l ).\nMarkdown (h t t p s ://d a r i n g f i r e b a l l . n e t /p r o j e c t s /m a r k d o w n /) is\nanother popular markup language, which is used in the open source\ncommunity. Unfortunately, Sphinx relies on some reST extensions, and\nhas limited support to Markdown via the recommonmark package. The\ngood news is that if you are familiar with Markdown, reST is not that\ndifferent.\nWhen you start a project with Sphinx using sphinx-quickstart, it generates a source tree\nwith an index.rst file, which is the landing page of your documentation. From there,\ncalling sphinx-build on it will create your documentation.\nFor example, if you want to generate an HTML documentation, you can add a docs\nenvironment in your tox.ini file, and let the tool build the documentation for you, as\nfollows:\n    [tox]\n    envlist = py35,docs\n    ...\n    [testenv:docs]\n    basepython=python\n    deps =\n        -rrequirements.txt\n        sphinx\n    commands=\n        sphinx-build -W -b html docs/source docs/build\nRunning tox -e docs will generate your documentation.",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 85 ]\nShowing code examples in Sphinx can be done by pasting your code in a literal block\nprefixed by a :: marker or a code-block directive. In HTML, Sphinx will render it using\nthe Pygments (h t t p ://p y g m e n t s . o r g /) syntax highlighter:\nFlask Application\n=============\nBelow is the first example of a **Flask** app in the Flask official doc:\n.. code-block:: python\n    from flask import Flask\n    app = Flask(__name__)\n    @app.route(\"/\")\n    def hello():\n        return \"Hello World!\"\n    if __name__ == \"__main__\":\n        app.run()\nThat snippet is a fully working app!\nBut adding code snippets in your documentation means that they might get deprecated as\nsoon as you change your code. To avoid deprecation, one method is to have every code\nsnippet displayed in your documentation extracted from the code itself.\nTo do this, you can document your modules, classes, and functions with their docstrings,\nand use the Autodoc Sphinx extension (h t t p ://w w w . s p h i n x - d o c . o r g /e n /l a t e s t /e x t /a u t\no d o c . h t m l ), which grabs docstrings to inject them in the documentation.\nThis is how Python documents its standard library at h t t p s ://d o c s . p y t h o n . o r g /3/l i b r a r\ny /i n d e x . h t m l . In the following example, the autofunction directive will catch the\ndocstring from the index function that's located in the myservice/views/home.py\nmodule:\nAPIS\n====\n**myservice** includes one view that's linked to the root path:\n.. autofunction :: myservice.views.home.index",
      "content_length": 1582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 86 ]\nWhen rendered in HTML, the page will look like this:\nThe other option is to use a literalinclude directive, which will let you point a file and\noffer some options to highlight sections of that file. And when the file is a Python module, it\ncan be included in the test suite to make sure it works.\nThe following is a full example of a project documentation using Sphinx:\nMyservice\n=========\n**myservice** is a simple JSON Flask application that uses **Flakon**.\nThe application is created with :func:`flakon.create_app`:\n.. literalinclude:: ../../myservice/app.py\nThe :file:`settings.ini` file which is passed to :func:`create_app`\ncontains options for running the Flask app, like the DEBUG flag:\n.. literalinclude:: ../../myservice/settings.ini\n   :language: ini\nBlueprint are imported from :mod:`myservice.views` and one\nBlueprint and view example was provided in :file:`myservice/views/home.py`:\n.. literalinclude:: ../../myservice/views/home.py\n   :name: home.py\n   :emphasize-lines: 13",
      "content_length": 1050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 87 ]\nViews can return simple mappings (as highlighted in the example above),\nin that case, they will be converted into a JSON response.\nWhen rendered in HTML the page will look like this:",
      "content_length": 243,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 88 ]\nOf course, using Autodoc and literalinclude will not fix your narratives or design\ndocuments--maintaining a proper documentation up to date is hard, and requires more\nwork than what developers want to put in it.\nSo anything that can be done to automate part of this documentation work is great.\nIn Chapter 4, Designing Runnerly, we will see how we can document, in\nSphinx, the microservice HTTP APIs by using Swagger and the sphinx-\nswagger extension.\nThe following points summarize this section:\nSphinx is a powerful tool to document your project\nTreating your documentation as source code will facilitate its maintenance\nTox can be used to rebuild the documentation when something changes\nIf your documentation points to your code, it will be easier to maintain\nContinuous Integration\nTox can automate every step you are doing when you change something in your project:\nrunning tests on various Python interpreters, verifying coverage and PEP 8 conformance,\nbuilding documentation, and so on.\nBut running all the checks on every change can be time and resource consuming, in\nparticular, if you support several interpreters.\nA Continuous Integration (CI) system solves this issue by taking care of this work every\ntime something changes in your project.\nPushing your project in a shared repository under a Distributed Version Control System\n(DVCS) like Git or Mercurial, on a server will let you trigger a CI every time someone\npushes a change on the server.\nIf you work on an open source software, and don't want to maintain your code server,\nGitHub (h t t p ://g i t h u b . c o m ), GitLab (h t t p ://g i t l a b . c o m ), and Bitbucket (h t t p s ://b i t b\nu c k e t . o r g /) are the most popular services. They will host your project for free if it's public,\nand offer social features, which will make it very easy for anyone to contribute to your\nproject. They all provide integration points to run whatever needs to run when some\nchanges are made in the project.",
      "content_length": 2036,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 89 ]\nFor example, on GitHub, if you see a typo in a reST document, you can change it directly\nfrom your browser, preview the result, and send a Pull Request (PR) to the project\nmaintainer with a few clicks. The project will automatically get rebuilt, and a build status\nmight even be displayed directly on your PR once it's done.\nA lot of open source projects use these services to create a prosperous community of\ncontributors. Mozilla uses GitHub for its Rust project (h t t p s ://g i t h u b . c o m /r u s t - l a n g ),\nand there's no doubt that it helped lower the bar for attracting contributors.\nTravis-CI\nGitHub integrates directly with some CIs. A very popular one is Travis-CI (h t t p s ://t r a v i\ns - c i . o r g /), which runs for free for open source projects. Once you have an account on\nTravis-CI, a settings page will let you directly activate it for some of your GitHub projects.\nTravis-CI relies on a .travis.yml YAML file, which needs to be located in the root of your\nrepository, and describes what should be done with your project when something changes.\nThe YAML file has an env section, which can be used to describe a matrix of builds. A\nmatrix is a collection of builds, which runs in parallel every time you change something in\nyour project.\nThat matrix can be matched with your Tox environments by running each one of them\nseparately via tox -e. By doing this, you will be able to know when a change breaks a\nparticular environment:\nlanguage: python\npython: 3.5\nenv:\n - TOX_ENV=py27\n - TOX_ENV=py35\n - TOX_ENV=docs\n - TOX_ENV=flake8\ninstall:\n - pip install tox\nscript:\n - tox -e $TOX_ENV",
      "content_length": 1675,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 90 ]\nTravis-CI includes everything needed to work with Python projects, so here, in the install\nsection, the pip command can be used to install Tox and get started.\nThe tox-travis project is an interesting one, which extends Tox to simply\nTravis integration. It provides features like an environment detection,\nwhich simplifies the writing of tox.ini files.\nIn case you have system-level dependencies, you can install them via the YAML file, and\neven run bash commands. The default environment runs Linux Debian, and you can type\napt-get commands directly in the YAML file in the before_install section.\nTravis also has support for setting up specific services like databases (refer to h t t p s ://d o c s\n. t r a v i s - c i . c o m /u s e r /d a t a b a s e - s e t u p /), which can get deployed for your projects via the\nservices section.\nIf your microservice uses PosgtreSQL, MySQL, or any other popular open source databases,\nthe chances are that it's available. If not, you can always compile it and run it on your build.\nThe Travis documentation (h t t p s ://d o c s . t r a v i s - c i . c o m /) is a good place to start when\nyou work with Travis-CI.\nTravis can trigger builds on Linux agents, and also has some limited\nsupport for macOS X. Unfortunately, there's no support for Windows yet.\nReadTheDocs\nIn the same vein as Travis, another service that can be hooked from within your GitHub \nrepository is ReadTheDocs (RTD) (h t t p s ://d o c s . r e a d t h e d o c s . i o ).\nIt generates and hosts the project documentation for you. There's nothing concrete to do in\nyour repository. You just configure RTD, so it creates the documentation out of a Sphinx\nHtmlDir, and the service finds the elements automatically.\nFor non-trivial integration, RTD can be configured via a YAML file. Once the\ndocumentation is ready, it will be accessible at\nhttps://<yourprojectname>.readthedocs.io.",
      "content_length": 1954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 91 ]\nRTD comes with versions support, which is useful when you release a new version of your\nservice. That feature scans Git tags, and lets you build and publish your documentation per\neach tag, and decide which one is the default.\nLike versions, RTD offers internationalization (i18n) support in case you want to write your\ndocumentation in several languages.\nCoveralls\nAnother popular service you can hook in your repository if you use Travis-CI and GitHub\nor Bitbucket is Coveralls (h t t p s ://c o v e r a l l s . i o /). This service displays your test code \ncoverage in a nice web UI.\nOnce you've added your repository in your Coveralls account, you can trigger a call to h t t p\n://c o v e r a l l s . i o directly from Travis-CI by instructing Tox to ping to h t t p ://c o v e r a l l s .\ni o after the tests are run.\nThe changes in the tox.ini file are done in the [testenv] section in bold.\n    [testenv]\n    passenv = TRAVIS TRAVIS_JOB_ID TRAVIS_BRANCH\n    deps = pytest\n       pytest-cov\n       coveralls\n       -rrequirements.txt\n    commands =.\n        pytest --cov-config .coveragerc --cov myservice myservice/tests\n        - coveralls\nThe coveralls-python package (called Coveralls in PyPI) is used to send the payload to\ncoveralls.io via its coveralls command after the pytest call is done.\nNotice that the call is prefixed with a hyphen (-). Like in Makefiles, this prefix will ignore\nany failure, and will prevent Tox from failing when you run it locally. Running coveralls\nlocally will always fail unless you set up a special .coveralls.yml file that contains an\nauthentication token. When coveralls is running from Travis-CI, there's no need to have it,\nthanks to the magic of tokens passed from GitHub to the various services.",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 92 ]\nFor Coveralls to work from Travis, there are a few environment variables that need to be\npassed via passenv; everything else should work automatically.\nEvery time you change your project and Travis-CI triggers a build, it will, in turn, trigger\nCoveralls to display an excellent summary of the coverage and how it evolves over time,\nlike in the preceding screenshot.\nMany other services can be hooked on GitHub or Travis-CI, Coveralls being one example.\nOnce you start to add services to your project, it's good practice to use badges in your\nproject's README so the community can see in one glance the status for each one of them\nwith links to the service.",
      "content_length": 718,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 93 ]\nFor example, add this README.rst file in your repository:\nmicroservice\n==========\nThis project is a template for building microservices with Flask.\n.. image::\nhttps://coveralls.io/repos/github/tarekziade/microservice/badge.svg?branch=\nmaster\n   :target:\nhttps://coveralls.io/github/tarekziade/microservice?branch=master\n.. image:: https://travis-ci.org/tarekziade/microservice.svg?branch=master\n   :target: https://travis-ci.org/tarekziade/microservice\n.. image::\nhttps://readthedocs.org/projects/microservice/badge/?version=latest\n   :target: https://microservice.readthedocs.io\nThe preceding file be displayed like this on GitHub on your project's landing page:",
      "content_length": 724,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "Coding, Testing, and Documenting - the Virtuous Cycle\n[ 94 ]\nSummary\nIn this chapter, we went through the different tests that can be written for your\nmicroservices projects. Functional tests are the tests you will write more often, and WebTest\nis a great tool to write them. To run the tests, pytest combined with Tox will make your\nlife easier.\nLast, but not the least, if you host your project on GitHub, you can set up a whole\ncontinuous integration system for free, thanks to Travis-CI. From there, numerous free\nservices can be hooked to complement Travis, like Coveralls. You can also automatically\nbuild and publish your documentation on ReadTheDocs.\nIf you want to look at how everything fits together, the microservice\nproject published on GitHub at h t t p s ://g i t h u b . c o m /R u n n e r l y /m i c r o s e r\nv i c e uses Travis-CI, RTD, and coveralls.io.\nNow that we've covered how a Flask project can be continuously developed, tested, and\ndocumented, we can look at how to design a full microservices-based project. The next\nchapter will go through the design of such an application.",
      "content_length": 1104,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "4\nDesigning Runnerly\nIn Chapter 1, Understanding Microservices, we said that the natural way to build a\nmicroservices-based app is to start with a monolithic version that implements all the\nfeatures, and then to split it into microservices where it makes sense. Trying to come up\nwith a perfect design based on several microservices on day one is a recipe for disaster. It's\nvery hard to know how the application is going to be organized and how it will evolve\nwhen it matures.\nIn this chapter, we will go through this process by building a monolithic application where\nwe're implementing the required features. Then we'll look at where the app can be\ndecomposed into smaller services. By the end of the chapter, we'll end up with a\nmicroservices-based design.\nThe chapter is organized into three main sections:\nPresentation of our Runnerly application and its user stories\nHow Runnerly can be built as a monolithic application\nHow a monolith can evolve into microservices\nOf course, in real life, the splitting process happens over time once the monolithic app\ndesign matures for a bit. But for the purpose of this book, we'll make the assumption that\nthe first version of the application was used for a while and offered us some insights to split\nit the right way, thanks to our time machine.",
      "content_length": 1294,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "Designing Runnerly\n[ 96 ]\nThe Runnerly application\nRunnerly is a toy application for runners that was created for this book. Don't look for it in\nthe Apple Store or the Play Store, as it's not released or deployed for real users.\nHowever, the application is working for real, and you can find and study its different\ncomponents on GitHub in the Runnerly organization at https://github.com/Runnerly.\nRunnerly offers a web view where users can see their runs, races, and training plans, all in\none glimpse. The view is responsive so the users can display the app on their phones or\ntheir desktop browser. Runnerly also sends monthly reports about the user activity.\nA user who is registered into Runnerly needs to hook his/her account to Strava\n(https://www.strava.com), thanks to its standard OAuth2 (https://oauth.net/2/)\nmechanism.\nThe OAuth2 standard is based on the idea of authorizing a third-party\napplication to call a service with an access token that is unique to the user.\nThe token is generated by the service and usually has a limited scope in\nwhat calls can be performed on the service. Strava has a full set of APIs\nthat can be used that way, documented at\nhttps://strava.github.io/api/v3/.\nAfter it has been authorized by the user, Runnerly pulls runs out of Strava to feed its\ndatabase. This flow simplifies a lot of the integration work to make the application\ncompatible with most running devices out there. If your device works with Strava, it will\nwork with Runnerly.\nOnce the database starts to get some content from Strava, the dashboard will display the\nlast 10 runs and will let the users use Runnerly's extra features: races, training plans, and\nmonthly reports.\nLet's dive into Runnerly's features through its user stories.\nUser stories\nThe best way to describe an application is through its user stories. User stories are very\nsimple descriptions of all the interactions a user can have with an application and is the first\nhigh-level document that is usually written when a project starts.",
      "content_length": 2016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "Designing Runnerly\n[ 97 ]\nThe level of detail for each interaction is at first very simple, then gets revisited every time a\nnew particular case appears. User stories are also helpful to detect when it's worth splitting\na feature into its microservice: a story that stands on its own could be a good candidate.\nFor Runnerly, we can start with this small set:\nAs a user, I can create an account on Runnerly with my email and activate it\nthrough a confirmation link I receive in my mailbox.\nAs a user, I can connect to Runnerly and link my profile to my Strava account.\nAs a connected user, I can see my last 10 runs appear in the dashboard.\nAs a connected user, I can add a race I want to participate in. Other users can see\nthe race as well in their dashboard.\nAs a registered user, I receive a monthly report by email that describes how I am\ndoing.\nAs a connected user, I can select a training plan for a race I am planning to do,\nand see a training schedule on the dashboard. A training plan is a simple list of\nruns that are not done yet.\nThere are already a few components emerging from this set of user stories. In no particular\norder, these are:\nThe app needs a registration mechanism that will add the user to our database and\nmake sure they own the email used for registration.\nThe app will authenticate users with a password.\nTo pull data out of Strava, a strava user token needs to be stored in the user profile\nand used to call the service.\nBesides runs, the database needs to store races and training plans.\nTraining programs are a list of runs to be performed at specific dates to be as\nperformant as possible for a given race. Creating a good training plan requires\ninformation about the user, such as their age, sex, weight, and fitness level.\nMonthly reports are built by querying the database and generating a summary sent by\nemail.\nThese descriptions are enough to get us started. The next section describes how the\napplication can be designed and coded.",
      "content_length": 1972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "Designing Runnerly\n[ 98 ]\nMonolithic design\nThis section presents extracts from the source code of the monolithic version of Runnerly.\nThe whole application can be found at h t t p s ://g i t h u b . c o m /R u n n e r l y /m o n o l i t h , if you\nwant to study it in detail.\nA design pattern that is often referred to when building applications is the Model-View-\nController (MVC), which separates the code into three parts:\nModel: This manages the data\nView: This displays the Model for a particular context (web view, PDF view, and\nso on)\nController: This manipulates the Model to change its state\nWhile it's clear that SQLAlchemy can be the Model part, the View and Controller distinction\ncan be a bit vague when it comes to Flask because what is called a view is a function that\nreceives a request and sends back a response. And that function can both display and\nmanipulate the data. So it can act as a View and as a Controller.\nThe Django project uses the Model-View-Template (MVT) acronym to describe that\npattern, where View is the Python callable, and Template is the template engine, or whatever\nis in charge of producing a response in a particular format, given some data.\nFor instance, in a JSON view, json.dumps() is the Template. When you render an HTML\npage with Jinja, the template is the HTML template that's called via render_template().\nIn any case, the first step of designing our application is to define the Model part.\nModel\nIn a Flask application based on SQLAlchemy, the model is described through classes, which\nrepresent the database schema.\nFor Runnerly, the database tables are:\nUser: This contains info about each user, including their credentials\nRun: This is a list of runs with all the info extracted from Strava, and runs for a\ntraining plan\nRace: This is a list of races added by users, with date, location, and distance\nPlan: This is a training plan that is defined by a collection of runs to be done",
      "content_length": 1938,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "Designing Runnerly\n[ 99 ]\nUsing the Flask-SQLAlchemy (http://flask-sqlalchemy.pocoo.org/) extension, you can \nspecify the tables using the Model class as a base class. The following is the definition for\nthe User table with the SQLAlchemy class:\n    from flask_sqlalchemy import SQLAlchemy\n    db = SQLAlchemy()\n    class User(db.Model):\n        __tablename__ = 'user'\n        id = db.Column(db.Integer, primary_key=True, autoincrement=True)\n        email = db.Column(db.Unicode(128), nullable=False)\n        firstname = db.Column(db.Unicode(128))\n        lastname = db.Column(db.Unicode(128))\n        password = db.Column(db.Unicode(128))\n        strava_token = db.Column(db.String(128))\n        age = db.Column(db.Integer)\n        weight = db.Column(db.Numeric(4, 1))\n        max_hr = db.Column(db.Integer)\n        rest_hr = db.Column(db.Integer)\n        vo2max = db.Column(db.Numeric(4, 2))\nWhen used in a Flask app, Flask-SQLAlchemy will take care of wrapping all the calls to\nSQLAlchemy and exposing a session object to your Flask app views to manipulate your\nmodel.\nView and Template\nWhen a request is received, and a view is invoked, Flask-SQLAlchemy will set up a\ndatabase session object inside an application context. The following is a fully functional\nFlask app that uses the schema described earlier in a View that can be queried from /users:\n    from flask import Flask, render_template\n    app = Flask(__name__)\n    @app.route('/users')\n    def users():\n        users = db.session.query(User)\n        return render_template(\"users.html\", users=users)\n    if __name__ == '__main__':\n        db.init_app(app)\n        db.create_all(app=app)\n        app.run()",
      "content_length": 1669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "Designing Runnerly\n[ 100 ]\nWhen the db.session.query() method is called, it performs a query on the database and\nturns every result from the User table into User objects that are passed to the users.html\nJinja template for rendering.\nIn the previous example, Jinja is called to produce an HTML page, which can display the\nuser info with a template that could look like this:\n    <html>\n      <body>\n        <h1>User List</h1>\n        <ul>\n          {% for user in users: %}\n          <li>\n          {{user.firstname}} {{user.lastname}}\n          </li>\n          {% endfor %}\n        </ul>\n      </body>\n    </html>\nFor editing data through the web, WTForms (http://wtforms.readthedocs.io) can be\nused to generate forms for each model. WTForms is a library that generates HTML forms\nwith Python definitions and takes care of extracting data from incoming requests and \nvalidating them before you update your model.\nThe Flask-WTF (https://flask-wtf.readthedocs.io/) project wraps WTForms for Flask\nand adds some useful integration, such as securing forms with Cross-Site Request Forgery\n(CSRF) tokens.\nCSRF tokens will ensure that a malicious third-party website can't send\nvalid forms to your app when you are logged in. Chapter 7, Securing Your\nServices, will explain in detail how CSRF works and why it's important for\nyour app security.\nThe following module implements a form for the User table, using FlaskForm as its basis:\n    from flask_wtf import FlaskForm\n    import wtforms as f\n    from wtforms.validators import DataRequired\n    class UserForm(FlaskForm):\n        email = f.StringField('email', validators=[DataRequired()])\n        firstname = f.StringField('firstname')\n        lastname = f.StringField('lastname')\n        password = f.PasswordField('password')\n        age = f.IntegerField('age')",
      "content_length": 1809,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "Designing Runnerly\n[ 101 ]\n        weight = f.FloatField('weight')\n        max_hr = f.IntegerField('max_hr')\n        rest_hr = f.IntegerField('rest_hr')\n        vo2max = f.FloatField('vo2max')\n        display = ['email', 'firstname', 'lastname', 'password',\n                   'age', 'weight', 'max_hr', 'rest_hr', 'vo2max']\nThe display attribute is just a helper to help the template iterate into a particular ordered list\nof fields when rendering the form. Everything else is using WTForms basic fields classes to\ncreate a form for the user table. The WTForm's Fields documentation provides the full list at\nhttp://wtforms.readthedocs.io/en/latest/fields.html.\nOnce created, UserForm can be used in a view that has two goals. The first one is to display\nthe form on GET calls, and the second one is to update the database on POST calls when the\nuser submits the form:\n    @app.route('/create_user', methods=['GET', 'POST'])\n    def create_user():\n        form = UserForm()\n        if request.method == 'POST':\n            if form.validate_on_submit():\n                new_user = User()\n                form.populate_obj(new_user)\n                db.session.add(new_user)\n                db.session.commit()\n                return redirect('/users')\n        return render_template('create_user.html', form=form)\nThe UserForm class has a method to validate the incoming POST data, as well as a method\nto serialize the values into a User object. When some data is invalid, the form instance will\nkeep the list of errors in field.errors in case the template wants to display them for the\nuser.\nThe create_user.html template iterates through the form field list and WTForm takes\ncare of rendering the proper HTML tags :\n    <html>\n     <body>\n      <form action=\"\" method=\"POST\">\n        {{ form.hidden_tag() }}\n        <dl>\n         {% for field in form.display %}\n         <dt>{{ form[field].label }}</dt>\n         <dd>{{ form[field]() }}</dd>\n           {% if form[field].errors %}",
      "content_length": 1981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "Designing Runnerly\n[ 102 ]\n             {% for e in form[field].errors %} <p>{{ e }}</p> {% endfor %}\n           {% endif %}\n         {% endfor %}\n        </dl>\n        <p>\n        <input type=submit value=\"Publish\">\n      </form>\n     </body>\n    </html>\nThe form.hidden_tag() method will render all hidden field, such as the CSRF token.\nOnce this form is working, it's easy to reuse the same pattern for every form needed in the\napp.\nFor Runnerly, we'll need to reproduce this pattern to create forms for adding training plans\nand races. The form part of the template can be reused for all forms and placed in a Jinja\nmacro since it's generic, and most of the work will consist of writing a form class per\nSQLAlchemy model.\nThere's a project called WTForms-Alchemy (https://wtforms-alchemy.readthedocs.io/)\nthat can be used to create forms out of SQLAlchemy models automatically. The same\nUserForm that we've manually created earlier would be much simpler with WTForms-\nAlchemy since the only step required is to point the SQLAlchemy model:\n    from wtforms_alchemy import ModelForm\n    class UserForm(ModelForm):\n        class Meta:\n            model = User\nBut in practice, forms are often tweaked to a point where it's easier to write them explicitly.\nBut starting with WTForms-Alchemy and seeing how the forms evolve along the way can\nbe a solution.\nLet's summarize what has been done so far to build the app:\nWe've created the database model using SQLAlchemy (Model)\nWe've created views and forms that are interacting with the database via the\nModel (View and Template)",
      "content_length": 1576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "Designing Runnerly\n[ 103 ]\nThere are two things missing to build our complete monolithic solution. They are:\nBackground tasks: This involves implementing the code that regularly retrieves\nStrava runs and generates monthly reports\nAuthentication and authorization: This lets our users log in and restrict editing\nto just their information\nBackground tasks\nThe code that fetches new runs from Strava to add them in the Runnerly database can poll\nStrava regularly, like every hour. The monthly report can also be called once per month to\ngenerate a report and send it to the user by email. Both features are part of the Flask\napplication and use the SQLAlchemy models to do their work.\nBut unlike user requests, they are background tasks, and they need to run on their own\noutside the HTTP request/response cycle.\nIf not using simple cron jobs, a popular way to run repetitive background tasks in Python\nweb apps is to use Celery (http://docs.celeryproject.org), a distributed task queue that\ncan execute some work in a standalone process.\nTo do this, an intermediate called a message broker is in charge of passing messages back and\nforth between the application and Celery. For instance, if the app wants Celery to run\nsomething, it will add a message in the broker. Celery will poll it and do the job.\nA message broker can be any service that can store messages and provide a way to retrieve\nthem. The Celery project works out of the box with Redis (http://redis.io), RabbitMQ\n(http://www.rabbitmq.com), and Amazon SQS (https://aws.amazon.com/sqs/) and\nprovides an abstraction for a Python app to work on both sides of it: to send and run jobs.\nThe part that runs the job is called a worker, and Celery provides a Celery class to start\none. To use celery from a Flask application, you can create a background.py module that\ninstantiates a Celery object and marks your background tasks with a @celery.task\ndecorator.\nIn the following example, we're using the stravalib (http://pythonhosted.org/stravalib)\nlibrary to grab runs from Strava for each user in Runnerly that has a Strava token:\n    from celery import Celery\n    from stravalib import Client\n    from monolith.database import db, User, Run\n    BACKEND = BROKER = 'redis://localhost:6379'",
      "content_length": 2246,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "Designing Runnerly\n[ 104 ]\n    celery = Celery(__name__, backend=BACKEND, broker=BROKER)\n_APP = None\n    def activity2run(user, activity):\n        “”””Used by fetch_runs to convert a strava run into a DB entry.\n        ”””\n        run = Run()\n        run.runner = user\n        run.strava_id = activity.id\n        run.name = activity.name\n        run.distance = activity.distance\n        run.elapsed_time = activity.elapsed_time.total_seconds()\n        run.average_speed = activity.average_speed\n        run.average_heartrate = activity.average_heartrate\n        run.total_elevation_gain = activity.total_elevation_gain\n        run.start_date = activity.start_date\n        return run\n    @celery.task\n    def fetch_all_runs():\n        global _APP\n        # lazy init\n        if _APP is None:\n            from monolith.app import app\n            db.init_app(app)\n            _APP = app\n        else:\n            app = _APP\n        runs_fetched = {}\n        with app.app_context():\n            q = db.session.query(User)\n            for user in q:\n                if user.strava_token is None:\n                    continue\n                runs_fetched[user.id] = fetch_runs(user)\n        return runs_fetched\n    def fetch_runs(user):\n        client = Client(access_token=user.strava_token)\n        runs = 0\n        for activity in client.get_activities(limit=10):\n            if activity.type != 'Run':\n                continue\n            q = db.session.query(Run).filter(Run.strava_id == activity.id)\n            run = q.first()",
      "content_length": 1527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "Designing Runnerly\n[ 105 ]\n            if run is None:\n                db.session.add(activity2run(activity)\n                runs += 1\n        db.session.commit()\n        return runs\nIn this example, the task looks for each user that has a Strava token, then imports their most\nrecent 10 run activities into Runnerly.\nThis module is a fully working Celery application that can accept jobs from a Redis broker.\nAfter we've used the pip-install command for celery and redis Python packages, we\ncan run this module with the celery -A background worker command, assuming a\nRedis instance is running on the machine.\nThis command will run a Celery worker server that will register the fetch_all_runs()\nfunction as being an invokable task and listen for incoming messages in Redis.\nFrom there, in your Flask app, you can import the same background.py module and call\nthat decorated function directly. You will get a future-like object that will call the Celery\nworker via Redis to run the function in a separate process:\n    from flask import Flask, jsonify\n    app = Flask(__name__)\n    @app.route('/fetch')\n    def fetch_runs():\n        from monolith.background import fetch_all_runs\n        res = fetch_all_runs.delay()\n        res.wait()\n        return jsonify(res.result)\nIn this example, we're waiting for the task to complete and the call to /fetch just sits there\nuntil the job is done. Of course, in Runnerly, we want to fire-and-forget the task and not call\n.wait() because it's going to take several seconds per user.\nIn a sense, since the Celery service is invoked by the Flask application by passing messages\nvia Redis, it could be considered as a microservice itself. That's also interesting in terms of\ndeployment since both the Redis server and the Celery app can be deployed on another\nserver. But since the code that's executed lives in the same code base, this is still considered\nas a monolithic design.",
      "content_length": 1916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "Designing Runnerly\n[ 106 ]\nAnother aspect of running background workers is when you want your jobs to be executed\nperiodically. Instead of having the Flask app trigger the job every hour, we can use Celery's\nPeriodic Task feature\n(http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html), which\nacts as a scheduler.\nThe Flask app, in that case, would schedule the periodic task the same way it triggered the\nsingle task.\nStrava token\nThe missing part of the puzzle to make those Strava imports work is to get a Strava token\nfor each user and store it in our user table.\nAs said earlier, this can be done via an OAuth2 dance where the connected user is\nredirected to Strava to authorize Runnerly, then redirected back to Runnerly with an\nOAuth2 code that can be converted into a token we can store.\nThe stravalib library provides some helpers to perform that dance. The first one is the\nauthorization_url() method, which returns a full URL that can be presented to the\nusers to initiate the OAuth2 dance:\n    app.config['STRAVA_CLIENT_ID'] = 'runnerly-strava-id'\n    app.config['STRAVA_CLIENT_SECRET'] = 'runnerly-strava-secret'\n    def get_strava_auth_url():\n        client = Client()\n        client_id = app.config['STRAVA_CLIENT_ID']\n        redirect = 'http://127.0.0.1:5000/strava_auth'\n        url = client.authorization_url(client_id=client_id,\n                                       redirect_uri=redirect)\n        return url\nIn the example, redirect is the URL Strava that will redirect once the application is\ngranted access. In this example, it's the app running locally. The get_strava_auth_url()\nmethod can be used to present a link to a connected Runnerly user.\nOnce the user authorizes Runnerly on the Strava site, the /strava_auth view will get a\ncode that can be exchanged for a token that will stay valid for future Strava requests on\nbehalf of that user. The stravalib library's Client class has an\nexchange_code_for_token() method to do the conversion.",
      "content_length": 1984,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "Designing Runnerly\n[ 107 ]\nThe view then simply copies the token into the user database entry:\n    @app.route('/strava_auth')\n    @login_required\n    def _strava_auth():\n        code = request.args.get('code')\n        client = Client()\n        xc = client.exchange_code_for_token\n        access_token = xc(client_id=app.config['STRAVA_CLIENT_ID'],\n          client_secret=app.config['STRAVA_CLIENT_SECRET'], code=code)\n        current_user.strava_token = access_token\n        db.session.add(current_user)\n        db.session.commit()\n        return redirect('/')\nIn that view, @login_required and current_user are part of the authentication and\nauthorization processes presented in the next section.\nAuthentication and authorization\nOur monolithic application is almost ready.\nOne last thing that we need to add is a way for users to authenticate. Runnerly needs to\nknow who's connected since the dashboard will display user-specific data. Forms also need\nto be secured. For instance, we don't want users to be able to edit other users' information.\nFor our monolithic solution, we'll implement a very simple basic authentication\n(https://en.wikipedia.org/wiki/Basic_access_authentication) scheme where the user\nsends its credentials in the Authorization header. From a security point of view, using basic\nauthentication is fine as long as the server uses SSL. When websites are called through\nHTTPS, the entire request is encrypted (including the query part of the URL), so the\ntransport is secured.\nAs far as passwords are concerned, the simplest form of protection is to make sure you\ndon't store them in the clear in the database, but instead store them in a hashed form that\ncan't be converted back to the original password. That will minimize the risk of leaking\npasswords if your server is compromised. For the authentication process, it just means that\nwhen the user logs in, you need to hash the incoming password to compare it to the stored\nhash.",
      "content_length": 1955,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "Designing Runnerly\n[ 108 ]\nThe transport layer is usually not the weak spot for an application\nsecurity. What happens to the service once the request is received is what\nmatters the most. When the authentication process happens, there's a\nwindow during which an attacker can intercept the password (in clear or\nhashed form). In Chapter 7, Securing Your Services, we'll talk about ways\nto reduce this attack surface.\nWerkzeug provides a few helpers to deal with password hashes,\ngenerate_password_hash() and check_password_hash(), which can be integrated\ninto our User class.\nBy default, Werkzeug uses PBKDF2 (https://en.wikipedia.org/wiki/PBKDF2) with\nSHA-1, which is a secure way to hash a value with salt.\nLet's extend our User class with methods to set and verify a password:\n    from werkzeug.security import generate_password_hash,\ncheck_password_hash\n    class User(db.Model):\n        __tablename__ = 'user'\n        # ... all the Columns ...\n        def __init__(self, *args, **kw):\n            super(User, self).__init__(*args, **kw)\n            self._authenticated = False\n        def set_password(self, password):\n            self.password = generate_password_hash(password)\n        @property\n        def is_authenticated(self):\n            return self._authenticated\n        def authenticate(self, password):\n            checked = check_password_hash(self.password, password)\n            self._authenticated = checked\n            return self._authenticated\nWhen creating new users in the database, the set_password() method can be used to\nhash and store a password in the User model. Any attempt to verify the password can be\nmade with authenticate(), which will compare hashes.",
      "content_length": 1688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "Designing Runnerly\n[ 109 ]\nOnce we have that mechanism in place, the Flask-Login\n(https://flask-login.readthedocs.io/) extension provides everything needed to log in\nand log out users, and to keep track of who's connected so you can change how your app\nworks.\nFlask-Login provides two functions to set a user in the current Flask session: login_user()\nand logout_user(). When the login_user() method is called, the user ID is stored in\nthe Flask session, and a cookie is set on the client side. The user will be remembered for the\nnext requests until they log out.\nTo have this mechanism in place, a LoginManager instance needs to be created on your\napplication at startup.\nHere's the implementation of the login and logout views, along with the LoginManager\ncreation:\n    from flask_login import LoginManager, login_user, logout_user\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            email, password = form.data['email'], form.data['password']\n            q = db.session.query(User).filter(User.email == email)\n            user = q.first()\n            if user is not None and user.authenticate(password):\n                login_user(user)\n                return redirect('/')\n        return render_template('login.html', form=form)\n    @app.route(\"/logout\")\n    def logout():\n        logout_user()\n        return redirect('/')\n    login_manager = LoginManager()\n    login_manager.init_app(app)\n    @login_manager.user_loader\n    def load_user(user_id):\n        user = User.query.get(user_id)\n        if user is not None:\n            user._authenticated = True\n        return user",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "Designing Runnerly\n[ 110 ]\nThe @login_manager.user_loader decorated function is used every time Flask-Login\nneeds to convert a stored user ID to an actual user instance.\nThe authentication part is done in the login view by calling user.authenticate(), and\nthen set in the session with login_user(user).\nThe last thing to do is to protect some of our views from unauthorized access. For instance,\nthe user edition form should not be accessible if you are not logged in. The\n@login_required decorator will reject any attempt to access a view if you are not logged\nin with a 401 Unauthorized error.\nIt needs to be placed after the @app.route() call:\n    @app.route('/create_user', methods=['GET', 'POST'])\n    @login_required\n    def create_user():\n        # ... code\nIn the code, @login_required will ensure that you are a valid user and that you've\nauthenticated.\nHowever, this decorator does not deal with permissions. Permissions handling is out of\nscope for the Flask-Login project, and an extension such as Flask-Principal\n(https://pythonhosted.org/Flask-Principal/) can be used to handle this on the top of\nFlask-Login.\nHowever, for our very simple use case, it might be overkill. One specific role Runnerly\nusers have is admin. Admins have super powers across the app, while simple users can only\nchange their info.\nIf we add an is_admin Boolean flag in the User model, we can create a similar decorator\nsuch as @login_required, which will also check this flag:\n    def admin_required(func):\n        @functools.wraps(func)\n        def _admin_required(*args, **kw):\n            admin = current_user.is_authenticated and current_user.is_admin\n            if not admin:\n                return app.login_manager.unauthorized()\n            return func(*args, **kw)\n        return _admin_required",
      "content_length": 1795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "Designing Runnerly\n[ 111 ]\nIn the same vein, more granular permission verifications can be done by looking at the\ncurrent_user variable Flask-Login sets in the application context. For example, you could\nuse this to allow a user to change their data, but prevent the user from changing other users'\ndata.\nPutting together the monolithic design\nThis monolithic design is excellent and should be the kind of result you would aim for in\nyour first development iteration. Everything should be built of course through TDD, as\nexplained in Chapter 3, Coding, Testing, and Documenting - The Virtuous Cycle.\nIt's a short and clean implementation on the top of a relational database that can be\ndeployed with a PostgreSQL or MySQLServer. Thanks to the SQLAlchemy abstractions, a\nlocal version can run with SQLite 3 and facilitate your day-to-day work and your tests.\nTo build this app, we've used the following extensions and library:\nFlask-SQLAlchemy and SQLAlchemy: These are used for the Model\nFlask-WTF and WTForms: These are used for all the forms\nCelery and Redis: These are used for background processes and periodic tasks\nFlask-Login: This is used for managing authentication and authorization\nThe overall design looks like the following diagram:",
      "content_length": 1245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "Designing Runnerly\n[ 112 ]\nA typical deployment will group the Flask app with one Redis and one Celery instance on\nthe same server and serve requests through a web server such as Apache or nginx. The\ndatabase can be located on the same server, or on a dedicated server.\nThe server can spawn several Flask processes and Celery processes to raise the number of\nrequests and users it can handle.\nWhen this deployment is not sufficient to serve the load, the first change that comes to mind\nis to add other application servers and dedicated servers for the database server and the\nRedis broker.\nThe third step, if needed, will be to have more Redis and PostgreSQL instances, and some\nthoughts will be required on the best approach because you will need to set up replication\nand maybe sharding strategies.\nWhen an application reaches that third step, solutions out of the box, such\nas Amazon SQS and Amazon RDS, might be a good fit, as we'll see in\nChapter 11, Deploying on AWS.\nSplitting the monolith\nLet's project into the world where Runnerly, as implemented previously, starts to be used\nby a lot of people. Features are added, bugs are fixed, and the database is steadily growing.\nThe first problem that we're facing is the background process that creates reports and calls\nStrava. Since we're having thousands of users, these tasks take most of the server resources,\nand users are experiencing slowdowns on the frontend.\nIt's getting obvious that we need to have them running on separate servers. With the\nmonolithic application using Celery and Redis, it's not an issue. We can dedicate a couple of\nnew servers for the background jobs.\nBut the biggest concern if we do this is that the Celery worker code needs to import the\nFlask application code to operate. So the deployment dedicated to the background workers\nneeds to include the whole Flask app. That also means that every time something changes\nin the app, we'll need to update the Celery workers as well to avoid regression.\nThat also means we'll have to install on a server where the only role is to pump data out of\nStrava, all the dependencies the Flask application has. If we use Bootstrap in our templates,\nwe'll have to deploy it on the Celery worker server!",
      "content_length": 2225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "Designing Runnerly\n[ 113 ]\nThis dependency issue begs the question: \"Why does the Celery worker need to be in the\nFlask application in the first place?\" That design was excellent when we started to code\nRunnerly, but it became obvious that it's fragile.\nThe interactions Celery has with the application are very specific. The Strava worker needs\nto:\nGet the Strava tokens\nAdd new runs\nInstead of using the Flask app code, the Celery worker code could be entirely independent\nand just interacts with the database directly.\nHaving the Celery worker acting as a separate microservice is a great first step to split our\nmonolithic app--let's call it the Strava Service. The worker that's in charge of building\nreports can be split the same way to run, on its own, the Reports Service. Each one of these\nCelery workers can focus on performing one single task.\nThe biggest design decision when doing this is whether these new microservices call the\ndatabase directly or whether they call it via an HTTP API that acts as an intermediate\nbetween the services and the database.\nDirect database calls seem like the simplest solution, but this introduces another problem.\nSince the original Flask app, the Strava service and the Reports Service will all share the\nsame database; every time something changes in it, they all get impacted.\nIf there's an intermediate layer that exposes to the different services just the info they need\nto do their jobs, it reduces the database dependency problem. If well designed, an HTTP\nAPI contract compatibility can be maintained when changes are made in the database\nschema.\nAs far as the Strava and Report microservices are concerned, they are Celery workers, so we\ndon't have to design any HTTP API for them. They get some work from the Redis broker\nand then interact with the service wrapping database calls. Let's call this new intermediate\nthe Data Service.",
      "content_length": 1889,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "Designing Runnerly\n[ 114 ]\nData Service\nThe following diagram describes our new application organization. Both the Reports and\nStrava service get some work from Redis and interact with the Data Service, as shown in\nthe following diagram:\nThe Data Service is an HTTP API that wraps the database containing all the users and runs\ndata. The dashboard is the frontend that implements the HTML user interface.\nWhen you have any doubt about whether it's a good idea to split out a\nnew microservice out of your main app, don't do it.\nSome of the information required by the Celery workers can be passed through the Redis\nbroker, such as the Strava tokens for the Strava service.\nFor the Reports service, however, it's not practical to send all the info through Redis because\nthe amount of data can be significant. If a runner is doing 30 runs per month, it's simpler to\nlet the Reports service pull them directly from the Data Service.",
      "content_length": 928,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "Designing Runnerly\n[ 115 ]\nThe Data service view needs to implement the following APIs:\nFor the Strava service--a POST endpoint to add runs\nFor the Reports service\nA GET endpoint to retrieve a list of user IDs\nA GET endpoint to get a list of runs given a user ID and a month\nAs you can see, the HTTP API is minimal--we want to expose as few entry points as\npossible. Although the structure of a run is going to be shared across all services, we need to\nexpose as a few number of fields as possible.\nFor our service implementation, we will rely on the Open API 2.0 standard.\nUsing Open API 2.0\nThe Open API 2.0 specification, also known as Swagger (https://www.openapis.org/) is a\nsimple description language that comes as a JSON or YAML file, that lists all your HTTP\nAPI endpoints, how they are used, and the structure of the data that comes in and out. It\nmakes the assumption that your service sends and receives JSON documents.\nSwagger has the same goal that WSDL\n(https://en.wikipedia.org/wiki/Web_Services_Description_Language) had back in the\nXML web services era, but it's much lighter and straight to the point.\nThe following example is a minimal Open API description file which defines one single\n/apis/users_ids endpoint and supports the GET method to retrieve the list of user IDs:\nswagger: \"2.0\"\ninfo:\n  title: Runnerly Data Service\n  description: returns info about Runnerly\n  license:\n    name: APLv2\n    url: https://www.apache.org/licenses/LICENSE-2.0.html\n  version: 0.1.0\nbasePath: /api\npaths:\n    /user_ids:\n      get:\n        operationId: getUserIds\n        description: Returns a list of ids\n        produces:\n        - application/json",
      "content_length": 1658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "Designing Runnerly\n[ 116 ]\n        responses:\n          '200':\n            description: List of Ids\n            schema:\n                type: array\n                items:\n                    type: integer\nThe full Open API 2.0 specification can be found at http://swagger.io/specification/.\nIt's very detailed and will let you describe metadata about the API, its endpoints, and the\ndata types it uses.\nThe data types described in the schema sections are following the JSON-Schema\nspecification (http://json-schema.org/latest/json-schema-core.html). Here, we're\ndescribing that the /get_ids endpoint returns an array of integers.\nYou can provide a lot of details about your API in that spec--things such as what headers\nshould be present in your requests, or what will be the content-type of some responses, can\nbe added to it.\nDescribing your HTTP endpoints with Swagger offers some excellent possibilities:\nThere are a plethora of Open API 2.0 clients that can consume your description\nand do something useful with it, such as building functional tests against your\nservice or validating data that's sent to it\nIt provides a standard, language-agnostic documentation for your API\nThe server can check that the requests and responses follow the spec\nSome web frameworks even use the Swagger spec to create all the routing and I/O data\nchecks for your microservices--for instance, Connexion\n(https://github.com/zalando/connexion), does this for Flask.\nThere are two schools of thought when people are building HTTP APIs with Swagger.\nThe specification-first one, where you create a Swagger spec file and then create\nyour app on the top of it, using all the info provided in the spec. That's the\nprinciple behind Connexion.\nThe specification-extracted one, where it's your code that generates the Swagger\nspec file. Some toolkits out there will do this by reading your view docstrings, for\ninstance.",
      "content_length": 1898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "Designing Runnerly\n[ 117 ]\nThe biggest advantage of the first approach is that your Swagger specification is guaranteed\nto be up-to-date since it drives the app. The second approach is still valuable, for example,\nwhen you want to introduce Swagger in an existing project.\nFor implementing a Flask app that uses the first approach, a framework such as Connexion\nwill give you excellent high-level helpers. All you have to do is pass the spec file and your\nfunctions, and Connexion, will generate a Flask app. Connexion uses the operationId\nfield to resolve which function will be called for each operation.\nA small caveat of this approach is that the Swagger file will include implementation details\n(the full path to the Python functions), which is a bit intrusive for a language-agnostic\nspecification. There's also an automatic resolver, that will look for the Python function given\nthe path and method of each operation. In that case, the implementation of GET\n/api/users_ids will need to be located at api.users_ids.get().\nFlakon (presented in Chapter 2, Discovering Flask) has a different approach. The project has\na special Blueprint class called SwaggerBlueprint, which won't require you to add the\nPython functions in the spec or try to guess where it should be given the operation.\nThis custom Blueprint takes a Swagger spec file and provides an @api.operation\ndecorator that is similar to @api.route. This decorator takes an operationId name instead\nof a route--so the Blueprint can link the view to the right route explicitly.\nIn the following example, we're creating a Swagger Blueprint and implementing the\ngetUserIds operation:\n    from flakon import SwaggerBlueprint\n    api = SwaggerBlueprint('swagger', spec='api.yml')\n    @api.operation('getUserIds')\n    def get_user_ids():\n        # .. do the work ..\nThe Python implementation can be renamed and moved around without having to change\nthe Swagger spec.\nThe rest of the Data Service API is implemented as described, and can be found in the\nRunnerly repository (h t t p s ://g i t h u b . c o m /R u n n e r l y ).",
      "content_length": 2082,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "Designing Runnerly\n[ 118 ]\nMore splitting\nSo far, we've split out of our monolith everything related to background tasks, and we've\nadded a few HTTP API views for the new microservices to interact with the main\napplication.\nSince the new API allows us to add runs, there's another part we can split out of the\nmonolith--the Training feature.\nThis feature can run on its own as long as it's able to generate new runs. When a user wants\nto start a new training plan, the main app can interact with the Training microservice and\nask it to generate new runs.\nAlternatively, the design could be reversed for better data isolation: the Training\nmicroservice publishes an API that returns a list of runs with their specific structure, exactly\nlike the Strava API that returns activities. The main Flask app can then convert them into\nRunnerly runs. The Training plan can work without any specific knowledge about the\nRunnerly users: it gets asked to generate a plan given a few params.\nBut doing this new split should happen for a good reason. \"Is the code behind the training\nalgorithm CPU consuming?\", \"Is it growing into a full expert system that's being used by\nother applications?\", \"Will the Training feature need other data in the future to work?\"\nEvery time you make the decision to split out a new microservice, there's a\nrisk of ending up with a bloated app.\nIn the same vein, the Race feature could probably be a standalone microservice at some\npoint, since a list of races could be completely independent of the Runnerly database.",
      "content_length": 1535,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "Designing Runnerly\n[ 119 ]\nThe following diagram shows the final Runnerly design, featuring four microservices and\nthe main Flask app. In Chapter 8, Deploying on AWS, we'll see how we can even get rid of\nthat main application, turn the Data Service as a full microservice, and build a JavaScript\napplication that integrates everything:\nSummary\nThe Runnerly app is a typical web app that interacts with a database and a few backend\nservices. And building it as a monolithic application is the way to go for the first few\niterations.\nIn this chapter, we've demonstrated how the monolith could be gradually split into\nmicroservices, and how tools such as Celery can help in that process. Each background\nprocess that can be split in an independent Celery task is a potential microservice.",
      "content_length": 785,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "Designing Runnerly\n[ 120 ]\nWe've also looked at Swagger, which is a great tool to help define APIs between\nmicroservices.\nThis splitting process should be conservative and progressive because it’s quite easy to end\nup with a system where the overhead for building and maintaining microservices\noutweighs the benefits to splitting those things out.\nIf you like software architecture, the last version of the app is pretty appealing. It offers a lot\nof options for deploying and scaling Runnerly.\nHowever, we've moved from a single application to many applications that need to interact\nwith each other. Every link in the preceding diagram can be a weak point for your\napplication. What happens, for instance, if Redis goes down? Or if there's a network split\nbetween the Data Service and the Strava Service in the middle of a process?\nThe same question goes for every new network link that was added in our architecture. We\nneed to be resilient when something goes wrong. We need to know where we're at and\nwhat to do when a service that was out gets back online.\nAll of these problems are addressed in the next chapter.",
      "content_length": 1119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "5\nInteracting with Other Services\nIn the previous chapter, the Runnerly monolithic app was split into several microservices,\nand more network interactions between the different parts were consequently added.\nWhen a user looks at the main web view, the application needs to fetch the list of runs and\nraces from the database and the races. The network calls that are triggered by that request\nare synchronous because we want to display the results immediately.\nOn the other hand, the Celery workers are doing their duty in the background, and they\nreceive their order via a Redis broker asynchronously.\nThere are also cases where a mix of synchronous and asynchronous calls are useful. For\ninstance, letting the user pick a new training plan can trigger the creation of a series of new\nruns in the background while displaying some info about the plan itself.\nIn future versions of Runnerly, we could also have more service-to-service interactions,\nwhere an event in a service triggers a series of reactions in other services. Having the ability\nto loosely couple different parts of the system via some asynchronous messaging is quite\nuseful to prevent interdependencies.\nIn any case, the bottom line is that we need to interact with other services through the\nnetwork synchronously and asynchronously. These interactions need to be efficient, and\nwhen something goes wrong, we need to have a plan.\nThe other problem introduced by adding more network connections is testing: how do we\ntest in isolation a microservice that needs to call other microservices to function?",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 122 ]\nIn this chapter, we'll explain:\nHow a service can call another service in a synchronous way, and make these\ncalls as efficient as possible\nHow a service can make asynchronous calls and communicate with other services\nvia events\nSome techniques to test services that have network dependencies\nSynchronous calls\nAs we've seen in the previous chapters, synchronous interactions between microservices\ncan be done via RESTful HTTP APIs using JSON payloads.\nThat's by far the most used pattern, because both HTTP and JSON are the golden standards.\nIf your web service implements an HTTP API that accepts JSON, any developer using any\nprogramming language will happily use it.\nFollowing a RESTful scheme, on the other hand, is not a requirement and is prone to\ninterpretation. Countless blog posts are debating the virtue of using POST versus PUT\nresponse on the internet.\nSome projects implement Remote Procedure Call (RPC) APIs over HTTP rather than REST\nAPIs. In RPC, the focus is on the action, which is part of the endpoint URL. In REST, the\nfocus is on the resource, and actions are defined by HTTP methods.\nSome projects are a mix of both and don't strictly follow a given standard. The most\nimportant thing is that your service behavior should be consistent and well-documented.\nThis book leans on REST rather than RPC, but is not strict about it, and\ndoes not have a strong opinion about all the PUT versus POST debates.\nSending and receiving JSON payloads is the simplest way for a microservice to interact\nwith the others, and only requires microservices to know the entry points and parameters to\npass using HTTP requests.\nTo do this, you just need to use an HTTP client. Python has one built-in in the\nhttp.client module, but the Requests library (h t t p s ://d o c s . p y t h o n - r e q u e s t s . o r g ) has\na better API to work with and offers built-in features that will make your life easier.",
      "content_length": 1948,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 123 ]\nHTTP requests in the requests library are built around the concept of session, and the best\nway to use it is to create a Session object that is reused every time you interact with any\nservice.\nA Session object can hold, among other things, authentication information and some\ndefault headers you want to set for all requests your application will make. In the following\nexample, the Session object will automatically create the right Authorization and\nContent-Type headers:\n    from requests import Session\n    s = Session()\n    s.headers['Content-Type'] = 'application/json'\n    s.auth = 'tarek', 'password'\n    # doing some calls, auth and headers are all set!\n    s.get('http://localhost:5000/api').json()\n    s.get('http://localhost:5000/api2').json()\nLet's see how we can generalize this pattern in a Flask app that needs to interact with other\nservices.\nUsing Session in a Flask app\nFlask's Application object has an extensions mapping that can be used to store utilities\nsuch as connectors. In our case, we want to store a Session object. We can create a function\nthat will initialize a placeholder in the app.extensions mapping and add a Session\nobject in it:\n    from requests import Session\n    def setup_connector(app, name='default', **options):\n        if not hasattr(app, 'extensions'):\n            app.extensions = {}\n        if 'connectors' not in app.extensions:\n            app.extensions['connectors'] = {}\n        session = Session()\n        if 'auth' in options:\n            session.auth = options['auth']\n        headers = options.get('headers', {})\n        if 'Content-Type' not in headers:\n            headers['Content-Type'] = 'application/json'",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 124 ]\n        session.headers.update(headers)\n        app.extensions['connectors'][name] = session\n        return session\n    def get_connector(app, name='default'):\n        return app.extensions['connectors'][name]\nIn this example, the setup_connector() function will create a Session object and store it\nin the app's extensions mapping. The created Session will set the Content-Type header to\napplication/json by default, so it's suitable for sending data to JSON-based\nmicroservices.\nUsing the session from a view can then be done with the get_connector() function once\nit has been set up on the app. In the following example, a Flask app running on port 5001\nwill synchronously call a microservice running on 5000 to serve its content:\n    from flask import Flask, jsonify\n    app = Flask(__name__)\n    setup_connector(app)\n    @app.route('/api', methods=['GET', 'POST'])\n    def my_microservice():\n        with get_connector(app) as conn:\n            sub_result = conn.get('http://localhost:5000/api').json()\n        return jsonify({'result': sub_result, 'Hello': 'World!'})\n    if __name__ == '__main__':\n        app.run(port=5001)\nA call to the service will propagate a call to the other service:\n$ curl http://127.0.0.1:5001/api\n{\n  \"Hello\": \"World!\",\n  \"result\": {\n    \"Hello\": \"World!\",\n    \"result\": \"OK\"\n  }\n}\nThis naive implementation is based on the hypothesis that everything will go smoothly. But\nwhat will happen if the microservice that's called lags and takes 30 seconds to return?",
      "content_length": 1534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 125 ]\nBy default, requests will hang indefinitely until the answer is ready, which is not a behavior\nwe'd want when calling microservices. The timeout option is useful in this case. Used\nwhen making a request, it will raise a ReadTimeout in case the remote server fails to\nanswer promptly.\nIn the following example, we drop the call if it's hanging for more than 2 seconds:\n    from requests.exceptions import ReadTimeout\n    @app.route('/api', methods=['GET', 'POST'])\n    def my_microservice():\n        with get_connector(app) as conn:\n            try:\n                result =\nconn.get('http://localhost:5000/api',timeout=2.0).json()\n        except ReadTimeout:\n            result = {}\n    return jsonify({'result': result, 'Hello': 'World!'})\nOf course, what should be done when a timeout happens depends on your service logic. In\nthis example, we silently ignore the problem and send back an empty result. But maybe in\nother cases, you will need to raise an error. In any case, handling timeouts is mandatory if\nyou want to build a robust service-to-service link.\nThe other error that can happen is when the connection completely drops, or the remote\nserver is not reachable at all. Requests will retry several times and eventually will raise a\nConnectionError you need to catch:\nfrom requests.exceptions import ReadTimeout, ConnectionError\n@app.route('/api', methods=['GET', 'POST'])\ndef my_microservice():\n    with get_connector(app) as conn:\n        try:\n            result = conn.get('http://localhost:5000/api',\n                               timeout=2.).json()\n            except (ReadTimeout, ConnectionError):\n                result = {}\n        return jsonify({'result': result, 'Hello': 'World!'})\nSince it's good practice to always use the timeout option, a better way would be to set a\ndefault one at the session level, so we don't have to provide it on every request call.",
      "content_length": 1924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 126 ]\nTo do this, the requests library has a way to set up custom transport adapters, where you\ncan define a behavior for a given host the session will call. It can be used to create a general\ntimeout, but also to offer a retries option in case you want to tweak how many retries\nshould be done when the service is not responsive.\nBack to our setup_connector() function. Using an adapter, we can add timeout and\nretries options that will be used by default for all requests:\n    from requests.adapters import HTTPAdapter\n    class HTTPTimeoutAdapter(HTTPAdapter):\n        def __init__(self, *args, **kw):\n            self.timeout = kw.pop('timeout', 30.)\n            super().__init__(*args, **kw)\n        def send(self, request, **kw):\n            timeout = kw.get('timeout')\n            if timeout is None:\n                kw['timeout'] = self.timeout\n            return super().send(request, **kw)\n    def setup_connector(app, name='default', **options):\n        if not hasattr(app, 'extensions'):\n            app.extensions = {}\n        if 'connectors' not in app.extensions:\n            app.extensions['connectors'] = {}\n        session = Session()\n        if 'auth' in options:\n            session.auth = options['auth']\n        headers = options.get('headers', {})\n        if 'Content-Type' not in headers:\n            headers['Content-Type'] = 'application/json'\n        session.headers.update(headers)\n        retries = options.get('retries', 3)\n        timeout = options.get('timeout', 30)\n        adapter = HTTPTimeoutAdapter(max_retries=retries, timeout=timeout)\n        session.mount('http://', adapter)\n        app.extensions['connectors'][name] = session\n        return session",
      "content_length": 1725,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 127 ]\nThe session.mount(host, adapter) call will tell requests to use the\nHTTPTimeoutAdapter every time a request for any HTTP service is made. The http://\nvalue for the host is a catch-all in this case.\nThe beautiful thing about the mount() function is that the session behavior can be tweaked\non a service-per-service basis depending on your app logic. For instance, you can mount\nanother instance on the adapter for a particular host if you need to set up some custom\ntimeouts and retries values:\n    adapter2 = HTTPTimeoutAdapter(max_retries=1, timeout=1.)\n    session.mount('http://myspecial.service', adapter2)\nThanks to this pattern, a single request Session object can be instantiated into your\napplication to interact with many other HTTP services.\nConnection pooling\nRequests use urllib3 under the hood, which will create one pool of connectors per host\nyou are calling and reuse them when the code calls a host.\nIn other words, if your service calls several other services, you don't need to worry about\nrecycling connections made to those services; requests should handle it for you.\nFlask is a synchronous framework, so if you are running with a single thread, which is the\ndefault behavior, then the requests library's connection pooling doesn't help you much.\nEvery call will happen one after the other. Requests should only keep one connector open\nper remote host.\nBut if you run your Flask application with several threads and have a lot of concurrent\nconnections, these pools can play a vital role in making sure you're controlling how many\nconnections are made to other services. You don't want your app to open an unlimited\nnumber of simultaneous connections to another service. It's a recipe for disaster.\nOur HTTPTimeoutAdapter class can be used to control the growth of our pools. The class\ninherits from HTTPAdapter, which surfaces urllib3 pool options.\nYou can pass these options to the constructor:\npool_connections: This helps you figure out how many simultaneous\nconnections are kept open.",
      "content_length": 2051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 128 ]\npool_maxsize: This helps you figure out the maximum number of connections\nthe pool handles.\nmax_retries: This helps you figure out the maximum number of retries per\nconnection.\npool_block: This helps you figure out whether the connection pool should block\nconnections and when the pool_maxsize is reached. If set to False, it will create\nnew connections even if the pool is full, but not add them in the pool. If set to\nTrue, it will not create new connections when the pool is full and wait. This is\nuseful to maximize the number of connections open to a host.\nFor example, our adapter could hold 25 simultaneous connections if the app is executed\nwith a web server that allows multiple threads:\n    adapter = HTTPTimeoutAdapter(max_retries=retries,\n                                 timeout=timeout, pool_connections=25)\nAllowing multiple threads can be a great way to improve your service performances, but it\ncomes with most significant risks. With its thread-local mechanism, Flask will ensure that\neach thread gets its version of flask.g (the global), flask.request or flask.response,\nso you don't have to deal with thread-safety, but your views will be visited concurrently by\nseveral threads, so you need to be careful about what's happening in them.\nIf you don't share any states outside flask.g and just calling the Request session, it\nshould work. Request's session is not thread-safe you should have one session per thread.\nBut if you are changing any shared state and don't do the proper locking work to avoid race\nconditions, you will be in trouble. If your views get too complicated to make sure it is\nthread-safe, it's best to run with a single thread and spawn multiple processes. In that case,\neach process will execute a Request session that has a single connection to the external\nservice, and that will serialize the calls.\nThis serialization is a limiting factor synchronous frameworks have, and it forces us to make\ndeployments that consume more memory to spawn all the processes or use implicit\nasynchronous tools such as Gevent.\nIn any case, if the single-threaded application is fast to respond, it mitigates this limitation a\nlot.\nOne way to speed up your application for calls to other services, is to make sure it uses\nHTTP cache headers.",
      "content_length": 2306,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 129 ]\nHTTP cache headers\nIn the HTTP protocol, there are a few cache mechanisms that can be used to indicate to a\nclient that a page it's trying to fetch has not changed since their last visit. Caching is\nsomething we can do in our microservices on all the read-only API endpoints such as GETs\nand HEADs.\nThe simplest way to implement it is to return along with a result an ETag header in the\nresponse. An ETag value is a string that can be considered as a version for the resource the\nclient is trying to get. It can be a timestamp, an incremental version, or a hash. It's up to the\nserver to decide what to put in it. But the idea is that it should be unique to the value of the\nresponse.\nLike web browsers, when the client fetches a response that contains such a header, it can\nbuild a local dictionary cache that stores the response bodies and ETags as its values, and\nthe URLs as its keys.\nWhen making a new request, the client can look up the dictionary and pass along a stored\nETag value in the If-Modified-Since header. If the server sends back a 304 response, it\nmeans that the response has not changed and the client can use the previously stored one.\nThis mechanism greatly reduces, the response times from the server since it can\nimmediately return an empty 304 response when the content has not changed. The 304\nresponse is also smaller data for the network, since it has no body.\nThere's a project called CacheControl (h t t p ://c a c h e c o n t r o l . r e a d t h e d o c s . i o ) that can\nbe used with the Request session, which implements this behavior for you fairly\ntransparently.\nIn our previous example, having the HTTPTimeoutAdapter class derives from\ncachecontrol.CacheControlAdapter instead of request.adapters.HTTPAdapter is\nthe only thing you need to do to activate the cache.\nOf course, this means the services that you are calling should implement this caching\nbehavior by adding the proper ETag support.\nIt's not possible to implement a generic solution for this because the cache logic depends on\nthe nature of the data your service is managing.",
      "content_length": 2113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 130 ]\nThe rule of thumb is to version each resource and change that version every time the data\nchanges. In the following example, the Flask app uses the current server time to create ETag\nvalues associated with users' entries. The ETag value is the current time since the epoch in\nmilliseconds and is stored in the modified field.\nThe get_user() method returns a user entry from _USERS and sets the ETag value with\nresponse.set_etag. When the view gets some calls, it also looks for the If-None-Match\nheader to compare it to the user's modified field, and returns a 304 response if it matches:\n    import time\n    from flask import Flask, jsonify, request, Response, abort\n    app = Flask(__name__)\n    def _time2etag(stamp=None):\n        if stamp is None:\n            stamp = time.time()\n        return str(int(stamp * 1000))\n    _USERS = {'1': {'name': 'Tarek', 'modified': _time2etag()}}\n    @app.route('/api/user/<user_id>', methods=['POST'])\n    def change_user(user_id):\n        user = request.json\n        # setting a new timestamp\n        user['modified'] = _time2etag()\n        _USERS[user_id] = user\n        resp = jsonify(user)\n        resp.set_etag(user['modified'])\n        return resp\n    @app.route('/api/user/<user_id>')\n    def get_user(user_id):\n        if user_id not in _USERS:\n            return abort(404)\n        user = _USERS[user_id]\n        # returning 304 if If-None-Match matches\n        if user['modified'] in request.if_none_match:\n            return Response(status=304)\n        resp = jsonify(user)\n        # setting the ETag\n        resp.set_etag(user['modified'])\n        return resp",
      "content_length": 1652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 131 ]\n    if __name__ == '__main__':\n        app.run()\nThe change_user() view sets a new modified value when the client POST a user. In the\nfollowing client session, we're changing the user and making sure we get a 304 response\nwhen providing the new ETag value:\n$ curl http://127.0.0.1:5000/api/user/1\n{\n  \"modified\": \"1486894514360\",\n  \"name\": \"Tarek\"\n}\n$ curl -H \"Content-Type: application/json\" -X POST -d\n'{\"name\":\"Tarek\",\"age\":40}' http://127.0.0.1:5000/api/user/1\n{\n  \"age\": 40,\n  \"modified\": \"1486894532705\",\n  \"name\": \"Tarek\"\n}\n$ curl http://127.0.0.1:5000/api/user/1\n{\n  \"age\": 40,\n  \"modified\": \"1486894532705\",\n  \"name\": \"Tarek\"\n}\n$ curl -v -H 'If-None-Match: 1486894532705'\nhttp://127.0.0.1:5000/api/user/1\n< HTTP/1.0 304 NOT MODIFIED\nThis demonstration is a toy implementation that might not work well in production because\nrelying on a server clock to store ETag values means you are sure that the clock is never set\nback in time and that if you have several servers, their clocks are all synchronized with a\nservice such as ntpdate.\nThere's also the problem of race conditions if two requests change the same entry within the\nsame millisecond. Depending on your app, maybe it's not an issue, but maybe it's a big one.\nA cleaner option is to have the modified field handled by your database system directly and\nmake sure its changes are done in serialized transactions.",
      "content_length": 1418,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 132 ]\nSome developers use hash functions for their ETag value because it's easy to compute in a\ndistributed architecture and it doesn't introduce any of the problems timestamps have. But\ncalculating a hash has a CPU cost, and it means you need to pull the whole entry to do it--so\nit might be as slow as if you were sending back the actual data. That said, with a dedicated\ntable in your database for all your hashes, you can probably come up with a solution that\nmakes your 304 response very fast to return.\nAs we said earlier, there's no generic solution to implement an efficient HTTP cache logic--\nbut it's worth performing it if your client is doing a lot of reads on your service.\nWhen you have no choice but to send some data back, there are several ways to make it as\nefficient as possible, as we'll see in the next section.\nImproving data transfer\nJSON is quite verbose. Verbosity is great when you need to interact with your data.\nEverything comes as clear text and is as easy to read as plain Python dictionary and lists.\nBut sending HTTP requests and responses with JSON payloads can add some bandwidth\noverhead in the long run. Serializing and deserializing data from Python objects to JSON\nstructures also adds a bit of CPU overhead.\nYou can reduce the size of data transfers and speed up processing times using compression\nor switching to binary payloads.\nGZIP compression\nThe first simple thing you can do to reduce the bandwidth is to use GZIP compression, so\neverything that is sent over the wire gets smaller. Web servers such as Apache or nginx\nprovide native support to compress responses on the fly, and it's better to avoid\nimplementing your ad hoc compression at the Python level.\nFor example, this nginx configuration will enable GZIP compression for any response\nproduced by the Flask app on port 5000, with an application/json content type:\n    http {\n        gzip  on;\n        gzip_types application/json;\n        gzip_proxied      any;\n        gzip_vary on;\n        server {",
      "content_length": 2037,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 133 ]\n            listen       80;\n            server_name  localhost;\n            location / {\n                proxy_pass http://localhost:5000;\n            }\n        }\nFrom the client-side, making an HTTP request to the nginx server at localhost:8080\nproxying for the application at localhost:5000 with an Accept-Encoding: gzip\nheader will trigger the compression:\n$ curl http://localhost:8080/api -H \"Accept-Encoding: gzip\"\n<some binary output>\nIn Python, request responses will automatically decompress responses that are gzip\nencoded, so you don't have to worry about doing it when your service is calling another\nservice. Unzipping the data adds some processing, but Python's gzip module relies on the\nzlib (h t t p ://w w w . z l i b . n e t /), which is very fast (and massively spiffy).\n>>> import requests\n>>> requests.get('http://localhost:8080/api', headers={'Accept-Encoding':\n'gzip'}).json()\n{'Hello': 'World!', u'result': 'OK'}\nTo compress the data you're sending to the server, you can use the gzip module and\nspecify a Content-Encoding header:\n>>> import gzip, json, requests\n>>> data = {'Hello': 'World!', 'result': 'OK'}\n>>> data = bytes(json.dumps(data), 'utf8')\n>>> data = gzip.compress(data)\n>>> headers = {'Content-Encoding': 'gzip'}\n>>> requests.post('http://localhost:8080/api',\n...               headers=headers,\n...               data=data)\n<Response [200]>\nIn that case, however, you will get the zipped content in your Flask application, and you\nwill need to decompress it in your Python code unless you implement something in nginx\nwith Lua to handle it. Apache, on the other hand, can decompress it for you with the\nmode_deflate module and its SetInputFilter option.",
      "content_length": 1731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 134 ]\nTo summarize, setting up GZIP compression for all your service responses is a no-brainer\nwith nginx or Apache, and your Python client can benefit from it by setting the right\nheader. Handling GZIP compression in HTTP requests is a little trickier because if you don't\nuse Apache, you need to implement decompression of incoming data in Python code or\nsomewhere else.\nIf you want to further reduce the size on HTTP request/response payloads, another option\nis to switch to binary payloads rather than JSON payloads compressed with gzip . That\nway, you don't have to deal with unzipping the data and will get a speedup. But we'll see\nthat the compression is not as good.\nBinary payloads\nWhile it's usually not relevant, if your microservice deals with a lot of data, using an\nalternative format can be an attractive option to increase performances and decrease the\nrequired network bandwidth without having to rely on GZIP.\nThe two widely used binary formats out there are Protocol Buffers (protobuf) and\nMessagePack.\nProtocol buffers (h t t p s ://d e v e l o p e r s . g o o g l e . c o m /p r o t o c o l - b u f f e r s ) requires you to \ndescribe the data that's being exchanged into some schema that will be used to index the\nbinary content.\nIt adds quite some work because all data that's transferred will need to be described in a\nschema and you will need to learn a new Domain Specific Language (DSL).\nThe following example is taken from the protobuf documentation:\n    package tutorial;\n    message Person {\n      required string name = 1;\n      required int32 id = 2;\n      optional string email = 3;\n      enum PhoneType {\n        MOBILE = 0;\n        HOME = 1;\n        WORK = 2;\n      }\n      message PhoneNumber {\n        required string number = 1;",
      "content_length": 1800,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 135 ]\n        optional PhoneType type = 2 [default = HOME];\n      }\n      repeated PhoneNumber phones = 4;\n    }\n    message AddressBook {\n      repeated Person people = 1;\n    }\nNeedless to say, it's not very Pythonic and looks more like a database schema. We could\nargue that describing the data that gets transferred is good practice, but it could become a\nbit redundant with the Swagger definition if the microservice uses that.\nMessagePack (h t t p ://m s g p a c k . o r g /), on the other hand, is schemaless and can compress\nand uncompress your data by just calling a function.\nIt's a simple alternative to JSON, and has implementations in most languages. The msgpack\npython library (installed using the pip install msgpack-python command) offers the\nsame level of integration as JSON:\n>>> import msgpack\n>>> data = {\"this\": \"is\", \"some\": \"data\", 1: 2}\n>>> msgpack.dumps(data)\nb'x83x01x02xa4thisxa2isxa4somexa4data'\n>>> msgpack.loads(msgpack.dumps(data))\n{1: 2, b'this': b'is', b'some': b'data'}\nNotice that the strings are converted into binaries when the data is\nserialized then deserialized back with the default serializer. This is\nsomething to take into account if you need to keep the original types.\nClearly, using MessagePack is quite simple compared to Protobuf--but which one is the\nfaster and provides the best compression ratio depends a lot on your data. In some rare\ncases, plain JSON might be even quicker to serialize than a binary format.\nIn terms of compression, you can expect a 10% to 20% compression with MessagePack, but\nif your JSON contains a lot of strings--which is often the case in microservices--GZIP will do\na much better job.",
      "content_length": 1698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 136 ]\nIn the following example, a huge JSON payload of 87k containing a lot of strings, is\nconverted using MessagePack and then gzipped in both cases:\n>>> import json, msgpack\n>>> with open('data.json') as f:\n...     data = f.read()\n...\n>>> python_data = json.loads(data)\n>>> len(json.dumps(python_data))\n88983\n>>> len(msgpack.dumps(python_data))\n60874\n>>> len(gzip.compress(bytes(json.dumps(data), 'utf8')))\n5925\n>>> len(gzip.compress(msgpack.dumps(data)))\n5892\nUsing MessagePack reduces the size of the payload by quite a lot, but GZIP is crushing it\nby making it 15 times smaller with both JSON and MessagePack payloads!\nIt's clear that whatever format you are using, the best way to reduce the payload sizes is to\nuse GZIP--and if your web server does not deal with decompression, it's straightforward in\nPython thanks to gzip.uncompress().\nNow, between using MessagePack and JSON, the binary format is usually faster and is\nmore Python friendly. For instance, if you pass a Python dictionary with integer keys, JSON\nwill convert them into strings while MessagePack will do the right thing:\n>>> import msgpack, json\n>>> json.loads(json.dumps({1: 2}))\n{'1': 2}\n>>> msgpack.loads(msgpack.dumps({1: 2}))\n{1: 2}\nBut there's also the problem of date representations: DateTime objects are not directly\nserializable in JSON and MessagePack, so you need to make sure you convert them.\nIn any case, in a world of microservices where JSON is the most accepted standard, sticking\nwith string keys and taking care of dates are minor annoyances to stick with a universally\nadopted standard.\nUnless all your services are in Python with well-defined structures, and\nyou need to speed up the serialization steps as much as possible, it's\nprobably simpler to stick with JSON.",
      "content_length": 1796,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 137 ]\nPutting it together\nWe will quickly recall what we covered in this section about performing synchronous calls:\nRequests can be used as the HTTP client to call other services. It offers all the\nfeatures necessary to deal with timeouts, errors, and has its pool of connectors.\nGoing multithread can improve your microservice's performance when it's\ncalling other services, since Flask is a synchronous framework, but it's dangerous.\nSolutions such as Gevent can be investigated.\nImplementing HTTP cache headers is a great way to speed up repeated requests\nfor data.\nGZIP compression is an efficient way to lessen the size of requests and responses\nand is easy to set up.\nBinary protocols are an attractive alternative to plain JSON, but might not be that\nuseful.\nThe next section will focus on asynchronous calls; everything your microservice can do that\ngoes beyond the request-response pattern.\nAsynchronous calls\nIn microservice architectures, asynchronous calls play a fundamental role when a process\nthat used to be performed in a single application now implicates several microservices.\nAsynchronous calls can be as simple as a separate thread or process within a microservice\napp, that's getting some work to be done and perform it without interfering with the HTTP\nrequest-responses round trips that are happening at the same time.\nBut doing everything directly from the same Python process is not very robust. What\nhappens if the process crashes and gets restarted? How do we scale background tasks if\nthey are built like that?\nIt's much more reliable to send a message that gets picked by another program, and let the\nmicroservice focus on its primary goal, which is to serve responses to clients.\nIn the previous chapter, we looked at how Celery could be used to build a microservice that\ngets some work from a message broker like Redis or RabbitMQ. In that design, the Celery\nworker blocks until a new message is added to the Redis queue.\nBut there are other ways to exchange messages between services that are not necessarily a\nworker blocking on a queue.",
      "content_length": 2106,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 138 ]\nTask queues\nThe pattern used by Celery workers is a push-pull tasks queue. One service pushes\nmessages into a specific queue, and some workers pick them up from the other end and \nperform an action on them. Each task goes to a single worker. Consider the following\ndiagram:\nThere's no bidirectional communication. The sender just deposits a message in the queue\nand leaves. The next available worker gets the next message.\nThis blind, unidirectional message passing is perfect when you want to perform some\nasynchronous parallel tasks, and that makes it easy to scale.\nPlus, once the sender has confirmed that the message was added in the broker, we can have\nmessage brokers such as RabbitMQ offer some message persistence. In other words, if all\nworkers go offline, we don't loose the messages that are in the queue.",
      "content_length": 857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 139 ]\nTopic queues\nA variation of the task queue pattern is the topic pattern. In that case, instead of having\nworkers blindly picking every message that is added to one or several queues, they subscribe\nto specific topics. A topic is just a label on a message, and workers can decide to filter the \nmessages they pick from the queue so that they match the topic.\nIn our microservices, this means we can have specialized workers that all register to the\nsame messaging broker and get a subset of the messages that are added to it.\nCelery is an excellent tool for building tasks queues, however, for more complex messaging,\nwe need to use another tool:\nTo implement complex messaging pattern, the good news is that we can use a Rabbit MQ\nmessage broker who still works with Celery and interacts with another library.",
      "content_length": 849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 140 ]\nTo install a RabbitMQ broker, you can look at the download page at h t t p ://w w w . r a b b i t m q\n. c o m /d o w n l o a d . h t m l and get started from there. A RabbitMQ broker is a TCP server that \nmanages queues internally and dispatches messages from publishers to subscribers via RPC\ncalls. Using it with Celery is just a small portion of what this system can offer.\nRabbitMQ implements the Advanced Message Queuing Protocol (AMQP). This protocol,\ndescribed in h t t p ://w w w . a m q p . o r g / is a complete standard that has been developed for\nyears by majority of the companies in the industry.\nAMQP is organized into three concepts: queues, exchanges, and bindings:\nA queue is a recipient that holds messages and waits for consumers to pick them\nAn exchange is an entry point for publishers to add new messages to the system\nA binding defines how messages are routed from exchanges to queues\nFor our topic queue, we need to set one exchange, so RabbitMQ accepts new messages, and\nall the queues we want for workers to pick messages. In the middle, we want to route the\nmessages to the different queues depending on the topics, using a binding.\nLet's say we have two workers, one that wants to receive messages about races and another\none about training plans.\nEvery time a message is about a race, it gets labeled race.id, where race is a fixed prefix,\nand id is a unique ID for the race. Similarly, for training plans, it is training.id.\nUsing the rabbitmqadmin command line that gets installed with RabbitMQ, we can create\nall the necessary parts:\n$ rabbitmqadmin declare exchange name=incoming type=topic\nexchange declared\n$ rabbitmqadmin declare queue name=race\nqueue declared\n$ rabbitmqadmin declare queue name=training\nqueue declared\n$ rabbitmqadmin declare binding source=\"incoming\" destination_type=\"queue\"\ndestination=\"race\" routing_key=\"race.*\"\nbinding declared\n$ rabbitmqadmin declare binding source=\"incoming\" destination_type=\"queue\"\ndestination=\"training\" routing_key=\"training.*\"\nbinding declared",
      "content_length": 2068,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 141 ]\nIn this setup, every message is sent to RabbitMQ, wherein, if the topic starts with race., it\nwill be pushed into the race queue, and the training. ones will end up in the training\nqueue.\nTo interact with RabbitMQ in the code, we can use Pika (h t t p s ://p i k a . r e a d t h e d o c s . i o ) a\nPython RPC client that implements all the RPC endpoints a Rabbit service publishes.\nEverything we do with Pika can be done on the command line using\nrabbitmqadmin. You can directly get the status of all parts of the system,\nsend and receive messages, and check what's in a queue. It's an excellent\nway to experiment with your messaging setup.\nThe following script shows how to publish two messages in RabbitMQ in the incoming\nexchange. One about Race 34 and one about Training 12:\n    from pika import BlockingConnection, BasicProperties\n    # assuming there’s a working local RabbitMQ server with a working\n      guest/guest account\n    def message(topic, message):\n        connection = BlockingConnection()\n        try:\n            channel = connection.channel()\n            props = BasicProperties(content_type='text/plain',\n                                    delivery_mode=1)\n            channel.basic_publish('incoming', topic, message, props)\n        finally:\n            connection.close()\n    # sending a message about race 34\n    message('race.34', 'We have some results!')\n    # training 12\n    message('training.12', \"It's time to do your long run\")\nThese RPC calls will end up adding one message respectively in the race and training\nqueues. A Race worker script that waits for news about races would look like this:\n    import pika\n    def on_message(channel, method_frame, header_frame, body):\n        race_id = method_frame.routing_key.split('.')[-1]\n        print('Race #%s: %s' % (race_id, body))\n        channel.basic_ack(delivery_tag=method_frame.delivery_tag)\n    print(\"Race NEWS!\")\n    connection = pika.BlockingConnection()\n    channel = connection.channel()",
      "content_length": 2021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 142 ]\n    channel.basic_consume(on_message, queue='race')\n    try:\n        channel.start_consuming()\n    except KeyboardInterrupt:\n        channel.stop_consuming()\n    connection.close()\nNotice that Pika is sends back an ACK to RabbitMQ about the message, so it can be safely\nremoved from the queue once the worker has succeeded.\nAn example of the output is as follows:\n$ bin/python pika_worker.py\nRace NEWS!\nRace #34: b'We have some results!'\nAMQP offers many patterns you can investigate to exchange messages. The tutorial page at\nh t t p ://w w w . r a b b i t m q . c o m /g e t s t a r t e d . h t m l has many examples, and they are all\nimplemented using Python and Pika.\nTo integrate these examples in our microservices, the publisher part is straightforward.\nYour Flask application can create a synchronous connection to RabbitMQ using\npika.BlockingConnection and send messages through it. Projects such as pika-pool (h t\nt p s ://g i t h u b . c o m /b n i n j a /p i k a - p o o l ) implement simple connection pools so you can\nmanage RabbitMQ channels without having to connect/disconnect every time you are\nsending something through RPC.\nThe consumers, on the other hand, are trickier to integrate into microservices.\nPika can be embedded into an event loop running in the same process as the Flask\napplication, and trigger a function when a message is received. That would be okay in an\nasynchronous framework, but for a Flask application, you will need to execute the code that\nuses the Pika client in a separate thread or process. The reason for this is that the event loop\nwould be blocked every time a request is received in Flask.\nThe most reliable way to use a Pika client in order to interact with RabbitMQ is to have a\nstandalone Python application that consumes messages on behalf of your Flask\nmicroservice and performs synchronous HTTP calls. It adds yet another intermediary, but\nwith the ability to acknowledge that a message was successfully received, and with all the\nRequests tricks we learned earlier in this chapter, we can build a reliable bridge:\n    import pika\n    import requests\n    from requests.exceptions import ReadTimeout, ConnectionError",
      "content_length": 2214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 143 ]\n    FLASK_ENDPOINT = 'http://localhost:5000/event'\n    def on_message(channel, method_frame, header_frame, body):\n        message = {'delivery_tag': method_frame.delivery_tag,\n                   'message': body}\n        try:\n            res = requests.post(FLASK_ENDPOINT, json=message,\n                                timeout=1.)\n        except (ReadTimeout, ConnectionError):\n            print('Failed to connect to %s.' % FLASK_ENDPOINT)\n            # need to implement a retry here\n            return\n        if res.status_code == 200:\n            print('Message forwarded to Flask')\n            channel.basic_ack(delivery_tag=method_frame.delivery_tag)\n    connection = pika.BlockingConnection()\n    channel = connection.channel()\n    channel.basic_consume(on_message, queue='race')\n    try:\n        channel.start_consuming()\n    except KeyboardInterrupt:\n        channel.stop_consuming()\n    connection.close()\nThis script will perform HTTP calls on Flask with the messages delivered in the queue.\nThere's also a RabbitMQ plugin that does something similar by pushing\nmessages to HTTP endpoints, but isolating this bridge into our little script\noffers more potential if we need to add logic-specific code. From a\nrobustness and performance point of view, it's also probably better to\navoid integrating HTTP pushes inside RabbitMQ.\nIn Flask, the /event endpoint can be a classical view:\n    from flask import Flask, jsonify, request\n    app = Flask(__name__)\n    @app.route('/event', methods=['POST'])\n    def event_received():\n        message = request.json['message']\n        # do something...\n        return jsonify({'status': 'OK'})",
      "content_length": 1681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 144 ]\n    if __name__ == '__main__':\n        app.run()\nPublish/subscribe\nThe previous pattern has workers that handle specific topics of messages, and the messages\nconsumed by a worker are completely gone from the queue. We even added code to\nacknowledge that the message was consumed.\nWhen you want a message to be published to several workers, the Publish/Subscribe\n(pubsub) pattern needs to be used.\nThis pattern is the basis for building a general event system and is implemented exactly like\nthe previous one where there is one exchange and several queues. The difference is that the\nexchange part has a fanout type.\nIn that setup, every queue that you bind to a fanout exchange will receive the same\nmessage.\nWith a pubsub in place, you can broadcast messages to all your microservices if you need\nto.\nRPC over AMQP\nAMQP also implements a synchronous request/response pattern, which means that we\ncould use RabbitMQ instead of the usual HTTP JSON calls to have our microservice directly\ninteract.\nThis pattern is a very appealing way to have two microservices communicate directly with\neach other. Some frameworks, such as Nameko (h t t p ://n a m e k o . r e a d t h e d o c s . i o ) are \nusing it to build microservices.\nBut the benefits of using RPC over AMQP rather than REST or RPC over HTTP are not that\nobvious unless the communication channel you want to set up is specific and maybe not\npart of the published API. Sticking with a single API is probably better to keep your\nmicroservices as simple as possible.",
      "content_length": 1559,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 145 ]\nPutting it together\nIn this section, we learned the following about asynchronous section:\nAsynchronous calls should be used every time a microservice can execute some\nwork out of band. There's no good reason to block a request if what you are\ndoing is not utilized in the response.\nCelery is a nice way to do some background processing.\nService-to-service communication is not always limited to task queues.\nSending events around is a good way to prevent services inter-dependencies.\nWe can build a full event system around a broker such as Rabbit MQ to make our\nmicroservices interact with each other via messages.\nPika can be used to coordinate all the message passing.\nTesting\nAs we learned in Chapter 3, Coding, Testing and Documenting - the Virtuous Cycle, the biggest\nchallenge when writing functional tests for a service that calls other services is to isolate all\nnetwork calls.\nIn this section, we'll see how we can mock synchronous calls made with Requests, and\nasynchronous calls for Celery workers and other asynchronous processes.\nMocking synchronous calls\nIf you are using Requests to perform all the calls--or you are using a library that is based on\nRequests and that does not customize it too much, this isolation work is easier to do, thanks\nto the transport adapters we saw earlier in this chapter.\nThe requests-mock project (h t t p s ://r e q u e s t s - m o c k . r e a d t h e d o c s . i o ) implements an \nadapter that will let you mock network calls in your tests.\nEarlier in this chapter, we saw an example of a Flask app that was an HTTP endpoint to\nserve some content on its /api endpoint.\nThat application used a Request session that was created by a setup_connector()\nfunction and retrieved in a view by a get_connector() function.",
      "content_length": 1802,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 146 ]\nIn the following test, we're mounting the requests_mock adapter into that session by\ncalling session.mount() with a fresh requests_mock.Adapter() instance:\n    import json\n    import unittest\n    from flask_application import app, get_connector\n    from flask_webtest import TestApp\n    import requests_mock\n    class TestAPI(unittest.TestCase):\n        def setUp(self):\n            self.app = TestApp(app)\n            # mocking the request calls\n            session = get_connector(app)\n            self.adapter = requests_mock.Adapter()\n            session.mount('http://', self.adapter)\n        def test_api(self):\n            mocked_value = json.dumps({'some': 'data'})\n            self.adapter.register_uri('GET', 'http://127.0.0.1:5000\n                                      /api', text=mocked_value)\n            res = self.app.get('/api')\n            self.assertEqual(res.json['result']['some'], 'data')\nUsing this adapter offers the ability to manually register responses through register_uri\nfor some given endpoints on the remote service (here h t t p ://127. 0. 0. 1:5000/a p i ). The\nadapter will intercept the call and immediately return the mocked value.\nIn the test_api() test, it will let us try out the application view and make sure it uses the\nprovided JSON data when it calls the external service.\nThe requests-mock will also let you match requests using regular expressions, so it's a\npretty powerful adapter to use in your tests to avoid a network dependency when they run.\nThat said, mocking responses from other services is still a fair amount of work and quite\npainful to maintain. It also means you need to keep an eye on how the other services are\nevolving over time, so your tests are not based on a mock that's not a reflection of the real\nAPI anymore.\nUsing mocks is encouraged to build good functional tests coverage, but make sure you are\ndoing integration tests as well, where the service is tested in a deployment where it calls\nother services for real.",
      "content_length": 2026,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 147 ]\nMocking asynchronous calls\nIf your application sends or receives calls asynchronously, setting up some testing is a little\nbit harder than for synchronous calls.\nAsynchronous calls mean that the application is sending something somewhere and don't\nexpect a result immediately--or just forgets about it altogether.\nIt can also means that the application may react to an event that is sent to it, as in the\ndifferent patterns we've looked at based around Pika.\nMocking Celery\nIf you are building tests for Celery workers, the simplest way to run your tests is to use a\nreal Redis server. A Redis server is straightforward to run on any platform. Even Travis-CI\nwill let you run one. So, instead of adding a lot of work to mock all the interactions, your\nFlask code will have with Redis to send some jobs to workers, and post them for real.\nUsing a real broker means that you can run your Celery worker in your test, just to validate\nthat the app sent the proper job payloads. Celery provides a pytest test fixture that will run\nfor you in a separate thread and shut it down once the test is over.\nThis is done by implementing a few fixtures to configure Celery to use Redis and to point\nyour tests's tasks. The first step is to create a tasks.py file inside your tests directory which\ncontains your Celery tasks.\nThe following is an example of such a file. Notice that we don't create a Celery instance--but\nuse the @shared_tasks decorator to mark functions as being celery tasks:\n    from celery import shared_task\n    import unittest\n    @shared_task(bind=True, name='echo')\n    def echo(app, msg):\n        return msg",
      "content_length": 1657,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 148 ]\nThis module implements a Celery task named echo that will echo back a string. To\nconfigure pytest to use it, you need to implement the celery_config and\ncelery_includes fixtures:\n    import pytest\n    @pytest.fixture(scope='session')\n    def celery_config():\n        return {\n            'broker_url': 'redis://localhost:6379',\n            'result_backend': 'redis://localhost:6379'\n        }\n    @pytest.fixture(scope='session')\n    def celery_includes():\n        return ['myproject.tests.tasks']\nThe celery_config function is used to pass all the options to create a Celery worker, and\ncelery_includes will just import the list of modules it returns. In our case, it will register\nthe echo task in the Celery tasks registry.\nFrom there, your tests can use the echo task, and have the worker get called for real:\n    from celery.execute import send_task\n    class TestCelery(unittest.TestCase):\n        @pytest.fixture(autouse=True)\n        def init_worker(self, celery_worker):\n            self.worker = celery_worker\n        def test_api(self):\n            async_result = send_task('echo', ['yeah'], {})\n            self.assertEqual(async_result.get(), 'yeah')\nNotice that, here, we've used send_task() to trigger the execution of the task.\nThis function can run any task that was registered as a broker by Celery, as long as the task\nhas a unique name.\nIt's good practice to name all your tasks and to make sure these names are unique\nthroughout all your microservices.\nThe reason is that when a microservice wants to run a task from a worker that is its\nmicroservice, we don't want to have to import that worker code just to get the task function.",
      "content_length": 1692,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 149 ]\nIn the following example, the echo task is running in a standalone microservice and we can\ntrigger it via a send_task() call just by knowing the task name--no need to import the\ncode; every interaction happens through Redis:\n>>> import celery\n>>> redis = 'redis://localhost:6379'\n>>> app = Celery(__name__, backend=redis, broker=redis)\n>>> f = app.send_task('echo', ['meh'])\n>>> f.get()\n'meh'\nBack to your testing, if your tests are mocking some Celery workers, make sure the remote\napplication that implements the tasks has a name for each one of them, and make sure that\nthe application you are testing uses send_task() throughout its code.\nThat way, your Celery fixtures will magically mock the workers for your app.\nLastly, the application will probably not wait for the Celery worker to return the result\nsynchronously--so you will need to inspect what the test worker has done after the API call.\nMocking other asynchronous calls\nIf you do some messaging with Pika and RabbitMQ, the Pika library directly uses the socket\nmodule to interact with the server, and that makes it painful to mock because we would\nneed to track what data is sent and received over the wire.\nLike for Celery, you could just run a local RabbitMQ server for your tests--Travis-CI also\nmaking it available (h t t p s ://d o c s . t r a v i s - c i . c o m /u s e r /d a t a b a s e - s e t u p /).\nSending messages, in that case, is done as usual, and you can create a script that picks them\nin the Rabbit queues to verify them.\nWhen you need to test a process where an event is received from RabbitMQ, if that happens\nvia an HTTP call, as in our little AMQP-to-HTTP bridge, you can simply manually trigger\nthe events from the tests.\nWhat's important is to make sure you can run your tests without depending on other\nmicroservices. But dependencies on messaging servers such as Redis or RabbitMQ are not a\nproblem as long as you can run them in a dedicated testing environment.",
      "content_length": 1996,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "Interacting with Other Services\n[ 150 ]\nSummary\nIn this chapter, we've looked at how a service can interact with other services\nsynchronously, by using a Requests session, and asynchronously, by using Celery workers\nor more advanced messaging patterns based on RabbitMQ.\nWe've also looked at ways to test a service in isolation by mocking other services, but\nwithout mocking the message brokers themselves.\nTesting each service in isolation is useful, but when something goes wrong, it's hard to\nknow what happened, in particular, if the bug happens in a series of asynchronous calls.\nIn that case, tracking what's going with a centralized logging system helps a lot. The next\nchapter will explain how we can tool our microservices to follow their activities.",
      "content_length": 759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "6\nMonitoring Your Services\nIn the previous chapter, we tested services that are interacting with each other in isolation.\nBut when something bad happens in a real deployment, we need to have a global overview\nof what's going on. For example, when a microservice calls another one which in turn calls a\nthird one, it can be hard to understand which one failed. We need to be able to track down\nall the interactions that a particular user had with the system that led to a problem.\nPython applications can emit logs to help you debug issues, but jumping from one server to\nanother to gather all the information you need to understand the problem can be hard.\nThankfully, we can centralize all the logs to monitor a distributed deployment.\nContinuously monitoring services are also important to assert the health of the whole\nsystem and follow how everything behaves. This involves answering questions such as, Is\nthere a service that's dangerously approaching 100% of RAM usage?, How many requests\nper minute is that particular microservice doing? Do we have too many servers deployed\nfor that API, can we remove a few boxes to reduce the price? Did a change we just deploy\naffect performance adversely?\nTo be able to answer questions like these continuously, every microservice we're deploying\nneeds to be tooled to report primary metrics to a monitoring system.\nThis chapter is organized into two main sections:\nCentralizing logs\nPerformance metrics\nBy the end of the chapter, you will have a full understanding of how to set up your\nmicroservices to monitor them.",
      "content_length": 1564,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 152 ]\nCentralizing logs\nPython comes with the logging package, which lets you stream logs to a variety of places\nincluding standard out, rotating log files, syslog, or a TCP or UDP socket.\nThere's even an SMTP backend. In the following example, the email_errors decorator\nwill send an email every time an exception is happening in the decorated function. Note\nthat the handler is doing a telnet session with the SMTP server to send the email, so if\nthere's any issue during that session, you might get a second exception when the\nlogger.exception() function is called:\n    import logging\n    from logging.handlers import SMTPHandler\n    host = \"smtp.example.com\", 25\n    handler = SMTPHandler(mailhost=host, fromaddr=\"tarek@ziade.org\",\n                          toaddrs=[\"tarek@ziade.org\"],\n                          subject=\"Service Exception\")\n    logger = logging.getLogger('theapp')\n    logger.setLevel(logging.INFO)\n    logger.addHandler(handler)\n    def email_errors(func):\n        def _email_errors(*args, **kw):\n            try:\n                return func(*args, **kw)\n            except Exception:\n                logger.exception('A problem has occured')\n                raise\n    return _email_errors\n    @email_errors\n    def function_that_raises():\n        print(i_dont_exist)\n    function_that_raises()",
      "content_length": 1344,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 153 ]\nIf the call works, an email will be received with the full traceback enclosed.\nPython has a lot of handlers built-in the logging package; refer to h t t p s\n://d o c s . p y t h o n . o r g /3/l i b r a r y /l o g g i n g . h a n d l e r s . h t m l .\nLogging to the standard output or a log file is fine when you are developing your service,\nbut as we said earlier, that won't scale in a distributed system.\nSending emails on errors is an improvement, but with high-traffic microservices, it's\ncommon to get the same exception a thousand times an hour. If you are spamming an email\nbox with a lot of emails, your server IP will get blacklisted by the SMTP server and your\nservice will be unresponsive because it will be busy sending out lots of emails.\nWe need something better for a distributed system. A way to collect logs from all\nmicroservices with the least overhead possible, and some user interface to visualize them.\nThere are several existing systems to centralize logs generated by Python applications. Most\nof them can receive logs in HTTP or UDP payloads, with a preference for the latter because\nit reduces the overhead when your app sends them.\nSentry (h t t p s ://s e n t r y . i o /) is a well-known tool in the Python community for\ncentralizing error logs and provides a nice UI to deal with tracebacks. When a problem\noccurs in a service, Sentry can detect and aggregate the errors. The UI has a little resolution\nworkflow that will let people deal with the problem.\nBut Sentry is focused on errors and is not well suited for general logging. If you want to get\nlogs other than errors, you need to use something else.\nAnother open-source solution is Graylog (h t t p ://g r a y l o g . o r g ), which is a general logging\napplication that comes with a powerful search engine based on Elasticsearch (h t t p s ://w w w\n. e l a s t i c . c o /) where the logs are stored. MongoDB (h t t p s ://w w w . m o n g o d b . c o m /) is also\nused to store application data.\nGraylog can receive any logs via its custom logging format or alternative formats, such as\nplain JSON. It has a built-in collector or can be configured to work with collectors such as\nfluentd (h t t p ://w w w . f l u e n t d . o r g /).",
      "content_length": 2256,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 154 ]\nSetting up Graylog\nA Graylog server is a Java application that uses MongoDB as its database, and stores all the\nlogs it receives into Elasticsearch. Needless to say, a Graylog stack has quite a lot of moving\nparts that are hard to set up and administrate, and you will need some dedicated people if\nyou deploy it yourself.\nA typical production setup will use a dedicated Elastic Search cluster and several Graylog\nnodes with a MondoDB instance on each. You can have a look at Graylog architecture\ndocumentation (h t t p ://d o c s . g r a y l o g . o r g /e n /l a t e s t /p a g e s /a r c h i t e c t u r e . h t m l ) for\nmore details.\nAn excellent way to try out Graylog is to use its Docker (h t t p s ://d o c s . d o c k e r . c o m ) image,\nas described here in h t t p ://d o c s . g r a y l o g . o r g /e n /l a t e s t /p a g e s /i n s t a l l a t i o n /d o c k e r\n. h t m l .\nChapter 10, Containerized Services, explains how to use Docker for\ndeploying microservices, and gives the basic knowledge you need to build\nand run Docker images.\nLike Sentry, Graylog is backed by a commercial company, which offers some hosting\nsolutions. Depending on your project's nature and size, it can be a good solution to avoid\nmaintaining this infrastructure yourself. For instance, if you run a commercial project that\nhas a Service-Level Agreement (SLA), operating an Elasticsearch cluster smoothly is not a\nsmall task and will require some attention.\nBut for projects that don't generate a lot of logs, or if having the log management down for a\nbit is not the end of the world, then running your Graylog stack can be a good solution.\nFor this chapter, we'll just use the Docker image and docker-compose (a tool that can run\nand bind several docker images from one call) and a minimal set up to demonstrate how\nour microservices can interact with Graylog.",
      "content_length": 1892,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 155 ]\nTo run a Graylog service locally, you need to have Docker installed (see Chapter 10,\nDockerizing your service) and use the following Docker compose configuration (taken from\nGraylog documentation):\nversion: '2'\nservices:\n  some-mongo:\n    image: \"mongo:3\"\n  some-elasticsearch:\n    image: \"elasticsearch:2\"\n    command: \"elasticsearch -Des.cluster.name='graylog'\"\n  graylog:\n    image: graylog2/server:2.1.1-1\n    environment:\n      GRAYLOG_PASSWORD_SECRET: somepasswordpepper\n      GRAYLOG_ROOT_PASSWORD_SHA2:\n8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918\n      GRAYLOG_WEB_ENDPOINT_URI: http://127.0.0.1:9000/api\n    links:\n      - some-mongo:mongo\n      - some-elasticsearch:elasticsearch\n    ports:\n      - \"9000:9000\"\n      - \"12201/udp:12201/udp\"\nIf you save that file in a docker-compose.yml file and run docker-compose up in the\ndirectory containing it, Docker will pull the MongoDB, Elasticsearch and Graylog images\nand run them.\nOnce it's running, you can reach the Graylog dashboard at http://localhost:9000 in\nyour browser, and access it with admin as the user and password.\nThe next step is to go to System | Inputs to add a new UDP input so Graylog can receive\nour microservices logs.",
      "content_length": 1250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 156 ]\nThis is done by launching a new GELF UDP input on port 12012, as shown in the following\nscreenshot:",
      "content_length": 132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 157 ]\nOnce the new input is in place, Graylog will bind the UDP port 12201 and will be ready to\nreceive data. The docker-compose.yml file has that port exposed for the Graylog image,\nso your Flask applications can send data via the localhost.\nIf you click on Show Received Messages for the new input, you will get a search result\ndisplaying all the collected logs:\nCongratulations! You are now ready to receive logs in a centralized place and watch them\nlive in the Graylog dashboards.\nSending logs to Graylog\nTo send logs to Graylog from Python, you can use Graypy (h t t p s ://g i t h u b . c o m /s e v e r b /g\nr a y p y ), which converts Python logs to the Graylog Extended Log Format (GELF) (h t t p ://d\no c s . g r a y l o g . o r g /e n /l a t e s t /p a g e s /g e l f . h t m l ).\nGraypy will send the logs via UDP by default, but can also send them via AMQP if you\nneed to be 100% sure that every log makes it to Graylog.\nIn most cases, UDP is good enough for centralizing logs. But unlike TCP,\nsome packets may be dropped, and you won't know it. If your logging\nstrategy needs more guarantee, a RabbitMQ-based transport will be more\nreliable.",
      "content_length": 1183,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 158 ]\nPlugging graypy consists of using the provided handler in place of one of the built-in\nhandlers:\n    handler = graypy.GELFHandler('localhost', 12201)\n    logger = logging.getLogger('theapp')\n    logger.setLevel(logging.INFO)\n    logger.addHandler(handler)\nThe graypy.GELFHandler class will convert the log into a UDP payload and send it to a\nGELF UDP Input. In the previous example, the input is listening on localhost, port 12201.\nIt's unlikely that the code that sends the UDP payload will raise an error, and the overhead\nwill be minimal since it's sending UDP datagrams without acknowledging that the other\nend has read it.\nTo integrate Graypy into your Flask application, you can add the handler directly on\napp.logger. You can also automatically log exceptions in an error handler registered every\ntime Flask aborts because of an exception (intended or unintended):\n    import logging\n    import graypy\n    import json\n    from flask import Flask, jsonify\n    from werkzeug.exceptions import HTTPException, default_exceptions\n    app = Flask(__name__)\n    def error_handling(error):\n        if isinstance(error, HTTPException):\n            result = {'code': error.code, 'description':\n                       error.description}\n        else:\n            description = default_exceptions[500].description\n            result = {'code': 500, 'description': description}\n        app.logger.exception(str(error), extra=result)\n        result['message'] = str(error)\n        resp = jsonify(result)\n        resp.status_code = result['code']\n        return resp\n    for code in default_exceptions.keys():\n        app.register_error_handler(code, error_handling)\n    @app.route('/api', methods=['GET', 'POST'])\n    def my_microservice():",
      "content_length": 1766,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 159 ]\n        app.logger.info(\"Logged into Graylog\")\n        resp = jsonify({'result': 'OK', 'Hello': 'World!'})\n        # this will also be logged\n        raise Exception('BAHM')\n        return resp\n    if __name__ == '__main__':\n        handler = graypy.GELFHandler('localhost', 12201)\n        app.logger.addHandler(handler)\n        app.run()\nWhen calling /api, this application will send a simple log to Graylog, then the exception\nwith its full traceback. In the following screenshot, we can see the traceback generated by\nthis example:\nThe user will get the error as well, in a JSON response.\nAdding extra fields\nGraypy adds some metadata fields to each log such as the following:\nThe remote address\nThe PID, process, and thread names\nThe name of the function from which the call was made",
      "content_length": 820,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 160 ]\nGraylog itself will add the hostname from which each log is received, as the source field,\nand a few other fields.\nFor a distributed system, we need to add more contextual information to be able to search\nefficiently in our logs. For example, knowing the username is useful to search for a\nsequence of calls that was made in the stack in the same user session.\nThis information is usually stored inside app.session in our microservices, and we can\nuse a logging.Filter class to add it in each logging record sent to Graylog:\n    from flask import session\n    import logging\n    class InfoFilter(logging.Filter):\n        def filter(self, record):\n            record.username = session.get('username', 'Anonymous')\n            return True\n    app.logger.addFilter(InfoFilter())\nBy adding this filter, we will have a username field added in each Graylog entry.\nAny contextual information you may think of that can be useful for understanding what's\ngoing on should go there. Although, keep in mind that adding more data in your logs can\nhave an opposite effect. If the logs have too many details, it might become hard to search\nthem efficiently, in particular, if a single request generates several log entries.\nTo conclude this part about log centralization, we've looked at how microservices can send\nall its logs to a centralized service with minimal overhead via UDP. Once the logs are\nstored, the centralized service should offer efficient search features.\nKeeping all the logs is extremely useful to investigate microservices issues, but this should\nnot happen that often, hopefully.\nOn the other hand, being able to monitor your applications continuously and server\nperformances will let you be proactive when one of the servers is on its knees.\nGraylog Enterprise is a hosted version of Graylog with extra features, like\narchiving older logs — h t t p s ://w w w . g r a y l o g . o r g /e n t e r p r i s e /f e a t u r e\n/a r c h i v i n g",
      "content_length": 1979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 161 ]\nPerformance metrics\nWhen a microservice eats 100% of server memory, bad things will happen. Some Linux\ndistributions will just kill the greedy process using the infamous out-of-memory killer\n(oomkiller).\nUsing too much RAM can happen for several reasons:\nThe microservice has a memory leak and steadily grows, sometimes at a very fast\npace. It's very common in Python C extensions to forget to dereference an object\nand leak it on every call.\nThe code uses memory without care. For example, a dictionary that's used as an\nad hoc memory cache can grow indefinitely over the days unless there's an upper\nlimit by design.\nThere's simply not enough memory allocated to the service--the server is getting\ntoo many requests or is too weak for the job.\nIt's important to be able to track memory usage over time to find out about these issues\nbefore it impacts users.\nReaching 100% of the CPU in production is also problematic. While it's desirable to\nmaximize the CPU usage, if the server is too busy when new requests are coming in, the\nservice will not be responsive.\nLastly, knowing that the server disk is almost full will prevent a service to crash when it's\nout of space.\nHopefully, most of these problems can be discovered with a load test before the project goes\nto production. A load test is a good way to determine how much load a server can hold\nduring the test and over time, and tweak the CPU/RAM resources depending on the\nexpected load.\nTo do this, let's tool our service to monitor the system resources continuously.\nSystem metrics\nA Linux-based system makes it simple to monitor the CPU, memory, and disk. There are\nsystem files that get continuously updated with this information and numerous tools to\nread them. Commands such as top will let you follow all the running processes and sort\nthem by RAM or CPU usage.",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 162 ]\nIn Python, the psutil (h t t p s ://p y t h o n h o s t e d . o r g /p s u t i l ) project is a cross-platform \nlibrary you can use to get all this info programmatically.\nCombined with the graypy package, you can write a small script to send system metrics to\nGraylog continuously.\nIn the following example, an asyncio loop sends the CPU usage in percent every second to\nGraylog:\n    import psutil\n    import asyncio\n    import signal\n    import graypy\n    import logging\n    import json\n    loop = asyncio.get_event_loop()\n    logger = logging.getLogger('sysmetrics')\n    def _exit():\n        loop.stop()\n    def _probe():\n        info = {'cpu_percent': psutil.cpu_percent(interval=None)}\n        logger.info(json.dumps(info))\n        loop.call_later(1., _probe)\n    loop.add_signal_handler(signal.SIGINT, _exit)\n    loop.add_signal_handler(signal.SIGTERM, _exit)\n    handler = graypy.GELFHandler('localhost', 12201)\n    logger.addHandler(handler)\n    logger.setLevel(logging.INFO)\n    loop.call_later(1., _probe)\n    try:\n        loop.run_forever()\n    finally:\n        loop.close()\nRunning this script as a daemon on your server will let you track its CPU usage.\nThe system-metrics (h t t p s ://g i t h u b . c o m /t a r e k z i a d e /s y s t e m - m e t r i c s /) project is \nroughly the same script but adds info about memory, disks, and network. If you use the\npip install command, a command-line script will be available to probe your system.",
      "content_length": 1486,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 163 ]\nOnce the script runs, you can create a dashboard with a few widgets in the Graylog web\napp, as described in h t t p ://d o c s . g r a y l o g . o r g /e n /l a t e s t /p a g e s /d a s h b o a r d s . h t m l and\ncreate an alert that will send you an email on specific conditions. Alerts in Graylog are\nconfigured on Streams, which are processing incoming messages in real-time.\nTo send an email when the CPU gets over 70%, you can create a stream that will collect the\ncpu_percent field sent by our psutil script using a stream rule:\nFrom there, you can manage alerts for the stream and add an email one that will get\ntriggered when the condition is met for some time.\nLike we did for sending logs, we can also add custom performance metrics inside our\nmicroservice code depending on our needs.",
      "content_length": 830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 164 ]\nCode metrics\nFor some microservices, it can also be useful to get performance metrics inside the code.\nNew Relic, for instance, will track Jinja2 and database call performances by wrapping some\ncalls inside Flask to measure how long it takes to generate a template or perform a database\ncall.\nBut adding your instrumentation inside the code needs to be done carefully if that\ninstrumentation ships in production. Slowing down the service is easy. For instance, it's\nunthinkable to use Python built-in profilers because they add a significant overhead.\nA simple pattern is to measure the time taken by some of your functions explicitly.\nIn the following example, the @timeit decorator will collect execution times for the\nfast_stuff() and some_slow_stuff() functions and a message will be sent to Graylog\nat the end of the request with the duration for each call:\n    import functools\n    import logging\n    import graypy\n    import json\n    import time\n    import random\n    from collections import defaultdict, deque\n    from flask import Flask, jsonify, g\n    app = Flask(__name__)\n    class Encoder(json.JSONEncoder):\n        def default(self, obj):\n            base = super(Encoder, self).default\n            # specific encoder for the timed functions\n            if isinstance(obj, deque):\n                calls = list(obj)\n                return {'num_calls': len(calls), 'min': min(calls),\n                        'max': max(calls), 'values': calls}\n            return base(obj)\n    def timeit(func):\n        @functools.wraps(func)\n        def _timeit(*args, **kw):\n            start = time.time()\n            try:\n                return func(*args, **kw)\n            finally:\n                if 'timers' not in g:",
      "content_length": 1754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 165 ]\n                    g.timers = defaultdict(functools.partial(deque,\nmaxlen=5))\n                g.timers[func.__name__].append(time.time() - start)\n        return _timeit\n    @timeit\n    def fast_stuff():\n        time.sleep(.001)\n    @timeit\n    def some_slow_stuff():\n        time.sleep(random.randint(1, 100) / 100.)\n    def set_view_metrics(view_func):\n        @functools.wraps(view_func)\n        def _set_view_metrics(*args, **kw):\n            try:\n                return view_func(*args, **kw)\n            finally:\n                app.logger.info(json.dumps(dict(g.timers), cls=Encoder))\n        return _set_view_metrics\n    def set_app_metrics(app):\n        for endpoint, func in app.view_functions.items():\n            app.view_functions[endpoint] = set_view_metrics(func)\n    @app.route('/api', methods=['GET', 'POST'])\n    def my_microservice():\n        some_slow_stuff()\n        for i in range(12):\n            fast_stuff()\n        resp = jsonify({'result': 'OK', 'Hello': 'World!'})\n        fast_stuff()\n        return resp\n    if __name__ == '__main__':\n        handler = graypy.GELFHandler('localhost', 12201)\n        app.logger.addHandler(handler)\n        app.logger.setLevel(logging.INFO)\n        set_app_metrics(app)\n        app.run()",
      "content_length": 1282,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 166 ]\nUsing such instrumentation, you will be able to track down each call duration in Graylog:\nWeb server metrics\nThe last metrics that we want to have in our centralized logger is everything related to the\nHTTP requests and responses performance. We could add those metrics inside the Flask\napplication alongside our timers, but it's better to do it at the web server level to reduce the\noverhead and to make metrics compatible with content that's not generated by Flask.\nFor instance, if nginx serves static files directly, we still want to track that. Graylog has a\nmarketplace (h t t p s ://m a r k e t p l a c e . g r a y l o g . o r g ) for extending the system with content\npacks, and there's an nginx content pack (h t t p s ://g i t h u b . c o m /G r a y l o g 2/g r a y l o g - c o n t e n\nt p a c k - n g i n x ) that will parse nginx's access and error logs to push them in Graylog.\nThe pack comes with a default dashboard, and its input is using the nginx ability to send\nlogs through UDP using syslog (h t t p ://n g i n x . o r g /e n /d o c s /s y s l o g . h t m l ).",
      "content_length": 1113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 167 ]\nUsing this configuration, you will be able to track valuable information such as:\nThe average response time\nThe number of requests per minute\nThe remote address\nThe endpoint and verb of the request\nThe status code and size of the response\nCombined with app-specific metrics and system metrics, all these logs will let you build live\ndashboards you can use to follow what's going on in your deployments:",
      "content_length": 435,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "Monitoring Your Services\n[ 168 ]\nSummary\nIn this chapter, we've seen how to add some instrumentation in our microservices and at\nthe web server level. We've also learned how to set up Graylog to centralize and use all the\ngenerated logs and performance metrics.\nGraylog uses Elasticsearch to store all the data, and that choice offers fantastic search\nfeatures that will make your life easier to look for what's going on. The ability to add alerts\nis also useful for being notified when something's wrong. But deploying Graylog should be\nconsidered carefully. An Elastic Search cluster is heavy to run and maintain once it has a lot\nof data.\nFor your metrics, time-series based systems such as InfluxDB (open source) from\nInfluxData (h t t p s ://w w w . i n f l u x d a t a . c o m /) is a faster and lightweight alternative. But it's\nnot meant to store raw logs and exceptions.\nSo if you just care about performance metrics and exceptions, maybe a good solution would\nbe to use a combination of tools: Sentry for your exceptions and InfluxDB for tracking\nperformances. In any case, as long as your applications and web servers generate logs and\nmetrics via UDP, it makes it easier to move from one tool to another.\nThe next chapter will focus on another important aspect of microservices development: how\nto secure your APIs, offer some authentication solutions, and avoid fraud and abuse.",
      "content_length": 1391,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "7\nSecuring Your Services\nSo far in this book, all the interactions between services were done without any form of\nauthentication or authorization. Each HTTP request would happily return a result. This\ncan't happen in production for two simple reasons: we need to know who is calling the\nservice (authentication) and we need to make sure that the caller is allowed to perform the\ncall (authorization). For instance, we probably don't want an anonymous caller to delete\nentries in a database.\nIn a monolithic web application, authentication happens with a login form, and once the\nuser is identified, a cookie is set and used for all subsequent requests.\nIn a microservice-based architecture, we can't use that scheme everywhere because services\nare not users and won't use web forms to authenticate. We need a way to allow or reject a\ncall between each service automatically.\nThe OAuth2 authorization protocol (h t t p s ://o a u t h . n e t /2/) gives us the flexibility to add\nauthentication and authorization in our microservices, that can be used to authenticate both\nusers and services. In this chapter, we'll discover some aspects of OAuth2 and how to \nimplement an authentication microservice. This service will be used to secure service-to-\nservice interactions.\nSecuring services also means we want to avoid any fraud and abuse of the system. For\ninstance, if a client starts to hammer one of our endpoints, whether it's malicious or an\nunintended bug, we need to detect that behavior and try to protect the system. There's not\nmuch we can do in case of a massive Distributed Denial Of Service (DDoS) attack, but\nsetting up a basic web application firewall is easy to do and a great way to protect the\nsystem from basic attacks.",
      "content_length": 1736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "Securing Your Services\n[ 170 ]\nLastly, a few things can be done at the code level to protect your services, such as\ncontrolling system calls or making sure HTTP redirects are not ending up in hostile web\npages. The last part of the chapter will enumerate some of them and demonstrate how you\ncan continuously scan your code for potential security issues.\nIn this chapter, we will cover the following topics:\nAn overview of the Oauth2 protocol\nHow token-based authentication works in practice\nWhat is the JWT standard and how to use it in a “token dealer” for securing\nmicroservices\nHow to implement a web application firewall\nSome best practices to secure your microservice code\nThe OAuth2 protocol\nOAuth2 is a widely adopted standard that secures web applications and their interactions\nwith users and other web applications, and yet it's hard to understand because it's based on\nmany RFCs that are quite complicated to grasp fully.\nThe core idea of OAuth2 is that a centralized service is in charge of authenticating a caller,\nand can grant some access in the form of codes or tokens; let's call them keys. Those keys\ncan be used by users or services to access a resource, as long as the service providing that\nresource accepts that key.\nThat's what we've used in Chapter 4, Designing Runnerly, to build the Strava microservice.\nThe service interacts with the Strava API on behalf of the users after it was granted access\nvia Strava's authentication service. This grant is called an Authorization Code Grant and is\nthe most commonly used grant. It's known as three-legged OAuth because it involves the\nuser, the authentication service, and a third-party application. Strava generates a code that\ncan be used to call their APIs, and the Strava Celery worker we've created uses it in every\ncall.",
      "content_length": 1795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "Securing Your Services\n[ 171 ]\nIn the preceding diagram, the typical flow is to have the user interact with an application\nthat wants to access a service like Strava. When the user calls app (1), they get redirected to\nthe Strava service to grant access to the Strava API by app (2). Once it's done, Some App\ngets an authorization code through an HTTP callback and can use the Strava API on behalf\nof user (3).\nFor a service-to-service authentication that doesn't necessarily involve a particular user,\nthere's another grant type called Client Credentials Grant (CCG), where service A can\nauthenticate to the authentication microservice and ask for a token that it can use to call\nservice B.\nFor more information you can refer to the CCG scenario described in the\nOAuth2 Authorization Framework section 4.4\n(https://tools.ietf.org/html/rfc6749#section-4.4).\nIt works like the authorization code, but the service is not redirected to a web page like a\nuser. Instead, it's implicitly authorized with a secret key that can be traded for a token.\nFor a microservices-based architecture, using these two type of grants will let us centralize\nevery aspect of authentication and authorization of the system. Building a microservice that\nimplements part of the OAuth2 protocol to authenticate services and keep track of how they\ninteract with each other is a good solution to reduce security issues; everything is\ncentralized in a single place.\nThe CCG flow is by far the most interesting part to look at in this chapter because it allows\nus to secure our microservices interactions independently from the users. It also simplifies\npermission management since we can issue tokens with different scopes depending on the\ncontext.",
      "content_length": 1719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "Securing Your Services\n[ 172 ]\nThe three-legged flow is something that can be added if some of our microservices are used\nby third-party on behalf of specific users, but we will focus on CCG.\nIf you don't want to implement and maintain the authentication part of\nyour application and you can trust a third party to manage this process,\nthen Auth0 is an excellent commercial solution that provides all the APIs\nneeded for a microservice-based application (h t t p s ://a u t h 0. c o m /).\nBefore we go ahead and implement our authentication microservice, let's look at how\ntoken-based authentication works from the ground. If you understand the next section\ncorrectly, everything else in OAuth2 should be easier to grasp.\nToken-based authentication\nAs we said earlier, when a service wants to get access to another service without any user\nintervention, we can use a CCG flow.\nThe idea behind CCG is that a service can authenticate to an authentication service exactly\nlike a user would do, and ask for a token that it can then use to authenticate against other\nservices.\nA token is a like a password. It's proof that you are allowed to access a particular resource.\nWhether you are a user or a microservice, if you own a token that the resource recognizes,\nit's your key to access that resource.\nTokens can hold any information that is useful for the authentication and authorization\nprocess. Some of them can be:\nThe user name or ID, if it's pertinent to the context\nThe scope, which indicates what the caller is allowed to do (read, write, and so\non)\nA timestamp indicating when the token was issued\nAn expiration timestamp, indicating how long the token is valid\nA token is usually built as a self-contained proof that you can use a service. Self-contained\nmeans that the service will be able to validate the token without having to call an external\nresource, which is an excellent way to avoid adding dependencies between services.\nDepending on the implementation, a token can also be used to access different\nmicroservices.",
      "content_length": 2029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "Securing Your Services\n[ 173 ]\nOAuth2 uses the JWT standard for its tokens.\nThere's nothing in OAuth2 that requires the use of JWT — they just\nhappen to be a good fit for what OAuth2 wants to do.\nThe JWT standard\nThe JSON Web Token (JWT) described in RFC 7519 (h t t p s ://t o o l s . i e t f . o r g /h t m l /r f c\n7519) is a standard that is commonly used to represent tokens.\nTokens are, in that case, a long string composed of three dot-separated parts:\nHeader: This provides info on the token, such as which hashing algorithm is used\nPayload: This is the actual data\nSignature: This is a signed hash of the token to check that it's legitimate\nJWT tokens are base64 encoded so they can be used in query strings.\nHere's a JWT token in its encoded form:\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9\n.\neyJ1c2VyIjoidGFyZWsifQ\n.\nOeMWz6ahNsf-TKg8LQNdNMnFHNtReb0x3NMs0eY64WA\nEach part in the token above is separated by a line break for display\npurpose. The original token is a single line.\nAnd if we use Python to decode it:\n>>> import base64\n>>> def decode(data):\n...     # adding extra = for padding if needed\n...     pad = len(data) % 4\n...     if pad > 0:\n...         data += '=' * (4 - pad)\n...     return base64.urlsafe_b64decode(data)\n...\n>>> decode('eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9')",
      "content_length": 1288,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "Securing Your Services\n[ 174 ]\nb'{\"alg\":\"HS256\",\"typ\":\"JWT\"}'\n>>> decode('eyJ1c2VyIjoidGFyZWsifQ')\nb'{\"user\":\"tarek\"}'\n>>> decode('OeMWz6ahNsf-TKg8LQNdNMnFHNtReb0x3NMs0eY64WA')\nb'9\\xe3\\x16\\xcf\\xa6\\xa16\\xc7\\xfeL\\xa8<-\n\\x03]4\\xc9\\xc5\\x1c\\xdbQy\\xbd1\\xdc\\xd3,\\xd1\\xe6:\\xe1'\nEvery part of the JWT token is a JSON mapping except the signature. The header usually\ncontains just the typ and the alg keys. The typ key says it's a JWT token, and the alg key\nindicates which hashing algorithm is used.\nIn the following header example, we have HS256, which stands for HMAC-SHA256:\n{\"typ\": \"JWT\",  \"alg\": \"HS256\"}\nThe payload contains whatever you need, and each field is called a JWT Claim in the RFC\n7519 jargon.\nThe RFC has a predefined list of claims that a token may contain, called Registered Claim\nNames. Here's a subset of them:\niss: This is the issuer, which is the name of the entity that generated the token.\nIt's typically the fully-qualified hostname, so the client can use it to discover its\npublic keys by requesting /.well-known/jwks.json.\nexp: This is the Expiration Time, which is a timestamp after which the token is\ninvalid\nnbf: This stands for Not Before Time, which is a timestamp before which the\ntoken is invalid\naud: This is the Audience, which is the recipient for whom the token was issued\niat: This stands for Issued At, which is a timestamp for when the token was\nissued\nIn the following payload example, we're providing the custom user_id value along with\ntimestamps that make the token valid 24h after it was issued. Once valid, that token can be\nused for 24h:\n{\n  \"iss\": \"https://tokendealer.example.com\",\n  \"aud\": \"runnerly.io\",\n  \"iat\": 1488796717,\n  \"nbt\": 1488883117,\n  \"exp\": 1488969517,\n  \"user_id\": 1234\n}",
      "content_length": 1731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "Securing Your Services\n[ 175 ]\nThese headers gives us a lot of flexibility to control how long our tokens will stay valid.\nDepending on the nature of the microservice, the token Time-To-Live (TTL) can be very\nshort or infinite. For instance, a microservice that interacts with other microservices within\nyour system should probably rely on tokens that are valid for a while to avoid having to\nregenerate tokens all the time. On the other hand, if your tokens are distributed in the wild,\nit's a good idea to make them short lived.\nThe last part of a JWT token is the signature. It contains a signed hash of the header and the\npayload. There are several algorithms used to sign and hash. Some are based on a secret\nkey, and some are based on public and private key pair.\nLet's see how we can deal with JWT tokens in Python.\nPyJWT\nIn Python, the PyJWT (h t t p s ://p y j w t . r e a d t h e d o c s . i o /) library provides all the tools you\nneed to generate and read back JWT tokens.\nOnce you've pip-installed pyjwt (and cryptography), you can use the encode() function\nand the decode() functions to create tokens.\nIn the following example, we're creating a JWT token using HMAC-SHA256 and reading it\nback. The signature is verified when the token is read, by providing the secret:\n>>> import jwt\n>>> def create_token(alg='HS256', secret='secret', **data):\n...     return jwt.encode(data, secret, algorithm=alg)\n...\n>>> def read_token(token, secret='secret', algs=['HS256']):\n...     return jwt.decode(token, secret)\n...\n>>> token = create_token(some='data', inthe='token')\n>>> print(token)\nb'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpbnRoZSI6InRva2VuIiwic29tZSI6ImRh\ndGEifQ.oKmFaNV-C2wHb_WaMAfIGDqBPnOCyOzVf-JWvh-6bRQ'\n>>> read = read_token(token)\n>>> print(read)\n       {'inthe': 'token', 'some': 'data'}",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "Securing Your Services\n[ 176 ]\nThe create_token() function calls jwt.decode() with the algorithms\nargument to make sure the token is verified with the right algorithm. This\nis good practice to prevent attacks where a malicious token can trick the\nserver into using an unexpected algorithm, as noted in\nh t t p s ://a u t h 0. c o m /b l o g /c r i t i c a l - v u l n e r a b i l i t i e s - i n - j s o n - w e b - t o\nk e n - l i b r a r i e s\nWhen executing this code, the token is displayed in its compressed and uncompressed\nform.\nIf you use one of the registered claims, PyJWT will control them. For instance, if the exp\nfield is provided and the token is outdated, the library will raise an error.\nUsing a secret for signing and verifying the signature is great when you have a few services\nrunning, but it can soon become a problem because it means you need to share the secret\namong all services that need to verify the signature. And when the secret needs to be\nchanged, it can be a challenge to change it across your stack securely.\nBasing your authentication on a secret that you are sharing around is also a weakness. If a\nsingle service is compromised and the secret is stolen, your whole authentication system is\ncompromised.\nA better technique is to use an asymmetric key composed of a public key and a private key.\nThe private key is used by the token issuer to sign the tokens, and the public key can be\nutilized by anyone to verify that the signature was signed by that issuer.\nOf course, if an attacker has access to the private key, or can convince clients that a forged\npublic key is the legitimate one, you would still be in trouble.\nBut using a public/private key pair reduces the attack surface of your authentication process\nby a lot. And, since the authentication microservice will be the only place that has the\nprivate key, you can focus on adding extra security to it. For instance, such sensible services\nare often deployed in a firewalled environment where all accesses are strictly controlled.\nLet's see how we can create asymmetric keys in practice.\nX.509 certificate-based authentication\nThe X.509 standard (h t t p s ://e n . w i k i p e d i a . o r g /w i k i /X . 509) is used to secure the Web.\nEvery website using SSL out there (serving pages on HTTPS), have an X.509 certificate on\ntheir web server and use it to encrypt and decrypt data on-the-fly.",
      "content_length": 2390,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "Securing Your Services\n[ 177 ]\nThese certificates are issued by a Certificate Authority (CA), and when your browser opens\na page that presents a certificate, it has to be published from one of the CAs supported by\nthe browser.\nThe reason why CA exists is to limit the risk of compromised certificates by having a\nlimited number of trusted entities that generates and manages them, independently from\nthe companies that use them.\nSince anyone can create a self-signed certificate in a shell, it would be quite easy to end up\nin a world where you don't know if you can trust a certificate. If the certificate is issued by\none of the CAs trusted by the browser, like Let's Encrypt (h t t p s ://l e t s e n c r y p t . o r g /), it \nshould be legitimate.\nFor our microservices, using a self-signed certificate can be good enough if\nwe own every part of the architecture, and that's what we'll demonstrate\nin the section. However, if your microservices are exposed to other third\nparties, or vice versa, it's better to rely on a trusted CA. Let's Encrypt is\nfree and is a pretty good one. This project aims at securing the Web, but by\nusing extend you can also use it to secure your microservices as long as\nyou own a domain name.\nFor now, let's create our self-signed certificate and see how it can be used to sign JWT\ntokens.\nIn a shell, you can use the openssl command to create a certificate and extract a public and\nprivate key pair out of a certificate.\nIf you are under the latest macOS operating system, you might need to\ninstall openssl from brew since it was removed from macOS.\n$ openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days\n365\nGenerating a 4096 bit RSA private key\n..........................++\n..........................++\nwriting new private key to 'key.pem'\nEnter PEM pass phrase:\nVerifying - Enter PEM pass phrase:\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields, but you can leave some blank",
      "content_length": 2097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "Securing Your Services\n[ 178 ]\n For some fields, there will be a default value,\n If you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:FR\nState or Province Name (full name) [Some-State]:\nLocality Name (eg, city) []:\nOrganization Name (e.g., company) [Internet Widgits Pty Ltd]:Runnerly\n Organizational Unit Name (eg, section) []:\nCommon Name (e.g. server FQDN or YOUR name) []:Tarek\nEmail Address []:tarek@ziade.org\n$ openssl x509 -pubkey -noout -in cert.pem > pubkey.pem\n$ openssl rsa -in key.pem -out privkey.pem\nEnter pass phrase for key.pem:\nwriting RSA key\nThese three calls generate four files:\nThe cert.pem file has the certificate\nThe pubkey.pem file has the public key extracted from the certificate\nThe key.pem file has the RSA private key, encrypted\nThe privkey.pem file has the RSA private key, in clear\nRSA stands for Rivest, Shamir, and Adleman, the three authors. The RSA\nencryption algorithm generates crypto keys that can go up to 4,096 bytes\nand are considered secure.\nFrom there, we can use pubkey.pem and privkey.pem in our PyJWT script to sign and\nverify the signature of the token, using RSASSA-PKCS1-v1_5 signature algorithm and the\nSHA-512 hash algorithm:\n    import jwt\n    with open('pubkey.pem') as f:\n        PUBKEY = f.read()\n    with open('privkey.pem') as f:\n        PRIVKEY = f.read()\n    def create_token(**data):\n        return jwt.encode(data, PRIVKEY, algorithm='RS512')",
      "content_length": 1440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "Securing Your Services\n[ 179 ]\n    def read_token(token):\n        return jwt.decode(token, PUBKEY)\n    token = create_token(some='data', inthe='token')\n    print(token)\n    read = read_token(token)\n    print(read)\nThe result is similar to the previous run, except that we get a much bigger token:\nb'eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzUxMiJ9.eyJzb21lIjoiZGF0YSIsImludGhlIjoidG9r\nZW4ifQ.VHKP2yO1dCUrS5YAOCZsGXF_mesMJNNYcnBHe4mFiPpBDCbMhrI8h10vr1BaCiN8rVEM\ncUXQ4Gc7183w6ga3spyEzONg3-Sv-eId4rPbTqbbmPErrnWPRIH9hQMHsMebVOlI9lOvNmV-\nJ3DIEmV4riqRluJMIFYuy_A7fB2r8IqeHBfrsEPWmvw2_tIZ3V3dJGU4ZBkn8zdzgfbou_LHc28\n_dyC32kR2Ec1nsRV3zRffEjx60cjzmNNFqB9kYZHun0IIzBqdh0IiRxPF4rgYG3oBKJXP3u2uyf\nBifNy3Bz4bMPJ8iRRmQleciyFdzDkm7J4SAyz5I0TKHSPOZA-9x6dgacQ9w_JAtmElH7u8_ES_2\nTxmvbBLqsXIzghAhG10CL79UeSKeXMTjc8DOQrIbWmaRCIbPy9AdlIJQxqul4UnCoUhUQ6PZwD6\nCEuaZTjKdPvql7n_-u1Tjrw7e339WC9QZS5DFCzMe2F0TY-kI52-AaNEoRaO8oSCwW3E7u-\nNcSt-\nbD019MdX3bxN0FdNvL62BUDqqxind7TFF7YFX3zTxTu15Pex2F64YvnhG1CDk337htROt8B9vH8\nCIUWo_2ujkair8zCdd9sfIdssOGFDnawIX2NPGd4vZ1dpw0DwHBaXw0gP8zzcRAsuZ7rfNMZeJT\nH6gB-kMc5UKf26nAc'\n{'some': 'data', 'inthe': 'token'}\nNotice that adding over 700 bytes of data to each request can add up over time, so the\nsecret-based JWT token technique is an option to keep in mind if you need to reduce the \nnetwork overhead.\nNow that we've learned how to deal with JWT tokens, let start to implement our\nauthentication microservice; we'll call it the TokenDealer.\nThe TokenDealer microservice\nOur first step to building the authentication microservice will be to implement everything\nneeded to perform a CCG flow. For that flow, the app receives requests from services that\nwant a token and generates them on-demand. The generated tokens will have a lifespan of\none day.\nThis service will be the only service to possess the private key that is used to sign the tokens\nand will expose the public key for other services that want to verify tokens. This service will\nalso be the only place where all the client IDs and secret keys are kept.",
      "content_length": 2016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "Securing Your Services\n[ 180 ]\nWe will greatly simplify the implementation by stating that once a service gets a token, it\ncan access any other service in our ecosystem. When a service is accessed with a token, it\ncan verify that token locally or call the TokenDealer to perform the verification. The first\noption, where checks happen locally, will remove one network roundtrip, but the tradeoff is\nthat it will add some CPU overhead when working with JWT tokens, which can be\nproblematic in some context. For example, if your microservice is doing some CPU-\nintensive work, adding the work required for checking the token might require to use a\nserver with bigger CPUs, which might add some extra costs.\nThat's the reason why it's good to have the two options.\nTo implement everything we've described, three endpoints will be created in this\nmicroservice:\nGET /.well-known/jwks.json: This is the public key published in the JSON\nWeb Key (JWK) format as described in RFC 7517 (h t t p s ://t o o l s . i e t f . o r g /h t m\nl /r f c 7517), when other microservices want to verify tokens on their own.\nPOST /oauth/token: This returns a token, given some credentials. Adding the\n/oauth prefix is a convention widely adopted since it's used in the OAuth RFC.\nPOST /verify_token: This returns the token payload, given a token. If the\ntoken is not valid, it returns a 400.\nUsing the microservice skeleton at h t t p s ://g i t h u b . c o m /R u n n e r l y /m i c r o s e r v i c e , we can\ncreate a very simple Flask blueprint that implements these three views.\nLet's look at the most important one, POST /oauth/token.\nThe POST/oauth/token implementation\nFor the CCG flow, the service that wants a token sends a POST request with an URL-\nencoded body that contains the following fields:\nclient_id: This is a unique string identifying the requester.\nclient_secret: This is a secret key that authenticates the requester. It should be\na random string generated up-front and registered with the auth service.\ngrant_type: This is the grant type, must be client_credentials.",
      "content_length": 2066,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "Securing Your Services\n[ 181 ]\nWe'll make a few assumptions to simplify the implementation:\nWe're keeping the list of secrets in a Python mapping\nclient_id is the name of the microservice\nThe secret is generated with binascii.hexlify(os.urandom(16))\nThe authentication part will just ensure that the secret is valid, then the service will create a\ntoken and return it:\n    import time\n    from flask import request, current_app, abort, jsonify\n    from werkzeug.exceptions import HTTPException\n    from flakon import JsonBlueprint\n    from flakon.util import error_handling\n    import jwt\n    home = JsonBlueprint('home', __name__)\n    def _400(desc):\n        exc = HTTPException()\n        exc.code = 400\n        exc.description = desc\n        return error_handling(exc)\n    _SECRETS = {'strava': 'f0fdeb1f1584fd5431c4250b2e859457'}\n    def is_authorized_app(client_id, client_secret):\n        return compare_digest(_SECRETS.get(client_id), client_secret)\n    @home.route('/oauth/token', methods=['POST'])\n    def create_token():\n        key = current_app.config['priv_key']\n        try:\n            data = request.form\n            if data.get('grant_type') != 'client_credentials':\n                return _400('Wrong grant_type')\n            client_id = data.get('client_id')\n            client_secret = data.get('client_secret')\n            aud = data.get('audience', '')\n            if not is_authorized_app(client_id, client_secret):\n                return abort(401)\n            now = int(time.time())\n            token = {'iss': 'https://tokendealer.example.com',",
      "content_length": 1569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "Securing Your Services\n[ 182 ]\n                     'aud': aud,\n                     'iat': now,\n                     'exp': now + 3600 * 24}\n            token = jwt.encode(token, key, algorithm='RS512')\n            return {'access_token': token.decode('utf8')}\n        except Exception as e:\n            return _400(str(e))\nThe create_token() view uses the private key found in the application configuration\nunder the priv_key key.\nThe hmac.compare_digest() function is used to compare the two\nsecrets to avoid a timing attack by a client which would try to guess the\nclient_secret one character at a time. It's equivalent to the \"==\"\noperator.\nFrom the documentation: This function uses an approach designed to prevent\ntiming analysis by avoiding content-based short circuiting behavior, making it\nappropriate for cryptography\nThis blueprint is all we need with a pair of keys to run a microservice that will take care of\ngenerating self-contained JWT tokens for all our microservices that require authentication.\nThe whole source code of the TokenDealer microservice can be found at h t\nt p s ://g i t h u b . c o m /R u n n e r l y /t o k e n d e a l e r where you can look at how\nthe two other views are implemented.\nThe microservice could offer more features around token generation. For instance, the\nability to manage scopes and make sure microservice A is not allowed to generate a token\nthat can be used in microservice B or managing a whitelist of services that are authorized to\nask for some tokens.",
      "content_length": 1511,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "Securing Your Services\n[ 183 ]\nBut the pattern we've implemented is the basis for an efficient token-based authentication\nsystem in a microservice environment, you can develop on your own, and is good enough\nfor our Runnerly app.\nIn the following diagram, training plans, data service, and races can use JWT tokens to\nrestrict access to their respective endpoints:\nJWT access in this diagram means that the service requires a JWT token. Those services may\nvalidate the token by calling the TokenDealer. The Flask app in this diagram needs to obtain\ntokens from the TokenDealer on behalf of its users (link not shown in the diagram).\nNow that we have a TokenDealer service that implements CCG, let's see in practice how it\ncan be used by our services the next section.",
      "content_length": 767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "Securing Your Services\n[ 184 ]\nUsing TokenDealer\nIn Runnerly, the Data Service | Strava worker link (3) is a good example of a place where\nauthentication is required. Adding runs via the Data Service needs to be restricted to\nauthorized services:\nAdding authentication for that link is done in four steps:\nThe TokenDealer keeps a client_id and client_secret pair for the Strava\n1.\nworker and shares it with the Strava worker developers (1).\nThe Strava worker uses client_id and client_secret to ask a token to the\n2.\nTokenDealer (2).\nThe Strava worker adds the token in each request against to the Data Service (3).\n3.\nThe Data Service verifies the token by calling the TokenDealer, or by performing\n4.\na local JWT verification (4)",
      "content_length": 731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "Securing Your Services\n[ 185 ]\nIn a full implementation, the first step is semiautomated. Generating a client secret is\nusually done through some web admin panel in the authentication service. That secret is\nthen provided to the Strava microservice developers.\nFrom there, the service can get a new token every time it needs it (because it's the first time\nor because the token is outdated) and add that token in the Authorization header when\ncalling Data Service.\nThe following is an example of such a call using the requests library--we have in that\nexample a TokenDealer running on localhost:5000 and a Data Service running on\nlocalhost:5001.\n    import requests\n    server = 'http://localhost:5000'\n    secret = 'f0fdeb1f1584fd5431c4250b2e859457'\n    data = [('client_id', 'strava'),\n            ('client_secret', secret),\n            ('audience', 'runnerly.io'),\n            ('grant_type', 'client_credentials')]\n    def get_token():\n        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n        url = server + '/oauth/token'\n        resp = requests.post(url, data=data, headers=headers)\n        return resp.json()['access_token']\nNotice that the /oauth/token is accepting form encoded data rather than\na JSON payload, since this is the standard implementation.\nThe get_token() function retrieves a token, which can then be used in the\nAuthorization header, when the code calls the Data Service:\n    _TOKEN = None\n    def get_auth_header(new=False):\n        global _TOKEN\n        if _TOKEN is None or new:\n            _TOKEN = get_token()\n        return 'Bearer ' + _TOKEN\n    _dataservice = 'http://localhost:5001'",
      "content_length": 1638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "Securing Your Services\n[ 186 ]\n    def _call_service(endpoint, token):\n        # not using session etc, to simplify the reading  :)\n        return requests.get(_dataservice + '/' + endpoint,\n                            headers={'Authorization': token})\n    def call_data_service(endpoint):\n        token = get_auth_header()\n        resp = _call_service(endpoint, token)\n        if resp.status_code == 401:\n            # the token might be revoked, let's try with a fresh one\n            token = get_auth_header(new=True)\n            resp = _call_service(endpoint, token)\n        return resp\nThe call_data_service() function will try to get a new token if the call to the Data\nService leads to a 401 response.\nThis refresh-token-on-401 pattern can be used in all your microservices to automate token\ngeneration.\nThis covers service-to-service authentication. You can find the full implementation in the\nRunnerly's GitHub repository to play with this JWT-based authentication scheme and use it\nas a basis for building your authentication process.\nThe next section of this chapter looks at another important aspect of securing your web\nservices, that is, adding a web application firewall.\nWeb application firewall\nWhen you're exposing HTTP endpoints to others, you are expecting callers to behave as\nintended. Each HTTP conversation is supposed to follow a scenario that you have\nprogrammed in the service.\nIn the real world, that's not always the case. If the caller has a bug or is just not calling your\nservice correctly, the expected behavior should be to send back a 4xx response and explain\nto the client why the request was rejected. That's also the case for malicious requests sent by\nattackers. Any unintended behavior should be dismissed.\nThe Open Web Application Security Project (OWASP) (h t t p s ://w w w . o w a s p . o r g ) is an \nexcellent resource to learn about ways to protect your web apps from bad behaviors. They\neven provide a set of rules for the ModSecurity (h t t p s ://m o d s e c u r i t y . o r g /c r s /) toolkit's\nWeb Application Framework (WAF) that can be used to avoid a lot of attacks.",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "Securing Your Services\n[ 187 ]\nIn microservices-based applications, anything that's published to the web can be attacked,\nbut, unlike monolithic applications, most of the system is not dealing directly with users via\nHTML user interfaces or public APIs, and that narrows down the spectrum of potential\nattacks.\nWe'll see in this section how to provide essential protection for our JSON-based\nmicroservices.\nBut before we do this, let's look at some of the most common attacks:\nSQL Injection: The attacker sends raw SQL statements in the request. If your\nserver uses some of the request content (typically the arguments) to build SQL\nqueries, it might perform the attacker's request on the database. In Python,\nthough, if you use SQLAlchemy and avoid raw SQL statements altogether, you\nwill be safe. If you use raw SQL, make sure every variable is correctly quoted.\nWe'll see that later in this chapter.\nCross Site Scripting (XSS): This attack happens only on web pages that display\nsome HTML. The attacker uses some of the query attributes to try to inject their\npiece of HTML on the page to trick the user into performing some actions\nthinking they are on the legitimate website.\nCross-Site Request Forgery (XSRF/CSRF): This attack is based on attacking a\nservice by reusing the user's credentials from another website. The typical CSRF\nattack happens with POST requests. For instance, a malicious website displays a\nlink to a user to trick that user to perform the POST request on your site using\ntheir existing credentials.\nMany other attacks are specifically targeting PHP-based systems because it's widespread\nand easy to find a PHP app that uses invalidated user input when the server is called.\nThings such as Local File Inclusion (LFI), Remote File Inclusion (RFI), or Remote Code\nExecution (RCE) are all attacks that trick the server to execute something via client input or\nreveal server files. They can happen of course in Python applications, but Python\nframeworks are known to have built-in protections to avoid those attacks.\nHowever, bad requests are not always how a client, whether it's malicious or not, can abuse\nyour system. It can send legitimate requests and just hammer your service with it, leading\nto a Denial of Service (DoS) because all the resources are used to handle requests from the\nattacker. This problem sometimes happens within distributed systems when clients have\nreplay features that are automatically recalling the same API. If nothing is done on the client\nside to throttle calls, you might end up with a service overloaded by legitimate clients.",
      "content_length": 2586,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "Securing Your Services\n[ 188 ]\nAdding a protection on the server-side to back-off such zealous clients is usually not hard to\ndo and goes a long way to protect your microservice stack.\nIn this section, we'll focus on creating a basic WAF that will explicitly reject a client that's\nmaking too many requests on our service.\nThe intent of this section is not to create a full WAF, but rather to give you\na good understanding of how WAF are implemented and used. That said,\nusing a fully featured WAF like ModSecurity is probably overkill for\nJSON-based microservices.\nWe could build our WAF in a Flask microservice, but it would add a lot of overhead if all\nthe traffic has to go through it. A much better solution is to rely directly on the web server.\nOpenResty - Lua and nginx\nOpenResty (h t t p ://o p e n r e s t y . o r g /e n /) is an nginx distribution that embeds a Lua (h t t p\n://w w w . l u a . o r g /) interpreter that can be used to script the web server.\nLua is an excellent, dynamically-typed programming language, which has a lightweight\ninterpreter, yet, very fast. The language offers a complete set of features and has built-in\nasync features. You can write coroutines directly in vanilla Lua.\nFor a Python developer, Lua feels quite Pythonic, and you can start to build scripts with it in\na matter of hours once you know the basic syntax. It has functions, classes, and a standard\nlibrary that will make you feel at home.\nIf you install Lua (refer to h t t p ://w w w . l u a . o r g /s t a r t . h t m l ), you can play with the \nlanguage using the Lua Read Eval Print Loop (REPL) exactly like how you would do with\nPython:\n$ lua\nLua 5.1.5  Copyright (C) 1994-2012 Lua.org, PUC-Rio\n> io.write(\"Hello world\\n\")\nHello world\n> mytable = {}\n> mytable[\"user\"] = \"tarek\"\n> = mytable[\"user\"]\ntarek\n> = string.upper(mytable[\"user\"])\nTAREK",
      "content_length": 1851,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "Securing Your Services\n[ 189 ]\nTo discover the Lua language, this is your starting page h t t p ://w w w . l u a .\no r g /d o c s . h t m l .\nLua is often a language of choice to get embedded in compiled apps. Its memory footprint is\nridiculously small, and it allows to add fast dynamic scripting features. That is what is\nhappening in OpenResty. Instead of building nginx modules that require compiling nginx\nwith them, you can extend the web server using Lua scripts and deploy them directly with\nOpenResty.\nWhen you invoke some Lua code from your nginx configuration, the LuaJIT (h t t p ://l u a j i\nt . o r g /) interpreter that's employed by OpenResty will run them in a very efficient way,\nand won't be slower than nginx code itself. Some performance benchmarks find that Lua\ncan be faster than C or C++ in some cases (refer to h t t p ://l u a j i t . o r g /p e r f o r m a n c e . h t m l ).\nThe functions you can add in nginx that way are coroutines that will run asynchronously in\nnginx, so the overhead is minimal even when your server receives a lot of concurrent\nrequests, which is exactly our need for a WAF.\nOpenResty comes as Docker image and a package for some Linux distributions. It can also\nbe compiled from the ground, refer to http://openresty.org/en/installation.html. On\nmacOS, you can use Brew and the brew install openresty command.\nOnce OpenResty is installed, you will get an openresty command, and you can use it\nexactly like nginx to serve your apps.\nIn the following example, the nginx configuration will proxy calls to a Flask application\nrunning on port 5000:\n    daemon off;\n    worker_processes  1;\n    pid openresty.pid;\n    error_log /dev/stdout info;\n    events {\n      worker_connections  1024;\n    }\n    http {\n      include       mime.types;\n      default_type  application/octet-stream;\n      sendfile        on;\n      keepalive_timeout  65;\n      access_log /dev/stdout;\n      server {\n        listen       8888;",
      "content_length": 1958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "Securing Your Services\n[ 190 ]\n        server_name  localhost;\n        location / {\n          proxy_pass http://localhost:5000;\n          proxy_set_header Host $host;\n          proxy_set_header X-Real-IP $remote_addr;\n          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        }\n      }\n    }\nThis configuration can be used with the openresty command line and will run in the\nforeground (daemon off) on port 8888 to proxy pass all requests to the Flask app running\non port 5000.\n$ openresty -c resty.conf\n2017/07/14 12:10:12 [notice] 49704#524185: using the \"kqueue\" event method\n2017/07/14 12:10:12 [notice] 49704#524185: openresty/1.11.2.3\n2017/07/14 12:10:12 [notice] 49704#524185: built by clang 8.0.0\n(clang-800.0.38)\n2017/07/14 12:10:12 [notice] 49704#524185: OS: Darwin 16.6.0\n2017/07/14 12:10:12 [notice] 49704#524185: hw.ncpu: 4\n2017/07/14 12:10:12 [notice] 49704#524185: net.inet.tcp.sendspace: 1042560\n2017/07/14 12:10:12 [notice] 49704#524185: kern.ipc.somaxconn: 2048\n2017/07/14 12:10:12 [notice] 49704#524185: getrlimit(RLIMIT_NOFILE):\n7168:9223372036854775807\n2017/07/14 12:10:12 [notice] 49704#524185: start worker processes\n2017/07/14 12:10:12 [notice] 49704#524185: start worker process 49705\nNote that this configuration can also be used in a plain nginx server, since we're not using\nany Lua yet. That's what's nice with OpenResty: it's a drop-in replacement for nginx and\ncan run your existing configuration files.\nThe code and configuration demonstrated in this section can be found at h\nt t p s ://g i t h u b . c o m /R u n n e r l y /w a f .\nLua can be invoked at different moments when a request comes in, the two that are\nattractive to this chapter are:\naccess_by_lua_block: This is called on every incoming request before a\nresponse is built. This is where we can build access rules in our WAF.\ncontent_by_lua_block: This uses Lua to generate a response.\nLet's see in the next section how we can rate-limit incoming requests.",
      "content_length": 1973,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "Securing Your Services\n[ 191 ]\nRate and concurrency limiting\nRate limiting consists of counting how many requests a server is accepting in a period of\ntime, and rejecting new ones when a limit is reached.\nConcurrency limiting consists of counting how many concurrent requests are being served\nby the web server to the same remote user and starting to reject new ones when it reaches a\ndefined threshold. Since many requests can reach the server simultaneously, a concurrency\nlimiter needs to have a small allowance in its threshold.\nBoth are implemented using the same technique. Let's look at how to build a concurrency\nlimiter.\nOpenResty ships with a rate limiting library written in Lua called lua-resty-limit-\ntraffic (h t t p s ://g i t h u b . c o m /o p e n r e s t y /l u a - r e s t y - l i m i t - t r a f f i c ); you can use it in\na acces_by_lua_block section.\nThe function uses Lua Shared Dict, which is a memory mapping that is shared by all nginx\nworkers within the same process. Using a memory dict means that the rate limiting will\nwork at the process level.\nSince we're typically deploying one nginx per service node, the rate\nlimiting will happen per web server. So, if you are deploying several\nnodes for the same microservice and doing some load balancing, you will\nhave to take this into account when you set the threshold.\nIn the following example, we're adding a lua_shared_dict definition and a\naccess_by_lua_block section to activate the rate limiting. Note that this example is a\nsimplified version of the example from the project's documentation:\n    ...\n    http {\n      ...\n      lua_shared_dict my_limit_req_store 100m;\n      server {\n        access_by_lua_block {\n          local limit_req = require \"resty.limit.req\"\n          local lim, err = limit_req.new(\"my_limit_req_store\",200, 100)\n          local key = ngx.var.binary_remote_addr\n          local delay, err = lim:incoming(key, true)\n          if not delay then\n            if err == \"rejected\" then\n              return ngx.exit(503)\n              end",
      "content_length": 2042,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "Securing Your Services\n[ 192 ]\n            end\n          if delay >= 0.001 then\n            ngx.sleep(delay)\n            end\n        }\n        proxy_pass ...\n      }\n    }\nThe access_by_lua_block section can be considered as a Lua function and can use some\nof the variables and function OpenResty exposes. For instance, ngx.var is a table\ncontaining all the nginx variables and ngx.exit() is a function that can be used to\nimmediately return a response to the user. In our case, a 503 when we need to reject a call\nbecause of rate-limiting.\nThe library uses the my_limit_req_store dict that is passed to the resty.limit.req\nfunction and every time a request reaches the server, it calls the incoming() function with\nthe binary_remote_addr value, which is the client address.\nThe incoming function will use the shared dict to maintain the number of active connections\nper remote address and send back a rejected value when that number reaches the threshold,\nfor example, when there are more than 300 concurrent requests.\nIf the connection is accepted, the incoming() function sends back a delay value. Lua will\nhold the request using that delay and the asynchronous ngx.sleep() function. The delay\nwill be 0 when the remote client has not reached the threshold of 200, and a small delay\nwhen between 200 and 300, so the server has a chance to unstack all the pending requests.\nThis elegant design will be quite efficient to avoid a service to get overwhelmed by many\nrequests. Setting up a ceiling like that is also a good way to avoid reaching a point where\nyou know your microservice will start to break.\nFor instance, if some of your benchmarks concluded that your service could not serve more\nthan 100 simultaneous requests before starting to crash, you can set the rate limiting, so it's\nnginx that rejects requests instead of letting your Flask microservice pile up error logs and\nheat the CPU just to handle rejections.\nThe key used to calculate the rate in this example is the remote address\nheader of the request. If your nginx server is itself behind a proxy, make\nsure you are using a header that contains the real remote address.\nOtherwise, you will rate limit a single remote client, the proxy server. It's\nusually in the X-Forwarded-For header in that case.",
      "content_length": 2270,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "Securing Your Services\n[ 193 ]\nIf you want a WAF with more features, the lua-resty-waf (h t t p s ://g i t h u b . c o m /p 0p r 0c\nk 5/l u a - r e s t y - w a f ) project works like lua-resty-limit-traffic, but offers a lot of \nother protections. It's also able to read ModSecurity rule files, so you can use the rule files\nfrom the OWASP project without having to use ModSecurity itself.\nOther OpenResty features\nOpenResty comes with many Lua scripts that can be useful to enhance nginx. Some\ndevelopers are even using it to serve their data directly.\nIf you look at the components page at h t t p ://o p e n r e s t y . o r g /e n /c o m p o n e n t s . h t m l , you\nwill find some useful tools to have nginx interact with databases, cache servers, and so on.\nThere's also a website for the community to publish OpenResty components, refer to h t t p s\n://o p m . o p e n r e s t y . o r g /.\nIf you are using OpenResty in front of your Flask microservices, there will probably be\nother use cases where you can transfer some code that's in the Flask app to a few lines of\nLua in OpenResty. The goal should not be to move the app's logic to OpenResty, but rather\nto leverage the web server to do anything that can be done before or after your Flask app is\ncalled.\nFor instance, if you are using a Redis or a Memcache server to cache some of your GET\nresources, you can directly call them from Lua to add or fetch a cached version for a given\nendpoint. The srcache-nginx-module (h t t p s ://g i t h u b . c o m /o p e n r e s t y /s r c a c h e - n g i n x - m\no d u l e ) is an implementation of such a behavior and will reduce the number of GET calls\nmade to your Flask apps if you can cache them.\nTo conclude this section about web application firewalls, OpenResty is a powerful nginx\ndistribution that can be used to create a simple WAF to protect your microservices. It also\noffers abilities that go beyond firewalling. In fact, if you adopt OpenResty to run your\nmicroservices, it opens a whole new world of possibilities, thanks to Lua.\nThe next section that ends this chapter will focus on what can be done at the code level to\nprotect your microservices.",
      "content_length": 2166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "Securing Your Services\n[ 194 ]\nSecuring your code\nIn the previous section, we've looked at how to set up a simple WAF. The rate limiting\nfeature we've added is useful but protects us from just one possible attack. Without being\nparanoid, as soon as you are exposing your app to the world, there are numerous possible\nattacks, and your code needs to be designed with that threat in mind.\nThe idea behind secure code is simple, yet hard to do well in practice. The two fundamental\nprinciples are:\nEvery request from the outside world should be carefully assessed before it does\nsomething in your application and data\nEverything your application is doing on a system should have a well-defined and\nlimited scope\nLet's look at how to implement these principles in practice.\nAsserting incoming data\nThe first principle, assert incoming data, just means that your application should not\nblindly execute incoming requests without making sure what will be the impact.\nFor instance, if you have an API that will let a caller delete a line in a database, you need to\nmake sure the caller is allowed to do it. This is why we've added authentication and\nauthorization earlier in this chapter.\nBut there are other ways to breach in. For example, if you have a Flask view that grabs\nJSON data from the incoming request and uses it to push data to a database, you should\nverify that the incoming request has the data you are expecting, and not blindly pass it over\nto your database backend. That's why it can be interesting to use Swagger to describe your\ndata as schemas and use them to validate incoming data.\nMicroservices usually use JSON, but if you happen to use templates, that's yet another place\nwhere you need to be careful in what the template is doing with variables.\nServer-Side Template Injection (SSTI) is a possible attack when your templates are blindly\nexecuting some Python statements. In 2016, such an injection vulnerability was found on\nUber's website (h t t p s ://h a c k e r o n e . c o m /r e p o r t s /125980) on a Jinja2 template because a\nraw formatting was done before the template was executed.",
      "content_length": 2111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "Securing Your Services\n[ 195 ]\nThe code was something similar to this small app:\n    from flask import Flask, request, render_template_string\n    app = Flask(__name__)\n    SECRET = 'oh no!'\n    _TEMPLATE = \"\"\"\\\n    Hello %s\n    Welcome to my API!\n    \"\"\"\n    class Extra(object):\n        def __init__(self, data):\n            self.data = data\n    @app.route('/')\n    def my_microservice():\n        user_id = request.args.get('user_id', 'Anynomous')\n        tmpl = _TEMPLATE % user_id\n        return render_template_string(tmpl, extra=Extra('something'))\nBy doing this preformatting on the template with a raw %s, the view creates a huge security\nhole in the app, since it allows attackers to inject what they want in the Jinja script before it\ngets executed.\nIn the following example, the user_id variable security hole is exploited to read the value\nof SECRET global variable from the module:\nhttp://localhost:5000/?user_id={{extra.__class__.__init__.__globals__[\"SECR\nET\"]}}\nThat's why it's quite important to avoid doing any manual formatting with incoming data\nwhen it's used to display a view.\nIf you need to evaluate untrusted code in a template, you can use Jinja's sandbox, refer to\nhttp://jinja.pocoo.org/docs/latest/sandbox/. This sandbox will reject any access to \nmethods and attributes from the object being evaluated. For instance, if you're passing a\ncallable in your template, you will be sure that its attributes such as __class__ cannot be\nused.",
      "content_length": 1463,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "Securing Your Services\n[ 196 ]\nThat said, Python sandboxes are tricky to get right, due to the nature of the language. It's\neasy to misconfigure a sandbox, or the sandbox itself can be compromised with a new\nversion of the language. The safest bet is to avoid evaluating untrusted code altogether and\nmake sure you're not directly relying on incoming data for templates.\nAnother common place where injection happens is in SQL statements. If some of your SQL\nqueries are built using raw SQL statements, you are exposing your app to SQL injections\nexploits.\nIn the following example, a simple select query that takes a user ID can be used to inject\nextra SQL queries, such as an insert query. From there, an attacker can hack a server in no\ntime:\n    import pymysql\n    connection = pymysql.connect(host='localhost', db='book')\n    def get_user(user_id):\n        query = 'select * from user where id = %s'\n        with connection.cursor() as cursor:\n            cursor.execute(query % user_id)\n            result = cursor.fetchone()\n        return result\n    extra_query = \"\"\"\\\n    insert into user(id, firstname, lastname, password)\n    values (999, 'pnwd', 'yup', 'somehashedpassword')\n    \"\"\"\n    # this call will get the user, but also add a new user!\n    get_user(\"'1'; %s\" % extra_query)\nThis can be prevented by quoting any value used to build raw SQL queries. In PyMySQL,\nyou just need to pass the values to the execute argument to avoid this problem:\n    def get_user(user_id):\n        query = 'select * from user where id = %s'\n        with connection.cursor() as cursor:\n            cursor.execute(query, (user_id,))\n            result = cursor.fetchone()\n        return result\nEvery database library has this feature. So as long as you are correctly using these libraries\nwhen building raw SQL, you should be okay.",
      "content_length": 1824,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "Securing Your Services\n[ 197 ]\nThe same precaution goes with redirects. One common mistake is to create a login view that\nmakes the assumption that the caller will be redirected to an internal page and use a plain\nURL for that redirect:\n    @app.route('/login')\n    def login():\n        from_url = request.args.get('from_url', '/')\n        # do some authentication\n        return redirect(from_url)\nThis view can redirect the caller to any website, which is a significant threat particularly\nduring the login process. A good practice is to avoid free strings when calling redirect(),\nby using the url_for() function, which will create a link about your app domain.\nBut if you need to redirect to third parties sometimes, you can't use the url_for() and the\nredirect() functions as they can potentially send your clients to unwanted places.\nOne solution is to create a restricted list of third-party domains your application is allowed\nto redirect to and make sure any redirection done by your application or underlying third-\nparty libraries are checked against that list.\nThis can be done with the after_request() hook that will be called if the response Flask\nis about to send out. In case the application tries to send back a 302, you can check that its \nlocation is safe, given a list of domains and ports:\n    from flask import make_response\n    from urllib.parse import urlparse\n    # domain:port\n    SAFE_DOMAINS = ['github.com:443', 'ziade.org:443']\n    @app.after_request\n    def check_redirect(response):\n        if response.status_code != 302:\n            return response\n        url = urlparse(response.location)\n        netloc = url.netloc\n        if url.scheme == 'http' and not netloc.endswith(':80'):\n            netloc += ':80'\n        if url.scheme == 'https' and not netloc.endswith(':443'):\n            netloc += ':443'\n        if netloc not in SAFE_DOMAINS:\n            # not using abort() here or it'll break the hook\n            return make_response('Forbidden', 403)\n        return response",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "Securing Your Services\n[ 198 ]\nTo summarize, you should always treat incoming data as a potential threat to injection of\nattacks in your system.\nLimiting your application scope\nEven if you're doing a good job at protecting your application from bad behaviors induced\nby incoming data, you should also make sure the application itself is not able to do some\ndamage in your microservice ecosystem.\nIf your microservice is authorized to interact with other microservices, these interactions\nshould be authenticated, as we've seen earlier in this chapter, but also limited to the strict\nminimum allowed. In other words, if a microservice is performing some read calls on\nanother microservice, it should not be able to do any POST call and restricted to read-only.\nThat scope limitation can be done with the JWT tokens by defining roles (such as\nread/write) and adding that information in the token under a permissions or scope key, for\nexample. The target microservice will then be able to reject a call on a POST that is made\nwith a token that is supposed only to read data.\nThis is what happens when you grant access to an application on your GitHub account, or\non your Android phone. A detailed list of what the app wants to do is displayed, and you\ncan grant or reject access.\nIf you are controlling all parts of your microservices ecosystem, you can also use strict\nfirewalls rules at the system level to whitelist the IPs that are allowed to interact with each\nmicroservice, but that kind of set up depends a lot on where you are deploying your\napplication. In the Amazon Web Services (AWS) cloud environment, you don't need to\nconfigure a Linux firewall. All you have to do is set simple access rules in the AWS console.\nChapter 11, Deploying on AWS, covers the basics of deploying your microservices on the\nAmazon cloud.\nBesides network accesses, any other resource your application can access should be limited\nwhenever possible. Running the application as a root user on Linux is not a good idea\nbecause, in case of a security issue, you are giving full power to the service.\nFor instance, if your application is calling the system and that call gets hacked by an\ninjection or another exploit, it's a backdoor for an attacker to own the whole operating\nsystem.",
      "content_length": 2266,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "Securing Your Services\n[ 199 ]\nRoot access to a system has become an indirect threat in modern deployments, since most\napplications are running in Virtual Machines (VM), but an unrestricted process can still do\na lot of damage even if jailed. If an attacker owns one of your VMs, it's the first step to own\nthe whole system.\nTo mitigate the problem, there are two rules you should follow:\nA web service process should be run by a non-root user\nBe very cautious when executing processes from your web service and avoid it if\nyou can.\nFor the first rule, the default behavior for web servers such as NGinx is to run its processes\nusing the www-data user and group, and that prevents these processes from being able to\nexecute anything on the system. The same rules apply to your Flask processes. We'll see in\nChapter 9, Packaging Runnerly, the best practices to run a stack in the user space on a Linux\nsystem.\nFor the second rule, any Python call to os.system(), subprocess, multiprocessing\nshould be double-checked to avoid making unwanted calls on the system. This is also true\nfor high-level network modules that send emails or connect to third-party servers via FTP,\nvia the local system.\nThere's a way to continuously check your code for potential security issues using the Bandit\nlinter.\nUsing Bandit linter\nThe OpenStack community (h t t p s ://w w w . o p e n s t a c k . o r g /) created a nice little security\nlinter called Bandit to try to catch insecure code (h t t p s ://w i k i . o p e n s t a c k . o r g /w i k i /S e c u\nr i t y /P r o j e c t s /B a n d i t ).\nThe tool uses the ast module to parse the code such as Flake8 or other linters. Bandit will\nscan for some known security issues in your code.\nOnce you've installed it with the pip install bandit command, you can run it against\nyour Python module using the bandit command.",
      "content_length": 1850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "Securing Your Services\n[ 200 ]\nThe following script is an example of three unsafe functions. The first one will let you load\nYAML content that might instantiate arbitrary objects, and the following ones are prone to\ninjection attacks:\n    import subprocess\n    from sqlalchemy import create_engine\n    from sqlalchemy.orm import sessionmaker\n    import yaml\n    def read_file(filename):\n        with open(filename) as f:\n            data = yaml.load(f.read())\n    def run_command(cmd):\n        return subprocess.check_call(cmd, shell=True)\n    db = create_engine('sqlite:///somedatabase')\n    Session = sessionmaker(bind=db)\n    def get_user(uid):\n        session = Session()\n        query = \"select * from user where id='%s'\" % uid\n        return session.execute(query)\nRunning Bandit over that script will detect the three issues and explain the problems in\ndetail:\n$ bandit bandit_example.py\n...\nRun started:2017-03-20 08:47:06.872002\nTest results:\n>> Issue: [B404:blacklist] Consider possible security implications\nassociated with subprocess module.\n   Severity: Low   Confidence: High\n   Location: bandit_example.py:1\n1  import subprocess\n2  from sqlalchemy import create_engine\n3  from sqlalchemy.orm import sessionmaker\n--------------------------------------------------\n>> Issue: [B506:yaml_load] Use of unsafe yaml load. Allows instantiation of\narbitrary objects. Consider yaml.safe_load().\n   Severity: Medium   Confidence: High\n   Location: bandit_example.py:9\n bandit_example.py",
      "content_length": 1490,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "Securing Your Services\n[ 201 ]\n8      with open(filename) as f:\n9          data = yaml.load(f.read())\n10\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call\nwith shell=True identified, security issue.\n   Severity: High   Confidence: High\n   Location: bandit_example.py:13\n12 def run_command(cmd):\n13     return subprocess.check_call(cmd, shell=True)\n14\n--------------------------------------------------\n>> Issue: [B608:hardcoded_sql_expressions] Possible SQL injection vector\nthrough string-based query construction.\n   Severity: Medium   Confidence: Low\n   Location: bandit_example.py:23\n22     session = Session()\n23     query = \"select * from user where id='%s'\" % uid\n24     return session.execute(query)\n--------------------------------------------------\n...\nFiles skipped (0):\nFor this book, we are using the version Bandit 1.4.0. It has 64 security checks included, and\nis very easy to extend if you want to create your own checks. You can also tweak its\nconfiguration by creating a configuration file in your project.\nOne security check, for instance, will emit a security warning in case your are running Flask\nin debug mode, since this is a security issue in production. Consider the following example:\n$ bandit flask_app.py\n...\nTest results:\n>> Issue: [B201:flask_debug_true] A Flask app appears to be run with\ndebug=True, which exposes the Werkzeug debugger and allows the execution of\narbitrary code.\n   Severity: High   Confidence: Medium\n   Location: flask_app.py:15\n14 if __name__ == '__main__':\n15     app.run(debug=True)",
      "content_length": 1611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "Securing Your Services\n[ 202 ]\nThis is a great check when shipping in production, but when developing your application,\nyou will want to turn this one off. Excluding your test's modules for security scanning is\nalso a good idea.\nThe following configuration file, which can be used with the ini option will ignore that\nissue and exclude tests/ files:\n[bandit]\nskips: B201\nexclude: tests\nAdding a bandit call in your continuous integration pipeline alongside\ntools such as coveralls, as described in Chapter 3, Coding, Testing, and\nDocumenting - The Virtuous Cycle, is a good way to catch potential security\nissues in your code.\nSummary\nIn this chapter, we've looked at how to centralize authentication and authorization in a\nmicroservices-based application environment using OAuth2 and JWT tokens. Tokens give\nus the ability to limit what and for how long a caller can do on one of the microservices.\nWhen used with public/private keys, it also prevents an attacker that breaks into one service\nto break the whole app, as long as it's not the token issuer that's compromised.\nBeyond system-level firewall rules, a Web Application Framework is also a good way to\nprevent some fraud and abuse on your endpoints and is very easy to do with a tool such as\nOpenResty, thanks to the power of the Lua programming language.\nOpenResty is also an excellent way to empower and speed up your microservices by doing\na few things at the web server level when it does not need to be done within the Flask\napplication.",
      "content_length": 1501,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "Securing Your Services\n[ 203 ]\nLastly, a secure code base is the first step to a secure application. You should follow good\ncoding practices and make sure your code does not do anything stupid when interacting\nwith incoming user data and resources. While a tool like Bandit will not magically make\nyour code safe and secure, it will catch the most obvious potential security issues, so there's\nno hesitation to continuously run it on your code base.\nOne part that we did not cover in this chapter is how an end user is securely interacting\nwith our microservices. This is covered in the next chapter, where we will wrap up\neverything and demonstrate how the Runnerly application can be used through a client-\nside JavaScript application.",
      "content_length": 737,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "8\nBringing It All Together\nMost of the work done so far has focused on building microservices, and making them\ninteract with each other. It is time to bring everything together by creating the tip of the\niceberg--the User Interface (UI) through which our end users use the whole system with a\nbrowser.\nModern web applications rely a lot on client-side JavaScript (JS). Some JS frameworks go\nall the way to provide a full Model-View-Controller (MVC) system, which runs in the\nbrowser and manipulates the Document Object Model (DOM), which is the structured\nrepresentation of the web page that's rendered in your browser.\nThe web development paradigm has shifted from rendering everything on the server side,\nto rendering everything on the client side with data collected from the server on demand.\nThe reason is that modern web applications change portions of a loaded web page\ndynamically instead of calling the server for a full rendering. It is faster, requires less\nnetwork bandwidth, and offers a richer user experience. One of the biggest examples of this\nshift is the Gmail app, which pioneered the client-side field circa 2004.\nTools like Facebook's ReactJS (h t t p s ://f a c e b o o k . g i t h u b . i o /r e a c t /) provide high-level\nAPIs to avoid manipulating the DOM directly, and offer a level of abstraction, which makes\nclient-side web development as comfortable as building Flask applications.\nThat said, there is a new JS framework every other week, and it is hard to decide which one\nshould be used. AngularJS (h t t p s ://a n g u l a r j s . o r g /) used to be the coolest toy, and now\nit seems many developers have switched to implement most of their application UIs with\nplain ReactJS. Moreover, maybe later in 2017, another new player will be popular.",
      "content_length": 1779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "Bringing It All Together\n[ 205 ]\nThis volatility is not a bad sign at all. It simply means much innovation is happening in the\nJavaScript and browsers ecosystem. Features like Service Workers (h t t p s ://d e v e l o p e r . m o\nz i l l a . o r g /e n /d o c s /W e b /A P I /S e r v i c e _ W o r k e r _ A P I ), for instance, are game changers in\nweb development, because they allow developers to run JS code in the background,\nnatively. A new wave of JS tools will probably emerge from that new feature.\nAs long as you have a clean separation between your UI and the rest of the system, moving\nfrom one JS framework to the other should not be too hard. That means, you should not\nchange how your microservices publish data to make them specific to a JS framework.\nFor Runnerly, we shall use ReactJS to build our little dashboard, and we will wrap it in a\ndedicated Flask application, which bridges it to the rest of the system. We will also see how\nthat app can interact with all our microservices.\nThis chapter is composed of the following three parts:\nBuilding a ReactJS dashboard--a short introduction to ReactJS with an example\nHow to embed ReactJS in a Flask app\nAuthentication and authorization\nBy the end of this chapter, you should have a good understanding of how to build a web UI\nin Flask, and how to make it interact with microservices whether you choose to use ReactJS\nor not.\nBuilding a ReactJS dashboard\nThe ReactJS framework implements its abstraction of the DOM, and makes all the event\nmachinery fast and efficient. Creating a UI using ReactJS consists of creating some classes\nwith a few methods, which are called by the engine when the page is created or updated.\nThis approach means that you do not have to worry about what will happen when the\nDOM changes anymore. All you have to do is implement some methods, and let React take\ncare of the rest.\nImplementing classes for React can be done in JavaScript or JSX. We will discuss about it in\nthe next section.",
      "content_length": 1985,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "Bringing It All Together\n[ 206 ]\nThe JSX syntax\nThe JSX syntax extension (h t t p s ://f a c e b o o k . g i t h u b . i o /j s x /) adds XML tags to JS, and\ncan be used by tools like ReactJS when the rendering of the page happens. It is promoted by\nthe ReactJS community as the best way to write React apps.\nIn the following example, a <script> section contains a div variable whose value is an\nXML tree representing a div. This syntax is valid JSX. From there, the ReactDOM.render()\nfunction can render the div variable in the DOM.\n    <!DOCTYPE html>\n    <html>\n      <head lang=\"en\">\n        <meta charset=\"UTF-8\">\n      </head>\n      <body>\n        <div id=\"content\"></div>\n        <script src=\"/static/react/react.min.js\"></script>\n        <script src=\"/static/react-dom.min.js\"></script>\n        <script src=\"/static/babel/browser.min.js\"></script>\n        <script type=\"text/babel\">\n          var div =\n              <div>\n                  Hello World\n              </div>\n         ReactDOM.render(div, document.getElementById('content'));\n        </script>\n      </body>\n    </html>\nThe two ReactJS scripts are part of the React distribution. The browser.min.js file is part\nof the Babel distribution, and needs to be loaded before the browser encounters any JSX\nsyntax. Babel converts JSX syntax into JS. This conversion is called transpilation.\nBabel (h t t p s ://b a b e l j s . i o /) is a transpiler, which can convert JSX to JS\non-the-fly, among other available conversions. To use it, you simply need\nto mark a script as being of type text/babel.\nThe JSX syntax is the only very specific thing to know about React, as everything else is\ndone with common JavaScript language. From there, building a ReactJS app consists of\ncreating JS classes--with or without JSX--which is used to render web pages.\nLet's now look at the heart of ReactJS, components.",
      "content_length": 1868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "Bringing It All Together\n[ 207 ]\nReact components\nReactJS is based on the idea that the page can be decomposed into basic components, which\nare called for rendering parts of the page.\nFor example, if you want to display a list of runs, you can create a Run class that is in charge\nof rendering a single run given its values, and a Runs class that iterates through a list of\nruns, and call the Run class to render each item.\nEach class is created with the React.createClass() function, which receives a mapping\ncontaining the future class methods. The createClass() function generates a new class,\nand sets a props attribute to hold some properties alongside the provided methods.\nIn the following example, in a new JavaScript file we define a Run class with a render()\nfunction, which returns a <div> tag, and a Runs class:\n    var Run = React.createClass( {\n      render: function()  {\n        return (\n          <div>{this.props.title} ({this.props.type})</div>\n        );\n      }\n    } );\n    var Runs = React.createClass( {\n      render: function()  {\n        var runNodes = this.props.data.map(function (run)  {\n          return (\n            <Run\n              title= {run.title}\n              type= {run.type}\n            />\n          );\n        } );\n        return (\n          <div>\n            {runNodes}\n          </div>\n        );\n      }\n    } );\nThe Run class returns in a div this value: {this.props.title} ({this.props.type}),\nwhich is rendered by visiting the props attribute in the Run instance.",
      "content_length": 1512,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "Bringing It All Together\n[ 208 ]\nThe props array is populated when the Run instance is created, and that is what happens in\nthe render() method of the Runs class. The runNode variable iterates through the\nRuns.props.data list, which contains a list of runs.\nThat is our last piece of the puzzle. We want to instantiate a Runs class, and put a list of\nruns to be rendered by React in its props.data list.\nIn our Runnerly app, this list can be provided by the microservice that publishes runs, and\nwe can create another React class, which loads this list asynchronously using an\nAsynchronous JavaScript and XML (AJAX) pattern via an HxmlHttpRequest class.\nThat is what happens in the loadRunsFromServer() method in the following example.\nThe code calls the server to get the data by making a GET request on the URL set in the\nprops, and sets the value of props.data by calling the setState() method.\n    var RunsBox = React.createClass( {\n      loadRunsFromServer: function()  {\n        var xhr = new XMLHttpRequest();\n        xhr.open('get', this.props.url, true);\n        xhr.onload = function()  {\n          var data = JSON.parse(xhr.responseText);\n          this.setState( { data: data } );\n        } .bind(this);\n        xhr.send();\n      } ,\n      getInitialState: function()  {\n        return  {data: []} ;\n      } ,\n      componentDidMount: function()  {\n        this.loadRunsFromServer();\n      } ,\n      render: function()  {\n        return (\n          <div>\n            <h2>Runs</h2>\n            <Runs data= {this.state.data}  />\n          </div>\n        );\n      }\n    } );\n    // this will expose RunsBox globally\n    window.RunsBox = RunsBox;",
      "content_length": 1654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "Bringing It All Together\n[ 209 ]\nWhen the state changes, it triggers the React class to update the DOM with the new data.\nThe framework calls the render() method, which displays the <div> containing Runs.\nThe Runs instance, and then each Run instance, are handed down in a cascade.\nTo trigger the loadRunsFromServer() method, the class implements the\ncomponentDidMount() method, which gets called once the class instance is created and\nmounted in React, ready to be displayed. Last, but not the least, the getInitialState()\nmethod is called on instantiation, and can be used to initialize the instance of the props\nattribute with an empty data array.\nThis whole process of decomposition and chaining may seem complicated, but once in\nplace, it is quite powerful, because it allows you to focus on rendering each component and\nletting React deal with how to do it in the most efficient way in the browser.\nEach component has a state, and when something changes, React first updates its own\ninternal representation of the DOM--the virtual DOM. Once that virtual DOM is changed,\nReact can apply the required changes efficiently on the actual DOM.\nAll the JSX code we've seen in this section can be saved in a JSX module, and used in an\nHTML page as follows:\n    <!DOCTYPE html>\n    <html>\n      <head lang=\"en\">\n        <meta charset=\"UTF-8\">\n        <title>Runnerly Dashboard</title>\n      </head>\n      <body>\n        <div class=\"container\">\n          <h1>Runnerly Dashboard</h1>\n          <br>\n          <div id=\"runs\"></div>\n        </div>\n        <script src=\"/static/react/react.js\"></script>\n        <script src=\"/static/react/react-dom.js\"></script>\n        <script src=\"/static/babel/browser.min.js\"></script>\n        <script src=\"/static/runs.jsx\" type=\"text/babel\"></script>\n        <script type=\"text/babel\">\n        ReactDOM.render(\n          <window.RunsBox url=\"/api/runs.json\" />,\n          document.getElementById('runs')\n        );\n       </script>\n      </body>\n    </html>",
      "content_length": 1989,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "Bringing It All Together\n[ 210 ]\nThe RunsBox class is instantiated with the /api/runs.json URL for this demo, and once\nthe page is displayed, React calls that URL, and expects to get back a list of runs, which it\npasses down to the Runs and Run instances.\nNotice that we have used window.RunsBox instead of RunBox, because the Babel transpiler\ndoes not expose the global variables from the runs.jsx file. That is why we had to set the\nvariable as an attribute of the window variable so it could be shared between the <script>\nsections.\nUsing transpiration directly into the browser is a bad idea. It is much better to transpile\nyour JSX files beforehand, as we will see in the next section.\nThis section described a very basic usage of the ReactJS library, and did\nnot dive into all its possibilities. If you want to get more info on React, you\nshould try the tutorial at h t t p s ://f a c e b o o k . g i t h u b . i o /r e a c t /t u t o r i a l\n/t u t o r i a l . h t m l as your first step. This tutorial shows you how your React\ncomponents can interact with the user through events, which is the next\nstep once you know how to do some basic rendering.\nNow that we have the basic layout for building a React-based UI, let's see how we can\nembed it in our Flask world.\nReactJS and Flask\nPeople building React apps usually code their server-side parts in Node.js (h t t p s ://n o d e j s\n. o r g /e n /), because it is simpler to stick with a single language and use its ecosystem for all\nthe tools that are used when working with an application.\nHowever, serving React apps with Flask is not a problem at all. The HTML page can be\nrendered using Jinja2, and the transpiled JSX files serve as static files like you would do for\nJavaScript files. Moreover, as we have seen in the previous section, we can get the React\ndistribution as JS files, and just add them into our Flask static directory alongside other files.",
      "content_length": 1920,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "Bringing It All Together\n[ 211 ]\nOur Flask app, let's name it dashboard, will start off with a simple structure like this:\nsetup.py\ndashboard/\n__init__.py\napp.py\ntemplates/\nindex.html\nstatic/\nruns.jsx\nAlso, the app.py file, a basic Flask application that serves the unique HTML file, will be\nlike this:\n    from flask import Flask, render_template,\n    app = Flask(__name__)\n    @app.route('/')\n    def index():\n        return render_template('index.html')\n    if __name__ == '__main__':\n        app.run()\nThanks to Flask's convention on static assets, all the files contained inside the static/\ndirectory is served under the /static URL.\nThe index.html template looks like the one described in the previous section, and can\ngrow into something Flask-specific later on.\nThat is all we need to serve a ReactJS-based app from Flask. However, dropping ReactJS\ndistributions into your Flask static repository is not the best way to maintain your project.\nWe need something better to manage JS dependencies. Moreover, the JavaScript world has\ngreat tools to do it as we will see in the next section.\nUsing Bower, npm, and Babel\nSo far, we have used static JavaScript files to build our React UI in a Flask app. However,\nlike the JS community does, it is much better to handle React and any other Javascript\nlibrary as a package we want to update--like how we do with Python packages regularly.",
      "content_length": 1388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "Bringing It All Together\n[ 212 ]\nTo do this, we can install the JavaScript package manager on our system npm (h t t p s ://w w w\n. n p m j s . c o m /). The npm package manager is installed via Node.js. On macOS, the brew\ninstall node command does the trick, or you can go to the Node.js home page (h t t p s\n://n o d e j s . o r g /e n /), and download it to the system.\nOnce Node.js and npm are installed, you should be able to call the npm command from the\nshell as follows:\n$ npm -v\n3.5.2\nTo manage JavaScript dependencies in our Flask project, we will use Bower (h t t p s ://b o w e\nr . i o /), a package manager for web applications, which leverages npm to package all JS\ndependencies required for a web app--like PIP does for Python packages.\nTo install Bower, use the npm command like this:\n$ npm install -g bower\nThe -g switch means that Bower is installed globally in your system's npm, and if the\ninstallation worked, you should get a new bower command-line utility.\nOnce Bower is installed, you can go to the root of your Flask Dashboard app, and run the\ninteractive init command like this:\n$ bower init\n? name dashboard\n? description A ReactJS based Dashboard for Runnerly\n? authors Tarek Ziade <tarek@ziade.org>\n...\n{\n  name: 'dashboard',\n  authors: [\n    'Tarek Ziade <tarek@ziade.org>'\n  ],\n  description: 'A ReactJS based Dashboard for Runnerly',\n  main: '',\n  license: 'MIT',\n  homepage: '',\n  ignore: [\n    '**/.*',\n    'node_modules',\n    'bower_components',\n    'test',\n    'tests'",
      "content_length": 1503,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "Bringing It All Together\n[ 213 ]\n  ]\n}\n? Looks good? Yes\nAfter a few questions, the call creates a bower.json configuration file, which is used by\nBower when grabbing the JavaScript libraries.\nSince we want to serve the JavaScript files from our Flask app (and in production from\nnginx, for instance), we also tell Bower the location of the static directory by adding a\n.bowerrc file with this content:\n{\"directory\": \"dashboard/static\"}\nNow, if we call Bower's install command to install React and jQuery, the static directory is\nautomatically populated with both libraries.\n$ bower install --save jquery react\n...\njquery#3.2.1 dashboard/static/jquery\nreact#15.4.2 dashboard/static/react\nThe preceding call will also populate the bower.json file with those dependencies. This\nmechanism is an excellent way to keep track of dependencies when the project gets\nreinstalled. Think of it as a PIP requirements.txt file automatically populated when you\ncall the pip install command.\nWe also need to install the Babel transpiler with npm to transpile the JSX files into JS files\nand its React preset, as follows:\n$ npm init\n$ npm install -save-dev babel-cli babel-preset-react\nThese preceding calls install the packages locally, and create a package.json file, which is\nsimilar to the bower.json one. Moreover, the babel command line is made available in\nnode_modules/.bin/.\nFrom there, running this command converts all our JSX files into a single, plain JS file called\ndashboard.js.\n$ node_modules/.bin/babel dashboard/static/*.jsx >\ndashboard/static/dashboard.js\nOnce this Babel command is called, our Flask template can use the JS version of the React\nclasses by pointing to the JS file instead of the JSX file. In that case, there's no need to do a\nclient-side transpilation on the fly.",
      "content_length": 1784,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "Bringing It All Together\n[ 214 ]\nIt also means that all the global variables we have in our JSX files are now visible\neverywhere, so, we do not need to hook them on the window variable.\nWe can also move the ReactDOM.render() method call, which we had in a dedicated\n<script> section, into a dedicated zrender.jsx file, like this:\nReactDOM.render(\n  <RunsBox url=\"/api/runs.json\" />,\n  document.getElementById('runs')\n);\nNotice that the file starts with a z to ensure that Babel injects it at the end of the\ndashboard.js file when it generates it - since scripts are treated in alphabetical order. This \nensures that the RunBox class and any other needed variable or JS element are defined\nbefore the render call.\nThere are other ways to handle inter-module dependencies. Tools like\nRequireJS (h t t p ://w w w . r e q u i r e j s . o r g /) offer an interesting approach to\nsolve this issue. However, for our little dashboard backed by Flask, which\ndoes not have a lot of JS files, what was presented should be good enough.\nWith all the changes, the final index.html file will look like this:\n    <!DOCTYPE html>\n    <html>\n      <head lang=\"en\">\n        <meta charset=\"UTF-8\">\n        <title>Runnerly Dashboard</title>\n      </head>\n      <body>\n        <div class=\"container\">\n          <h1>Runnerly Dashboard</h1>\n          <br>\n          <div id=\"runs\"></div>\n        </div>\n        <script src=\"/static/react/react.js\"></script>\n        <script src=\"/static/react/react-dom.js\"></script>\n        <script src=\"/static/dashboard.js\"></script>\n      </body>\n    </html>\nThroughout this section, we have worked with the assumption that the JSON data that\nReact picked was served by the same Flask app at the /api/runs.json endpoint.",
      "content_length": 1733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "Bringing It All Together\n[ 215 ]\nDoing AJAX calls on the same domain is not an issue, but in case you need to call a\nmicroservice that belongs to another domain, there are a few changes required on both the\nserver and the client side.\nLet's see how to do cross-domain calls in the next section.\nCross-origin resource sharing\nAllowing client-side JavaScript AJAX to perform cross-domains requests is a potential\nsecurity risk. If the JS code that's executed in the client page for your domain tries to call\nanother domain that you don't own, it could potentially run malicious JS code and harm\nyour users.\nThat is why all browsers in the market have a Same-Origin Policy when an asynchronous call\nis made. They ensure that the request is made on the same domain.\nBeyond security, it is also a good way to prevent someone from using your bandwidth for\ntheir web app. For instance, if you provide a few font files on your website, you might not\nwant another website to use them on their page, and use your bandwidth without any\ncontrol.\nHowever, there are legitimate use cases for wanting to share your resources to other\ndomains, and you can set up rules on your service to allow other domains to reach your\nresources.\nThat is what Cross-Origin Resource Sharing (CORS) is all about. When the browser sends\nan AJAX request to your service, an Origin header is added, and you can control that it is\nin the list of authorized domains.\nIf not, the CORS protocol requires that you send back a few headers listing the allowed\ndomains.\nThere's also a preflight mechanism, where the browser pokes the endpoint via an OPTIONS\ncall to know if the request it wants to make is authorized.\nOn the client side, you do not have to worry about setting up these mechanisms. The\nbrowser makes the decisions for you depending on your requests.",
      "content_length": 1822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "Bringing It All Together\n[ 216 ]\nHowever, on the server side, you need to make sure your endpoints answer to the OPTIONS\ncalls, and you need to decide which domains are allowed to reach your resources. If your\nservice is public, you can authorize all domains with a wildcard. However, for a\nmicroservice-based application where you control the client side, you should restrict the\ndomains.\nIn Flask, you can use Flakon's crossdomain() decorator to add CORS support to an API\nendpoint. In the following Flask app, the /api/runs.json endpoint can be used by any\ndomain:\n    from flask import Flask, jsonify\n    from flakon import crossdomain\n    app = Flask(__name__)\n    @app.route('/api/runs.json')\n    @crossdomain()\n    def _runs():\n        run1 = {'title': 'Endurance', 'type': 'training'}\n    run2 = {'title': '10K de chalon', 'type': 'race'}\n    _data = [run1, run2]\n    return jsonify(_data)\n    if __name__ == '__main__':\n        app.run(port=5002)\nWhen running this app and using cURL to do a GET request, we can see that the Access-\nControl-Allow-Origin:* header is added:\n$ curl -v http://localhost:5002/api/runs.json\n*   Trying localhost...\n* TCP_NODELAY set\n* Connected to localhost (127.0.0.1) port 5002 (#0)\n> GET /api/runs.json HTTP/1.1\n> Host: localhost:5002\n> User-Agent: curl/7.51.0\n> Accept: */*\n>\n* HTTP 1.0, assume close after body\n< HTTP/1.0 200 OK\n< Content-Type: application/json\n< Access-Control-Allow-Origin: *\n< Content-Length: 122\n< Server: Werkzeug/0.12.1 Python/3.5.2\n< Date: Tue, 04 Apr 2017 07:39:48 GMT\n<\n[",
      "content_length": 1539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "Bringing It All Together\n[ 217 ]\n  {\n    \"title\": \"Endurance\",\n    \"type\": \"training\"\n  },\n  {\n    \"title\": \"10K de chalon\",\n    \"type\": \"race\"\n  }\n]\n* Curl_http_done: called premature == 0\n* Closing connection 0\nThis is the default permissive behavior of the crossdomain() decorator, but you can set up\nfine-grained permissions for each endpoint, and restrict them to specific domains. You can\neven whitelist allowed HTTP verbs. Flakon also has CORS features at the blueprint level.\nFor our use case, allowing a domain is good enough. If your JS app is served by a Flask app\nthe runs on localhost:5000 for instance, you can restrict calls to that domain with the\nfollowing:\n    @app.route('/api/runs.json')\n    @crossdomain(origins=['http://localhost:5000'])\n    def _runs():\n        ...\nIn case a call is made from a browser with origin other than http://localhost:5000, the\ndata is not returned.\nNotice that in the case of rejection on a disallowed domain, the decorator returns a 403\nresponse. The CORS protocol does not define what should be the status code when a\nrejection happens, so, that is an implementation choice.\nFor an in-depth understanding of CORS, the MDN page is a great resource\nand can be found at the following link: h t t p s ://d e v e l o p e r . m o z i l l a . o r g\n/e n - U S /d o c s /W e b /H T T P /A c c e s s _ c o n t r o l _ C O R S\nIn this section, we have looked at how to set up CORS headers in our services to allow\ncross-domain calls, which are useful in JS apps.\nWhat's still missing to make our JS app fully functional is authentication and authorization.",
      "content_length": 1598,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "Bringing It All Together\n[ 218 ]\nAuthentication and authorization\nThe React dashboard needs to be able to authenticate its users, and perform authorized calls\non some microservices. It also needs to let the user grant access to Strava.\nWe make the assumption that the dashboard only works when you are authenticated, and\nthat there are two kinds of users: first-time user and returning user.\nFollowing is the user story for first-time users:\nAs a first-time user, when I visit the dashboard, there's a \"login\" link. When I click on it,\nthe dashboard redirects me to Strava to grant access to my resources. Strava then redirects\nme back to the dashboard, and I am connected. The dashboard then starts to fill with my\ndata.\nAs described, our Flask app performs an OAuth2 dance with Strava to authenticate users.\nConnecting to Strava also means we need to store the access token into the Runnerly user\nprofile so we can use it to fetch runs later on.\nBefore going further, we need to make a design decision: do we want the dashboard\nmerged with the DataService, or do we want to have two separate apps?\nInteracting with Data Service\nWe have said in Chapter 4, Designing Runnerly, that a safe approach to designing\nmicroservices is to avoid creating new ones without a good reason.\nThe database that holds user data is served by the DataService microservice, which is used\nby the Celery workers. The first option that comes to mind is to have a single Flask\napplication, which manages that database, and serves both our end users with its HTML\nand JS content and other microservices with its JSON APIs.\nThe benefit of this approach is that we do not need to worry about implementing yet\nanother network interaction between the dashboard and DataService. Moreover, besides the\nReactJS app, there's not a lot we need to add on top of DataService to make it usable for\nboth use cases.\nHowever, by doing this, we are not benefiting from one of the advantages of microservices.\nEach microservice focuses on doing a single thing.",
      "content_length": 2019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "Bringing It All Together\n[ 219 ]\nWhile it is always safer to start with a conservative approach, let's think for a minute how a\nsplit would impact our design. If the dashboard is on its own, it needs to drive DataService\nto create and change users' info in DataService. This means that DataService needs to\nexpose some HTTP APIs to do this. The biggest risk of exposing a database via HTTP is that\nwhenever it changes, the API might get impacted.\nHowever, that risk can be limited if the exposed endpoints hide the database structure as\nmuch as possible, the opposite of CRUD-like APIs.\nFor example, the API to create a user in DataService could be a POST that just asks for the\nuser's Strava token and e-mail, and returns some user ID. This information should rarely\nchange, and the dashboard can simply act as a proxy between the users and DataService.\nA significant benefit of having the Dashboard app isolated from the DataService is stability.\nWhen building an application like Runnerly, developers often reach a point where the core\nof the application is stable, and then they iterate a lot on the User Interface (UI) and User\nExperience (UX). In other words, the dashboard is probably going to evolve a lot while the\nDataService app should reach a stable point quite fast.\nFor all those reasons, having two separate apps for the dashboard and DataService sounds\nlike a low risk.\nNow that the design decision is made, let's look at how to perform the OAuth2 dance with\nStrava.\nGetting the Strava token\nStrava provides a typical three-legged OAuth2 implementation, and stravalib (h t t p s ://g i t\nh u b . c o m /h o z n /s t r a v a l i b ), all the tools to use it.\nImplementing the dance is done by redirecting the user to Strava and exposing an endpoint\nthe user is redirected to once granted access to Strava.\nWhat we get in return is the user info from its Strava account along with the token access.\nWe can store all this info in the Flask session, use it as our login mechanism, and pass the e-\nmail and token values to DataService so that the Celery strava worker can also use the\ntoken.",
      "content_length": 2102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "Bringing It All Together\n[ 220 ]\nLike we did in Chapter 4, Designing Runnerly, let's implement the function that generates\nthe URL to send the user to, as follows:\n    from stravalib.client import Client\n    def get_strava_url():\n        client = Client()\n        cid = app.config['STRAVA_CLIENT_ID']\n        redirect = app.config['STRAVA_REDIRECT']\n        url = client.authorization_url(client_id=cid,\nredirect_uri=redirect)\n        return url\nThat function takes client_id from the Runnerly application (generated in the Strava API\nsettings panel) and the redirect URL defined for the dashboard, and returns a URL we can\npresent to the user.\nThe dashboard view can be changed accordingly to pass that URL to the template.\n    from flask import session\n    @app.route('/')\n    def index():\n        strava_url = get_strava_url()\n        user = session.get('user')\n        return render_template('index.html', strava_url=strava_url,\n                                user=user)\nWe also pass a user variable if there's any stored into the session. The template can then\nuse the Strava URL to display a login/logout link as follows:\n      {% if not user %}\n      <a href=\"{{strava_url}}\">Login via Strava</a>\n      {% else %}\n      Hi {{user}}!\n      <a href=\"/logout\">Logout</a>\n      {% endif %}\nWhen the user clicks on the login link, she is redirected to Strava and back to our \napplication on the endpoint defined by STRAVA_REDIRECT.\nThe implementation of that view can be like this\n    @app.route('/strava_redirect')\n    def strava_login():\n        code = request.args.get('code')\n        client = Client()\n        cid = app.config['STRAVA_CLIENT_ID']",
      "content_length": 1653,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "Bringing It All Together\n[ 221 ]\n        csecret = app.config['STRAVA_CLIENT_SECRET']\n        access_token = client.exchange_code_for_token(client_id=cid,\nclient_secret=csecret, code=code)\n        athlete = client.get_athlete()\n        email = athlete.email\n        session['user'] = email\n        session['token'] = access_token\n        send_user_to_dataservice(email, access_token)\n        return redirect('/')\nThe stravalib library's Client class converts the code with a token we can store in the\nsession, and lets us grab some info on the user using the get_athlete() method.\nLastly, the send_user_to_dataservice(email, access_token) can interact with the\nDataService microservice to make sure the e-mail and access tokens are stored there, using a\nJWT-based access.\nWe are not detailing how Dashboard interacts with the TokenDealer, since we have already\nshown it in Chapter 7, Securing Your Services. The process is similar--the Dashboard app\ngets a token from TokenDealer, and uses it to access DataService.\nThe last part of authentication is in the ReactJS code, as we will see in the next section.\nJavaScript authentication\nWhen the Dashboard app performs the OAuth2 dance with Strava, it stores user \ninformation into the session, which is perfect to have the user authenticate the dashboard.\nHowever, when the ReactJS UI calls the DataService microservice to display the user runs,\nwe need to provide an authentication header.\nThere are the following two ways to handle this problem:\nProxy all the calls to the microservices via the Dashboard web app using the\nexisting session information\nGenerate a JWT token for the end user, which they can store and use against\nanother microservice\nThe proxy solution is the simplest one by all means, because it removes the need to generate\none token per user for accessing DataService. It also prevents us from exposing DataService\npublicly. Hiding everything behind the dashboard means we have more flexibility to\nchange the internals while keeping the UI compatible.",
      "content_length": 2020,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "Bringing It All Together\n[ 222 ]\nThe problem with that approach, though, is that we are forcing all the traffic through the\nDashboard service even when it is not needed. Ideally, updating the list of displayed runs\nshould not be something that the Dashboard server should worry about.\nThe second solution is more elegant with the microservices design. We are dealing tokens,\nand the Web UI is just one of the clients for some microservices. However, that also means\nthe client has to deal with two authentication loops. If the JWT token gets revoked, the\nClient app needs to authenticate back even if the Strava token is still valid.\nThe first solution is probably the best bet for a first version. Proxying calls to microservices\non behalf of the user means that the Dashboard application uses its JWT token to call\nDataService to grab the user data.\nDataService, as explained in Chapter 4, Designing Runnerly, uses the following API pattern\nto return runs: GET /runs/<user_id>/<year>/<month>.\nIf we make the assumption that the Dashboard keeps track of the (e-mail, user ID) tuples,\nthe proxy view for that API can be GET /api/runs/<year>/<month>. From there, the\nDashboard code can find back the user ID given the user e-mail currently logged into the\nsession via Strava.\nThe proxy code can look like this:\n    @app.route('/api/runs/<int:year>/<int:month>')\n    def _runs(year, month):\n        if 'user' not in session:\n            abort(401)\n        uid = email_to_uid(session['user'])\n        endpoint = '/runs/%d/%d/%d' % (uid, year, month)\n        resp = call_data_service(endpoint)\n        return jsonify(resp.json())\nThe call_data_service() function calls the DataService endpoint with a JWT token, and\nemail_to_uid() converts the e-mail to the corresponding user ID.",
      "content_length": 1776,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "Bringing It All Together\n[ 223 ]\nLast, to make sure this approach works, you need to use the withCredentials option on\nevery xhr calls so that cookies and authentication headers are sent when AJAX calls are\nmade.\n    var xhr = new XMLHttpRequest();\n    xhr.open('get', URL, true);\n    xhr.withCredentials = true;\n    xhr.onload = function()  {\n      var data = JSON.parse(xhr.responseText);\n      ...\n    } .bind(this);\n    xhr.send();\nSummary\nIn this chapter, we looked at how to build a ReactJS UI wrapped into a Flask application\n(Dashboard). ReactJS is an excellent way to build a modern interactive UI in the browser--it\nintroduces a new syntax called JSX, which speeds up JS execution.\nWe also looked at how to use a toolchain based on npm, Bower, and Babel to manage JS\ndependencies, and transpile JSX files.\nThe Dashboard application uses Strava's three-legged OAuth API to connect users and get\nback a token from the Strava service. We made the design decision to separate the\nDashboard application from DataService, so the token is sent to the DataService\nmicroservice for storage. That token can then be used by the Strava Celery worker to fetch\nruns on behalf of the user.\nLastly, the calls made to different services to build the dashboard are proxied through the\nDashboard server to simplify the client side--which deals with a single server and a single\nauthentication and authorization process.",
      "content_length": 1410,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "Bringing It All Together\n[ 224 ]\nThe following is a diagram of the new architecture, which includes the Dashboard app:\nYou can find the full code of the Dashboard in the Runnerly org at h t t p s\n://g i t h u b . c o m /r u n n e r l y /d a s h b o a r d .\nWith now six different Flask apps that compose it, developing an application like Runnerly\ncan be a challenge when you are a developer.\nThere's an obvious need to be able to run all microservices in a single dev box without too\nmuch pain.\nAfter a dive into how packaging works in Python, the next chapter explains how to package\nour Python microservices, and how to run them in development mode in a single box via a\nprocess manager.",
      "content_length": 690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "9\nPackaging and Running\nRunnerly\nWhen the Python programming language was first released in the early 1990s, a Python\napplication was run by pointing the Python scripts to the interpreter. Everything related to\npackaging, releasing, and distributing Python projects was done manually. There was no\nreal standard back then, and each project had a long README on how to install it with all\nits dependencies.\nBigger projects used the system packaging tools to release their work--whether it was\nDebian packages, RPM packages for Red-Hat Linux distributions, or things like MSI\npackages under Windows. Eventually, the Python modules from those projects all ended up\nin the site-packages directory of the Python installation, sometimes after a compilation phase,\nif you had a C extension.\nThe Python packaging ecosystem has evolved a lot since then. In 1998, Distutils was added\nin the standard library to provide essential support to create installable distributions for\nPython projects. Between then and now, a lot of new tools have emerged from the\ncommunity to improve how a Python project can be packaged, released, and distributed.\nThis chapter is going to explain how to use the latest Python packaging tools for your\nmicroservices.\nThe other hot topic around packaging is how it fits in your day-to-day work. When building\nmicroservices-based software, you need to deal with many moving parts. When you are\nworking in a particular microservice, you can get away with it most of the time by using the\nTDD and mocking approach, which we discussed in Chapter 3, Coding, Testing, and\nDocumenting - The Virtuous Cycle.",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 226 ]\nHowever, if you want to do some realistic testing, where poking around each service is\nneeded, you have to have the whole stack running in a single box. Moreover, developing in\nsuch a context can be tedious if you need to reinstall new versions of your microservices all\nthe time.\nIt begs one question: how can you correctly install the whole stack in your environment and\ndevelop in it?\nIt also means you have to run all the microservices if you want to play with the app. In the\ncase of Runnerly, having to open six different shells to run all the microservices is not\nsomething a developer would want to do every time they need to run the app.\nIn this chapter, we are going to look at how we can leverage the packaging tools to run all\nmicroservices from the same environment, and then how to run them all from a single\ncommand-line interface by using a dedicated process manager.\nHowever, first, let's look at how to package your projects, and which tools should be\nutilized.\nThe packaging toolchain\nPython has come a long way in the past ten years on packaging. Numerous Python\nEnhancement Proposals (PEPs) were written to improve how to install, release, and\ndistribute Python projects.\nDistutils had some flaws, which made it a little tedious to release apps. The biggest pain\npoints were its lack of dependencies management and the way it handled compilation and\nbinary releases. For everything related to compiling, what worked well in the nineties\nstarted to get old fashioned ten years later. No one in the core team made the library evolve\nfor lack of interest, and because Distutils was good enough to compile Python and most\nprojects. People who needed advanced toolchains used other tools, like SCons (h t t p ://s c o\nn s . o r g /).\nIn any case, improving the toolchain was not an easy task because of the existing legacy\nsystem based on Distutils. Starting a new packaging system from scratch was quite hard,\nsince Distutils was part of the standard library, but introducing backward-compatible\nchanges was also hard to do properly. The improvements were made in-between. Projects\nlike Setuptools and Virtualenv were created outside the standard library, and some\nchanges were made directly into Python.",
      "content_length": 2260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 227 ]\nAs of today, you still find the scars from these changes, and it is still quite hard to know\nexactly how things should be done. For instance, the pyvenv command was added in\nPython and then removed in Python 3.6, but Python still ships with its virtual environment\nmodule, which competes with the Virtualenv project with some respect.\nThe best bet is to use the tools that are developed and maintained outside the standard\nlibrary, because their release cycle is shorter than Python's. In other words, a change in the\nstandard library takes months to be released, whereas a change in a third-party project can\nbe made available much faster.\nAll third-party projects that are considered as being part of the de facto standard packaging\ntoolchain are now all grouped under the PyPA (h t t p s ://w w w . p y p a . i o ) umbrella project.\nBesides developing the tools, PyPA also works on improving the packaging standards\nthrough proposing PEPs for Python and developing its early specifications--refer to h t t p s\n://w w w . p y p a . i o /e n /l a t e s t /r o a d m a p /. In 2017, we are still in a confusing state for\npackaging, as we have a few competing standards, but things have improved, and the\nfuture should look better.\nBefore we start to look at the tools that should be used, we need to go through a few\ndefinitions to avoid any confusion.\nA few definitions\nWhen we talk about packaging Python projects, a few terms can be confusing, because their\ndefinitions have evolved over time, and also because they can mean slightly different things\noutside the Python world.\nWe need to define, what's a Python package, a Python project, a Python library, and a\nPython application. They are defined as follows:\nA Python package is a directory tree containing Python modules. You can import\nit, and it is part of the module namespace.\nA Python project can contain several packages and other resources, and is what\nyou release. Each microservice you build with Flask is a Python project.\nA Python application is a Python project that can be directly used through a user\ninterface. The user interface can be a command-line script or a web server.\nLastly, a Python library is a specific kind of Python project which provides\nfeatures to be used in other Python projects, and has no direct user interface.",
      "content_length": 2343,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 228 ]\nThe distinction between an application and a library can be quite vague, since some\nlibraries sometimes offer some command-line tools to use some of its features, even if the\nfirst use case is to provide Python packages for other projects. Moreover, sometimes, a\nproject that was a library becomes an application.\nTo simplify the process, the best option is to make no distinction between applications and\nlibraries. The only technical difference is that applications ship with more data files and\nconsole scripts.\nNow that we have defined Python package, project, application, and library, let's look at\nhow to package your projects.\nPackaging\nWhen you package your Python project, there are three necessary files you need to have\nalongside your Python packages:\nsetup.py: A special module, which drives everything\nrequirements.txt: A file listing dependencies\nMANIFEST.in: A template file to list the files to be included in the releases\nLet's look at each one of them in detail.\nThe setup.py file\nThe setup.py file is what governs everything when you want to interact with a Python\nproject. When the setup() method is executed, it generates a static metadata file, which\nfollows the PEP 314 format. The metadata file holds all the metadata for the project, but you\nneed to regenerate it via a setup() call to get it to the Python environment you are using.\nThe reason why you cannot use a static version is that the author of a project might have\nplatform-specific code in setup.py, which generates a different metadata file depending on\nthe platform and Python versions.\nTo rely on running a Python module to extract static information about a project has always\nbeen a problem. You need to make sure that the code in the module can run in the target\nPython interpreter. If you are going to make your microservices available to the\ncommunity, you need to keep that in mind, as the installation happens in many different\nPython environments.",
      "content_length": 1983,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 229 ]\nPEP 390 (2009) was the first attempt to get rid of the setup.py file for\nmetadata. PEP 426, PEP 508, and PEP 518 are new attempts at fixing this\nissue with smaller chunks, but in 2017, we still don't have tools that\nsupport static metadata, and it is probably going to take a while before\neveryone uses them. So setup.py is going to stick around for years.\nA very common mistake when creating the setup.py file is to import your package in it\nwhen you have third-party dependencies. If a tool like PIP tries to read the metadata by\nrunning setup.py, it might raise an import error before it has a chance to list all the\ndependencies to install.\nThe only dependency you can afford to import directly in your setup.py file is Setuptools,\nbecause you can make the assumption that anyone trying to install your project is likely to\nhave it in their environment.\nAnother important consideration is the metadata you want to include to describe your\nproject. Your project can work with just a name, a version, a URL, and an author, but this is \nobviously not enough information to describe your project.\nMetadata fields are set through setup() arguments. Some of them match directly with the\nname of the metadata, some don't.\nThe following is the minimal set of arguments you should use for your microservices\nprojects:\nname: The name of the package, should be a short lowercase name\nversion: The version of the project, as defined in PEP 440\nurl: A URL for the project; can be its repository or home page\ndescription: One sentence to describe the project\nlong_description: A reStructuredText document\nauthor, and author_email: The name and email of the author--can be an\norganization\nlicense: The license used for the project (MIT, Apache2, GPL, and so on)\nclassifiers: A list of classifiers picked from a fixed list, as defined in PEP 301\nkeywords: Tags to describe your project--this is useful if you publish the project\nto the Python Package Index (PyPI)\npackages: A list of packages that your project includes--Setuptools can populate\nthat option automatically with the find_packages() method",
      "content_length": 2129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 230 ]\ninstall_requires: A list of dependencies (this is a Setuptools option)\nentry_points: A list of Setuptools hooks, like console scripts (this is a\nSetuptools option)\ninclude_package_data: A flag that simplifies the inclusion of non-Python files\nzip_safe: This is a flag that prevents Setuptools to install the project as a ZIP\nfile, which is a standard from the past (executable eggs)\nThe following is an example of a setup.py file that includes those options:\n    from setuptools import setup, find_packages\n    with open('README.rst') as f:\n        LONG_DESC = f.read()\n    setup(name='MyProject',\n          version='1.0.0',\n          url='http://example.com',\n          description='This is a cool microservice based on strava.',\n          long_description=LONG_DESC,\n          author='Tarek', author_email='tarek@ziade.org',\n          license='MIT',\n          classifiers=[\n             'Development Status :: 3 - Alpha',\n             'License :: OSI Approved :: MIT License',\n             'Programming Language :: Python :: 2',\n             'Programming Language :: Python :: 3'],\n          keywords=['flask', 'microservice', 'strava'],\n          packages=find_packages(),\n          include_package_data=True,\n          zip_safe=False,\n          entry_points=\"\"\"\n          [console_scripts]\n          mycli = mypackage.mymodule:myfunc\n          \"\"\",\n          install_requires=['stravalib'])\n    )\nThe long_description option is usually pulled from a README.rst file, so you do not \nhave to deal with including a large piece of reStructuredText string in your function.\nThe restructured text-lint project (h t t p s ://g i t h u b . c o m /t w o l f s o n /r e s t r u\nc t u r e d t e x t - l i n t ) is a linter that you can use to verify a reST file syntax.",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 231 ]\nThe other benefit of separating the description is that it's automatically recognized, parsed,\nand displayed by most editors. For instance, GitHub uses it as your project landing page in\nyour repository, and also offers an inline reStructuredText editor to change it directly\nfrom the browser. PyPI does the same to display the front page of the project.\nThe license field is free-form, as long as people can recognize the license being used. If\nyou use the Apache Public Licence Version 2 (APL v2),it works. In any case, you should\nadd, alongside your setup.py file, a LICENCE file with the official text of that license.\nThe classifiers option is probably the most painful one to write. You need to use strings\nfrom h t t p s ://p y p i . p y t h o n . o r g /p y p i ?%3A a c t i o n =l i s t _ c l a s s i f i e r s , which classify your\nproject. The three most common classifiers that developers use are the list of supported\nPython versions, the license (which duplicates and should match the license option), and\nthe development status, which is a hint about the maturity of the project.\nKeywords are a good way to make your project visible if you publish it to the Python\nPackage Index. For instance, if you are creating a Flask microservice, you should use flask\nand microservice as keywords.\nThe Trove classifier is a machine-parseable metadata that can be used, for\ninstance, by tools interacting with PyPI. For example, the zc.buildout tool\nlooks for packages with the Framework :: Buildout :: Recipe\nclassifier.\nThe entry_points section is an INI-like string that defines Setuptools entry points, which\nare callables that can be used as plugins once the project is installed in Python. The most\ncommon entry point type is the console script. When you add functions in that section, a\ncommand-line script will be installed alongside the Python interpreter, and the function\nhooked to it via the entry point. This is a good way to create a Command-Line Interface\n(CLI) for your project. In the example, mycli should be directly reachable in the shell when\nthe project is installed. Python's Distutils has a similar feature, but the one in Setuptools\ndoes a better job, because it allows you to point to a specific function.\nLastly, install_requires lists all the dependencies. This list of Python projects the\nproject uses, and can be used by projects like PIP when the installation occurs. The tool will\ngrab them if they are published in the PyPI, and install them.\nOnce this setup.py file is created, a good way to try it is by creating a local virtual\nenvironment.",
      "content_length": 2618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 232 ]\nAssuming you have virtualenv installed, if you run these commands in the directory\ncontaining the setup.py file, it will create a few directories including a bin directory\ncontaining a local Python interpreter, and drop you into a local shell.\n$ virtualenv .\n$ source bin/activate\n(thedir) $\nFrom there, running the pip install -e command will install the project in editable mode.\nThis command installs the project by reading its setup file, but unlike install, the installation\noccurs in-place. Installing in-place means that you will be able to work directly on the\nPython modules in the project, and they will be linked to the local Python installation via its\nsite-packages directory.\nUsing a vanilla install call would have created copies of the files into the local site-\npackages directory, and changing the source code would have had no impact on the installed\nversion.\nThe PIP call also generates a MyProject.egg-info directory, which contains the metadata.\nPIP generates version 1.1 of the metadata spec, under the PKG-INFO name.\n$ more MyProject.egg-info/PKG-INFO\nMetadata-Version: 1.1\nName: MyProject\nVersion: 1.0.0\nSummary: This is a cool project.\nHome-page: http://example.com\nAuthor: Tarek\nAuthor-email: tarek@ziade.org\nLicense: MIT\nDescription: MyProject\n        ---------\n        I am the **long** description.\nKeywords: flask,microservice,strava\nPlatform: UNKNOWN\nClassifier: Development Status :: 3 - Alpha\nClassifier: License :: OSI Approved :: MIT License\nClassifier: Programming Language :: Python :: 2\nClassifier: Programming Language :: Python :: 3\nThis metadata file is what describes your project and is what is used to register it to the\nPyPI via other commands, as we will see later in the chapter.",
      "content_length": 1766,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 233 ]\nThe PIP call also pulls all the project dependencies by looking from them in the PyPI on h t t\np s ://p y p i . p y t h o n . o r g /p y p i and installs them in the local site-packages. Running this \ncommand is a good way to make sure everything works as expected.\nOne thing that we need to discuss further is the install_requires option. It competes\nwith another way of listing the project dependencies, the requirements.txt file, which is \nexplained in the next section.\nThe requirements.txt file\nOne standard that emerged from the PIP community is to use a requirements.txt\nfile,which lists all the project dependencies, but also proposes an extended syntax to install \neditable dependencies. Refer to h t t p s ://p i p . r e a d t h e d o c s . i o /e n /s t a b l e /r e f e r e n c e /p i p\n_ i n s t a l l /#r e q u i r e m e n t s - f i l e - f o r m a t .\nThe following is an example of such a file:\narrow\npython-dateutil\npytz\nrequests\nsix\nstravalib\nunits\nUsing this file has been widely adopted by the community, because it makes it easier to\ndocument your dependencies. You can create as many requirements files as you want in a\nproject, and have your users call the pip install -r thefile.txt command to install\nthe packages described in them.\nFor instance, you could have a dev-requirements.txt file, which contains extra tools for\ndevelopment, and a prod-requirements.txt, which has production-specific things. The\nformat allows inheritance to help you manage requirements files' collections.\nBut using requirements files adds a new problem. It duplicates some of the information\ncontained in thesetup.py file's install_requires section.\nTo solve this new issue, some developers make a distinction between dependencies defined\nfor their Python libraries and the one defined for their Python applications.",
      "content_length": 1859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 234 ]\nThey use install_requires in their library's setup.py file, and the PIP requirement file\nin their application deployments. In other words, a Python application won't have a\nsetup.py file's install_requires option filled with its dependencies.\nBut that means the application installation will require a specific installation process, where\nthe dependencies are first installed via the requirements file. It also means that we'd lose the\nbenefits of having requirements files for libraries .\nAnd we've said earlier in the chapter, we do not want to make our life complicated by\nhaving two different ways to describe Python projects dependencies, since the distinction\nbetween an application and a library can be quite vague.\nTo avoid duplicating the information in both places, there are some tools in the community,\nwhich offer some syncing automation between setup.py and requirements files.\nThe pip-tools (h t t p s ://g i t h u b . c o m /j a z z b a n d /p i p - t o o l s ) tool is one of them, and it \ngenerates a requirements.txt file (or any other filename) via a pip-compile CLI, as\nfollows:\n$ pip install pip-tools\n...\n$ pip-compile\n#\n# This file is autogenerated by pip-compile\n# To update, run:\n#\n#    pip-compile --output-file requirements.txt setup.py\n#\narrow==0.10.0             # via stravalib\npython-dateutil==2.6.0    # via arrow\npytz==2017.2              # via stravalib\nrequests==2.13.0          # via stravalib\nsix==1.10.0               # via python-dateutil, stravalib\nstravalib==0.6.6\nunits==0.7                # via stravalib\nNotice that the generated file contains versions for each package. This is called version\npinning and is done by looking at the versions locally installed.\nWhen declaring dependencies, it's good practice to pin all the dependencies before you\nrelease your project. That will ensure that you document the versions that were used and\ntested for the release.",
      "content_length": 1943,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 235 ]\nIf you don't use pip-tools, PIP has a built-in command called freeze, which you can use to\ngenerate a list of all the current versions that are installed in your Python. This is done as\nfollows:\n$ pip freeze\ncffi==1.9.1\nclick==6.6\ncryptography==1.7.2\ndominate==2.3.1\nflake8==3.2.1\nFlask==0.11.1\n...\nThe only problem when you pin your dependencies is when another project has the same\ndependencies, but is pinned with other versions. PIP will complain and fail to meet both the\nrequirements sets and you won't be able to install everything.\nThe simplest way to fix this issue is to leave the dependencies unpinned in the setup.py\nfile and pinned in the requirements.txt file. That way, PIP can install the latest version\nfor each package, and when you deploy, specifically in stage or production, you can refresh\nthe versions by running the pip install -r requirements.txt command. PIP will\nthen upgrade/downgrade all the dependencies to match the versions, and in case you need\nto, you can tweak them in the requirements file.\nTo summarize, defining dependencies should be done in each project's setup.py file, and\nrequirements files can be provided with pinned dependencies as long as you have a\nreproducible process to generate them from the setup.py file to avoid duplication.\nThe last mandatory file your projects should have is the MANIFEST.in file.\nThe MANIFEST.in file\nWhen creating a source or binary release, Setuptools will include all the packages modules\nand data files, the setup.py file, and a few other files automatically in the tarball. But files\nlike the PIP requirements will not be included for you.\nIn order to add them to your distribution, you need to add a MANIFEST.in file, which\ncontains the list of files to include.",
      "content_length": 1782,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 236 ]\nThe file follows a simple glob-like syntax, described at h t t p s ://d o c s . p y t h o n . o r g /3/d i s t u\nt i l s /c o m m a n d r e f . h t m l #c r e a t i n g - a - s o u r c e - d i s t r i b u t i o n - t h e - s d i s t - c o m m a n d , where\nyou point a file or a directory (including glob patterns) and say if you want to include or\nprune the matches.\nHere's an example from Runnerly:\n    include requirements.txt\n    include README.rst\n    include LICENSE\n    recursive-include myservice *.ini\n    recursive-include docs *.rst *.png *.svg *.css *.html conf.py\n    prune docs/build/*\nThe docs/directory containing the Sphinx doc will be integrated in the source distribution,\nbut any artifact generated locally in docs/build/ when the doc is built will be pruned.\nOnce you have the MANIFEST.in file in place, all the files should be added in your\ndistribution when you'll release your project. Notice that you can use the check-manifest\ndistutils command to check the syntax of the file and its effect.\nA typical microservice project, as described in this book, will have the following list of files:\nsetup.py: The setup file\nREADME.rst: The content of the long_description option\nMANIFEST.in: The MANIFEST template\nrequirements.txt: PIP requirement files generated from install_requires\ndocs/: The Sphinx documentation\npackage/: The package containing the microservice code\nFrom there, releasing your project consists of creating a source distribution, which is\nbasically an archive of this structure. If you have some C extensions, you can also create a\nbinary distribution.\nBefore we learn how to create those releases, let's look at how to pick version numbers for\nyour microservices.\nVersioning\nPython packaging tools do not enforce a specific versioning pattern. The version field can\nbe any string. This freedom became a problem, because projects followed their own\nversioning schemes and, sometimes, they were not compatible with installers and tools.",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 237 ]\nTo understand a versioning scheme, an installer needs to know how to sort and compare\nversions. The installer needs to be able to parse the string, and know if a version is older\nthan another one.\nEarly software used schemes based on the date of release, like 20170101 if your software is\nreleased on 1st January 2017. But that scheme won't work anymore if you do branch\nreleases. For instance, if your software has a version 2, which is backward incompatible,\nyou might start to release updates for version 1 in parallel of releases for version 2. In that\ncase, using dates will make some of your version 1 releases appear as if they were more\nrecent than some version 2 release.\nSome software combine incremental versions and dates for that reason, but it became\nobvious that using dates was not the best way to handle branches.\nAnd then, there's the problem of beta, alpha, release candidates, and dev versions.\nDevelopers want to have the ability to mark releases as being pre-releases.\nFor instance, when Python is about to ship a new version, it will ship release candidates using\na rcX marker so that the community can try it before the final release is shipped. For\nexample 3.6.0rc1, 3.6.0rc2, and so on.\nFor a microservice that you are not releasing to the community, using such markers is often\nan overkill--but when you start to have people from outside your organization using your\nsoftware, it may become useful.\nRelease candidates can be useful, for example, if you are about to ship a backward\nincompatible version of a project. It's always a good idea to have your users try it out before\nit's published. For the usual release though, using candidate releases is probably an overkill,\nas publishing a new release when a problem is found is cheap.\nPIP does a fairly good job at figuring out most patterns, ultimately falling\nback to some alphanumeric sorting, but the world would be a better place\nif all projects were using the same versioning scheme.\nPEP 386, and then 440, were written to try to come up with a versioning scheme for the\nPython community. It's derived from the standard MAJOR.MINOR[.PATCH] scheme,\nwhich's widely adopted among developers, with some specific rules for pre- and post-\nversions.\nThe Semantic Versioning (SemVer) (h t t p ://s e m v e r . o r g /) scheme is another standard\nthat emerged in the community, which is used in many places outside Python. If you use\nSemVer, you will be compatible with PEP 440 and the PIP installer as long as you don't use\npre-release markers. For instance, 3.6.0rc2 translates to 3.6.0-rc2 in SemVer.",
      "content_length": 2617,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 238 ]\nUnlike PEP 440, SemVer asks that you always provide the three version numbers. For\ninstance, 1.0 should be 1.0.0.\nAdopting SemVer is a good idea as long as you remove the dash it uses to separate the\nversion from a marker.\nHere's an example of a sorted list of versions for a project that will work in Python, and\nwhich will be close to SemVer:\n9.0\n0.0a1\n0.0a2\n0.0b1\n0.0rc1\n0.0\n1.0\nFor your microservice project, or any Python project for that matter, you should start with\nthe 0.1.0 version, make it clear that it's still an unstable project, and that backward\ncompatibility is not guaranteed. From there, you can increment the MINOR number at will\nuntil you feel the software is mature enough.\nOnce maturity has been reached, a common pattern is to release 1.0.0, and then start to\nfollowing these rules:\nMAJOR is incremented when you introduce a backward incompatible change for\nthe existing API\nMINOR is incremented when you add new features that don't break the existing\nAPI\nPATCH is incremented just for bug fixes\nBeing strict about this scheme with the 0.x.x series when the software is in its early phase\ndoes not make much sense, because you will do a lot of backward incompatible changes,\nand your MAJOR version would reach a high number in no time.\nThe 1.0.0 release is often emotionally charged for developers.\nThey want it to be the first stable release they'll give to the world--that's\nwhy it's frequent to use the 0.x.x versions and bump to 1.0.0 when the\nsoftware is deemed stable.\nFor a library, what we call the API are all the public and documented functions and classes\none may import and use.",
      "content_length": 1653,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 239 ]\nFor a microservice, there's a distinction between the code API and the HTTP API. You may\ncompletely change the whole implementation in a microservice project and still implement\nthe exact same HTTP API. You need to treat those two versions distinctly.\nBoth versions can follow the pattern described here, but one version will be on your\nsetup.py (the code) and one may be published in your Swagger specification file, or \nwherever you document your HTTP API. The two versions will have a different release\ncycle.\nNow that we know how to deal with version number, let's do some releasing.\nReleasing\nTo release your project, a simple command called sdist is provided in Python's Distutils.\nDistutils has a series of commands that can be invoked with the python setup.py\n<COMMAND> command. Running the python setup.py sdist command in the root of\nyour project will generate an archive containing the source code of your project.\nIn the following example, sdist is called in Runnerly's tokendealer project:\n$ python setup.py sdist\nrunning sdist\n[...]\ncreating runnerly-tokendealer-0.1.0\ncreating runnerly-tokendealer-0.1.0/runnerly_tokendealer.egg-info\ncreating runnerly-tokendealer-0.1.0/tokendealer\ncreating runnerly-tokendealer-0.1.0/tokendealer/tests\ncreating runnerly-tokendealer-0.1.0/tokendealer/views\ncopying files to runnerly-tokendealer-0.1.0...\ncopying README.rst -> runnerly-tokendealer-0.1.0\n[...]\ncopying tokendealer/tests/__init__.py -> runnerly-\ntokendealer-0.1.0/tokendealer/tests\ncopying tokendealer/tests/test_home.py -> runnerly-\ntokendealer-0.1.0/tokendealer/tests\ncopying tokendealer/views/__init__.py -> runnerly-\ntokendealer-0.1.0/tokendealer/views\ncopying tokendealer/views/home.py -> runnerly-\ntokendealer-0.1.0/tokendealer/views\nWriting runnerly-tokendealer-0.1.0/setup.cfg\ncreating dist\nCreating tar archive\nremoving 'runnerly-tokendealer-0.1.0' (and everything under it)",
      "content_length": 1934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 240 ]\nThe sdist command reads the info from setup.py and MANIFEST.in, and grabs all the\nfiles to put them in an archive. The result is created in the dist directory.\n$ ls dist/\nrunnerly-tokendealer-0.1.0.tar.gz\nNotice that the name of the archive is composed of the name of the project and its version.\nThis archive can be used directly with PIP to install the project as follows:\n$ pip install dist/runnerly-tokendealer-0.1.0.tar.gz\nProcessing ./dist/runnerly-tokendealer-0.1.0.tar.gz\n  Requirement already satisfied (use --upgrade to upgrade): runnerly-\ntokendealer==0.1.0   [...]\nSuccessfully built runnerly-tokendealer\nSource releases are good enough when you don't have any extension that needs to be\ncompiled. If you do, the target system will need to compile them again when the\ninstallation happens. That means the target system needs to have a compiler, which is not\nalways the case.\nAnother option is to precompile and create binary distributions for each target system.\nDistutils has several bdist_xxx commands to do it, but they are not really maintained\nanymore. The new format to use is the Wheel format as defined in PEP 427. The Wheel\nformat is a ZIP file containing all the files that will be deployed on the target system,\nwithout having to rerun commands at install time.\nIf your project has no C extension, it's still interesting to ship Wheel distributions, because\nthe installation process will be faster than with sdist; PIP is just going to move files around\nwithout running any command.\nTo build a Wheel archive, you need to install the wheel project, then to call the\nbdist_wheel command--that will create a new archive in dist.\n$ pip install wheel\n$ python setup.py bdist_wheel --universal\n$ ls dist/\nrunnerly-tokendealer-0.1.0.tar.gz\nrunnerly_tokendealer-0.1.0-py2.py3-none-any.whl\nNotice that we've used the -universal flag when bdist_wheel was called in this\nexample.",
      "content_length": 1930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 241 ]\nThis flag tells the command to generate a source release that can be installed on both Python\n2 and 3 if your code is compatible with both, with no extra steps (like a 2 to 3 conversion).\nWithout the flag, a runnerly_tokendealer-0.1.0-py3-none-any.whl file would have\nbeen created, indicating that the release works only for Python 3.\nIn case you have some C extensions, bdist_wheel will detect it and create a platform-\nspecific distribution with the compiled extension. In that case, none in the filename is\nreplaced by the platform.\nCreating platform-specific releases is fine if your C extensions are not linking to specific\nsystem libraries. If they do, there are good chances your binary release will not work\neverywhere, in particular, if the target system has a different version of that library.\nShipping a proper binary release that will work in all circumstances is really hard. Some\nprojects ship statically linked extensions together will all the libraries the extension is using.\nIn general, you rarely need to ship a C extension when you write a microservice, so a source\ndistribution is good enough.\nShipping a sdist and a Wheel distribution is the best practice. Installers like PIP will pick\nthe wheel, and the project will get installed faster than with sdist. The sdist release on\nthe other hand can be used by older installers or for manual installations.\nOnce you have your archive ready, you can distribute them, let's see how.\nDistributing\nIf you are developing in an open source project, it's good practice to publish your project to\nthe PyPI at h t t p s ://p y p i . p y t h o n . o r g /p y p i .\nLike most modern language ecosystem, this index can be browsed by installers that are\nlooking for releases to download.\nWhen you call the pip install <project> command, PIP will browse the PyPI index to\nsee if that project exists, and if there are some suitable releases for your platform.\nThe public name is the name you use in your setup.py file and you need to register it at\nPyPI in order to be able to publish some releases. The index uses the first-come, first-serve\nprinciple, so if the name you've picked is taken, you will have to choose another one.\nWhen creating microservices for an application or an organization, you can use a common\nprefix for all your projects' names. For Runnerly, runnerly- is used.\nAt the package level, a prefix can also sometimes be useful to avoid conflicts.",
      "content_length": 2461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 242 ]\nPython has a namespace package feature, which allows you to create a top-level package\nname (like runnerly), and then have packages in separate Python projects, which will end\nup being installed under the top-level runnerly package.\nThe effect is that every package gets a common runnerly namespace when you import\nthem, which is quite an elegant way to group your code under the same banner. The feature\nis available through the pkgutil module from the standard library.\nTo do this, you just need to create the same top-level directory in every project, with the\n__init__.py file containing and prefixing all absolute imports with the top-level name.\nfrom pkgutil import extend_path\n__path__ = extend_path(__path__, __name__)\nFor example, in Runnerly, if we decide to release everything under the same namespace,\neach project can have the same top-level package name. For example, in the token dealer, it \ncould be as follows:\nrunnerly\n__init__.py: Contains the extend_path call\ntokendealer/\n.. the actual code...\nAnd then in the dataservice one, like this:\nrunnerly\n__init__.py: Contains the extend_path call\ndataservice/\n.. the actual code...\nBoth will ship a runnerly top-level package, and when PIP installs them, the\ntokendealer and dataservice packages will both end up in the same directory, site-\npackages/runnerly.\nThis feature is not that useful in production, where each microservice is deployed in a\nseparate installation, but it does not hurt and is good to have, as it can be useful if you start\nto create a lot of libraries that are used across projects.\nFor now, we'll make the assumption that each project is independent, and each name is\navailable at PyPI.",
      "content_length": 1714,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 243 ]\nTo publish the releases at PyPI, you first need to register a new user using the form at h t t p s\n://p y p i . p y t h o n . o r g /p y p i ?%3A a c t i o n =r e g i s t e r _ f o r m , shown as follows:\nOnce you have a username and a password, you should create, in your home directory, a\n.pypirc file containing your credentials, like this:\n[pypi]\nusername = <username>\npassword = <password>\nThis file will be used every time you interact with the PyPI index to create a Basic\nAuthentication header.\nPython Distutils has a register and upload command to register a new project at PyPI, but it\nis better to use Twine (h t t p s ://g i t h u b . c o m /p y p a /t w i n e ), which comes with a slightly\nbetter user interface.\nOnce you've installed Twine (using the pip install twine command), the next step is to\nregister your package with this command:\n$ twine register dist/runnerly-tokendealer-0.1.0.tar.gz\nThe preceding command will create a new entry in the index using your package metadata.\nOnce it's done, you can go ahead and upload the releases as follows:\n$ twine upload dist/*\nFrom there, your package should appear in the index, with an HTML home page at\nhttps://pypi.python.org/pypi/<project>. And the pip install <project>\ncommand should work!",
      "content_length": 1298,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 244 ]\nNow that we know how to package each microservice, let's see how to run them all in the\nsame box for development purposes.\nRunning all microservices\nRunning a microservice can be done by using the built-in Flask web server. Running the\nFlask apps via this script requires to set up an environment variable, which points to the\nmodule that contains the flask application.\nIn the following example, the application for Runnerly, the dataservice microservice is\nlocated in the app module in runnerly.dataservice and can be launched from the root\ndirectory with this command:\n$ FLASK_APP=runnerly/dataservice/app.py bin/flask run\n * Serving Flask app \"runnerly.dataservice.app\"\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n127.0.0.1 - - [01/May/2017 10:18:37] \"GET / HTTP/1.1\" 200 -\nRunning apps using Flask's command line is fine, but it restricts us to use its interface\noptions. If we want to pass a few arguments to run our microservice, we would need to\nstart to add environment variables.\nAnother option is to create our own launcher using the argparse module (h t t p s ://d o c s . p y\nt h o n . o r g /3/l i b r a r y /a r g p a r s e . h t m l ), so that we can add for each microservice any option\nwe want.\nThe following example is a full working launcher, which will run a Flask application via an\nargparse-based command-line script. It takes a single option, -config-file, which is the\nconfiguration file that contains everything needed by the microservice to run.\n    import argparse\n    import sys\n    import signal\n    from .app import create_app\n    def _quit(signal, frame):\n        print(\"Bye!\")\n        # add any cleanup code here\n        sys.exit(0)\n    def main(args=sys.argv[1:]):\n        parser = argparse.ArgumentParser(description='Runnerly\n                                         Dataservice')\n        parser.add_argument('--config-file', help='Config file',",
      "content_length": 1936,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 245 ]\n                            type=str, default=None)\n        args = parser.parse_args(args=args)\n        app = create_app(args.config_file)\n        host = app.config.get('host', '0.0.0.0')\n        port = app.config.get('port', 5000)\n        debug = app.config.get('DEBUG', False)\n        signal.signal(signal.SIGINT, _quit)\n        signal.signal(signal.SIGTERM, _quit)\n        app.run(debug=debug, host=host, port=port)\n    if __name__ == \"__main__\":\n        main()\nThis approach offers a lot of flexibility. In order to make that script a console script, you\nneed to pass it to your setup class's function via the entry_points option as follows:\n    from setuptools import setup, find_packages\n    from runnerly.dataservice import __version__\n    setup(name='runnerly-data',\n          version=__version__,\n          packages=find_packages(),\n          include_package_data=True,\n          zip_safe=False,\n          entry_points=\"\"\"\n          [console_scripts]\n          runnerly-dataservice = runnerly.dataservice.run:main\n          \"\"\")\nWith this option, a runnerly-dataservice console script will be created and linked to the\nmain() function we've seen earlier.\n$ runnerly-dataservice --help\nusage: runnerly-dataservice [-h] [--config-file CONFIG_FILE]\nRunnerly Dataservice\noptional arguments:\n  -h, --help            show this help message and exit\n  --config-file CONFIG_FILE\n                        Config file\n$ runnerly-dataservice\n * Running on http://127.0.0.1:5001/ (Press CTRL+C to quit)\n * Restarting with stat\n * Debugger is active!\n * Debugger pin code: 216-834-670",
      "content_length": 1618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 246 ]\nWe have used the -e option earlier in PIP to run a project in develop mode. If we use the\nsame option for all our microservices from within the same Python, we will be able to run\nall of them in the same box using their respective launchers.\nYou can create a new virtualenv, and simply link each development directory using -e in\na requirements.txt file that lists all your microservices.\nPIP can also recognize Git URLs, and clone the repositories in your environment for you,\nwhich makes it convenient to create a root directory with all the code inside it.\nFor example, the following requirements.txt file points to two GitHub repositories:\n-e git+https://github.com/Runnerly/tokendealer.git#egg=runnerly-tokendealer\n-e git+https://github.com/Runnerly/data-service.git#egg=runnerly-data\nFrom there, running the pip install -r requirements.txt command will clone the\ntwo projects in a src directory and install them in develop mode, meaning that you can \nchange and commit the code directly from src/<project>.\nLastly, assuming you have created console scripts everywhere you needed to run your\nmicroservices, they will be added in the virtualenv bin directory.\nThe last piece of the puzzle is to avoid having to run each console script in a separate bash\nwindow. We want to manage those processes with a single script. Let's see in the next\nsection how we can do this with a process manager.\nProcess management\nWe've seen in Chapter 2, Discovering Flask, that Flask-based applications, in general, run in\na single-threaded environment.\nTo add concurrency, the most common pattern is to use a prefork model. Serving several\nclients concurrently is done by forking several processes (called workers), which accept\nincoming connections from the same inherited socket. The socket can be a TCP Socket or a\nUnix Socket. Unix sockets can be used when both the clients and the server are running on\nthe same machine. They are based on exchanging data via a file and are slightly faster than\nTCP sockets, since they don't have to deal with the network protocol overhead. Using Unix\nSockets to run Flask apps is common when the application is proxied via a front server like\nNGinx.",
      "content_length": 2213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 247 ]\nIn any case, Unix or TCP, every time a request reaches the socket, the first available process\naccepts the request and handles it. Which process gets which request is done at the system\nlevel by the system socket API with a lock mechanism. This round-robin mechanism load\nbalances requests among all processes and is pretty efficient.\nTo use this model for your Flask app, you can use uWSGI (h t t p ://u w s g i - d o c s . r e a d t h e d o\nc s . i o ), for instance, which will prefork several processes with its processes option, and\nserve the Flask app from there.\nThe uWSGI tool is pretty amazing with a lot of options, and even has its own binary\nprotocol to communicate through TCP. Running uWSGI with its binary protocol behind an\nnginx HTTP server is a great solution for serving Flask apps. uWSGI takes care of managing\nits processes, and interacts with nginx, with whatever HTTP proxy you use. Or directly\nwith the end user.\nThe uWSGI tool, however, specializes in running web apps. If you want to deploy for a\ndevelopment environment, a few other processes, like a Redis instance, which need to run\nalongside your microservices on the same box, you will need to use another process\nmanager.\nA good alternative is a tool like Circus (h t t p ://c i r c u s . r e a d t h e d o c s . i o ), which can run\nany kind of process even if they are not a WSGI application, it also has the ability to bind\nsockets, and make them available for the managed processes. In other words, Circus can\nrun a Flask app with several processes, and can also manage some other processes if\nneeded.\nCircus is a Python application, so, to use it, you can simply run the pip install circus\ncommand. Once Circus is installed, you will get a few new commands. The two principal\ncommands are: circusd, which is the process manager, and circusctl, which lets you\ndrive the process manager from the command line.\nCircus uses an INI-like configuration file, where you can list the commands to run in\ndedicated sections, and for each one of them, the number of processes you want to use.\nCircus can also bind sockets, and let the forked process use them via their file descriptors.\nWhen a socket is created on your system, it uses a file descriptor (FD), which is a system\nhandle a program can use to reach a file or an I/O resource like sockets. A process that is\nforked from another one inherits all its file descriptors. That is, through this mechanism, all\nthe processes launched by Circus can share the same sockets.",
      "content_length": 2540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 248 ]\nIn the following example, two commands are being run. One will run five processes for the\nFlask application located in the server.py module, and one will run one Redis server\nprocess.\n[watcher:web]\ncmd = chaussette --fd $(circus.sockets.web) server.application\nuse_sockets = True\nnumprocesses = 5\n[watcher:redis]\ncmd = /usr/local/bin/redis-server\nuse_sockets = False\nnumprocesses = 1\n[socket:web]\nhost = 0.0.0.0\nport = 8000\nThe socket:web section describes what host and port to use to bind the TCP socket, and\nthe watcher:web section uses it via the $(circus.sockets.web) variable. When Circus\nruns, it replaces that variable with the FD value for the socket.\nTo run this script, you can use the circusd command line.\n$ circusd myconfig.ini\nThere are a few WSGI web servers out there that provide an option to run against a file\ndescriptor, but most of them don't expose that option, and bind a new socket themselves\ngiven a host and a port.\nThe Chaussette (h t t p ://c h a u s s e t t e . r e a d t h e d o c s . i o /) project was created to let you run \nmost existing WSGI web servers out there via an FD. Once you've run the pip install\nchaussette command, you can run the Flask app with a variety of backend listed at h t t p\n://c h a u s s e t t e . r e a d t h e d o c s . i o /e n /l a t e s t /#b a c k e n d s .\nFor our microservices, using Circus means we can simply create a watcher and a socket\nsection per service and start them all using the circusd command.\nThe only difference is that, if we use our own launcher instead of the Chaussette console, it\nneeds to be adapted in order to be able to run with file descriptors.",
      "content_length": 1678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 249 ]\nThe main() function from the microservice can use the make_server() function from\nChaussette and use it in case an -fd option is passed when launched.\n    from chaussette.server import make_server\n    def main(args=sys.argv[1:]):\n        parser = argparse.ArgumentParser(description='Runnerly\n                                         Dataservice')\n        parser.add_argument('--fd', type=int, default=None)\n        parser.add_argument('--config-file', help='Config file',\n                            type=str, default=None)\n        args = parser.parse_args(args=args)\n        app = create_app(args.config_file)\n        host = app.config.get('host', '0.0.0.0')\n        port = app.config.get('port', 5000)\n        debug = app.config.get('DEBUG', False)\n        signal.signal(signal.SIGINT, _quit)\n        signal.signal(signal.SIGTERM, _quit)\n        def runner():\n            if args.fd is not None:\n                # use chaussette\n                httpd = make_server(app, host='fd://%d' % args.fd)\n                httpd.serve_forever()\n            else:\n                app.run(debug=debug, host=host, port=port)\n        if not debug:\n            runner()\n        else:\n            from werkzeug.serving import run_with_reloader\n            run_with_reloader(runner)\nAnd then, in the circus.ini file:\n[watcher:web]\ncmd = runnerly-dataservice --fd $(circus.sockets.web)\nuse_sockets = True\nnumprocesses = 5\n[socket:web]\nhost = 0.0.0.0\nport = 8000\nFrom there, if you need to debug a specific microservice, a common pattern is to add a\npdb.set_trace() call inside the Flask view you are going to call.",
      "content_length": 1637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "Packaging and Running Runnerly\n[ 250 ]\nOnce the call is added in the code, you can stop the microservice via circusctl in order to\nrun it manually in another shell, so you can get access to the debug prompt.\nCircus also offers options to redirect the stdout and stderr streams to\nlog files to facilitate the debugging and numerous other features you can\nfind at h t t p s ://c i r c u s . r e a d t h e d o c s . i o /e n /l a t e s t /f o r - o p s /c o n f i g u r\na t i o n /.\nSummary\nIn this chapter, we've looked at how to package, release, and distribute each microservice.\nThe current state of the art in Python packaging still requires some knowledge about the\nlegacy tools, and this will be the case for some years until all the ongoing work in Python\nand PyPA become mainstream.\nBut as long as you have a standard, reproducible, and documented way to package and\ninstall your microservices, you should be fine.\nHaving numerous projects to run a single application adds a lot of complexity when you\nare developing it, and it's important to be able to run all pieces from within the same box.\nTools like PIP's development mode and Circus are useful for this, as it allows you to\nsimplify how you run the whole stack--but they still require that you install things on your\nsystem even if it's inside a Virtualenv.\nThe other issue with running everything from your box is that you might not use an\noperating system that will be used to run your services in production, or have some\nlibraries installed for other purposes, which might interfere.\nThe best way to prevent this problem is to run your stack in full isolation inside a virtual\nbox. This is what the next chapter will cover, for example, how to run your services inside\nDocker.",
      "content_length": 1743,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "10\nContainerized Services\nIn the previous chapter, we ran our different microservices directly in the host operating\nsystem--so, all the dependencies and data that your application uses were installed directly\non the system.\nMost of the time, it is fine to do so, because running a Python application in a virtual\nenvironment downloads and installs dependencies inside a single directory. However, if\nthe application requires a database system, you need that database to run on your system,\nunless it is just an SQLite file. For some Python libraries, you might also need some system\nheaders to compile extensions.\nIn no time, your system is going to have various software running, which were installed\nalong the way when developing your microservices. It is not a problem for your\ndevelopment environment as long as you don’t need to work with different versions of a\nservice you are working on. However, if some potential contributors try to install your\napplications locally, and are forced to install much of the software at the system level, it\ncould be a dealbreaker.\nThat is where VMs are a great solution to run your applications. In the past ten years, many\nsoftware projects that required an elaborate setup to run started to provide ready-to-run\nVMs, using tools such as VMWare or VirtualBox. Those VMs included the whole stack, like\nprefilled databases. Demos became easily runnable on most platforms with a single\ncommand. That was progress.\nHowever, some of those tools were not fully open source, and they were very slow to run,\nand greedy in memory and CPU and terrible with disk I/O. It was unthinkable to run them\nin production, and they were mostly used for demos.\nThe big revolution came with Docker, an open source virtualization tool, which was first\nreleased in 2013, and became hugely popular. Moreover, unlike VMWare or VirtualBox,\nDocker can run your applications in production at native speed.",
      "content_length": 1920,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "Containerized Services\n[ 252 ]\nIn other words, creating images for your application is not only for demonstration and\ndevelopment purposes anymore. It can be used for real deployments.\nIn this chapter, we present Docker, and explain how to run Flask-based microservices with\nit. Then, we look at some of the tools in the Docker ecosystem. We conclude the chapter by\nan introduction to clusters.\nWhat is Docker?\nThe Docker (h t t p s ://w w w . d o c k e r . c o m /) project is a container platform, which lets you run\nyour applications in isolated environments. Docker leverages existing Linux technologies\nlike cgroups (h t t p s ://e n . w i k i p e d i a . o r g /w i k i /C g r o u p s ) to provide a set of high-level\ntools to drive a collection of running processes. On Windows and macOS, Docker interacts \nwith a Linux Virtual Machine, since a Linux kernel is required.\nAs a Docker user, you just need to point which image you want to run, and Docker does all\nthe heavy lifting by interacting with the Linux kernel. An image in that context is the sum of\nall the instructions required to create a set of running processes on the top of a Linux kernel\nto run one container. An image includes all the resources necessary to run a Linux\ndistribution. For instance, you can run whatever version of Ubuntu you want in a Docker\ncontainer even if the host OS is a different distribution.\nWhile it is possible to use Windows, Flask microservices should always be\ndeployed under a Linux or BSD-based system--the rest of this chapter\nmakes the assumption that everything is installed under a Linux\ndistribution such as Debian.\nIf you have already installed Docker in Chapter 6, Monitoring Your Services, to set up a\nGraylog instance, you can jump directly to the next section of this chapter, Docker 101.\nIf not, you can visit the Get Docker section of the page at h t t p s ://w w w . d o c k e r . c o m /g e t - d o c\nk e r to install it. The community edition is good enough for building, running, and installing\ncontainers. Installing Docker on Linux is a no-brainer-- you can probably find a package for\nyour Linux distribution.\nFor macOS, Docker uses a VM to run a Linux Kernel. The latest versions are based on\nHyperKit (h t t p s ://g i t h u b . c o m /m o b y /h y p e r k i t ), which leverages bhyve, a BSD\nHypervisor. Running Docker via a VM adds a bit of overhead, but it is quite lightweight,\nand works well with modern hardware. Hypervisors are becoming a commodity in all\nmajor operating systems.",
      "content_length": 2512,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "Containerized Services\n[ 253 ]\nUnder Windows, Docker uses the Windows built-in Hyper-V, which might need to be\nenabled. The feature can usually be enabled via a command-line shell with a DSIM call, as\nfollows:\n$ DISM /Online /Enable-Feature /All /FeatureName:Microsoft-Hyper-V\nIf the installation was successful, you should be able to run the docker command in your\nshell. Try the version command to verify your installation like this:\n$ docker version\nClient:\n Version:      17.03.1-ce\n API version:  1.27\n Go version:   go1.7.5\n Git commit:   c6d412e\n Built:        Tue Mar 28 00:40:02 2017\n OS/Arch:      darwin/amd64\nServer:\n Version:      17.03.1-ce\n API version:  1.27 (minimum version 1.12)\n Go version:   go1.7.5\n Git commit:   c6d412e\n Built:        Fri Mar 24 00:00:50 2017\n OS/Arch:      linux/amd64\n Experimental: true\nA Docker installation is composed of a Docker server (the engine that's being executed by a\ndaemon) and a Docker client (the shell commands like docker).\nThe server provides an HTTP API, which can be reached locally through a UNIX socket\n(usually, /var/run/docker.sock) or through the network.\nIn other words, the Docker client can interact with Docker daemons running on other boxes.\nFor managing Docker manually, the Docker command line is great.\nHowever, in case you need to script some of your Docker manipulation, a\nPython library like docker-py (h t t p s ://g i t h u b . c o m /d o c k e r /d o c k e r - p y )\nlets you do everything from Python. It uses requests to perform HTTP\ncalls against the Docker daemon.\nNow that Docker is installed on your system, let's discover how it works.",
      "content_length": 1625,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "Containerized Services\n[ 254 ]\nDocker 101\nRunning a container in Docker is done by executing a series of commands which starts a\ngroup of processes, which the tool isolates from the rest of the system.\nDocker can be used to run a single process, but in practice we want to run a full Linux\ndistribution. Not to worry, everything needed to run a full Linux inside Docker is already\navailable.\nEvery existing Linux distribution out there provides a base image, which lets you run the\ndistribution in Docker. The typical way you use images is by creating a Dockerfile (h t t p s\n://d o c s . d o c k e r . c o m /e n g i n e /r e f e r e n c e /b u i l d e r /), where you point the base image you\nwant to use, and add some extra commands to be run to create the container.\nThe following is a basic example of a Docker file:\nFROM ubuntu\nRUN apt-get update && apt-get install -y python\nCMD [\"bash\"]\nA Dockerfile is a text file with a set of instructions. Each line starts with the instruction in\nuppercase, followed by its arguments.\nIn our example, there are these three instructions:\nFROM: Points the base image to use\nRUN: Runs the commands in the container once the base image is installed\nCMD: The command to run when the container is executed by Docker\nTo create that image and then run it, you can use the docker build and run commands\nfrom within the directory where the Dockerfile file is located. Notice the full stop (.) at\nthe end.\n$ docker build -t runnerly/python .\nSending build context to Docker daemon 6.656 kB\nStep 1/3 : FROM ubuntu\n ---> 0ef2e08ed3fa\nStep 2/3 : RUN apt-get update && apt-get install -y python\n ---> Using cache\n ---> 48a5a722c81c\nStep 3/3 : CMD bash\n ---> Using cache\n ---> 78e9a6fd9295\nSuccessfully built 78e9a6fd9295\n$ docker run -it --rm runnerly/python",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "Containerized Services\n[ 255 ]\nroot@ebdbb644edb1:/# python\nPython 2.7.12 (default, Nov 19 2016, 06:48:10)\n[GCC 5.4.0 20160609] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\nThe -t option in the preceding code snippet adds a label to the image. In our example, the\nimage is tagged runnerly/python. One convention is to prefix the label with the project or\norganization name so that you can group your images under the same namespace.\nWhen Docker creates images, it creates a cache which has every instruction from the\nDockerfile. If you run the build command a second time, without changing the file, it\nshould be done within seconds. Permuting or changing instructions rebuilds the image\nstarting at the first change. For this reason, a good strategy when writing these files is to\nsort instructions so that the most stable ones (the ones you rarely change) are at the top.\nOne great feature that Docker offers is the ability to share, publish, and reuse images with\nother developers. The Docker Hub (h t t p s ://h u b . d o c k e r . c o m ) is to Docker containers\nwhat PyPI is to Python packages.\nIn the previous example, the ubuntu base image was pulled from the Hub by Docker, and\nthere are numerous pre-existing images you can use.\nFor instance, if you want to launch a Linux distribution that is tweaked for Python, you can\nlook at the Python page on the official Docker Hub and pick one (h t t p s ://h u b . d o c k e r . c o m\n/_ /p y t h o n /).\nThe python:version images are Debian-based, and are an excellent starting point for any\nPython project.\nThe Python images based on Alpine Linux (refer to h t t p ://g l i d e r l a b s . v i e w d o c s . i o /d o c\nk e r - a l p i n e /) are also quite popular, because they produce the smallest images to run\nPython. They are more than ten times smaller than other ones, which means they are way\nfaster to download and set up for people wanting to run your project in Docker.\nTo use Python 3.6 in Alpine, you can create a Dockerfile like this:\nFROM python:3.6-alpine\nCMD [\"python3.6\"]",
      "content_length": 2083,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "Containerized Services\n[ 256 ]\nBuilding and running this Dockerfile drops you in a Python 3.6 shell. The Alpine set is\ngreat if you run a Python application that does not require a lot of system-level\ndependencies or any compilation. Alpine has a very specific set of compilation tools, which\nare sometimes incompatible with some projects.\nFor a Flask-based microservice project, the Debian-based one is probably a simpler choice\nbecause of its standard compilation environment and stability. Moreover, once the base\nimage is downloaded, it is cached and reused, so you do not need to download everything\nagain.\nNotice that it's important to use images from trusted people and\norganizations on Docker Hub since anyone can upload an image. Beyond\nthe risk of running malicious code, there's also the problem of using a\nLinux image that is not up-to-date with the latest security patches.\nRunning Flask in Docker\nTo run a Flask application in Docker, we can use the base Python image.\nFrom there, installing the app and its dependencies can be done via PIP, which is already\ninstalled in the Python image.\nAssuming your project has a requirements.txt file for its pinned dependencies, and a\nsetup.py file that installs the project, creating an image for your project can be done by\ninstructing Docker how to use the pip command.\nIn the following example, we add two new instructions--the COPY command recursively\ncopies a directory structure inside the Docker image, and the RUN command runs PIP via\nshell commands:\nFROM python:3.6\nCOPY . /app\nRUN pip install -r /app/requirements.txt\nRUN pip install /app/\nEXPOSE 5000\nCMD runnerly-tokendealer\nThe 3.6 tag here will get the latest Python 3.6 image that was uploaded to the Hub.",
      "content_length": 1725,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "Containerized Services\n[ 257 ]\nThe COPY command automatically creates the top-level app directory in the container, and\ncopies everything from \".\" in it. One important detail to remember with the COPY command\nis that any change to the local directory (\".\") invalidates the Docker cache, and builds from\nthat step.\nTo tweak this mechanism, you can create a .dockerignore file where you can list files and\ndirectories that should be ignored by Docker.\nLet's try to build that Dockerfile as follows:\n$ docker build -t runnerly/tokendealer .\nSending build context to Docker daemon 148.5 MB\nStep 1/6 : FROM python:3.6\n ---> 21289e3715bd\nStep 2/6 : COPY . /app\n ---> 01cebcda7d1c\nRemoving intermediate container 36f0d93f5d78\nStep 3/6 : RUN pip install -r /app/requirements.txt\n ---> Running in 90200690f834\nCollecting pyjwt (from -r /app/requirements.txt (line 1))\n[...]\nSuccessfully built d2444a66978d\nOnce PIP has installed the dependencies, it installs the project with the second call by\npointing to the app directory. When the pip command is pointed to a directory, it looks for\na setup.py file, and runs it.\nIn the Token Dealer project, a runnerly-tokendealer console script is added to the\nsystem when installed. We are not using Virtualenv here--it would be an overkill, since we\nare already in a container. So, the runnerly-tokendealer script is installed directly\nalongside the Python executable so that they are both reachable directly in the shell.\nThat is why the CMD instruction that points which command should be run when the\ncontainer is executed points directly to runnerly-tokendealer.\nLastly, an EXPOSE instruction was added to let the container listen for connections to the\ninbound TCP port 5000--the one where the Flask app runs.\nNotice that, once the port is exposed, you still need to bridge it to the host system by\nmapping a local port with the exposed port at runtime.\nBridging the port is done with the -p option. In the following example, the container\nbridges its 5000 port with the local 5555 port:\n$ docker run -p 5555:5000 -t runnerly/tokendealer",
      "content_length": 2074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "Containerized Services\n[ 258 ]\nThe last thing we need to do for a fully functional image is to run a web server in front of\nthe Flask application.\nLet's see in the next section how we can do that.\nThe full stack - OpenResty, Circus and Flask\nWhen you release microservices as Docker images, there are two strategies for including a\nweb server.\nThe first one consists of ignoring it and exposing the Flask application directly. A web\nserver like OpenResty could then run in its docker container, proxying calls to your Flask\ncontainer.\nHowever, if you are using some power features in nginx, like a Lua-based application\nfirewall as we have seen in Chapter 7, Securing Your Services, it can be better to include\neverything within the same container, together with a dedicated process manager.\nIn the diagram that follows, the docker container implements the second strategy, and runs\nboth the web server and the Flask service. Circus is used to launch and watch one nginx\nprocess and a few Flask processes:",
      "content_length": 1005,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "Containerized Services\n[ 259 ]\nIn this section, we will implement this container by adding in our Dockerfile the\nfollowing steps:\nDownload, compile, and install OpenResty.\n1.\nAdd an nginx configuration file.\n2.\nDownload and install Circus and Chaussette.\n3.\nAdd a Circus configuration file to run nginx and the Flask app.\n4.\nOpenResty\nThe base Python image uses Debian's apt package manager, and OpenResty (the nginx\ndistribution that includes Lua and other good extensions) is not available directly in the\nstable Debian repository. However, it is quite simple to compile and install OpenResty from\nits source release.\nIn the Dockerfile, we first want to make sure the Debian environment has all the required\npackages to compile OpenResty.\nThe following instructions first update the packages list, then install everything needed:\nRUN apt-get -y update && \\\n    apt-get -y install libreadline-dev libncurses5-dev && \\\n    apt-get -y install libpcre3-dev libssl-dev perl make\nNotice that the three commands in the preceding code are merged as a single RUN\ninstruction to limit the number of instructions the Dockerfile has. By doing so, you can\nlimit the final image size.\nThe next step is to download the OpenResty source code, and perform the compilation step.\ncURL is already available in the Python base image and you can pipe it with the tar\nmodule to decompress the OpenResty release tarball directly from its URL.",
      "content_length": 1420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "Containerized Services\n[ 260 ]\nThe following configure and make calls are straight from OpenResty's documentation,\nand compile and install everything:\n RUN curl -sSL https://openresty.org/download/openresty-1.11.2.3.tar.gz \\\n    | tar -xz && \\\n    cd openresty-1.11.2.3 && \\\n    ./configure -j2 && \\\n    make -j2 && \\\n    make install\nOnce the compilation is over, OpenResty is installed in /usr/local/openresty and you\ncan add an ENV instruction to make sure that the nginx executable is available directly in the\ncontainer's PATH variable:\nENV PATH \"/usr/local/openresty/bin:/usr/local/openresty/nginx/sbin:$PATH\"\nThe last thing we need to do for OpenResty is to include an nginx configuration file to start\nthe web server.\nIn the following minimal example, nginx proxies all calls made to the 8080 port, to the\ncontainer 5000 port--as shown in the previous diagram:\n    worker_processes  4;\n    error_log /logs/nginx-error.log;\n    daemon off;\n    events {\n        worker_connections 1024;\n    }\n    http {\n        server {\n            listen 8080;\n            location / {\n                proxy_pass http://localhost:5000;\n                proxy_set_header Host $host;\n                proxy_set_header X-Real-IP $remote_addr;\n            }\n        }\n    }\nNotice that the error_log path uses the /logs/ directory. It's the root directory within the\ncontainer for our logs.",
      "content_length": 1375,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "Containerized Services\n[ 261 ]\nThat directory needs to be created via a RUN instruction, and a mount point can be added\nthanks to the VOLUME instruction:\nRUN mkdir /logs\nVOLUME /logs\nBy doing this, you will be able to mount the /logs directory on a local directory on the host\nat runtime, and keep the log files even if the container is killed.\nA Docker container filesystem should always be considered as a volatile\nvolume, which can be lost at any moment.\nIf the processes in a container produce anything precious, the resulting\ndata should be copied to a directory that is mounted as a volume outside\nthe container's filesystem.\nThis configuration file is a full nginx configuration and can be used directly with nginx's -c\noption with this call:\n$ nginx -c nginx.conf\nOnce nginx runs, it makes the assumption that Circus listens to incoming TCP connections\non port 5000 and nginx listens itself on port 8080.\nLet's now configure Circus so that it binds a socket on that port and spawns a few Flask\nprocesses.\nCircus\nIf we reuse the Circus and Chaussette setup from Chapter 9, Packaging Runnerly, Circus can\nbind a socket on port 5000, and fork a few Flask processes, which will accept connection on\nthat socket. Circus can also watch the single nginx process we want to run in our container.\nThe first step to using Circus as a process manager in our container is to install it, together\nwith Chaussette, as follows:\nRUN pip install circus chaussette\nFrom there, the following Circus configuration is similar to what we had in the previous\nchapter, except that we have one extra section for nginx:\n[watcher:web]\ncmd = runnerly-tokendealer --fd $(circus.sockets.web)\nuse_sockets = True\nnumprocesses = 5",
      "content_length": 1705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "Containerized Services\n[ 262 ]\ncopy_env = True\n[socket:web]\nhost = 0.0.0.0\nport = 5000\n[watcher:nginx]cmd =  nginx -c /app/nginx.confnumprocesses = 1copy_env =\nTrue\nThe copy_env flag is used, so both Circus and the spawned processes have access to the\ncontainer environment variables. That is why the configuration calls nginx directly without\nindicating its path since the PATH variable was set in our Dockerfile.\nOnce this INI file is created, it can be launched with the circusd command.\nWith all the previous changes, the finalized Dockerfile for our container looks like the\nfollowing:\nFROM python:3.6\n# OpenResty installation\nRUN apt-get -y update && \\\n    apt-get -y install libreadline-dev libncurses5-dev && \\\n    apt-get -y install libpcre3-dev libssl-dev perl make\nRUN curl -sSL https://openresty.org/download/openresty-1.11.2.3.tar.gz \\\n    | tar -xz && \\\n    cd openresty-1.11.2.3 && \\\n    ./configure -j2 && \\\n    make -j2 && \\\n    make install\nENV PATH \"/usr/local/openresty/bin:/usr/local/openresty/nginx/sbin:$PATH\"\n# config files\nCOPY docker/circus.ini /app/circus.ini\nCOPY docker/nginx.conf /app/nginx.conf\nCOPY docker/settings.ini /app/settings.ini\nCOPY docker/pubkey.pem /app/pubkey.pem\nCOPY docker/privkey.pem /app/privkey.pem\n# copying the whole app directory\nCOPY . /app\n# pip installs\nRUN pip install circus chaussette\nRUN pip install -r /app/requirements.txt\nRUN pip install /app/",
      "content_length": 1406,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "Containerized Services\n[ 263 ]\n# logs directory\nRUN mkdir /logs\nVOLUME /logs\n# exposing Nginx's socket\nEXPOSE 8080\n# command that runs when the container is executed\nCMD circusd /app/circus.ini\nIn the Dockerfile example above, the SSH key are directly available in the\nrepository to simplify the example for the book. In a real project, the\nproduction keys should be made available from outside the image\nthrough a mount point.\nAssuming this Docker file is located in a /docker subdirectory in the microservice project,\nit can be built and then run with the following calls:\n$ docker build -t runnerly/tokendealer -f docker/Dockerfile .\n$ docker run --rm --v /tmp/logs:/logs -p 8080:8080 --name tokendealer -it\nrunnerly/tokendealer\nThe /logs mount point is mounted to a local /tmp/logs in this example, and the logs files\nare written in it.\nThe -i option makes sure that stopping the run with a Ctrl + C forwards the termination\nsignal to Circus so that it shuts down everything properly. This option is useful when you\nrun a Docker container in a console. If you do not use -i, and kill the run with Ctrl + C, the\nDocker image will still run, and you will need to terminate it manually via a docker\nterminate call.\nThe --rm option deletes the container when stopped, and the --name option gives a unique\nname to the container in the Docker environment.\nNumerous tweaks can be added in this Dockerfile example. For instance, the sockets\nused by the Circus UIs (both web and command line) to control the Circus daemon could be\nexposed if you want to interact with it from outside the container.\nYou can also expose some of the running options, like the number of Flask processes you\nwant to start--like environment variables--and pass them at run time via Docker with -e.",
      "content_length": 1770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "Containerized Services\n[ 264 ]\nThe fully working Dockerfile can be found at h t t p s ://g i t h u b . c o m /R u n\nn e r l y /t o k e n d e a l e r /t r e e /m a s t e r /d o c k e r .\nIn the next section, we will look at how containers can interact with each other.\nDocker-based deployments\nOnce you have microservices running inside containers, you need them to interact with\neach other. Since we are bridging the container sockets with some local sockets on the host,\nit is pretty transparent from an external client. Each host can have a public DNS or IP, and\nprograms can simply use it to connect to the various services. In other words, a service\ndeployed inside a container on host A can talk to a service deployed inside a container on\nhost B as long as host A and B have a public address and expose the local sockets that are\nbridged with the containers sockets.\nHowever, when two containers need to run on the same host, using the public DNS to make\nthem interact with each other is less than optimal, particularly, if one of the containers is\nprivate to the host. For example, if you run a container in Docker for internal needs, like a\ncaching service, its access should be restricted to the localhost.\nTo make this use case easier to implement, Docker provides a user-defined network feature,\nwhich lets you create local virtual networks. Containers can be added to them, and if they\nrun with a --name option, Docker acts as a DNS resolver, and makes them available in\nthose networks via their names.\nLet's create a new runnerly network with the network command as follows:\n$ docker network create -driver=bridge runnerly\n4a08e29d305b17f875a7d98053b77ea95503f620df580df03d83c6cd1011fb67\nOnce this network is created, we can run containers in it, using the --net option. Let's run\none container with the tokendealer name, like this:\n$ docker run --rm --net=runnerly --name=tokendealer -v /tmp/logs:/logs -p\n5555:8080 -it runnerly/tokendealer\n2017-05-18 19:42:46 circus[5] [INFO] Starting master on pid 5\n2017-05-18 19:42:46 circus[5] [INFO] sockets started\n2017-05-18 19:42:46 circus[5] [INFO] Arbiter now waiting for commands\n2017-05-18 19:42:46 circus[5] [INFO] nginx started\n2017-05-18 19:42:46 circus[5] [INFO] web started",
      "content_length": 2239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "Containerized Services\n[ 265 ]\nIf we run a second container with the same image and a different name on the same\nnetwork, it can ping the first container directly with its tokendealer name:\n$ docker run --rm --net=runnerly --name=tokendealer2 -v /tmp/logs:/logs -p\n8082:8080 -it runnerly/tokendealer ping tokendealer\nPING tokendealer (172.20.0.2): 56 data bytes\n64 bytes from 172.20.0.2: icmp_seq=0 ttl=64 time=0.474 ms\n64 bytes from 172.20.0.2: icmp_seq=1 ttl=64 time=0.177 ms\n64 bytes from 172.20.0.2: icmp_seq=2 ttl=64 time=0.218 ms\n^C\nUsing dedicated Docker networks for your microservices container when you deploy them\nis good practice even if you have a single container running. You can always attach new \ncontainers within the same network, or tweak the network permissions from the shell.\nDocker has other network strategies you can look at in h t t p s ://d o c s . d o c\nk e r . c o m /e n g i n e /u s e r g u i d e /n e t w o r k i n g /.\nHaving to deploy several containers to run one microservice requires you to make sure that\nboth containers are properly configured when launched.\nTo make that configuration easier, Docker has a high-level tool called Docker Compose,\npresented in the next section.\nDocker Compose\nThe command-lines required to run several containers on the same host can be quite long\nonce you need to add names and networks and bind several sockets.\nDocker Compose (h t t p s ://d o c s . d o c k e r . c o m /c o m p o s e /) simplifies the task by letting you \ndefine multiple containers' configuration in a single configuration file.\nThis utility is pre-installed on macOS and Windows when you install Docker. For Linux\ndistributions, you need to get the script and add it to your system. It is a single script, which\nyou can download or even install with PIP (refer to h t t p s ://d o c s . d o c k e r . c o m /c o m p o s e /i\nn s t a l l /).\nOnce the script is installed on your system, you need to create a YAML file named docker-\ncompose.yml, which contains a services section to enumerate your Docker containers.",
      "content_length": 2059,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "Containerized Services\n[ 266 ]\nThe Compose's configuration file has many options that let you define\nevery aspect of the deployment of several containers.\nIt replaces all the commands one usually puts in a Makefile to set and run\ncontainers. This URL lists all options: h t t p s ://d o c s . d o c k e r . c o m /c o m p o s e\n/c o m p o s e - f i l e /.\nIn the following example, the file is placed in one of the Runnerly microservice and defines\ntwo services--microservice, which picks the local Dockerfile, and redis, which uses\nthe Redis's image from the Docker Hub:\nversion: '2'\nnetworks:\n  runnerly:\nservices:\n  microservice:\n    networks:\n     - runnerly\n    build:\n        context: .\n        dockerfile: docker/Dockerfile\n    ports:\n     - \"8080:8080\"\n    volumes:\n     - /tmp/logs:/logs\n  redis:\n    image: \"redis:alpine\"\n    networks:.\n     - runnerly\nThe Compose file also creates networks with its networks sections, so you do not have to\ncreate it manually on your host before you deploy your containers.\nTo build and run those two containers, you can use the up command as follows:\n$ docker-compose up\nStarting tokendealer_microservice_1\nStarting tokendealer_redis_1\nAttaching to tokendealer_microservice_1, tokendealer_redis_1\n[...]\nredis_1         | 1:M 19 May 20:04:07.842 * DB loaded from disk: 0.000\nseconds\nredis_1         | 1:M 19 May 20:04:07.842 * The server is now ready to\naccept connections on port 6379\nmicroservice_1  | 2017-05-19 20:04:08 circus[5] [INFO] Starting master on\npid 5\nmicroservice_1  | 2017-05-19 20:04:08 circus[5] [INFO] sockets started\nmicroservice_1  | 2017-05-19 20:04:08 circus[5] [INFO] Arbiter now waiting",
      "content_length": 1656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "Containerized Services\n[ 267 ]\nfor commands\nmicroservice_1  | 2017-05-19 20:04:08 circus[5] [INFO] nginx started\nmicroservice_1  | 2017-05-19 20:04:08 circus[5] [INFO] web started\nThe first time that command is executed, the microservice image is created.\nUsing Docker Compose is great when you want to provide a full working stack for your\nmicroservices, which includes every piece of software needed to run it.\nFor instance, if you are using a Postgres database, you can use the Postgres image (h t t p s\n://h u b . d o c k e r . c o m /_ /p o s t g r e s /), and link it to your service in a Docker Compose file.\nContainerizing everything, even the databases, is great to showcase your software or for\ndevelopment purposes. However, as we stated earlier, a Docker container should be seen as\na volatile filesystem. So if you use a container for your database, make sure that the\ndirectory where the data is written is mounted on the host file system.\nHowever, in most cases, the database service is usually its dedicated server on a production\ndeployment. Using a container does not make much sense and adds a little bit of overhead\nand risks.\nSo far in this chapter, we have looked at how to run apps in Docker containers, and how to\ndeploy several containers per host and have them interact with each other.\nWhen you deploy a microservice that needs scaling, it is often required to run several\ninstances of the same service to be able to support the load.\nThe next section discusses various options to run several instances of the same container in\nparallel.\nIntroduction to Clustering and Provisioning\nDeploying a microservice at scale can be done by running several containers spread across\none or several hosts.\nOnce your Docker image is created, every host that runs a Docker daemon can be used to\nrun as many containers as you want within the limits of the physical resources. Of course, if\nyou run several instances of the same container on the same host, you need to use a\ndifferent name and socket ports for each instance to differentiate them.",
      "content_length": 2058,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "Containerized Services\n[ 268 ]\nThe collection of containers running the same image is called a cluster, and there are a\nfew tools available to manage clusters.\nDocker has a built-in cluster functionality called swarm mode (h t t p s ://d o c s . d o c k e r . c o m\n/e n g i n e /s w a r m /). This mode has an impressive list of features, which lets you manage all\nyour clusters from a single utility.\nOnce you have deployed a cluster, you need to set up a load balancer so that all the\ninstances of your cluster are sharing the workload. The load balancer can be nginx or\nHAProxy, for instance, and is the entry point to distribute the incoming requests on\nclusters.\nWhile Docker tries to provide all the tools to deal with clusters of containers, managing\nthem can become quite complex. When done properly, it requires to share some\nconfiguration across hosts, and to make sure that bringing containers up and down is\npartially automated. A service discovery feature is needed to ensure that the addition and\nremoval of new containers is detected by the load balancer, for instance.\nService discovery and sharing configuration can be done by tools like Consul (h t t p s ://w w w\n. c o n s u l . i o /) or Etcd (h t t p s ://c o r e o s . c o m /e t c d /) and Docker's swarm mode can be\nconfigured to interact with those tools.\nThe other aspect of setting up clusters is provisioning. This term describes the process of\ncreating new hosts, and therefore, clusters, given the description of the stack you are\ndeploying in some declarative form.\nFor instance, a poor mans provisioning tool can be a custom Python script that follows these\nsteps:\nRead a configuration file that describes the instances needed via a few Docker\n1.\nCompose files.\nStart a few VMs on the cloud provider.\n2.\nWait for all VMs to be up and running.\n3.\nMake sure everything needed to run services on the VM is set.\n4.\nInteract with the Docker daemon on each VM to start some containers.\n5.\nPing whatever service needs to be pinged to make sure the new instances are all\n6.\ninterlinked.\nOnce the task of deploying containers is automated, it can be used to spin off new VMs if\nsome of them crash, for example.",
      "content_length": 2184,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "Containerized Services\n[ 269 ]\nHowever, doing all this work in a Python script has its limits and there are dedicated tools\nto handle this, like Ansible (h t t p s ://w w w . a n s i b l e . c o m /) or Salt (h t t p s ://d o c s . s a l t s t a c\nk . c o m ). Those tools provide a DevOps-friendly environment to deploy and manage hosts.\nKubernetes (h t t p s ://k u b e r n e t e s . i o /) is yet another tool, that can be used to deploy\nclusters containers on hosts. Unlike Ansible or Salt, Kubernetes specializes in deploying\ncontainers and tries to provide a generic solution that works anywhere.\nFor example, Kubernetes can interact with major cloud providers through their API,\nmeaning that once an application deployment is defined, it can be deployed on AWS,\nDigital Ocean, OpenStack, and the like. However, that begs the question of whether this\nability is useful for your project.\nUsually, if you pick a cloud provider for an application, and decide, for some reason, to\nmove to another cloud provider, it is rarely as simple as pushing a new stack. There are\nmany subtle details that make the transition more complex, and the deployment is rarely\nsimilar. For instance, some cloud providers offer data storage solutions that are cheaper to\nuse than running your own PostgreSQL or MySQL deployment, while others make their\ncaching solution much more expensive than running your Redis instances.\nSome teams deploy their services across several cloud providers, but in general, they do not\ndeploy the same microservice on several providers. That would make the cluster\nmanagement too complex.\nMoreover, each major cloud provider offers a full range of built-in tools to manage the\napplications they host, including features like load balancing, discoverability, and auto-\nscaling. They are often the simplest option to deploy clusters of microservices.\nIn the next chapter, we will look at how to deploy applications using AWS.\nOverall, the toolset to use to deploy microservices is dependent on where you are deploying.\nIf you manage your servers, Kubernetes can be an excellent solution to automate many\nsteps, and can be installed directly on a Linux distribution like Ubuntu. That tool can use\nyour Docker images as its basis for deploying your application.\nIf you opt for a hosted solution, looking at what tools are already offered by the provider is\nthe first step before you invest in your toolset.",
      "content_length": 2416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "Containerized Services\n[ 270 ]\nSummary\nIn this chapter, we looked at how microservices can be containerized with Docker, and how\nyou can create a deployment entirely based on Docker images.\nDocker is still a young technology, but it is mature enough to be used in production. The\nmost important thing to keep in mind is that a containerized application can be trashed at\nany time, and any data that's not externalized via a mount point is lost.\nFor provisioning and clustering your services, there's no generic solution, and tons of tools,\nwhich can be combined to create a good solution. There is much innovation right now in\nthat field, and the best choice depends on where you deploy your services, and how your\nteams work.\nThe best way to tackle this problem is to take baby steps by first deploying everything\nmanually, then automating much where it makes sense. Automation is great, but can\nrapidly become a nightmare if you use a toolset you do not fully grasp.\nIn that vein, to make their services easier to use and more appealing, cloud providers have\nbuilt-in features to handle deployments. One of the biggest players is Amazon Web\nServices (AWS) and the next chapter demonstrates how microservices can be deployed on\ntheir platform. Of course, the goal here is not to tell you to use AWS; there are many good\nsolutions out there. However, it gives you a sense of what it is like to deploy your services\non a hosted solution.",
      "content_length": 1436,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "11\nDeploying on AWS\nUnless you are Google or Amazon, and need to run thousands of servers, managing your\nhardware in some data center does not provide many benefits in 2017.\nCloud providers offer to host a solution that is often cheaper than deploying and\nmaintaining your infrastructure. Amazon Web Services (AWS) and others have numerous\nservices that let you manage virtual machines from a web console, and they add new\nfeatures every year.\nOne of the latest AWS additions, for example, is Amazon Lambda. Lambda lets you trigger\na Python script when something happens in your deployments. With Lambda, you do not\nhave to worry about setting up a server and a cron job, or some form of messaging. AWS\ntakes care of executing your script in a VM automatically, and you only pay for execution\ntime.\nCombined with what Docker has to offer, this kind of feature really changes how\napplications can be deployed in the cloud, and provide a fair amount of flexibility. For\ninstance, you do not have to spend too much money to set up a service that might see a\npeak in activity and then slow down. You can deploy a world-class infrastructure that can\nhold an enormous amount of requests, and it stays, in most cases, cheaper than running\nyour hardware.",
      "content_length": 1246,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "Deploying on AWS\n[ 272 ]\nMoving to your own datacenter might save you money in some cases, but it adds a\nmaintenance burden, and it is a challenge to make your deployments as reliable as if they\nwere running at a cloud provider.\nWhile they make much noise in the press, Amazon or Google outages are\nrare events (a few hours a year at most), and their reliability is very high.\nThe Service Level Agreement (SLA) for EC2, for example, guarantees an\nuptime of 99.95% per region or you get some money back. In reality, it is\noften closer to five nines (99.999%).\nYou can track cloud providers' uptime values with online tools like\nhttps://cloudharmony.com/status-1year-for-aws, but their results\nshould be taken with a pinch of salt because some partial outages are not\ncounted sometimes.\nIn this chapter, we are going to do two things:\nDiscover some of the features AWS offers\nDeploy a Flask application on it\nThe goal of this chapter is not to deploy a complete stack, as it is too long, but to give you a\ngood overview of how microservices can be implemented there.\nLet's start with an overview of what AWS has to offer.\nAWS overview\nAmazon Web Service began in 2006 with Amazon Elastic Compute Cloud (Amazon EC2),\nand has extended its services since then. At present (2017), there are countless services. We\nwill not go through all of them in this chapter, but just focus on the ones you usually deal\nwith when you start to deploy microservices:",
      "content_length": 1445,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "Deploying on AWS\n[ 273 ]\nThe AWS services we are interested in can be organized into four five main groups as seen\nin the diagram:\nRouting: Services that redirect requests to the right place, such as DNS services\nand load balancers\nExecution: Services that execute your code, such as EC2 or Lambda\nStorage: Services that store data-storage volumes, caching, regular databases,\nlong-term storage, or CDN\nMessaging: Services that send notifications, emails, and so on\nOne extra group of service that is not displayed in the diagram is everything related to\nprovisioning and deployment.\nLet's have a look at each group.\nIf you want to read the official documentation for an Amazon Service, the\nusual link to reach the root page of each service is\nhttps://aws.amazon.com/<service name>.",
      "content_length": 782,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "Deploying on AWS\n[ 274 ]\nRouting - Route53, ELB, and AutoScaling\nRoute53 (https://aws.amazon.com/route53/) refers to the TCP port 53 that's used for DNS\nservers, and is Amazon's DNS service. Similar to what you would do with BIND\n(http://www.isc.org/downloads/bind/), you can define DNS entries in Route53, and set\nup the service to automatically route the requests to specific AWS services that host\napplications or files.\nDNS is a critical part of a deployment. It needs to be highly available, and to route each\nincoming request as fast as possible. If you are deploying your services on AWS, it is highly\nrecommended to use Route53 or to use the DNS provider of the company where you\nbought the domain, and not deal with DNS yourself.\nRoute53 can work in close cooperation with Elastic Load Balancing (ELB)\n(https://aws.amazon.com/elasticloadbalancing/), which is a load balancer that can be \nconfigured to distribute incoming requests to several backends. Typically, if you are\ndeploying several VMs for the same microservice to create a cluster, ELB can be used to\ndistribute the load among them. ELB monitors all instances through health checks and\nunhealthy nodes can automatically get taken out of rotation.\nThe last interesting service for routing is AutoScaling\n(https://aws.amazon.com/autoscaling/). This service can add instances automatically\ndepending on some events. For instance, if one node is unresponsive or has crashed, it is\ndetected by an ELB Health Check event that can be picked up by AutoScaling. From there,\nthe incriminated VM can be automatically terminated and a new one started.\nWith these three services, you can set up a robust routing system for your microservices. In\nthe next section, let's see what services are used to run the actual code.\nExecution - EC2 and Lambda\nThe core of AWS is EC2 (https://aws.amazon.com/ec2/), which lets you create Virtual\nMachines. Amazon uses the Xen hypervisor (https://www.xenproject.org/) to run Virtual\nMachines, and Amazon Machine Images (AMIs) to install them.\nAWS has a huge list of AMIs you can choose from; you can also create your own AMIs by\ntweaking an existing AMI. Working with AMIs is quite similar to working with Docker\nimages. Once you have picked an AMI from the Amazon console, you can launch an\ninstance, and, after it has booted, you can use SSH into it and start working.",
      "content_length": 2362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "Deploying on AWS\n[ 275 ]\nAt any moment, you can snapshot the VM and create an AMI that saves the instance state.\nThis feature is quite useful if you want to manually set up a server, then use it as a basis for\ndeploying clusters.\nAn EC2 instance comes in different series (https://aws.amazon.com/ec2/instance-\ntypes/). The T2, M3, and M4 series are for a general purpose. The T series uses a bursting\ntechnology, which boosts the baseline performance of the instance when there's a workload\npeak.\nThe C3 and C4 series are for CPU-intensive applications (up to 32 Xeon CPUs), and the X1\nand R4 ones have a lot of RAM (up to 1,952 GiB).\nOf course, the more RAM or CPU, the more expensive the instance is. For Python\nmicroservices, assuming you are not hosting any database on the application instance, a\nt2.xxx or an m3.xx can be a good choice. You need to avoid the t2.nano or t2.micro\nthough, which are fine for running some testing, but too limited for running anything in\nproduction. The size you need to choose depends on the resources taken by the operating\nsystem and your application.\nHowever, since we are deploying our microservices as Docker images, we do not need to\nrun a fancy Linux distribution. The only feature that matters is to choose an AMI that's\ntweaked to run Docker containers.\nIn AWS, the built-in way to perform Docker deployments is to use the EC2 Container\nService (ECS) (https://aws.amazon.com/ecs). ECS offers features that are similar to\nKubernetes, and integrates well with other services. ECS uses its own Linux AMI to run\nDocker containers, but you can configure the service to run another AMI. For instance,\nCoreOS (https://coreos.com/) is a Linux distribution whose sole purpose is to run Docker\ncontainers. If you use CoreOS, that is one part which won't be a locked-in AWS.\nLastly, Lambda (https://aws.amazon.com/lambda/) is a service you can use to trigger the\nexecution of a Lambda Function. A Lambda Function is a piece of code that you can write in\nNode.js, Java, C#, or Python 2.7 or 3.6, and that is deployed as a deployment package, which is\na ZIP file containing your script and all its dependencies. If you use Python, the ZIP file is\nusually a Virtualenv with all the dependencies needed to run the function.\nLambda functions can replace Celery workers, since they can be triggered asynchronously\nvia some AWS events. The benefit of running a Lambda function is that you do not have to\ndeploy a Celery microservice that needs to run 24/7 to pick messages from a queue.\nDepending on the message frequency, using Lambda can reduce costs. However, again,\nusing Lambda means you are locked in AWS services.\nLet's now look at the storage solutions.",
      "content_length": 2689,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "Deploying on AWS\n[ 276 ]\nStorage - EBS, S3, RDS, ElasticCache, and\nCloudFront\nWhen you create an EC2 instance, it works with one or several Elastic Block Stores (EBS)\n(https://aws.amazon.com/ebs/). An EBS is a replicated storage volume EC2 instances can\nmount to use as their filesystem. When you create a new EC2 instance, you can create a new\nEBS, and decide if it runs on an SSD or an HDD disk, the initial size, and some other\noptions. Depending on your choices, the volume is more or less expensive.\nSimple Storage Service (S3) (https://aws.amazon.com/s3/) is a storage service that \norganizes data into buckets. Buckets are, roughly, namespaces that you can use to organize\nyour data. A bucket can be seen as a key-value storage, where a value is data you want to\nstore. There is no upper limit for the size of the data, and S3 provides everything needed to\nstream big files in and out of its buckets. S3 is often used to distribute files, since each entry\nin a bucket can be exposed as a unique, public URL. CloudFront can be configured to use S3\nas a backend.\nOne interesting feature is that S3 provides different storage backend depending on how\noften the files are written or accessed. For instance, Glacier\n(https://aws.amazon.com/glacier/) can be used as a backend when you want to store big\nfiles that are rarely accessed. One use case can be backups. It is quite easy to interact with S3\nfrom your Python applications, and pretty common to see S3 as a data backend in\nmicroservices.\nElasticCache (https://aws.amazon.com/elasticache/) is a cache service that has two\nbackends--Redis and Memcached. ElasticCache leverages Redis' shard and replication\nfeatures, and lets you deploy a cluster of Redis nodes. If you host a lot of data in Redis and\nmight go over the RAM capacity, Redis shards can spread the data across several nodes and\nraise Redis' capacity.\nRelational Database Service (RDS) (https://aws.amazon.com/rds/) is a database service\nthat has many database backends available; in particular, MySQL and PostgreSQL.\nAWS has an online calculator you can use to estimate the cost of your\ndeployments; see http://calculator.s3.amazonaws.com/index.html.",
      "content_length": 2170,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "Deploying on AWS\n[ 277 ]\nThe big advantage of using RDS over your database deployment is that AWS takes care of\nmanaging clusters of nodes, and offers high availability and reliability for your database\nwithout having to worry about doing any maintenance work yourself. The recent addition\nof PostgreSQL in RDS backends made this service very popular, and is often one of the\nreasons people host their application on AWS.\nAnother recently added backend is the proprietary, locked-in Amazon Aurora\n(https://aws.amazon.com/rds/aurora/details/), which implements MySQL 5.x, but is\nsupposed to run much faster (5x faster, according to Amazon).\nLastly, CloudFront (https://aws.amazon.com/cloudfront/) is Amazon's Content Delivery\nNetwork (CDN). If you have static files you want to serve, this is the best way to do it when\nyour users are spread all over the world. Amazon caches the files, and makes them\navailable with the minimum latency possible by routing the client's requests to the closest\nserver. A CDN is what you need to use to serve video, CSS, and JS files--one thing to look\nat, though, is the cost. If you have a few assets to serve for your microservice, it might be\nsimpler to serve them directly from your EC2 instance.\nMessaging - SES, SQS, and SNS\nFor all messaging needs, AWS provides these three major services:\nSimple Email Service (SES): An email service\nSimple Queue Service (SQS): A queue system like RabbitMQ\nSimple Notification Service (SNS): A pub/sub and push notification system that\nworks with SNS\nSimple Email Service (SES)\nIf you build services that send out emails to users, it is hard to make sure they all end up in\ntheir inbox. If you use the local SMTP service from the application's server that sends the\nemail out, it takes much work to configure the system properly so that the emails are not\nflagged as spam by the target mail servers.\nMoreover, even if you do a good job, if the server's IP is part of an IP block that was\nblacklisted because a spammer used an IP close to yours to send out spam, there's not much\nyou can do besides trying to remove your IP from the blacklisting services. The worst case\nscenario is when you get an IP that was used by spammers before you got it.",
      "content_length": 2219,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "Deploying on AWS\n[ 278 ]\nMaking sure your emails end up where they are supposed to is hard, and that is why it is\noften a good idea to use a third-party service that's specializes in sending emails--even if\nyou do not host your microservices in the cloud.\nThere are many of them on the market, and AWS has Simple Email Service (SES)\n(https://aws.amazon.com/ses/ ). Sending emails with SES simply requires you to uses\nSES's SMTP endpoint. They also provide an API, but sticking with SMTP is a good idea so\nthat your services can use a local SMTP when you are doing some development or testing.\nSimple Queue Service (SQS)\nSQS (https://aws.amazon.com/sqs/) is a subset of what you get with RabbitMQ, but it is\noften good enough for most use cases.\nYou can create two types of queue. A First-In-First-Out (FIFO) stores messages in the order\nthey are received, and ensures that a message that's retrieved from the queue is read just\nonce. They are useful when you want to store a stream of messages that need to be picked\nup by workers, like what you would do with Celery and Redis. They have a limit of 20,000\nin-flight messages.\nThe second type (standard) is similar, except that the ordering is not entirely guaranteed.\nThat makes it much faster than the FIFOs, and has a higher limit (120,000).\nThe messages stored in SQS are replicated in several AZs in the AWS cloud, making them\nreliable.\nAWS is organized into Regions and in each Region, Availability Zones.\nRegions are isolated one from each other to ensure fault tolerance and\nstability. AZ are also isolated but they are attached with low-latency links.\nInstances spread across different AZ in the same region can be used\nbehind the same load balancer in AWS.\nSince the maximum size of a message is 256 KB, the volume you can store in a FIFO queue\nis 5 GB, and it is 30 GB for the standard one. In other words, there are no real limitations\nbesides the price.\nSimple Notification Service (SNS)\nThe last service in the messaging tools is SNS (https://aws.amazon.com/sns/), which \noffers two messaging APIs.",
      "content_length": 2061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "Deploying on AWS\n[ 279 ]\nThe first one is a pub/sub API, which can be used to trigger actions in your stack. The\npublisher can be one of the Amazon service or your application, and the subscriber can be\nan SQS queue, a Lambda Function, or any HTTP endpoint such as one of your\nmicroservices.\nThe second one is a push API, which can be used to send messages to mobile devices. SNS\ninteracts, in that case, with third-party APIs such as Google Cloud Messaging (GCM) to\nreach phones or simple text messages via SMS.\nThe SQS and SNS services can be an interesting combo to replace a custom deployment of\nyour messaging system like RabbitMQ. However, you need to check that their features are\ngood enough for your needs.\nIn the next section, we are going to look at the AWS services you can use to provision and\ndeploy services.\nProvisioning and deployment - CloudFormation\nand ECS\nAs described in Chapter 10, Containerized Services, there are many different ways to\nprovision and deploy your Docker containers in the cloud, and tools like Kubernetes can be\nused on AWS to manage all your running instances.\nAWS also offers its service to deploy clusters of containerized applications; it is called EC2\nContainer Service-ECS (https://aws.amazon.com/ecs) and leverages another service\ncalled CloudFormation (https://aws.amazon.com/cloudformation/).\nCloudFormation lets you describe the different instances you want to run on Amazon via\nJSON files, and drives everything automatically on AWS, from deploying instances to\nautoscaling.\nECS is, basically, a set of dashboards to visualize and operate clusters deployed via\nCloudFormation using predefined templates. The AMI used for running the Docker\ndaemon is tweaked for that purpose, such as CoreOS.",
      "content_length": 1743,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "Deploying on AWS\n[ 280 ]\nWhat's convenient with ECS is that you can create and run a cluster for a given Docker\nimage in a matter of minutes by simply filling a couple of forms. The ECS console provides\nsome basic metrics for the cluster, and offers features like scheduling new deployments\ndepending on the CPU or memory usage.\nBeyond the initial form-based setup, clusters deployed via ECS are driven by Task\nDefinitions that define the whole lifecycle for your instances. Those definitions describe the\nDocker containers to run, and the behavior for some events.\nDeploying on AWS - the basics\nNow that we have looked at the major AWS services, let's see how to deploy a microservice\non them in practice.\nTo understand how AWS works, it is good to know how to manually deploy an EC2\ninstance, and run a microservice on it. This section describes how to deploy a CoreOS\ninstance, and run a Docker container in it. Then, we will look at automated clusters'\ndeployments using ECS. Lastly, we will see how Route53 can be used to publish your\nclusters of services under a domain name.\nFirst of all, let's create an AWS account.\nSetting up your AWS account\nThe first step in deploying on Amazon is to create an account at https://aws.amazon.com.\nYou have to enter your credit card information to register, but you can use some of the\nservices with a basic plan for free for a while under some conditions.\nThe services that are offered for free are good enough to evaluate AWS.\nOnce you have registered, you are redirected to the AWS console. The first thing you need\ndo is pick the US East (N. Virginia) region from the top-right corner menu that's under your\nlogin name. North Virginia is the region to use to set up specific billing alerts.",
      "content_length": 1738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "Deploying on AWS\n[ 281 ]\nThe second thing you should do is to configure the alarm in the Billing Console by visiting\nhttps://console.aws.amazon.com/billing/home#/ (or navigating to it from the menu),\nand in the preferences, check the Receive Billing Alerts checkbox:",
      "content_length": 266,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "Deploying on AWS\n[ 282 ]\nOnce the option is set, you need to go to the CloudWatch panel at h t t p s ://c o n s o l e . a w s . a\nm a z o n . c o m /c l o u d w a t c h /h o m e , and select Alarms | Billing in the left panel to create a new\nalert. A new popup window opens, and we can set a notification in case one of the services\nwe use starts to cost money. Setting up $0.01 as the maximum charge does the trick. This\nnotification prevents you from spending money if you are just doing some testing:\nAt any time, you can reach any service by clicking on the Services menu in the top-left\ncorner. It opens a panel with all the services.",
      "content_length": 639,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "Deploying on AWS\n[ 283 ]\nIf you click on EC2, you are redirected to the EC2 console at\nhttps://console.aws.amazon.com/ec2/v2/home, where you can create new instances:",
      "content_length": 166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "Deploying on AWS\n[ 284 ]\nDeploying on EC2 with CoreOS\nLet's click on the Launch Instance blue button, and pick an AMI to run a new VM:\nUnder Community AMIs, you can search for an instance of CoreOS to list all the available\nCoreOS AMIs.\nThere are two types of AMI: Paravirtual (PV) or Hardware Virtual Machine (HVM). These\nare the two different levels of virtualization in the Xen hypervisor. PV is full virtualization,\nwhereas HVM is partial virtualization. Depending on the Linux distribution, you might not\nbe able to run all types of VMs under PV.\nIf you just want to play around, select the first PV AMI in the list. Then, in the next screen,\npick a t1.micro, and go on directly with the Review And Launch option. Lastly, hit the\nLaunch button.",
      "content_length": 749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "Deploying on AWS\n[ 285 ]\nJust before it creates the VM, the console asks you to create a new SSH key pair, which is a\ncrucial step if you want to be able to access the VM. You should generate a new key pair per\nVM, give the key pair a unique name, and download the file. You get a .pem file, which you\ncan add to your ~/.ssh.\nDo not lose this file, AWS does not store it for you for security reasons.\nOnce you have launched your instance, it is listed in the EC2 console-you can see it if you\nclick on the Instances menu on the left-hand side.",
      "content_length": 543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "Deploying on AWS\n[ 286 ]\nYou can see the status of the VM in the status checks column. It takes a few minutes for\nAWS to deploy the VM. Once everything is ready, you should be able to SSH into the box\nby using the .pem file as a key and the public DNS of the VM.\nThe default user for CoreOS is core, and, once you're connected, everything needed to run\nDocker containers is available, but will need an update. While CoreOS self-updates\ncontinuously, you can force an update of the system with update_engine_client, and\nreboot the VM with the sudo reboot command, as follows:\n$ ssh -i ~/.ssh/runnerly.pem\ncore@ec2-34-224-101-250.compute-1.amazonaws.com\nCoreOS (alpha)\ncore@ip-172-31-24-180 ~ $\ncore@ip-172-31-24-180 ~ $ update_engine_client -update\n[0530/083245:INFO:update_engine_client.cc(245)] Initiating update check and\ninstall.\n[0530/083245:INFO:update_engine_client.cc(250)] Waiting for update to\ncomplete.\nLAST_CHECKED_TIME=1496132682\nPROGRESS=0.000000\nCURRENT_OP=UPDATE_STATUS_UPDATED_NEED_REBOOT\nNEW_VERSION=0.0.0.0\nNEW_SIZE=282041956\ncore@ip-172-31-24-180 ~ $ sudo reboot\nConnection to ec2-34-224-101-250.compute-1.amazonaws.com closed by remote\nhost.\nConnection to ec2-34-224-101-250.compute-1.amazonaws.com closed.\nOnce the VM is back, you should have a recent version of Docker, and you can try it by\nechoing hello from a busybox Docker container, shown as follows:\n$ ssh -i ~/.ssh/runnerly.pem\ncore@ec2-34-224-101-250.compute-1.amazonaws.com\nLast login: Tue May 30 08:24:26 UTC 2017 from 91.161.42.131 on pts/0\nContainer Linux by CoreOS alpha (1423.0.0)\ncore@ip-172-31-24-180 ~ $\ndocker -v Docker version 17.05.0-ce, build 89658be\ncore@ip-172-31-24-180 ~ $ docker run busybox /bin/echo hello\nUnable to find image 'busybox:latest' locally\nlatest: Pulling from library/busybox\n1cae461a1479: Pull complete\nDigest:\nsha256:c79345819a6882c31b41bc771d9a94fc52872fa651b36771fbe0c8461d7ee558\nStatus: Downloaded newer image for busybox:latest hello\ncore@ip-172-31-24-180 ~ $",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "Deploying on AWS\n[ 287 ]\nIf the previous call was successful, you now have a fully working Docker environment.\nLet's try to run a web app in it now using the docker-flask image from the Docker Hub:\ncore@ip-172-31-24-180 ~ $\ndocker run -d -p 80:80 p0bailey/docker-flask\nUnable to find image 'p0bailey/docker-flask:latest' locally\nlatest:\nPulling from p0bailey/docker-flask\nbf5d46315322: Pull complete\n9f13e0ac480c: Pull complete\ne8988b5b3097: Pull complete\n40af181810e7: Pull complete\ne6f7c7e5c03e: Pull complete\nef4a9c1b628c: Pull complete\nd4792c0323df: Pull complete\n6ed446a13dca: Pull complete\n886152aa6422: Pull complete\nb0613c27c0ab: Pull complete\nDigest:\nsha256:1daed864d5814b602092b44958d7ee6aa9f915c6ce5f4d662d7305e46846353b\nStatus: Downloaded newer image for p0bailey/docker-flask:latest\n345632b94f02527c972672ad42147443f8d905d5f9cd735c48c35effd978e971\nBy default, AWS opens only port 22 for SSH access. To reach port 80, you need to go to the\nInstances list in the EC2 console and click on the Security Group that was created for the\ninstance (usually named launch-wizard-xx).",
      "content_length": 1085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "Deploying on AWS\n[ 288 ]\nClicking on it brings up the Security Group page, where you can edit the Inbound Rules to\nadd HTTP. This immediately opens port 80, and you should be able to visit your Flask app\nusing the public DNS in your browser.\nThis is what it takes to run a Docker image on AWS, and it is the basis for any deployment.\nFrom there, you can deploy clusters by creating groups of instances managed by the\nAutoScaling and ELB services.\nThe higher-level tool, CloudFormation, can take care of all these steps automatically using\ntemplate definitions. However, ECS is the ultimate level of deployment automation on\nAWS when you are using Docker-let's see how to use it in the next section.\nDeploying with ECS\nAs described earlier in this chapter, ECS takes care of deploying Docker images\nautomatically, and sets up all the services needed around the instances.\nYou do not need, in this case, to create EC2 instances yourself. ECS uses its own AMI, which\nis tweaked to run Docker containers on EC2. It is pretty similar to CoreOS, as it comes with\na Docker daemon, but it is integrated with the AWS infrastructure for sharing configuration\nand triggering events.\nAn ECS cluster deployment is composed of many elements:\nAn Elastic Load Balancer (in EC2) to distribute the requests among the instance\nA Task Definition, which is used to determine which Docker image needs to be\ndeployed, and what ports should be bound between the host and the container\nA Service, which uses the Task Definition to drive the creation of EC2 instances,\nand run the Docker container in them\nA Cluster, which groups Services, Task Definitions, and an ELB",
      "content_length": 1642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "Deploying on AWS\n[ 289 ]\nDeploying a cluster on ECS when you are not used to it is complex, because it requires\ncreating elements in a specific order. For instance, the ELB needs to be set up before\neverything else.\nFortunately, everything can be created for you in the right ordering via the first run wizard.\nThis wizard is displayed when you go to the ECS service on the console for the first time,\nand will bootstrap everything for you. Once you are on the landing page, Click on the Get\nStarted button to launch the wizard.\nYou can check the Deploy a sample application onto Amazon ECS Cluster option, and get\ngoing:",
      "content_length": 621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "Deploying on AWS\n[ 290 ]\nThis action brings up a task definition creation dialog, where you can define a name for the\ntask and the container to be used for that task:",
      "content_length": 166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "Deploying on AWS\n[ 291 ]\nIn the preceding example, we deployed the same Flask application that we deployed earlier\non EC2, so we provide that image name on the container form for the task definition. The\nDocker image needs to be present on Docker Hub or AWS's own Docker images repository.\nIn that form, you can also set up all the port mapping between the Docker container and the\nhost system. That option is used by ECS when the image is run. Here, we bind port 80,\nwhich is where the Flask docker image that we are using exposes the app.\nThe next step in the wizard is the Service configuration:",
      "content_length": 598,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "Deploying on AWS\n[ 292 ]\nWe add three tasks into that service, as we want to run three instances in our cluster with\none Docker container running on each one of them. In the Application Load Balancer\nsection, we use the container name and the port defined earlier. Lastly, we need to\nconfigure a cluster, where we set up an instance type, some instances, and the SSH key pair\nto use:",
      "content_length": 383,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "Deploying on AWS\n[ 293 ]\nOnce you validate that last step, the ECS wizard works for a little while to create all parts,\nand once it is ready, you end up with a view service button, which is enabled once all the\nparts are created. The Service page summarizes all the parts of the deployment, and has\nseveral tabs to check every service in detail. The deployment done by the ECS wizard can be\nsummarized as follows:\nA task definition was created to run the Docker container\nA cluster of three EC2 instances was added, and in that cluster\nA Service was added to the cluster, and Task Definition was used to deploy\nDocker containers in the EC2 instance\nThe deployment was load-balanced by the ELB created earlier\nIf you go back to the EC2 console, and visit the Load Balancing | Load Balancers menu on\nthe left, you will find the newly created ECS-first-run-alb ELB that is used to serve\nyour ECS cluster.",
      "content_length": 901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "Deploying on AWS\n[ 294 ]\nThis ELB has a public DNS name, which you can use in your browser to visit your Flask\napp. The URL is in the form of http://<ELB name>.<region>.elb.amazonaws.com.\nThe next section explains how to link this ELB URL to a clean domain name.\nRoute53\nRoute53 can be used to create an alias with your domain name. If you visit the service\nconsole at https://console.aws.amazon.com/route53, and click on the hosted zones\nmenu, you can add a new hosted zone for your domain name, which is an alias to the ELB\npreviously set.\nAssuming that you already own the domain name from a registrar, you can simply redirect\nthe domain to AWS's DNS. Click on Create Hosted Zone, and add your domain.\nOnce it is created, you can go to Create a Record Set, and select a type A record. The record\nhas to be an Alias, and in the target input, a dropdown appears with the list of available\ntargets:",
      "content_length": 898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "Deploying on AWS\n[ 295 ]\nThe ELB load balancer that was previously created by the wizard should appear in that list,\nand selecting it links your domain name to that ELB:",
      "content_length": 169,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "Deploying on AWS\n[ 296 ]\nThis step is all it takes to link a domain name to your deployed ECS cluster; and you can\nadd more entries with a subdomain, for instance, for each one of your deployed\nmicroservice.\nRoute53 has DNS servers all over the world, and other interesting features like a health\ncheck that you can use to ping your ELB and underlying services regularly. In case there's a\nfailure, Route53 can send an alarm to CloudWatch, and automatically, even fail over all the\ntraffic to another healthy ELB if you have several ones set.\nSummary\nContainerized applications are becoming the norm for deploying microservices, and cloud\nvendors are all following that trend.\nGoogle, Amazon, and all the other big players are now able to deploy and manage clusters\nof Docker containers. So, if your application is dockerized, you should be able to deploy it\neasily. In this chapter, we have looked at how to do it in AWS, which has its service (ECS) to\nmanage Docker images that is tightly integrated with all the other main AWS services.\nOnce you are familiar with all the AWS services, it is a pretty powerful platform that can be\ntweaked to publish not only large-scale microservices-based applications, but also smaller\napplications for a fraction of the price you would pay if you were running your own data\ncenter.\nIn the next chapter, we conclude this book by giving a few leads for going further into the\nart of building microservices.",
      "content_length": 1444,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "12\nWhat Next?\nFive years ago, choosing a Python version was driven by these two factors:\nThe operating system used to deploy your applications\nThe availability of the libraries your application used\nOne extreme example of how the operating system influences this decision is when CentOS\nis used. CentOS is really close to Red Hat Enterprise Linux (RHEL) minus the commercial\nsupport, and many companies that started off with RHEL and grew internal teams, ended\nup moving to CentOS. There are a lot of good reasons to use CentOS. This Linux\ndistribution is popular and based on a robust set of management tools.\nHowever, using CentOS means you cannot use the latest Python version for your projects\nunless you install a custom Python instance on the system. Moreover, that is often\nconsidered to be bad practice from an Ops point of view because you go out of the\nsupported versions. For that reason, some developers were forced to use 2.6 for a very long\ntime, and that prevented them from using the newest Python syntax and features.\nThe other reason people stayed on Python 2 was that a few essential libraries were still not\nported to Python 3. However, this is not the case anymore--if you start a new microservice\nproject in 2017, everything is available for Python 3.\nThose two reasons to stick with older Python versions are gone nowadays; you can pick the\nlatest Python 3, and ship your app on whatever Linux distribution is inside a Docker\ncontainer.",
      "content_length": 1459,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "What Next?\n[ 298 ]\nAs we've seen in Chapter 10, Containerized services, Docker seems to be the new standard\nfor containerizing applications. But, maybe, other players will become serious alternatives,\nlike CoreOs's rkt (h t t p s ://c o r e o s . c o m /r k t /). In any case, the maturity of the containers\ntechnology will be reached the day all containers engines are based on a universal standard\nto describe images--and that is the goal of organizations such as Open Container Initiative\n(OCI) (h t t p s ://w w w . o p e n c o n t a i n e r s . o r g /), which is driven by all the big containers and\ncloud players.\nFor all these reasons, using the latest Python 3 and Docker for your microservices is a safe\nbet. Your Dockerfile syntax is probably going to be very close to whatever syntax an\ninitiative like OCI will build.\nSo, if Python 3.6 or the next versions have great features, nothing will prevent you from\nmoving forward and using them for your next microservice--and as we've said throughout\nthe book, it's fine to use different stacks or Python versions for each microservice.\nIn this book, Flask was picked, because that framework is excellent to build microservices,\nand has a vast and mature ecosystem. But since Python 3.5, web frameworks based on the\nasyncio library (h t t p s ://d o c s . p y t h o n . o r g /3/l i b r a r y /a s y n c i o . h t m l ) along with the\nasync and await new language keywords are starting to become serious alternatives.\nThere are good chances that in a couple of years, one of them will replace Flask as the most\npopular framework, because the benefits regarding the performances of I/O bound\nmicroservices are huge, and developers are starting to adopt asynchronous programming.\nIn this last chapter, we are going to look at how asynchronous programming works in\nPython 3.5+, and discover two web frameworks that can be used to build microservices\nasynchronously.\nIterators and generators\nTo understand how asynchronous programming works in Python, it is important to first\nunderstand how iterators and generators work because they are the basis of asynchronous\nfeatures in Python.\nAn iterator in Python is a class that implements the Iterator protocol. The class must\nimplement the following two methods:\n__iter__(): Returns the actual iterator. It often returns self\nnext(): Returns the next value until StopIteration() is raised",
      "content_length": 2387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "What Next?\n[ 299 ]\nIn the following example, we'll implement the Fibonacci sequence as an iterator:\n    class Fibo:\n        def __init__(self, max=10):\n            self.a, self.b = 0, 1\n            self.max = max\n            self.count = 0\n        def __iter__(self):\n            return self\n        def next(self):\n            try:\n                return self.a\n            finally:\n                if self.count == self.max:\n                    raise StopIteration()\n                self.a, self.b = self.b, self.a + self.b\n                self.count += 1\nIterators can be used directly in loops, as follows:\n>>> for number in Fibo(10):\n...     print(number)\n...\n0\n1\n1\n2\n3\n5\n8\n13\n21\n34\nTo make iterators more Pythonic, generators were added to Python. They have introduced\nthe yield keyword. When yield is used by a function instead of return, this function is\nconverted into a generator. Each time the yield keyword is encountered, the function\nreturns the yielded value and pauses its execution.\n    def fibo(max=10):\n        a, b = 0, 1\n        cpt = 0\n        while cpt < max:\n            yield a\n            a, b = b, a + b\n            cpt += 1",
      "content_length": 1151,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "What Next?\n[ 300 ]\nThis behavior makes generators a bit similar to coroutines found in other languages, except\nthat coroutines are bidirectional. They return a value as yield does, but they can also receive\na value for its next iteration.\nBeing able to pause the execution of a function and communicate with it both ways is the\nbasis for asynchronous programming--once you have this ability, you can use an event\nloop, and pause and resume functions.\nThe yield call was extended to support receiving values from the caller via the sender()\nmethod. In the next example, a terminal() function simulates a console, which\nimplements three instructions, echo, exit, and eval:\n    def terminal():\n        while True:\n            msg = yield    # msg gets the value sent via a send() call\n            if msg == 'exit':\n                print(\"Bye!\")\n                break\n            elif msg.startswith('echo'):\n                print(msg.split('echo ', 1)[1])\n            elif msg.startswith('eval'):\n                print(eval(msg.split('eval', 1)[1]))\nWhen instantiated, this generator can receive data via its send() method:\n>>> t = terminal()\n>>> t.next()    # call to initialise the generator - similar to send(None)\n>>> t.send(\"echo hey\")\nhey\n>>> t.send(\"eval 1+1\")\n2\n>>> t.send(\"exit\")\nBye!\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration\nThanks to this addition, Python generators became similar to coroutines.\nAnother extension that was added to yield is yield from, which lets you chain-call another\ngenerator.",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "What Next?\n[ 301 ]\nConsider the following example, where a generator is uses two other generators to yield\nvalues:\n    def gen1():\n        for i in [1, 2, 3]:\n            yield i\n    def gen2():\n        for i in 'abc':\n            yield i\n    def gen():\n        for val in gen1():\n            yield val\n        for val in gen2():\n            yield val\nThe two for loops in the gen() function can be replaced by a single yield from call as\nfollows:\n    def gen():\n        yield from gen1()\n        yield from gen2()\nHere's an example of calling the gen() method until each sub generator gets exhausted:\n>>> list(gen())\n[1, 2, 3, 'a', 'b', 'c']\nCalling several other coroutines and waiting for their completion is a prevalent pattern in\nasynchronous programming. It allows developers to split their logic into small functions\nand assemble them in sequence. Each yield call is an opportunity for the function to pause\nits execution and let another function take over.\nWith these features, Python got one step closer to supporting asynchronous programming\nnatively. Iterators and generators were used as building blocks to create native coroutines.\nCoroutines\nTo make asynchronous programming more straightforward, the await and async\nkeywords were introduced in Python 3.5, along with the coroutine type. The await call is\nalmost equivalent to yield from, as its goal is to let you call a coroutine from another\ncoroutine.",
      "content_length": 1419,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "What Next?\n[ 302 ]\nThe difference is that you can't use the await call to call a generator (yet).\nThe async keyword marks a function, a for or a with loop, as being a native coroutine, and\nif you try to use that function, you will not retrieve a generator but a coroutine object.\nThe native coroutine type that was added in Python is like a fully symmetric generator, but\nall the back and forth is delegated to an event loop, which is in charge of coordinating the\nexecution.\nIn the example that follows, the asyncio library is used to run main(), which, in turn, calls\nseveral coroutines in parallel:\n    import asyncio\n    async def compute():\n        for i in range(5):\n            print('compute %d' % i)\n            await asyncio.sleep(.1)\n    async def compute2():\n        for i in range(5):\n            print('compute2 %d' % i)\n            await asyncio.sleep(.2)\n    async def main():\n        await asyncio.gather(compute(), compute2())\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())\n    loop.close()\nWhat's compelling about such an application is that, besides the async and await\nkeywords, it looks like plain sequential Python--making it very readable. And since\ncoroutines work by ceding control and not by interrupting, it's deterministic and the events\noccur in the same way every time it runs unlike programming with threads.\nNotice that the asyncio.sleep() function is a coroutine, so it is called with the await\nkeyword.\nIf you run this program, you will get the following output:\n$ python async.py\ncompute 0\ncompute2 0\ncompute 1\ncompute2 1\ncompute 2",
      "content_length": 1592,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "What Next?\n[ 303 ]\ncompute 3\ncompute2 2\ncompute 4\ncompute2 3\ncompute2 4\nIn the next section, we will take a closer look at the asyncio library.\nThe asyncio library\nThe asyncio (h t t p s ://d o c s . p y t h o n . o r g /3/l i b r a r y /a s y n c i o . h t m l ) library, which was \noriginally an experiment called Tulip run by Guido, provides all the infrastructure to build\nasynchronous programs based on an event loop.\nThe library predates the introduction of async, await, and native coroutines in the\nlanguage.\nThe asyncio library is inspired by Twisted, and offers classes that mimic Twisted transports\nand protocols. Building a network application based on these consists of combining a\ntransport class (like TCP) and a protocol class (such as HTTP), and using callbacks to\norchestrate the execution of the various parts.\nBut, with the introduction of native coroutines, callback-style programming is less\nappealing, since it's much more readable to orchestrate the execution order via await calls.\nYou can use coroutine with asyncio protocol and transport classes, but the original design\nwas not meant for that and requires a bit of extra work.\nHowever, the central feature is the event loop API and all the functions used to schedule\nhow the coroutines will get executed. An event loop uses the operating system I/O poller\n(devpoll, epoll, and kqueue) to register the execution of a function given an I/O event.\nFor instance, the loop can wait for some data to be available in a socket to trigger a function\nthat will treat the data. But that pattern can be generalized to any event. For instance, when\ncoroutine A awaits for coroutine B to be finished, the call to asyncio sets an I/O event, which\nis triggered when coroutine B is over and makes coroutine A wait for that event to resume.\nThe result is that if your program is split into a lot of interdependent coroutines, their\nexecutions are interleaved. The beauty of this pattern is that a single-threaded application\ncan run thousands of coroutines concurrently without having to be thread-safe and without\nall the complexity that it entails.",
      "content_length": 2110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "What Next?\n[ 304 ]\nTo build an asynchronous microservice, the typical pattern is like this:\n    async def my_view(request):\n        query = await process_request(request)\n        data = await some_database.query(query)\n        response = await build_response(data)\n        return response\nAn event loop running this coroutine for each incoming request will be able to accept\nhundreds of new requests while waiting for each step to finish.\nIf the same service were built with Flask, and typically run with a single thread, each new\nrequest would have to wait for the completion of the previous one to get the attention of the\nFlask app. Hammering the service with several hundred concurrent requests will issue\ntimeouts in no time.\nThe execution time for a single request is the same in both cases, but the ability to run many\nrequests concurrently and interleave their execution is what makes asynchronous\napplications better for I/O-bound microservices. Our application can do a lot of things with\nthe CPU while waiting for a call to a database to return.\nAnd if some of your services have CPU-bound tasks, asyncio provides a function to run\nthe code in a separate thread or process from within the loop.\nIn the next two sections, we will present two frameworks based on asyncio, which can be\nused to build microservices.\nThe aiohttp framework\nThe aiohttp (h t t p ://a i o h t t p . r e a d t h e d o c s . i o /) framework is a popular asynchronous \nframework based on the asyncio library, which has been around since the first days of the\nlibrary.\nLike Flask, it provides a request object and a router to redirect queries to functions that\nhandle them.\nThe asyncio library's event loop is wrapped into an Application object, which handles\nmost of the orchestration work. As a microservice developer, you can just focus on building\nyour views as you would do with Flask.",
      "content_length": 1873,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "What Next?\n[ 305 ]\nIn the following example, the api() coroutine returns some JSON response when the\napplication is called on /api:\n    from aiohttp import web\n    async def api(request):\n        return web.json_response({'some': 'data'})\n    app = web.Application()\n    app.router.add_get('/api', api)\n    web.run_app(app)\nThe aiohttp framework has a built-in web server, which is used to run this script via the\nrun_app() method, and, overall, if you are used to Flask, the biggest difference is that you\ndo not use decorators to route requests to your views.\nThis framework provides helpers like those you find in Flask, plus some original features\nsuch as its Middleware, which will let you register coroutines to perform specific tasks such\nas custom error handling.\nSanic\nSanic (h t t p ://s a n i c . r e a d t h e d o c s . i o /) is another interesting project, which specifically\ntries to provide a Flask-like experience with coroutines.\nSanic uses uvloop (h t t p s ://g i t h u b . c o m /M a g i c S t a c k /u v l o o p ) for its event loop, which is a\nCython implementation of the asyncio loop protocol using libuv, allegedly making it\nfaster. The difference might be negligible in most of your microservices, but is good to take\nany speed gain when it is just a transparent switch to a specific event loop implementation.\nIf we write the previous example in Sanic, it's very close to Flask:\n    from sanic import Sanic, response\n    app = Sanic(__name__)\n    @app.route(\"/api\")\n    async def api(request):\n        return response.json({'some': 'data'})\n    app.run()",
      "content_length": 1582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "What Next?\n[ 306 ]\nNeedless to say, the whole framework is inspired by Flask, and you will find most of the\nfeatures that made it a success, such as Blueprints.\nSanic also has its original features, like the ability to write your views in a class\n(HTTPMethodView) that represents one endpoint, with one method per verb (GET, POST,\nPATCH, and so on).\nThe framework also provides middleware to change the request or response.\nIn the next example, if a view returns a dictionary, it will be automatically converted to\nJSON:\n    from sanic import Sanic\n    from sanic.response import json\n    app = Sanic(__name__)\n    @app.middleware('response')\n    async def convert(request, response):\n        if isinstance(response, dict):\n            return json(response)\n        return response\n    @app.route(\"/api\")\n    async def api(request):\n        return {'some': 'data'}\n    app.run()\nThis little middleware function simplifies your views if your microservice produces only\nJSON mappings.\nAsynchronous versus synchronous\nSwitching to an asynchronous model means you will need to use asynchronous code all the\nway down.\nFor example, if your microservice uses a Requests library that is not asynchronous, every\ncall made to query an HTTP endpoint will block the event loop, and you will not benefit\nfrom asynchronicity.",
      "content_length": 1311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "What Next?\n[ 307 ]\nAnd making an existing project asynchronous is not an easy task because it changes the\ndesign completely. Most projects that want to support asynchronous calls are redesigning\neverything from scratch.\nThe good news is that there are more and more asynchronous libraries\navailable, which can be used to build a microservice. On PyPI, you can\nsearch for aio or asyncio.\nThis wiki page (h t t p s ://g i t h u b . c o m /p y t h o n /a s y n c i o /w i k i /T h i r d P a r t y\n) is also a good place to look at.\nHere's a short list of those that are relevant to building microservices:\naiohttp.Client: Can replace the requests package\naiopg: PostgreSQL driver on top of Psycopg\naiobotocore: AWS client--might be merged with the official boto3 project at\nsome point\naioredis: Redis client\naiomysql: MySQL client, built with PyMySQL\nIn case you cannot find a replacement for one of your libraries, asyncio provides a way to\nrun blocking code in a separate thread or process via an executor. This function is a\ncoroutine, and uses a ThreadPoolExecutor or a ProcessPoolExecutor class from the\nconcurrent module under the hood.\nIn the example that follows, the requests library is used via a pool of threads:\n    import asyncio\n    from concurrent.futures import ThreadPoolExecutor\n    import requests\n    # blocking code\n    def fetch(url):\n        return requests.get(url).text\n    URLS = ['http://ziade.org', 'http://python.org', 'http://mozilla.org']\n    # coroutine\n    async def example(loop):\n        executor = ThreadPoolExecutor(max_workers=3)\n        tasks = []\n        for url in URLS:\n            tasks.append(loop.run_in_executor(executor, fetch, url))",
      "content_length": 1677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "What Next?\n[ 308 ]\n        completed, pending = await asyncio.wait(tasks)\n        for task in completed:\n            print(task.result())\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(example(loop))\n    loop.close()\nEach call to run_in_executor() returns a Future object, which can be used to set some\nsynchronization points in your asynchronous program. The Future objects keep an eye on\nthe state of the execution, and provide a method for retrieving the result once it is available.\nPython 3 has two Future classes that are slightly different, and that can be\nconfusing. The asyncio.Future is a class you can use directly with the\nevent loop, while concurrent.futures.Future is a class that is used in\nthe ThreadPoolExecutor or ProcessPoolExecutor class.\nTo avoid any confusion, you should isolate the code that is working with\nrun_in_executor(), and get back the results as soon as they are\navailable.\nKeeping Future objects around is a recipe for disaster.\nThe asyncio.wait() function can wait for all the Futures to complete, so the example()\nfunction here will block until all the Futures return. The Wait() function can take a timeout\nvalue, so the function returns a tuple composed of the list of completed Futures and the\nones that are still running. When not using a timeout, it waits indefinitely (unless you have\na general timeout on the socket library).\nYou can use processes instead of threads, but, in that case, all the data that goes in and out\nof your blocking function needs to be pickable. To avoid blocking code altogether is the best\noption, particularly, if the code is I/O bound.\nThat said, if you have a function that is CPU bound, it can be worthwhile to run it in a\nseparate process to use all the CPU cores available, and speed up your microservice.",
      "content_length": 1796,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "What Next?\n[ 309 ]\nSummary\nIn this final chapter, we have looked at how we can write microservices using asynchronous\nprogramming in Python. While Flask is a great framework, asynchronous programming\nmight be the next big revolution in Python for writing microservices that are usually I/O\nbound.\nThere are more and more asynchronous frameworks and libraries based on Python 3.5 and\nbeyond, which makes this approach appealing.\nSwitching from Flask to one of these frameworks for one of your microservices can be a\ngood way to experiment with limited risks.",
      "content_length": 557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "Index\nA\nAdvanced Message Queuing Protocol (AMQP)\n   about  140\n   binding  140\n   exchange  140\n   queue  140\n   RPC, using over  144\n   URL  140\naiohttp framework\n   about  304\n   URL  27, 304\nAlpine Linux\n   about  255\n   URL  255\nAmazon Aurora\n   URL  277\nAmazon Elastic Compute Cloud (Amazon EC2) \n272\nAmazon Lambda  271\nAmazon Machine Images (AMIs)  274\nAmazon RDS  112\nAmazon SQS\n   about  112\n   URL  103\nAmazon Web Services (AWS)\n   about  198, 271\n   account, setting up  280\n   deployment on  280\n   overview  272\n   URL  280\nAngularJS\n   about  204\n   URL  204\nAnsible\n   URL  269\nApache  22\nApache Bench (AB)\n   URL  75\nApache Public Licence Version 2 (APL v2)  231\nargparse module\n   about  244\n   URL  244\nAssociation for Computing Machinery (ACM)  21\nasynchronous calls\n   about  137\n   Celery, mocking  147\n   mocking  147, 149\n   performing  145\n   Publish/Subscribe (pubsub) pattern  144\n   RPC, over AMQP  144\n   task queues  138\n   topic queues  139\nAsynchronous JavaScript and XML (AJAX)  208\nasynchronous libraries\n   reference  307\nasynchronous\n   versus synchronous  306\nasyncio library\n   about  26, 303\n   URL  27, 298, 303\nAuth0\n   about  172\n   URL  172\nauthenticated users\n   first-time user  218\n   returning user  218\nauthentication\n   about  218\n   Data Service, interacting with  218, 219\n   JavaScript authentication  221, 222, 223\n   reference  107\n   Strava token  219, 220\nauthorization  218",
      "content_length": 1428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "[ 311 ]\nAuthorization Code Grant  170\nAutodoc Sphinx extension\n   URL  85\nAutoScaling\n   about  274\n   URL  274\nAWS services\n   execution  273\n   messaging  273\n   routing  273\n   storage  273\nB\nBabel\n   about  206\n   URL  206\n   using  212, 214\nBandit linter\n   reference  199\n   using  199, 200, 202\nBandit security linter\n   about  61\n   URL  61\nbhyve  252\nBilling Console\n   URL  281\nBIND\n   URL  274\nBitbucket\n   URL  88\nBlinker\n   about  49\n   URL  49\nblueprints  56\nBoom\n   URL  75\nBottle\n   URL  22, 33\nBower\n   URL  212\n   using  212, 214\nbuilt-in features, Flask\n   about  47\n   blueprints  56\n   configuration  54\n   debugging  57\n   error handling  57\n   extensions  51\n   globals  48\n   middlewares  51\n   session object  47\n   signals  49\n   templates  52\nC\nCacheControl project\n   URL  129\nCelery\n   mocking  147\n   URL  103\nCentOS  297\nCertificate Authority (CA)  177\ncgroups\n   about  252\n   URL  252\nChaussette\n   about  248\n   URL  248\ncheck-manifest distutils command  236\nCircus\n   about  247\n   configuring  261\n   reference  250\n   URL  247\nclassifiers option\n   URL  231\nClient Credentials Grant (CCG)\n   about  171\n   reference  171\nCloudFormation\n   about  279\n   URL  279\nCloudFront\n   about  276\n   URL  277\nCloudWatch panel\n   URL  282\nclustering  267, 268, 269\ncode metrics  164\ncode, securing",
      "content_length": 1323,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "[ 312 ]\n   about  194\n   application scope, limiting  198, 199\n   Bandit linter, using  199, 200, 202\n   incoming data, asserting  194, 195, 196, 197,\n198\nCommand-Line Interface (CLI)  231\nCommon Gateway Interface (CGI)  22\ncomponents, monolithic application\n   Authentication  15\n   Booking UI  14\n   Payments  15\n   PDF reporting service  14\n   Reservations  15\n   Search  14\n   Users  15\nconfiguration  54\nconnection pooling  127\nConnexion\n   URL  116\nConsul\n   URL  268\nContent Delivery Network (CDN)  277\nContinuous Integration (CI)\n   about  88\n   Coveralls  91, 92\n   ReadTheDocs (RTD)  90\n   Travis-CI  89, 90\nconverters  40\nCoreOS\n   deploying  280\n   URL  275\n   with EC2, for deployment  284\nCornice  28\ncoroutines  301\ncoverage tool\n   URL  81\nCoveralls\n   about  91, 92\n   URL  91\nCreate, Read, Update, and Delete (CRUD) tool  57\ncreate_token() function  176\nCross Site Scripting (XSS)  187\nCross-Origin Resource Sharing (CORS)\n   about  215, 217\n   reference  217\nCross-Site Request Forgery (XSRF/CSRF)  187\ncURL  259\ncurl command\n   about  36\n   URL  36\nCython\n   URL  30\nD\nData Service\n   about  113, 114\n   interacting with  218, 219\ndata transfer\n   improving  132\n   improving, with binary payloads  134\n   improving, with GZIP compression  132\ndebugging\n   about  57\n   debug mode  60\nDenial of Service (DoS)  187\ndeployment\n   on EC2, with CoreOS  284\n   with CloudFormation  279\n   with EC2 Container Service (ECS)  279, 288,\n291, 292, 294\ndeveloper documentation  83, 84, 85, 88\nDistributed Denial Of Service (DDoS) attack  169\nDistributed Version Control System (DVCS)  88\nDistutils  225\nDocker Compose\n   about  265, 266\n   URL  265\n   URL for installation  265\nDocker Hub\n   URL  255\nDocker-based deployments\n   about  264, 265\n   clustering  267, 268, 269\n   provisioning  267, 268, 269\n   with Docker Compose  265, 266, 267\ndocker-py\n   about  253\n   URL  253\nDocker\n   about  252, 253",
      "content_length": 1913,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "[ 313 ]\n   container, executing  254, 256\n   Flask, executing  256, 257, 258\n   URL  154, 252\nDockerfile\n   about  254\n   URL  254, 264\nDocument Object Model (DOM)  78, 204\nDomain Specific Language (DSL)  134\nE\nEC2 console\n   URL  283\nEC2 Container Service (ECS)\n   about  279\n   Cluster  288\n   Elastic Load Balancer  288\n   Service  288\n   Task Definition  288\n   URL  275, 279\n   used, for deployment  288, 291, 292, 294\nEC2 instance\n   URL  275\nEC2\n   about  274\n   URL  274\n   with CoreOS, for deployment  284\nElastic Block Stores (EBS)\n   about  276\n   URL  276\nElastic Load Balancing (ELB)\n   about  274\n   URL  274\nElasticCache\n   about  276\n   URL  276\nElasticsearch\n   about  154\n   URL  153\nend-to-end tests  67, 77, 78\nerror handling\n   about  57\n   custom error handler  58\nETag header  129\nEtcd\n   URL  268\nexecution\n   via EC2  274\n   via Lambda  274\nextensions\n   about  51\n   reference  51\nF\nfile descriptor (FD)  247\nFirst-In-First-Out (FIFO)  278\nFlake8\n   URL  81\nFlakon\n   about  62\n   URL  62\nFlask app\n   Session, using  123\nFlask-Login\n   URL  109\nFlask-Principal\n   reference  110\nflask-profiler\n   URL  76\nFlask-Restless\n   about  57\n   URL  57\nFlask-SQLAlchemy\n   about  34\n   URL  99\nflask-webtest package\n   URL  79\nFlask-WTF\n   URL  100\nFlask\n   about  13, 28, 33\n   built-in features  47\n   Circus, configuring  258, 259, 261\n   executing, in Docker  256, 257, 258\n   OpenResty, configuring  258, 259, 260, 261\n   requests, handling  35, 37, 39, 44\n   response, handling  45\n   routing  39\n   with ReactJS  210, 211\nfluentd\n   URL  153",
      "content_length": 1566,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "[ 314 ]\nfunctional tests\n   about  67, 71\n   asynchronous calls, mocking  147\n   synchronous calls, mocking  145\n   writing  145\nG\ngenerators  298, 301\ngetUserIds operation  117\nGevent\n   about  23\n   URL  24\nGilectomy\n   about  29\n   URL  29\nGitHub\n   URL  88\nGitLab\n   URL  88\nGlacier\n   about  276\n   URL  276\nGlobal Interpreter Lock (GIL)  29\nGoogle Cloud Messaging (GCM)  279\nGraphite\n   URL  76\nGraylog Enterprise\n   URL  160\nGraylog Extended Log Format (GELF)\n   URL  157\nGraylog\n   logs, sending  157\n   setting up  154\n   URL  153, 154\nGraypy\n   URL  157\nGreenlet\n   about  23\n   URL  24\nH\nHardware Virtual Machine (HVM)  284\nHMAC-SHA256 (HS256)  174\nHMAC\n   URL  47\nHyperKit\n   about  252\n   URL  252\nI\nintegration tests  67, 73, 74\nInter-Process Communication (IPC)  10\niterators  298, 301\nitsdangerous\n   about  47\n   URL  47\nJ\nJavaScript (JS)  204\nJavaScript authentication  221, 222, 223\nJinja's sandbox\n   URL  195\nJinja\n   about  54\n   URL  52, 54\nJSON Web Key (JWK) format  180\nJSON Web Token (JWT)\n   about  173, 175\n   header  173\n   payload  173\n   signature  173\n   URL  173\nJSON-Schema specification\n   URL  116\nJSX syntax\n   about  206\n   URL  206\nJust-In-Time (JIT) compiler  31\nJWT Claim  174\nK\nKonfig project\n   URL  55\nKubernetes\n   about  269, 275\n   URL  269\nL\nLambda",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "[ 315 ]\n   about  275\n   URL  275\nLAMP (Linux-Apache-MySQL-Perl/PHP/Python) \n11\nLet's Encrypt\n   URL  177\nlibuv  305\nload tests  67, 74, 75, 76\nLocal File Inclusion (LFI)  187\nlocal helper\n   URL  44\nlocust.io\n   URL  77\nlogs\n   centralizing  152\n   extra fields, adding  159\n   Graylog, setting up  154\n   sending, to Graylog  157\nLua Shared Dict  191\nlua-resty-waf\n   URL  193\nLua\n   about  27, 188\n   URL  188\nLuaJIT\n   URL  189\nM\nMANIFEST.in file\n   about  235, 236\n   URL  236\nMarkdown\n   about  84\n   URL  84\nMemcache  17\nmessage broker  103\nMessagePack\n   about  134\n   URL  135\nmessaging\n   with Simple Email Service (SES)  277\n   with Simple Notification Service (SNS)  277\n   with Simple Queue Service (SQS)  277\nmicroservice approach  14, 15\nmicroservice project\n   skeleton  61\n   URL  62\nmicroservices, benefits\n   about  16\n   deployment  17\n   scaling  17\n   separation of concerns  16\n   smaller projects  16\nmicroservices, pitfalls\n   about  18\n   compatibility issues  20\n   data storing  19\n   data, sharing  19\n   illogical splitting  18\n   more network interactions  19\n   testing  20\nmicroservices\n   about  15\n   executing  244, 246\n   implementing, with Python  21\nmiddlewares  51\nmocking  68\nmocks\n   avoiding  68\nModel-View-Controller (MVC)\n   about  204\n   controller  98\n   model  98\n   view  98\nModel-View-Template (MVT)  98\nModSecurity\n   URL  186\nMolotov\n   about  76\n   URL  76\nMongoDB\n   URL  153\nmonolithic approach\n   about  10, 11\n   pros and cons  12\nmonolithic design\n   about  98\n   authentication  107\n   authorization  107\n   background tasks  103, 105",
      "content_length": 1593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "[ 316 ]\n   implementing  111\n   model  98\n   splitting  112, 118\n   Strava token, obtaining  106\n   template  99, 101, 103\n   view  99, 101, 103\nN\nNameko\n   URL  144\nnetwork strategies, Docker\n   reference  265\nnginx content pack\n   URL  166\nnginx\n   about  22\n   function, adding  189\nNode.js\n   about  23\n   URL  210, 212\nNose\n   URL  80\nnpm\n   URL  212\n   using  212, 214\nntpdate service  131\nO\nOAuth2\n   about  96, 169, 170, 171, 172\n   URL  96, 169\nObject-Relational Mapper (ORM)  33\nOpen API 2.0\n   using  115\nOpen Container Initiative (OCI)\n   URL  298\nOpen Source Software (OSS)  13\nOpen Web Application Security Project (OWASP)\n   about  186\n   URL  186\nOpenResty\n   about  188\n   concurrency limiting  191\n   configuring  259, 260, 261\n   features  193\n   Lua  188\n   nginx  188\n   rate limiting  191\n   URL  188, 193\nOpenStack community\n   URL  199\nOperation person (Ops)  21\nout-of-memory killer (oomkiller)  161\nP\npackaging toolchain\n   about  226, 227\n   definitions  227, 228\n   project, distributing  241, 242, 243\n   project, releasing  239, 240, 241\n   Python project, packaging  228\n   versioning  236, 237, 238, 239\nParavirtual (PV)  284\nPaste project  79\nPBKDF2\n   reference  108\nPEP (Python Environment Proposal)\n   URL  44\nperformance metrics\n   about  161\n   code metrics  164\n   system metrics  161\n   web server metrics  166\nPeriodic Task feature\n   reference  106\npika-pool\n   URL  142\nPika\n   URL  141\nPip  61\npip-tools\n   about  234\n   URL  234\nPostgres image\n   URL  267\npreflight mechanism  215\nprocess management  246, 247, 248, 249, 250\nProtocol Buffers (protobuf)\n   about  134\n   URL  134",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "[ 317 ]\nprovisioning  267, 268, 269\npsutil\n   URL  162\nPsycopg  307\nPublish/Subscribe (pubsub) pattern  144\nPull Request (PR)  89\npush-pull tasks queue  138\nPyCharm\n   about  61\n   URL  61\nPygments\n   URL  85\nPyJWT\n   about  175, 176\n   URL  175\nPylons project\n   URL  33\nPyMySQL  196\nPyPA\n   about  227\n   URL  227\nPyPI\n   URL  233, 241\n   URL, for registration  243\nPyPy interpreter\n   URL  31\nPypy Speed Center\n   URL  31\nPyramid  28\npytest package\n   URL  81\npytest-cov  81\npytest-flake8  81\npytest\n   URL  78\n   using  80, 82, 83\nPython application  227\nPython Enhancement Proposals (PEPs)  226\nPython library  227\nPython package  227\nPython Package Index (PyPI)  13, 229\nPython project  227\nPython project, packaging\n   MANIFEST.in file  235, 236\n   requirements.txt file  233, 234, 235\n   setup.py file  228, 229, 230, 231, 232, 233\nPython, for microservice implementation\n   asyncio  26\n   Gevent  23\n   Greenlet  23\n   language, performances  29\n   Twisted  25\n   WSGI standard  22\nPython\n   microservices, implementing  21\n   selecting, for Flask  35\nR\nRabbitMQ broker\n   about  140\n   URL  140\nRabbitMQ\n   URL  50, 103\nrabbitmqadmin  141\nRaven  50\nReactJS\n   about  204\n   Babel, using  212, 214\n   Bower, using  212, 214\n   components  207, 208, 209, 210\n   Cross-Origin Resource Sharing (CORS)  215,\n217\n   dashboard, building  205\n   Flask  210, 211\n   JSX syntax  206\n   npm, using  212, 214\n   reference  210\n   URL  204\nRead-Eval-Print Loop (REPL)  43, 188\nReadTheDocs (RTD)\n   about  90\n   URL  90\nrecommonmark package  84\nRed Hat Enterprise Linux (RHEL)  297\nRedis\n   about  17\n   URL  103\nRegistered Claim Names  174\nRelational Database Service (RDS)\n   about  276",
      "content_length": 1684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "[ 318 ]\n   URL  276\nRemote Code Execution (RCE)  187\nRemote File Inclusion (RFI)  187\nRemote Procedure Call (RPC)  122\nRemote Procedure Calls (RPC)  10\nReportlab  13\nReports Service  113\nrequest_mock library\n   URL  69\nrequests library\n   URL  68\nrequests per second (RPS)  75\nrequests-mock project\n   URL  145\nRequireJS\n   about  214\n   URL  214\nrequirements.txt file\n   about  233, 234, 235\n   URL  233\nrestructured text-lint project\n   URL  230\nreStructuredText (reST)\n   URL  84\nRFC 7517\n   URL  180\nrkt\n   URL  298\nRoute53\n   about  274, 294\n   URL  274, 294\nrouting\n   about  39, 274\n   AutoScaling  274\n   converters  40\n   Elastic Load Balancing (ELB)  274\n   Route53  274\n   url_for function  43\n   variables  40\nRPC\n   using, over AMQP  144\nRSA encryption algorithm  178\nRunnerly repository\n   URL  117\nRunnerly\n   about  96\n   URL  96\n   user stories  96\nRust project\n   URL  89\nS\nSalt\n   URL  269\nSanic\n   about  305\n   URL  305\nSCons\n   URL  226\nSelenium\n   URL  78\nSemantic Versioning (SemVer)\n   about  237\n   URL  237\nSentry\n   about  153\n   URL  50, 153\nServer-Side Template Injection (SSTI)\n   about  194\n   reference  194\nService Level Agreement (SLA)  154, 272\nService Workers\n   about  205\n   URL  205\nService-Oriented Architecture (SOA)\n   origins  9\nsetup.py file  228, 229, 230, 231, 232, 233\nSetuptools  61, 226\nSHA1  48\nSimple Email Service (SES)\n   about  277\n   URL  278\nSimple Notification Service (SNS)\n   about  278\n   URL  278\nSimple Queue Service (SQS)\n   about  278\n   URL  278\nSimple Storage Service (S3)\n   about  276",
      "content_length": 1553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "[ 319 ]\n   URL  276\nSix\n   about  35\n   URL  35\nSOA Manifesto\n   URL  9\nSphinx HtmlDir  90\nSphinx tool\n   URL  83\nSpinnaker\n   URL  8\nSQL Injection  187\nSQLAlchemy (SA)  34\nsrcache-nginx-module\n   about  193\n   URL  193\nStackless project  24\nStatsD\n   URL  76\nstorage\n   CloudFront  277\n   Elastic Block Stores (EBS)  276\n   ElasticCache  276\n   Relational Database Service (RDS)  276\n   Simple Storage Service (S3)  276\nStrava Service  113\nStrava token\n   obtaining  219, 220\nStrava\n   about  96, 112\n   token, obtaining  106\n   URL  96\nstravalib\n   URL  103, 219\nSwagger specification file  239\nSwagger\n   URL  115\nswarm mode\n   about  268\n   URL  268\nswitching  24\nsynchronous calls\n   about  122\n   connection, pooling  127\n   data transfer, improving  132\n   HTTP cache headers  129\n   mocking  145\n   performing  137\n   Session, using in Flask app  123\nsynchronous\n   versus asynchronous  306\nsyslog\n   URL  166\nsystem metrics\n   about  161\n   URL  162\nT\nTask Definitions  280\ntask queues  138\ntemplates  52\nTest-Driven Development (TDD)\n   about  65\n   reference  66\ntests\n   end-to-end tests  67\n   functional tests  67\n   integration tests  67\n   load tests  67\n   unit tests  67\nthreading.local\n   URL  44\nthree-legged OAuth  170\nTime-To-Live (TTL)  175\nTIOBE index\n   URL  21\ntoken dealer  242\ntoken-based authentication\n   about  172\n   JSON Web Token (JWT)  173, 175\n   PyJWT  175, 176\n   TokenDealer microservice  179, 180\n   TokenDealer, using  184, 186\n   X.509 certificate-based authentication  176, 179\nTokenDealer microservice\n   about  179, 180\n   POST/oauth/token, implementation  180, 182,\n183\n   using  184, 185\ntop command  161",
      "content_length": 1651,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "topic queues  139\nTornado\n   about  23, 25\n   URL  25\ntox-travis project  90\nTox\n   URL  82\n   using  80, 82, 83\ntranspilation  206\nTravis-CI\n   about  89, 90\n   reference  90\n   URL  89\nTulip  26, 303\nTwine\n   installing  243\n   URL  243\nTwisted\n   about  23, 25\n   URL  25\nU\nunit tests  67, 68, 71\nurl_for function  43\nUser Interface (UI)  15, 77, 204\nuvloop\n   about  305\n   URL  305\nuWSGI\n   about  247\n   URL  247\nV\nvariables  40\nversion pinning  234\nVirtual Machines (VM)  199\nVirtualenv\n   about  226\n   URL  35\nW\nweb application firewall\n   about  186, 187, 188\n   OpenResty  188, 189\nWeb Application Framework (WAF)  186\nWeb Server Gateway Interface (WSGI)  22, 35\nweb server metrics\n   about  166\nWebOb\n   URL  79\nWebTest\n   URL  72, 79\n   using  79\nWerkzeug WSGI toolkit\n   URL  33\nWSDL\n   URL  115\nWSGIProxy2 library\n   URL  79\nWTForms-Alchemy\n   about  102\n   URL  102\nWTForms\n   about  100\n   URL  100\nX\nX.509 certificate-based authentication  176, 179\nXen hypervisor\n   URL  274\nZ\nzlib\n   about  47\n   URL  133",
      "content_length": 1025,
      "extraction_method": "Direct"
    }
  ]
}