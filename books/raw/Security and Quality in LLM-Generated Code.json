{
  "metadata": {
    "title": "Security and Quality in LLM-Generated Code",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 12,
    "conversion_date": "2025-12-19T17:43:36.098325",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Security and Quality in LLM-Generated Code.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "RELATED WORK\n\nS",
      "start_page": 2,
      "end_page": 5,
      "detection_method": "regex_numeric",
      "content": "cluding remarks and future work in section 6.\n\n2 RELATED WORK\n\nSeveral works explored the use of LLMs in a variety of soft- ware development activities [4], [5], [6], [7], [8], [9], [13], [14], [16], [19], [22], [23], [24], [25], [26]. This section examines key studies, emphasizing their methods, applications, and the analyzed features. The comparative overview helps place our work in the broader research context.\n\nHuang et al. [23] reviewed pre-trained models and LLMs for generative tasks in software engineering, highlighting models like BERT, general transformers, and ChatGPT. They categorized tasks into requirements generation, code generation, test case generation, patch generation, optimiza- tion, summarization, and code translation. Our research expands on this by examining the effectiveness of LLMs, speciﬁcally in code generation, enhancing the understand- ing of their strengths and limitations in this area.\n\nPerry et al. [6] investigated the security of LLM-based codes, ﬁnding that such codes often had more security ﬂaws than human-written codes, with LLMs displaying overcon- ﬁdence in code security. Sandoval et al. [13] observed a 10% increase in vulnerabilities in LLM-based C programming. Asare et al. [4] found that GitHub Copilot could enhance security for complex problems but had minimal impact on simpler tasks. These studies highlight the need for caution when using AI tools. Our research expands by assessing four LLMs across additional programming languages, C++ and Java, for further evaluation.\n\nYetis¸tiren et al. [16] assessed the quality of code produced by three AI code assistants using the HumanEval bench- mark dataset [17], ﬁnding correctness rates of 65.2%, 46.3%, and 31.1% for ChatGPT, GitHub Copilot, and Amazon CodeWhisperer, respectively. Factors like function names, input, and descriptions were evaluated, and SonarQube [27] was used to assess security, maintainability, and reliability. All three tools were found capable of generating secure code. In contrast, our study expands to four languages and incorporates a broader range of dataset scenarios.\n\nAsare et al. [14] compared GitHub Copilot’s code generation to human-written code for security vulnerabil- ities, using a dataset by Fan et al. [15]. They found that Copilot recreated the same vulnerabilities in 33.3% of cases and remedied 25.5% of them. Copilot showed incon- sistencies, especially with older vulnerabilities, but gener- ally produced fewer security ﬂaws than humans. Khoury et al. [7] tested ChatGPT’s ability to generate secure code in various languages, showing that it often failed to meet security standards but improved with follow-up prompts. ChatGPT corrected 12 of the 21 programs when prompted. Nair et al. [9] explored ChatGPT’s effectiveness in generating hardware code using Common Vulnerability Enumerations (CWE-1194), demonstrating the possibility of guiding AI to avoid common security ﬂaws. Elgedawy et al. [5] analyzed GPT-3.5, GPT-4, Bard, and Gemini, show- ing that using security personas reduced vulnerabilities, especially in GPT-3.5, GPT-4, and Bard. Siddiq et al. [8] introduced the SALLMS framework to evaluate LLMs systematically for security. Schuster et al. [22] and Wu et al. [19] highlighted challenges, such as data poisoning and\n\nLLMs’ reduced effectiveness in handling complex security issues, emphasizing the need for stronger defenses and better vulnerability detection.\n\nTable 1 summarizes related work based on the program- ming languages used in LLM-generated code, the LLM(s) employed, and a summary of the prompt descriptions. Table 2 outlines related work by the quality characteristics addressed and the security analysis methods used to evalu- ate the generated code. Additionally, the table provides the overall impact of AI code generation on code quality based on the respective study’s experiments and results. Our Work. Most existing research focuses on exploring LLM code generation behavior but lacks in-depth analysis of factors affecting the quality and security of the generated code, beyond user demographics and seniority level. In contrast, our work advances the literature by examining the relationship between programming language features and the quality of LLM-generated code. We evaluate the validity, correctness, security, maintainability, consistency, intentionality, adaptability, and responsibility of the code across four programming languages and ﬁve LLMs.\n\n3 METHODOLOGY\n\n3.1 LLMs Selection\n\nThe choice of LLMs is based on several metrics, such as popularity, user base, reputation, support of diverse programming languages, and performance (accuracy, efﬁ- ciency) in the code generation process. These metrics are derived from academic literature and industry benchmarks, ensuring that our study is representative. The LLMs selected in this study, Table 3, cover a wide range, each with distinct strengths in terms of efﬁciency in the code generation pro- cess, language support, and overall accuracy. A signiﬁcant factor in our selection is the diversity of their underlying LLM architecture, whereas they vary in the context of parameter size, training datasets, and decoding strategies. These differences might directly inﬂuence the security and quality of the generated code, making a comparative analy- sis of its output important to understand the strengths and weaknesses of each of these models.\n\nAs is known, the internal architecture and training methodologies of these LLMs, i.e., can introduce signiﬁcant variation in the code that is produced. This study aims to provide information on how these variations inﬂuence the security posture of generated code, focusing on the importance of selecting the right LLM based on speciﬁc development needs. Table 4 highlights the context window and other conﬁgurations used when generating code using each model. Max tokens refers to the maximum number of tokens to generate in completion. Top p changes how the model selects tokens for output. Tokens are selected from the most probable to least until the sum of their probabilities equals the top-p value. The model tempera- ture is used to control the randomness in generating the output. The context window is the maximum token count a model can handle in one forward pass, covering both the input (prompt) and the output. It essentially determines the amount of text the model can process at a time.\n\n2\n\nTable 1: A summary of the related work. Highlighted the evaluated programming languages and the LLMs. Languages: ① C, ② C++, ③ Java, ④ Python, ⑤ JavaScript, ⑥ HTML, ⑦ Verilog. LLMs: ⑧ Copilot, ⑨ Codex, ⑩ Whisper, ❶ Gemini, ❷ Bard, ❸ GPTs, ❹ Llama-3, ❺ Claude-3.5, ❻ Codestral, ❼ StarCoder, ❽ CodeGen.\n\nReference\n\nAsare et al. [4] Perry et al. [6] Sandoval et al. [13] Asare et al. [14] Yetistiren et al. [16] Khoury et al. [7] Nair et al. [9] Elgedawy et al. [5] Wu et al. [19] Siddiq et al. [8] Schuster et al. [22] This work\n\nProgramming Languages Year ① ② ③ ④ ⑤ ⑥ ⑦ ✗ ✗ ✗ ✗ 2024 ✔ ✗ ✗ ✗ ✔ ✔ ✗ 2023 ✔ ✗ ✗ ✗ 2023 ✔ ✗ ✗ ✗ ✗ 2023 ✔ ✔ ✗ ✗ ✗ ✗ 2023 2023 ✔ ✔ ✔ ✔ ✗ ✔ ✗ ✗ ✔ ✗ ✗ ✗ ✗ 2023 ✗ ✗ ✗ ✗ ✔ ✗ 2023 ✗ ✗ ✗ ✔ ✔ ✗ 2023 ✗ ✗ ✗ ✔ ✗ ✗ 2023 ✗ ✗ ✗ ✔ ✗ ✗ 2021 ✗ ✗ 2024 ✔ ✔ ✔ ✔ ✗\n\n✗\n\n✗ ✗ ✗ ✗ ✗ ✔ ✗\n\n✗\n\n✗ ✗ ✗ ✗ ✗\n\nLarge Language Models\n\nPrompt Scenario\n\n⑧ ⑨ ⑩ ❶ ❷ ❸ ❹ ❺ ❻ ❼ ❽ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✔ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✔ ✔ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✔ ✗ ✔ ✔ ✔ ✔ ✗ ✗ ✗\n\n✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗\n\n✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✔ LLMSecEval dataset [21] ✗\n\n✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗\n\nTwo problems [11] Six tasks [12] Shopping list function Big-Vul dataset [15] HumanEval dataset [17] 21 tasks [18] Scenarios from the selected CWEs Nine tasks SARD and Juliet datasets [20]\n\n✗ ✗ ✗\n\n✗ ✗\n\n200 tasks\n\nTable 2: A summary of the related work, highlighting the features and methods used to evaluate the quality of gener- ated code. Overall Impact (OI): Negative (N), Positive (P), and Negative impact due to the use of an inappropriate dataset for testing code security (N*). Attributes: Validity, Correctness, Security, Reliability, and Maintainability. Meth- ods: Manual, Static Scan, and Runtime Scan.\n\nTable 4: LLMs, (MaxT), context window (CW), and Top P (TopP).\n\ntemperature (Temp), maximum tokens\n\nModel GPT-4o llama-3 claude-3.5 codestral gemini-1.5\n\nTemp MaxT 4,096 4,096 4,096 4,096 4,096\n\n0.9 0.9 0.9 0.9 0.9\n\nTopP 0.9 0.9 0.9 0.9 0.9\n\nCW 128k 32k 200k 32k 128k\n\nQuality Attributes Re Ma C ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✔ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✔\n\nAnalysis Method\n\nReference\n\nSe ✔ Asare et al. [4] ✔ Perry et al. [6] ✔ Sandoval et al. [13] ✔ Asare et al. [14] Yetistiren et al. [16] ✔ ✔ ✔ ✔ ✗ ✗ Khoury et al. [7] ✗ ✔ ✗ Nair et al. [9] ✔ ✔ ✔ Elgedawy et al. [5] ✔ ✗ ✗ Wu et al. [19] ✔ ✗ ✗ Siddiq et al. [8] ✗ ✔ ✗ Schuster et al. [22] ✔ ✔ ✔ This work Table 3: LLMs used for evaluation and their short names.\n\nV ✗ ✗ ✗ ✗\n\nPe M S ✔ ✗ ✗ ✔ ✗ ✗ ✔ ✔ ✗ ✔ ✔ ✗ ✔ ✔ ✗ ✔ ✗ ✗ ✔ ✗ ✗ ✔ ✔ ✔ ✔ ✗ ✗ ✗ ✗ ✔ ✔ ✗ ✗ ✔ ✔ ✗\n\nR ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗\n\nProvider OpenAI Perplexity CLAUDE Mistral Google\n\nRef Model [28] GPT-4o [29] [30] [31] [32]\n\nllama-3-sonar-large-32k-chat claude-3-5-sonnet-20240620 codestral-2405 gemini-1.5-pro-001\n\nShort GPT-4o llama-3 claude-3.5 codestral gemini-1.5\n\n3.2 Programming Languages Selection\n\nOne of the key motivations and contributions of this study is the comparative exploration of the performance and security of LLMs under the same evaluation settings for different programming languages. We determine a range of programming languages to be evaluated in our evaluation, covering both statically and dynamically typed languages. As such, we choose the following programming languages as a preliminary set that meets those metrics: ① C; ② C++; ③ Java; ④ and Python. Although our choice of programming language is limited by the capabilities of LLMs and the programming languages they support, we believe that these programming languages are representative, so they are among the top ﬁve most used programming languages [33]. Each programming language possesses distinct at- tributes that inﬂuence security outcomes in code genera- tion (i.e., Python’s dynamic typing versus C++’s static typ- ing can introduce different vulnerabilities and bugs). Java and Python beneﬁt from automatic memory management, reducing memory-related errors compared to the manual memory management required in C and C++. Thus, the selected languages provide a comprehensive view of how LLMs address these differences. Additionally, this selection ensures that the study’s ﬁndings are applicable to a wider range of real-world software development scenarios, as\n\nOI\n\nN N N P N* N P N N N N N\n\nthese languages rank among the top ﬁve most used in 2024, making the research highly relevant to many users.\n\n3.3 Dataset\n\nTo thoroughly evaluate the performance of LLMs, we cu- rated a set of 200 prompt descriptions aimed at testing multiple code generation facets. The selection of tasks was undertaken to ensure comprehensive coverage of key programming paradigms and secure coding practices. The dataset is detailed in section 4.\n\n3.4 Environment Setup\n\nTo ensure consistency and reproducibility, we created a stan- dardized environment for generating, compiling, executing, and validating code. Designed for diverse programming languages and tasks, it supports multiple languages and unit test execution. All experiments were conducted on a Lenovo ThinkPad E570 with a 7th-gen Intel® Core™ i7, 16 GB DDR4 RAM, and a 256 GB SSD. This hardware was chosen for its availability, efﬁciency, and portability, making it ideal for language model integration, code generation, and multi-language compilation. Debian 12 was selected for its stability, efﬁcient package management, and minimal resource usage, providing a reliable platform for cross- language development and testing. The following software packages and versions were used: ❶ Java. To handle Java code compilation and execution, we used the long-term support (LTS) Java version (OpenJDK version 17.0.8). This version supports the latest features of the Java language, ensuring compatibility with mod- ern programming constructs and practices generated by the LLMs.\n\n❷ Python. For Python code, we utilized Python version 3.11.9. This version was chosen for its compatibility with the latest Python libraries and features, ensuring that the generated Python code was evaluated in an up-to-date runtime environment.\n\n❸ C and C++. The compilation of the C and C++ codes was carried out using CMake version 3.28.6. For C++ specif- ically, we conﬁgured the CMAKE CXX STANDARD to\n\n3\n\nversion 17, which gained popularity after its release in 2017 [34], ensuring that all C++ code generated by the models adhered to the C++17 standard. This choice was made to support modern C++ features, such as struc- tured bindings and inline variables, which are common in LLM-generated code. This setup offered a stable basis for assessing the code produced by the ﬁve chosen LLMs. By preserving identical hardware and software settings, we ensured that any dif- ferences in code compilation, run-time, or accuracy between programming languages or models are due to the models themselves and not to environmental factors.\n\n3.5 LLM Integration and Code Generation\n\nBased on the LLMs selected in Table 3, each model was used to generate responses in the four selected programming lan- guages. To streamline the code generation process, the LLMs were interfaced with a custom written Python program that methodically dispatched task prompts in sequential order to each model and collected the resultant code output. Upon code generation, another custom-written Python program was used to systematically parse and arrange the results within a structured ﬁle system, assigning appropriate ex- tensions pertinent to the programming language (i.e., .py for Python, .java for Java). Each ﬁle was named according to the task prompt identiﬁer, the LLM model used, and the language, facilitating traceability and ease of comparison.\n\n4,000 code ﬁles were generated, comprising 200 ﬁles per language for each LLM. This dataset formed the foundation for the subsequent evaluation procedures, ensuring that each LLM was evaluated using an identical set of tasks and programming languages.\n\n3.6 Quality Evaluation\n\nTo evaluate the quality of the generated code, analysis is performed based on several quality metrics, including syntax validity, functional correctness, code lines, reliability, maintainability, and security [8], [16]. We use two types of evaluation methods: ① Manual evaluation by a human expert, where two developers participated in the semantic evaluation and wrote the unit testing ﬁles; ② Automatic static secure code scanning using the SonarQube [27], a static code scanning tool. Although human evaluation met- rics may not scale, they are still considered the golden stan- dard for evaluating the output of LLMs and NLP tasks. Us- ing automatic analysis tools, we will use human metrics to evaluate a small-scale set of LLM examples and generalize beyond the small sample of human evaluators. A review of recent literature that identiﬁes these evaluation methods as an important approach in code quality evaluation supports the selection of the metrics and tools mentioned above.\n\n3.6.1 Manual Method\n\nTwo developers, with two and ten years of software devel- opment experience, are hired to participate in subsequent evaluations. In addition, the ﬁrst and second authors partic- ipated in the process of progress coordination and reviews. Compilation-Time Errors. Before evaluating functionality and quality, we ﬁrst ensured the code met syntactic stan- dards and compiled without errors. This step was crucial for valid and reliable assessments.\n\nThe automated scripts were designed and written for each programming language to perform syntactic checks. For compiled languages such as C, C++, and Java, each source ﬁle was compiled, and the results of these compila- tion attempts were recorded. For interpreted languages such as Python, syntax validation was performed using Python’s native syntax-checking functionality.\n\nThe results were documented in a comprehensive matrix with 200 rows (representing the prompts) and 20 columns (corresponding to 5 LLMs and 4 languages). Each cell in the matrix was annotated with a binary value indicating syntactical validity and successful code compilation. This methodology facilitated the identiﬁcation of potential is- sues, such as syntax errors or missing imports, enabling the resolution of missing library imports or the exclusion of problematic code before proceeding to semantic analysis.\n\nSemantic and Functional Correctness. We evaluated the se- mantics of the generated code after verifying and reviewing all syntactic errors. This phase involved assessing whether the code produced by each LLM accurately implemented the logic and functionality speciﬁed in the task prompts.\n\nInitially, our goal was to develop a single unit test ﬁle per language for each task prompt that could be uni- versally applied to all LLM-generated solutions. However, during testing, we observed variations in the function signa- tures produced by different LLMs. Differences in parameter types, return values, function names, and parameter order made it impractical to use a single unit test ﬁle for all code generated by the selected LLMs for the same task.\n\nTo address this challenge, we created custom unit test ﬁles for each individual code ﬁle. Each test ﬁle contained ten unit test cases speciﬁcally designed to evaluate the correctness of that particular LLM output. For example, if an LLM-generated function had a unique signature—such as one function using an array data type as input and another using a vector data type–the unit test cases were adapted accordingly to match the expected input/output format.\n\nTogether, 4,000 different unit test ﬁles are required, 200 test ﬁles for each programming language per LLM. The results of these unit tests were documented using another matrix with the same matrix dimensions referenced in compilation-time error validation. Each unit test ﬁle re- ceives a score ranging from 0% (all tests failed) to 100% (all tests passed), depending on how many test cases were successfully executed. This process is implemented by three developers with one round of review cycle for the writ- ten unit testing ﬁles. Through this process, we were able to measure the semantic accuracy of the generated code, providing a comprehensive insight into how effectively the LLMs comprehended the prompt speciﬁcations.\n\nIn summary, the evaluation faced key challenges, in- cluding inconsistencies in function signatures across LLMs, which hindered standardized unit testing. We addressed this by creating individualized unit test ﬁles, increasing complexity but ensuring accuracy. Additionally, variations in code quality and completeness required manual adjust- ments, such as adding missing imports, to enable proper compilation and testing.\n\n4\n\n3.6.2 Tool-Based Method\n\nThere are three evaluation metrics generated using the static code scanning tool. Static Features. These features for codes generated by the LLMs are collected using SonarQube tool [27] which sup- ports all selected languages in this study. These features are: ① Lines of Code (LoC) measures the program’s lines of code, excluding whitespace. LoC is a predictive metric used to evaluate effort and maintainability.\n\n② Cyclomatic Complexity (CyC) calculates code complexity using a control ﬂow graph (CFG). With E as edges, N as nodes, and Q as connected components, CyC is computed as M = E + 2Q − N [35].\n\n③ Cyclomatic Complexity Density (CCD) measures how cy- clomatic complexity spreads across the codebase. With cl as the total code lines, CCD is calculated as Md = (E + 2Q − N)/cl [36].\n\n④ Cognitive Complexity (CoC) measures the difﬁculty of un- derstanding code [37]. CoC considers structure like con- n i=1 nc, where trol ﬂow and nesting, using C = Cbase + P nc represents increases due to nesting and conditionals.\n\nSoftware Quality Attributes. For our analysis, we assess the software quality of the generated code using Sonar- Qube [27], a well-known tool for code quality inspection. Evaluate the source code on the basis of multiple quality metrics. Our evaluation focuses on four software quality indicators: ① reliability, ② security, ③ maintainability, ④ security hotspots. In total, we examined 97,412 lines of code generated in four languages by ﬁve different LLMs. ① Reliability measures how well the code operates under predeﬁned conditions. The tool identiﬁes bugs that may cause errors or unpredictable behavior. By ﬁnding and ﬁxing these bugs, we ensure adherence to best practices and mitigate potential run-time issues. We analyzed bug density in the produced code to conﬁrm high reliability, helping to prevent execution problems.\n\n② Security ensures the code is free from vulnerabilities that malicious actors could exploit [38], [39]. The tool highlights security vulnerabilities, such as improper in- put handling or weak encryption, which may lead to breaches. We evaluated the number and severity of these vulnerabilities to ensure that the generated code meets modern security standards, protecting sensitive data and preventing unauthorized access.\n\n③ Maintainability refers to the ease of understanding, mod- ifying, and extending the code over time. The tool de- tects code smells, indicating inefﬁcient design choices that may hinder future development. We assessed the maintainability score by considering factors like code complexity and adherence to coding standards, ensuring the code remains ﬂexible and manageable.\n\n④ Security Hotspots are areas of code that may not be vulner- abilities but are sensitive and could lead to security issues if mismanaged. These often involve security-critical func- tions such as authentication and data validation. The tool ﬂags these areas for developer review, ensuring proper handling. We included an analysis of security hotspots to prevent the generated code from unintentionally in- troducing risks in critical sections.\n\nClean Code Attributes. When evaluating the attributes of clean code, we consider four primary dimensions: ① con- sistency, ② intentionality, ③ adaptability, ④ accountability. These characteristics offer a structure for evaluating the quality, readability, and maintainability of code produced from clean code practices and standards. ① Consistency assesses the formatting, naming conventions, and structural design of the code. It ensures that the generated code follows a standardized framework across different languages and prompts. This includes uni- formity in spacing, indentation, and identiﬁer casing. Maintaining consistency improves readability, facilitates collaboration, and reduces cognitive load during code review and maintenance.\n\n② Intentionality evaluates the clarity and effectiveness of the code in fulﬁlling its purpose. It examines whether the generated code is logical, complete, and efﬁcient, ensur- ing that it conveys its functionality without unnecessary complexity while maintaining coherent logic.\n\n③ Adaptability focuses on the ease in modifying the code. It measures whether the code is modular and structured to allow localized updates with minimal risk of introducing errors. Well-adaptable code maintains a clear separation of concerns, ensuring that each function or component serves a deﬁned purpose, reducing overall complexity. ④ Responsibility ensures adherence to ethical and profes- sional standards. It evaluates whether the generated code complies with legal and licensing requirements, safeguards sensitive information, and uses inclusive lan- guage. This attribute helps maintain trust, professional- ism, and ethical integrity in AI-generated code.\n\nAnalysis. We analyze the evaluation results, concentrating on essential quality attributes such as validity, accuracy, se- curity, reliability, maintainability and clean code attributes. A detailed analysis of the results is available in section 5.\n\n4 DATASET\n\nThe dataset created to evaluate the proposed concept was meticulously crafted to effectively assess the code produced by the LLMs, such as those discussed above. 200 program- ming tasks were manually deﬁned and classiﬁed to ensure comprehensive coverage of a wide range of programming concepts. The tasks were divided into different categories: ① problem-solving, general coding challenges that involve solving algorithmic problems, aimed at testing the logical and problem-solving capabilities of LLMs; ②algorithms, tasks that focus on implementing fundamental algorithms, such as sorting, searching, dynamic programming, and graph-based algorithms; ③ data structures, tasks designed to assess the correct usage, manipulation, and implemen- tation of data structures such as arrays, linked lists, trees, graphs, and hashmaps; ④ secure coding, security-related prompts, carefully chosen based on the selected CWEs from MITRE Common Weakness Enumeration (CWE) beyond CWE Top 25, aimed at testing the LLM’s ability to gener- ate code free from common vulnerabilities such as buffer overﬂows, and injection ﬂaws; ⑤ concurrency and multi- threading, tasks aimed at testing how well the generated code handles parallelism, thread synchronization, and race\n\n5",
      "page_number": 2
    },
    {
      "number": 2,
      "title": "RESULTS AND DISCUSSION\n\nW",
      "start_page": 6,
      "end_page": 12,
      "detection_method": "regex_numeric",
      "content": "conditions; ⑥ programming and system, problems requir- ing the manipulation of ﬁle handling, database operations, networking, and error handling.\n\nThe prompts were chosen carefully and designed to ensure their solvability in various programming languages, some of these were derived from code challenge websites and dataset [40], [41], [42], [43]. This approach facilitated the assessment of LLM performance in Python, Java, C++, and C, while preserving uniformity in problem complexity and intent across all languages and LLMs. Features. The dataset consists of several attributes to guide the evaluation, including: ① task number, which refers to unique identiﬁer assigned to each question; ② prompt title, which refers to brief title that summarizes the problem statement; ③ description, a detailed description of the task, outlining the problem to be solved; ④ hints, which refers to instructions that provided to AI model to guide the gener- ation of the code; ⑤ solutions, represent the code generated by the LLM for each programming language along with the name of the model used; ⑥ source, which refers to the origin of the task, whether manually created or adapted; ⑦ test cases, which refers to 10 test cases per prompt were written to evaluate the semantic of the generated code; ⑧ tags, which refers to labels assigned to each task to facilitate the process of ﬁltrating, categorization, and statistical analysis; ⑨ comments, which refers to notes made by reviewers to document the evaluation process, including issues encoun- tered during the generation process. Purpose and Scope. The dataset provides a comprehensive resource for evaluating LLM-generated code across multiple languages. It consists of 200 unique programming chal- lenges, each solved by ﬁve LLMs in four languages, yielding 4,000 records. Each record represents a solution generated by a speciﬁc AI tool for a given task in a particular language, enabling detailed performance analysis. Creation Guidelines. Creating the dataset followed a strict guideline to ensure that the generated data set refers to a high-quality dataset. The main guidelines are as follows: ✧ Task Design. Each programming task is created to pro- vide a clear statement and make sure that the tasks created cover different topics, such as algorithms, data structure, and CWE-based questions. ✧ Task Distribution. The generated tasks were categorized into different topics to facilitate the evaluation of the AI- generated code in different problem domains. ✧ AI Tools Comparison. Different solutions were pro- posed by different LLMs (claude-3.5, gemini-1.5, codestral, GPT-4o, llama-3) in four programming lan- guages (Python, Java, C++, and C). ✧ Partial Solutions. Each of the generated codes was evaluated; if LLMs fail to generate a complete solution for a speciﬁc problem or provide only partial code, this was documented. In this case, a partial score is given based on the number of passed test cases.\n\n5 RESULTS AND DISCUSSION\n\nWe analyze the performance of different LLMs with respect to code generation, including correctness, security, and reli- ability. The evaluation process depends on different metrics, i.e., compilation-time errors, security, and overall accuracy\n\nTable 5: Breakdown (%) of LLM-generated code ﬁles with- out compilation-time errors (error free) and semantic issues.\n\nModel\n\nclaude-3.5 gemini-1.5 codestral GPT-4o llama-3\n\nCompilation-time error-free Java C 96.0 95.0 90.5 88.5 91.5 88.5 91.5 94.0 88.0 88.0\n\nPython 99.5 100.0 100.0 100.0 100.0\n\nC++ 81.5 77.5 80.0 89.0 77.0\n\nJava 95.0 94.2 85.9 92.2 86.2\n\nSemantic error-free\n\nPython 96.7 97.3 94.4 96.4 91.6\n\nC++ 92.0 88.2 94.1 91.4 85.5\n\nC 88.7 83.9 89.0 87.7 83.1\n\nacross different languages. Each LLM’s output is compared to deﬁne the best generation tool.\n\n5.1 Compilation-Time Errors\n\nThe ﬁrst part of Table 5 presents insights into the perfor- mance of different LLMs in generating compilable code without errors in four programming languages (Java, Python, C++, and C). To calculate the compilation success rate, let P be the programming language, C be the number of compilable ﬁles in P, T be the total number of ﬁles from in P, and SP be the compilation success rate (in percentage). n i=1 Ci, where Ci is 1 if ﬁle i is com- We then deﬁne C = P pileable successfully and 0 otherwise. Moreover, we deﬁne T = n, where n is the total number of ﬁles. We also deﬁne the compilation success rate in P as SP = (CP/TP) × 100. The ﬁndings reveal that all models achieve high per- centages of compilable code in Python, using three LLMs (gemini-1.5, codestral, and GPT-4o), achieving a suc- cess rate 100%. This suggests that Python’s simpler syntax and dynamic nature make it easier for the AI tool to generate accurate and error-free code. Takeaway. Python had the highest success rates for com- pilable code across all models. This suggests that Python’s simpler syntax and dynamic nature contribute to the accu- racy and reliability of code generation by AI models.\n\nThe variability in the compilation-time error-free rates observed in Table 5 for Java, C++, and C suggests that LLMs exhibit different strengths depending on the lan- guage. For Java, performance varies more signiﬁcantly compared to Python, with success rates ranging between 88.00% and 95.00%. Some models, such as claude-3.5 and GPT-4o, outperform others, while llama-3, gemini-1.5, and codestral show slightly lower success rates at 88%, 88.5%, and 88.5%, respectively.\n\nThese results can be attributed to Java’s verbose, stati- cally typed nature, which reduces compilation-time errors when an LLM correctly understands its syntax and princi- ples. The slight variations likely stem from differences in how well each LLM handles Java’s syntax rules, exceptions, and object-oriented features (i.e., encapsulation). Models like claude-3.5 and GPT-4o may be better ﬁne-tuned for Java-speciﬁc dependencies and required imports, as missing import statements—such as java.utils.*—are a common issue affecting compilation success.\n\nTakeaway. Models like claude-3.5 and GPT-4o perform better in Java, due to better handling of Java’s syntax rules and package dependencies, while models like llama-3, gemini-1.5, and codestral show lower success rates. This highlights the importance of ﬁne-tuning for Java- speciﬁc aspects to improve LLM performance.\n\nC++ shows the lowest success rates across all models, with scores ranging from 77.00% to 89.00%. The worst\n\n6\n\nerror-free score in C++ is with llama-3 and gemini-1.5. The performance of LLMs in generating C++ code appears to be constrained by missing include statements, incor- rect type handling, and misinterpretation of APIs. C++ is a complex language with many different standards (i.e., C++11, C++17), and LLMs struggle when dealing with the complexities of the language’s syntax, library usage, and type system, leading to frequent compile-time errors when generating more advanced code involving libraries like STL (Standard Template Library) or 3rd party APIs/libraries.\n\nTakeaway. LLMs suffer with C++ with missing include statements, incorrect type handling, different standards (i.e., C++11, C++17), and misinterpretation of APIs/func- tions. C++ syntax complexity, library usage, and type sys- tem contribute to frequent compile-time errors with STL and 3rd party APIs/libraries.\n\nThe varying success rates in C code generation are due to their ability in handling C programming standards and dependencies. The high success rate of claude-3.5 (96%) suggests that it effectively complied with C stan- dards and included the necessary headers, avoiding com- mon problems (i.e., undeclared types and missing libraries). Meanwhile, llama-3 with the lowest success rates (88. 00%), encountered frequent errors related to unknown types (i.e., bool), missing headers (i.e., cgi/cgi.h), and incorrect function arguments. These issues point to a reliance on non-standard libraries or incorrect assumptions about the development environment. These inconsistencies highlight the need to ensure that the generated code adheres to stan- dard C practices, incorporates all required dependencies, and conforms to suitable function signatures to enhance the probability of successful compilation.\n\nTakeaway. The frequent errors in C are related to unknown types (i.e., bool), missing headers (i.e., cgi/cgi.h), and incorrect function arguments, are due to reliance on non- standard libraries. Ensuring generated code aligns with standard C practices and includes all dependencies with its source is crucial for successful compilation.\n\nThe percentage of code ﬁles without compilation er- rors provides insight into how well these AI tools are tuned for different programming paradigms and syntax complexities. Regarding claude-3.5, it performs well in Java with 95% as a result, and in C with 96%, showing its good performance with both object-oriented and func- tional programming languages. Less performance with the C++ programming language with 81.5%. GPT-4o achieved strong results in all program languages, especially C++ (89%), showing that it is reliable and adaptable for gener- ating error-free code. Regarding gemini-1.5, this model achieved the highest performance in Python and good in C, but this model achieved the lowest performance as re- sults regarding Java and C++ compared to claude-3.5 and GPT-4o. Lastly, regarding llama-3, performs lower in most languages except Python. It struggles with Java (88%), C++ (77%), and C (88%), showing that it is not as well tuned for more complex languages. The reason behind the results shown for each of the AI tools refers to a set of factors such as training data, since each model trained on large, more diverse, and higher-quality datasets is better at understanding various programming languages\n\n[44]. Another important factor regarding the performance of the AI tool refers to model size and complexity, since a larger model with more parameters such as GPT-4o can better understand and generate.\n\nTypes of compilation-time errors. A breakdown of the compilation-time errors is summarized in Table 6. Python’s breakdown is excluded from the table since it lists only error types occurring more than once across all LLMs. In the following, compilation errors are grouped into categories to understand the common failure modes across the models.\n\n① Library Errors The most common error category across all models was related to missing imports or incorrect package references. Errors such as ”cannot ﬁnd symbol” and failure to recognize or include required third-party libraries, speciﬁcly missing import for java.utils.*, par- ticularly for tasks that involved non-standard libraries, occurred frequently. This indicates a need for improved handling of Java libraries and packages, possibly by training LLMs on more diverse codebases with proper import statements. In addition, LLMs may beneﬁt from an enhanced contextual understanding of external de- pendencies in programming tasks.\n\n② Exception Handling Missing or incorrect exception han- dling was another area where the models faltered. gemini-1.5, codestral, and llama-3 were partic- ularly prone to this type of error, indicating that these models could improve by focusing on the need to handle exceptions based on the exception that can be triggered by code in scope for a particular code block or method. ③ Missing Class/Member An often recurrent issue ﬂagged by codestral is the absence of a class or class member, such as setter and/or getter methods. This error arises when the model presumes the presence of a speciﬁc method or class without including the pertinent code or even suggesting it within comments.\n\n④ Syntax Syntax errors, including incorrect symbols, miss- ing semicolons, or invalid syntax, were found primarily in codestral. This may suggest a need for ﬁne-tuning on Java syntax conventions or a deeper understanding of language-speciﬁc rules.\n\n⑤ Type Compatibility Errors related to incompatible types, such as attempts to assign a java.lang.Object to a java.lang.String, or java.io.IOException cannot be con- verted to java.lang.SecurityException. These errors indi- cate the models’ ability to infer the data types in context. ⑥ Variable Identiﬁer Errors arising from the use of undeﬁned variables were found in multiple models. This indicates that the models occasionally fail to maintain variable state consistency across different sections of the code. ⑦ Undeclared Var./Fun. Errors where a variable or function is used before being declared, leading to a failure in recognizing identiﬁers. This may happen if a necessary declaration or deﬁnition is missing.\n\n⑧ Binding/Qualiﬁer Errors related to failing to bind a vari- able correctly or mismatches with qualiﬁers. These errors often occur in function parameters or references.\n\n⑨ Constant Undeclared Errors with undeclared constants like\n\ntrue and nullptr.\n\n7\n\n8 Table 6: Breakdown of LLM-generated code ﬁles compilation-time error types. LLMs: ❶ claude-3.5, ❷ gemini-1.5, ❸ codestral, ❹ GPT-4o, ❺ llama-3.\n\nError Type\n\nLibrary Errors Exception Handling Class/Member Syntax Type Compatibility Variable def. related\n\n❶ 10 0 0 0 0 0\n\n❷ 17 3 0 1 1 1\n\nJava ❸ 15 2 4 1 1 1\n\n❹ 8 0 0 2 0 1\n\n❺ 17 4 1 0 2 1\n\nError Type\n\nLibrary Errors Binding/Qualifier Class/Member Syntax Variable def. related Undeclared Var./Fun.\n\n❶ 20 0 3 4 6 2\n\n❷ 8 1 2 2 15 13\n\nC++ ❸ 4 0 9 6 5 11\n\n❹ 11 0 3 1 4 2\n\n❺ 22 2 0 6 4 6\n\nError Type\n\nLibrary Error Constant Undeclared Function Argument Undeclared Var./Fun. Incomplete Code Other\n\n❶ 7 0 2 1 1 2\n\n❷ 14 1 0 1 1 3\n\nC ❸ ❹ 3 3 0 2 0 0 2 4 1 1 1 7\n\n5.2 Semantic\n\nThe results shown in Table 5 under the semantic error- free part present the percentage of valid semantic code generated. The observed variance in success rates within the same LLM, with most models differing by approximately 8.50% in their ability to generate correct code in different languages, except gemini-1.5 which shows a range of 13.50%, highlights considerable variations in the way these models process the same coding task depending on the target programming language. Despite the same reasoning capabilities required, this variation suggests discrepancies in how each model interprets and implements program- ming logic, potentially due to differences in training data, model architecture, or optimization strategies. The wider range of gemini-1.5 may indicate particular challenges in understanding or generating code for certain languages or complex programming constructs, reﬂecting either gaps in its training data or special consideration in its design.\n\nTakeaway. The variance in success rates among different LLMs, with a range of 13.50% for gemini-1.5, indicates notable differences in how these models handle coding tasks, likely due to variations in training data, model design, and optimization strategies.\n\nFor Java, the models generally showed strong perfor- mance, with claude-3.5 achieving the highest success rate of 95.00%. This indicates a good proﬁciency in creating effective and precise Java code, probably because of Java’s object-oriented design and consistent libraries, which ap- parently correspond well with the models’ training data. gemini-1.5 also performed remarkably well, reaching a success rate of 94.22%, demonstrating its proﬁciency in man- aging Java syntax and standard programming constructs. codestral and GPT-4o showed marginally lower success rates at 85.87% and 92.22%, respectively. The variation in performance, especially for codestral, could indicate dif- ﬁculties in addressing Java’s more complex class structures or standard library usage, which aligns with the low success rate for the code compilation success rate as well. Conse- quently, the high success rates from all models indicate that these models are proﬁcient in producing Java code, while also emphasizing the necessity for ongoing improvements to tackle the more complex elements of the language and reasoning capabilities.\n\nTakeaway. LLMs perform well in the generation of Java code, with claude-3.5 leading at 95.00%, probably be- cause of Java’s object-oriented design and consistent li- braries. gemini-1.5 also excels, while codestral and GPT-4o show some challenges with complex structures. This highlights strong proﬁciency but also points to a need for improvement in handling Java’s complexities.\n\nAmong the four languages evaluated, Python achieved the highest success rates, with gemini-1.5 at the top\n\nwith a success rate of 97.34%, followed by claude-3.5 and GPT-4o with a success rate 96.72% and 96.35% re- spectively. Python’s simplicity and dynamic typing con- tribute to these high success rates, as its syntax is gen- erally less strict and more ﬂexible compared to statically typed languages. The high performance across all models in Python suggests they are proﬁcient on the language and its constructs. Furthermore, llama-3, with a lower success rate of 91.56%, indicates that even models with generally high success rates can face challenges in generating correct Python code, potentially due to variations in handling lan- guage features and reasoning capabilities. codestral and GPT-4o demonstrated signiﬁcant performance in generat- ing functional C++ code, achieving success rates of 94.08% and 91.43%, respectively, reﬂecting effective handling of complex features in C++, followed by gemini-1.5 in third place with a success rate of 88.16% and llama-3 with the lowest success rate of 85.51%. Takeaway. codestral and GPT-4o outperform at gener- ating C++ code with success rates of 94.08% and 91.43%, effectively handling complex C++ features. Generation of C codes presents more challenges, with lower success rates in all models due to strict memory management and header ﬁle requirements. codestral leads in C at 88.97%, while llama-3 is lowest at 83.08%. These results highlight the need for improvements in handling C’s rigorous syntax and memory management demands.\n\nThe success rates in C were considerably lower for all models, with codestral achieving 88.97% and llama-3 reaching 83.08%. This lower performance can be attributed to strict C requirements for accurate memory management, header ﬁle inclusion, and model failures in solving a few tasks in C. claude-3.5 and GPT-4o performed slightly below codestral with success rates of 88.72% and 87.69% respectively. These results emphasize the need for targeted improvements in AI code generation to address the de- manding syntax, header ﬁle inclusion, and memory man- agement requirements.\n\n5.3 Static Features\n\nThis section evaluates the static code features of LLMs generated code across Java, Python, C++, and C using several complexity metrics, such as lines of code (LoC), cyclic complexity (CyC), cyclic complexity density (CCD) and cognitive complexity (CoC). The analysis included code generated by various LLMs identiﬁed as claude-3.5, gemini-1.5, codestral, GPT-4o, and llama-3. The results are provided in Table 7.\n\nJava code, as summarized in Table 7, shows that claude-3.5 produced the highest LoC (6,480) and the highest CoC (891). gemini-1.5 showed the high- est CCD (0.23), indicating higher complexity. Conversely,\n\nFor\n\n❺ 5 10 3 6 0 0\n\ncodestral generated relatively less complex code with the lowest CyC (730) and CoC (585).\n\nThe evaluation of the Python code showed that among the models, gemini-1.5 had the highest CCD (0.37), re- ﬂecting a very dense complexity despite having a lower LoC (2,077). claude-3.5 produced the highest LoC (3,474) and the highest CoC (881) the same as gemini-1.5. codestral presented the lowest values in all metrics, indicating simpler and less complex code.\n\nFor the C++ code, the results indicate that claude-3.5 generated the highest LoC (6,725) and the highest CoC (1,096). The highest CCD was observed with gemini-1.5 (0.25), suggesting more intricate code patterns. In contrast, GPT-4o showed the lowest CyC (676) and CoC (510), indi- cating simpler, more maintainable code.\n\nWith respect to the C code, claude-3.5 again led with the highest LoC (7,509) and CoC (1,463). gemini-1.5 had the highest CCD (0.28), while codestral presented with the least CyC (894) and CoC (869), indicating a simpler code. the CCD varies be- tween languages, reﬂecting differences in code density and distribution. CoC closely follows CyC but offers ad- ditional insight into code readability and maintainabil- ity. claude-3.5 generally produces more verbose and complex code across all languages, suggesting greater dif- ﬁculty in readability and even the runtime overhead. Lastly, codestral and GPT-4o tend to generate simpler and more maintainable code, with lower complexity metrics.\n\nBased on the analysis above,\n\n5.4 Security and Quality Attribute\n\nThe security and quality evaluation is based on the different metrics in section 3, including LoC, the number of lines of code, which affects the complexity, since a large LoC can indicate more functionality, but may also lead to higher maintenance demands [45].\n\nTable 8 presents Java results based on key quality at- tributes. For the LoC, claude-3.5 produces the highest LoC, increasing both complexity and variability. In contrast, codestral is the most efﬁcient model regarding LoC. In terms of Security, which measures protection against unau- thorized access and vulnerabilities, gemini-1.5 produces the most vulnerabilities among the LLMs for Java code. This result likely stems from a lack of security-focused training examples compared to other models.\n\nAccording to Table 8, codestral shows the fewest re- liability issues, with a score of 37, indicating its ability to generate Java code with minimal crashes or errors. This may be linked to the lower line of code produced by codestral compared to other models. In terms of maintainability, codestral also performs best, with 535 issues, reﬂecting that its code is the easiest to understand, modify, and extend. This can be attributed to the use of clear structures, proper comments, and function decomposition. For security hotspots, llama-3 performs the best, with only 27 issues.\n\nFor the Python code evaluation in Table 8, the models show notable variations in quality. claude-3.5, generating 3,474 lines, has 82 maintainability issues and 44 security hotspots, indicating areas needing security improvements. codestral, with 2,077 lines, performs better with 50 main- tainability issues and 39 security hotspots, reﬂecting better\n\noverall control. gemini-1.5 and GPT-4o, producing 3,064 and 2,906 lines respectively, show similar outcomes with maintainability issues (83 and 75) and security hotspots (36 and 43). llama-3, generating 2,633 lines, has the fewest security hotspots (34) and moderate maintainability issues (63), indicating balanced but not ﬂawless performance.\n\nTable 8 reveals that claude-3.5, with 6,725 LoC, has the highest maintainability issues (590) but few security hotspots (36), suggesting strong security but complex main- tenance. codestral, generating 4,293 LoC, has the fewest maintainability issues (470) and security hotspots (20), in- dicating a balanced performance. gemini-1.5, producing 5,822 LoC and with a higher count of both security is- sues (17) and maintainability concerns (587). GPT-4o and llama-3, with 5,263 and 5,077 lines, respectively, demon- strate moderate and balanced performance in both areas.\n\nFor C, Table 8 shows that claude-3.5, with 7,509 LoC, has a high number of maintainability issues (449) and security hotspots (186), indicating difﬁculties in managing large and secure code. codestral, producing 4,532 lines, has fewer security hotspots (115) and maintainability issues (314), though it scores lower in reliability (39). gemini-1.5, with 6,555 lines, shows the most security issues (33) and moderate maintainability concerns (352), signaling signiﬁ- cant security problems. GPT-4o, generating 5,926 lines, bal- ances moderate security hotspots (133) and maintainability issues (380), while llama-3, with 5,006 lines, shows higher reliability (54) but faces substantial security hotspots (182).\n\nCWE Categories of Security Quality Attribute. The detec- tion of security ﬂaws in the code produced by ﬁve Large Language Models (LLMs) for Java, Python, C, and C++ highlights substantial differences in both the quality and security of the code among the models and programming languages. The Table 9 presents a breakdown of identiﬁed CWE categories, reﬂecting the different LLMs strengths and weaknesses in secure code generation. This analysis sheds light on common security challenges that various LLMs might present when producing code, identifying opportu- nities to enhance LLM-driven code generation.\n\nIn Java, the most observed CWE is CWE-780 (Use of RSA Algorithm without OAEP), which remains consistently high in frequency across various models, notably with models claude-3.5, gemini-1.5, and GPT-4o. This suggests that while generating code for encryption tasks, many LLMs fail to apply the necessary secure padding scheme (OAEP), which is essential for RSA encryption security. This could lead to insecure cryptographic implementations if used in real-world applications. Additionally, CWE-259 (Use of Hard-coded Password) and CWE-295 (Improper Certiﬁcate Validation) appear frequently, suggesting that certain mod- els tend to generate credentials or handle certiﬁcates in an insecure manner. Issues such as CWE-611 (Improper Restriction of XML External Entity Reference) indicate a common oversight in XML handling, potentially exposing generated code to XML external entity (XXE) attacks.\n\nIn Python, CWE-780 is also frequently observed, though less than in Java, indicating that RSA padding issues are not exclusive to Java but persist across languages. Another no- table vulnerability in Python is CWE-259, which highlights the common use of hard-coded credentials, posing a risk\n\n9\n\nTable 7: Static features: lines of code (LoC), cyclomatic complexity (CyC), density (CCD), and cognitive complexity (CoC).\n\nModel\n\nclaude-3.5 gemini-1.5 codestral GPT-4o llama-3 Summary\n\nLoC 6,480 5,606 4,143 5,328 4,993 26,550\n\nJava\n\nCyC 1,097 965 730 908 913 4,613\n\nCCD 0.17 0.23 0.13 0.17 0.18 0.17\n\nCoC 891 902 585 670 809 3,857\n\nLoC 3,474 3,064 2,077 2,906 2,633 14,154\n\nPython\n\nCyC 895 765 553 665 677 3,555\n\nCCD 0.26 0.37 0.18 0.23 0.26 0.25\n\nCoC 881 881 491 610 631 3,494\n\nLoC 6,725 5,822 4,293 5,263 5,077 27,180\n\nC++\n\nCyC 1,261 1,084 825 676 979 4,825\n\nCCD 0.19 0.25 0.14 0.13 0.19 0.18\n\nCoC 1,096 1,044 732 510 884 4,266\n\nLoC 7,509 6,555 4,532 5,926 5,006 29,528\n\nCyC 1,464 1,256 894 1,133 1,004 5,751\n\nC\n\nCCD 0.19 0.28 0.14 0.19 0.20 0.19\n\nCoC 1,463 1,331 869 994 1,012 5,669\n\nTable 8: Quality attributes against security (S), reliability (R), maintainability (M), and security hotspots (SH).\n\nModel\n\nclaude-3.5 gemini-1.5 codestral GPT-4o llama-3 Summary\n\nS 15 16 12 13 13 69\n\nR 51 46 37 52 46 232\n\nJava\n\nM 810 694 535 925 932 3896\n\nSH 61 33 40 76 27 237\n\nS 5 9 9 8 9 40\n\nPython R M SH 44 82 0 36 83 1 39 50 1 43 75 0 34 63 1 196 353 3\n\nS 9 17 10 7 10 53\n\nR 6 8 5 6 10 35\n\nC++\n\nM 590 587 470 459 565 2671\n\nSH 36 18 20 17 18 109\n\nS 19 33 21 27 22 122\n\nR 60 50 39 37 54 240\n\nC\n\nM 449 352 314 380 322 1817\n\nSH 186 146 115 133 182 762\n\nto sensitive information if deployed. Moreover, Python’s handling of CWE-79 (Improper Neutralization of Input Dur- ing Web Page Generation) reveals weaknesses in web-based output, leading to cross-site scripting (XSS) vulnerabilities. C++ exhibits a range of vulnerabilities, with CWE-295, CWE-326, and CWE-327 (related to improper certiﬁcate validation, inadequate encryption strength, and the use of a broken or risky cryptographic algorithm, respectively) being highly prevalent. These vulnerabilities suggest that cryptographic practices and certiﬁcate handling in C++ code generated by LLMs are notably insecure. Addition- ally, CWE-780 (RSA without OAEP) and CWE-611 (XXE) are also present, indicating potential security weaknesses in cryptographic and XML-handling implementations. This underscores that certain models do not enforce secure prac- tices when generating security-sensitive code in C++. The widespread presence of CWE-297 (Improper Validation of Certiﬁcate with Host Mismatch) further highlights weak- nesses in certiﬁcate validation, which could expose systems to man-in-the-middle (MITM) attack vectors if deployed.\n\nTable 9: Breakdown of the security quality attributes based on the CWE categories. LLMs: ❶ claude-3.5, ❷ gemini-1.5, ❸ codestral, ❹ GPT-4o, ❺ llama-3.\n\nCWE ID\n\n780 502 22 918 259 295 611 79 521 759\n\nJava ❶ ❷ ❸ ❹ ❺ 4 4 6 0 1 0 3 2 3 0 0 0 4 2 2 0 0 2 1 2 2 0 1 0 1 0 0 0 0 0\n\n7 1 3 0 2 0 2 1 0 0\n\n8 0 2 0 1 0 2 0 0 0\n\nCWE ID\n\n780 502 22 918 259 295 611 79 521 759\n\nPython ❶ ❷ ❸ ❹ ❺ 3 5 3 0 0 0 0 0 0 0 0 0 3 2 2 0 0 0 0 0 0 2 1 0 1 1 0 0 0 0\n\n4 0 0 1 2 0 0 0 1 1\n\n3 0 0 0 2 0 0 1 2 0\n\nCWE ID\n\n120 295, 326, 327 297 780 611\n\nC++ ❶ ❷ ❸ ❹ ❺ 0 0 0 7 6 9 2 1 0 0 3 0 1 0 0\n\n0 8 2 7 0\n\n0 5 1 1 0\n\nCWE ID\n\n120 295, 326, 327 297 131, 788 780\n\n❶ 9 9 1 0 0\n\n❷ 19 5 2 1 6\n\nC ❸ 14 3 1 0 3\n\n❹ 13 8 2 1 3\n\nand secure API use in Java and Python.\n\nFor C, there is a high prevalence of CWE-120 (Buffer Copy without Checking Size of Input), reﬂecting common buffer overﬂow vulnerabilities in low-level languages that require manual memory management. The frequent oc- currences of CWE-295, CWE-326, and CWE-327 indicate that LLMs often generate C code with poor cryptographic practices and inadequate certiﬁcate validation, a trend also observed in C++. Additionally, CWE-131 (Incorrect Calcu- lation of Buffer Size) and CWE-788 (Access of Memory Location After End of Buffer) highlight inadequate buffer management, posing serious security risks such as memory corruption and arbitrary code execution. CWE-780 appears frequently in gemini-1.5, codestral, and GPT-4o, in- dicating RSA padding issues, though at lower rates than in higher-level languages like Java and Python.\n\nIn summary, there is considerable variation in the secu- rity quality of the generated code across different program- ming languages. gemini-1.5 exhibited the highest over- all reported vulnerabilities, suggesting less conservative or secure default behaviors. However, some vulnerabilities persist across all models and languages.\n\nVulnerabilities in languages like C and C++ are more skewed towards memory management issues (i.e., buffer overﬂows, incorrect buffer size calculations) compared to Java and Python, where cryptographic and XML-related vulnerabilities are more common. This highlights inherent language-speciﬁc risks, such as memory safety in C and C++\n\n5.5 Clean Code Attribute\n\nTable 10 shows the clean code analysis for Java, where most models handled consistency well, with zero issues reported for all except GPT-4o (8) and llama-3 (61). How- ever, intentionality issues were prominent, particularly for claude-3.5 (203) and gemini-1.5 (244), suggesting that the clarity of code purpose could be improved. Adaptability scores were highest for claude-3.5 (532) and lowest for codestral (182), indicating varying levels of code ﬂexibil- ity for future changes.\n\nIn the same table, the clean code analysis for Python shows slightly more noticeable consistency issues, espe- cially for claude-3.5 (54) and llama-3 (51), while the intentionality attribute remains a common challenge across all models. Adaptability and responsibility issues were low across the board, highlighting Python’s inherent simplicity and ﬂexibility. This suggests that while Python code is generally adaptable, models need to improve its clarity.\n\nWith regard to C++, Table 10 also shows signiﬁcant chal- lenges, particularly with intentionality. claude-3.5 (438) and gemini-1.5 (421) had the highest intentionality issues, reﬂecting difﬁculties in generating code that clearly commu- nicates its purpose. Consistency was also a challenge for all models, with no signiﬁcant peaks in reducing the problems. In addition, for the C language, we can see that intention- ality and consistency issues are prevalent, with GPT-4o (197) and gemini-1.5 (188) ranking highest in consistency\n\n10\n\n❺ 11 7 3 1 0\n\n11\n\nTable 10: Evaluation of the clean code in terms of consistency (C), intentionality (I), adaptability (A), and responsibility (R).\n\nModel\n\nclaude-3.5 gemini-1.5 codestral GPT-4o llama-3 Summary\n\nC 0 0 0 8 61 69\n\nI 203 244 181 148 200 976\n\nJava\n\nA 532 387 182 392 360 1,853\n\nR 10 9 6 9 9 43\n\nC 54 47 35 47 51 234\n\nPython I 23 34 17 27 15 116\n\nA 5 5 1 4 1 16\n\nR 5 7 7 5 6 30\n\nC 116 136 117 93 121 583\n\nC++\n\nI 438 421 335 334 408 1,936\n\nA 42 40 24 39 49 194\n\nR 9 15 9 6 7 46\n\nC 162 188 155 197 160 862\n\nI 321 197 186 210 205 1,119\n\nC\n\nA 36 38 27 25 25 151\n\nR 9 11 6 11 7 44\n\nissues. All models exhibited relatively high adaptability is- sues, particularly claude-3.5 (36) and gemini-1.5 (38), reﬂecting C’s complexity in managing clear and adaptable code. These results show that C presents challenges in clean code generation in all models.\n\n[7] R. Khoury, A. R. Avila,\n\nB. M. code generated by chatgpt?” [Online]. Available:\n\nJ.\n\nBrunelle,\n\nand\n\nCamara, in SMC. https://doi.org/10.1109/SMC53992.2023.10394237\n\n“How secure\n\nis IEEE, 2023, pp. 2445–2451.\n\n[8] M. L. Siddiq and J. C. S. Santos, “Generate and pray: Using SALLMS to evaluate the security of LLM generated code,” CoRR, vol. abs/2311.00889, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2311.00889\n\n6 CONCLUSION AND FUTURE WORK\n\nLLM-generated code quality varies signiﬁcantly across pro- gramming languages, with models excelling in some ar- eas but lacking in others. While certain models demon- strate reasonable security, security hotspots persist, requir- ing stronger safeguards. Reliability and maintainability also differ—some models produce reusable, stable code, while others struggle with long-term upkeep. Java code exhibits better consistency and intentionality, whereas Python and C++ suffer from adaptability and responsibility gaps.\n\nLLMs also fail to integrate modern compiler features, with outdated practices over enhanced security. For exam- ple, despite Java 17’s security, LLMs still rely on legacy methods, such as insecure random number generation. C++ code generation faces critical issues with missing include statements, incorrect type handling, and API misinterpreta- tion, leading to frequent compile-time errors.\n\n[9] M. Nair, R. Sadhukhan, and D. Mukhopadhyay, “Generating to cwes,” IACR [Online]. Available:\n\nsecure hardware using chatgpt Cryptol. 212, https://eprint.iacr.org/2023/212\n\nresistant 2023.\n\nePrint Arch., p.\n\n[10] G. Ras, N. Xie, M. van Gerven, and D. Doran, “Explainable deep learning: A ﬁeld guide for the uninitiated,” J. Artif. Intell. Res., vol. 73, pp. 329–396, 2022. [Online]. Available: https://doi.org/10.1613/jair.1.13200\n\n[11] ,\n\n“ppdb1123/copilot-user-study-supp,” 05\n\nhttps://github.com/ppdb1123/copilot-user-study-supp, 2024, (Accessed on 05/14/2024).\n\n[12] ,\n\n“Neilaperry/do-users-write-more-insecure-code- ui users ”do assistants?”,”\n\ncontaining with-ai-assistants: for data and write more ai with https://github.com/NeilAPerry/Do-Users-Write-More-Insecure-Code-with-AI- 05 2024, (Accessed on 05/14/2024).\n\nRepository participant code\n\nthe\n\nanonymized\n\ninsecure\n\n[13] G. Sandoval, H. Pearce, T. Nys, R. Karri, S. Garg, and B. Dolan- Gavitt, “Lost at C: A user study on the security implications large language model code assistants,” in 32nd USENIX of Security Symposium, 2023, pp. 2205–2222. [Online]. Available: https://www.usenix.org/conference/usenixsecurity23/presentation/sandoval\n\nSemantic evaluation success rates further highlight dis- crepancies, averaging 8.50% across models, with Gemini- 1.5 exhibiting a 13.50% variance. These differences suggest training data and model design signiﬁcantly inﬂuence code logic processing. Optimizing LLMs through pseudocode- driven training and language conversion research is crucial for improving accuracy and adaptability.\n\nREFERENCES\n\n[14] O. Asare, M. Nagappan, and N. Asokan, “Is github’s copilot as bad as humans at introducing vulnerabilities in code?” Empir. Softw. Eng., vol. 28, no. 6, p. 129, 2023. [Online]. Available: https://doi.org/10.1007/s10664-023-10380-1\n\n[15] J. Fan, Y. Li, S. Wang, and T. N. Nguyen, “A C/C++ code vulnerability dataset with code changes and CVE summaries,” in MSR. ACM, [Online]. Available: 508–512. https://doi.org/10.1145/3379597.3387501\n\n2020, pp.\n\n[16] B. Yetistiren, I. ¨Ozsoy, M. Ayerdem, and E. T¨uz¨un, “Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt,” CoRR, vol. abs/2304.10778, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2304.10778\n\n[17] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto trained on [Online]. Available:\n\nlarge\n\n“Evaluating\n\nal.,\n\net code,” CoRR, vol. abs/2107.03374, 2021. https://arxiv.org/abs/2107.03374\n\nlanguage models\n\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeuRIPS, 2017, pp. 5998–6008. [Online]. Available: https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html , https://marketplace.visualstudio.com/items?itemName=VisualStudioExptTeam.vscodeintellicode, 05 2024, (Accessed on 05/12/2024). , pair https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer/, 05 2024, (Accessed on 05/12/2024).\n\n“Raphaelkhoury/programsgeneratedbychat- chatgpt,”\n\n[18] ,\n\ngenerated\n\nPrograms\n\ngpt: https://github.com/RaphaelKhoury/ProgramsGeneratedByChatGPT, 05 2024, (Accessed on 05/14/2024).\n\nby\n\n\n\nmarketplace,”\n\n[2]\n\n“Intellicode\n\nvisual\n\nstudio\n\n[3]\n\n“Introducing\n\nyour\n\n[19] F. Wu, Q. Zhang, A. P. Bajaj, T. Bao, N. Zhang, R. Wang, and C. Xiao, “Exploring the limits of chatgpt in software security applications,” CoRR, vol. abs/2312.05275, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2312.05275 [20] P. Black, “Juliet 1.3 test suite: Changes from 1.2,” Jun. 2018. [21] , “tuhh-softsec/llmseceval,” https://github.com/tuhh-softsec/LLMSecEval,\n\nai blog,”\n\ncopilot:\n\ngithub -\n\nprogrammer\n\nthe\n\ngithub\n\n[4] O. Asare, M. Nagappan, and N. Asokan, “A user-centered security evaluation of copilot,” in ICSE. ACM, 2024, pp. 1–11. [Online]. Available: https://doi.org/10.1145/3597503.3639154\n\n05 2024, (Accessed on 05/14/2024).\n\n[5] R. Elgedawy,\n\nJ. Sadik, S. Dutta, A. Gautam, K. Georgiou, Ji, K. Lim, Q. Liu, and S. Ruoti, F. Gholamrezae, F. “Ocassionally secure: A comparative analysis of code generation assistants,” CoRR, vol. abs/2402.00689, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2402.00689\n\n[22] R.\n\nSchuster, C.\n\nSong, E. Tromer,\n\nShmatikov, in vulnerabilities 30th USENIX Security [Online]. Available:\n\nand V.\n\n“You neural Symposium, https://www.usenix.org/conference/usenixsecurity21/presentation/schuster\n\nautocomplete me:\n\nPoisoning\n\ncode\n\ncompletion,” pp.\n\nin 1559–1575.\n\n2021,\n\n[6] N. Perry, M. Srivastava, D. Kumar, and D. Boneh, “Do assistants?” [Online]. Available:\n\nusers write more in CCS. ACM, 2023, pp. 2785–2799. https://doi.org/10.1145/3576915.3623157\n\ninsecure\n\ncode with AI\n\n[23] Y. Huang, Y. Chen, X. Chen,\n\nJ. Chen, R. Peng, Z. Tang, software [Online].\n\n“Generative J. Huang, F. Xu, engineering,” CoRR, 2024. vol. Available: https://doi.org/10.48550/arXiv.2403.02583\n\nand Z. Zheng,\n\nabs/2403.02583,\n\n12\n\n[24] S. Choi and D. Mohaisen, “Attributing chatgpt-generated source codes,” IEEE Transactionson Dependable and Secure Computing, 2025. [25] S. Choi, Y. K. Tan, M. H. Meng, M. Ragab, S. Mondal, D. Mohaisen, and K. M. M. Aung, “I can ﬁnd you in seconds! leveraging large language models for code authorship attribution,” arXiv preprint arXiv:2501.08165, 2025.\n\n[47] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, and J. Gao, “Large language models: A survey,” CoRR, vol. abs/2402.06196, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2402.06196\n\n[48] , “Pieces for developers — ai-enabled developer productivity,”\n\nhttps://pieces.app/, 05 2024, (Accessed on 05/12/2024).\n\n[26] M. Omar and D. Mohaisen, “Making adversarially-trained lan- guage models forget with model retraining: A case study on hate speech detection,” in Companion Proceedings of the Web Conference 2022, 2022, pp. 887–893.\n\n[27] , “Code quality, security & static analysis tool with sonarqube — sonar,” https://www.sonarsource.com/products/sonarqube/, 05 2024, (Accessed on 05/12/2024).\n\n[49] ,\n\npair https://github.com/features/copilot, 05 2024, 05/12/2024).\n\n“Github\n\ncopilot\n\n\n\nyour\n\nai\n\nprogrammer,” (Accessed on\n\n[50] , “Ai\n\naws,” https://aws.amazon.com/codewhisperer/, 05 2024, (Accessed on 05/12/2024).\n\ncode generator\n\namazon codewhisperer\n\n[51] , “Tabnine ai coding assistant — private, personalized, protected,”\n\n[28] , “Gpt-4 — openai,” https://openai.com/index/gpt-4/, 05 2024,\n\n(Accessed on 05/12/2024).\n\n[29] , “Perplexity,” https://www.perplexity.ai/, 09 2024, (Accessed on\n\n09/05/2024).\n\n[30] , “Claude,” https://claude.ai/new, 09 2024,\n\n(Accessed on\n\n09/05/2024).\n\n[31] , “Mistral ai — frontier ai in your hands,” https://mistral.ai/, 05\n\nhttps://www.tabnine.com/, 05 2024, (Accessed on 05/12/2024). intelligent https://www.ﬁgstack.com/, 05 2024, (Accessed on 05/12/2024).\n\n[52] ,\n\n“Figstack:\n\nYour\n\ncoding\n\ncompanion,”\n\n[53] H. Vasconcelos, G. Bansal, A. Fourney, Q. V. Liao, and J. W. Vaughan, “Generation probabilities are not enough: Exploring the effectiveness of uncertainty highlighting in ai-powered code completions,” CoRR, vol. abs/2302.07248, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2302.07248\n\n2024, (Accessed on 05/12/2024). -\n\n[32] ,\n\nideas,” https://gemini.google.com/, 05 2024, (Accessed on 05/12/2024). [33] , “Most popular programming languages in 2024 & beyond,”\n\n“Gemini\n\nchat\n\nto\n\nsupercharge\n\nyour\n\n[54] , “Cwe - new to cwe,” https://cwe.mitre.org/about/new to cwe.html,\n\n05 2024, (Accessed on 05/12/2024).\n\nhttps://www.orientsoftware.com/blog/most-popular-programming-languages/, 02 2024, (Accessed on 05/12/2024).\n\n[34] ,\n\n“C++\n\nprogramming\n\n\n\ndevel- infographic — jetbrains: teams,” and 09\n\nthe\n\nstate\n\nof\n\noper Developer https://www.jetbrains.com/lp/devecosystem-2023/cpp/, 2024, (Accessed on 09/07/2024).\n\necosystem in\n\n2023\n\ntools\n\nfor\n\nprofessionals\n\n[35] A. H. Watson, D. R. Wallace, and T. J. McCabe, Structured testing: A testing methodology using the cyclomatic complexity metric. US Department of Commerce, Technology Administration, National Institute of . . . , 1996, vol. 500, no. 235.\n\n[36] N. E. Fenton and M. Neil, “A critique of software defect prediction models,” IEEE Trans. Software Eng., vol. 25, no. 5, pp. 675–689, 1999. [Online]. Available: https://doi.org/10.1109/32.815326 [37] M. M. Bar´on, M. Wyrich, and S. Wagner, “An empirical validation of cognitive complexity as a measure of source code understandability,” in ESEM. ACM, 2020, pp. 5:1–5:12. [Online]. Available: https://doi.org/10.1145/3382494.3410636\n\n[38] A. Mohaisen, O. Alrawi, and M. Mohaisen, “AMAL: high-ﬁdelity, behavior-based automated malware analysis and classiﬁcation,” Comput. Secur., vol. 52, pp. 251–266, 2015. [Online]. Available: https://doi.org/10.1016/j.cose.2015.04.001\n\n[39] H. Alasmary, A. Khormali, A. Anwar,\n\nJ. Choi, A. Abusnaina, A. Awad, D. Nyang, and A. Mohaisen, “Analyzing things malware: A graph-based approach,” IEEE Internet Things J., vol. 6, no. 5, pp. 8977–8988, 2019. [Online]. Available: https://doi.org/10.1109/JIOT.2019.2925929\n\nJ. Park,\n\nand detecting\n\nemerging\n\ninternet\n\nof\n\n[40] , “Problems - leetcode,” https://leetcode.com/problemset/, 07\n\n2024, (Accessed on 09/09/2024).\n\n[41] , “Edabit // learn to code with 10,000+ interactive challenges,”\n\nhttps://edabit.com/, 07 2024, (Accessed on 09/09/2024).\n\n[42] , “Codewars - achieve mastery through coding practice and developer mentorship,” https://www.codewars.com/, 07 2024, (Accessed on 09/09/2024).\n\n[43] M. L. Siddiq and J. C. Santos, “Securityeval dataset: mining vul- nerability examples to evaluate machine learning-based code gen- eration techniques,” in Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security, 2022, pp. 29–33.\n\n[44] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhari- wal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Lan- guage models are few-shot learners,” Advances in neural informa- tion processing systems, vol. 33, pp. 1877–1901, 2020.\n\n[45] A. Trisovic, M. K. Lau, T. Pasquier, and M. Crosas, “A large-scale study on research code quality and execution,” Scientiﬁc Data, vol. 9, no. 1, p. 60, 2022.\n\n[46] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie, “A survey on evaluation of large language models,” CoRR, vol. abs/2307.03109, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2307.03109",
      "page_number": 6
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "5 2 0 2\n\nb e F 3\n\n]\n\nR C . s c [\n\n1 v 3 5 8 1 0 . 2 0 5 2 : v i X r a\n\nSecurity and Quality in LLM-Generated Code: A Multi-Language, Multi-Model Analysis\n\nMohammed F. Kharma , Soohyeon Choi\n\n, Mohammad Alkhanafseh , David Mohaisen\n\nAbstract—Artiﬁcial Intelligence (AI)-driven code generation tools are increasingly used throughout the software development lifecycle to accelerate coding tasks. However, the security of AI-generated code using Large Language Models (LLMs) remains underexplored, with studies revealing various risks and weaknesses. This paper analyzes the security of code generated by LLMs across different programming languages. We introduce a dataset of 200 tasks grouped into six categories to evaluate the performance of LLMs in generating secure and maintainable code. Our research shows that while LLMs can automate code creation, their security effectiveness varies by language. Many models fail to utilize modern security features in recent compiler and toolkit updates, such as Java 17. Moreover, outdated methods are still commonly used, particularly in C++. This highlights the need for advancing LLMs to enhance security and quality while incorporating emerging best practices in programming languages.\n\nIndex Terms—LLM, AI-generated Code, Security, Measurement\n\n✦\n\n1 INTRODUCTION\n\nThe rapid advancement of AI technologies has led to the widespread use of LLMs in generating source code for vari- ous programming tasks. LLMs have performed remarkably in various natural language processing and generation tasks since a signiﬁcant turning point with transformer mod- els [1]. Different LLMs have emerged as powerful resources for developers since these tools can generate functional code across multiple programming languages.\n\nIn 2018, Microsoft Visual Studio released the IntelliCode extension [2], which offers AI-powered development fea- tures that provide limited insights by analyzing code context using machine learning. In 2021, GitHub introduced Copi- lot, an AI-driven code assistant designed to improve coding quality by training on extensive real-world code reposito- ries [3], enabling it to provide coding recommendations across various programming languages and frameworks. Since GitHub introduced Copilot, AI-driven code gener- ation adoption has grown in the software development lifecycle [4] where complex models are used to perform speciﬁc tasks like writing code for software engineers [5].\n\nOne major challenge for AI-driven code generation is ensuring code quality, where key metrics include validity, correctness, security, reliability, and maintainability. Current research stresses the importance of ensuring security in LLM-generated code [6], [7]. Several other studies have shown that LLMs are capable of producing code that is both secure and vulnerable [5], [6], [8]. The factors affecting the security properties of the code generated by these models are not identiﬁed. Multiple studies highlight the need for better user guidelines and awareness when interacting with AI tools [6], [7], [9] to enhance the quality properties of\n\nD. Mohaisen and S. Choi are with the Department of Computer Science, University of Central Florida, Orlando, FL 32816 USA. D. Mohaisen is the corresponding author (E-mail: mohaisen@ucf.edu).\n\ngenerated code. The security quality of the code generated for identical scenarios may differ depending on the chosen programming language, which is considered an explainable issue in artiﬁcial intelligence [7], [10].\n\nThis study addresses the gap in the literature by in- vestigating the factors that impact the security of code produced by LLMs, with a focus on the inﬂuence of pro- gramming language selection. Our contributions include the development of a comprehensive dataset designed to eval- uate AI-generated code across multiple domains, such as problem-solving, algorithms, and other key areas. Addition- ally, we evaluate the ability of various LLMs (claude-3.5, gemini-1.5, codestral, GPT-4o, llama-3) to generate secure and functional code in different languages (Python, Java, C++, and C). Moreover, we conduct a security analysis of AI-generated code using static security analysis tools (SAST) to identify common vulnerabilities and weaknesses. Lastly, this research conducts a comparative study of the security properties of AI-generated code across different programming languages, highlighting the main strengths and limitations of each AI tool in contexts such as semantic correctness and security. Contributions. We make the following contributions. (1) New dataset for LLM-based coding. We introduce a new manu- ally vetted dataset of 200 programming tasks classiﬁed into six categories that can be used by the research community for evaluating the performance of LLMs. (2) Comprehensive analysis LLM-generated codes. We comprehensively explore the quality attributes and security of the code generated by LLM under the same evaluation condition and using the proposed dataset. (3) In-depth security analysis. We conduct a comprehensive study in different languages that highlight characteristics, issues, and language-speciﬁc differences in security vulnerabilities with LLMs used to generate code. Organization. We provide a review of related work in section 2, the research methodology is described in section 3, the proposed dataset is presented in section 4, analysis results and discussion in section 5, and the con-\n\nM. Kharma and M. Alkhanafseh are with the Department of Computer Science, Birzeit University, Ramallah, Palestine.\n\n1",
      "content_length": 5547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 2,
      "content": "cluding remarks and future work in section 6.\n\n2 RELATED WORK\n\nSeveral works explored the use of LLMs in a variety of soft- ware development activities [4], [5], [6], [7], [8], [9], [13], [14], [16], [19], [22], [23], [24], [25], [26]. This section examines key studies, emphasizing their methods, applications, and the analyzed features. The comparative overview helps place our work in the broader research context.\n\nHuang et al. [23] reviewed pre-trained models and LLMs for generative tasks in software engineering, highlighting models like BERT, general transformers, and ChatGPT. They categorized tasks into requirements generation, code generation, test case generation, patch generation, optimiza- tion, summarization, and code translation. Our research expands on this by examining the effectiveness of LLMs, speciﬁcally in code generation, enhancing the understand- ing of their strengths and limitations in this area.\n\nPerry et al. [6] investigated the security of LLM-based codes, ﬁnding that such codes often had more security ﬂaws than human-written codes, with LLMs displaying overcon- ﬁdence in code security. Sandoval et al. [13] observed a 10% increase in vulnerabilities in LLM-based C programming. Asare et al. [4] found that GitHub Copilot could enhance security for complex problems but had minimal impact on simpler tasks. These studies highlight the need for caution when using AI tools. Our research expands by assessing four LLMs across additional programming languages, C++ and Java, for further evaluation.\n\nYetis¸tiren et al. [16] assessed the quality of code produced by three AI code assistants using the HumanEval bench- mark dataset [17], ﬁnding correctness rates of 65.2%, 46.3%, and 31.1% for ChatGPT, GitHub Copilot, and Amazon CodeWhisperer, respectively. Factors like function names, input, and descriptions were evaluated, and SonarQube [27] was used to assess security, maintainability, and reliability. All three tools were found capable of generating secure code. In contrast, our study expands to four languages and incorporates a broader range of dataset scenarios.\n\nAsare et al. [14] compared GitHub Copilot’s code generation to human-written code for security vulnerabil- ities, using a dataset by Fan et al. [15]. They found that Copilot recreated the same vulnerabilities in 33.3% of cases and remedied 25.5% of them. Copilot showed incon- sistencies, especially with older vulnerabilities, but gener- ally produced fewer security ﬂaws than humans. Khoury et al. [7] tested ChatGPT’s ability to generate secure code in various languages, showing that it often failed to meet security standards but improved with follow-up prompts. ChatGPT corrected 12 of the 21 programs when prompted. Nair et al. [9] explored ChatGPT’s effectiveness in generating hardware code using Common Vulnerability Enumerations (CWE-1194), demonstrating the possibility of guiding AI to avoid common security ﬂaws. Elgedawy et al. [5] analyzed GPT-3.5, GPT-4, Bard, and Gemini, show- ing that using security personas reduced vulnerabilities, especially in GPT-3.5, GPT-4, and Bard. Siddiq et al. [8] introduced the SALLMS framework to evaluate LLMs systematically for security. Schuster et al. [22] and Wu et al. [19] highlighted challenges, such as data poisoning and\n\nLLMs’ reduced effectiveness in handling complex security issues, emphasizing the need for stronger defenses and better vulnerability detection.\n\nTable 1 summarizes related work based on the program- ming languages used in LLM-generated code, the LLM(s) employed, and a summary of the prompt descriptions. Table 2 outlines related work by the quality characteristics addressed and the security analysis methods used to evalu- ate the generated code. Additionally, the table provides the overall impact of AI code generation on code quality based on the respective study’s experiments and results. Our Work. Most existing research focuses on exploring LLM code generation behavior but lacks in-depth analysis of factors affecting the quality and security of the generated code, beyond user demographics and seniority level. In contrast, our work advances the literature by examining the relationship between programming language features and the quality of LLM-generated code. We evaluate the validity, correctness, security, maintainability, consistency, intentionality, adaptability, and responsibility of the code across four programming languages and ﬁve LLMs.\n\n3 METHODOLOGY\n\n3.1 LLMs Selection\n\nThe choice of LLMs is based on several metrics, such as popularity, user base, reputation, support of diverse programming languages, and performance (accuracy, efﬁ- ciency) in the code generation process. These metrics are derived from academic literature and industry benchmarks, ensuring that our study is representative. The LLMs selected in this study, Table 3, cover a wide range, each with distinct strengths in terms of efﬁciency in the code generation pro- cess, language support, and overall accuracy. A signiﬁcant factor in our selection is the diversity of their underlying LLM architecture, whereas they vary in the context of parameter size, training datasets, and decoding strategies. These differences might directly inﬂuence the security and quality of the generated code, making a comparative analy- sis of its output important to understand the strengths and weaknesses of each of these models.\n\nAs is known, the internal architecture and training methodologies of these LLMs, i.e., can introduce signiﬁcant variation in the code that is produced. This study aims to provide information on how these variations inﬂuence the security posture of generated code, focusing on the importance of selecting the right LLM based on speciﬁc development needs. Table 4 highlights the context window and other conﬁgurations used when generating code using each model. Max tokens refers to the maximum number of tokens to generate in completion. Top p changes how the model selects tokens for output. Tokens are selected from the most probable to least until the sum of their probabilities equals the top-p value. The model tempera- ture is used to control the randomness in generating the output. The context window is the maximum token count a model can handle in one forward pass, covering both the input (prompt) and the output. It essentially determines the amount of text the model can process at a time.\n\n2",
      "content_length": 6410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Table 1: A summary of the related work. Highlighted the evaluated programming languages and the LLMs. Languages: ① C, ② C++, ③ Java, ④ Python, ⑤ JavaScript, ⑥ HTML, ⑦ Verilog. LLMs: ⑧ Copilot, ⑨ Codex, ⑩ Whisper, ❶ Gemini, ❷ Bard, ❸ GPTs, ❹ Llama-3, ❺ Claude-3.5, ❻ Codestral, ❼ StarCoder, ❽ CodeGen.\n\nReference\n\nAsare et al. [4] Perry et al. [6] Sandoval et al. [13] Asare et al. [14] Yetistiren et al. [16] Khoury et al. [7] Nair et al. [9] Elgedawy et al. [5] Wu et al. [19] Siddiq et al. [8] Schuster et al. [22] This work\n\nProgramming Languages Year ① ② ③ ④ ⑤ ⑥ ⑦ ✗ ✗ ✗ ✗ 2024 ✔ ✗ ✗ ✗ ✔ ✔ ✗ 2023 ✔ ✗ ✗ ✗ 2023 ✔ ✗ ✗ ✗ ✗ 2023 ✔ ✔ ✗ ✗ ✗ ✗ 2023 2023 ✔ ✔ ✔ ✔ ✗ ✔ ✗ ✗ ✔ ✗ ✗ ✗ ✗ 2023 ✗ ✗ ✗ ✗ ✔ ✗ 2023 ✗ ✗ ✗ ✔ ✔ ✗ 2023 ✗ ✗ ✗ ✔ ✗ ✗ 2023 ✗ ✗ ✗ ✔ ✗ ✗ 2021 ✗ ✗ 2024 ✔ ✔ ✔ ✔ ✗\n\n✗\n\n✗ ✗ ✗ ✗ ✗ ✔ ✗\n\n✗\n\n✗ ✗ ✗ ✗ ✗\n\nLarge Language Models\n\nPrompt Scenario\n\n⑧ ⑨ ⑩ ❶ ❷ ❸ ❹ ❺ ❻ ❼ ❽ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✔ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✔ ✔ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✔ ✗ ✔ ✔ ✔ ✔ ✗ ✗ ✗\n\n✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗\n\n✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✔ LLMSecEval dataset [21] ✗\n\n✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗\n\nTwo problems [11] Six tasks [12] Shopping list function Big-Vul dataset [15] HumanEval dataset [17] 21 tasks [18] Scenarios from the selected CWEs Nine tasks SARD and Juliet datasets [20]\n\n✗ ✗ ✗\n\n✗ ✗\n\n200 tasks\n\nTable 2: A summary of the related work, highlighting the features and methods used to evaluate the quality of gener- ated code. Overall Impact (OI): Negative (N), Positive (P), and Negative impact due to the use of an inappropriate dataset for testing code security (N*). Attributes: Validity, Correctness, Security, Reliability, and Maintainability. Meth- ods: Manual, Static Scan, and Runtime Scan.\n\nTable 4: LLMs, (MaxT), context window (CW), and Top P (TopP).\n\ntemperature (Temp), maximum tokens\n\nModel GPT-4o llama-3 claude-3.5 codestral gemini-1.5\n\nTemp MaxT 4,096 4,096 4,096 4,096 4,096\n\n0.9 0.9 0.9 0.9 0.9\n\nTopP 0.9 0.9 0.9 0.9 0.9\n\nCW 128k 32k 200k 32k 128k\n\nQuality Attributes Re Ma C ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✔ ✗ ✗ ✗ ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✔ ✔\n\nAnalysis Method\n\nReference\n\nSe ✔ Asare et al. [4] ✔ Perry et al. [6] ✔ Sandoval et al. [13] ✔ Asare et al. [14] Yetistiren et al. [16] ✔ ✔ ✔ ✔ ✗ ✗ Khoury et al. [7] ✗ ✔ ✗ Nair et al. [9] ✔ ✔ ✔ Elgedawy et al. [5] ✔ ✗ ✗ Wu et al. [19] ✔ ✗ ✗ Siddiq et al. [8] ✗ ✔ ✗ Schuster et al. [22] ✔ ✔ ✔ This work Table 3: LLMs used for evaluation and their short names.\n\nV ✗ ✗ ✗ ✗\n\nPe M S ✔ ✗ ✗ ✔ ✗ ✗ ✔ ✔ ✗ ✔ ✔ ✗ ✔ ✔ ✗ ✔ ✗ ✗ ✔ ✗ ✗ ✔ ✔ ✔ ✔ ✗ ✗ ✗ ✗ ✔ ✔ ✗ ✗ ✔ ✔ ✗\n\nR ✗ ✗ ✔ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗\n\nProvider OpenAI Perplexity CLAUDE Mistral Google\n\nRef Model [28] GPT-4o [29] [30] [31] [32]\n\nllama-3-sonar-large-32k-chat claude-3-5-sonnet-20240620 codestral-2405 gemini-1.5-pro-001\n\nShort GPT-4o llama-3 claude-3.5 codestral gemini-1.5\n\n3.2 Programming Languages Selection\n\nOne of the key motivations and contributions of this study is the comparative exploration of the performance and security of LLMs under the same evaluation settings for different programming languages. We determine a range of programming languages to be evaluated in our evaluation, covering both statically and dynamically typed languages. As such, we choose the following programming languages as a preliminary set that meets those metrics: ① C; ② C++; ③ Java; ④ and Python. Although our choice of programming language is limited by the capabilities of LLMs and the programming languages they support, we believe that these programming languages are representative, so they are among the top ﬁve most used programming languages [33]. Each programming language possesses distinct at- tributes that inﬂuence security outcomes in code genera- tion (i.e., Python’s dynamic typing versus C++’s static typ- ing can introduce different vulnerabilities and bugs). Java and Python beneﬁt from automatic memory management, reducing memory-related errors compared to the manual memory management required in C and C++. Thus, the selected languages provide a comprehensive view of how LLMs address these differences. Additionally, this selection ensures that the study’s ﬁndings are applicable to a wider range of real-world software development scenarios, as\n\nOI\n\nN N N P N* N P N N N N N\n\nthese languages rank among the top ﬁve most used in 2024, making the research highly relevant to many users.\n\n3.3 Dataset\n\nTo thoroughly evaluate the performance of LLMs, we cu- rated a set of 200 prompt descriptions aimed at testing multiple code generation facets. The selection of tasks was undertaken to ensure comprehensive coverage of key programming paradigms and secure coding practices. The dataset is detailed in section 4.\n\n3.4 Environment Setup\n\nTo ensure consistency and reproducibility, we created a stan- dardized environment for generating, compiling, executing, and validating code. Designed for diverse programming languages and tasks, it supports multiple languages and unit test execution. All experiments were conducted on a Lenovo ThinkPad E570 with a 7th-gen Intel® Core™ i7, 16 GB DDR4 RAM, and a 256 GB SSD. This hardware was chosen for its availability, efﬁciency, and portability, making it ideal for language model integration, code generation, and multi-language compilation. Debian 12 was selected for its stability, efﬁcient package management, and minimal resource usage, providing a reliable platform for cross- language development and testing. The following software packages and versions were used: ❶ Java. To handle Java code compilation and execution, we used the long-term support (LTS) Java version (OpenJDK version 17.0.8). This version supports the latest features of the Java language, ensuring compatibility with mod- ern programming constructs and practices generated by the LLMs.\n\n❷ Python. For Python code, we utilized Python version 3.11.9. This version was chosen for its compatibility with the latest Python libraries and features, ensuring that the generated Python code was evaluated in an up-to-date runtime environment.\n\n❸ C and C++. The compilation of the C and C++ codes was carried out using CMake version 3.28.6. For C++ specif- ically, we conﬁgured the CMAKE CXX STANDARD to\n\n3",
      "content_length": 6181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "version 17, which gained popularity after its release in 2017 [34], ensuring that all C++ code generated by the models adhered to the C++17 standard. This choice was made to support modern C++ features, such as struc- tured bindings and inline variables, which are common in LLM-generated code. This setup offered a stable basis for assessing the code produced by the ﬁve chosen LLMs. By preserving identical hardware and software settings, we ensured that any dif- ferences in code compilation, run-time, or accuracy between programming languages or models are due to the models themselves and not to environmental factors.\n\n3.5 LLM Integration and Code Generation\n\nBased on the LLMs selected in Table 3, each model was used to generate responses in the four selected programming lan- guages. To streamline the code generation process, the LLMs were interfaced with a custom written Python program that methodically dispatched task prompts in sequential order to each model and collected the resultant code output. Upon code generation, another custom-written Python program was used to systematically parse and arrange the results within a structured ﬁle system, assigning appropriate ex- tensions pertinent to the programming language (i.e., .py for Python, .java for Java). Each ﬁle was named according to the task prompt identiﬁer, the LLM model used, and the language, facilitating traceability and ease of comparison.\n\n4,000 code ﬁles were generated, comprising 200 ﬁles per language for each LLM. This dataset formed the foundation for the subsequent evaluation procedures, ensuring that each LLM was evaluated using an identical set of tasks and programming languages.\n\n3.6 Quality Evaluation\n\nTo evaluate the quality of the generated code, analysis is performed based on several quality metrics, including syntax validity, functional correctness, code lines, reliability, maintainability, and security [8], [16]. We use two types of evaluation methods: ① Manual evaluation by a human expert, where two developers participated in the semantic evaluation and wrote the unit testing ﬁles; ② Automatic static secure code scanning using the SonarQube [27], a static code scanning tool. Although human evaluation met- rics may not scale, they are still considered the golden stan- dard for evaluating the output of LLMs and NLP tasks. Us- ing automatic analysis tools, we will use human metrics to evaluate a small-scale set of LLM examples and generalize beyond the small sample of human evaluators. A review of recent literature that identiﬁes these evaluation methods as an important approach in code quality evaluation supports the selection of the metrics and tools mentioned above.\n\n3.6.1 Manual Method\n\nTwo developers, with two and ten years of software devel- opment experience, are hired to participate in subsequent evaluations. In addition, the ﬁrst and second authors partic- ipated in the process of progress coordination and reviews. Compilation-Time Errors. Before evaluating functionality and quality, we ﬁrst ensured the code met syntactic stan- dards and compiled without errors. This step was crucial for valid and reliable assessments.\n\nThe automated scripts were designed and written for each programming language to perform syntactic checks. For compiled languages such as C, C++, and Java, each source ﬁle was compiled, and the results of these compila- tion attempts were recorded. For interpreted languages such as Python, syntax validation was performed using Python’s native syntax-checking functionality.\n\nThe results were documented in a comprehensive matrix with 200 rows (representing the prompts) and 20 columns (corresponding to 5 LLMs and 4 languages). Each cell in the matrix was annotated with a binary value indicating syntactical validity and successful code compilation. This methodology facilitated the identiﬁcation of potential is- sues, such as syntax errors or missing imports, enabling the resolution of missing library imports or the exclusion of problematic code before proceeding to semantic analysis.\n\nSemantic and Functional Correctness. We evaluated the se- mantics of the generated code after verifying and reviewing all syntactic errors. This phase involved assessing whether the code produced by each LLM accurately implemented the logic and functionality speciﬁed in the task prompts.\n\nInitially, our goal was to develop a single unit test ﬁle per language for each task prompt that could be uni- versally applied to all LLM-generated solutions. However, during testing, we observed variations in the function signa- tures produced by different LLMs. Differences in parameter types, return values, function names, and parameter order made it impractical to use a single unit test ﬁle for all code generated by the selected LLMs for the same task.\n\nTo address this challenge, we created custom unit test ﬁles for each individual code ﬁle. Each test ﬁle contained ten unit test cases speciﬁcally designed to evaluate the correctness of that particular LLM output. For example, if an LLM-generated function had a unique signature—such as one function using an array data type as input and another using a vector data type–the unit test cases were adapted accordingly to match the expected input/output format.\n\nTogether, 4,000 different unit test ﬁles are required, 200 test ﬁles for each programming language per LLM. The results of these unit tests were documented using another matrix with the same matrix dimensions referenced in compilation-time error validation. Each unit test ﬁle re- ceives a score ranging from 0% (all tests failed) to 100% (all tests passed), depending on how many test cases were successfully executed. This process is implemented by three developers with one round of review cycle for the writ- ten unit testing ﬁles. Through this process, we were able to measure the semantic accuracy of the generated code, providing a comprehensive insight into how effectively the LLMs comprehended the prompt speciﬁcations.\n\nIn summary, the evaluation faced key challenges, in- cluding inconsistencies in function signatures across LLMs, which hindered standardized unit testing. We addressed this by creating individualized unit test ﬁles, increasing complexity but ensuring accuracy. Additionally, variations in code quality and completeness required manual adjust- ments, such as adding missing imports, to enable proper compilation and testing.\n\n4",
      "content_length": 6424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "3.6.2 Tool-Based Method\n\nThere are three evaluation metrics generated using the static code scanning tool. Static Features. These features for codes generated by the LLMs are collected using SonarQube tool [27] which sup- ports all selected languages in this study. These features are: ① Lines of Code (LoC) measures the program’s lines of code, excluding whitespace. LoC is a predictive metric used to evaluate effort and maintainability.\n\n② Cyclomatic Complexity (CyC) calculates code complexity using a control ﬂow graph (CFG). With E as edges, N as nodes, and Q as connected components, CyC is computed as M = E + 2Q − N [35].\n\n③ Cyclomatic Complexity Density (CCD) measures how cy- clomatic complexity spreads across the codebase. With cl as the total code lines, CCD is calculated as Md = (E + 2Q − N)/cl [36].\n\n④ Cognitive Complexity (CoC) measures the difﬁculty of un- derstanding code [37]. CoC considers structure like con- n i=1 nc, where trol ﬂow and nesting, using C = Cbase + P nc represents increases due to nesting and conditionals.\n\nSoftware Quality Attributes. For our analysis, we assess the software quality of the generated code using Sonar- Qube [27], a well-known tool for code quality inspection. Evaluate the source code on the basis of multiple quality metrics. Our evaluation focuses on four software quality indicators: ① reliability, ② security, ③ maintainability, ④ security hotspots. In total, we examined 97,412 lines of code generated in four languages by ﬁve different LLMs. ① Reliability measures how well the code operates under predeﬁned conditions. The tool identiﬁes bugs that may cause errors or unpredictable behavior. By ﬁnding and ﬁxing these bugs, we ensure adherence to best practices and mitigate potential run-time issues. We analyzed bug density in the produced code to conﬁrm high reliability, helping to prevent execution problems.\n\n② Security ensures the code is free from vulnerabilities that malicious actors could exploit [38], [39]. The tool highlights security vulnerabilities, such as improper in- put handling or weak encryption, which may lead to breaches. We evaluated the number and severity of these vulnerabilities to ensure that the generated code meets modern security standards, protecting sensitive data and preventing unauthorized access.\n\n③ Maintainability refers to the ease of understanding, mod- ifying, and extending the code over time. The tool de- tects code smells, indicating inefﬁcient design choices that may hinder future development. We assessed the maintainability score by considering factors like code complexity and adherence to coding standards, ensuring the code remains ﬂexible and manageable.\n\n④ Security Hotspots are areas of code that may not be vulner- abilities but are sensitive and could lead to security issues if mismanaged. These often involve security-critical func- tions such as authentication and data validation. The tool ﬂags these areas for developer review, ensuring proper handling. We included an analysis of security hotspots to prevent the generated code from unintentionally in- troducing risks in critical sections.\n\nClean Code Attributes. When evaluating the attributes of clean code, we consider four primary dimensions: ① con- sistency, ② intentionality, ③ adaptability, ④ accountability. These characteristics offer a structure for evaluating the quality, readability, and maintainability of code produced from clean code practices and standards. ① Consistency assesses the formatting, naming conventions, and structural design of the code. It ensures that the generated code follows a standardized framework across different languages and prompts. This includes uni- formity in spacing, indentation, and identiﬁer casing. Maintaining consistency improves readability, facilitates collaboration, and reduces cognitive load during code review and maintenance.\n\n② Intentionality evaluates the clarity and effectiveness of the code in fulﬁlling its purpose. It examines whether the generated code is logical, complete, and efﬁcient, ensur- ing that it conveys its functionality without unnecessary complexity while maintaining coherent logic.\n\n③ Adaptability focuses on the ease in modifying the code. It measures whether the code is modular and structured to allow localized updates with minimal risk of introducing errors. Well-adaptable code maintains a clear separation of concerns, ensuring that each function or component serves a deﬁned purpose, reducing overall complexity. ④ Responsibility ensures adherence to ethical and profes- sional standards. It evaluates whether the generated code complies with legal and licensing requirements, safeguards sensitive information, and uses inclusive lan- guage. This attribute helps maintain trust, professional- ism, and ethical integrity in AI-generated code.\n\nAnalysis. We analyze the evaluation results, concentrating on essential quality attributes such as validity, accuracy, se- curity, reliability, maintainability and clean code attributes. A detailed analysis of the results is available in section 5.\n\n4 DATASET\n\nThe dataset created to evaluate the proposed concept was meticulously crafted to effectively assess the code produced by the LLMs, such as those discussed above. 200 program- ming tasks were manually deﬁned and classiﬁed to ensure comprehensive coverage of a wide range of programming concepts. The tasks were divided into different categories: ① problem-solving, general coding challenges that involve solving algorithmic problems, aimed at testing the logical and problem-solving capabilities of LLMs; ②algorithms, tasks that focus on implementing fundamental algorithms, such as sorting, searching, dynamic programming, and graph-based algorithms; ③ data structures, tasks designed to assess the correct usage, manipulation, and implemen- tation of data structures such as arrays, linked lists, trees, graphs, and hashmaps; ④ secure coding, security-related prompts, carefully chosen based on the selected CWEs from MITRE Common Weakness Enumeration (CWE) beyond CWE Top 25, aimed at testing the LLM’s ability to gener- ate code free from common vulnerabilities such as buffer overﬂows, and injection ﬂaws; ⑤ concurrency and multi- threading, tasks aimed at testing how well the generated code handles parallelism, thread synchronization, and race\n\n5",
      "content_length": 6339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "conditions; ⑥ programming and system, problems requir- ing the manipulation of ﬁle handling, database operations, networking, and error handling.\n\nThe prompts were chosen carefully and designed to ensure their solvability in various programming languages, some of these were derived from code challenge websites and dataset [40], [41], [42], [43]. This approach facilitated the assessment of LLM performance in Python, Java, C++, and C, while preserving uniformity in problem complexity and intent across all languages and LLMs. Features. The dataset consists of several attributes to guide the evaluation, including: ① task number, which refers to unique identiﬁer assigned to each question; ② prompt title, which refers to brief title that summarizes the problem statement; ③ description, a detailed description of the task, outlining the problem to be solved; ④ hints, which refers to instructions that provided to AI model to guide the gener- ation of the code; ⑤ solutions, represent the code generated by the LLM for each programming language along with the name of the model used; ⑥ source, which refers to the origin of the task, whether manually created or adapted; ⑦ test cases, which refers to 10 test cases per prompt were written to evaluate the semantic of the generated code; ⑧ tags, which refers to labels assigned to each task to facilitate the process of ﬁltrating, categorization, and statistical analysis; ⑨ comments, which refers to notes made by reviewers to document the evaluation process, including issues encoun- tered during the generation process. Purpose and Scope. The dataset provides a comprehensive resource for evaluating LLM-generated code across multiple languages. It consists of 200 unique programming chal- lenges, each solved by ﬁve LLMs in four languages, yielding 4,000 records. Each record represents a solution generated by a speciﬁc AI tool for a given task in a particular language, enabling detailed performance analysis. Creation Guidelines. Creating the dataset followed a strict guideline to ensure that the generated data set refers to a high-quality dataset. The main guidelines are as follows: ✧ Task Design. Each programming task is created to pro- vide a clear statement and make sure that the tasks created cover different topics, such as algorithms, data structure, and CWE-based questions. ✧ Task Distribution. The generated tasks were categorized into different topics to facilitate the evaluation of the AI- generated code in different problem domains. ✧ AI Tools Comparison. Different solutions were pro- posed by different LLMs (claude-3.5, gemini-1.5, codestral, GPT-4o, llama-3) in four programming lan- guages (Python, Java, C++, and C). ✧ Partial Solutions. Each of the generated codes was evaluated; if LLMs fail to generate a complete solution for a speciﬁc problem or provide only partial code, this was documented. In this case, a partial score is given based on the number of passed test cases.\n\n5 RESULTS AND DISCUSSION\n\nWe analyze the performance of different LLMs with respect to code generation, including correctness, security, and reli- ability. The evaluation process depends on different metrics, i.e., compilation-time errors, security, and overall accuracy\n\nTable 5: Breakdown (%) of LLM-generated code ﬁles with- out compilation-time errors (error free) and semantic issues.\n\nModel\n\nclaude-3.5 gemini-1.5 codestral GPT-4o llama-3\n\nCompilation-time error-free Java C 96.0 95.0 90.5 88.5 91.5 88.5 91.5 94.0 88.0 88.0\n\nPython 99.5 100.0 100.0 100.0 100.0\n\nC++ 81.5 77.5 80.0 89.0 77.0\n\nJava 95.0 94.2 85.9 92.2 86.2\n\nSemantic error-free\n\nPython 96.7 97.3 94.4 96.4 91.6\n\nC++ 92.0 88.2 94.1 91.4 85.5\n\nC 88.7 83.9 89.0 87.7 83.1\n\nacross different languages. Each LLM’s output is compared to deﬁne the best generation tool.\n\n5.1 Compilation-Time Errors\n\nThe ﬁrst part of Table 5 presents insights into the perfor- mance of different LLMs in generating compilable code without errors in four programming languages (Java, Python, C++, and C). To calculate the compilation success rate, let P be the programming language, C be the number of compilable ﬁles in P, T be the total number of ﬁles from in P, and SP be the compilation success rate (in percentage). n i=1 Ci, where Ci is 1 if ﬁle i is com- We then deﬁne C = P pileable successfully and 0 otherwise. Moreover, we deﬁne T = n, where n is the total number of ﬁles. We also deﬁne the compilation success rate in P as SP = (CP/TP) × 100. The ﬁndings reveal that all models achieve high per- centages of compilable code in Python, using three LLMs (gemini-1.5, codestral, and GPT-4o), achieving a suc- cess rate 100%. This suggests that Python’s simpler syntax and dynamic nature make it easier for the AI tool to generate accurate and error-free code. Takeaway. Python had the highest success rates for com- pilable code across all models. This suggests that Python’s simpler syntax and dynamic nature contribute to the accu- racy and reliability of code generation by AI models.\n\nThe variability in the compilation-time error-free rates observed in Table 5 for Java, C++, and C suggests that LLMs exhibit different strengths depending on the lan- guage. For Java, performance varies more signiﬁcantly compared to Python, with success rates ranging between 88.00% and 95.00%. Some models, such as claude-3.5 and GPT-4o, outperform others, while llama-3, gemini-1.5, and codestral show slightly lower success rates at 88%, 88.5%, and 88.5%, respectively.\n\nThese results can be attributed to Java’s verbose, stati- cally typed nature, which reduces compilation-time errors when an LLM correctly understands its syntax and princi- ples. The slight variations likely stem from differences in how well each LLM handles Java’s syntax rules, exceptions, and object-oriented features (i.e., encapsulation). Models like claude-3.5 and GPT-4o may be better ﬁne-tuned for Java-speciﬁc dependencies and required imports, as missing import statements—such as java.utils.*—are a common issue affecting compilation success.\n\nTakeaway. Models like claude-3.5 and GPT-4o perform better in Java, due to better handling of Java’s syntax rules and package dependencies, while models like llama-3, gemini-1.5, and codestral show lower success rates. This highlights the importance of ﬁne-tuning for Java- speciﬁc aspects to improve LLM performance.\n\nC++ shows the lowest success rates across all models, with scores ranging from 77.00% to 89.00%. The worst\n\n6",
      "content_length": 6469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "error-free score in C++ is with llama-3 and gemini-1.5. The performance of LLMs in generating C++ code appears to be constrained by missing include statements, incor- rect type handling, and misinterpretation of APIs. C++ is a complex language with many different standards (i.e., C++11, C++17), and LLMs struggle when dealing with the complexities of the language’s syntax, library usage, and type system, leading to frequent compile-time errors when generating more advanced code involving libraries like STL (Standard Template Library) or 3rd party APIs/libraries.\n\nTakeaway. LLMs suffer with C++ with missing include statements, incorrect type handling, different standards (i.e., C++11, C++17), and misinterpretation of APIs/func- tions. C++ syntax complexity, library usage, and type sys- tem contribute to frequent compile-time errors with STL and 3rd party APIs/libraries.\n\nThe varying success rates in C code generation are due to their ability in handling C programming standards and dependencies. The high success rate of claude-3.5 (96%) suggests that it effectively complied with C stan- dards and included the necessary headers, avoiding com- mon problems (i.e., undeclared types and missing libraries). Meanwhile, llama-3 with the lowest success rates (88. 00%), encountered frequent errors related to unknown types (i.e., bool), missing headers (i.e., cgi/cgi.h), and incorrect function arguments. These issues point to a reliance on non-standard libraries or incorrect assumptions about the development environment. These inconsistencies highlight the need to ensure that the generated code adheres to stan- dard C practices, incorporates all required dependencies, and conforms to suitable function signatures to enhance the probability of successful compilation.\n\nTakeaway. The frequent errors in C are related to unknown types (i.e., bool), missing headers (i.e., cgi/cgi.h), and incorrect function arguments, are due to reliance on non- standard libraries. Ensuring generated code aligns with standard C practices and includes all dependencies with its source is crucial for successful compilation.\n\nThe percentage of code ﬁles without compilation er- rors provides insight into how well these AI tools are tuned for different programming paradigms and syntax complexities. Regarding claude-3.5, it performs well in Java with 95% as a result, and in C with 96%, showing its good performance with both object-oriented and func- tional programming languages. Less performance with the C++ programming language with 81.5%. GPT-4o achieved strong results in all program languages, especially C++ (89%), showing that it is reliable and adaptable for gener- ating error-free code. Regarding gemini-1.5, this model achieved the highest performance in Python and good in C, but this model achieved the lowest performance as re- sults regarding Java and C++ compared to claude-3.5 and GPT-4o. Lastly, regarding llama-3, performs lower in most languages except Python. It struggles with Java (88%), C++ (77%), and C (88%), showing that it is not as well tuned for more complex languages. The reason behind the results shown for each of the AI tools refers to a set of factors such as training data, since each model trained on large, more diverse, and higher-quality datasets is better at understanding various programming languages\n\n[44]. Another important factor regarding the performance of the AI tool refers to model size and complexity, since a larger model with more parameters such as GPT-4o can better understand and generate.\n\nTypes of compilation-time errors. A breakdown of the compilation-time errors is summarized in Table 6. Python’s breakdown is excluded from the table since it lists only error types occurring more than once across all LLMs. In the following, compilation errors are grouped into categories to understand the common failure modes across the models.\n\n① Library Errors The most common error category across all models was related to missing imports or incorrect package references. Errors such as ”cannot ﬁnd symbol” and failure to recognize or include required third-party libraries, speciﬁcly missing import for java.utils.*, par- ticularly for tasks that involved non-standard libraries, occurred frequently. This indicates a need for improved handling of Java libraries and packages, possibly by training LLMs on more diverse codebases with proper import statements. In addition, LLMs may beneﬁt from an enhanced contextual understanding of external de- pendencies in programming tasks.\n\n② Exception Handling Missing or incorrect exception han- dling was another area where the models faltered. gemini-1.5, codestral, and llama-3 were partic- ularly prone to this type of error, indicating that these models could improve by focusing on the need to handle exceptions based on the exception that can be triggered by code in scope for a particular code block or method. ③ Missing Class/Member An often recurrent issue ﬂagged by codestral is the absence of a class or class member, such as setter and/or getter methods. This error arises when the model presumes the presence of a speciﬁc method or class without including the pertinent code or even suggesting it within comments.\n\n④ Syntax Syntax errors, including incorrect symbols, miss- ing semicolons, or invalid syntax, were found primarily in codestral. This may suggest a need for ﬁne-tuning on Java syntax conventions or a deeper understanding of language-speciﬁc rules.\n\n⑤ Type Compatibility Errors related to incompatible types, such as attempts to assign a java.lang.Object to a java.lang.String, or java.io.IOException cannot be con- verted to java.lang.SecurityException. These errors indi- cate the models’ ability to infer the data types in context. ⑥ Variable Identiﬁer Errors arising from the use of undeﬁned variables were found in multiple models. This indicates that the models occasionally fail to maintain variable state consistency across different sections of the code. ⑦ Undeclared Var./Fun. Errors where a variable or function is used before being declared, leading to a failure in recognizing identiﬁers. This may happen if a necessary declaration or deﬁnition is missing.\n\n⑧ Binding/Qualiﬁer Errors related to failing to bind a vari- able correctly or mismatches with qualiﬁers. These errors often occur in function parameters or references.\n\n⑨ Constant Undeclared Errors with undeclared constants like\n\ntrue and nullptr.\n\n7",
      "content_length": 6446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "8 Table 6: Breakdown of LLM-generated code ﬁles compilation-time error types. LLMs: ❶ claude-3.5, ❷ gemini-1.5, ❸ codestral, ❹ GPT-4o, ❺ llama-3.\n\nError Type\n\nLibrary Errors Exception Handling Class/Member Syntax Type Compatibility Variable def. related\n\n❶ 10 0 0 0 0 0\n\n❷ 17 3 0 1 1 1\n\nJava ❸ 15 2 4 1 1 1\n\n❹ 8 0 0 2 0 1\n\n❺ 17 4 1 0 2 1\n\nError Type\n\nLibrary Errors Binding/Qualifier Class/Member Syntax Variable def. related Undeclared Var./Fun.\n\n❶ 20 0 3 4 6 2\n\n❷ 8 1 2 2 15 13\n\nC++ ❸ 4 0 9 6 5 11\n\n❹ 11 0 3 1 4 2\n\n❺ 22 2 0 6 4 6\n\nError Type\n\nLibrary Error Constant Undeclared Function Argument Undeclared Var./Fun. Incomplete Code Other\n\n❶ 7 0 2 1 1 2\n\n❷ 14 1 0 1 1 3\n\nC ❸ ❹ 3 3 0 2 0 0 2 4 1 1 1 7\n\n5.2 Semantic\n\nThe results shown in Table 5 under the semantic error- free part present the percentage of valid semantic code generated. The observed variance in success rates within the same LLM, with most models differing by approximately 8.50% in their ability to generate correct code in different languages, except gemini-1.5 which shows a range of 13.50%, highlights considerable variations in the way these models process the same coding task depending on the target programming language. Despite the same reasoning capabilities required, this variation suggests discrepancies in how each model interprets and implements program- ming logic, potentially due to differences in training data, model architecture, or optimization strategies. The wider range of gemini-1.5 may indicate particular challenges in understanding or generating code for certain languages or complex programming constructs, reﬂecting either gaps in its training data or special consideration in its design.\n\nTakeaway. The variance in success rates among different LLMs, with a range of 13.50% for gemini-1.5, indicates notable differences in how these models handle coding tasks, likely due to variations in training data, model design, and optimization strategies.\n\nFor Java, the models generally showed strong perfor- mance, with claude-3.5 achieving the highest success rate of 95.00%. This indicates a good proﬁciency in creating effective and precise Java code, probably because of Java’s object-oriented design and consistent libraries, which ap- parently correspond well with the models’ training data. gemini-1.5 also performed remarkably well, reaching a success rate of 94.22%, demonstrating its proﬁciency in man- aging Java syntax and standard programming constructs. codestral and GPT-4o showed marginally lower success rates at 85.87% and 92.22%, respectively. The variation in performance, especially for codestral, could indicate dif- ﬁculties in addressing Java’s more complex class structures or standard library usage, which aligns with the low success rate for the code compilation success rate as well. Conse- quently, the high success rates from all models indicate that these models are proﬁcient in producing Java code, while also emphasizing the necessity for ongoing improvements to tackle the more complex elements of the language and reasoning capabilities.\n\nTakeaway. LLMs perform well in the generation of Java code, with claude-3.5 leading at 95.00%, probably be- cause of Java’s object-oriented design and consistent li- braries. gemini-1.5 also excels, while codestral and GPT-4o show some challenges with complex structures. This highlights strong proﬁciency but also points to a need for improvement in handling Java’s complexities.\n\nAmong the four languages evaluated, Python achieved the highest success rates, with gemini-1.5 at the top\n\nwith a success rate of 97.34%, followed by claude-3.5 and GPT-4o with a success rate 96.72% and 96.35% re- spectively. Python’s simplicity and dynamic typing con- tribute to these high success rates, as its syntax is gen- erally less strict and more ﬂexible compared to statically typed languages. The high performance across all models in Python suggests they are proﬁcient on the language and its constructs. Furthermore, llama-3, with a lower success rate of 91.56%, indicates that even models with generally high success rates can face challenges in generating correct Python code, potentially due to variations in handling lan- guage features and reasoning capabilities. codestral and GPT-4o demonstrated signiﬁcant performance in generat- ing functional C++ code, achieving success rates of 94.08% and 91.43%, respectively, reﬂecting effective handling of complex features in C++, followed by gemini-1.5 in third place with a success rate of 88.16% and llama-3 with the lowest success rate of 85.51%. Takeaway. codestral and GPT-4o outperform at gener- ating C++ code with success rates of 94.08% and 91.43%, effectively handling complex C++ features. Generation of C codes presents more challenges, with lower success rates in all models due to strict memory management and header ﬁle requirements. codestral leads in C at 88.97%, while llama-3 is lowest at 83.08%. These results highlight the need for improvements in handling C’s rigorous syntax and memory management demands.\n\nThe success rates in C were considerably lower for all models, with codestral achieving 88.97% and llama-3 reaching 83.08%. This lower performance can be attributed to strict C requirements for accurate memory management, header ﬁle inclusion, and model failures in solving a few tasks in C. claude-3.5 and GPT-4o performed slightly below codestral with success rates of 88.72% and 87.69% respectively. These results emphasize the need for targeted improvements in AI code generation to address the de- manding syntax, header ﬁle inclusion, and memory man- agement requirements.\n\n5.3 Static Features\n\nThis section evaluates the static code features of LLMs generated code across Java, Python, C++, and C using several complexity metrics, such as lines of code (LoC), cyclic complexity (CyC), cyclic complexity density (CCD) and cognitive complexity (CoC). The analysis included code generated by various LLMs identiﬁed as claude-3.5, gemini-1.5, codestral, GPT-4o, and llama-3. The results are provided in Table 7.\n\nJava code, as summarized in Table 7, shows that claude-3.5 produced the highest LoC (6,480) and the highest CoC (891). gemini-1.5 showed the high- est CCD (0.23), indicating higher complexity. Conversely,\n\nFor\n\n❺ 5 10 3 6 0 0",
      "content_length": 6286,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "codestral generated relatively less complex code with the lowest CyC (730) and CoC (585).\n\nThe evaluation of the Python code showed that among the models, gemini-1.5 had the highest CCD (0.37), re- ﬂecting a very dense complexity despite having a lower LoC (2,077). claude-3.5 produced the highest LoC (3,474) and the highest CoC (881) the same as gemini-1.5. codestral presented the lowest values in all metrics, indicating simpler and less complex code.\n\nFor the C++ code, the results indicate that claude-3.5 generated the highest LoC (6,725) and the highest CoC (1,096). The highest CCD was observed with gemini-1.5 (0.25), suggesting more intricate code patterns. In contrast, GPT-4o showed the lowest CyC (676) and CoC (510), indi- cating simpler, more maintainable code.\n\nWith respect to the C code, claude-3.5 again led with the highest LoC (7,509) and CoC (1,463). gemini-1.5 had the highest CCD (0.28), while codestral presented with the least CyC (894) and CoC (869), indicating a simpler code. the CCD varies be- tween languages, reﬂecting differences in code density and distribution. CoC closely follows CyC but offers ad- ditional insight into code readability and maintainabil- ity. claude-3.5 generally produces more verbose and complex code across all languages, suggesting greater dif- ﬁculty in readability and even the runtime overhead. Lastly, codestral and GPT-4o tend to generate simpler and more maintainable code, with lower complexity metrics.\n\nBased on the analysis above,\n\n5.4 Security and Quality Attribute\n\nThe security and quality evaluation is based on the different metrics in section 3, including LoC, the number of lines of code, which affects the complexity, since a large LoC can indicate more functionality, but may also lead to higher maintenance demands [45].\n\nTable 8 presents Java results based on key quality at- tributes. For the LoC, claude-3.5 produces the highest LoC, increasing both complexity and variability. In contrast, codestral is the most efﬁcient model regarding LoC. In terms of Security, which measures protection against unau- thorized access and vulnerabilities, gemini-1.5 produces the most vulnerabilities among the LLMs for Java code. This result likely stems from a lack of security-focused training examples compared to other models.\n\nAccording to Table 8, codestral shows the fewest re- liability issues, with a score of 37, indicating its ability to generate Java code with minimal crashes or errors. This may be linked to the lower line of code produced by codestral compared to other models. In terms of maintainability, codestral also performs best, with 535 issues, reﬂecting that its code is the easiest to understand, modify, and extend. This can be attributed to the use of clear structures, proper comments, and function decomposition. For security hotspots, llama-3 performs the best, with only 27 issues.\n\nFor the Python code evaluation in Table 8, the models show notable variations in quality. claude-3.5, generating 3,474 lines, has 82 maintainability issues and 44 security hotspots, indicating areas needing security improvements. codestral, with 2,077 lines, performs better with 50 main- tainability issues and 39 security hotspots, reﬂecting better\n\noverall control. gemini-1.5 and GPT-4o, producing 3,064 and 2,906 lines respectively, show similar outcomes with maintainability issues (83 and 75) and security hotspots (36 and 43). llama-3, generating 2,633 lines, has the fewest security hotspots (34) and moderate maintainability issues (63), indicating balanced but not ﬂawless performance.\n\nTable 8 reveals that claude-3.5, with 6,725 LoC, has the highest maintainability issues (590) but few security hotspots (36), suggesting strong security but complex main- tenance. codestral, generating 4,293 LoC, has the fewest maintainability issues (470) and security hotspots (20), in- dicating a balanced performance. gemini-1.5, producing 5,822 LoC and with a higher count of both security is- sues (17) and maintainability concerns (587). GPT-4o and llama-3, with 5,263 and 5,077 lines, respectively, demon- strate moderate and balanced performance in both areas.\n\nFor C, Table 8 shows that claude-3.5, with 7,509 LoC, has a high number of maintainability issues (449) and security hotspots (186), indicating difﬁculties in managing large and secure code. codestral, producing 4,532 lines, has fewer security hotspots (115) and maintainability issues (314), though it scores lower in reliability (39). gemini-1.5, with 6,555 lines, shows the most security issues (33) and moderate maintainability concerns (352), signaling signiﬁ- cant security problems. GPT-4o, generating 5,926 lines, bal- ances moderate security hotspots (133) and maintainability issues (380), while llama-3, with 5,006 lines, shows higher reliability (54) but faces substantial security hotspots (182).\n\nCWE Categories of Security Quality Attribute. The detec- tion of security ﬂaws in the code produced by ﬁve Large Language Models (LLMs) for Java, Python, C, and C++ highlights substantial differences in both the quality and security of the code among the models and programming languages. The Table 9 presents a breakdown of identiﬁed CWE categories, reﬂecting the different LLMs strengths and weaknesses in secure code generation. This analysis sheds light on common security challenges that various LLMs might present when producing code, identifying opportu- nities to enhance LLM-driven code generation.\n\nIn Java, the most observed CWE is CWE-780 (Use of RSA Algorithm without OAEP), which remains consistently high in frequency across various models, notably with models claude-3.5, gemini-1.5, and GPT-4o. This suggests that while generating code for encryption tasks, many LLMs fail to apply the necessary secure padding scheme (OAEP), which is essential for RSA encryption security. This could lead to insecure cryptographic implementations if used in real-world applications. Additionally, CWE-259 (Use of Hard-coded Password) and CWE-295 (Improper Certiﬁcate Validation) appear frequently, suggesting that certain mod- els tend to generate credentials or handle certiﬁcates in an insecure manner. Issues such as CWE-611 (Improper Restriction of XML External Entity Reference) indicate a common oversight in XML handling, potentially exposing generated code to XML external entity (XXE) attacks.\n\nIn Python, CWE-780 is also frequently observed, though less than in Java, indicating that RSA padding issues are not exclusive to Java but persist across languages. Another no- table vulnerability in Python is CWE-259, which highlights the common use of hard-coded credentials, posing a risk\n\n9",
      "content_length": 6665,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Table 7: Static features: lines of code (LoC), cyclomatic complexity (CyC), density (CCD), and cognitive complexity (CoC).\n\nModel\n\nclaude-3.5 gemini-1.5 codestral GPT-4o llama-3 Summary\n\nLoC 6,480 5,606 4,143 5,328 4,993 26,550\n\nJava\n\nCyC 1,097 965 730 908 913 4,613\n\nCCD 0.17 0.23 0.13 0.17 0.18 0.17\n\nCoC 891 902 585 670 809 3,857\n\nLoC 3,474 3,064 2,077 2,906 2,633 14,154\n\nPython\n\nCyC 895 765 553 665 677 3,555\n\nCCD 0.26 0.37 0.18 0.23 0.26 0.25\n\nCoC 881 881 491 610 631 3,494\n\nLoC 6,725 5,822 4,293 5,263 5,077 27,180\n\nC++\n\nCyC 1,261 1,084 825 676 979 4,825\n\nCCD 0.19 0.25 0.14 0.13 0.19 0.18\n\nCoC 1,096 1,044 732 510 884 4,266\n\nLoC 7,509 6,555 4,532 5,926 5,006 29,528\n\nCyC 1,464 1,256 894 1,133 1,004 5,751\n\nC\n\nCCD 0.19 0.28 0.14 0.19 0.20 0.19\n\nCoC 1,463 1,331 869 994 1,012 5,669\n\nTable 8: Quality attributes against security (S), reliability (R), maintainability (M), and security hotspots (SH).\n\nModel\n\nclaude-3.5 gemini-1.5 codestral GPT-4o llama-3 Summary\n\nS 15 16 12 13 13 69\n\nR 51 46 37 52 46 232\n\nJava\n\nM 810 694 535 925 932 3896\n\nSH 61 33 40 76 27 237\n\nS 5 9 9 8 9 40\n\nPython R M SH 44 82 0 36 83 1 39 50 1 43 75 0 34 63 1 196 353 3\n\nS 9 17 10 7 10 53\n\nR 6 8 5 6 10 35\n\nC++\n\nM 590 587 470 459 565 2671\n\nSH 36 18 20 17 18 109\n\nS 19 33 21 27 22 122\n\nR 60 50 39 37 54 240\n\nC\n\nM 449 352 314 380 322 1817\n\nSH 186 146 115 133 182 762\n\nto sensitive information if deployed. Moreover, Python’s handling of CWE-79 (Improper Neutralization of Input Dur- ing Web Page Generation) reveals weaknesses in web-based output, leading to cross-site scripting (XSS) vulnerabilities. C++ exhibits a range of vulnerabilities, with CWE-295, CWE-326, and CWE-327 (related to improper certiﬁcate validation, inadequate encryption strength, and the use of a broken or risky cryptographic algorithm, respectively) being highly prevalent. These vulnerabilities suggest that cryptographic practices and certiﬁcate handling in C++ code generated by LLMs are notably insecure. Addition- ally, CWE-780 (RSA without OAEP) and CWE-611 (XXE) are also present, indicating potential security weaknesses in cryptographic and XML-handling implementations. This underscores that certain models do not enforce secure prac- tices when generating security-sensitive code in C++. The widespread presence of CWE-297 (Improper Validation of Certiﬁcate with Host Mismatch) further highlights weak- nesses in certiﬁcate validation, which could expose systems to man-in-the-middle (MITM) attack vectors if deployed.\n\nTable 9: Breakdown of the security quality attributes based on the CWE categories. LLMs: ❶ claude-3.5, ❷ gemini-1.5, ❸ codestral, ❹ GPT-4o, ❺ llama-3.\n\nCWE ID\n\n780 502 22 918 259 295 611 79 521 759\n\nJava ❶ ❷ ❸ ❹ ❺ 4 4 6 0 1 0 3 2 3 0 0 0 4 2 2 0 0 2 1 2 2 0 1 0 1 0 0 0 0 0\n\n7 1 3 0 2 0 2 1 0 0\n\n8 0 2 0 1 0 2 0 0 0\n\nCWE ID\n\n780 502 22 918 259 295 611 79 521 759\n\nPython ❶ ❷ ❸ ❹ ❺ 3 5 3 0 0 0 0 0 0 0 0 0 3 2 2 0 0 0 0 0 0 2 1 0 1 1 0 0 0 0\n\n4 0 0 1 2 0 0 0 1 1\n\n3 0 0 0 2 0 0 1 2 0\n\nCWE ID\n\n120 295, 326, 327 297 780 611\n\nC++ ❶ ❷ ❸ ❹ ❺ 0 0 0 7 6 9 2 1 0 0 3 0 1 0 0\n\n0 8 2 7 0\n\n0 5 1 1 0\n\nCWE ID\n\n120 295, 326, 327 297 131, 788 780\n\n❶ 9 9 1 0 0\n\n❷ 19 5 2 1 6\n\nC ❸ 14 3 1 0 3\n\n❹ 13 8 2 1 3\n\nand secure API use in Java and Python.\n\nFor C, there is a high prevalence of CWE-120 (Buffer Copy without Checking Size of Input), reﬂecting common buffer overﬂow vulnerabilities in low-level languages that require manual memory management. The frequent oc- currences of CWE-295, CWE-326, and CWE-327 indicate that LLMs often generate C code with poor cryptographic practices and inadequate certiﬁcate validation, a trend also observed in C++. Additionally, CWE-131 (Incorrect Calcu- lation of Buffer Size) and CWE-788 (Access of Memory Location After End of Buffer) highlight inadequate buffer management, posing serious security risks such as memory corruption and arbitrary code execution. CWE-780 appears frequently in gemini-1.5, codestral, and GPT-4o, in- dicating RSA padding issues, though at lower rates than in higher-level languages like Java and Python.\n\nIn summary, there is considerable variation in the secu- rity quality of the generated code across different program- ming languages. gemini-1.5 exhibited the highest over- all reported vulnerabilities, suggesting less conservative or secure default behaviors. However, some vulnerabilities persist across all models and languages.\n\nVulnerabilities in languages like C and C++ are more skewed towards memory management issues (i.e., buffer overﬂows, incorrect buffer size calculations) compared to Java and Python, where cryptographic and XML-related vulnerabilities are more common. This highlights inherent language-speciﬁc risks, such as memory safety in C and C++\n\n5.5 Clean Code Attribute\n\nTable 10 shows the clean code analysis for Java, where most models handled consistency well, with zero issues reported for all except GPT-4o (8) and llama-3 (61). How- ever, intentionality issues were prominent, particularly for claude-3.5 (203) and gemini-1.5 (244), suggesting that the clarity of code purpose could be improved. Adaptability scores were highest for claude-3.5 (532) and lowest for codestral (182), indicating varying levels of code ﬂexibil- ity for future changes.\n\nIn the same table, the clean code analysis for Python shows slightly more noticeable consistency issues, espe- cially for claude-3.5 (54) and llama-3 (51), while the intentionality attribute remains a common challenge across all models. Adaptability and responsibility issues were low across the board, highlighting Python’s inherent simplicity and ﬂexibility. This suggests that while Python code is generally adaptable, models need to improve its clarity.\n\nWith regard to C++, Table 10 also shows signiﬁcant chal- lenges, particularly with intentionality. claude-3.5 (438) and gemini-1.5 (421) had the highest intentionality issues, reﬂecting difﬁculties in generating code that clearly commu- nicates its purpose. Consistency was also a challenge for all models, with no signiﬁcant peaks in reducing the problems. In addition, for the C language, we can see that intention- ality and consistency issues are prevalent, with GPT-4o (197) and gemini-1.5 (188) ranking highest in consistency\n\n10\n\n❺ 11 7 3 1 0",
      "content_length": 6231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "11\n\nTable 10: Evaluation of the clean code in terms of consistency (C), intentionality (I), adaptability (A), and responsibility (R).\n\nModel\n\nclaude-3.5 gemini-1.5 codestral GPT-4o llama-3 Summary\n\nC 0 0 0 8 61 69\n\nI 203 244 181 148 200 976\n\nJava\n\nA 532 387 182 392 360 1,853\n\nR 10 9 6 9 9 43\n\nC 54 47 35 47 51 234\n\nPython I 23 34 17 27 15 116\n\nA 5 5 1 4 1 16\n\nR 5 7 7 5 6 30\n\nC 116 136 117 93 121 583\n\nC++\n\nI 438 421 335 334 408 1,936\n\nA 42 40 24 39 49 194\n\nR 9 15 9 6 7 46\n\nC 162 188 155 197 160 862\n\nI 321 197 186 210 205 1,119\n\nC\n\nA 36 38 27 25 25 151\n\nR 9 11 6 11 7 44\n\nissues. All models exhibited relatively high adaptability is- sues, particularly claude-3.5 (36) and gemini-1.5 (38), reﬂecting C’s complexity in managing clear and adaptable code. These results show that C presents challenges in clean code generation in all models.\n\n[7] R. Khoury, A. R. Avila,\n\nB. M. code generated by chatgpt?” [Online]. Available:\n\nJ.\n\nBrunelle,\n\nand\n\nCamara, in SMC. https://doi.org/10.1109/SMC53992.2023.10394237\n\n“How secure\n\nis IEEE, 2023, pp. 2445–2451.\n\n[8] M. L. Siddiq and J. C. S. Santos, “Generate and pray: Using SALLMS to evaluate the security of LLM generated code,” CoRR, vol. abs/2311.00889, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2311.00889\n\n6 CONCLUSION AND FUTURE WORK\n\nLLM-generated code quality varies signiﬁcantly across pro- gramming languages, with models excelling in some ar- eas but lacking in others. While certain models demon- strate reasonable security, security hotspots persist, requir- ing stronger safeguards. Reliability and maintainability also differ—some models produce reusable, stable code, while others struggle with long-term upkeep. Java code exhibits better consistency and intentionality, whereas Python and C++ suffer from adaptability and responsibility gaps.\n\nLLMs also fail to integrate modern compiler features, with outdated practices over enhanced security. For exam- ple, despite Java 17’s security, LLMs still rely on legacy methods, such as insecure random number generation. C++ code generation faces critical issues with missing include statements, incorrect type handling, and API misinterpreta- tion, leading to frequent compile-time errors.\n\n[9] M. Nair, R. Sadhukhan, and D. Mukhopadhyay, “Generating to cwes,” IACR [Online]. Available:\n\nsecure hardware using chatgpt Cryptol. 212, https://eprint.iacr.org/2023/212\n\nresistant 2023.\n\nePrint Arch., p.\n\n[10] G. Ras, N. Xie, M. van Gerven, and D. Doran, “Explainable deep learning: A ﬁeld guide for the uninitiated,” J. Artif. Intell. Res., vol. 73, pp. 329–396, 2022. [Online]. Available: https://doi.org/10.1613/jair.1.13200\n\n[11] ,\n\n“ppdb1123/copilot-user-study-supp,” 05\n\nhttps://github.com/ppdb1123/copilot-user-study-supp, 2024, (Accessed on 05/14/2024).\n\n[12] ,\n\n“Neilaperry/do-users-write-more-insecure-code- ui users ”do assistants?”,”\n\ncontaining with-ai-assistants: for data and write more ai with https://github.com/NeilAPerry/Do-Users-Write-More-Insecure-Code-with-AI- 05 2024, (Accessed on 05/14/2024).\n\nRepository participant code\n\nthe\n\nanonymized\n\ninsecure\n\n[13] G. Sandoval, H. Pearce, T. Nys, R. Karri, S. Garg, and B. Dolan- Gavitt, “Lost at C: A user study on the security implications large language model code assistants,” in 32nd USENIX of Security Symposium, 2023, pp. 2205–2222. [Online]. Available: https://www.usenix.org/conference/usenixsecurity23/presentation/sandoval\n\nSemantic evaluation success rates further highlight dis- crepancies, averaging 8.50% across models, with Gemini- 1.5 exhibiting a 13.50% variance. These differences suggest training data and model design signiﬁcantly inﬂuence code logic processing. Optimizing LLMs through pseudocode- driven training and language conversion research is crucial for improving accuracy and adaptability.\n\nREFERENCES\n\n[14] O. Asare, M. Nagappan, and N. Asokan, “Is github’s copilot as bad as humans at introducing vulnerabilities in code?” Empir. Softw. Eng., vol. 28, no. 6, p. 129, 2023. [Online]. Available: https://doi.org/10.1007/s10664-023-10380-1\n\n[15] J. Fan, Y. Li, S. Wang, and T. N. Nguyen, “A C/C++ code vulnerability dataset with code changes and CVE summaries,” in MSR. ACM, [Online]. Available: 508–512. https://doi.org/10.1145/3379597.3387501\n\n2020, pp.\n\n[16] B. Yetistiren, I. ¨Ozsoy, M. Ayerdem, and E. T¨uz¨un, “Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt,” CoRR, vol. abs/2304.10778, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2304.10778\n\n[17] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto trained on [Online]. Available:\n\nlarge\n\n“Evaluating\n\nal.,\n\net code,” CoRR, vol. abs/2107.03374, 2021. https://arxiv.org/abs/2107.03374\n\nlanguage models\n\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeuRIPS, 2017, pp. 5998–6008. [Online]. Available: https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html , https://marketplace.visualstudio.com/items?itemName=VisualStudioExptTeam.vscodeintellicode, 05 2024, (Accessed on 05/12/2024). , pair https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer/, 05 2024, (Accessed on 05/12/2024).\n\n“Raphaelkhoury/programsgeneratedbychat- chatgpt,”\n\n[18] ,\n\ngenerated\n\nPrograms\n\ngpt: https://github.com/RaphaelKhoury/ProgramsGeneratedByChatGPT, 05 2024, (Accessed on 05/14/2024).\n\nby\n\n\n\nmarketplace,”\n\n[2]\n\n“Intellicode\n\nvisual\n\nstudio\n\n[3]\n\n“Introducing\n\nyour\n\n[19] F. Wu, Q. Zhang, A. P. Bajaj, T. Bao, N. Zhang, R. Wang, and C. Xiao, “Exploring the limits of chatgpt in software security applications,” CoRR, vol. abs/2312.05275, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2312.05275 [20] P. Black, “Juliet 1.3 test suite: Changes from 1.2,” Jun. 2018. [21] , “tuhh-softsec/llmseceval,” https://github.com/tuhh-softsec/LLMSecEval,\n\nai blog,”\n\ncopilot:\n\ngithub -\n\nprogrammer\n\nthe\n\ngithub\n\n[4] O. Asare, M. Nagappan, and N. Asokan, “A user-centered security evaluation of copilot,” in ICSE. ACM, 2024, pp. 1–11. [Online]. Available: https://doi.org/10.1145/3597503.3639154\n\n05 2024, (Accessed on 05/14/2024).\n\n[5] R. Elgedawy,\n\nJ. Sadik, S. Dutta, A. Gautam, K. Georgiou, Ji, K. Lim, Q. Liu, and S. Ruoti, F. Gholamrezae, F. “Ocassionally secure: A comparative analysis of code generation assistants,” CoRR, vol. abs/2402.00689, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2402.00689\n\n[22] R.\n\nSchuster, C.\n\nSong, E. Tromer,\n\nShmatikov, in vulnerabilities 30th USENIX Security [Online]. Available:\n\nand V.\n\n“You neural Symposium, https://www.usenix.org/conference/usenixsecurity21/presentation/schuster\n\nautocomplete me:\n\nPoisoning\n\ncode\n\ncompletion,” pp.\n\nin 1559–1575.\n\n2021,\n\n[6] N. Perry, M. Srivastava, D. Kumar, and D. Boneh, “Do assistants?” [Online]. Available:\n\nusers write more in CCS. ACM, 2023, pp. 2785–2799. https://doi.org/10.1145/3576915.3623157\n\ninsecure\n\ncode with AI\n\n[23] Y. Huang, Y. Chen, X. Chen,\n\nJ. Chen, R. Peng, Z. Tang, software [Online].\n\n“Generative J. Huang, F. Xu, engineering,” CoRR, 2024. vol. Available: https://doi.org/10.48550/arXiv.2403.02583\n\nand Z. Zheng,\n\nabs/2403.02583,",
      "content_length": 7267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "12\n\n[24] S. Choi and D. Mohaisen, “Attributing chatgpt-generated source codes,” IEEE Transactionson Dependable and Secure Computing, 2025. [25] S. Choi, Y. K. Tan, M. H. Meng, M. Ragab, S. Mondal, D. Mohaisen, and K. M. M. Aung, “I can ﬁnd you in seconds! leveraging large language models for code authorship attribution,” arXiv preprint arXiv:2501.08165, 2025.\n\n[47] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, and J. Gao, “Large language models: A survey,” CoRR, vol. abs/2402.06196, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2402.06196\n\n[48] , “Pieces for developers — ai-enabled developer productivity,”\n\nhttps://pieces.app/, 05 2024, (Accessed on 05/12/2024).\n\n[26] M. Omar and D. Mohaisen, “Making adversarially-trained lan- guage models forget with model retraining: A case study on hate speech detection,” in Companion Proceedings of the Web Conference 2022, 2022, pp. 887–893.\n\n[27] , “Code quality, security & static analysis tool with sonarqube — sonar,” https://www.sonarsource.com/products/sonarqube/, 05 2024, (Accessed on 05/12/2024).\n\n[49] ,\n\npair https://github.com/features/copilot, 05 2024, 05/12/2024).\n\n“Github\n\ncopilot\n\n\n\nyour\n\nai\n\nprogrammer,” (Accessed on\n\n[50] , “Ai\n\naws,” https://aws.amazon.com/codewhisperer/, 05 2024, (Accessed on 05/12/2024).\n\ncode generator\n\namazon codewhisperer\n\n[51] , “Tabnine ai coding assistant — private, personalized, protected,”\n\n[28] , “Gpt-4 — openai,” https://openai.com/index/gpt-4/, 05 2024,\n\n(Accessed on 05/12/2024).\n\n[29] , “Perplexity,” https://www.perplexity.ai/, 09 2024, (Accessed on\n\n09/05/2024).\n\n[30] , “Claude,” https://claude.ai/new, 09 2024,\n\n(Accessed on\n\n09/05/2024).\n\n[31] , “Mistral ai — frontier ai in your hands,” https://mistral.ai/, 05\n\nhttps://www.tabnine.com/, 05 2024, (Accessed on 05/12/2024). intelligent https://www.ﬁgstack.com/, 05 2024, (Accessed on 05/12/2024).\n\n[52] ,\n\n“Figstack:\n\nYour\n\ncoding\n\ncompanion,”\n\n[53] H. Vasconcelos, G. Bansal, A. Fourney, Q. V. Liao, and J. W. Vaughan, “Generation probabilities are not enough: Exploring the effectiveness of uncertainty highlighting in ai-powered code completions,” CoRR, vol. abs/2302.07248, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2302.07248\n\n2024, (Accessed on 05/12/2024). -\n\n[32] ,\n\nideas,” https://gemini.google.com/, 05 2024, (Accessed on 05/12/2024). [33] , “Most popular programming languages in 2024 & beyond,”\n\n“Gemini\n\nchat\n\nto\n\nsupercharge\n\nyour\n\n[54] , “Cwe - new to cwe,” https://cwe.mitre.org/about/new to cwe.html,\n\n05 2024, (Accessed on 05/12/2024).\n\nhttps://www.orientsoftware.com/blog/most-popular-programming-languages/, 02 2024, (Accessed on 05/12/2024).\n\n[34] ,\n\n“C++\n\nprogramming\n\n\n\ndevel- infographic — jetbrains: teams,” and 09\n\nthe\n\nstate\n\nof\n\noper Developer https://www.jetbrains.com/lp/devecosystem-2023/cpp/, 2024, (Accessed on 09/07/2024).\n\necosystem in\n\n2023\n\ntools\n\nfor\n\nprofessionals\n\n[35] A. H. Watson, D. R. Wallace, and T. J. McCabe, Structured testing: A testing methodology using the cyclomatic complexity metric. US Department of Commerce, Technology Administration, National Institute of . . . , 1996, vol. 500, no. 235.\n\n[36] N. E. Fenton and M. Neil, “A critique of software defect prediction models,” IEEE Trans. Software Eng., vol. 25, no. 5, pp. 675–689, 1999. [Online]. Available: https://doi.org/10.1109/32.815326 [37] M. M. Bar´on, M. Wyrich, and S. Wagner, “An empirical validation of cognitive complexity as a measure of source code understandability,” in ESEM. ACM, 2020, pp. 5:1–5:12. [Online]. Available: https://doi.org/10.1145/3382494.3410636\n\n[38] A. Mohaisen, O. Alrawi, and M. Mohaisen, “AMAL: high-ﬁdelity, behavior-based automated malware analysis and classiﬁcation,” Comput. Secur., vol. 52, pp. 251–266, 2015. [Online]. Available: https://doi.org/10.1016/j.cose.2015.04.001\n\n[39] H. Alasmary, A. Khormali, A. Anwar,\n\nJ. Choi, A. Abusnaina, A. Awad, D. Nyang, and A. Mohaisen, “Analyzing things malware: A graph-based approach,” IEEE Internet Things J., vol. 6, no. 5, pp. 8977–8988, 2019. [Online]. Available: https://doi.org/10.1109/JIOT.2019.2925929\n\nJ. Park,\n\nand detecting\n\nemerging\n\ninternet\n\nof\n\n[40] , “Problems - leetcode,” https://leetcode.com/problemset/, 07\n\n2024, (Accessed on 09/09/2024).\n\n[41] , “Edabit // learn to code with 10,000+ interactive challenges,”\n\nhttps://edabit.com/, 07 2024, (Accessed on 09/09/2024).\n\n[42] , “Codewars - achieve mastery through coding practice and developer mentorship,” https://www.codewars.com/, 07 2024, (Accessed on 09/09/2024).\n\n[43] M. L. Siddiq and J. C. Santos, “Securityeval dataset: mining vul- nerability examples to evaluate machine learning-based code gen- eration techniques,” in Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security, 2022, pp. 29–33.\n\n[44] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhari- wal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Lan- guage models are few-shot learners,” Advances in neural informa- tion processing systems, vol. 33, pp. 1877–1901, 2020.\n\n[45] A. Trisovic, M. K. Lau, T. Pasquier, and M. Crosas, “A large-scale study on research code quality and execution,” Scientiﬁc Data, vol. 9, no. 1, p. 60, 2022.\n\n[46] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie, “A survey on evaluation of large language models,” CoRR, vol. abs/2307.03109, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2307.03109",
      "content_length": 5558,
      "extraction_method": "Unstructured"
    }
  ]
}