{
  "metadata": {
    "title": "Machine Learning in Production - Suhas Pote",
    "author": "Suhas Pote",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 462,
    "conversion_date": "2025-12-19T17:34:36.835909",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Machine Learning in Production - Suhas Pote.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 2-12)",
      "start_page": 2,
      "end_page": 12,
      "detection_method": "topic_boundary",
      "content": "Machine Learning in Production\n\nMaster the art of delivering robust Machine Learning solutions with MLOps\n\nSuhas Pote\n\nwww.bpbonline.com\n\n i\n\nii \n\nCopyright © 2023 BPB Online\n\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.\n\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the author, nor BPB Online or its dealers and distributors, will be held liable for any damages caused or alleged to have been caused directly or indirectly by this book.\n\nBPB Online has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, BPB Online cannot guarantee the accuracy of this information.\n\nFirst published: 2023\n\nPublished by BPB Online WeWork 119 Marylebone Road London NW1 5PU\n\nUK | UAE | INDIA | SINGAPORE\n\nISBN 978-93-55518-101\n\nwww.bpbonline.com\n\nDedicated to\n\nMy parents:\n\nI am so grateful for your belief in me, and\n\nfor the many sacrifices you have made throughout\n\nmy life. Your love and support mean the world to me.\n\n iii\n\niv \n\nAbout the Author\n\nSuhas Pote has over eight years of multidisciplinary experience in data science, playing central roles in numerous projects as a technical leader and data scientist, delivering projects using open-source technologies for big companies, including successful projects in South America, Europe, and the United States. He is experienced in client engagement and working collaboratively with different teams. Currently, he is a process manager at Eclerx and is an accomplished postgraduate, having completed a degree in Data Science, Business Analytics, and Big Data. He holds a Bachelor's degree focused on Electronics and Telecommunication Engineering. In the meantime, he successfully got many certifications in data science and related tools. Furthermore, the author participates as a speaker in Data Science conferences and writes technical articles on machine learning and related topics. He also contributes to technical communities worldwide, such as Stack Overflow.\n\nAbout the Reviewer\n\nKaran Vijay Singh is a passionate Big Data / ML Engineer with extensive professional experience in the domain of Cloud Cost Optimization. With his optimisation skills, he has saved his current organization 0.5M dollar a year in cost. He also has co-authored a research paper in 2019 in the field of cloud computing which has over 270 citations worldwide.\n\nKaran did his Master’s in Mathematics (Computer Science) from University of Waterloo in Canada. He is currently working as a Data Engineer at Lightspeed Commerce in Canada.\n\n v\n\nvi \n\nAcknowledgement\n\nI want to express my deepest gratitude to my family and friends for their unwavering support and encouragement throughout this book's writing, especially my wife, Dr. Archana and my son, Eshansh.\n\nI am also grateful to BPB Publications for their guidance and expertise in bringing this book to fruition. It was a long journey of revising this book, with valuable participation and collaboration of reviewers, technical experts, and editors.\n\nI would also like to acknowledge the valuable contributions of my colleagues and co-worker during many years working in the tech industry, who have taught me so much and provided valuable feedback on my work.\n\nFinally, I would like to thank all the readers who have taken an interest in my book and for their support in making it a reality. Your encouragement has been invaluable.\n\nPreface\n\nProductionizing the machine learning model is a complex task that requires a comprehensive understanding of the latest technologies and CI/CD pipeline. MLOps become increasingly popular in the field of Data Science.\n\nThis book is designed to provide a comprehensive guide to building and deploying ML applications with MLOps. It covers a wide range of topics, including the basics of Python programming, Git, Machine Learning life cycle, Docker, and advanced concepts such as packaging Python code for ML models, monitoring, model security, Kubernetes, testing using pytest and the use of CI/CD pipeline for building and deploying robust and scalable ML applications on cloud platforms, including Azure, GCP, and AWS.\n\nThroughout the book, you will learn about the MLOps, various tools, and techniques to deploy ML models. You will also learn how to use them to productionize ML models and applications that are efficient, scalable, and easy to maintain. Additionally, you will learn about best practices and design patterns for MLOps.\n\nThis book is intended for data scientists, software developers, data engineers, data analysts, and managers who are new to MLOps and want to learn how to productionize ML models. It is also helpful for experienced data scientists and ML engineers who want to expand their knowledge of these technologies and improve their skills in deploying ML models in production.\n\nWith this book, you will gain the knowledge and skills to become proficient in the field of MLOps. I hope you will find this book informative and helpful.\n\nChapter 1: Python 101 – explains the Python fundamental concepts needed for the reader to equip with the prerequisites required for this book, including Python installation, data structures, control statements, loops, functions, and data manipulation using pandas. This is a quick refresher for those who have worked with Python. If you are a beginner, this chapter will cover the most common Python commands needed to build and deploy ML models in production. It allows the reader to learn fundamental concepts related to the Object-Oriented Programming paradigm using Python.\n\nChapter 2: Git and GitHub Fundamentals – presents a detailed overview of Git workflow, including common Git commands with practical examples. This is\n\n vii\n\nviii \n\nessential content for the entire book, as this chapter covers fundamental aspects of Git and GitHub that influence technical decisions to build the CI/CD pipelines to deploy ML models in the production environment.\n\nChapter 3: Challenges in ML Model Deployment – covers the various stages of the ML life cycle, including details on the challenges of each stage. It also covers the common challenges in deploying models in the production environment and how MLOps can help to overcome them. Additionally, the chapter discusses different approaches to deploying ML models in production.\n\nChapter 4: Packaging ML Models – allows the reader to learn fundamental concepts related to modular programming using the Python language, including Python packaging, dependency management, and good practices of software development to develop stable, readable, and extensible code for robust enterprise applications. Furthermore, the chapter explains the virtual environment, testing code using pytest, serializing, and deserializing ML models. It covers packaging ML models, code, and dependencies so that the package can be installed and consumed on another machine or server.\n\nChapter 5: MLflow-Platform to Manage the ML Life Cycle – gives special attention to streamlining machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models. It demonstrates how to train, deploy, and reuse ML models using MLflow through practical examples based on the use case. This chapter explains the role of MLflow in an ML life cycle.\n\nChapter 6: Docker for ML – shows the basic concepts of Docker and provides practical examples of common Docker commands with a use case for the reader. Learning these commands allows the reader to package ML code with its dependencies and run the application inside Docker containers. This chapter includes practical examples of Docker objects such as Docker images and containers.\n\nChapter 7: Build ML Web Apps Using API – explains in detail the most commonly used frameworks for building web-based ML apps using the Python language, including FastAPI, Streamlite, and Flask. It also allows the reader to learn the basics of Gunicorn, NGINX, and APIs, including an explanation of REST APIs, and much more.\n\nChapter 8: Build Native ML Apps – is dedicated to building native ML applications in Python to give the reader more familiarity with converting Python apps into Windows and Android apps and ways to consume them. This chapter covers practical examples of working with Tkinter, kivy, kivyMD, pyinstaller, and buildozer.\n\nChapter 9: CI/CD for ML – allows the reader to learn and implement the different stages of the CI/CD pipeline using GitHub and Jenkins, including committing, building, testing, and deploying. This chapter explains the GitHub and Jenkins integrations to build an automated CI/CD pipeline for deploying ML apps using Python and Docker.\n\nChapter 10: Deploying ML Models on Heroku – will guide the reader in configuring and building a CI/CD pipeline using GitHub Actions to deploy a web-based ML app on the Heroku platform. This chapter also explains three methods for deploying the web app on Heroku: Heroku Git, GitHub integration, and Container registry. It covers practical examples for creating workflow (YAML) files for GitHub Actions, including using tox and pytest for testing the code, and an automated Heroku pipeline for faster deployments.\n\nChapter 11: Deploying ML Models on Microsoft Azure – explains the step-by- step approach to building CI/CD pipeline using GitHub Actions to deploy web- based ML apps to Azure web services. In the other approach, the reader will learn to build a CI/CD pipeline and deploy web-based scalable ML apps to Azure Container Instances (ACI) and Azure Kubernetes Service (AKS) using Azure DevOps and Azure Machine Learning (AML) Service.\n\nChapter 12: Deploying ML Models on Google Cloud Platform – Shows how to build an automated CI/CD pipeline to deploy ML models on Google Kubernetes Engine (GKE), without the need to integrate any external tool, service, or platform. This chapter allows the reader to learn and implement Kubernetes functionality to run scalable ML apps using Google Kubernetes Engine (GKE).\n\nChapter 13: Deploying ML Models on Amazon Web Services – presents an overview of various cloud compute services offered by Amazon Web Services (AWS). This chapter allows the reader to learn and implement an automated CI/CD pipeline with Continuous Training (CT) to deploy scalable enterprise ML apps on Amazon Elastic Container Service (ECS). Additionally, it covers the integration of Application Load Balancer (ALB), Amazon Virtual Private Cloud\n\n ix\n\nx \n\n(Amazon VPC), and security groups into Amazon Elastic Container Service (ECS) for building robust and secure enterprise ML apps.\n\nChapter 14: Monitoring and Debugging – is dedicated to monitoring ML and operational metrics, including servers, cost of services, drifts in ML, input data, ML models, and much more. This chapter shows the importance and fundamental concepts of monitoring in the ML life cycle. It also covers practical examples using whylogs and WhyLabs for ML model monitoring, and Prometheus and Grafana for operational monitoring.\n\nChapter 15: Post-Productionizing ML Models – presents a detailed overview of adversarial machine learning, including different types of adversarial attacks and how to mitigate them. It also covers fundamental concepts of A/B testing and the future scope of MLOps in the industry.\n\nCode Bundle and Coloured Images Please follow the link to download the Code Bundle and the Coloured Images of the book:\n\nhttps://rebrand.ly/vlv0nzp\n\nThe is also hosted on GitHub at https://github.com/bpbpublications/Machine-Learning-in-Production. In case there's an update to the code, it will be updated on the existing GitHub repository.\n\ncode bundle\n\nfor\n\nthe book\n\nWe have code bundles from our rich catalogue of books and videos available at https://github.com/bpbpublications. Check them out!\n\nErrata\n\nWe take immense pride in our work at BPB Publications and follow best practices to ensure the accuracy of our content to provide with an indulging reading experience to our subscribers. Our readers are our mirrors, and we use their inputs to reflect and improve upon human errors, if any, that may have occurred during the publishing processes involved. To let us maintain the quality and help us reach out to any readers who might be having difficulties due to any unforeseen errors, please write to us at :\n\nerrata@bpbonline.com\n\nYour support, suggestions and feedbacks are highly appreciated by the BPB Publications’ Family.\n\nDid you know that BPB offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at www.bpbonline.com and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at :\n\nbusiness@bpbonline.com for more details.\n\nAt www.bpbonline.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on BPB books and eBooks.\n\n xi",
      "page_number": 2
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 13-20)",
      "start_page": 13,
      "end_page": 20,
      "detection_method": "topic_boundary",
      "content": "xii \n\nPiracy\n\nIf you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at business@bpbonline.com with a link to the material.\n\nIf you are interested in becoming an author\n\nIf there is a topic that you have expertise in, and you are interested in either writing or contributing to a book, please visit www.bpbonline.com. We have worked with thousands of developers and tech professionals, just like you, to help them share their insights with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.\n\nReviews\n\nPlease leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions. We at BPB can understand what you think about our products, and our authors can see your feedback on their book. Thank you!\n\nFor more information about BPB, please visit www.bpbonline.com.\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com\n\nTable of Contents\n\n1. Python 101 .....................................................................................................................1\n\nIntroduction ............................................................................................................1\n\nStructure ..................................................................................................................1\n\nObjectives ................................................................................................................2\n\nInstall Python .........................................................................................................2\n\nOn Windows and Mac OS ................................................................................2\n\nOn Linux ...........................................................................................................2\n\nInstall Anaconda ....................................................................................................2\n\nInstall code editor ..................................................................................................3\n\nHello World! ...........................................................................................................3\n\nExecution of Python file .....................................................................................3\n\nData structures .......................................................................................................4\n\nCommon data structures ...................................................................................4\n\nControl statements and loops ..............................................................................6\n\nFunctions .................................................................................................................8\n\nObject Oriented Programming (OOP) ................................................................9\n\nNumerical Python (NumPy) ..............................................................................10\n\nPandas ...................................................................................................................12\n\nConclusion ............................................................................................................15\n\nPoints to remember .............................................................................................15\n\nMultiple choice questions ...................................................................................16\n\nAnswers ...........................................................................................................16\n\nQuestions ..............................................................................................................16\n\nKey terms ..............................................................................................................16\n\n2. Git and GitHub Fundamentals ...............................................................................17\n\nIntroduction ..........................................................................................................17\n\nStructure ................................................................................................................17\n\nObjectives ..............................................................................................................18\n\nGit concepts ..........................................................................................................18\n\n xiii\n\nxiv \n\nGit + Hub = GitHub ........................................................................................19\n\nCommon Git workflow .......................................................................................19\n\nInstall Git and create a GitHub account ...........................................................20\n\nLinux (Debian/Ubuntu) ..................................................................................20\n\nGit for all platforms .........................................................................................20\n\nGUI clients .......................................................................................................20\n\nCreate a GitHub account .................................................................................20\n\nCommon Git commands .....................................................................................21\n\nSetup ................................................................................................................21\n\nNew repository .................................................................................................21\n\nUpdate ..............................................................................................................21\n\nChanges ............................................................................................................21\n\nRevert ...............................................................................................................22\n\nLet’s Git .................................................................................................................22\n\nConfiguration ...................................................................................................22\n\nInitialize the Git repository ..............................................................................22\n\nCheck Git status ...............................................................................................22\n\nAdd a new file ..................................................................................................23\n\nConclusion ............................................................................................................26\n\nPoints to remember .............................................................................................26\n\nMultiple choice questions ..................................................................................26\n\nAnswers ...........................................................................................................27\n\n3. Challenges in ML Model Deployment .................................................................29\n\nIntroduction ..........................................................................................................29\n\nStructure ................................................................................................................29\n\nObjectives ..............................................................................................................30\n\nML life cycle ..........................................................................................................30\n\nBusiness impact ...............................................................................................31\n\nData collection .................................................................................................31\n\nData preparation ..............................................................................................32\n\nFeature engineering .........................................................................................32\n\nBuild and train the model ................................................................................33\n\nTest and evaluate ..............................................................................................34\n\nModel deployment ............................................................................................34\n\nMonitoring and optimization ..........................................................................35\n\nTypes of model deployment ...............................................................................35\n\nBatch predictions .............................................................................................35\n\nWeb service/REST API ....................................................................................36\n\nMobile and edge devices ...................................................................................36\n\nReal-time ..........................................................................................................37\n\nChallenges in deploying models in the production environment ...............37\n\nTeam coordination ............................................................................................37\n\nData-related challenges ....................................................................................38\n\nPortability ........................................................................................................38\n\nScalability ........................................................................................................38\n\nRobustness .......................................................................................................38\n\nSecurity ............................................................................................................39\n\nMLOps ...................................................................................................................39\n\nBenefits of MLOps ...............................................................................................40\n\nEfficient management of ML life cycle .............................................................41\n\nReproducibility ................................................................................................41\n\nAutomation ......................................................................................................41\n\nTracking and feedback loop ..............................................................................42\n\nConclusion ............................................................................................................42\n\nPoints to remember .............................................................................................42\n\nMultiple choice questions ...................................................................................43\n\nAnswers ...........................................................................................................43\n\nQuestions ..............................................................................................................43\n\nKey terms ..............................................................................................................43\n\n4. Packaging ML Models ..............................................................................................45\n\nIntroduction ..........................................................................................................45\n\nStructure ................................................................................................................45\n\nObjectives ..............................................................................................................46\n\nVirtual environments ..........................................................................................46\n\n xv\n\nxvi \n\nRequirements file .................................................................................................47\n\nSerializing and de-serializing ML models .......................................................48\n\nTesting Python code with pytest .......................................................................48\n\npytest fixtures ..................................................................................................49\n\nPython packaging and dependency management .........................................49\n\nModular programming ....................................................................................50\n\nModule .............................................................................................................50\n\nPackage ............................................................................................................50\n\nDeveloping, building, and deploying ML packages ......................................51\n\nBusiness problem .............................................................................................52\n\nData .................................................................................................................52\n\nBuilding the ML model ....................................................................................53\n\nDeveloping the package ....................................................................................53\n\nSet up environment variables and paths ..........................................................70\n\nBuild the package .............................................................................................71\n\nInstall the package ............................................................................................71\n\nPackage usage with example ............................................................................73\n\nConclusion ............................................................................................................74\n\nPoints to remember .............................................................................................74\n\nMultiple choice questions ...................................................................................74\n\nAnswers ...........................................................................................................75\n\nQuestions ..............................................................................................................75\n\nKey terms ..............................................................................................................75\n\n5. MLflow-Platform to Manage the ML Life Cycle .................................................77\n\nIntroduction ..........................................................................................................77\n\nStructure ................................................................................................................77\n\nObjectives ..............................................................................................................78\n\nIntroduction to MLflow ......................................................................................78\n\nSet up your environment and install MLflow .................................................79\n\nMiniconda installation ..........................................................................................79\n\nMLflow components ........................................................................................82\n\nMLflow tracking ..................................................................................................83\n\n xvii\n\nLog data into the run .......................................................................................84\n\nMLflow projects ...................................................................................................94\n\nMLflow models ....................................................................................................97\n\nMLflow registry .................................................................................................100\n\nSet up the MySQL server for MLflow ...........................................................101\n\nStart the MLflow server .................................................................................103\n\nConclusion ..........................................................................................................109\n\nPoints to remember ...........................................................................................109\n\nMultiple choice questions .................................................................................109\n\nAnswers ......................................................................................................... 110\n\nQuestions ............................................................................................................ 110\n\n6. Docker for ML .......................................................................................................... 111\n\nIntroduction .........................................................................................................111\n\nStructure ...............................................................................................................111\n\nObjectives ............................................................................................................ 112\n\nIntroduction to Docker ...................................................................................... 112\n\nIt works on my machine! ............................................................................... 112\n\nLong setup ...................................................................................................... 112\n\nSetting up your environment and installing Docker .................................... 113\n\nDocker installation ......................................................................................... 113\n\nUninstall old versions ......................................................................................... 113\n\nInstall Docker Engine .................................................................................... 114\n\nDocker compose .............................................................................................. 115\n\nHello World with Docker ................................................................................. 116\n\nDocker objects .................................................................................................... 117\n\nDockerfile ............................................................................................................. 117\n\nDocker image ....................................................................................................... 117\n\nDocker containers ................................................................................................ 118\n\nDocker container networking .............................................................................. 118\n\nCreate a Dockerfile ............................................................................................ 119\n\nBuild a Docker image ........................................................................................120\n\nRun a Docker container ....................................................................................120\n\nxviii \n\nDockerize and deploy the ML model .............................................................122\n\nCommon Docker commands ...........................................................................127\n\nConclusion ..........................................................................................................127\n\nPoints to remember ...........................................................................................127\n\nMultiple choice questions .................................................................................128\n\nAnswers .........................................................................................................128\n\nQuestions ............................................................................................................128\n\n7. Build ML Web Apps Using API ...........................................................................129\n\nIntroduction ........................................................................................................129\n\nStructure ..............................................................................................................129\n\nObjectives ............................................................................................................130\n\nRest APIs .............................................................................................................130\n\nFastAPI ................................................................................................................131\n\nStreamlit ..............................................................................................................139\n\nFlask .....................................................................................................................143\n\nGunicorn ........................................................................................................150\n\nNGINX .........................................................................................................150\n\nConclusion ..........................................................................................................154\n\nPoints to remember ...........................................................................................154\n\nMultiple choice questions .................................................................................155\n\nAnswers .........................................................................................................155\n\n8. Build Native ML Apps ...........................................................................................157\n\nIntroduction ........................................................................................................157\n\nStructure ..............................................................................................................157\n\nObjectives ............................................................................................................158\n\nIntroduction to Tkinter .....................................................................................158\n\nHello World app using Tkinter ......................................................................159\n\nBuild an ML-based app using Tkinter ............................................................160\n\nTkinter app .....................................................................................................163\n\nConvert Python app into Windows EXE file .................................................167\n\nBuild an ML-based app using kivy and kivyMD .........................................170\n\nKivyMD app ..................................................................................................173\n\nConvert the Python app into an Android app ..............................................179\n\nConclusion ..........................................................................................................180\n\nPoints to remember ...........................................................................................181\n\nMultiple choice questions .................................................................................181\n\nAnswers .........................................................................................................181\n\nQuestions ............................................................................................................181\n\n9. CI/CD for ML ...........................................................................................................183\n\nIntroduction ........................................................................................................183\n\nStructure ..............................................................................................................183\n\nObjectives ............................................................................................................184\n\nCI/CD pipeline for ML .....................................................................................184\n\nContinuous Integration (CI) .............................................................................185\n\nContinuous Delivery/Deployment (CD) .......................................................186\n\nContinuous Training (CT) ................................................................................186\n\nIntroduction to Jenkins .....................................................................................187\n\nInstallation .....................................................................................................187\n\nBuild CI/CD pipeline using GitHub, Docker, and Jenkins .........................189\n\nDevelop codebase ............................................................................................189\n\nCreate a Personal Access Token (PAT) on GitHub ........................................196\n\nCreate a webhook on the GitHub repository ..................................................197\n\nConfigure Jenkins ..............................................................................................199\n\nCreate CI/CD pipeline using Jenkins .............................................................202\n\nStage 1: 1-GitHub-to-container .....................................................................203\n\nStage 2: 2-training .........................................................................................205\n\nStage 3: 3-testing ...........................................................................................207\n\nStage 4: 4-deployment-status-email ..............................................................210\n\nConclusion ..........................................................................................................217\n\nPoints to remember ...........................................................................................217\n\nMultiple choice questions .................................................................................218\n\nAnswers .........................................................................................................218\n\nQuestions ............................................................................................................218\n\n xix",
      "page_number": 13
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 21-28)",
      "start_page": 21,
      "end_page": 28,
      "detection_method": "topic_boundary",
      "content": "xx \n\n10. Deploying ML Models on Heroku ......................................................................219\n\nIntroduction ........................................................................................................219\n\nStructure ..............................................................................................................219\n\nObjectives ............................................................................................................220\n\nHeroku .................................................................................................................220\n\nSetting up Heroku .............................................................................................221\n\nDeployment with Heroku Git ..........................................................................222\n\nDeployment with GitHub repository integration .........................................222\n\nREVIEW APPS .............................................................................................223\n\nSTAGING ......................................................................................................223\n\nPRODUCTION ............................................................................................224\n\nHeroku Pipeline flow ......................................................................................224\n\nDeployment with Container Registry.............................................................225\n\nGitHub Actions ..................................................................................................225\n\nConfiguration .................................................................................................226\n\nCI/CD pipeline using GitHub Actions and Heroku ....................................227\n\nConclusion ..........................................................................................................246\n\nPoints to remember ...........................................................................................246\n\nMultiple choice questions .................................................................................247\n\nAnswers .........................................................................................................247\n\nQuestions ............................................................................................................247\n\n11. Deploying ML Models on Microsoft Azure .......................................................249\n\nIntroduction ........................................................................................................249\n\nStructure ..............................................................................................................249\n\nObjectives ............................................................................................................250\n\nAzure ...................................................................................................................250\n\nSet up an Azure account ...................................................................................251\n\nDeployment using GitHub Actions ................................................................251\n\nInfrastructure setup .......................................................................................263\n\nAzure Container Registry ...................................................................................263\n\nAzure App Service ...............................................................................................267\n\nGitHub Actions .............................................................................................270\n\nService principal ............................................................................................273\n\nConfigure Azure App Service to use GitHub Actions for CD ......................274\n\nDeployment using Azure DevOps and Azure ML .......................................278\n\nAzure Machine Learning (AML) service ........................................................279\n\nWorkspace ......................................................................................................280\n\nExperiments ...................................................................................................280\n\nRuns ...............................................................................................................280\n\nConfigure CI pipeline ........................................................................................290\n\nConfigure CD pipeline ......................................................................................292\n\nConclusion ..........................................................................................................299\n\nPoints to remember ...........................................................................................299\n\nMultiple choice questions .................................................................................299\n\nAnswers .........................................................................................................300\n\nQuestions ............................................................................................................300\n\n12. Deploying ML Models on Google Cloud Platform ..........................................301\n\nIntroduction ........................................................................................................301\n\nStructure ..............................................................................................................301\n\nObjectives ............................................................................................................302\n\nGoogle Cloud Platform (GCP) .........................................................................302\n\nSet up the GCP account .................................................................................303\n\nCloud Source Repositories ...............................................................................304\n\nCloud Build ........................................................................................................309\n\nContainer Registry ............................................................................................. 311\n\nKubernetes ..........................................................................................................312\n\nGoogle Kubernetes Engine (GKE) ...................................................................313\n\nDeployment using Cloud Shell – Manual Trigger ........................................316\n\nCI/CD pipeline using Cloud Build .................................................................317\n\nCreate a trigger in Cloud Build .....................................................................318\n\nConclusion ..........................................................................................................323\n\nPoints to remember ...........................................................................................323\n\nMultiple choice questions .................................................................................323\n\nAnswers .........................................................................................................324\n\n xxi\n\nxxii \n\nQuestions ............................................................................................................324\n\n13. Deploying ML Models on Amazon Web Services ............................................325\n\nIntroduction ........................................................................................................325\n\nStructure ..............................................................................................................325\n\nObjectives ............................................................................................................326\n\nIntroduction to Amazon Web Services (AWS) ...............................................326\n\nAWS compute services ...................................................................................326\n\nAmazon Elastic Compute Cloud (EC2) .........................................................327\n\nAmazon Elastic Container Service (ECS) .......................................................327\n\nAmazon Elastic Kubernetes Service (EKS) ....................................................327\n\nAmazon Elastic Container Registry (ECR) ....................................................327\n\nAWS Fargate ..................................................................................................328\n\nAWS Lambda .................................................................................................328\n\nAmazon SageMaker ..........................................................................................328\n\nSet up an AWS account .....................................................................................329\n\nAWS CodeCommit ............................................................................................331\n\nContinuous Training .....................................................................................336\n\nAmazon Elastic Container Registry (ECR) ....................................................336\n\nDocker Hub rate limit ....................................................................................337\n\nAWS CodeBuild .................................................................................................339\n\nAttach container registry access to CodeBuild’s service role .........................345\n\nAmazon Elastic Container Service (ECS) .......................................................346\n\nAWS ECS deployment models .......................................................................347\n\nEC2 instance .......................................................................................................347\n\nFargate .................................................................................................................348\n\nTask definition ................................................................................................349\n\nRunning task with the task definition .................................................................350\n\nAmazon VPC and subnets ..................................................................................351\n\nLoad balancing ...............................................................................................352\n\nTarget group ........................................................................................................353\n\nSecurity Groups ..................................................................................................356\n\nApplication Load Balancers (ALB) .....................................................................358\n\n xxiii\n\nService ............................................................................................................363\n\nCI/CD pipeline using CodePipeline ..............................................................369\n\nAWS CodePipeline .........................................................................................371\n\nMonitoring ..........................................................................................................378\n\nConclusion ..........................................................................................................379\n\nPoints to remember ...........................................................................................379\n\nMultiple choice questions .................................................................................380\n\nAnswers .........................................................................................................380\n\nQuestions ............................................................................................................380\n\n14. Monitoring and Debugging ..................................................................................381\n\nIntroduction ........................................................................................................381\n\nStructure ..............................................................................................................381\n\nObjectives ............................................................................................................382\n\nImportance of monitoring ................................................................................382\n\nFundamentals of ML monitoring ....................................................................383\n\nMetrics for monitoring your ML system ........................................................385\n\nDrift in ML ..........................................................................................................386\n\nTypes of drift in ML .......................................................................................386\n\nTechniques to detect the drift in ML ..............................................................388\n\nAddressing the drift in ML ............................................................................389\n\nOperational monitoring with Prometheus and Grafana ..............................390\n\nML model monitoring with whylogs and WhyLabs ....................................402\n\nwhylogs ..........................................................................................................403\n\nConstraints for data quality validation .........................................................404\n\nWhyLabs ........................................................................................................407\n\nConclusion ..........................................................................................................416\n\nPoints to remember ...........................................................................................416\n\nMultiple choice questions .................................................................................417\n\nAnswers .........................................................................................................417\n\nQuestions ............................................................................................................417\n\nxxiv \n\n15. Post-Productionizing ML Models ........................................................................419\n\nIntroduction ........................................................................................................419\n\nStructure ..............................................................................................................419\n\nObjectives ............................................................................................................420\n\nBridging the gap between the ML model and the creation of business value ....................................................................................................420\n\nModel security ....................................................................................................420\n\nAdversarial attack ..........................................................................................421\n\nData poisoning attack ....................................................................................422\n\nDistributed Denial of Service attack (DDoS) ................................................422\n\nData privacy attack ........................................................................................422\n\nMitigate the risk of model attacks ..................................................................423\n\nA/B testing .........................................................................................................423\n\nMLOps is the future ..........................................................................................424\n\nConclusion ..........................................................................................................425\n\nPoints to remember ...........................................................................................425\n\nMultiple choice questions .................................................................................425\n\nAnswers .........................................................................................................426\n\nQuestions ............................................................................................................426\n\nIndex ...................................................................................................................427-434\n\nPython 101  1\n\nChapter 1 Python 101\n\nIntroduction Python 101 guides you with everything from the installation of Python to data manipulation in Pandas DataFrame. In a nutshell, this chapter is designed to equip you with the prerequisites required for this book. It is a quick refresher for those who have worked with Python. And if you are a beginner, this chapter is going to cover the most common Python commands required to build and deploy machine models.\n\nStructure\n\nThis chapter covers the following topics: • Installation of Python\n\nHello World! •\t Data structures •\t Control statements and loops •\t Functions •\t OOPs •\t NumPy •\t Pandas\n\n2  Machine Learning in Production\n\nObjectives After studying this chapter, you should be able to install Python on your machine, and store and manage data using common data structures in Python. You should be able to use Python's data processing packages for data manipulation, and you should also know how to create classes, methods, and objects.\n\nInstall Python Make sure the Python version you are installing is compatible with the libraries and applications you will work on. Also, maintain the same Python version throughout the project to avoid any errors or exceptions. Here, you will install Python version 3.6.10.\n\nOn Windows and Mac OS Here’s the link to install Python 3.6.10:\n\nhttps://www.python.org/downloads/release/python-3610/\n\nOn Linux Ubuntu 16.10 and 17.04: sudo apt update\n\nsudo apt install python3.6\n\nUbuntu 17.10, 18.04 (Bionic) and onward:\n\nUbuntu 17.10 and 18.04 already come with Python 3.6 as default. Just run python3 in the terminal to invoke it.\n\nInstall Anaconda Anaconda Individual Edition contains conda and Anaconda Navigator, as well as Python and hundreds of scientific packages. When you install Anaconda, all of these will also get installed.\n\nhttps://docs.anaconda.com/anaconda/install/\n\nNote: Review the system requirements listed on the site before installing Anaconda Individual Edition.\n\nPython 101  3\n\nInstall code editor Any of the following code editors can be chosen; however, the preferred one is Visual Studio Code:\n\nVisual Studio Code (https://code/visualstudio.com)\n\nSublime Text (https://www.sublimetext.com)\n\nNotepad++ (https://notepad-plus-plus.org/downloads/)\n\nHello World! Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its simple, easy-to-learn syntax emphasizes readability and therefore, reduces the cost of program maintenance. Python supports modules and packages, which encourages modular programming and code reusability. It is developed under an OSI-approved open-source license, making it freely usable and distributable, even for commercial use. The Python Package Index (PyPI) hosts thousands of third-party modules for Python.\n\nOpen the terminal and type a python to open the Python console.\n\nNow you are in the Python console.\n\nExample: 1.1 Hello world in Python\n\n1. print(\"Hello World\")\n\n2. Hello World\n\nTo come out of the console you can use:\n\n1. exit()\n\nExecution of Python file Let’s create a file named hello_world.py and add the print(\"Hello World\") to it.\n\nNow, run the file in the terminal:\n\npython hello_world.py\n\nYou should get the following output:\n\nHello World",
      "page_number": 21
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 29-38)",
      "start_page": 29,
      "end_page": 38,
      "detection_method": "topic_boundary",
      "content": "4  Machine Learning in Production\n\nData structures Data structures are nothing but particular ways of storing and managing data in memory so that it can be easily accessed and modified later. Python comes with a comprehensive set of data structures, which play an important role in programming because they are reusable, easily accessible, and manageable.\n\nCommon data structures You will study some of the common data structures in this section.\n\nArray\n\nArrays are collections of homogeneous items. One can use the same data type in a single array.\n\nExample: 1.2 Array data type\n\n1. import array as arr\n\n2.\n\n3. my_array = arr.array(\"i\", (1, 2, 3, 4, 5))\n\n4.\n\n5. print(my_array)\n\n6. array('i', [1, 2, 3, 4, 5])\n\n7.\n\n8. print(my_array[1])\n\n9. 2\n\nDictionary\n\nDictionaries are defined as comma separated key:value pairs enclosed in curly braces.\n\nExample: 1.3 Dictionary data type\n\n1. my_dict = {'name': 'Adam', 'emp_id': 3521, 'dept':'Marketing'}\n\n2. print(my_dict['name'])\n\n3. Adam\n\nList\n\nIt is a collection of heterogeneous items enclosed in square brackets. One can use the same or different data types in a list.\n\nPython 101  5\n\nExample: 1.4 List data type\n\n1. my_list=['apple', 4, 'banana', 6]\n\n2. print(my_list[0])\n\n3. apple\n\nSet\n\nA set is a collection of unique (non-duplicate) elements enclosed in curly braces. A set can take heterogeneous elements.\n\nExample: 1.5 Set data type\n\n1. my_set = {3,5,6}\n\n2. print(my_set)\n\n3. {3, 5, 6}\n\n1. my_set = {1, 2.0, (3,5,6), 'Test'}\n\n2. print(my_set)\n\n3. {1, 2.0, 'Test', (3, 5, 6)}\n\nString\n\nA string is used to store text data. It can be represented by single quotes ('') or double quotes (\"\").\n\nExample: 1.6 String data type\n\n1. print('The cat was chasing the mouse')\n\n2. The cat was chasing the mouse\n\nTuple\n\nTuples are collections of elements surrounded by round brackets (optional), and they are immutable, that is, they cannot be changed.\n\nExample: 1.7 Tuple data type\n\n1. my_tuple_1 = ('apple', 4, 'banana', 6)\n\n2. print(my_tuple_1[0])\n\n3. apple\n\nTuple can be declared without round brackets, as follows:\n\n1. my_tuple_2 = 1, 2.0, (3,5,6), 'Test'\n\n2. print(my_tuple_2[0])\n\n3. 1\n\n6  Machine Learning in Production\n\nControl statements and loops Python has several types of control statements and loops. Let’s look at them.\n\nif…else The if statement executes a block of code if the specified condition is true. Whereas, the else statement executes a block of code if the specified condition is false. Furthermore, you can use the elif (else if) statement to check the new condition if the first condition is false.\n\nSyntax:\n\nif condition1:\n\nCode to be executed\n\nelif condition2:\n\nCode to be executed\n\nelse:\n\nCode to be executed\n\nExample: 1.8 If…else control statement\n\n1. x = 26\n\n2. y = 17\n\n3.\n\n4. if(x > y):\n\n5. print('x is greater than y')\n\n6. elif(x == y):\n\n7. print('x and y are equal')\n\n8. else:\n\n9. print('y is greater than x')\n\n10.\n\n11. x is greater than y\n\nfor loop It is used when you want to iterate over a sequence or iterable such as a string, list, dictionary, or tuple. It will execute a block of code a certain number of times.\n\nSyntax:\n\nPython 101  7\n\nfor val in sequence:\n\nCode to be executed\n\nExample: 1.9 For loop\n\n1. num = 2\n\n2. for i in range(1, 11):\n\n3. print(num, 'X', i, '=', num*i)\n\n4.\n\n5. 2 X 1 = 2\n\n6. 2 X 2 = 4\n\n7. 2 X 3 = 6\n\n8. 2 X 4 = 8\n\n9. 2 X 5 = 10\n\n10. 2 X 6 = 12\n\n11. 2 X 7 = 14\n\n12. 2 X 8 = 16\n\n13. 2 X 9 = 18\n\n14. 2 X 10 = 20\n\nIn the preceding example, the range() function generates a sequential set of numbers using start, stop, and step parameters.\n\nwhile loop It is used when you want to execute a block of code indefinitely until the specified condition is met. Furthermore, you can use an else statement to execute a block of code once when the first condition is false.\n\nSyntax:\n\nwhile condition:\n\nCode to be executed\n\nExample: 1.10 While loop\n\n1. num = 1\n\n2. while num<= 10:\n\n3. if num % 2 == 0:\n\n4. print(num)\n\n5. num = num + 1\n\n8  Machine Learning in Production\n\n6.\n\n7. 2\n\n8. 4\n\n9. 6\n\n10. 8\n\n11. 10\n\npass statement In Python, the pass statement acts as a placeholder. If you are planning to add code later in loops, function definitions, class definitions, or if statements, you can write a pass to avoid any errors as it does nothing. It allows developers to write the logic or condition afterward and continue the execution of the remaining code.\n\nSyntax:\n\nif condition:\n\npass\n\nExample: 1.11 Pass statement\n\n1. num = 10\n\n2. if num % 2 == 0:\n\n3. pass\n\nFunctions Developers use functions to perform the same task multiple times. A function is a block of code that can take arguments, perform some operations, and return values. Python enables developers to use predefined or built-in functions, or they can write user-defined functions to perform a specific task.\n\nExample: 1.12 Pre-defined or built-in functions\n\n1. print(\"Hello World\")\n\n2. Hello World # Output\n\nExample: 1.13 User-defined functions\n\n1. a = 5\n\n2. b = 2\n\n3.\n\n4. # Function definition\n\nPython 101  9\n\n5. def add():\n\n6. print(a+b)\n\n7.\n\n8. # Calling a function\n\n9. add()\n\n10.\n\n11. # Output\n\n12. 7\n\nObject Oriented Programming (OOP) Python is an object-oriented language, so everything in Python is an object. Let’s understand its key concepts and their importance:\n\nClass: A class acts like a template for the objects. It is defined using the class keyword like the def keyword is used while creating a new function. A Python class contains objects and methods, and they can be accessed using the period (.).\n\nObject: An object is an instance of a class. It depicts the structure of the class and contains class variables, instance variables, and methods.\n\nMethod: In simple words, it is a function defined while creating a class.\n\n__init__: For the automatic initialization of data members by assigning values, you should use the __init__ method while creating an instance of a class. It gets called every time you create an object of the class. The usage is equivalent to the constructor in C++ and Java.\n\nSelf: It helps us access the methods and attributes of the class. You are free to name it anything; however, as per convention and for readability, it is better to declare it as self.\n\nExample: 1.14 Class definition\n\n1. class Multiply:\n\n2. def mul(self):\n\n3. result = self.num1 * self.num2\n\nClass definition\n\n4. return result\n\n5.\n\n6. def __init__(self,num1,num2):\n\n7. self.num1 = num1\n\n8. self.num2 = num2\n\n10  Machine Learning in Production\n\n9.\n\n10.\n\na = Multiply(5,6)\n\n11.\n\nPassing parameters to class\n\n12. a.mul() 13. 30\n\nCalling method of class\n\nNote: Follow standard naming conventions for variables, functions, and methods. For example:\n\nFor variables, functions, methods, packages, and modules: my_variable •\t For classes and exceptions: MyClass •\t For constants: MY_CONSTANT\n\nNumerical Python (NumPy) In a nutshell, it is an array processing package. NumPy stands for Numerical Python. By default, the data type of the NumPy array is defined by its elements. In NumPy, the dimension of arrays is referred to by rank; for example, a 2-D array means Rank 2 array.\n\nNumPy is popular because of its speed. While working on a large dataset, you will see the difference if you use NumPy.\n\nYou can install the NumPy package using pip:\n\npip install numpy\n\nExample: 1.15 NumPy array\n\n1. import numpy as np\n\n2. # Rank 0 Array (scaler)\n\n3. my_narray = np.array(42)\n\n4. print(my_narray)\n\n5. 42\n\n6.\n\n7. print(\"Dimension: \", my_narray.ndim)\n\n8. Dimension: 0\n\n9.\n\n10.\n\nprint(\"Shape: \", my_narray.shape)\n\n11.\n\nShape: ()\n\n12.\n\nPython 101  11\n\n13.\n\nprint(\"Size: \", my_narray.size)\n\n14.\n\nSize: 1\n\n1. # Rank 1 Array (vector)\n\n2. my_narray = np.array([4,5,6])\n\n3. print(my_narray)\n\n4. [4 5 6]\n\n5. print(\"Dimension: \", my_narray.ndim)\n\n6. Dimension: 1\n\n7. print(\"Shape: \", my_narray.shape)\n\n8. Shape: (3,)\n\n9. print(\"Size: \", my_narray.size)\n\n10.\n\nSize: 3\n\n1. # Rank 2 Array (matrix)\n\n2. my_narray = np.array([[1,2,3],[4,5,6]])\n\n3. print(my_narray)\n\n4. [[1 2 3]\n\n5. [4 5 6]]\n\n6. print(\"Dimension: \", my_narray.ndim)\n\n7. Dimension: 2\n\n8. print(\"Shape: \", my_narray.shape)\n\n9. Shape: (2, 3)\n\n10.\n\nprint(\"Size: \", my_narray.size)\n\n11.\n\nSize: 6\n\n1. # Rank 3 Array\n\n2. my_narray = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\n\n3. print(my_narray)\n\n4. [[[ 1 2 3]\n\n5. [ 4 5 6]]\n\n6. [[ 7 8 9]\n\n7. [10 11 12]]]\n\n8. print(\"Dimension: \", my_narray.ndim)\n\n9. Dimension: 3\n\n10.\n\nprint(\"Shape: \", my_narray.shape)\n\n11.\n\nShape: (2, 2, 3)\n\n12  Machine Learning in Production\n\n12.\n\nprint(\"Size: \", my_narray.size)\n\n13.\n\nSize: 12\n\nReshaping array Suppose you want to change the shape of a ndarray (N-dimension array) without losing the data; it can be done using the reshape() or flatten() method.\n\nExample: 1.16 Reshaping NumPy array\n\n1. # Rank 1 Array\n\n2. my_1array = np.array([1,2,3,4,5,6])\n\n3. print(my_1array)\n\n4. [1 2 3 4 5 6]\n\n1. # converting Rank 1 array to Rank 2 array\n\n2. my_2array = my_1array.reshape(2, 3)\n\n3. print(my_2array)\n\n4. [[1 2 3]\n\n5. [4 5 6]]\n\n1. #converting Rank 2 array to Rank 1 array\n\n2. my_1array = my_2array.flatten()\n\n3. print(my_1array)\n\n4. [1 2 3 4 5 6]\n\nNote: Here, you can use my_2array.reshape(6) instead of flatten().\n\nPandas Pandas is an open-source, BSD-licensed library providing high-performance, easy- to-use data structures and data analysis tools for the Python programming language. A DataFrame is a 2-dimensional labeled data structure with columns of potentially different types.\n\nYou can install the Pandas package using pip:\n\npip install pandas\n\nExample: 1.17 Pandas DataFrame usage\n\n1. import pandas as pd\n\n2. df = pd.DataFrame(data={'fruit': ['apple', 'banana', 'orange',\n\nPython 101  13\n\n'mango',\n\n3. 'size': ['large', 'medium', 'small',\n\n'medium'],\n\n4. 'quantity': [5,4,8, 6]})\n\n5.\n\n6. print(df)\n\n7. fruit size quantity\n\n8. 0 apple large 5\n\n9. 1 banana medium 4\n\n10. 2 orange small 8\n\n11. 3 mango medium 6\n\nIn the following example, head() retrieves the top rows of the Pandas DataFrame.\n\nExample: 1.18 Get first n rows of DataFrame\n\n1. df.head(2)\n\n2. fruit size quantity\n\n3. 0 apple large 5\n\n4. 1 banana medium 4\n\nWhereas tail() retrieves the bottom rows of the Pandas DataFrame.\n\nExample: 1.19 Get the last n rows of DataFrame\n\n1. df.tail(2)\n\n2. fruit size quantity\n\n3. 2 orange small 8\n\n4. 3 mango medium 6\n\nIn the following example, describe() shows a quick statistical summary of numerical columns of the DataFrame.\n\nExample: 1.20 Get basic statistical information\n\n1. df.describe()\n\n2. quantity\n\n3. count 4.000000\n\n4. mean 5.750000\n\n5. std 1.707825\n\n6. min 4.000000",
      "page_number": 29
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 39-53)",
      "start_page": 39,
      "end_page": 53,
      "detection_method": "topic_boundary",
      "content": "14  Machine Learning in Production\n\n7. 25% 4.750000\n\n8. 50% 5.500000\n\n9. 75% 6.500000\n\n10. max 8.000000\n\n1. df['fruit']\n\n2. 0 apple\n\n3. 1 banana\n\n4. 2 orange\n\n5. 3 mango\n\n6. Name: fruit, dtype: object\n\nloc - selection by label A loc gets rows (and/or columns) with specific labels. To get all the rows in which fruit size is medium, loc is written as follows:\n\nExample: 1.21 Get the data by location\n\n1. df.loc[df['size'] == 'medium']\n\n2. fruit size quantity\n\n3. 1 banana medium 4\n\n4. 3 mango medium 6\n\niloc - selection by position iloc gets rows (and/or columns) at the index’s locations. To get the row at index 2, iloc is written as follows:\n\nExample: 1.22 Get the data by position\n\n1. df.iloc[2]\n\n2. fruit orange\n\n3. size small\n\n4. quantity 8\n\n5. Name: 2, dtype: object\n\nTo retrieve the rows from index 2 to 3 and columns 0 to 1, iloc is written as follows:\n\n1. df.iloc[2:4, 0:2]\n\n2. fruit size\n\n3. 2 orange small\n\n4. 3 mango medium\n\nPython 101  15\n\nYou can call groupby() and pass the size, that is, the column you want the group on, and then use the sum() aggregate method.\n\nExample: 1.23 Group the data, then use an aggregate function\n\n1. df.groupby(['size'])['quantity'].sum()\n\n2. size\n\n3. large 5\n\n4. medium 10\n\n5. small 8\n\n6. Name: quantity, dtype: int64\n\nSave and load CSV files Example: 1.24 Save and load CSV files\n\n1. df.to_csv(\"fruit.csv\")\n\n2. df = pd.read_csv(\"fruit.csv\")\n\nSave and load Excel files Example: 1.25 Save and load Excel files\n\n1. df.to_excel(\"fruit.xlsx\", sheet_name=\"Sheet1\")\n\n2. df = pd.read_excel(\"fruit.xlsx\", \"Sheet1\", index_col=None)\n\nFor more information, you can refer to the official documentation at https://pandas. pydata.org/docs/.\n\nConclusion This chapter discussed the essential concepts of the Python language with examples. At the beginning of this chapter, you learned how to install Python in the system. Then, you studied the common data structures and object-oriented programming concepts in Python with examples. Further on, data pre-processing commands were covered using packages like NumPy and Pandas, which will play a major role in developing machine learning models.\n\nIn the next chapter, you will learn about git and GitHub with examples.\n\nPoints to remember\n\nEnsure to use the same Python version throughout the machine learning project.\n\n16  Machine Learning in Production\n\nThe __init()__ method in a class is used for auto initialization of data members and methods. However, it is not mandatory.\n\nTuples are immutable, that is, they are not changeable, but if they contain elements like a list, then the list is mutable but not the entire tuple.\n\nMultiple choice questions\n\n1. What is the output of np.array([[1,2,3],[4,5,6]])?\n\na) Rank 1 array b) Rank 2 array\n\nc) Rank 2 array d) All of the above\n\n2.\n\nSets can take _________ elements.\n\na) heterogenous\n\nb) homogeneous\n\nc) only integer d) only string\n\nAnswers 1. b\n\n2. a\n\nQuestions\n\n1. What is the difference between a list and a tuple? 2. What is the use of __init__()?\n\n3. When should you use a pass statement?\n\nKey terms Object-Oriented Programming: It refers to a programming paradigm based on the concept of objects, which can contain data and methods.\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com\n\nGit and GitHub Fundamentals  17\n\nChapter 2 Git and GitHub Fundamentals\n\nIntroduction It is difficult to maintain multiple versions of files while working in a team. Git solves this problem by allowing developers to collaborate and share files with other team members. This chapter covers the most needed basic Git and GitHub concepts that form the foundation of this book. It explains Git configuration, forking Git repo, pushing your codebase to GitHub, and the cloning process. In addition, it covers an introduction to GitHub Actions, common Git commands with syntax, and examples. Maintaining multiple versions of files when working in a team can be difficult, but Git is the solution.\n\nStructure The following topics will be covered in this chapter:\n\nGit concepts •\t Common Git workflow •\n\nInstall Git and create a GitHub account\n\nCommon Git commands •\t Let's Git\n\n18  Machine Learning in Production\n\nObjectives After studying this chapter, you should know how to execute Git commands and connect to remote GitHub repositories. Understand and execute Git processes. You should know how to push, fetch, and revert changes to the remote Git repository, and you should be familiar with how to install and perform the basic configuration of Git on your machine or server.\n\nGit concepts Git is an open-source Distributed Version Control System (DVCS). A version control system allows you to record changes to files over a period.\n\nGit is used to maintain the historical and current versions of source code. In a project, developers have a copy of all versions of the code stored in the central server.\n\nGit allows developers to do the following:\n\nTrack the changes, who made the changes, and when •\t Rollback/restore changes •\t Allow multiple developers to coordinate and work on the same files •\t Maintain a copy of the files at the remote and local level\n\nThe following image depicts Git as a VCS in a team where developers can work simultaneously on the same files and keep track of who made the changes. Here, F1, F2, and F3 are file names.\n\nFigure 2.1: Git scenario in team\n\nGit and GitHub Fundamentals  19\n\nGit + Hub = GitHub Git and GitHub are separate entities. Git is a command-line tool, whereas GitHub is a platform for collaboration. You can store files and folders on GitHub and implement changes to existing projects. By creating a separate branch, you can isolate these changes from your existing project files.\n\nGitHub Actions makes it easy to automate all your software workflows with Continuous Integration/Continuous Deployment. You can build, test, and deploy the code right from GitHub.\n\nCommon Git workflow The following figure depicts the general Git workflow. It covers operations like creating or cloning the Git repository, updating the local repository by pulling files from the remote repository, and pushing the local changes to the remote repository.\n\nFigure 2.2: Git workflow\n\n20  Machine Learning in Production\n\nInstall Git and create a GitHub account First off, you will need to install Git and then create an account on GitHub. To install Git on your machine, follow the given instructions.\n\nLinux (Debian/Ubuntu) In Ubuntu, open the terminal and install Git using the following commands: $ sudo apt-get update\n\n$ sudo apt-get install git\n\nGit for all platforms Download Git for your machine from the official website:\n\nhttps://git-scm.com/downloads\n\nGUI clients Git comes with built-in GUI tools; however, you can explore other third-party GUI tools at https://git-scm.com/downloads/guis.\n\nClick on the download link for your operating system and then follow the installation steps.\n\nAfter installing it, start your terminal and type git --version to verify the Git installation.\n\nIf everything goes well, you will see the Git installed successfully.\n\nCreate a GitHub account If you are new to GitHub, then can join GitHub at https://github.com/join.\n\nIf you’re an existing user, sign in to your account.\n\nCreate a new repository by clicking on the plus sign + (top-right corner):\n\nFigure 2.3: Creating a new repository on GitHub\n\nGit and GitHub Fundamentals  21\n\nHit the Create repository button. Complete the required fields, and your Git repo will be created.\n\nYou can download and install the GitHub desktop from https://desktop.github. com/.\n\nCommon Git commands Some of the common Git commands are discussed in this section.\n\nSetup Set a name that is identifiable for credit when reviewing the version history: git config --global user.name “[firstname lastname]”\n\nSet an email address that will be associated with each history marker: git config --global user.email “[valid-email-id]”\n\nNew repository Initialize an existing directory as a Git repository: git init\n\nRetrieve an entire repository from a hosted location via URL: git clone [url]\n\nUpdate Fetch and merge any commits from the remote branch: git pull\n\nFetch all the branches from the remote Git repo: git fetch [alias]\n\nChanges View modified files in the working directory staged for your next commit: git status\n\nAdd a file to your next commit (stage): git add [file]\n\n22  Machine Learning in Production\n\nCommit your staged content as a new commit snapshot: git commit -m “[descriptive message]”\n\nTransfer local branch commits to the remote repository branch: git push [alias] [branch]\n\nRevert View all the commits in the current branch’s history: git log\n\nSwitch to another branch: git checkout ['branch_name']\n\nLet’s Git Create a new directory code and switch to the code directory: suhas@test:~/code$ sudo chmod -R 777 /home/suhas/code\n\nConfiguration Provide the username and the user’s email: suhas@test:~/code$ git config --global user.name \"Suhas\"\n\nsuhas@test:~/code$ git config --global user.email \"suhasp.ds@gmail.com\"\n\nNote: Always read the output of commands to know what happened in the background.\n\nInitialize the Git repository suhas@test:~/code$ git init\n\nInitialized empty Git repository in /home/suhas/code/.git/\n\nCheck Git status suhas@test:~/code$ git status\n\nOn branch master\n\nNo commits yet\n\nnothing to commit (create/copy files and use \"git add\" to track)\n\nGit and GitHub Fundamentals  23\n\nAdd a new file A new file hello.py has been added using the touch command: suhas@test:~/code$ touch hello.py\n\nYou can notice the change in the output of the git status: suhas@test:~/code$ git status\n\nOn branch master\n\nNo commits yet\n\nUntracked files:\n\n(use \"git add <file>...\" to include in what will be committed)\n\nhello.py\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\nAdd the hello.py file to staging: suhas@test:~/code$ git add hello.py\n\nAgain, check the output of the Git status: suhas@test:~/code$ git status\n\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n\n(use \"git rm --cached <file>...\" to unstage)\n\nnew file: hello.py\n\nCommit with the message New file:\n\nsuhas@test:~/code$ git commit -m \"New file\"\n\n[master (root-commit) bd49a5e] New file\n\n1 file changed, 0 insertions(+), 0 deletions(-)\n\ncreate mode 100755 hello.py\n\nCheck the output of Git status: suhas@test:~/code$ git status\n\nOn branch master\n\nnothing to commit, working tree clean\n\nUpdate the hello.py file using the nano command: suhas@test:~/code$ sudo nano hello.py\n\n24  Machine Learning in Production\n\nNow, add the print (“Hello Word!”) to the hello.py file.\n\nYou can see the changes in the output of the git status: suhas@test:~/code$ git status\n\nOn branch master\n\nChanges not staged for commit:\n\n(use \"git add <file>...\" to update what will be committed)\n\n(use \"git checkout -- <file>...\" to discard changes in working directory)\n\nmodified: hello.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\nAdd the hello.py file to staging: suhas@test:~/code$ git add hello.py\n\nCommit along with the message added print text: suhas@test:~/code$ git commit -m \"added print text\"\n\n[master af7cfeb] added print text\n\n1 file changed, 1 insertion(+)\n\nView the logs using the git log command: suhas@test:~/code$ git log\n\ncommit af7cfeb53ff6dc49126d24aedb20a065c54ef4a0 (HEAD -> master)\n\nAuthor: “Suhas Pote” <“suhasp.ds@gmail.com”>\n\nDate: Sun Jul 5 21:34:15 2020 +0530\n\nadded print text\n\ncommit bd49a5e5280de35811848a80299d900e6e6509ce\n\nAuthor: “Suhas Pote” <“suhasp.ds@gmail.com”>\n\nDate: Sun Jul 5 21:26:12 2020 +0530\n\nNew file\n\nGit identifies each commit uniquely using the SHA1 hash function, based on the contents of the committed files. So, each commit is identified with a 40-character- long hexadecimal string. suhas@test:~/code$ git checkout af7cfeb53ff6dc49126d24aedb20a065c54ef4a0\n\nNote: checking out 'bd49a5e5280de35811848a80299d900e6e6509ce'.\n\nYou are in a 'detached HEAD' state. You can look around, make experimental\n\nGit and GitHub Fundamentals  25\n\nchanges and commit them, and you can discard any commits you make in this\n\nstate without impacting any branches by performing another checkout.\n\nIf you want to create a new branch to retain commits you create, you may\n\ndo so (now or later) by using -b with the checkout command again. Example:\n\ngit checkout -b <new-branch-name>\n\nHEAD is now at bd49a5e New file\n\nIf you notice, the hello.py file is backed to the previous state, that is, a blank file. It is like a time machine!\n\nNow, let’s get back to the latest version of hello.py. suhas@test:~/code$ git reset --hard af7cfeb53ff6dc49126d24aedb20a065c54ef4a0\n\nHEAD is now at af7cfeb added print text\n\nIt’s time to push the files to the GitHub repo.\n\nsuhas@test:~/code$ git remote add origin https://github.com/suhas-ds/ myrepo.git\n\nsuhas@test:~/code$ git push -u origin master\n\nUsername for 'https://github.com': suhas-ds\n\nPassword for 'https://suhas-ds@github.com':\n\nCounting objects: 6, done.\n\nCompressing objects: 100% (2/2), done.\n\nWriting objects: 100% (6/6), 477 bytes | 477.00 KiB/s, done.\n\nTotal 6 (delta 0), reused 0 (delta 0)\n\nTo https://github.com/suhas-ds/myrepo\n\n[new branch] master -> master\n\nBranch 'master' is set up to track remote branch 'master' from 'origin'.\n\nNote: Here, the origin acts as an alias or label for the URL, and the master is a branch name.\n\nTo pull the latest version of the file execute the following command. suhas@test:~/code$ git pull origin master\n\nFrom https://github.com/suhas-ds/myrepo\n\nbranch master\n\n> FETCH_HEAD\n\nAlready up to date.\n\n26  Machine Learning in Production\n\nIf you want to load files from another GitHub repository, or if you are working on another system and want to load files from your GitHub repository, you can achieve this by cloning it. suhas@test:~/myrepo$ git clone https://github.com/suhas-ds/myrepo.git\n\nCloning into 'myrepo'...\n\nremote: Enumerating objects: 6, done.\n\nremote: Counting objects: 100% (6/6), done.\n\nremote: Compressing objects: 100% (2/2), done.\n\nremote: Total 6 (delta 0), reused 6 (delta 0), pack-reused 0\n\nUnpacking objects: 100% (6/6), done.\n\nYou can practice frequently used Git commands with more files and Git repositories.\n\nConclusion In this chapter, you explored Git and GitHub, and you learned the basic Git commands, such as git commit and git push. Then, you installed and configured Git, and you pushed and cloned files from a remote GitHub repository. To use the GitHub platform, it is essential to know the Git workflow.\n\nIn the next chapter, you will learn about the challenges faced while deploying Machine Learning models in production and understand how to overcome them.\n\nPoints to remember\n\nGit identifies each commit uniquely using the SHA1 hash function, based on the contents of the committed files.\n\nGit is a command-line tool, whereas GitHub is a platform for collaboration. •\t Git is used to maintain the historical and current versions of source code.\n\nMultiple choice questions\n\n1. The ______ command shows all commits in the current branch’s history.\n\na) git checkout\n\nb) git push [alias] [branch]\n\nc) git log\n\nd) git pull\n\nGit and GitHub Fundamentals  27\n\n2. Which of the following statements does not apply to git?\n\na) Tracks who created/made what changes and when b) Rollback/restore changes c) Allow multiple developers to coordinate and work on the same files d) Does not maintain a copy of the files at a remote and local repository\n\nAnswers 1. c\n\n2. d\n\nQuestions\n\n1. What is Git? What is GitHub?\n\n2. What is the command to upload changes to the remote repository?\n\n3. How do you roll back/revert the changes?\n\n4. How do you initialize a new repository?\n\n5. What is the use of the Git status command?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com\n\n28  Machine Learning in Production",
      "page_number": 39
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 54-61)",
      "start_page": 54,
      "end_page": 61,
      "detection_method": "topic_boundary",
      "content": "Challenges in ML Model Deployment  29\n\nChapter 3 Challenges in ML Model Deployment\n\nIntroduction A study found that 87% of Data Science and Machine Learning projects never make it into production. In this chapter, you will study common problems you may face while deploying machine learning models.\n\nDeploying ML models is entirely based on your end goals, such as frequency of predictions, latency, number of users, single or batch predictions, and accessibility.\n\nStructure\n\nThis chapter covers the following topics: •\t ML life cycle •\t Types of model deployment •\t Challenges while deploying models •\t MLOps •\t Benefits of MLOps\n\n30  Machine Learning in Production\n\nObjectives This chapter will cover the various approaches to deploy ML models in production. After studying this chapter, you should be able to use MLOps to overcome challenges in the manual deployment of ML models. You will also study the different phases of the ML life cycle in this chapter.\n\nML life cycle The machine learning life cycle is a periodic process. It starts with the business problem, and the last stage is monitoring and optimization. However, it is not so straightforward. For instance, if there is a modification in the business requirement, you may have to execute all the stages of the ML life cycle again. In some scenarios, you may have to go back to the previous stages of the ML life cycle to fulfill the criteria of that specific stage. For example, if you get lower accuracy of the model than the threshold, you need to revisit the previous stages to improve the accuracy. It can be done by adding new features, optimizing model parameters, and so on.\n\nThe following figure shows the different stages of the ML life cycle.\n\nFigure 3.1: ML life cycle\n\nChallenges in ML Model Deployment  31\n\nBusiness impact The first and most important stage is defining an idea and its impact on the project. Starting a project without considering the business impact, complexities, expected results, and time required may lead to delayed delivery, repetitive work, and poor resource utilization.\n\nThe business impact could be anything, like an increase in revenue, a decrease in expenses, or reducing human errors. One needs to understand the business pain points and try to assess the possibilities of having an ML solution that will solve multiple business problems. In some scenarios, getting quick results is more important.\n\nData collection Data collection is the process of gathering data from one or multiple sources. Although it looks easy, it is not. Before collecting the data, you need to answer the following questions:\n\nWhat data needs to be collected? •\t What are the different sources of the data? •\t What is the type of data? •\t What is the size of the data?\n\nIn the real world, you may have to collect data from different sources, like relational databases, NoSQL databases, the web, and so on. To avoid any glitches or delays, you must build the pipeline for data collection.\n\nFor better results, you can combine the available data with external data, such as social media sites, weather data, and public data.\n\nHere are a few ways to collect data:\n\nE-surveys •\t Authorized web scraping tool •\t Click data •\t Social media platforms •\t Website tracking •\t Subscription/ registration data • Image data - CCTV/ camera\n\n32  Machine Learning in Production\n\nVoice data - customer care\n\nSet up a meeting with clients/stakeholders and domain experts to get to know the data. They will help you understand the data so that you can decide which data needs to be collected for model building.\n\nChallenges:\n\nData is scattered in different locations •\t No uniformity in the data •\t Combining data from different sources •\t 3Vs: volume, velocity, variety •\t Data storage\n\nData preparation The data collected usually is in a raw format. One needs to process it so that it can be used for further analysis and model development. This process of cleaning, restructuring, and standardization is known as data preparation.\n\nAround 70%-80% of the time goes into this stage of the ML project. This is a tedious task, but it is unavoidable. Data reduction technique is used when you have a large amount of data to be processed.\n\nData preparation aims to transform the raw data into a format so that EDA, that is, Exploratory Data Analysis, can be performed efficiently to gain insights.\n\nChallenges:\n\nMissing values •\t Outliers •\t Disparate data format •\t Data standardization •\t Noise in the data\n\nFeature engineering In this stage, you prepare the input data that can be fed to the model, which makes it easier for the machine learning model by deriving meaningful features, data transformations, and so on.\n\nChallenges in ML Model Deployment  33\n\nSuppose you are combining the features to create a new one that could help ML models to understand the data better and identify hidden patterns. For instance, you mix sugar, water, and lemon juice to make lemonade rather than consuming them separately.\n\nHere are a few feature engineering techniques:\n\nLabel encoding •\t Combining features to create a new one •\t One hot encoding • Imputation\n\nScaling •\t Removing unwanted features •\t Log transformation\n\nChallenges:\n\nLack of domain knowledge •\t Creating new features from the existing set of features •\t Selecting useful features from a set of features\n\nBuild and train the model ML model building requires the previous stages to be completed successfully. First, you should have the training, test, and validation sets ready (for supervised algorithms). After a baseline model is created, it can be compared with new models.\n\nThis process involves the development of multiple models to see which one is more efficient and gives better results. You must consider the computing resource requirements for this stage.\n\nOnce the final model is built on the training data, the next step is to check its performance against the unseen, that is, the test data.\n\nChallenges:\n\nModel complexity •\t Computational power •\n\nIdentifying a suitable model\n\nModel training time\n\n34  Machine Learning in Production\n\nTest and evaluate Here, you will build the test cases and check how the model is performing against the new data. Pre-production or pre-deployment activities are done here. Performance results are analyzed and, if required, you have to go back to the previous stages to fix the issue.\n\nAfter passing this stage, you can push the ML model into the production phase.\n\nChallenges:\n\n\n\nInsufficient test data\n\nReiterating the process until the output fulfills the requirements • Identifying the platform to evaluate model performance on real data\n\nDeciding which test to use •\t Logging and analyzing test results\n\nModel deployment This refers to exposing the trained ML models to real-world users. Your model is performing well on test and validation sets, but if another system or users cannot utilize it, then it does not meet its purpose.\n\nFor instance, you have built a model to predict customers who are likely to churn, but it is not done until you deploy a model that will start delivering the predictions on real data.\n\nModel deployment is crucial because you have to consider the following factors:\n\nNumber of times predictions to be delivered •\t Latency of the predictions •\t ML system architecture •\t ML model deployment and maintenance cost •\t Complexity of infrastructure •\t This will be covered in detail in the following chapters.\n\nChallenges:\n\nPortability issues •\t Scalability issues •\t Data-related challenges •\t Security threats\n\nChallenges in ML Model Deployment  35\n\nMonitoring and optimization Last but not least, monitoring and optimization is the stage where observing and tracking are required. You will have to check the model performance as it degrades over time.\n\nIf the model metrics, such as accuracy, go below the predefined threshold, then it needs to be tracked, and a model needs to be retrained, either automatically or manually. Similarly, input data needs to be monitored because it may happen that the input data schema does not match or it contains missing values.\n\nApart from this, the infrastructure metrics, such as RAM, free space, and system issues, need to be tracked.\n\nIt is good to maintain the log records of metrics, intermediate outputs, warnings, and errors.\n\nChallenges:\n\nData drift •\t Deciding the threshold value for different metrics •\t Anomalies •\t Finalizing model evaluation metrics that need to be tracked\n\nTypes of model deployment There are various ways to deploy ML models into production; however, there is no generic way to do it. This section will walk you through popular ways to deploy ML models.\n\nBatch predictions This is the simplest method. Here, the ML model is trained on static data to make predictions, which are saved in the database, such as MS-SQL, and can be integrated into existing applications or accessed by the business intelligence team.\n\nGenerally, ML model artifacts are used for making predictions as it saves time. Model artifact needs to be updated on new data for better predictions.\n\nThis method is well-suited for small organizations and beginners. You can schedule the cron job to make predictions after certain time intervals.\n\nPros:\n\nAffordable •\t Less complex\n\n36  Machine Learning in Production\n\nEasy to implement\n\nCons:\n\nModerate latency •\t Not suitable for ML-centric organizations\n\nWeb service/REST API Web service/REST API is the popular method for deploying models. Unlike batch predictions, it does not process a bunch of records; it processes a single record at a time. In near real-time, it takes the parameters from users or existing applications and makes predictions.\n\nIt can take inputs as well as return the outputs in JSON format. JSON is a popular and compatible format that makes it easy for software or website developers to integrate it into existing applications.\n\nWhen an event gets triggered, REST API passes the input parameters to the ML model and returns the predictions.\n\nPros:\n\nEasy to integrate •\t Flexible •\t Economical (pay-as-you-go plan) •\t Near real-time predictions\n\nCons:\n\nScalability issues •\t Prone to security threats\n\nMobile and edge devices When there are situations such as actions/ decisions that need to be taken immediately or there is no internet connectivity, the ML model needs to be deployed on these devices.\n\nEdge devices include sensors, smartwatches, and cameras installed on robots.\n\nThis type of deployment is different from the preceding methods. In this, input data may not go to remote servers for making predictions. There are cloud service providers, such as Microsoft Azure, that offer the required infrastructure for this. Tiny Machine Learning (TinyML) is another such alternative capable of performing on-device sensor data analytics at an extremely low power.",
      "page_number": 54
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 62-69)",
      "start_page": 62,
      "end_page": 69,
      "detection_method": "topic_boundary",
      "content": "Challenges in ML Model Deployment  37\n\nML models deployed on mobile devices are useful. Developing ML-based android/ IOS applications, such as voice assistant or camera image-based attendance, are use cases of ML models deployed on mobiles.\n\nPros:\n\nLow power requirement •\t Cost-effective •\t Smaller sizes\n\nCons:\n\nLimited hardware resources •\t A complex and tedious process\n\nReal-time\n\nHere, analysis is to be done on streaming data. In this approach, the user passes input data for prediction and expects (near) real-time prediction. This is quite a challenging approach compared to the ones you studied previously. •\t You can decrease the latency of the predictions by small-sized models, caching predictions, No-SQL databases, and so on.\n\nPros:\n\nVery low or no latency •\t Can work with streaming data\n\nCons:\n\nComplex architecture requirements •\t Computationally expensive\n\nChallenges in deploying models in the production environment Deploying machine models in production is crucial. While deploying ML models, technical challenges are not the only ones. The following are the common challenges you may come across.\n\nTeam coordination If you are working in a small organization or a small data science team, you might need to play different roles and will be responsible for end-to-end model development.\n\n38  Machine Learning in Production\n\nHowever, in large organizations, there are separate teams for different stages.\n\nThese teams interact with each other to carry out the entire process. Most of the time, the production or the succeeding team is not aware of the preceding stages. Lack of coordination and miscommunication can lead to the failure of the ML life cycle.\n\nData-related challenges Usually, data scientists develop models on a limited amount of data in the experimentation phase, but they might face challenges while deploying models in the production environment as large-scale data can impact the model performance.\n\nNew data keeps on changing its behavior, but the data preparation/pre-processing stage may not be able to handle these new issues. For instance, string records are present in a numeric column. At times, re-running the experiments by fixing the issues is required to get reliable results.\n\nPortability Portability issues arise when moving the codebase from your local machine to the server and vice versa. For instance, suppose you developed a model and want to move the code to the server to rerun the model. However, you may face issues, like the library not being compatible with the current OS version, and the line of code working with Python’s x version and not with the currently installed version.\n\nIn such cases, creating a virtual environment and re-installing the packages are required.\n\nScalability Suppose you deployed models using web service and 100x more users are trying to access the same API. REST API may stop responding or the latency may increase.\n\nManaging ML models on a small scale is relatively easy compared to large organizations, where multiple models are being served on a scale. Deploying models on a scale is still a new thing for many organizations.\n\nRobustness In a nutshell, the robustness of ML models is measured by the reliable output delivered despite the variations in the input data. In the real world, delivering 100% accuracy is nearly impossible. So, one can say that prediction error on unseen data should be close to training error.\n\nChallenges in ML Model Deployment  39\n\nUser behavior can be unpredictable. For instance, suppose a model requires a string value, but the user provides an alphanumeric value. In such scenarios, the model output might not be reliable.\n\nSecurity ML systems are prone to security breaches. They may be forced to deliver false predictions by deliberately providing poisonous data or adding noise to the data. The models perform better when trained on new data. However, a person or a group of people can deliberately pass the poisonous data to the model, intending to change the model predictions.\n\nData security is at risk as the model keeps training on new data. While training the model, attackers can steal sensitive information.\n\nMLOps MLOps combines machine learning processes and best practices of DevOps to deliver consistent output with automated pipelines and management. MLOps has been an emerging space in the industry for the last few years.\n\nThe following figure shows the increasing popularity of MLOps over time.\n\nFigure 3.2: The rise of MLOps. Source: Google trends\n\nMLOps bridges the gap between machine learning experiments and model deployment in the production environment. Nowadays, many data scientists are forced to execute the MLOps processes manually.\n\nThe MLOps process involves multiple professionals, and data scientists play a critical role. Subject matter experts understand and collect the requirements from the client. Then, data engineers collect the data from multiple sources and execute the ETL jobs. Once this is done, data scientists build the models and the DevOps\n\n40  Machine Learning in Production\n\nteam builds the CI/CD pipeline and monitors it. Finally, feedback is sent to the data scientist or the concerned team for validation.\n\nMLOps streamline and automate this process to speed up delivery and build efficient products/services.\n\nMLOps is a combination of three disciplines as shown in the following figure\n\nFigure 3.3: Machine learning operations\n\nMLOps is different from DevOps because the code is usually static in the latter, but that’s not the case in MLOps.\n\nIn MLOps, the model keeps training on new data, so a new model version gets generated recurrently. If it meets the requirements, it can be pushed into the production environment. This is why MLOps requires Continuous Training (CT) along with Continuous Integration (CI) and Continuous Delivery/Deployment (CD).\n\nIn DevOps, developers write the code as per the requirements and then release it to the production environment, but in the case of machine learning, developers first need to collect the data and clean it. They write code for model building and then build the ML model. Finally, they release it into the production environment.\n\nBenefits of MLOps MLOps processes not only speed up the ML journey from experiments to production, but also reduce the number of errors. MLOps automates Machine Learning and Deep Learning model deployment in a production environment. Moreover, it reduces dependency on other teams by streamlining the processes.\n\nChallenges in ML Model Deployment  41\n\nEfficient management of ML life cycle MLOps takes care of the ML life cycle by following the fundamental principles of machine learning projects. It is based on agile methodology. An automated CI/ CD pipeline helps speed up the process of retraining models on new data, testing before the deployment, monitoring, and feedback loops. The current stage depends on the output of the preceding stages, so there are fewer chances of issues in the deployment process.\n\nIf the amount of traffic or number of requests increase for a deployed model, it increases the required resources; similarly, the required resources decrease when the amount of traffic or number of requests reduces.\n\nThere are many platforms (like GitHub Actions) available in the market that help you set up MLOps workflow.\n\nReproducibility Developer: It works on my machine.\n\nManager: Then, we are going to ship your machine to the client.\n\nReproducibility plays a crucial role in the ML life cycle. While transferring the code files to another machine or server, reproducibility reduces debugging time. MLOps works on DRY (Don’t Repeat Yourself) principles and allows you to get consistent output.\n\nGiven the same input, the replicated workflow should produce an identical output. For this, developers use container orchestration tools like Docker so that it will create and set up the same environment with dependencies in another machine or server for consistent output.\n\nAutomation Mostly, developers deploy models several times before the final deployment to ensure that everything is in place and the output is as expected. Without automation, this would be a time-consuming and tedious process. Automation increases productivity, as you are less likely to test, deploy, scale, and monitor ML models manually.\n\nAt every stage, certain rules and conditions are implemented, and the model moves to the next stage only when these conditions are met. This reduces the active involvement of other teams every time you are planning to deploy it in production.\n\nThere is a low or no delayed deployment with testing since one inter-team dependency is reduced.\n\n42  Machine Learning in Production\n\nAfter making the required changes in the code, speedy deployment is done in an automated fashion (if it passes the test cases and satisfies the conditions). This increases the overall productivity of the team.\n\nTracking and feedback loop Tracking model performance, metrics, test results, and output becomes easy if you set up MLOps workflow properly. The model’s performance may degrade over time, hence one may need to retrain the model on new data.\n\nThanks to tracking and feedback loops, it sends alerts about the model’s performance, metrics, and so on.\n\nFor instance, if the feedback loop sends the information that model accuracy has dropped below 68%, then the model needs to be retrained. Again, it will check the model’s performance. If it is above the threshold level, it will pass on to the next stage; else, the model needs to be recalibrated using the latest data.\n\nConclusion In this chapter, you studied the key challenges faced while deploying ML models in production. At the beginning of this chapter, you learned how the ML life cycle works, and you understood its different stages and their challenges. Next, you got exposure to different types of model serving techniques and looked at their pros and cons. Then, you analyzed the challenges you may face while deploying ML models in production. Finally, you learned the benefits of MLOps.\n\nIn the next chapter, you will learn to develop, build and install custom python packages for ML models using a use case.\n\nPoints to remember\n\nMLOps bridges the gap between the Machine Learning experiments stage and its model deployment in the production environment.\n\nModel deployment strategy depends on business requirements, users, and applications.\n\nMLOps combines machine learning processes and best practices of DevOps to deliver consistent output with automated pipelines and management.\n\nChallenges in ML Model Deployment  43\n\nMultiple choice questions\n\n1. What are the different ways of collecting data for an ML problem?\n\na) E-surveys\n\nb) Web scraping\n\nc) Click data\n\nd) All the above\n\n2.\n\nIf you provide a similar input as the original workflow to the replicated workflow, it should produce an identical output known as: a) Portability\n\nb) Reproducibility\n\nc) Scalability\n\nd) Label encoding\n\nAnswers 1. d\n\n2. b\n\nQuestions\n\n1. What are the different ways of deploying ML models?\n\n2. How do MLOps differ from DevOps?\n\n3. What are the challenges faced while deploying ML models?\n\n4. What are the benefits of MLOps?\n\nKey terms\n\nContainer: A container is a standard unit of software that packages up the code and all its dependencies so that the application runs quickly and reliably despite the computing environment.\n\nContinuous Integration (CI): It is an automated process to build, test, and execute different pieces of code.\n\nContinuous Delivery/Deployment (CD): It is an automated process of frequently building tested code and deploying it to production.\n\n44  Machine Learning in Production\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "page_number": 62
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 70-77)",
      "start_page": 70,
      "end_page": 77,
      "detection_method": "topic_boundary",
      "content": "Packaging ML Models  45\n\nChapter 4 Packaging ML Models\n\nIntroduction In this chapter, you will learn how to modularize Python code and build ML packages that can be installed and consumed on another machine or server. Modular programs simplify code debugging, reusability, and maintainability. You will start by creating a virtual environment and then data pre-processing, followed by building the ML model and finally, creating test cases for the package.\n\nStructure This chapter covers the following topics:\n\nVirtual environment •\t Requirements file •\t Serializing and de-serializing ML models •\t Testing Python code using pytest •\t Python packaging and dependency management •\t Developing, building, and deploying ML packages •\t Set up environment variables and paths\n\n46  Machine Learning in Production\n\nObjectives After completing this chapter, you should be able to create a virtual environment and install the required dependencies in it. You should also be able to package the modules and dependencies required to build an ML model that can be installed on local machines or remote servers. Additionally, you should be able to save the trained ML model by pickling it. This chapter will also help you learn to modularize code and build test cases using pytest to check integrity and functionality.\n\nVirtual environments While working on multiple projects, ProjectA requires PackageY_version2.6, whereas ProjectB requires PackageY_version2.8. In such scenarios, you can’t keep both versions of the same package globally.\n\nThe virtual environment is the solution. It allows you to isolate dependencies for each project. You can create the virtual environment anywhere and install the required packages in it.\n\nWithout the virtual environment, packages are installed globally at the default Python location:\n\n/home/suhas/.local/lib/python3.6/site-packages\n\nWith the virtual environment, packages are installed inside the virtual environment’s Python location:\n\n/home/suhas/code/packages/venv_package/lib/python3.6/site-packages\n\nInstalling different versions of the same package in different virtual environments for different projects is possible. You can have five packages in venv1 and nine packages in venv2.\n\nThe following figure best depicts the scenario wherein three different projects have separate virtual environments with different versions of Python and packages installed in them.\n\nPackaging ML Models  47\n\nFigure 4.1: Virtual environment\n\nVirtual environment installation:\n\npip install virtualenv\n\nCreating a virtual environment:\n\nvirtualenv venv_package\n\nActivating a virtual environment:\n\nsource venv_package/bin/activate\n\n(venv_package) suhas@suhasVM:~/code/packages/prediction_model$\n\nDeactivating a virtual environment:\n\ndeactivate\n\nTo see the list of packages installed in the virtual environment: pip list\n\nOr pip freeze\n\nRequirements file The requirements file holds the list of packages that can be installed using pip.\n\n48  Machine Learning in Production\n\nTo create the requirements file:\n\npip freeze > requirements.txt\n\nTo install the list of packages from the requirements file, you can use the following command:\n\npip install -r requirements.txt\n\nWhere -r refers to –-requirement.\n\nIt instructs pip to install all the packages from the given requirements file.\n\nSerializing and de-serializing ML models Serializing is a process through which a Python object hierarchy is converted into a byte stream, whereas deserialize is the inverse operation, that is, a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy. In Python, serialization and deserialization refer to pickling and unpickling, respectively.\n\nHere, joblib.dump() and joblib.load() will be used as a replacement for a pickle.dump() and pickle.load respectively, to work efficiently on arbitrary Python objects containing large data, large NumPy arrays in particular, as shown here.\n\nImport the joblib library:\n\n1. import joblib\n\nThen create an object to be persisted:\n\n2. joblib.dump(ML_model_object, filename)\n\nAn object can be reloaded:\n\n3. joblib.load(filename)\n\nNote: If you are switching between Python versions, you may need to save a different joblib dump for each Python version.\n\nTesting Python code with pytest Testing your code assures you that it is giving the expected results and its functions are working bug-free. pytest tool allows you to build and run tests with ease. It comes with simple syntax and several features.\n\npytest can be installed using pip: pip install pytest\n\nPackaging ML Models  49\n\nTo run the pytest, simply switch to the package directory and run the following command: pytest\n\nIt searches for test_*.py or *_test.py files.\n\nThe pytest output can be any of the following: •\t A dot (.) means that the test has passed. •\t An F means that the test has failed. •\t An E means that the test raised an unexpected exception.\n\npytest -v\n\nv or --verbose flag allows you to see whether individual tests are passing or failing.\n\npytest fixtures pytest fixtures are basic functions, and they run before the test functions are executed. pytest fixtures are useful when you are running multiple test cases with the same function return value.\n\nIt can be declared by the @pytest.fixture marker. The following is an example:\n\n1. @pytest.fixture\n\n2. def xyz_func():\n\n3. return “ABC”\n\nWhen pytest runs a test case, it looks at the parameters in that test function’s signature and then searches for fixtures that have the same names as those parameters. Once pytest finds them, it runs those fixtures, captures what they returned (if any), and passes those objects into the test functions as arguments.\n\nYou can configure pytest using the pytest.ini file.\n\nPython packaging and dependency management Suppose you have created the final ML model in a Python notebook. This Python notebook holds all the steps right from loading the data to predicting the test data.\n\nHowever, this notebook should not be used in the production environment for the following reasons:\n\n50  Machine Learning in Production\n\nDifficult to debug •\t Require changes at multiple locations •\t Lots of dependencies •\t No modularity in the code •\t Conflict of variables and functions •\t Duplicate code snippets\n\nModular programming Python is a modular programming language. Modular programming is a design approach in which code gets divided into separate files, such that each file contains everything necessary to execute a defined piece of logic and can return the expected output when imported by other files that will act as input for them. These separate files are called modules.\n\nModule A module is a Python file that can hold classes, functions, and variables. For instance, load.py is a module, and its name is load.\n\nPackage A package contains one or more (relevant) modules, such that they are interlinked with each other. A package can contain a subpackage that holds the modules. It uses the inbuilt file hierarchy of directories for ease of access. A directory with subdirectories can be called a package if it contains the __init__.py file.\n\nPackages will be installed in the production environment as part of the deployment, so before you package anything, you’ll want to have answers to the following deployment questions:\n\nWho are your app’s users? Will your app be installed by other developers doing software development, operations people in a data center, or a less software-savvy group?\n\n\n\nIs your app intended to run on servers, desktops, mobile clients (phones, tablets, and so on), or embedded in dedicated devices?\n\n\n\nIs your app installed individually, or in large deployment batches?\n\nThe following figure depicts the sample structure of the Python package, where f1, f2, f3, f4, and f5 are modules of the package.\n\nPackaging ML Models  51\n\nFigure 4.2: Python package structure\n\nImport module syntax:\n\nimport <module_name>\n\nimport <module_name> as <alt_name>\n\nImport f4 module:\n\n1. from My_Package import f4\n\n2. from My_Package import f4 as load\n\nImport f1 module to use x():\n\n1. from My_Package.Sub_PackageA import f1\n\n2. f1.func()\n\nOr you can use the following method to import the module:\n\n1. from My_Package.Sub_PackageA.f1 import func\n\n2. func()\n\nDeveloping, building, and deploying ML packages In this section, you will study a use case that will begin by defining the business problem and end with installing and consuming an ML package. You will learn to\n\n52  Machine Learning in Production\n\ndevelop and build a custom Python package for ML models. The custom package is portable and can be reused for the given project. You will be able to install a custom package like any other Python package and make predictions on the new data.\n\nBusiness problem A company wants to automate the loan eligibility process based on customer details provided by filling out online application forms. It is a classification problem where you must predict whether a loan would be approved.\n\nData The data corresponds to a set of financial transactions associated with individuals. The data has been standardized, de-trended, and anonymized. Refer to the following table for details of the data columns:\n\nVariable\n\nDescription\n\nLoan_ID\n\nUnique Loan ID\n\nGender\n\nMale/ Female\n\nMarried\n\nApplicant’s marital status(Y/N)\n\nDependents\n\nNumber of dependents\n\nEducation\n\nApplicant’s Education (Graduate/Under Graduate)\n\nSelf_Employed\n\nSelf-employed (Y/N)\n\nApplicantIncome\n\nApplicant’s income\n\nCoapplicantIncome\n\nCo-applicant’s income\n\nLoanAmount\n\nLoan amounts in thousands\n\nLoan_Amount_Term\n\nTerm of the loan in months\n\nCredit_History\n\nCredit history meets guidelines (Y/N)\n\nProperty_Area\n\nUrban/ Semi Urban/ Rural\n\nLoan_Status\n\nLoan approved (Y/N)\n\nTable 4.1: Data description",
      "page_number": 70
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 78-89)",
      "start_page": 78,
      "end_page": 89,
      "detection_method": "topic_boundary",
      "content": "Packaging ML Models  53\n\nBuilding the ML model At this point, you may build and compare different ML algorithms using a Jupyter notebook.\n\nIdentify the model that is performing better than the others and finalize the best parameters (and hyperparameters) for the model. In this experimentation stage, you should be ready with information like the variables needed to transform, missing value imputation, the list of numerical variables, and the data path.\n\nWhen you are done with the experimentation stage, you can start building the package. The purpose of packaging is reusability, portability, fewer errors, and automation of machine learning processes.\n\nStart by creating a separate directory for building the package. Maintain separate modules (Python files) for different stages and operations, for instance, separate modules for pre-processing of the data. This will modularize the code and make debugging the code easy.\n\nBuild the test cases that will run the code files and verify the integrity of the files and the expected output.\n\nThis chapter does not focus on optimizing ML models, so you can further improve the model performance by fine-tuning it. This package would be limited to the current project, that is, you should not use the same package for different projects.\n\nDeveloping the package Following is the directory structure of the package: prediction_model\n\n├── MANIFEST.in\n\n├── prediction_model\n\n│ ├── config\n\n│ │ ├── config.py\n\n│ │ └── __init__.py\n\n│ ├── datasets\n\n│ │ ├── __init__.py\n\n│ │ ├── test.csv\n\n│ │ └── train.csv\n\n│ ├── __init__.py\n\n│ ├── pipeline.py\n\n│ ├── predict.py\n\n54  Machine Learning in Production\n\n│ ├── processing\n\n│ │ ├── data_management.py\n\n│ │ ├── __init__.py\n\n│ │ └── preprocessors.py\n\n│ ├── trained_models\n\n│ │ ├── classification_v1.pkl\n\n│ │ └── __init__.py\n\n│ ├── train_pipeline.py\n\n│ └── VERSION\n\n├── README.md\n\n├── requirements.txt\n\n├── setup.py\n\n└── tests\n\n├── pytest.ini\n\n└── test_predict.py\n\n__init__.py\n\nThe __init__.py module is usually empty. However, it facilitates importing other Python modules.\n\nThis file indicates that the directory should be treated as a package.\n\nMANIFEST.in\n\nA MANIFEST.in file consists of commands, one per line, instructing setuptools to add or remove a set of files from the sdist (source distribution). In Python, distribution refers to the set of files that allows packaging, building, and distributing the modules. sdist contains archives of files, such as source files, data files, and setup.py file, in a compressed tar file (.tar.gz) on Unix.\n\nThe following table lists the commands and the descriptions. However, this chapter will only cover a few of them.\n\nYou can update MANIFEST.in as per the project requirements. Refer to the following table for general commands being used in the manifest file.\n\nCommand\n\nDescription\n\ninclude pat1 pat2 ...\n\nAdd all files matching any of the listed patterns.\n\nexclude pat1 pat2 ...\n\nRemove all files matching any of the listed patterns.\n\nPackaging ML Models  55\n\nrecursive-include pattern pat1 pat2 ...\n\ndir-\n\nAdd all files under directories matching the dir- pattern that matches any of the listed patterns.\n\nrecursive-exclude pattern pat1 pat2 ...\n\ndir-\n\nRemove all files under directories matching the dir- pattern that matches any of the listed patterns.\n\nglobal-include pat1 pat2 ...\n\nAdd all files anywhere in the source tree matching any of the listed patterns.\n\nglobal-exclude pat1 pat2 ...\n\nRemove all files anywhere in the source tree matching any of the listed patterns.\n\ngraft dir-pattern\n\nprune dir-pattern\n\nAdd all files under directories matching the dir- pattern. Remove all files under directories matching the dir- pattern.\n\nTable 4.2: Commands for the manifest file\n\nLet’s add the commands to the manifest file to include or exclude files:\n\n1. include *.txt\n\n2. include *.md\n\n3. include *.cfg\n\n4. include *.pkl\n\n5. recursive-include ./prediction_model/*\n\n6.\n\n7. include prediction_model/datasets/train.csv\n\n8. include prediction_model/datasets/test.csv\n\n9. include prediction_model/trained_models/*.pkl\n\n10. include prediction_model/VERSION\n\n11. include ./requirements.txt\n\n12. exclude *.log\n\n13. recursive-exclude * __pycache__\n\n14. recursive-exclude * *.py[co]\n\nconfig.py\n\nA configuration module contains constant variables, the path to the directories, and initial settings. Variables and functions can be accessed by importing this module into other modules. For instance, the TARGET variable holds the dependent column name Loan_Status.\n\n56  Machine Learning in Production\n\nHere, specify the path for the package’s root directory, data directory, train file name, test file name, features, and path of other files and directories.\n\n1. #Import Libraries\n\n2. import pathlib\n\n3. import os\n\n4. import prediction_model\n\n5.\n\n6. PACKAGE_ROOT = pathlib.Path(prediction_model.__file__).resolve(). parent 7.\n\n8. DATAPATH=os.path.join(PACKAGE_ROOT, 'datasets')\n\n9. SAVED_MODEL_PATH=os.path.join(PACKAGE_ROOT, 'trained_models')\n\n10.\n\n11. TRAIN_FILE='train.csv'\n\n12. TEST_FILE='test.csv'\n\n13.\n\n14. TARGET='Loan_Status'\n\n15.\n\n16. #Features to keep\n\n17. FEATURES=['Gender','Married','Dependents',\n\n18. 'Education','Self_Employed','ApplicantIncome',\n\n19. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n20. 'Credit_History','Property_Area'] # Final feature to keep in data\n\n21.\n\n22. NUMERICAL_FEATURES=['ApplicantIncome', 'LoanAmount', 'Loan_ Amount_Term'] d 23.\n\n24. CATEGORICAL_FEATURES=['Gender','Married','Dependents',\n\n25. 'Education','Self_Employed','Credit_ History',\n\n26. 'Property_Area'] #Categorical\n\n27.\n\n28. FEATURES_TO_ENCODE=['Gender','Married','Dependents',\n\n29. 'Education','Self_Employed','Credit_History',\n\nPackaging ML Models  57\n\n30. 'Property_Area'] #Features to Encode\n\n31.\n\n32. TEMPORAL_FEATURES=['ApplicantIncome']\n\n33. TEMPORAL_ADDITION='CoapplicantIncome'\n\n34. LOG_FEATURES=['ApplicantIncome', 'LoanAmount'] #Features for Log Transformation\n\n35. DROP_FEATURES=['CoapplicantIncome'] #Features to Drop\n\ndata_management.py\n\nThis module contains functions required for loading the data, saving serialized ML model, and loading deserialized ML model using joblib.\n\n1. #Import Libraries\n\n2. import os\n\n3. import pandas as pd\n\n4. import joblib\n\n5.\n\n6. #Import other files/modules\n\n7. from prediction_model.config import config\n\n8.\n\n9. def load_dataset(file_name):\n\n10. \"\"\"Read Data\"\"\"\n\n11. file_path = os.path.join(config.DATAPATH,file_name)\n\n12. _data = pd.read_csv(file_path)\n\n13. return _data\n\n14.\n\n15. def save_pipeline(pipeline_to_save):\n\n16. \"\"\"Store Output Of Pipeline\n\n17. Exporting pickle file of trained Model \"\"\"\n\n18. save_file_name = 'classification_v1.pkl'\n\n19. save_path = os.path.join(config.SAVED_MODEL_PATH, save_file_ name)\n\n58  Machine Learning in Production\n\n20. joblib.dump(pipeline_to_save, save_path)\n\n21. print(\"Saved Pipeline : \",save_file_name)\n\n22.\n\n23. def load_pipeline(pipeline_to_load):\n\n24. \"\"\"Importing pickle file of trained Model\"\"\"\n\n25. save_path = os.path.join(config.SAVED_MODEL_PATH, pipeline_to_ load)\n\n26. trained_model = joblib.load(save_path)\n\n27. return trained_model\n\npreprocessors.py\n\nThis module holds all the fit and transform functions required by the sklearn pipeline:\n\n1. #Import Libraries\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4. from sklearn.base import BaseEstimator, TransformerMixin\n\n5. from sklearn.preprocessing import LabelEncoder\n\n6.\n\n7. #Import other files/modules\n\n8. from prediction_model.config import config\n\nThe mean of the feature is used for imputing missing numerical values. However, you can use other missing value imputation techniques, such as stochastic regression imputation, interpolation, and cold deck imputation.\n\n1. #Numerical Imputer\n\n2. class NumericalImputer(BaseEstimator,TransformerMixin):\n\n3. \"\"\"Numerical Data Missing Value Imputer\"\"\"\n\n4. def __init__(self, variables=None):\n\n5. self.variables = variables\n\n6.\n\n7. def fit(self, X,y=None):\n\n8. self.imputer_dict_={}\n\nPackaging ML Models  59\n\n9. for feature in self.variables:\n\n10. self.imputer_dict_[feature] = X[feature].mean()\n\n11. return self\n\n12.\n\n13. def transform(self,X):\n\n14. X=X.copy()\n\n15. for feature in self.variables:\n\n16. X[feature].fillna(self.imputer_dict_[feature],inplace=True)\n\n17. return X\n\nThe most frequent value of the feature is used for imputing missing categorical values, but you can also use other missing value imputation techniques, such as missing indicator imputation, Multivariate Imputation by Chained Equations (MICE) algorithm, and systematic random sampling imputation.\n\n1. #Categorical Imputer\n\n2. class CategoricalImputer(BaseEstimator,TransformerMixin):\n\n3. \"\"\"Categorical Data Missing Value Imputer\"\"\"\n\n4. def __init__(self, variables=None):\n\n5. self.variables = variables\n\n6.\n\n7. def fit(self, X,y=None):\n\n8. self.imputer_dict_={}\n\n9. for feature in self.variables:\n\n10. self.imputer_dict_[feature] = X[feature].mode()[0]\n\n11. return self\n\n12.\n\n13. def transform(self, X):\n\n14. X=X.copy()\n\n15. for feature in self.variables:\n\n16. X[feature].fillna(self.imputer_dict_[feature],inplace=True)\n\n17. return X\n\n60  Machine Learning in Production\n\nPost that, categorical features are encoded for model training.\n\n1. #Categorical Encoder\n\n2. class CategoricalEncoder(BaseEstimator,TransformerMixin):\n\n3. \"\"\"Categorical Data Encoder\"\"\"\n\n4. def __init__(self, variables=None):\n\n5. self.variables=variables\n\n6.\n\n7. def fit(self, X,y):\n\n8. self.encoder_dict_ = {}\n\n9. for var in self.variables:\n\n10. t = X[var].value_counts().sort_values(ascending=True). index\n\n11. self.encoder_dict_[var] = {k:i for i,k in enumerate(t,0)}\n\n12. return self\n\n13.\n\n14. def transform(self,X):\n\n15. X=X.copy()\n\n16. #This part assumes that the encoder does not introduce a NANs\n\n17. #In that case, a check needs to be done and the code should break\n\n18. for feature in self.variables:\n\n19. X[feature] = X[feature].map(self.encoder_dict_ [feature])\n\n20. return X\n\nThe following code snippet is used for creating new features:\n\n1. #Temporal Variables\n\n2. class TemporalVariableEstimator(BaseEstimator,TransformerMixin):\n\n3. \"\"\"Feature Engineering\"\"\"\n\n4. def __init__(self, variables=None, reference_variable = None):\n\nPackaging ML Models  61\n\n5. self.variables=variables\n\n6. self.reference_variable = reference_variable\n\n7.\n\n8. def fit(self, X,y=None):\n\n9. #No need to put anything, needed for Sklearn Pipeline\n\n10. return self\n\n11.\n\n12. def transform(self, X):\n\n13. X=X.copy()\n\n14. for var in self.variables:\n\n15. X[var] = X[var]+X[self.reference_variable]\n\n16. return X\n\nApply log transformation on variables for making patterns in the data more interpretable.\n\n1. # Log Transformations\n\n2. class LogTransformation(BaseEstimator, TransformerMixin):\n\n3. \"\"\"Transforming variables using Log Transformations\"\"\"\n\n4. def __init__(self, variables=None):\n\n5. self.variables = variables\n\n6.\n\n7. def fit(self, X,y):\n\n8. return self\n\n9.\n\n10. #Need to check in advance if the features are <= 0\n\n11. #If yes, needs to be transformed properly (E.g., np.log1p(X[var]))\n\n12. def transform(self,X):\n\n13. X=X.copy()\n\n14. for var in self.variables:\n\n15. X[var] = np.log(X[var])\n\n16. return X\n\n62  Machine Learning in Production\n\nThe following code snippet is used for dropping features that are insignificant to the model:\n\n1. # Drop Features\n\n2. class DropFeatures(BaseEstimator, TransformerMixin):\n\n3. \"\"\"Dropping Features Which Are Less Significant\"\"\"\n\n4. def __init__(self, variables_to_drop=None):\n\n5. self.variables_to_drop = variables_to_drop\n\n6.\n\n7. def fit(self, X,y=None):\n\n8. return self\n\n9.\n\n10. def transform(self, X):\n\n11. X=X.copy()\n\n12. X=X.drop(self.variables_to_drop, axis=1)\n\n13. return X\n\npipeline.py\n\nIn this module, sklearn-pipeline is used. The model can be deployed without a sklearn-pipeline, but it is recommended to build a sklearn-pipeline. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\n\n1. #Import Libraries\n\n2. from sklearn.pipeline import Pipeline\n\n3. from sklearn.preprocessing import MinMaxScaler\n\n4. from sklearn.linear_model import LogisticRegression\n\n5.\n\n6. #Import other files/modules\n\n7. from prediction_model.config import config\n\n8. import prediction_model.processing.preprocessors as pp\n\n9.\n\nPackaging ML Models  63\n\n10. loan_pipe=Pipeline([\n\n11. ('Numerical Imputer',pp.NumericalImputer(variables=config. NUMERICAL_FEATURES)),\n\n12. ('Categorical Imputer',pp.CategoricalImputer(variables=config. CATEGORICAL_FEATURES)),\n\n13. ('Temporal Features', pp.TemporalVariableEstimator(variables=config.TEMPORAL_FEATURES,\n\n14. reference_variable=config.TEMPORAL_ADDITION)),\n\n15. ('Categorical Encoder', pp.CategoricalEncoder(variables=config.FEATURES_TO_ENCODE)),\n\n16. ('Log Transform', pp.LogTransformation(variables=config.LOG_ FEATURES)),\n\n17. ('Drop Features', pp.DropFeatures(variables_to_drop=config. DROP_FEATURES)),\n\n18. ('Scaler Transform', MinMaxScaler()),\n\n19. ('Linear Model', LogisticRegression(random_state=1))\n\n20. ])\n\npredict.py\n\nA predict.py module loads the saved ML model (.pkl) and makes predictions on the new data.\n\n1. #Import Libraries\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4. import joblib\n\n5.\n\n6. #Import other files/modules\n\n7. from prediction_model.config import config\n\n8. from prediction_model.processing.data_management import load_ pipeline\n\n9.\n\n10. pipeline_file_name = 'classification_v1.pkl'\n\n64  Machine Learning in Production\n\n11.\n\n12. _loan_pipe = load_pipeline(pipeline_file_name)\n\n13.\n\n14. def make_prediction(input_data):\n\n15. \"\"\"Predicting the output\"\"\"\n\n16.\n\n17. # Read Data\n\n18. data = pd.DataFrame(input_data)\n\n19.\n\n20. # prediction\n\n21. prediction = _loan_pipe.predict(data[config.FEATURES])\n\n22. output = np.where(prediction==1, 'Y', 'N').tolist()\n\n23. results = {'prediction': output}\n\n24. return results\n\nrequirements.txt\n\nThe following libraries are included in this file:\n\n1. # Model Building Requirements\n\n2. joblib==0.16.0\n\n3. numpy==1.19.0\n\n4. pandas==1.0.5\n\n5. scikit-learn==0.23.1\n\n6. scipy==1.5.1\n\n7. sklearn==0.0\n\n8.\n\n9. # testing requirements\n\n10. pytest<5.0.0,>=4.6.6\n\n11.\n\n12. # packaging",
      "page_number": 78
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 90-99)",
      "start_page": 90,
      "end_page": 99,
      "detection_method": "topic_boundary",
      "content": "Packaging ML Models  65\n\n13. setuptools==40.6.3\n\n14. wheel==0.32.3\n\nclassification_v1.pkl\n\nThis is a pickled file from the trained model.\n\ntrain_pipeline.py\n\nThis module loads the training data and passes it to the pipeline, then saves the pickle file of the model to the local directory.\n\n1. #Import Libraries\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4.\n\n5. #Import other files/modules\n\n6. from prediction_model.config import config\n\n7. from prediction_model.processing.data_management import load_ dataset, save_pipeline\n\n8. import prediction_model.processing.preprocessors as pp\n\n9. import prediction_model.pipeline as pl\n\n10. from prediction_model.predict import make_prediction\n\n11.\n\n12. def run_training():\n\n13. \"\"\"Train the model\"\"\"\n\n14. #Read Data\n\n15. train = load_dataset(config.TRAIN_FILE)\n\n16.\n\n17. #separating Loan_status in y\n\n18. y = train[config.TARGET].map({'N':0 , 'Y':1})\n\n19. pl.loan_pipe.fit(train[config.FEATURES],y)\n\n20. save_pipeline(pipeline_to_save=pl.loan_pipe)\n\n66  Machine Learning in Production\n\n21. if __name__=='__main__':\n\n22. run_training()\n\nsetup.py\n\nTo configure and install packages from the source directory, create a setup.py file. It is specific to the package. PIP will use the setup.py file to install packages. Go to the directory where the setup.py file is located and install the packages using the pip install . (period) command.\n\n1. import io\n\n2. import os\n\n3. from pathlib import Path\n\n4.\n\n5. from setuptools import find_packages, setup\n\n6.\n\n7. # Package meta-data\n\n8. NAME = 'prediction_model'\n\n9. DESCRIPTION = 'Train and deploy prediction model.'\n\n10. URL = 'https://github.com/suhas-ds/prediction_model'\n\n11. EMAIL = 'suhasp.ds@gmail.com'\n\n12. AUTHOR = 'Suhas Pote'\n\n13. REQUIRES_PYTHON = '3.6'\n\n14.\n\n15. here = os.path.abspath(os.path.dirname(__file__))\n\n16.\n\n17. # What packages are required for this module to be executed?\n\n18. def list_reqs(fname='requirements.txt'):\n\n19. with io.open(os.path.join(here, fname), encoding='utf-8') as fd:\n\n20. return fd.read().splitlines()\n\n21. try:\n\nPackaging ML Models  67\n\n22. with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f:\n\n23. long_description = '\\n' + f.read()\n\n24. except FileNotFoundError:\n\n25. long_description = DESCRIPTION\n\n26.\n\n27. # Load the package's __version__.py module as a dictionary.\n\n28. ROOT_DIR = Path(__file__).resolve().parent\n\n29. PACKAGE_DIR = ROOT_DIR / NAME\n\n30. about = {}\n\n31. with open(PACKAGE_DIR / 'VERSION') as f:\n\n32. _version = f.read().strip()\n\n33. about['__version__'] = _version\n\n34.\n\n35. setup(\n\n36. name=NAME,\n\n37. version=about['__version__'],\n\n38. description=DESCRIPTION,\n\n39. long_description=long_description,\n\n40. long_description_content_type='text/markdown',\n\n41. author=AUTHOR,\n\n42. author_email=EMAIL,\n\n43. python_requires=REQUIRES_PYTHON,\n\n44. url=URL,\n\n45. packages=find_packages(exclude=('tests',)),\n\n46. package_data={'prediction_model': ['VERSION']},\n\n47. install_requires=list_reqs(),\n\n48. extras_require={},\n\n68  Machine Learning in Production\n\n49. include_package_data=True,\n\n50. license='MIT',\n\n51. classifiers=[\n\n52. 'License :: OSI Approved :: MIT License',\n\n53. 'Programming Language :: Python',\n\n54. 'Programming Language :: Python :: 3',\n\n55. 'Programming Language :: Python :: 3.6',\n\n56. 'Programming Language :: Python :: Implementation :: CPython',\n\n57. 'Programming Language :: Python :: Implementation :: PyPy'\n\n58. ]\n\n59. )\n\nVERSION\n\nThis file holds the version of the package. Here, a major.minor.micro scheme is used.\n\n1. 0.1.0\n\npytest.ini\n\nTo disable warnings while running pytest, you have to configure the pytest.ini file.\n\n1. [pytest]\n\n2. addopts = -p no:warnings\n\ntest_predict.py\n\nIt fetches a single record from the validation data and verifies the output using assert statements.\n\nIt validates the following checks: •\t The output is not null. •\t The output data type is str. •\t The output is Y for given data (fixed).\n\nPackaging ML Models  69\n\nThese checks are run using pytest.\n\n1. #Import libraries\n\n2. import pytest\n\n3. #Import files/modules\n\n4. from prediction_model.config import config\n\n5. from prediction_model.processing.data_management import load_ dataset\n\n6. from prediction_model.predict import make_prediction\n\n7.\n\n8. @pytest.fixture\n\n9. def single_prediction():\n\n10. ''' This function will predict the result for a single record'''\n\n11. test_data = load_dataset(file_name=config.TEST_FILE)\n\n12. single_test = test_data[0:1]\n\n13. result = make_prediction(single_test)\n\n14. return result\n\n15.\n\n16. #Test Prediction\n\n17. def test_single_prediction_not_none(single_prediction):\n\n18. ''' This function will check if the result of prediction is not None'''\n\n19. assert single_prediction is not None\n\n20. def test_single_prediction_dtype(single_prediction):\n\n21. ''' This function will check if the data type of the result of the prediction is str i.e. string '''\n\n22. assert isinstance(single_prediction.get('prediction')[0], str)\n\n23. def test_single_prediction_output(single_prediction):\n\n24. ''' This function will check if the result of the prediction is Y '''\n\n25. assert single_prediction.get('prediction')[0] == 'Y'\n\n70  Machine Learning in Production\n\nSet the environment variable PYTHONPATH and go to the package directory, and then run the following command:\n\npytest -v\n\nIf everything goes well, you should get an output as follows:\n\nFigure 4.3: pytest output\n\nsdist\n\nPython’s sdists are compressed archives (.tar.gz files) containing one or more packages or modules.\n\nThis creates a dist directory containing a compressed archive of the package (for example, <PACKAGE_NAME>-<VERSION>.tar.gz in Linux).\n\nwheel\n\nThis is the binary distribution or bdist, and it supports Windows, Mac, and Linux.\n\nWheels will speed up the installation if you have compiled code extensions, as the build step is not required. A wheel distribution is a built distribution for the current platform. The installable wheel will be created under the dist directory, and a build directory will also be created with the built code.\n\nSet up environment variables and paths You may need to add the path to the environment variables. It allows you to import modules and functions:\n\nOpen the .bashrc file using terminal\n\nsudo nano ~/.bashrc\n\nAdd the path to the package directory. Here’s an example:\n\nPYTHONPATH=/home/suhas/code/packages/prediction_model:$PYTHONPATH\n\nexport PYTHONPATH\n\nPackaging ML Models  71\n\nThen run source ~/.bashrc.\n\nReopen the terminal and test using the following:\n\necho $PYTHONPATH\n\nBuild the package\n\n1. Go to the project directory and install dependencies:\n\npip install -r requirements.txt\n\n2. Create a pickle file:\n\npython prediction_model/train_pipeline.py\n\n3. Creating a source distribution and wheel: python setup.py sdist bdist_wheel\n\nInstall the package Go to the project directory where the setup.py file is located and install this project with the pip command:\n\nTo install the package in editable or developer mode:\n\npip install -e .\n\n. refers to the current directory. •\t -e refers to --editable mode.\n\nNormal installation of the package:\n\npip install .\n\n. refers to the current directory.\n\nYou can push the entire package to the GitHub repository.\n\nTo install it from the GitHub repository.\n\nWith git:\n\npip install git+https://github.com/suhas-ds/prediction_model.git\n\nWithout git:\n\npip install https://github.com/suhas-ds/prediction_model/tarball/master\n\nOr:\n\n72  Machine Learning in Production\n\npip install https://github.com/suhas-ds/prediction_model/zipball/master\n\nOr:\n\npip install https://github.com/suhas-ds/prediction_model/archive/master. zip\n\nAfter building the package, your directory structure should look like this: prediction_model\n\n|── build\n\n| |── bdist.linux-x86_64\n\n| |── lib\n\n| |── prediction_model\n\n|── dist\n\n| |── prediction_model-0.1.0-py3-none-any.whl\n\n| |── prediction_model-0.1.0.tar.gz\n\n|── MANIFEST.in\n\n|── prediction_model\n\n| |── config\n\n| | |── config.py\n\n| | |── __init__.py\n\n| |── datasets\n\n| | |── __init__.py\n\n| | |── test.csv\n\n| | |── train.csv\n\n| |── __init__.py\n\n| |── pipeline.py\n\n| |── predict.py\n\n| |── processing\n\n| | |── data_management.py\n\n| | |── __init__.py\n\n| | |── preprocessors.py\n\n| |── trained_models\n\n| | |── classification_v1.pkl\n\nPackaging ML Models  73\n\n| | |── __init__.py\n\n| |── train_pipeline.py\n\n| |── VERSION\n\n|── prediction_model.egg-info\n\n| |── dependency_links.txt\n\n| |── PKG-INFO\n\n| |── requires.txt\n\n| |── SOURCES.txt\n\n| |── top_level.txt\n\n|── README.md\n\n|── requirements.txt\n\n|── setup.py\n\n|── tests\n\n|── pytest.ini\n\n|── test_predict.py\n\nPackage usage with example Start a Python console (in a virtual environment):\n\n(venv_package) suhas@ds:~/code/packages$ python\n\n1. Python 3.6.9\n\n2. [GCC 8.4.0] on linux\n\n3. Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\nImport the prediction_model package and make the predictions:\n\n1. import prediction_model\n\n2. from prediction_model import train_pipeline\n\n3. from prediction_model.predict import make_prediction\n\n4. import pandas as pd\n\n5.\n\n6. train_pipeline.run_training() # Save the pickle object of the trained model\n\n74  Machine Learning in Production\n\nSaved Pipeline : classification_v1.pkl # Output\n\n7. test_data = pd.read_csv(\"/home/suhas/code/test.csv\") # Load the data\n\n8. result = make_prediction(test_data[0:1])#Make prediction on the first row\n\n9. print(result)\n\n{'prediction': ['Y']}\n\nYou should get the output as Y (that is, ‘Yes’) for the given set of input data.\n\nConclusion This chapter discussed the importance of modular programming in the production environment; virtual environments play a crucial role when you are working on multiple projects. After that, you explored the steps to create and activate a virtual environment. You can list the packages to be installed inside the virtual environment in the requirements.txt file so that all the required packages can be installed in one go. Writing test cases boosts confidence about the package, functions, and overall integrity. Packages are portable and reusable, and they can be easily debugged.\n\nIn the next chapter, you will learn to leverage ML flow for tracking the ML experiments and saving the serialized models.\n\nPoints to remember\n\nThe Python package holds multiple modules, and each module contains functions, classes, and variables.\n\nThe MANIFEST.in file contains the list of files to be included and excluded. •\t The requirements.txt files hold the list of packages to be installed using pip. •\t Python refers to serialization and deserialization by the terms pickling and unpickling, respectively.\n\nThe __Init__.py file indicates that the directory should be treated as a package.\n\nMultiple choice questions\n\n1. Identify a Python module.\n\na)\n\nload.py\n\nb) requirements.txt c) transform.ccm d) MANIFEST.in",
      "page_number": 90
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 100-111)",
      "start_page": 100,
      "end_page": 111,
      "detection_method": "topic_boundary",
      "content": "Packaging ML Models  75\n\n2. Python fixtures can be declared by which of the following?\n\na) fixture b) pytest.fixture c) @pytest.fixture d) fixture.pytest\n\nAnswers 1. a\n\n2. c\n\nQuestions\n\n1. What is a Python module?\n\n2. How can you interpret the output of the pytest command?\n\n3. How can you install Python packages in editable mode?\n\nKey terms\n\nPYTHONPATH: It augments the default search path for modules. The PYTHONPATH variable contains a list of directories whose modules are to be accessed in the Python environment.\n\n\n\nJoblib: Joblib is a set of tools to provide lightweight pipelining in Python. It is used to persist the model for future use, the following in particular:\n\no Transparent disk-caching of functions and lazy re-evaluation (memorize\n\npattern).\n\no Simple parallel computing.\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com\n\n76  Machine Learning in Production\n\nMLflow-Platform to Manage the ML Life Cycle  77\n\nChapter 5 MLflow-Platform to Manage the ML Life Cycle\n\nIntroduction The Machine Learning life cycle involves many challenges. For instance, data scientists need to try different models containing multiple parameters and hyperparameters. They need to keep track of the model that is performing well and its parameters. Next, they need to save the serialized model for reusability. This chapter explains the role of MLflow in an ML life cycle. MLflow is a platform for streamlining machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models. It can manage a complete ML life cycle.\n\nStructure This chapter discusses the following topics:\n\n\n\nIntroduction to MLflow\n\nMLflow tracking •\t MLflow projects •\t MLflow models •\t MLflow model registry\n\n78  Machine Learning in Production\n\nObjectives After studying this chapter, you should be able to train, reuse and deploy ML models using MLflow. You should also be able to track model evaluation metrics and model parameters, pickle the trained models, and compare two model results on MLflow UI. MLflow supports many downstream tools when it comes to deployment. You should be able to pick any iteration (by run ID) that is outperforming others and consume it through Python code, terminal, or postman.\n\nIntroduction to MLflow MLflow is an open-source platform for managing the end-to-end machine learning life cycle.\n\nMLflow allows data scientists to run as many experiments as they want before deploying the model into production; however, it keeps track of model evaluation metrics, such as RMSE and AUC. It also tracks the hyperparameters used while building the model. It enables you to save the trained model along with its best hyperparameters. Finally, it allows you to deploy an ML model into a production server or cloud. You can even keep track of the models being used in staging and production so that other team members can be aware of this information.\n\nMLflow is library-agnostic, that is, you can use any popular ML library with it. Moreover, you can use any popular programming language for it as MLflow functions can be accessed via REST API and Command Line Interface (CLI).\n\nIntegrating MLflow with your existing code is quite easy as it requires minimal changes. If you are working on a local system, it will automatically create a mlrun directory, wherein it stores the output, artifacts, and metadata. It creates a separate directory for each run. However, you can specify the path to create the mlrun directory. MLflow allows you to store information of each run into databases, such as MySQL or PostgreSQL.\n\nMLflow is more useful in the following scenarios:\n\nComparing different models: MLflow offers a UI that allows users to compare different models. You can compare Random Forest vs Logistic regression side by side, along with their model metric and parameters used. MLflow supports a wide range of model frameworks.\n\nCyclic model deployment: In production, it is required to push a new version of the model after every data change, when new requirements come up, or after building a model better than the current one. In these scenarios, MLflow helps keep track of the models that are in staging (pre-production) and models that are in production with versions and brief descriptions.\n\nMLflow-Platform to Manage the ML Life Cycle  79\n\nMultiple dependencies: If you are working on different projects or different frameworks, each of them will have a different set of dependencies. MLflow helps you to maintain dependencies along with your model.\n\nWorking with a large data science team: MLflow stores the model metrics, parameters, time created, versions, users, and so on. This information is accessible to other team members working on the same projects. They can track all the metadata using MLflow UI or SQL table (if you are storing it in the database).\n\nSet up your environment and install MLflow In this section, you will install miniconda and then install mlflow in the conda environment. However, if you already have anaconda or miniconda installed, you can create a conda environment and install mlflow using PIP.\n\nMiniconda installation If you have anaconda installed on your machine, then you can skip the next step, that is, installing and setting up Miniconda.\n\nYou can download Miniconda from the following link:\n\nhttps://docs.conda.io/en/latest/miniconda.html#linux-installers\n\nOr you can simply execute the following command in the terminal (for Linux):\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.10.3- Linux-x86_64.sh\n\nRun the installation: bash Miniconda3-latest-Linux-x86_64.sh\n\nRead and follow the prompts on the installer screens. Restart the terminal when the miniconda installation is completed.\n\nIn the terminal, you will see that the base conda environment is auto-activated. You can disable the auto-activation of the base environment by running the following command in the terminal: conda config --set auto_activate_base false\n\nVerify the conda installation using the following command: conda list\n\nRun the following command to update conda (optional): conda update conda\n\n80  Machine Learning in Production\n\nLet’s create a virtual environment: conda create -n venv python=3.7\n\nWhere:\n\n-n refers to the name of the virtual environment. •\t venv is the name of the virtual environment.\n\nThe preceding command will create a virtual environment with Python version 3.7.\n\nOnce the conda environment is created, it needs to be activated by running the following command: conda activate venv\n\nFinally, install MLflow using pip: pip install mlflow\n\nNote: By default, the MLflow project uses conda for installing dependencies; however, you can proceed without conda by using the –no-conda option, for instance, mlflow run . –no-conda.\n\nAfter installing MLflow, type mlflow in the terminal and hit enter to check its usage, options, and commands. The following figure shows the usage and options of mlflow:\n\nFigure 5.1: MLflow usage, options, and commands\n\nTo open the web UI of MLflow, run the following command in the terminal (refer to figure 5.2):\n\nmlflow ui\n\nMLflow-Platform to Manage the ML Life Cycle  81\n\nFigure 5.2: Starting MLflow UI\n\nOpen the browser; you should see a web UI of MLflow, as shown in the following figure:\n\nFigure 5.3: MLflow UI\n\nYou can execute a series of experiments and capture multiple runs of the experiments. Each experiment can contain multiple runs. You can also capture notes for the experiments. Apart from this, there are many customization options available, such as sorting by the columns, showing or hiding the columns, and changing the view of the table.\n\n82  Machine Learning in Production\n\nNote: Suppose you interrupted the running MLflow UI service and rerun the mlflow ui command; in that case, you may get the error as shown in the following figure:\n\nFigure 5.4: MLflow UI error\n\nIt is because the address port is in use; so, you have to release it first, and then you can rerun the mlflow ui. It can be done using the following command:\n\nsudo fuser -k 5000/tcp\n\nMLflow components MLflow is categorized into four components:\n\nMLflow tracking •\t MLflow projects •\t MLflow models •\t MLflow registry\n\nThese components are devised such that they can work together seamlessly; however, you are free to use individual components as you need. For example, you are free to serve a model using MLflow without using a tracking component.\n\nThe following figure shows the MLflow’s components and their major role:\n\nMLflow-Platform to Manage the ML Life Cycle  83\n\nFigure 5.5: MLflow components\n\nThis chapter will cover each component’s functionality with examples.\n\nMLflow tracking MLflow tracking is an API and UI for logging parameters, code versions, metrics, and artifacts when running your machine learning code and for visualizing the results.\n\nMLflow captures the following information in the form of runs, where each run means executing a block of code:\n\nStart and end time: It records the start and end times of an experiment. •\t Source: It can be the name of the file to launch the run or the MLproject name.\n\nParameters: They contain the data in key-value pairs. These are nothing but the model input parameters you want to capture, such as the number of trees used in a random forest algorithm. For instance, n_estimators is key, and its value is 100. You need to call MLflow’s log_param() to store the parameters.\n\nMetrics: A metric is used to measure the performance of the model, such as the accuracy of the model. It holds the data in a key-value pair; however, the value should be numeric only. You need to call MLflow’s log_metric() to store the metric.\n\n84  Machine Learning in Production\n\nArtifacts: When you want to store a file or object (such as a pickle file of the trained model), then the function of the artifacts comes to the rescue. You can store a serialized trained model, plot, or CSV file using this function, and it can be called using log_artifacts().\n\nFirst, you have to store the file or object in the local directory, and from there, you can save the file or object by providing the path of that directory.\n\nLog data into the run mlflow.set_tracking_uri()\n\nIt connects to MLflow tracking Uniform Resource Identifier (URI). By default, tracking URI is set to the mlruns directory; however, you can set it to a remote server (HTTP/HTTPS), local directory path, or a database like MySQL.\n\n1. import mlflow\n\n2. mlflow.set_tracking_uri('http://localhost:5000')\n\nmlflow.tracking.get_tracking_uri()\n\nThis function will return the current tracking URI of MLflow. mlflow.create_experiment()\n\nIt will create a new experiment. You can capture the runs by providing the experiment ID while executing mlflow.start_run. The experiment name should be unique.\n\n1. exp_id = mlflow.create_experiment(\"Loan_Prediction\")\n\nmlflow.set_experiment()\n\nThis method activates the experiment so that the runs will be captured under the provided experiment. A new experiment is created in case the mentioned experiment does not exist. By default, the experiment is set to ‘Default’, as shown in figure 5.3.\n\nmlflow.start_run()\n\nIt starts a new run or returns the currently active run. You can pass the run name, run ID, and experiment’s name under the current run that needs to be tracked.\n\n1. # For single iteration\n\n2. run = mlflow.start_run()\n\n3.\n\n4. # For multiple iterations\n\n5. with mlflow.start_run(run_name=\"test_ololo\") as run:\n\nmlflow.end_run()\n\nMLflow-Platform to Manage the ML Life Cycle  85\n\nIt ends the currently active run (if any). mlflow.log_param()\n\nIt logs a single key-value parameter in the currently active run. The key and value are both strings. Use mlflow.log_params() to log multiple parameters at once.\n\n1. n_estimators = 100\n\n2. mlflow.log_param(\"n_estimators:\", n_estimators)\n\nmlflow.log_metric()\n\nThis MLflow’s function will track the model metrics, such as the model’s accuracy. To track multiple metrics, use mlflow.log_metrics().\n\n1. accuracy = 0.8\n\n2. mlflow.log_metric(\"accuracy\", accuracy)\n\nmlflow.set_tag()\n\nIt stores the data in a key-value pair. In this, you can set labels for the identification or any specific metric you want to track. To set multiple tags, use mlflow.set_tags().\n\n1. import mlflow\n\n2. with mlflow.start_run():\n\n3. mlflow.set_tag(\"model_version\", \"0.1.0\")\n\nmlflow.log_artifact()\n\nThis function will log or store the files or objects in the artifacts directory; however, you would need to store it in the local directory first, and then it can pull the files or objects.\n\n1. import pandas as pd\n\n2. import mlflow\n\n3.\n\n4. dir_name = 'data_dir'\n\n5. file_name = 'data_dir/cust_sale.csv'\n\n6. data = pd.DataFrame({'Cust_id': [461,462,463], 'Sales': [2631,8462,4837]})\n\n7. data.to_csv(file_name, index=False)\n\n8. mlflow.log_artifacts(dir_name)\n\nmlflow.get_artifact_uri()\n\n86  Machine Learning in Production\n\nIt will return the path to the artifact's root directory, where the artifacts are stored.\n\n1. import mlflow\n\n2. mlflow.get_artifact_uri()\n\n3. './mlruns/0/be1cd88ebd704e9ab7629fd364747e1e/artifacts'\n\nLet’s consider the scenario of loan prediction, where the objective is to predict whether a customer is eligible for a loan. First, import the required libraries.\n\n1. # Importing required packages\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4.\n\n5. from sklearn.linear_model import LogisticRegression\n\n6. from sklearn.ensemble import RandomForestClassifier\n\n7. from sklearn.tree import DecisionTreeClassifier\n\n8.\n\n9. from matplotlib import pyplot as plt\n\n10.\n\n11. from sklearn import preprocessing\n\n12. from sklearn.model_selection import train_test_split, GridSearchCV\n\n13. from sklearn import metrics\n\n14.\n\n15. import mlflow\n\nThe next step is to load the datasets and capture numerical and categorical column names in separate variables:\n\n1. # Reading the data\n\n2. data = pd.read_csv(\"loan_dataset.csv\")\n\n3. num_col = data.select_dtypes(include=['int64','float64']).columns. tolist()\n\n4. cat_col = data.select_dtypes(include=['object']).columns.tolist()\n\n5. cat_col.remove('Loan_Status')\n\n6. cat_col.remove('Loan_ID')",
      "page_number": 100
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 112-124)",
      "start_page": 112,
      "end_page": 124,
      "detection_method": "topic_boundary",
      "content": "MLflow-Platform to Manage the ML Life Cycle  87\n\nTreat missing values for categorical and numerical columns:\n\n1. # Creating a list of categorical and numerical variables\n\n2. for col in cat_col:\n\n3. data[col].fillna(data[col].mode()[0], inplace=True)\n\n4.\n\n5. for col in num_col:\n\n6. data[col].fillna(data[col].median(), inplace=True)\n\nCap extreme values to the 5th and 95th percentile for numerical data.\n\n1. # Clipping extreme values\n\n2. data[num_col] quantile([0.05, 0.95]))) =\n\ndata[num_col].apply(lambda\n\nx:\n\nx.clip(*x.\n\nCreate a new feature named TotalIncome, which is the sum of the applicant's income and the co-applicant’s income.\n\n1. # creating a new feature as Total Income\n\n2. data['LoanAmount'] = np.log(data['LoanAmount']).copy()\n\n3. data['TotalIncome'] = data['ApplicantIncome'] + data['CoapplicantIncome']\n\n4. data['TotalIncome'] = np.log(data['TotalIncome']).copy()\n\nDrop the applicant income and co-applicant income columns.\n\n1. # Dropping ApplicantIncome and CoapplicantIncome\n\n2. data = data.drop(['ApplicantIncome','CoapplicantIncome'], axis=1)\n\nConvert categorical columns to numeric columns using a label encoding technique.\n\n1. # Label encoding categorical variables\n\n2. for col in cat_col:\n\n3. le = preprocessing.LabelEncoder()\n\n4. data[col] = le.fit_transform(data[col])\n\n5.\n\n6. data['Loan_Status'] = le.fit_transform(data['Loan_Status'])\n\nSplit the data into train and test (70:30).\n\n1. # Train test split\n\n2. X = data.drop(['Loan_Status', 'Loan_ID'],1)\n\n88  Machine Learning in Production\n\n3. y = data.Loan_Status\n\n4.\n\n5. SEED = 1\n\n6.\n\n7. X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.3, random_state = SEED)\n\nBuild a logistic regression model using a grid search cross-validation approach.\n\n1. #______________Logistic Regresssion__________________________#\n\n2.\n\n3. lr = LogisticRegression(random_state=SEED)\n\n4. lr_param_grid = {\n\n5. 'C': [100, 10, 1.0, 0.1, 0.01],\n\n6. 'penalty': ['l1','l2'],\n\n7. 'solver':['liblinear']\n\n8. }\n\n9.\n\n10. lr_gs = GridSearchCV(\n\n11. estimator=lr,\n\n12. param_grid=lr_param_grid,\n\n13. cv=5,\n\n14. n_jobs=-1,\n\n15. scoring='accuracy',\n\n16. verbose=0\n\n17. )\n\n18. lr_model = lr_gs.fit(X_train, y_train)\n\nBuild a decision tree model using a grid search cross-validation approach.\n\n1. #___________________Decision Tree__________________________#\n\n2.\n\n3. dt = DecisionTreeClassifier(\n\n4. random_state=SEED\n\n5. )\n\n6.\n\nMLflow-Platform to Manage the ML Life Cycle  89\n\n7. dt_param_grid = {\n\n8. \"max_depth\": [3, 5, 7, 9, 11, 13],\n\n9. 'criterion' : [\"gini\", \"entropy\"],\n\n10. }\n\n11.\n\n12. dt_gs = GridSearchCV(\n\n13. estimator=dt,\n\n14. param_grid=dt_param_grid,\n\n15. cv=5,\n\n16. n_jobs=-1,\n\n17. scoring='accuracy',\n\n18. verbose=0\n\n19. )\n\n20. dt_model = dt_gs.fit(X_train, y_train)\n\nBuild a random forest model using a grid search cross-validation approach.\n\n1. #_______________Random Forest___________________________#\n\n2.\n\n3. rf = RandomForestClassifier(random_state=SEED)\n\n4. rf_param_grid = {\n\n5. 'n_estimators': [400, 700],\n\n6. 'max_depth': [15,20,25],\n\n7. 'criterion' : [\"gini\", \"entropy\"],\n\n8. 'max_leaf_nodes': [50, 100]\n\n9. }\n\n10.\n\n11. rf_gs = GridSearchCV(\n\n12. estimator=rf,\n\n13. param_grid=rf_param_grid,\n\n14. cv=5,\n\n15. n_jobs=-1,\n\n16. scoring='accuracy',\n\n17. verbose=0\n\n90  Machine Learning in Production\n\n18. )\n\n19. rf_model = rf_gs.fit(X_train, y_train)\n\nCreate a function for evaluating the model’s metrics.\n\n1. # Model evaluation metrics\n\n2. def model_metrics(actual, pred):\n\n3. accuracy = metrics.accuracy_score(y_test, pred)\n\n4. f1 = metrics.f1_score(actual, pred, pos_label=1)\n\n5. fpr, tpr, thresholds1 = metrics.roc_curve(y_test, pred)\n\n6. auc = metrics.auc(fpr, tpr)\n\n7. plt.figure(figsize=(8,8))\n\n8. # plot auc\n\n9. plt.plot(fpr, tpr, color='blue', label='ROC curve area = %0.2f'%auc)\n\n10. plt.plot([0,1],[0,1], 'r--')\n\n11. plt.xlim([-0.1, 1.1])\n\n12. plt.ylim([-0.1, 1.1])\n\n13. plt.xlabel('False Positive Rate', size=14)\n\n14. plt.ylabel('True Positive Rate', size=14)\n\n15. plt.legend(loc='lower right')\n\n16.\n\n17. # Save plot\n\n18. plt.savefig(\"plots/ROC_curve.png\")\n\n19.\n\n20. # Close plot\n\n21. plt.close()\n\n22.\n\n23. return(accuracy, f1, auc)\n\nCreate a function for capturing information like the model parameters and model metrics using MLflow.\n\n1. # MLflow's logging functions\n\n2. def mlflow_logs(model, X, y, name):\n\nMLflow-Platform to Manage the ML Life Cycle  91\n\n3.\n\n4. with mlflow.start_run(run_name = name) as run:\n\n5.\n\n6. # Run id\n\n7. run_id = run.info.run_id\n\n8. mlflow.set_tag(\"run_id\", run_id)\n\n9.\n\n10. # Make predictions\n\n11. pred = model.predict(X)\n\n12.\n\n13. # Generate performance metrics\n\n14. (accuracy, f1, auc) = model_metrics(y, pred)\n\n15.\n\n16. # Logging best parameters\n\n17. mlflow.log_params(model.best_params_)\n\n18.\n\n19. # Logging model metric\n\n20. mlflow.log_metric(\"Mean cv score\", model.best_score_)\n\n21. mlflow.log_metric(\"Accuracy\", accuracy)\n\n22. mlflow.log_metric(\"f1-score\", f1)\n\n23. mlflow.log_metric(\"AUC\", auc)\n\n24.\n\n25. # Logging artifacts and model\n\n26. mlflow.log_artifact(\"plots/ROC_curve.png\")\n\n27. mlflow.sklearn.log_model(model, name)\n\n28.\n\n29. mlflow.end_run()\n\nPredict on test data and capture the model metrics and parameters using the preceding function.\n\n1. # Make predictions using ML models\n\n2.\n\n92  Machine Learning in Production\n\n3. mlflow_logs(dt_model, X_test, y_test, \"DecisionTreeClassifier\")\n\n4. mlflow_logs(lr_model, X_test, y_test, \"LogisticRegression\")\n\n5. mlflow_logs(rf_model, X_test, y_test, \"RandomForestClassifier\")\n\nActivate the virtual environment: conda activate venv\n\nThe following figure shows the activation and deactivation commands after successfully creating a conda environment.\n\nFigure 5.6: Conda activation command\n\nRun the train.py file, which will capture the parameters and metrics and store the artifacts at a given location.\n\nStart the MLflow UI and then run the train.py file, as shown in the following figure.\n\nFigure 5.7: MLflow UI command for keeping it running in the background\n\npython train.py\n\nThis does not print anything, so one can check the output on MLflow’s UI.\n\nFor demonstration, three different classifiers are implemented, namely, logistic regression, decision tree, and random forest, to compare their performances.\n\nThe following figure shows that model metrics, parameters, model names, and run ID have been captured. Overall, the logistic regression’s accuracy is 78.9%, which is slightly better than the others.\n\nMLflow-Platform to Manage the ML Life Cycle  93\n\nFigure 5.8: MLflow UI for train.py output\n\nCapture MLflow logs under the Loan_prediction experiment instead of the Default experiment.\n\n1. # Make predictions using ML models\n\n2. mlflow.set_experiment(\"Loan_prediction\")\n\n3. mlflow_logs(dt_model, X_test, y_test, \"DecisionTreeClassifier\")\n\n4. mlflow_logs(lr_model, X_test, y_test, \"LogisticRegression\")\n\n5. mlflow_logs(rf_model, X_test, y_test, \"RandomForestClassifier\")\n\nBy default, MLflow will capture the information under the experiment name Default. However, you can specify the experiment name in the command itself, as follows:\n\npython train.py –experiment-name Loan_prediction\n\nThis command won’t print any output in the terminal as there are no print statements in the train.py file.\n\n94  Machine Learning in Production\n\nNow, the information is getting captured under the Loan_prediction experiment, as shown in the following figure:\n\nFigure 5.9: Mlflow UI for a new experiment\n\nYou can deactivate the virtual environment using the following command:\n\nconda deactivate\n\nMLflow projects Once you are done with the experimentation phase, your next step would be packaging all the code as a project with its dependencies. Let’s say you want to shift the codebase and dependencies to the server or to another machine; MLflow will do the job for you.\n\nMLflow allows you to package the codebase and its dependencies to make it reproducible and reusable. MLflow projects provide API and CLI capabilities that will help you integrate your model in MLOps.\n\nYou can run the MLflow project directly from the remote git repository (provided it should contain all the necessary files); alternatively, you can run it from the local CLI.\n\nFollowing are the fields of the MLproject file:\n\nName: It is the name of the project, and it can be any text. •\t Environment: This is the environment that will be used at the time of execution of the entry point command. This will contain dependencies/ packages required by the entry point or MLflow project.\n\nMLflow-Platform to Manage the ML Life Cycle  95\n\nEntry point: The entry point section holds the command to be executed inside the MLflow project environment. This command can take arguments; it is a mandatory field and cannot be left blank.\n\nParameters: This section holds one or more arguments that will be used by the entry point commands, but it is optional.\n\nCreate a conda.yaml file:\n\n1. name: Loan_prediction\n\n2.\n\n3. channels:\n\n4. - defaults\n\n5.\n\n6. dependencies:\n\n7. - python=3.7\n\n8. - pip\n\n9. - pip:\n\n10. - mlflow\n\n11. - numpy==1.19.5\n\n12. - pandas==1.1.5\n\n13. - matplotlib==3.3.4\n\n14. - scikit-learn==0.24.2\n\nThen, create an MLproject file:\n\n1. name: Loan_prediction\n\n2.\n\n3. conda_env: conda.yaml\n\n4.\n\n5. entry_points:\n\n6. main:\n\n7. command: “python train.py”\n\nSwitch to the directory where the MLproject file and the conda environment are present. Locate the YAML file and run: mlflow run .\n\n96  Machine Learning in Production\n\nYou can create and set the experiment name using the CLI, as shown in the following figure.\n\nmlflow.set_experiment(\"Loan_prediction\")\n\nFigure 5.10: Creating a new experiment using the command\n\nmlflow run . --experiment-name Loan_prediction\n\nThe following figure shows the output of the preceding command:\n\nFigure 5.11: Output of ML project’s run command\n\nRun the following command in your terminal to run MLproject from the GitHub repository:\n\nmlflow run https://github.com/suhas-ds/mlflow_loan_prediction --experiment-name Loan_prediction\n\nThe following figure shows the output of the preceding command:\n\nFigure 5.12: Running ML project from GitHub\n\nMLflow-Platform to Manage the ML Life Cycle  97\n\nYou can see the new experiment Loan_prediction created and the information captured in it.\n\nWhen you open the details of the LogisticRegression model, you can see Git Commit, as this was run directly from GitHub.\n\nThe following figure shows the output for logistic regression after running the MLflow project from the GitHub repository.\n\nFigure 5.13: Logistic regression results\n\nMLflow models The MLflow models module lets you package the model in different ways, such as python function, Scikit-learn (sklearn), and Spark MLlib (spark). This flexibility helps you to connect associated downstream tools effortlessly.\n\nWhen you log the model using mlflow.sklearn.log_model(model, name), a model directory gets created, and it stores the files and metadata associated with the models. You will see the following directory structure:\n\nLogisticRegression/\n\n├── conda.yaml\n\n├── MLmodel\n\n├── model.pkl\n\n└── requirements.txt\n\n98  Machine Learning in Production\n\nThe following figure shows the MLmodel details for the logistic regression model:\n\nFigure 5.14: MLmodel\n\nNow, let’s predict by following the instruction under Predict on a Pandas DataFrame, as shown in the following figure:\n\nFigure 5.15: MLflow model\n\nNow open the Python console by typing python in the terminal. Here, the aim is to create a pandas DataFrame and pass it to the predict function. You can load the DataFrame from the local directory.\n\nIn the following figure, a pandas DataFrame is used for prediction using MLflow’s model.\n\nMLflow-Platform to Manage the ML Life Cycle  99\n\nFigure 5.16: Making predictions on pandas DataFrame using MLflow model\n\nNow, deploy a local REST server to serve the predictions using the MLmodel.\n\nBy default, the server runs on port 5000. If the port is already in use, you can use the --port or -p option to provide a different port.\n\nFor instance, mlflow models serve -m runs: /<RUN_ID>/model --port 1234.\n\nTo deploy to the server, run the following command:\n\nmlflow models serve -m /home/suhas/mlb/mlflow/ mlruns/0cc5b1a8724f4e818b4e92776ca21b73/0/artifacts/LogisticRegression/ -p 1234\n\nIn the preceding command, replace the model path with the model path used in your machine and provide a port 1234 to run on.\n\nIn the following figure, the MLflow model generated using logistic regression is deployed:\n\nFigure 5.17: Deploying MLflow model\n\nThe preceding figure illustrates that the server is listening at http://127.0.0.1:1234/.\n\nCall the REST API using the following curl command: curl -X POST -H \"Content-Type:application/json; format=pan- das-split\" --data '{\"columns\":[\"Gender\",\"Married\",\"Depen- dents\",\"Education\",\"Self_Employed\",\"LoanAmount\",\"Loan_Amount_ Term\",\"Credit_History\",\"Property_Area\",\"TotalIncome\"],\"da- ta\":[[1.0,0.0,0.0,0.0,0.0,4.85203026,360.0,1.0,2.0,8.67402599]]}' http://127.0.0.1:1234/invocations",
      "page_number": 112
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 125-133)",
      "start_page": 125,
      "end_page": 133,
      "detection_method": "topic_boundary",
      "content": "100  Machine Learning in Production\n\nThe following figure depicts calling the REST API of the deployed model using the curl command:\n\nFigure 5.18: Curl command output\n\nOptionally, you can call the REST API of the deployed model using the curl command, as shown in the following figure:\n\nFigure 5.19: Curl command output\n\nThe following figure shows the output of a different set of input data:\n\nFigure 5.20: Curl command output\n\nThe server should respond with an output similar to [0] or [1].\n\nHere, [0] is labeled as No, and [1] is labeled as Yes.\n\nMLflow registry MLflow registry is a platform for storing and managing ML models through UI and a set of APIs.\n\nIt keeps track of the model lineage, different versions, and transitions of the models from one state to another, like from staging to production. Every authorized team member can track all the preceding information.\n\nMLflow-Platform to Manage the ML Life Cycle  101\n\nTo explore this component, the database needs to be connected to MLflow. MLflow components explored earlier can be connected to a database for storing the information.\n\nSet up MLflow’s tracking URI using the following command: export MLFLOW_TRACKING_URI=http://localhost:5000\n\nSet up the MySQL server for MLflow You can install MySQL server if it is not installed already. For this, you can also refer to the official document at https://dev.mysql.com/doc/mysql-apt-repo-quick- guide/en/.\n\nFirst, create mlflow_user in MySQL using the following command: mysql -u mlflow_user\n\nCREATE USER 'mlflow_user'@'localhost' IDENTIFIED BY 'mlflow'\n\nGRANT ALL ON db_mlflow.* TO 'mlflow_user'@'localhost';\n\nFLUSH PRIVILEGES;\n\nEnter the password when prompted.\n\nCreate and select the database: CREATE DATABASE IF NOT EXISTS db_mlflow;\n\nuse db_mlflow;\n\nTo know which user(s) have the access to db_mlflow database and its privileges, you can execute the following command:\n\nSELECT * FROM mysql.db WHERE Db = 'db_mlflow'\\G;\n\n102  Machine Learning in Production\n\nThe following figure shows the privileges of mlflow_user associated with the db_ mlflow database:\n\nFigure 5.21: Permissions of MLflow_user\n\nInstall the MySQLdb module:\n\nsudo apt-get install python3-mysqldb\n\nInstall MySQL client for Python:\n\npip install mysqlclient\n\nHere are the concepts and key features of the model registry:\n\nRegistered model: Once the model is registered using MLflow’s UI or API, the model is considered a registered model. MLflow’s model registry captures model versions and keeps track of the model’s stages (for example, production, and staging) and other metadata.\n\nModel version: MLflow’s model registry maintains the version of each model after registering it. For instance, if you saved a model name with a classification model, then it would be assigned to version 1 by default. However, after saving that model with the same name again, it will be saved as version 2.\n\nModel stage: For each model version, you can assign different stages, like staging, production, and archived. However, you cannot assign two stages to the same version of the model.\n\nMLflow-Platform to Manage the ML Life Cycle  103\n\nAnnotations and descriptions: You can add comments, short descriptions, and annotations for models. Your team members will come to know about the model through the descriptions you add.\n\nStart the MLflow server All the required steps to start the MLflow server with MySQL as a database have been completed.\n\nTo start the MLflow server, you can use the following command:\n\nmlflow server --host 0.0.0.0 --port 5000 --backend-store-uri mysql:// mlflow_user:mlflow@localhost/db_mlflow --default-artifact-root $PWD/mlruns\n\nThe syntax to start the MLflow server is mlflow server <args>. In the preceding command, MLflow uses the backend store as MySQL server (provide MLflow username and database name) and the default artifacts store as the mlruns directory.\n\nYou can run the MLproject and pass the experiment name as a parameter (optional). mlflow run . --experiment-name ‘Loan_prediction’\n\nYou can see, in the MLflow UI, that the version column shows alphanumeric values. The model’s metrics parameters are displayed under the Loan_prediction experiment.\n\nThe following figure shows the output of the Loan_prediction experiment using MySQL as a backend store:\n\nFigure 5.22: Output of ML project on MLflow UI\n\n104  Machine Learning in Production\n\nUsing the show tables; command, you can see that new tables have been created, such as metrics and experiments. Refer to Figure 5.23 to see the output of the command:\n\nFigure 5.23: MLflow tables in MySQL database\n\nLet’s check the experiments table using the command shown in Figure 5.24. As of now, Default and Loan_prediction are the two experiments being displayed.\n\nFigure 5.24: MLflow’s list of experiments is MySQL database\n\nIf you click on the Models tab on the top, you should see the following figure. As the model is not registered, the table does not contain any model information.\n\nFigure 5.25: Initial UI of MLflow models\n\nMLflow-Platform to Manage the ML Life Cycle  105\n\nNow, go to the run ID on the Experiment tab and scroll down; then, click on the model directory in the UI.\n\nThe Register Model button will appear, as shown in the following figure:\n\nFigure 5.26: Register Model button\n\nClick on the Register Model button, and a pop-up window will appear, as shown in the following figure. In the current case, the Create New Model option is selected from the drop-down menu, along with a model name; however, you can give any human-readable name to it. Finally, click on the Register button.\n\nFigure 5.27: Register model window\n\nAs shown in the following figure, the Register Model button will change to the model’s name (with hyperlink).\n\nFigure 5.28: Registered model\n\nNow, you should see the registered model on the Models tab. By default, it will label it as version 1. Since a state has not been assigned, you should see – (dash) in the staging and production columns.\n\n106  Machine Learning in Production\n\nThe following figure shows the list of registered models:\n\nFigure 5.29: Registered model in MLflow Models UI\n\nWe can change the model’s state using MLflow UI or terminal. Here, the state is being changed to staging using MLflow’s UI.\n\nAs shown in the following figure, the model’s state is being changed from None to Staging:\n\nFigure 5.30: Registered model’s state transition\n\nIn the following figure, you can see that it is showing Version 1 in the staging column of the registered model.\n\nMLflow-Platform to Manage the ML Life Cycle  107\n\nFigure 5.31: Registered model’s state transitioned to staging\n\nThe registered model’s information can be found in the registered_models table in MySQL, as shown in the following figure:\n\nFigure 5.32: List of registered models in MySQL table\n\nWhen you are working in a team, you can add descriptions and tags to the registered model. It helps other team members to learn more about the model.\n\nNow, add descriptions and tags for Prediction_Model_LR.\n\nFigure 5.33: Registered model’s tags and description\n\n108  Machine Learning in Production\n\nYou have learned how to register models in MLflow, and now you are going to learn how to serve the model to make predictions on the given data.\n\nDeploy the model from MLflow’s registry using the following command:\n\nmlflow models serve -m \"models:/Prediction_model_LR/Staging\"\n\n1. import mlflow.pyfunc\n\n2.\n\n3. model_name = \"Prediction_model_LR\"\n\n4. stage = 'Staging'\n\n5. model=mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/ {stage}\")\n\n6.\n\n7. model.predict([[1.,1.,0.,1.,0.,4.55387689,360.,1.,2.,8.25556865]])\n\n8. array([1])\n\nNote: You can call the model’s REST API using postman, as shown in the following figure.\n\nFigure 5.34: Calling REST API of the deployed model using postman\n\nYou can get postman from the following link:\n\nhttps://www.postman.com/downloads/",
      "page_number": 125
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 134-147)",
      "start_page": 134,
      "end_page": 147,
      "detection_method": "topic_boundary",
      "content": "MLflow-Platform to Manage the ML Life Cycle  109\n\nThere are various ways to consume the REST API of deployed models. You can also use this REST API in other applications.\n\nConclusion This chapter explained the importance of MLflow in the ML life cycle and the production environment. You explored the role, functionality, and usage of MLflow, with examples of four components of MLflow. Then, you learned how MLflow helps data science developers at various stages of the ML life cycle. MLflow tracking allows one to choose the best model by keeping track of model metrics, hyperparameters used, and other useful information. The MLflow project component helps you to manage dependencies, and it can be run from the GitHub repository. The MLflow registry acts as a central location for registered models and changing the states, such as staging and production.\n\nIn the next chapter, you will learn how to use a docker for portability in transferring ML projects from one machine to another or to the server.\n\nPoints to remember\n\nMLflow helps you from the experimentation stage to the deployment stage of an ML project.\n\nExcept for the model registry, all the components can be used without being integrated with a database like MYSQL; however, it is a good practice to integrate them with a database like MYSQL.\n\nBy default, the MLflow project uses conda for installing dependencies; however, you can proceed without conda by using the –no-conda option.\n\nEach MLflow component can be accessed separately; however, you can connect them to create the flow.\n\nMultiple choice questions\n\n1. Which one of the following is not a MLflow component?\n\na) MLflow tracking\n\nb) MLflow develop\n\nc) MLflow models\n\nd) MLflow registry\n\n110  Machine Learning in Production\n\n2. In the MLflow registry, the model state from staging to production can be changed using which of the following?\n\na) MLflow command\n\nb) MLflow UI\n\nc) Both a and b\n\nd) Mode state cannot be changed from staging to production\n\nAnswers 1. b\n\n2. c\n\nQuestions\n\n1. What is MLflow?\n\n2. What is the role of the conda.yaml file?\n\n3. What is the command to serve the model?\n\nDocker for ML  111\n\nChapter 6 Docker for ML\n\nIntroduction Containers are an abstraction at the app layer that packages code and dependencies together. Multiple containers can be run on the same machine and share the OS kernel. Each running container is considered an isolated process in user space.\n\nDocker automates the repetitive and time-consuming configuration task, saving both time and effort for the developer. Docker is a one-stop solution that includes UI, APIs, CLIs, and last but not least, security. Docker runs on Linux, Windows, and Mac OS.\n\nStructure This chapter discusses the following topics: •\t Role of Docker in Machine Learning •\t Hello World with Docker •\t Create a Dockerfile •\t Build Docker image •\t Run Docker container •\t Dockerize and deploy the ML model\n\n112  Machine Learning in Production\n\nObjectives After studying this chapter, you should be able to build and deploy ML models into production as a container or orchestration service using Docker. You should be able to create a Dockerfile, which contains all the commands to be executed. Using Dockerfile, you should be able to build and run Docker images. You should also be able to run container service in the background using a detached mode. Finally, you can deploy an ML model using Docker.\n\nIntroduction to Docker Docker is a containerization platform to package applications and their dependencies in the form of a container. It ensures that all the required libraries and dependencies are wrapped in an isolated environment to run the application smoothly in the development, test, and production environments. Docker is popular among developers as it is lightweight, fast, portable, secure, and more efficient than virtual machines. A containerized application will start running as soon as you run the Docker container.\n\nDocker has its own Docker registry, called Docker hub. Docker hub allows developers to store and distribute container images over the internet. An image tag enables developers to differentiate images. A Docker registry has public and private repositories. A developer can store a container image on the Docker hub using the push command and retrieve one using the pull command.\n\nIt works on my machine! Many a time, the code works perfectly on your machine but throws an error when you run it on another machine. This happens with developers and data scientists as well. The reason could be anything from a different OS to a different release of an OS, different python versions, or dependency issues. So, when they face this issue, they might end up spending a lot of time fixing it. With Docker, you should not encounter these issues, as Docker packages require files, configuration, and commands for seamless flow.\n\nLong setup The traditional method of deploying in production environments takes a lot of time, as it needs to move the necessary files, install dependencies, configure, and save the output manually. Many a time, developers need to set up the environment first. It is more time-consuming when you have to repeat this process for different stages of the project, such as development, pre-production, and production. Manual deployment scripts are difficult to manage.\n\nDocker for ML  113\n\nWith Docker, you can put all the required files in the directory and write down the configuration, OS version, and commands to be executed sequentially in a Dockerfile. You can also connect the two Docker containers with the same network. Additionally, you can use the same Dockerfile for development, pre-production, and production.\n\nDocker ensures reproducibility, portability, easy deployment, granular updates, lightness, and simplicity.\n\nSetting up your environment and installing Docker You can refer to the instructions at https://docs.docker.com/installation to install the latest Docker-maintained package on your preferred operating system and take a look at the official Docker engine installation guide at https://docs.docker.com/ engine/install/ubuntu/.\n\nDocker installation Here are the general steps you can follow to install Docker on your Ubuntu machine. The Docker engine installation requires one of the following 64-bit Ubuntu versions:\n\nUbuntu Kinetic 22.10 •\t Ubuntu Jammy 22.04 (LTS) •\t Ubuntu Focal 20.04 (LTS) •\t Ubuntu Bionic 18.04 (LTS)\n\nUninstall old versions First off, uninstall old versions of Docker (if any): sudo apt-get remove docker docker-engine docker.io containerd runc\n\nSet up the repository:\n\n$ sudo apt-get update\n\n$ sudo apt-get install \\\n\nca-certificates \\\n\ncurl \\\n\ngnupg \\\n\nlsb-release\n\n114  Machine Learning in Production\n\nAdd Docker’s official GPG key.\n\nThe GNU Privacy Guard (GPG or GnuPG) is a command-line tool that enables the implementation of public-key encryption and verification services. GPG is commonly used in Linux to sign files digitally, which assures the authenticity of software project files.\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\nUse the following command to set up the repository: $ echo \\\n\n\"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\n\n$(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker. list > /dev/null\n\nInstall Docker Engine sudo apt-get update\n\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n\nsudo docker –version\n\nThe following figure shows the current Docker version:\n\nFigure 6.1: Docker version\n\nRun the following command in the terminal to check the status of the Docker service:\n\nsudo systemctl status docker\n\nThe following figure shows the output of the preceding command:\n\nFigure 6.2: Docker status check\n\nDocker for ML  115\n\nDocker compose Docker compose enables developers to configure and run more than one container. It reads the configuration from the docker-compose.yml file. A single docker- compose up command can start the services and run the multi-container applications. On the other hand, you can destroy all of this using the docker-compose down command. You can also remove the volumes by adding the -- volumes flag.\n\nYou can install the Docker compose using the following command: sudo curl -L \"https://github.com/docker/compose/releases/ download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/ bin/docker-compose\n\nThe following figure shows the Docker compose installation:\n\nFigure 6.3: Docker compose installation\n\nAuthorize docker-compose to execute files: sudo chmod +x /usr/local/bin/docker-compose\n\nFinally, verify the installation: docker-compose --version\n\nThe following figure shows the current Docker compose version:\n\nFigure 6.4: Docker compose version\n\nThe installation provides the following:\n\nDocker Engine •\t Docker CLI client •\t Docker Compose\n\n116  Machine Learning in Production\n\nHello World with Docker This is a sample Docker image to test whether Docker is working properly. By running the following command, you will create the first Docker container:\n\nsudo docker run hello-world\n\nThe following figure shows the execution of the preceding command:\n\nFigure 6.5: Docker image - hello-world\n\nIt also generates the following output, which tells you about what happened behind the scenes:\n\nTo generate this message, Docker took the following steps:\n\n1. The Docker client contacted the Docker daemon.\n\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64)\n\n3. The Docker daemon created a new container from that image, which runs the executable that produces the output you are currently reading.\n\n4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.\n\nYou executed the docker run command, followed by the image name. The required image was not there in the system, so rather than stopping further execution, it pulled the image from the Docker hub.\n\nThe following figure shows the Docker structure in a system. As you can see, Docker can have multiple containers running in an isolated environment while sharing the same infrastructure.\n\nDocker for ML  117\n\nFigure 6.6: Docker stack\n\nDocker objects While working with Docker, you can create and use Docker objects like images, containers, volumes, and networks. In this chapter, you are going to learn about some of these objects.\n\nDockerfile Dockerfile can be considered as a set of commands or instructions that enables developers to build Docker images. These commands or instructions get executed sequentially. It is a plain text document with no extension.\n\nDocker image To create a Docker container, a Docker image needs to be created. It stores all the code and dependencies required to run the application and acts as a template to run a container instance. A Docker image can be uploaded on the Docker hub, from where it can be pulled to the server or system to run the container.\n\nTo view the list of available Docker images, execute the following command in the terminal:\n\ndocker images\n\n118  Machine Learning in Production\n\nDocker containers An instance of a container is created when you run the Docker image. You can use the same Docker image to run as many containers as you want. It is an important component of the Docker ecosystem. Docker runs containers in an isolated environment.\n\nAn additional layer called a container layer, gets automatically created on top of the existing image layers when the developer runs a container. A Docker container has its own read and write layer, which allows developers to make changes that are specific to that container. Suppose you are running three containers using the same Docker image, and you install another version of the python package inside a running container. This will not affect the existing version of the python package in other containers. Docker manages the data within the Docker container using Docker Volumes.\n\nAll the files that you created in an image or a container are part and parcel of the Union file system. However, the data volume is part of the Docker host file system, and it is simply mounted inside the container.\n\nIt is initialized when the container is created. By default, it is not deleted when the container is stopped. Data volumes can be shared across containers too and can be mounted in read-only mode as well.\n\nThe command to check all the running containers is docker ps.\n\nThe command to check all the running and stopped containers is docker ps -a.\n\nDetached mode To run Docker in the background, run the container in detached mode. You can use the -d flag to run the container in detached mode.\n\nDocker container networking Typically, a Docker host comprises multiple Docker containers. Docker containers also need to interact and collaborate with local as well as remote ones to come out with distributed applications. The bridge network is the default network interface that Docker Engine assigns to a container. docker network ls\n\nThis command will show you the list of networks and their scope. The output contains the following headers: NETWORK ID, NAME, DRIVER, SCOPE\n\nTo check the network details of any Docker container, use the following command, followed by the network ID:\n\nDocker for ML  119\n\ndocker network inspect <NETWORK ID>\n\nPort mapping\n\nThe -p flag is used to map container ports to host ports. Consider this example:\n\ndocker run -p <myport>:<containerport> nginx\n\nNote: To observe the output, use docker logs [container_id].\n\nCreate a Dockerfile Dockerfile follows a simple and easy-to-understand structure, that is, # comment, followed by an instruction and an argument. It is a standard practice to write instructions in the uppercase to differentiate between instructions and arguments.\n\nFROM\n\nThe FROM instruction sets the base image for subsequent instructions. •\t A valid Dockerfile must have a FROM instruction. •\t FROM can occur multiple times in the Dockerfile.\n\nCMD\n\nCMD defines a default command to execute when a container is being created. •\t CMD does not get executed while building an image. •\t Can be overridden at runtime.\n\nRUN\n\nIt executes a command in a new layer on top of the current image and commits the results.\n\nCOPY\n\nThe COPY instruction copies new files or directories from <src> and adds them to the file system of the container at the path <dest>.\n\nENTRYPOINT\n\nThis helps you to configure the container as an executable; it is similar to CMD. There can be at max one instruction for ENTRYPOINT; if more than one is specified, only the last one will be honored.\n\n120  Machine Learning in Production\n\nWORKDIR <path>\n\nThis sets the working directory for the RUN, CMD, and ENTRYPOINT instructions that follow it.\n\nEXPOSE\n\nThis exposes the network ports on the container, on which it will listen at runtime.\n\nENV\n\nThis will set the environment variables <key> to <value> within the container. When a container is running from the resulting image, it will pass and persist all the information to the application running inside the container.\n\nBuild a Docker image Using the docker build command, users can automate a docker build that executes several command-line instructions in succession.\n\nThe docker build command builds an image from a Dockerfile and a context:\n\ndocker build -t ImageName:TagName dir\n\nWhere:\n\n-t: Image tag ● ImageName: The name you want to give to your image ● TagName: The tag you want to give to your image ● dir: The directory where the Dockerfile is present\n\nFor the current directory, simply use . (period):\n\nsudo docker build –t myimage:v1 .\n\nCheck the newly created image using the docker images command.\n\nThe next step is to build the container from the newly created image.\n\nNote: To check all the commands run against that image, execute the following command docker history [Image_id]\n\nRun a Docker container A Docker container is a runtime instance of a Docker image. The docker run command can run the Docker image as follows: docker run --name test -it myimage:v1\n\nDocker for ML  121\n\nWhere:\n\n-it: It is used to mention that you want to run the container in interactive mode.\n\n--name: It is used to give a name to the container. •\t myimage: It is the image name that is to be run. •\t v1: It is the tag of the image.\n\nThe docker inspect [container_id] command will populate the complete information of the container in JSON format.\n\nThe docker top [container_id] command will show top-level processes within a container.\n\nThe following figure shows the lifecycle and flow of the Docker container:\n\nFigure 6.7: Docker container lifecycle\n\n122  Machine Learning in Production\n\nDockerize and deploy the ML model Let’s consider the scenario of loan prediction, where you need to predict whether a customer’s loan will be approved.\n\n1. # Importing the required packages\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4. import warnings\n\n5. warnings.filterwarnings('ignore')\n\n6. from sklearn.linear_model import LogisticRegression\n\n7.\n\n8. from sklearn import preprocessing\n\n9. from sklearn.model_selection import train_test_split\n\n10. from sklearn import metrics\n\n11. import joblib\n\nAfter that, load the data sets and capture numerical and categorical column names in separate variables for the next step:\n\n1. # Loading the data\n\n2. data = pd.read_csv(\"loan_dataset.csv\")\n\n3. # Missing value treatment (if found)\n\n4. num_col = data.select_dtypes(include=['int64','float64']).columns. tolist()\n\n5. cat_col = data.select_dtypes(include=['object']).columns.tolist()\n\n6. cat_col.remove('Loan_Status')\n\nHandle missing values for categorical and numerical columns:\n\n1. for col in cat_col:\n\n2. data[col].fillna(data[col].mode()[0], inplace=True)\n\n3.\n\n4. for col in num_col:\n\n5. data[col].fillna(data[col].median(), inplace=True)",
      "page_number": 134
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 148-156)",
      "start_page": 148,
      "end_page": 156,
      "detection_method": "topic_boundary",
      "content": "Docker for ML  123\n\nClip extreme values for numerical data:\n\n1. # Outlier treatment (if found)\n\n2. data[num_col] = data[num_col].apply(\n\n3. lambda x: x.clip(*x.quantile([0.05, 0.95])))\n\nCreate a new feature as TotalIncome, which is the sum of applicant income and co-applicant income:\n\n1. # Creating a new variable\n\n2. data['TotalIncome'] = data['ApplicantIncome'] + data['CoapplicantIncome']\n\n3. data = data.drop(['ApplicantIncome','CoapplicantIncome'], axis=1)\n\nConvert categorical to numeric columns using the label encoding technique:\n\n1. cat_col.remove('Loan_ID')\n\n2.\n\n3. # Encoding categorical features\n\n4. for col in cat_col:\n\n5. le = preprocessing.LabelEncoder()\n\n6. data[col] = le.fit_transform(data[col])\n\n7.\n\n8. data['Loan_Status'] = le.fit_transform(data['Loan_Status'])\n\n9.\n\n10. # Model building\n\n11. X = data.drop(['Loan_Status', 'Loan_ID'],1)\n\n12. y = data.Loan_Status\n\n13. features = X.columns.tolist()\n\n14.\n\n15. model = LogisticRegression(solver='lbfgs', max_iter=1000, random_ state=1)\n\n16. model.fit(X, y)\n\n17.\n\n18. joblib.dump(model, 'LR_model.pkl')\n\n124  Machine Learning in Production\n\nNow, create a menu-driven prediction inside the while loop, which will take the user input and make the predictions:\n\n1. # Menu driven\n\n2.\n\n3. print(\"Type 'exit' to terminate.....\\n\")\n\n4. print('''Gender: Female = 0, Male=1\n\n5. Married: No = 0, Yes = 1\n\n6. Education: Graduate = 0 , Under-graduate = 1\n\n7. Self_Employed: No = 0, Yes = 1\n\n8. Property_Area: Urban = 2, Semiurban = 1, Rural = 0\n\n9. Loan_Status: No = 0, Yes = 1\\n''')\n\n10.\n\n11. print('''Pass the data in following sequence separated by comma\n\n12. Gender, Married, Dependents,Education,Self_ Employed,LoanAmount,Loan_Amount_Term,Credit_History,Property_ Area,TotalIncome\\n''') 13.\n\n14. # model = joblib.load('LR_model.pkl')\n\n15.\n\n16. while True:\n\n17. user_data=input(\"Enter your data: \")\n\n18.\n\n19. if(user_data==\"exit\"):\n\n20. break\n\n21.\n\n22. data = list(map(float, user_data.split(',')))\n\n23.\n\n24. # exception handling\n\n25. if(len(data)<10):\n\n26. print(\"Incomplete data provided!!\")\n\n27. else:\n\nDocker for ML  125\n\n28.\n\n29. # predicting the value\n\n30. predicted_value=model.predict([data])\n\n31. print(\"/_______________________________________________/\")\n\n32. if (predicted_value[0]):\n\n33. print(\"\\tCongratulations! your loan approval request is processed\")\n\n34. else:\n\n35. print(\"\\tSorry! your loan approval request is rejected\")\n\n36. print(\"/_______________________________________________/\")\n\nNext, create a requirements.txt file of dependencies.\n\n1. numpy==1.19.5\n\n2. pandas==1.1.5\n\n3. scikit-learn==0.24.2\n\n4. joblib==1.0.1\n\nNow, create a Dockerfile for the preceding application with dependencies.\n\n1. # STEP 1: Install base image. Optimized for Python\n\n2. FROM python:3.7-slim-buster\n\n3.\n\n4. # STEP 2: Upgrading pip\n\n5. RUN pip install --upgrade pip\n\n6.\n\n7. # STEP 3: Copying all the files to the app directory\n\n8. COPY . /app\n\n9.\n\n10. # STEP 4: Set the working directory to the previously added app directory\n\n11. WORKDIR /app\n\n12.\n\n13. # STEP 5: Giving permissions to python file\n\n14. RUN chmod +x train.py\n\n126  Machine Learning in Production\n\n15.\n\n16. # STEP 6: Install required python dependencies from the requirements file\n\n17. RUN pip install -r requirements.txt\n\n18.\n\n19. # STEP 7: Run the train.py file\n\n20. ENTRYPOINT [\"python\"]\n\n21.\n\n22. CMD [\"train.py\"]\n\nWhen you have all the files at one place, you can start building the Docker image using the docker build command, as shown in the following figure:\n\nFigure 6.8: Docker image build started\n\nThis may take a few minutes to complete. After completion, you should see the last message, as shown in the following figure:\n\nFigure 6.9: Docker image built\n\nThe next step is to run the image to start the Docker container instance using the docker run command. Finally, it’s time to make the prediction. Add a brief description before taking the input from the user and, in this case, input data needs to pass in the sequence given in the description.\n\nExecute the docker run command with the -it flag to run the image in an interactive mode, as shown in the following figure:\n\nDocker for ML  127\n\nFigure 6.10: Docker container - ML model prediction\n\nTo come out of the application, the user can pass exit; this will break the while loop.\n\nCommon Docker commands Just run the docker command in the terminal, and you will get the list of commonly used Docker commands. You can also refer to the exhaustive list of Docker CLI commands at https://docs.docker.com/engine/reference/commandline/docker/.\n\nConclusion A Docker container runs in an isolated environment. You learned the importance of the Docker framework in the ML lifecycle and production environment. In this chapter, you explored the role, functionality, and usage of Docker components, with examples. You also learned how to create a Dockerfile and build and run an image in a container. You learned to create and deploy an ML model using Docker in interactive mode.\n\nThe next chapter begins with REST APIs and explains how to use API for deploying ML models. Then, it will cover web frameworks, and finally, you will learn to build a UI for ML model API.\n\nPoints to remember\n\nDocker is a containerization platform for packaging applications and their dependencies in the form of a container.\n\n128  Machine Learning in Production\n\nDocker ensures reproducibility, portability, easy deployment, granular updates, lightness, and simplicity.\n\nDocker manages data within the Docker container using Docker Volumes. •\t An instance of a container gets created when you run the Docker image.\n\nMultiple choice questions\n\n1.\n\nWhat is the command to check the history of the Docker image?\n\na) docker history [Image_id] b) docker all commands [Image_id] c) docker hist [Image_id] d) history [Image_id]\n\n2. Which one of the following sentences is incorrect?\n\na) Docker host comprises multiple Docker containers.\n\nb) A Docker container is a runtime instance of a Docker image. c) The command to check running containers is docker ps.\n\nd) A Docker container instance cannot be created using a Docker image.\n\nAnswers 1. a\n\n2. d\n\nQuestions\n\n1. What is Docker?\n\n2. What is the role of Dockerfile?\n\n3. What is the command to build the Docker image from Dockerfile?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com\n\nBuild ML Web Apps Using API  129\n\nChapter 7 Build ML Web Apps Using API\n\nIntroduction When developing a web app in Python, you are very likely to use a framework for it. A framework is a library that eases the life of a developer while building scalable, standard, and production-ready web applications.\n\nThis chapter will begin with REST APIs and how to use APIs for deploying ML models. Moving on, it will cover different web frameworks and finally, illustrate the steps to build a UI for ML model API. Toward the end of this part, you should know how to deploy an ML web app.\n\nStructure In this chapter, the following topics will be discussed:\n\nREST APIs •\t FastAPI •\t Streamlit •\t Flask •\t Build ML Web App\n\n130  Machine Learning in Production\n\nObjectives After studying this chapter, you should be able to build and deploy ML-based web apps. You should also be able to create an API for your ML model and call it by passing the parameters. Additionally, this chapter will teach you to create web applications using FastAPI, Streamlit, and Flask frameworks. It will cover integrating NGINX and Gunicorn with Flask to create automated ML-based web applications, and you should be comfortable developing web apps in any framework out of the three most commonly used frameworks: FastAPI, Streamlit, and Flask.\n\nREST APIs REST is an acronym for Representational State Transfer. REST is an architectural style mainly created to guide the development and design of the architecture for the World Wide Web (WWW). In simple words, a web service or web API following REST architecture is a REST API.\n\nREST is a pattern to make APIs that can be used to access resources like images, videos, text, JSON, and XML hosted on the server. RESTful API provides a common platform for communicating between applications built in different programming languages. Refer to Figure 7.1:\n\nFigure 7.1: REST API\n\nThe interesting thing about the API is that the client does not need to know the internal operations performed at the server’s end and vice versa. REST API treats any data requested/processed by the user as a resource; it can be text, image, video, and so on.\n\nREST API is stateless; it means that the client should provide all the parameters in the request every time the API is called. The server will not store previous parameters passed with the request by the client.\n\nBuild ML Web Apps Using API  131\n\nFastAPI FastAPI is a web framework for developing RESTful APIs in Python. FastAPI is a lightweight (compared to Django), easy-to-install, easy-to-code yet high-performing framework. It enables the development of REST API with a minimal code requirement. FastAPI comes with built-in standard and interactive documentation. Once you develop and run the API, you can access the documentation for your application at {API endpoint}/docs or {API endpoint}/redoc.\n\nNote: FastAPI requires Python 3.6 and above\n\nLet’s install FastAPI and uvicorn, an Asynchronous Server Gateway Interface (ASGI) server, for production. pip install fastapi uvicorn\n\nLet’s consider a loan prediction scenario, where the goal is to predict whether a customer’s loan will be approved. The focus will not be on hyperparameter tuning and model optimization. However, you can optimize a model to improve its overall performance.\n\n1. # Importing the required packages\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4. import warnings\n\n5. warnings.filterwarnings('ignore')\n\n6. from sklearn.ensemble import RandomForestClassifier\n\n7.\n\n8. from sklearn import preprocessing\n\n9. from sklearn.model_selection import train_test_split\n\n10. from sklearn import metrics\n\n11. import pickle\n\nLoad the datasets, and capture numerical and categorical column names in separate variables for the next step:\n\n1. # Loading the data\n\n2. data = pd.read_csv(\"loan_dataset.csv\")\n\n3. # Capturing numerical and categorical column names",
      "page_number": 148
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 157-164)",
      "start_page": 157,
      "end_page": 164,
      "detection_method": "topic_boundary",
      "content": "132  Machine Learning in Production\n\n4. num_col = data.select_dtypes(include=['int64','float64']).columns. tolist()\n\n5. cat_col = data.select_dtypes(include=['object']).columns.tolist()\n\n6. cat_col.remove('Loan_Status')\n\nTo handle missing values, you can replace categorical missing values with mode and numerical missing values with the median.\n\n1. # Missing value treatment (if found)\n\n2. for col in cat_col:\n\n3. data[col].fillna(data[col].mode()[0], inplace=True)\n\n4.\n\n5. for col in num_col:\n\n6. data[col].fillna(data[col].median(), inplace=True)\n\nInstead of removing extreme values (outliers) from numerical data, you can cap them at 5% on the lower side and 95% on the upper side.\n\nIn other words, data points lower than the 5% quantile will be replaced by a value at the 5% quintile, and on the other hand data points higher than the 95% quantile will be replaced by a value at the 95% quintile.\n\n1. # Outlier treatment (if found)\n\n2. data[num_col] = data[num_col].apply(\n\n3. lambda x: x.clip(*x.quantile([0.05, 0.95])))\n\nCreate a new feature, TotalIncome, which is the sum of applicant income and co- applicant income.\n\n1. # Creating a new variable\n\n2. data['TotalIncome'] = data['ApplicantIncome'] + data['CoapplicantIncome']\n\n3. data = data.drop(['ApplicantIncome','CoapplicantIncome'], axis=1)\n\nConvert categorical to numeric columns using the label encoding technique. The target column Loan Status will be encoded into numeric.\n\n1. cat_col.remove('Loan_ID')\n\n2.\n\n3. # Encoding categorical features\n\nBuild ML Web Apps Using API  133\n\n4. for col in cat_col:\n\n5. le = preprocessing.LabelEncoder()\n\n6. data[col] = le.fit_transform(data[col])\n\n7.\n\n8. data['Loan_Status'] = le.fit_transform(data['Loan_Status'])\n\nNow, you will build an ML model using a random forest classifier and will store the trained models in a pickle object. Here, you can use other ML algorithms and different ML techniques, like GridSearchCV, to improve accuracy. Here, you will build a baseline ML model.\n\n1. # Model building\n\n2. X = data[['Gender', 'Married', 'TotalIncome', 'LoanAmount','Credit_ History']]\n\n3. y = data['Loan_Status']\n\n4. features = X.columns.tolist()\n\n5.\n\n6. model = RandomForestClassifier(max_depth=4, random_state = 10)\n\n7. model.fit(X, y)\n\n8.\n\n9. # saving the model\n\n10. pickle_model = open(\"trained_model/model_rf.pkl\", mode = \"wb\")\n\n11. pickle.dump(model, pickle_model)\n\n12. pickle_model.close()\n\n13.\n\n14. # loading the trained model\n\n15. pickle_model = open('trained_model/model_rf.pkl', 'rb')\n\n16. model_rf = pickle.load(pickle_model)\n\n17.\n\n18. prediction = model_rf.predict([[1, 1, 6000, 150, 0]])\n\n19. print(prediction)\n\n134  Machine Learning in Production\n\nNext, create a requirements.txt file for the dependencies.\n\n1. numpy==1.19.5\n\n2. pandas==1.1.5\n\n3. scikit-learn==0.24.2\n\nNow, create a loan_pred_app.py file. First, load the dependencies and pickle the object of the trained model. Furthermore, create a FastAPI instance and assign it to the app. This will make the app a point of interaction while creating the API.\n\n1. # Importing Dependencies\n\n2. from fastapi import FastAPI\n\n3. from pydantic import BaseModel\n\n4. import uvicorn\n\n5. import pickle\n\n6. import numpy as np\n\n7. import pandas as pd\n\n8.\n\n9. app = FastAPI()\n\n10. # loading the trained model\n\n11. trained_model = 'trained_model/model_rf.pkl'\n\n12. model = pickle.load(open(trained_model, 'rb'))\n\nCreate a class LoanPred that defines the input data type expected from the client.\n\nYou will use the LoanPred class for the data model that will define the data type expected from the users. It is inherited from BaseModel. Then add root view with the function that returns 'message': 'Loan Prediction App' on the home page.\n\n1. class LoanPred(BaseModel):\n\n2. Gender: float\n\n3. Married: float\n\n4. ApplicantIncome: float\n\n5. LoanAmount: float\n\n6. Credit_History: float\n\n7.\n\nBuild ML Web Apps Using API  135\n\n8. @app.get('/')\n\n9. def index():\n\n10. return {'message': 'Loan Prediction App'}\n\nThe following function will create the UI for user input. Here, the /predict class is created as an endpoint, also known as a route. Then, pass the data model, that is, the LoanPred class, to the predict_loan_status() function as a parameter.\n\n1. # Define the function, which will make the prediction using the input data provided by the user\n\n2. @app.post('/predict')\n\n3. def predict_loan_status(loan_details: LoanPred):\n\n4. data = loan_details.dict()\n\n5. gender = data['Gender']\n\n6. married = data['Married']\n\n7. income = data['ApplicantIncome']\n\n8. loan_amt = data['LoanAmount']\n\n9. credit_hist = data['Credit_History']\n\n10.\n\n11. # Make predictions\n\n12. prediction = model.predict([[gender,married,income,loan_ amt,credit_hist]])\n\n13.\n\n14. if prediction == 0:\n\n15. pred = 'Rejected'\n\n16. else:\n\n17. pred = 'Approved'\n\n18.\n\n19. return {'status':pred}\n\n20.\n\n21. if __name__ == '__main__':\n\n22. uvicorn.run(app, host='127.0.0.1', port=8000)\n\n136  Machine Learning in Production\n\nNow, it’s time to run the app and see standard UI auto-generated by FastAPI, which uses swagger, now known as openAPI. uvicorn loan_pred_app:app --reload\n\nThe preceding command can be interpreted as follows:\n\nloan_pred_app refers to the name of the file where the API is created. •\t The app is the instance defined in it. •\t --reload will simply restart the FastAPI server every time a change is made in the app file.\n\nThe following figure shows the terminal output after running the preceding command:\n\nFigure 7.2: Running the FastAPI app in terminal\n\nOnce you see the Application startup complete on the terminal, open the browser and go to the path mentioned in the terminal. In this case, it is 127.0.0.1:8000.\n\nThe following figure shows the UI of the FastAPI app in the browser. Here, the text message Loan Prediction App is displayed.\n\nFigure 7.3: FastAPI app in terminal\n\nThe following figure shows the Swagger (openAPI) UI of the FastAPI app on the browser. Simply add /docs to the end of the URL, and you should see auto-generated docs for the app.\n\nBuild ML Web Apps Using API  137\n\nFigure 7.4: Docs of the FastAPI app\n\nFastAPI provides the functionality to validate the data type. It detects the invalid data type at runtime and returns the bad input to the client, which eventually reduces the burden of managing exceptions at the developer’s end.\n\nThe following figure shows the schema and the data type of the data expected from the client. As there are five variables in the mentioned model, and they can be seen in the figure, along with their data type.\n\nFigure 7.5: Schema of FastAPI app\n\n138  Machine Learning in Production\n\nThe following figure explores the predict() of the app. Here, a sample JSON input is entered, which is expected from the client. By default, the values of all the variables are zero in the schema. To pre-test the app, you have to click on the Try it out button.\n\nFigure 7.6: Predict() of FastAPI app\n\nAfter clicking on the Try it out button, you can pass the variable values or data in JSON format. Figure 7.7 shows the data for given variables being provided by the user. Finally, click on the Execute button at the bottom to see the prediction returned by the API.\n\nFigure 7.7: Parameters passing in the predict function\n\nBuild ML Web Apps Using API  139\n\nThe following figure shows the predicted outcome under the Response body in JSON format. Based on the data provided, the predicted status of the loan is returned as ‘Approved’. Optionally, you can run the curl command to get the output in the terminal. Response code 200 shows that the request has succeeded without any error.\n\nFigure 7.8: Prediction outcome of FastAPI\n\nStreamlit Streamlit is an open-source library in Python that enables users to build and share attractive UI for machine learning models. It comes with extensive documentation to learn and explore. With streamlit, you can add beautiful and interactive widgets to get the user inputs with a few lines of code, such as a dropdown selection box and a slider to change the values.\n\nAccording to the makers, streamlit is the fastest way to build ML apps and deploy them on the cloud. It is a great framework to deploy ML apps using Python. In streamlit, everything can be coded in Python without the need for any front-end skills such as JavaScript to develop stunning UI for the app. Streamlit is a framework",
      "page_number": 157
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 165-172)",
      "start_page": 165,
      "end_page": 172,
      "detection_method": "topic_boundary",
      "content": "140  Machine Learning in Production\n\nthat converts the Python code into interactive apps and enables data scientists to build data and model-based apps quickly.\n\nLet’s install streamlit using the following command:\n\npip install streamlit\n\nVerify the streamlit installation using the following:\n\nstreamlit hello\n\nNow, create a streamlit_app.py file for the ML-based streamlit application:\n\n1. import pickle\n\n2. import streamlit as st\n\n3.\n\n4. # loading the trained model\n\n5. trained_model = 'trained_model/model_rf.pkl'\n\n6. model = pickle.load(open(trained_model, 'rb'))\n\n7.\n\n8. @st.cache()\n\nThe following function will make the prediction based on input received from the front end:\n\n1. # The following function will make the prediction based on data provided by the user\n\n2. def prediction(Gender, Married, ApplicantIncome, LoanAmount, Credit_History):\n\n3. # Pre-processing user input\n\n4. if Gender == \"Male\":\n\n5. Gender = 1\n\n6. else:\n\n7. Gender = 0\n\n8.\n\n9. if Married == \"Unmarried\":\n\n10. Married = 1\n\n11. else:\n\nBuild ML Web Apps Using API  141\n\n12. Married = 0\n\n13.\n\n14. if Credit_History == \"Unclear Debts\":\n\n15. Credit_History = 1\n\n16. else:\n\n17. Credit_History = 0\n\n18.\n\n19. # Making predictions\n\n20. prediction = model.predict(\n\n21. [[Gender, Married, ApplicantIncome, LoanAmount, Credit_ History]])\n\n22.\n\n23. if prediction == 0:\n\n24. pred = 'Rejected'\n\n25. else:\n\n26. pred = 'Approved'\n\n27. return pred\n\nThe following is a function that contains streamlit code, which will be responsible for taking input data from the user and displaying the prediction of the model:\n\n1. # The Following function is to define the home page of the streamlit application\n\n2. def main():\n\n3.\n\n4. # Front-end view for the app\n\n5. html_temp = \"\"\"\n\n6. <div style =\"background-color:green;padding:1px\">\n\n7. <h1 style =\"color:black;text-align:center;\">\n\n8. Loan Prediction App\n\n9. </h1>\n\n10. </div>\n\n11. \"\"\"\n\n12.\n\n142  Machine Learning in Production\n\n13. # display the front-end aspect\n\n14. st.markdown(html_temp, unsafe_allow_html = True)\n\n15.\n\n16. # Following code is to create a box field to get user data\n\n17. Gender = st.selectbox('Gender',(\"Male\",\"Female\"))\n\n18. Married = st.selectbox('Marital Status',(\"Unmarried\",\"Married\"))\n\n19. ApplicantIncome = st.number_input(\"Applicants monthly income\")\n\n20. LoanAmount = st.number_input(\"Total loan amount\")\n\n21. Credit_History = st.selectbox('Credit_History',(\"Unclear Debts\",\n\n22. \"No Unclear Debts\"))\n\n23. result =\"\"\n\n24.\n\n25. # When Predict button is clicked it will make the prediction and display it\n\n26. if st.button(\"Predict\"):\n\n27. result = prediction(Gender, Married, ApplicantIncome,\n\n28. LoanAmount, Credit_History)\n\n29. st.success('Your loan is {}'.format(result))\n\n30.\n\n31. if __name__=='__main__':\n\n32. main()\n\nOpen the terminal and run the following command where the streamlit_app.py file is located:\n\nstreamlit run streamlit_app.py\n\nOr\n\nstreamlit run streamlit_app.py &>/dev/null&\n\nFigure 7.9 shows the front end of the streamlit app. Here, the prediction is based on the data provided by the user.\n\nBuild ML Web Apps Using API  143\n\nFigure 7.9: Streamlit app\n\nTo deploy your Streamlit app on the streamlit cloud, you can use Streamlit sharing. Upload your files with the requirements.txt file on GitHub. Create an account on their website at https://streamlit.io/cloud and then provide a GitHub link and streamlit app file. This will deploy your app on the streamlit cloud.\n\nFlask Flask is a web framework that allows you to build web applications using Python. It is a lightweight framework compared to Django. It follows the REST architecture. You can develop simple web applications with Flask, as it requires less base code. Flask is based on the Web Server Gateway Interface (WSGI) and Jinja2 engine.\n\n144  Machine Learning in Production\n\nTo start a Flask application, you need to use the run() function. If you set debug=True inside the run() function, then it becomes easy to track the error. When you enable debug mode, the server will restart every time you make changes in the app file and save it. If an error occurs, then it shows the reason in the browser itself. However, you should not use this feature while deploying models in the production environment.\n\nIn a Flask, you can call static files like CSS or JavaScript files to render the web page of the app. The route() function guides the Flask to the URL called by the function.\n\npip install Flask\n\nTo run the Flask application in the terminal, go to the directory where app.py and other required files are located:\n\npython app.py\n\nCreate an index.html file with the following HTML code to build the front end for the users. This will enable the users to provide the input data through UI.\n\n1. <html>\n\n2. <head>\n\n3. <title>LOAN PREDICTION</title>\n\n4. </head>\n\n5. <body>\n\n6. <h2 align=\"center\">LOAN PREDICTION</h2>\n\n7. <br/>\n\n8. <form action=\"/predict\" method=\"POST\">\n\n9. <table align=\"center\" cellpadding = \"10\">\n\n10. <!----- First Name ------------------------------------------ ----------->\n\n11. <tr>\n\n12. <td>FIRST NAME</td>\n\n13. <td>\n\n14. <input type=\"text\" name=\"First_Name\" maxlength=\"30\" placeholder=\"characters a-z A-Z\"/>\n\n15. </td>\n\n16. </tr>\n\n17. <!----- Last Name ------------------------------------------- ----------->\n\nBuild ML Web Apps Using API  145\n\n18. <tr>\n\n19. <td>LAST NAME</td>\n\n20. <td>\n\n21. <input type=\"text\" name=\"Last_Name\" maxlength=\"30\" placeholder=\"characters a-z A-Z\" />\n\n22. </td>\n\n23. </tr>\n\n24. <!----- Gender---------------------------------------------------->\n\n25. <tr>\n\n26. <td>GENDER</td>\n\n27. <td>\n\n28. <input type=\"radio\" name=\"gender\" id=\"gender\" value=\"1\">Male</input>\n\n29. <input type=\"radio\" name=\"gender\" id=\"gender\" value=\"0\">Female</input>\n\n30. </td>\n\n31. </tr>\n\n32. <!-----Marital Status--------------------------------------------------->\n\n33. <tr>\n\n34. <td>MARRIED</td>\n\n35. <td>\n\n36. <input type=\"radio\" name=\"married\" id=\"married\" value=\"1\"> Yes </input>\n\n37. <input type=\"radio\" name=\"married\" id=\"married\" value=\"0\"> No </input>\n\n38. </td>\n\n39. </tr>\n\n40. <!----- Income ---------------------------------------------- ----------->\n\n41. <tr>\n\n42. <td>TOTAL INCOME</td>\n\n146  Machine Learning in Production\n\n43. <td>\n\n44. <input type=\"text\" name=\"total_income\" maxlength=\"30\" placeholder=\"$(thousands)\" />\n\n45. </td>\n\n46. </tr>\n\n47. <!----- Loan Amount ----------------------------------------- ----------->\n\n48. <tr>\n\n49. <td>LOAN AMOUNT</td>\n\n50. <td>\n\n51. <input type=\"text\" name=\"loan_amt\" maxlength=\"30\" placeholder=\"$(thousands)\" />\n\n52. </td>\n\n53. </tr>\n\n54. <!----- Credit History -------------------------------------- ----------->\n\n55. <tr>\n\n56. <td>CREDIT HISTORY</td>\n\n57. <td>\n\n58. <input type=\"radio\" name=\"credit_history\" id=\"credit_ history\" value=\"1\"> Yes </input>\n\n59. <input type=\"radio\" name=\"credit_history\" id=\"credit_ history\" value=\"0\"> No </input>\n\n60. </td>\n\n61. </tr>\n\n62. <!----- Submit and Reset ------------------------------------ ----------->\n\n63. <tr>\n\n64. <td colspan=\"2\" align=\"center\">\n\n65. <input type=\"submit\" value=\"Submit\">\n\n66. &nbsp;&nbsp; <!----- to add extra space -------->\n\n67. <input type=\"reset\" value=\"Reset\" onclick=\"location. href='/';\">\n\nBuild ML Web Apps Using API  147\n\n68. </td>\n\n69. </tr>\n\n70. </table>\n\n71. </form>\n\n72. <h3 align=\"center\"> {{ prediction }} </h3>\n\n73. </body>\n\n74. </html>\n\nHere, the result.html file will display the prediction output to the users:\n\n1. <!doctype html>\n\n2. <html>\n\n3. <body>\n\n4. <h1>{{ prediction }}</h1>\n\n5. </body>\n\n6. </html>\n\nNow, create an app.py file for the Flask application:\n\n1. # Importing Dependencies\n\n2. from flask import Flask,render_template,url_for,request\n\n3. import pandas as pd\n\n4. from sklearn.ensemble import RandomForestClassifier\n\n5. import numpy as np\n\n6. import pickle\n\n7. import os\n\nThe next step is to load the serialized model object to make the prediction:\n\n1. app = Flask(__name__)\n\n2. port = int(os.environ.get(\"PORT\", 80))\n\n3. # Loading the trained model\n\n4. pickle_in = open('trained_model/model_rf.pkl', 'rb')\n\n5. model = pickle.load(pickle_in)",
      "page_number": 165
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 173-180)",
      "start_page": 173,
      "end_page": 180,
      "detection_method": "topic_boundary",
      "content": "148  Machine Learning in Production\n\nNow, create the views for the Flask application. The following code will render the HTML template, which will be the landing page of the Flask application. app. route(‘/’) will route to the home page URL; the trailing slash ‘/’is generally used as a convention for the home page.\n\n1. # Views\n\n2. @app.route('/')\n\n3. def home():\n\n4. return render_template('index.html')\n\nNow, create a view for the predict() function. app.route(‘/predict) will route to the home page URL. The POST method type allows you to send data to a server to update or create a target resource.\n\n5. @app.route('/predict',methods=['POST'])\n\n6. def predict():\n\n7. if request.method == 'POST':\n\n8. # Fetch Value for Gender\n\n9. gender = request.form['gender']\n\n10. if gender == \"Female\":\n\n11. gender = int(0.0)\n\n12. if gender == \"Male\":\n\n13. gender = int(1.0)\n\n14.\n\n15. # Fetch Value for Married\n\n16. married = request.form['married']\n\n17. if married == \"No\":\n\n18. married = int(0.0)\n\n19. if married == \"Yes\":\n\n20. married = int(1.0)\n\n21.\n\n22. # Fetch value for LoanAmount\n\n23. loan_amt = float(request.form['loan_amt'])\n\nBuild ML Web Apps Using API  149\n\n24.\n\n25. # Fetch value for Total_Income\n\n26. total_income = float(request.form['total_income'])\n\n27.\n\n28. # Fetch value for Prior Credit_Score\n\n29. credit_history = request.form['credit_history']\n\n30. if credit_history == \"No\":\n\n31. credit_history = int(0.0)\n\n32. if credit_history == \"Yes\":\n\n33. credit_history = int(1.0)\n\n34.\n\n35. to_predict_list = [gender, married, total_income, loan_amt, credit_hi story]\n\n36. prediction_array = np.array(to_predict_list, dtype=np. float32).reshape(1, 5)\n\n37.\n\n38. # Making Predictions using the trained model\n\n39. prediction = model.predict(prediction_array)\n\n40. prediction_value = prediction[0]\n\n41. # print(prediction_value )\n\n42.\n\n43.\n\n44. if int(prediction_value) == 1:\n\n45. status=\"Congratulations! your loan approval request is processed\"\n\n46. if int(prediction_value) == 0:\n\n47. status=\"Sorry! your loan approval request is rejected\"\n\n48.\n\n49. return render_template('index.html',prediction = status)\n\n150  Machine Learning in Production\n\nAdd error handling functionality, as follows:\n\n1. @app.errorhandler(500)\n\n2. def internal_error(error):\n\n3. return \"500: Something went wrong\"\n\n4.\n\n5. @app.errorhandler(404)\n\n6. def not_found(error):\n\n7. return \"404: Page not found\",404\n\nFinally, you will define the main function as shown below:\n\n1. if __name__ == '__main__':\n\n2. app.run(host='0.0.0.0', port=port)\n\nGunicorn The Gunicorn is an application server for running a Python app. Gunicorn is WSGI compatible, so it can communicate with multiple WSGI applications. In the current case, Gunicorn translates the request received from Ngnix for the Flask app and vice versa.\n\nTo install Gunicorn, execute the following command: sudo apt-get install gunicorn3\n\nGo to the directory where the app.py file is located and run: gunicorn3 app:app\n\nThis command helps to know which IP and port are being used by Gunicorn. In this case, it is 8000 and IP can be 0.0.0.0 or 127.0.0.1.\n\nNGINX NGINX is a high-performance, highly scalable open-source, and reverse proxy web server. It can perform load balancing and caching application instances. It accepts incoming connections and decides where they should go next. In the current case, it sits on top of a Gunicorn.\n\nTo install NGINX, execute the following command: sudo apt-get install nginx\n\nBuild ML Web Apps Using API  151\n\nYou can check the status of NGINX using the following command:\n\nsudo service nginx status\n\nNow, go to the following path:\n\ncd /etc/nginx/sites-enabled/\n\nNote: NGINX configuration files do not have any extension, and every line should be closed using ; (semicolons).\n\nCreate a new configuration file for the Flask app: sudo nano flask_app\n\nThen, add the following snippet and save the file:\n\n1. server{\n\n2. listen 80;\n\n3. server_name 0.0.0.0;\n\n4.\n\n5. location / {\n\n6. proxy_pass http://unix:/home/suhas/webapi/flask/ flaskapp.sock;\n\n7. }\n\n8. }\n\nHere, you are dictating NGINX to listen to port 80. Inside the location block, pass the request to the socket using proxy_pass. After changing the NGINX file, restart the NGINX service using the following command:\n\nsudo service nginx restart or sudo systemctl restart nginx\n\nYou can check the status of NGINX using the following command:\n\nsudo service nginx status\n\nGo to the directory where the app.py file is located and run the following command to start the Gunicorn server:\n\ngunicorn3 app:app\n\nOpen a new tab in the browser and enter the IP in the address bar. You should see the Flask app up and running.\n\n152  Machine Learning in Production\n\nCreate and run the service in the background:\n\nCreate a systemd unit file that allows the Ubuntu boot system to start Gunicorn automatically and serve up the Flask application every time the server starts.\n\nFirst, go to the system directory using the following command:\n\ncd /etc/systemd/system\n\nNext, you need to create a service for the Flask app\n\nsudo nano flaskapp.service\n\nThen, add the following commands to it and save it:\n\n1. [Unit]\n\n2. Description=Flaskapp Gunicorn Service\n\n3. After=network.target\n\n4.\n\n5. [Service]\n\n6. User=suhas\n\n7. Group=www-data\n\n8. WorkingDirectory=/home/suhas/webapi/flask\n\n9.\n\n10. ExecStart=/usr/bin/gunicorn3 --workers 3 --bind unix:flaskapp. sock -m 007 app:app\n\n11. Restart=on-failure\n\n12. RestartSec=10\n\n13.\n\n14. [Install]\n\n15. WantedBy=multi-user.target\n\nIn the preceding service, you are instructing Gunicorn to start three worker processes, which can be updated afterward. Then, create and link the UNIX socket file. In the current scenario, 007 is used for access so that the socket file allows access to the owner and group and restricts others. Then, pass the filename of the app.\n\nsudo systemctl daemon-reload\n\nsudo service flaskapp restart\n\nBuild ML Web Apps Using API  153\n\nRestart the flaskapp service by running the preceding command, and you should see flaskapp.sock file created in the app directory.\n\nThe following figure shows that the request received from the client goes to NGINX first. Next, it passes to the Gunicorn server; the Gunicorn translates and passes it to the Flask app and then back to the client.\n\nFigure 7.10: Client-server interaction\n\nIf everything goes well, open the browser and enter the IP in the address bar. You should see the app up and running.\n\nThe following figure shows the Loan Prediction app running in the browser. Provide the user data.\n\nFigure 7.11: Passing parameters in the Flask app\n\n154  Machine Learning in Production\n\nThe following figure shows the prediction after providing user data and clicking on the Submit button.\n\nFigure 7.12: Prediction outcome\n\nConclusion This chapter demonstrated how to develop an ML-based web application using the most commonly used frameworks, viz FastAPI, Streamlit, and Flask with NGINX and Gunicorn. You created a user-friendly and simple UI to receive input data from users, and you studied the REST API; it’s working between the client and server. This chapter also touched upon the salient features of each framework.\n\nThe next chapter will discuss building native applications for PC and Android devices.\n\nPoints to remember\n\nRESTful API provides a common platform for communicating with an application built in different programming languages.\n\nFastAPI requires Python 3.6 and above. •\t Streamlit is an open-source library in Python that enables users to build and share attractive UI for machine learning models.\n\nFastAPI comes up with a built-in standard and interactive documentation. •\t NGINX configuration files do not have any extension and every line should be closed using a ; (semicolon).\n\nBuild ML Web Apps Using API  155\n\nMultiple choice questions\n\n1. How can you verify the installation of streamlit?\n\na) streamlit hello\n\nb) streamlit hello world\n\nc) streamlit run test\n\nd) streamlit --\n\n2. Docs of the FastAPI application can be viewed on a browser using which of the following?\n\na) /docs\n\nb) /redoc\n\nc) Both a and b\n\nd) It is not accessible\n\nAnswers 1. a\n\n2. c\n\nQuestions\n\n1. What is NGINX?\n\n2. What is the role of a Gunicorn?\n\n3. What is the command to run a streamlit app?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "page_number": 173
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 181-188)",
      "start_page": 181,
      "end_page": 188,
      "detection_method": "topic_boundary",
      "content": "156  Machine Learning in Production\n\nBuild Native ML Apps  157\n\nChapter 8 Build Native ML Apps\n\nIntroduction In order to consume ML models as a native application, a GUI should be built for the users. In this chapter, you will learn to build native applications using Tkinter and Kivy packages, which can be converted into desktop applications for Windows and Android devices, respectively.\n\nPython allows multiple ways to build a Graphical User Interface (GUI), which is an application that contains the main window, buttons, widgets, input fields, and the like. End users can interact with it and see the response based on their actions and inputs. Python has numerous GUI frameworks or toolkits available.\n\nStructure This chapter discusses the following topics:\n\n\n\nIntroduction to Tkinter\n\nBuild an ML-based app using Tkinter •\t Convert Python app into Windows EXE file using Pyinstaller • Introduction to kivy\n\n158  Machine Learning in Production\n\nBuild an ML-based app using kivy and kivyMD •\t Convert Python app into Android app using Buildozer\n\nObjectives After studying this chapter, you should be able to build and deploy ML-based native apps, such as Windows and Android apps. This chapter is divided into two sections. By the end of the first section, you should be comfortable using Tkinter for developing ML-based Windows apps. You can also convert a Python app to an independent Windows executable file. By the end of the second section, you should be able to develop ML-based kivy applications and convert them into Android apps using the Buildozer package.\n\nIntroduction to Tkinter Tkinter is a free software released under a Python license.\n\nTkinter is the Python interface to the Tk GUI library. Tk is derived from the Tool Command Language (TCL), which is a scripting language. The best part is that you do not need to install Tkinter explicitly as it comes with Python since 1994.\n\nTkinter enables developers to create widgets (GUI elements) easily, which are packaged in the Tk toolkit. With widgets, developers can create buttons, menus, and input fields to get user data. These widgets can be linked to Python features, methods, data, or other widgets.\n\nFor instance, a button widget can be used to perform a certain action upon click event, or it can call the function and pass the user's data as parameters.\n\nNote: Get the latest version of Tkinter by installing Python 3.7 or a later version.\n\nYou can verify the Tkinter installation using the following command: python -m tkinter\n\nA pop-up window will appear, as shown below, which contains TCL/Tk version and text as This should be a cedilla: ç.\n\nFigure 8.1: Tkinter\n\nBuild Native ML Apps  159\n\nNote: A cedilla is a symbol that is written under the letter 'ç' in French, Portuguese, and some other languages to show that you pronounce it like a letter 's'.\n\nHello World app using Tkinter Let’s create a basic Tkinter app to give you a glimpse of Tkinter functionality. The following code creates a simple app displaying the Hello World text with the Exit button to close the app.\n\n1. from tkinter import *\n\n2. win = Tk()\n\n3.\n\n4. a = Label(win, text =\"Hello World\")\n\n5. a.pack()\n\n6.\n\n7. b=Button(win, text='Exit', command=tk.destroy)\n\n8. b.pack()\n\n9.\n\n10. win.mainloop()\n\nIn the preceding code, first, the tkinter package is imported to create the main window for the app using the Tk(), where the win is the name of the main window object that has been created using Tk(). A Label() widget is used to display text or images, and the win parameter is used to denote the parent window of the app.\n\nThe pack() organizes the widgets in blocks (such as buttons and labels). If you do not organize the widget, your app will still run, but that widget will be invisible. A Button() widget is used to add buttons, and here it is used to close the Tkinter app. When the user clicks on the Exit button, the Tkinter app window will close. Finally, the mainloop() is to keep the application running and execute user commands.\n\nThe following figure shows the output of the preceding code. It is a simple Tkinter app with a label and a button.\n\nFigure 8.2: Hello World app using Tkinter\n\n160  Machine Learning in Production\n\nHere are some salient features of Tkinter:\n\nTkinter is easy to use and quick to develop desktop applications. •\t The syntax of Tkinter and its widget is simple to understand. •\t Tkinter is shipped with Python, so you do not need to install it explicitly. •\t Tkinter is open-source and free to use.\n\nBuild an ML-based app using Tkinter Here, you need to develop the Python code for data loading, data cleaning, feature engineering, model building and finally, saving trained models to serialized objects like pickles. Then you can use trained models to get predictions on the Tkinter app.\n\nLet’s consider a loan prediction scenario, where the goal is to predict whether a customer’s loan will be approved. The focus will not be on hyperparameter tuning and model optimization. However, you can optimize a model to improve its overall performance. First, create train.py file as follows:\n\n1. # Import the required packages\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4. import warnings\n\n5. warnings.filterwarnings('ignore')\n\n6. from sklearn.ensemble import RandomForestClassifier\n\n7.\n\n8. from sklearn import preprocessing\n\n9. from sklearn.model_selection import train_test_split\n\n10. from sklearn import metrics\n\n11. import pickle\n\nFor the next step, load the datasets and capture numerical and categorical column names in separate variables.\n\n1. # Load the data\n\n2. data = pd.read_csv(\"loan_dataset.csv\")\n\n3. # Missing value treatment (if found)\n\nBuild Native ML Apps  161\n\n4. num_col = data.select_dtypes(include=['int64','float64']).columns. tolist()\n\n5. cat_col = data.select_dtypes(include=['object']).columns.tolist()\n\n6. cat_col.remove('Loan_Status')\n\nTo handle missing values, you can replace categorical missing values with mode and numerical missing values with the median.\n\n1. # Missing value treatment (if found)\n\n2. for col in cat_col:\n\n3. data[col].fillna(data[col].mode()[0], inplace=True)\n\n4.\n\n5. for col in num_col:\n\n6. data[col].fillna(data[col].median(), inplace=True)\n\nInstead of removing extreme values (outliers) from numerical data, you can cap them at 5% on the lower side and 95% on the upper side.\n\nIn other words, data points lower than the 5% quantile will be replaced by a value at the 5% quintile, and on the other hand data points higher than the 95% quantile will be replaced by a value at the 95% quintile.\n\n1. # Outlier treatment (if found)\n\n2. data[num_col] = data[num_col].apply(\n\n3. lambda x: x.clip(*x.quantile([0.05, 0.95])))\n\nThere are many instances where a co-applicant income is not available. Create a new feature, TotalIncome which is the sum of applicant income and co-applicant income.\n\n1. # Create a new variable\n\n2. data['TotalIncome'] = data['ApplicantIncome'] + data['Coapplican- tIncome']\n\n3. data = data.drop(['ApplicantIncome','CoapplicantIncome'], axis=1)\n\nYou should avoid passing text data, such as Gender and Married (Marital status), to the model. Convert categorical features to numeric features using the label encoding technique. The target column Loan Status will be encoded into numeric.\n\n1. cat_col.remove('Loan_ID')\n\n2.\n\n162  Machine Learning in Production\n\n3. # Encode categorical features\n\n4. for col in cat_col:\n\n5. le = preprocessing.LabelEncoder()\n\n6. data[col] = le.fit_transform(data[col])\n\n7.\n\n8. data['Loan_Status'] = le.fit_transform(data['Loan_Status'])\n\nNow, you will build an ML model using a random forest classifier and will store the trained models in a pickle object. Here, you can use other ML algorithms and different ML techniques, like GridSearchCV, to improve accuracy. Here, you will build a baseline ML model.\n\n1. # Model building\n\n2. X = data[['Gender', 'Married', 'TotalIncome', 'LoanAmount','Cred- it_History']]\n\n3. y = data['Loan_Status']\n\n4. features = X.columns.tolist()\n\n5.\n\n6. model = RandomForestClassifier(max_depth=4, random_state = 10)\n\n7.\n\n8. model.fit(X, y)\n\n9.\n\n10. # save the model\n\n11. pickle_model = open(\"trained_model/model_rf.pkl\", mode = \"wb\")\n\n12. pickle.dump(model, pickle_model)\n\n13. pickle_model.close()\n\n14.\n\n15. # load the trained model\n\n16. pickle_model = open('trained_model/model_rf.pkl', 'rb')\n\n17. model_rf = pickle.load(pickle_model)\n\n18.\n\n19. prediction = model_rf.predict([[1, 1, 6000, 150, 0]])\n\n20. print(prediction)\n\nBuild Native ML Apps  163\n\nNext, create a requirements.txt file of dependencies, as follows:\n\n1. pandas==1.1.5\n\n2. pyinstaller==4.1\n\n3. scikit-learn==0.24.0\n\nTkinter app When you complete the preceding part, you should have a pickled object of the trained model, which will be used by the Tkinter app. Here, you will need to import two packages, viz tkinter to build GUI and joblib to load pickled ML model objects.\n\nCreate a Python file and name it ml_app.py.\n\nImport the tkinter module to create a Tkinter desktop application.\n\n1. from tkinter import *\n\n2.\n\n3. import joblib\n\n4.\n\n5. trained_model = 'E:/tkinter_ml_app/trained_model/model.pkl'\n\n6. model = joblib.load(trained_model)\n\nDefine MyWindow class for an ML app. In this class, add Tkinter widgets for GUI and the predict() function for making predictions based on user input. In the following code, labels for input files have been created using the Label() widget.\n\n1. class MyWindow:\n\n2. def __init__(self, win):\n\n3. # Create a text Label\n\n4. self.lbl0=Label(win, text=\"Loan Prediction App\", font=(25))\n\n5. self.lbl0.pack(pady=10)\n\n6. self.lbl1=Label(win, text='Gender')\n\n7. self.lbl2=Label(win, text='Married')\n\n8. self.lbl3=Label(win, text='Total Income')\n\n9. self.lbl4=Label(win, text='Loan Amount')",
      "page_number": 181
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 189-196)",
      "start_page": 189,
      "end_page": 196,
      "detection_method": "topic_boundary",
      "content": "164  Machine Learning in Production\n\n10. self.lbl5=Label(win, text='Credit History')\n\n11. self.lbl6=Label(win, text='Loan Status')\n\nThe next step is to declare input boxes using the Entry() widget for the preceding fields. In this, declare the optional parameter bd, that is, the border inside the Entry() widget for the input box.\n\nThe insert() widget inserts the text at the given position. 0 (zero) is the first character, so it inserts the default text at the beginning.\n\nThe syntax of the bind() is as follows: widget.bind(event, handler)\n\nIn the current case, an entry widget is linked to a FocusIn event using bind(). It means a specific entry widget is focused, that is, when the cursor is active in a given entry widget, then it should execute the lambda function defined inside the bind(). The lambda function is used to delete the content of the entry widget as soon as the user clicks on it.\n\nCreate Predict button using Button().\n\n1. # Create an entry widget to accept the user input\n\n2. self.t1=Entry(bd=2)\n\n3. self.t1.insert(0, \"0:F, 1:M\")\n\n4. self.t1.bind(\"<FocusIn>\", lambda args: self. t1.delete('0', 'end'))\n\n5.\n\n6. self.t2=Entry(bd=2)\n\n7. self.t2.insert(0, \"0:No, 1:Yes\")\n\n8. self.t2.bind(\"<FocusIn>\", lambda args: self. t2.delete('0', 'end'))\n\n9.\n\n10. self.t3=Entry(bd=2)\n\n11. self.t3.insert(0, \"E.g. 6000\")\n\n12. self.t3.bind(\"<FocusIn>\", lambda args: self. t3.delete('0', 'end'))\n\n13.\n\n14. self.t4=Entry(bd=2)\n\nBuild Native ML Apps  165\n\n15. self.t4.insert(0, \"E.g. 150\")\n\n16. self.t4.bind(\"<FocusIn>\", lambda args: self. t4.delete('0', 'end'))\n\n17.\n\n18. self.t5=Entry(bd=2)\n\n19. self.t5.insert(0, \"0:Clear Debts, 1:Unclear Debts\")\n\n20. self.t5.bind(\"<FocusIn>\", lambda args: self. t5.delete('0', 'end'))\n\n21.\n\n22. self.t6=Entry(bd=2)\n\n23.\n\n24. # Create a Predict button\n\n25. self.btn1 = Button(win, text='Predict')\n\nTkinter comes with three geometry managers: grid, place, and pack. A geometry manager’s job is to arrange widgets in specific positions. The place geometry manager gives you more flexibility compared to the other two geometry managers. You can declare the vertical and horizontal position of the widget. The following code mentions the vertical and horizontal positions of each widget, including the Predict button.\n\n1. # Organize widgets appropriately\n\n2. self.lbl1.place(x=100, y=50)\n\n3. self.t1.place(x=200, y=50)\n\n4.\n\n5. self.lbl2.place(x=100, y=100)\n\n6. self.t2.place(x=200, y=100)\n\n7.\n\n8. self.lbl3.place(x=100, y=150)\n\n9. self.t3.place(x=200, y=150)\n\n10.\n\n11. self.lbl4.place(x=100, y=200)\n\n12. self.t4.place(x=200, y=200)\n\n166  Machine Learning in Production\n\n13.\n\n14. self.lbl5.place(x=100, y=250)\n\n15. self.t5.place(x=200, y=250, width=165)\n\n16.\n\n17. self.b1=Button(win, text='Predict', command=self. predict, fg='blue')\n\n18. self.b1.place(x=170, y=300)\n\n19.\n\n20. self.lbl6.place(x=100, y=350)\n\n21. self.t6.place(x=200, y=350)\n\nFinally, define the predict() to make the prediction using the ML model based on user inputs.\n\n1. # For making predictions\n\n2. def predict(self):\n\n3. self.t6.delete(0, 'end')\n\n4. gender = float(self.t1.get())\n\n5. married = float(self.t2.get())\n\n6. income = float(self.t3.get())\n\n7. loan_amt = float(self.t4.get())\n\n8. credit_hist = float(self.t5.get())\n\n9. prediction = model. predict([[gender, married, income, loan_amt, credit_hist]])[0]\n\n10. if prediction == 0:\n\n11. pred = 'Rejected'\n\n12. else:\n\n13. pred = 'Approved'\n\n14.\n\n15. self.t6.insert(END, str(pred))\n\nBuild Native ML Apps  167\n\nThe class declaration is done here.\n\nUse the Tk class to create the main window and call the mainloop() to keep the window displayed.\n\nAlso, the application window does not appear on the screen till mainloop() is not called, as it takes all the objects and widgets and renders them on screen.\n\n1. window=Tk()\n\n2.\n\n3. mywin=MyWindow(window)\n\n4.\n\n5. # Create a title\n\n6. window.title('ML App')\n\n7.\n\n8. # Define the size of the window\n\n9. window.geometry(\"400x400+10+10\")\n\n10.\n\n11. window.mainloop() #Keep the window displaying\n\nCreate a ml_app.py file for the Tkinter app and run it. It should make predictions based on the given input.\n\nConvert Python app into Windows EXE file Now, you can convert this Tkinter - ML app to Windows executable application using Pyinstaller, with the following steps. Pyinstaller packages a Python application and its dependencies into a single package that can be run independently without installing a Python interpreter.\n\nLet’s create a virtual environment and install all the required dependencies.\n\nHere, a virtual environment is created with Python version 3.7 using conda, as follows: conda create -n venv_tkinter python=3.7 -y\n\nOnce it is created, activate it using: source activate venv_tkinter\n\nNote: To remove the conda environment, you can execute the following command conda env remove -n venv_tkinter:\n\n168  Machine Learning in Production\n\npyinstaller --noconfirm --onefile --windowed --icon \"E:/tkinter_ml_app/ python_104451.ico\" --add-data \"E:/tkinter_ml_app/trained_model\n\n/model.pkl;.\" --hidden-import \"sklearn\" --hidden-import \"sklearn. ensemble._forest\" --hidden-import \"sklearn.utils._weight_vector\" --hidden-import \"sklearn. neighbors._quad_tree\" \"E:/tkinter_ml_app/ml_app.py\"\n\n--hidden-import\n\n\"sklearn.neighbors._typedefs\"\n\nThe preceding command will convert the Python-based Tkinter app to a Windows executable file that can be used for prediction.\n\nWhere:\n\n-y, --noconfirm: Will replace output directory without asking for confirmation\n\n-F, --onefile: Will create one file bundle executable •\t -D, --onedir: Will create one folder bundle containing the Windows executable file\n\n--add-data: Additional non-binary files or folders to be added to the executable\n\n\n\nc, --console, --nowindowed: Opens a console window for standard i/o (default)\n\n-w, --windowed, --noconsole: Used to not provide a console window for standard i/o\n\n--hidden-import MODULENAME: Used to name an import not visible in the code of the script(s); this option can be used multiple times\n\nFor more details, refer to:\n\nhttps://pyinstaller.org/en/stable/usage.html\n\nhe following figure shows the status message after executing the preceding command. It states the EXE file creation success message.\n\nFigure 8.3: Converting Python app to Windows app\n\nNow, open the ml_app.exe file located in the dist folder.\n\nBuild Native ML Apps  169\n\nThe following figure shows the files and folders of the ML-based Tkinter app:\n\nFigure 8.4: Tkinter ML app\n\nOpen the app; you should see the window of the Tkinter app. It should display the prediction based on user input.\n\nThe following figure displays the prediction based on user data provided:\n\nFigure 8.5: Tkinter ML app\n\n170  Machine Learning in Production\n\nBuild an ML-based app using kivy and kivyMD In this section, using the pickle object of the trained model generated earlier in this chapter, let’s develop and deploy the FastAPI endpoint on the remote server, i.e., Heroku. You will learn how to deploy ML models on the Heroku platform with CI/ CD pipeline in the upcoming chapter.\n\nMoreover, you can deploy this FastAPI app using any other hosting service, such as PythonAnywhere or Amazon EC2. In the current scenario, a remote endpoint is used to make predictions in the Kivy app.\n\nSo, whenever users pass the data in the kivy app and press the Predict button, it will send the parameters to the API endpoint, where it will pass the received parameters to the model object and in return, send the prediction to API. The Kivy app will fetch the output (prediction) provided by the API.\n\nLet’s install FastAPI and uvicorn (an ASGI (Asynchronous Server Gateway Interface) server for production) as shown below:\n\npip install fastapi uvicorn\n\nNote: FastAPI requires Python 3.6+\n\nNow, create an app.py file. First, load the dependencies and pickle the object of the trained model. Also, create a FastAPI instance and assign it to the app. So, the app will be the point of interaction while creating API.\n\n1. # Import dependencies\n\n2. from fastapi import FastAPI\n\n3. from pydantic import BaseModel\n\n4. import uvicorn\n\n5. import pickle\n\n6. from fastapi.middleware.cors import CORSMiddleware\n\n7.\n\n8. app = FastAPI()\n\n9.\n\n10. origins = [\"*\"]\n\n11.\n\nBuild Native ML Apps  171\n\n12. app.add_middleware(\n\n13. CORSMiddleware,\n\n14. allow_origins=origins,\n\n15. allow_credentials=True,\n\n16. allow_methods=[\"*\"],\n\n17. allow_headers=[\"*\"],)\n\n18.\n\n19. # load the trained model\n\n20. trained_model = 'trained_model/model_rf.pkl'\n\n21. # pickle_in = open(trained_model, 'rb')\n\n22. model = pickle.load(open(trained_model, 'rb'))\n\nDefine the class LoanPred, which defines the datatype expected from the client.\n\nThe LoanPred class is used for the data model that is inherited from BaseModel. Add a root view of the function that returns 'message': 'Loan Prediction App' for the home page.\n\n1. class LoanPred(BaseModel):\n\n2. Gender: float\n\n3. Married: float\n\n4. ApplicantIncome: float\n\n5. LoanAmount: float\n\n6. Credit_History: float\n\n7.\n\n8. @app.get('/')\n\n9. def index():\n\n10. return {'message': 'Loan Prediction App'}\n\nThe following function will create a UI for user input. Here, create a /predict as an endpoint, also known as the route. Then, add the parameter of the type data model created, which is LoanPred.",
      "page_number": 189
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 197-204)",
      "start_page": 197,
      "end_page": 204,
      "detection_method": "topic_boundary",
      "content": "172  Machine Learning in Production\n\n1. # Defining the function which will make the prediction us- ing the data which the user inputs\n\n2. @app.post('/predict_status')\n\n3. def predict_loan_status(loan_details: LoanPred):\n\n4. data = loan_details.dict()\n\n5. gender = data['Gender']\n\n6. married = data['Married']\n\n7. income = data['ApplicantIncome']\n\n8. loan_amt = data['LoanAmount']\n\n9. credit_hist = data['Credit_History']\n\n10.\n\n11. # Making predictions\n\n12. prediction=model.predict([[gender, married, income, loan_ amt, credit_hist]])\n\n13.\n\n14. if prediction == 0:\n\n15. pred = 'Rejected'\n\n16. else:\n\n17. pred = 'Approved'\n\n18.\n\n19. return {'status':pred}\n\n20.\n\n21. @app.get('/predict')\n\n22. def get_loan_details(gender: float, married: float, income: float,\n\n23. loan_amt: float, credit_hist: float):\n\n24. prediction = model.predict([[gender, married, income, loan_ amt, credit_hist]]).tolist()[0]\n\n25. print(prediction)\n\n26. if prediction == 0:\n\n27. pred = 'Rejected'\n\n28. else:\n\nBuild Native ML Apps  173\n\n29. pred = 'Approved'\n\n30.\n\n31. return {'status':pred}\n\n32.\n\n33. if __name__ == '__main__':\n\n34. uvicorn.run(app, host='0.0.0.0', port=4000)\n\nNow, it’s time to run the app and see standard UI auto-generated by FastAPI, which uses swagger, now known as openAPI. uvicorn app:app --reload\n\nThe preceding command can be interpreted as follows:\n\nThe app refers to the name of the file in which the API is created. •\t The app is the instance defined in it or a file. •\t --reload will simply restart the FastAPI server every time changes are made in the app file.\n\nKivyMD app Kivy is an open-source software library for the quick development of apps with a simple GUI. Kivy is written in Python, so in order to use it, you should have Python installed on your system. Unlike Tkinter, it is not pre-installed with Python; you need to install it separately.\n\nKivyMD is an extension of the kivy framework. It is a collection of Material Design (MD) widgets and is mainly used for GUI building along with kivy. It is a good idea to create a virtual environment first, as it helps maintain different packages easily. You can install kivy and KivyMD using the following commands:\n\npip install kivy\n\npip install kivymd\n\nUse the deployed endpoint on the remote server. Keep it simple for now. Import the required dependencies first. Here, import MDApp, which is a base app. Build the app on top of this base app as it takes care of initialization and booting up the app.\n\nThen, import the Builder function for the front end of the app.\n\nNow create a file named main.py and add the following code to it. First, import the required dependencies into it.\n\n174  Machine Learning in Production\n\n1. from kivymd.app import MDApp\n\n2. from kivy.lang.builder import Builder\n\n3. from kivy.uix.screenmanager import Screen, ScreenManager\n\n4. from kivy.core.window import Window\n\n5. from kivy.network.urlrequest import UrlRequest\n\n6. import certifi as cfi\n\nThe screen is the base, and other components will be placed on top of it. It is similar to the web page. Here, you can assign a name to the screen. As you can see in the following code, a hierarchy is being maintained.\n\nThe screen managers mainly manage the different screens. Let’s understand each component. MDLabel is used to display the text with properties like horizontal alignment, font style, and so on.\n\nMDTextField is used to get user inputs. Here, hint_text is added to guide users while entering the data into the app. It has an id parameter, which will fetch the data entered by the user.\n\nMDButton is used to perform actions. Here, it is being used to call the predict() with the parameter on_press to complete the action.\n\n1. Builder_string = '''\n\n2. ScreenManager:\n\n3. Main:\n\n4. <Main>:\n\n5. name : ‹main›\n\n6. MDLabel:\n\n7. text: ‹Loan Prediction App›\n\n8. halign: ‹center›\n\n9. pos_hint: {‹center_y›:0.9}\n\n10. font_style: ‹H4›\n\n11.\n\n12. MDLabel:\n\n13. text: ‹Gender›\n\n14. pos_hint: {‹center_y›:0.75}\n\nBuild Native ML Apps  175\n\n15.\n\n16. MDTextField:\n\n17. id: input_1\n\n18. hint_text: ‹0:Female, 1:Male'\n\n19. width: 100\n\n20. size_hint_x: None\n\n21. pos_hint: {‹center_y›:0.75, ‹center_x›:0.50}\n\n22.\n\n23. MDLabel:\n\n24. text: ‹Marital Status›\n\n25. pos_hint: {‹center_y›:0.68}\n\n26.\n\n27. MDTextField:\n\n28. id: input_2\n\n29. hint_text: ‹0:No, 1:Yes'\n\n30. width: 100\n\n31. size_hint_x: None\n\n32. pos_hint: {‹center_y›:0.68, ‹center_x›:0.50}\n\n33.\n\n34. MDLabel:\n\n35. text: ‹Applicant Income›\n\n36. pos_hint: {‹center_y›:0.61}\n\n37.\n\n38. MDTextField:\n\n39. id: input_3\n\n40. hint_text: ‹6000›\n\n41. width: 100\n\n42. size_hint_x: None\n\n43. pos_hint: {‹center_y›:0.61, ‹center_x›:0.50}\n\n44.\n\n176  Machine Learning in Production\n\n45. MDLabel:\n\n46. text: ‹Loan Amount›\n\n47. pos_hint: {‹center_y›:0.54}\n\n48.\n\n49. MDTextField:\n\n50. id: input_4\n\n51. hint_text: ‹150›\n\n52. width: 100\n\n53. size_hint_x: None\n\n54. pos_hint: {‹center_y›:0.54, ‹center_x›:0.50}\n\n55.\n\n56. MDLabel:\n\n57. text: ‹Credit History›\n\n58. pos_hint: {‹center_y›:0.47}\n\n59.\n\n60. MDTextField:\n\n61. id: input_5\n\n62. hint_text: ‹0:Clear Debts, 1:Unclear Debts›\n\n63. width: 100\n\n64. size_hint_x: None\n\n65. pos_hint: {‹center_y›:0.47, ‹center_x›:0.50}\n\n66.\n\n67. MDLabel:\n\n68. pos_hint: {‹center_y›:0.2}\n\n69. halign: ‹center›\n\n70. text: ‹›\n\n71. id: output_text\n\n72. theme_text_color: «Custom»\n\n73. text_color: 0, 1, 1, 1\n\n74.\n\nBuild Native ML Apps  177\n\n75. MDRaisedButton:\n\n76. pos_hint: {‹center_y›:0.1, ‹center_x›:0.5}\n\n77. text: ‹Predict›\n\n78. on_press: app.predict()\n\n79. '''\n\nFirst store ScreenManager() into the sm object for ease of access, then add the main screen widget. After that, define the MainApp class (import MDApp in it as a base), which holds the core logic of the app. Define the build() for adding text and input widgets. The predict() will get the user data from the id directly as shown in the code.\n\nProvide these input parameters to the remote API endpoint, which will return the prediction for it.\n\nIn kivy, you can use UrlRequest() to parse the URL and the on_success parameter to be triggered on request completion. In the current case, use UrlRequest() to call res() on completion of the API request, which will replace the blank text of MDLabel defined earlier: MDRaisedButton.\n\n1. class Main(Screen):\n\n2. pass\n\n3.\n\n4. sm = ScreenManager()\n\n5. sm.add_widget(Main(name='main'))\n\n6.\n\n7. class MainApp(MDApp):\n\n8. def build(self):\n\n9. self.help_string = Builder.load_string(Builder_string)\n\n10. return self.help_string\n\n11.\n\n12. def predict(self):\n\n13. Gender = self.help_string.get_screen('main').ids.input_1. text\n\n14. Married = self.help_string.get_screen('main').ids. input_2.text\n\n15. ApplicantIncome = self.help_string.get_screen('main').ids.\n\n178  Machine Learning in Production\n\ninput_3.text\n\n16. LoanAmount = self.help_string.get_screen('main').ids. input_4.text\n\n17. Credit_History = self.help_string.get_screen('main').ids. input_5.text\n\n18. url = f'https://fastapi- appl. herokuapp.com/predict?gender={Gender}&married={Married}&income={Ap- plicantIncome}&loan_amt={LoanAmount}&credit_hist={Credit_History}'\n\n19. self.request = UrlRequest(url=url, on_success=self.res, ca_ file=cfi.where(), verify=True)\n\n20.\n\n21. def res(self, *args):\n\n22. self.data = self.request.result\n\n23. ans = self.data\n\n24. self.help_string.get_screen('main').ids.output_text. text = ans['status']\n\n25.\n\n26. MainApp().run()\n\nRun the app using python main.py and if everything goes well, you should see the app up and running.\n\nThe following figure shows the kivy app running successfully.\n\nFigure 8.6: Kivy ML app\n\nBuild Native ML Apps  179\n\nConvert the Python app into an Android app Now, you will learn to convert this into an Android app using the Buildozer package. Make sure your kivy filename is the main.py for this step. You need to install the Buildozer package first, if it is not installed already.\n\npip install buildozer\n\nInstall the required dependencies, as follows:\n\npip install cython==0.29.19\n\nsudo apt-get install -y \\\n\npython3-pip \\\n\nbuild-essential \\\n\ngit \\\n\npython3 \\\n\npython3-dev \\\n\nffmpeg \\\n\nlibsdl2-dev \\\n\nlibsdl2-image-dev \\\n\nlibsdl2-mixer-dev \\\n\nlibsdl2-ttf-dev \\\n\nlibportmidi-dev \\\n\nlibswscale-dev \\\n\nlibavformat-dev \\\n\nlibavcodec-dev \\\n\nzlib1g-dev\n\nsudo apt-get install -y \\\n\nlibgstreamer1.0 \\\n\ngstreamer1.0-plugins-base \\\n\ngstreamer1.0-plugins-good\n\nsudo apt-get install build-essential libsqlite3-dev sqlite3 bzip2 libbz2-",
      "page_number": 197
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 205-212)",
      "start_page": 205,
      "end_page": 212,
      "detection_method": "topic_boundary",
      "content": "180  Machine Learning in Production\n\ndev zlib1g-dev libssl-dev openssl libgdbm-dev libgdbm-compat-dev liblzma- dev libreadline-dev libncursesw5-dev libffi-dev uuid-dev libffi6\n\nsudo apt-get install libffi-dev\n\nYou may need to install additional dependencies as per system requirements.\n\nNow, create the buildozer.spec file by running the following command: buildozer init\n\nThe preceding command will create a standard buildozer.spec file. However, you need to modify it as per requirement. For instance, providing dependencies next to requirements under the Application requirements section, providing package name, title, and domain, providing internet permission under the Permissions section, and so on.\n\nFinally, build and package the Android app using the following command: buildozer -v android debug\n\nOptionally, you can use Google Colab notebook.\n\nThe following figure shows the files and directories for the KivyML app:\n\nFigure 8.7: Kivy ML app\n\nYou have successfully built and run ML-based Tkinter and Kivy apps. Also, you have learned to package them in native applications.\n\nConclusion In this chapter, you learned to develop an ML application using the most open- source frameworks: Tkinter, kivy, and KivyMD. You created a user-friendly yet\n\nBuild Native ML Apps  181\n\nsimple UI to receive input data from users and display responses. You explored the functionalities of Tkinter and kivy and converted the Tkinter app to a desktop app with Windows executable file using Pyinstaller and the kivyMD app to an Android app using Buildozer.\n\nIn the next chapter, you will learn how to build CI/CD pipelines for ML.\n\nPoints to remember\n\nTkinter comes pre-installed with Python. The best way to get the latest version of Tkinter is to install Python version 3.7 or later.\n\nKivyMD is an extension of the kivy framework. •\t Tkinter and kivy are both open-source and free to use. •\t MDLabel is used to display text in kivyMD.\n\nMultiple choice questions\n\n1.\n\nIn Tkinter, what is/are the geometry manager(s) is/are?\n\na) Grid\n\nb) Place\n\nc) Pack\n\nd) All the above\n\n2. The use of the mainloop()in Tkinter is\n\na) Keep app running\n\nb) Execute any loop defined\n\nc) Both\n\nd) None of them\n\nAnswers 1. a\n\n2. d\n\nQuestions\n\n1. What is Tkinter?\n\n2. What is the role of the pack() in Tkinter?\n\n3. How to define buttons in the kivy app?\n\n182  Machine Learning in Production\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com\n\nCI/CD for ML  183\n\nChapter 9 CI/CD for ML\n\nIntroduction Nowadays, automation is all around us. By automating the manual process, you save time, effort, and cost. The best part about automation is that it reduces human errors. You can automate most of the parts while incorporating new changes or updates in the application.\n\nCI/CD pipeline enables you to deploy the updates to the application in an automated way. This chapter will touch upon the elements of the CI/CD pipeline and ways to leverage it for ML applications. In this chapter, you will learn the different stages of the CI/CD pipeline, their importance in MLOps, and how to build one.\n\nStructure This chapter discusses the following topics:\n\nCI/CD pipeline for ML\n\no Continuous Integration (CI)\n\no Continuous Delivery/Deployment (CD)\n\no Continuous Training (CT)\n\n•\t Tools and platforms for building CI/CD pipeline\n\n184  Machine Learning in Production\n\n\n\nIntroduction to Jenkins\n\nBuilding CI/CD pipeline using GitHub, Docker, and Jenkins\n\nObjectives After studying this chapter, you should be able to build a CI/CD pipeline to deploy ML models in production, integrate GitHub with Jenkins, run the tests using Jenkins’s job and generate a test summary in Jenkins, build a Docker image, and run a Docker container using Jenkins. Finally, you should be able to integrate Jenkins with an email account to get the status of the build and deployment.\n\nCI/CD pipeline for ML is an acronym for Continuous Integration/Continuous Delivery/ CI/CD Deployment (CI/CD). The purpose of the CI/CD pipeline is to automate the chain of interconnected steps to deploy an application or release a new version of the software.\n\nWhen a new feature gets added to the application, any improvement needs to be integrated with the application. However, it involves different teams that execute multiple tasks and validate them before moving on to the production stage. Mostly, this is a manual and time-consuming process, and it can cause a delay in the release of the new version.\n\nCI/CD pipeline helps in automating the testing, running error-free code for the application, faster deployments, saving time and cost for developers, high reliability, and so on.\n\nCI/CD pipeline enables you to push the changes from development to deployment quickly, which usually consist of four stages:\n\nCommit code changes: After making changes to the file or code, the developer pushes the updates to the source repository. This activity is often performed in a team. CI/CD pipeline enables any team member to check the integrity of the code. Hence, you can automatically push the changes to the repository after it passes the tests.\n\nBuild: In this phase, it fetches the changes from the repository for the build. It keeps a watch on the source repository for any changes. As soon as it detects the changes, it initiates the build process and validates the build results after build completion.\n\nTest: The test phase runs the automated tests, such as unit tests, pytest, and API tests, on top of the build. It is a vital stage of the CI/CD pipeline. This\n\nCI/CD for ML  185\n\nensures the overall integrity of the code and prevents any broken code from passing on to the next phase.\n\nDeploy: This phase deploys the changes to the production environment.\n\nThe following figure shows the different stages of the CI/CD pipeline. However, organizations can modify it as per their goals.\n\nFigure 9.1: CI/CD Pipeline\n\nListing down some popular tools for CI/CD pipeline:\n\n\n\nJenkins\n\nGitHub Actions •\t Bamboo •\t CircleCI •\t GitLab CI/CD •\t Travis CI\n\nContinuous Integration (CI) In Continuous Integration (CI), the team of developers builds, run, and test code in local environments first. If everything goes well then, they push the updates to the repository. After this, the chain of steps starts to run, which involves the build, run, and test stages. The project members get notified at each step and get timely updates, such as the build outcome and test outcomes. Finally, the artifacts get stored and the report of the current status is sent via email or notified via Slack.\n\nWhen a team of developers is working on the same application, they push the code changes to the repository. However, due to changes in the environment, such as the\n\n186  Machine Learning in Production\n\nproduction environment, the code can break or throw an error. On the other hand, there could be a conflict between updates, when multiple developers try to push the updates of the application to the central code repository. This issue is taken care of by the CI stage, where developers can push the changes that will pass through the defined stages, such as build, run, and test, to ensure that the code is working properly without any issues. However, if any of the CI stages fail, then you will be notified, and the further process stops. This way, you can avoid integrating any broken code into production. Developers can frequently push and check the functioning of the code, flow, and integrity of the code or applications before pushing it to the next stage for deployment.\n\nContinuous Delivery/Deployment (CD) CD refers to Continuous Delivery or Continuous Deployment (the terms are used interchangeably) based on the level of automation you are planning to implement. The CD stage depends on the CI stage. Once the CI stage is completed, it triggers the CD stage in the pipeline. The purpose of the Continuous Delivery (CD) stage is to deliver an error-free codebase or artifact to the pre-production environment. In this stage, you can add a series of test cases (wherever required) to ensure a stable build and functional application. It sends the test status report to the team or developer, and then the application is manually pushed to the production environment. If any of the steps fail, you may need to carry out the entire process again with the required updates.\n\nOn the other hand, continuous deployment goes one step further and deploys the application from the pre-production environment to the production environment quickly. This removes the step of manual deployment of an application to the production environment. However, it is optional, as it depends on the developer and operation team to choose the level of automation they want to implement as per the business and application’s nature.\n\nContinuous Training (CT) A new stage introduced in the traditional CI/CD pipeline is Continuous Training (CT). In this stage, you expect the models to be trained continuously as new data comes in or any event occurs, such as the accuracy of the model dropping below the acceptable threshold. This may add slight complexity to CI/CD pipeline, but it is essential for Machine Learning deployment.\n\nWith CI/CD, Continuous Training (CT) is equally important in MLOps. Model retraining depends on scenarios and various other factors, such as how frequently data is changing and the schema of input data. It also depends on events such as accuracy dropping below an acceptable threshold, specific periods such as the end of every week, or manual triggers.\n\nCI/CD for ML  187\n\nIntroduction to Jenkins Jenkins is an open-source, modular CI/CD automation tool written in Java that comes with several plugins. Jenkins enables the smooth and continuous flow of building, testing, and deploying apps with a recently updated or developed codebase. It has a large community support and is popular among developers.\n\nIf the build is successful, then Jenkins automatically executes a series of steps from the code repository, and if everything goes well, it deploys the application to the server.\n\nHere are some salient features of Jenkins:\n\n\n\nJenkins is an open-source, free, and modular tool.\n\n\n\nIt is created by developers and for developers.\n\n\n\nJenkins is easy to install, configure and can be installed on Linux, MacOS, and Windows.\n\nA large number of Jenkins plugins are available for popular cloud platforms. • Jenkins’s master can distribute the load to multiple slaves and enable faster processing.\n\nInstallation Jenkins can be installed on any server that supports JAVA, as it is written in JAVA. Jenkins installation is described here with various options.\n\nhttps://www.jenkins.io/doc/book/\n\nDocker •\t Kubernetes •\t Linux •\t MacOS •\t WAR files •\t Windows •\t Other systems •\t Offline installations • Initial settings",
      "page_number": 205
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 213-220)",
      "start_page": 213,
      "end_page": 220,
      "detection_method": "topic_boundary",
      "content": "188  Machine Learning in Production\n\nAs you are installing Jenkins on Ubuntu, you can refer to the following link for a step-by-step installation guide.\n\nhttps://www.jenkins.io/doc/book/installing/linux/#debianubuntu\n\nStep 1: Install Jenkins\n\nAdd the repository key:\n\nwget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add –\n\nAppend the Debian package repository address to the server’s sources.list: sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/ apt/sources.list.d/jenkins.list'\n\nExecute the following command so that the apt will use the latest repository: sudo apt update\n\nFinally, install Jenkins with its dependencies:\n\nsudo apt install Jenkins\n\nStep 2: Start Jenkins service\n\nNow, start the Jenkins service using the following command:\n\nsudo systemctl start Jenkins\n\nTo check the status of Jenkins, use the following command:\n\nsudo systemctl status Jenkins\n\nStep 3: Allow Jenkins’s default port in the Firewall By default, Jenkins will run on its default port, that is, 8080. Hence, port 8080 needs to be allowed in the firewall. To do so, run the following command in the terminal:\n\nsudo ufw allow 8080\n\nThen, to confirm it, use the following command:\n\nsudo ufw status\n\nStep 4: Set up Jenkins\n\nYou can use Jenkins’s web-based UI server IP or domain name to access Jenkins: http://<server_ip_or_domain>:8080\n\nInitially, you will get a screen asking for an administrator password; you can get this by using the following command:\n\nCI/CD for ML  189\n\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword\n\nCopy the alphanumeric password from the terminal and paste it into the Administrator password field.\n\nIn the next window, you can continue as admin or create the first admin user by filling up the form.\n\nNow, add the Jenkins user to the Docker group using the following command:\n\nsudo usermod -aG docker jenkins\n\nBuild CI/CD pipeline using GitHub, Docker, and Jenkins Here, Jenkins is used for automated ML workflow. First off, you need to create a codebase in the local machine and run the ML app locally to make sure it is working properly on the local machine. Next, you will push the changes to the GitHub repository. Then, integrate the GitHub repo and Jenkins by webhook. Jenkins is to be installed on pre-production or production servers. Jenkins will pull the latest codebase from the linked GitHub repo and deploy the ML app on the server.\n\nDevelop codebase Let’s consider the scenario of loan prediction, where you need to predict whether a customer’s loan will be approved. The focus will not be on hyperparameter tuning and model optimization. However, you can optimize a model to improve its overall performance.\n\nIn this chapter, you will use a machine learning package developed earlier.\n\nYou can access the code repository at the following link:\n\nhttps://github.com/suhas-ds/docker_pkg_jenkins\n\nThe following is the directory structure for codebase:\n\n.\n\n| Dockerfile\n\n| main.py\n\n| requirements.txt\n\n\\---src\n\n| MANIFEST.in\n\n| README.md\n\n190  Machine Learning in Production\n\n| requirements.txt\n\n| setup.py\n\n| tox.ini\n\n+---prediction_model\n\n| | pipeline.py\n\n| | predict.py\n\n| | train_pipeline.py\n\n| | VERSION\n\n| | __init__.py\n\n| +---config\n\n| | config.py\n\n| | __init__.py\n\n| +---datasets\n\n| | test.csv\n\n| | train.csv\n\n| | __init__.py\n\n| +---processing\n\n| | data_management.py\n\n| | preprocessors.py\n\n| | __init__.py\n\n| \\---trained_models\n\n| classification_v1.pkl\n\n| __init__.py\n\n\\---tests\n\npytest.ini\n\ntest_predict.py\n\nmain.py\n\nFastAPI is a web framework for developing RESTful APIs in Python. Create a main. py file to run the FastAPI app.\n\nNote: FastAPI requires Python 3.6+.\n\nCI/CD for ML  191\n\n1. # Importing Dependencies\n\n2. from fastapi import FastAPI\n\n3. from pydantic import BaseModel\n\n4. import uvicorn\n\n5. import pickle\n\n6. import numpy as np\n\n7. import pandas as pd\n\n8. from fastapi.middleware.cors import CORSMiddleware\n\n9. from prediction_model.predict import make_prediction\n\n10. import pandas as pd\n\nCreate a FastAPI instance and assign it to the app so that the app will be a point of interaction while creating the API.\n\n1. app = FastAPI(\n\n2. title=\"Loan Prediction Model API\",\n\n3. description=\"A simple API that uses ML model to predict the Loan application status\",\n\n4. version=\"0.1\",\n\n5. )\n\nCORS (Cross-Origin Resource Sharing) refers to the situation when a front end running in a browser has JavaScript code that communicates with a back end, and the back end is of a different origin than the front end. However, it depends on your application and requirement whether to use it or not.\n\n1. origins = [\"*\"]\n\n2.\n\n3. app.add_middleware(\n\n4. CORSMiddleware,\n\n5. allow_origins=origins,\n\n6. allow_credentials=True,\n\n7. allow_methods=[\"*\"],\n\n8. allow_headers=[\"*\"],\n\n9. )\n\n192  Machine Learning in Production\n\nDefine the class LoanPred, which defines the data type expected from the client.\n\nYou can use the LoanPred class for the data model that is inherited from BaseModel. Then, add a root view of the function that returns 'message': 'Loan Prediction App' for the home page.\n\n1. class LoanPred(BaseModel):\n\n2. Gender: str\n\n3. Married: str\n\n4. Dependents: str\n\n5. Education: str\n\n6. Self_Employed: str\n\n7. ApplicantIncome: float\n\n8. CoapplicantIncome: float\n\n9. LoanAmount: float\n\n10. Loan_Amount_Term: float\n\n11. Credit_History: float\n\n12. Property_Area: str\n\n13.\n\n14. @app.get('/')\n\n15. def index():\n\n16. return {'message': 'Loan Prediction App'}\n\nHere, create /predict_status as an endpoint, also known as the route. Then, add predict_loan_status() with a parameter of the type data model that you created as LoanPred.\n\n1. #Defining the function which will make the prediction using the data which the user inputs\n\n2. @app.post('/predict_status')\n\n3. def predict_loan_status(loan_details: LoanPred):\n\n4. data = loan_details.dict()\n\n5. Gender = data['Gender']\n\n6. Married = data['Married']\n\n7. Dependents = data['Dependents']\n\nCI/CD for ML  193\n\n8. Education = data['Education']\n\n9. Self_Employed = data['Self_Employed']\n\n10. ApplicantIncome = data['ApplicantIncome']\n\n11. CoapplicantIncome = data['CoapplicantIncome']\n\n12. LoanAmount = data['LoanAmount']\n\n13. Loan_Amount_Term = data['Loan_Amount_Term']\n\n14. Credit_History = data['Credit_History']\n\n15. Property_Area = data['Property_Area']\n\n16.\n\n17. # Making predictions\n\n18. input_data = [Gender, Married, Dependents, Education,\n\n19. Self_Employed, ApplicantIncome, CoapplicantIncome,\n\n20. LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]\n\n21. cols = ['Gender','Married','Dependents',\n\n22. 'Education','Self_Employed','ApplicantIncome',\n\n23. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n24. 'Credit_History','Property_Area']\n\n25. data_dict = dict(zip(cols,input_data))\n\n26. prediction = make_prediction([data_dict])['prediction'][0]\n\n27.\n\n28. if prediction == 'Y':\n\n29. pred = 'Approved'\n\n30. else:\n\n31. pred = 'Rejected'\n\n32.\n\n33. return {'status':pred}\n\nThe following function will create the UI for user input. Here, create /predict as an endpoint, also known as a route, and declare input data types expected from users.\n\n1. @app.post('/predict')\n\n2. def get_loan_details(Gender: str, Married: str, Dependents: str,\n\n194  Machine Learning in Production\n\n3. Education: str, Self_Employed: str, ApplicantIncome: float,\n\n4. CoapplicantIncome: float, LoanAmount: float, Loan_Amount_Term: float,\n\n5. Credit_History: float, Property_Area: str):\n\n6.\n\n7. input_data = [Gender, Married, Dependents, Education,\n\n8. Self_Employed, ApplicantIncome, CoapplicantIncome,\n\n9. LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]\n\n10. cols = ['Gender','Married','Dependents',\n\n11. 'Education','Self_Employed','ApplicantIncome',\n\n12. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n13. 'Credit_History','Property_Area']\n\n14.\n\n15. data_dict = dict(zip(cols,input_data))\n\n16. prediction = make_prediction([data_dict])['prediction'][0]\n\n17. if prediction == 'Y':\n\n18. pred = 'Approved'\n\n19. else:\n\n20. pred = 'Rejected'\n\n21.\n\n22. return {'status':pred}\n\n23.\n\n24. if __name__ == '__main__':\n\n25. uvicorn.run(app)\n\nThe file for the FastAPI app is completed.\n\nrequirements.txt\n\nNow, create the requirements.txt file, as follows. In this file, model requirements, test requirements, and FastAPI requirements are defined separately for better understanding and ease of management.\n\n1. # Model building requirements\n\n2. joblib==0.16.0\n\nCI/CD for ML  195\n\n3. numpy==1.19.0\n\n4. pandas==1.0.5\n\n5. scikit-learn==0.23.1\n\n6. scipy==1.5.1\n\n7. sklearn==0.0\n\n8.\n\n9. # testing requirements\n\n10. pytest<5.0.0,>=4.6.6\n\n11.\n\n12. # packaging\n\n13. setuptools==40.6.3\n\n14. wheel==0.32.3\n\n15.\n\n16. # FastAPI app requirements\n\n17. fastapi>=0.68.0,<0.69.0\n\n18. pydantic>=1.8.0,<2.0.0\n\n19. uvicorn>=0.15.0,<0.16.0\n\n20. gunicorn>=20.1.0\n\nDockerfile\n\nCreate a Dockerfile and name it Dockerfile without any file extension, and add the following commands to it.\n\n1. FROM python:3.7-slim-buster\n\n2.\n\n3. RUN apt-get update && apt-get install -y \\\n\n4. build-essential \\\n\n5. libpq-dev \\\n\n6. && rm -rf /var/lib/apt/lists/*\n\n7.\n\n8. RUN pip install --upgrade pip\n\n9.",
      "page_number": 213
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 221-231)",
      "start_page": 221,
      "end_page": 231,
      "detection_method": "topic_boundary",
      "content": "196  Machine Learning in Production\n\n10. COPY . /code\n\n11.\n\n12. RUN chmod +x /code/src\n\n13.\n\n14. RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\n\n15.\n\n16. EXPOSE 8005\n\n17.\n\n18. WORKDIR /code/src\n\n19.\n\n20. ENV PYTHONPATH \"${PYTHONPATH}:/code/src\"\n\n21.\n\n22. CMD pip install -e .\n\nIn the preceding Dockerfile, it will first pull the base image, that is, Python 3.7 slim buster, and install the necessary packages. After that, packages from the requirements file will be installed, and the 8005 port will be exposed so that it can be accessed outside the Docker container. Finally, it will assign PYTHONPATH in the ENV instruction and will execute the command mentioned in the CMD instruction to install the Python package in editable mode.\n\nCreate a Personal Access Token (PAT) on GitHub If you perform any action like pull, push, or clone using the git cli command, then it won’t work anymore with the GitHub password. As a matter of fact, GitHub has removed password authentication. Instead, you have to use a Personal Access Token (PAT).\n\nPATs are an alternative to using passwords for authentication to GitHub Enterprise Server when using the GitHub API or the command line.\n\nFirst, you need to create a PAT. This is described in the following link:\n\nhttps://docs.github.com/en/enterprise-server@3.4/authentication/keeping-your- account-and-data-secure/creating-a-personal-access-token\n\nTo generate or regenerate your PAT, you can go to Settings | Tokens (login required):\n\nhttps://github.com/settings/tokens\n\nCI/CD for ML  197\n\nIf you are not prompted for your username and password, your credentials may be cached in your machine.\n\nCreate a webhook on the GitHub repository A webhook can be considered a lightweight API that enables one-way data sharing after being triggered by certain events. In GitHub, a webhook can be triggered when actions, such as push and pull, are performed on a repository. With the help of the GitHub webhook, you can trigger the CI stage on a remote server.\n\nGo to your GitHub repo | Settings | Webhooks.\n\nAlternatively, you can directly jump to that page using the following URL:\n\nhttps://github.com/<username>/<your-repo-name>/settings/hooks\n\nIn the current case, it is as follows:\n\nhttps://github.com/suhas-ds/docker_pkg_jenkins/settings/hooks\n\nYou cannot use https://localhost:8080/github-webhook/ in GitHub webhook’s payload URL as the localhost URL needs to be exposed to the internet. You can use ngrok for exposing localhost URLs on the internet. Refer to the official site of ngrok https://ngrok.com/ for more details.\n\nHere, ngrok is being used for demonstration purposes; however, the remote server’s IP or domain can be used in the deployment stage.\n\nUse the command ngrok http 8080 to expose the 8080 port publicly.\n\nFigure 9.2: ngrok\n\n198  Machine Learning in Production\n\nProvide a payload URL, as shown in the following figure. The payload URL format was discussed earlier in this chapter. Select the Content type as an application/json format. The Secret field is optional. Let’s leave it blank for now.\n\nFigure 9.3: GitHub Webhook\n\nChoose the Enabled SSL verification and Just push the event options, as shown in the following figure, so that it will send data only when someone pushes updates to the repository. However, you can also select the individual events, that is, Let me select individual events.\n\nNow, click on the Add Webhook button to save Jenkins GitHub Webhook.\n\nFigure 9.4: GitHub Webhook\n\nCI/CD for ML  199\n\nConfigure Jenkins Now, mainly two sections are to be configured for the current scenario: GitHub webhook integration and extended email integration.\n\n1. Go to Jenkins dashboard.\n\n2. Go to Manage-Jenkins | Manage Plugin.\n\n3.\n\nInstall Git, GitHub, and email plugins without restart.\n\nIf the preceding plugins are already selected, you can continue to the next step.\n\nNow, let’s configure the GitHub webhook in Jenkins.\n\nGo to Manage Jenkins | Configure System.\n\nScroll down and update the API URL in the GitHub section, as follows:\n\nhttps://api.github.com\n\nIn the Credentials section, add GitHub’s PAT generated in the previous step. You can leave the Name field blank.\n\nFinally, click on the Save button.\n\nThe following figure shows GitHub webhook integration in Jenkins:\n\nFigure 9.5: GitHub configuration in Jenkins\n\n200  Machine Learning in Production\n\nNow, whenever the developer pushes the updates to the repository, Jenkins will detect the changes and start executing a series of commands one by one.\n\nNext, configure the email for a feedback loop or status updates, for example, succeeded or failed.\n\nGo to Manage Jenkins | Configure System.\n\nScroll down and update the fields in the GitHub section.\n\nIn the following figure, the Default Subject field is updated by adding Status at the beginning. Also, an email body is added in the Default Content field. It is highly customizable, so you can update it as per requirement.\n\nFigure 9.6: Email configuration\n\nIn the Default Triggers section, choose the Always checkbox so that Jenkins sends the status email for both success and failure scenarios, as shown in the following figure:\n\nFigure 9.7: Email configuration - trigger\n\nNow, select HTML (text/html) from the dropdown in the Default Content Type field and add your email ID in Default Recipients, as shown in the following figure:\n\nCI/CD for ML  201\n\nFigure 9.8: Email configuration – content type and recipients\n\nIn the Extended E-mail Notification section, provide details for the email notifications. In the current scenario, Gmail details are provided, as follows:\n\nsmtp.gmail.com to SMTP server •\t 465 to SMTP Port •\t Add email login credentials •\t Check the Use SSL box •\t Click on the Save button\n\nProvide your email details as shown in the following figure:\n\nFigure 9.9: Extended email notification\n\n202  Machine Learning in Production\n\nScroll down and click on the Apply and Save buttons to save the changes, as shown in the following figure:\n\nFigure 9.10: Extended email notification\n\nIf you are integrating your Gmail account for email notifications, you will have to allow less secure apps in the settings of your Gmail account. You can go to the following URL to turn it on if it is off.\n\nhttps://myaccount.google.com/lesssecureapps\n\nFor more details, visit https://support.google.com/accounts/answer/6010255?hl=en.\n\nThe required configuration for Jenkins is complete.\n\nCreate CI/CD pipeline using Jenkins Now, you will build a simple CI/CD pipeline using GitHub and Jenkins. When developers push the updates to the GitHub repository, the GitHub webhook detects the changes and sends the notification to Jenkins. Jenkins pulls the latest code from the GitHub repository, builds the Docker image, and runs the container using that image. Next, it trains the model and exports the pickle object from the trained model. In the next step, pytest results are exported and displayed in Jenkins. After passing all the tests, it will run the ML web app. Finally, it will send the feedback via email to the developer or concerned team.\n\nFigure 9.11: CI/CD pipeline using Jenkins\n\nCI/CD for ML  203\n\nStage 1: 1-GitHub-to-container At this stage, Jenkins will pull the latest code and files from GitHub as soon as the developer pushes the updates to the linked GitHub repository. Let’s understand this process step by step.\n\nWhen the developer pushes the updates to the GitHub repository, the webhook detects the changes, and it gets triggered. GitHub webhook sends a message to Jenkins that new updates have been detected, and then Jenkins pulls the latest code files to start building the Docker image.\n\nAs shown in the following figure, select the New Item from the left panel of the Jenkins dashboard.\n\nFigure 9.12: Jenkins’s dashboard – New Item\n\nAs shown in the following figure, provide a job name in the Enter an item name field and choose a Freestyle project. In the current case, it is named 1-GitHub-to- docker-container.\n\nFigure 9.13: Jenkins’s dashboard – Freestyle project\n\n204  Machine Learning in Production\n\nAs shown in the following figure, select the Git radio button under the Source Code Management tab and provide the GitHub repository URL. By default, it will be the master branch, but you can update it. If your repository is private, then you may need to pass the credentials for the same.\n\nFigure 9.14: 1-GitHub-to-container\n\nAs shown in the following figure, choose the GitHub hook trigger for GITScm polling under the Build Triggers tab, to pull the latest updates from the GitHub repository.\n\nFigure 9.15: 1-GitHub-to-container - build triggers\n\nIn the following figure, Docker commands are being executed for building and running Docker images with the latest changes from the GitHub repository.\n\nCI/CD for ML  205\n\nFigure 9.16: 1-GitHub-to-container - Build\n\nAs you can see in the following figure, Jenkins started building the Docker image as soon as the updates were pushed to the GitHub repository. The first line of output read Started by GitHub push by suhas-ds; it is showing who pushed the updates to the GitHub repository.\n\nFigure 9.17: 1-GitHub-to-container – Console Output\n\nStage 2: 2-training At this stage, the model will be trained on training data by running the train_pipeline. py file inside the Docker container. As shown in the following figure, select the None\n\n206  Machine Learning in Production\n\nradio button under the Source Code Management tab, as it is the next stage; no need to pull the source code again.\n\nFigure 9.18: 2-training – Source code management\n\nAs shown in the following figure, select the Build after other projects are built option under the Build Triggers tab. Next, provide the name of the previous stage in the Projects to watch field. Then, select the first radio button, that is, Trigger only if the build is stable, which means stage 2 will start executing only after the successful completion of the previous stage.\n\nFigure 9.19: 2-training – Build triggers\n\nAs shown in the following figure, write Docker commands for training the model pipeline and exporting the latest pickle object of the trained model, under the Build tab.",
      "page_number": 221
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 232-241)",
      "start_page": 232,
      "end_page": 241,
      "detection_method": "topic_boundary",
      "content": "CI/CD for ML  207\n\nFigure 9.20: 2-training – Build\n\nThe console output of stage 2 is shown in the following figure. It tells you what caused stage 2 to trigger and the status of stage 2. After that, Jenkins is triggering stage 3 on the successful completion of stage 2.\n\nFigure 9.21: 2-training – Console output\n\nStage 3: 3-testing In this stage, Jenkins is running pytest and publishing test results using JUnit. Jenkins understands the JUnit test report in XML format. JUnit enables Jenkins to provide test results, including historical test result trends, failure tracking (if any), and a neat and clean web UI for viewing test reports. Choose the None radio button under the Source Code Management tab.\n\n208  Machine Learning in Production\n\nAs shown in the following figure, select Build after other projects are built, and Trigger only if build is stable under Build Triggers, which denotes that this stage is the next one and will start execution after completion of the previous stage.\n\nFigure 9.22: 3-testing – Build triggers\n\nAs shown in the following figure, under the Build tab, Jenkins is running tests inside the running Docker container using the Docker command. Next, the Docker command will export the pytest results in a .xml file. Then, the mkdir command will create a new directory as reports and copy test results to it.\n\nFigure 9.23: 3-testing – Build\n\nFinally, it is publishing the test results using the JUnit plugin.\n\nAs shown in the following figure, choose Publish JUnit test results report, under Post-build Actions.\n\nCI/CD for ML  209\n\nFigure 9.24: 3-testing – Post-build actions\n\nAs shown in the following figure, provide a path where you want to store the test results. Jenkins will fetch the test results from the mentioned path and publish the test results.\n\nFigure 9.25: 3-testing – JUnit test result report\n\nIn the following figure, you can see the build numbers of Stage 1 and Stage 2. Then, it runs the pytest and exports test results in a .xml file using JUnit.\n\nFigure 9.26: 3-testing – Console output\n\n210  Machine Learning in Production\n\nIn the following figure, you can see the test results of the three tests and the time taken for each test.\n\nFigure 9.27: 3-testing – Test results\n\nIn the following figure, you can see the status of test results in a graphical format.\n\nFigure 9.28: 3-testing – Test result trend\n\nStage 4: 4-deployment-status-email At this stage, Jenkins is deploying an ML web app using FastAPI and a trained model.\n\nCI/CD for ML  211\n\nAs shown in the following figure, select Build after other projects are built, and Trigger only if build is stable under Build Triggers, which means this stage is the next one and will start execution after completion of the previous stage.\n\nFigure 9.29: 4-deployment-status-email – Build triggers\n\nAs shown in the following figure, select Execute shell from the dropdown Add build step, under the Build tab, as it will execute the commands from the shell.\n\nFigure 9.30: 4-deployment-status-email – Execute shell\n\nIn the following figure, Jenkins is executing a FastAPI command inside a running Docker container named model1, and -d in the following command is to instruct the Docker container to run the command in detached mode. And -w /code means\n\n212  Machine Learning in Production\n\nit will change the working directory to /code. Finally, --host and --port are the address and port on which the FastAPI web app will be running, respectively.\n\nFigure 9.31: 4-deployment-status-email – Build\n\nAs shown in the following figure, choose the Extended Email Notification from the Add post-build action dropdown.\n\nFigure 9.32: 4-deployment-status-email – Post build action\n\nAs shown in the following figure, select the Content Type as HTML (text/html) so that you can use HTML tags in the body; however, it is optional.\n\nFigure 9.33: 4-deployment-status-email – Content type\n\nCI/CD for ML  213\n\nAs shown in the following figure, select the Always option under the Triggers section. This enables Jenkins to send an email despite the build status (Success or Failure). It will send the email to the mentioned people.\n\nFigure 9.34: 4-deployment-status-email – Triggers\n\nAs shown in the following figure, choose the Compress and Attach Build Log option from the Attach Build Log dropdown. By enabling this option, people will get the console output of the current stage so that people come to know what happened in the job without visiting Jenkin’s job; however, it is optional.\n\nFigure 9.35: 4-deployment-status-email – Attach build logs\n\n214  Machine Learning in Production\n\nAs you can see in the following figure, an email shows the result of stage 4. If the job fails due to any reason, then this email will be triggered because you selected the Always option in the extended email plugin. This email contains the job name, build number, and the status of the job. Here, the Attach Build Log option is not selected.\n\nFigure 9.36: 4-deployment-status-email – Failure email\n\nThe following figure contains the output of stage 4. As you can see, it shows the status of the preceding stages with the build number. In the end, it also shows the process of sending the email.\n\nFigure 9.37: 4-deployment-status-email – Console output\n\nCI/CD for ML  215\n\nAs you can see in the following figure, the received email is the result of the successful completion of stage 4. After completion of the job without any errors, this email will be triggered as the Always option was chosen in the extended email plugin. This email contains the job name, build number, and the status of the job. You can see the build output of the job.\n\nFigure 9.38: 4-deployment-status-email – Success email\n\nThe following figure shows the status of all jobs with the last success, last failure status, and duration to complete the job. You can see this status on the Jenkins dashboard.\n\nFigure 9.39: Jenkins’s dashboard – jobs overview\n\n216  Machine Learning in Production\n\nIt’s time to check the deployed loan prediction web app. This web app is running on the 8005 port, so you can access it by the server’s IP address (or domain name), followed by port 8005. When any changes are committed to a linked GitHub repository, Jenkins will trigger the first and subsequent stages one by one, and the latest web app will be deployed again. In short, after pushing the updates to the GitHub repository, you can sit and watch the status of each stage or just wait for the email.\n\nProvide the user data and hit the Execute button, as shown in the following figure.\n\nFigure 9.40: FastAPI – ML app\n\nThe following figure is the result of the preceding action. You can see the status as Approved in the response body. You can also integrate this API endpoint into other applications.",
      "page_number": 232
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 242-253)",
      "start_page": 242,
      "end_page": 253,
      "detection_method": "topic_boundary",
      "content": "CI/CD for ML  217\n\nFigure 9.41: FastAPI - Response\n\nThus, you learned to create a simple CI/CD pipeline using the open-source tool Jenkins. You can modify it as per business and application requirements.\n\nConclusion In this chapter, you explored the process of Continuous Integration (CI), Continuous Delivery (CD), Continuous Deployment (CD), and Continuous Training (CT) in the CI/CD pipeline. Also, you learned to create a simple CI/CD pipeline using the popular open-source tool, Jenkins. Moving on, you integrated GitHub and Jenkins using GitHub webhook, and you built a Docker image and ran the container as Jenkins’s job. You also executed and exported pytest results using the JUnit plugin. In the last stage, an ML web app was deployed on port 8005. Finally, it triggered the email with the status of the build, the job name, and the build logs.\n\nIn the next chapter, you will learn to build CI/CD pipelines that deploy ML apps on the Heroku platform using GitHub Actions.\n\nPoints to remember\n\nCD refers to Continuous Delivery or Continuous Deployment interchangeably, based on the level of automation you are planning to implement.\n\n218  Machine Learning in Production\n\n\n\nJenkins is an open-source, modular CI/CD automation tool written in Java, which comes with a large number of plugins.\n\nFastAPI requires Python 3.6 and above. •\n\nJenkins understands the JUnit test report XML format.\n\nMultiple choice questions\n\n1. Which plugin is used to display the test output in Jenkins?\n\na) pytest\n\nb) Selenium\n\nc) JUnit\n\nd) GitHub\n\n2. CD refers to which of the following?\n\na) Continuous Delivery\n\nb) Continuous Deployment\n\nc) Create Dictionary\n\nd) Both a and b\n\nAnswers 1. c\n\n2. d\n\nQuestions\n\n1. What is CT in a CI/CD pipeline?\n\n2. What are popular CI/CD tools/platforms?\n\n3. Which plugin is required to show test results in Jenkins?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com\n\nDeploying ML Models on Heroku  219\n\nChapter 10 Deploying ML Models on Heroku\n\nIntroduction Heroku is a Platform as a Service (PaaS) platform that enables developers to build, run, and operate applications entirely in the cloud. You can push the Docker container to Heroku or provide GitHub repository details, such as the branch name, to auto-deploy the web app as soon as you push new changes to the model. This way, you can make the ML model accessible on the web, ready to make predictions.\n\nIf you are a beginner, then GitHub CI/CD is the simplest platform to build an end- to-end CI/CD pipeline. It is easy to manage as everything is in GitHub. You only need a GitHub repository to create and run a GitHub Actions workflow.\n\nRefer to the previous chapters for the concepts discussed, such as packaging ML models, FastAPI, docker, and CI/CD pipeline.\n\nStructure This chapter discusses the following topics:\n\nCreate a Heroku account and install Heroku CLI •\t Create a Heroku app •\t Deploy the web app to Heroku using Heroku CLI •\t Build CI/CD pipeline using GitHub Actions\n\n220  Machine Learning in Production\n\nObjectives After studying this chapter, you should be able to deploy the ML model on Heroku - Platform as a Service (PaaS) and integrate the GitHub repository into Heroku for automated deployment. Create the Heroku app from Heroku web UI and the terminal. You will learn to build and run CI/CD pipelines using GitHub Actions and Heroku. You will also learn to execute multiple tests using pytest and tox on GitHub Actions. Create YAML files for GitHub Actions to run the workflow.\n\nHeroku Heroku is a container-based cloud Platform as a Service (PaaS) platform. Developers use Heroku to deploy, manage, and scale modern apps. It has a pretty easy process to deploy your applications.\n\nHeroku saves you time by removing the difficulty of maintaining servers, hardware, or infrastructure. It supports the most popular languages, such as Node, Ruby, Java, Clojure, Scala, Go, Python, and PHP.\n\nSimply put, Heroku allows you to make your apps available on the internet for others to access, like a website. With a few steps, it allows you to make your app accessible to others. You can focus on app development without worrying about infrastructure, servers, and other such things.\n\nIt enables easy integration with GitHub to make it easy to deploy code available on the GitHub repository to apps that are running on Heroku. When GitHub integration is configured for a Heroku app, Heroku can automatically build and release (if the build is successful) the updates when pushed to the specified repository.\n\nHeroku apps can be scaled to run on multiple dynos (a container that runs a Heroku app’s code) simultaneously (except on Free or Hobby dynos) with simple steps to avoid any downtime. Heroku offers manual and auto scaling of the dynos. You can scale it horizontally by adding more dynos to handle the heavy traffic of requests. On the other hand, you can scale it vertically by increasing resources like memory and CPU as required.\n\nHeroku offers Continuous Integration and Continuous Deployment. It has built-in test facilities to achieve Continuous Deployment. Heroku apps that share the same codebase can be organized into deployment pipelines, promoted easily from one stage to the next, and managed through a visual interface.\n\nThere are three methods to deploy the web app on the Heroku platform:\n\nDeployment with Heroku git •\t Deployment with GitHub repository integration\n\nDeploying ML Models on Heroku  221\n\nDeployment with Container Registry\n\nNow that you must have got the gist of it, you are going to deploy a Machine Learning model to Heroku. After studying this chapter, you will be in a better position to work on Heroku for ML deployments.\n\nSetting up Heroku You need to create a Heroku account. This is a one-time activity. Go to the Heroku platform at https://www.heroku.com/\n\nStep 1:\n\nCreate a Heroku account (if you don’t have one) and log in.\n\nSelect the primary development language as Python and create a free account.\n\nStep 2:\n\nAfter logging in to the account, create a new app, as shown in the following figure:\n\nFigure 10.1: Create a new app\n\nClick on the Create new app button and add the app name. In this case, it is docker- ml-cicd.\n\nHit the Create app button and complete the process.\n\nNote: Two or more Heroku apps can’t have the same name.\n\nStep 3:\n\nYou can click the Open app button to see the app. For the current scenario, it is https://docker-ml-cicd.herokuapp.com/\n\nIt will display the following text:\n\nHeroku | Welcome to your new app!\n\nNow, go back to your Heroku app dashboard and install the Heroku Command Line Interface (CLI) for ease of access through the terminal. Heroku's CLI installation steps are mentioned at https://devcenter.heroku.com/articles/heroku-cli. You can\n\n222  Machine Learning in Production\n\nexecute Heroku commands through the local terminal. At this stage, create one more app for production, and name it docker-ml-cicd-prod.\n\nAt this stage, your Heroku account should be ready with the apps you have created.\n\nDeployment with Heroku Git This method is pretty much straightforward. You need Heroku CLI for this method. Add Heroku’s Git remote repository and push the changes to it directly.\n\nFrom the Heroku application dashboard, go to the Deploy tab. You will see three methods of deployment there:\n\nHeroku Git •\t GitHub •\t Container Registry\n\nThe following figure shows the deployment methods available on the Heroku platform:\n\nFigure 10.2: Deployment method\n\nFrom the preceding methods, you need to select the first method, that is, the Heroku Git method, and follow the steps given below that option.\n\nDeployment with GitHub repository integration Heroku comes with the GitHub integration method, which enables you to automate the deployment process. Post GitHub and Heroku integration, any changes pushed to the repository will trigger the process that deploys the Heroku app at the end.\n\nOn Heroku, you can create a pipeline and add both apps to it. Heroku allows you to create review (temporary) apps before deploying the staging app. This additional option enables you to see the working of the app before pushing it to the staging or pre-deployment stage.\n\nDeploying ML Models on Heroku  223\n\nThe pipeline has been divided into three stages:\n\nREVIEW APPS •\t STAGING •\t PRODUCTION\n\nREVIEW APPS This creates review apps for open pull requests. Each review apps has a unique URL that can be shared for testing purposes. For standardizing it, you can set the pattern of the review app’s URL.\n\nWhen there is an open pull request, it gets created automatically for review purposes, and after the pull request is merged, the review apps gets deleted. It means that the app is no longer accessible.\n\nNote: You should enable both Heroku Pipelines and GitHub integration for the Heroku app to use review apps.\n\nUnder REVIEW APPS, hit the Enable Review Apps button.\n\nAfter that, in the pop-up window, select the checkboxes for the following:\n\nCreate new review apps for new pull requests automatically: When enabled, every new pull request opened will create a review app automatically, and the pull request that is closed will delete that Review app.\n\nDestroy stale review apps automatically: You get the dropdown option for the lifespan of the app, like After 1 day, 2 days, 5 days, or 30 days. The default value is 5 days.\n\nClick on Enable Review Apps.\n\nSTAGING Staging apps can be used to preview code changes and features before being deployed to production, which is known as the pre-production stage. Directly deploying it to the production environment by skipping staging deployment is not recommended. It may happen that an app that is working on your local machine may not work on Heroku (cloud). You might spend a lot of time finding and fixing an issue and lose face in front of customers. Once you verify an app that is working properly in the staging environment, you should push it to the production environment by using the Promote to production button.\n\nUnder STAGING, add the staging app.\n\n224  Machine Learning in Production\n\nPRODUCTION Production apps run your customer-facing code. It is recommended to promote your code from a staging app only after it has been tested.\n\nUnder PRODUCTION, you should create a new app for production.\n\nHeroku Pipeline flow When someone pushes the changes to a branch (other than the master branch), then on the GitHub repository, hit the Compare & pull request button and click on the Create pull request button. You should see a new app getting built (under REVIEW APPS) upon pull request on GitHub. Once the app is ready, you can see that the review app is up and running.\n\nIf everything goes well, the lead developer can go back to GitHub and merge the pull request to the master branch of the GitHub repository. Within a few seconds, the staging app should start building. Once it is deployed, you can open it and verify the staging app. However, if you push the changes directly to the master branch, the staging app will start building, and it will bypass the REVIEW APPS stage.\n\nYou can pass the staging app to the tester or QA team and if everything goes well, one can finally push it to PRODUCTION.\n\nWhen you hit the Promote to production button on Heroku, your production app will be live.\n\nThe following figure shows the workflow of the automated Heroku pipeline when integrated with the GitHub repository on Heroku.\n\nFigure 10.3: Heroku-automated pipeline\n\nDeploying ML Models on Heroku  225\n\nDeployment with Container Registry Heroku Container Registry allows you to deploy your Docker images to Heroku.\n\nMake sure you have a working Docker installation and are logged in to Heroku using the following command:\n\nheroku login\n\nYou’ll be prompted to enter any key to go to your web browser to complete the login process.\n\nA new browser window will open up and will ask you to log in. You may add the -i or --interactive option to stay in the terminal and pass the login details when asked.\n\nMake sure you are in the project directory where Dockerfile is located.\n\nNow, log in to Heroku Container Registry:\n\nheroku container:login\n\nBuild the image and push it to the container registry on Heroku using the following syntax:\n\nheroku container:push <process-type> --app [app name]\n\nIn this case, it is a web process, and the --app parameter holds Heroku’s app name.\n\nFinally, release the image to your app:\n\nheroku container:release <process-type> --app [app name]\n\nRun the following command to see the app running in the browser:\n\nheroku open --app [app name]\n\nNote: Pipeline promotions are not supported in the container registry method.\n\nGitHub Actions GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that helps you automate your development workflows at the same place as your code (that is, GitHub repository) and collaborate on pull requests and issues. You can write individual configurable tasks known as actions and combine them to create a custom workflow. Workflows are custom automated processes within GitHub.\n\n226  Machine Learning in Production\n\nThe following figure shows the workflow created for the current use case:\n\nFigure 10.4: GitHub workflow\n\nUnlike Jenkins, GitHub Actions comes with the runners (GitHub-owned servers available for common OSs like Windows, Linux, and macOS) to build, test and deploy the work. GitHub supports a plethora of languages, such as Node.js, Python, Java, Ruby, PHP, Go, Rust, and .NET.\n\nGitHub Actions automation is managed using workflows. Workflows are nothing but the yml or YAML files you placed in the .github/workflows directory in the same project repository.\n\nGitHub Actions are free to use, but some limits are set for them.\n\nYou can read more about GitHub Actions at https://docs.github.com/en/actions.\n\nConfiguration GitHub Actions need a minimal GitHub configuration. Firstly, go to the GitHub repository where the codebase and required files are available. Next, by default, you will be redirected to the Code tab; switch to the Settings tab, and from the left panel, choose the Secrets option under the Security section. After that, select Actions under the Secrets menu. Finally, use the New repository secret button to add the following secrets:\n\nHEROKU_API_KEY: You can get the Heroku API key from the following path on the Heroku platform.\n\nHeroku Login | Account settings | Account | API Key | Reveal/Regenerate API Key\n\nHEROKU_APP_NAME: This is the Heroku app name. Here, provide the app name that you have created for staging, that is, docker-ml-cicd.\n\nHEROKU_PROD_APP_NAME: This is another app that is to be created for production. In this case, it is docker-ml-cicd-prod.\n\nThe following figure shows the repository secrets. You can either update or remove the secrets, but cannot see them.\n\nDeploying ML Models on Heroku  227\n\nFigure 10.5: GitHub – Repository secrets\n\nCI/CD pipeline using GitHub Actions and Heroku Let’s consider the scenario of loan prediction, where you need to predict whether a customer’s loan will be approved. The focus will not be on hyperparameter tuning and model optimization. However, you can optimize a model to improve its overall performance.\n\nIn this chapter, the Machine learning package that was developed earlier (refer to Chapter 4: Packaging ML Models) will be used.\n\nFirstly, create a package of ML code and build a web app using FastAPI. After that, create the test cases and dependencies file, along with the Dockerfile and docker- compose.yml file. Finally, create the GitHub workflow files (.yml) for GitHub Actions.\n\nYou can access the code repository at the following link:\n\nhttps://github.com/suhas-ds/heroku-docker-cicd\n\nThe following directory structure shows the CI/CD pipeline files: .\n\n├── .github/\n\n│ └── workflows/\n\n│ ├── production.yml\n\n│ └── workflow.yml\n\n├── src/\n\n│ ├── prediction_model/\n\n│ │ ├── config/\n\n│ │ │ ├── __init__.py\n\n│ │ │ └── config.py\n\n228  Machine Learning in Production\n\n│ │ ├── datasets/\n\n│ │ │ ├── __init__.py\n\n│ │ │ ├── test.csv\n\n│ │ │ └── train.csv\n\n│ │ ├── processing/\n\n│ │ │ ├── __init__.py\n\n│ │ │ ├── data_management.py\n\n│ │ │ └── preprocessors.py\n\n│ │ ├── trained_models/\n\n│ │ │ ├── __init__.py\n\n│ │ │ └── classification_v1.pkl\n\n│ │ ├── VERSION\n\n│ │ ├── __init__.py\n\n│ │ ├── pipeline.py\n\n│ │ ├── predict.py\n\n│ │ └── train_pipeline.py\n\n│ ├── tests/\n\n│ │ ├── pytest.ini\n\n│ │ └── test_predict.py\n\n│ ├── MANIFEST.in\n\n│ ├── README.md\n\n│ ├── requirements.txt\n\n│ ├── setup.py\n\n│ └── tox.ini\n\n├── .gitignore\n\n├── Dockerfile\n\n├── README.md\n\n├── docker-compose.yml\n\n├── main.py\n\n├── pytest.ini\n\n├── requirements.txt\n\n├── runtime.txt\n\n├── start.sh\n\n├── test.py\n\n└── tox.ini",
      "page_number": 242
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 254-261)",
      "start_page": 254,
      "end_page": 261,
      "detection_method": "topic_boundary",
      "content": "Deploying ML Models on Heroku  229\n\nIn the src directory, files from Chapter 4: Packaging ML Models are being used.\n\n.gitignore\n\nThis file contains files that Git should ignore.\n\n1. venv/\n\n2. __pycache__/\n\n3. .pytest_cache\n\n4. .tox\n\nmain.py\n\nFirst, load the dependencies and modules from prediction_model:\n\n1. # Importing Dependencies\n\n2. from fastapi import FastAPI\n\n3. from pydantic import BaseModel\n\n4. import uvicorn\n\n5. import pickle\n\n6. import os\n\n7. import numpy as np\n\n8. import pandas as pd\n\n9. from fastapi.middleware.cors import CORSMiddleware\n\n10. from prediction_model.predict import make_prediction\n\n11. import pandas as pd\n\nCreate a FastAPI instance and assign it to the app so that the app will be a point of interaction while creating the API.\n\n1. app = FastAPI(\n\n2. title=\"Loan Prediction Model API\",\n\n3. description=\"A simple API that uses ML model to predict the Loan application status\",\n\n4. version=\"0.1\",\n\n5. )\n\nCORS (Cross-Origin Resource Sharing) refers to situations when the front end running on a browser has JavaScript code that communicates with the back end,\n\n230  Machine Learning in Production\n\nand the back end is of a different origin from the front end. However, it depends on your application and requirement whether to use it.\n\n1. origins = [\n\n2. \"*\"\n\n3. ]\n\n4.\n\n5. app.add_middleware(\n\n6. CORSMiddleware,\n\n7. allow_origins=origins,\n\n8. allow_credentials=True,\n\n9. allow_methods=[\"*\"],\n\n10. allow_headers=[\"*\"],\n\n11. )\n\nDefine the class LoanPred, which defines the data type expected from the client.\n\nThe LoanPred class for the data model is inherited from BaseModel. Then, add a root view with a function, which returns 'message': 'Loan Prediction App' for the home page.\n\n1. class LoanPred(BaseModel):\n\n2. Gender: str\n\n3. Married: str\n\n4. Dependents: str\n\n5. Education: str\n\n6. Self_Employed: str\n\n7. ApplicantIncome: float\n\n8. CoapplicantIncome: float\n\n9. LoanAmount: float\n\n10. Loan_Amount_Term: float\n\n11. Credit_History: float\n\n12. Property_Area: str\n\n13.\n\n14. @app.get('/')\n\nDeploying ML Models on Heroku  231\n\n15. def index():\n\n16. return {'message': 'Loan Prediction App'}\n\n17.\n\n18. @app.get('/health')\n\n19. def healthcheck():\n\n20. return {'status':'ok'}\n\nHere, create /predict_status as an endpoint, also known as a route. Then, add predict_loan_status() with a parameter of the type data model, that is, LoanPred.\n\n1. #Defining the function which will make the prediction using the data which the user inputs\n\n2. @app.post('/predict_status')\n\n3. def predict_loan_status(loan_details: LoanPred):\n\n4. data = loan_details.dict()\n\n5. Gender = data['Gender']\n\n6. Married = data['Married']\n\n7. Dependents = data['Dependents']\n\n8. Education = data['Education']\n\n9. Self_Employed = data['Self_Employed']\n\n10. ApplicantIncome = data['ApplicantIncome']\n\n11. CoapplicantIncome = data['CoapplicantIncome']\n\n12. LoanAmount = data['LoanAmount']\n\n13. Loan_Amount_Term = data['Loan_Amount_Term']\n\n14. Credit_History = data['Credit_History']\n\n15. Property_Area = data['Property_Area']\n\n16.\n\n17. # Making predictions\n\n18. input_data = [Gender, Married, Dependents, Education,\n\n19. Self_Employed, ApplicantIncome, CoapplicantIncome,\n\n20. LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]\n\n21. cols = ['Gender','Married','Dependents',\n\n22. 'Education','Self_Employed','ApplicantIncome',\n\n232  Machine Learning in Production\n\n23. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n24. 'Credit_History','Property_Area']\n\n25. data_dict = dict(zip(cols,input_data))\n\n26. prediction = make_prediction([data_dict])['prediction'][0]\n\n27.\n\n28. if prediction == 'Y':\n\n29. pred = 'Approved'\n\n30. else:\n\n31. pred = 'Rejected'\n\n32.\n\n33. return {'status':pred}\n\nThe following function will create the UI for user input. Here, create /predict as an endpoint, also known as a route, and declare input data types expected from users.\n\n1. @app.post('/predict')\n\n2. def get_loan_details(Gender: str, Married: str, Dependents: str,\n\n3. Education: str, Self_Employed: str, ApplicantIncome: float,\n\n4. CoapplicantIncome: float, LoanAmount: float, Loan_Amount_Term: float,\n\n5. Credit_History: float, Property_Area: str):\n\n6.\n\n7. input_data = [Gender, Married, Dependents, Education,\n\n8. Self_Employed, ApplicantIncome, CoapplicantIncome,\n\n9. LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]\n\n10. cols = ['Gender','Married','Dependents',\n\n11. 'Education','Self_Employed','ApplicantIncome',\n\n12. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n13. 'Credit_History','Property_Area']\n\n14.\n\n15. data_dict = dict(zip(cols,input_data))\n\n16. prediction = make_prediction([data_dict])['prediction'][0]\n\n17. if prediction == 'Y':\n\n18. pred = 'Approved'\n\nDeploying ML Models on Heroku  233\n\n19. else:\n\n20. pred = 'Rejected'\n\n21.\n\n22. return {'status':pred}\n\n23.\n\n24.\n\n25. if __name__ == '__main__':\n\n26. pass\n\nThe file for FastAPI is completed.\n\nrequirements.txt\n\nNow, create the requirements.txt file, as follows. In this file, you can define the model requirements, test requirements, and FastAPI requirements separately for better understanding and ease of management.\n\n1. # Model building requirements\n\n2. joblib==0.16.0\n\n3. numpy==1.19.0\n\n4. pandas==1.0.5\n\n5. scikit-learn==0.23.1\n\n6. scipy==1.5.1\n\n7. sklearn==0.0\n\n8.\n\n9. # testing requirements\n\n10. pytest<5.0.0,>=4.6.6\n\n11. requests\n\n12.\n\n13. # packaging\n\n14. setuptools==40.6.3\n\n15. wheel==0.32.3\n\n16.\n\n17. # FastAPI app requirements\n\n18. fastapi>=0.68.0,<0.69.0\n\n234  Machine Learning in Production\n\n19. pydantic>=1.8.0,<2.0.0\n\n20. uvicorn>=0.15.0,<0.16.0\n\n21. gunicorn>=20.1.0\n\nDockerfile\n\nDockerfile contains a list of commands or instructions to be executed while building the Docker image. Docker uses this file to build the Docker image.\n\n1. FROM python:3.7-slim-buster\n\n2.\n\n3. COPY ./start.sh /start.sh\n\n4.\n\n5. RUN chmod +x /start.sh\n\n6.\n\n7. ENV PYTHONPATH \"${PYTHONPATH}:app/src/\"\n\n8.\n\n9. COPY . /app\n\n10.\n\n11. RUN chmod +x /app\n\n12.\n\n13. RUN pip install --no-cache-dir --upgrade -r app/requirements.txt\n\n14.\n\n15. CMD [\"./start.sh\"]\n\ndocker-compose.yml\n\nThe web service builds from the Dockerfile in the current directory and mounts the app directory on the host to /app inside the container.\n\n1. version: \"3.9\" # optional since v1.27.0\n\n2. services:\n\n3. web:\n\n4. build: .\n\n5. volumes:\n\n6. - ./app:/app\n\npytest.ini\n\nDeploying ML Models on Heroku  235\n\nPytest allows you to use a global configuration file, that is, pytest.ini, where you can keep the settings and additional arguments that are to be passed whenever you run the command. Add -p no:warnings to the addopts option, which will suppress the warnings. This section will execute the parameters when pytest runs.\n\n1. [pytest]\n\n2. addopts = -p no:warnings\n\nruntime.txt\n\nDeclare the Python version to be used in this file.\n\n1. python-3.7\n\nstart.sh\n\nThis is a shell script that contains a series of commands to be executed by the bash shell. The first line of this file tells which interpreter should be used to execute this script.\n\nIt will install the prediction_model package from src/ placed in the app/ folder and run the FastAPI app in the next command. Here, it will get the PORT from the Heroku environment, so you do not need to declare it explicitly. Heroku will run the app on any available port. The main is the file name located in the app directory, followed by :app, which is a FastAPI object to be called.\n\n1. #/bin/sh\n\n2. pip install app/src/\n\n3. uvicorn app.main:app --host 0.0.0.0 --port $PORT\n\nMake sure you remove the --reload option if you are using it. The --reload option consumes much more resources; moreover, it is unstable. You can use it during development but should not use it in production.\n\ntest.py\n\nThis file will enable you to test the FastAPI app using TestClient. FastAPI provides the same starlette.testclient as fastapi.testclient; however, it comes directly from Starlette. With this, you can check the app’s routes without running the app explicitly.\n\nThis file will test the root path of the app and sample prediction by passing the data.\n\n1. # Importing dependencies\n\n2. from main import app\n\n3. from fastapi.testclient import TestClient\n\n236  Machine Learning in Production\n\n4. import pytest\n\n5. import requests\n\n6. import json\n\nCreate a TestClient() by passing a FastAPI application to it as an argument:\n\n1. client = TestClient(app)\n\nDefine the functions with a name starting with test_ as per standard pytest conventions:\n\n1. def test_read_main():\n\n2. response = client.get(\"/\")\n\n3. assert response.status_code == 200\n\n4. assert response.json() == {'message': 'Loan Prediction App'}\n\nWrite simple assert statements to check the output:\n\n1. def test_pred():\n\n2. data = {\n\n3. \"Gender\":\"Male\",\n\n4. \"Married\":\"Yes\",\n\n5. \"Dependents\":\"0\",\n\n6. \"Education\":\"Graduate\",\n\n7. \"Self_Employed\":\"No\",\n\n8. \"ApplicantIncome\":5720,\n\n9. \"CoapplicantIncome\":0,\n\n10. \"LoanAmount\":110,\n\n11. \"Loan_Amount_Term\":360,\n\n12. \"Credit_History\":1,\n\n13. \"Property_Area\":\"Urban\"\n\n14. }\n\n15.\n\n16. response = client.post(\"/predict_status\", json=data)\n\n17. assert response.json()[\"status\"] != ''\n\n18. assert response.json() == {\"status\": \"Approved\"}",
      "page_number": 254
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 262-273)",
      "start_page": 262,
      "end_page": 273,
      "detection_method": "topic_boundary",
      "content": "Deploying ML Models on Heroku  237\n\ntox.ini\n\nThis is the tox configuration file. Tox automates and standardizes the testing in Python. It is a virtualenv management and test command-line tool to check if your package is compatible with different Python versions. Tox will first create a virtual environment based on the configuration provided, install dependencies and finally execute the commands provided in the configuration.\n\ntox-gh-actions is a plugin that enables tox to run on GitHub Actions. Hence, you need to install tox-gh-actions in the GitHub Actions workflow before running the tox command.\n\nThis file aims to check the functioning of the package against Python-3.7. It will install the required dependencies and run the pytest command.\n\n1. [tox]\n\n2. envlist = py37\n\n3. skipsdist=True\n\n4.\n\n5. [gh-actions]\n\n6. python =\n\n7. 3.7: py37\n\n8.\n\n9. [testenv]\n\n10. install_command = pip install {opts} {packages}\n\n11. deps =\n\n12. -r requirements.txt\n\n13.\n\n14. setenv =\n\n15. PYTHONPATH=src/\n\n16.\n\n17. commands=\n\n18. pip install requests\n\n19. pytest -v test.py\n\n20. pytest -v src/tests/\n\nworkflow.yml\n\n238  Machine Learning in Production\n\nGitHub Actions will look for this file for executing the workflow. This workflow is used for staging or pre-production app deployment on Heroku. It gets triggered on a push event, which means any updates pushed to the project’s master branch of the GitHub repository will trigger this workflow to run. Under jobs, you should declare the OS and Python version on which it should run. Then, it will run the pytest using tox. After passing all tests, it will start the deployment on Heroku. For this, it will use the secrets defined in the settings of the GitHub repository.\n\n1. name: Docker-ml-cicd-Heroku\n\n2.\n\n3. on:\n\n4. push:\n\n5. branches:\n\n6. - master\n\n7.\n\n8. jobs:\n\n9. build:\n\n10. runs-on: ubuntu-18.04\n\n11. strategy:\n\n12. matrix:\n\n13. python-version: ['3.7']\n\n14.\n\n15. steps:\n\n16. - uses: actions/checkout@v2\n\n17. - name: Permissions\n\n18. run: chmod +x start.sh\n\n19. - name: Set up Python ${{ matrix.python-version }}\n\n20. uses: actions/setup-python@v2\n\n21. with:\n\n22. python-version: ${{ matrix.python-version }}\n\n23. - name: Install dependencies\n\n24. run: |\n\n25. python -m pip install --upgrade pip\n\nDeploying ML Models on Heroku  239\n\n26. python -m pip install tox tox-gh-actions\n\n27. - name: Test with tox\n\n28. run: tox\n\n29. - name: Log in to Heroku Container registry\n\n30. env:\n\n31. HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n\n32. run: heroku container:login\n\n33. - name: Build and push\n\n34. env:\n\n35. HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n\n36. run: heroku container:push web --app ${{ secrets.HEROKU_APP_ NAME }}\n\n37. - name: Release\n\n38. env:\n\n39. HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n\n40. run: heroku container:release web --app ${{ secrets.HEROKU_ APP_NAME }}\n\nproduction.yml\n\nOn successful completion of workflow.yml for staging, production.yml workflow will run. This workflow will simply deploy a production app on Heroku using the latest updates, assuming everything is working fine.\n\n1. name: Production\n\n2.\n\n3. on:\n\n4. workflow_run:\n\n5. workflows: [Docker-ml-cicd-Heroku]\n\n6. types:\n\n7. - completed\n\n8. jobs:\n\n9. build:\n\n10. runs-on: ubuntu-18.04\n\n240  Machine Learning in Production\n\n11. steps:\n\n12. - uses: actions/checkout@v2\n\n13. - name: Log in to Heroku Container registry\n\n14. env:\n\n15. HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n\n16. run: heroku container:login\n\n17. - name: Build and push\n\n18. env:\n\n19. HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n\n20. run: heroku container:push web --app ${{ secrets.HEROKU_PROD_ APP_NAME }}\n\n21. - name: Release\n\n22. env:\n\n23. HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n\n24. run: heroku container:release web --app ${{ secrets.HEROKU_ PROD_APP_NAME }}\n\nOnce all files and codebase are ready, run the app on the local machine. If the app is running on a local machine, then deploy the app to the Heroku container registry. For this, you can refer to the Deployment with Container Registry section discussed earlier in this chapter. If everything goes well, the app should run on the Heroku platform post deployment.\n\nNext, create a repository on GitHub for the current scenario (if not created already) and push code files to it. Go to the Actions tab on GitHub; you will see GitHub has already started executing the workflow.yml file for staging or pre-deployment app.\n\nDeploying ML Models on Heroku  241\n\nAs you can see in the following figure, updates have been pushed with the comment as Staging.\n\nFigure 10.6: GitHub workflow runs\n\nClick on Staging; it will take you to the summary of this workflow. You will see the status of the jobs you have defined in the workflow. Other execution details are mentioned, such as the time taken to complete this workflow, the username means who pushed the updates to the master branch, and the name of the workflow file.\n\n242  Machine Learning in Production\n\nIn the following figure, you can see the summary of the staging stage:\n\nFigure 10.7: GitHub workflow - staging\n\nThe following figure shows the steps executed in the workflow and their output. To see the output of the step, click on the > icon on the left side of the step.\n\nDeploying ML Models on Heroku  243\n\nFigure 10.8: GitHub workflow – staging steps\n\nThe following figure shows the output of the tox command, which executed pytest commands using the py37 environment.\n\nFigure 10.9: GitHub workflow – tox\n\n244  Machine Learning in Production\n\nThe following figure shows the app running on the Heroku platform on successful completion of workflow.yml workflow.\n\nFigure 10.10: Heroku app – docker-ml-cicd\n\nClick on Production; it will take you to the summary of this workflow. You will see the status of the jobs you have defined in the workflow. Other execution details are mentioned, such as the time taken to complete the workflow, the username that pushed the updates to the master branch, the name of the workflow file, and build no., in this case, it is #2.\n\nDeploying ML Models on Heroku  245\n\nIn the following figure, you can see the summary and status of the production stage:\n\nFigure 10.11: GitHub workflow – production steps\n\nThe following figure shows the steps executed in the workflow and their output. To see the output of the step, click on the > icon on the left side of the step.\n\nFigure 10.12: GitHub workflow – production steps\n\n246  Machine Learning in Production\n\nThe following figure shows the app running on the Heroku platform on successful completion of the production.yml workflow.\n\nFigure 10.13: Heroku app – docker-ml-cicd-prod\n\nThus, you have now learned to create a simple CI/CD pipeline using GitHub to deploy ML apps on the Heroku platform. Furthermore, you can modify it as per business and application requirements.\n\nConclusion In this chapter, you have learned about the ML app deployment on the Heroku platform (PaaS), built an automated CI/CD pipeline using GitHub Actions, and deployed staging or pre-deployment and production app on the Heroku platform using GitHub CI/CD pipeline. Finally, you integrated tox with GitHub Actions to run the test as a part of the CI/CD pipeline before deploying the staging app to Heroku.\n\nIn the next chapter, you will explore the Azure platform for deploying ML apps.\n\nPoints to remember\n\nGitHub Actions come with runners (Github-owned servers for major OSs such as Windows, Linux, and macOS) to build, test and deploy the workflow.\n\nDeploying ML Models on Heroku  247\n\nThe Heroku app must enable both Heroku Pipelines and GitHub integration to use review apps.\n\nYou can either update or remove the GitHub secrets but cannot see them. •\t GitHub Actions is free to use; however, some limits are set for them.\n\nMultiple choice questions\n\n1. _________ is a plugin that enables tox running on GitHub Actions.\n\na) JUnit\n\nb) tox-gh-actions\n\nc) GitHub’\n\nd) tox-github\n\n2. GitHub Actions automation is managed using _________.\n\na)\n\n.git\n\nb) tox\n\nc) Workflow\n\nd) All the above\n\nAnswers 1. b\n\n2. c\n\nQuestions\n\n1. What is the file format of the GitHub Actions workflow?\n\n2. What are the three methods to deploy the app on the Heroku platform?\n\n3. Explain the working stages of the Heroku pipeline.\n\n248  Machine Learning in Production\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "page_number": 262
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 274-283)",
      "start_page": 274,
      "end_page": 283,
      "detection_method": "topic_boundary",
      "content": "Deploying ML Models on Microsoft Azure  249\n\nChapter 11 Deploying ML Models on Microsoft Azure\n\nIntroduction Microsoft Azure is a popular cloud platform among developers, as it comes with a wide variety of services that help developers and organizations to deliver quality solutions to customers with less effort and less time in a secure environment.\n\nIn this chapter, you will be acquainted with MLaaS, that is, Machine Learning as a Service, offered by Microsoft Azure. This chapter is mainly divided into two parts. In the first part, GitHub Actions and Azure web app containers will be used in the CI/CD pipeline to deploy the ML app on the Azure cloud. In the second part, Azure DevOps and Azure Machine Learning (AML) service will be used to deploy ML apps on the Azure cloud.\n\nStructure This chapter discusses the following topics:\n\nCreate an Azure account and install Azure CLI •\t Run the ML app using the Docker container locally •\t Deploy the app to Azure web service using Azure container\n\n250  Machine Learning in Production\n\nBuild a CI/CD pipeline using GitHub Actions •\t Azure Machine Learning (AML) service •\t Build a CI/CD pipeline using Azure Machine Learning (AML) service and Azure DevOps\n\nObjectives After studying this chapter, you should be able to deploy ML models on Platform as a Service (PaaS) and ML as a Service (MLaaS). You should also be able to integrate the GitHub repository to Azure for automated deployment of the app. You should know how to create a web app and container for it. Additionally, you will be familiar with how to build and run CI/CD pipelines using GitHub Actions and Azure and execute multiple test cases using pytest and tox on GitHub Actions. Further on in the chapter, you will learn how to create YAML files for GitHub Actions to run the workflow. By the end of the chapter, you should also be able to build and run CI/CD pipelines using Azure DevOps.\n\nAzure Microsoft Azure (also known as Azure) is a cloud computing service offered by Microsoft. Azure provides different forms of cloud computing options, such as Software as a Service (SaaS), Platform as a Service (PaaS), ML as a Service (MLaaS), and Infrastructure as a Service (IaaS), for deploying applications and services on Azure.\n\nMicrosoft first introduced its cloud computing services as Windows Azure in 2008, but it was commercially launched in 2010. Later, in 2014, they expanded their services and re-launched it as Microsoft Azure.\n\nAzure is a ready-to-go, resourceful, flexible, and fast yet economical cloud platform. It comes with 200+ products and cloud services; however, running Virtual Machines (VM) or containers is popular among developers. It is compatible with open-source technologies like Docker, Jenkins, and Kubernetes. Azure has a built-in Continuous Integration and Continuous Deployment pipeline.\n\nAzure enables integration with GitHub to make it easy to deploy code available on the GitHub repository to apps running on Azure. When GitHub integration is configured for an Azure app, Azure can automatically build and release (if the stage is completed successfully) and push it to Azure.\n\nAzure App Service can be scaled using Azure Kubernetes Service (AKS). You can choose different tiers that come with a set of computational power and set the auto scale limit. When required, it will make replicas of the service to serve the requests seamlessly.\n\nDeploying ML Models on Microsoft Azure  251\n\nThere are numerous ways to deploy the app on the Azure platform, but you will explore two methods in this chapter:\n\nDeployment using GitHub Actions •\t Deployment using Azure DevOps and Azure ML\n\nAzure primarily uses a pay-as-you-go pricing plan, which allows you to pay only for the services you have used. If any application uses multiple services, then each service will be billed based on the plan/tier obtained for it. Microsoft offers a discounted rate if the user or organization is looking for a long-term commitment.\n\nAzure is a paid service, but it allows new users to explore its functionality and services for free for a limited duration using free credits. After that, you would have to activate a pay-as-you-go pricing plan.\n\nSet up an Azure account You need to set up an Azure account. This is a one-time activity, and once it is done, it is ready for use. Go to the Azure platform https://azure.microsoft.com/.\n\nCreate an Azure account (if you don’t have one), and log in.\n\nNote: You can explore Azure for free; however, you need to provide credit/debit card details. Azure will send a notification if any payment is to be made. Also, you will get access to popular services for free for 12 months, with an additional $200 credit for 30 days.\n\nAzure DevOps will be used in this chapter. For this, you need to log in to Azure DevOps.\n\nhttps://azure.microsoft.com/en-us/services/devops/\n\nTo connect and communicate with Azure, the Azure command-line tool is required, that is, azure-cli and azureml-sdk.\n\nInstall Azure CLI with your local terminal:\n\npip install azure-cli==2.37.0\n\npip install --upgrade azureml-sdk[cli]\n\nYou can create a resource group. It is a container that holds related resources for Azure projects. You can allocate the resources to this resource group as per requirement.\n\nDeployment using GitHub Actions In this part, an ML app will be deployed in Azure web service using Docker. GitHub Actions will be used for building, testing, and deploying ML apps to Azure. First\n\n252  Machine Learning in Production\n\noff, GitHub Actions will build and push a Docker image to the Azure Container Registry (ACR) and pull it into the Azure App Service.\n\nFirstly, prepare the required codebase for this implementation. Next, create an ACR, and then build and push the Docker image to the ACR. In the next step, create a web app in the Azure App Service, which will pull container images from the ACR to deploy a web-based ML app.\n\nThe following figure shows the workflow of deployment on Azure App Service (web app) using GitHub Actions:\n\nFigure 11.1: GitHub Actions and web app workflow\n\nYou can access the code at the following GitHub repository:\n\nhttps://github.com/suhas-ds/mlapp-cd\n\nThe following directory structure shows the files that will be used for the CI/CD pipeline: .\n\n├─ .github\n\n│ └─ workflows\n\n│ └─ prod.workflow.yml\n\n├─ src\n\n│ ├─ prediction_model\n\n│ │ ├─ config\n\n│ │ │ ├─ __init__.py\n\n│ │ │ └─ config.py\n\n│ │ ├─ datasets\n\n│ │ │ ├─ __init__.py\n\nDeploying ML Models on Microsoft Azure  253\n\n│ │ │ ├─ test.csv\n\n│ │ │ └─ train.csv\n\n│ │ ├─ processing\n\n│ │ │ ├─ __init__.py\n\n│ │ │ ├─ data_management.py\n\n│ │ │ └─ preprocessors.py\n\n│ │ ├─ trained_models\n\n│ │ │ ├─ __init__.py\n\n│ │ │ └─ classification_v1.pkl\n\n│ │ ├─ VERSION\n\n│ │ ├─ __init__.py\n\n│ │ ├─ pipeline.py\n\n│ │ ├─ predict.py\n\n│ │ └─ train_pipeline.py\n\n│ ├─ tests\n\n│ │ ├─ pytest.ini\n\n│ │ └─ test_predict.py\n\n│ ├─ MANIFEST.in\n\n│ ├─ README.md\n\n│ ├─ requirements.txt\n\n│ ├─ setup.py\n\n│ └─ tox.ini\n\n├─ Dockerfile\n\n├─ docker-compose.yml\n\n├─ main.py\n\n├─ pytest.ini\n\n├─ requirements.txt\n\n├─ runtime.txt\n\n├─ start.sh\n\n├─ test.py\n\n└─ tox.ini\n\nLet’s discuss the code and the concepts from the preceding files. In the src directory, files from Chapter 4: Packaging ML Models will be used.\n\n.gitignore\n\nThis file contains the names of files and directories that Git should ignore.\n\n254  Machine Learning in Production\n\n1. venv/\n\n2. __pycache__/\n\n3. .pytest_cache\n\n4. .tox\n\nmain.py\n\nThis file will load the pickle object of the model and run a FastAPI app. First, load the dependencies and modules from prediction_model:\n\n1. # Importing Dependencies\n\n2. from fastapi import FastAPI\n\n3. from pydantic import BaseModel\n\n4. import uvicorn\n\n5. import pickle\n\n6. import os\n\n7. import numpy as np\n\n8. import pandas as pd\n\n9. from fastapi.middleware.cors import CORSMiddleware\n\n10. from prediction_model.predict import make_prediction\n\n11. import pandas as pd\n\nCreate a FastAPI instance and assign it to the app so that the app becomes a point of interaction while creating the API.\n\n1. app = FastAPI(\n\n2. title=\"Loan Prediction Model API\",\n\n3. description=\"A simple API that uses ML model to predict the Loan application status\",\n\n4. version=\"0.1\",\n\n5. )\n\nCORS (Cross-Origin Resource Sharing) refers to the situations when a front end running in a browser has JavaScript code that communicates with a back end, and the back end is of a different origin than the front end. However, it depends on your application and requirement whether to use it.\n\n1. origins = [\n\nDeploying ML Models on Microsoft Azure  255\n\n2. \"*\"\n\n3. ]\n\n4.\n\n5. app.add_middleware(\n\n6. CORSMiddleware,\n\n7. allow_origins=origins,\n\n8. allow_credentials=True,\n\n9. allow_methods=[\"*\"],\n\n10. allow_headers=[\"*\"],\n\n11. )\n\nDefine the class LoanPred, which defines the data type expected from the client.\n\nThe LoanPred class for the data model is inherited from BaseModel. Then, add a root view with a function, which returns 'message': 'Loan Prediction App' for the home page.\n\n1. class LoanPred(BaseModel):\n\n2. Gender: str\n\n3. Married: str\n\n4. Dependents: str\n\n5. Education: str\n\n6. Self_Employed: str\n\n7. ApplicantIncome: float\n\n8. CoapplicantIncome: float\n\n9. LoanAmount: float\n\n10. Loan_Amount_Term: float\n\n11. Credit_History: float\n\n12. Property_Area: str\n\n13.\n\n14. @app.get('/')\n\n15. def index():\n\n16. return {'message': 'Loan Prediction App'}\n\n17.\n\n256  Machine Learning in Production\n\n18. @app.get('/health')\n\n19. def healthcheck():\n\n20. return {'status':'ok'}\n\nHere, create /predict_status as an endpoint, also known as the route. Then, add predict_loan_status() with a parameter of the type data model, that is, LoanPred.\n\n1. #Defining the function which will make the prediction using the data which the user inputs\n\n2. @app.post('/predict_status')\n\n3. def predict_loan_status(loan_details: LoanPred):\n\n4. data = loan_details.dict()\n\n5. Gender = data['Gender']\n\n6. Married = data['Married']\n\n7. Dependents = data['Dependents']\n\n8. Education = data['Education']\n\n9. Self_Employed = data['Self_Employed']\n\n10. ApplicantIncome = data['ApplicantIncome']\n\n11. CoapplicantIncome = data['CoapplicantIncome']\n\n12. LoanAmount = data['LoanAmount']\n\n13. Loan_Amount_Term = data['Loan_Amount_Term']\n\n14. Credit_History = data['Credit_History']\n\n15. Property_Area = data['Property_Area']\n\n16.\n\n17. # Making predictions\n\n18. input_data = [Gender, Married, Dependents, Education,\n\n19. Self_Employed, ApplicantIncome, CoapplicantIncome,\n\n20. LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]\n\n21. cols = ['Gender','Married','Dependents',\n\n22. 'Education','Self_Employed','ApplicantIncome',\n\n23. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n24. 'Credit_History','Property_Area']\n\nDeploying ML Models on Microsoft Azure  257\n\n25. data_dict = dict(zip(cols,input_data))\n\n26. prediction = make_prediction([data_dict])['prediction'][0]\n\n27.\n\n28. if prediction == 'Y':\n\n29. pred = 'Approved'\n\n30. else:\n\n31. pred = 'Rejected'\n\n32.\n\n33. return {'status':pred}\n\nThe following function will create the UI for user input. Here, create /predict as an endpoint, also known as a route, and declare the input data types expected from users.\n\n1. @app.post('/predict')\n\n2. def get_loan_details(Gender: str, Married: str, Dependents: str,\n\n3. Education: str, Self_Employed: str, ApplicantIncome: float,\n\n4. CoapplicantIncome: float, LoanAmount: float, Loan_Amount_Term: float,\n\n5. Credit_History: float, Property_Area: str):\n\n6.\n\n7. input_data = [Gender, Married, Dependents, Education,\n\n8. Self_Employed, ApplicantIncome, CoapplicantIncome,\n\n9. LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]\n\n10. cols = ['Gender','Married','Dependents',\n\n11. 'Education','Self_Employed','ApplicantIncome',\n\n12. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n13. 'Credit_History','Property_Area']\n\n14.\n\n15. data_dict = dict(zip(cols,input_data))\n\n16. prediction = make_prediction([data_dict])['prediction'][0]\n\n17. if prediction == 'Y':\n\n18. pred = 'Approved'\n\n258  Machine Learning in Production\n\n19. else:\n\n20. pred = 'Rejected'\n\n21.\n\n22. return {'status':pred}\n\n23.\n\n24. if __name__ == '__main__':\n\n25. uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=port, reload=False)\n\nThe FastAPI file is completed.\n\nrequirements.txt\n\nNow, let’s create a requirements.txt file, as follows. In this file, model requirements, test requirements, and FastAPI requirements are defined separately for better understanding and ease of management.\n\n1. # Model building requirements\n\n2. joblib==0.16.0\n\n3. numpy==1.19.0\n\n4. pandas==1.0.5\n\n5. scikit-learn==0.23.1\n\n6. scipy==1.5.1\n\n7. sklearn==0.0\n\n8.\n\n9. # testing requirements\n\n10. pytest<5.0.0,>=4.6.6\n\n11. requests\n\n12.\n\n13. # packaging\n\n14. setuptools==40.6.3\n\n15. wheel==0.32.3\n\n16.\n\n17. # FastAPI app requirements\n\n18. fastapi>=0.68.0,<0.69.0",
      "page_number": 274
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 284-291)",
      "start_page": 284,
      "end_page": 291,
      "detection_method": "topic_boundary",
      "content": "Deploying ML Models on Microsoft Azure  259\n\n19. pydantic>=1.8.0,<2.0.0\n\n20. uvicorn>=0.15.0,<0.16.0\n\n21. gunicorn>=20.1.0\n\nDockerfile\n\nDockerfile contains a list of commands or instructions to be executed while building the Docker image.\n\n1. FROM python:3.7-slim-buster\n\n2.\n\n3. COPY ./start.sh /start.sh\n\n4.\n\n5. RUN chmod +x /start.sh\n\n6.\n\n7. ENV PYTHONPATH \"${PYTHONPATH}:app/src/\"\n\n8.\n\n9. COPY . /app\n\n10.\n\n11. RUN chmod +x /app\n\n12.\n\n13. # expose the port that uvicorn will run the app on\n\n14. ENV PORT=8000\n\n15. EXPOSE 8000\n\n16.\n\n17. RUN pip install --no-cache-dir --upgrade -r app/requirements.txt\n\n18.\n\n19. CMD [\"./start.sh\"]\n\ndocker-compose.yml\n\nThis builds the web service from the Dockerfile in the current directory and mounts the app directory on the host to /app inside the container.\n\n1. version: \"3.9\" # optional since v1.27.0\n\n2. services:\n\n260  Machine Learning in Production\n\n3. web:\n\n4. build: .\n\n5. volumes:\n\n6. - ./app:/app\n\npytest.ini\n\nPytest allows you to use a global configuration file, that is, pytest.ini, where you can keep settings and additional arguments that you pass whenever you run the command. You can add -p no:warnings to addopts field, which will suppress the warnings. This section will execute the parameters when pytest runs.\n\n1. [pytest]\n\n2. addopts = -p no:warnings\n\nruntime.txt\n\nThis file is optional. In this file, you can declare the Python version to be used.\n\n1. python-3.7\n\nstart.sh\n\nThis is a shell script that contains a series of commands to be executed by the bash shell. The first line of this file dictates which interpreter should be used to execute this script.\n\nIt will install the prediction_model package from src/ placed in app/ and run the FastAPI app in the next command. The main.py is the file name located in the app directory.\n\n1. #!/bin/bash\n\n2. pip install app/src/\n\n3. python app/main.py\n\nMake sure you remove the --reload option if you are using it. The --reload option consumes much more resources, and it is also unstable. You can use it during development but should not use it in production.\n\ntest.py\n\nThis test file will enable testing the FastAPI app using testclient. FastAPI provides the same starlette.testclient as fastapi.testclient; however, it comes directly from Starlette. With this, you can check the app’s routes without running them from the app explicitly.\n\nDeploying ML Models on Microsoft Azure  261\n\nThis file will test the root path of the app and model predictions by passing the data.\n\n1. # Importing dependencies\n\n2. from main import app\n\n3. from fastapi.testclient import TestClient\n\n4. import pytest\n\n5. import requests\n\n6. import json\n\nCreate a TestClient() and pass a FastAPI application to it.\n\n1. client = TestClient(app)\n\nDefine the test functions with a name starting with test_ as per standard naming conventions of pytest:\n\n1. def test_read_main():\n\n2. response = client.get(\"/\")\n\n3. assert response.status_code == 200\n\n4. assert response.json() == {'message': 'Loan Prediction App'}\n\nWrite simple assert statements in the test function to check the output.\n\n1. def test_pred():\n\n2. data = {\n\n3. \"Gender\":\"Male\",\n\n4. \"Married\":\"Yes\",\n\n5. \"Dependents\":\"0\",\n\n6. \"Education\":\"Graduate\",\n\n7. \"Self_Employed\":\"No\",\n\n8. \"ApplicantIncome\":5720,\n\n9. \"CoapplicantIncome\":0,\n\n10. \"LoanAmount\":110,\n\n11. \"Loan_Amount_Term\":360,\n\n12. \"Credit_History\":1,\n\n13. \"Property_Area\":\"Urban\"\n\n14. }\n\n262  Machine Learning in Production\n\n15.\n\n16. response = client.post(\"/predict_status\", json=data)\n\n17. assert response.json()[\"status\"] != ''\n\n18. assert response.json() == {\"status\": \"Approved\"}\n\ntox.ini\n\nThis is the tox configuration file. The tox automates and standardizes the testing in Python. It is a virtualenv management and test command-line tool to check whether your package is compatible with different Python versions. The tox will first create a virtual environment based on the configuration provided and then install dependencies. Finally, it will execute the commands provided in the configuration.\n\ntox-gh-actions is a plugin that enables tox to run on GitHub Actions. Hence, you need to install tox-gh-actions in the GitHub Actions workflow before running the tox command.\n\nThis file will check the package compatibility with Python-3.7. First, it will install the required dependencies and then run the pytest commands.\n\n1. [tox]\n\n2. envlist = py37\n\n3. skipsdist=True\n\n4.\n\n5. [gh-actions]\n\n6. python =\n\n7. 3.7: py37\n\n8.\n\n9. [testenv]\n\n10. install_command = pip install {opts} {packages}\n\n11. deps =\n\nDeploying ML Models on Microsoft Azure  263\n\n12. -r requirements.txt\n\n13.\n\n14. setenv =\n\n15. PYTHONPATH=src/\n\n16.\n\n17. commands=\n\n18. pip install requests\n\n19. pytest -v test.py\n\n20. pytest -v src/tests/\n\nOnce all files and codebase are ready, run the app on the local machine. If the app is running on a local machine, then deploy the app to the Azure Container Registry.\n\nBuild a Docker image on a local machine using the following command: sudo docker build . -t mlappcd.azurecr.io/mlapp-cd:v1\n\nThen, run the Docker image using the following command: sudo docker push mlappcd.azurecr.io/mlapp-cd:v1\n\nAt this point, you should see an ML app up and running on the local machine.\n\nInfrastructure setup First off, set up infrastructure on Azure cloud. In this section, you will learn to set up an Azure Container Registry (ACR) and create a web app container using the Azure App Service.\n\nAzure Container Registry To begin with, push the Docker image to the Azure Container Registry from the local machine, and from the next time onward, GitHub Actions will push the image. This Docker image will be pulled by the Azure App Service to run a web app container.\n\n264  Machine Learning in Production\n\nCreate an Azure Container Registry, as shown in the following figure:\n\nFigure 11.2: Azure Container Registry\n\nAfter providing the values to fields, complete the process by clicking on the Review + create button.\n\nAfter completing the preceding process, you can check the container registry to see the status. The following figure shows the status of the container registry as OK:\n\nDeploying ML Models on Microsoft Azure  265\n\nFigure 11.3: Deployment status of Azure Container Registry\n\nNext, enable the Admin user option in the Access keys. The following figure shows that admin user access is enabled under Settings:\n\nFigure 11.4: Access keys of container registry\n\n266  Machine Learning in Production\n\nAfter completing the preceding process, go back to your local terminal. Log in to Azure using Azure CLI, as follows: az login\n\nAfter executing the preceding command in the local terminal, a browser window will open where you need to log in. Then, you can close the browser window and continue in the terminal. The following figure shows the output of the preceding command:\n\nFigure 11.5: Azure login\n\nAfter that, log in to the Azure Container Registry using the following command in the terminal:\n\nsudo az acr login --name mlappcd\n\nThe following figure shows the execution of the preceding command in the terminal:\n\nFigure 11.6: Azure Container Registry login\n\nFirst, build the Docker image in the local machine: sudo docker build . -t mlappcd.azurecr.io/mlapp-cd:v1\n\nIn the following figure, you can see the image has been built and tagged successfully:\n\nFigure 11.7: Docker image built and tagged in local machine\n\nNow, push that image to Azure container registry: sudo docker push mlappcd.azurecr.io/mlapp-cd:v1\n\nAs shown in the following figure, the image has been pushed to the Azure Container Registry:",
      "page_number": 284
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 292-306)",
      "start_page": 292,
      "end_page": 306,
      "detection_method": "topic_boundary",
      "content": "Deploying ML Models on Microsoft Azure  267\n\nFigure 11.8: Docker image pushed to ACR\n\nAs shown in the following figure, verify that the image (which is pushed to the repo) is available in the Azure Container Registry. In this case, you can see that the image mlapp-cd is available in the repo.\n\nFigure 11.9: Azure Container Registry\n\nAzure App Service Now, create an Azure App Service resource. Using the Azure App Service, you can create a container-based web app. This will pull the image from the ACR. Azure App Service enables you to build web, mobile, and API apps quickly, which can be scaled as per requirement.\n\n268  Machine Learning in Production\n\nAs shown in the following figure, select the subscription and service group under the Basic tab. Then, provide the name of the instance as mlapp-cd. Choose the Docker Container radio button, as the ML app is based on the Docker container.\n\nFigure 11.10: Azure Service\n\nNext, choose the operating system Linux and select the region where the web app needs to be deployed. Finally, select the Sku and size from the plan. You can change the App Service Plan as per the application and business requirements.\n\nDeploying ML Models on Microsoft Azure  269\n\nFigure 11.11: Azure Service\n\nIn the next tab, that is, Docker configuration, make sure the image from the Azure Container Registry is selected. As shown in the following figure, select the Image Source as Azure Container Registry, and then select the registry, image, and tag from the Azure container registry options section.\n\nFigure 11.12: Azure Service-Docker configuration\n\n270  Machine Learning in Production\n\nAfter providing the values to fields, complete the process by clicking on the Review + create button. You should see the status of the web app in the newly created resource. The following figure shows the status of the Azure App Service resource:\n\nFigure 11.13: Azure Service resource-status\n\nAfter completing the preceding process, your app should be deployed. You can access the ML web app from anywhere via the internet.\n\nGitHub Actions GitHub Actions will automate the deployment on Azure. To automate the deployment of the ML app, the workflow (.yml) file is used, as follows:\n\nprod.workflow.yml\n\nGitHub Actions will look for this file for executing the workflow. This workflow is used for app deployment on Azure. It gets triggered by push events, which means any changes pushed to the repo’s master branch of the GitHub repository will trigger this workflow to run. Under jobs, declared the OS and Python versions to be used. Then, it will run the pytest using tox. After passing all tests, it will start the deployment on Azure. For this, it will use the secrets defined in the settings of the\n\nDeploying ML Models on Microsoft Azure  271\n\nGitHub repository. First off, log in to the Azure Container Registry, and then build and push the Docker image to the registry. Finally, this workflow will deploy the app to the Azure web app service and log out from Azure.\n\n1. name: Build and deploy to production\n\n2.\n\n3. on:\n\n4. push:\n\n5. branches:\n\n6. - master\n\n7.\n\n8. jobs:\n\n9. build-and-deploy:\n\n10. runs-on: ubuntu-18.04\n\n11. strategy:\n\n12. matrix:\n\n13. python-version: ['3.7']\n\n14.\n\n15. steps:\n\n16. - name: Checkout GitHub Actions\n\n17. uses: actions/checkout@master\n\n18.\n\n19. - name: Set up Python ${{ matrix.python-version }}\n\n20. uses: actions/setup-python@v2\n\n21. with:\n\n22. python-version: ${{ matrix.python-version }}\n\n23.\n\n24. - name: Install dependencies\n\n25. run: |\n\n26. python -m pip install --upgrade pip\n\n27. python -m pip install tox tox-gh-actions\n\n28.\n\n29. - name: Test with tox\n\n272  Machine Learning in Production\n\n30. run: |\n\n31. tox\n\n32.\n\n33. - name: Login via Azure CLI\n\n34. uses: azure/login@v1\n\n35. with:\n\n36. creds: ${{ secrets.AZURE_CREDENTIALS }}\n\n37.\n\n38. - name: Login to Azure Container Registry\n\n39. uses: azure/docker-login@v1\n\n40. with:\n\n41. login-server: mlappcd.azurecr.io\n\n42. username: ${{ secrets.REGISTRY_USERNAME }}\n\n43. password: ${{ secrets.REGISTRY_PASSWORD }}\n\n44.\n\n45. - name: Build and push container image to the registry\n\n46. run: |\n\n47. docker build . -t mlappcd.azurecr.io/mlapp-cd:${{ github. sha }}\n\n48. docker push mlappcd.azurecr.io/mlapp-cd:${{ github.sha }}\n\n49.\n\n50. - name: Deploy to App Service\n\n51. uses: azure/webapps-deploy@v2\n\n52. with:\n\n53. app-name: 'mlapp-cd'\n\n54. images: 'mlappcd.azurecr.io/mlapp-cd:${{ github.sha }}'\n\n55.\n\n56. - name: Azure logout\n\n57. run: |\n\n58. az logout\n\nNext, create the repository on GitHub for this (if not created already) and push the codebase into that repo.\n\nDeploying ML Models on Microsoft Azure  273\n\nService principal You need to provide a service principal in the GitHub Actions workflow for Azure authentication and app deployment. To get the credentials, execute the following command with your subscription id in the terminal.\n\naz ad sp create-for-rbac --name \"github-actions\" --role contributor --scopes /subscriptions/<Subscription ID>/resourceGroups/mlapp-cd --sdk- auth\n\nWith the preceding command, you will create an Azure Role Based Access Control (RBAC) named github-actions with a contributor role and scope.\n\nNote: You can get the subscription ID from the subscription you have used while creating a resource group.\n\nAzure RBAC is an authorization system built on Azure Resource Manager that provides access management of Azure resources.\n\nYou should get the following output:\n\n{\n\n\"clientId\": \"██████████████████████████\",\n\n\"clientSecret\": \"██████████████████████████\",\n\n\"subscriptionId\": \"██████████████████████████\",\n\n\"tenantId\": \"██████████████████████████\",\n\n\"activeDirectoryEndpointUrl\": \"https://login.microsoftonline.com\",\n\n\"resourceManagerEndpointUrl\": \"https://management.azure.com/\",\n\n\"activeDirectoryGraphResourceId\": \"https://graph.windows.net/\",\n\n\"sqlManagementEndpointUrl\":\n\n\"https://management.core.windows.\n\nnet:8443/\",\n\n\"galleryEndpointUrl\": \"https://gallery.azure.com/\",\n\n\"managementEndpointUrl\": \"https://management.core.windows.net/\"\n\n}\n\nIn the preceding response, clientId, clientSecret, subscription, and tenantId are unique for the individual account. Copy and save the response of the preceding command for future use.\n\nNow, go to GitHub repo settings and create three GitHub secrets:\n\n274  Machine Learning in Production\n\nAZURE CREDENTIALS: Entire JSON response from preceding •\t REGISTRY_USERNAME: clientId value from JSON response •\t REGISTRY_PASSWORD: clientSecret value from JSON response\n\nConfigure Azure App Service to use GitHub Actions for CD You need to configure the Azure App Service so that it can be automated using GitHub Actions. Head over to the Azure App Service | Deployment Center and link the GitHub repo master branch, as shown in the following figure.\n\nFigure 11.14: Azure Service-GitHub configuration\n\nYou need to authorize GitHub Actions to automate CD to provide the required details. You can add deployment slots, such as staging or pre-production. It is useful when you don’t want to expose the updated Azure App Service directly to production. You need to do the same configuration for the deployment center as per the previous step.\n\nDeploying ML Models on Microsoft Azure  275\n\nAfter completing the configuration in the app service, place the prod.workflow. yml workflow file in the linked GitHub repository. The path would be .github/ workflows/prod.workflow.yml. When you push any changes to the GitHub repo, it will trigger the GitHub Actions workflow. After completing all the stages, the ML app will be deployed on Azure.\n\nGo to the Actions tab in the GitHub repository, and you will see that GitHub has already started running the prod.workflow.yml workflow for app deployment on Azure.\n\nPush the changes with a Deploy text as a comment, as you can see in the following figure.\n\nFigure 11.15: GitHub Actions-workflow runs\n\n276  Machine Learning in Production\n\nClick on Deploy, and it will take you to the summary of this workflow. You will see the status of the jobs you have defined in the workflow. Other execution details are also mentioned, such as the time taken to complete this workflow, the username that pushed the updates to the master branch, and the name of the workflow file.\n\nFigure 11.16: GitHub Actions-workflow steps\n\nThe following figure shows the output of app deployment to the Azure step from GitHub Actions. You can see that the app is successfully deployed and running on Azure. An app service application URL is printed at the end.\n\nFigure 11.17: GitHub Actions-deployment status\n\nDeploying ML Models on Microsoft Azure  277\n\nHead over to the Azure App Service; you should see container logs in the Azure App Service, as shown in the following figure. This helps you debug the issue while deploying the app.\n\nFigure 11.18: App service-container logs\n\nIf everything goes well, the app should work on Azure post deployment.\n\nThe following figure shows the app running on the Azure platform on successful completion of the prod.workflow.yml workflow.\n\nFigure 11.19: App service-ML web app running\n\n278  Machine Learning in Production\n\nAzure provides a monitoring service to track the usage of the Azure app. You can select metrics from the dropdown.\n\nThe following figure shows the monitoring service of the Azure app.\n\nFigure 11.20: App service-monitoring service\n\nThus, you have learned to create a simple CI/CD pipeline using GitHub to deploy the ML app on the Azure platform. However, you can modify it as per business and application requirements.\n\nDeployment using Azure DevOps and Azure ML In this part, you will use Azure DevOps and AML to deploy an ML app on the Azure cloud. Unlike the previous part, the CI pipeline and CD pipeline are in the Azure cloud. This approach requires parallelism to be enabled. If you are using free credits, then by default, parallelism is not enabled; however, you can activate it by sending the request via email. You can use Azure Git repo or other sources, such as GitHub. This approach gives more flexibility, such as integration with other platforms, manual approval before deploying to the production, auto redeploy triggers, and such.\n\nDeploying ML Models on Microsoft Azure  279\n\nThe following figure shows the automated ML workflow:\n\nFigure 11.21: Azure DevOps pipeline for ML\n\nAzure Machine Learning (AML) service Azure Machine Learning (AML) service enable the creation of a reproducible CI/ CD pipeline for ML. It is an Azure cloud-hosted environment that allows developers and organizations to deploy ML models quickly in the production environment with minimal code. In Azure Machine Learning Studio, you can run the notebooks on the cloud, use Automated ML for automated training and tuning the ML model using a metric, or use a designer to deploy a model from data preparation using a drag-and- drop interface.\n\nThe following are the salient features of Azure Machine Learning Studio:\n\nStorage provision to store your data and artifacts •\t MLFlow to track your experiments and log the run details like timestamp, model metrics, and so on\n\nModel registry to store trained ML models for reusability •\t Key vault to store credentials and variables •\t Supports open-source libraries and frameworks •\t Compute instance, which enables building and training in a secure VM •\t Pipelines and CI/CD for faster training and deployment\n\n280  Machine Learning in Production\n\nEndpoints for ML model •\t Data drift functionality to deliver consistent accuracy and predictions •\t Monitoring service to track the model, logs, and resources\n\nWorkspace A machine learning workspace is the main resource of AML. It contains experiments, models, endpoints, datasets, and such. It also includes other Azure resources, such as Azure Container Registry (ACR), which registers Docker containers at the time of Docker image deployment, storage, application insights, and key vault.\n\nExperiments An experiment consists of several runs initiated from the script. Run details and metadata are stored under the experiment. When you run the script, the experiment name needs to be provided to store the run information. However, it will create a new experiment if the name provided in the script does not exist in the given workspace.\n\nRuns A run can be defined as a single iteration of the training script. Multiple runs are recorded under an experiment. A run logs model metrics and metadata, such as timestamps.\n\nThe best thing about AML is that you do not need to create a separate web service API using frameworks like Flask, Django, and FastAPI. AML creates endpoints for trained ML models and tracks the web service.\n\nIn this section, you will be using the Azure Machine Learning template and modifying it as per your requirements. You can build your codebase from the scratch; however, this template saves time as it has reusable and required steps already defined. The Azure DevOps demo generator can be accessed at the following link:\n\nhttps://azuredevopsdemogenerator.azurewebsites.net/environment/createproject\n\nGo to DevOps Labs, select the Azure Machine Learning template and add it to your project by providing the project name and organization in the next step.\n\nAlternatively, you can directly go to the following URL:\n\nhttps://azuredevopsdemogenerator.azurewebsites.net/?name=machinelearning\n\nDeploying ML Models on Microsoft Azure  281\n\nFigure 11.22: Azure DevOps Demo Generator\n\nAs you can see, a codebase is imported along with the template. This template contains code and pipeline definitions for a machine learning project to demonstrate how to automate the end-to-end ML/AI project. In this, most steps can be reused with minor modifications wherever required. A codebase is residing in the Azure Git repository; however, you can import a codebase from the GitHub repository. Note that this template is built for a regression algorithm and uses a diabetes dataset.",
      "page_number": 292
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 307-315)",
      "start_page": 307,
      "end_page": 315,
      "detection_method": "topic_boundary",
      "content": "282  Machine Learning in Production\n\nHowever, you need to update the files for the classification algorithm, and the loan dataset will be used for it.\n\nFigure 11.23: Azure code repository\n\nNext, head over to the pipeline section from the left menu. You can see that the steps have already been defined in the template. Don’t worry if you want to create all the steps from the beginning. First off, create a new project, and inside that project, go to the pipeline section from the left menu. Next, select a codebase from the available source options, such as GitHub, Azure Repos, and Git. Then, create a new pipeline from a YAML file, or use the classic editor to create a pipeline without YAML.\n\nNext, go to the newly created project settings and create a new service connection, if it is not created. Choose Service Principal (automatic), and then choose the subscription and resource group from the dropdown and provide the service connection name in the next window. Make sure the Grant access permission to all pipeline checkbox is selected.\n\nDeploying ML Models on Microsoft Azure  283\n\nThe following figures show the steps discussed:\n\nFigure 11.24: New Azure service connection\n\nNow, you need to update the files that will be used in the Azure CI/CD pipeline. You can refer to the following code to modify respective scripts. Only code modifications are discussed here, as you can keep the rest of the code as it is in the original script.\n\ncode/training/train.py\n\nThis file will build a classification model using logistic regression. After model training, it will log Cross-Validation (CV) scores and accuracy for later use. Then, it will save the model as a pickle object. This script will be called by the aml_service/10- TrainOnLocal.py script.\n\nThe following code update is incorporated into the existing script:\n\n1. import pickle\n\n2. from azureml.core import Workspace\n\n3. from azureml.core.run import Run\n\n4. import os\n\n5. from sklearn.linear_model import LogisticRegression\n\n6. from sklearn.model_selection import train_test_split, cross_val_ score\n\n7. from sklearn.metrics import accuracy_score\n\n8. from sklearn.preprocessing import LabelEncoder\n\n9. from sklearn.externals import joblib\n\n284  Machine Learning in Production\n\n10. import pandas as pd\n\n11. import numpy as np\n\n12. import json\n\n13. import subprocess\n\n14. from typing import Tuple, List\n\n15.\n\n16. run = Run.get_submitted_run()\n\n17.\n\n18. url = \"https://gist.githubusercontent.com/ suhas-ds/a318d2b1dda8d8cbf2d6990a8f0b7e8a/ raw/9b548fab0952dd12b8fdf057188038c8950428f1/loan_dataset.csv\"\n\n19. df = pd.read_csv(url)\n\n20.\n\n21. # fill the missing values for numerical cols with the median\n\n22. num_col = ['LoanAmount','Loan_Amount_Term','Credit_History']\n\n23. for col in num_col:\n\n24. df[col].fillna(df[col].median(), inplace=True)\n\n25.\n\n26. # fill the missing values for categorical cols with mode\n\n27. cat_col = ['Gender','Married','Dependents','Self_Employed']\n\n28. for col in cat_col:\n\n29. df[col].fillna(df[col].mode()[0], inplace=True)\n\n30.\n\n31. # Total Income = Applicant Income + Coapplicant Income\n\n32. df['Total_Income'] = df['ApplicantIncome'] + df['CoapplicantIncome'] 33.\n\n34. # drop unnecessary columns\n\n35. cols = ['ApplicantIncome', 'CoapplicantIncome', \"LoanAmount\", \"Loan_Amount_Term\", 'Loan_ID']\n\n36. df = df.drop(columns=cols, axis=1)\n\n37.\n\n38. # Label encoding\n\n39. cols = ['Gender',\"Married\",\"Education\",'Self_Employed',\"Property_ Area\",\"Loan_Status\",\"Dependents\"]\n\nDeploying ML Models on Microsoft Azure  285\n\n40. le = LabelEncoder()\n\n41. for col in cols:\n\n42. df[col] = le.fit_transform(df[col])\n\n43.\n\n44. # Train test data preparation\n\n45. target = 'Loan_Status'\n\n46.\n\n47. X = df.drop(columns=['Loan_Status'], axis=1)\n\n48. y = df['Loan_Status']\n\n49. X_train, X_test, y_train, y_test = train_test_split(X, y, test_ size=0.30, random_state=42) 50.\n\n51. print(\"Running train.py\")\n\n52.\n\n53. model = LogisticRegression()\n\n54. model.fit(X_train, y_train)\n\n55. print(\"Accuracy is\", model.score(X_test, y_test)*100)\n\n56. # cross validation - it is used for better validation of the model\n\n57. # eg: cv-5, train-4, test-1\n\n58. cv_score = cross_val_score(model, X, y, cv=5)\n\n59. print(\"Cross-validation is\",np.mean(cv_score)*100)\n\n60. run.log(\"CV_score\", np.mean(cv_score)*100)\n\n61. y_pred = model.predict(X_test)\n\n62. print(\"Accuracy = \" , accuracy_score(y_test, y_pred))\n\n63. run.log(\"accuracy\", accuracy_score(y_test, y_pred))\n\n64.\n\n65. # Save the model as part of the run history\n\n66. model_name = \"sklearn_classification_model.pkl\"\n\n67. # model_name = \".\"\n\n68.\n\n69. with open(model_name, \"wb\") as file:\n\n70. joblib.dump(value=model, filename=model_name)\n\n286  Machine Learning in Production\n\ncode/score/score.py\n\nThis script will be used for testing the web service. Here, update the saved model path, and then update the input test data that is to be predicted using web service.\n\nThe following code update is incorporated into the existing script:\n\n1. model_path = Model.get_model_path(model_name=\"sklearn_ classification_model.pkl\")\n\nUpdate test data as per the ML model requirement:\n\n1. test_row = '{\"data\":[[1,1,0,1,0,1,2,3849],[1,1,3,0,0,0,1,5540]]}'\n\naml_config/config.json\n\nHere, you need to update the details from the Azure portal. Other scripts will use the variables declared in this script.\n\nThe following code update is incorporated into the existing script:\n\n1. {\n\n2. \"subscription_id\": \"██████████████████████████\",\n\n3. \"resource_group\": \"mlops\",\n\n4. \"workspace_name\": \"mlops_ws\",\n\n5. \"location\": \"centralus\"\n\n6. }\n\nenvironment_setup/install_requirements.sh\n\nThis bash script is responsible for installing Python packages using pip.\n\nThe following code update is incorporated into the existing script:\n\n1. python --version\n\n2. pip install azure-cli==2.37.0 #==2.0.69\n\n3. pip install --upgrade azureml-sdk[cli]\n\n4. pip install -r requirements.txt\n\naml_service/00-WorkSpace.py\n\nThis file will get the details of the existing workspace; however, if not found, it will create a new one.\n\naml_service/10-TrainOnLocal.py\n\nThis script triggers the code/training/train.py script to run on the local compute (host agent in case of build pipeline). If you are training on a remote VM, you do not need this script in the build pipeline. All the training scripts generate an output file aml_\n\nDeploying ML Models on Microsoft Azure  287\n\nconfig/run_id.json, which records the run_id and run history name of the training run. run_id.json is used by 20-RegisterModel.py to get the trained model. Update the experiment name to mlops-classification.\n\nThe following code update is incorporated into the existing script:\n\n1. experiment_name = \"mlops-classification\"\n\naml_service/15-EvaluateModel.py\n\nAt this step, you will get the run history for both the production model and the newly trained model to compare the accuracy. If the new model’s accuracy is better than that of the existing model, the new model will be promoted to production; otherwise, the existing model will remain as it is. However, for the first time, there won’t be any model to compare, so it will go to the except block and promote the new model. You can change the metrics that need to be compared and conditions that decide whether the new model is to be promoted to production. At the end of the script, it will capture the experiment name and run id of the new model in the aml_config/run_id.json file if the new model is promoted to production.\n\nThe following code update is incorporated into the existing script:\n\n1. production_model_acc = production_model_run.get_metrics(). get(\"accuracy\")\n\n2. new_model_acc = new_model_run.get_metrics().get(\"accuracy\")\n\n3. print(\n\n4. \"Current Production model accuracy: {}, New trained model accuracy: {}\".format(\n\n5. production_model_acc, new_model_acc\n\n6. )\n\n7. )\n\n8.\n\n9. promote_new_model = False\n\n10. if new_model_acc > production_model_acc:\n\n11. promote_new_model = True\n\n12. print(\"New trained model performs better, thus it will be registered\")\n\naml_service/20-RegisterModel.py\n\nThis script is responsible for registering new models. This will look for aml_config/ run_id.json; if run_id is not found, it will print the message No new model to\n\n288  Machine Learning in Production\n\nregister as production model perform better written inside the except block and exit from the code.\n\nIf the run id is found in aml_config/run_id.json, it will register the model. Finally, it will write the registered model details to /aml_config/model.json.\n\nThe following code update is incorporated into the existing script:\n\n1. # Download Model to Project root directory\n\n2. model_name = \"sklearn_classification_model.pkl\"\n\n3. run.download_file(\n\n4. name=\"./outputs/\" + model_name, output_file_path=\"./model/\" + model_name\n\n5. )\n\n6. print(\"Downloaded model {} to Project root directory\". format(model_name))\n\n7. os.chdir(\"./model\")\n\n8. model = Model.register(\n\n9. model_path=model_name, # This points to a local file\n\n10. model_name=model_name, # This is the name the model is registered as\n\n11. tags={\"area\": \"loan\", \"type\": \"classification\", \"run_id\": run_ id},\n\n12. description=\"Classification model for loan dataset\",\n\n13. workspace=ws,\n\n14. )\n\naml_service/30-CreateScoringImage.py\n\nIn the beginning, it will look for the aml_config/model.json file generated from the previous step. If it is not found, the script will be terminated with the message No new model to register thus no need to create new scoring image.\n\nThe following code update is incorporated into the existing script:\n\n1. image_name = \"loan-model-score\"\n\n2.\n\n3. image_config = ContainerImage.image_configuration(\n\n4. execution_script=\"score.py\",\n\n5. runtime=\"python-slim\",\n\nDeploying ML Models on Microsoft Azure  289\n\n6. conda_file=\"conda_dependencies.yml\",\n\n7. description=\"Image with logistic regression model\",\n\n8. tags={\"area\": \"loan\", \"type\": \"classification\"},\n\n9. )\n\nThis will create the image for the model, and at the end, capture the image details, such as name, version, creation_state, image_location, and image_build_ log_uri in the /aml_config/image.json.\n\naml_service/50-deployOnAci.py\n\nAzure Container Instances (ACI) is an Azure service that enables users to run a container on the Azure cloud without demanding the use of a Virtual Machine (VM).\n\nIn the beginning, it will look for the aml_config/image.json file generated from the previous step. If it is not found, the script will be terminated with the message No new model, thus no deployment on ACI.\n\nThe following code update is incorporated into the existing script:\n\n1. aciconfig = AciWebservice.deploy_configuration(\n\n2. cpu_cores=1,\n\n3. memory_gb=1,\n\n4. tags={\"area\": \"loan\", \"type\": \"classification\"},\n\n5. description=\"A loan prediction app\",\n\n6. )\n\nThis script will create a web service and capture the ACI details in /aml_config/aci_ webservice.json at the end of the script.\n\naml_service/60-AciWebserviceTest.py\n\nIn the beginning, it will look for the aml_config/aci_webservice.json file generated from the previous step. If it is not found, the script will be terminated with the message No new model, thus no deployment on ACI.\n\nThe following code update is incorporated into the existing script:\n\n1. # Input for Model with all features\n\n2. input_j = [[1,1,0,1,0,1,2,3849],[1,1,3,0,0,0,1,5540]]\n\n290  Machine Learning in Production\n\nConfigure CI pipeline At this stage, you will configure the CI pipeline. This pipeline will execute the following tasks:\n\nSet up Python version 3.6. • Install required dependencies.\n\nCreate or get Azure Machine Learning (AML) workspace. •\t Build and train the ML model. •\t Evaluate newly trained model performance against the existing model to decide whether the model is to be promoted to production. •\t Register model in Azure Machine Learning (AML) service. •\t Create a scoring Docker image with the required dependencies. •\t Publish the artifacts (all files) to the repo.\n\nThe following figure shows the steps of the CI pipeline:\n\nFigure 11.25: CI pipeline for ML",
      "page_number": 307
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 316-324)",
      "start_page": 316,
      "end_page": 324,
      "detection_method": "topic_boundary",
      "content": "Deploying ML Models on Microsoft Azure  291\n\nNote: You need to select Azure service connection from the Azure subscription dropdown wherever necessary.\n\nNext, in the Triggers tab, select Enable continuous integration checkbox. This enables the CI pipeline to run as soon as any changes are pushed to the code repository.\n\nYou can trigger the CI pipeline by pushing any changes to the repository. The Azure CI pipeline detects the changes in the repo and starts building the pipeline. To see the progress of the CI pipeline, switch to the Summary tab.\n\nThe following figure displays the Summary tab. You can see the commit message on the top, followed by the summary and the jobs. The job status and progress of steps can be seen by clicking on Phase 1.\n\nFigure 11.26: CI pipeline for ML-Jobs summary\n\n292  Machine Learning in Production\n\nAfter clicking on Phase 1 under the Jobs section, you should see the progress of the CI pipeline. The following figure shows the terminal output of each CI pipeline step:\n\nFigure 11.27: CI pipeline for ML-logs\n\nConfigure CD pipeline Now that you have completed the CI pipeline configuration, it’s time to configure the CD pipeline. The CD pipeline will deploy the image generated through the CI pipeline to the Azure Container Instance (ACI) and Azure Kubernetes Service (AKS). In the current case, the scope of the project is limited to ACI. However, you can deploy it on AKS to handle large amounts of traffic and make it scalable.\n\nThis pipeline executes the following tasks:\n\nSet up Python version 3.6. •\n\nInstall the required dependencies.\n\nDeploy a web service on the Azure container instance (ACI). •\t Test the ACI web service by passing data that needs to be tested.\n\nDeploying ML Models on Microsoft Azure  293\n\nDeploy a web service on Azure container instance (AKS). •\t Test the AKS service by passing the data to be tested.\n\nThe following figure shows the CD pipeline:\n\nFigure 11.28: CD pipeline for ML\n\nNow, navigate to Pipeline | Releases, select Deploy Web service and click on Edit pipeline.\n\nThe following figure shows the QA – Deploy on ACI stage of the CD pipeline when switched to the Tasks tab.\n\nFigure 11.29: CD pipeline for ML-QA stage\n\n294  Machine Learning in Production\n\nNote: You need to select Azure service connection from the Azure subscription dropdown wherever necessary.\n\nIn the following figure, you can see the entire file path provided by the artifacts that are retrieved from the CI pipeline:\n\nFigure 11.30: CD pipeline for ML-QA stage requirement step\n\nWhen you update the CD pipeline, you can run the CD pipeline manually by hitting the Create Release button or setting the auto trigger, which means that the CD pipeline starts executing upon completion of the CI pipeline. You need to make sure that continuous deployment triggers are enabled in the Artifacts stage and the QA -Deploy on ACI stage of the CD pipeline.\n\nIn the following figure, you can see the CD pipeline triggers upon completion of the CI pipeline, which caused the QA -Deploy on ACI stage to start executing:\n\nDeploying ML Models on Microsoft Azure  295\n\nFigure 11.31: CD pipeline for ML-triggered after CI pipeline\n\nIt takes a few minutes to execute all the steps. You should see the number of tasks completed and the status of the stage. If you want to see the details of each step, click on the Logs option.\n\nThe following figure shows the status of each step with logs.\n\nFigure 11.32: CD pipeline for ML-QA stage logs\n\n296  Machine Learning in Production\n\nWhen ACI deployment is completed, it waits for manual approval from the developer but for a limited time. You can update the time from the options. After approval, it will deploy to the AKS. If you want to automate this step, you can remove the manual approval or set auto approval. However, it is recommended to keep manual approval before deploying it on AKS.\n\nIn the following figure, you can see that the QA – Deploy on ACI stage is completed, and it is pending approval for deploying it on AKS. You can set the time for approval requests to be active. After approval, the pipeline will deploy the app using AKS.\n\nFigure 11.33: Azure pipeline for ML-completed\n\nNow, head over to the Azure Machine Learning studio, and go to Jobs from the left panel. You should see the experiment with the name provided in the training script, that is, mlops-classification. On the right side, you can see the details like the experiment name, the date of creation, and the last submission. When you go to the experiment, you will see the run information with status and other metadata. On the top, you can see metrics details with graphs. In the current case, accuracy and CV score are being logged. You can assess each job’s details by clicking on it.\n\nThe following figure shows the model metrics, that is, accuracy and CV score, along with run information and other details. In the graphs, you can see the value of each metric logged in the runs separately.\n\nDeploying ML Models on Microsoft Azure  297\n\nFigure 11.34: Azure Machine Learning-Model metrics\n\nWhen a model is deployed in an Azure Container Instance (ACI), you will get the scoring URI, which can be consumed by the subsequent applications.\n\nThe following figure shows the endpoint created by the script in the CI/CD pipeline:\n\nFigure 11.35: AML Real-time endpoints\n\nYou can assess the model endpoints by evaluating the outcome after passing the input data. Moreover, you can consume the endpoint using other languages, such as C#, Python, and R.\n\n298  Machine Learning in Production\n\nIn the following figure, you can see the output produced by the model’s endpoints under Test results after passing the test data:\n\nFigure 11.36: AML Real-time endpoints testing\n\nYou can assess the same endpoint in the Postman as well. The following figure shows the testing of scoring URI in the Postman:\n\nFigure 11.37: AML Real-time endpoints testing in Postman\n\nTill now, you have learned to automate CI/CD workflow using Azure DevOps and Azure Machine Learning Service. You have explored GitHub Actions and\n\nDeploying ML Models on Microsoft Azure  299\n\nAzure DevOps approaches to build an automated workflow to deploy models in production. By now, you should be familiar with the Azure cloud and the services provided by Azure and its implementation.\n\nConclusion In this chapter, you learned to deploy the app on the Azure platform (PaaS). You also built an automated CI/CD pipeline using GitHub Actions. Deployed ML web app in the Azure App Service using Azure Container Registry (ACR). Further on, you integrated tox with GitHub Actions to run the test cases as a part of the CI/CD pipeline before deploying the ML web app to Azure.\n\nAfter that, you learned to deploy an ML app using Azure DevOps and Azure Machine Learning service (AML), which provides ML as a service (MLaaS). You analyzed the metrics logged by runs in the Azure Machine Learning studio, and you passed the sample data to the endpoint and assessed the outcome.\n\nIn the next chapter, you will explore the GCP platform to deploy ML apps.\n\nPoints to remember\n\nYou need to authorize GitHub Actions to automate the CD part of the pipeline and provide the required details.\n\nAzure Role Based Access Control (RBAC) is an authorization system built on Azure Resource Manager that provides fine-grained access management of Azure resources.\n\nAn experiment consists of several runs initiated from the script. • In the Azure DevOps template, the aml_service/15-EvaluateModel.py script will compare the performance metric of the latest model against the deployed model before the deployment stage. It means the set of rules is written in this script, which will decide whether the model can be promoted to the next step.\n\nMultiple choice questions\n\n1. _________ can be defined as a single iteration of the training script.\n\na) A run\n\nb) An experiment\n\nc) A workspace\n\nd) ACR",
      "page_number": 316
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 325-332)",
      "start_page": 325,
      "end_page": 332,
      "detection_method": "topic_boundary",
      "content": "300  Machine Learning in Production\n\n2. Azure primarily uses the _________ pricing plan.\n\na) Unlimited (for all Azure services)\n\nb) Quarterly\n\nc) Monthly\n\nd) Pay-as-you-go\n\nAnswers 1. a\n\n2. d\n\nQuestions\n\n1. What is the use of Azure Machine Learning (AML) service?\n\n2. How can you log in to Azure from the local terminal?\n\n3. Explain the working of the Azure pipeline stages.\n\nDeploying ML Models on Google Cloud Platform  301\n\nChapter 12 Deploying ML Models on Google Cloud Platform\n\nIntroduction Cloud computing is the on-demand delivery of computer system resources, such as servers, databases, analytics, storage, and networking. These resources and services are available off-premises; however, they can be accessed over the cloud (the internet) as per requirement. This means you do not need to set up big infrastructure on- premises. This is beneficial for businesses and individuals as they get the required system resources and services instantly under a pay-as-you-go plan. Resources and services can be added or removed quickly, which allows you to spend money efficiently.\n\nRefer to the previous chapters to learn about concepts like Packaging ML Models, FastAPI, Docker, and CI/CD pipeline, as they are pre-requisites for this chapter.\n\nStructure In this chapter, the following topics will be covered:\n\nCreate and set up an account on the Google Cloud Platform (GCP) •\t Cloud Source Repositories •\t Cloud Build\n\n302  Machine Learning in Production\n\nContainer Registry •\t Deploy web-based ML app to Google Kubernetes Engine (GKE) with manual trigger\n\nBuild an automated CI/CD pipeline to deploy web-based ML apps on Google Kubernetes Engine (GKE)\n\nObjectives After studying this chapter, you should be able to deploy an ML model on Google Kubernetes Engine (GKE). You can create a fully automated CI/CD pipeline with simple steps, without the need to integrate any external tool, service, or platform. You will create a remote Git repository on GCP using Cloud Source Repository and Kubernetes cluster to make a scalable ML app. You will also learn to create manifest files for Kubernetes in this chapter. By the end of it, you should be able to create triggers in Cloud Build for the automated deployment of ML apps.\n\nGoogle Cloud Platform (GCP) Google Cloud Platform (GCP) comprises cloud computing services offered by Google, which uses the same infrastructure as the one used by YouTube, Gmail, and other Google platforms or services. The platform offers a range of services for compute, Machine learning and AI, networking, IoT, and BigData. Here are a few services offered by the GCP:\n\nGoogle's compute engine provides VM instances to run the code and deploy the apps.\n\nAI and Machine learning services like Vertex AI, which is an end-to-end ML life cycle management and unified platform. Data Scientists can upload the data, build, train, and test ML models easily.\n\nAI building blocks, such as Vision AI, help derive insights from images using AutoML.\n\nContainer services, such as Container Registry and Google Kubernetes Engine (GKE), manage Docker images and allow developers to build scalable applications.\n\nBigQuery and data proc to process and analyze large amounts of data. •\t Databases, such as Cloud SQL and Cloud Bigtable, store data on the cloud. •\t Developer tools, such as Cloud Build, Cloud Source Repositories, and Google Cloud Deploy to automate CI/CD process.\n\nDeploying ML Models on Google Cloud Platform  303\n\nManagement tools, such as Deployment Manager and Cost Management, help you to track the deployment and cost of tools or services used in projects. •\t Networking services, such as Virtual Private Cloud (VPC), let you create a virtual private cloud environment within a public cloud. Multiple projects created in different regions can communicate with each other without openly communicating through the public internet.\n\nSecurity services, such as cloud key management, firewalls, and security center.\n\nStorage services, such as Cloud Storage, allow you to store artifacts. •\t Serverless computing, such as Cloud Function, is an event-driven serverless compute platform. This Function as a Service (FaaS) lets you run the code with no server or containers.\n\nOperations services, such as Cloud Logging and Cloud Monitoring, let you track the performance, delay, and such of the deployed models or applications.\n\n\n\nIt also provides other services, such as migration, IoT, event management, identity and access, hybrid and multi-cloud, backup, and recovery.\n\nYou must have got the gist of GCP and its services; now, you are going to learn to deploy the Machine Learning model on GCP. After completing this chapter, you will be in a better position to work on GCP for ML model deployments.\n\nGCP offers a free trial account for 90 days with $300 credits. It will give hands-on experience to new customers so that they can explore the services offered by GCP.\n\nSet up the GCP account First of all, set up a GCP account; however, this is a one-time activity. It should be ready for use immediately on logging in:\n\nStep 1:\n\nCreate a GCP account (if you don’t have one) and log in. The free trial account can be created on the GCP platform at https://cloud.google.com/free.\n\nFirst, log in using your Gmail ID; it will then redirect you to the GCP Free-trial page. Select your country from the dropdown, accept the Terms of Service, and then click on Continue. Next, choose Individual (for personal use) or Business (if it is a business account). After that, provide personal details like name, address, and city. Finally, provide payment mode details and complete the process.\n\n304  Machine Learning in Production\n\nStep 2:\n\nAfter logging in to the account, create a new project, as shown in the following figure:\n\nFigure 12.1: Create a new project\n\nClick on the NEW PROJECT button and provide the project name. In this case, it is MLOps, and the project ID will be auto-generated; however, it can be edited. In this case, it is mlops-54321. Location can be pre-selected or can be changed.\n\nHit the Create button to complete the process.\n\nNote: The project ID is unique to each project.\n\nCloud Source Repositories In this case, the Git repository, that is, the Cloud Source Repository, is used; however, other remote Git repositories, such as GitHub, can also be used. Cloud Source Repositories are private Git repositories hosted on GCP. Multiple Git repositories can be created within a single project. It supports the standard set of Git commands, such as push, pull, clone, and log. Cloud Source Repositories can be added to a local Git repository as remote repositories. It allows collaboration and provides security. However, it is recommended not to store any personal or confidential data in it. The good part is that GCP’s Cloud Source Repositories allow you to store up to 50 GB per month for free.\n\nNow, search and enable Cloud Source Repositories API, which allows access to source code repositories hosted by Google.\n\nNext, create a new repository in Cloud Source Repositories, and provide the repository name and project ID, as shown in the following figure:\n\nDeploying ML Models on Google Cloud Platform  305\n\nFigure 12.2: Create a new repository in Cloud Source Repositories\n\nAfter creating a Cloud Source Repository, it will be empty. So, choose Clone your repository to a local Git repository option. Next, head to Manually generated credentials and follow the instructions to generate credentials by following the link Generate and store Git credentials. Simply copy the commands and run them in the local terminal. In the following figure, an empty Cloud Source Repository is created and options are selected, as discussed:\n\nFigure 12.3: Cloning repository to local Git repository\n\n306  Machine Learning in Production\n\nHereafter, the repository should be accessible from the local terminal. Now, create a new directory and clone the Cloud Source Repository to your local machine, include the code files in it, and push it to the Cloud Source Repository from the local machine.\n\nLet’s consider the scenario of loan prediction, where the problem statement is to predict whether a customer’s loan will be approved. Feel free to implement hyper- parameter tuning and optimize the model.\n\nFirst, create a package of ML code, and build a web app using FastAPI. Then, create the test cases and dependencies file, Dockerfile, and docker-compose file. Finally, create configuration files for GCP deployment, namely, cloudbuild.yaml, deployment.yaml, service.yaml.\n\nNote: For code files in the src directory, refer to the ML model package developed in Chapter 4: Packaging ML Models, and for the rest of the code files, refer to Chapter 11: Deploying ML Models on Microsoft Azure. New code files added are cloudbuild.yaml, deployment.yaml, and service.yaml. Remove workflow directory .github. Also, the tox.ini file is updated for this scenario.\n\nThe code repository is also available on GitHub at https://github.com/suhas-ds/ GCP.\n\nThe following directory structure shows the CI/CD pipeline files:\n\n.\n\n├── k8s\n\n│ ├── deployment.yaml\n\n│ └── service.yaml\n\n├── src\n\n│ ├── build\n\n│ │ ├── bdist.linux-x86_64\n\n│ │ └── lib\n\n│ ├── prediction_model\n\n│ │ ├── config\n\n│ │ ├── datasets\n\n│ │ ├── processing\n\n│ │ ├── __pycache__\n\n│ │ ├── trained_models\n\n│ │ ├── __init__.py\n\n│ │ ├── pipeline.py\n\n│ │ ├── predict.py\n\nDeploying ML Models on Google Cloud Platform  307\n\n│ │ ├── train_pipeline.py\n\n│ │ └── VERSION\n\n│ ├── prediction_model.egg-info\n\n│ │ ├── dependency_links.txt\n\n│ │ ├── PKG-INFO\n\n│ │ ├── requires.txt\n\n│ │ ├── SOURCES.txt\n\n│ │ └── top_level.txt\n\n│ ├── tests\n\n│ │ ├── pytest.ini\n\n│ │ └── test_predict.py\n\n│ ├── MANIFEST.in\n\n│ ├── README.md\n\n│ ├── requirements.txt\n\n│ ├── setup.py\n\n│ └── tox.ini\n\n├── cloudbuild.yaml\n\n├── docker-compose.yml\n\n├── Dockerfile\n\n├── main.py\n\n├── pytest.ini\n\n├── requirements.txt\n\n├── runtime.txt\n\n├── start.sh\n\n├── test.py\n\n└── tox.ini\n\n13 directories, 31 files\n\ntox.ini\n\nThe tox is a free and open-source tool used for testing Python packages or applications. It creates the virtual environment and installs the required dependencies in it. Finally, it runs the test cases for that Python package prediction_model. Update the tox.ini file as shown in the following code:",
      "page_number": 325
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 333-341)",
      "start_page": 333,
      "end_page": 341,
      "detection_method": "topic_boundary",
      "content": "308  Machine Learning in Production\n\n1. [tox]\n\n2. envlist = my_env\n\n3. skipsdist=True\n\n4.\n\n5. [testenv]\n\n6. install_command = pip install {opts} {packages}\n\n7. deps =\n\n8. -r requirements.txt\n\n9.\n\n10. setenv =\n\n11. PYTHONPATH=src/\n\n12.\n\n13. commands=\n\n14. pip install requests\n\n15. pytest -v test.py --junitxml=test_log.xml\n\n16. pytest -v src/tests/ --junitxml=src_test_log.xml\n\nThe skipsdist=True flag indicates not to perform a packaging operation. It means the package will not be installed in the virtual environment before performing any test. Set PYTHONPATH to the src directory, where Python package files are placed. Under commands, pytest commands will be executed, and the results of the tests will be exported in .xml files.\n\nAfter a successful push from the local terminal, code files will be displayed in the Cloud Source Repository, as shown in the following figure:\n\nDeploying ML Models on Google Cloud Platform  309\n\nFigure 12.4: Code files pushed to Cloud Source Repository\n\nCloud Build Cloud Build is a serverless platform that allows users to automate build, test, and deploy containers in the cloud quickly. Cloud Build works with deployment environments of the App Engine, Kubernetes Engine, Cloud Run, Cloud Functions, and Firebase. It will charge you for build minutes that are utilized if your usage exceeds the free quota allotted by GCP.\n\nCloud Build config file type can be YAML or JSON. This file contains a series of steps and commands to execute the build, specified by the developer. Each build step runs in its own Docker container. However, these containers are connected through a local Docker network with a Cloud Build. This permits build steps to communicate and share information.\n\nSearch and enable Cloud Build API in APIs and services. Cloud Build uses a special service account to execute builds on your behalf. The email format of the Cloud Build service account is [PROJECT_NUMBER]@cloudbuild.gserviceaccount.com. It has permission to execute tasks like fetching code from the repository, getting project details, and writing objects to storage associated with the project. However, this default service account can be changed to the developer’s own service account to execute builds on the developer’s behalf, and the developer can set specific permissions.\n\n310  Machine Learning in Production\n\ncloudbuild.yaml\n\nCloud builders are container images with common languages and tools installed in them. Cloud Build offers pre-built Docker images that can be used in the config file to execute commands. These pre-built images are available in the Container Registry at gcr.io/cloud-builders/.\n\nIn this case, the cloudbuild.yaml file will run the tests using tox and pytest. Next, it will build the Docker container image with the latest tag. Following that, it will push the image to the Container Registry and run the bash command to check the list of files in the current directory. Finally, it will pull the image from the Docker container to deploy it on Google Kubernetes Engine (GKE).\n\n1. steps:\n\n2. # Run test\n\n3. - name: 'python:3.7-slim'\n\n4. id: Test\n\n5. entrypoint: /bin/sh\n\n6. args:\n\n7. - -c\n\n8. - 'python -m pip install tox && tox'\n\n9.\n\n10. # Build the image\n\n11. - name: 'gcr.io/cloud-builders/docker'\n\n12. id: Build\n\n13. args: ['build', '-t', 'gcr.io/$PROJECT_ID/mlapp:latest', '.']\n\n14. timeout: 200s\n\n15.\n\n16. # Push the image\n\n17. - name: 'gcr.io/cloud-builders/docker'\n\n18. id: Push\n\n19. args: ['push', 'gcr.io/$PROJECT_ID/mlapp:latest']\n\n20.\n\n21. - name: 'gcr.io/cloud-builders/gcloud'\n\n22. id: Bash\n\n23. entrypoint: /bin/sh\n\nDeploying ML Models on Google Cloud Platform  311\n\n24. args:\n\n25. - -c\n\n26. - |\n\n27. echo \"List of files and directories within the current working directory\"\n\n28. ls -l\n\n29.\n\n30. # Deploy container image to GKE\n\n31. - name: \"gcr.io/cloud-builders/gke-deploy\"\n\n32. id: Deploy on GKE\n\n33. args:\n\n34. - run\n\n35. - --filename=k8s/\n\n36. - --image=gcr.io/$PROJECT_ID/mlapp:latest\n\n37. - --location=us-west1-b\n\n38. - --cluster=mlkube\n\nIn the preceding file, mainly three (supported) builder images are used:\n\ngcr.io/cloud-builders/docker: To build and push the Docker container image to the Container Registry\n\ngcr.io/cloud-builders/gcloud: To run the inline bash script\n\ngcr.io/cloud-builders/gke-deploy: To deploy containerized app in GKE\n\nAlso, python:3.7-slim is the publically available image used for testing the code using tox. If any image is to be used from the Docker hub, then simply provide the image name in single quotes. However, if an image is from other registries, then the full registry path needs to be specified in single quotes. The args field of a build step accepts a list of arguments and passes them to the image referred to by its name field.\n\nContainer Registry Container Registry is a container image managing and storing private container images service offered by GCP. It allows users to push and pull container images securely. In this case, the Docker container image will be pushed to the Container Registry. The Kubernetes engine will pull the image to deploy the ML app on the cloud.\n\n312  Machine Learning in Production\n\nContainer Registry uses the hostname, project ID, image, and tag or image digest to access images, where image digest is the sha256 hash value of the image contents. The format is as follows:\n\nHOSTNAME/PROJECT-ID/IMAGE:TAG\n\nOr\n\nHOSTNAME/PROJECT-ID/IMAGE:@IMAGE-DIGEST\n\nThe following figure shows the GCP Container Registry after enabling the Container Registry API, where mlapp is an app name and gcr.io is a hostname. Currently, gcr.io hosts the images in the United States.\n\nFigure 12.5: Container Registry\n\nKubernetes Container orchestration is an automation of the operational process needed to run containerized workloads and services. It automates tasks like deployment, scaling up-down, lifecycle management, load balancing, configuration, security, resource allocation, health monitoring, and networking of containers.\n\nKubernetes (also known as K8s or Kube) is an open-source platform for container orchestration. It allows the application to scale on the fly, without interrupting the application running in production. It creates multiple replicas of containerized applications quickly to handle increased traffic and automatically reduces the number of replicas when traffic decreases.\n\nDeployment on Kubernetes creates Pods with containers inside them. Pods are the smallest unit in the Kubernetes environment. A Pod can have one or more containers. It always runs on a Node; however, a Node can have multiple Pods. A Node is just a worker (VM or physical machine) in a Kubernetes environment. All nodes are managed by a control plane.\n\nDeploying ML Models on Google Cloud Platform  313\n\nEvery Node runs the Kubelet and container runtime, such as Docker. Kubelet is the medium of communication between the control plane and the Node. It also manages Pods and running containers inside the Node. Container runtime such as Docker will pull the image from the registry and it will run the containerized application.\n\nGoogle Kubernetes Engine (GKE) Google Kubernetes Engine (GKE) offers the infrastructure to manage, deploy, and scale containerized applications. The underlying architecture of GKE consists of a set of compute engine instances assembled to form a cluster.\n\nManifest is a file (JSON or YAML) containing a description of all the components you want to deploy. These manifest files guide Kubernetes to network between containers. Kubernetes schedules the deployment of containers into clusters and identifies the best host for the container. After deciding on a host, it manages the lifecycle of the container based on pre-planned specifications.\n\nSearch and enable Kubernetes Engine API in the GCP console, as shown in the following figure:\n\nFigure 12.6: Kubernetes Engine API\n\nNext, click on the Activate Cloud Shell icon in the top-right corner of the GCP account. It will open a cloud-based terminal:\n\ngcloud container clusters create mlkube --zone \"us-west1-b\" --machine- type \"n1-standard-1\" --num-nodes \"1\" --project mlops-54321\n\nFigure 12.7: Creating Kubernetes cluster with cloud shell\n\n314  Machine Learning in Production\n\nAfter the successful creation of the Kubernetes cluster, its details can be checked using the GKE service, as shown in the following figure:\n\nFigure 12.8: Kubernetes cluster\n\nAll manifest files contain the apiVersion field. Each Kubernetes configuration file has three sections: metadata, specification, and status. Metadata and specification are provided by the developers in the configuration files; however, the status part is managed by Kubernetes. For instance, if 2 replicas are declared but only one replica is up and running, then Kubernetes scrutinize the status and creates one more replica to match the desired state and the actual state. As shown as follows, Kubernetes configuration files are placed in the k8s directory. The labels and selectors are responsible for connecting service and deployment files. The same labels should be used in the service.yaml and deployment.yaml files for successful communication between service and deployment.\n\nk8s\n\n├── deployment.yaml\n\n└── service.yaml\n\ndeployment.yaml\n\nThis file contains the configuration of Kubernetes deployment, such as the number of replicas to be created and the container image to be deployed. The role of deployment.yaml is to launch a Pod with a containerized app and ensure that the necessary number of replicas are always up and running in the Kubernetes cluster. In the following file, a template is a blueprint for a Pod. It has its own metadata and specification. In this case, the replicas field is set to 2, which means it will create two replicas of the Pods.\n\n1. apiVersion: apps/v1\n\n2. kind: Deployment\n\n3. metadata:\n\nDeploying ML Models on Google Cloud Platform  315\n\n4. name: loan-prediction\n\n5. spec:\n\n6. replicas: 2\n\n7. selector:\n\n8. matchLabels:\n\n9. app: mlapp\n\n10. template:\n\n11. metadata:\n\n12. labels:\n\n13. app: mlapp\n\n14. spec:\n\n15. containers:\n\n16. - name: loan-prediction-app\n\n17. image: gcr.io/mlops-54321/mlapp:latest\n\n18. ports:\n\n19. - containerPort: 8000\n\nservice.yaml\n\nThis file connects the containerized application to the end user. The role of service. yaml is to expose an interface to a Pod created by deployment.yaml, which enables network access between cluster and service. It exposes the deployed application to the external world. Port 8000 of containerized application is mapped with port 80. In ports, you can specify more than one port. In this case, the type of service is LoadBalancer, which will expose the service through GCP’s (cloud provider’s) load balancer. Label selector helps to locate Pods. In this case, mlapp is a label.\n\n1. apiVersion: v1\n\n2. kind: Service\n\n3. metadata:\n\n4. name: mlapp\n\n5. spec:\n\n6. type: LoadBalancer\n\n7. selector:\n\n8. app: mlapp\n\n9. ports:\n\n316  Machine Learning in Production\n\n10. - port: 80\n\n11. targetPort: 8000\n\nNote: In the targetPort field, the uppercase letter ‘P’ is used, as it is a key-value pair. In the key-value pair, the second word’s first character should be uppercase.\n\nDeployment using Cloud Shell – Manual Trigger First, open a cloud shell and clone the remote Git repository using the git clone command. Now, code files are accessible in the GCP cloud shell terminal. Run the cloudbuild.yaml file manually with the following command:\n\ngcloud builds submit --config cloudbuild.yaml --project=mlops-54321\n\nThis command will take a few minutes to execute. In this scenario, the pipeline is triggered manually, where a series of steps will be executed sequentially from the cloudbuild.yaml file.\n\nThe following figure shows the output of the preceding command, in which details of the deployed app using Kubernetes are given, such as the IP address of the deployed ML app, source, duration, status, and GKE URLs for workloads, services, configuration, and storage.\n\nFigure 12.9: Deploying ML app on GKE with manual trigger",
      "page_number": 333
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 342-349)",
      "start_page": 342,
      "end_page": 349,
      "detection_method": "topic_boundary",
      "content": "Deploying ML Models on Google Cloud Platform  317\n\nCI/CD pipeline using Cloud Build GCP comes with a set of services for the CI/CD pipeline. In the previous section, the build was triggered manually with the cloud shell terminal. To automate the manual process, CI/CD pipeline needs to be built with an automated trigger. The following figure shows the list of CI/CD services provided by GCP:\n\nFigure 12.10: CI/CD services provided by GCP\n\nWhen an update is pushed to Cloud Source Repository (CSR), it will trigger the CI/CD pipeline. Cloud Build configuration file first runs the tests, and then builds and pushes the container image to the Container Registry. Simultaneously, it reads the Kubernetes manifest and deploys it on the Kubernetes engine. Kubernetes engine will pull the container image pushed by Cloud Build and eventually run the containerized ML app that is accessible to the end user.\n\n318  Machine Learning in Production\n\nThe following figure shows the automated CI/CD pipeline to deploy the containerized app on GKE:\n\nFigure 12.11: CI/CD pipeline to deploy ML app on GKE\n\nCreate a trigger in Cloud Build In this section, create a trigger in Cloud Build that will look for a Cloud Build configuration file, that is, a YAML file, to build and push the Docker image to the Container Registry. It is also responsible for applying the Kubernetes manifest in the Kubernetes engine.\n\nFirst, head to the Triggers section from the left side panel in Cloud Build. For the first time, it will show No triggers found in global, as shown in the following figure:\n\nFigure 12.12.: Creating trigger in Cloud Build\n\nCreate a new trigger using the CREATE TRIGGER button. Next, provide a unique name within the project’s region for triggers and select the region from the dropdown. Description and Tags are optional.\n\nIn this case, the Name is AutoDeploy, the Region is global(non-regional) and the Description is Mlapp, as shown in the following figure:\n\nDeploying ML Models on Google Cloud Platform  319\n\nFigure 12.13: Creating trigger in Cloud Build – providing a name and selecting a region\n\nFollowing that, select the repository event that will trigger the pipeline. It provides multiple options; however, Push to a branch is selected. So, any changes pushed to the master branch of the repository will trigger the pipeline. This will look for changes in the repository and clone the repository when the trigger is invoked.\n\nIn the following figure, the Push to a branch option is selected, along with the Cloud Source Repository and the master branch as a source:\n\nFigure 12.14: Creating trigger in Cloud Build – selecting an event and source\n\n320  Machine Learning in Production\n\nFinally, complete the process by selecting the configuration file type and location. In this case, the Cloud Build configuration file (YAML or JSON) option is selected for Configuration Type and the cloudbuild.yaml file location from the repository is provided, as shown in the following figure:\n\nFigure 12.15: Creating trigger in Cloud Build – selecting configuration file type and its location\n\nNow, any update pushed to the Cloud Source Repository will trigger the pipeline. To see the build history, go to the History section from the side panel or view it through the link from the dashboard.\n\nThe following figure shows the build history with Status, Build ID, the Source - loan_pred, that is, the Cloud Source Repository in this case, and Ref - the master branch of that repository in this case. It also shows the Trigger type, Trigger name, and Duration, that is, the time taken to complete the build.\n\nFigure 12.16: Build history\n\nDeploying ML Models on Google Cloud Platform  321\n\nYou can see the live build logs for each step. It helps you to debug the errors that occurred during the build process. In the last step, you should see GKE deployment details, such as status, name, and URLs to access the app.\n\nThe following figure shows the build summary with the build ID and duration or time taken by each step:\n\nFigure 12.17: Build details – Build log\n\nThe dashboard of the Cloud Build display summarizes details, such as the status of the Latest Build, Duration, Build History of Pass-Fail builds, the Average Duration of the build, and Pass-Fail%, as shown in the following figure:\n\nFigure 12.18: Dashboard - Cloud Build\n\n322  Machine Learning in Production\n\nAfter successful deployment, you should see the ML app up and running in GKE, as shown in the following figure:\n\nFigure 12.19: ML app running in GKE\n\nGoogle Kubernetes also offers an app monitoring service. Go to the service ingress URL in GKE. This includes resource usage, details, events, and logs. The following figure shows the overview of the monitoring of deployed apps:\n\nFigure 12.20: Monitoring mlapp\n\nDeploying ML Models on Google Cloud Platform  323\n\nYou have now learned how to create a simple CI/CD pipeline using GitHub to deploy ML apps on the GCP platform. However, you can modify it as per business and application requirements.\n\nConclusion In this chapter, the ML app was deployed on Google Kubernetes Engine (GKE) with CI/CD pipeline. First off, you created a GCP account and created code files, including cloudbuild.yaml and Kubernetes manifest files for GCP deployment, and pushed them to Cloud Source Repositories (CSR). Then, you created a Container Registry to push-pull Docker container images and a Kubernetes cluster ml-kube to deploy an ML app. Finally, you enabled Cloud Build API and created an AutoDeploy trigger by integrating Cloud Source Repositories (CSR) to automate ML app deployment.\n\nIn the next chapter, you will learn to deploy an ML app on Amazon Web Services (AWS).\n\nPoints to remember\n\nKubernetes cluster can be shared among multiple projects. •\t Gmail email ID is required to create an account on GCP. •\t Cloud Build is a serverless platform that allows you to automate build, test, and deploy containers quickly.\n\nCloud builders are container images with common languages and tools installed in them.\n\nThe same labels should be used in service.yaml and deployment.yaml files for successful communication between service and deployment.\n\nMultiple choice questions\n\n1. _________ is CI/CD service provided by Google.\n\na) Cloud Build\n\nb) BigQuery\n\nc) Virtual Private Cloud (VPC)\n\nd) Pub/Sub\n\n324  Machine Learning in Production\n\n2. Manifest file’s recommended file format is _________.\n\na)\n\n.git\n\nb) YAML\n\nc) Python\n\nd) Shell script\n\nAnswers 1. a\n\n2. b\n\nQuestions\n\n1. What is the location of the Cloud builder’s pre-built container images?\n\n2. What is the role of a Container Registry?\n\n3. What is container orchestration?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "page_number": 342
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 350-358)",
      "start_page": 350,
      "end_page": 358,
      "detection_method": "topic_boundary",
      "content": "Deploying ML Models on Amazon Web Services  325\n\nChapter 13 Deploying ML Models on Amazon Web Services\n\nIntroduction Cloud computing refers to storing and accessing data and applications over the internet. Usually, data is stored on a remote server. Simply put, you can access data and applications from anywhere on the internet without worrying about the physical or on-premises infrastructure, which makes cloud computing more popular among organizations.\n\nAmazon Web Services (AWS) helps solve on-premises infrastructure issues. AWS can spin up 100-1000 servers in a few minutes and, extra or unused servers will be removed. It is easy to scale applications with AWS. You can add more storage for applications or data. AWS helps focus on building and deploying applications on the cloud without worrying about setting up infrastructure from scratch.\n\nRefer to the previous chapters if you need to study the concepts discussed, such as packaging ML models, FastAPI, Docker, and CI/CD pipeline.\n\nStructure In this chapter, the following topics will be covered:\n\n\n\nIntroduction to Amazon Web Services (AWS)\n\n326  Machine Learning in Production\n\nAWS Elastic Container Registry (ECR) •\t AWS CodeCommit •\t Amazon Elastic Container Service (ECS) •\t AWS CodeBuild •\t Application Load Balancer (ALB) •\t Deploy web-based ML app to Elastic Container Service (ECS) with service and Application Load Balancer (ALB)\n\nAWS CodePipeline •\t Build an automated CI/CD CodePipeline to deploy a web-based ML app on Amazon Web Services (AWS)\n\nObjectives After studying this chapter, you should be able to deploy an ML model on Amazon Elastic Container Service (ECS) without the need to integrate any external tool, service, or platform except AWS. You will create a remote Git repository on AWS using AWS CodeCommit and Amazon Elastic Container Service (ECS) cluster to run scalable ML apps. You will also learn to integrate Application Load Balancer (ALB) with Amazon Elastic Container Service (ECS) for routing requests coming from the external world. Moving on, you will integrate the service port with the Docker port via port mapping, create a security group for Application Load Balancer (ALB) and learn to push the Docker container image to AWS Elastic Container Registry (ECR). You should be able to create a fully automated CI/CD pipeline with AWS CodePipeline, AWS CodeBuild, AWS Elastic Container Registry (ECR), Application Load Balancer (ALB), and Amazon Elastic Container Service (ECS) after completing this chapter.\n\nIntroduction to Amazon Web Services (AWS) Amazon Web Services (AWS) is a cloud infrastructure where you can host applications. In 2006, AWS started offering IT services to the market in the form of web services. AWS is one of the leading cloud service providers.\n\nAWS compute services Amazon Web Services (AWS) offers compute services for managing workloads that comprise many servers or instances.\n\nDeploying ML Models on Amazon Web Services  327\n\nHere are some of the widely used compute services offered by AWS that can be used as per the business or application requirements. These services will be discussed later in the chapter.\n\nAmazon Elastic Compute Cloud (EC2) Amazon EC2 is a virtual machine that represents a remote server. Amazon EC2 service is grouped under Infrastructure-as-a-Service (IaaS). Amazon EC2 enables applications with resizable computing capacity, and these EC2 machines are known as instances. You can create multiple instances with different computing sizes. You can even upgrade the ram, vCPU, and so on after creation, so you do not need to recreate a new instance and configure it again. This is why the elastic term is used.\n\nAmazon Elastic Container Service (ECS) Amazon Elastic Container Service (ECS) enables you to deploy, scale, and manage containerized applications. It manages containers and enables developers to run containerized applications across the cluster of EC2 instances. However, it is not based on Kubernetes. Amazon ECS is free, meaning you don’t need to pay for the ECS cluster. However, additional charges are to be paid for EC2 instances running in ECS tasks. There are mainly two ways to launch an ECS clusters:\n\nFargate Launch •\t EC2 Launch\n\nAmazon ECS is a technology owned exclusively by AWS. ECS easily integrates with AWS Application Load Balancer (ALB) and Network Load Balancer (NLB).\n\nAmazon Elastic Kubernetes Service (EKS) Amazon Elastic Kubernetes Service (EKS) is a Kubernetes service backed by AWS, which enables you to build Kubernetes clusters on AWS without manually installing Kubernetes on EC2. Amazon Elastic Kubernetes Service (EKS) allows you to manage or orchestrate containers in a Kubernetes environment. You need to pay for the EKS cluster, with additional charges for EC2 instances running inside the Kubernetes pod. Amazon EKS service set up and manages the Kubernetes control plane for you.\n\nIt is a good choice if you are looking for multi-cloud functionality and additional features compared to the Amazon Elastic Container Service (ECS).\n\nAmazon Elastic Container Registry (ECR) Amazon Elastic Container Registry (ECR) allows developers to store, share and deploy Docker images on Amazon ECS. It provides security to images stored in\n\n328  Machine Learning in Production\n\nit. Amazon Elastic Container Registry (ECR) is integrated with Amazon ECS. It is similar to the Docker Hub container registry but is managed by AWS. It allows you to store private Docker container images in it.\n\nAWS Fargate Simply put, AWS Fargate is a serverless compute for containers. You pay for the usage per minute for the resources used by containers like virtual CPU (vCPU) and memory. AWS Fargate is compatible with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\n\nAWS Lambda AWS Lambda is simple and less expensive. It is a serverless and event-driven compute service; it is a Function as a Service (FaaS). That means you don’t need to manage servers or clusters. Events could be any simple activity, such as a user clicking on links to get the latest news. The term Lambda is supposed to be borrowed from functions of lambda calculus and programming. Mostly, it comprises three components:\n\nA function: It is the actual code to perform the task. •\t A configuration: It dictates the execution of a function. •\t An event source: This is the event that triggers the function. However, this component is optional.\n\nAWS Lambda is a good choice for event-driven programming or when you need to access several services.\n\nAmazon SageMaker AWS also provides a machine learning platform as a service, that is, Amazon SageMaker. It removes the overhead of managing and maintaining servers manually. It also reduces the time and cost of machine learning model deployment on the AWS cloud.\n\nAmazon SageMaker is a cloud-based machine learning platform that enables data scientists and developers to build, train, tweak, and deploy machine learning models in production environments. It uses Amazon Simple Storage Service (S3) to store the data and comes with over 15 most commonly used built-in machine learning algorithms for training the data. It deploys the ML models to SageMaker endpoints. Amazon SageMaker uses the Amazon Elastic Container Registry (ECR) to store container images.\n\nDeploying ML Models on Amazon Web Services  329\n\nYou must have got the gist of AWS and its services. You are going to deploy a Machine Learning model to the Amazon Elastic Kubernetes Service (EKS). After this chapter, you will be comfortable working on AWS for ML model deployment.\n\nSet up an AWS account First of all, set up an AWS account; however, this is a one-time activity. It is ready to use immediately on log in.\n\nThe AWS Free Tier provides customers the ability to explore and try out AWS services free of charge up to specified limits for each service. The Free Tier consists of three different types of offerings, a 12-month Free Tier, an Always Free offer, and short-term trials.\n\nStep 1: Create or log in to the existing AWS account\n\nCreate an AWS account (if you don’t have one) and sign in. A free trial account can be created on the AWS platform at https://aws.amazon.com/free.\n\nFirst, click on the Create a Free Account button. Next, provide the login details, such as email ID, password, and AWS account name. Then, select the account type (Professional or Personal) and provide contact information. After that, issue PAN and payment details. Finally, complete the verification process on the identity verification page.\n\nAfter a few minutes, the account will be activated and ready to use. Select a plan as per your requirements.\n\nStep 2: Create an IAM user and provide the required permissions\n\nAfter signing in to the AWS Management Console, open the IAM console - https:// console.aws.amazon.com/iam/. IAM stands for Identity and Access management. Next, click on Users in the navigation pane and choose Add users. Then, provide the user name for that user and choose the access type. After that, select the existing policies from the attached existing policies or create a new one for that user. In this scenario, Administrator access is given to the IAM user. Tags are optional. Finally, review and complete the process. Do not forget to save the user details like Access key ID, Secret access key, and user name. In this case, an IAM user will be used to execute all the tasks. Hit the Create user button and complete the process.\n\n330  Machine Learning in Production\n\nThe following figure shows that the IAM user is created:\n\nFigure 13.1: AWS IAM user\n\nThe Secret access key is available only when it is created, so you need to download and save it. If lost, then you have to create another one.\n\nCreating access keys for the AWS account root user is not recommended unless required. Rather, create one or more IAM users with the required permissions to execute the tasks.\n\nStep 3: Install and configure AWS CLI on the local machine\n\nAWS Command-Line Interface (CLI) is a tool that allows the management of several AWS services. It is an open-source tool that enables you to interact with AWS services through commands. First, download and install it, then configure it with AWS credentials. To get started with AWS CLI, follow this link https://docs.aws. amazon.com/cli/latest/userguide/getting-started-install.html.\n\nFor Linux, download the installation file using the curl command, where the -o option specifies the filename that the downloaded package is written to: curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n\nUnzip the installer as follows: unzip awscliv2.zip\n\nThe installation command uses a file name install from unzipped aws directory: sudo ./aws/install\n\nVerify the installation by checking the version of AWS CLI using the following command:\n\nDeploying ML Models on Amazon Web Services  331\n\naws --version\n\nRestart the terminal if the aws command cannot be found.\n\nNext, configure the AWS CLI with AWS IAM credentials, as shown in the following figure:\n\nFigure 13.2: AWS CLI configuration\n\nAWS CodeCommit In this case, the Git repository, that is, AWS CodeCommit, is used; however, other remote Git repositories like GitHub can be used. AWS CodeCommit is a private Git repository hosted on AWS. It allows you to create multiple Git repositories and supports a standard set of Git commands, such as push, pull, clone, and log. AWS CodeCommit can be added to a local Git repository as a remote. It allows collaboration and provides security; however, it is a standard practice not to store any personal or sensitive data in it. The good part is that AWS CodeCommit is available to both new and existing users for free up to a certain limit. It does not expire even after 12 months of free tier usage. It is free for the first 5 active users, which includes 1,000 repositories per account, 50 GB of storage per month, and 10,000 Git requests per month.\n\nGo to CodeCommit from Services:\n\nServices | All Services | Developer Tools | CodeCommit\n\nCreate a new repository in CodeCommit and then provide the repository name; the description is optional. Repository names are included in the URL of that repository.\n\nOnce the repository is created, specific permissions need to be provided; then, create Git credentials to access the CodeCommit repository from the local machine. Go to the IAM console, select Users from the navigation pane and choose the user for which CodeCommit is to be configured. Then, attach the AWSCodeCommitPowerUser policy from the policies list and complete the process. In this case, the IAM user has admin privileges, so there is no need to attach the above-mentioned policy to the IAM user.\n\nChoose the same IAM user and locate HTTPS Git credentials for AWS CodeCommit. Next, select that user and click on the Generate credentials button. It will populate Git credentials, that is, username and password. Password can be seen and\n\n332  Machine Learning in Production\n\ndownloaded, save it for future use. This password cannot be recovered later on; however, it can be regenerated.\n\nFigure 13.3: CodeCommit–Git credentials\n\nAfter creating the code commit repository, clone it to the local machine. Copy project code files into the directory on the local machine. Re-initialize the Git repository using the git init command. Finally, commit and push changes to the CodeCommit repository from the local machine.\n\nLet’s consider the scenario of loan prediction where the problem statement is to predict whether a customer’s loan will be approved. Feel free to implement hyperparameter tuning and tweak the model.\n\nFirst, create an installable package of ML code, and then build a web app using FastAPI; then, create the tests and dependencies file. After that, create Dockerfile and docker-compose.yml files. Finally, create the configuration file buildspec.yaml for AWS deployment.\n\nNote: For code files, refer to the previous chapters. You should create buildspec. yaml file and update the tox.ini file for the current scenario. You need to update src\\tests\\test_predict.py, src\\prediction_model\\predict.py, and start.sh for continuous training.\n\nYou access the code repository at https://github.com/suhas-ds/AWS-CICD\n\nDeploying ML Models on Amazon Web Services  333\n\nThe following directory structure shows the code files for a CI/CD pipeline:\n\n.\n\n├─ src\n\n│ ├─ prediction_model\n\n│ │ ├─ config\n\n│ │ │ ├─ __init__.py\n\n│ │ │ └─ config.py\n\n│ │ ├─ datasets\n\n│ │ │ ├─ __init__.py\n\n│ │ │ ├─ test.csv\n\n│ │ │ └─ train.csv\n\n│ │ ├─ processing\n\n│ │ │ ├─ __init__.py\n\n│ │ │ ├─ data_management.py\n\n│ │ │ └─ preprocessors.py\n\n│ │ ├─ trained_models\n\n│ │ │ ├─ __init__.py\n\n│ │ │ └─ classification_v1.pkl\n\n│ │ ├─ VERSION\n\n│ │ ├─ __init__.py\n\n│ │ ├─ pipeline.py\n\n│ │ ├─ predict.py\n\n│ │ └─ train_pipeline.py\n\n│ ├─ tests\n\n│ │ ├─ pytest.ini\n\n│ │ └─ test_predict.py\n\n│ ├─ MANIFEST.in\n\n│ ├─ README.md\n\n│ ├─ requirements.txt\n\n│ ├─ setup.py\n\n│ └─ tox.ini\n\n├─ .gitignore\n\n├─ Dockerfile",
      "page_number": 350
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 359-367)",
      "start_page": 359,
      "end_page": 367,
      "detection_method": "topic_boundary",
      "content": "334  Machine Learning in Production\n\n├─ README.md\n\n├─ buildspec.yaml\n\n├─ docker-compose.yml\n\n├─ main.py\n\n├─ pytest.ini\n\n├─ requirements.txt\n\n├─ runtime.txt\n\n├─ start.sh\n\n├─ test.py\n\n└─ tox.ini\n\ntox.ini\n\nThe tox is a free and open-source tool used for testing Python packages or applications. It creates the virtual environment and installs the required dependencies in it; finally, it runs the tests for that Python package. You need to update the tox.ini file as shown in the following code.\n\n1. [tox]\n\n2. envlist = my_env\n\n3. skipsdist=True\n\n4.\n\n5. [testenv]\n\n6. install_command = pip install {opts} {packages}\n\n7. deps =\n\n8. -r requirements.txt\n\n9.\n\n10. setenv =\n\n11. PYTHONPATH=src/\n\n12.\n\n13. commands=\n\n14. pip install requests\n\n15. pytest -v src/tests/ --junitxml=pytest_reports/Prediction_ test_report.xml\n\n16. pytest -v test.py --junitxml=pytest_reports/API_test_report. xml\n\nDeploying ML Models on Amazon Web Services  335\n\nThe skipsdist=True flag indicates not to perform a packaging operation. It means the package will not be installed in a virtual environment before performing any test. Set PYTHONPATH to the src/ directory where Python package files are placed. In the commands section, pytest commands will be executed, and the results of the tests will be exported in .xml files in the pytest_reports directory.\n\n.\\src\\prediction_model\\predict.py\n\nHere, add the train_accuracy function to the predict.py file that will return the accuracy score.\n\n1. def train_accuracy(input_data):\n\n2. \"\"\" Checking accuracy score of training data \"\"\"\n\n3.\n\n4. # Read Data\n\n5. data = pd.DataFrame(input_data)\n\n6. y_train = np.where(data['Loan_Status']=='Y', 1, 0).tolist()\n\n7.\n\n8. # Prediction\n\n9. prediction = _loan_pipe.predict(data[config.FEATURES])\n\n10. y_pred = prediction.tolist()\n\n11. score = accuracy_score(y_train,y_pred)*100\n\n12. return score\n\n.\\src\\tests\\test_predict.py\n\nNow, add the test_score function to the test_predict.py file that will test whether the training accuracy score is between 70 to 95. You can change this range.\n\n1. from prediction_model.predict import train_accuracy\n\n2.\n\n3. def test_score():\n\n4. ''' This function will check the accuracy score of training data '''\n\n5. data = load_dataset(file_name=config.TRAIN_FILE)\n\n6. score = train_accuracy(data)\n\n7. assert 70 <= score <= 95\n\nAfter a successful push from the local terminal, code files will be displayed in the cloud source repository.\n\n336  Machine Learning in Production\n\nContinuous Training With a CI/CD pipeline, you can add Continuous Training (CT) stage. So when the CI/CD pipeline will run, it will also train the model on the latest available data as per the configuration settings.\n\nstart.sh\n\nTo train the model on the latest data, you need to add the following command to the start.sh file: python app/src/prediction_model/train_pipeline.py\n\nThis will run the train_pipeline.py file and generate the latest pickle file of the trained model. This file will first install the package, then train the model, and finally, run the FastAPI app.\n\n1. #!/bin/bash\n\n2.\n\n3. pip install app/src/\n\n4. python app/src/prediction_model/train_pipeline.py\n\n5. python app/main.py\n\nAmazon Elastic Container Registry (ECR) Amazon Elastic Container Registry (ECR) is a container registry service managed by AWS. It stores, manages, and provides security to private container images. If these container images are to be accessed through code or other services, such as CodeBuild, then container registry access needs to be provided to the specified service roles.\n\nGo to Elastic Container Registry from Services:\n\nServices | All Services | Containers | Elastic Container Registry\n\nCreate a repository with the name mlapp-cicd. By default, the visibility of the repository will be private; it means access will be managed by IAM and pre-defined permissions will be granted to the repository policy. The repository name should be concise yet meaningful, that is, it should be based on the content of the repository. The name must start with a letter and can only contain lowercase letters, numbers, hyphens, underscores, periods, and forward slashes.\n\nThen, go to the repository and click on the View push commands button in the top- right corner. It will display the push commands and instructions you have to follow while pushing a Docker container image from a local machine using AWS CLI. First,\n\nDeploying ML Models on Amazon Web Services  337\n\nyou need to log in to AWS ECR using the command given in the pop-up window. Next, build a Docker image on a local machine using the docker build command. If the Docker image is already built, the Docker build step can be skipped. Then, tag the Docker image and push the image to the AWS ECR repository using the commands given in the pop-up window.\n\nThe following figure shows the push commands for the mlapp-cicd repository:\n\nFigure 13.4: Amazon ECR – Push commands\n\nNote: It is recommended to install and configure the latest version of AWS CLI and Docker on a local machine.\n\nDocker Hub rate limit From November 20, 2020, anonymous and free user are limited to 100 and 200 container image pull requests every 6 hours. After the specified limit, it will throw an error that contains ERROR: toomanyrequests: Too Many Requests. or You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limits. To increase your pull rate limits, you can upgrade your account to a Docker Pro or Team subscription. Docker Pro and Docker Team accounts enable 5,000 pulls in 24 hours from Docker Hub.\n\n338  Machine Learning in Production\n\nTo avoid this error, create a new repository on AWS ECR for the current scenario. Next, pull the base Python 3.7 image from the Docker Hub into the local machine or VM. Then, push that base Python 3.7 image to the newly created AWS ECR repository. In the current scenario, the base image is being maintained in a separate repository, as shown in the following figure:\n\nFigure 13.5: Amazon ECR–Repositories\n\nUse this image path in the Dockerfile so that when the docker build command runs for Dockerfile; it will pull the base Python 3.7 images from AWS. For this, you need to update Dockerfile as follows:\n\n1. # Pull base Python:3.7 image from AWS ECR repository\n\n2. ARG REPO=692601447418.dkr.ecr.us-west-2.amazonaws.com\n\n3.\n\n4. FROM ${REPO}/python:3.7\n\n5.\n\n6. COPY ./start.sh /start.sh\n\n7.\n\n8. RUN chmod +x /start.sh\n\n9.\n\n10. ENV PYTHONPATH \"${PYTHONPATH}:app/src/\"\n\n11.\n\nDeploying ML Models on Amazon Web Services  339\n\n12. COPY . /app\n\n13.\n\n14. RUN chmod +x /app\n\n15.\n\n16. # Exposing the port that uvicorn will run the app on\n\n17. ENV PORT=8000\n\n18. EXPOSE 8000\n\n19.\n\n20. RUN pip install --upgrade pip\n\n21.\n\n22. RUN pip install --no-cache-dir --upgrade -r app/requirements.txt\n\n23.\n\n24. CMD [\"./start.sh\"]\n\nIn the current scenario, the python:3.7-slim-buster image is built and tagged with 3.7, and then it is pushed to AWS ECR for reusability. This way, every time, the python:3.7-slim-buster image will be pulled from AWS ECR instead of the Docker Hub to avoid any rate limit error.\n\nAWS CodeBuild A CodeBuild is a fully managed Continuous Integration (CI) service backed by AWS infrastructure that allows developers to automate the build, test, and deploy containers or packages quickly. It works on an on-demand or pay-as-you-go model, that is, it charges users based on the minute for the compute resources they have used.\n\nThe CodeBuild config file type is YAML. This file contains a series of phases and commands to execute the build specified by the developer.\n\nbuildspec.yaml\n\nIn this case, the buildspec.yaml file first installs the tox package in the install phase. Next, it will run the tests using tox and log in to AWS ECR in the pre_build phase. Then, the buildspec.yaml file will build a Docker container image with the latest tag in the build phase and push the image to the AWS ECR repository in the post_build phase. Finally, specify the details for pytest reports, such as files, a base directory, and file format. Also, provide the filename to be stored as an artifact in the S3 bucket.\n\n340  Machine Learning in Production\n\n1. version: 0.2\n\n2.\n\n3. phases:\n\n4. install:\n\n5. runtime-versions:\n\n6. python: 3.7\n\n7. commands:\n\n8. - pip install tox\n\n9. pre_build:\n\n10. commands:\n\n11. - echo Running test...\n\n12. - tox\n\n13. - echo Logging into Amazon ECR...\n\n14. - aws --version\n\n15. - aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 692601447418.dkr.ecr.us- west-2.amazonaws.com\n\n16. - REPOSITORY_URI=692601447418.dkr.ecr.us-west-2.amazonaws. com/mlapp-cicd\n\n17. - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)\n\n18. - IMAGE_TAG=${COMMIT_HASH:=latest}\n\n19. build:\n\n20. commands:\n\n21. - echo Build started on `date`\n\n22. - echo Building the Docker image...\n\n23. - docker build -t $REPOSITORY_URI:latest .\n\n24. - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_ TAG\n\n25. post_build:\n\n26. commands:\n\n27. - echo Build completed on `date`\n\n28. - echo Pushing the Docker images...\n\nDeploying ML Models on Amazon Web Services  341\n\n29. - docker push $REPOSITORY_URI:latest\n\n30. - docker push $REPOSITORY_URI:$IMAGE_TAG\n\n31. - echo Writing container image definitions file...\n\n32. - printf '[{\"name\":\"mlapp-cicd\",\"imageUri\":\"%s\"}]' $REPOSITORY_URI:$IMAGE_TAG > imagedefinitions.json\n\n33. reports:\n\n34. pytest_reports:\n\n35. files:\n\n36. - Prediction_test_report.xml\n\n37. - API_test_report.xml\n\n38. base-directory: pytest_reports/\n\n39. file-format: JUNITXML\n\n40. artifacts:\n\n41. files: imagedefinitions.json\n\nIn the preceding file, mainly three supported builder images are used.\n\nYou can see, python:3.7 is the publicly available image used for testing the code using tox. If any image is to be used from the Docker Hub, then simply provide the image name in single quotes. However, if an image is from other registries, then the full registry path needs to be specified in single quotes. The args field of a build phase accepts a list of arguments and passes them to the image referenced by the name field.\n\nTo create a build, CodeBuild is used. Search and select CodeBuild from developer tools and choose Build projects from the Build section. Click on the Create build project button to create a new one. First off, under the Project configuration, provide the project name.\n\n342  Machine Learning in Production\n\nIn this case, the project name mlapp-cicd is provided, as shown in the following figure:\n\nFigure 13.6: CodeBuild–Project configuration\n\nNext, under the Source section, select the Source provider from the dropdown list and choose the subsequent details, such as Repository and Branch. Source providers contain details and code files to be used as input source code for the build project. In the current scenario, choose AWS CodeCommit as a Source provider, as shown in the following figure. However, other source providers can be selected instead of AWS CodeCommit, such as BitBucket, GitHub, and S3.",
      "page_number": 359
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 368-377)",
      "start_page": 368,
      "end_page": 377,
      "detection_method": "topic_boundary",
      "content": "Deploying ML Models on Amazon Web Services  343\n\nFigure 13.7: CodeBuild–Source\n\nThen, under the Environment section, choose the Managed Image option and choose the operating system as Ubuntu from the dropdown. It provides other operating systems such as Windows Server and Amazon Linux. In the current scenario, aws/ CodeBuild/standard:4.0 image is chosen. The remaining selections can be kept as default. Make sure you select the Privileged checkbox, which allows you to build Docker images.\n\n344  Machine Learning in Production\n\nThe following figure shows the selection made for the Environment section:\n\nFigure 13.8: CodeBuild–Environment\n\nAfter that, under the Buildspec section, choose Use a buildpsec file, as shown in the following figure:\n\nFigure 13.9: CodeBuild–Buildspec\n\nDeploying ML Models on Amazon Web Services  345\n\nFinally, under the Logs section, select Cloudwatch logs. This will upload build output logs to cloudwatch. This enables you to analyze the logs and output generated after building the project. However, this is optional. At the bottom of the page, click on the Create build project button and complete the configuration process.\n\nAttach container registry access to CodeBuild’s service role Now, you need to provide access to the service role of CodeBuild so that it can build the Docker container images. For this, go to IAM management console | Roles and select the CodeBuild service role. In the current scenario, it is CodeBuild- mlapp-cicd-service-role. Click on it, and the summary page will open. Click on Attach policies, and then search for EC2ContainerRegistry and select AmazonEC2ContainerRegistryFullAccess. It provides administrative access to Amazon ECR resources. If the required access is not provided, CodeBuild will be unable to build Docker images.\n\nThe following figure shows container registry access allowed for CodeBuild’s service role:\n\nFigure 13.10: CodeBuild–Service role\n\nNow, go to the main page of CodeBuild, and let’s manually execute the build phase by hitting the Start build button in the top-right corner. Build status will be displayed as In Progress. Logs are available under the Build logs tab. By default, it will show the last 1000 lines of the build log. After completion of the build phase, the latest container image should be available in Amazon Elastic Container Registry\n\n346  Machine Learning in Production\n\n(ECR). Also, the test report should be available under the Reports tab, as shown in the following figure:\n\nFigure 13.11: CodeBuild–Test report\n\nAmazon Elastic Container Service (ECS) Amazon Elastic Container Service (ECS) is a container orchestration tool used for managing and running Docker containers. Amazon Elastic Container Service (ECS) is a fully managed container management service offered by AWS. In the current scenario, ECS with the Fargate model will be used. It will take care of managing the cluster and load balancing. It will also make sure the application is up and running.\n\nLet’s understand the terms used in the Amazon Elastic Container Service (ECS):\n\nTask Definition: The task definition allows you to specify which Docker image to use, which ports to expose, how much CPU and memory to allot, how to collect logs, and how to define environment variables. This is a blueprint that dictates how a Docker container should launch.\n\nTask: A task resembles an instance of task definition. It can be created independently, that is, without an Amazon ECS cluster. This will spin up a container with an application running in it. It will not replace itself automatically if it stops or fails due to some error.\n\nDeploying ML Models on Amazon Web Services  347\n\nService: A Service is responsible for creating and maintaining the desired number of tasks up and running all the time. If any task fails or is stopped due to some error, then the ECS Service will replace that task with a new one. It refers to the task definition file to create tasks. •\t Cluster: It is a logical group of container instances. •\t Container: This is the Docker container created during task instantiation.\n\nThe following figure resembles the Amazon Elastic Container Service (ECS) architecture.\n\nFigure 13.12: Amazon Elastic Container Service (ECS)–Architecture\n\nAWS ECS deployment models AWS ECS Deployment model can be chosen as per your requirement. Let’s look at the models AWS Elastic Container Service (ECS) mainly offers for cluster deployment.\n\nEC2 instance EC2 (Elastic Compute Cloud) is a virtual machine in the cloud. First, configure and deploy EC2 instances in the cluster to run the containers. It provides more granular control over the instances. You can choose the instances as per requirement.\n\nThis model is a better choice if you want to do the following:\n\nRun containerized applications continuously\n\n348  Machine Learning in Production\n\nHave better control over the auto-scaling configuration •\t Use a Classic Load Balancer (CLB) to distribute workloads •\t Use Graphical Processing Unit (GPU) •\t Use Elastic Block Storage (EBS) •\t Deploy large and complex applications\n\nFargate This is a serverless pay-as-you-go model. You will be charged based on the computing capacity selected and the time of usage. ECS with Fargate is a container orchestration tool used for running Docker based containers without having to manage the underlying infrastructure.\n\nThis model is a better choice if you want to do the following:\n\nRun the task occasionally or for a short period •\t Containerized applications should be able to handle sudden spikes in incoming traffic\n\nUse application and network load balancers to distribute workloads •\t Save time from different configurations, regular maintenance, and security management\n\nNote: In this chapter, classic UI is used. By default, you might see a new UI; however; you can switch back to the classic UI.\n\nGo to Elastic Container Service from Services:\n\nServices | All Services | Containers | Elastic Container Service\n\nFirst off, go to the cluster page and create an ECS cluster by clicking on the Create Cluster button. AWS provides templates for creating clusters to simplify the process of cluster creation. In the current scenario, Networking only (AWS Fargate) is selected as the type of instance. On the next screen, under Cluster configurations, the Cluster name is to be provided. In this scenario, it is mlapp-cluster, as shown in the following figure:\n\nDeploying ML Models on Amazon Web Services  349\n\nFigure 13.13: Amazon Elastic Container Service (ECS)–Cluster configuration\n\nThen, click on the Create button, and it will launch the ECS cluster. You will see ECS cluster is created with the type Fargate, but no instances are running.\n\nTask definition After that, create a task definition. Go to the Task Definition page and click on Create new Task Definition. Choose the type as Fargate in step 1. In step 2, provide the task definition name. In this case, it is mlapp-cicd. Select the Task role ecsTaskExecutionRole from the dropdown and select the Operating system family as Linux, as shown in the following figure:\n\nFigure 13.14: Amazon Elastic Container Service (ECS) –task definition\n\n350  Machine Learning in Production\n\nKeep the Task execution role as default, that is, ecsTaskExecutionRole. Under the Task size section, choose the required memory and CPU to be allotted for tasks based on application requirements or use cases. A Task memory (GB) of 1GB and Task CPU (vCPU) of 0.25 vCPU is chosen for the current scenario, as shown in the following figure:\n\nFigure 13.15: Task definition–Task Size\n\nFinally, add the container details, such as container name, image URI to be taken from AWS ECR, and memory limits (in MiB) for containers. Hard and soft limits correspond to the memory and memoryReservation parameters in task definitions. The Port mappings parameter is important. Here, you need to provide a port used by the container to run the application and expose it in the Dockerfile. In the current scenario, the container port is 8000. You can leave the rest of the configurations as they are and add a container.\n\nIn the end, click on Create and complete step 2 of the task definition.\n\nRunning task with the task definition The task can be run independently using the task definition created earlier.\n\nThe following figure shows the various options available for task definition. Now, to run the task, choose the first option, that is, Run Task.\n\nFigure 13.16: Running an independent task\n\nDeploying ML Models on Amazon Web Services  351\n\nNext, choose the launch type. In the current scenario, FARGATE is chosen. Choose mlapp-cluster from the Cluster dropdown.\n\nAmazon VPC and subnets Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined. These virtual networks are similar to the traditional networks used in data centers. The merit of a virtual network is that it comes with scalable infrastructure managed by AWS. Each VPC network consists of one or more IP address ranges called subnets.\n\nNext, under VPC and security groups, choose cluster VPC and choose subnets. In the current scenario, choose 2a and 2b; however, the remaining 2c and 2d can also be chosen. Make sure the Auto-assign public IP is ENABLED. This will allow you to automatically assign available public IPs to access the ML app from anywhere.\n\nThen, edit the default security group, and the security groups window will show up. Here, create a new security group that will be used in a later stage. Choose Create new security group from Assigned security groups and provide the name for the security group as mlapp-sg, where sg stands for the security group. After that, two ports are allowed, that is, port 80 and port 8000, for the communication of containerized applications with the external world.\n\nAfter that, add HTTP port 80 and Custom TCP port 8000, on which the containerized application is running. Choose Anywhere in the source option for both ports.\n\nFinally, in the bottom-right corner, hit the Run Task button. It will take some time to get the task up and running. The task’s status will be shown as RUNNING in the Tasks tab, as shown in the following figure:\n\nFigure 13.17: Running an independent task–task created\n\n352  Machine Learning in Production\n\nThe public IP of the task is 54.191.99.95. It means an ML app can be accessed with this public IP from anywhere on the internet.\n\nThe issue with running a task independently is that if the task fails or stops due to any reason, then the application also stops; this makes the application inaccessible to the external world. To overcome this issue, ECS service can be used. Don’t forget to stop the running task. With ECS service, multiple tasks can be created.\n\nLoad balancing In parallel computing, load balancing is a process of distributing application traffic across different available computes or resources in order to make overall request handling more efficient. The main objective of the technique is to avoid any downtime for end users or customers.\n\nThe following figure resembles the high-level architecture of an Application Load Balancer (ALB):\n\nFigure 13.18: Application Load Balancer (ALB)\n\nAn application load balancer can only be added to a service while creating a service. Next, go to the load balancing part and create the target groups, which takes care of dynamically adding the IP addresses that get created while creating the tasks.\n\nGo to load balancer from Services:\n\nServices | All Services | Developer Tools | Load balancing\n\nThe following figure shows the components of Load Balancing:",
      "page_number": 368
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 378-388)",
      "start_page": 378,
      "end_page": 388,
      "detection_method": "topic_boundary",
      "content": "Deploying ML Models on Amazon Web Services  353\n\nFigure 13.19: AWS-Load Balancing\n\nLet’s understand the role and configuration of the two components of Load Balancing.\n\nTarget group With a load balancer, a single DNS name is enough to access the containerized application. A load balancer will take the user requests on port 80 and route them to tasks with dynamic IP through the service. The target group will ensure connectivity between the load balancer and tasks with dynamic IP through service.\n\nGo to the target group and create a new target group by hitting the Create target group button.\n\nCreating a target is a three-step process:\n\nSpecify group details •\t Register targets •\t Review IP targets to include in your group\n\nStep 1: Specify group details\n\nA configuration setting cannot be changed after the creation of a target group. In this step, the target type is IP addresses; its features are listed in the following figure:\n\nFigure 13.20: Target Groups–Specify group details\n\n354  Machine Learning in Production\n\nNext, provide the Target group name as mlapp-tg. The Target group name should not start or end with a hyphen (-). Then, provide port 8000 with HTTP type Protocol as the application is exposed to port 8000. Leave the rest of the selections as they are. This configuration is shown in the following figure:\n\nFigure 13.21: Target Groups–Basic configuration\n\nUnder the health check section, by default, the target group will check the root or index path (‘/’), or you can specify a custom path if preferred. The target group will periodically send the requests to this endpoint of the application to check whether it is healthy.\n\nStep 2: Register targets\n\nIn this step, IP addresses and ports can be specified manually from the selected network. By default, the VPC network will be selected. Remove the pre-selected IP, as this part is optional. This can be configured later on. The following figure shows this step:\n\nDeploying ML Models on Amazon Web Services  355\n\nFigure 13.22: Target Groups–Register targets\n\nAfter that, specify the ports for routing to this target. In the current case, the application’s port 8000 is specified, as shown in the following figure:\n\nFigure 13.23: Target Groups – Review targets and ports for routing to target\n\n356  Machine Learning in Production\n\nStep 3: Review IP targets to include in your group\n\nAt this point, you can repeat steps 1 and 2 if you want to add additional IP targets. However, you have not specified any IP address is specified in step 2, so go ahead and hit the Create target group button at the bottom. It will take you back to the main page of the targets group and display a successful message, as shown in the following figure:\n\nFigure 13.24: Target Groups–mlapp-tg\n\nAmazon Resource Names (ARNs) are unique identifiers of AWS resources. AWS ARN format could be any one of the following formats:\n\narn:partition:service:region:account-id:resource-id •\t arn:partition:service:region:account-id:resource-type/resource-id •\t arn:partition:service:region:account-id:resource-type:resource-id\n\nA partition is a group of AWS regions, and each account has a limited scope: one partition.\n\nSecurity Groups A security group is a set of firewall rules that control the traffic toward the load balancer. It has not been created yet, so let’s go ahead and create a new security group. Follow these steps:\n\nServices | All Services | EC2 | Network & Security | Security Groups\n\nFirst off, specify basic details for the security group, such as name and description, as shown in the following figure. A Security group name is mlapp-lb-sg, where lb stands for the load balancer and sg stands for the security group.\n\nDeploying ML Models on Amazon Web Services  357\n\nFigure 13.25: Security Group for load balancer–Create a security group\n\nNext, configure inbound rules for the security group. Hit the Add rule button. A load balancer should be able to access port 80 from anywhere, so specify port 80 under the Port range and choose Source type as anywhere, as shown in the following figure. More rules can be added through the Add rule button.\n\nFigure 13.26: Security Group for load balancer–Inbound rules\n\n358  Machine Learning in Production\n\nThen, hit the Create security group button in the bottom-right corner of the page and complete the process. The following figure shows the details of the security group after creation:\n\nFigure 13.27: Security Group for load balancer–mlapp-lb-sg\n\nThis security group, mlapp-lb-sg will be used for the load balancer.\n\nApplication Load Balancers (ALB) Application Load Balancer (ALB) accepts incoming requests and routes them to registered targets, such as EC2 instances. It also checks the health of its registered targets (by default, the root path is set to ‘/’; however, it can be changed to a different path of application). Application Load Balancer (ALB) is client-facing. It routes the traffic to healthy targets only. It can be easily integrated with Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Container Service for Kubernetes (Amazon EKS), AWS Fargate, and AWS Lambda.\n\nOnce security and target group is created, create a load balancer. Go to the Load balancing section again and choose Load Balancers.\n\nServices | All Services | EC2 | Load balancing | Load Balancers\n\nThen, hit the Create Load Balancer button. The following figure shows the main page of the load balancer:\n\nDeploying ML Models on Amazon Web Services  359\n\nFigure 13.28: Application Load Balancer-Create Load Balancer\n\nAfter clicking the Create Load Balancer button, it will show three options with brief descriptions and architecture to choose from, as shown in the following figure:\n\nFigure 13.29: Application Load Balancer–Select load balancer type\n\nThe application load balancer distributes the incoming requests across targets, such as containers, microservices, and EC2 instances, depending on the requests. Before passing the incoming requests to the targets, it evaluates whether the listeners’ rules are configured.\n\nIn the current scenario, choose the first type, which is Application Load Balancer, by hitting the Create button.\n\nNow, configure the load balancer. First, specify the name for the load balancer mlapp-lb, where lb stands for the load balancer. Next, choose the scheme as Internet-facing. The main difference between the Internet-facing and the Internal scheme is the internet. An Internet-facing load balancer routes the requests coming from clients over the internet to specified targets. On the other hand, an Internal load balancer will only route the request from clients with private IPs to specified\n\n360  Machine Learning in Production\n\ntargets. Select the IP address type as IPv4, which is used by specified subnets. The following figure shows the basic configuration of the Application Load Balancer:\n\nFigure 13.30: Application Load Balancer–Basic configuration\n\nThen, configure the network mapping. Network mapping enables the load balancer to route incoming requests to specific subnets and IP addresses. For mapping, select at least two availability zones and one subnet per zone where the load balancer can route the traffic. By default, it will display the available zones for selection. A load balancer will route the requests to targets in these availability zones only. Choose the default VPC and two Subnet (2a and 2b) from the dropdown, as shown in the following figure:\n\nDeploying ML Models on Amazon Web Services  361\n\nFigure 13.31: Application Load Balancer–Network mapping\n\nAfter that, remove the default security group and choose the security group from the dropdown created in the previous section. A security group can also be created through the Create new security group link. It will take you to the main page of the security group only, as seen in the previous section. The following figure shows the security group mlapp-lb-sg chosen for the load balancer:\n\nFigure 13.32: Application Load Balancer–Security groups\n\n362  Machine Learning in Production\n\nFinally, choose the target group you created earlier, that is, mlapp-tg with port 80, as shown in the following figure:\n\nFigure 13.33: Application Load Balancer–Listeners and routing\n\nScroll down and hit the Create load balancer button in the bottom-right corner. The following figure shows the main page of the load balancer, which shows the newly created load balancer is created:\n\nFigure 13.34: Application Load Balancer–mlapp-lb\n\nDeploying ML Models on Amazon Web Services  363\n\nNow, first, delete the service if you created it earlier, as the load balancer setting cannot be added after the creation of the service. Next, create a service with a load balancer.\n\nService An AWS ECS service allows you to keep running a specified number of instances of a task definition simultaneously in an AWS ECS cluster. If the task(s) fails or stops due to any reason, the ECS service will launch another instance of task definition to ensure that the desired number of tasks are up and running. The benefit of this is that the application will be up and running despite the failure of any task.\n\nGo to the Services tab on the mlapp-cluster page and hit the Create button to create a new service.\n\nService creation involves four steps, as shown in the following figure. The configuration and setting for each step are discussed in the following figure:\n\nFigure 13.35: ECS Service creation steps\n\nStep 1: Configure service\n\nIn this step, select the launch type. As the ECS cluster is configured with FARGATE, choose the launch type as FARGATE. The operating system family will be Linux.",
      "page_number": 378
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 389-403)",
      "start_page": 389,
      "end_page": 403,
      "detection_method": "topic_boundary",
      "content": "364  Machine Learning in Production\n\nChoose Task Definition as mlapp and Cluster as mlapp-cluster from the dropdown as shown in the following figure:\n\nFigure 13.36: ECS Service–Configure service\n\nNext, in the same step, issue the name for the service; in this scenario, it is mlapp- cicd, and Set the Number of tasks to 2, which refers to the desired number of tasks that will be influenced by the service. However, this count can be updated (1 and above) as per the requirement. The remaining options can be kept as it is. The following figure is the continuation of step 1:\n\nFigure 13.37: ECS Service–Configure service\n\nDeploying ML Models on Amazon Web Services  365\n\nStep 2: Configure network\n\nIn this step, the first choose Cluster VPC and the Subnets associated with it from the dropdown. Again, choose two subnets, that is, 2a and 2b, similar to standalone tasks but with additional configuration, as shown in the following figure. Make sure the Auto-assign public IP is ENABLED. This will allow you to assign available public IPs to access the ML app from anywhere.\n\nFigure 13.38: ECS Service–Configure network\n\nNext, click on the Edit button in the security groups. It will open a window to configure security groups. Here, the existing security group is chosen, which you created for running independent tasks. It has inbound rules defined for ports 80\n\n366  Machine Learning in Production\n\nand 8000, as shown in the following figure. Save this configuration for the security group.\n\nFigure 13.39: ECS Service–Configure security groups\n\nThen, select Application Load Balancer as the load balancer type. This enables it to distribute across the tasks running in the service without letting the end user know. A load balancer of the type Application Load Balancer allows containers to use dynamic host port mapping, that is, multiple tasks are allowed per container instance. Multiple services can use the same listener port on a single load balancer with a rule-based routing path.\n\nFigure 13.40: ECS Service – Load balancer type\n\nDeploying ML Models on Amazon Web Services  367\n\nAfter that, choose the Load balancer name from the dropdown as shown in the following figure. A load balancer will listen to HTTP port 80. Choose the Target group name as mlapp-tg from the dropdown, which you created and configured in the load balancer section.\n\nFigure 13.41: ECS Service–target and port\n\nStep 3: Set Auto Scaling (optional)\n\nAuto-scaling is optional. This will automatically update the service’s desired count within a specified range based on CloudWatch alarms. In the current scenario, the default is used, that is, Do not adjust the service’s desired count. Click on Next step.\n\nStep 4: Review\n\nIn this step, review the configuration for the service. Here, you can go to the previous step to update the configuration. If it looks fine, then go ahead and create a service by hitting the Create Service button, as shown in the following figure:\n\nFigure 13.42: ECS Service–Review\n\nAfter step 4, the service will be created; however, it will take a few minutes to create the desired number of tasks for that service. The progress of task creation can be\n\n368  Machine Learning in Production\n\nchecked under the Events and Logs tab. After a few minutes, the tasks’ status will change to RUNNING, as shown in the following figure:\n\nFigure 13.43: ECS Service–mlapp-cicd\n\nNow, click on the first task, copy its public IP, and enter the public IP, followed by port 8000, in the browser. It should display a message, as shown in the following figure:\n\nFigure 13.44: ECS Service–Accessing ml app with a public IP of the first task\n\nGo back and click on the second task, copy its public IP, and enter the public IP followed by port 8000 in the browser. It should display a message, as shown in the following figure:\n\nFigure 13.45: ECS Service–Accessing ml app with a public IP of the second task\n\nDeploying ML Models on Amazon Web Services  369\n\nThe good thing about service over standalone tasks is that if any one of the tasks fails or shuts down due to any reason, the service will replace it with a new task to maintain the desired number of tasks up and running. The issue with a service without a load balancer is that when it replaces a task or creates a new one, the public IP will get changed, which makes it a bit difficult to keep a track of the latest or running tasks for that service.\n\nThese public IPs can be accessed by the external world through the internet. However, you can restrict access to the public IP of tasks by updating inbound rules in the security group of service mlapp-sg. Go to the mlapp-sg security group, remove inbound rules (if they exist), add a new rule and specify container port, that is, 8000 in Port range, and choose the security group of the load balancer, that is, mlapp- lb-sg instead of 0.0.0.0/0 or anywhere in the source. Finally, you can save the rules. This will restrict the direct access to public IPs of tasks for the external world through the internet.\n\nLet’s check the deployed containerized app with an application load balancer. Go to the load balancer page, copy its DNS name, and run it on the browser (paste and enter). It should display a text message. This is the root page of the application.\n\nThen, pass the input data in <ALB DNS name>/docs and check the prediction output, as shown in the following figure. Here, you don’t need to pass the port and IP address as the load balancer is configured with port and target group, which will handle dynamic IPs of tasks.\n\nFigure 13.46: Application Load Balancer–Prediction response of ML app\n\nNote: A load balancing setting can only be set on service creation.\n\nCI/CD pipeline using CodePipeline An AWS comes with a set of tools and services for the CI/CD pipeline. In the previous section, the build was triggered manually with CodeBuild. To automate the manual process, you must build the CI/CD pipeline with an automated trigger.\n\n370  Machine Learning in Production\n\nTo achieve this, AWS CodePipeline will be used. Before creating a CI/CD pipeline, make sure previous services and topics are studied and implemented on the AWS cloud. An AWS CodePipeline will connect those services to create an automated pipeline. AWS services like CodeCommit, ALB, ECS FARGATE cluster, and such need to be created and configured. Make sure you provide the necessary permissions to services wherever applicable. For the current scenario, any external service, such as GitHub, is not being used. However, you can integrate external services or third- party services into this process. Here, AWS services are leveraged to deploy an application.\n\nWhen there is any update pushed to CodeCommit, it will trigger the CI/CD pipeline. A CodeBuild configuration file buildspec.yaml will first run the tests. Next, log in to Amazon ECR, then build and push Docker container images to the Elastic Container Registry (ECR). It will also write a container image definitions file imagedefinitions. json, in which a repository, followed by an image tag, will be captured. This container image definitions file will be stored in an Amazon S3 bucket as an artifact. A CodeBuild will export the test reports (.xml) in the pytest_reports/ directory. These test results can be seen on the CodeBuild page. After that, in the deploy stage, specify the ECS cluster details. ECS cluster will pull the latest container image from Elastic Container Registry (ECR). Finally, the containerized app will be deployed into the ECS cluster. Logs will be captured in CloudWatch.\n\nWhen the client or end user will access this ML app through Domain Name System (DNS) name, DNS will translate the human-readable name to numerical IP addresses that the machine can understand. This request will go to Application Load Balancer (ALB). Next, the Application Load Balancer (ALB) will check the specified listener's rules, such as port numbers and IP addresses. Then, it will send incoming requests to a specific target group which will route requests toward running tasks located in the Elastic Container Service (ECS) FARGATE cluster. The application will process the request and send the prediction to the client or end user.\n\nThe following figure shows the architecture of the automated CI/CD pipeline to deploy the containerized app on Elastic Container Service (ECS):\n\nDeploying ML Models on Amazon Web Services  371\n\nFigure 13.47: CI/CD pipeline with ECS reference architecture\n\nAWS CodePipeline A pipeline is a workflow that is responsible for automating the deployment or release of the application. A CodePipeline is a fully managed continuous delivery (CD) service in the AWS cloud. It enables faster and easy deployment of an application. It automates the different phases involved in the CI/CD process, such as build, test, and deploy. Each phase comprises a series of actions to be performed. AWS CodePipeline can easily integrate with third-party services like GitHub.\n\n372  Machine Learning in Production\n\nThe following figure shows the flow of CodePipeline and AWS services used to create a CI/CD pipeline.\n\nFigure 13.48: CI/CD pipeline using CodePipeline\n\nGo to CodePipeline from Services.\n\nServices | All Services | Developer Tools | CodePipeline\n\nLet’s start by creating a pipeline. Hit the Create pipeline button. It consists of 5 steps, namely:\n\nChoose pipeline settings •\t Add source stage •\t Add build stage •\t Add deploy stage •\t Review\n\nStep 1: Choose pipeline settings\n\nIn this step, you need to specify the Pipeline name as mlapp-cicd. A pipeline name cannot be changed after creation. Choose a New service role for CodePipeline. Check to Allow AWS CodePipeline to create a service role so it can be used with this new pipeline checkbox. The Role name should auto populate.\n\nDeploying ML Models on Amazon Web Services  373\n\nFigure 13.49: CodePipeline – Choose pipeline settings\n\nStep 2: Add source stage\n\nIn this step, you need to specify the details of the source from which the latest code and updates are to be pulled. Choose AWS CodeCommit as a Source provider. Next, choose the Repository name as loan_pred. Then, choose the main branch from which the latest code and updates are to be pulled. From the Change detection options, choose Amazon Cloudwatch Events (recommended) as shown in the\n\n374  Machine Learning in Production\n\nfollowing figure. This will trigger the pipeline as soon as it detects the changes in the source repository’s branch.\n\nFigure 13.50: CodePipeline–Add source stage\n\nStep 3: Add build stage\n\nHere, first off, you need to choose the AWS CodeBuild as the Build provider. Alternatively, Jenkins can be chosen as the Build provider. The region US West (Oregon) is chosen. Next, choose the build Project name that you have already created in the CodeBuild console, or create a build project in the CodeBuild console. The Create project link will redirect you to the CodeBuild console. In the Build type section, you need to choose Single build as it is expected to trigger a single build.\n\nDeploying ML Models on Amazon Web Services  375\n\nFigure 13.51: CodePipeline–Add build stage\n\nStep 4: Add the deploy stage\n\nIn this step, provide deployment details where the application needs to deploy. This step depends on the previous step. Once the build is complete, this step will deploy the application to Elastic Container Service (ECS) cluster.\n\nFirst, choose the deploy provider as Amazon ECS. The region is US West (Oregon). Then, choose the cluster name that you have already created in the Amazon ECS console or create a cluster in the Amazon ECS console and complete this step. After\n\n376  Machine Learning in Production\n\nthat, specify the cluster and service name that was created in the previous section. Refer to the following figure for step 4 configuration.\n\nFigure 13.52: CodePipeline–Add deploy stage\n\nStep 5: Review\n\nIn this step, review the configuration of the previous steps.\n\nIf the configuration is fine, hit Create pipeline.\n\nRun CodePipeline By default, it will start building the pipeline for the first time soon after its creation. When any updates are pushed to the source, that is, AWS CodeCommit, pipeline execution will begin. You can see the progress and status of the pipeline on the main page of CodePipeline. Go to pipeline mlapp-cicd, and you can see the status of each phase. It displays the link to that service. For instance, the progress of the build can be seen through the link under the Build step.\n\nGo to the details in the Build section; you can see the source and submitter details. In the following figure, who started the build job and the source version are displayed. In the first row, the build job was started by CodePipeline, and in the next row, the\n\nDeploying ML Models on Amazon Web Services  377\n\nbuild job was started manually by the IAM user.\n\nFigure 13.53: CodePipeline–Build status\n\nThe main page of CodePipeline is shown in the following figure. It displays details like the latest status of the pipeline and the latest source revision.\n\nFigure 13.54: Running a CodePipeline-Succeeded\n\nTo access the app, you can use the DNS name shown on the main page of the load balancer. A DNS name will not change even if a new task is created or if the public\n\n378  Machine Learning in Production\n\nIP of the task changes. The following figure shows the prediction after passing the input data using FastAPI UI.\n\nFigure 13.55: Running a CodePipeline–ML app prediction using DNS of ALB\n\nMonitoring Monitoring is an essential part of the deployment. It is recommended to monitor ECS clusters and other integrated services to ensure the high availability of applications. This enables you to check the overall health of ECS containers. First, create a plan for the resources and services that need to be monitored. Next, decide the action to be taken if a specified event is detected. Decide who should be notified of the event or error, where to monitor metrics and events, and so on.\n\nECS provides cluster-level statistics, such as:\n\nCPU Utilization–Current% of CPU utilized by the ECS cluster •\t Memory Utilization–Current% of Memory utilized by the ECS cluster\n\nAmazon ECS metric data is automatically sent to CloudWatch in 1-minute periods that get captured for 2 weeks. Amazon CloudWatch Container Insights allows you to aggregate and analyze metrics and logs from containerized applications. It helps to monitor utilization metrics such as CPU, memory, disk, and network. It also provides diagnostic information about container restart failures.\n\nYou can create an AWS Lambda function that will get triggered if a specified event is detected and will send the alert to the slack channel. This way, team members and developers will be notified about the alert.\n\nThus, you have learned to create a simple CI/CD pipeline using CodePipeline to deploy the ML app on the Amazon ECS Fargate cluster. However, you can modify it as per business and application requirements. Reference figures in this chapter are from classic UI. If you see a new UI for creating and managing services, you can switch back to the classic UI, or you can continue with the new UI. The underlying flow and services will remain the same for the new UI, but you might see selection",
      "page_number": 389
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 404-411)",
      "start_page": 404,
      "end_page": 411,
      "detection_method": "topic_boundary",
      "content": "Deploying ML Models on Amazon Web Services  379\n\nchanges. Alternatively, you can use AWS CloudFormation or Terraform to automate the creation of infrastructure or services.\n\nConclusion In this chapter, you created an Amazon Elastic Container Service (ECS) cluster with Fargate to deploy the ML app. First off, you created an AWS account and created a codebase including a buildspec.yaml file for Amazon Elastic Container Service (ECS) deployment. You added the Continuous Training stage in CI/CD pipeline and accuracy test cases in the pytest file. Next, you pushed the codebase and dependencies to AWS CodeCommit. After that, you created AWS Elastic Container Registry (ECR) to store and manage Docker container images, and you used AWS CodeBuild to build and push the Docker images to AWS Elastic Container Registry (ECR) using buildspec.yaml. You created an Application Load Balancer (ALB) with a security and target group to distribute incoming traffic across the Fargate tasks, created ECS Service with a security group, and added an Application Load Balancer (ALB) to it. Finally, you created AWS CodePipeline and integrated these services to automate ML app deployment.\n\nIn the next chapter, you will study the drift in ML models and the monitoring process for the deployed model.\n\nPoints to remember\n\nAmazon Elastic Container Service (ECS) offers deployment models: EC2 and Fargate.\n\nApplication Load Balancer (ALB) can be specified at the time of service creation only.\n\nApplication Load Balancer (ALB) checks the health of its registered targets (by default, the root path is set to ‘/’; however, it can be changed to a different path of application) and routes the traffic to healthy targets only.\n\n\n\nIt is recommended to install and configure the latest version of AWS CLI and Docker on the local machine.\n\nAmazon ECS cluster is a logical grouping of tasks or services. •\t Create one or more IAM users with the required permissions to run the services and avoid using root user to run the services directly.\n\n380  Machine Learning in Production\n\nMultiple choice questions\n\n1. _________ is serverless pay-as-you-go compute for containers.\n\na) EC2\n\nb) Fargate\n\nc) CloudWatch\n\nd) Amazon Lightsail\n\n2. The target group for load balancing _________\n\na) takes the request from Application Load Balancer (ALB)\n\nb) routes the request to specified targets\n\nc) handles dynamic IP of the tasks\n\nd) all of the above\n\nAnswers 1. b\n\n2. d\n\nQuestions\n\nWhat is a task in the Amazon ECS cluster? •\t What is the use of an Application Load Balancer (ALB)? •\t What are ECS services and their role?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com\n\nMonitoring and Debugging  381\n\nChapter 14 Monitoring and Debugging\n\nIntroduction So far, you have learned various techniques to deploy ML models in production. However, deploying a model into production is not the end; monitoring is the next important step. This chapter talks about concepts and techniques of model monitoring. Model monitoring is not limited to the final endpoint where the model is deployed; you should also integrate it into the intermediate stages wherever required. The static ML model is trained offline on historical data. However, this model may not deliver consistent performance forever. The main reasons could be changes in input data, business requirements, and model degradation over time.\n\nStructure The following topics will be covered in this chapter:\n\n\n\nImportance of Monitoring\n\nFundamentals of ML monitoring •\t Metrics for monitoring your ML system •\t Drift in ML - Types and detection techniques •\t Operational monitoring using Prometheus and Grafana •\t ML model monitoring with whylogs and WhyLabs\n\n382  Machine Learning in Production\n\nObjectives After studying this chapter, you can start monitoring ML models. You should be able to understand the importance and fundamentals of monitoring, decide on the metrics to monitor for your project and understand the concept of drift in ML, its types, and techniques to detect the drift. You should also be able to monitor drift in the data using whylogs, an open-source lightweight library, and WhyLabs, an observability platform. You should be able to integrate Prometheus and Grafana with FastAPI, and operational monitoring with open-source tools, that is, Prometheus and Grafana.\n\nImportance of monitoring Once an ML model is deployed in production, it is essential to monitor it in order to ensure that the model’s performance stays up to the mark and it continues to deliver reliable output seamlessly. As a matter of fact, there are many reasons for failure in ML apps or services, such as pipeline failure, model degradation over time, change in model input data, system or server failure, and change in the schema.\n\nMultiple teams are involved while deploying models to production, which usually includes a team of Data Scientists, Data Engineers, and DevOps. If prediction errors increase, who will be held responsible? Who is the owner of the model in production?\n\nDue to COVID-19, banks’ ML model's performance was heavily affected. The model’s predictions were far away from the actual figures. This is the case of a shift in input data. This should give you an overview of the challenges post-deploying models into production and the necessity of monitoring ML models.\n\nMonitoring is essential when it comes to comparing new and old models’ performance and their predictions over time. There could be latency issues, that is, delayed output. For tracking and investigating latency issues, efficient monitoring is needed. You need to monitor input data to ensure that production input data is being processed like the training data. To track and handle extreme values, out of the range values, or special cases before passing them to the models, monitoring is required. Last but not least is model security, that is, monitoring is required to track any external attack on the model or system.\n\nThe objectives of ML monitoring are as follows:\n\nTo detect the issues or failures at early stages so that necessary steps can be taken\n\nTo keep a track of resource usage and model prediction to evaluate model and system performance in the production environment\n\nMonitoring and Debugging  383\n\nTo detect the change in distribution, schema, and anomalies in input data that may cause an error in the predictions\n\nTo ensure the availability of model predictions and that they are explainable •\t To track and store metrics in specified storage or database\n\nModel monitoring helps data scientists to reduce model failures, avoid downtime, and ensure reliable outcomes for users.\n\nFundamentals of ML monitoring ML monitoring refers to tracking the performance, errors, metrics, and such of deployed models and sending alerts (when required) to ensure models continue performing above an acceptable threshold. It is not limited to tracking input data or model degradation. However, it should take all the things into account that may affect model performance directly or indirectly. ML monitoring helps decide whether an existing model needs to be updated.\n\nThe following are the essential steps to consider:\n\nMonitoring is the key: Monitoring is essential once a model is deployed in production. Monitoring enables you to track and fix issues or errors in a faster manner. Monitoring can be broadly divided into two types: o Functional monitoring: In the case of ML, functional monitoring refers to tracking metrics, errors, and performance related to ML models, such as accuracy and outliers.\n\no Operational monitoring: Operational monitoring refers to tracking system-specific metrics, such as CPU and RAM utilization, uptime, and throughput.\n\nScalable integration: Monitoring should be easy to integrate with your existing infrastructure and workflow. The monitoring system should be seamless and scalable to integrate. It should be able to track multiple models if there is more than one model. The monitoring solution should be platform agnostic so that it can be used for different types of deployment, having different tech stacks.\n\nMetrics tracking: Tracking accuracy is not enough. Monitoring systems should be able to track all the metrics that can affect model and system performance over time. A centralized monitoring system should use multiple performance metrics to give the overall status of the solution. Use different metrics for different types of features. For instance, min, max, mean, standard deviation, and outliers can be used for numerical data. The metrics records and logs need to be stored in a database so that they can be analyzed later on.\n\n384  Machine Learning in Production\n\nAlert system for important events: You cannot track 100+ metrics manually 24x7; hence, the alert system should be part of the monitoring solution. Not everything needs an alert. You may be tracking 20+ metrics; however, only a few metrics need high attention. For instance, if input data drift is exceeding the threshold, then the alert system should send the notification to the responsible team or Data Scientists. The whole purpose of the alert system is to notify concerned teams or individuals so that issues or errors can be avoided or can be solved at the earliest to ensure consistency in ML model performance.\n\nRoot cause analysis and debugging: Once you get the alert, you may need to act on it. You can start the root cause analysis to determine the issue and fix it by debugging. For instance, if model accuracy is going below the threshold, it could be because of a change in the distribution of input data or anomalies.\n\nTo design an efficient monitoring system, you need to consider existing pipelines and infrastructure. There are mainly three pillars of monitoring:\n\nProcessing and Storage: It processes and stores the critical metrics in a database or data source with a timestamp. You can query the metrics when required.\n\nGraphs and Dashboard: It will query or fetch the monitoring metrics from a connected database or data source. You can choose the visualization as per the type of metrics. Here, you have to decide the metrics that need to be displayed. It should summarize overall monitoring with intuitive graphs. •\t Alert: Finally, it should send an alert to the concerned team or data scientists to take immediate action, if required. This helps to prevent future failures or mitigate the impact.\n\nThe following figures show the three pillars of the monitoring system for effective monitoring:\n\nFigure 14.1: Pillars of the monitoring system\n\nMonitoring and Debugging  385\n\nBefore setting up any monitoring system, here is the list of questions that need to be answered:\n\nWhat do you plan to monitor? •\t What tools and language are you using? •\t What platform or library will be used for monitoring? •\t How do you plan to integrate the monitoring setup with the existing environment and tools?\n\nWhat threshold is to be used for alerts? •\t What action should be taken after detecting an issue or failure?\n\nFinally, as per the model life cycle, monitoring should complete the feedback loop, that is, after detecting an issue or failure, it should send a notification to the concerned team or data scientists so that they can take necessary action, such as retraining the ML model.\n\nMetrics for monitoring your ML system You need to decide which type of metric to monitor. Operational metrics will enable developers to track system or server-related failures or warnings, such as high resource usage that can increase the latency, which will affect end-user experience. Tracking operational metrics is essential because the server should be in good condition to run the ML model and other tasks. Another factor is the cost. Operational teams have to set limits on the cost and need to track which resource or service causes higher costs.\n\nHere are a few important operational metrics:\n\nSystem or Server\n\no Resource usage\n\no Availability\n\no Latency\n\no Throughput\n\nCost o\n\nInfrastructure cost\n\no Storage cost\n\no Additional service (if any)\n\nOnce the server is up and running without any issues, you can focus on functional or ML model metrics. You will study the detection of model monitoring metrics and ways to address them in this chapter.\n\n386  Machine Learning in Production\n\nThe following are a few important model metrics:\n\n\n\nInput data\n\no Model version\n\no Data drift\n\no Outliers\n\nML model\n\no Model version\n\no Model hyperparameters\n\no Metadata\n\no Predictions or Output\n\no Classification model evaluation metrics\n\n▪ Accuracy\n\n▪ Confusion Matrix\n\n▪ ROC-AUC Score\n\n▪ Precision and Recall Scores\n\n▪ F1-Score\n\no Regression model evaluation metrics\n\n▪ Root Mean Square Error (RMSE)\n\n▪ R-Squared and Adjusted R-Square\n\n▪ Mean Absolute Error (MAE)\n\n▪ Mean Absolute Percentage Error (MAPE)\n\nPrediction drift\n\nYou can decide which metrics to track closely and which metrics need alerts.\n\nDrift in ML Model drift means a change in the behavior of ML models or that the model is not performing as expected or per the Service Level Agreement (SLA). Model performance may degrade after deploying it to production, as the model can receive data that was not introduced during model training.\n\nTypes of drift in ML There are mainly three types of drift in ML:",
      "page_number": 404
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 412-427)",
      "start_page": 412,
      "end_page": 427,
      "detection_method": "topic_boundary",
      "content": "Monitoring and Debugging  387\n\nData drift •\t Prediction drift •\t Concept shift\n\nData drift\n\nData drift is when the distribution or characteristics of input features change with reference to training data. It is also known as feature drift, population drift, or covariate shift. If the distribution of input data changes, then its predictions might get affected as the model is not prepared for it.\n\nIf a new category gets added to the feature post-deployment, then it may cause an error while making the prediction, as it was not there at the time of model training.\n\nAnother example could be, suppose there is a feature called Credit Rating in training data, and it has high weightage, which means a change in this feature may cause a major change in model output. Now, the business decided to go with Moody’s credit rating instead of S&P. This will cause a change in input data. For instance, S&P’s AA- is equivalent to Moody’s Aa3 rating.\n\nThe following equation shows that the distribution of training data does not match the distribution of reference (production) data.\n\nMathematically, data drift can be defined as follows:\n\nP(X) ≠ Pref(X)\n\nWhere P(X) denotes the input data probability distribution.\n\nPrediction drift\n\nAs input data changes with data drift, this may cause a change in the target or prediction variable. It refers to a change in predictions over time. It is also named a prior probability shift, label drift, or unconditional class shift. This can also occur due to the removal or addition of new classes. Retraining the model can help mitigate the model degradation owing to prediction drift.\n\nMathematically, prediction drift can be defined as follows:\n\nP(Y) ≠ Pref(Y)\n\nWhere P(Y) denotes the prior probability distribution of target labels.\n\nConcept shift\n\nA concept shift occurs when the relationship between independent variables and dependent or target variables changes. It is also known as posterior class shift, conditional change, or real concept drift. It refers to changes in the relationship\n\n388  Machine Learning in Production\n\nbetween the input variables and the target variables. If you detect any significant concept shift, it is very likely that your model’s predictions are unreliable. A Concept in Concept shift refers to the relationship between independent and dependent variables.\n\nFor instance, a car insurance company changes its claim policy that a claim for a specific part of the car will be rejected. Suppose that part of the car got damaged in an accident, the customer’s claim for it will be rejected. In this scenario, mapping between the input feature and target feature changes even though the distribution of input data remains the same.\n\nMathematically, concept shift can be defined as follows:\n\nP(Y|X) ≠ Pref(Y|X)\n\nWhere P(Y|X) denotes the posterior probability distribution of the target labels.\n\nFollowing are the patterns of concept shift:\n\nFigure 14.2: Different patterns of concept shift\n\nTechniques to detect the drift in ML Statistical distance measurement using distance metrics between two distributions is useful for detecting drift in ML.\n\nIf there are many independent variables in the dataset, then you can use dimensionality reduction techniques, such as PCA. Tracking many features can increase the load on the monitoring system and sometimes it becomes difficult to mitigate drift by targeting specific features.\n\nBasic statistical metrics, such as mean value, standard deviation, correlation, and minimum and maximum values comparison, can be used for calculating the drift between training and current independent variables.\n\nDistance measures like Population Stability Index (PSI), Characteristic stability index (CSI), Kullback–Leibler divergence (KL-Divergence), Jensen–Shannon divergence (JS-Divergence), and Kolmogorov-Smirnov (KS) statistics can be used for continuous features.\n\nMonitoring and Debugging  389\n\nThe cardinality checks, Chi-squared test, and entropy can be used for categorical variables.\n\nControl charts and histogram intersections can be used to detect a drift in data.\n\nFinally, there are several platforms for model monitoring, such as WhyLabs, and libraries, such as deepchecks, and alibi-detect. They come with easy integrations and a ready framework for drift detection. The best part about this is that most of them provide a drift detection framework, storage for logs and historical data, intuitive monitoring dashboards, and alert mechanisms for critical events.\n\nAddressing the drift in ML Once you detect the drift in the ML model, it can be addressed in the following ways:\n\nData quality issues\n\nIf there is an issue with input data, then it can be easily fixed. For instance, high- resolution images were provided for training face recognition models; however, low-resolution images were passed to the deployed model.\n\nRetraining the model\n\nAfter detecting the data or concept shift, retraining the model with recent data can improve its performance. Sometimes production data is not sufficient to train the model. In that case, you can combine historical data with recent production data and give more weight to recent data.\n\nHere are four strategies for retraining the model:\n\nPeriodically retraining: Scheduling it at a fixed time, for instance, every Monday at 10 PM\n\nData or event-driven: When new data is available •\t Model or metric driven: When accuracy is lower than a threshold or SLA •\t Online learning: Where the model continuously learns in real-time or near real-time on the latest data\n\nRebuilding or tuning the model\n\nIf retraining the model doesn’t work, then you may need to consider rebuilding it or tuning it on recent data. You can automate this using a pipeline.\n\n390  Machine Learning in Production\n\nOperational monitoring with Prometheus and Grafana Prometheus is an open-source system used for event monitoring and alerting. It scrapes the real-time data from instrumented jobs and stores it with a timestamp in the database. The word instrument refers to the use of a client library that allows Prometheus to track and scrape its metrics and store them locally. Prometheus offers client libraries that can be used to instrument your application. In this scenario, you will be using the Prometheus Python client in the FastAPI application to expose its metrics that need to be tracked.\n\nA Prometheus server scrapes and stores metrics from instrumented jobs in time series format. This data can be fetched using PromQL - query language, and it can be used to visualize the metrics. It comes with an alert manager to handle alerts.\n\nAccording to GrafanaLabs, Grafana is an open-source interactive visualization and monitoring platform that enables users to visualize metrics, logs, and traces collected from deployed applications. Grafana is easy to integrate with most common databases, such as Prometheus, Influx DB, ElasticSearch, MySQL, and PostgreSQL.\n\nAs Grafana is an open source tool, hence you can write an integration plugin from scratch to connect with multiple data sources. The Grafana dashboard fetches the data from connected data sources and allows you to pick up the visualization type from plenty of visualization options, such as heat maps, bar charts, and line graphs. You can easily run the query, visualize metrics, and set up alerts for critical events.\n\nIn this scenario, you will be using Grafana and Prometheus. Both Prometheus and Grafana are open-source tools. As a matter of fact, Prometheus and Grafana are popular combinations in the industry for monitoring systems. Grafana dashboard will be used for visualization and alert management. It will fetch the data from the Prometheus database for querying metrics and will display the intuitive visualization on the dashboard.\n\nPrometheus and Grafana can be separately installed and configured on a local machine or a remote server. However, you will be using docker images of Prometheus and Grafana by executing the docker-compose.yaml file.\n\nTo maintain consistency, similar files from the previous chapters with minor modifications will be used. The files required for Prometheus and Grafana dashboards are added as shown in the following file structure:\n\n.\n\n├── .gitignore\n\n├── config.monitoring\n\n├── datasource.yml\n\n├── docker-compose.yaml\n\n├── Dockerfile\n\n├── fastapi-dashboard.json\n\n├── LICENSE\n\n├── main.py\n\n├── prometheus.yml\n\n├── pytest.ini\n\n├── README.md\n\n├── requirements.txt\n\n├── runtime.txt\n\n├── start.sh\n\n├── test.py\n\n├── tox.ini\n\n├── app\n\n└── src/\n\n├── MANIFEST.in\n\n├── README.md\n\n├── requirements.txt\n\n├── setup.py\n\n├── tox.ini\n\n└── prediction_model/\n\n├── pipeline.py\n\n├── predict.py\n\n├── train_pipeline.py\n\n├── VERSION\n\n├── __init__.py\n\n├── config/\n\n│ ├── config.py\n\n│ └── __init__.py\n\n├── datasets/\n\n│ ├── test.csv\n\n│ ├── train.csv\n\n│ └── __init__.py\n\nMonitoring and Debugging  391\n\n392  Machine Learning in Production\n\n├── processing/\n\n│ ├── data_management.py\n\n│ ├── preprocessors.py\n\n│ └── __init__.py\n\n├── trained_models/\n\n│ ├── classification_v1.pkl\n\n│ └── __init__.py\n\n└── tests/\n\n└── pytest.ini/\n\n└── test_predict.py\n\nconfig.monitoring\n\nThe extension of this file is .monitoring. In this file, configure the admin password and user permission for sign up.\n\n1. GF_SECURITY_ADMIN_PASSWORD=pass@123\n\n2. GF_USERS_ALLOW_SIGN_UP=false\n\nThe Docker container currently allows new Grafana users to sign up, so anyone can create an account and view the dashboard. Therefore, for security purposes, the admin password and allowing new users to sign up should be set to false.\n\nprometheus.yml\n\nThis file contains Prometheus configurations, such as scrape_configs and scrape_ intervals.\n\n1. # Global config\n\n2. global:\n\n3. scrape_interval: 15s\n\n4. evaluation_interval: 15s\n\n5. external_labels:\n\n6. monitor: \"app\"\n\n7.\n\n8. rule_files:\n\n9.\n\n10. scrape_configs:\n\n11. - job_name: \"prometheus\"\n\nMonitoring and Debugging  393\n\n12.\n\n13. static_configs:\n\n14. - targets: [\"localhost:9090\"]\n\n15.\n\n16. - job_name: \"app\"\n\n17. dns_sd_configs:\n\n18. - names: [\"app\"]\n\n19. port: 8000\n\n20. type: A\n\n21. refresh_interval: 5s\n\nIn this file, the scrape interval is set to 15 seconds (15s) in the global config, which means it will scrape the metrics data after every 15 seconds. It is targeting an app, that is, the FastAPI app here. Port is the app’s port, that is, 8000, and the refresh interval is set to 5 seconds (5s).\n\nDockerfile\n\nThis Dockerfile will install the dependencies, set the working directory, expose ports, and finally, run the ML app.\n\n1. FROM python:3.7-slim-buster\n\n2.\n\n3. ENV PYTHONPATH \"${PYTHONPATH}:src/\"\n\n4.\n\n5. WORKDIR /app/\n\n6.\n\n7. COPY . .\n\n8.\n\n9. RUN chmod +x /app\n\n10.\n\n11. COPY ./start.sh /start.sh\n\n12.\n\n13. RUN chmod +x /start.sh\n\n14.\n\n15. # Expose the port that uvicorn will run the app on\n\n394  Machine Learning in Production\n\n16. ENV PORT=8000\n\n17. EXPOSE 8000\n\n18.\n\n19. RUN pip install --upgrade pip\n\n20.\n\n21. RUN pip install --upgrade -r requirements.txt --no-cache-dir\n\n22.\n\n23. CMD [\"/start.sh\"]\n\ndocker-compose.yaml\n\nThis docker-compose file will set up and run three services:\n\nThe app, that is, FastAPI app: For the ML model. It will run on port 8000. •\t Prometheus server: It will run on port 9090. •\t Grafana dashboard: It will run on port 3000.\n\nIn the following file, you can see that the Grafana service depends on the Prometheus service:\n\n1. version: \"2.2\"\n\n2.\n\n3. services:\n\n4. app:\n\n5. build: .\n\n6. restart: unless-stopped\n\n7. container_name: app\n\n8. ports:\n\n9. - 8000:8000\n\n10. networks:\n\n11. example-network:\n\n12. ipv4_address: 172.16.238.10\n\n13.\n\n14. prometheus:\n\n15. image: prom/prometheus:latest\n\n16. restart: unless-stopped\n\nMonitoring and Debugging  395\n\n17. container_name: prometheus\n\n18. ports:\n\n19. - 9090:9090\n\n20. volumes:\n\n21. - ./prometheus.yml:/etc/prometheus/prometheus.yml\n\n22. command:\n\n23. - \"--config.file=/etc/prometheus/prometheus.yml\"\n\n24. networks:\n\n25. example-network:\n\n26. ipv4_address: 172.16.238.11\n\n27.\n\n28. grafana:\n\n29. image: grafana/grafana:latest\n\n30. restart: unless-stopped\n\n31. user: \"472\"\n\n32. container_name: grafana\n\n33. depends_on:\n\n34. - prometheus\n\n35. ports:\n\n36. - 3000:3000\n\n37. volumes:\n\n38. - ./datasource.yml:/etc/grafana/provisioning/datasource.yml\n\n39. env_file:\n\n40. - ./config.monitoring\n\n41. networks:\n\n42. example-network:\n\n43. ipv4_address: 172.16.238.12\n\n44.\n\n45. networks:\n\n46. example-network:\n\n47. # name: example-network\n\n396  Machine Learning in Production\n\n48. driver: bridge\n\n49. ipam:\n\n50. driver: default\n\n51. config:\n\n52. - subnet: 172.16.238.0/24\n\nmain.py\n\nThis file contains code for the FastAPI app. In this code, add the following two lines to instrument the FastAPI app. In Prometheus, the instrumentation refers to the use of a library in application code (in this scenario, it is FastAPI) so that Prometheus can scrape metrics exposed by the app.\n\nFirst, add prometheus-fastapi-instrumentator with version 5.7.1 to the requirements.txt file. Then, import the Prometheus FastAPI instrumentator as follows:\n\n1. from prometheus_fastapi_instrumentator import Instrumentator\n\nThen, at the end of the file, add the following line to instrument FastAPI app to the Prometheus FastAPI instrumentator:\n\n1. Instrumentator().instrument(app).expose(app)\n\nWith this single line, FastAPI will be instrumented and all Prometheus metrics used in the FastAPI app can be scraped via the added /metrics endpoint.\n\nYou have already studied the rest of the files in the previous chapters. Refer to the previous chapters for files mentioned in the preceding directory structure.\n\nYou will need to run the docker containers by running the following docker-compose command in the terminal:\n\ndocker-compose up\n\nNow you have access to three containers and their respective ports:\n\nFastAPI: Running at http://localhost:8000/ •\t Prometheus: Running at http://localhost:9090/ •\t Grafana dashboard: Running at http://localhost:3000/\n\nOn the FastAPI app, you can access the /metrics endpoint to view the data that Prometheus is scraping from it, as shown in the following figure:\n\nMonitoring and Debugging  397\n\nFigure 14.3: FastAPI metrics\n\nYou can access Prometheus UI on port 9000 and run the PromQL query. To get the total number of requests, execute prometheus_http_requests_total. You will get the HTTP status code, handler, instance, job name, and count, as shown in the following figure:\n\nFigure 14.4: Prometheus UI–Query execution\n\n398  Machine Learning in Production\n\nPrometheus allows you to view these query results in graphical format by switching to the Graph tab in Prometheus web UI.\n\nYou have seen Prometheus web UI, and PromQL is fetching the metric data as well. Now, in the next part, you will connect Prometheus to the Grafana dashboard as a data source. Metrics data fetched from the Prometheus server will be visualized on the Grafana dashboard. Go to Grafana dashboard web UI and click on the add your first data source option, as shown in the following figure:\n\nFigure 14.5: Grafana–Adding data source\n\nNext, select the Prometheus database from the Time series databases, as shown in the following figure:\n\nFigure 14.6: Grafana–Prometheus data source\n\nIn the settings, provide Prometheus as a name in the Name field. Then, mention http://prometheus:9090 in the URL field, as shown in the following figure, as Prometheus service is running on port 9090:\n\nMonitoring and Debugging  399\n\nFigure 14.7: Grafana–Prometheus configuration\n\nFinally, proceed with the basic configuration; however, you can also configure other fields. Scroll to the bottom of the page and hit the Save & test button, as shown in the following figure:\n\nFigure 14.8: Grafana–saving and testing Prometheus configuration\n\nIf it is configured properly, you should see the integration status success message after you save and test the configuration.\n\nOnce Prometheus integration is done, create a new dashboard with the New Dashboard option in Grafana web UI, as shown in the following figure:\n\nFigure 14.9: Grafana–creating a new dashboard\n\n400  Machine Learning in Production\n\nThe Grafana dashboards can be created from scratch or by importing JSON files from storage. Grafana also lets you import the dashboard via Grafana.com, as shown in the following figure:\n\nFigure 14.10: Grafana–uploading JSON file\n\nIn this scenario, import a JSON file from the directory. You can refer to one of the original public Grafana instances, hosted by Grafana Labs, using the following link:\n\nhttps://play.grafana.org/d/000000012/grafana-play-home?orgId=1\n\nFor various dashboard visualizations and sources, refer to the following link https:// grafana.com/grafana/dashboards/.\n\nNext, provide the dashboard name and Prometheus source, as shown in the following figure:\n\nFigure 14.11: Grafana–Prometheus as a data source\n\nMonitoring and Debugging  401\n\nFinally, open the newly created dashboard from the web UI. After importing the dashboard from the JSON file, you can customize the dashboard by changing the chart type, as shown in the following figure.\n\nFigure 14.12: Grafana–Dashboard\n\nThe next step is to send alerts for critical events. Grafana includes built-in support for the Prometheus Alert manager. Grafana provides mainly three strategies for setting alerts:\n\nCloud Alert manager: Cloud Alert manager runs in Grafana Cloud. It supports alerts from Grafana, Mimir, and Loki.\n\nGrafana Alert manager: Grafana Alert manager is an internal Alert manager. It is pre-configured and supports alerts from Grafana; however, it cannot receive alerts outside Grafana.\n\nAn external Alert manager: You can configure Grafana to use an external alert manager. This is useful when you are looking for a centralized alert manager that can receive alerts from different sources.\n\n402  Machine Learning in Production\n\nYou can add the alert rules, labels, and notification policy in Grafana. The following figure shows the Alert UI in Grafana:\n\nFigure 14.13: Grafana–Alerting UI\n\nML model monitoring with whylogs and WhyLabs WhyLabs is a model monitoring and observability platform built to monitor data drift, model performance degradation, data quality, model evaluation metrics, and sending alerts to teams or individuals. It is built on top of an open-source package whylogs. You can monitor whylogs profiles continuously with the WhyLabs Observability Platform.\n\nFollowing are the salient features of WhyLabs:\n\nMinimal setup time: You can start by installing lightweight open-source library whylogs with pip install whylogs. In no time, a summary of the dataset can be generated, that is, whylogs profiles, which helps you do the following: o Track changes in datasets\n\no Visualize the summary statistics of datasets\n\no Check the data quality by defining data constraints\n\nSeamless integration: It can be integrated with the existing data pipelines, model life cycle, model framework, and MLOps Ecosystem easily.\n\nData privacy: WhyLabs works with a statistical summary of the data created by the whylogs agent, so your actual data is not getting transferred or stored in the WhyLabs platform.\n\nScalability: It can be easily scaled up to handle large amounts of data and multiple projects.\n\nCentralized monitoring: With the WhyLabs platform, you can monitor multiple projects under a single umbrella. It allows you to create multiple isolated projects within a single account, and it also keeps track of input and output data, model performance, and such.",
      "page_number": 412
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 428-435)",
      "start_page": 428,
      "end_page": 435,
      "detection_method": "topic_boundary",
      "content": "Monitoring and Debugging  403\n\nwhylogs Without further delay, let’s start using the whylogs logging functionality. First, install whylogs using pip install whylogs if it’s not installed already. Next, import a loan prediction dataset and start logging it with whylogs.\n\n1. # Import packages\n\n2. import numpy as np\n\n3. import pandas as pd\n\n4. from sklearn.model_selection import train_test_split\n\n5. pd.set_option(\"display.max_columns\", None) #To show all columns in DataFrame\n\n6. import whylogs as why\n\n7.\n\n8. df = pd.read_csv(\"https://gist.githubusercontent. com/suhas-ds/a318d2b1dda8d8cbf2d6990a8f0b7e8a/ raw/9b548fab0952dd12b8fdf057188038c8950428f1/loan_dataset.csv\") 9.\n\n10. profile1 = why.log(df)\n\n11. profile_view1 = profile1.view()\n\n12. profile_view1.to_pandas()\n\nTo view the statistical summary, that is, the whylogs profile, convert it to the pandas DataFrame. The pandas DataFrame of the profile contains the following metrics:\n\n\n\ncardinality/est\n\n\n\ncardinality/lower_1\n\n\n\ncardinality/upper_1\n\n\n\ncounts/n\n\n\n\ncounts/null •\t distribution/max •\t distribution/mean •\t distribution/median •\t distribution/min •\t distribution/n •\t distribution/q_01\n\n404  Machine Learning in Production\n\ndistribution/q_05 •\t distribution/q_10 •\t distribution/q_25 •\t distribution/q_75 •\t distribution/q_90 •\t distribution/q_95 •\t distribution/q_99 •\t distribution/stddev •\n\nfrequent_items/frequent_strings\n\n\n\nints/max\n\n\n\nints/min\n\n\n\ntype\n\n\n\ntypes/boolean\n\n\n\ntypes/fractional\n\n\n\ntypes/integral\n\n\n\ntypes/object\n\n\n\ntypes/string\n\nThis covers pretty much all the information required about the input data, such as cardinality, types, frequent_items, distribution, and counts.\n\nConstraints for data quality validation With whylogs, you can use constraints for performing data validation. This feature can be used in unit tests or CI/CD pipelines so that further errors can be avoided. You can set constraints on profile metrics, such as count, distribution, and type. The whylogs also supports user-defined custom metrics. You need a whylogs profile to set the constraints on data.\n\nSetting a new constraint is simple; you need to assign values to the following:\n\nname: It can be any string to describe the constraint. •\t condition: It is a lambda expression. •\t metric_selector: It is the type of metric to be measured or validated.\n\n1. from whylogs.core.constraints import (Constraints,\n\n2. ConstraintsBuilder,\n\nMonitoring and Debugging  405\n\n3. MetricsSelector,\n\n4. MetricConstraint)\n\n5.\n\n6. # Function with Constraints for Data Quality Validation\n\n7. def validate_feat(profile_view, verbose=False, viz=False):\n\n8. builder = ConstraintsBuilder(profile_view)\n\n9. # Define a constraint for data validation\n\n10. builder.add_constraint(MetricConstraint(\n\n11. name=\"Credit_History == 0 or == 1\",\n\n12. condition=lambda x: x.min == 0 or x.max == 1,\n\n13. metric_selector=MetricsSelector(metric_name='distribution',\n\n14. column_name='Credit_History')\n\n15. ))\n\n16.\n\n17. # Build the constraints and return the report\n\n18. constraints: Constraints = builder.build()\n\n19.\n\n20. if verbose:\n\n21. print(constraints.report())\n\n22.\n\n23. # return constraints.report()\n\n24. return constraints\n\n25. # Call the function\n\n26. const = validate_feat(profile_view1, True)\n\nIn the current scenario, constraints are set to ensure that the Credit_History feature should contain either 0 or 1. If it receives any value other than 0 or 1, it will fail. You can add multiple constraints to validate multiple conditions at once. The following is the output of the preceding code:\n\n[('Credit_History == 0 or == 1', 1, 0)]\n\nThis can be read as [('Constraint Name', Pass, Fail)]. It can be visualized with the whylogs visualization functionality in the notebook.\n\n406  Machine Learning in Production\n\n1. from whylogs.viz import NotebookProfileVisualizer\n\n2. visualization = NotebookProfileVisualizer()\n\n3. visualization.constraints_report(const, cell_height=300)\n\nFigure 14.14 depicts the constraints report, that is, the output of the preceding code, which visualizes the outcome of the constraint:\n\nFigure 14.14: whylogs–Constraints Report\n\nIn order to validate all constraints in one go, you can simply use validate() on top of the constraint's outcome.\n\n1. # check all constraints for passing:\n\n2. constraints_valid = const.validate()\n\n3. print(constraints_valid)\n\nAs it is validating only one condition, it will check whether it is true. As data contains 0 or 1 in the Credit_History feature, the test is passed and True is printed.\n\nTrue\n\nNext, with minimal code, you can generate a drift report. For this, you need two whylogs profiles of the data: the target profile and the reference profile.\n\n1. from whylogs.viz import NotebookProfileVisualizer\n\n2.\n\n3. visualization = NotebookProfileVisualizer()\n\n4. visualization.set_profiles(target_profile_view=profile_view1, reference_profile_view=profile_view2)\n\n5.\n\n6. visualization.profile_summary()\n\n7.\n\n8. visualization.summary_drift_report()\n\nMonitoring and Debugging  407\n\nWith summary_drift_report(), you get an easy-to-read drift report. It is an interactive feature-by-feature comparison of two profiles. This report contains the following:\n\nTarget: Histogram of the target profile •\t Reference: Histogram of the reference profile •\t p-value: Ranges from 0 to 1 •\t Total count: Count of records or instances in the dataset •\t Mean: Mean of the numeric feature\n\nWhyLabs The next step is to start uploading these profiles to the WhyLabs platform.\n\nThe first step is to sign up for an account at https://whylabs.ai/whylabs-free-sign- up. If you have already done this, then log in to the account. Next, create a new project from the Project Management section. Depending on the requirement, there are different types of projects available to choose from, such as an ML model and a data pipeline. In this scenario, choose a Classification, that is, an ML project so that you can analyze the model’s performance and input-output data.\n\nAs shown in the following figure, provide the Project Name as ML_monitor (it can be updated later on) and Type as Classification:\n\nFigure 14.15: WhyLabs–Project Management\n\n408  Machine Learning in Production\n\nThen, you need three things to start uploading your whylogs profiles to the WhyLabs platform:\n\nA WhyLabs API Key •\t The organization ID •\t A different Dataset or Model ID for each project\n\nCreate a new access token from the Access Tokens tab, as shown in the following figure. Save the access token and org ID as it can be used multiple times while accessing the project in WhyLabs.\n\nFigure 14.16: WhyLabs–Access Token\n\nWhyLabs allow you to set the notification workflow for important events via the following:\n\nEmails •\t Slack •\t PagerDuty\n\nThe following figure shows the notification options offered by WhyLabs under the Notifications page:\n\nFigure 14.17: WhyLabs–Notifications\n\nMonitoring and Debugging  409\n\nIn the User Management tab, you can see the team members and their roles. You can also control the privilege and access of the users. In this scenario, one user with an Admin role is shown in the following figure:\n\nFigure 14.18: WhyLabs–User management\n\nNext, ensure that the target project is selected from the Select project dropdown. You can update the default values and selections of the monitor. You can also set the threshold, trailing window, action, severity, and so on, as shown in the following figure:\n\nFigure 14.19: WhyLabs–Monitor Manager\n\n410  Machine Learning in Production\n\nWhyLabs offer preset monitors to target common detection. You can use these easy- to-use preset monitors for your project rather than setting them up from the scratch. Once you configure it, you can enable it with a single click. WhyLabs offers the following preset monitors:\n\nDrift\n\no Data drift in all discrete model inputs compared to a trailing 7 days\n\nbaseline\n\no Data drift in all non-discrete model inputs compared to a trailing 7\n\ndays baseline\n\nData quality\n\no Missing values\n\no Unique values\n\no Data types\n\no Percentage change\n\nModel performance o F1 score o Precision\n\no Recall\n\no Accuracy\n\n\n\nIntegration health\n\no Data availability\n\nThe following figure shows the presets available in WhyLabs to check input data quality:\n\nFigure 14.20: WhyLabs–Data quality presets",
      "page_number": 428
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 436-443)",
      "start_page": 436,
      "end_page": 443,
      "detection_method": "topic_boundary",
      "content": "Monitoring and Debugging  411\n\nNow, let’s install whylogs, if it’s not already installed, using the following command, or simply install whylogs using pip: pip install -q \"whylogs[whylabs]\"\n\nFirst, declare variables with org id, access key, and project id. Next, load the data and build the model.\n\n1. whylabs_org = 'org-ELqbpF'\n\n2. whylabs_key = '0Ji9w2gC7q. lQQ4pxgQBEUdx*********************D0vvDrL'\n\n3. whylabs_project = 'model-5'\n\n4.\n\n5. # Importing the required packages\n\n6. import numpy as np\n\n7. import pandas as pd\n\n8. from sklearn.linear_model import LogisticRegression\n\n9.\n\n10. from sklearn import preprocessing\n\n11. from sklearn.model_selection import train_test_split, GridSearchCV\n\n12. from sklearn import metrics\n\n13. import whylogs as why\n\n14. import os\n\n15. from whylogs.api.writer.whylabs import WhyLabsWriter\n\n16. import datetime as dt\n\n17. writer = WhyLabsWriter()\n\n18. os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = whylabs_org # ORG-ID is case sensitive\n\n19. os.environ[\"WHYLABS_API_KEY\"] = whylabs_key\n\n20. os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = whylabs_project\n\n21.\n\n22. # Read the data\n\n23. data = pd.read_csv(\"https://gist.githubusercontent. com/suhas-ds/a318d2b1dda8d8cbf2d6990a8f0b7e8a/ raw/9b548fab0952dd12b8fdf057188038c8950428f1/loan_dataset.csv\")\n\n412  Machine Learning in Production\n\nHere, the baseline model is built as the focus is on model monitoring. After building the model, it delivered 80.9% accuracy on test data. Now, create additional columns in the existing DataFrame to store model predictions and scores.\n\n1. # Make Prediction\n\n2. y_pred = lr_model.predict(X)\n\n3. # Predict probability\n\n4. y_pred_prob = lr_model.predict_proba(X)\n\n5.\n\n6. # Get a maximum probability value of class\n\n7. y_pred_prob_max = [max(p) for p in y_pred_prob]\n\n8.\n\n9. # Output column name should contain the 'output' word\n\n10. data.rename(columns={'Loan_Status':'output_loan_status'}, inplace=True)\n\n11. data['output_prediction'] = y_pred\n\n12. data['output_score'] = y_pred_prob_max\n\nWith this, you should have a DataFrame with input data and output of the predictions. Here, consider the probability of the class that is predicted by the model. In order to show the output of daily batches, divide that DataFrame into four DataFrames. Subtract 1 day in every for-loop iteration and assign that time stamp to a given dataset.\n\n1. # Splitting the final DataFrame into four\n\n2. df1, df2, df3, df4 = np.array_split(data, 4)\n\n3.\n\n4. daily_batches = [df1, df2, df3, df4]\n\n5.\n\n6. for i, data_frame in enumerate(daily_batches):\n\n7. date_time = dt.datetime.now(tz=dt.timezone.utc) - dt.timedelta(days=i)\n\n8.\n\n9. df = data_frame\n\n10. print(\"logging data for date {}\".format(date_time))\n\n11. results = why.log_classification_metrics(\n\nMonitoring and Debugging  413\n\n12. df,\n\n13. target_column = \"output_loan_status\",\n\n14. prediction_column = \"output_prediction\",\n\n15. score_column=\"output_score\"\n\n16. )\n\n17.\n\n18. profile = results.profile()\n\n19. profile.set_dataset_timestamp(date_time)\n\n20.\n\n21. print(\"writing profiles to whylabs...\")\n\n22. results.writer(\"whylabs\").write()\n\nSince this is a classification example, use log_classification_metrics() to log the classification model metrics and pass the following four arguments as to why. log_classification_metrics():\n\ninput DataFrame: Entire DataFrame df •\t target_column: Actual target column •\t prediction_column: Predicted target column •\t score_column: score\n\nWhyLabs will identify a column in data as an output column if the column header contains output text.\n\nWhyLabsWriter() will publish the summary on the WhyLabs platform. Go to the Summary tab in WhyLabs. It offers a compact and single view for multiple widgets like Data profiles, Input Health, Model Health, Model Performance, and alerts, as shown in the following figure:\n\nFigure 14.21: WhyLabs–Summary\n\n414  Machine Learning in Production\n\nUnder the Profiles tab, an overall data summary is displayed. It includes metrics like histogram, feature count, feature type, frequency of items, and null values, as shown in the following figure:\n\nFigure 14.22: WhyLabs–Profiles\n\nSimilarly, you can see input features metrics, such as data type, total project anomalies, and drift distance, in the Inputs tab, as shown in the following figure:\n\nFigure 14.23: WhyLabs–Inputs\n\nHere, you can see the total count over time, hence the line graph is displayed.\n\nMonitoring and Debugging  415\n\nUnder the Output tab, you can see similar metrics, but for output columns.\n\nFigure 14.24: WhyLabs–Outputs\n\nFinally, in the Performance tab, you can see the model metrics across different time periods. In the current scenario, you have passed the data on a daily basis. Therefore, you can see a change in accuracy on a daily basis. It also displays other model evaluation metrics for classification models, such as ROC, Precision-Recall, Confusion matrix, and f1 score.\n\nIn the following figure, you can see recent model accuracy with timestamps and plots for accuracy, ROC, Precision-Recall, and Confusion matrix:\n\nFigure 14.25: WhyLabs–Performance\n\n416  Machine Learning in Production\n\nThe Performance tab also displays the total input and output data count on a daily basis, as shown in the following figure:\n\nFigure 14.26: WhyLabs–Total input and output count\n\nYou can refer to the whylogs code in Google Colab notebook at the following link:\n\nhttps://colab.research.google.com/gist/suhas-ds/8bc0c895b7aa95839eaa5a5df549 2a42/whylogs-and-whylabs.ipynb\n\nOverall, WhyLabs supports integration with existing pipelines and tools in the MLOps environment. It requires low or zero maintenance as it runs on minimal configuration, and it is dynamic, meaning it can handle changes in data properties and characteristics.\n\nConclusion In this chapter, you learned the importance of monitoring in ML and the fundamentals of ML monitoring. You also studied various techniques to detect and address drift in ML, and you explored different metrics to be tracked for operational monitoring and functional monitoring. Next, you learned to integrate Prometheus and Grafana with FastAPI for operational monitoring. Further on in the chapter, you implemented an ML model monitoring solution with whylogs and WhyLabs. And finally, you covered the alert system to complete the feedback loop of the ML life cycle.\n\nIn the next chapter, you will study the steps to follow after deploying an ML model in production. You will also learn different types of ML attacks and model security.\n\nPoints to remember\n\nMake sure the column name contains the word output in order to show output or prediction data under the Output tab in the WhyLabs platform.\n\nMonitoring and Debugging  417\n\nML monitoring refers to tracking the performance, errors, metrics, and such of the deployed model and sending alerts (when required) to ensure that the model continues performing above the acceptable threshold.\n\nOnce an ML model is deployed into production, it is essential to monitor it in order to ensure that its performance is up to the mark and that it continues to deliver reliable output seamlessly.\n\nA concept shift occurs when the relationship between independent variables and dependent or target variables changes.\n\nMonitoring should complete the feedback loop, that is, after detecting an issue or failure, it should send a notification to the concerned team or data scientists so that they can take the necessary actions.\n\nMultiple choice questions\n\n1. A _________ occurs when the relationship between independent variables and dependent or target variables changes.\n\na) Concept shift\n\nb) Data drift\n\nc) Covariate Shift\n\nd) Label drift\n\n2. Monitoring infrastructure cost is a type of _________.\n\na) Model monitoring\n\nb) Recurring monitoring\n\nc) Operational monitoring\n\nd) Recurring monitoring\n\nAnswers 1. a\n\n2. c\n\nQuestions\n\n1. What is the concept of shift?\n\n2. What is the role of monitoring in the ML life cycle?\n\n3. What are the different techniques to detect a drift in ML?\n\n418  Machine Learning in Production\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "page_number": 436
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 444-451)",
      "start_page": 444,
      "end_page": 451,
      "detection_method": "topic_boundary",
      "content": "Post-Productionizing ML Models  419\n\nChapter 15 Post-Productionizing ML Models\n\nIntroduction Till now, you have studied different stages of the ML life cycle and ways to package, deploy, and monitor ML models. You have also learned to implement tests to ensure the integrity and working of modules. You have created local applications out of ML models that can work on Windows and Android devices; deployed models in popular cloud platforms like Microsoft Azure, GCP, and AWS; and built monitoring solutions for operational and ML model monitoring.\n\nHowever, the next part is to get business value out of it. After deployment, when a new model is ready, you will have to decide whether the new model is outperforming the existing model in order to deliver reliable predictions. Model security is also essential after deploying models into production.\n\nStructure In this chapter, the following topics will be covered:\n\nBridging the gap between the ML models and the creation of business value •\t Model security •\t A/B testing\n\n420  Machine Learning in Production\n\nMLOps is the future\n\nObjectives After studying this chapter, you should be able to mitigate the risk of different types of attacks in the ML life cycle. In this chapter, you will learn the importance of converting MLOps solutions to business value and making better decisions while deploying an ML model by comparing two or more ML models with the help of A/B testing and Multi-Armed Bandit (MAB) testing. You will also study the future scope of MLOps in the industry.\n\nBridging the gap between the ML model and the creation of business value Business users leverage the ML model predictions to build the business strategy and make decisions. Achieving good accuracy doesn’t mean having a good business impact. Sometimes, less accurate models increase a company’s revenue. You need to fill the gap between building ML solutions and creating business value out of them. Data scientists should be able to convert model predictions into easy-to-understand actions. For instance, with a classification model, you can create five buckets based on the probability score of classes, and then rank them such that the highest probability will have the highest ranking.\n\nAfter deploying the ML model into the production environment with a monitoring system, the next step is to make sure the MLOps solution will benefit the business users. You can get feedback from the stakeholders, business users, or customers as they are consuming the model output. For instance, you can ask business users if any feature has become more important than the others after deploying the models so that you can give higher weightage to that feature. This way, you can tweak the model to deliver predictions that are more useful for business users.\n\nData scientists should be able to explain the working of the model (on a high level) to business users, as this will help them to trust the model outputs because many of them are not aware of ML terms. Business users or customers prefer to consume the output in a readable form, such as interactive dashboards or chatbots. You can also use Business Intelligence (BI) tools, such as Tableau or Power BI, to create a dashboard.\n\nModel security Model security is an essential part of MLOps. While processing the data, it might be important to protect the sensitive information it may contain. Attacks can take\n\nPost-Productionizing ML Models  421\n\nplace in the model training stage or the production stage. In this chapter, you will get familiar with different types of attacks so that you can implement adequate solutions to prevent them.\n\nFollowing are the terms related to model security:\n\nPoisoning: Passing malicious data to the training process aiming to change the model output\n\nExtraction: Reconstructing a new model from the target model that will function the same as the targeted model\n\nEvasion: Attempting to change the label to a particular class by making small variations in input\n\n\n\nInference: Figuring out whether a specific dataset was part of the training data\n\n\n\nInversion: Getting information about training data from the trained model by reverse engineering\n\nAdversarial attack It is a method of generating hostile data inputs. Attackers intentionally send malicious data to the model so that the model makes incorrect predictions. As the model learns the new data patterns, this attack causes an error in the predictions of the model. This malicious input data may seem normal to humans; however, small changes in input data may have a large impact on model predictions.\n\nAdversarial attacks can be classified into two main categories based on the goal of attackers:\n\nTargeted attacks: Attackers attempt to change the label to a particular target. For this, they can change the input data source to a specific target. It requires more time and effort.\n\nNon-targeted attacks: Attackers do not have any specific target that the model should predict. However, they change the label without any specific target. It requires less time and effort.\n\nAttackers can use the following methods for adversarial attacks:\n\nBlack-box method: In this method, attackers can send the input data to the model and get the output based on it.\n\nWhite-box method: In this method, the attacker is aware of almost everything about the ML model, such as training data and the weights assigned to features.\n\n422  Machine Learning in Production\n\nData poisoning attack In a data poisoning attack, attackers have access to input or source data. They change the input data in such a way that the model will make incorrect predictions that are unreliable for making any business decisions or taking any action. Attackers can add some noise to the original data to cause alterations in the prediction of the model. This attack targets the training data and modifies it intelligently.\n\nDistributed Denial of Service attack (DDoS) It refers to passing complex data to models that will take more time to make predictions. This type of attack limits the use of models for the users. For this, attackers can inject some malware to control the system or server.\n\nData privacy attack Data privacy refers to the confidentiality of Personally Identifiable Information (PII) and Personal Health Information (PHI). Attackers attempt to learn this type of sensitive information through this type of attack. It can be information about the model or training data. ML models like Support Vector Machines (SVMs) may leak the information, as support vectors are data points from training data.\n\nData privacy attacks can be broadly classified into the following categories:\n\nMembership inference attack: The goal of this attack is to determine whether input X is part of the training data. Mostly, shadow models are used in this type of privacy attack. Shadow model training uses a shadow dataset in order to imitate the target model. The output of the shadow models is then passed as an input to the meta-model. Finally, the output of the meta- model is used to extrapolate properties of training data or model. Over-fitted models are prone to data privacy attacks.\n\n\n\nInput inference attack: It is also known as model inversion or data extraction attack. It is the most common type of attack. The goal of this attack is to extract information from the training dataset by reverse engineering the model. It can also target learning the statistical properties of input data, such as probability distribution. Attackers can attempt to learn the features that are not encoded explicitly while training the model.\n\nModel extraction attack: It is also known as a parameter inference attack. The goal of this attack is to learn the hyper-parameters of the model and then reconstruct the model that behaves like the targeted model. Interestingly, over-fitted models are difficult to extract due to high prediction errors based on test data.\n\nPost-Productionizing ML Models  423\n\nMitigate the risk of model attacks The ML pipeline can be broadly divided into two phases: the training phase and the test phase. You can address various ML model attacks based on these phases.\n\nTraining phase A data scientist performs activities like data gathering, data cleansing, feature engineering, choosing suitable algorithms, hyperparameter tuning, and model building. Attackers target this phase via attacks like data poisoning. If a model is trained on poisoned data, you cannot rely on its predictions.\n\nYou can implement the following techniques to mitigate the risk of attacks during the training phase:\n\nData encryption •\t Protect the integrity of training data •\t Robust statistics •\t Data sanitization\n\nTest phase In this phase, attackers target ML models. Model extraction attacks are common among attackers. They attempt to determine whether input X is part of training data or try to steal model parameters defined during the training phase.\n\nThe following techniques can be implemented to mitigate the risk during the test phase:\n\nAdversarial training •\t Autoencoders •\t Distillation •\t Ensemble techniques •\t Limit the number of requests per user\n\nYou can use a Python library Adversarial Robustness Toolbox (ART) and a command-line tool Counterfit for ML model security. This library will defend against the most common types of ML attacks. It also supports popular ML frameworks, libraries, and data types.\n\nA/B testing A/B testing is widely used in marketing, website designs, and email campaigns to learn and understand user preferences. The goal of A/B testing is to increase the conversion rate, success rate, revenue, and so on.\n\n424  Machine Learning in Production\n\nA/B testing involves splitting the audiences or customers from the population into equal sets. These sets will route to the control version and the experimental version. The control version is the existing version, whereas the experimental version is the new or challenger version. First off, define the problem statement for the A/B test, the null hypothesis, and the alternative hypothesis for the problem statement. Next, design the experiment to track and analyze the metric, and then run and validate the experiment. After that, compare the statistics of the output. Finally, make the decision based on the results. If the A/B test is not performed correctly, then its output will be unreliable.\n\nData scientists perform an offline evaluation of ML models by dividing data into training and validation sets. However, the A/B test allows you to perform the online evaluation of ML models by measuring business metrics or success rates. The A/B test can be implemented when you perform operations on training data, such as scaling and normalizing or applying different algorithms, and hyper-parameters while model building. An MLOps engineer or a data scientist can deploy multiple models simultaneously to test models in production. Google Cloud (GCP) and Amazon SageMaker allow the deployment of multiple models into the production behind the same endpoint to decide which model is performing better from the business point of view.\n\nA Multi-Armed Bandit (MAB) is an advanced version of A/B testing. It is more complex than the A/B test, as it uses ML algorithms while dynamically allocating more traffic to the better-performing version and less traffic to the underperforming version based on the data.\n\nMLOps is the future The field of Machine Learning is booming. Industries are making ML a critical part of the business development process, and many new challenges are being addressed by the ML system.\n\nMLOps can handle the complexity and scalability of ML models. Hence, the demand for MLOps is increasing across the industry. There is a shortage of MLOps engineers in the industry, as MLOps is an intersection of Machine Learning and software development. Companies need to set up separate teams for MLOps engineers, or they can upskill their data scientists to execute MLOps tasks. MLOps cut costs and reduce the manual efforts of the overall ML life cycle. This allows data scientists and ML engineers to focus on other productive tasks.\n\nMLOps is getting more popular than DevOps. Your model may be performing well in the local environment; however, if it is not reaching end users or customers, then its business impact is low. More than 85% of ML models are not deployed into the production environment. MLOps engineers can fill the gap between research\n\nPost-Productionizing ML Models  425\n\nenvironments and the production environment. This is a highly demanding field, as data scientists alone are not enough to convert ML models to business value.\n\nConclusion In this chapter, you learned the importance of model security and studied the various attacks in the ML life cycle. Building ML models and MLOps solutions is not enough; data scientists should work on delivering business value from them. A/B testing and Multi-Armed Bandit (MAB) play crucial roles while comparing existing models against newer models in order to improve the overall outcome and business metric. Finally, you learned how MLOps demand is increasing in the industry.\n\nPoints to remember\n\nModel security is an essential part of MLOps. •\t A/B testing enables you to perform an online evaluation of ML models. •\t Attackers can use the Black-box method or the White-box method while attempting an adversarial attack.\n\nA Python library Adversarial Robustness Toolbox (ART) and a command- line tool Counterfit can be used for ML model security.\n\nA Multi-Armed Bandit (MAB) is an advanced version of A/B testing. It is smart enough to decide which model should get more traffic by evaluating multiple models.\n\nMultiple choice questions\n\n1. In _________, attackers intentionally send malicious data to the model so that the model makes incorrect predictions.\n\na) phishing\n\nb) an adversarial attack\n\nc) shoulder surfing\n\nd) piggybacking\n\n426  Machine Learning in Production\n\n2. _________ refers to getting information about training data from the trained model by reverse engineering. a) Extraction b) Evasion c) Inversion d) Monitoring\n\nAnswers 1. b\n\n2. c\n\nQuestions\n\n1. What is a data poisoning attack?\n\n2. What is A/B testing?\n\n3. What are the different techniques that can be implemented to mitigate the model security risk during the training phase?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "page_number": 444
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 452-459)",
      "start_page": 452,
      "end_page": 459,
      "detection_method": "topic_boundary",
      "content": "Symbols\n\n__init__ method 9\n\nA\n\nA/B testing 423, 424\n\nActivate Cloud Shell icon 313\n\nAdversarial Robustness Toolbox\n\n(ART) 423\n\nAmazon CloudWatch Container\n\nInsights 378\n\nAmazon Elastic Compute Cloud\n\n(EC2) 327\n\nAmazon Elastic Container Registry\n\n(ECR) 327, 336, 337\n\nAmazon Elastic Container Service\n\n(ECS) 327, 346\n\ncluster 347\n\ncontainer 347\n\nservice 347\n\nIndex  427\n\nIndex\n\ntask 346\n\ntask definition 346, 349\n\nAmazon Elastic Kubernetes Service (EKS) 327\n\nAmazon SageMaker 328\n\nAmazon Simple Storage\n\nService (S3) 328\n\nAmazon Virtual Private Cloud (Amazon VPC) 351, 352\n\nAmazon Web Services (AWS) 326\n\nAnaconda\n\ninstalling 2\n\nApplication Load Balancer\n\n(ALB) 352, 358-362\n\narray 4\n\nAWS account\n\nsetting up 329, 330\n\nAWS Application Load Balancer\n\n(ALB) 327\n\n428  Machine Learning in Production\n\nAWS CodeBuild 339-344\n\ncontainer registry access, attaching\n\nto service role 345, 346\n\nAWS CodeCommit 331-335\n\nAWS CodePipeline 371, 372\n\nbuild stage, adding 374\n\ndeploy stage, adding 375, 376\n\nmonitoring 378\n\nrunning 376-378\n\nsettings 372\n\nsource stage, adding 373, 374\n\nAWS Command-Line Interface\n\n(CLI) 330\n\nAWS compute services 326, 327\n\nAWS ECS deployment models 347\n\nEC2 instance 347, 348\n\nFargate 348, 349\n\nAWS Fargate 328\n\nAWS Lambda 328\n\nAzure 250, 251\n\ndeployment, with GitHub Actions 251-253\n\ninfrastructure setup 263\n\nAzure account\n\nsetting up 251\n\nAzure App Service\n\nconfiguring, for using GitHub Actions for CD 274-278\n\ncreating 267-270\n\nAzure Container Instance\n\n(ACI) 289, 292, 297\n\nAzure Container Registry (ACR) 252, 263\n\ncreating 264-267\n\nAzure Kubernetes Service (AKS) 250, 292\n\nAzure Machine Learning\n\n(AML) service 249, 279\n\nAzure Machine Learning Studio\n\nexperiments 280\n\nfeatures 279\n\nruns 280-289\n\nworkspace 280\n\nC\n\nCD pipeline\n\nconfiguring 292-298\n\nCI/CD pipeline\n\nbuilding 189\n\ncodebase, developing 189-196\n\ncreating, with Jenkins 202\n\nfor ML 184, 185\n\nGitHub Actions and Heroku, using\n\n227\n\nPersonal Access Token (PAT), creating\n\non GitHub 196\n\nusing, Cloud Build 317, 318\n\nusing, CodePipeline 369, 370\n\nwebhook, creating on GitHub repository 197, 198\n\nCI/CD pipeline, with Jenkins 202\n\ndeployment-status-email 210-217\n\nGitHub-to-container 203-205\n\ntesting 207-210\n\ntraining 205-207\n\nCI pipeline\n\nconfiguring 290-292\n\nclass 9\n\nCloud Alert manager 401\n\nCloud Build 309-311\n\ntrigger, creating 318-322\n\nCloud Source Repositories 304-308, 317\n\nCloudwatch logs 345\n\nCodeBuild 339\n\ncode editor\n\ninstalling 3\n\nContainer Registry 311, 312\n\nContinuous Delivery (CD) 186\n\nContinuous Deployment 186\n\nContinuous Integration (CI) 185, 186,\n\n339\n\nContinuous Training (CT) 186, 336\n\ncontrol statements and loops\n\nfor loop 6, 7\n\nif...else 6\n\npass statement 8\n\nwhile loop 7\n\nCORS (Cross-Origin Resource\n\nSharing) 191, 229, 254\n\nCSV files\n\nloading 15\n\nsaving 15\n\nD\n\ndata privacy attacks\n\ninput inference attack 422\n\nmembership inference attack 422\n\nmodel extraction attack 422\n\ndata structures 4\n\narray 4\n\ndictionary 4\n\nlist 4\n\nset 5\n\nstring 5\n\ntuple 5\n\ndictionary 4\n\nDistributed Version Control\n\nSystem (DVCS) 18\n\nDocker 112\n\ndetached mode 118\n\nIndex  429\n\nenvironment, setting up 113\n\nHello World example 116\n\ninstalling 113\n\nold versions, uninstalling 113, 114\n\nDocker commands 127\n\nDocker compose 115\n\nDocker container\n\nML model, deploying 122-127\n\nrunning 120, 121\n\nDocker Engine\n\ninstalling 114\n\nDockerfile\n\ncreating 119, 120\n\nDocker Hub rate limit 337-339\n\nDocker image\n\nbuilding 120\n\nDocker objects\n\nDocker container networking 118\n\nDocker containers 118\n\nDockerfile 117\n\nDocker image 117\n\nDomain Name System (DNS) 370\n\ndrift, in ML 386\n\naddressing 389\n\nconcept drift 387, 388\n\ndata drift 387\n\ndata quality issues 389\n\ndetecting techniques 388, 389\n\nmodel, rebuilding 389\n\nmodel, retaining 389\n\nmodel, tuning 389\n\ntypes 386\n\nDRY (Don’t Repeat Yourself) principles\n\n41\n\nE\n\nEC2 (Elastic Compute Cloud) 347\n\n430  Machine Learning in Production\n\nECS service\n\nauto-scaling 367\n\nconfiguration, reviewing 367-369\n\nconfiguring 363, 364\n\ncreating 363\n\nnetwork, configuring 365-367\n\nElastic Container Registry (ECR) 370\n\nenvironment variables\n\npackage, building 71\n\npackage, installing 71, 72\n\npackage usage, with example 73, 74\n\npaths, adding 70\n\nsetting up 70\n\nExcel files\n\nloading 15\n\nsaving 15\n\nexternal Alert manager 401\n\nF\n\nFastAPI 131-139\n\nFlask 143-150\n\nfor loop 6, 7\n\nFunction as a Service (FaaS) 328\n\nfunctions 8\n\nG\n\nGCP account\n\nsetting up 303, 304\n\nGCP cloud shell\n\nusing, for deployment 316\n\nGCP platform\n\nURL 303\n\nGit 17, 18\n\nconfiguration 22\n\nfile, adding 23-26\n\nGUI clients 20\n\ninstalling 20\n\ninstalling, for all platforms 20\n\ninstalling, in Linux 20\n\nstatus, checking 22\n\nworkflow 19\n\nGit commands\n\nchanges 21, 22\n\nnew repository 21\n\nrevert 22\n\nsetup 21\n\nupdate 21\n\nGitHub 17, 19\n\nGitHub account\n\ncreating 20, 21\n\nGitHub Actions 225-270\n\nconfiguration 226\n\nservice principal 273\n\nGit repository\n\ninitializing 22\n\nGNU Privacy Guard (GPG) 114\n\nGoogle Cloud Platform (GCP) 302\n\nservices 302, 303\n\nGoogle Kubernetes Engine\n\n(GKE) 302, 313, 314\n\ndeployment.yaml 314\n\nservice.yaml 315\n\nGrafana 390\n\nGrafana Alert manager 401\n\nGraphical User Interface (GUI) 157\n\nGunicorn 150\n\nH\n\nHello World! example 3\n\nHeroku 219-221\n\ndeployment, with Container\n\nRegistry 225\n\ndeployment, with GitHub\n\nrepository integration 222\n\npipeline flow 224\n\nsetting up 221\n\nHeroku deployment\n\nPRODUCTION 224\n\nREVIEW APPS 223\n\nSTAGING 223\n\nHeroku Git\n\ndeploying with 222\n\nI\n\nIAM management console 345\n\nif...else statement 6\n\niloc 14, 15\n\ninstrument 390\n\nJ\n\nJenkins 187\n\nconfiguring 199-202\n\ninstallation 187-189\n\nK\n\nKivyMD app 173-178\n\nKubernetes 312\n\nKubernetes Engine API 313\n\nL\n\nlist 4\n\nload balancing 352\n\nApplication Load Balancer\n\n(ALB) 358-362\n\nsecurity group 356-358\n\ntarget group 353-356\n\nloc 14\n\nM\n\nmethod 9\n\nMICE algorithm 59\n\nMicrosoft Azure 250\n\nML as a Service (MLaaS) 250\n\nIndex  431\n\nML-based app\n\nbuilding, with kivy and kivyMD 170-173\n\nbuilding, with Tkinter 160-162\n\nMLflow 78\n\ncomponents 82\n\nenvironment, setting up 79\n\ninstalling 79\n\nMiniconda installation 79-81\n\nsceanarios 78, 79\n\nMLflow projects 94-97\n\nMLflow models 97-100\n\nMLflow registry 100, 101\n\nfeatures 102, 103\n\nMLflow server, starting 103-108\n\nMySQLdb module, installing 102\n\nMySQL server, setting 101\n\nMLflow tracking 83\n\nartifacts 84\n\nlog data, into run 84-94\n\nmetric 83\n\nparameters 83\n\nsource 83\n\nstart and end time 83\n\nML life cycle 30\n\nbusiness impact 31\n\ndata collection 31, 32\n\ndata preparation 32\n\nfeature engineering 32, 33\n\nmodel building 33\n\nmodel deployment 34\n\nmodel evaluation 34\n\nmodel testing 34\n\nmodel training 33\n\nmonitoring 35\n\noptimization 35\n\n432  Machine Learning in Production\n\nML model\n\ndeployment, with Azure DevOps and Azure ML 278, 279\n\nde-serializing 48\n\ngap bridging between business\n\nvalue creation 420\n\npost-productionizing 419\n\nrequirements file 47, 48\n\nserializing 48\n\nvirtual environments 46, 47\n\nML model attacks\n\nrisk mitigation 423\n\nML monitoring\n\nconstraints, for data quality validation 404-407\n\nfunctional monitoring 383\n\nfundamentals 383-385\n\nmetrics 385, 386\n\nmetrics tracking 383, 384\n\noperational monitoring 383\n\nscalable integration 383\n\nwith whylogs and WhyLabs 402\n\nMLOps 39, 40\n\nautomation 41\n\nbenefits 40\n\nefficient management 41\n\nfeatures 424\n\nreproducibility 41\n\ntracking and feedback loop 42\n\nML packages\n\nbusiness problem 52\n\nclassification_v1.pkl 65\n\nconfiguration module 55, 56\n\ndata 52\n\ndata_management.py 57\n\ndeveloping 51, 53\n\n__init__.py module 54\n\nMANIFEST.in file 54, 55\n\nML model, building 53\n\npipeline.py 62\n\npredict.py module 63\n\npreprocessors.py 58-62\n\npytest.ini 68\n\nrequirements.txt 64\n\nsdist 70\n\nsetup.py 66\n\ntest_predict.py 68-70\n\ntrain_pipeline.py 65\n\nversion 68\n\nwheel 70\n\nmodel deployment 35\n\nbatch predictions 35\n\nmobile and edge devices 36, 37\n\nreal-time predictions 37\n\nweb service/REST API 36\n\nmodel deployment challenges, in\n\nproduction environment\n\ndata-related challenges 38\n\nportability 38\n\nrobustness 38, 39\n\nscalability 38\n\nsecurity 39\n\nteam coordination 37, 38\n\nmodel security 420, 421\n\nadversarial attack 421\n\ndata poisoning attack 422, 423\n\ndata privacy attack 422\n\nDistributed Denial of Service attack (DDoS) 422\n\nmonitoring 382, 383\n\nMulti-Armed Bandit (MAB) 424\n\nN\n\nNetwork Load Balancer (NLB) 327\n\nNGINX 150-154\n\nNumPy 10\n\nNumPy array 10\n\nreshaping 12\n\nO\n\nobject 9\n\nObject Oriented Programming (OOP) 9\n\nclass 9\n\n__init__ method 9\n\nmethod 9\n\nobject 9\n\nself 9\n\noperational monitoring\n\nwith Prometheus and Grafana 390-401\n\nP\n\nPandas 12\n\nPandas DataFrame\n\nusing 12, 13\n\npass statement 8\n\nPersonal Access Token (PAT) 196\n\nPersonal Health Information (PHI) 422\n\nPersonally Identifiable Information\n\n(PII) 422\n\nPlatform as a Service (PaaS) 219, 250\n\nprod.workflow.yml 270-272\n\nPrometheus 390\n\npytest fixtures 49\n\nPython\n\ninstalling 2\n\ninstalling, on Mac OS 2\n\ninstalling, on Linux 2\n\ninstalling on Windows 2\n\nPython 101 1\n\nPython app\n\nconverting, into Android app 179, 180\n\nIndex  433\n\nPython-based Tkinter app\n\nconverting, into Windows EXE file 167-169\n\nPython code\n\ntesting, with pytest 48, 49\n\nPython file\n\nexecuting 3\n\nPython Package Index (PyPI) 3\n\nPython packaging\n\nand dependency management 49\n\nmodular programming 50\n\nmodule 50\n\npackage 50, 51\n\nR\n\nRepresentational State Transfer (REST)\n\n130\n\nRest APIs 130\n\nrisk mitigation, ML model attacks\n\ntest phase 423\n\ntraining phase 423\n\nS\n\nService Principal (automatic) 282\n\nset 5\n\nsrc directory\n\ndocker-compose.yml 234, 259\n\nDockerfile 234, 259\n\n.gitignore 229, 253\n\nmain.py 229-232, 254-257\n\nproduction.yml 239-243\n\npytest.ini 235, 260\n\nrequirements.txt 258\n\nrequirements.txt file 233\n\nruntime.txt 235, 260\n\nstart.sh 235, 260\n\ntest.py 235, 236, 260, 261\n\ntox.ini 237, 262, 263\n\n434  Machine Learning in Production\n\nworkflow.yml 238\n\nworkflow.yml workflow 244-246\n\nStaging 241\n\nStreamlit 139-143\n\nstring 5\n\nT\n\ntask definition 349, 350\n\ntask, running with 350\n\nTiny Machine Learning (TinyML) 36\n\nTkinter 158\n\nfeatures 160\n\nHello World app 159\n\nML-based app, building 160-162\n\nTkinter app 163-167\n\nTool Command Language (TCL) 158\n\ntuple 5\n\nV\n\nVirtual Machines (VM) 250\n\nVirtual Private Cloud (VPC) 303\n\nW\n\nWeb Server Gateway Interface (WSGI)\n\n143\n\nwhile loop 7\n\nWhyLabs 402, 407\n\naccess token 408\n\nfeatures 402\n\nNotifications page 408\n\noutput tab 415, 416\n\nprofile tab 414\n\nproject management 407, 408\n\nuser management 409-413\n\nwhylogs 403, 404",
      "page_number": 452
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 460-461)",
      "start_page": 460,
      "end_page": 461,
      "detection_method": "topic_boundary",
      "content": " i",
      "page_number": 460
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": " i",
      "content_length": 3,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Machine Learning in Production\n\nMaster the art of delivering robust Machine Learning solutions with MLOps\n\nSuhas Pote\n\nwww.bpbonline.com\n\n i",
      "content_length": 141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "ii \n\nCopyright © 2023 BPB Online\n\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.\n\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the author, nor BPB Online or its dealers and distributors, will be held liable for any damages caused or alleged to have been caused directly or indirectly by this book.\n\nBPB Online has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, BPB Online cannot guarantee the accuracy of this information.\n\nFirst published: 2023\n\nPublished by BPB Online WeWork 119 Marylebone Road London NW1 5PU\n\nUK | UAE | INDIA | SINGAPORE\n\nISBN 978-93-55518-101\n\nwww.bpbonline.com",
      "content_length": 1090,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Dedicated to\n\nMy parents:\n\nI am so grateful for your belief in me, and\n\nfor the many sacrifices you have made throughout\n\nmy life. Your love and support mean the world to me.\n\n iii",
      "content_length": 181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "iv \n\nAbout the Author\n\nSuhas Pote has over eight years of multidisciplinary experience in data science, playing central roles in numerous projects as a technical leader and data scientist, delivering projects using open-source technologies for big companies, including successful projects in South America, Europe, and the United States. He is experienced in client engagement and working collaboratively with different teams. Currently, he is a process manager at Eclerx and is an accomplished postgraduate, having completed a degree in Data Science, Business Analytics, and Big Data. He holds a Bachelor's degree focused on Electronics and Telecommunication Engineering. In the meantime, he successfully got many certifications in data science and related tools. Furthermore, the author participates as a speaker in Data Science conferences and writes technical articles on machine learning and related topics. He also contributes to technical communities worldwide, such as Stack Overflow.",
      "content_length": 993,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "About the Reviewer\n\nKaran Vijay Singh is a passionate Big Data / ML Engineer with extensive professional experience in the domain of Cloud Cost Optimization. With his optimisation skills, he has saved his current organization 0.5M dollar a year in cost. He also has co-authored a research paper in 2019 in the field of cloud computing which has over 270 citations worldwide.\n\nKaran did his Master’s in Mathematics (Computer Science) from University of Waterloo in Canada. He is currently working as a Data Engineer at Lightspeed Commerce in Canada.\n\n v",
      "content_length": 553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "vi \n\nAcknowledgement\n\nI want to express my deepest gratitude to my family and friends for their unwavering support and encouragement throughout this book's writing, especially my wife, Dr. Archana and my son, Eshansh.\n\nI am also grateful to BPB Publications for their guidance and expertise in bringing this book to fruition. It was a long journey of revising this book, with valuable participation and collaboration of reviewers, technical experts, and editors.\n\nI would also like to acknowledge the valuable contributions of my colleagues and co-worker during many years working in the tech industry, who have taught me so much and provided valuable feedback on my work.\n\nFinally, I would like to thank all the readers who have taken an interest in my book and for their support in making it a reality. Your encouragement has been invaluable.",
      "content_length": 845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Preface\n\nProductionizing the machine learning model is a complex task that requires a comprehensive understanding of the latest technologies and CI/CD pipeline. MLOps become increasingly popular in the field of Data Science.\n\nThis book is designed to provide a comprehensive guide to building and deploying ML applications with MLOps. It covers a wide range of topics, including the basics of Python programming, Git, Machine Learning life cycle, Docker, and advanced concepts such as packaging Python code for ML models, monitoring, model security, Kubernetes, testing using pytest and the use of CI/CD pipeline for building and deploying robust and scalable ML applications on cloud platforms, including Azure, GCP, and AWS.\n\nThroughout the book, you will learn about the MLOps, various tools, and techniques to deploy ML models. You will also learn how to use them to productionize ML models and applications that are efficient, scalable, and easy to maintain. Additionally, you will learn about best practices and design patterns for MLOps.\n\nThis book is intended for data scientists, software developers, data engineers, data analysts, and managers who are new to MLOps and want to learn how to productionize ML models. It is also helpful for experienced data scientists and ML engineers who want to expand their knowledge of these technologies and improve their skills in deploying ML models in production.\n\nWith this book, you will gain the knowledge and skills to become proficient in the field of MLOps. I hope you will find this book informative and helpful.\n\nChapter 1: Python 101 – explains the Python fundamental concepts needed for the reader to equip with the prerequisites required for this book, including Python installation, data structures, control statements, loops, functions, and data manipulation using pandas. This is a quick refresher for those who have worked with Python. If you are a beginner, this chapter will cover the most common Python commands needed to build and deploy ML models in production. It allows the reader to learn fundamental concepts related to the Object-Oriented Programming paradigm using Python.\n\nChapter 2: Git and GitHub Fundamentals – presents a detailed overview of Git workflow, including common Git commands with practical examples. This is\n\n vii",
      "content_length": 2305,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "viii \n\nessential content for the entire book, as this chapter covers fundamental aspects of Git and GitHub that influence technical decisions to build the CI/CD pipelines to deploy ML models in the production environment.\n\nChapter 3: Challenges in ML Model Deployment – covers the various stages of the ML life cycle, including details on the challenges of each stage. It also covers the common challenges in deploying models in the production environment and how MLOps can help to overcome them. Additionally, the chapter discusses different approaches to deploying ML models in production.\n\nChapter 4: Packaging ML Models – allows the reader to learn fundamental concepts related to modular programming using the Python language, including Python packaging, dependency management, and good practices of software development to develop stable, readable, and extensible code for robust enterprise applications. Furthermore, the chapter explains the virtual environment, testing code using pytest, serializing, and deserializing ML models. It covers packaging ML models, code, and dependencies so that the package can be installed and consumed on another machine or server.\n\nChapter 5: MLflow-Platform to Manage the ML Life Cycle – gives special attention to streamlining machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models. It demonstrates how to train, deploy, and reuse ML models using MLflow through practical examples based on the use case. This chapter explains the role of MLflow in an ML life cycle.\n\nChapter 6: Docker for ML – shows the basic concepts of Docker and provides practical examples of common Docker commands with a use case for the reader. Learning these commands allows the reader to package ML code with its dependencies and run the application inside Docker containers. This chapter includes practical examples of Docker objects such as Docker images and containers.\n\nChapter 7: Build ML Web Apps Using API – explains in detail the most commonly used frameworks for building web-based ML apps using the Python language, including FastAPI, Streamlite, and Flask. It also allows the reader to learn the basics of Gunicorn, NGINX, and APIs, including an explanation of REST APIs, and much more.",
      "content_length": 2297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Chapter 8: Build Native ML Apps – is dedicated to building native ML applications in Python to give the reader more familiarity with converting Python apps into Windows and Android apps and ways to consume them. This chapter covers practical examples of working with Tkinter, kivy, kivyMD, pyinstaller, and buildozer.\n\nChapter 9: CI/CD for ML – allows the reader to learn and implement the different stages of the CI/CD pipeline using GitHub and Jenkins, including committing, building, testing, and deploying. This chapter explains the GitHub and Jenkins integrations to build an automated CI/CD pipeline for deploying ML apps using Python and Docker.\n\nChapter 10: Deploying ML Models on Heroku – will guide the reader in configuring and building a CI/CD pipeline using GitHub Actions to deploy a web-based ML app on the Heroku platform. This chapter also explains three methods for deploying the web app on Heroku: Heroku Git, GitHub integration, and Container registry. It covers practical examples for creating workflow (YAML) files for GitHub Actions, including using tox and pytest for testing the code, and an automated Heroku pipeline for faster deployments.\n\nChapter 11: Deploying ML Models on Microsoft Azure – explains the step-by- step approach to building CI/CD pipeline using GitHub Actions to deploy web- based ML apps to Azure web services. In the other approach, the reader will learn to build a CI/CD pipeline and deploy web-based scalable ML apps to Azure Container Instances (ACI) and Azure Kubernetes Service (AKS) using Azure DevOps and Azure Machine Learning (AML) Service.\n\nChapter 12: Deploying ML Models on Google Cloud Platform – Shows how to build an automated CI/CD pipeline to deploy ML models on Google Kubernetes Engine (GKE), without the need to integrate any external tool, service, or platform. This chapter allows the reader to learn and implement Kubernetes functionality to run scalable ML apps using Google Kubernetes Engine (GKE).\n\nChapter 13: Deploying ML Models on Amazon Web Services – presents an overview of various cloud compute services offered by Amazon Web Services (AWS). This chapter allows the reader to learn and implement an automated CI/CD pipeline with Continuous Training (CT) to deploy scalable enterprise ML apps on Amazon Elastic Container Service (ECS). Additionally, it covers the integration of Application Load Balancer (ALB), Amazon Virtual Private Cloud\n\n ix",
      "content_length": 2425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "x \n\n(Amazon VPC), and security groups into Amazon Elastic Container Service (ECS) for building robust and secure enterprise ML apps.\n\nChapter 14: Monitoring and Debugging – is dedicated to monitoring ML and operational metrics, including servers, cost of services, drifts in ML, input data, ML models, and much more. This chapter shows the importance and fundamental concepts of monitoring in the ML life cycle. It also covers practical examples using whylogs and WhyLabs for ML model monitoring, and Prometheus and Grafana for operational monitoring.\n\nChapter 15: Post-Productionizing ML Models – presents a detailed overview of adversarial machine learning, including different types of adversarial attacks and how to mitigate them. It also covers fundamental concepts of A/B testing and the future scope of MLOps in the industry.",
      "content_length": 833,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Code Bundle and Coloured Images Please follow the link to download the Code Bundle and the Coloured Images of the book:\n\nhttps://rebrand.ly/vlv0nzp\n\nThe is also hosted on GitHub at https://github.com/bpbpublications/Machine-Learning-in-Production. In case there's an update to the code, it will be updated on the existing GitHub repository.\n\ncode bundle\n\nfor\n\nthe book\n\nWe have code bundles from our rich catalogue of books and videos available at https://github.com/bpbpublications. Check them out!\n\nErrata\n\nWe take immense pride in our work at BPB Publications and follow best practices to ensure the accuracy of our content to provide with an indulging reading experience to our subscribers. Our readers are our mirrors, and we use their inputs to reflect and improve upon human errors, if any, that may have occurred during the publishing processes involved. To let us maintain the quality and help us reach out to any readers who might be having difficulties due to any unforeseen errors, please write to us at :\n\nerrata@bpbonline.com\n\nYour support, suggestions and feedbacks are highly appreciated by the BPB Publications’ Family.\n\nDid you know that BPB offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at www.bpbonline.com and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at :\n\nbusiness@bpbonline.com for more details.\n\nAt www.bpbonline.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on BPB books and eBooks.\n\n xi",
      "content_length": 1644,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "xii \n\nPiracy\n\nIf you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at business@bpbonline.com with a link to the material.\n\nIf you are interested in becoming an author\n\nIf there is a topic that you have expertise in, and you are interested in either writing or contributing to a book, please visit www.bpbonline.com. We have worked with thousands of developers and tech professionals, just like you, to help them share their insights with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.\n\nReviews\n\nPlease leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions. We at BPB can understand what you think about our products, and our authors can see your feedback on their book. Thank you!\n\nFor more information about BPB, please visit www.bpbonline.com.\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 1342,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Table of Contents\n\n1. Python 101 .....................................................................................................................1\n\nIntroduction ............................................................................................................1\n\nStructure ..................................................................................................................1\n\nObjectives ................................................................................................................2\n\nInstall Python .........................................................................................................2\n\nOn Windows and Mac OS ................................................................................2\n\nOn Linux ...........................................................................................................2\n\nInstall Anaconda ....................................................................................................2\n\nInstall code editor ..................................................................................................3\n\nHello World! ...........................................................................................................3\n\nExecution of Python file .....................................................................................3\n\nData structures .......................................................................................................4\n\nCommon data structures ...................................................................................4\n\nControl statements and loops ..............................................................................6\n\nFunctions .................................................................................................................8\n\nObject Oriented Programming (OOP) ................................................................9\n\nNumerical Python (NumPy) ..............................................................................10\n\nPandas ...................................................................................................................12\n\nConclusion ............................................................................................................15\n\nPoints to remember .............................................................................................15\n\nMultiple choice questions ...................................................................................16\n\nAnswers ...........................................................................................................16\n\nQuestions ..............................................................................................................16\n\nKey terms ..............................................................................................................16\n\n2. Git and GitHub Fundamentals ...............................................................................17\n\nIntroduction ..........................................................................................................17\n\nStructure ................................................................................................................17\n\nObjectives ..............................................................................................................18\n\nGit concepts ..........................................................................................................18\n\n xiii",
      "content_length": 3491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "xiv \n\nGit + Hub = GitHub ........................................................................................19\n\nCommon Git workflow .......................................................................................19\n\nInstall Git and create a GitHub account ...........................................................20\n\nLinux (Debian/Ubuntu) ..................................................................................20\n\nGit for all platforms .........................................................................................20\n\nGUI clients .......................................................................................................20\n\nCreate a GitHub account .................................................................................20\n\nCommon Git commands .....................................................................................21\n\nSetup ................................................................................................................21\n\nNew repository .................................................................................................21\n\nUpdate ..............................................................................................................21\n\nChanges ............................................................................................................21\n\nRevert ...............................................................................................................22\n\nLet’s Git .................................................................................................................22\n\nConfiguration ...................................................................................................22\n\nInitialize the Git repository ..............................................................................22\n\nCheck Git status ...............................................................................................22\n\nAdd a new file ..................................................................................................23\n\nConclusion ............................................................................................................26\n\nPoints to remember .............................................................................................26\n\nMultiple choice questions ..................................................................................26\n\nAnswers ...........................................................................................................27\n\n3. Challenges in ML Model Deployment .................................................................29\n\nIntroduction ..........................................................................................................29\n\nStructure ................................................................................................................29\n\nObjectives ..............................................................................................................30\n\nML life cycle ..........................................................................................................30\n\nBusiness impact ...............................................................................................31\n\nData collection .................................................................................................31\n\nData preparation ..............................................................................................32\n\nFeature engineering .........................................................................................32\n\nBuild and train the model ................................................................................33",
      "content_length": 3724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Test and evaluate ..............................................................................................34\n\nModel deployment ............................................................................................34\n\nMonitoring and optimization ..........................................................................35\n\nTypes of model deployment ...............................................................................35\n\nBatch predictions .............................................................................................35\n\nWeb service/REST API ....................................................................................36\n\nMobile and edge devices ...................................................................................36\n\nReal-time ..........................................................................................................37\n\nChallenges in deploying models in the production environment ...............37\n\nTeam coordination ............................................................................................37\n\nData-related challenges ....................................................................................38\n\nPortability ........................................................................................................38\n\nScalability ........................................................................................................38\n\nRobustness .......................................................................................................38\n\nSecurity ............................................................................................................39\n\nMLOps ...................................................................................................................39\n\nBenefits of MLOps ...............................................................................................40\n\nEfficient management of ML life cycle .............................................................41\n\nReproducibility ................................................................................................41\n\nAutomation ......................................................................................................41\n\nTracking and feedback loop ..............................................................................42\n\nConclusion ............................................................................................................42\n\nPoints to remember .............................................................................................42\n\nMultiple choice questions ...................................................................................43\n\nAnswers ...........................................................................................................43\n\nQuestions ..............................................................................................................43\n\nKey terms ..............................................................................................................43\n\n4. Packaging ML Models ..............................................................................................45\n\nIntroduction ..........................................................................................................45\n\nStructure ................................................................................................................45\n\nObjectives ..............................................................................................................46\n\nVirtual environments ..........................................................................................46\n\n xv",
      "content_length": 3704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "xvi \n\nRequirements file .................................................................................................47\n\nSerializing and de-serializing ML models .......................................................48\n\nTesting Python code with pytest .......................................................................48\n\npytest fixtures ..................................................................................................49\n\nPython packaging and dependency management .........................................49\n\nModular programming ....................................................................................50\n\nModule .............................................................................................................50\n\nPackage ............................................................................................................50\n\nDeveloping, building, and deploying ML packages ......................................51\n\nBusiness problem .............................................................................................52\n\nData .................................................................................................................52\n\nBuilding the ML model ....................................................................................53\n\nDeveloping the package ....................................................................................53\n\nSet up environment variables and paths ..........................................................70\n\nBuild the package .............................................................................................71\n\nInstall the package ............................................................................................71\n\nPackage usage with example ............................................................................73\n\nConclusion ............................................................................................................74\n\nPoints to remember .............................................................................................74\n\nMultiple choice questions ...................................................................................74\n\nAnswers ...........................................................................................................75\n\nQuestions ..............................................................................................................75\n\nKey terms ..............................................................................................................75\n\n5. MLflow-Platform to Manage the ML Life Cycle .................................................77\n\nIntroduction ..........................................................................................................77\n\nStructure ................................................................................................................77\n\nObjectives ..............................................................................................................78\n\nIntroduction to MLflow ......................................................................................78\n\nSet up your environment and install MLflow .................................................79\n\nMiniconda installation ..........................................................................................79\n\nMLflow components ........................................................................................82\n\nMLflow tracking ..................................................................................................83",
      "content_length": 3618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": " xvii\n\nLog data into the run .......................................................................................84\n\nMLflow projects ...................................................................................................94\n\nMLflow models ....................................................................................................97\n\nMLflow registry .................................................................................................100\n\nSet up the MySQL server for MLflow ...........................................................101\n\nStart the MLflow server .................................................................................103\n\nConclusion ..........................................................................................................109\n\nPoints to remember ...........................................................................................109\n\nMultiple choice questions .................................................................................109\n\nAnswers ......................................................................................................... 110\n\nQuestions ............................................................................................................ 110\n\n6. Docker for ML .......................................................................................................... 111\n\nIntroduction .........................................................................................................111\n\nStructure ...............................................................................................................111\n\nObjectives ............................................................................................................ 112\n\nIntroduction to Docker ...................................................................................... 112\n\nIt works on my machine! ............................................................................... 112\n\nLong setup ...................................................................................................... 112\n\nSetting up your environment and installing Docker .................................... 113\n\nDocker installation ......................................................................................... 113\n\nUninstall old versions ......................................................................................... 113\n\nInstall Docker Engine .................................................................................... 114\n\nDocker compose .............................................................................................. 115\n\nHello World with Docker ................................................................................. 116\n\nDocker objects .................................................................................................... 117\n\nDockerfile ............................................................................................................. 117\n\nDocker image ....................................................................................................... 117\n\nDocker containers ................................................................................................ 118\n\nDocker container networking .............................................................................. 118\n\nCreate a Dockerfile ............................................................................................ 119\n\nBuild a Docker image ........................................................................................120\n\nRun a Docker container ....................................................................................120",
      "content_length": 3729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "xviii \n\nDockerize and deploy the ML model .............................................................122\n\nCommon Docker commands ...........................................................................127\n\nConclusion ..........................................................................................................127\n\nPoints to remember ...........................................................................................127\n\nMultiple choice questions .................................................................................128\n\nAnswers .........................................................................................................128\n\nQuestions ............................................................................................................128\n\n7. Build ML Web Apps Using API ...........................................................................129\n\nIntroduction ........................................................................................................129\n\nStructure ..............................................................................................................129\n\nObjectives ............................................................................................................130\n\nRest APIs .............................................................................................................130\n\nFastAPI ................................................................................................................131\n\nStreamlit ..............................................................................................................139\n\nFlask .....................................................................................................................143\n\nGunicorn ........................................................................................................150\n\nNGINX .........................................................................................................150\n\nConclusion ..........................................................................................................154\n\nPoints to remember ...........................................................................................154\n\nMultiple choice questions .................................................................................155\n\nAnswers .........................................................................................................155\n\n8. Build Native ML Apps ...........................................................................................157\n\nIntroduction ........................................................................................................157\n\nStructure ..............................................................................................................157\n\nObjectives ............................................................................................................158\n\nIntroduction to Tkinter .....................................................................................158\n\nHello World app using Tkinter ......................................................................159\n\nBuild an ML-based app using Tkinter ............................................................160\n\nTkinter app .....................................................................................................163\n\nConvert Python app into Windows EXE file .................................................167\n\nBuild an ML-based app using kivy and kivyMD .........................................170\n\nKivyMD app ..................................................................................................173",
      "content_length": 3713,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Convert the Python app into an Android app ..............................................179\n\nConclusion ..........................................................................................................180\n\nPoints to remember ...........................................................................................181\n\nMultiple choice questions .................................................................................181\n\nAnswers .........................................................................................................181\n\nQuestions ............................................................................................................181\n\n9. CI/CD for ML ...........................................................................................................183\n\nIntroduction ........................................................................................................183\n\nStructure ..............................................................................................................183\n\nObjectives ............................................................................................................184\n\nCI/CD pipeline for ML .....................................................................................184\n\nContinuous Integration (CI) .............................................................................185\n\nContinuous Delivery/Deployment (CD) .......................................................186\n\nContinuous Training (CT) ................................................................................186\n\nIntroduction to Jenkins .....................................................................................187\n\nInstallation .....................................................................................................187\n\nBuild CI/CD pipeline using GitHub, Docker, and Jenkins .........................189\n\nDevelop codebase ............................................................................................189\n\nCreate a Personal Access Token (PAT) on GitHub ........................................196\n\nCreate a webhook on the GitHub repository ..................................................197\n\nConfigure Jenkins ..............................................................................................199\n\nCreate CI/CD pipeline using Jenkins .............................................................202\n\nStage 1: 1-GitHub-to-container .....................................................................203\n\nStage 2: 2-training .........................................................................................205\n\nStage 3: 3-testing ...........................................................................................207\n\nStage 4: 4-deployment-status-email ..............................................................210\n\nConclusion ..........................................................................................................217\n\nPoints to remember ...........................................................................................217\n\nMultiple choice questions .................................................................................218\n\nAnswers .........................................................................................................218\n\nQuestions ............................................................................................................218\n\n xix",
      "content_length": 3482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "xx \n\n10. Deploying ML Models on Heroku ......................................................................219\n\nIntroduction ........................................................................................................219\n\nStructure ..............................................................................................................219\n\nObjectives ............................................................................................................220\n\nHeroku .................................................................................................................220\n\nSetting up Heroku .............................................................................................221\n\nDeployment with Heroku Git ..........................................................................222\n\nDeployment with GitHub repository integration .........................................222\n\nREVIEW APPS .............................................................................................223\n\nSTAGING ......................................................................................................223\n\nPRODUCTION ............................................................................................224\n\nHeroku Pipeline flow ......................................................................................224\n\nDeployment with Container Registry.............................................................225\n\nGitHub Actions ..................................................................................................225\n\nConfiguration .................................................................................................226\n\nCI/CD pipeline using GitHub Actions and Heroku ....................................227\n\nConclusion ..........................................................................................................246\n\nPoints to remember ...........................................................................................246\n\nMultiple choice questions .................................................................................247\n\nAnswers .........................................................................................................247\n\nQuestions ............................................................................................................247\n\n11. Deploying ML Models on Microsoft Azure .......................................................249\n\nIntroduction ........................................................................................................249\n\nStructure ..............................................................................................................249\n\nObjectives ............................................................................................................250\n\nAzure ...................................................................................................................250\n\nSet up an Azure account ...................................................................................251\n\nDeployment using GitHub Actions ................................................................251\n\nInfrastructure setup .......................................................................................263\n\nAzure Container Registry ...................................................................................263\n\nAzure App Service ...............................................................................................267\n\nGitHub Actions .............................................................................................270",
      "content_length": 3650,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Service principal ............................................................................................273\n\nConfigure Azure App Service to use GitHub Actions for CD ......................274\n\nDeployment using Azure DevOps and Azure ML .......................................278\n\nAzure Machine Learning (AML) service ........................................................279\n\nWorkspace ......................................................................................................280\n\nExperiments ...................................................................................................280\n\nRuns ...............................................................................................................280\n\nConfigure CI pipeline ........................................................................................290\n\nConfigure CD pipeline ......................................................................................292\n\nConclusion ..........................................................................................................299\n\nPoints to remember ...........................................................................................299\n\nMultiple choice questions .................................................................................299\n\nAnswers .........................................................................................................300\n\nQuestions ............................................................................................................300\n\n12. Deploying ML Models on Google Cloud Platform ..........................................301\n\nIntroduction ........................................................................................................301\n\nStructure ..............................................................................................................301\n\nObjectives ............................................................................................................302\n\nGoogle Cloud Platform (GCP) .........................................................................302\n\nSet up the GCP account .................................................................................303\n\nCloud Source Repositories ...............................................................................304\n\nCloud Build ........................................................................................................309\n\nContainer Registry ............................................................................................. 311\n\nKubernetes ..........................................................................................................312\n\nGoogle Kubernetes Engine (GKE) ...................................................................313\n\nDeployment using Cloud Shell – Manual Trigger ........................................316\n\nCI/CD pipeline using Cloud Build .................................................................317\n\nCreate a trigger in Cloud Build .....................................................................318\n\nConclusion ..........................................................................................................323\n\nPoints to remember ...........................................................................................323\n\nMultiple choice questions .................................................................................323\n\nAnswers .........................................................................................................324\n\n xxi",
      "content_length": 3584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "xxii \n\nQuestions ............................................................................................................324\n\n13. Deploying ML Models on Amazon Web Services ............................................325\n\nIntroduction ........................................................................................................325\n\nStructure ..............................................................................................................325\n\nObjectives ............................................................................................................326\n\nIntroduction to Amazon Web Services (AWS) ...............................................326\n\nAWS compute services ...................................................................................326\n\nAmazon Elastic Compute Cloud (EC2) .........................................................327\n\nAmazon Elastic Container Service (ECS) .......................................................327\n\nAmazon Elastic Kubernetes Service (EKS) ....................................................327\n\nAmazon Elastic Container Registry (ECR) ....................................................327\n\nAWS Fargate ..................................................................................................328\n\nAWS Lambda .................................................................................................328\n\nAmazon SageMaker ..........................................................................................328\n\nSet up an AWS account .....................................................................................329\n\nAWS CodeCommit ............................................................................................331\n\nContinuous Training .....................................................................................336\n\nAmazon Elastic Container Registry (ECR) ....................................................336\n\nDocker Hub rate limit ....................................................................................337\n\nAWS CodeBuild .................................................................................................339\n\nAttach container registry access to CodeBuild’s service role .........................345\n\nAmazon Elastic Container Service (ECS) .......................................................346\n\nAWS ECS deployment models .......................................................................347\n\nEC2 instance .......................................................................................................347\n\nFargate .................................................................................................................348\n\nTask definition ................................................................................................349\n\nRunning task with the task definition .................................................................350\n\nAmazon VPC and subnets ..................................................................................351\n\nLoad balancing ...............................................................................................352\n\nTarget group ........................................................................................................353\n\nSecurity Groups ..................................................................................................356\n\nApplication Load Balancers (ALB) .....................................................................358",
      "content_length": 3524,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": " xxiii\n\nService ............................................................................................................363\n\nCI/CD pipeline using CodePipeline ..............................................................369\n\nAWS CodePipeline .........................................................................................371\n\nMonitoring ..........................................................................................................378\n\nConclusion ..........................................................................................................379\n\nPoints to remember ...........................................................................................379\n\nMultiple choice questions .................................................................................380\n\nAnswers .........................................................................................................380\n\nQuestions ............................................................................................................380\n\n14. Monitoring and Debugging ..................................................................................381\n\nIntroduction ........................................................................................................381\n\nStructure ..............................................................................................................381\n\nObjectives ............................................................................................................382\n\nImportance of monitoring ................................................................................382\n\nFundamentals of ML monitoring ....................................................................383\n\nMetrics for monitoring your ML system ........................................................385\n\nDrift in ML ..........................................................................................................386\n\nTypes of drift in ML .......................................................................................386\n\nTechniques to detect the drift in ML ..............................................................388\n\nAddressing the drift in ML ............................................................................389\n\nOperational monitoring with Prometheus and Grafana ..............................390\n\nML model monitoring with whylogs and WhyLabs ....................................402\n\nwhylogs ..........................................................................................................403\n\nConstraints for data quality validation .........................................................404\n\nWhyLabs ........................................................................................................407\n\nConclusion ..........................................................................................................416\n\nPoints to remember ...........................................................................................416\n\nMultiple choice questions .................................................................................417\n\nAnswers .........................................................................................................417\n\nQuestions ............................................................................................................417",
      "content_length": 3399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "xxiv \n\n15. Post-Productionizing ML Models ........................................................................419\n\nIntroduction ........................................................................................................419\n\nStructure ..............................................................................................................419\n\nObjectives ............................................................................................................420\n\nBridging the gap between the ML model and the creation of business value ....................................................................................................420\n\nModel security ....................................................................................................420\n\nAdversarial attack ..........................................................................................421\n\nData poisoning attack ....................................................................................422\n\nDistributed Denial of Service attack (DDoS) ................................................422\n\nData privacy attack ........................................................................................422\n\nMitigate the risk of model attacks ..................................................................423\n\nA/B testing .........................................................................................................423\n\nMLOps is the future ..........................................................................................424\n\nConclusion ..........................................................................................................425\n\nPoints to remember ...........................................................................................425\n\nMultiple choice questions .................................................................................425\n\nAnswers .........................................................................................................426\n\nQuestions ............................................................................................................426\n\nIndex ...................................................................................................................427-434",
      "content_length": 2285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Python 101  1\n\nChapter 1 Python 101\n\nIntroduction Python 101 guides you with everything from the installation of Python to data manipulation in Pandas DataFrame. In a nutshell, this chapter is designed to equip you with the prerequisites required for this book. It is a quick refresher for those who have worked with Python. And if you are a beginner, this chapter is going to cover the most common Python commands required to build and deploy machine models.\n\nStructure\n\nThis chapter covers the following topics: • Installation of Python\n\nHello World! •\t Data structures •\t Control statements and loops •\t Functions •\t OOPs •\t NumPy •\t Pandas",
      "content_length": 644,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "2  Machine Learning in Production\n\nObjectives After studying this chapter, you should be able to install Python on your machine, and store and manage data using common data structures in Python. You should be able to use Python's data processing packages for data manipulation, and you should also know how to create classes, methods, and objects.\n\nInstall Python Make sure the Python version you are installing is compatible with the libraries and applications you will work on. Also, maintain the same Python version throughout the project to avoid any errors or exceptions. Here, you will install Python version 3.6.10.\n\nOn Windows and Mac OS Here’s the link to install Python 3.6.10:\n\nhttps://www.python.org/downloads/release/python-3610/\n\nOn Linux Ubuntu 16.10 and 17.04: sudo apt update\n\nsudo apt install python3.6\n\nUbuntu 17.10, 18.04 (Bionic) and onward:\n\nUbuntu 17.10 and 18.04 already come with Python 3.6 as default. Just run python3 in the terminal to invoke it.\n\nInstall Anaconda Anaconda Individual Edition contains conda and Anaconda Navigator, as well as Python and hundreds of scientific packages. When you install Anaconda, all of these will also get installed.\n\nhttps://docs.anaconda.com/anaconda/install/\n\nNote: Review the system requirements listed on the site before installing Anaconda Individual Edition.",
      "content_length": 1329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "Python 101  3\n\nInstall code editor Any of the following code editors can be chosen; however, the preferred one is Visual Studio Code:\n\nVisual Studio Code (https://code/visualstudio.com)\n\nSublime Text (https://www.sublimetext.com)\n\nNotepad++ (https://notepad-plus-plus.org/downloads/)\n\nHello World! Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its simple, easy-to-learn syntax emphasizes readability and therefore, reduces the cost of program maintenance. Python supports modules and packages, which encourages modular programming and code reusability. It is developed under an OSI-approved open-source license, making it freely usable and distributable, even for commercial use. The Python Package Index (PyPI) hosts thousands of third-party modules for Python.\n\nOpen the terminal and type a python to open the Python console.\n\nNow you are in the Python console.\n\nExample: 1.1 Hello world in Python\n\n1. print(\"Hello World\")\n\n2. Hello World\n\nTo come out of the console you can use:\n\n1. exit()\n\nExecution of Python file Let’s create a file named hello_world.py and add the print(\"Hello World\") to it.\n\nNow, run the file in the terminal:\n\npython hello_world.py\n\nYou should get the following output:\n\nHello World",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "4  Machine Learning in Production\n\nData structures Data structures are nothing but particular ways of storing and managing data in memory so that it can be easily accessed and modified later. Python comes with a comprehensive set of data structures, which play an important role in programming because they are reusable, easily accessible, and manageable.\n\nCommon data structures You will study some of the common data structures in this section.\n\nArray\n\nArrays are collections of homogeneous items. One can use the same data type in a single array.\n\nExample: 1.2 Array data type\n\n1. import array as arr\n\n2.\n\n3. my_array = arr.array(\"i\", (1, 2, 3, 4, 5))\n\n4.\n\n5. print(my_array)\n\n6. array('i', [1, 2, 3, 4, 5])\n\n7.\n\n8. print(my_array[1])\n\n9. 2\n\nDictionary\n\nDictionaries are defined as comma separated key:value pairs enclosed in curly braces.\n\nExample: 1.3 Dictionary data type\n\n1. my_dict = {'name': 'Adam', 'emp_id': 3521, 'dept':'Marketing'}\n\n2. print(my_dict['name'])\n\n3. Adam\n\nList\n\nIt is a collection of heterogeneous items enclosed in square brackets. One can use the same or different data types in a list.",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Python 101  5\n\nExample: 1.4 List data type\n\n1. my_list=['apple', 4, 'banana', 6]\n\n2. print(my_list[0])\n\n3. apple\n\nSet\n\nA set is a collection of unique (non-duplicate) elements enclosed in curly braces. A set can take heterogeneous elements.\n\nExample: 1.5 Set data type\n\n1. my_set = {3,5,6}\n\n2. print(my_set)\n\n3. {3, 5, 6}\n\n1. my_set = {1, 2.0, (3,5,6), 'Test'}\n\n2. print(my_set)\n\n3. {1, 2.0, 'Test', (3, 5, 6)}\n\nString\n\nA string is used to store text data. It can be represented by single quotes ('') or double quotes (\"\").\n\nExample: 1.6 String data type\n\n1. print('The cat was chasing the mouse')\n\n2. The cat was chasing the mouse\n\nTuple\n\nTuples are collections of elements surrounded by round brackets (optional), and they are immutable, that is, they cannot be changed.\n\nExample: 1.7 Tuple data type\n\n1. my_tuple_1 = ('apple', 4, 'banana', 6)\n\n2. print(my_tuple_1[0])\n\n3. apple\n\nTuple can be declared without round brackets, as follows:\n\n1. my_tuple_2 = 1, 2.0, (3,5,6), 'Test'\n\n2. print(my_tuple_2[0])\n\n3. 1",
      "content_length": 1012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "6  Machine Learning in Production\n\nControl statements and loops Python has several types of control statements and loops. Let’s look at them.\n\nif…else The if statement executes a block of code if the specified condition is true. Whereas, the else statement executes a block of code if the specified condition is false. Furthermore, you can use the elif (else if) statement to check the new condition if the first condition is false.\n\nSyntax:\n\nif condition1:\n\nCode to be executed\n\nelif condition2:\n\nCode to be executed\n\nelse:\n\nCode to be executed\n\nExample: 1.8 If…else control statement\n\n1. x = 26\n\n2. y = 17\n\n3.\n\n4. if(x > y):\n\n5. print('x is greater than y')\n\n6. elif(x == y):\n\n7. print('x and y are equal')\n\n8. else:\n\n9. print('y is greater than x')\n\n10.\n\n11. x is greater than y\n\nfor loop It is used when you want to iterate over a sequence or iterable such as a string, list, dictionary, or tuple. It will execute a block of code a certain number of times.\n\nSyntax:",
      "content_length": 970,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Python 101  7\n\nfor val in sequence:\n\nCode to be executed\n\nExample: 1.9 For loop\n\n1. num = 2\n\n2. for i in range(1, 11):\n\n3. print(num, 'X', i, '=', num*i)\n\n4.\n\n5. 2 X 1 = 2\n\n6. 2 X 2 = 4\n\n7. 2 X 3 = 6\n\n8. 2 X 4 = 8\n\n9. 2 X 5 = 10\n\n10. 2 X 6 = 12\n\n11. 2 X 7 = 14\n\n12. 2 X 8 = 16\n\n13. 2 X 9 = 18\n\n14. 2 X 10 = 20\n\nIn the preceding example, the range() function generates a sequential set of numbers using start, stop, and step parameters.\n\nwhile loop It is used when you want to execute a block of code indefinitely until the specified condition is met. Furthermore, you can use an else statement to execute a block of code once when the first condition is false.\n\nSyntax:\n\nwhile condition:\n\nCode to be executed\n\nExample: 1.10 While loop\n\n1. num = 1\n\n2. while num<= 10:\n\n3. if num % 2 == 0:\n\n4. print(num)\n\n5. num = num + 1",
      "content_length": 821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "8  Machine Learning in Production\n\n6.\n\n7. 2\n\n8. 4\n\n9. 6\n\n10. 8\n\n11. 10\n\npass statement In Python, the pass statement acts as a placeholder. If you are planning to add code later in loops, function definitions, class definitions, or if statements, you can write a pass to avoid any errors as it does nothing. It allows developers to write the logic or condition afterward and continue the execution of the remaining code.\n\nSyntax:\n\nif condition:\n\npass\n\nExample: 1.11 Pass statement\n\n1. num = 10\n\n2. if num % 2 == 0:\n\n3. pass\n\nFunctions Developers use functions to perform the same task multiple times. A function is a block of code that can take arguments, perform some operations, and return values. Python enables developers to use predefined or built-in functions, or they can write user-defined functions to perform a specific task.\n\nExample: 1.12 Pre-defined or built-in functions\n\n1. print(\"Hello World\")\n\n2. Hello World # Output\n\nExample: 1.13 User-defined functions\n\n1. a = 5\n\n2. b = 2\n\n3.\n\n4. # Function definition",
      "content_length": 1023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Python 101  9\n\n5. def add():\n\n6. print(a+b)\n\n7.\n\n8. # Calling a function\n\n9. add()\n\n10.\n\n11. # Output\n\n12. 7\n\nObject Oriented Programming (OOP) Python is an object-oriented language, so everything in Python is an object. Let’s understand its key concepts and their importance:\n\nClass: A class acts like a template for the objects. It is defined using the class keyword like the def keyword is used while creating a new function. A Python class contains objects and methods, and they can be accessed using the period (.).\n\nObject: An object is an instance of a class. It depicts the structure of the class and contains class variables, instance variables, and methods.\n\nMethod: In simple words, it is a function defined while creating a class.\n\n__init__: For the automatic initialization of data members by assigning values, you should use the __init__ method while creating an instance of a class. It gets called every time you create an object of the class. The usage is equivalent to the constructor in C++ and Java.\n\nSelf: It helps us access the methods and attributes of the class. You are free to name it anything; however, as per convention and for readability, it is better to declare it as self.\n\nExample: 1.14 Class definition\n\n1. class Multiply:\n\n2. def mul(self):\n\n3. result = self.num1 * self.num2\n\nClass definition\n\n4. return result\n\n5.\n\n6. def __init__(self,num1,num2):\n\n7. self.num1 = num1\n\n8. self.num2 = num2",
      "content_length": 1426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "10  Machine Learning in Production\n\n9.\n\n10.\n\na = Multiply(5,6)\n\n11.\n\nPassing parameters to class\n\n12. a.mul() 13. 30\n\nCalling method of class\n\nNote: Follow standard naming conventions for variables, functions, and methods. For example:\n\nFor variables, functions, methods, packages, and modules: my_variable •\t For classes and exceptions: MyClass •\t For constants: MY_CONSTANT\n\nNumerical Python (NumPy) In a nutshell, it is an array processing package. NumPy stands for Numerical Python. By default, the data type of the NumPy array is defined by its elements. In NumPy, the dimension of arrays is referred to by rank; for example, a 2-D array means Rank 2 array.\n\nNumPy is popular because of its speed. While working on a large dataset, you will see the difference if you use NumPy.\n\nYou can install the NumPy package using pip:\n\npip install numpy\n\nExample: 1.15 NumPy array\n\n1. import numpy as np\n\n2. # Rank 0 Array (scaler)\n\n3. my_narray = np.array(42)\n\n4. print(my_narray)\n\n5. 42\n\n6.\n\n7. print(\"Dimension: \", my_narray.ndim)\n\n8. Dimension: 0\n\n9.\n\n10.\n\nprint(\"Shape: \", my_narray.shape)\n\n11.\n\nShape: ()\n\n12.",
      "content_length": 1110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Python 101  11\n\n13.\n\nprint(\"Size: \", my_narray.size)\n\n14.\n\nSize: 1\n\n1. # Rank 1 Array (vector)\n\n2. my_narray = np.array([4,5,6])\n\n3. print(my_narray)\n\n4. [4 5 6]\n\n5. print(\"Dimension: \", my_narray.ndim)\n\n6. Dimension: 1\n\n7. print(\"Shape: \", my_narray.shape)\n\n8. Shape: (3,)\n\n9. print(\"Size: \", my_narray.size)\n\n10.\n\nSize: 3\n\n1. # Rank 2 Array (matrix)\n\n2. my_narray = np.array([[1,2,3],[4,5,6]])\n\n3. print(my_narray)\n\n4. [[1 2 3]\n\n5. [4 5 6]]\n\n6. print(\"Dimension: \", my_narray.ndim)\n\n7. Dimension: 2\n\n8. print(\"Shape: \", my_narray.shape)\n\n9. Shape: (2, 3)\n\n10.\n\nprint(\"Size: \", my_narray.size)\n\n11.\n\nSize: 6\n\n1. # Rank 3 Array\n\n2. my_narray = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\n\n3. print(my_narray)\n\n4. [[[ 1 2 3]\n\n5. [ 4 5 6]]\n\n6. [[ 7 8 9]\n\n7. [10 11 12]]]\n\n8. print(\"Dimension: \", my_narray.ndim)\n\n9. Dimension: 3\n\n10.\n\nprint(\"Shape: \", my_narray.shape)\n\n11.\n\nShape: (2, 2, 3)",
      "content_length": 897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "12  Machine Learning in Production\n\n12.\n\nprint(\"Size: \", my_narray.size)\n\n13.\n\nSize: 12\n\nReshaping array Suppose you want to change the shape of a ndarray (N-dimension array) without losing the data; it can be done using the reshape() or flatten() method.\n\nExample: 1.16 Reshaping NumPy array\n\n1. # Rank 1 Array\n\n2. my_1array = np.array([1,2,3,4,5,6])\n\n3. print(my_1array)\n\n4. [1 2 3 4 5 6]\n\n1. # converting Rank 1 array to Rank 2 array\n\n2. my_2array = my_1array.reshape(2, 3)\n\n3. print(my_2array)\n\n4. [[1 2 3]\n\n5. [4 5 6]]\n\n1. #converting Rank 2 array to Rank 1 array\n\n2. my_1array = my_2array.flatten()\n\n3. print(my_1array)\n\n4. [1 2 3 4 5 6]\n\nNote: Here, you can use my_2array.reshape(6) instead of flatten().\n\nPandas Pandas is an open-source, BSD-licensed library providing high-performance, easy- to-use data structures and data analysis tools for the Python programming language. A DataFrame is a 2-dimensional labeled data structure with columns of potentially different types.\n\nYou can install the Pandas package using pip:\n\npip install pandas\n\nExample: 1.17 Pandas DataFrame usage\n\n1. import pandas as pd\n\n2. df = pd.DataFrame(data={'fruit': ['apple', 'banana', 'orange',",
      "content_length": 1180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Python 101  13\n\n'mango',\n\n3. 'size': ['large', 'medium', 'small',\n\n'medium'],\n\n4. 'quantity': [5,4,8, 6]})\n\n5.\n\n6. print(df)\n\n7. fruit size quantity\n\n8. 0 apple large 5\n\n9. 1 banana medium 4\n\n10. 2 orange small 8\n\n11. 3 mango medium 6\n\nIn the following example, head() retrieves the top rows of the Pandas DataFrame.\n\nExample: 1.18 Get first n rows of DataFrame\n\n1. df.head(2)\n\n2. fruit size quantity\n\n3. 0 apple large 5\n\n4. 1 banana medium 4\n\nWhereas tail() retrieves the bottom rows of the Pandas DataFrame.\n\nExample: 1.19 Get the last n rows of DataFrame\n\n1. df.tail(2)\n\n2. fruit size quantity\n\n3. 2 orange small 8\n\n4. 3 mango medium 6\n\nIn the following example, describe() shows a quick statistical summary of numerical columns of the DataFrame.\n\nExample: 1.20 Get basic statistical information\n\n1. df.describe()\n\n2. quantity\n\n3. count 4.000000\n\n4. mean 5.750000\n\n5. std 1.707825\n\n6. min 4.000000",
      "content_length": 901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "14  Machine Learning in Production\n\n7. 25% 4.750000\n\n8. 50% 5.500000\n\n9. 75% 6.500000\n\n10. max 8.000000\n\n1. df['fruit']\n\n2. 0 apple\n\n3. 1 banana\n\n4. 2 orange\n\n5. 3 mango\n\n6. Name: fruit, dtype: object\n\nloc - selection by label A loc gets rows (and/or columns) with specific labels. To get all the rows in which fruit size is medium, loc is written as follows:\n\nExample: 1.21 Get the data by location\n\n1. df.loc[df['size'] == 'medium']\n\n2. fruit size quantity\n\n3. 1 banana medium 4\n\n4. 3 mango medium 6\n\niloc - selection by position iloc gets rows (and/or columns) at the index’s locations. To get the row at index 2, iloc is written as follows:\n\nExample: 1.22 Get the data by position\n\n1. df.iloc[2]\n\n2. fruit orange\n\n3. size small\n\n4. quantity 8\n\n5. Name: 2, dtype: object\n\nTo retrieve the rows from index 2 to 3 and columns 0 to 1, iloc is written as follows:\n\n1. df.iloc[2:4, 0:2]\n\n2. fruit size\n\n3. 2 orange small\n\n4. 3 mango medium",
      "content_length": 937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Python 101  15\n\nYou can call groupby() and pass the size, that is, the column you want the group on, and then use the sum() aggregate method.\n\nExample: 1.23 Group the data, then use an aggregate function\n\n1. df.groupby(['size'])['quantity'].sum()\n\n2. size\n\n3. large 5\n\n4. medium 10\n\n5. small 8\n\n6. Name: quantity, dtype: int64\n\nSave and load CSV files Example: 1.24 Save and load CSV files\n\n1. df.to_csv(\"fruit.csv\")\n\n2. df = pd.read_csv(\"fruit.csv\")\n\nSave and load Excel files Example: 1.25 Save and load Excel files\n\n1. df.to_excel(\"fruit.xlsx\", sheet_name=\"Sheet1\")\n\n2. df = pd.read_excel(\"fruit.xlsx\", \"Sheet1\", index_col=None)\n\nFor more information, you can refer to the official documentation at https://pandas. pydata.org/docs/.\n\nConclusion This chapter discussed the essential concepts of the Python language with examples. At the beginning of this chapter, you learned how to install Python in the system. Then, you studied the common data structures and object-oriented programming concepts in Python with examples. Further on, data pre-processing commands were covered using packages like NumPy and Pandas, which will play a major role in developing machine learning models.\n\nIn the next chapter, you will learn about git and GitHub with examples.\n\nPoints to remember\n\nEnsure to use the same Python version throughout the machine learning project.",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "16  Machine Learning in Production\n\nThe __init()__ method in a class is used for auto initialization of data members and methods. However, it is not mandatory.\n\nTuples are immutable, that is, they are not changeable, but if they contain elements like a list, then the list is mutable but not the entire tuple.\n\nMultiple choice questions\n\n1. What is the output of np.array([[1,2,3],[4,5,6]])?\n\na) Rank 1 array b) Rank 2 array\n\nc) Rank 2 array d) All of the above\n\n2.\n\nSets can take _________ elements.\n\na) heterogenous\n\nb) homogeneous\n\nc) only integer d) only string\n\nAnswers 1. b\n\n2. a\n\nQuestions\n\n1. What is the difference between a list and a tuple? 2. What is the use of __init__()?\n\n3. When should you use a pass statement?\n\nKey terms Object-Oriented Programming: It refers to a programming paradigm based on the concept of objects, which can contain data and methods.\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 1074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "Git and GitHub Fundamentals  17\n\nChapter 2 Git and GitHub Fundamentals\n\nIntroduction It is difficult to maintain multiple versions of files while working in a team. Git solves this problem by allowing developers to collaborate and share files with other team members. This chapter covers the most needed basic Git and GitHub concepts that form the foundation of this book. It explains Git configuration, forking Git repo, pushing your codebase to GitHub, and the cloning process. In addition, it covers an introduction to GitHub Actions, common Git commands with syntax, and examples. Maintaining multiple versions of files when working in a team can be difficult, but Git is the solution.\n\nStructure The following topics will be covered in this chapter:\n\nGit concepts •\t Common Git workflow •\n\nInstall Git and create a GitHub account\n\nCommon Git commands •\t Let's Git",
      "content_length": 869,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "18  Machine Learning in Production\n\nObjectives After studying this chapter, you should know how to execute Git commands and connect to remote GitHub repositories. Understand and execute Git processes. You should know how to push, fetch, and revert changes to the remote Git repository, and you should be familiar with how to install and perform the basic configuration of Git on your machine or server.\n\nGit concepts Git is an open-source Distributed Version Control System (DVCS). A version control system allows you to record changes to files over a period.\n\nGit is used to maintain the historical and current versions of source code. In a project, developers have a copy of all versions of the code stored in the central server.\n\nGit allows developers to do the following:\n\nTrack the changes, who made the changes, and when •\t Rollback/restore changes •\t Allow multiple developers to coordinate and work on the same files •\t Maintain a copy of the files at the remote and local level\n\nThe following image depicts Git as a VCS in a team where developers can work simultaneously on the same files and keep track of who made the changes. Here, F1, F2, and F3 are file names.\n\nFigure 2.1: Git scenario in team",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "Git and GitHub Fundamentals  19\n\nGit + Hub = GitHub Git and GitHub are separate entities. Git is a command-line tool, whereas GitHub is a platform for collaboration. You can store files and folders on GitHub and implement changes to existing projects. By creating a separate branch, you can isolate these changes from your existing project files.\n\nGitHub Actions makes it easy to automate all your software workflows with Continuous Integration/Continuous Deployment. You can build, test, and deploy the code right from GitHub.\n\nCommon Git workflow The following figure depicts the general Git workflow. It covers operations like creating or cloning the Git repository, updating the local repository by pulling files from the remote repository, and pushing the local changes to the remote repository.\n\nFigure 2.2: Git workflow",
      "content_length": 827,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "20  Machine Learning in Production\n\nInstall Git and create a GitHub account First off, you will need to install Git and then create an account on GitHub. To install Git on your machine, follow the given instructions.\n\nLinux (Debian/Ubuntu) In Ubuntu, open the terminal and install Git using the following commands: $ sudo apt-get update\n\n$ sudo apt-get install git\n\nGit for all platforms Download Git for your machine from the official website:\n\nhttps://git-scm.com/downloads\n\nGUI clients Git comes with built-in GUI tools; however, you can explore other third-party GUI tools at https://git-scm.com/downloads/guis.\n\nClick on the download link for your operating system and then follow the installation steps.\n\nAfter installing it, start your terminal and type git --version to verify the Git installation.\n\nIf everything goes well, you will see the Git installed successfully.\n\nCreate a GitHub account If you are new to GitHub, then can join GitHub at https://github.com/join.\n\nIf you’re an existing user, sign in to your account.\n\nCreate a new repository by clicking on the plus sign + (top-right corner):\n\nFigure 2.3: Creating a new repository on GitHub",
      "content_length": 1157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Git and GitHub Fundamentals  21\n\nHit the Create repository button. Complete the required fields, and your Git repo will be created.\n\nYou can download and install the GitHub desktop from https://desktop.github. com/.\n\nCommon Git commands Some of the common Git commands are discussed in this section.\n\nSetup Set a name that is identifiable for credit when reviewing the version history: git config --global user.name “[firstname lastname]”\n\nSet an email address that will be associated with each history marker: git config --global user.email “[valid-email-id]”\n\nNew repository Initialize an existing directory as a Git repository: git init\n\nRetrieve an entire repository from a hosted location via URL: git clone [url]\n\nUpdate Fetch and merge any commits from the remote branch: git pull\n\nFetch all the branches from the remote Git repo: git fetch [alias]\n\nChanges View modified files in the working directory staged for your next commit: git status\n\nAdd a file to your next commit (stage): git add [file]",
      "content_length": 1006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "22  Machine Learning in Production\n\nCommit your staged content as a new commit snapshot: git commit -m “[descriptive message]”\n\nTransfer local branch commits to the remote repository branch: git push [alias] [branch]\n\nRevert View all the commits in the current branch’s history: git log\n\nSwitch to another branch: git checkout ['branch_name']\n\nLet’s Git Create a new directory code and switch to the code directory: suhas@test:~/code$ sudo chmod -R 777 /home/suhas/code\n\nConfiguration Provide the username and the user’s email: suhas@test:~/code$ git config --global user.name \"Suhas\"\n\nsuhas@test:~/code$ git config --global user.email \"suhasp.ds@gmail.com\"\n\nNote: Always read the output of commands to know what happened in the background.\n\nInitialize the Git repository suhas@test:~/code$ git init\n\nInitialized empty Git repository in /home/suhas/code/.git/\n\nCheck Git status suhas@test:~/code$ git status\n\nOn branch master\n\nNo commits yet\n\nnothing to commit (create/copy files and use \"git add\" to track)",
      "content_length": 1008,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "Git and GitHub Fundamentals  23\n\nAdd a new file A new file hello.py has been added using the touch command: suhas@test:~/code$ touch hello.py\n\nYou can notice the change in the output of the git status: suhas@test:~/code$ git status\n\nOn branch master\n\nNo commits yet\n\nUntracked files:\n\n(use \"git add <file>...\" to include in what will be committed)\n\nhello.py\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\nAdd the hello.py file to staging: suhas@test:~/code$ git add hello.py\n\nAgain, check the output of the Git status: suhas@test:~/code$ git status\n\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n\n(use \"git rm --cached <file>...\" to unstage)\n\nnew file: hello.py\n\nCommit with the message New file:\n\nsuhas@test:~/code$ git commit -m \"New file\"\n\n[master (root-commit) bd49a5e] New file\n\n1 file changed, 0 insertions(+), 0 deletions(-)\n\ncreate mode 100755 hello.py\n\nCheck the output of Git status: suhas@test:~/code$ git status\n\nOn branch master\n\nnothing to commit, working tree clean\n\nUpdate the hello.py file using the nano command: suhas@test:~/code$ sudo nano hello.py",
      "content_length": 1114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "24  Machine Learning in Production\n\nNow, add the print (“Hello Word!”) to the hello.py file.\n\nYou can see the changes in the output of the git status: suhas@test:~/code$ git status\n\nOn branch master\n\nChanges not staged for commit:\n\n(use \"git add <file>...\" to update what will be committed)\n\n(use \"git checkout -- <file>...\" to discard changes in working directory)\n\nmodified: hello.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\nAdd the hello.py file to staging: suhas@test:~/code$ git add hello.py\n\nCommit along with the message added print text: suhas@test:~/code$ git commit -m \"added print text\"\n\n[master af7cfeb] added print text\n\n1 file changed, 1 insertion(+)\n\nView the logs using the git log command: suhas@test:~/code$ git log\n\ncommit af7cfeb53ff6dc49126d24aedb20a065c54ef4a0 (HEAD -> master)\n\nAuthor: “Suhas Pote” <“suhasp.ds@gmail.com”>\n\nDate: Sun Jul 5 21:34:15 2020 +0530\n\nadded print text\n\ncommit bd49a5e5280de35811848a80299d900e6e6509ce\n\nAuthor: “Suhas Pote” <“suhasp.ds@gmail.com”>\n\nDate: Sun Jul 5 21:26:12 2020 +0530\n\nNew file\n\nGit identifies each commit uniquely using the SHA1 hash function, based on the contents of the committed files. So, each commit is identified with a 40-character- long hexadecimal string. suhas@test:~/code$ git checkout af7cfeb53ff6dc49126d24aedb20a065c54ef4a0\n\nNote: checking out 'bd49a5e5280de35811848a80299d900e6e6509ce'.\n\nYou are in a 'detached HEAD' state. You can look around, make experimental",
      "content_length": 1472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Git and GitHub Fundamentals  25\n\nchanges and commit them, and you can discard any commits you make in this\n\nstate without impacting any branches by performing another checkout.\n\nIf you want to create a new branch to retain commits you create, you may\n\ndo so (now or later) by using -b with the checkout command again. Example:\n\ngit checkout -b <new-branch-name>\n\nHEAD is now at bd49a5e New file\n\nIf you notice, the hello.py file is backed to the previous state, that is, a blank file. It is like a time machine!\n\nNow, let’s get back to the latest version of hello.py. suhas@test:~/code$ git reset --hard af7cfeb53ff6dc49126d24aedb20a065c54ef4a0\n\nHEAD is now at af7cfeb added print text\n\nIt’s time to push the files to the GitHub repo.\n\nsuhas@test:~/code$ git remote add origin https://github.com/suhas-ds/ myrepo.git\n\nsuhas@test:~/code$ git push -u origin master\n\nUsername for 'https://github.com': suhas-ds\n\nPassword for 'https://suhas-ds@github.com':\n\nCounting objects: 6, done.\n\nCompressing objects: 100% (2/2), done.\n\nWriting objects: 100% (6/6), 477 bytes | 477.00 KiB/s, done.\n\nTotal 6 (delta 0), reused 0 (delta 0)\n\nTo https://github.com/suhas-ds/myrepo\n\n[new branch] master -> master\n\nBranch 'master' is set up to track remote branch 'master' from 'origin'.\n\nNote: Here, the origin acts as an alias or label for the URL, and the master is a branch name.\n\nTo pull the latest version of the file execute the following command. suhas@test:~/code$ git pull origin master\n\nFrom https://github.com/suhas-ds/myrepo\n\nbranch master\n\n> FETCH_HEAD\n\nAlready up to date.",
      "content_length": 1566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "26  Machine Learning in Production\n\nIf you want to load files from another GitHub repository, or if you are working on another system and want to load files from your GitHub repository, you can achieve this by cloning it. suhas@test:~/myrepo$ git clone https://github.com/suhas-ds/myrepo.git\n\nCloning into 'myrepo'...\n\nremote: Enumerating objects: 6, done.\n\nremote: Counting objects: 100% (6/6), done.\n\nremote: Compressing objects: 100% (2/2), done.\n\nremote: Total 6 (delta 0), reused 6 (delta 0), pack-reused 0\n\nUnpacking objects: 100% (6/6), done.\n\nYou can practice frequently used Git commands with more files and Git repositories.\n\nConclusion In this chapter, you explored Git and GitHub, and you learned the basic Git commands, such as git commit and git push. Then, you installed and configured Git, and you pushed and cloned files from a remote GitHub repository. To use the GitHub platform, it is essential to know the Git workflow.\n\nIn the next chapter, you will learn about the challenges faced while deploying Machine Learning models in production and understand how to overcome them.\n\nPoints to remember\n\nGit identifies each commit uniquely using the SHA1 hash function, based on the contents of the committed files.\n\nGit is a command-line tool, whereas GitHub is a platform for collaboration. •\t Git is used to maintain the historical and current versions of source code.\n\nMultiple choice questions\n\n1. The ______ command shows all commits in the current branch’s history.\n\na) git checkout\n\nb) git push [alias] [branch]\n\nc) git log\n\nd) git pull",
      "content_length": 1558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Git and GitHub Fundamentals  27\n\n2. Which of the following statements does not apply to git?\n\na) Tracks who created/made what changes and when b) Rollback/restore changes c) Allow multiple developers to coordinate and work on the same files d) Does not maintain a copy of the files at a remote and local repository\n\nAnswers 1. c\n\n2. d\n\nQuestions\n\n1. What is Git? What is GitHub?\n\n2. What is the command to upload changes to the remote repository?\n\n3. How do you roll back/revert the changes?\n\n4. How do you initialize a new repository?\n\n5. What is the use of the Git status command?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "28  Machine Learning in Production",
      "content_length": 35,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Challenges in ML Model Deployment  29\n\nChapter 3 Challenges in ML Model Deployment\n\nIntroduction A study found that 87% of Data Science and Machine Learning projects never make it into production. In this chapter, you will study common problems you may face while deploying machine learning models.\n\nDeploying ML models is entirely based on your end goals, such as frequency of predictions, latency, number of users, single or batch predictions, and accessibility.\n\nStructure\n\nThis chapter covers the following topics: •\t ML life cycle •\t Types of model deployment •\t Challenges while deploying models •\t MLOps •\t Benefits of MLOps",
      "content_length": 632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "30  Machine Learning in Production\n\nObjectives This chapter will cover the various approaches to deploy ML models in production. After studying this chapter, you should be able to use MLOps to overcome challenges in the manual deployment of ML models. You will also study the different phases of the ML life cycle in this chapter.\n\nML life cycle The machine learning life cycle is a periodic process. It starts with the business problem, and the last stage is monitoring and optimization. However, it is not so straightforward. For instance, if there is a modification in the business requirement, you may have to execute all the stages of the ML life cycle again. In some scenarios, you may have to go back to the previous stages of the ML life cycle to fulfill the criteria of that specific stage. For example, if you get lower accuracy of the model than the threshold, you need to revisit the previous stages to improve the accuracy. It can be done by adding new features, optimizing model parameters, and so on.\n\nThe following figure shows the different stages of the ML life cycle.\n\nFigure 3.1: ML life cycle",
      "content_length": 1114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Challenges in ML Model Deployment  31\n\nBusiness impact The first and most important stage is defining an idea and its impact on the project. Starting a project without considering the business impact, complexities, expected results, and time required may lead to delayed delivery, repetitive work, and poor resource utilization.\n\nThe business impact could be anything, like an increase in revenue, a decrease in expenses, or reducing human errors. One needs to understand the business pain points and try to assess the possibilities of having an ML solution that will solve multiple business problems. In some scenarios, getting quick results is more important.\n\nData collection Data collection is the process of gathering data from one or multiple sources. Although it looks easy, it is not. Before collecting the data, you need to answer the following questions:\n\nWhat data needs to be collected? •\t What are the different sources of the data? •\t What is the type of data? •\t What is the size of the data?\n\nIn the real world, you may have to collect data from different sources, like relational databases, NoSQL databases, the web, and so on. To avoid any glitches or delays, you must build the pipeline for data collection.\n\nFor better results, you can combine the available data with external data, such as social media sites, weather data, and public data.\n\nHere are a few ways to collect data:\n\nE-surveys •\t Authorized web scraping tool •\t Click data •\t Social media platforms •\t Website tracking •\t Subscription/ registration data • Image data - CCTV/ camera",
      "content_length": 1566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "32  Machine Learning in Production\n\nVoice data - customer care\n\nSet up a meeting with clients/stakeholders and domain experts to get to know the data. They will help you understand the data so that you can decide which data needs to be collected for model building.\n\nChallenges:\n\nData is scattered in different locations •\t No uniformity in the data •\t Combining data from different sources •\t 3Vs: volume, velocity, variety •\t Data storage\n\nData preparation The data collected usually is in a raw format. One needs to process it so that it can be used for further analysis and model development. This process of cleaning, restructuring, and standardization is known as data preparation.\n\nAround 70%-80% of the time goes into this stage of the ML project. This is a tedious task, but it is unavoidable. Data reduction technique is used when you have a large amount of data to be processed.\n\nData preparation aims to transform the raw data into a format so that EDA, that is, Exploratory Data Analysis, can be performed efficiently to gain insights.\n\nChallenges:\n\nMissing values •\t Outliers •\t Disparate data format •\t Data standardization •\t Noise in the data\n\nFeature engineering In this stage, you prepare the input data that can be fed to the model, which makes it easier for the machine learning model by deriving meaningful features, data transformations, and so on.",
      "content_length": 1372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Challenges in ML Model Deployment  33\n\nSuppose you are combining the features to create a new one that could help ML models to understand the data better and identify hidden patterns. For instance, you mix sugar, water, and lemon juice to make lemonade rather than consuming them separately.\n\nHere are a few feature engineering techniques:\n\nLabel encoding •\t Combining features to create a new one •\t One hot encoding • Imputation\n\nScaling •\t Removing unwanted features •\t Log transformation\n\nChallenges:\n\nLack of domain knowledge •\t Creating new features from the existing set of features •\t Selecting useful features from a set of features\n\nBuild and train the model ML model building requires the previous stages to be completed successfully. First, you should have the training, test, and validation sets ready (for supervised algorithms). After a baseline model is created, it can be compared with new models.\n\nThis process involves the development of multiple models to see which one is more efficient and gives better results. You must consider the computing resource requirements for this stage.\n\nOnce the final model is built on the training data, the next step is to check its performance against the unseen, that is, the test data.\n\nChallenges:\n\nModel complexity •\t Computational power •\n\nIdentifying a suitable model\n\nModel training time",
      "content_length": 1350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "34  Machine Learning in Production\n\nTest and evaluate Here, you will build the test cases and check how the model is performing against the new data. Pre-production or pre-deployment activities are done here. Performance results are analyzed and, if required, you have to go back to the previous stages to fix the issue.\n\nAfter passing this stage, you can push the ML model into the production phase.\n\nChallenges:\n\n\n\nInsufficient test data\n\nReiterating the process until the output fulfills the requirements • Identifying the platform to evaluate model performance on real data\n\nDeciding which test to use •\t Logging and analyzing test results\n\nModel deployment This refers to exposing the trained ML models to real-world users. Your model is performing well on test and validation sets, but if another system or users cannot utilize it, then it does not meet its purpose.\n\nFor instance, you have built a model to predict customers who are likely to churn, but it is not done until you deploy a model that will start delivering the predictions on real data.\n\nModel deployment is crucial because you have to consider the following factors:\n\nNumber of times predictions to be delivered •\t Latency of the predictions •\t ML system architecture •\t ML model deployment and maintenance cost •\t Complexity of infrastructure •\t This will be covered in detail in the following chapters.\n\nChallenges:\n\nPortability issues •\t Scalability issues •\t Data-related challenges •\t Security threats",
      "content_length": 1479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Challenges in ML Model Deployment  35\n\nMonitoring and optimization Last but not least, monitoring and optimization is the stage where observing and tracking are required. You will have to check the model performance as it degrades over time.\n\nIf the model metrics, such as accuracy, go below the predefined threshold, then it needs to be tracked, and a model needs to be retrained, either automatically or manually. Similarly, input data needs to be monitored because it may happen that the input data schema does not match or it contains missing values.\n\nApart from this, the infrastructure metrics, such as RAM, free space, and system issues, need to be tracked.\n\nIt is good to maintain the log records of metrics, intermediate outputs, warnings, and errors.\n\nChallenges:\n\nData drift •\t Deciding the threshold value for different metrics •\t Anomalies •\t Finalizing model evaluation metrics that need to be tracked\n\nTypes of model deployment There are various ways to deploy ML models into production; however, there is no generic way to do it. This section will walk you through popular ways to deploy ML models.\n\nBatch predictions This is the simplest method. Here, the ML model is trained on static data to make predictions, which are saved in the database, such as MS-SQL, and can be integrated into existing applications or accessed by the business intelligence team.\n\nGenerally, ML model artifacts are used for making predictions as it saves time. Model artifact needs to be updated on new data for better predictions.\n\nThis method is well-suited for small organizations and beginners. You can schedule the cron job to make predictions after certain time intervals.\n\nPros:\n\nAffordable •\t Less complex",
      "content_length": 1708,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "36  Machine Learning in Production\n\nEasy to implement\n\nCons:\n\nModerate latency •\t Not suitable for ML-centric organizations\n\nWeb service/REST API Web service/REST API is the popular method for deploying models. Unlike batch predictions, it does not process a bunch of records; it processes a single record at a time. In near real-time, it takes the parameters from users or existing applications and makes predictions.\n\nIt can take inputs as well as return the outputs in JSON format. JSON is a popular and compatible format that makes it easy for software or website developers to integrate it into existing applications.\n\nWhen an event gets triggered, REST API passes the input parameters to the ML model and returns the predictions.\n\nPros:\n\nEasy to integrate •\t Flexible •\t Economical (pay-as-you-go plan) •\t Near real-time predictions\n\nCons:\n\nScalability issues •\t Prone to security threats\n\nMobile and edge devices When there are situations such as actions/ decisions that need to be taken immediately or there is no internet connectivity, the ML model needs to be deployed on these devices.\n\nEdge devices include sensors, smartwatches, and cameras installed on robots.\n\nThis type of deployment is different from the preceding methods. In this, input data may not go to remote servers for making predictions. There are cloud service providers, such as Microsoft Azure, that offer the required infrastructure for this. Tiny Machine Learning (TinyML) is another such alternative capable of performing on-device sensor data analytics at an extremely low power.",
      "content_length": 1563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Challenges in ML Model Deployment  37\n\nML models deployed on mobile devices are useful. Developing ML-based android/ IOS applications, such as voice assistant or camera image-based attendance, are use cases of ML models deployed on mobiles.\n\nPros:\n\nLow power requirement •\t Cost-effective •\t Smaller sizes\n\nCons:\n\nLimited hardware resources •\t A complex and tedious process\n\nReal-time\n\nHere, analysis is to be done on streaming data. In this approach, the user passes input data for prediction and expects (near) real-time prediction. This is quite a challenging approach compared to the ones you studied previously. •\t You can decrease the latency of the predictions by small-sized models, caching predictions, No-SQL databases, and so on.\n\nPros:\n\nVery low or no latency •\t Can work with streaming data\n\nCons:\n\nComplex architecture requirements •\t Computationally expensive\n\nChallenges in deploying models in the production environment Deploying machine models in production is crucial. While deploying ML models, technical challenges are not the only ones. The following are the common challenges you may come across.\n\nTeam coordination If you are working in a small organization or a small data science team, you might need to play different roles and will be responsible for end-to-end model development.",
      "content_length": 1309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "38  Machine Learning in Production\n\nHowever, in large organizations, there are separate teams for different stages.\n\nThese teams interact with each other to carry out the entire process. Most of the time, the production or the succeeding team is not aware of the preceding stages. Lack of coordination and miscommunication can lead to the failure of the ML life cycle.\n\nData-related challenges Usually, data scientists develop models on a limited amount of data in the experimentation phase, but they might face challenges while deploying models in the production environment as large-scale data can impact the model performance.\n\nNew data keeps on changing its behavior, but the data preparation/pre-processing stage may not be able to handle these new issues. For instance, string records are present in a numeric column. At times, re-running the experiments by fixing the issues is required to get reliable results.\n\nPortability Portability issues arise when moving the codebase from your local machine to the server and vice versa. For instance, suppose you developed a model and want to move the code to the server to rerun the model. However, you may face issues, like the library not being compatible with the current OS version, and the line of code working with Python’s x version and not with the currently installed version.\n\nIn such cases, creating a virtual environment and re-installing the packages are required.\n\nScalability Suppose you deployed models using web service and 100x more users are trying to access the same API. REST API may stop responding or the latency may increase.\n\nManaging ML models on a small scale is relatively easy compared to large organizations, where multiple models are being served on a scale. Deploying models on a scale is still a new thing for many organizations.\n\nRobustness In a nutshell, the robustness of ML models is measured by the reliable output delivered despite the variations in the input data. In the real world, delivering 100% accuracy is nearly impossible. So, one can say that prediction error on unseen data should be close to training error.",
      "content_length": 2109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Challenges in ML Model Deployment  39\n\nUser behavior can be unpredictable. For instance, suppose a model requires a string value, but the user provides an alphanumeric value. In such scenarios, the model output might not be reliable.\n\nSecurity ML systems are prone to security breaches. They may be forced to deliver false predictions by deliberately providing poisonous data or adding noise to the data. The models perform better when trained on new data. However, a person or a group of people can deliberately pass the poisonous data to the model, intending to change the model predictions.\n\nData security is at risk as the model keeps training on new data. While training the model, attackers can steal sensitive information.\n\nMLOps MLOps combines machine learning processes and best practices of DevOps to deliver consistent output with automated pipelines and management. MLOps has been an emerging space in the industry for the last few years.\n\nThe following figure shows the increasing popularity of MLOps over time.\n\nFigure 3.2: The rise of MLOps. Source: Google trends\n\nMLOps bridges the gap between machine learning experiments and model deployment in the production environment. Nowadays, many data scientists are forced to execute the MLOps processes manually.\n\nThe MLOps process involves multiple professionals, and data scientists play a critical role. Subject matter experts understand and collect the requirements from the client. Then, data engineers collect the data from multiple sources and execute the ETL jobs. Once this is done, data scientists build the models and the DevOps",
      "content_length": 1601,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "40  Machine Learning in Production\n\nteam builds the CI/CD pipeline and monitors it. Finally, feedback is sent to the data scientist or the concerned team for validation.\n\nMLOps streamline and automate this process to speed up delivery and build efficient products/services.\n\nMLOps is a combination of three disciplines as shown in the following figure\n\nFigure 3.3: Machine learning operations\n\nMLOps is different from DevOps because the code is usually static in the latter, but that’s not the case in MLOps.\n\nIn MLOps, the model keeps training on new data, so a new model version gets generated recurrently. If it meets the requirements, it can be pushed into the production environment. This is why MLOps requires Continuous Training (CT) along with Continuous Integration (CI) and Continuous Delivery/Deployment (CD).\n\nIn DevOps, developers write the code as per the requirements and then release it to the production environment, but in the case of machine learning, developers first need to collect the data and clean it. They write code for model building and then build the ML model. Finally, they release it into the production environment.\n\nBenefits of MLOps MLOps processes not only speed up the ML journey from experiments to production, but also reduce the number of errors. MLOps automates Machine Learning and Deep Learning model deployment in a production environment. Moreover, it reduces dependency on other teams by streamlining the processes.",
      "content_length": 1462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Challenges in ML Model Deployment  41\n\nEfficient management of ML life cycle MLOps takes care of the ML life cycle by following the fundamental principles of machine learning projects. It is based on agile methodology. An automated CI/ CD pipeline helps speed up the process of retraining models on new data, testing before the deployment, monitoring, and feedback loops. The current stage depends on the output of the preceding stages, so there are fewer chances of issues in the deployment process.\n\nIf the amount of traffic or number of requests increase for a deployed model, it increases the required resources; similarly, the required resources decrease when the amount of traffic or number of requests reduces.\n\nThere are many platforms (like GitHub Actions) available in the market that help you set up MLOps workflow.\n\nReproducibility Developer: It works on my machine.\n\nManager: Then, we are going to ship your machine to the client.\n\nReproducibility plays a crucial role in the ML life cycle. While transferring the code files to another machine or server, reproducibility reduces debugging time. MLOps works on DRY (Don’t Repeat Yourself) principles and allows you to get consistent output.\n\nGiven the same input, the replicated workflow should produce an identical output. For this, developers use container orchestration tools like Docker so that it will create and set up the same environment with dependencies in another machine or server for consistent output.\n\nAutomation Mostly, developers deploy models several times before the final deployment to ensure that everything is in place and the output is as expected. Without automation, this would be a time-consuming and tedious process. Automation increases productivity, as you are less likely to test, deploy, scale, and monitor ML models manually.\n\nAt every stage, certain rules and conditions are implemented, and the model moves to the next stage only when these conditions are met. This reduces the active involvement of other teams every time you are planning to deploy it in production.\n\nThere is a low or no delayed deployment with testing since one inter-team dependency is reduced.",
      "content_length": 2162,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "42  Machine Learning in Production\n\nAfter making the required changes in the code, speedy deployment is done in an automated fashion (if it passes the test cases and satisfies the conditions). This increases the overall productivity of the team.\n\nTracking and feedback loop Tracking model performance, metrics, test results, and output becomes easy if you set up MLOps workflow properly. The model’s performance may degrade over time, hence one may need to retrain the model on new data.\n\nThanks to tracking and feedback loops, it sends alerts about the model’s performance, metrics, and so on.\n\nFor instance, if the feedback loop sends the information that model accuracy has dropped below 68%, then the model needs to be retrained. Again, it will check the model’s performance. If it is above the threshold level, it will pass on to the next stage; else, the model needs to be recalibrated using the latest data.\n\nConclusion In this chapter, you studied the key challenges faced while deploying ML models in production. At the beginning of this chapter, you learned how the ML life cycle works, and you understood its different stages and their challenges. Next, you got exposure to different types of model serving techniques and looked at their pros and cons. Then, you analyzed the challenges you may face while deploying ML models in production. Finally, you learned the benefits of MLOps.\n\nIn the next chapter, you will learn to develop, build and install custom python packages for ML models using a use case.\n\nPoints to remember\n\nMLOps bridges the gap between the Machine Learning experiments stage and its model deployment in the production environment.\n\nModel deployment strategy depends on business requirements, users, and applications.\n\nMLOps combines machine learning processes and best practices of DevOps to deliver consistent output with automated pipelines and management.",
      "content_length": 1892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Challenges in ML Model Deployment  43\n\nMultiple choice questions\n\n1. What are the different ways of collecting data for an ML problem?\n\na) E-surveys\n\nb) Web scraping\n\nc) Click data\n\nd) All the above\n\n2.\n\nIf you provide a similar input as the original workflow to the replicated workflow, it should produce an identical output known as: a) Portability\n\nb) Reproducibility\n\nc) Scalability\n\nd) Label encoding\n\nAnswers 1. d\n\n2. b\n\nQuestions\n\n1. What are the different ways of deploying ML models?\n\n2. How do MLOps differ from DevOps?\n\n3. What are the challenges faced while deploying ML models?\n\n4. What are the benefits of MLOps?\n\nKey terms\n\nContainer: A container is a standard unit of software that packages up the code and all its dependencies so that the application runs quickly and reliably despite the computing environment.\n\nContinuous Integration (CI): It is an automated process to build, test, and execute different pieces of code.\n\nContinuous Delivery/Deployment (CD): It is an automated process of frequently building tested code and deploying it to production.",
      "content_length": 1072,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "44  Machine Learning in Production\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Packaging ML Models  45\n\nChapter 4 Packaging ML Models\n\nIntroduction In this chapter, you will learn how to modularize Python code and build ML packages that can be installed and consumed on another machine or server. Modular programs simplify code debugging, reusability, and maintainability. You will start by creating a virtual environment and then data pre-processing, followed by building the ML model and finally, creating test cases for the package.\n\nStructure This chapter covers the following topics:\n\nVirtual environment •\t Requirements file •\t Serializing and de-serializing ML models •\t Testing Python code using pytest •\t Python packaging and dependency management •\t Developing, building, and deploying ML packages •\t Set up environment variables and paths",
      "content_length": 771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "46  Machine Learning in Production\n\nObjectives After completing this chapter, you should be able to create a virtual environment and install the required dependencies in it. You should also be able to package the modules and dependencies required to build an ML model that can be installed on local machines or remote servers. Additionally, you should be able to save the trained ML model by pickling it. This chapter will also help you learn to modularize code and build test cases using pytest to check integrity and functionality.\n\nVirtual environments While working on multiple projects, ProjectA requires PackageY_version2.6, whereas ProjectB requires PackageY_version2.8. In such scenarios, you can’t keep both versions of the same package globally.\n\nThe virtual environment is the solution. It allows you to isolate dependencies for each project. You can create the virtual environment anywhere and install the required packages in it.\n\nWithout the virtual environment, packages are installed globally at the default Python location:\n\n/home/suhas/.local/lib/python3.6/site-packages\n\nWith the virtual environment, packages are installed inside the virtual environment’s Python location:\n\n/home/suhas/code/packages/venv_package/lib/python3.6/site-packages\n\nInstalling different versions of the same package in different virtual environments for different projects is possible. You can have five packages in venv1 and nine packages in venv2.\n\nThe following figure best depicts the scenario wherein three different projects have separate virtual environments with different versions of Python and packages installed in them.",
      "content_length": 1628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "Packaging ML Models  47\n\nFigure 4.1: Virtual environment\n\nVirtual environment installation:\n\npip install virtualenv\n\nCreating a virtual environment:\n\nvirtualenv venv_package\n\nActivating a virtual environment:\n\nsource venv_package/bin/activate\n\n(venv_package) suhas@suhasVM:~/code/packages/prediction_model$\n\nDeactivating a virtual environment:\n\ndeactivate\n\nTo see the list of packages installed in the virtual environment: pip list\n\nOr pip freeze\n\nRequirements file The requirements file holds the list of packages that can be installed using pip.",
      "content_length": 548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "48  Machine Learning in Production\n\nTo create the requirements file:\n\npip freeze > requirements.txt\n\nTo install the list of packages from the requirements file, you can use the following command:\n\npip install -r requirements.txt\n\nWhere -r refers to –-requirement.\n\nIt instructs pip to install all the packages from the given requirements file.\n\nSerializing and de-serializing ML models Serializing is a process through which a Python object hierarchy is converted into a byte stream, whereas deserialize is the inverse operation, that is, a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy. In Python, serialization and deserialization refer to pickling and unpickling, respectively.\n\nHere, joblib.dump() and joblib.load() will be used as a replacement for a pickle.dump() and pickle.load respectively, to work efficiently on arbitrary Python objects containing large data, large NumPy arrays in particular, as shown here.\n\nImport the joblib library:\n\n1. import joblib\n\nThen create an object to be persisted:\n\n2. joblib.dump(ML_model_object, filename)\n\nAn object can be reloaded:\n\n3. joblib.load(filename)\n\nNote: If you are switching between Python versions, you may need to save a different joblib dump for each Python version.\n\nTesting Python code with pytest Testing your code assures you that it is giving the expected results and its functions are working bug-free. pytest tool allows you to build and run tests with ease. It comes with simple syntax and several features.\n\npytest can be installed using pip: pip install pytest",
      "content_length": 1581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "Packaging ML Models  49\n\nTo run the pytest, simply switch to the package directory and run the following command: pytest\n\nIt searches for test_*.py or *_test.py files.\n\nThe pytest output can be any of the following: •\t A dot (.) means that the test has passed. •\t An F means that the test has failed. •\t An E means that the test raised an unexpected exception.\n\npytest -v\n\nv or --verbose flag allows you to see whether individual tests are passing or failing.\n\npytest fixtures pytest fixtures are basic functions, and they run before the test functions are executed. pytest fixtures are useful when you are running multiple test cases with the same function return value.\n\nIt can be declared by the @pytest.fixture marker. The following is an example:\n\n1. @pytest.fixture\n\n2. def xyz_func():\n\n3. return “ABC”\n\nWhen pytest runs a test case, it looks at the parameters in that test function’s signature and then searches for fixtures that have the same names as those parameters. Once pytest finds them, it runs those fixtures, captures what they returned (if any), and passes those objects into the test functions as arguments.\n\nYou can configure pytest using the pytest.ini file.\n\nPython packaging and dependency management Suppose you have created the final ML model in a Python notebook. This Python notebook holds all the steps right from loading the data to predicting the test data.\n\nHowever, this notebook should not be used in the production environment for the following reasons:",
      "content_length": 1488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "50  Machine Learning in Production\n\nDifficult to debug •\t Require changes at multiple locations •\t Lots of dependencies •\t No modularity in the code •\t Conflict of variables and functions •\t Duplicate code snippets\n\nModular programming Python is a modular programming language. Modular programming is a design approach in which code gets divided into separate files, such that each file contains everything necessary to execute a defined piece of logic and can return the expected output when imported by other files that will act as input for them. These separate files are called modules.\n\nModule A module is a Python file that can hold classes, functions, and variables. For instance, load.py is a module, and its name is load.\n\nPackage A package contains one or more (relevant) modules, such that they are interlinked with each other. A package can contain a subpackage that holds the modules. It uses the inbuilt file hierarchy of directories for ease of access. A directory with subdirectories can be called a package if it contains the __init__.py file.\n\nPackages will be installed in the production environment as part of the deployment, so before you package anything, you’ll want to have answers to the following deployment questions:\n\nWho are your app’s users? Will your app be installed by other developers doing software development, operations people in a data center, or a less software-savvy group?\n\n\n\nIs your app intended to run on servers, desktops, mobile clients (phones, tablets, and so on), or embedded in dedicated devices?\n\n\n\nIs your app installed individually, or in large deployment batches?\n\nThe following figure depicts the sample structure of the Python package, where f1, f2, f3, f4, and f5 are modules of the package.",
      "content_length": 1749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Packaging ML Models  51\n\nFigure 4.2: Python package structure\n\nImport module syntax:\n\nimport <module_name>\n\nimport <module_name> as <alt_name>\n\nImport f4 module:\n\n1. from My_Package import f4\n\n2. from My_Package import f4 as load\n\nImport f1 module to use x():\n\n1. from My_Package.Sub_PackageA import f1\n\n2. f1.func()\n\nOr you can use the following method to import the module:\n\n1. from My_Package.Sub_PackageA.f1 import func\n\n2. func()\n\nDeveloping, building, and deploying ML packages In this section, you will study a use case that will begin by defining the business problem and end with installing and consuming an ML package. You will learn to",
      "content_length": 647,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "52  Machine Learning in Production\n\ndevelop and build a custom Python package for ML models. The custom package is portable and can be reused for the given project. You will be able to install a custom package like any other Python package and make predictions on the new data.\n\nBusiness problem A company wants to automate the loan eligibility process based on customer details provided by filling out online application forms. It is a classification problem where you must predict whether a loan would be approved.\n\nData The data corresponds to a set of financial transactions associated with individuals. The data has been standardized, de-trended, and anonymized. Refer to the following table for details of the data columns:\n\nVariable\n\nDescription\n\nLoan_ID\n\nUnique Loan ID\n\nGender\n\nMale/ Female\n\nMarried\n\nApplicant’s marital status(Y/N)\n\nDependents\n\nNumber of dependents\n\nEducation\n\nApplicant’s Education (Graduate/Under Graduate)\n\nSelf_Employed\n\nSelf-employed (Y/N)\n\nApplicantIncome\n\nApplicant’s income\n\nCoapplicantIncome\n\nCo-applicant’s income\n\nLoanAmount\n\nLoan amounts in thousands\n\nLoan_Amount_Term\n\nTerm of the loan in months\n\nCredit_History\n\nCredit history meets guidelines (Y/N)\n\nProperty_Area\n\nUrban/ Semi Urban/ Rural\n\nLoan_Status\n\nLoan approved (Y/N)\n\nTable 4.1: Data description",
      "content_length": 1295,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Packaging ML Models  53\n\nBuilding the ML model At this point, you may build and compare different ML algorithms using a Jupyter notebook.\n\nIdentify the model that is performing better than the others and finalize the best parameters (and hyperparameters) for the model. In this experimentation stage, you should be ready with information like the variables needed to transform, missing value imputation, the list of numerical variables, and the data path.\n\nWhen you are done with the experimentation stage, you can start building the package. The purpose of packaging is reusability, portability, fewer errors, and automation of machine learning processes.\n\nStart by creating a separate directory for building the package. Maintain separate modules (Python files) for different stages and operations, for instance, separate modules for pre-processing of the data. This will modularize the code and make debugging the code easy.\n\nBuild the test cases that will run the code files and verify the integrity of the files and the expected output.\n\nThis chapter does not focus on optimizing ML models, so you can further improve the model performance by fine-tuning it. This package would be limited to the current project, that is, you should not use the same package for different projects.\n\nDeveloping the package Following is the directory structure of the package: prediction_model\n\n├── MANIFEST.in\n\n├── prediction_model\n\n│ ├── config\n\n│ │ ├── config.py\n\n│ │ └── __init__.py\n\n│ ├── datasets\n\n│ │ ├── __init__.py\n\n│ │ ├── test.csv\n\n│ │ └── train.csv\n\n│ ├── __init__.py\n\n│ ├── pipeline.py\n\n│ ├── predict.py",
      "content_length": 1604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "54  Machine Learning in Production\n\n│ ├── processing\n\n│ │ ├── data_management.py\n\n│ │ ├── __init__.py\n\n│ │ └── preprocessors.py\n\n│ ├── trained_models\n\n│ │ ├── classification_v1.pkl\n\n│ │ └── __init__.py\n\n│ ├── train_pipeline.py\n\n│ └── VERSION\n\n├── README.md\n\n├── requirements.txt\n\n├── setup.py\n\n└── tests\n\n├── pytest.ini\n\n└── test_predict.py\n\n__init__.py\n\nThe __init__.py module is usually empty. However, it facilitates importing other Python modules.\n\nThis file indicates that the directory should be treated as a package.\n\nMANIFEST.in\n\nA MANIFEST.in file consists of commands, one per line, instructing setuptools to add or remove a set of files from the sdist (source distribution). In Python, distribution refers to the set of files that allows packaging, building, and distributing the modules. sdist contains archives of files, such as source files, data files, and setup.py file, in a compressed tar file (.tar.gz) on Unix.\n\nThe following table lists the commands and the descriptions. However, this chapter will only cover a few of them.\n\nYou can update MANIFEST.in as per the project requirements. Refer to the following table for general commands being used in the manifest file.\n\nCommand\n\nDescription\n\ninclude pat1 pat2 ...\n\nAdd all files matching any of the listed patterns.\n\nexclude pat1 pat2 ...\n\nRemove all files matching any of the listed patterns.",
      "content_length": 1365,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Packaging ML Models  55\n\nrecursive-include pattern pat1 pat2 ...\n\ndir-\n\nAdd all files under directories matching the dir- pattern that matches any of the listed patterns.\n\nrecursive-exclude pattern pat1 pat2 ...\n\ndir-\n\nRemove all files under directories matching the dir- pattern that matches any of the listed patterns.\n\nglobal-include pat1 pat2 ...\n\nAdd all files anywhere in the source tree matching any of the listed patterns.\n\nglobal-exclude pat1 pat2 ...\n\nRemove all files anywhere in the source tree matching any of the listed patterns.\n\ngraft dir-pattern\n\nprune dir-pattern\n\nAdd all files under directories matching the dir- pattern. Remove all files under directories matching the dir- pattern.\n\nTable 4.2: Commands for the manifest file\n\nLet’s add the commands to the manifest file to include or exclude files:\n\n1. include *.txt\n\n2. include *.md\n\n3. include *.cfg\n\n4. include *.pkl\n\n5. recursive-include ./prediction_model/*\n\n6.\n\n7. include prediction_model/datasets/train.csv\n\n8. include prediction_model/datasets/test.csv\n\n9. include prediction_model/trained_models/*.pkl\n\n10. include prediction_model/VERSION\n\n11. include ./requirements.txt\n\n12. exclude *.log\n\n13. recursive-exclude * __pycache__\n\n14. recursive-exclude * *.py[co]\n\nconfig.py\n\nA configuration module contains constant variables, the path to the directories, and initial settings. Variables and functions can be accessed by importing this module into other modules. For instance, the TARGET variable holds the dependent column name Loan_Status.",
      "content_length": 1523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "56  Machine Learning in Production\n\nHere, specify the path for the package’s root directory, data directory, train file name, test file name, features, and path of other files and directories.\n\n1. #Import Libraries\n\n2. import pathlib\n\n3. import os\n\n4. import prediction_model\n\n5.\n\n6. PACKAGE_ROOT = pathlib.Path(prediction_model.__file__).resolve(). parent 7.\n\n8. DATAPATH=os.path.join(PACKAGE_ROOT, 'datasets')\n\n9. SAVED_MODEL_PATH=os.path.join(PACKAGE_ROOT, 'trained_models')\n\n10.\n\n11. TRAIN_FILE='train.csv'\n\n12. TEST_FILE='test.csv'\n\n13.\n\n14. TARGET='Loan_Status'\n\n15.\n\n16. #Features to keep\n\n17. FEATURES=['Gender','Married','Dependents',\n\n18. 'Education','Self_Employed','ApplicantIncome',\n\n19. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n20. 'Credit_History','Property_Area'] # Final feature to keep in data\n\n21.\n\n22. NUMERICAL_FEATURES=['ApplicantIncome', 'LoanAmount', 'Loan_ Amount_Term'] d 23.\n\n24. CATEGORICAL_FEATURES=['Gender','Married','Dependents',\n\n25. 'Education','Self_Employed','Credit_ History',\n\n26. 'Property_Area'] #Categorical\n\n27.\n\n28. FEATURES_TO_ENCODE=['Gender','Married','Dependents',\n\n29. 'Education','Self_Employed','Credit_History',",
      "content_length": 1176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "Packaging ML Models  57\n\n30. 'Property_Area'] #Features to Encode\n\n31.\n\n32. TEMPORAL_FEATURES=['ApplicantIncome']\n\n33. TEMPORAL_ADDITION='CoapplicantIncome'\n\n34. LOG_FEATURES=['ApplicantIncome', 'LoanAmount'] #Features for Log Transformation\n\n35. DROP_FEATURES=['CoapplicantIncome'] #Features to Drop\n\ndata_management.py\n\nThis module contains functions required for loading the data, saving serialized ML model, and loading deserialized ML model using joblib.\n\n1. #Import Libraries\n\n2. import os\n\n3. import pandas as pd\n\n4. import joblib\n\n5.\n\n6. #Import other files/modules\n\n7. from prediction_model.config import config\n\n8.\n\n9. def load_dataset(file_name):\n\n10. \"\"\"Read Data\"\"\"\n\n11. file_path = os.path.join(config.DATAPATH,file_name)\n\n12. _data = pd.read_csv(file_path)\n\n13. return _data\n\n14.\n\n15. def save_pipeline(pipeline_to_save):\n\n16. \"\"\"Store Output Of Pipeline\n\n17. Exporting pickle file of trained Model \"\"\"\n\n18. save_file_name = 'classification_v1.pkl'\n\n19. save_path = os.path.join(config.SAVED_MODEL_PATH, save_file_ name)",
      "content_length": 1036,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "58  Machine Learning in Production\n\n20. joblib.dump(pipeline_to_save, save_path)\n\n21. print(\"Saved Pipeline : \",save_file_name)\n\n22.\n\n23. def load_pipeline(pipeline_to_load):\n\n24. \"\"\"Importing pickle file of trained Model\"\"\"\n\n25. save_path = os.path.join(config.SAVED_MODEL_PATH, pipeline_to_ load)\n\n26. trained_model = joblib.load(save_path)\n\n27. return trained_model\n\npreprocessors.py\n\nThis module holds all the fit and transform functions required by the sklearn pipeline:\n\n1. #Import Libraries\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4. from sklearn.base import BaseEstimator, TransformerMixin\n\n5. from sklearn.preprocessing import LabelEncoder\n\n6.\n\n7. #Import other files/modules\n\n8. from prediction_model.config import config\n\nThe mean of the feature is used for imputing missing numerical values. However, you can use other missing value imputation techniques, such as stochastic regression imputation, interpolation, and cold deck imputation.\n\n1. #Numerical Imputer\n\n2. class NumericalImputer(BaseEstimator,TransformerMixin):\n\n3. \"\"\"Numerical Data Missing Value Imputer\"\"\"\n\n4. def __init__(self, variables=None):\n\n5. self.variables = variables\n\n6.\n\n7. def fit(self, X,y=None):\n\n8. self.imputer_dict_={}",
      "content_length": 1219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "Packaging ML Models  59\n\n9. for feature in self.variables:\n\n10. self.imputer_dict_[feature] = X[feature].mean()\n\n11. return self\n\n12.\n\n13. def transform(self,X):\n\n14. X=X.copy()\n\n15. for feature in self.variables:\n\n16. X[feature].fillna(self.imputer_dict_[feature],inplace=True)\n\n17. return X\n\nThe most frequent value of the feature is used for imputing missing categorical values, but you can also use other missing value imputation techniques, such as missing indicator imputation, Multivariate Imputation by Chained Equations (MICE) algorithm, and systematic random sampling imputation.\n\n1. #Categorical Imputer\n\n2. class CategoricalImputer(BaseEstimator,TransformerMixin):\n\n3. \"\"\"Categorical Data Missing Value Imputer\"\"\"\n\n4. def __init__(self, variables=None):\n\n5. self.variables = variables\n\n6.\n\n7. def fit(self, X,y=None):\n\n8. self.imputer_dict_={}\n\n9. for feature in self.variables:\n\n10. self.imputer_dict_[feature] = X[feature].mode()[0]\n\n11. return self\n\n12.\n\n13. def transform(self, X):\n\n14. X=X.copy()\n\n15. for feature in self.variables:\n\n16. X[feature].fillna(self.imputer_dict_[feature],inplace=True)\n\n17. return X",
      "content_length": 1129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "60  Machine Learning in Production\n\nPost that, categorical features are encoded for model training.\n\n1. #Categorical Encoder\n\n2. class CategoricalEncoder(BaseEstimator,TransformerMixin):\n\n3. \"\"\"Categorical Data Encoder\"\"\"\n\n4. def __init__(self, variables=None):\n\n5. self.variables=variables\n\n6.\n\n7. def fit(self, X,y):\n\n8. self.encoder_dict_ = {}\n\n9. for var in self.variables:\n\n10. t = X[var].value_counts().sort_values(ascending=True). index\n\n11. self.encoder_dict_[var] = {k:i for i,k in enumerate(t,0)}\n\n12. return self\n\n13.\n\n14. def transform(self,X):\n\n15. X=X.copy()\n\n16. #This part assumes that the encoder does not introduce a NANs\n\n17. #In that case, a check needs to be done and the code should break\n\n18. for feature in self.variables:\n\n19. X[feature] = X[feature].map(self.encoder_dict_ [feature])\n\n20. return X\n\nThe following code snippet is used for creating new features:\n\n1. #Temporal Variables\n\n2. class TemporalVariableEstimator(BaseEstimator,TransformerMixin):\n\n3. \"\"\"Feature Engineering\"\"\"\n\n4. def __init__(self, variables=None, reference_variable = None):",
      "content_length": 1077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Packaging ML Models  61\n\n5. self.variables=variables\n\n6. self.reference_variable = reference_variable\n\n7.\n\n8. def fit(self, X,y=None):\n\n9. #No need to put anything, needed for Sklearn Pipeline\n\n10. return self\n\n11.\n\n12. def transform(self, X):\n\n13. X=X.copy()\n\n14. for var in self.variables:\n\n15. X[var] = X[var]+X[self.reference_variable]\n\n16. return X\n\nApply log transformation on variables for making patterns in the data more interpretable.\n\n1. # Log Transformations\n\n2. class LogTransformation(BaseEstimator, TransformerMixin):\n\n3. \"\"\"Transforming variables using Log Transformations\"\"\"\n\n4. def __init__(self, variables=None):\n\n5. self.variables = variables\n\n6.\n\n7. def fit(self, X,y):\n\n8. return self\n\n9.\n\n10. #Need to check in advance if the features are <= 0\n\n11. #If yes, needs to be transformed properly (E.g., np.log1p(X[var]))\n\n12. def transform(self,X):\n\n13. X=X.copy()\n\n14. for var in self.variables:\n\n15. X[var] = np.log(X[var])\n\n16. return X",
      "content_length": 958,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "62  Machine Learning in Production\n\nThe following code snippet is used for dropping features that are insignificant to the model:\n\n1. # Drop Features\n\n2. class DropFeatures(BaseEstimator, TransformerMixin):\n\n3. \"\"\"Dropping Features Which Are Less Significant\"\"\"\n\n4. def __init__(self, variables_to_drop=None):\n\n5. self.variables_to_drop = variables_to_drop\n\n6.\n\n7. def fit(self, X,y=None):\n\n8. return self\n\n9.\n\n10. def transform(self, X):\n\n11. X=X.copy()\n\n12. X=X.drop(self.variables_to_drop, axis=1)\n\n13. return X\n\npipeline.py\n\nIn this module, sklearn-pipeline is used. The model can be deployed without a sklearn-pipeline, but it is recommended to build a sklearn-pipeline. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\n\n1. #Import Libraries\n\n2. from sklearn.pipeline import Pipeline\n\n3. from sklearn.preprocessing import MinMaxScaler\n\n4. from sklearn.linear_model import LogisticRegression\n\n5.\n\n6. #Import other files/modules\n\n7. from prediction_model.config import config\n\n8. import prediction_model.processing.preprocessors as pp\n\n9.",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Packaging ML Models  63\n\n10. loan_pipe=Pipeline([\n\n11. ('Numerical Imputer',pp.NumericalImputer(variables=config. NUMERICAL_FEATURES)),\n\n12. ('Categorical Imputer',pp.CategoricalImputer(variables=config. CATEGORICAL_FEATURES)),\n\n13. ('Temporal Features', pp.TemporalVariableEstimator(variables=config.TEMPORAL_FEATURES,\n\n14. reference_variable=config.TEMPORAL_ADDITION)),\n\n15. ('Categorical Encoder', pp.CategoricalEncoder(variables=config.FEATURES_TO_ENCODE)),\n\n16. ('Log Transform', pp.LogTransformation(variables=config.LOG_ FEATURES)),\n\n17. ('Drop Features', pp.DropFeatures(variables_to_drop=config. DROP_FEATURES)),\n\n18. ('Scaler Transform', MinMaxScaler()),\n\n19. ('Linear Model', LogisticRegression(random_state=1))\n\n20. ])\n\npredict.py\n\nA predict.py module loads the saved ML model (.pkl) and makes predictions on the new data.\n\n1. #Import Libraries\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4. import joblib\n\n5.\n\n6. #Import other files/modules\n\n7. from prediction_model.config import config\n\n8. from prediction_model.processing.data_management import load_ pipeline\n\n9.\n\n10. pipeline_file_name = 'classification_v1.pkl'",
      "content_length": 1134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "64  Machine Learning in Production\n\n11.\n\n12. _loan_pipe = load_pipeline(pipeline_file_name)\n\n13.\n\n14. def make_prediction(input_data):\n\n15. \"\"\"Predicting the output\"\"\"\n\n16.\n\n17. # Read Data\n\n18. data = pd.DataFrame(input_data)\n\n19.\n\n20. # prediction\n\n21. prediction = _loan_pipe.predict(data[config.FEATURES])\n\n22. output = np.where(prediction==1, 'Y', 'N').tolist()\n\n23. results = {'prediction': output}\n\n24. return results\n\nrequirements.txt\n\nThe following libraries are included in this file:\n\n1. # Model Building Requirements\n\n2. joblib==0.16.0\n\n3. numpy==1.19.0\n\n4. pandas==1.0.5\n\n5. scikit-learn==0.23.1\n\n6. scipy==1.5.1\n\n7. sklearn==0.0\n\n8.\n\n9. # testing requirements\n\n10. pytest<5.0.0,>=4.6.6\n\n11.\n\n12. # packaging",
      "content_length": 722,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Packaging ML Models  65\n\n13. setuptools==40.6.3\n\n14. wheel==0.32.3\n\nclassification_v1.pkl\n\nThis is a pickled file from the trained model.\n\ntrain_pipeline.py\n\nThis module loads the training data and passes it to the pipeline, then saves the pickle file of the model to the local directory.\n\n1. #Import Libraries\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4.\n\n5. #Import other files/modules\n\n6. from prediction_model.config import config\n\n7. from prediction_model.processing.data_management import load_ dataset, save_pipeline\n\n8. import prediction_model.processing.preprocessors as pp\n\n9. import prediction_model.pipeline as pl\n\n10. from prediction_model.predict import make_prediction\n\n11.\n\n12. def run_training():\n\n13. \"\"\"Train the model\"\"\"\n\n14. #Read Data\n\n15. train = load_dataset(config.TRAIN_FILE)\n\n16.\n\n17. #separating Loan_status in y\n\n18. y = train[config.TARGET].map({'N':0 , 'Y':1})\n\n19. pl.loan_pipe.fit(train[config.FEATURES],y)\n\n20. save_pipeline(pipeline_to_save=pl.loan_pipe)",
      "content_length": 996,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "66  Machine Learning in Production\n\n21. if __name__=='__main__':\n\n22. run_training()\n\nsetup.py\n\nTo configure and install packages from the source directory, create a setup.py file. It is specific to the package. PIP will use the setup.py file to install packages. Go to the directory where the setup.py file is located and install the packages using the pip install . (period) command.\n\n1. import io\n\n2. import os\n\n3. from pathlib import Path\n\n4.\n\n5. from setuptools import find_packages, setup\n\n6.\n\n7. # Package meta-data\n\n8. NAME = 'prediction_model'\n\n9. DESCRIPTION = 'Train and deploy prediction model.'\n\n10. URL = 'https://github.com/suhas-ds/prediction_model'\n\n11. EMAIL = 'suhasp.ds@gmail.com'\n\n12. AUTHOR = 'Suhas Pote'\n\n13. REQUIRES_PYTHON = '3.6'\n\n14.\n\n15. here = os.path.abspath(os.path.dirname(__file__))\n\n16.\n\n17. # What packages are required for this module to be executed?\n\n18. def list_reqs(fname='requirements.txt'):\n\n19. with io.open(os.path.join(here, fname), encoding='utf-8') as fd:\n\n20. return fd.read().splitlines()\n\n21. try:",
      "content_length": 1049,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Packaging ML Models  67\n\n22. with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f:\n\n23. long_description = '\\n' + f.read()\n\n24. except FileNotFoundError:\n\n25. long_description = DESCRIPTION\n\n26.\n\n27. # Load the package's __version__.py module as a dictionary.\n\n28. ROOT_DIR = Path(__file__).resolve().parent\n\n29. PACKAGE_DIR = ROOT_DIR / NAME\n\n30. about = {}\n\n31. with open(PACKAGE_DIR / 'VERSION') as f:\n\n32. _version = f.read().strip()\n\n33. about['__version__'] = _version\n\n34.\n\n35. setup(\n\n36. name=NAME,\n\n37. version=about['__version__'],\n\n38. description=DESCRIPTION,\n\n39. long_description=long_description,\n\n40. long_description_content_type='text/markdown',\n\n41. author=AUTHOR,\n\n42. author_email=EMAIL,\n\n43. python_requires=REQUIRES_PYTHON,\n\n44. url=URL,\n\n45. packages=find_packages(exclude=('tests',)),\n\n46. package_data={'prediction_model': ['VERSION']},\n\n47. install_requires=list_reqs(),\n\n48. extras_require={},",
      "content_length": 939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "68  Machine Learning in Production\n\n49. include_package_data=True,\n\n50. license='MIT',\n\n51. classifiers=[\n\n52. 'License :: OSI Approved :: MIT License',\n\n53. 'Programming Language :: Python',\n\n54. 'Programming Language :: Python :: 3',\n\n55. 'Programming Language :: Python :: 3.6',\n\n56. 'Programming Language :: Python :: Implementation :: CPython',\n\n57. 'Programming Language :: Python :: Implementation :: PyPy'\n\n58. ]\n\n59. )\n\nVERSION\n\nThis file holds the version of the package. Here, a major.minor.micro scheme is used.\n\n1. 0.1.0\n\npytest.ini\n\nTo disable warnings while running pytest, you have to configure the pytest.ini file.\n\n1. [pytest]\n\n2. addopts = -p no:warnings\n\ntest_predict.py\n\nIt fetches a single record from the validation data and verifies the output using assert statements.\n\nIt validates the following checks: •\t The output is not null. •\t The output data type is str. •\t The output is Y for given data (fixed).",
      "content_length": 931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "Packaging ML Models  69\n\nThese checks are run using pytest.\n\n1. #Import libraries\n\n2. import pytest\n\n3. #Import files/modules\n\n4. from prediction_model.config import config\n\n5. from prediction_model.processing.data_management import load_ dataset\n\n6. from prediction_model.predict import make_prediction\n\n7.\n\n8. @pytest.fixture\n\n9. def single_prediction():\n\n10. ''' This function will predict the result for a single record'''\n\n11. test_data = load_dataset(file_name=config.TEST_FILE)\n\n12. single_test = test_data[0:1]\n\n13. result = make_prediction(single_test)\n\n14. return result\n\n15.\n\n16. #Test Prediction\n\n17. def test_single_prediction_not_none(single_prediction):\n\n18. ''' This function will check if the result of prediction is not None'''\n\n19. assert single_prediction is not None\n\n20. def test_single_prediction_dtype(single_prediction):\n\n21. ''' This function will check if the data type of the result of the prediction is str i.e. string '''\n\n22. assert isinstance(single_prediction.get('prediction')[0], str)\n\n23. def test_single_prediction_output(single_prediction):\n\n24. ''' This function will check if the result of the prediction is Y '''\n\n25. assert single_prediction.get('prediction')[0] == 'Y'",
      "content_length": 1212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "70  Machine Learning in Production\n\nSet the environment variable PYTHONPATH and go to the package directory, and then run the following command:\n\npytest -v\n\nIf everything goes well, you should get an output as follows:\n\nFigure 4.3: pytest output\n\nsdist\n\nPython’s sdists are compressed archives (.tar.gz files) containing one or more packages or modules.\n\nThis creates a dist directory containing a compressed archive of the package (for example, <PACKAGE_NAME>-<VERSION>.tar.gz in Linux).\n\nwheel\n\nThis is the binary distribution or bdist, and it supports Windows, Mac, and Linux.\n\nWheels will speed up the installation if you have compiled code extensions, as the build step is not required. A wheel distribution is a built distribution for the current platform. The installable wheel will be created under the dist directory, and a build directory will also be created with the built code.\n\nSet up environment variables and paths You may need to add the path to the environment variables. It allows you to import modules and functions:\n\nOpen the .bashrc file using terminal\n\nsudo nano ~/.bashrc\n\nAdd the path to the package directory. Here’s an example:\n\nPYTHONPATH=/home/suhas/code/packages/prediction_model:$PYTHONPATH\n\nexport PYTHONPATH",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "Packaging ML Models  71\n\nThen run source ~/.bashrc.\n\nReopen the terminal and test using the following:\n\necho $PYTHONPATH\n\nBuild the package\n\n1. Go to the project directory and install dependencies:\n\npip install -r requirements.txt\n\n2. Create a pickle file:\n\npython prediction_model/train_pipeline.py\n\n3. Creating a source distribution and wheel: python setup.py sdist bdist_wheel\n\nInstall the package Go to the project directory where the setup.py file is located and install this project with the pip command:\n\nTo install the package in editable or developer mode:\n\npip install -e .\n\n. refers to the current directory. •\t -e refers to --editable mode.\n\nNormal installation of the package:\n\npip install .\n\n. refers to the current directory.\n\nYou can push the entire package to the GitHub repository.\n\nTo install it from the GitHub repository.\n\nWith git:\n\npip install git+https://github.com/suhas-ds/prediction_model.git\n\nWithout git:\n\npip install https://github.com/suhas-ds/prediction_model/tarball/master\n\nOr:",
      "content_length": 1012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "72  Machine Learning in Production\n\npip install https://github.com/suhas-ds/prediction_model/zipball/master\n\nOr:\n\npip install https://github.com/suhas-ds/prediction_model/archive/master. zip\n\nAfter building the package, your directory structure should look like this: prediction_model\n\n|── build\n\n| |── bdist.linux-x86_64\n\n| |── lib\n\n| |── prediction_model\n\n|── dist\n\n| |── prediction_model-0.1.0-py3-none-any.whl\n\n| |── prediction_model-0.1.0.tar.gz\n\n|── MANIFEST.in\n\n|── prediction_model\n\n| |── config\n\n| | |── config.py\n\n| | |── __init__.py\n\n| |── datasets\n\n| | |── __init__.py\n\n| | |── test.csv\n\n| | |── train.csv\n\n| |── __init__.py\n\n| |── pipeline.py\n\n| |── predict.py\n\n| |── processing\n\n| | |── data_management.py\n\n| | |── __init__.py\n\n| | |── preprocessors.py\n\n| |── trained_models\n\n| | |── classification_v1.pkl",
      "content_length": 820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Packaging ML Models  73\n\n| | |── __init__.py\n\n| |── train_pipeline.py\n\n| |── VERSION\n\n|── prediction_model.egg-info\n\n| |── dependency_links.txt\n\n| |── PKG-INFO\n\n| |── requires.txt\n\n| |── SOURCES.txt\n\n| |── top_level.txt\n\n|── README.md\n\n|── requirements.txt\n\n|── setup.py\n\n|── tests\n\n|── pytest.ini\n\n|── test_predict.py\n\nPackage usage with example Start a Python console (in a virtual environment):\n\n(venv_package) suhas@ds:~/code/packages$ python\n\n1. Python 3.6.9\n\n2. [GCC 8.4.0] on linux\n\n3. Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\nImport the prediction_model package and make the predictions:\n\n1. import prediction_model\n\n2. from prediction_model import train_pipeline\n\n3. from prediction_model.predict import make_prediction\n\n4. import pandas as pd\n\n5.\n\n6. train_pipeline.run_training() # Save the pickle object of the trained model",
      "content_length": 868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "74  Machine Learning in Production\n\nSaved Pipeline : classification_v1.pkl # Output\n\n7. test_data = pd.read_csv(\"/home/suhas/code/test.csv\") # Load the data\n\n8. result = make_prediction(test_data[0:1])#Make prediction on the first row\n\n9. print(result)\n\n{'prediction': ['Y']}\n\nYou should get the output as Y (that is, ‘Yes’) for the given set of input data.\n\nConclusion This chapter discussed the importance of modular programming in the production environment; virtual environments play a crucial role when you are working on multiple projects. After that, you explored the steps to create and activate a virtual environment. You can list the packages to be installed inside the virtual environment in the requirements.txt file so that all the required packages can be installed in one go. Writing test cases boosts confidence about the package, functions, and overall integrity. Packages are portable and reusable, and they can be easily debugged.\n\nIn the next chapter, you will learn to leverage ML flow for tracking the ML experiments and saving the serialized models.\n\nPoints to remember\n\nThe Python package holds multiple modules, and each module contains functions, classes, and variables.\n\nThe MANIFEST.in file contains the list of files to be included and excluded. •\t The requirements.txt files hold the list of packages to be installed using pip. •\t Python refers to serialization and deserialization by the terms pickling and unpickling, respectively.\n\nThe __Init__.py file indicates that the directory should be treated as a package.\n\nMultiple choice questions\n\n1. Identify a Python module.\n\na)\n\nload.py\n\nb) requirements.txt c) transform.ccm d) MANIFEST.in",
      "content_length": 1670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "Packaging ML Models  75\n\n2. Python fixtures can be declared by which of the following?\n\na) fixture b) pytest.fixture c) @pytest.fixture d) fixture.pytest\n\nAnswers 1. a\n\n2. c\n\nQuestions\n\n1. What is a Python module?\n\n2. How can you interpret the output of the pytest command?\n\n3. How can you install Python packages in editable mode?\n\nKey terms\n\nPYTHONPATH: It augments the default search path for modules. The PYTHONPATH variable contains a list of directories whose modules are to be accessed in the Python environment.\n\n\n\nJoblib: Joblib is a set of tools to provide lightweight pipelining in Python. It is used to persist the model for future use, the following in particular:\n\no Transparent disk-caching of functions and lazy re-evaluation (memorize\n\npattern).\n\no Simple parallel computing.\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "76  Machine Learning in Production",
      "content_length": 35,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "MLflow-Platform to Manage the ML Life Cycle  77\n\nChapter 5 MLflow-Platform to Manage the ML Life Cycle\n\nIntroduction The Machine Learning life cycle involves many challenges. For instance, data scientists need to try different models containing multiple parameters and hyperparameters. They need to keep track of the model that is performing well and its parameters. Next, they need to save the serialized model for reusability. This chapter explains the role of MLflow in an ML life cycle. MLflow is a platform for streamlining machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models. It can manage a complete ML life cycle.\n\nStructure This chapter discusses the following topics:\n\n\n\nIntroduction to MLflow\n\nMLflow tracking •\t MLflow projects •\t MLflow models •\t MLflow model registry",
      "content_length": 864,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "78  Machine Learning in Production\n\nObjectives After studying this chapter, you should be able to train, reuse and deploy ML models using MLflow. You should also be able to track model evaluation metrics and model parameters, pickle the trained models, and compare two model results on MLflow UI. MLflow supports many downstream tools when it comes to deployment. You should be able to pick any iteration (by run ID) that is outperforming others and consume it through Python code, terminal, or postman.\n\nIntroduction to MLflow MLflow is an open-source platform for managing the end-to-end machine learning life cycle.\n\nMLflow allows data scientists to run as many experiments as they want before deploying the model into production; however, it keeps track of model evaluation metrics, such as RMSE and AUC. It also tracks the hyperparameters used while building the model. It enables you to save the trained model along with its best hyperparameters. Finally, it allows you to deploy an ML model into a production server or cloud. You can even keep track of the models being used in staging and production so that other team members can be aware of this information.\n\nMLflow is library-agnostic, that is, you can use any popular ML library with it. Moreover, you can use any popular programming language for it as MLflow functions can be accessed via REST API and Command Line Interface (CLI).\n\nIntegrating MLflow with your existing code is quite easy as it requires minimal changes. If you are working on a local system, it will automatically create a mlrun directory, wherein it stores the output, artifacts, and metadata. It creates a separate directory for each run. However, you can specify the path to create the mlrun directory. MLflow allows you to store information of each run into databases, such as MySQL or PostgreSQL.\n\nMLflow is more useful in the following scenarios:\n\nComparing different models: MLflow offers a UI that allows users to compare different models. You can compare Random Forest vs Logistic regression side by side, along with their model metric and parameters used. MLflow supports a wide range of model frameworks.\n\nCyclic model deployment: In production, it is required to push a new version of the model after every data change, when new requirements come up, or after building a model better than the current one. In these scenarios, MLflow helps keep track of the models that are in staging (pre-production) and models that are in production with versions and brief descriptions.",
      "content_length": 2517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "MLflow-Platform to Manage the ML Life Cycle  79\n\nMultiple dependencies: If you are working on different projects or different frameworks, each of them will have a different set of dependencies. MLflow helps you to maintain dependencies along with your model.\n\nWorking with a large data science team: MLflow stores the model metrics, parameters, time created, versions, users, and so on. This information is accessible to other team members working on the same projects. They can track all the metadata using MLflow UI or SQL table (if you are storing it in the database).\n\nSet up your environment and install MLflow In this section, you will install miniconda and then install mlflow in the conda environment. However, if you already have anaconda or miniconda installed, you can create a conda environment and install mlflow using PIP.\n\nMiniconda installation If you have anaconda installed on your machine, then you can skip the next step, that is, installing and setting up Miniconda.\n\nYou can download Miniconda from the following link:\n\nhttps://docs.conda.io/en/latest/miniconda.html#linux-installers\n\nOr you can simply execute the following command in the terminal (for Linux):\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.10.3- Linux-x86_64.sh\n\nRun the installation: bash Miniconda3-latest-Linux-x86_64.sh\n\nRead and follow the prompts on the installer screens. Restart the terminal when the miniconda installation is completed.\n\nIn the terminal, you will see that the base conda environment is auto-activated. You can disable the auto-activation of the base environment by running the following command in the terminal: conda config --set auto_activate_base false\n\nVerify the conda installation using the following command: conda list\n\nRun the following command to update conda (optional): conda update conda",
      "content_length": 1831,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "80  Machine Learning in Production\n\nLet’s create a virtual environment: conda create -n venv python=3.7\n\nWhere:\n\n-n refers to the name of the virtual environment. •\t venv is the name of the virtual environment.\n\nThe preceding command will create a virtual environment with Python version 3.7.\n\nOnce the conda environment is created, it needs to be activated by running the following command: conda activate venv\n\nFinally, install MLflow using pip: pip install mlflow\n\nNote: By default, the MLflow project uses conda for installing dependencies; however, you can proceed without conda by using the –no-conda option, for instance, mlflow run . –no-conda.\n\nAfter installing MLflow, type mlflow in the terminal and hit enter to check its usage, options, and commands. The following figure shows the usage and options of mlflow:\n\nFigure 5.1: MLflow usage, options, and commands\n\nTo open the web UI of MLflow, run the following command in the terminal (refer to figure 5.2):\n\nmlflow ui",
      "content_length": 980,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "MLflow-Platform to Manage the ML Life Cycle  81\n\nFigure 5.2: Starting MLflow UI\n\nOpen the browser; you should see a web UI of MLflow, as shown in the following figure:\n\nFigure 5.3: MLflow UI\n\nYou can execute a series of experiments and capture multiple runs of the experiments. Each experiment can contain multiple runs. You can also capture notes for the experiments. Apart from this, there are many customization options available, such as sorting by the columns, showing or hiding the columns, and changing the view of the table.",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "82  Machine Learning in Production\n\nNote: Suppose you interrupted the running MLflow UI service and rerun the mlflow ui command; in that case, you may get the error as shown in the following figure:\n\nFigure 5.4: MLflow UI error\n\nIt is because the address port is in use; so, you have to release it first, and then you can rerun the mlflow ui. It can be done using the following command:\n\nsudo fuser -k 5000/tcp\n\nMLflow components MLflow is categorized into four components:\n\nMLflow tracking •\t MLflow projects •\t MLflow models •\t MLflow registry\n\nThese components are devised such that they can work together seamlessly; however, you are free to use individual components as you need. For example, you are free to serve a model using MLflow without using a tracking component.\n\nThe following figure shows the MLflow’s components and their major role:",
      "content_length": 851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "MLflow-Platform to Manage the ML Life Cycle  83\n\nFigure 5.5: MLflow components\n\nThis chapter will cover each component’s functionality with examples.\n\nMLflow tracking MLflow tracking is an API and UI for logging parameters, code versions, metrics, and artifacts when running your machine learning code and for visualizing the results.\n\nMLflow captures the following information in the form of runs, where each run means executing a block of code:\n\nStart and end time: It records the start and end times of an experiment. •\t Source: It can be the name of the file to launch the run or the MLproject name.\n\nParameters: They contain the data in key-value pairs. These are nothing but the model input parameters you want to capture, such as the number of trees used in a random forest algorithm. For instance, n_estimators is key, and its value is 100. You need to call MLflow’s log_param() to store the parameters.\n\nMetrics: A metric is used to measure the performance of the model, such as the accuracy of the model. It holds the data in a key-value pair; however, the value should be numeric only. You need to call MLflow’s log_metric() to store the metric.",
      "content_length": 1157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "84  Machine Learning in Production\n\nArtifacts: When you want to store a file or object (such as a pickle file of the trained model), then the function of the artifacts comes to the rescue. You can store a serialized trained model, plot, or CSV file using this function, and it can be called using log_artifacts().\n\nFirst, you have to store the file or object in the local directory, and from there, you can save the file or object by providing the path of that directory.\n\nLog data into the run mlflow.set_tracking_uri()\n\nIt connects to MLflow tracking Uniform Resource Identifier (URI). By default, tracking URI is set to the mlruns directory; however, you can set it to a remote server (HTTP/HTTPS), local directory path, or a database like MySQL.\n\n1. import mlflow\n\n2. mlflow.set_tracking_uri('http://localhost:5000')\n\nmlflow.tracking.get_tracking_uri()\n\nThis function will return the current tracking URI of MLflow. mlflow.create_experiment()\n\nIt will create a new experiment. You can capture the runs by providing the experiment ID while executing mlflow.start_run. The experiment name should be unique.\n\n1. exp_id = mlflow.create_experiment(\"Loan_Prediction\")\n\nmlflow.set_experiment()\n\nThis method activates the experiment so that the runs will be captured under the provided experiment. A new experiment is created in case the mentioned experiment does not exist. By default, the experiment is set to ‘Default’, as shown in figure 5.3.\n\nmlflow.start_run()\n\nIt starts a new run or returns the currently active run. You can pass the run name, run ID, and experiment’s name under the current run that needs to be tracked.\n\n1. # For single iteration\n\n2. run = mlflow.start_run()\n\n3.\n\n4. # For multiple iterations\n\n5. with mlflow.start_run(run_name=\"test_ololo\") as run:\n\nmlflow.end_run()",
      "content_length": 1791,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "MLflow-Platform to Manage the ML Life Cycle  85\n\nIt ends the currently active run (if any). mlflow.log_param()\n\nIt logs a single key-value parameter in the currently active run. The key and value are both strings. Use mlflow.log_params() to log multiple parameters at once.\n\n1. n_estimators = 100\n\n2. mlflow.log_param(\"n_estimators:\", n_estimators)\n\nmlflow.log_metric()\n\nThis MLflow’s function will track the model metrics, such as the model’s accuracy. To track multiple metrics, use mlflow.log_metrics().\n\n1. accuracy = 0.8\n\n2. mlflow.log_metric(\"accuracy\", accuracy)\n\nmlflow.set_tag()\n\nIt stores the data in a key-value pair. In this, you can set labels for the identification or any specific metric you want to track. To set multiple tags, use mlflow.set_tags().\n\n1. import mlflow\n\n2. with mlflow.start_run():\n\n3. mlflow.set_tag(\"model_version\", \"0.1.0\")\n\nmlflow.log_artifact()\n\nThis function will log or store the files or objects in the artifacts directory; however, you would need to store it in the local directory first, and then it can pull the files or objects.\n\n1. import pandas as pd\n\n2. import mlflow\n\n3.\n\n4. dir_name = 'data_dir'\n\n5. file_name = 'data_dir/cust_sale.csv'\n\n6. data = pd.DataFrame({'Cust_id': [461,462,463], 'Sales': [2631,8462,4837]})\n\n7. data.to_csv(file_name, index=False)\n\n8. mlflow.log_artifacts(dir_name)\n\nmlflow.get_artifact_uri()",
      "content_length": 1367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "86  Machine Learning in Production\n\nIt will return the path to the artifact's root directory, where the artifacts are stored.\n\n1. import mlflow\n\n2. mlflow.get_artifact_uri()\n\n3. './mlruns/0/be1cd88ebd704e9ab7629fd364747e1e/artifacts'\n\nLet’s consider the scenario of loan prediction, where the objective is to predict whether a customer is eligible for a loan. First, import the required libraries.\n\n1. # Importing required packages\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4.\n\n5. from sklearn.linear_model import LogisticRegression\n\n6. from sklearn.ensemble import RandomForestClassifier\n\n7. from sklearn.tree import DecisionTreeClassifier\n\n8.\n\n9. from matplotlib import pyplot as plt\n\n10.\n\n11. from sklearn import preprocessing\n\n12. from sklearn.model_selection import train_test_split, GridSearchCV\n\n13. from sklearn import metrics\n\n14.\n\n15. import mlflow\n\nThe next step is to load the datasets and capture numerical and categorical column names in separate variables:\n\n1. # Reading the data\n\n2. data = pd.read_csv(\"loan_dataset.csv\")\n\n3. num_col = data.select_dtypes(include=['int64','float64']).columns. tolist()\n\n4. cat_col = data.select_dtypes(include=['object']).columns.tolist()\n\n5. cat_col.remove('Loan_Status')\n\n6. cat_col.remove('Loan_ID')",
      "content_length": 1258,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "MLflow-Platform to Manage the ML Life Cycle  87\n\nTreat missing values for categorical and numerical columns:\n\n1. # Creating a list of categorical and numerical variables\n\n2. for col in cat_col:\n\n3. data[col].fillna(data[col].mode()[0], inplace=True)\n\n4.\n\n5. for col in num_col:\n\n6. data[col].fillna(data[col].median(), inplace=True)\n\nCap extreme values to the 5th and 95th percentile for numerical data.\n\n1. # Clipping extreme values\n\n2. data[num_col] quantile([0.05, 0.95]))) =\n\ndata[num_col].apply(lambda\n\nx:\n\nx.clip(*x.\n\nCreate a new feature named TotalIncome, which is the sum of the applicant's income and the co-applicant’s income.\n\n1. # creating a new feature as Total Income\n\n2. data['LoanAmount'] = np.log(data['LoanAmount']).copy()\n\n3. data['TotalIncome'] = data['ApplicantIncome'] + data['CoapplicantIncome']\n\n4. data['TotalIncome'] = np.log(data['TotalIncome']).copy()\n\nDrop the applicant income and co-applicant income columns.\n\n1. # Dropping ApplicantIncome and CoapplicantIncome\n\n2. data = data.drop(['ApplicantIncome','CoapplicantIncome'], axis=1)\n\nConvert categorical columns to numeric columns using a label encoding technique.\n\n1. # Label encoding categorical variables\n\n2. for col in cat_col:\n\n3. le = preprocessing.LabelEncoder()\n\n4. data[col] = le.fit_transform(data[col])\n\n5.\n\n6. data['Loan_Status'] = le.fit_transform(data['Loan_Status'])\n\nSplit the data into train and test (70:30).\n\n1. # Train test split\n\n2. X = data.drop(['Loan_Status', 'Loan_ID'],1)",
      "content_length": 1479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "88  Machine Learning in Production\n\n3. y = data.Loan_Status\n\n4.\n\n5. SEED = 1\n\n6.\n\n7. X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.3, random_state = SEED)\n\nBuild a logistic regression model using a grid search cross-validation approach.\n\n1. #______________Logistic Regresssion__________________________#\n\n2.\n\n3. lr = LogisticRegression(random_state=SEED)\n\n4. lr_param_grid = {\n\n5. 'C': [100, 10, 1.0, 0.1, 0.01],\n\n6. 'penalty': ['l1','l2'],\n\n7. 'solver':['liblinear']\n\n8. }\n\n9.\n\n10. lr_gs = GridSearchCV(\n\n11. estimator=lr,\n\n12. param_grid=lr_param_grid,\n\n13. cv=5,\n\n14. n_jobs=-1,\n\n15. scoring='accuracy',\n\n16. verbose=0\n\n17. )\n\n18. lr_model = lr_gs.fit(X_train, y_train)\n\nBuild a decision tree model using a grid search cross-validation approach.\n\n1. #___________________Decision Tree__________________________#\n\n2.\n\n3. dt = DecisionTreeClassifier(\n\n4. random_state=SEED\n\n5. )\n\n6.",
      "content_length": 907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "MLflow-Platform to Manage the ML Life Cycle  89\n\n7. dt_param_grid = {\n\n8. \"max_depth\": [3, 5, 7, 9, 11, 13],\n\n9. 'criterion' : [\"gini\", \"entropy\"],\n\n10. }\n\n11.\n\n12. dt_gs = GridSearchCV(\n\n13. estimator=dt,\n\n14. param_grid=dt_param_grid,\n\n15. cv=5,\n\n16. n_jobs=-1,\n\n17. scoring='accuracy',\n\n18. verbose=0\n\n19. )\n\n20. dt_model = dt_gs.fit(X_train, y_train)\n\nBuild a random forest model using a grid search cross-validation approach.\n\n1. #_______________Random Forest___________________________#\n\n2.\n\n3. rf = RandomForestClassifier(random_state=SEED)\n\n4. rf_param_grid = {\n\n5. 'n_estimators': [400, 700],\n\n6. 'max_depth': [15,20,25],\n\n7. 'criterion' : [\"gini\", \"entropy\"],\n\n8. 'max_leaf_nodes': [50, 100]\n\n9. }\n\n10.\n\n11. rf_gs = GridSearchCV(\n\n12. estimator=rf,\n\n13. param_grid=rf_param_grid,\n\n14. cv=5,\n\n15. n_jobs=-1,\n\n16. scoring='accuracy',\n\n17. verbose=0",
      "content_length": 857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "90  Machine Learning in Production\n\n18. )\n\n19. rf_model = rf_gs.fit(X_train, y_train)\n\nCreate a function for evaluating the model’s metrics.\n\n1. # Model evaluation metrics\n\n2. def model_metrics(actual, pred):\n\n3. accuracy = metrics.accuracy_score(y_test, pred)\n\n4. f1 = metrics.f1_score(actual, pred, pos_label=1)\n\n5. fpr, tpr, thresholds1 = metrics.roc_curve(y_test, pred)\n\n6. auc = metrics.auc(fpr, tpr)\n\n7. plt.figure(figsize=(8,8))\n\n8. # plot auc\n\n9. plt.plot(fpr, tpr, color='blue', label='ROC curve area = %0.2f'%auc)\n\n10. plt.plot([0,1],[0,1], 'r--')\n\n11. plt.xlim([-0.1, 1.1])\n\n12. plt.ylim([-0.1, 1.1])\n\n13. plt.xlabel('False Positive Rate', size=14)\n\n14. plt.ylabel('True Positive Rate', size=14)\n\n15. plt.legend(loc='lower right')\n\n16.\n\n17. # Save plot\n\n18. plt.savefig(\"plots/ROC_curve.png\")\n\n19.\n\n20. # Close plot\n\n21. plt.close()\n\n22.\n\n23. return(accuracy, f1, auc)\n\nCreate a function for capturing information like the model parameters and model metrics using MLflow.\n\n1. # MLflow's logging functions\n\n2. def mlflow_logs(model, X, y, name):",
      "content_length": 1056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "MLflow-Platform to Manage the ML Life Cycle  91\n\n3.\n\n4. with mlflow.start_run(run_name = name) as run:\n\n5.\n\n6. # Run id\n\n7. run_id = run.info.run_id\n\n8. mlflow.set_tag(\"run_id\", run_id)\n\n9.\n\n10. # Make predictions\n\n11. pred = model.predict(X)\n\n12.\n\n13. # Generate performance metrics\n\n14. (accuracy, f1, auc) = model_metrics(y, pred)\n\n15.\n\n16. # Logging best parameters\n\n17. mlflow.log_params(model.best_params_)\n\n18.\n\n19. # Logging model metric\n\n20. mlflow.log_metric(\"Mean cv score\", model.best_score_)\n\n21. mlflow.log_metric(\"Accuracy\", accuracy)\n\n22. mlflow.log_metric(\"f1-score\", f1)\n\n23. mlflow.log_metric(\"AUC\", auc)\n\n24.\n\n25. # Logging artifacts and model\n\n26. mlflow.log_artifact(\"plots/ROC_curve.png\")\n\n27. mlflow.sklearn.log_model(model, name)\n\n28.\n\n29. mlflow.end_run()\n\nPredict on test data and capture the model metrics and parameters using the preceding function.\n\n1. # Make predictions using ML models\n\n2.",
      "content_length": 922,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "92  Machine Learning in Production\n\n3. mlflow_logs(dt_model, X_test, y_test, \"DecisionTreeClassifier\")\n\n4. mlflow_logs(lr_model, X_test, y_test, \"LogisticRegression\")\n\n5. mlflow_logs(rf_model, X_test, y_test, \"RandomForestClassifier\")\n\nActivate the virtual environment: conda activate venv\n\nThe following figure shows the activation and deactivation commands after successfully creating a conda environment.\n\nFigure 5.6: Conda activation command\n\nRun the train.py file, which will capture the parameters and metrics and store the artifacts at a given location.\n\nStart the MLflow UI and then run the train.py file, as shown in the following figure.\n\nFigure 5.7: MLflow UI command for keeping it running in the background\n\npython train.py\n\nThis does not print anything, so one can check the output on MLflow’s UI.\n\nFor demonstration, three different classifiers are implemented, namely, logistic regression, decision tree, and random forest, to compare their performances.\n\nThe following figure shows that model metrics, parameters, model names, and run ID have been captured. Overall, the logistic regression’s accuracy is 78.9%, which is slightly better than the others.",
      "content_length": 1171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "MLflow-Platform to Manage the ML Life Cycle  93\n\nFigure 5.8: MLflow UI for train.py output\n\nCapture MLflow logs under the Loan_prediction experiment instead of the Default experiment.\n\n1. # Make predictions using ML models\n\n2. mlflow.set_experiment(\"Loan_prediction\")\n\n3. mlflow_logs(dt_model, X_test, y_test, \"DecisionTreeClassifier\")\n\n4. mlflow_logs(lr_model, X_test, y_test, \"LogisticRegression\")\n\n5. mlflow_logs(rf_model, X_test, y_test, \"RandomForestClassifier\")\n\nBy default, MLflow will capture the information under the experiment name Default. However, you can specify the experiment name in the command itself, as follows:\n\npython train.py –experiment-name Loan_prediction\n\nThis command won’t print any output in the terminal as there are no print statements in the train.py file.",
      "content_length": 790,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "94  Machine Learning in Production\n\nNow, the information is getting captured under the Loan_prediction experiment, as shown in the following figure:\n\nFigure 5.9: Mlflow UI for a new experiment\n\nYou can deactivate the virtual environment using the following command:\n\nconda deactivate\n\nMLflow projects Once you are done with the experimentation phase, your next step would be packaging all the code as a project with its dependencies. Let’s say you want to shift the codebase and dependencies to the server or to another machine; MLflow will do the job for you.\n\nMLflow allows you to package the codebase and its dependencies to make it reproducible and reusable. MLflow projects provide API and CLI capabilities that will help you integrate your model in MLOps.\n\nYou can run the MLflow project directly from the remote git repository (provided it should contain all the necessary files); alternatively, you can run it from the local CLI.\n\nFollowing are the fields of the MLproject file:\n\nName: It is the name of the project, and it can be any text. •\t Environment: This is the environment that will be used at the time of execution of the entry point command. This will contain dependencies/ packages required by the entry point or MLflow project.",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "MLflow-Platform to Manage the ML Life Cycle  95\n\nEntry point: The entry point section holds the command to be executed inside the MLflow project environment. This command can take arguments; it is a mandatory field and cannot be left blank.\n\nParameters: This section holds one or more arguments that will be used by the entry point commands, but it is optional.\n\nCreate a conda.yaml file:\n\n1. name: Loan_prediction\n\n2.\n\n3. channels:\n\n4. - defaults\n\n5.\n\n6. dependencies:\n\n7. - python=3.7\n\n8. - pip\n\n9. - pip:\n\n10. - mlflow\n\n11. - numpy==1.19.5\n\n12. - pandas==1.1.5\n\n13. - matplotlib==3.3.4\n\n14. - scikit-learn==0.24.2\n\nThen, create an MLproject file:\n\n1. name: Loan_prediction\n\n2.\n\n3. conda_env: conda.yaml\n\n4.\n\n5. entry_points:\n\n6. main:\n\n7. command: “python train.py”\n\nSwitch to the directory where the MLproject file and the conda environment are present. Locate the YAML file and run: mlflow run .",
      "content_length": 901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "96  Machine Learning in Production\n\nYou can create and set the experiment name using the CLI, as shown in the following figure.\n\nmlflow.set_experiment(\"Loan_prediction\")\n\nFigure 5.10: Creating a new experiment using the command\n\nmlflow run . --experiment-name Loan_prediction\n\nThe following figure shows the output of the preceding command:\n\nFigure 5.11: Output of ML project’s run command\n\nRun the following command in your terminal to run MLproject from the GitHub repository:\n\nmlflow run https://github.com/suhas-ds/mlflow_loan_prediction --experiment-name Loan_prediction\n\nThe following figure shows the output of the preceding command:\n\nFigure 5.12: Running ML project from GitHub",
      "content_length": 686,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "MLflow-Platform to Manage the ML Life Cycle  97\n\nYou can see the new experiment Loan_prediction created and the information captured in it.\n\nWhen you open the details of the LogisticRegression model, you can see Git Commit, as this was run directly from GitHub.\n\nThe following figure shows the output for logistic regression after running the MLflow project from the GitHub repository.\n\nFigure 5.13: Logistic regression results\n\nMLflow models The MLflow models module lets you package the model in different ways, such as python function, Scikit-learn (sklearn), and Spark MLlib (spark). This flexibility helps you to connect associated downstream tools effortlessly.\n\nWhen you log the model using mlflow.sklearn.log_model(model, name), a model directory gets created, and it stores the files and metadata associated with the models. You will see the following directory structure:\n\nLogisticRegression/\n\n├── conda.yaml\n\n├── MLmodel\n\n├── model.pkl\n\n└── requirements.txt",
      "content_length": 969,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "98  Machine Learning in Production\n\nThe following figure shows the MLmodel details for the logistic regression model:\n\nFigure 5.14: MLmodel\n\nNow, let’s predict by following the instruction under Predict on a Pandas DataFrame, as shown in the following figure:\n\nFigure 5.15: MLflow model\n\nNow open the Python console by typing python in the terminal. Here, the aim is to create a pandas DataFrame and pass it to the predict function. You can load the DataFrame from the local directory.\n\nIn the following figure, a pandas DataFrame is used for prediction using MLflow’s model.",
      "content_length": 576,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "MLflow-Platform to Manage the ML Life Cycle  99\n\nFigure 5.16: Making predictions on pandas DataFrame using MLflow model\n\nNow, deploy a local REST server to serve the predictions using the MLmodel.\n\nBy default, the server runs on port 5000. If the port is already in use, you can use the --port or -p option to provide a different port.\n\nFor instance, mlflow models serve -m runs: /<RUN_ID>/model --port 1234.\n\nTo deploy to the server, run the following command:\n\nmlflow models serve -m /home/suhas/mlb/mlflow/ mlruns/0cc5b1a8724f4e818b4e92776ca21b73/0/artifacts/LogisticRegression/ -p 1234\n\nIn the preceding command, replace the model path with the model path used in your machine and provide a port 1234 to run on.\n\nIn the following figure, the MLflow model generated using logistic regression is deployed:\n\nFigure 5.17: Deploying MLflow model\n\nThe preceding figure illustrates that the server is listening at http://127.0.0.1:1234/.\n\nCall the REST API using the following curl command: curl -X POST -H \"Content-Type:application/json; format=pan- das-split\" --data '{\"columns\":[\"Gender\",\"Married\",\"Depen- dents\",\"Education\",\"Self_Employed\",\"LoanAmount\",\"Loan_Amount_ Term\",\"Credit_History\",\"Property_Area\",\"TotalIncome\"],\"da- ta\":[[1.0,0.0,0.0,0.0,0.0,4.85203026,360.0,1.0,2.0,8.67402599]]}' http://127.0.0.1:1234/invocations",
      "content_length": 1327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "100  Machine Learning in Production\n\nThe following figure depicts calling the REST API of the deployed model using the curl command:\n\nFigure 5.18: Curl command output\n\nOptionally, you can call the REST API of the deployed model using the curl command, as shown in the following figure:\n\nFigure 5.19: Curl command output\n\nThe following figure shows the output of a different set of input data:\n\nFigure 5.20: Curl command output\n\nThe server should respond with an output similar to [0] or [1].\n\nHere, [0] is labeled as No, and [1] is labeled as Yes.\n\nMLflow registry MLflow registry is a platform for storing and managing ML models through UI and a set of APIs.\n\nIt keeps track of the model lineage, different versions, and transitions of the models from one state to another, like from staging to production. Every authorized team member can track all the preceding information.",
      "content_length": 878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "MLflow-Platform to Manage the ML Life Cycle  101\n\nTo explore this component, the database needs to be connected to MLflow. MLflow components explored earlier can be connected to a database for storing the information.\n\nSet up MLflow’s tracking URI using the following command: export MLFLOW_TRACKING_URI=http://localhost:5000\n\nSet up the MySQL server for MLflow You can install MySQL server if it is not installed already. For this, you can also refer to the official document at https://dev.mysql.com/doc/mysql-apt-repo-quick- guide/en/.\n\nFirst, create mlflow_user in MySQL using the following command: mysql -u mlflow_user\n\nCREATE USER 'mlflow_user'@'localhost' IDENTIFIED BY 'mlflow'\n\nGRANT ALL ON db_mlflow.* TO 'mlflow_user'@'localhost';\n\nFLUSH PRIVILEGES;\n\nEnter the password when prompted.\n\nCreate and select the database: CREATE DATABASE IF NOT EXISTS db_mlflow;\n\nuse db_mlflow;\n\nTo know which user(s) have the access to db_mlflow database and its privileges, you can execute the following command:\n\nSELECT * FROM mysql.db WHERE Db = 'db_mlflow'\\G;",
      "content_length": 1057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "102  Machine Learning in Production\n\nThe following figure shows the privileges of mlflow_user associated with the db_ mlflow database:\n\nFigure 5.21: Permissions of MLflow_user\n\nInstall the MySQLdb module:\n\nsudo apt-get install python3-mysqldb\n\nInstall MySQL client for Python:\n\npip install mysqlclient\n\nHere are the concepts and key features of the model registry:\n\nRegistered model: Once the model is registered using MLflow’s UI or API, the model is considered a registered model. MLflow’s model registry captures model versions and keeps track of the model’s stages (for example, production, and staging) and other metadata.\n\nModel version: MLflow’s model registry maintains the version of each model after registering it. For instance, if you saved a model name with a classification model, then it would be assigned to version 1 by default. However, after saving that model with the same name again, it will be saved as version 2.\n\nModel stage: For each model version, you can assign different stages, like staging, production, and archived. However, you cannot assign two stages to the same version of the model.",
      "content_length": 1119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "MLflow-Platform to Manage the ML Life Cycle  103\n\nAnnotations and descriptions: You can add comments, short descriptions, and annotations for models. Your team members will come to know about the model through the descriptions you add.\n\nStart the MLflow server All the required steps to start the MLflow server with MySQL as a database have been completed.\n\nTo start the MLflow server, you can use the following command:\n\nmlflow server --host 0.0.0.0 --port 5000 --backend-store-uri mysql:// mlflow_user:mlflow@localhost/db_mlflow --default-artifact-root $PWD/mlruns\n\nThe syntax to start the MLflow server is mlflow server <args>. In the preceding command, MLflow uses the backend store as MySQL server (provide MLflow username and database name) and the default artifacts store as the mlruns directory.\n\nYou can run the MLproject and pass the experiment name as a parameter (optional). mlflow run . --experiment-name ‘Loan_prediction’\n\nYou can see, in the MLflow UI, that the version column shows alphanumeric values. The model’s metrics parameters are displayed under the Loan_prediction experiment.\n\nThe following figure shows the output of the Loan_prediction experiment using MySQL as a backend store:\n\nFigure 5.22: Output of ML project on MLflow UI",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "104  Machine Learning in Production\n\nUsing the show tables; command, you can see that new tables have been created, such as metrics and experiments. Refer to Figure 5.23 to see the output of the command:\n\nFigure 5.23: MLflow tables in MySQL database\n\nLet’s check the experiments table using the command shown in Figure 5.24. As of now, Default and Loan_prediction are the two experiments being displayed.\n\nFigure 5.24: MLflow’s list of experiments is MySQL database\n\nIf you click on the Models tab on the top, you should see the following figure. As the model is not registered, the table does not contain any model information.\n\nFigure 5.25: Initial UI of MLflow models",
      "content_length": 671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "MLflow-Platform to Manage the ML Life Cycle  105\n\nNow, go to the run ID on the Experiment tab and scroll down; then, click on the model directory in the UI.\n\nThe Register Model button will appear, as shown in the following figure:\n\nFigure 5.26: Register Model button\n\nClick on the Register Model button, and a pop-up window will appear, as shown in the following figure. In the current case, the Create New Model option is selected from the drop-down menu, along with a model name; however, you can give any human-readable name to it. Finally, click on the Register button.\n\nFigure 5.27: Register model window\n\nAs shown in the following figure, the Register Model button will change to the model’s name (with hyperlink).\n\nFigure 5.28: Registered model\n\nNow, you should see the registered model on the Models tab. By default, it will label it as version 1. Since a state has not been assigned, you should see – (dash) in the staging and production columns.",
      "content_length": 956,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "106  Machine Learning in Production\n\nThe following figure shows the list of registered models:\n\nFigure 5.29: Registered model in MLflow Models UI\n\nWe can change the model’s state using MLflow UI or terminal. Here, the state is being changed to staging using MLflow’s UI.\n\nAs shown in the following figure, the model’s state is being changed from None to Staging:\n\nFigure 5.30: Registered model’s state transition\n\nIn the following figure, you can see that it is showing Version 1 in the staging column of the registered model.",
      "content_length": 527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "MLflow-Platform to Manage the ML Life Cycle  107\n\nFigure 5.31: Registered model’s state transitioned to staging\n\nThe registered model’s information can be found in the registered_models table in MySQL, as shown in the following figure:\n\nFigure 5.32: List of registered models in MySQL table\n\nWhen you are working in a team, you can add descriptions and tags to the registered model. It helps other team members to learn more about the model.\n\nNow, add descriptions and tags for Prediction_Model_LR.\n\nFigure 5.33: Registered model’s tags and description",
      "content_length": 553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "108  Machine Learning in Production\n\nYou have learned how to register models in MLflow, and now you are going to learn how to serve the model to make predictions on the given data.\n\nDeploy the model from MLflow’s registry using the following command:\n\nmlflow models serve -m \"models:/Prediction_model_LR/Staging\"\n\n1. import mlflow.pyfunc\n\n2.\n\n3. model_name = \"Prediction_model_LR\"\n\n4. stage = 'Staging'\n\n5. model=mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/ {stage}\")\n\n6.\n\n7. model.predict([[1.,1.,0.,1.,0.,4.55387689,360.,1.,2.,8.25556865]])\n\n8. array([1])\n\nNote: You can call the model’s REST API using postman, as shown in the following figure.\n\nFigure 5.34: Calling REST API of the deployed model using postman\n\nYou can get postman from the following link:\n\nhttps://www.postman.com/downloads/",
      "content_length": 811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "MLflow-Platform to Manage the ML Life Cycle  109\n\nThere are various ways to consume the REST API of deployed models. You can also use this REST API in other applications.\n\nConclusion This chapter explained the importance of MLflow in the ML life cycle and the production environment. You explored the role, functionality, and usage of MLflow, with examples of four components of MLflow. Then, you learned how MLflow helps data science developers at various stages of the ML life cycle. MLflow tracking allows one to choose the best model by keeping track of model metrics, hyperparameters used, and other useful information. The MLflow project component helps you to manage dependencies, and it can be run from the GitHub repository. The MLflow registry acts as a central location for registered models and changing the states, such as staging and production.\n\nIn the next chapter, you will learn how to use a docker for portability in transferring ML projects from one machine to another or to the server.\n\nPoints to remember\n\nMLflow helps you from the experimentation stage to the deployment stage of an ML project.\n\nExcept for the model registry, all the components can be used without being integrated with a database like MYSQL; however, it is a good practice to integrate them with a database like MYSQL.\n\nBy default, the MLflow project uses conda for installing dependencies; however, you can proceed without conda by using the –no-conda option.\n\nEach MLflow component can be accessed separately; however, you can connect them to create the flow.\n\nMultiple choice questions\n\n1. Which one of the following is not a MLflow component?\n\na) MLflow tracking\n\nb) MLflow develop\n\nc) MLflow models\n\nd) MLflow registry",
      "content_length": 1716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "110  Machine Learning in Production\n\n2. In the MLflow registry, the model state from staging to production can be changed using which of the following?\n\na) MLflow command\n\nb) MLflow UI\n\nc) Both a and b\n\nd) Mode state cannot be changed from staging to production\n\nAnswers 1. b\n\n2. c\n\nQuestions\n\n1. What is MLflow?\n\n2. What is the role of the conda.yaml file?\n\n3. What is the command to serve the model?",
      "content_length": 402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "Docker for ML  111\n\nChapter 6 Docker for ML\n\nIntroduction Containers are an abstraction at the app layer that packages code and dependencies together. Multiple containers can be run on the same machine and share the OS kernel. Each running container is considered an isolated process in user space.\n\nDocker automates the repetitive and time-consuming configuration task, saving both time and effort for the developer. Docker is a one-stop solution that includes UI, APIs, CLIs, and last but not least, security. Docker runs on Linux, Windows, and Mac OS.\n\nStructure This chapter discusses the following topics: •\t Role of Docker in Machine Learning •\t Hello World with Docker •\t Create a Dockerfile •\t Build Docker image •\t Run Docker container •\t Dockerize and deploy the ML model",
      "content_length": 782,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "112  Machine Learning in Production\n\nObjectives After studying this chapter, you should be able to build and deploy ML models into production as a container or orchestration service using Docker. You should be able to create a Dockerfile, which contains all the commands to be executed. Using Dockerfile, you should be able to build and run Docker images. You should also be able to run container service in the background using a detached mode. Finally, you can deploy an ML model using Docker.\n\nIntroduction to Docker Docker is a containerization platform to package applications and their dependencies in the form of a container. It ensures that all the required libraries and dependencies are wrapped in an isolated environment to run the application smoothly in the development, test, and production environments. Docker is popular among developers as it is lightweight, fast, portable, secure, and more efficient than virtual machines. A containerized application will start running as soon as you run the Docker container.\n\nDocker has its own Docker registry, called Docker hub. Docker hub allows developers to store and distribute container images over the internet. An image tag enables developers to differentiate images. A Docker registry has public and private repositories. A developer can store a container image on the Docker hub using the push command and retrieve one using the pull command.\n\nIt works on my machine! Many a time, the code works perfectly on your machine but throws an error when you run it on another machine. This happens with developers and data scientists as well. The reason could be anything from a different OS to a different release of an OS, different python versions, or dependency issues. So, when they face this issue, they might end up spending a lot of time fixing it. With Docker, you should not encounter these issues, as Docker packages require files, configuration, and commands for seamless flow.\n\nLong setup The traditional method of deploying in production environments takes a lot of time, as it needs to move the necessary files, install dependencies, configure, and save the output manually. Many a time, developers need to set up the environment first. It is more time-consuming when you have to repeat this process for different stages of the project, such as development, pre-production, and production. Manual deployment scripts are difficult to manage.",
      "content_length": 2415,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "Docker for ML  113\n\nWith Docker, you can put all the required files in the directory and write down the configuration, OS version, and commands to be executed sequentially in a Dockerfile. You can also connect the two Docker containers with the same network. Additionally, you can use the same Dockerfile for development, pre-production, and production.\n\nDocker ensures reproducibility, portability, easy deployment, granular updates, lightness, and simplicity.\n\nSetting up your environment and installing Docker You can refer to the instructions at https://docs.docker.com/installation to install the latest Docker-maintained package on your preferred operating system and take a look at the official Docker engine installation guide at https://docs.docker.com/ engine/install/ubuntu/.\n\nDocker installation Here are the general steps you can follow to install Docker on your Ubuntu machine. The Docker engine installation requires one of the following 64-bit Ubuntu versions:\n\nUbuntu Kinetic 22.10 •\t Ubuntu Jammy 22.04 (LTS) •\t Ubuntu Focal 20.04 (LTS) •\t Ubuntu Bionic 18.04 (LTS)\n\nUninstall old versions First off, uninstall old versions of Docker (if any): sudo apt-get remove docker docker-engine docker.io containerd runc\n\nSet up the repository:\n\n$ sudo apt-get update\n\n$ sudo apt-get install \\\n\nca-certificates \\\n\ncurl \\\n\ngnupg \\\n\nlsb-release",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "114  Machine Learning in Production\n\nAdd Docker’s official GPG key.\n\nThe GNU Privacy Guard (GPG or GnuPG) is a command-line tool that enables the implementation of public-key encryption and verification services. GPG is commonly used in Linux to sign files digitally, which assures the authenticity of software project files.\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\nUse the following command to set up the repository: $ echo \\\n\n\"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\n\n$(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker. list > /dev/null\n\nInstall Docker Engine sudo apt-get update\n\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n\nsudo docker –version\n\nThe following figure shows the current Docker version:\n\nFigure 6.1: Docker version\n\nRun the following command in the terminal to check the status of the Docker service:\n\nsudo systemctl status docker\n\nThe following figure shows the output of the preceding command:\n\nFigure 6.2: Docker status check",
      "content_length": 1170,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "Docker for ML  115\n\nDocker compose Docker compose enables developers to configure and run more than one container. It reads the configuration from the docker-compose.yml file. A single docker- compose up command can start the services and run the multi-container applications. On the other hand, you can destroy all of this using the docker-compose down command. You can also remove the volumes by adding the -- volumes flag.\n\nYou can install the Docker compose using the following command: sudo curl -L \"https://github.com/docker/compose/releases/ download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/ bin/docker-compose\n\nThe following figure shows the Docker compose installation:\n\nFigure 6.3: Docker compose installation\n\nAuthorize docker-compose to execute files: sudo chmod +x /usr/local/bin/docker-compose\n\nFinally, verify the installation: docker-compose --version\n\nThe following figure shows the current Docker compose version:\n\nFigure 6.4: Docker compose version\n\nThe installation provides the following:\n\nDocker Engine •\t Docker CLI client •\t Docker Compose",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "116  Machine Learning in Production\n\nHello World with Docker This is a sample Docker image to test whether Docker is working properly. By running the following command, you will create the first Docker container:\n\nsudo docker run hello-world\n\nThe following figure shows the execution of the preceding command:\n\nFigure 6.5: Docker image - hello-world\n\nIt also generates the following output, which tells you about what happened behind the scenes:\n\nTo generate this message, Docker took the following steps:\n\n1. The Docker client contacted the Docker daemon.\n\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64)\n\n3. The Docker daemon created a new container from that image, which runs the executable that produces the output you are currently reading.\n\n4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.\n\nYou executed the docker run command, followed by the image name. The required image was not there in the system, so rather than stopping further execution, it pulled the image from the Docker hub.\n\nThe following figure shows the Docker structure in a system. As you can see, Docker can have multiple containers running in an isolated environment while sharing the same infrastructure.",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "Docker for ML  117\n\nFigure 6.6: Docker stack\n\nDocker objects While working with Docker, you can create and use Docker objects like images, containers, volumes, and networks. In this chapter, you are going to learn about some of these objects.\n\nDockerfile Dockerfile can be considered as a set of commands or instructions that enables developers to build Docker images. These commands or instructions get executed sequentially. It is a plain text document with no extension.\n\nDocker image To create a Docker container, a Docker image needs to be created. It stores all the code and dependencies required to run the application and acts as a template to run a container instance. A Docker image can be uploaded on the Docker hub, from where it can be pulled to the server or system to run the container.\n\nTo view the list of available Docker images, execute the following command in the terminal:\n\ndocker images",
      "content_length": 910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "118  Machine Learning in Production\n\nDocker containers An instance of a container is created when you run the Docker image. You can use the same Docker image to run as many containers as you want. It is an important component of the Docker ecosystem. Docker runs containers in an isolated environment.\n\nAn additional layer called a container layer, gets automatically created on top of the existing image layers when the developer runs a container. A Docker container has its own read and write layer, which allows developers to make changes that are specific to that container. Suppose you are running three containers using the same Docker image, and you install another version of the python package inside a running container. This will not affect the existing version of the python package in other containers. Docker manages the data within the Docker container using Docker Volumes.\n\nAll the files that you created in an image or a container are part and parcel of the Union file system. However, the data volume is part of the Docker host file system, and it is simply mounted inside the container.\n\nIt is initialized when the container is created. By default, it is not deleted when the container is stopped. Data volumes can be shared across containers too and can be mounted in read-only mode as well.\n\nThe command to check all the running containers is docker ps.\n\nThe command to check all the running and stopped containers is docker ps -a.\n\nDetached mode To run Docker in the background, run the container in detached mode. You can use the -d flag to run the container in detached mode.\n\nDocker container networking Typically, a Docker host comprises multiple Docker containers. Docker containers also need to interact and collaborate with local as well as remote ones to come out with distributed applications. The bridge network is the default network interface that Docker Engine assigns to a container. docker network ls\n\nThis command will show you the list of networks and their scope. The output contains the following headers: NETWORK ID, NAME, DRIVER, SCOPE\n\nTo check the network details of any Docker container, use the following command, followed by the network ID:",
      "content_length": 2190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Docker for ML  119\n\ndocker network inspect <NETWORK ID>\n\nPort mapping\n\nThe -p flag is used to map container ports to host ports. Consider this example:\n\ndocker run -p <myport>:<containerport> nginx\n\nNote: To observe the output, use docker logs [container_id].\n\nCreate a Dockerfile Dockerfile follows a simple and easy-to-understand structure, that is, # comment, followed by an instruction and an argument. It is a standard practice to write instructions in the uppercase to differentiate between instructions and arguments.\n\nFROM\n\nThe FROM instruction sets the base image for subsequent instructions. •\t A valid Dockerfile must have a FROM instruction. •\t FROM can occur multiple times in the Dockerfile.\n\nCMD\n\nCMD defines a default command to execute when a container is being created. •\t CMD does not get executed while building an image. •\t Can be overridden at runtime.\n\nRUN\n\nIt executes a command in a new layer on top of the current image and commits the results.\n\nCOPY\n\nThe COPY instruction copies new files or directories from <src> and adds them to the file system of the container at the path <dest>.\n\nENTRYPOINT\n\nThis helps you to configure the container as an executable; it is similar to CMD. There can be at max one instruction for ENTRYPOINT; if more than one is specified, only the last one will be honored.",
      "content_length": 1325,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "120  Machine Learning in Production\n\nWORKDIR <path>\n\nThis sets the working directory for the RUN, CMD, and ENTRYPOINT instructions that follow it.\n\nEXPOSE\n\nThis exposes the network ports on the container, on which it will listen at runtime.\n\nENV\n\nThis will set the environment variables <key> to <value> within the container. When a container is running from the resulting image, it will pass and persist all the information to the application running inside the container.\n\nBuild a Docker image Using the docker build command, users can automate a docker build that executes several command-line instructions in succession.\n\nThe docker build command builds an image from a Dockerfile and a context:\n\ndocker build -t ImageName:TagName dir\n\nWhere:\n\n-t: Image tag ● ImageName: The name you want to give to your image ● TagName: The tag you want to give to your image ● dir: The directory where the Dockerfile is present\n\nFor the current directory, simply use . (period):\n\nsudo docker build –t myimage:v1 .\n\nCheck the newly created image using the docker images command.\n\nThe next step is to build the container from the newly created image.\n\nNote: To check all the commands run against that image, execute the following command docker history [Image_id]\n\nRun a Docker container A Docker container is a runtime instance of a Docker image. The docker run command can run the Docker image as follows: docker run --name test -it myimage:v1",
      "content_length": 1434,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "Docker for ML  121\n\nWhere:\n\n-it: It is used to mention that you want to run the container in interactive mode.\n\n--name: It is used to give a name to the container. •\t myimage: It is the image name that is to be run. •\t v1: It is the tag of the image.\n\nThe docker inspect [container_id] command will populate the complete information of the container in JSON format.\n\nThe docker top [container_id] command will show top-level processes within a container.\n\nThe following figure shows the lifecycle and flow of the Docker container:\n\nFigure 6.7: Docker container lifecycle",
      "content_length": 571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "122  Machine Learning in Production\n\nDockerize and deploy the ML model Let’s consider the scenario of loan prediction, where you need to predict whether a customer’s loan will be approved.\n\n1. # Importing the required packages\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4. import warnings\n\n5. warnings.filterwarnings('ignore')\n\n6. from sklearn.linear_model import LogisticRegression\n\n7.\n\n8. from sklearn import preprocessing\n\n9. from sklearn.model_selection import train_test_split\n\n10. from sklearn import metrics\n\n11. import joblib\n\nAfter that, load the data sets and capture numerical and categorical column names in separate variables for the next step:\n\n1. # Loading the data\n\n2. data = pd.read_csv(\"loan_dataset.csv\")\n\n3. # Missing value treatment (if found)\n\n4. num_col = data.select_dtypes(include=['int64','float64']).columns. tolist()\n\n5. cat_col = data.select_dtypes(include=['object']).columns.tolist()\n\n6. cat_col.remove('Loan_Status')\n\nHandle missing values for categorical and numerical columns:\n\n1. for col in cat_col:\n\n2. data[col].fillna(data[col].mode()[0], inplace=True)\n\n3.\n\n4. for col in num_col:\n\n5. data[col].fillna(data[col].median(), inplace=True)",
      "content_length": 1179,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Docker for ML  123\n\nClip extreme values for numerical data:\n\n1. # Outlier treatment (if found)\n\n2. data[num_col] = data[num_col].apply(\n\n3. lambda x: x.clip(*x.quantile([0.05, 0.95])))\n\nCreate a new feature as TotalIncome, which is the sum of applicant income and co-applicant income:\n\n1. # Creating a new variable\n\n2. data['TotalIncome'] = data['ApplicantIncome'] + data['CoapplicantIncome']\n\n3. data = data.drop(['ApplicantIncome','CoapplicantIncome'], axis=1)\n\nConvert categorical to numeric columns using the label encoding technique:\n\n1. cat_col.remove('Loan_ID')\n\n2.\n\n3. # Encoding categorical features\n\n4. for col in cat_col:\n\n5. le = preprocessing.LabelEncoder()\n\n6. data[col] = le.fit_transform(data[col])\n\n7.\n\n8. data['Loan_Status'] = le.fit_transform(data['Loan_Status'])\n\n9.\n\n10. # Model building\n\n11. X = data.drop(['Loan_Status', 'Loan_ID'],1)\n\n12. y = data.Loan_Status\n\n13. features = X.columns.tolist()\n\n14.\n\n15. model = LogisticRegression(solver='lbfgs', max_iter=1000, random_ state=1)\n\n16. model.fit(X, y)\n\n17.\n\n18. joblib.dump(model, 'LR_model.pkl')",
      "content_length": 1070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "124  Machine Learning in Production\n\nNow, create a menu-driven prediction inside the while loop, which will take the user input and make the predictions:\n\n1. # Menu driven\n\n2.\n\n3. print(\"Type 'exit' to terminate.....\\n\")\n\n4. print('''Gender: Female = 0, Male=1\n\n5. Married: No = 0, Yes = 1\n\n6. Education: Graduate = 0 , Under-graduate = 1\n\n7. Self_Employed: No = 0, Yes = 1\n\n8. Property_Area: Urban = 2, Semiurban = 1, Rural = 0\n\n9. Loan_Status: No = 0, Yes = 1\\n''')\n\n10.\n\n11. print('''Pass the data in following sequence separated by comma\n\n12. Gender, Married, Dependents,Education,Self_ Employed,LoanAmount,Loan_Amount_Term,Credit_History,Property_ Area,TotalIncome\\n''') 13.\n\n14. # model = joblib.load('LR_model.pkl')\n\n15.\n\n16. while True:\n\n17. user_data=input(\"Enter your data: \")\n\n18.\n\n19. if(user_data==\"exit\"):\n\n20. break\n\n21.\n\n22. data = list(map(float, user_data.split(',')))\n\n23.\n\n24. # exception handling\n\n25. if(len(data)<10):\n\n26. print(\"Incomplete data provided!!\")\n\n27. else:",
      "content_length": 993,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "Docker for ML  125\n\n28.\n\n29. # predicting the value\n\n30. predicted_value=model.predict([data])\n\n31. print(\"/_______________________________________________/\")\n\n32. if (predicted_value[0]):\n\n33. print(\"\\tCongratulations! your loan approval request is processed\")\n\n34. else:\n\n35. print(\"\\tSorry! your loan approval request is rejected\")\n\n36. print(\"/_______________________________________________/\")\n\nNext, create a requirements.txt file of dependencies.\n\n1. numpy==1.19.5\n\n2. pandas==1.1.5\n\n3. scikit-learn==0.24.2\n\n4. joblib==1.0.1\n\nNow, create a Dockerfile for the preceding application with dependencies.\n\n1. # STEP 1: Install base image. Optimized for Python\n\n2. FROM python:3.7-slim-buster\n\n3.\n\n4. # STEP 2: Upgrading pip\n\n5. RUN pip install --upgrade pip\n\n6.\n\n7. # STEP 3: Copying all the files to the app directory\n\n8. COPY . /app\n\n9.\n\n10. # STEP 4: Set the working directory to the previously added app directory\n\n11. WORKDIR /app\n\n12.\n\n13. # STEP 5: Giving permissions to python file\n\n14. RUN chmod +x train.py",
      "content_length": 1020,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "126  Machine Learning in Production\n\n15.\n\n16. # STEP 6: Install required python dependencies from the requirements file\n\n17. RUN pip install -r requirements.txt\n\n18.\n\n19. # STEP 7: Run the train.py file\n\n20. ENTRYPOINT [\"python\"]\n\n21.\n\n22. CMD [\"train.py\"]\n\nWhen you have all the files at one place, you can start building the Docker image using the docker build command, as shown in the following figure:\n\nFigure 6.8: Docker image build started\n\nThis may take a few minutes to complete. After completion, you should see the last message, as shown in the following figure:\n\nFigure 6.9: Docker image built\n\nThe next step is to run the image to start the Docker container instance using the docker run command. Finally, it’s time to make the prediction. Add a brief description before taking the input from the user and, in this case, input data needs to pass in the sequence given in the description.\n\nExecute the docker run command with the -it flag to run the image in an interactive mode, as shown in the following figure:",
      "content_length": 1025,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Docker for ML  127\n\nFigure 6.10: Docker container - ML model prediction\n\nTo come out of the application, the user can pass exit; this will break the while loop.\n\nCommon Docker commands Just run the docker command in the terminal, and you will get the list of commonly used Docker commands. You can also refer to the exhaustive list of Docker CLI commands at https://docs.docker.com/engine/reference/commandline/docker/.\n\nConclusion A Docker container runs in an isolated environment. You learned the importance of the Docker framework in the ML lifecycle and production environment. In this chapter, you explored the role, functionality, and usage of Docker components, with examples. You also learned how to create a Dockerfile and build and run an image in a container. You learned to create and deploy an ML model using Docker in interactive mode.\n\nThe next chapter begins with REST APIs and explains how to use API for deploying ML models. Then, it will cover web frameworks, and finally, you will learn to build a UI for ML model API.\n\nPoints to remember\n\nDocker is a containerization platform for packaging applications and their dependencies in the form of a container.",
      "content_length": 1177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "128  Machine Learning in Production\n\nDocker ensures reproducibility, portability, easy deployment, granular updates, lightness, and simplicity.\n\nDocker manages data within the Docker container using Docker Volumes. •\t An instance of a container gets created when you run the Docker image.\n\nMultiple choice questions\n\n1.\n\nWhat is the command to check the history of the Docker image?\n\na) docker history [Image_id] b) docker all commands [Image_id] c) docker hist [Image_id] d) history [Image_id]\n\n2. Which one of the following sentences is incorrect?\n\na) Docker host comprises multiple Docker containers.\n\nb) A Docker container is a runtime instance of a Docker image. c) The command to check running containers is docker ps.\n\nd) A Docker container instance cannot be created using a Docker image.\n\nAnswers 1. a\n\n2. d\n\nQuestions\n\n1. What is Docker?\n\n2. What is the role of Dockerfile?\n\n3. What is the command to build the Docker image from Dockerfile?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "Build ML Web Apps Using API  129\n\nChapter 7 Build ML Web Apps Using API\n\nIntroduction When developing a web app in Python, you are very likely to use a framework for it. A framework is a library that eases the life of a developer while building scalable, standard, and production-ready web applications.\n\nThis chapter will begin with REST APIs and how to use APIs for deploying ML models. Moving on, it will cover different web frameworks and finally, illustrate the steps to build a UI for ML model API. Toward the end of this part, you should know how to deploy an ML web app.\n\nStructure In this chapter, the following topics will be discussed:\n\nREST APIs •\t FastAPI •\t Streamlit •\t Flask •\t Build ML Web App",
      "content_length": 711,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "130  Machine Learning in Production\n\nObjectives After studying this chapter, you should be able to build and deploy ML-based web apps. You should also be able to create an API for your ML model and call it by passing the parameters. Additionally, this chapter will teach you to create web applications using FastAPI, Streamlit, and Flask frameworks. It will cover integrating NGINX and Gunicorn with Flask to create automated ML-based web applications, and you should be comfortable developing web apps in any framework out of the three most commonly used frameworks: FastAPI, Streamlit, and Flask.\n\nREST APIs REST is an acronym for Representational State Transfer. REST is an architectural style mainly created to guide the development and design of the architecture for the World Wide Web (WWW). In simple words, a web service or web API following REST architecture is a REST API.\n\nREST is a pattern to make APIs that can be used to access resources like images, videos, text, JSON, and XML hosted on the server. RESTful API provides a common platform for communicating between applications built in different programming languages. Refer to Figure 7.1:\n\nFigure 7.1: REST API\n\nThe interesting thing about the API is that the client does not need to know the internal operations performed at the server’s end and vice versa. REST API treats any data requested/processed by the user as a resource; it can be text, image, video, and so on.\n\nREST API is stateless; it means that the client should provide all the parameters in the request every time the API is called. The server will not store previous parameters passed with the request by the client.",
      "content_length": 1652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Build ML Web Apps Using API  131\n\nFastAPI FastAPI is a web framework for developing RESTful APIs in Python. FastAPI is a lightweight (compared to Django), easy-to-install, easy-to-code yet high-performing framework. It enables the development of REST API with a minimal code requirement. FastAPI comes with built-in standard and interactive documentation. Once you develop and run the API, you can access the documentation for your application at {API endpoint}/docs or {API endpoint}/redoc.\n\nNote: FastAPI requires Python 3.6 and above\n\nLet’s install FastAPI and uvicorn, an Asynchronous Server Gateway Interface (ASGI) server, for production. pip install fastapi uvicorn\n\nLet’s consider a loan prediction scenario, where the goal is to predict whether a customer’s loan will be approved. The focus will not be on hyperparameter tuning and model optimization. However, you can optimize a model to improve its overall performance.\n\n1. # Importing the required packages\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4. import warnings\n\n5. warnings.filterwarnings('ignore')\n\n6. from sklearn.ensemble import RandomForestClassifier\n\n7.\n\n8. from sklearn import preprocessing\n\n9. from sklearn.model_selection import train_test_split\n\n10. from sklearn import metrics\n\n11. import pickle\n\nLoad the datasets, and capture numerical and categorical column names in separate variables for the next step:\n\n1. # Loading the data\n\n2. data = pd.read_csv(\"loan_dataset.csv\")\n\n3. # Capturing numerical and categorical column names",
      "content_length": 1514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "132  Machine Learning in Production\n\n4. num_col = data.select_dtypes(include=['int64','float64']).columns. tolist()\n\n5. cat_col = data.select_dtypes(include=['object']).columns.tolist()\n\n6. cat_col.remove('Loan_Status')\n\nTo handle missing values, you can replace categorical missing values with mode and numerical missing values with the median.\n\n1. # Missing value treatment (if found)\n\n2. for col in cat_col:\n\n3. data[col].fillna(data[col].mode()[0], inplace=True)\n\n4.\n\n5. for col in num_col:\n\n6. data[col].fillna(data[col].median(), inplace=True)\n\nInstead of removing extreme values (outliers) from numerical data, you can cap them at 5% on the lower side and 95% on the upper side.\n\nIn other words, data points lower than the 5% quantile will be replaced by a value at the 5% quintile, and on the other hand data points higher than the 95% quantile will be replaced by a value at the 95% quintile.\n\n1. # Outlier treatment (if found)\n\n2. data[num_col] = data[num_col].apply(\n\n3. lambda x: x.clip(*x.quantile([0.05, 0.95])))\n\nCreate a new feature, TotalIncome, which is the sum of applicant income and co- applicant income.\n\n1. # Creating a new variable\n\n2. data['TotalIncome'] = data['ApplicantIncome'] + data['CoapplicantIncome']\n\n3. data = data.drop(['ApplicantIncome','CoapplicantIncome'], axis=1)\n\nConvert categorical to numeric columns using the label encoding technique. The target column Loan Status will be encoded into numeric.\n\n1. cat_col.remove('Loan_ID')\n\n2.\n\n3. # Encoding categorical features",
      "content_length": 1510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Build ML Web Apps Using API  133\n\n4. for col in cat_col:\n\n5. le = preprocessing.LabelEncoder()\n\n6. data[col] = le.fit_transform(data[col])\n\n7.\n\n8. data['Loan_Status'] = le.fit_transform(data['Loan_Status'])\n\nNow, you will build an ML model using a random forest classifier and will store the trained models in a pickle object. Here, you can use other ML algorithms and different ML techniques, like GridSearchCV, to improve accuracy. Here, you will build a baseline ML model.\n\n1. # Model building\n\n2. X = data[['Gender', 'Married', 'TotalIncome', 'LoanAmount','Credit_ History']]\n\n3. y = data['Loan_Status']\n\n4. features = X.columns.tolist()\n\n5.\n\n6. model = RandomForestClassifier(max_depth=4, random_state = 10)\n\n7. model.fit(X, y)\n\n8.\n\n9. # saving the model\n\n10. pickle_model = open(\"trained_model/model_rf.pkl\", mode = \"wb\")\n\n11. pickle.dump(model, pickle_model)\n\n12. pickle_model.close()\n\n13.\n\n14. # loading the trained model\n\n15. pickle_model = open('trained_model/model_rf.pkl', 'rb')\n\n16. model_rf = pickle.load(pickle_model)\n\n17.\n\n18. prediction = model_rf.predict([[1, 1, 6000, 150, 0]])\n\n19. print(prediction)",
      "content_length": 1120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "134  Machine Learning in Production\n\nNext, create a requirements.txt file for the dependencies.\n\n1. numpy==1.19.5\n\n2. pandas==1.1.5\n\n3. scikit-learn==0.24.2\n\nNow, create a loan_pred_app.py file. First, load the dependencies and pickle the object of the trained model. Furthermore, create a FastAPI instance and assign it to the app. This will make the app a point of interaction while creating the API.\n\n1. # Importing Dependencies\n\n2. from fastapi import FastAPI\n\n3. from pydantic import BaseModel\n\n4. import uvicorn\n\n5. import pickle\n\n6. import numpy as np\n\n7. import pandas as pd\n\n8.\n\n9. app = FastAPI()\n\n10. # loading the trained model\n\n11. trained_model = 'trained_model/model_rf.pkl'\n\n12. model = pickle.load(open(trained_model, 'rb'))\n\nCreate a class LoanPred that defines the input data type expected from the client.\n\nYou will use the LoanPred class for the data model that will define the data type expected from the users. It is inherited from BaseModel. Then add root view with the function that returns 'message': 'Loan Prediction App' on the home page.\n\n1. class LoanPred(BaseModel):\n\n2. Gender: float\n\n3. Married: float\n\n4. ApplicantIncome: float\n\n5. LoanAmount: float\n\n6. Credit_History: float\n\n7.",
      "content_length": 1214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "Build ML Web Apps Using API  135\n\n8. @app.get('/')\n\n9. def index():\n\n10. return {'message': 'Loan Prediction App'}\n\nThe following function will create the UI for user input. Here, the /predict class is created as an endpoint, also known as a route. Then, pass the data model, that is, the LoanPred class, to the predict_loan_status() function as a parameter.\n\n1. # Define the function, which will make the prediction using the input data provided by the user\n\n2. @app.post('/predict')\n\n3. def predict_loan_status(loan_details: LoanPred):\n\n4. data = loan_details.dict()\n\n5. gender = data['Gender']\n\n6. married = data['Married']\n\n7. income = data['ApplicantIncome']\n\n8. loan_amt = data['LoanAmount']\n\n9. credit_hist = data['Credit_History']\n\n10.\n\n11. # Make predictions\n\n12. prediction = model.predict([[gender,married,income,loan_ amt,credit_hist]])\n\n13.\n\n14. if prediction == 0:\n\n15. pred = 'Rejected'\n\n16. else:\n\n17. pred = 'Approved'\n\n18.\n\n19. return {'status':pred}\n\n20.\n\n21. if __name__ == '__main__':\n\n22. uvicorn.run(app, host='127.0.0.1', port=8000)",
      "content_length": 1057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "136  Machine Learning in Production\n\nNow, it’s time to run the app and see standard UI auto-generated by FastAPI, which uses swagger, now known as openAPI. uvicorn loan_pred_app:app --reload\n\nThe preceding command can be interpreted as follows:\n\nloan_pred_app refers to the name of the file where the API is created. •\t The app is the instance defined in it. •\t --reload will simply restart the FastAPI server every time a change is made in the app file.\n\nThe following figure shows the terminal output after running the preceding command:\n\nFigure 7.2: Running the FastAPI app in terminal\n\nOnce you see the Application startup complete on the terminal, open the browser and go to the path mentioned in the terminal. In this case, it is 127.0.0.1:8000.\n\nThe following figure shows the UI of the FastAPI app in the browser. Here, the text message Loan Prediction App is displayed.\n\nFigure 7.3: FastAPI app in terminal\n\nThe following figure shows the Swagger (openAPI) UI of the FastAPI app on the browser. Simply add /docs to the end of the URL, and you should see auto-generated docs for the app.",
      "content_length": 1096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Build ML Web Apps Using API  137\n\nFigure 7.4: Docs of the FastAPI app\n\nFastAPI provides the functionality to validate the data type. It detects the invalid data type at runtime and returns the bad input to the client, which eventually reduces the burden of managing exceptions at the developer’s end.\n\nThe following figure shows the schema and the data type of the data expected from the client. As there are five variables in the mentioned model, and they can be seen in the figure, along with their data type.\n\nFigure 7.5: Schema of FastAPI app",
      "content_length": 547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "138  Machine Learning in Production\n\nThe following figure explores the predict() of the app. Here, a sample JSON input is entered, which is expected from the client. By default, the values of all the variables are zero in the schema. To pre-test the app, you have to click on the Try it out button.\n\nFigure 7.6: Predict() of FastAPI app\n\nAfter clicking on the Try it out button, you can pass the variable values or data in JSON format. Figure 7.7 shows the data for given variables being provided by the user. Finally, click on the Execute button at the bottom to see the prediction returned by the API.\n\nFigure 7.7: Parameters passing in the predict function",
      "content_length": 660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "Build ML Web Apps Using API  139\n\nThe following figure shows the predicted outcome under the Response body in JSON format. Based on the data provided, the predicted status of the loan is returned as ‘Approved’. Optionally, you can run the curl command to get the output in the terminal. Response code 200 shows that the request has succeeded without any error.\n\nFigure 7.8: Prediction outcome of FastAPI\n\nStreamlit Streamlit is an open-source library in Python that enables users to build and share attractive UI for machine learning models. It comes with extensive documentation to learn and explore. With streamlit, you can add beautiful and interactive widgets to get the user inputs with a few lines of code, such as a dropdown selection box and a slider to change the values.\n\nAccording to the makers, streamlit is the fastest way to build ML apps and deploy them on the cloud. It is a great framework to deploy ML apps using Python. In streamlit, everything can be coded in Python without the need for any front-end skills such as JavaScript to develop stunning UI for the app. Streamlit is a framework",
      "content_length": 1109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "140  Machine Learning in Production\n\nthat converts the Python code into interactive apps and enables data scientists to build data and model-based apps quickly.\n\nLet’s install streamlit using the following command:\n\npip install streamlit\n\nVerify the streamlit installation using the following:\n\nstreamlit hello\n\nNow, create a streamlit_app.py file for the ML-based streamlit application:\n\n1. import pickle\n\n2. import streamlit as st\n\n3.\n\n4. # loading the trained model\n\n5. trained_model = 'trained_model/model_rf.pkl'\n\n6. model = pickle.load(open(trained_model, 'rb'))\n\n7.\n\n8. @st.cache()\n\nThe following function will make the prediction based on input received from the front end:\n\n1. # The following function will make the prediction based on data provided by the user\n\n2. def prediction(Gender, Married, ApplicantIncome, LoanAmount, Credit_History):\n\n3. # Pre-processing user input\n\n4. if Gender == \"Male\":\n\n5. Gender = 1\n\n6. else:\n\n7. Gender = 0\n\n8.\n\n9. if Married == \"Unmarried\":\n\n10. Married = 1\n\n11. else:",
      "content_length": 1013,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "Build ML Web Apps Using API  141\n\n12. Married = 0\n\n13.\n\n14. if Credit_History == \"Unclear Debts\":\n\n15. Credit_History = 1\n\n16. else:\n\n17. Credit_History = 0\n\n18.\n\n19. # Making predictions\n\n20. prediction = model.predict(\n\n21. [[Gender, Married, ApplicantIncome, LoanAmount, Credit_ History]])\n\n22.\n\n23. if prediction == 0:\n\n24. pred = 'Rejected'\n\n25. else:\n\n26. pred = 'Approved'\n\n27. return pred\n\nThe following is a function that contains streamlit code, which will be responsible for taking input data from the user and displaying the prediction of the model:\n\n1. # The Following function is to define the home page of the streamlit application\n\n2. def main():\n\n3.\n\n4. # Front-end view for the app\n\n5. html_temp = \"\"\"\n\n6. <div style =\"background-color:green;padding:1px\">\n\n7. <h1 style =\"color:black;text-align:center;\">\n\n8. Loan Prediction App\n\n9. </h1>\n\n10. </div>\n\n11. \"\"\"\n\n12.",
      "content_length": 883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "142  Machine Learning in Production\n\n13. # display the front-end aspect\n\n14. st.markdown(html_temp, unsafe_allow_html = True)\n\n15.\n\n16. # Following code is to create a box field to get user data\n\n17. Gender = st.selectbox('Gender',(\"Male\",\"Female\"))\n\n18. Married = st.selectbox('Marital Status',(\"Unmarried\",\"Married\"))\n\n19. ApplicantIncome = st.number_input(\"Applicants monthly income\")\n\n20. LoanAmount = st.number_input(\"Total loan amount\")\n\n21. Credit_History = st.selectbox('Credit_History',(\"Unclear Debts\",\n\n22. \"No Unclear Debts\"))\n\n23. result =\"\"\n\n24.\n\n25. # When Predict button is clicked it will make the prediction and display it\n\n26. if st.button(\"Predict\"):\n\n27. result = prediction(Gender, Married, ApplicantIncome,\n\n28. LoanAmount, Credit_History)\n\n29. st.success('Your loan is {}'.format(result))\n\n30.\n\n31. if __name__=='__main__':\n\n32. main()\n\nOpen the terminal and run the following command where the streamlit_app.py file is located:\n\nstreamlit run streamlit_app.py\n\nOr\n\nstreamlit run streamlit_app.py &>/dev/null&\n\nFigure 7.9 shows the front end of the streamlit app. Here, the prediction is based on the data provided by the user.",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "Build ML Web Apps Using API  143\n\nFigure 7.9: Streamlit app\n\nTo deploy your Streamlit app on the streamlit cloud, you can use Streamlit sharing. Upload your files with the requirements.txt file on GitHub. Create an account on their website at https://streamlit.io/cloud and then provide a GitHub link and streamlit app file. This will deploy your app on the streamlit cloud.\n\nFlask Flask is a web framework that allows you to build web applications using Python. It is a lightweight framework compared to Django. It follows the REST architecture. You can develop simple web applications with Flask, as it requires less base code. Flask is based on the Web Server Gateway Interface (WSGI) and Jinja2 engine.",
      "content_length": 707,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "144  Machine Learning in Production\n\nTo start a Flask application, you need to use the run() function. If you set debug=True inside the run() function, then it becomes easy to track the error. When you enable debug mode, the server will restart every time you make changes in the app file and save it. If an error occurs, then it shows the reason in the browser itself. However, you should not use this feature while deploying models in the production environment.\n\nIn a Flask, you can call static files like CSS or JavaScript files to render the web page of the app. The route() function guides the Flask to the URL called by the function.\n\npip install Flask\n\nTo run the Flask application in the terminal, go to the directory where app.py and other required files are located:\n\npython app.py\n\nCreate an index.html file with the following HTML code to build the front end for the users. This will enable the users to provide the input data through UI.\n\n1. <html>\n\n2. <head>\n\n3. <title>LOAN PREDICTION</title>\n\n4. </head>\n\n5. <body>\n\n6. <h2 align=\"center\">LOAN PREDICTION</h2>\n\n7. <br/>\n\n8. <form action=\"/predict\" method=\"POST\">\n\n9. <table align=\"center\" cellpadding = \"10\">\n\n10. <!----- First Name ------------------------------------------ ----------->\n\n11. <tr>\n\n12. <td>FIRST NAME</td>\n\n13. <td>\n\n14. <input type=\"text\" name=\"First_Name\" maxlength=\"30\" placeholder=\"characters a-z A-Z\"/>\n\n15. </td>\n\n16. </tr>\n\n17. <!----- Last Name ------------------------------------------- ----------->",
      "content_length": 1494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "Build ML Web Apps Using API  145\n\n18. <tr>\n\n19. <td>LAST NAME</td>\n\n20. <td>\n\n21. <input type=\"text\" name=\"Last_Name\" maxlength=\"30\" placeholder=\"characters a-z A-Z\" />\n\n22. </td>\n\n23. </tr>\n\n24. <!----- Gender---------------------------------------------------->\n\n25. <tr>\n\n26. <td>GENDER</td>\n\n27. <td>\n\n28. <input type=\"radio\" name=\"gender\" id=\"gender\" value=\"1\">Male</input>\n\n29. <input type=\"radio\" name=\"gender\" id=\"gender\" value=\"0\">Female</input>\n\n30. </td>\n\n31. </tr>\n\n32. <!-----Marital Status--------------------------------------------------->\n\n33. <tr>\n\n34. <td>MARRIED</td>\n\n35. <td>\n\n36. <input type=\"radio\" name=\"married\" id=\"married\" value=\"1\"> Yes </input>\n\n37. <input type=\"radio\" name=\"married\" id=\"married\" value=\"0\"> No </input>\n\n38. </td>\n\n39. </tr>\n\n40. <!----- Income ---------------------------------------------- ----------->\n\n41. <tr>\n\n42. <td>TOTAL INCOME</td>",
      "content_length": 890,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "146  Machine Learning in Production\n\n43. <td>\n\n44. <input type=\"text\" name=\"total_income\" maxlength=\"30\" placeholder=\"$(thousands)\" />\n\n45. </td>\n\n46. </tr>\n\n47. <!----- Loan Amount ----------------------------------------- ----------->\n\n48. <tr>\n\n49. <td>LOAN AMOUNT</td>\n\n50. <td>\n\n51. <input type=\"text\" name=\"loan_amt\" maxlength=\"30\" placeholder=\"$(thousands)\" />\n\n52. </td>\n\n53. </tr>\n\n54. <!----- Credit History -------------------------------------- ----------->\n\n55. <tr>\n\n56. <td>CREDIT HISTORY</td>\n\n57. <td>\n\n58. <input type=\"radio\" name=\"credit_history\" id=\"credit_ history\" value=\"1\"> Yes </input>\n\n59. <input type=\"radio\" name=\"credit_history\" id=\"credit_ history\" value=\"0\"> No </input>\n\n60. </td>\n\n61. </tr>\n\n62. <!----- Submit and Reset ------------------------------------ ----------->\n\n63. <tr>\n\n64. <td colspan=\"2\" align=\"center\">\n\n65. <input type=\"submit\" value=\"Submit\">\n\n66. &nbsp;&nbsp; <!----- to add extra space -------->\n\n67. <input type=\"reset\" value=\"Reset\" onclick=\"location. href='/';\">",
      "content_length": 1018,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "Build ML Web Apps Using API  147\n\n68. </td>\n\n69. </tr>\n\n70. </table>\n\n71. </form>\n\n72. <h3 align=\"center\"> {{ prediction }} </h3>\n\n73. </body>\n\n74. </html>\n\nHere, the result.html file will display the prediction output to the users:\n\n1. <!doctype html>\n\n2. <html>\n\n3. <body>\n\n4. <h1>{{ prediction }}</h1>\n\n5. </body>\n\n6. </html>\n\nNow, create an app.py file for the Flask application:\n\n1. # Importing Dependencies\n\n2. from flask import Flask,render_template,url_for,request\n\n3. import pandas as pd\n\n4. from sklearn.ensemble import RandomForestClassifier\n\n5. import numpy as np\n\n6. import pickle\n\n7. import os\n\nThe next step is to load the serialized model object to make the prediction:\n\n1. app = Flask(__name__)\n\n2. port = int(os.environ.get(\"PORT\", 80))\n\n3. # Loading the trained model\n\n4. pickle_in = open('trained_model/model_rf.pkl', 'rb')\n\n5. model = pickle.load(pickle_in)",
      "content_length": 879,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "148  Machine Learning in Production\n\nNow, create the views for the Flask application. The following code will render the HTML template, which will be the landing page of the Flask application. app. route(‘/’) will route to the home page URL; the trailing slash ‘/’is generally used as a convention for the home page.\n\n1. # Views\n\n2. @app.route('/')\n\n3. def home():\n\n4. return render_template('index.html')\n\nNow, create a view for the predict() function. app.route(‘/predict) will route to the home page URL. The POST method type allows you to send data to a server to update or create a target resource.\n\n5. @app.route('/predict',methods=['POST'])\n\n6. def predict():\n\n7. if request.method == 'POST':\n\n8. # Fetch Value for Gender\n\n9. gender = request.form['gender']\n\n10. if gender == \"Female\":\n\n11. gender = int(0.0)\n\n12. if gender == \"Male\":\n\n13. gender = int(1.0)\n\n14.\n\n15. # Fetch Value for Married\n\n16. married = request.form['married']\n\n17. if married == \"No\":\n\n18. married = int(0.0)\n\n19. if married == \"Yes\":\n\n20. married = int(1.0)\n\n21.\n\n22. # Fetch value for LoanAmount\n\n23. loan_amt = float(request.form['loan_amt'])",
      "content_length": 1126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Build ML Web Apps Using API  149\n\n24.\n\n25. # Fetch value for Total_Income\n\n26. total_income = float(request.form['total_income'])\n\n27.\n\n28. # Fetch value for Prior Credit_Score\n\n29. credit_history = request.form['credit_history']\n\n30. if credit_history == \"No\":\n\n31. credit_history = int(0.0)\n\n32. if credit_history == \"Yes\":\n\n33. credit_history = int(1.0)\n\n34.\n\n35. to_predict_list = [gender, married, total_income, loan_amt, credit_hi story]\n\n36. prediction_array = np.array(to_predict_list, dtype=np. float32).reshape(1, 5)\n\n37.\n\n38. # Making Predictions using the trained model\n\n39. prediction = model.predict(prediction_array)\n\n40. prediction_value = prediction[0]\n\n41. # print(prediction_value )\n\n42.\n\n43.\n\n44. if int(prediction_value) == 1:\n\n45. status=\"Congratulations! your loan approval request is processed\"\n\n46. if int(prediction_value) == 0:\n\n47. status=\"Sorry! your loan approval request is rejected\"\n\n48.\n\n49. return render_template('index.html',prediction = status)",
      "content_length": 982,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "150  Machine Learning in Production\n\nAdd error handling functionality, as follows:\n\n1. @app.errorhandler(500)\n\n2. def internal_error(error):\n\n3. return \"500: Something went wrong\"\n\n4.\n\n5. @app.errorhandler(404)\n\n6. def not_found(error):\n\n7. return \"404: Page not found\",404\n\nFinally, you will define the main function as shown below:\n\n1. if __name__ == '__main__':\n\n2. app.run(host='0.0.0.0', port=port)\n\nGunicorn The Gunicorn is an application server for running a Python app. Gunicorn is WSGI compatible, so it can communicate with multiple WSGI applications. In the current case, Gunicorn translates the request received from Ngnix for the Flask app and vice versa.\n\nTo install Gunicorn, execute the following command: sudo apt-get install gunicorn3\n\nGo to the directory where the app.py file is located and run: gunicorn3 app:app\n\nThis command helps to know which IP and port are being used by Gunicorn. In this case, it is 8000 and IP can be 0.0.0.0 or 127.0.0.1.\n\nNGINX NGINX is a high-performance, highly scalable open-source, and reverse proxy web server. It can perform load balancing and caching application instances. It accepts incoming connections and decides where they should go next. In the current case, it sits on top of a Gunicorn.\n\nTo install NGINX, execute the following command: sudo apt-get install nginx",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Build ML Web Apps Using API  151\n\nYou can check the status of NGINX using the following command:\n\nsudo service nginx status\n\nNow, go to the following path:\n\ncd /etc/nginx/sites-enabled/\n\nNote: NGINX configuration files do not have any extension, and every line should be closed using ; (semicolons).\n\nCreate a new configuration file for the Flask app: sudo nano flask_app\n\nThen, add the following snippet and save the file:\n\n1. server{\n\n2. listen 80;\n\n3. server_name 0.0.0.0;\n\n4.\n\n5. location / {\n\n6. proxy_pass http://unix:/home/suhas/webapi/flask/ flaskapp.sock;\n\n7. }\n\n8. }\n\nHere, you are dictating NGINX to listen to port 80. Inside the location block, pass the request to the socket using proxy_pass. After changing the NGINX file, restart the NGINX service using the following command:\n\nsudo service nginx restart or sudo systemctl restart nginx\n\nYou can check the status of NGINX using the following command:\n\nsudo service nginx status\n\nGo to the directory where the app.py file is located and run the following command to start the Gunicorn server:\n\ngunicorn3 app:app\n\nOpen a new tab in the browser and enter the IP in the address bar. You should see the Flask app up and running.",
      "content_length": 1189,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "152  Machine Learning in Production\n\nCreate and run the service in the background:\n\nCreate a systemd unit file that allows the Ubuntu boot system to start Gunicorn automatically and serve up the Flask application every time the server starts.\n\nFirst, go to the system directory using the following command:\n\ncd /etc/systemd/system\n\nNext, you need to create a service for the Flask app\n\nsudo nano flaskapp.service\n\nThen, add the following commands to it and save it:\n\n1. [Unit]\n\n2. Description=Flaskapp Gunicorn Service\n\n3. After=network.target\n\n4.\n\n5. [Service]\n\n6. User=suhas\n\n7. Group=www-data\n\n8. WorkingDirectory=/home/suhas/webapi/flask\n\n9.\n\n10. ExecStart=/usr/bin/gunicorn3 --workers 3 --bind unix:flaskapp. sock -m 007 app:app\n\n11. Restart=on-failure\n\n12. RestartSec=10\n\n13.\n\n14. [Install]\n\n15. WantedBy=multi-user.target\n\nIn the preceding service, you are instructing Gunicorn to start three worker processes, which can be updated afterward. Then, create and link the UNIX socket file. In the current scenario, 007 is used for access so that the socket file allows access to the owner and group and restricts others. Then, pass the filename of the app.\n\nsudo systemctl daemon-reload\n\nsudo service flaskapp restart",
      "content_length": 1222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "Build ML Web Apps Using API  153\n\nRestart the flaskapp service by running the preceding command, and you should see flaskapp.sock file created in the app directory.\n\nThe following figure shows that the request received from the client goes to NGINX first. Next, it passes to the Gunicorn server; the Gunicorn translates and passes it to the Flask app and then back to the client.\n\nFigure 7.10: Client-server interaction\n\nIf everything goes well, open the browser and enter the IP in the address bar. You should see the app up and running.\n\nThe following figure shows the Loan Prediction app running in the browser. Provide the user data.\n\nFigure 7.11: Passing parameters in the Flask app",
      "content_length": 688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "154  Machine Learning in Production\n\nThe following figure shows the prediction after providing user data and clicking on the Submit button.\n\nFigure 7.12: Prediction outcome\n\nConclusion This chapter demonstrated how to develop an ML-based web application using the most commonly used frameworks, viz FastAPI, Streamlit, and Flask with NGINX and Gunicorn. You created a user-friendly and simple UI to receive input data from users, and you studied the REST API; it’s working between the client and server. This chapter also touched upon the salient features of each framework.\n\nThe next chapter will discuss building native applications for PC and Android devices.\n\nPoints to remember\n\nRESTful API provides a common platform for communicating with an application built in different programming languages.\n\nFastAPI requires Python 3.6 and above. •\t Streamlit is an open-source library in Python that enables users to build and share attractive UI for machine learning models.\n\nFastAPI comes up with a built-in standard and interactive documentation. •\t NGINX configuration files do not have any extension and every line should be closed using a ; (semicolon).",
      "content_length": 1157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "Build ML Web Apps Using API  155\n\nMultiple choice questions\n\n1. How can you verify the installation of streamlit?\n\na) streamlit hello\n\nb) streamlit hello world\n\nc) streamlit run test\n\nd) streamlit --\n\n2. Docs of the FastAPI application can be viewed on a browser using which of the following?\n\na) /docs\n\nb) /redoc\n\nc) Both a and b\n\nd) It is not accessible\n\nAnswers 1. a\n\n2. c\n\nQuestions\n\n1. What is NGINX?\n\n2. What is the role of a Gunicorn?\n\n3. What is the command to run a streamlit app?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "156  Machine Learning in Production",
      "content_length": 36,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "Build Native ML Apps  157\n\nChapter 8 Build Native ML Apps\n\nIntroduction In order to consume ML models as a native application, a GUI should be built for the users. In this chapter, you will learn to build native applications using Tkinter and Kivy packages, which can be converted into desktop applications for Windows and Android devices, respectively.\n\nPython allows multiple ways to build a Graphical User Interface (GUI), which is an application that contains the main window, buttons, widgets, input fields, and the like. End users can interact with it and see the response based on their actions and inputs. Python has numerous GUI frameworks or toolkits available.\n\nStructure This chapter discusses the following topics:\n\n\n\nIntroduction to Tkinter\n\nBuild an ML-based app using Tkinter •\t Convert Python app into Windows EXE file using Pyinstaller • Introduction to kivy",
      "content_length": 877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "158  Machine Learning in Production\n\nBuild an ML-based app using kivy and kivyMD •\t Convert Python app into Android app using Buildozer\n\nObjectives After studying this chapter, you should be able to build and deploy ML-based native apps, such as Windows and Android apps. This chapter is divided into two sections. By the end of the first section, you should be comfortable using Tkinter for developing ML-based Windows apps. You can also convert a Python app to an independent Windows executable file. By the end of the second section, you should be able to develop ML-based kivy applications and convert them into Android apps using the Buildozer package.\n\nIntroduction to Tkinter Tkinter is a free software released under a Python license.\n\nTkinter is the Python interface to the Tk GUI library. Tk is derived from the Tool Command Language (TCL), which is a scripting language. The best part is that you do not need to install Tkinter explicitly as it comes with Python since 1994.\n\nTkinter enables developers to create widgets (GUI elements) easily, which are packaged in the Tk toolkit. With widgets, developers can create buttons, menus, and input fields to get user data. These widgets can be linked to Python features, methods, data, or other widgets.\n\nFor instance, a button widget can be used to perform a certain action upon click event, or it can call the function and pass the user's data as parameters.\n\nNote: Get the latest version of Tkinter by installing Python 3.7 or a later version.\n\nYou can verify the Tkinter installation using the following command: python -m tkinter\n\nA pop-up window will appear, as shown below, which contains TCL/Tk version and text as This should be a cedilla: ç.\n\nFigure 8.1: Tkinter",
      "content_length": 1730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "Build Native ML Apps  159\n\nNote: A cedilla is a symbol that is written under the letter 'ç' in French, Portuguese, and some other languages to show that you pronounce it like a letter 's'.\n\nHello World app using Tkinter Let’s create a basic Tkinter app to give you a glimpse of Tkinter functionality. The following code creates a simple app displaying the Hello World text with the Exit button to close the app.\n\n1. from tkinter import *\n\n2. win = Tk()\n\n3.\n\n4. a = Label(win, text =\"Hello World\")\n\n5. a.pack()\n\n6.\n\n7. b=Button(win, text='Exit', command=tk.destroy)\n\n8. b.pack()\n\n9.\n\n10. win.mainloop()\n\nIn the preceding code, first, the tkinter package is imported to create the main window for the app using the Tk(), where the win is the name of the main window object that has been created using Tk(). A Label() widget is used to display text or images, and the win parameter is used to denote the parent window of the app.\n\nThe pack() organizes the widgets in blocks (such as buttons and labels). If you do not organize the widget, your app will still run, but that widget will be invisible. A Button() widget is used to add buttons, and here it is used to close the Tkinter app. When the user clicks on the Exit button, the Tkinter app window will close. Finally, the mainloop() is to keep the application running and execute user commands.\n\nThe following figure shows the output of the preceding code. It is a simple Tkinter app with a label and a button.\n\nFigure 8.2: Hello World app using Tkinter",
      "content_length": 1505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "160  Machine Learning in Production\n\nHere are some salient features of Tkinter:\n\nTkinter is easy to use and quick to develop desktop applications. •\t The syntax of Tkinter and its widget is simple to understand. •\t Tkinter is shipped with Python, so you do not need to install it explicitly. •\t Tkinter is open-source and free to use.\n\nBuild an ML-based app using Tkinter Here, you need to develop the Python code for data loading, data cleaning, feature engineering, model building and finally, saving trained models to serialized objects like pickles. Then you can use trained models to get predictions on the Tkinter app.\n\nLet’s consider a loan prediction scenario, where the goal is to predict whether a customer’s loan will be approved. The focus will not be on hyperparameter tuning and model optimization. However, you can optimize a model to improve its overall performance. First, create train.py file as follows:\n\n1. # Import the required packages\n\n2. import pandas as pd\n\n3. import numpy as np\n\n4. import warnings\n\n5. warnings.filterwarnings('ignore')\n\n6. from sklearn.ensemble import RandomForestClassifier\n\n7.\n\n8. from sklearn import preprocessing\n\n9. from sklearn.model_selection import train_test_split\n\n10. from sklearn import metrics\n\n11. import pickle\n\nFor the next step, load the datasets and capture numerical and categorical column names in separate variables.\n\n1. # Load the data\n\n2. data = pd.read_csv(\"loan_dataset.csv\")\n\n3. # Missing value treatment (if found)",
      "content_length": 1486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "Build Native ML Apps  161\n\n4. num_col = data.select_dtypes(include=['int64','float64']).columns. tolist()\n\n5. cat_col = data.select_dtypes(include=['object']).columns.tolist()\n\n6. cat_col.remove('Loan_Status')\n\nTo handle missing values, you can replace categorical missing values with mode and numerical missing values with the median.\n\n1. # Missing value treatment (if found)\n\n2. for col in cat_col:\n\n3. data[col].fillna(data[col].mode()[0], inplace=True)\n\n4.\n\n5. for col in num_col:\n\n6. data[col].fillna(data[col].median(), inplace=True)\n\nInstead of removing extreme values (outliers) from numerical data, you can cap them at 5% on the lower side and 95% on the upper side.\n\nIn other words, data points lower than the 5% quantile will be replaced by a value at the 5% quintile, and on the other hand data points higher than the 95% quantile will be replaced by a value at the 95% quintile.\n\n1. # Outlier treatment (if found)\n\n2. data[num_col] = data[num_col].apply(\n\n3. lambda x: x.clip(*x.quantile([0.05, 0.95])))\n\nThere are many instances where a co-applicant income is not available. Create a new feature, TotalIncome which is the sum of applicant income and co-applicant income.\n\n1. # Create a new variable\n\n2. data['TotalIncome'] = data['ApplicantIncome'] + data['Coapplican- tIncome']\n\n3. data = data.drop(['ApplicantIncome','CoapplicantIncome'], axis=1)\n\nYou should avoid passing text data, such as Gender and Married (Marital status), to the model. Convert categorical features to numeric features using the label encoding technique. The target column Loan Status will be encoded into numeric.\n\n1. cat_col.remove('Loan_ID')\n\n2.",
      "content_length": 1638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "162  Machine Learning in Production\n\n3. # Encode categorical features\n\n4. for col in cat_col:\n\n5. le = preprocessing.LabelEncoder()\n\n6. data[col] = le.fit_transform(data[col])\n\n7.\n\n8. data['Loan_Status'] = le.fit_transform(data['Loan_Status'])\n\nNow, you will build an ML model using a random forest classifier and will store the trained models in a pickle object. Here, you can use other ML algorithms and different ML techniques, like GridSearchCV, to improve accuracy. Here, you will build a baseline ML model.\n\n1. # Model building\n\n2. X = data[['Gender', 'Married', 'TotalIncome', 'LoanAmount','Cred- it_History']]\n\n3. y = data['Loan_Status']\n\n4. features = X.columns.tolist()\n\n5.\n\n6. model = RandomForestClassifier(max_depth=4, random_state = 10)\n\n7.\n\n8. model.fit(X, y)\n\n9.\n\n10. # save the model\n\n11. pickle_model = open(\"trained_model/model_rf.pkl\", mode = \"wb\")\n\n12. pickle.dump(model, pickle_model)\n\n13. pickle_model.close()\n\n14.\n\n15. # load the trained model\n\n16. pickle_model = open('trained_model/model_rf.pkl', 'rb')\n\n17. model_rf = pickle.load(pickle_model)\n\n18.\n\n19. prediction = model_rf.predict([[1, 1, 6000, 150, 0]])\n\n20. print(prediction)",
      "content_length": 1158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Build Native ML Apps  163\n\nNext, create a requirements.txt file of dependencies, as follows:\n\n1. pandas==1.1.5\n\n2. pyinstaller==4.1\n\n3. scikit-learn==0.24.0\n\nTkinter app When you complete the preceding part, you should have a pickled object of the trained model, which will be used by the Tkinter app. Here, you will need to import two packages, viz tkinter to build GUI and joblib to load pickled ML model objects.\n\nCreate a Python file and name it ml_app.py.\n\nImport the tkinter module to create a Tkinter desktop application.\n\n1. from tkinter import *\n\n2.\n\n3. import joblib\n\n4.\n\n5. trained_model = 'E:/tkinter_ml_app/trained_model/model.pkl'\n\n6. model = joblib.load(trained_model)\n\nDefine MyWindow class for an ML app. In this class, add Tkinter widgets for GUI and the predict() function for making predictions based on user input. In the following code, labels for input files have been created using the Label() widget.\n\n1. class MyWindow:\n\n2. def __init__(self, win):\n\n3. # Create a text Label\n\n4. self.lbl0=Label(win, text=\"Loan Prediction App\", font=(25))\n\n5. self.lbl0.pack(pady=10)\n\n6. self.lbl1=Label(win, text='Gender')\n\n7. self.lbl2=Label(win, text='Married')\n\n8. self.lbl3=Label(win, text='Total Income')\n\n9. self.lbl4=Label(win, text='Loan Amount')",
      "content_length": 1265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "164  Machine Learning in Production\n\n10. self.lbl5=Label(win, text='Credit History')\n\n11. self.lbl6=Label(win, text='Loan Status')\n\nThe next step is to declare input boxes using the Entry() widget for the preceding fields. In this, declare the optional parameter bd, that is, the border inside the Entry() widget for the input box.\n\nThe insert() widget inserts the text at the given position. 0 (zero) is the first character, so it inserts the default text at the beginning.\n\nThe syntax of the bind() is as follows: widget.bind(event, handler)\n\nIn the current case, an entry widget is linked to a FocusIn event using bind(). It means a specific entry widget is focused, that is, when the cursor is active in a given entry widget, then it should execute the lambda function defined inside the bind(). The lambda function is used to delete the content of the entry widget as soon as the user clicks on it.\n\nCreate Predict button using Button().\n\n1. # Create an entry widget to accept the user input\n\n2. self.t1=Entry(bd=2)\n\n3. self.t1.insert(0, \"0:F, 1:M\")\n\n4. self.t1.bind(\"<FocusIn>\", lambda args: self. t1.delete('0', 'end'))\n\n5.\n\n6. self.t2=Entry(bd=2)\n\n7. self.t2.insert(0, \"0:No, 1:Yes\")\n\n8. self.t2.bind(\"<FocusIn>\", lambda args: self. t2.delete('0', 'end'))\n\n9.\n\n10. self.t3=Entry(bd=2)\n\n11. self.t3.insert(0, \"E.g. 6000\")\n\n12. self.t3.bind(\"<FocusIn>\", lambda args: self. t3.delete('0', 'end'))\n\n13.\n\n14. self.t4=Entry(bd=2)",
      "content_length": 1432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "Build Native ML Apps  165\n\n15. self.t4.insert(0, \"E.g. 150\")\n\n16. self.t4.bind(\"<FocusIn>\", lambda args: self. t4.delete('0', 'end'))\n\n17.\n\n18. self.t5=Entry(bd=2)\n\n19. self.t5.insert(0, \"0:Clear Debts, 1:Unclear Debts\")\n\n20. self.t5.bind(\"<FocusIn>\", lambda args: self. t5.delete('0', 'end'))\n\n21.\n\n22. self.t6=Entry(bd=2)\n\n23.\n\n24. # Create a Predict button\n\n25. self.btn1 = Button(win, text='Predict')\n\nTkinter comes with three geometry managers: grid, place, and pack. A geometry manager’s job is to arrange widgets in specific positions. The place geometry manager gives you more flexibility compared to the other two geometry managers. You can declare the vertical and horizontal position of the widget. The following code mentions the vertical and horizontal positions of each widget, including the Predict button.\n\n1. # Organize widgets appropriately\n\n2. self.lbl1.place(x=100, y=50)\n\n3. self.t1.place(x=200, y=50)\n\n4.\n\n5. self.lbl2.place(x=100, y=100)\n\n6. self.t2.place(x=200, y=100)\n\n7.\n\n8. self.lbl3.place(x=100, y=150)\n\n9. self.t3.place(x=200, y=150)\n\n10.\n\n11. self.lbl4.place(x=100, y=200)\n\n12. self.t4.place(x=200, y=200)",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "166  Machine Learning in Production\n\n13.\n\n14. self.lbl5.place(x=100, y=250)\n\n15. self.t5.place(x=200, y=250, width=165)\n\n16.\n\n17. self.b1=Button(win, text='Predict', command=self. predict, fg='blue')\n\n18. self.b1.place(x=170, y=300)\n\n19.\n\n20. self.lbl6.place(x=100, y=350)\n\n21. self.t6.place(x=200, y=350)\n\nFinally, define the predict() to make the prediction using the ML model based on user inputs.\n\n1. # For making predictions\n\n2. def predict(self):\n\n3. self.t6.delete(0, 'end')\n\n4. gender = float(self.t1.get())\n\n5. married = float(self.t2.get())\n\n6. income = float(self.t3.get())\n\n7. loan_amt = float(self.t4.get())\n\n8. credit_hist = float(self.t5.get())\n\n9. prediction = model. predict([[gender, married, income, loan_amt, credit_hist]])[0]\n\n10. if prediction == 0:\n\n11. pred = 'Rejected'\n\n12. else:\n\n13. pred = 'Approved'\n\n14.\n\n15. self.t6.insert(END, str(pred))",
      "content_length": 870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Build Native ML Apps  167\n\nThe class declaration is done here.\n\nUse the Tk class to create the main window and call the mainloop() to keep the window displayed.\n\nAlso, the application window does not appear on the screen till mainloop() is not called, as it takes all the objects and widgets and renders them on screen.\n\n1. window=Tk()\n\n2.\n\n3. mywin=MyWindow(window)\n\n4.\n\n5. # Create a title\n\n6. window.title('ML App')\n\n7.\n\n8. # Define the size of the window\n\n9. window.geometry(\"400x400+10+10\")\n\n10.\n\n11. window.mainloop() #Keep the window displaying\n\nCreate a ml_app.py file for the Tkinter app and run it. It should make predictions based on the given input.\n\nConvert Python app into Windows EXE file Now, you can convert this Tkinter - ML app to Windows executable application using Pyinstaller, with the following steps. Pyinstaller packages a Python application and its dependencies into a single package that can be run independently without installing a Python interpreter.\n\nLet’s create a virtual environment and install all the required dependencies.\n\nHere, a virtual environment is created with Python version 3.7 using conda, as follows: conda create -n venv_tkinter python=3.7 -y\n\nOnce it is created, activate it using: source activate venv_tkinter\n\nNote: To remove the conda environment, you can execute the following command conda env remove -n venv_tkinter:",
      "content_length": 1374,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "168  Machine Learning in Production\n\npyinstaller --noconfirm --onefile --windowed --icon \"E:/tkinter_ml_app/ python_104451.ico\" --add-data \"E:/tkinter_ml_app/trained_model\n\n/model.pkl;.\" --hidden-import \"sklearn\" --hidden-import \"sklearn. ensemble._forest\" --hidden-import \"sklearn.utils._weight_vector\" --hidden-import \"sklearn. neighbors._quad_tree\" \"E:/tkinter_ml_app/ml_app.py\"\n\n--hidden-import\n\n\"sklearn.neighbors._typedefs\"\n\nThe preceding command will convert the Python-based Tkinter app to a Windows executable file that can be used for prediction.\n\nWhere:\n\n-y, --noconfirm: Will replace output directory without asking for confirmation\n\n-F, --onefile: Will create one file bundle executable •\t -D, --onedir: Will create one folder bundle containing the Windows executable file\n\n--add-data: Additional non-binary files or folders to be added to the executable\n\n\n\nc, --console, --nowindowed: Opens a console window for standard i/o (default)\n\n-w, --windowed, --noconsole: Used to not provide a console window for standard i/o\n\n--hidden-import MODULENAME: Used to name an import not visible in the code of the script(s); this option can be used multiple times\n\nFor more details, refer to:\n\nhttps://pyinstaller.org/en/stable/usage.html\n\nhe following figure shows the status message after executing the preceding command. It states the EXE file creation success message.\n\nFigure 8.3: Converting Python app to Windows app\n\nNow, open the ml_app.exe file located in the dist folder.",
      "content_length": 1484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "Build Native ML Apps  169\n\nThe following figure shows the files and folders of the ML-based Tkinter app:\n\nFigure 8.4: Tkinter ML app\n\nOpen the app; you should see the window of the Tkinter app. It should display the prediction based on user input.\n\nThe following figure displays the prediction based on user data provided:\n\nFigure 8.5: Tkinter ML app",
      "content_length": 351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "170  Machine Learning in Production\n\nBuild an ML-based app using kivy and kivyMD In this section, using the pickle object of the trained model generated earlier in this chapter, let’s develop and deploy the FastAPI endpoint on the remote server, i.e., Heroku. You will learn how to deploy ML models on the Heroku platform with CI/ CD pipeline in the upcoming chapter.\n\nMoreover, you can deploy this FastAPI app using any other hosting service, such as PythonAnywhere or Amazon EC2. In the current scenario, a remote endpoint is used to make predictions in the Kivy app.\n\nSo, whenever users pass the data in the kivy app and press the Predict button, it will send the parameters to the API endpoint, where it will pass the received parameters to the model object and in return, send the prediction to API. The Kivy app will fetch the output (prediction) provided by the API.\n\nLet’s install FastAPI and uvicorn (an ASGI (Asynchronous Server Gateway Interface) server for production) as shown below:\n\npip install fastapi uvicorn\n\nNote: FastAPI requires Python 3.6+\n\nNow, create an app.py file. First, load the dependencies and pickle the object of the trained model. Also, create a FastAPI instance and assign it to the app. So, the app will be the point of interaction while creating API.\n\n1. # Import dependencies\n\n2. from fastapi import FastAPI\n\n3. from pydantic import BaseModel\n\n4. import uvicorn\n\n5. import pickle\n\n6. from fastapi.middleware.cors import CORSMiddleware\n\n7.\n\n8. app = FastAPI()\n\n9.\n\n10. origins = [\"*\"]\n\n11.",
      "content_length": 1526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Build Native ML Apps  171\n\n12. app.add_middleware(\n\n13. CORSMiddleware,\n\n14. allow_origins=origins,\n\n15. allow_credentials=True,\n\n16. allow_methods=[\"*\"],\n\n17. allow_headers=[\"*\"],)\n\n18.\n\n19. # load the trained model\n\n20. trained_model = 'trained_model/model_rf.pkl'\n\n21. # pickle_in = open(trained_model, 'rb')\n\n22. model = pickle.load(open(trained_model, 'rb'))\n\nDefine the class LoanPred, which defines the datatype expected from the client.\n\nThe LoanPred class is used for the data model that is inherited from BaseModel. Add a root view of the function that returns 'message': 'Loan Prediction App' for the home page.\n\n1. class LoanPred(BaseModel):\n\n2. Gender: float\n\n3. Married: float\n\n4. ApplicantIncome: float\n\n5. LoanAmount: float\n\n6. Credit_History: float\n\n7.\n\n8. @app.get('/')\n\n9. def index():\n\n10. return {'message': 'Loan Prediction App'}\n\nThe following function will create a UI for user input. Here, create a /predict as an endpoint, also known as the route. Then, add the parameter of the type data model created, which is LoanPred.",
      "content_length": 1049,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "172  Machine Learning in Production\n\n1. # Defining the function which will make the prediction us- ing the data which the user inputs\n\n2. @app.post('/predict_status')\n\n3. def predict_loan_status(loan_details: LoanPred):\n\n4. data = loan_details.dict()\n\n5. gender = data['Gender']\n\n6. married = data['Married']\n\n7. income = data['ApplicantIncome']\n\n8. loan_amt = data['LoanAmount']\n\n9. credit_hist = data['Credit_History']\n\n10.\n\n11. # Making predictions\n\n12. prediction=model.predict([[gender, married, income, loan_ amt, credit_hist]])\n\n13.\n\n14. if prediction == 0:\n\n15. pred = 'Rejected'\n\n16. else:\n\n17. pred = 'Approved'\n\n18.\n\n19. return {'status':pred}\n\n20.\n\n21. @app.get('/predict')\n\n22. def get_loan_details(gender: float, married: float, income: float,\n\n23. loan_amt: float, credit_hist: float):\n\n24. prediction = model.predict([[gender, married, income, loan_ amt, credit_hist]]).tolist()[0]\n\n25. print(prediction)\n\n26. if prediction == 0:\n\n27. pred = 'Rejected'\n\n28. else:",
      "content_length": 980,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "Build Native ML Apps  173\n\n29. pred = 'Approved'\n\n30.\n\n31. return {'status':pred}\n\n32.\n\n33. if __name__ == '__main__':\n\n34. uvicorn.run(app, host='0.0.0.0', port=4000)\n\nNow, it’s time to run the app and see standard UI auto-generated by FastAPI, which uses swagger, now known as openAPI. uvicorn app:app --reload\n\nThe preceding command can be interpreted as follows:\n\nThe app refers to the name of the file in which the API is created. •\t The app is the instance defined in it or a file. •\t --reload will simply restart the FastAPI server every time changes are made in the app file.\n\nKivyMD app Kivy is an open-source software library for the quick development of apps with a simple GUI. Kivy is written in Python, so in order to use it, you should have Python installed on your system. Unlike Tkinter, it is not pre-installed with Python; you need to install it separately.\n\nKivyMD is an extension of the kivy framework. It is a collection of Material Design (MD) widgets and is mainly used for GUI building along with kivy. It is a good idea to create a virtual environment first, as it helps maintain different packages easily. You can install kivy and KivyMD using the following commands:\n\npip install kivy\n\npip install kivymd\n\nUse the deployed endpoint on the remote server. Keep it simple for now. Import the required dependencies first. Here, import MDApp, which is a base app. Build the app on top of this base app as it takes care of initialization and booting up the app.\n\nThen, import the Builder function for the front end of the app.\n\nNow create a file named main.py and add the following code to it. First, import the required dependencies into it.",
      "content_length": 1664,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "174  Machine Learning in Production\n\n1. from kivymd.app import MDApp\n\n2. from kivy.lang.builder import Builder\n\n3. from kivy.uix.screenmanager import Screen, ScreenManager\n\n4. from kivy.core.window import Window\n\n5. from kivy.network.urlrequest import UrlRequest\n\n6. import certifi as cfi\n\nThe screen is the base, and other components will be placed on top of it. It is similar to the web page. Here, you can assign a name to the screen. As you can see in the following code, a hierarchy is being maintained.\n\nThe screen managers mainly manage the different screens. Let’s understand each component. MDLabel is used to display the text with properties like horizontal alignment, font style, and so on.\n\nMDTextField is used to get user inputs. Here, hint_text is added to guide users while entering the data into the app. It has an id parameter, which will fetch the data entered by the user.\n\nMDButton is used to perform actions. Here, it is being used to call the predict() with the parameter on_press to complete the action.\n\n1. Builder_string = '''\n\n2. ScreenManager:\n\n3. Main:\n\n4. <Main>:\n\n5. name : ‹main›\n\n6. MDLabel:\n\n7. text: ‹Loan Prediction App›\n\n8. halign: ‹center›\n\n9. pos_hint: {‹center_y›:0.9}\n\n10. font_style: ‹H4›\n\n11.\n\n12. MDLabel:\n\n13. text: ‹Gender›\n\n14. pos_hint: {‹center_y›:0.75}",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Build Native ML Apps  175\n\n15.\n\n16. MDTextField:\n\n17. id: input_1\n\n18. hint_text: ‹0:Female, 1:Male'\n\n19. width: 100\n\n20. size_hint_x: None\n\n21. pos_hint: {‹center_y›:0.75, ‹center_x›:0.50}\n\n22.\n\n23. MDLabel:\n\n24. text: ‹Marital Status›\n\n25. pos_hint: {‹center_y›:0.68}\n\n26.\n\n27. MDTextField:\n\n28. id: input_2\n\n29. hint_text: ‹0:No, 1:Yes'\n\n30. width: 100\n\n31. size_hint_x: None\n\n32. pos_hint: {‹center_y›:0.68, ‹center_x›:0.50}\n\n33.\n\n34. MDLabel:\n\n35. text: ‹Applicant Income›\n\n36. pos_hint: {‹center_y›:0.61}\n\n37.\n\n38. MDTextField:\n\n39. id: input_3\n\n40. hint_text: ‹6000›\n\n41. width: 100\n\n42. size_hint_x: None\n\n43. pos_hint: {‹center_y›:0.61, ‹center_x›:0.50}\n\n44.",
      "content_length": 668,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "176  Machine Learning in Production\n\n45. MDLabel:\n\n46. text: ‹Loan Amount›\n\n47. pos_hint: {‹center_y›:0.54}\n\n48.\n\n49. MDTextField:\n\n50. id: input_4\n\n51. hint_text: ‹150›\n\n52. width: 100\n\n53. size_hint_x: None\n\n54. pos_hint: {‹center_y›:0.54, ‹center_x›:0.50}\n\n55.\n\n56. MDLabel:\n\n57. text: ‹Credit History›\n\n58. pos_hint: {‹center_y›:0.47}\n\n59.\n\n60. MDTextField:\n\n61. id: input_5\n\n62. hint_text: ‹0:Clear Debts, 1:Unclear Debts›\n\n63. width: 100\n\n64. size_hint_x: None\n\n65. pos_hint: {‹center_y›:0.47, ‹center_x›:0.50}\n\n66.\n\n67. MDLabel:\n\n68. pos_hint: {‹center_y›:0.2}\n\n69. halign: ‹center›\n\n70. text: ‹›\n\n71. id: output_text\n\n72. theme_text_color: «Custom»\n\n73. text_color: 0, 1, 1, 1\n\n74.",
      "content_length": 690,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "Build Native ML Apps  177\n\n75. MDRaisedButton:\n\n76. pos_hint: {‹center_y›:0.1, ‹center_x›:0.5}\n\n77. text: ‹Predict›\n\n78. on_press: app.predict()\n\n79. '''\n\nFirst store ScreenManager() into the sm object for ease of access, then add the main screen widget. After that, define the MainApp class (import MDApp in it as a base), which holds the core logic of the app. Define the build() for adding text and input widgets. The predict() will get the user data from the id directly as shown in the code.\n\nProvide these input parameters to the remote API endpoint, which will return the prediction for it.\n\nIn kivy, you can use UrlRequest() to parse the URL and the on_success parameter to be triggered on request completion. In the current case, use UrlRequest() to call res() on completion of the API request, which will replace the blank text of MDLabel defined earlier: MDRaisedButton.\n\n1. class Main(Screen):\n\n2. pass\n\n3.\n\n4. sm = ScreenManager()\n\n5. sm.add_widget(Main(name='main'))\n\n6.\n\n7. class MainApp(MDApp):\n\n8. def build(self):\n\n9. self.help_string = Builder.load_string(Builder_string)\n\n10. return self.help_string\n\n11.\n\n12. def predict(self):\n\n13. Gender = self.help_string.get_screen('main').ids.input_1. text\n\n14. Married = self.help_string.get_screen('main').ids. input_2.text\n\n15. ApplicantIncome = self.help_string.get_screen('main').ids.",
      "content_length": 1350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "178  Machine Learning in Production\n\ninput_3.text\n\n16. LoanAmount = self.help_string.get_screen('main').ids. input_4.text\n\n17. Credit_History = self.help_string.get_screen('main').ids. input_5.text\n\n18. url = f'https://fastapi- appl. herokuapp.com/predict?gender={Gender}&married={Married}&income={Ap- plicantIncome}&loan_amt={LoanAmount}&credit_hist={Credit_History}'\n\n19. self.request = UrlRequest(url=url, on_success=self.res, ca_ file=cfi.where(), verify=True)\n\n20.\n\n21. def res(self, *args):\n\n22. self.data = self.request.result\n\n23. ans = self.data\n\n24. self.help_string.get_screen('main').ids.output_text. text = ans['status']\n\n25.\n\n26. MainApp().run()\n\nRun the app using python main.py and if everything goes well, you should see the app up and running.\n\nThe following figure shows the kivy app running successfully.\n\nFigure 8.6: Kivy ML app",
      "content_length": 850,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "Build Native ML Apps  179\n\nConvert the Python app into an Android app Now, you will learn to convert this into an Android app using the Buildozer package. Make sure your kivy filename is the main.py for this step. You need to install the Buildozer package first, if it is not installed already.\n\npip install buildozer\n\nInstall the required dependencies, as follows:\n\npip install cython==0.29.19\n\nsudo apt-get install -y \\\n\npython3-pip \\\n\nbuild-essential \\\n\ngit \\\n\npython3 \\\n\npython3-dev \\\n\nffmpeg \\\n\nlibsdl2-dev \\\n\nlibsdl2-image-dev \\\n\nlibsdl2-mixer-dev \\\n\nlibsdl2-ttf-dev \\\n\nlibportmidi-dev \\\n\nlibswscale-dev \\\n\nlibavformat-dev \\\n\nlibavcodec-dev \\\n\nzlib1g-dev\n\nsudo apt-get install -y \\\n\nlibgstreamer1.0 \\\n\ngstreamer1.0-plugins-base \\\n\ngstreamer1.0-plugins-good\n\nsudo apt-get install build-essential libsqlite3-dev sqlite3 bzip2 libbz2-",
      "content_length": 838,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "180  Machine Learning in Production\n\ndev zlib1g-dev libssl-dev openssl libgdbm-dev libgdbm-compat-dev liblzma- dev libreadline-dev libncursesw5-dev libffi-dev uuid-dev libffi6\n\nsudo apt-get install libffi-dev\n\nYou may need to install additional dependencies as per system requirements.\n\nNow, create the buildozer.spec file by running the following command: buildozer init\n\nThe preceding command will create a standard buildozer.spec file. However, you need to modify it as per requirement. For instance, providing dependencies next to requirements under the Application requirements section, providing package name, title, and domain, providing internet permission under the Permissions section, and so on.\n\nFinally, build and package the Android app using the following command: buildozer -v android debug\n\nOptionally, you can use Google Colab notebook.\n\nThe following figure shows the files and directories for the KivyML app:\n\nFigure 8.7: Kivy ML app\n\nYou have successfully built and run ML-based Tkinter and Kivy apps. Also, you have learned to package them in native applications.\n\nConclusion In this chapter, you learned to develop an ML application using the most open- source frameworks: Tkinter, kivy, and KivyMD. You created a user-friendly yet",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "Build Native ML Apps  181\n\nsimple UI to receive input data from users and display responses. You explored the functionalities of Tkinter and kivy and converted the Tkinter app to a desktop app with Windows executable file using Pyinstaller and the kivyMD app to an Android app using Buildozer.\n\nIn the next chapter, you will learn how to build CI/CD pipelines for ML.\n\nPoints to remember\n\nTkinter comes pre-installed with Python. The best way to get the latest version of Tkinter is to install Python version 3.7 or later.\n\nKivyMD is an extension of the kivy framework. •\t Tkinter and kivy are both open-source and free to use. •\t MDLabel is used to display text in kivyMD.\n\nMultiple choice questions\n\n1.\n\nIn Tkinter, what is/are the geometry manager(s) is/are?\n\na) Grid\n\nb) Place\n\nc) Pack\n\nd) All the above\n\n2. The use of the mainloop()in Tkinter is\n\na) Keep app running\n\nb) Execute any loop defined\n\nc) Both\n\nd) None of them\n\nAnswers 1. a\n\n2. d\n\nQuestions\n\n1. What is Tkinter?\n\n2. What is the role of the pack() in Tkinter?\n\n3. How to define buttons in the kivy app?",
      "content_length": 1069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "182  Machine Learning in Production\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "CI/CD for ML  183\n\nChapter 9 CI/CD for ML\n\nIntroduction Nowadays, automation is all around us. By automating the manual process, you save time, effort, and cost. The best part about automation is that it reduces human errors. You can automate most of the parts while incorporating new changes or updates in the application.\n\nCI/CD pipeline enables you to deploy the updates to the application in an automated way. This chapter will touch upon the elements of the CI/CD pipeline and ways to leverage it for ML applications. In this chapter, you will learn the different stages of the CI/CD pipeline, their importance in MLOps, and how to build one.\n\nStructure This chapter discusses the following topics:\n\nCI/CD pipeline for ML\n\no Continuous Integration (CI)\n\no Continuous Delivery/Deployment (CD)\n\no Continuous Training (CT)\n\n•\t Tools and platforms for building CI/CD pipeline",
      "content_length": 877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "184  Machine Learning in Production\n\n\n\nIntroduction to Jenkins\n\nBuilding CI/CD pipeline using GitHub, Docker, and Jenkins\n\nObjectives After studying this chapter, you should be able to build a CI/CD pipeline to deploy ML models in production, integrate GitHub with Jenkins, run the tests using Jenkins’s job and generate a test summary in Jenkins, build a Docker image, and run a Docker container using Jenkins. Finally, you should be able to integrate Jenkins with an email account to get the status of the build and deployment.\n\nCI/CD pipeline for ML is an acronym for Continuous Integration/Continuous Delivery/ CI/CD Deployment (CI/CD). The purpose of the CI/CD pipeline is to automate the chain of interconnected steps to deploy an application or release a new version of the software.\n\nWhen a new feature gets added to the application, any improvement needs to be integrated with the application. However, it involves different teams that execute multiple tasks and validate them before moving on to the production stage. Mostly, this is a manual and time-consuming process, and it can cause a delay in the release of the new version.\n\nCI/CD pipeline helps in automating the testing, running error-free code for the application, faster deployments, saving time and cost for developers, high reliability, and so on.\n\nCI/CD pipeline enables you to push the changes from development to deployment quickly, which usually consist of four stages:\n\nCommit code changes: After making changes to the file or code, the developer pushes the updates to the source repository. This activity is often performed in a team. CI/CD pipeline enables any team member to check the integrity of the code. Hence, you can automatically push the changes to the repository after it passes the tests.\n\nBuild: In this phase, it fetches the changes from the repository for the build. It keeps a watch on the source repository for any changes. As soon as it detects the changes, it initiates the build process and validates the build results after build completion.\n\nTest: The test phase runs the automated tests, such as unit tests, pytest, and API tests, on top of the build. It is a vital stage of the CI/CD pipeline. This",
      "content_length": 2202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "CI/CD for ML  185\n\nensures the overall integrity of the code and prevents any broken code from passing on to the next phase.\n\nDeploy: This phase deploys the changes to the production environment.\n\nThe following figure shows the different stages of the CI/CD pipeline. However, organizations can modify it as per their goals.\n\nFigure 9.1: CI/CD Pipeline\n\nListing down some popular tools for CI/CD pipeline:\n\n\n\nJenkins\n\nGitHub Actions •\t Bamboo •\t CircleCI •\t GitLab CI/CD •\t Travis CI\n\nContinuous Integration (CI) In Continuous Integration (CI), the team of developers builds, run, and test code in local environments first. If everything goes well then, they push the updates to the repository. After this, the chain of steps starts to run, which involves the build, run, and test stages. The project members get notified at each step and get timely updates, such as the build outcome and test outcomes. Finally, the artifacts get stored and the report of the current status is sent via email or notified via Slack.\n\nWhen a team of developers is working on the same application, they push the code changes to the repository. However, due to changes in the environment, such as the",
      "content_length": 1181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "186  Machine Learning in Production\n\nproduction environment, the code can break or throw an error. On the other hand, there could be a conflict between updates, when multiple developers try to push the updates of the application to the central code repository. This issue is taken care of by the CI stage, where developers can push the changes that will pass through the defined stages, such as build, run, and test, to ensure that the code is working properly without any issues. However, if any of the CI stages fail, then you will be notified, and the further process stops. This way, you can avoid integrating any broken code into production. Developers can frequently push and check the functioning of the code, flow, and integrity of the code or applications before pushing it to the next stage for deployment.\n\nContinuous Delivery/Deployment (CD) CD refers to Continuous Delivery or Continuous Deployment (the terms are used interchangeably) based on the level of automation you are planning to implement. The CD stage depends on the CI stage. Once the CI stage is completed, it triggers the CD stage in the pipeline. The purpose of the Continuous Delivery (CD) stage is to deliver an error-free codebase or artifact to the pre-production environment. In this stage, you can add a series of test cases (wherever required) to ensure a stable build and functional application. It sends the test status report to the team or developer, and then the application is manually pushed to the production environment. If any of the steps fail, you may need to carry out the entire process again with the required updates.\n\nOn the other hand, continuous deployment goes one step further and deploys the application from the pre-production environment to the production environment quickly. This removes the step of manual deployment of an application to the production environment. However, it is optional, as it depends on the developer and operation team to choose the level of automation they want to implement as per the business and application’s nature.\n\nContinuous Training (CT) A new stage introduced in the traditional CI/CD pipeline is Continuous Training (CT). In this stage, you expect the models to be trained continuously as new data comes in or any event occurs, such as the accuracy of the model dropping below the acceptable threshold. This may add slight complexity to CI/CD pipeline, but it is essential for Machine Learning deployment.\n\nWith CI/CD, Continuous Training (CT) is equally important in MLOps. Model retraining depends on scenarios and various other factors, such as how frequently data is changing and the schema of input data. It also depends on events such as accuracy dropping below an acceptable threshold, specific periods such as the end of every week, or manual triggers.",
      "content_length": 2807,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "CI/CD for ML  187\n\nIntroduction to Jenkins Jenkins is an open-source, modular CI/CD automation tool written in Java that comes with several plugins. Jenkins enables the smooth and continuous flow of building, testing, and deploying apps with a recently updated or developed codebase. It has a large community support and is popular among developers.\n\nIf the build is successful, then Jenkins automatically executes a series of steps from the code repository, and if everything goes well, it deploys the application to the server.\n\nHere are some salient features of Jenkins:\n\n\n\nJenkins is an open-source, free, and modular tool.\n\n\n\nIt is created by developers and for developers.\n\n\n\nJenkins is easy to install, configure and can be installed on Linux, MacOS, and Windows.\n\nA large number of Jenkins plugins are available for popular cloud platforms. • Jenkins’s master can distribute the load to multiple slaves and enable faster processing.\n\nInstallation Jenkins can be installed on any server that supports JAVA, as it is written in JAVA. Jenkins installation is described here with various options.\n\nhttps://www.jenkins.io/doc/book/\n\nDocker •\t Kubernetes •\t Linux •\t MacOS •\t WAR files •\t Windows •\t Other systems •\t Offline installations • Initial settings",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "188  Machine Learning in Production\n\nAs you are installing Jenkins on Ubuntu, you can refer to the following link for a step-by-step installation guide.\n\nhttps://www.jenkins.io/doc/book/installing/linux/#debianubuntu\n\nStep 1: Install Jenkins\n\nAdd the repository key:\n\nwget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add –\n\nAppend the Debian package repository address to the server’s sources.list: sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/ apt/sources.list.d/jenkins.list'\n\nExecute the following command so that the apt will use the latest repository: sudo apt update\n\nFinally, install Jenkins with its dependencies:\n\nsudo apt install Jenkins\n\nStep 2: Start Jenkins service\n\nNow, start the Jenkins service using the following command:\n\nsudo systemctl start Jenkins\n\nTo check the status of Jenkins, use the following command:\n\nsudo systemctl status Jenkins\n\nStep 3: Allow Jenkins’s default port in the Firewall By default, Jenkins will run on its default port, that is, 8080. Hence, port 8080 needs to be allowed in the firewall. To do so, run the following command in the terminal:\n\nsudo ufw allow 8080\n\nThen, to confirm it, use the following command:\n\nsudo ufw status\n\nStep 4: Set up Jenkins\n\nYou can use Jenkins’s web-based UI server IP or domain name to access Jenkins: http://<server_ip_or_domain>:8080\n\nInitially, you will get a screen asking for an administrator password; you can get this by using the following command:",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "CI/CD for ML  189\n\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword\n\nCopy the alphanumeric password from the terminal and paste it into the Administrator password field.\n\nIn the next window, you can continue as admin or create the first admin user by filling up the form.\n\nNow, add the Jenkins user to the Docker group using the following command:\n\nsudo usermod -aG docker jenkins\n\nBuild CI/CD pipeline using GitHub, Docker, and Jenkins Here, Jenkins is used for automated ML workflow. First off, you need to create a codebase in the local machine and run the ML app locally to make sure it is working properly on the local machine. Next, you will push the changes to the GitHub repository. Then, integrate the GitHub repo and Jenkins by webhook. Jenkins is to be installed on pre-production or production servers. Jenkins will pull the latest codebase from the linked GitHub repo and deploy the ML app on the server.\n\nDevelop codebase Let’s consider the scenario of loan prediction, where you need to predict whether a customer’s loan will be approved. The focus will not be on hyperparameter tuning and model optimization. However, you can optimize a model to improve its overall performance.\n\nIn this chapter, you will use a machine learning package developed earlier.\n\nYou can access the code repository at the following link:\n\nhttps://github.com/suhas-ds/docker_pkg_jenkins\n\nThe following is the directory structure for codebase:\n\n.\n\n| Dockerfile\n\n| main.py\n\n| requirements.txt\n\n\\---src\n\n| MANIFEST.in\n\n| README.md",
      "content_length": 1526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "190  Machine Learning in Production\n\n| requirements.txt\n\n| setup.py\n\n| tox.ini\n\n+---prediction_model\n\n| | pipeline.py\n\n| | predict.py\n\n| | train_pipeline.py\n\n| | VERSION\n\n| | __init__.py\n\n| +---config\n\n| | config.py\n\n| | __init__.py\n\n| +---datasets\n\n| | test.csv\n\n| | train.csv\n\n| | __init__.py\n\n| +---processing\n\n| | data_management.py\n\n| | preprocessors.py\n\n| | __init__.py\n\n| \\---trained_models\n\n| classification_v1.pkl\n\n| __init__.py\n\n\\---tests\n\npytest.ini\n\ntest_predict.py\n\nmain.py\n\nFastAPI is a web framework for developing RESTful APIs in Python. Create a main. py file to run the FastAPI app.\n\nNote: FastAPI requires Python 3.6+.",
      "content_length": 638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "CI/CD for ML  191\n\n1. # Importing Dependencies\n\n2. from fastapi import FastAPI\n\n3. from pydantic import BaseModel\n\n4. import uvicorn\n\n5. import pickle\n\n6. import numpy as np\n\n7. import pandas as pd\n\n8. from fastapi.middleware.cors import CORSMiddleware\n\n9. from prediction_model.predict import make_prediction\n\n10. import pandas as pd\n\nCreate a FastAPI instance and assign it to the app so that the app will be a point of interaction while creating the API.\n\n1. app = FastAPI(\n\n2. title=\"Loan Prediction Model API\",\n\n3. description=\"A simple API that uses ML model to predict the Loan application status\",\n\n4. version=\"0.1\",\n\n5. )\n\nCORS (Cross-Origin Resource Sharing) refers to the situation when a front end running in a browser has JavaScript code that communicates with a back end, and the back end is of a different origin than the front end. However, it depends on your application and requirement whether to use it or not.\n\n1. origins = [\"*\"]\n\n2.\n\n3. app.add_middleware(\n\n4. CORSMiddleware,\n\n5. allow_origins=origins,\n\n6. allow_credentials=True,\n\n7. allow_methods=[\"*\"],\n\n8. allow_headers=[\"*\"],\n\n9. )",
      "content_length": 1109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "192  Machine Learning in Production\n\nDefine the class LoanPred, which defines the data type expected from the client.\n\nYou can use the LoanPred class for the data model that is inherited from BaseModel. Then, add a root view of the function that returns 'message': 'Loan Prediction App' for the home page.\n\n1. class LoanPred(BaseModel):\n\n2. Gender: str\n\n3. Married: str\n\n4. Dependents: str\n\n5. Education: str\n\n6. Self_Employed: str\n\n7. ApplicantIncome: float\n\n8. CoapplicantIncome: float\n\n9. LoanAmount: float\n\n10. Loan_Amount_Term: float\n\n11. Credit_History: float\n\n12. Property_Area: str\n\n13.\n\n14. @app.get('/')\n\n15. def index():\n\n16. return {'message': 'Loan Prediction App'}\n\nHere, create /predict_status as an endpoint, also known as the route. Then, add predict_loan_status() with a parameter of the type data model that you created as LoanPred.\n\n1. #Defining the function which will make the prediction using the data which the user inputs\n\n2. @app.post('/predict_status')\n\n3. def predict_loan_status(loan_details: LoanPred):\n\n4. data = loan_details.dict()\n\n5. Gender = data['Gender']\n\n6. Married = data['Married']\n\n7. Dependents = data['Dependents']",
      "content_length": 1158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "CI/CD for ML  193\n\n8. Education = data['Education']\n\n9. Self_Employed = data['Self_Employed']\n\n10. ApplicantIncome = data['ApplicantIncome']\n\n11. CoapplicantIncome = data['CoapplicantIncome']\n\n12. LoanAmount = data['LoanAmount']\n\n13. Loan_Amount_Term = data['Loan_Amount_Term']\n\n14. Credit_History = data['Credit_History']\n\n15. Property_Area = data['Property_Area']\n\n16.\n\n17. # Making predictions\n\n18. input_data = [Gender, Married, Dependents, Education,\n\n19. Self_Employed, ApplicantIncome, CoapplicantIncome,\n\n20. LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]\n\n21. cols = ['Gender','Married','Dependents',\n\n22. 'Education','Self_Employed','ApplicantIncome',\n\n23. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n24. 'Credit_History','Property_Area']\n\n25. data_dict = dict(zip(cols,input_data))\n\n26. prediction = make_prediction([data_dict])['prediction'][0]\n\n27.\n\n28. if prediction == 'Y':\n\n29. pred = 'Approved'\n\n30. else:\n\n31. pred = 'Rejected'\n\n32.\n\n33. return {'status':pred}\n\nThe following function will create the UI for user input. Here, create /predict as an endpoint, also known as a route, and declare input data types expected from users.\n\n1. @app.post('/predict')\n\n2. def get_loan_details(Gender: str, Married: str, Dependents: str,",
      "content_length": 1268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "194  Machine Learning in Production\n\n3. Education: str, Self_Employed: str, ApplicantIncome: float,\n\n4. CoapplicantIncome: float, LoanAmount: float, Loan_Amount_Term: float,\n\n5. Credit_History: float, Property_Area: str):\n\n6.\n\n7. input_data = [Gender, Married, Dependents, Education,\n\n8. Self_Employed, ApplicantIncome, CoapplicantIncome,\n\n9. LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]\n\n10. cols = ['Gender','Married','Dependents',\n\n11. 'Education','Self_Employed','ApplicantIncome',\n\n12. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n13. 'Credit_History','Property_Area']\n\n14.\n\n15. data_dict = dict(zip(cols,input_data))\n\n16. prediction = make_prediction([data_dict])['prediction'][0]\n\n17. if prediction == 'Y':\n\n18. pred = 'Approved'\n\n19. else:\n\n20. pred = 'Rejected'\n\n21.\n\n22. return {'status':pred}\n\n23.\n\n24. if __name__ == '__main__':\n\n25. uvicorn.run(app)\n\nThe file for the FastAPI app is completed.\n\nrequirements.txt\n\nNow, create the requirements.txt file, as follows. In this file, model requirements, test requirements, and FastAPI requirements are defined separately for better understanding and ease of management.\n\n1. # Model building requirements\n\n2. joblib==0.16.0",
      "content_length": 1205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "CI/CD for ML  195\n\n3. numpy==1.19.0\n\n4. pandas==1.0.5\n\n5. scikit-learn==0.23.1\n\n6. scipy==1.5.1\n\n7. sklearn==0.0\n\n8.\n\n9. # testing requirements\n\n10. pytest<5.0.0,>=4.6.6\n\n11.\n\n12. # packaging\n\n13. setuptools==40.6.3\n\n14. wheel==0.32.3\n\n15.\n\n16. # FastAPI app requirements\n\n17. fastapi>=0.68.0,<0.69.0\n\n18. pydantic>=1.8.0,<2.0.0\n\n19. uvicorn>=0.15.0,<0.16.0\n\n20. gunicorn>=20.1.0\n\nDockerfile\n\nCreate a Dockerfile and name it Dockerfile without any file extension, and add the following commands to it.\n\n1. FROM python:3.7-slim-buster\n\n2.\n\n3. RUN apt-get update && apt-get install -y \\\n\n4. build-essential \\\n\n5. libpq-dev \\\n\n6. && rm -rf /var/lib/apt/lists/*\n\n7.\n\n8. RUN pip install --upgrade pip\n\n9.",
      "content_length": 700,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "196  Machine Learning in Production\n\n10. COPY . /code\n\n11.\n\n12. RUN chmod +x /code/src\n\n13.\n\n14. RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\n\n15.\n\n16. EXPOSE 8005\n\n17.\n\n18. WORKDIR /code/src\n\n19.\n\n20. ENV PYTHONPATH \"${PYTHONPATH}:/code/src\"\n\n21.\n\n22. CMD pip install -e .\n\nIn the preceding Dockerfile, it will first pull the base image, that is, Python 3.7 slim buster, and install the necessary packages. After that, packages from the requirements file will be installed, and the 8005 port will be exposed so that it can be accessed outside the Docker container. Finally, it will assign PYTHONPATH in the ENV instruction and will execute the command mentioned in the CMD instruction to install the Python package in editable mode.\n\nCreate a Personal Access Token (PAT) on GitHub If you perform any action like pull, push, or clone using the git cli command, then it won’t work anymore with the GitHub password. As a matter of fact, GitHub has removed password authentication. Instead, you have to use a Personal Access Token (PAT).\n\nPATs are an alternative to using passwords for authentication to GitHub Enterprise Server when using the GitHub API or the command line.\n\nFirst, you need to create a PAT. This is described in the following link:\n\nhttps://docs.github.com/en/enterprise-server@3.4/authentication/keeping-your- account-and-data-secure/creating-a-personal-access-token\n\nTo generate or regenerate your PAT, you can go to Settings | Tokens (login required):\n\nhttps://github.com/settings/tokens",
      "content_length": 1529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "CI/CD for ML  197\n\nIf you are not prompted for your username and password, your credentials may be cached in your machine.\n\nCreate a webhook on the GitHub repository A webhook can be considered a lightweight API that enables one-way data sharing after being triggered by certain events. In GitHub, a webhook can be triggered when actions, such as push and pull, are performed on a repository. With the help of the GitHub webhook, you can trigger the CI stage on a remote server.\n\nGo to your GitHub repo | Settings | Webhooks.\n\nAlternatively, you can directly jump to that page using the following URL:\n\nhttps://github.com/<username>/<your-repo-name>/settings/hooks\n\nIn the current case, it is as follows:\n\nhttps://github.com/suhas-ds/docker_pkg_jenkins/settings/hooks\n\nYou cannot use https://localhost:8080/github-webhook/ in GitHub webhook’s payload URL as the localhost URL needs to be exposed to the internet. You can use ngrok for exposing localhost URLs on the internet. Refer to the official site of ngrok https://ngrok.com/ for more details.\n\nHere, ngrok is being used for demonstration purposes; however, the remote server’s IP or domain can be used in the deployment stage.\n\nUse the command ngrok http 8080 to expose the 8080 port publicly.\n\nFigure 9.2: ngrok",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "198  Machine Learning in Production\n\nProvide a payload URL, as shown in the following figure. The payload URL format was discussed earlier in this chapter. Select the Content type as an application/json format. The Secret field is optional. Let’s leave it blank for now.\n\nFigure 9.3: GitHub Webhook\n\nChoose the Enabled SSL verification and Just push the event options, as shown in the following figure, so that it will send data only when someone pushes updates to the repository. However, you can also select the individual events, that is, Let me select individual events.\n\nNow, click on the Add Webhook button to save Jenkins GitHub Webhook.\n\nFigure 9.4: GitHub Webhook",
      "content_length": 673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "CI/CD for ML  199\n\nConfigure Jenkins Now, mainly two sections are to be configured for the current scenario: GitHub webhook integration and extended email integration.\n\n1. Go to Jenkins dashboard.\n\n2. Go to Manage-Jenkins | Manage Plugin.\n\n3.\n\nInstall Git, GitHub, and email plugins without restart.\n\nIf the preceding plugins are already selected, you can continue to the next step.\n\nNow, let’s configure the GitHub webhook in Jenkins.\n\nGo to Manage Jenkins | Configure System.\n\nScroll down and update the API URL in the GitHub section, as follows:\n\nhttps://api.github.com\n\nIn the Credentials section, add GitHub’s PAT generated in the previous step. You can leave the Name field blank.\n\nFinally, click on the Save button.\n\nThe following figure shows GitHub webhook integration in Jenkins:\n\nFigure 9.5: GitHub configuration in Jenkins",
      "content_length": 835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "200  Machine Learning in Production\n\nNow, whenever the developer pushes the updates to the repository, Jenkins will detect the changes and start executing a series of commands one by one.\n\nNext, configure the email for a feedback loop or status updates, for example, succeeded or failed.\n\nGo to Manage Jenkins | Configure System.\n\nScroll down and update the fields in the GitHub section.\n\nIn the following figure, the Default Subject field is updated by adding Status at the beginning. Also, an email body is added in the Default Content field. It is highly customizable, so you can update it as per requirement.\n\nFigure 9.6: Email configuration\n\nIn the Default Triggers section, choose the Always checkbox so that Jenkins sends the status email for both success and failure scenarios, as shown in the following figure:\n\nFigure 9.7: Email configuration - trigger\n\nNow, select HTML (text/html) from the dropdown in the Default Content Type field and add your email ID in Default Recipients, as shown in the following figure:",
      "content_length": 1024,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "CI/CD for ML  201\n\nFigure 9.8: Email configuration – content type and recipients\n\nIn the Extended E-mail Notification section, provide details for the email notifications. In the current scenario, Gmail details are provided, as follows:\n\nsmtp.gmail.com to SMTP server •\t 465 to SMTP Port •\t Add email login credentials •\t Check the Use SSL box •\t Click on the Save button\n\nProvide your email details as shown in the following figure:\n\nFigure 9.9: Extended email notification",
      "content_length": 475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "202  Machine Learning in Production\n\nScroll down and click on the Apply and Save buttons to save the changes, as shown in the following figure:\n\nFigure 9.10: Extended email notification\n\nIf you are integrating your Gmail account for email notifications, you will have to allow less secure apps in the settings of your Gmail account. You can go to the following URL to turn it on if it is off.\n\nhttps://myaccount.google.com/lesssecureapps\n\nFor more details, visit https://support.google.com/accounts/answer/6010255?hl=en.\n\nThe required configuration for Jenkins is complete.\n\nCreate CI/CD pipeline using Jenkins Now, you will build a simple CI/CD pipeline using GitHub and Jenkins. When developers push the updates to the GitHub repository, the GitHub webhook detects the changes and sends the notification to Jenkins. Jenkins pulls the latest code from the GitHub repository, builds the Docker image, and runs the container using that image. Next, it trains the model and exports the pickle object from the trained model. In the next step, pytest results are exported and displayed in Jenkins. After passing all the tests, it will run the ML web app. Finally, it will send the feedback via email to the developer or concerned team.\n\nFigure 9.11: CI/CD pipeline using Jenkins",
      "content_length": 1275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "CI/CD for ML  203\n\nStage 1: 1-GitHub-to-container At this stage, Jenkins will pull the latest code and files from GitHub as soon as the developer pushes the updates to the linked GitHub repository. Let’s understand this process step by step.\n\nWhen the developer pushes the updates to the GitHub repository, the webhook detects the changes, and it gets triggered. GitHub webhook sends a message to Jenkins that new updates have been detected, and then Jenkins pulls the latest code files to start building the Docker image.\n\nAs shown in the following figure, select the New Item from the left panel of the Jenkins dashboard.\n\nFigure 9.12: Jenkins’s dashboard – New Item\n\nAs shown in the following figure, provide a job name in the Enter an item name field and choose a Freestyle project. In the current case, it is named 1-GitHub-to- docker-container.\n\nFigure 9.13: Jenkins’s dashboard – Freestyle project",
      "content_length": 905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "204  Machine Learning in Production\n\nAs shown in the following figure, select the Git radio button under the Source Code Management tab and provide the GitHub repository URL. By default, it will be the master branch, but you can update it. If your repository is private, then you may need to pass the credentials for the same.\n\nFigure 9.14: 1-GitHub-to-container\n\nAs shown in the following figure, choose the GitHub hook trigger for GITScm polling under the Build Triggers tab, to pull the latest updates from the GitHub repository.\n\nFigure 9.15: 1-GitHub-to-container - build triggers\n\nIn the following figure, Docker commands are being executed for building and running Docker images with the latest changes from the GitHub repository.",
      "content_length": 738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "CI/CD for ML  205\n\nFigure 9.16: 1-GitHub-to-container - Build\n\nAs you can see in the following figure, Jenkins started building the Docker image as soon as the updates were pushed to the GitHub repository. The first line of output read Started by GitHub push by suhas-ds; it is showing who pushed the updates to the GitHub repository.\n\nFigure 9.17: 1-GitHub-to-container – Console Output\n\nStage 2: 2-training At this stage, the model will be trained on training data by running the train_pipeline. py file inside the Docker container. As shown in the following figure, select the None",
      "content_length": 585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "206  Machine Learning in Production\n\nradio button under the Source Code Management tab, as it is the next stage; no need to pull the source code again.\n\nFigure 9.18: 2-training – Source code management\n\nAs shown in the following figure, select the Build after other projects are built option under the Build Triggers tab. Next, provide the name of the previous stage in the Projects to watch field. Then, select the first radio button, that is, Trigger only if the build is stable, which means stage 2 will start executing only after the successful completion of the previous stage.\n\nFigure 9.19: 2-training – Build triggers\n\nAs shown in the following figure, write Docker commands for training the model pipeline and exporting the latest pickle object of the trained model, under the Build tab.",
      "content_length": 796,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "CI/CD for ML  207\n\nFigure 9.20: 2-training – Build\n\nThe console output of stage 2 is shown in the following figure. It tells you what caused stage 2 to trigger and the status of stage 2. After that, Jenkins is triggering stage 3 on the successful completion of stage 2.\n\nFigure 9.21: 2-training – Console output\n\nStage 3: 3-testing In this stage, Jenkins is running pytest and publishing test results using JUnit. Jenkins understands the JUnit test report in XML format. JUnit enables Jenkins to provide test results, including historical test result trends, failure tracking (if any), and a neat and clean web UI for viewing test reports. Choose the None radio button under the Source Code Management tab.",
      "content_length": 707,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "208  Machine Learning in Production\n\nAs shown in the following figure, select Build after other projects are built, and Trigger only if build is stable under Build Triggers, which denotes that this stage is the next one and will start execution after completion of the previous stage.\n\nFigure 9.22: 3-testing – Build triggers\n\nAs shown in the following figure, under the Build tab, Jenkins is running tests inside the running Docker container using the Docker command. Next, the Docker command will export the pytest results in a .xml file. Then, the mkdir command will create a new directory as reports and copy test results to it.\n\nFigure 9.23: 3-testing – Build\n\nFinally, it is publishing the test results using the JUnit plugin.\n\nAs shown in the following figure, choose Publish JUnit test results report, under Post-build Actions.",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "CI/CD for ML  209\n\nFigure 9.24: 3-testing – Post-build actions\n\nAs shown in the following figure, provide a path where you want to store the test results. Jenkins will fetch the test results from the mentioned path and publish the test results.\n\nFigure 9.25: 3-testing – JUnit test result report\n\nIn the following figure, you can see the build numbers of Stage 1 and Stage 2. Then, it runs the pytest and exports test results in a .xml file using JUnit.\n\nFigure 9.26: 3-testing – Console output",
      "content_length": 495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "210  Machine Learning in Production\n\nIn the following figure, you can see the test results of the three tests and the time taken for each test.\n\nFigure 9.27: 3-testing – Test results\n\nIn the following figure, you can see the status of test results in a graphical format.\n\nFigure 9.28: 3-testing – Test result trend\n\nStage 4: 4-deployment-status-email At this stage, Jenkins is deploying an ML web app using FastAPI and a trained model.",
      "content_length": 436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "CI/CD for ML  211\n\nAs shown in the following figure, select Build after other projects are built, and Trigger only if build is stable under Build Triggers, which means this stage is the next one and will start execution after completion of the previous stage.\n\nFigure 9.29: 4-deployment-status-email – Build triggers\n\nAs shown in the following figure, select Execute shell from the dropdown Add build step, under the Build tab, as it will execute the commands from the shell.\n\nFigure 9.30: 4-deployment-status-email – Execute shell\n\nIn the following figure, Jenkins is executing a FastAPI command inside a running Docker container named model1, and -d in the following command is to instruct the Docker container to run the command in detached mode. And -w /code means",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "212  Machine Learning in Production\n\nit will change the working directory to /code. Finally, --host and --port are the address and port on which the FastAPI web app will be running, respectively.\n\nFigure 9.31: 4-deployment-status-email – Build\n\nAs shown in the following figure, choose the Extended Email Notification from the Add post-build action dropdown.\n\nFigure 9.32: 4-deployment-status-email – Post build action\n\nAs shown in the following figure, select the Content Type as HTML (text/html) so that you can use HTML tags in the body; however, it is optional.\n\nFigure 9.33: 4-deployment-status-email – Content type",
      "content_length": 621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "CI/CD for ML  213\n\nAs shown in the following figure, select the Always option under the Triggers section. This enables Jenkins to send an email despite the build status (Success or Failure). It will send the email to the mentioned people.\n\nFigure 9.34: 4-deployment-status-email – Triggers\n\nAs shown in the following figure, choose the Compress and Attach Build Log option from the Attach Build Log dropdown. By enabling this option, people will get the console output of the current stage so that people come to know what happened in the job without visiting Jenkin’s job; however, it is optional.\n\nFigure 9.35: 4-deployment-status-email – Attach build logs",
      "content_length": 659,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "214  Machine Learning in Production\n\nAs you can see in the following figure, an email shows the result of stage 4. If the job fails due to any reason, then this email will be triggered because you selected the Always option in the extended email plugin. This email contains the job name, build number, and the status of the job. Here, the Attach Build Log option is not selected.\n\nFigure 9.36: 4-deployment-status-email – Failure email\n\nThe following figure contains the output of stage 4. As you can see, it shows the status of the preceding stages with the build number. In the end, it also shows the process of sending the email.\n\nFigure 9.37: 4-deployment-status-email – Console output",
      "content_length": 690,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "CI/CD for ML  215\n\nAs you can see in the following figure, the received email is the result of the successful completion of stage 4. After completion of the job without any errors, this email will be triggered as the Always option was chosen in the extended email plugin. This email contains the job name, build number, and the status of the job. You can see the build output of the job.\n\nFigure 9.38: 4-deployment-status-email – Success email\n\nThe following figure shows the status of all jobs with the last success, last failure status, and duration to complete the job. You can see this status on the Jenkins dashboard.\n\nFigure 9.39: Jenkins’s dashboard – jobs overview",
      "content_length": 673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "216  Machine Learning in Production\n\nIt’s time to check the deployed loan prediction web app. This web app is running on the 8005 port, so you can access it by the server’s IP address (or domain name), followed by port 8005. When any changes are committed to a linked GitHub repository, Jenkins will trigger the first and subsequent stages one by one, and the latest web app will be deployed again. In short, after pushing the updates to the GitHub repository, you can sit and watch the status of each stage or just wait for the email.\n\nProvide the user data and hit the Execute button, as shown in the following figure.\n\nFigure 9.40: FastAPI – ML app\n\nThe following figure is the result of the preceding action. You can see the status as Approved in the response body. You can also integrate this API endpoint into other applications.",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "CI/CD for ML  217\n\nFigure 9.41: FastAPI - Response\n\nThus, you learned to create a simple CI/CD pipeline using the open-source tool Jenkins. You can modify it as per business and application requirements.\n\nConclusion In this chapter, you explored the process of Continuous Integration (CI), Continuous Delivery (CD), Continuous Deployment (CD), and Continuous Training (CT) in the CI/CD pipeline. Also, you learned to create a simple CI/CD pipeline using the popular open-source tool, Jenkins. Moving on, you integrated GitHub and Jenkins using GitHub webhook, and you built a Docker image and ran the container as Jenkins’s job. You also executed and exported pytest results using the JUnit plugin. In the last stage, an ML web app was deployed on port 8005. Finally, it triggered the email with the status of the build, the job name, and the build logs.\n\nIn the next chapter, you will learn to build CI/CD pipelines that deploy ML apps on the Heroku platform using GitHub Actions.\n\nPoints to remember\n\nCD refers to Continuous Delivery or Continuous Deployment interchangeably, based on the level of automation you are planning to implement.",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "218  Machine Learning in Production\n\n\n\nJenkins is an open-source, modular CI/CD automation tool written in Java, which comes with a large number of plugins.\n\nFastAPI requires Python 3.6 and above. •\n\nJenkins understands the JUnit test report XML format.\n\nMultiple choice questions\n\n1. Which plugin is used to display the test output in Jenkins?\n\na) pytest\n\nb) Selenium\n\nc) JUnit\n\nd) GitHub\n\n2. CD refers to which of the following?\n\na) Continuous Delivery\n\nb) Continuous Deployment\n\nc) Create Dictionary\n\nd) Both a and b\n\nAnswers 1. c\n\n2. d\n\nQuestions\n\n1. What is CT in a CI/CD pipeline?\n\n2. What are popular CI/CD tools/platforms?\n\n3. Which plugin is required to show test results in Jenkins?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 894,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "Deploying ML Models on Heroku  219\n\nChapter 10 Deploying ML Models on Heroku\n\nIntroduction Heroku is a Platform as a Service (PaaS) platform that enables developers to build, run, and operate applications entirely in the cloud. You can push the Docker container to Heroku or provide GitHub repository details, such as the branch name, to auto-deploy the web app as soon as you push new changes to the model. This way, you can make the ML model accessible on the web, ready to make predictions.\n\nIf you are a beginner, then GitHub CI/CD is the simplest platform to build an end- to-end CI/CD pipeline. It is easy to manage as everything is in GitHub. You only need a GitHub repository to create and run a GitHub Actions workflow.\n\nRefer to the previous chapters for the concepts discussed, such as packaging ML models, FastAPI, docker, and CI/CD pipeline.\n\nStructure This chapter discusses the following topics:\n\nCreate a Heroku account and install Heroku CLI •\t Create a Heroku app •\t Deploy the web app to Heroku using Heroku CLI •\t Build CI/CD pipeline using GitHub Actions",
      "content_length": 1076,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "220  Machine Learning in Production\n\nObjectives After studying this chapter, you should be able to deploy the ML model on Heroku - Platform as a Service (PaaS) and integrate the GitHub repository into Heroku for automated deployment. Create the Heroku app from Heroku web UI and the terminal. You will learn to build and run CI/CD pipelines using GitHub Actions and Heroku. You will also learn to execute multiple tests using pytest and tox on GitHub Actions. Create YAML files for GitHub Actions to run the workflow.\n\nHeroku Heroku is a container-based cloud Platform as a Service (PaaS) platform. Developers use Heroku to deploy, manage, and scale modern apps. It has a pretty easy process to deploy your applications.\n\nHeroku saves you time by removing the difficulty of maintaining servers, hardware, or infrastructure. It supports the most popular languages, such as Node, Ruby, Java, Clojure, Scala, Go, Python, and PHP.\n\nSimply put, Heroku allows you to make your apps available on the internet for others to access, like a website. With a few steps, it allows you to make your app accessible to others. You can focus on app development without worrying about infrastructure, servers, and other such things.\n\nIt enables easy integration with GitHub to make it easy to deploy code available on the GitHub repository to apps that are running on Heroku. When GitHub integration is configured for a Heroku app, Heroku can automatically build and release (if the build is successful) the updates when pushed to the specified repository.\n\nHeroku apps can be scaled to run on multiple dynos (a container that runs a Heroku app’s code) simultaneously (except on Free or Hobby dynos) with simple steps to avoid any downtime. Heroku offers manual and auto scaling of the dynos. You can scale it horizontally by adding more dynos to handle the heavy traffic of requests. On the other hand, you can scale it vertically by increasing resources like memory and CPU as required.\n\nHeroku offers Continuous Integration and Continuous Deployment. It has built-in test facilities to achieve Continuous Deployment. Heroku apps that share the same codebase can be organized into deployment pipelines, promoted easily from one stage to the next, and managed through a visual interface.\n\nThere are three methods to deploy the web app on the Heroku platform:\n\nDeployment with Heroku git •\t Deployment with GitHub repository integration",
      "content_length": 2419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "Deploying ML Models on Heroku  221\n\nDeployment with Container Registry\n\nNow that you must have got the gist of it, you are going to deploy a Machine Learning model to Heroku. After studying this chapter, you will be in a better position to work on Heroku for ML deployments.\n\nSetting up Heroku You need to create a Heroku account. This is a one-time activity. Go to the Heroku platform at https://www.heroku.com/\n\nStep 1:\n\nCreate a Heroku account (if you don’t have one) and log in.\n\nSelect the primary development language as Python and create a free account.\n\nStep 2:\n\nAfter logging in to the account, create a new app, as shown in the following figure:\n\nFigure 10.1: Create a new app\n\nClick on the Create new app button and add the app name. In this case, it is docker- ml-cicd.\n\nHit the Create app button and complete the process.\n\nNote: Two or more Heroku apps can’t have the same name.\n\nStep 3:\n\nYou can click the Open app button to see the app. For the current scenario, it is https://docker-ml-cicd.herokuapp.com/\n\nIt will display the following text:\n\nHeroku | Welcome to your new app!\n\nNow, go back to your Heroku app dashboard and install the Heroku Command Line Interface (CLI) for ease of access through the terminal. Heroku's CLI installation steps are mentioned at https://devcenter.heroku.com/articles/heroku-cli. You can",
      "content_length": 1337,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "222  Machine Learning in Production\n\nexecute Heroku commands through the local terminal. At this stage, create one more app for production, and name it docker-ml-cicd-prod.\n\nAt this stage, your Heroku account should be ready with the apps you have created.\n\nDeployment with Heroku Git This method is pretty much straightforward. You need Heroku CLI for this method. Add Heroku’s Git remote repository and push the changes to it directly.\n\nFrom the Heroku application dashboard, go to the Deploy tab. You will see three methods of deployment there:\n\nHeroku Git •\t GitHub •\t Container Registry\n\nThe following figure shows the deployment methods available on the Heroku platform:\n\nFigure 10.2: Deployment method\n\nFrom the preceding methods, you need to select the first method, that is, the Heroku Git method, and follow the steps given below that option.\n\nDeployment with GitHub repository integration Heroku comes with the GitHub integration method, which enables you to automate the deployment process. Post GitHub and Heroku integration, any changes pushed to the repository will trigger the process that deploys the Heroku app at the end.\n\nOn Heroku, you can create a pipeline and add both apps to it. Heroku allows you to create review (temporary) apps before deploying the staging app. This additional option enables you to see the working of the app before pushing it to the staging or pre-deployment stage.",
      "content_length": 1413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "Deploying ML Models on Heroku  223\n\nThe pipeline has been divided into three stages:\n\nREVIEW APPS •\t STAGING •\t PRODUCTION\n\nREVIEW APPS This creates review apps for open pull requests. Each review apps has a unique URL that can be shared for testing purposes. For standardizing it, you can set the pattern of the review app’s URL.\n\nWhen there is an open pull request, it gets created automatically for review purposes, and after the pull request is merged, the review apps gets deleted. It means that the app is no longer accessible.\n\nNote: You should enable both Heroku Pipelines and GitHub integration for the Heroku app to use review apps.\n\nUnder REVIEW APPS, hit the Enable Review Apps button.\n\nAfter that, in the pop-up window, select the checkboxes for the following:\n\nCreate new review apps for new pull requests automatically: When enabled, every new pull request opened will create a review app automatically, and the pull request that is closed will delete that Review app.\n\nDestroy stale review apps automatically: You get the dropdown option for the lifespan of the app, like After 1 day, 2 days, 5 days, or 30 days. The default value is 5 days.\n\nClick on Enable Review Apps.\n\nSTAGING Staging apps can be used to preview code changes and features before being deployed to production, which is known as the pre-production stage. Directly deploying it to the production environment by skipping staging deployment is not recommended. It may happen that an app that is working on your local machine may not work on Heroku (cloud). You might spend a lot of time finding and fixing an issue and lose face in front of customers. Once you verify an app that is working properly in the staging environment, you should push it to the production environment by using the Promote to production button.\n\nUnder STAGING, add the staging app.",
      "content_length": 1839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "224  Machine Learning in Production\n\nPRODUCTION Production apps run your customer-facing code. It is recommended to promote your code from a staging app only after it has been tested.\n\nUnder PRODUCTION, you should create a new app for production.\n\nHeroku Pipeline flow When someone pushes the changes to a branch (other than the master branch), then on the GitHub repository, hit the Compare & pull request button and click on the Create pull request button. You should see a new app getting built (under REVIEW APPS) upon pull request on GitHub. Once the app is ready, you can see that the review app is up and running.\n\nIf everything goes well, the lead developer can go back to GitHub and merge the pull request to the master branch of the GitHub repository. Within a few seconds, the staging app should start building. Once it is deployed, you can open it and verify the staging app. However, if you push the changes directly to the master branch, the staging app will start building, and it will bypass the REVIEW APPS stage.\n\nYou can pass the staging app to the tester or QA team and if everything goes well, one can finally push it to PRODUCTION.\n\nWhen you hit the Promote to production button on Heroku, your production app will be live.\n\nThe following figure shows the workflow of the automated Heroku pipeline when integrated with the GitHub repository on Heroku.\n\nFigure 10.3: Heroku-automated pipeline",
      "content_length": 1414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "Deploying ML Models on Heroku  225\n\nDeployment with Container Registry Heroku Container Registry allows you to deploy your Docker images to Heroku.\n\nMake sure you have a working Docker installation and are logged in to Heroku using the following command:\n\nheroku login\n\nYou’ll be prompted to enter any key to go to your web browser to complete the login process.\n\nA new browser window will open up and will ask you to log in. You may add the -i or --interactive option to stay in the terminal and pass the login details when asked.\n\nMake sure you are in the project directory where Dockerfile is located.\n\nNow, log in to Heroku Container Registry:\n\nheroku container:login\n\nBuild the image and push it to the container registry on Heroku using the following syntax:\n\nheroku container:push <process-type> --app [app name]\n\nIn this case, it is a web process, and the --app parameter holds Heroku’s app name.\n\nFinally, release the image to your app:\n\nheroku container:release <process-type> --app [app name]\n\nRun the following command to see the app running in the browser:\n\nheroku open --app [app name]\n\nNote: Pipeline promotions are not supported in the container registry method.\n\nGitHub Actions GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that helps you automate your development workflows at the same place as your code (that is, GitHub repository) and collaborate on pull requests and issues. You can write individual configurable tasks known as actions and combine them to create a custom workflow. Workflows are custom automated processes within GitHub.",
      "content_length": 1598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "226  Machine Learning in Production\n\nThe following figure shows the workflow created for the current use case:\n\nFigure 10.4: GitHub workflow\n\nUnlike Jenkins, GitHub Actions comes with the runners (GitHub-owned servers available for common OSs like Windows, Linux, and macOS) to build, test and deploy the work. GitHub supports a plethora of languages, such as Node.js, Python, Java, Ruby, PHP, Go, Rust, and .NET.\n\nGitHub Actions automation is managed using workflows. Workflows are nothing but the yml or YAML files you placed in the .github/workflows directory in the same project repository.\n\nGitHub Actions are free to use, but some limits are set for them.\n\nYou can read more about GitHub Actions at https://docs.github.com/en/actions.\n\nConfiguration GitHub Actions need a minimal GitHub configuration. Firstly, go to the GitHub repository where the codebase and required files are available. Next, by default, you will be redirected to the Code tab; switch to the Settings tab, and from the left panel, choose the Secrets option under the Security section. After that, select Actions under the Secrets menu. Finally, use the New repository secret button to add the following secrets:\n\nHEROKU_API_KEY: You can get the Heroku API key from the following path on the Heroku platform.\n\nHeroku Login | Account settings | Account | API Key | Reveal/Regenerate API Key\n\nHEROKU_APP_NAME: This is the Heroku app name. Here, provide the app name that you have created for staging, that is, docker-ml-cicd.\n\nHEROKU_PROD_APP_NAME: This is another app that is to be created for production. In this case, it is docker-ml-cicd-prod.\n\nThe following figure shows the repository secrets. You can either update or remove the secrets, but cannot see them.",
      "content_length": 1741,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "Deploying ML Models on Heroku  227\n\nFigure 10.5: GitHub – Repository secrets\n\nCI/CD pipeline using GitHub Actions and Heroku Let’s consider the scenario of loan prediction, where you need to predict whether a customer’s loan will be approved. The focus will not be on hyperparameter tuning and model optimization. However, you can optimize a model to improve its overall performance.\n\nIn this chapter, the Machine learning package that was developed earlier (refer to Chapter 4: Packaging ML Models) will be used.\n\nFirstly, create a package of ML code and build a web app using FastAPI. After that, create the test cases and dependencies file, along with the Dockerfile and docker- compose.yml file. Finally, create the GitHub workflow files (.yml) for GitHub Actions.\n\nYou can access the code repository at the following link:\n\nhttps://github.com/suhas-ds/heroku-docker-cicd\n\nThe following directory structure shows the CI/CD pipeline files: .\n\n├── .github/\n\n│ └── workflows/\n\n│ ├── production.yml\n\n│ └── workflow.yml\n\n├── src/\n\n│ ├── prediction_model/\n\n│ │ ├── config/\n\n│ │ │ ├── __init__.py\n\n│ │ │ └── config.py",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "228  Machine Learning in Production\n\n│ │ ├── datasets/\n\n│ │ │ ├── __init__.py\n\n│ │ │ ├── test.csv\n\n│ │ │ └── train.csv\n\n│ │ ├── processing/\n\n│ │ │ ├── __init__.py\n\n│ │ │ ├── data_management.py\n\n│ │ │ └── preprocessors.py\n\n│ │ ├── trained_models/\n\n│ │ │ ├── __init__.py\n\n│ │ │ └── classification_v1.pkl\n\n│ │ ├── VERSION\n\n│ │ ├── __init__.py\n\n│ │ ├── pipeline.py\n\n│ │ ├── predict.py\n\n│ │ └── train_pipeline.py\n\n│ ├── tests/\n\n│ │ ├── pytest.ini\n\n│ │ └── test_predict.py\n\n│ ├── MANIFEST.in\n\n│ ├── README.md\n\n│ ├── requirements.txt\n\n│ ├── setup.py\n\n│ └── tox.ini\n\n├── .gitignore\n\n├── Dockerfile\n\n├── README.md\n\n├── docker-compose.yml\n\n├── main.py\n\n├── pytest.ini\n\n├── requirements.txt\n\n├── runtime.txt\n\n├── start.sh\n\n├── test.py\n\n└── tox.ini",
      "content_length": 737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Deploying ML Models on Heroku  229\n\nIn the src directory, files from Chapter 4: Packaging ML Models are being used.\n\n.gitignore\n\nThis file contains files that Git should ignore.\n\n1. venv/\n\n2. __pycache__/\n\n3. .pytest_cache\n\n4. .tox\n\nmain.py\n\nFirst, load the dependencies and modules from prediction_model:\n\n1. # Importing Dependencies\n\n2. from fastapi import FastAPI\n\n3. from pydantic import BaseModel\n\n4. import uvicorn\n\n5. import pickle\n\n6. import os\n\n7. import numpy as np\n\n8. import pandas as pd\n\n9. from fastapi.middleware.cors import CORSMiddleware\n\n10. from prediction_model.predict import make_prediction\n\n11. import pandas as pd\n\nCreate a FastAPI instance and assign it to the app so that the app will be a point of interaction while creating the API.\n\n1. app = FastAPI(\n\n2. title=\"Loan Prediction Model API\",\n\n3. description=\"A simple API that uses ML model to predict the Loan application status\",\n\n4. version=\"0.1\",\n\n5. )\n\nCORS (Cross-Origin Resource Sharing) refers to situations when the front end running on a browser has JavaScript code that communicates with the back end,",
      "content_length": 1090,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "230  Machine Learning in Production\n\nand the back end is of a different origin from the front end. However, it depends on your application and requirement whether to use it.\n\n1. origins = [\n\n2. \"*\"\n\n3. ]\n\n4.\n\n5. app.add_middleware(\n\n6. CORSMiddleware,\n\n7. allow_origins=origins,\n\n8. allow_credentials=True,\n\n9. allow_methods=[\"*\"],\n\n10. allow_headers=[\"*\"],\n\n11. )\n\nDefine the class LoanPred, which defines the data type expected from the client.\n\nThe LoanPred class for the data model is inherited from BaseModel. Then, add a root view with a function, which returns 'message': 'Loan Prediction App' for the home page.\n\n1. class LoanPred(BaseModel):\n\n2. Gender: str\n\n3. Married: str\n\n4. Dependents: str\n\n5. Education: str\n\n6. Self_Employed: str\n\n7. ApplicantIncome: float\n\n8. CoapplicantIncome: float\n\n9. LoanAmount: float\n\n10. Loan_Amount_Term: float\n\n11. Credit_History: float\n\n12. Property_Area: str\n\n13.\n\n14. @app.get('/')",
      "content_length": 928,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "Deploying ML Models on Heroku  231\n\n15. def index():\n\n16. return {'message': 'Loan Prediction App'}\n\n17.\n\n18. @app.get('/health')\n\n19. def healthcheck():\n\n20. return {'status':'ok'}\n\nHere, create /predict_status as an endpoint, also known as a route. Then, add predict_loan_status() with a parameter of the type data model, that is, LoanPred.\n\n1. #Defining the function which will make the prediction using the data which the user inputs\n\n2. @app.post('/predict_status')\n\n3. def predict_loan_status(loan_details: LoanPred):\n\n4. data = loan_details.dict()\n\n5. Gender = data['Gender']\n\n6. Married = data['Married']\n\n7. Dependents = data['Dependents']\n\n8. Education = data['Education']\n\n9. Self_Employed = data['Self_Employed']\n\n10. ApplicantIncome = data['ApplicantIncome']\n\n11. CoapplicantIncome = data['CoapplicantIncome']\n\n12. LoanAmount = data['LoanAmount']\n\n13. Loan_Amount_Term = data['Loan_Amount_Term']\n\n14. Credit_History = data['Credit_History']\n\n15. Property_Area = data['Property_Area']\n\n16.\n\n17. # Making predictions\n\n18. input_data = [Gender, Married, Dependents, Education,\n\n19. Self_Employed, ApplicantIncome, CoapplicantIncome,\n\n20. LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]\n\n21. cols = ['Gender','Married','Dependents',\n\n22. 'Education','Self_Employed','ApplicantIncome',",
      "content_length": 1307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "232  Machine Learning in Production\n\n23. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n24. 'Credit_History','Property_Area']\n\n25. data_dict = dict(zip(cols,input_data))\n\n26. prediction = make_prediction([data_dict])['prediction'][0]\n\n27.\n\n28. if prediction == 'Y':\n\n29. pred = 'Approved'\n\n30. else:\n\n31. pred = 'Rejected'\n\n32.\n\n33. return {'status':pred}\n\nThe following function will create the UI for user input. Here, create /predict as an endpoint, also known as a route, and declare input data types expected from users.\n\n1. @app.post('/predict')\n\n2. def get_loan_details(Gender: str, Married: str, Dependents: str,\n\n3. Education: str, Self_Employed: str, ApplicantIncome: float,\n\n4. CoapplicantIncome: float, LoanAmount: float, Loan_Amount_Term: float,\n\n5. Credit_History: float, Property_Area: str):\n\n6.\n\n7. input_data = [Gender, Married, Dependents, Education,\n\n8. Self_Employed, ApplicantIncome, CoapplicantIncome,\n\n9. LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]\n\n10. cols = ['Gender','Married','Dependents',\n\n11. 'Education','Self_Employed','ApplicantIncome',\n\n12. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n13. 'Credit_History','Property_Area']\n\n14.\n\n15. data_dict = dict(zip(cols,input_data))\n\n16. prediction = make_prediction([data_dict])['prediction'][0]\n\n17. if prediction == 'Y':\n\n18. pred = 'Approved'",
      "content_length": 1354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "Deploying ML Models on Heroku  233\n\n19. else:\n\n20. pred = 'Rejected'\n\n21.\n\n22. return {'status':pred}\n\n23.\n\n24.\n\n25. if __name__ == '__main__':\n\n26. pass\n\nThe file for FastAPI is completed.\n\nrequirements.txt\n\nNow, create the requirements.txt file, as follows. In this file, you can define the model requirements, test requirements, and FastAPI requirements separately for better understanding and ease of management.\n\n1. # Model building requirements\n\n2. joblib==0.16.0\n\n3. numpy==1.19.0\n\n4. pandas==1.0.5\n\n5. scikit-learn==0.23.1\n\n6. scipy==1.5.1\n\n7. sklearn==0.0\n\n8.\n\n9. # testing requirements\n\n10. pytest<5.0.0,>=4.6.6\n\n11. requests\n\n12.\n\n13. # packaging\n\n14. setuptools==40.6.3\n\n15. wheel==0.32.3\n\n16.\n\n17. # FastAPI app requirements\n\n18. fastapi>=0.68.0,<0.69.0",
      "content_length": 767,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "234  Machine Learning in Production\n\n19. pydantic>=1.8.0,<2.0.0\n\n20. uvicorn>=0.15.0,<0.16.0\n\n21. gunicorn>=20.1.0\n\nDockerfile\n\nDockerfile contains a list of commands or instructions to be executed while building the Docker image. Docker uses this file to build the Docker image.\n\n1. FROM python:3.7-slim-buster\n\n2.\n\n3. COPY ./start.sh /start.sh\n\n4.\n\n5. RUN chmod +x /start.sh\n\n6.\n\n7. ENV PYTHONPATH \"${PYTHONPATH}:app/src/\"\n\n8.\n\n9. COPY . /app\n\n10.\n\n11. RUN chmod +x /app\n\n12.\n\n13. RUN pip install --no-cache-dir --upgrade -r app/requirements.txt\n\n14.\n\n15. CMD [\"./start.sh\"]\n\ndocker-compose.yml\n\nThe web service builds from the Dockerfile in the current directory and mounts the app directory on the host to /app inside the container.\n\n1. version: \"3.9\" # optional since v1.27.0\n\n2. services:\n\n3. web:\n\n4. build: .\n\n5. volumes:\n\n6. - ./app:/app\n\npytest.ini",
      "content_length": 859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "Deploying ML Models on Heroku  235\n\nPytest allows you to use a global configuration file, that is, pytest.ini, where you can keep the settings and additional arguments that are to be passed whenever you run the command. Add -p no:warnings to the addopts option, which will suppress the warnings. This section will execute the parameters when pytest runs.\n\n1. [pytest]\n\n2. addopts = -p no:warnings\n\nruntime.txt\n\nDeclare the Python version to be used in this file.\n\n1. python-3.7\n\nstart.sh\n\nThis is a shell script that contains a series of commands to be executed by the bash shell. The first line of this file tells which interpreter should be used to execute this script.\n\nIt will install the prediction_model package from src/ placed in the app/ folder and run the FastAPI app in the next command. Here, it will get the PORT from the Heroku environment, so you do not need to declare it explicitly. Heroku will run the app on any available port. The main is the file name located in the app directory, followed by :app, which is a FastAPI object to be called.\n\n1. #/bin/sh\n\n2. pip install app/src/\n\n3. uvicorn app.main:app --host 0.0.0.0 --port $PORT\n\nMake sure you remove the --reload option if you are using it. The --reload option consumes much more resources; moreover, it is unstable. You can use it during development but should not use it in production.\n\ntest.py\n\nThis file will enable you to test the FastAPI app using TestClient. FastAPI provides the same starlette.testclient as fastapi.testclient; however, it comes directly from Starlette. With this, you can check the app’s routes without running the app explicitly.\n\nThis file will test the root path of the app and sample prediction by passing the data.\n\n1. # Importing dependencies\n\n2. from main import app\n\n3. from fastapi.testclient import TestClient",
      "content_length": 1820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "236  Machine Learning in Production\n\n4. import pytest\n\n5. import requests\n\n6. import json\n\nCreate a TestClient() by passing a FastAPI application to it as an argument:\n\n1. client = TestClient(app)\n\nDefine the functions with a name starting with test_ as per standard pytest conventions:\n\n1. def test_read_main():\n\n2. response = client.get(\"/\")\n\n3. assert response.status_code == 200\n\n4. assert response.json() == {'message': 'Loan Prediction App'}\n\nWrite simple assert statements to check the output:\n\n1. def test_pred():\n\n2. data = {\n\n3. \"Gender\":\"Male\",\n\n4. \"Married\":\"Yes\",\n\n5. \"Dependents\":\"0\",\n\n6. \"Education\":\"Graduate\",\n\n7. \"Self_Employed\":\"No\",\n\n8. \"ApplicantIncome\":5720,\n\n9. \"CoapplicantIncome\":0,\n\n10. \"LoanAmount\":110,\n\n11. \"Loan_Amount_Term\":360,\n\n12. \"Credit_History\":1,\n\n13. \"Property_Area\":\"Urban\"\n\n14. }\n\n15.\n\n16. response = client.post(\"/predict_status\", json=data)\n\n17. assert response.json()[\"status\"] != ''\n\n18. assert response.json() == {\"status\": \"Approved\"}",
      "content_length": 982,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "Deploying ML Models on Heroku  237\n\ntox.ini\n\nThis is the tox configuration file. Tox automates and standardizes the testing in Python. It is a virtualenv management and test command-line tool to check if your package is compatible with different Python versions. Tox will first create a virtual environment based on the configuration provided, install dependencies and finally execute the commands provided in the configuration.\n\ntox-gh-actions is a plugin that enables tox to run on GitHub Actions. Hence, you need to install tox-gh-actions in the GitHub Actions workflow before running the tox command.\n\nThis file aims to check the functioning of the package against Python-3.7. It will install the required dependencies and run the pytest command.\n\n1. [tox]\n\n2. envlist = py37\n\n3. skipsdist=True\n\n4.\n\n5. [gh-actions]\n\n6. python =\n\n7. 3.7: py37\n\n8.\n\n9. [testenv]\n\n10. install_command = pip install {opts} {packages}\n\n11. deps =\n\n12. -r requirements.txt\n\n13.\n\n14. setenv =\n\n15. PYTHONPATH=src/\n\n16.\n\n17. commands=\n\n18. pip install requests\n\n19. pytest -v test.py\n\n20. pytest -v src/tests/\n\nworkflow.yml",
      "content_length": 1104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "238  Machine Learning in Production\n\nGitHub Actions will look for this file for executing the workflow. This workflow is used for staging or pre-production app deployment on Heroku. It gets triggered on a push event, which means any updates pushed to the project’s master branch of the GitHub repository will trigger this workflow to run. Under jobs, you should declare the OS and Python version on which it should run. Then, it will run the pytest using tox. After passing all tests, it will start the deployment on Heroku. For this, it will use the secrets defined in the settings of the GitHub repository.\n\n1. name: Docker-ml-cicd-Heroku\n\n2.\n\n3. on:\n\n4. push:\n\n5. branches:\n\n6. - master\n\n7.\n\n8. jobs:\n\n9. build:\n\n10. runs-on: ubuntu-18.04\n\n11. strategy:\n\n12. matrix:\n\n13. python-version: ['3.7']\n\n14.\n\n15. steps:\n\n16. - uses: actions/checkout@v2\n\n17. - name: Permissions\n\n18. run: chmod +x start.sh\n\n19. - name: Set up Python ${{ matrix.python-version }}\n\n20. uses: actions/setup-python@v2\n\n21. with:\n\n22. python-version: ${{ matrix.python-version }}\n\n23. - name: Install dependencies\n\n24. run: |\n\n25. python -m pip install --upgrade pip",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "Deploying ML Models on Heroku  239\n\n26. python -m pip install tox tox-gh-actions\n\n27. - name: Test with tox\n\n28. run: tox\n\n29. - name: Log in to Heroku Container registry\n\n30. env:\n\n31. HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n\n32. run: heroku container:login\n\n33. - name: Build and push\n\n34. env:\n\n35. HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n\n36. run: heroku container:push web --app ${{ secrets.HEROKU_APP_ NAME }}\n\n37. - name: Release\n\n38. env:\n\n39. HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n\n40. run: heroku container:release web --app ${{ secrets.HEROKU_ APP_NAME }}\n\nproduction.yml\n\nOn successful completion of workflow.yml for staging, production.yml workflow will run. This workflow will simply deploy a production app on Heroku using the latest updates, assuming everything is working fine.\n\n1. name: Production\n\n2.\n\n3. on:\n\n4. workflow_run:\n\n5. workflows: [Docker-ml-cicd-Heroku]\n\n6. types:\n\n7. - completed\n\n8. jobs:\n\n9. build:\n\n10. runs-on: ubuntu-18.04",
      "content_length": 980,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "240  Machine Learning in Production\n\n11. steps:\n\n12. - uses: actions/checkout@v2\n\n13. - name: Log in to Heroku Container registry\n\n14. env:\n\n15. HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n\n16. run: heroku container:login\n\n17. - name: Build and push\n\n18. env:\n\n19. HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n\n20. run: heroku container:push web --app ${{ secrets.HEROKU_PROD_ APP_NAME }}\n\n21. - name: Release\n\n22. env:\n\n23. HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n\n24. run: heroku container:release web --app ${{ secrets.HEROKU_ PROD_APP_NAME }}\n\nOnce all files and codebase are ready, run the app on the local machine. If the app is running on a local machine, then deploy the app to the Heroku container registry. For this, you can refer to the Deployment with Container Registry section discussed earlier in this chapter. If everything goes well, the app should run on the Heroku platform post deployment.\n\nNext, create a repository on GitHub for the current scenario (if not created already) and push code files to it. Go to the Actions tab on GitHub; you will see GitHub has already started executing the workflow.yml file for staging or pre-deployment app.",
      "content_length": 1173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "Deploying ML Models on Heroku  241\n\nAs you can see in the following figure, updates have been pushed with the comment as Staging.\n\nFigure 10.6: GitHub workflow runs\n\nClick on Staging; it will take you to the summary of this workflow. You will see the status of the jobs you have defined in the workflow. Other execution details are mentioned, such as the time taken to complete this workflow, the username means who pushed the updates to the master branch, and the name of the workflow file.",
      "content_length": 492,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "242  Machine Learning in Production\n\nIn the following figure, you can see the summary of the staging stage:\n\nFigure 10.7: GitHub workflow - staging\n\nThe following figure shows the steps executed in the workflow and their output. To see the output of the step, click on the > icon on the left side of the step.",
      "content_length": 310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "Deploying ML Models on Heroku  243\n\nFigure 10.8: GitHub workflow – staging steps\n\nThe following figure shows the output of the tox command, which executed pytest commands using the py37 environment.\n\nFigure 10.9: GitHub workflow – tox",
      "content_length": 235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "244  Machine Learning in Production\n\nThe following figure shows the app running on the Heroku platform on successful completion of workflow.yml workflow.\n\nFigure 10.10: Heroku app – docker-ml-cicd\n\nClick on Production; it will take you to the summary of this workflow. You will see the status of the jobs you have defined in the workflow. Other execution details are mentioned, such as the time taken to complete the workflow, the username that pushed the updates to the master branch, the name of the workflow file, and build no., in this case, it is #2.",
      "content_length": 556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "Deploying ML Models on Heroku  245\n\nIn the following figure, you can see the summary and status of the production stage:\n\nFigure 10.11: GitHub workflow – production steps\n\nThe following figure shows the steps executed in the workflow and their output. To see the output of the step, click on the > icon on the left side of the step.\n\nFigure 10.12: GitHub workflow – production steps",
      "content_length": 383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "246  Machine Learning in Production\n\nThe following figure shows the app running on the Heroku platform on successful completion of the production.yml workflow.\n\nFigure 10.13: Heroku app – docker-ml-cicd-prod\n\nThus, you have now learned to create a simple CI/CD pipeline using GitHub to deploy ML apps on the Heroku platform. Furthermore, you can modify it as per business and application requirements.\n\nConclusion In this chapter, you have learned about the ML app deployment on the Heroku platform (PaaS), built an automated CI/CD pipeline using GitHub Actions, and deployed staging or pre-deployment and production app on the Heroku platform using GitHub CI/CD pipeline. Finally, you integrated tox with GitHub Actions to run the test as a part of the CI/CD pipeline before deploying the staging app to Heroku.\n\nIn the next chapter, you will explore the Azure platform for deploying ML apps.\n\nPoints to remember\n\nGitHub Actions come with runners (Github-owned servers for major OSs such as Windows, Linux, and macOS) to build, test and deploy the workflow.",
      "content_length": 1059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "Deploying ML Models on Heroku  247\n\nThe Heroku app must enable both Heroku Pipelines and GitHub integration to use review apps.\n\nYou can either update or remove the GitHub secrets but cannot see them. •\t GitHub Actions is free to use; however, some limits are set for them.\n\nMultiple choice questions\n\n1. _________ is a plugin that enables tox running on GitHub Actions.\n\na) JUnit\n\nb) tox-gh-actions\n\nc) GitHub’\n\nd) tox-github\n\n2. GitHub Actions automation is managed using _________.\n\na)\n\n.git\n\nb) tox\n\nc) Workflow\n\nd) All the above\n\nAnswers 1. b\n\n2. c\n\nQuestions\n\n1. What is the file format of the GitHub Actions workflow?\n\n2. What are the three methods to deploy the app on the Heroku platform?\n\n3. Explain the working stages of the Heroku pipeline.",
      "content_length": 753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "248  Machine Learning in Production\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "Deploying ML Models on Microsoft Azure  249\n\nChapter 11 Deploying ML Models on Microsoft Azure\n\nIntroduction Microsoft Azure is a popular cloud platform among developers, as it comes with a wide variety of services that help developers and organizations to deliver quality solutions to customers with less effort and less time in a secure environment.\n\nIn this chapter, you will be acquainted with MLaaS, that is, Machine Learning as a Service, offered by Microsoft Azure. This chapter is mainly divided into two parts. In the first part, GitHub Actions and Azure web app containers will be used in the CI/CD pipeline to deploy the ML app on the Azure cloud. In the second part, Azure DevOps and Azure Machine Learning (AML) service will be used to deploy ML apps on the Azure cloud.\n\nStructure This chapter discusses the following topics:\n\nCreate an Azure account and install Azure CLI •\t Run the ML app using the Docker container locally •\t Deploy the app to Azure web service using Azure container",
      "content_length": 1001,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "250  Machine Learning in Production\n\nBuild a CI/CD pipeline using GitHub Actions •\t Azure Machine Learning (AML) service •\t Build a CI/CD pipeline using Azure Machine Learning (AML) service and Azure DevOps\n\nObjectives After studying this chapter, you should be able to deploy ML models on Platform as a Service (PaaS) and ML as a Service (MLaaS). You should also be able to integrate the GitHub repository to Azure for automated deployment of the app. You should know how to create a web app and container for it. Additionally, you will be familiar with how to build and run CI/CD pipelines using GitHub Actions and Azure and execute multiple test cases using pytest and tox on GitHub Actions. Further on in the chapter, you will learn how to create YAML files for GitHub Actions to run the workflow. By the end of the chapter, you should also be able to build and run CI/CD pipelines using Azure DevOps.\n\nAzure Microsoft Azure (also known as Azure) is a cloud computing service offered by Microsoft. Azure provides different forms of cloud computing options, such as Software as a Service (SaaS), Platform as a Service (PaaS), ML as a Service (MLaaS), and Infrastructure as a Service (IaaS), for deploying applications and services on Azure.\n\nMicrosoft first introduced its cloud computing services as Windows Azure in 2008, but it was commercially launched in 2010. Later, in 2014, they expanded their services and re-launched it as Microsoft Azure.\n\nAzure is a ready-to-go, resourceful, flexible, and fast yet economical cloud platform. It comes with 200+ products and cloud services; however, running Virtual Machines (VM) or containers is popular among developers. It is compatible with open-source technologies like Docker, Jenkins, and Kubernetes. Azure has a built-in Continuous Integration and Continuous Deployment pipeline.\n\nAzure enables integration with GitHub to make it easy to deploy code available on the GitHub repository to apps running on Azure. When GitHub integration is configured for an Azure app, Azure can automatically build and release (if the stage is completed successfully) and push it to Azure.\n\nAzure App Service can be scaled using Azure Kubernetes Service (AKS). You can choose different tiers that come with a set of computational power and set the auto scale limit. When required, it will make replicas of the service to serve the requests seamlessly.",
      "content_length": 2390,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "Deploying ML Models on Microsoft Azure  251\n\nThere are numerous ways to deploy the app on the Azure platform, but you will explore two methods in this chapter:\n\nDeployment using GitHub Actions •\t Deployment using Azure DevOps and Azure ML\n\nAzure primarily uses a pay-as-you-go pricing plan, which allows you to pay only for the services you have used. If any application uses multiple services, then each service will be billed based on the plan/tier obtained for it. Microsoft offers a discounted rate if the user or organization is looking for a long-term commitment.\n\nAzure is a paid service, but it allows new users to explore its functionality and services for free for a limited duration using free credits. After that, you would have to activate a pay-as-you-go pricing plan.\n\nSet up an Azure account You need to set up an Azure account. This is a one-time activity, and once it is done, it is ready for use. Go to the Azure platform https://azure.microsoft.com/.\n\nCreate an Azure account (if you don’t have one), and log in.\n\nNote: You can explore Azure for free; however, you need to provide credit/debit card details. Azure will send a notification if any payment is to be made. Also, you will get access to popular services for free for 12 months, with an additional $200 credit for 30 days.\n\nAzure DevOps will be used in this chapter. For this, you need to log in to Azure DevOps.\n\nhttps://azure.microsoft.com/en-us/services/devops/\n\nTo connect and communicate with Azure, the Azure command-line tool is required, that is, azure-cli and azureml-sdk.\n\nInstall Azure CLI with your local terminal:\n\npip install azure-cli==2.37.0\n\npip install --upgrade azureml-sdk[cli]\n\nYou can create a resource group. It is a container that holds related resources for Azure projects. You can allocate the resources to this resource group as per requirement.\n\nDeployment using GitHub Actions In this part, an ML app will be deployed in Azure web service using Docker. GitHub Actions will be used for building, testing, and deploying ML apps to Azure. First",
      "content_length": 2051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "252  Machine Learning in Production\n\noff, GitHub Actions will build and push a Docker image to the Azure Container Registry (ACR) and pull it into the Azure App Service.\n\nFirstly, prepare the required codebase for this implementation. Next, create an ACR, and then build and push the Docker image to the ACR. In the next step, create a web app in the Azure App Service, which will pull container images from the ACR to deploy a web-based ML app.\n\nThe following figure shows the workflow of deployment on Azure App Service (web app) using GitHub Actions:\n\nFigure 11.1: GitHub Actions and web app workflow\n\nYou can access the code at the following GitHub repository:\n\nhttps://github.com/suhas-ds/mlapp-cd\n\nThe following directory structure shows the files that will be used for the CI/CD pipeline: .\n\n├─ .github\n\n│ └─ workflows\n\n│ └─ prod.workflow.yml\n\n├─ src\n\n│ ├─ prediction_model\n\n│ │ ├─ config\n\n│ │ │ ├─ __init__.py\n\n│ │ │ └─ config.py\n\n│ │ ├─ datasets\n\n│ │ │ ├─ __init__.py",
      "content_length": 977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "Deploying ML Models on Microsoft Azure  253\n\n│ │ │ ├─ test.csv\n\n│ │ │ └─ train.csv\n\n│ │ ├─ processing\n\n│ │ │ ├─ __init__.py\n\n│ │ │ ├─ data_management.py\n\n│ │ │ └─ preprocessors.py\n\n│ │ ├─ trained_models\n\n│ │ │ ├─ __init__.py\n\n│ │ │ └─ classification_v1.pkl\n\n│ │ ├─ VERSION\n\n│ │ ├─ __init__.py\n\n│ │ ├─ pipeline.py\n\n│ │ ├─ predict.py\n\n│ │ └─ train_pipeline.py\n\n│ ├─ tests\n\n│ │ ├─ pytest.ini\n\n│ │ └─ test_predict.py\n\n│ ├─ MANIFEST.in\n\n│ ├─ README.md\n\n│ ├─ requirements.txt\n\n│ ├─ setup.py\n\n│ └─ tox.ini\n\n├─ Dockerfile\n\n├─ docker-compose.yml\n\n├─ main.py\n\n├─ pytest.ini\n\n├─ requirements.txt\n\n├─ runtime.txt\n\n├─ start.sh\n\n├─ test.py\n\n└─ tox.ini\n\nLet’s discuss the code and the concepts from the preceding files. In the src directory, files from Chapter 4: Packaging ML Models will be used.\n\n.gitignore\n\nThis file contains the names of files and directories that Git should ignore.",
      "content_length": 874,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "254  Machine Learning in Production\n\n1. venv/\n\n2. __pycache__/\n\n3. .pytest_cache\n\n4. .tox\n\nmain.py\n\nThis file will load the pickle object of the model and run a FastAPI app. First, load the dependencies and modules from prediction_model:\n\n1. # Importing Dependencies\n\n2. from fastapi import FastAPI\n\n3. from pydantic import BaseModel\n\n4. import uvicorn\n\n5. import pickle\n\n6. import os\n\n7. import numpy as np\n\n8. import pandas as pd\n\n9. from fastapi.middleware.cors import CORSMiddleware\n\n10. from prediction_model.predict import make_prediction\n\n11. import pandas as pd\n\nCreate a FastAPI instance and assign it to the app so that the app becomes a point of interaction while creating the API.\n\n1. app = FastAPI(\n\n2. title=\"Loan Prediction Model API\",\n\n3. description=\"A simple API that uses ML model to predict the Loan application status\",\n\n4. version=\"0.1\",\n\n5. )\n\nCORS (Cross-Origin Resource Sharing) refers to the situations when a front end running in a browser has JavaScript code that communicates with a back end, and the back end is of a different origin than the front end. However, it depends on your application and requirement whether to use it.\n\n1. origins = [",
      "content_length": 1175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "Deploying ML Models on Microsoft Azure  255\n\n2. \"*\"\n\n3. ]\n\n4.\n\n5. app.add_middleware(\n\n6. CORSMiddleware,\n\n7. allow_origins=origins,\n\n8. allow_credentials=True,\n\n9. allow_methods=[\"*\"],\n\n10. allow_headers=[\"*\"],\n\n11. )\n\nDefine the class LoanPred, which defines the data type expected from the client.\n\nThe LoanPred class for the data model is inherited from BaseModel. Then, add a root view with a function, which returns 'message': 'Loan Prediction App' for the home page.\n\n1. class LoanPred(BaseModel):\n\n2. Gender: str\n\n3. Married: str\n\n4. Dependents: str\n\n5. Education: str\n\n6. Self_Employed: str\n\n7. ApplicantIncome: float\n\n8. CoapplicantIncome: float\n\n9. LoanAmount: float\n\n10. Loan_Amount_Term: float\n\n11. Credit_History: float\n\n12. Property_Area: str\n\n13.\n\n14. @app.get('/')\n\n15. def index():\n\n16. return {'message': 'Loan Prediction App'}\n\n17.",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "256  Machine Learning in Production\n\n18. @app.get('/health')\n\n19. def healthcheck():\n\n20. return {'status':'ok'}\n\nHere, create /predict_status as an endpoint, also known as the route. Then, add predict_loan_status() with a parameter of the type data model, that is, LoanPred.\n\n1. #Defining the function which will make the prediction using the data which the user inputs\n\n2. @app.post('/predict_status')\n\n3. def predict_loan_status(loan_details: LoanPred):\n\n4. data = loan_details.dict()\n\n5. Gender = data['Gender']\n\n6. Married = data['Married']\n\n7. Dependents = data['Dependents']\n\n8. Education = data['Education']\n\n9. Self_Employed = data['Self_Employed']\n\n10. ApplicantIncome = data['ApplicantIncome']\n\n11. CoapplicantIncome = data['CoapplicantIncome']\n\n12. LoanAmount = data['LoanAmount']\n\n13. Loan_Amount_Term = data['Loan_Amount_Term']\n\n14. Credit_History = data['Credit_History']\n\n15. Property_Area = data['Property_Area']\n\n16.\n\n17. # Making predictions\n\n18. input_data = [Gender, Married, Dependents, Education,\n\n19. Self_Employed, ApplicantIncome, CoapplicantIncome,\n\n20. LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]\n\n21. cols = ['Gender','Married','Dependents',\n\n22. 'Education','Self_Employed','ApplicantIncome',\n\n23. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n24. 'Credit_History','Property_Area']",
      "content_length": 1337,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "Deploying ML Models on Microsoft Azure  257\n\n25. data_dict = dict(zip(cols,input_data))\n\n26. prediction = make_prediction([data_dict])['prediction'][0]\n\n27.\n\n28. if prediction == 'Y':\n\n29. pred = 'Approved'\n\n30. else:\n\n31. pred = 'Rejected'\n\n32.\n\n33. return {'status':pred}\n\nThe following function will create the UI for user input. Here, create /predict as an endpoint, also known as a route, and declare the input data types expected from users.\n\n1. @app.post('/predict')\n\n2. def get_loan_details(Gender: str, Married: str, Dependents: str,\n\n3. Education: str, Self_Employed: str, ApplicantIncome: float,\n\n4. CoapplicantIncome: float, LoanAmount: float, Loan_Amount_Term: float,\n\n5. Credit_History: float, Property_Area: str):\n\n6.\n\n7. input_data = [Gender, Married, Dependents, Education,\n\n8. Self_Employed, ApplicantIncome, CoapplicantIncome,\n\n9. LoanAmount, Loan_Amount_Term, Credit_History, Property_Area]\n\n10. cols = ['Gender','Married','Dependents',\n\n11. 'Education','Self_Employed','ApplicantIncome',\n\n12. 'CoapplicantIncome','LoanAmount','Loan_Amount_Term',\n\n13. 'Credit_History','Property_Area']\n\n14.\n\n15. data_dict = dict(zip(cols,input_data))\n\n16. prediction = make_prediction([data_dict])['prediction'][0]\n\n17. if prediction == 'Y':\n\n18. pred = 'Approved'",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "258  Machine Learning in Production\n\n19. else:\n\n20. pred = 'Rejected'\n\n21.\n\n22. return {'status':pred}\n\n23.\n\n24. if __name__ == '__main__':\n\n25. uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=port, reload=False)\n\nThe FastAPI file is completed.\n\nrequirements.txt\n\nNow, let’s create a requirements.txt file, as follows. In this file, model requirements, test requirements, and FastAPI requirements are defined separately for better understanding and ease of management.\n\n1. # Model building requirements\n\n2. joblib==0.16.0\n\n3. numpy==1.19.0\n\n4. pandas==1.0.5\n\n5. scikit-learn==0.23.1\n\n6. scipy==1.5.1\n\n7. sklearn==0.0\n\n8.\n\n9. # testing requirements\n\n10. pytest<5.0.0,>=4.6.6\n\n11. requests\n\n12.\n\n13. # packaging\n\n14. setuptools==40.6.3\n\n15. wheel==0.32.3\n\n16.\n\n17. # FastAPI app requirements\n\n18. fastapi>=0.68.0,<0.69.0",
      "content_length": 816,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "Deploying ML Models on Microsoft Azure  259\n\n19. pydantic>=1.8.0,<2.0.0\n\n20. uvicorn>=0.15.0,<0.16.0\n\n21. gunicorn>=20.1.0\n\nDockerfile\n\nDockerfile contains a list of commands or instructions to be executed while building the Docker image.\n\n1. FROM python:3.7-slim-buster\n\n2.\n\n3. COPY ./start.sh /start.sh\n\n4.\n\n5. RUN chmod +x /start.sh\n\n6.\n\n7. ENV PYTHONPATH \"${PYTHONPATH}:app/src/\"\n\n8.\n\n9. COPY . /app\n\n10.\n\n11. RUN chmod +x /app\n\n12.\n\n13. # expose the port that uvicorn will run the app on\n\n14. ENV PORT=8000\n\n15. EXPOSE 8000\n\n16.\n\n17. RUN pip install --no-cache-dir --upgrade -r app/requirements.txt\n\n18.\n\n19. CMD [\"./start.sh\"]\n\ndocker-compose.yml\n\nThis builds the web service from the Dockerfile in the current directory and mounts the app directory on the host to /app inside the container.\n\n1. version: \"3.9\" # optional since v1.27.0\n\n2. services:",
      "content_length": 856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "260  Machine Learning in Production\n\n3. web:\n\n4. build: .\n\n5. volumes:\n\n6. - ./app:/app\n\npytest.ini\n\nPytest allows you to use a global configuration file, that is, pytest.ini, where you can keep settings and additional arguments that you pass whenever you run the command. You can add -p no:warnings to addopts field, which will suppress the warnings. This section will execute the parameters when pytest runs.\n\n1. [pytest]\n\n2. addopts = -p no:warnings\n\nruntime.txt\n\nThis file is optional. In this file, you can declare the Python version to be used.\n\n1. python-3.7\n\nstart.sh\n\nThis is a shell script that contains a series of commands to be executed by the bash shell. The first line of this file dictates which interpreter should be used to execute this script.\n\nIt will install the prediction_model package from src/ placed in app/ and run the FastAPI app in the next command. The main.py is the file name located in the app directory.\n\n1. #!/bin/bash\n\n2. pip install app/src/\n\n3. python app/main.py\n\nMake sure you remove the --reload option if you are using it. The --reload option consumes much more resources, and it is also unstable. You can use it during development but should not use it in production.\n\ntest.py\n\nThis test file will enable testing the FastAPI app using testclient. FastAPI provides the same starlette.testclient as fastapi.testclient; however, it comes directly from Starlette. With this, you can check the app’s routes without running them from the app explicitly.",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "Deploying ML Models on Microsoft Azure  261\n\nThis file will test the root path of the app and model predictions by passing the data.\n\n1. # Importing dependencies\n\n2. from main import app\n\n3. from fastapi.testclient import TestClient\n\n4. import pytest\n\n5. import requests\n\n6. import json\n\nCreate a TestClient() and pass a FastAPI application to it.\n\n1. client = TestClient(app)\n\nDefine the test functions with a name starting with test_ as per standard naming conventions of pytest:\n\n1. def test_read_main():\n\n2. response = client.get(\"/\")\n\n3. assert response.status_code == 200\n\n4. assert response.json() == {'message': 'Loan Prediction App'}\n\nWrite simple assert statements in the test function to check the output.\n\n1. def test_pred():\n\n2. data = {\n\n3. \"Gender\":\"Male\",\n\n4. \"Married\":\"Yes\",\n\n5. \"Dependents\":\"0\",\n\n6. \"Education\":\"Graduate\",\n\n7. \"Self_Employed\":\"No\",\n\n8. \"ApplicantIncome\":5720,\n\n9. \"CoapplicantIncome\":0,\n\n10. \"LoanAmount\":110,\n\n11. \"Loan_Amount_Term\":360,\n\n12. \"Credit_History\":1,\n\n13. \"Property_Area\":\"Urban\"\n\n14. }",
      "content_length": 1037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "262  Machine Learning in Production\n\n15.\n\n16. response = client.post(\"/predict_status\", json=data)\n\n17. assert response.json()[\"status\"] != ''\n\n18. assert response.json() == {\"status\": \"Approved\"}\n\ntox.ini\n\nThis is the tox configuration file. The tox automates and standardizes the testing in Python. It is a virtualenv management and test command-line tool to check whether your package is compatible with different Python versions. The tox will first create a virtual environment based on the configuration provided and then install dependencies. Finally, it will execute the commands provided in the configuration.\n\ntox-gh-actions is a plugin that enables tox to run on GitHub Actions. Hence, you need to install tox-gh-actions in the GitHub Actions workflow before running the tox command.\n\nThis file will check the package compatibility with Python-3.7. First, it will install the required dependencies and then run the pytest commands.\n\n1. [tox]\n\n2. envlist = py37\n\n3. skipsdist=True\n\n4.\n\n5. [gh-actions]\n\n6. python =\n\n7. 3.7: py37\n\n8.\n\n9. [testenv]\n\n10. install_command = pip install {opts} {packages}\n\n11. deps =",
      "content_length": 1121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "Deploying ML Models on Microsoft Azure  263\n\n12. -r requirements.txt\n\n13.\n\n14. setenv =\n\n15. PYTHONPATH=src/\n\n16.\n\n17. commands=\n\n18. pip install requests\n\n19. pytest -v test.py\n\n20. pytest -v src/tests/\n\nOnce all files and codebase are ready, run the app on the local machine. If the app is running on a local machine, then deploy the app to the Azure Container Registry.\n\nBuild a Docker image on a local machine using the following command: sudo docker build . -t mlappcd.azurecr.io/mlapp-cd:v1\n\nThen, run the Docker image using the following command: sudo docker push mlappcd.azurecr.io/mlapp-cd:v1\n\nAt this point, you should see an ML app up and running on the local machine.\n\nInfrastructure setup First off, set up infrastructure on Azure cloud. In this section, you will learn to set up an Azure Container Registry (ACR) and create a web app container using the Azure App Service.\n\nAzure Container Registry To begin with, push the Docker image to the Azure Container Registry from the local machine, and from the next time onward, GitHub Actions will push the image. This Docker image will be pulled by the Azure App Service to run a web app container.",
      "content_length": 1159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "264  Machine Learning in Production\n\nCreate an Azure Container Registry, as shown in the following figure:\n\nFigure 11.2: Azure Container Registry\n\nAfter providing the values to fields, complete the process by clicking on the Review + create button.\n\nAfter completing the preceding process, you can check the container registry to see the status. The following figure shows the status of the container registry as OK:",
      "content_length": 417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "Deploying ML Models on Microsoft Azure  265\n\nFigure 11.3: Deployment status of Azure Container Registry\n\nNext, enable the Admin user option in the Access keys. The following figure shows that admin user access is enabled under Settings:\n\nFigure 11.4: Access keys of container registry",
      "content_length": 285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "266  Machine Learning in Production\n\nAfter completing the preceding process, go back to your local terminal. Log in to Azure using Azure CLI, as follows: az login\n\nAfter executing the preceding command in the local terminal, a browser window will open where you need to log in. Then, you can close the browser window and continue in the terminal. The following figure shows the output of the preceding command:\n\nFigure 11.5: Azure login\n\nAfter that, log in to the Azure Container Registry using the following command in the terminal:\n\nsudo az acr login --name mlappcd\n\nThe following figure shows the execution of the preceding command in the terminal:\n\nFigure 11.6: Azure Container Registry login\n\nFirst, build the Docker image in the local machine: sudo docker build . -t mlappcd.azurecr.io/mlapp-cd:v1\n\nIn the following figure, you can see the image has been built and tagged successfully:\n\nFigure 11.7: Docker image built and tagged in local machine\n\nNow, push that image to Azure container registry: sudo docker push mlappcd.azurecr.io/mlapp-cd:v1\n\nAs shown in the following figure, the image has been pushed to the Azure Container Registry:",
      "content_length": 1146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "Deploying ML Models on Microsoft Azure  267\n\nFigure 11.8: Docker image pushed to ACR\n\nAs shown in the following figure, verify that the image (which is pushed to the repo) is available in the Azure Container Registry. In this case, you can see that the image mlapp-cd is available in the repo.\n\nFigure 11.9: Azure Container Registry\n\nAzure App Service Now, create an Azure App Service resource. Using the Azure App Service, you can create a container-based web app. This will pull the image from the ACR. Azure App Service enables you to build web, mobile, and API apps quickly, which can be scaled as per requirement.",
      "content_length": 619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "268  Machine Learning in Production\n\nAs shown in the following figure, select the subscription and service group under the Basic tab. Then, provide the name of the instance as mlapp-cd. Choose the Docker Container radio button, as the ML app is based on the Docker container.\n\nFigure 11.10: Azure Service\n\nNext, choose the operating system Linux and select the region where the web app needs to be deployed. Finally, select the Sku and size from the plan. You can change the App Service Plan as per the application and business requirements.",
      "content_length": 542,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "Deploying ML Models on Microsoft Azure  269\n\nFigure 11.11: Azure Service\n\nIn the next tab, that is, Docker configuration, make sure the image from the Azure Container Registry is selected. As shown in the following figure, select the Image Source as Azure Container Registry, and then select the registry, image, and tag from the Azure container registry options section.\n\nFigure 11.12: Azure Service-Docker configuration",
      "content_length": 422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "270  Machine Learning in Production\n\nAfter providing the values to fields, complete the process by clicking on the Review + create button. You should see the status of the web app in the newly created resource. The following figure shows the status of the Azure App Service resource:\n\nFigure 11.13: Azure Service resource-status\n\nAfter completing the preceding process, your app should be deployed. You can access the ML web app from anywhere via the internet.\n\nGitHub Actions GitHub Actions will automate the deployment on Azure. To automate the deployment of the ML app, the workflow (.yml) file is used, as follows:\n\nprod.workflow.yml\n\nGitHub Actions will look for this file for executing the workflow. This workflow is used for app deployment on Azure. It gets triggered by push events, which means any changes pushed to the repo’s master branch of the GitHub repository will trigger this workflow to run. Under jobs, declared the OS and Python versions to be used. Then, it will run the pytest using tox. After passing all tests, it will start the deployment on Azure. For this, it will use the secrets defined in the settings of the",
      "content_length": 1139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "Deploying ML Models on Microsoft Azure  271\n\nGitHub repository. First off, log in to the Azure Container Registry, and then build and push the Docker image to the registry. Finally, this workflow will deploy the app to the Azure web app service and log out from Azure.\n\n1. name: Build and deploy to production\n\n2.\n\n3. on:\n\n4. push:\n\n5. branches:\n\n6. - master\n\n7.\n\n8. jobs:\n\n9. build-and-deploy:\n\n10. runs-on: ubuntu-18.04\n\n11. strategy:\n\n12. matrix:\n\n13. python-version: ['3.7']\n\n14.\n\n15. steps:\n\n16. - name: Checkout GitHub Actions\n\n17. uses: actions/checkout@master\n\n18.\n\n19. - name: Set up Python ${{ matrix.python-version }}\n\n20. uses: actions/setup-python@v2\n\n21. with:\n\n22. python-version: ${{ matrix.python-version }}\n\n23.\n\n24. - name: Install dependencies\n\n25. run: |\n\n26. python -m pip install --upgrade pip\n\n27. python -m pip install tox tox-gh-actions\n\n28.\n\n29. - name: Test with tox",
      "content_length": 895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "272  Machine Learning in Production\n\n30. run: |\n\n31. tox\n\n32.\n\n33. - name: Login via Azure CLI\n\n34. uses: azure/login@v1\n\n35. with:\n\n36. creds: ${{ secrets.AZURE_CREDENTIALS }}\n\n37.\n\n38. - name: Login to Azure Container Registry\n\n39. uses: azure/docker-login@v1\n\n40. with:\n\n41. login-server: mlappcd.azurecr.io\n\n42. username: ${{ secrets.REGISTRY_USERNAME }}\n\n43. password: ${{ secrets.REGISTRY_PASSWORD }}\n\n44.\n\n45. - name: Build and push container image to the registry\n\n46. run: |\n\n47. docker build . -t mlappcd.azurecr.io/mlapp-cd:${{ github. sha }}\n\n48. docker push mlappcd.azurecr.io/mlapp-cd:${{ github.sha }}\n\n49.\n\n50. - name: Deploy to App Service\n\n51. uses: azure/webapps-deploy@v2\n\n52. with:\n\n53. app-name: 'mlapp-cd'\n\n54. images: 'mlappcd.azurecr.io/mlapp-cd:${{ github.sha }}'\n\n55.\n\n56. - name: Azure logout\n\n57. run: |\n\n58. az logout\n\nNext, create the repository on GitHub for this (if not created already) and push the codebase into that repo.",
      "content_length": 959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "Deploying ML Models on Microsoft Azure  273\n\nService principal You need to provide a service principal in the GitHub Actions workflow for Azure authentication and app deployment. To get the credentials, execute the following command with your subscription id in the terminal.\n\naz ad sp create-for-rbac --name \"github-actions\" --role contributor --scopes /subscriptions/<Subscription ID>/resourceGroups/mlapp-cd --sdk- auth\n\nWith the preceding command, you will create an Azure Role Based Access Control (RBAC) named github-actions with a contributor role and scope.\n\nNote: You can get the subscription ID from the subscription you have used while creating a resource group.\n\nAzure RBAC is an authorization system built on Azure Resource Manager that provides access management of Azure resources.\n\nYou should get the following output:\n\n{\n\n\"clientId\": \"██████████████████████████\",\n\n\"clientSecret\": \"██████████████████████████\",\n\n\"subscriptionId\": \"██████████████████████████\",\n\n\"tenantId\": \"██████████████████████████\",\n\n\"activeDirectoryEndpointUrl\": \"https://login.microsoftonline.com\",\n\n\"resourceManagerEndpointUrl\": \"https://management.azure.com/\",\n\n\"activeDirectoryGraphResourceId\": \"https://graph.windows.net/\",\n\n\"sqlManagementEndpointUrl\":\n\n\"https://management.core.windows.\n\nnet:8443/\",\n\n\"galleryEndpointUrl\": \"https://gallery.azure.com/\",\n\n\"managementEndpointUrl\": \"https://management.core.windows.net/\"\n\n}\n\nIn the preceding response, clientId, clientSecret, subscription, and tenantId are unique for the individual account. Copy and save the response of the preceding command for future use.\n\nNow, go to GitHub repo settings and create three GitHub secrets:",
      "content_length": 1667,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "274  Machine Learning in Production\n\nAZURE CREDENTIALS: Entire JSON response from preceding •\t REGISTRY_USERNAME: clientId value from JSON response •\t REGISTRY_PASSWORD: clientSecret value from JSON response\n\nConfigure Azure App Service to use GitHub Actions for CD You need to configure the Azure App Service so that it can be automated using GitHub Actions. Head over to the Azure App Service | Deployment Center and link the GitHub repo master branch, as shown in the following figure.\n\nFigure 11.14: Azure Service-GitHub configuration\n\nYou need to authorize GitHub Actions to automate CD to provide the required details. You can add deployment slots, such as staging or pre-production. It is useful when you don’t want to expose the updated Azure App Service directly to production. You need to do the same configuration for the deployment center as per the previous step.",
      "content_length": 877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "Deploying ML Models on Microsoft Azure  275\n\nAfter completing the configuration in the app service, place the prod.workflow. yml workflow file in the linked GitHub repository. The path would be .github/ workflows/prod.workflow.yml. When you push any changes to the GitHub repo, it will trigger the GitHub Actions workflow. After completing all the stages, the ML app will be deployed on Azure.\n\nGo to the Actions tab in the GitHub repository, and you will see that GitHub has already started running the prod.workflow.yml workflow for app deployment on Azure.\n\nPush the changes with a Deploy text as a comment, as you can see in the following figure.\n\nFigure 11.15: GitHub Actions-workflow runs",
      "content_length": 695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "276  Machine Learning in Production\n\nClick on Deploy, and it will take you to the summary of this workflow. You will see the status of the jobs you have defined in the workflow. Other execution details are also mentioned, such as the time taken to complete this workflow, the username that pushed the updates to the master branch, and the name of the workflow file.\n\nFigure 11.16: GitHub Actions-workflow steps\n\nThe following figure shows the output of app deployment to the Azure step from GitHub Actions. You can see that the app is successfully deployed and running on Azure. An app service application URL is printed at the end.\n\nFigure 11.17: GitHub Actions-deployment status",
      "content_length": 681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "Deploying ML Models on Microsoft Azure  277\n\nHead over to the Azure App Service; you should see container logs in the Azure App Service, as shown in the following figure. This helps you debug the issue while deploying the app.\n\nFigure 11.18: App service-container logs\n\nIf everything goes well, the app should work on Azure post deployment.\n\nThe following figure shows the app running on the Azure platform on successful completion of the prod.workflow.yml workflow.\n\nFigure 11.19: App service-ML web app running",
      "content_length": 513,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "278  Machine Learning in Production\n\nAzure provides a monitoring service to track the usage of the Azure app. You can select metrics from the dropdown.\n\nThe following figure shows the monitoring service of the Azure app.\n\nFigure 11.20: App service-monitoring service\n\nThus, you have learned to create a simple CI/CD pipeline using GitHub to deploy the ML app on the Azure platform. However, you can modify it as per business and application requirements.\n\nDeployment using Azure DevOps and Azure ML In this part, you will use Azure DevOps and AML to deploy an ML app on the Azure cloud. Unlike the previous part, the CI pipeline and CD pipeline are in the Azure cloud. This approach requires parallelism to be enabled. If you are using free credits, then by default, parallelism is not enabled; however, you can activate it by sending the request via email. You can use Azure Git repo or other sources, such as GitHub. This approach gives more flexibility, such as integration with other platforms, manual approval before deploying to the production, auto redeploy triggers, and such.",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "Deploying ML Models on Microsoft Azure  279\n\nThe following figure shows the automated ML workflow:\n\nFigure 11.21: Azure DevOps pipeline for ML\n\nAzure Machine Learning (AML) service Azure Machine Learning (AML) service enable the creation of a reproducible CI/ CD pipeline for ML. It is an Azure cloud-hosted environment that allows developers and organizations to deploy ML models quickly in the production environment with minimal code. In Azure Machine Learning Studio, you can run the notebooks on the cloud, use Automated ML for automated training and tuning the ML model using a metric, or use a designer to deploy a model from data preparation using a drag-and- drop interface.\n\nThe following are the salient features of Azure Machine Learning Studio:\n\nStorage provision to store your data and artifacts •\t MLFlow to track your experiments and log the run details like timestamp, model metrics, and so on\n\nModel registry to store trained ML models for reusability •\t Key vault to store credentials and variables •\t Supports open-source libraries and frameworks •\t Compute instance, which enables building and training in a secure VM •\t Pipelines and CI/CD for faster training and deployment",
      "content_length": 1197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "280  Machine Learning in Production\n\nEndpoints for ML model •\t Data drift functionality to deliver consistent accuracy and predictions •\t Monitoring service to track the model, logs, and resources\n\nWorkspace A machine learning workspace is the main resource of AML. It contains experiments, models, endpoints, datasets, and such. It also includes other Azure resources, such as Azure Container Registry (ACR), which registers Docker containers at the time of Docker image deployment, storage, application insights, and key vault.\n\nExperiments An experiment consists of several runs initiated from the script. Run details and metadata are stored under the experiment. When you run the script, the experiment name needs to be provided to store the run information. However, it will create a new experiment if the name provided in the script does not exist in the given workspace.\n\nRuns A run can be defined as a single iteration of the training script. Multiple runs are recorded under an experiment. A run logs model metrics and metadata, such as timestamps.\n\nThe best thing about AML is that you do not need to create a separate web service API using frameworks like Flask, Django, and FastAPI. AML creates endpoints for trained ML models and tracks the web service.\n\nIn this section, you will be using the Azure Machine Learning template and modifying it as per your requirements. You can build your codebase from the scratch; however, this template saves time as it has reusable and required steps already defined. The Azure DevOps demo generator can be accessed at the following link:\n\nhttps://azuredevopsdemogenerator.azurewebsites.net/environment/createproject\n\nGo to DevOps Labs, select the Azure Machine Learning template and add it to your project by providing the project name and organization in the next step.\n\nAlternatively, you can directly go to the following URL:\n\nhttps://azuredevopsdemogenerator.azurewebsites.net/?name=machinelearning",
      "content_length": 1953,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "Deploying ML Models on Microsoft Azure  281\n\nFigure 11.22: Azure DevOps Demo Generator\n\nAs you can see, a codebase is imported along with the template. This template contains code and pipeline definitions for a machine learning project to demonstrate how to automate the end-to-end ML/AI project. In this, most steps can be reused with minor modifications wherever required. A codebase is residing in the Azure Git repository; however, you can import a codebase from the GitHub repository. Note that this template is built for a regression algorithm and uses a diabetes dataset.",
      "content_length": 579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "282  Machine Learning in Production\n\nHowever, you need to update the files for the classification algorithm, and the loan dataset will be used for it.\n\nFigure 11.23: Azure code repository\n\nNext, head over to the pipeline section from the left menu. You can see that the steps have already been defined in the template. Don’t worry if you want to create all the steps from the beginning. First off, create a new project, and inside that project, go to the pipeline section from the left menu. Next, select a codebase from the available source options, such as GitHub, Azure Repos, and Git. Then, create a new pipeline from a YAML file, or use the classic editor to create a pipeline without YAML.\n\nNext, go to the newly created project settings and create a new service connection, if it is not created. Choose Service Principal (automatic), and then choose the subscription and resource group from the dropdown and provide the service connection name in the next window. Make sure the Grant access permission to all pipeline checkbox is selected.",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "Deploying ML Models on Microsoft Azure  283\n\nThe following figures show the steps discussed:\n\nFigure 11.24: New Azure service connection\n\nNow, you need to update the files that will be used in the Azure CI/CD pipeline. You can refer to the following code to modify respective scripts. Only code modifications are discussed here, as you can keep the rest of the code as it is in the original script.\n\ncode/training/train.py\n\nThis file will build a classification model using logistic regression. After model training, it will log Cross-Validation (CV) scores and accuracy for later use. Then, it will save the model as a pickle object. This script will be called by the aml_service/10- TrainOnLocal.py script.\n\nThe following code update is incorporated into the existing script:\n\n1. import pickle\n\n2. from azureml.core import Workspace\n\n3. from azureml.core.run import Run\n\n4. import os\n\n5. from sklearn.linear_model import LogisticRegression\n\n6. from sklearn.model_selection import train_test_split, cross_val_ score\n\n7. from sklearn.metrics import accuracy_score\n\n8. from sklearn.preprocessing import LabelEncoder\n\n9. from sklearn.externals import joblib",
      "content_length": 1156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "284  Machine Learning in Production\n\n10. import pandas as pd\n\n11. import numpy as np\n\n12. import json\n\n13. import subprocess\n\n14. from typing import Tuple, List\n\n15.\n\n16. run = Run.get_submitted_run()\n\n17.\n\n18. url = \"https://gist.githubusercontent.com/ suhas-ds/a318d2b1dda8d8cbf2d6990a8f0b7e8a/ raw/9b548fab0952dd12b8fdf057188038c8950428f1/loan_dataset.csv\"\n\n19. df = pd.read_csv(url)\n\n20.\n\n21. # fill the missing values for numerical cols with the median\n\n22. num_col = ['LoanAmount','Loan_Amount_Term','Credit_History']\n\n23. for col in num_col:\n\n24. df[col].fillna(df[col].median(), inplace=True)\n\n25.\n\n26. # fill the missing values for categorical cols with mode\n\n27. cat_col = ['Gender','Married','Dependents','Self_Employed']\n\n28. for col in cat_col:\n\n29. df[col].fillna(df[col].mode()[0], inplace=True)\n\n30.\n\n31. # Total Income = Applicant Income + Coapplicant Income\n\n32. df['Total_Income'] = df['ApplicantIncome'] + df['CoapplicantIncome'] 33.\n\n34. # drop unnecessary columns\n\n35. cols = ['ApplicantIncome', 'CoapplicantIncome', \"LoanAmount\", \"Loan_Amount_Term\", 'Loan_ID']\n\n36. df = df.drop(columns=cols, axis=1)\n\n37.\n\n38. # Label encoding\n\n39. cols = ['Gender',\"Married\",\"Education\",'Self_Employed',\"Property_ Area\",\"Loan_Status\",\"Dependents\"]",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "Deploying ML Models on Microsoft Azure  285\n\n40. le = LabelEncoder()\n\n41. for col in cols:\n\n42. df[col] = le.fit_transform(df[col])\n\n43.\n\n44. # Train test data preparation\n\n45. target = 'Loan_Status'\n\n46.\n\n47. X = df.drop(columns=['Loan_Status'], axis=1)\n\n48. y = df['Loan_Status']\n\n49. X_train, X_test, y_train, y_test = train_test_split(X, y, test_ size=0.30, random_state=42) 50.\n\n51. print(\"Running train.py\")\n\n52.\n\n53. model = LogisticRegression()\n\n54. model.fit(X_train, y_train)\n\n55. print(\"Accuracy is\", model.score(X_test, y_test)*100)\n\n56. # cross validation - it is used for better validation of the model\n\n57. # eg: cv-5, train-4, test-1\n\n58. cv_score = cross_val_score(model, X, y, cv=5)\n\n59. print(\"Cross-validation is\",np.mean(cv_score)*100)\n\n60. run.log(\"CV_score\", np.mean(cv_score)*100)\n\n61. y_pred = model.predict(X_test)\n\n62. print(\"Accuracy = \" , accuracy_score(y_test, y_pred))\n\n63. run.log(\"accuracy\", accuracy_score(y_test, y_pred))\n\n64.\n\n65. # Save the model as part of the run history\n\n66. model_name = \"sklearn_classification_model.pkl\"\n\n67. # model_name = \".\"\n\n68.\n\n69. with open(model_name, \"wb\") as file:\n\n70. joblib.dump(value=model, filename=model_name)",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "286  Machine Learning in Production\n\ncode/score/score.py\n\nThis script will be used for testing the web service. Here, update the saved model path, and then update the input test data that is to be predicted using web service.\n\nThe following code update is incorporated into the existing script:\n\n1. model_path = Model.get_model_path(model_name=\"sklearn_ classification_model.pkl\")\n\nUpdate test data as per the ML model requirement:\n\n1. test_row = '{\"data\":[[1,1,0,1,0,1,2,3849],[1,1,3,0,0,0,1,5540]]}'\n\naml_config/config.json\n\nHere, you need to update the details from the Azure portal. Other scripts will use the variables declared in this script.\n\nThe following code update is incorporated into the existing script:\n\n1. {\n\n2. \"subscription_id\": \"██████████████████████████\",\n\n3. \"resource_group\": \"mlops\",\n\n4. \"workspace_name\": \"mlops_ws\",\n\n5. \"location\": \"centralus\"\n\n6. }\n\nenvironment_setup/install_requirements.sh\n\nThis bash script is responsible for installing Python packages using pip.\n\nThe following code update is incorporated into the existing script:\n\n1. python --version\n\n2. pip install azure-cli==2.37.0 #==2.0.69\n\n3. pip install --upgrade azureml-sdk[cli]\n\n4. pip install -r requirements.txt\n\naml_service/00-WorkSpace.py\n\nThis file will get the details of the existing workspace; however, if not found, it will create a new one.\n\naml_service/10-TrainOnLocal.py\n\nThis script triggers the code/training/train.py script to run on the local compute (host agent in case of build pipeline). If you are training on a remote VM, you do not need this script in the build pipeline. All the training scripts generate an output file aml_",
      "content_length": 1641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "Deploying ML Models on Microsoft Azure  287\n\nconfig/run_id.json, which records the run_id and run history name of the training run. run_id.json is used by 20-RegisterModel.py to get the trained model. Update the experiment name to mlops-classification.\n\nThe following code update is incorporated into the existing script:\n\n1. experiment_name = \"mlops-classification\"\n\naml_service/15-EvaluateModel.py\n\nAt this step, you will get the run history for both the production model and the newly trained model to compare the accuracy. If the new model’s accuracy is better than that of the existing model, the new model will be promoted to production; otherwise, the existing model will remain as it is. However, for the first time, there won’t be any model to compare, so it will go to the except block and promote the new model. You can change the metrics that need to be compared and conditions that decide whether the new model is to be promoted to production. At the end of the script, it will capture the experiment name and run id of the new model in the aml_config/run_id.json file if the new model is promoted to production.\n\nThe following code update is incorporated into the existing script:\n\n1. production_model_acc = production_model_run.get_metrics(). get(\"accuracy\")\n\n2. new_model_acc = new_model_run.get_metrics().get(\"accuracy\")\n\n3. print(\n\n4. \"Current Production model accuracy: {}, New trained model accuracy: {}\".format(\n\n5. production_model_acc, new_model_acc\n\n6. )\n\n7. )\n\n8.\n\n9. promote_new_model = False\n\n10. if new_model_acc > production_model_acc:\n\n11. promote_new_model = True\n\n12. print(\"New trained model performs better, thus it will be registered\")\n\naml_service/20-RegisterModel.py\n\nThis script is responsible for registering new models. This will look for aml_config/ run_id.json; if run_id is not found, it will print the message No new model to",
      "content_length": 1870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "288  Machine Learning in Production\n\nregister as production model perform better written inside the except block and exit from the code.\n\nIf the run id is found in aml_config/run_id.json, it will register the model. Finally, it will write the registered model details to /aml_config/model.json.\n\nThe following code update is incorporated into the existing script:\n\n1. # Download Model to Project root directory\n\n2. model_name = \"sklearn_classification_model.pkl\"\n\n3. run.download_file(\n\n4. name=\"./outputs/\" + model_name, output_file_path=\"./model/\" + model_name\n\n5. )\n\n6. print(\"Downloaded model {} to Project root directory\". format(model_name))\n\n7. os.chdir(\"./model\")\n\n8. model = Model.register(\n\n9. model_path=model_name, # This points to a local file\n\n10. model_name=model_name, # This is the name the model is registered as\n\n11. tags={\"area\": \"loan\", \"type\": \"classification\", \"run_id\": run_ id},\n\n12. description=\"Classification model for loan dataset\",\n\n13. workspace=ws,\n\n14. )\n\naml_service/30-CreateScoringImage.py\n\nIn the beginning, it will look for the aml_config/model.json file generated from the previous step. If it is not found, the script will be terminated with the message No new model to register thus no need to create new scoring image.\n\nThe following code update is incorporated into the existing script:\n\n1. image_name = \"loan-model-score\"\n\n2.\n\n3. image_config = ContainerImage.image_configuration(\n\n4. execution_script=\"score.py\",\n\n5. runtime=\"python-slim\",",
      "content_length": 1485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "Deploying ML Models on Microsoft Azure  289\n\n6. conda_file=\"conda_dependencies.yml\",\n\n7. description=\"Image with logistic regression model\",\n\n8. tags={\"area\": \"loan\", \"type\": \"classification\"},\n\n9. )\n\nThis will create the image for the model, and at the end, capture the image details, such as name, version, creation_state, image_location, and image_build_ log_uri in the /aml_config/image.json.\n\naml_service/50-deployOnAci.py\n\nAzure Container Instances (ACI) is an Azure service that enables users to run a container on the Azure cloud without demanding the use of a Virtual Machine (VM).\n\nIn the beginning, it will look for the aml_config/image.json file generated from the previous step. If it is not found, the script will be terminated with the message No new model, thus no deployment on ACI.\n\nThe following code update is incorporated into the existing script:\n\n1. aciconfig = AciWebservice.deploy_configuration(\n\n2. cpu_cores=1,\n\n3. memory_gb=1,\n\n4. tags={\"area\": \"loan\", \"type\": \"classification\"},\n\n5. description=\"A loan prediction app\",\n\n6. )\n\nThis script will create a web service and capture the ACI details in /aml_config/aci_ webservice.json at the end of the script.\n\naml_service/60-AciWebserviceTest.py\n\nIn the beginning, it will look for the aml_config/aci_webservice.json file generated from the previous step. If it is not found, the script will be terminated with the message No new model, thus no deployment on ACI.\n\nThe following code update is incorporated into the existing script:\n\n1. # Input for Model with all features\n\n2. input_j = [[1,1,0,1,0,1,2,3849],[1,1,3,0,0,0,1,5540]]",
      "content_length": 1606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "290  Machine Learning in Production\n\nConfigure CI pipeline At this stage, you will configure the CI pipeline. This pipeline will execute the following tasks:\n\nSet up Python version 3.6. • Install required dependencies.\n\nCreate or get Azure Machine Learning (AML) workspace. •\t Build and train the ML model. •\t Evaluate newly trained model performance against the existing model to decide whether the model is to be promoted to production. •\t Register model in Azure Machine Learning (AML) service. •\t Create a scoring Docker image with the required dependencies. •\t Publish the artifacts (all files) to the repo.\n\nThe following figure shows the steps of the CI pipeline:\n\nFigure 11.25: CI pipeline for ML",
      "content_length": 705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "Deploying ML Models on Microsoft Azure  291\n\nNote: You need to select Azure service connection from the Azure subscription dropdown wherever necessary.\n\nNext, in the Triggers tab, select Enable continuous integration checkbox. This enables the CI pipeline to run as soon as any changes are pushed to the code repository.\n\nYou can trigger the CI pipeline by pushing any changes to the repository. The Azure CI pipeline detects the changes in the repo and starts building the pipeline. To see the progress of the CI pipeline, switch to the Summary tab.\n\nThe following figure displays the Summary tab. You can see the commit message on the top, followed by the summary and the jobs. The job status and progress of steps can be seen by clicking on Phase 1.\n\nFigure 11.26: CI pipeline for ML-Jobs summary",
      "content_length": 800,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "292  Machine Learning in Production\n\nAfter clicking on Phase 1 under the Jobs section, you should see the progress of the CI pipeline. The following figure shows the terminal output of each CI pipeline step:\n\nFigure 11.27: CI pipeline for ML-logs\n\nConfigure CD pipeline Now that you have completed the CI pipeline configuration, it’s time to configure the CD pipeline. The CD pipeline will deploy the image generated through the CI pipeline to the Azure Container Instance (ACI) and Azure Kubernetes Service (AKS). In the current case, the scope of the project is limited to ACI. However, you can deploy it on AKS to handle large amounts of traffic and make it scalable.\n\nThis pipeline executes the following tasks:\n\nSet up Python version 3.6. •\n\nInstall the required dependencies.\n\nDeploy a web service on the Azure container instance (ACI). •\t Test the ACI web service by passing data that needs to be tested.",
      "content_length": 912,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "Deploying ML Models on Microsoft Azure  293\n\nDeploy a web service on Azure container instance (AKS). •\t Test the AKS service by passing the data to be tested.\n\nThe following figure shows the CD pipeline:\n\nFigure 11.28: CD pipeline for ML\n\nNow, navigate to Pipeline | Releases, select Deploy Web service and click on Edit pipeline.\n\nThe following figure shows the QA – Deploy on ACI stage of the CD pipeline when switched to the Tasks tab.\n\nFigure 11.29: CD pipeline for ML-QA stage",
      "content_length": 482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "294  Machine Learning in Production\n\nNote: You need to select Azure service connection from the Azure subscription dropdown wherever necessary.\n\nIn the following figure, you can see the entire file path provided by the artifacts that are retrieved from the CI pipeline:\n\nFigure 11.30: CD pipeline for ML-QA stage requirement step\n\nWhen you update the CD pipeline, you can run the CD pipeline manually by hitting the Create Release button or setting the auto trigger, which means that the CD pipeline starts executing upon completion of the CI pipeline. You need to make sure that continuous deployment triggers are enabled in the Artifacts stage and the QA -Deploy on ACI stage of the CD pipeline.\n\nIn the following figure, you can see the CD pipeline triggers upon completion of the CI pipeline, which caused the QA -Deploy on ACI stage to start executing:",
      "content_length": 858,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "Deploying ML Models on Microsoft Azure  295\n\nFigure 11.31: CD pipeline for ML-triggered after CI pipeline\n\nIt takes a few minutes to execute all the steps. You should see the number of tasks completed and the status of the stage. If you want to see the details of each step, click on the Logs option.\n\nThe following figure shows the status of each step with logs.\n\nFigure 11.32: CD pipeline for ML-QA stage logs",
      "content_length": 412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "296  Machine Learning in Production\n\nWhen ACI deployment is completed, it waits for manual approval from the developer but for a limited time. You can update the time from the options. After approval, it will deploy to the AKS. If you want to automate this step, you can remove the manual approval or set auto approval. However, it is recommended to keep manual approval before deploying it on AKS.\n\nIn the following figure, you can see that the QA – Deploy on ACI stage is completed, and it is pending approval for deploying it on AKS. You can set the time for approval requests to be active. After approval, the pipeline will deploy the app using AKS.\n\nFigure 11.33: Azure pipeline for ML-completed\n\nNow, head over to the Azure Machine Learning studio, and go to Jobs from the left panel. You should see the experiment with the name provided in the training script, that is, mlops-classification. On the right side, you can see the details like the experiment name, the date of creation, and the last submission. When you go to the experiment, you will see the run information with status and other metadata. On the top, you can see metrics details with graphs. In the current case, accuracy and CV score are being logged. You can assess each job’s details by clicking on it.\n\nThe following figure shows the model metrics, that is, accuracy and CV score, along with run information and other details. In the graphs, you can see the value of each metric logged in the runs separately.",
      "content_length": 1486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "Deploying ML Models on Microsoft Azure  297\n\nFigure 11.34: Azure Machine Learning-Model metrics\n\nWhen a model is deployed in an Azure Container Instance (ACI), you will get the scoring URI, which can be consumed by the subsequent applications.\n\nThe following figure shows the endpoint created by the script in the CI/CD pipeline:\n\nFigure 11.35: AML Real-time endpoints\n\nYou can assess the model endpoints by evaluating the outcome after passing the input data. Moreover, you can consume the endpoint using other languages, such as C#, Python, and R.",
      "content_length": 550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "298  Machine Learning in Production\n\nIn the following figure, you can see the output produced by the model’s endpoints under Test results after passing the test data:\n\nFigure 11.36: AML Real-time endpoints testing\n\nYou can assess the same endpoint in the Postman as well. The following figure shows the testing of scoring URI in the Postman:\n\nFigure 11.37: AML Real-time endpoints testing in Postman\n\nTill now, you have learned to automate CI/CD workflow using Azure DevOps and Azure Machine Learning Service. You have explored GitHub Actions and",
      "content_length": 547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "Deploying ML Models on Microsoft Azure  299\n\nAzure DevOps approaches to build an automated workflow to deploy models in production. By now, you should be familiar with the Azure cloud and the services provided by Azure and its implementation.\n\nConclusion In this chapter, you learned to deploy the app on the Azure platform (PaaS). You also built an automated CI/CD pipeline using GitHub Actions. Deployed ML web app in the Azure App Service using Azure Container Registry (ACR). Further on, you integrated tox with GitHub Actions to run the test cases as a part of the CI/CD pipeline before deploying the ML web app to Azure.\n\nAfter that, you learned to deploy an ML app using Azure DevOps and Azure Machine Learning service (AML), which provides ML as a service (MLaaS). You analyzed the metrics logged by runs in the Azure Machine Learning studio, and you passed the sample data to the endpoint and assessed the outcome.\n\nIn the next chapter, you will explore the GCP platform to deploy ML apps.\n\nPoints to remember\n\nYou need to authorize GitHub Actions to automate the CD part of the pipeline and provide the required details.\n\nAzure Role Based Access Control (RBAC) is an authorization system built on Azure Resource Manager that provides fine-grained access management of Azure resources.\n\nAn experiment consists of several runs initiated from the script. • In the Azure DevOps template, the aml_service/15-EvaluateModel.py script will compare the performance metric of the latest model against the deployed model before the deployment stage. It means the set of rules is written in this script, which will decide whether the model can be promoted to the next step.\n\nMultiple choice questions\n\n1. _________ can be defined as a single iteration of the training script.\n\na) A run\n\nb) An experiment\n\nc) A workspace\n\nd) ACR",
      "content_length": 1826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "300  Machine Learning in Production\n\n2. Azure primarily uses the _________ pricing plan.\n\na) Unlimited (for all Azure services)\n\nb) Quarterly\n\nc) Monthly\n\nd) Pay-as-you-go\n\nAnswers 1. a\n\n2. d\n\nQuestions\n\n1. What is the use of Azure Machine Learning (AML) service?\n\n2. How can you log in to Azure from the local terminal?\n\n3. Explain the working of the Azure pipeline stages.",
      "content_length": 375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "Deploying ML Models on Google Cloud Platform  301\n\nChapter 12 Deploying ML Models on Google Cloud Platform\n\nIntroduction Cloud computing is the on-demand delivery of computer system resources, such as servers, databases, analytics, storage, and networking. These resources and services are available off-premises; however, they can be accessed over the cloud (the internet) as per requirement. This means you do not need to set up big infrastructure on- premises. This is beneficial for businesses and individuals as they get the required system resources and services instantly under a pay-as-you-go plan. Resources and services can be added or removed quickly, which allows you to spend money efficiently.\n\nRefer to the previous chapters to learn about concepts like Packaging ML Models, FastAPI, Docker, and CI/CD pipeline, as they are pre-requisites for this chapter.\n\nStructure In this chapter, the following topics will be covered:\n\nCreate and set up an account on the Google Cloud Platform (GCP) •\t Cloud Source Repositories •\t Cloud Build",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "302  Machine Learning in Production\n\nContainer Registry •\t Deploy web-based ML app to Google Kubernetes Engine (GKE) with manual trigger\n\nBuild an automated CI/CD pipeline to deploy web-based ML apps on Google Kubernetes Engine (GKE)\n\nObjectives After studying this chapter, you should be able to deploy an ML model on Google Kubernetes Engine (GKE). You can create a fully automated CI/CD pipeline with simple steps, without the need to integrate any external tool, service, or platform. You will create a remote Git repository on GCP using Cloud Source Repository and Kubernetes cluster to make a scalable ML app. You will also learn to create manifest files for Kubernetes in this chapter. By the end of it, you should be able to create triggers in Cloud Build for the automated deployment of ML apps.\n\nGoogle Cloud Platform (GCP) Google Cloud Platform (GCP) comprises cloud computing services offered by Google, which uses the same infrastructure as the one used by YouTube, Gmail, and other Google platforms or services. The platform offers a range of services for compute, Machine learning and AI, networking, IoT, and BigData. Here are a few services offered by the GCP:\n\nGoogle's compute engine provides VM instances to run the code and deploy the apps.\n\nAI and Machine learning services like Vertex AI, which is an end-to-end ML life cycle management and unified platform. Data Scientists can upload the data, build, train, and test ML models easily.\n\nAI building blocks, such as Vision AI, help derive insights from images using AutoML.\n\nContainer services, such as Container Registry and Google Kubernetes Engine (GKE), manage Docker images and allow developers to build scalable applications.\n\nBigQuery and data proc to process and analyze large amounts of data. •\t Databases, such as Cloud SQL and Cloud Bigtable, store data on the cloud. •\t Developer tools, such as Cloud Build, Cloud Source Repositories, and Google Cloud Deploy to automate CI/CD process.",
      "content_length": 1971,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "Deploying ML Models on Google Cloud Platform  303\n\nManagement tools, such as Deployment Manager and Cost Management, help you to track the deployment and cost of tools or services used in projects. •\t Networking services, such as Virtual Private Cloud (VPC), let you create a virtual private cloud environment within a public cloud. Multiple projects created in different regions can communicate with each other without openly communicating through the public internet.\n\nSecurity services, such as cloud key management, firewalls, and security center.\n\nStorage services, such as Cloud Storage, allow you to store artifacts. •\t Serverless computing, such as Cloud Function, is an event-driven serverless compute platform. This Function as a Service (FaaS) lets you run the code with no server or containers.\n\nOperations services, such as Cloud Logging and Cloud Monitoring, let you track the performance, delay, and such of the deployed models or applications.\n\n\n\nIt also provides other services, such as migration, IoT, event management, identity and access, hybrid and multi-cloud, backup, and recovery.\n\nYou must have got the gist of GCP and its services; now, you are going to learn to deploy the Machine Learning model on GCP. After completing this chapter, you will be in a better position to work on GCP for ML model deployments.\n\nGCP offers a free trial account for 90 days with $300 credits. It will give hands-on experience to new customers so that they can explore the services offered by GCP.\n\nSet up the GCP account First of all, set up a GCP account; however, this is a one-time activity. It should be ready for use immediately on logging in:\n\nStep 1:\n\nCreate a GCP account (if you don’t have one) and log in. The free trial account can be created on the GCP platform at https://cloud.google.com/free.\n\nFirst, log in using your Gmail ID; it will then redirect you to the GCP Free-trial page. Select your country from the dropdown, accept the Terms of Service, and then click on Continue. Next, choose Individual (for personal use) or Business (if it is a business account). After that, provide personal details like name, address, and city. Finally, provide payment mode details and complete the process.",
      "content_length": 2218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "304  Machine Learning in Production\n\nStep 2:\n\nAfter logging in to the account, create a new project, as shown in the following figure:\n\nFigure 12.1: Create a new project\n\nClick on the NEW PROJECT button and provide the project name. In this case, it is MLOps, and the project ID will be auto-generated; however, it can be edited. In this case, it is mlops-54321. Location can be pre-selected or can be changed.\n\nHit the Create button to complete the process.\n\nNote: The project ID is unique to each project.\n\nCloud Source Repositories In this case, the Git repository, that is, the Cloud Source Repository, is used; however, other remote Git repositories, such as GitHub, can also be used. Cloud Source Repositories are private Git repositories hosted on GCP. Multiple Git repositories can be created within a single project. It supports the standard set of Git commands, such as push, pull, clone, and log. Cloud Source Repositories can be added to a local Git repository as remote repositories. It allows collaboration and provides security. However, it is recommended not to store any personal or confidential data in it. The good part is that GCP’s Cloud Source Repositories allow you to store up to 50 GB per month for free.\n\nNow, search and enable Cloud Source Repositories API, which allows access to source code repositories hosted by Google.\n\nNext, create a new repository in Cloud Source Repositories, and provide the repository name and project ID, as shown in the following figure:",
      "content_length": 1494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "Deploying ML Models on Google Cloud Platform  305\n\nFigure 12.2: Create a new repository in Cloud Source Repositories\n\nAfter creating a Cloud Source Repository, it will be empty. So, choose Clone your repository to a local Git repository option. Next, head to Manually generated credentials and follow the instructions to generate credentials by following the link Generate and store Git credentials. Simply copy the commands and run them in the local terminal. In the following figure, an empty Cloud Source Repository is created and options are selected, as discussed:\n\nFigure 12.3: Cloning repository to local Git repository",
      "content_length": 627,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "306  Machine Learning in Production\n\nHereafter, the repository should be accessible from the local terminal. Now, create a new directory and clone the Cloud Source Repository to your local machine, include the code files in it, and push it to the Cloud Source Repository from the local machine.\n\nLet’s consider the scenario of loan prediction, where the problem statement is to predict whether a customer’s loan will be approved. Feel free to implement hyper- parameter tuning and optimize the model.\n\nFirst, create a package of ML code, and build a web app using FastAPI. Then, create the test cases and dependencies file, Dockerfile, and docker-compose file. Finally, create configuration files for GCP deployment, namely, cloudbuild.yaml, deployment.yaml, service.yaml.\n\nNote: For code files in the src directory, refer to the ML model package developed in Chapter 4: Packaging ML Models, and for the rest of the code files, refer to Chapter 11: Deploying ML Models on Microsoft Azure. New code files added are cloudbuild.yaml, deployment.yaml, and service.yaml. Remove workflow directory .github. Also, the tox.ini file is updated for this scenario.\n\nThe code repository is also available on GitHub at https://github.com/suhas-ds/ GCP.\n\nThe following directory structure shows the CI/CD pipeline files:\n\n.\n\n├── k8s\n\n│ ├── deployment.yaml\n\n│ └── service.yaml\n\n├── src\n\n│ ├── build\n\n│ │ ├── bdist.linux-x86_64\n\n│ │ └── lib\n\n│ ├── prediction_model\n\n│ │ ├── config\n\n│ │ ├── datasets\n\n│ │ ├── processing\n\n│ │ ├── __pycache__\n\n│ │ ├── trained_models\n\n│ │ ├── __init__.py\n\n│ │ ├── pipeline.py\n\n│ │ ├── predict.py",
      "content_length": 1610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "Deploying ML Models on Google Cloud Platform  307\n\n│ │ ├── train_pipeline.py\n\n│ │ └── VERSION\n\n│ ├── prediction_model.egg-info\n\n│ │ ├── dependency_links.txt\n\n│ │ ├── PKG-INFO\n\n│ │ ├── requires.txt\n\n│ │ ├── SOURCES.txt\n\n│ │ └── top_level.txt\n\n│ ├── tests\n\n│ │ ├── pytest.ini\n\n│ │ └── test_predict.py\n\n│ ├── MANIFEST.in\n\n│ ├── README.md\n\n│ ├── requirements.txt\n\n│ ├── setup.py\n\n│ └── tox.ini\n\n├── cloudbuild.yaml\n\n├── docker-compose.yml\n\n├── Dockerfile\n\n├── main.py\n\n├── pytest.ini\n\n├── requirements.txt\n\n├── runtime.txt\n\n├── start.sh\n\n├── test.py\n\n└── tox.ini\n\n13 directories, 31 files\n\ntox.ini\n\nThe tox is a free and open-source tool used for testing Python packages or applications. It creates the virtual environment and installs the required dependencies in it. Finally, it runs the test cases for that Python package prediction_model. Update the tox.ini file as shown in the following code:",
      "content_length": 895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "308  Machine Learning in Production\n\n1. [tox]\n\n2. envlist = my_env\n\n3. skipsdist=True\n\n4.\n\n5. [testenv]\n\n6. install_command = pip install {opts} {packages}\n\n7. deps =\n\n8. -r requirements.txt\n\n9.\n\n10. setenv =\n\n11. PYTHONPATH=src/\n\n12.\n\n13. commands=\n\n14. pip install requests\n\n15. pytest -v test.py --junitxml=test_log.xml\n\n16. pytest -v src/tests/ --junitxml=src_test_log.xml\n\nThe skipsdist=True flag indicates not to perform a packaging operation. It means the package will not be installed in the virtual environment before performing any test. Set PYTHONPATH to the src directory, where Python package files are placed. Under commands, pytest commands will be executed, and the results of the tests will be exported in .xml files.\n\nAfter a successful push from the local terminal, code files will be displayed in the Cloud Source Repository, as shown in the following figure:",
      "content_length": 880,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "Deploying ML Models on Google Cloud Platform  309\n\nFigure 12.4: Code files pushed to Cloud Source Repository\n\nCloud Build Cloud Build is a serverless platform that allows users to automate build, test, and deploy containers in the cloud quickly. Cloud Build works with deployment environments of the App Engine, Kubernetes Engine, Cloud Run, Cloud Functions, and Firebase. It will charge you for build minutes that are utilized if your usage exceeds the free quota allotted by GCP.\n\nCloud Build config file type can be YAML or JSON. This file contains a series of steps and commands to execute the build, specified by the developer. Each build step runs in its own Docker container. However, these containers are connected through a local Docker network with a Cloud Build. This permits build steps to communicate and share information.\n\nSearch and enable Cloud Build API in APIs and services. Cloud Build uses a special service account to execute builds on your behalf. The email format of the Cloud Build service account is [PROJECT_NUMBER]@cloudbuild.gserviceaccount.com. It has permission to execute tasks like fetching code from the repository, getting project details, and writing objects to storage associated with the project. However, this default service account can be changed to the developer’s own service account to execute builds on the developer’s behalf, and the developer can set specific permissions.",
      "content_length": 1420,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "310  Machine Learning in Production\n\ncloudbuild.yaml\n\nCloud builders are container images with common languages and tools installed in them. Cloud Build offers pre-built Docker images that can be used in the config file to execute commands. These pre-built images are available in the Container Registry at gcr.io/cloud-builders/.\n\nIn this case, the cloudbuild.yaml file will run the tests using tox and pytest. Next, it will build the Docker container image with the latest tag. Following that, it will push the image to the Container Registry and run the bash command to check the list of files in the current directory. Finally, it will pull the image from the Docker container to deploy it on Google Kubernetes Engine (GKE).\n\n1. steps:\n\n2. # Run test\n\n3. - name: 'python:3.7-slim'\n\n4. id: Test\n\n5. entrypoint: /bin/sh\n\n6. args:\n\n7. - -c\n\n8. - 'python -m pip install tox && tox'\n\n9.\n\n10. # Build the image\n\n11. - name: 'gcr.io/cloud-builders/docker'\n\n12. id: Build\n\n13. args: ['build', '-t', 'gcr.io/$PROJECT_ID/mlapp:latest', '.']\n\n14. timeout: 200s\n\n15.\n\n16. # Push the image\n\n17. - name: 'gcr.io/cloud-builders/docker'\n\n18. id: Push\n\n19. args: ['push', 'gcr.io/$PROJECT_ID/mlapp:latest']\n\n20.\n\n21. - name: 'gcr.io/cloud-builders/gcloud'\n\n22. id: Bash\n\n23. entrypoint: /bin/sh",
      "content_length": 1282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "Deploying ML Models on Google Cloud Platform  311\n\n24. args:\n\n25. - -c\n\n26. - |\n\n27. echo \"List of files and directories within the current working directory\"\n\n28. ls -l\n\n29.\n\n30. # Deploy container image to GKE\n\n31. - name: \"gcr.io/cloud-builders/gke-deploy\"\n\n32. id: Deploy on GKE\n\n33. args:\n\n34. - run\n\n35. - --filename=k8s/\n\n36. - --image=gcr.io/$PROJECT_ID/mlapp:latest\n\n37. - --location=us-west1-b\n\n38. - --cluster=mlkube\n\nIn the preceding file, mainly three (supported) builder images are used:\n\ngcr.io/cloud-builders/docker: To build and push the Docker container image to the Container Registry\n\ngcr.io/cloud-builders/gcloud: To run the inline bash script\n\ngcr.io/cloud-builders/gke-deploy: To deploy containerized app in GKE\n\nAlso, python:3.7-slim is the publically available image used for testing the code using tox. If any image is to be used from the Docker hub, then simply provide the image name in single quotes. However, if an image is from other registries, then the full registry path needs to be specified in single quotes. The args field of a build step accepts a list of arguments and passes them to the image referred to by its name field.\n\nContainer Registry Container Registry is a container image managing and storing private container images service offered by GCP. It allows users to push and pull container images securely. In this case, the Docker container image will be pushed to the Container Registry. The Kubernetes engine will pull the image to deploy the ML app on the cloud.",
      "content_length": 1514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "312  Machine Learning in Production\n\nContainer Registry uses the hostname, project ID, image, and tag or image digest to access images, where image digest is the sha256 hash value of the image contents. The format is as follows:\n\nHOSTNAME/PROJECT-ID/IMAGE:TAG\n\nOr\n\nHOSTNAME/PROJECT-ID/IMAGE:@IMAGE-DIGEST\n\nThe following figure shows the GCP Container Registry after enabling the Container Registry API, where mlapp is an app name and gcr.io is a hostname. Currently, gcr.io hosts the images in the United States.\n\nFigure 12.5: Container Registry\n\nKubernetes Container orchestration is an automation of the operational process needed to run containerized workloads and services. It automates tasks like deployment, scaling up-down, lifecycle management, load balancing, configuration, security, resource allocation, health monitoring, and networking of containers.\n\nKubernetes (also known as K8s or Kube) is an open-source platform for container orchestration. It allows the application to scale on the fly, without interrupting the application running in production. It creates multiple replicas of containerized applications quickly to handle increased traffic and automatically reduces the number of replicas when traffic decreases.\n\nDeployment on Kubernetes creates Pods with containers inside them. Pods are the smallest unit in the Kubernetes environment. A Pod can have one or more containers. It always runs on a Node; however, a Node can have multiple Pods. A Node is just a worker (VM or physical machine) in a Kubernetes environment. All nodes are managed by a control plane.",
      "content_length": 1586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "Deploying ML Models on Google Cloud Platform  313\n\nEvery Node runs the Kubelet and container runtime, such as Docker. Kubelet is the medium of communication between the control plane and the Node. It also manages Pods and running containers inside the Node. Container runtime such as Docker will pull the image from the registry and it will run the containerized application.\n\nGoogle Kubernetes Engine (GKE) Google Kubernetes Engine (GKE) offers the infrastructure to manage, deploy, and scale containerized applications. The underlying architecture of GKE consists of a set of compute engine instances assembled to form a cluster.\n\nManifest is a file (JSON or YAML) containing a description of all the components you want to deploy. These manifest files guide Kubernetes to network between containers. Kubernetes schedules the deployment of containers into clusters and identifies the best host for the container. After deciding on a host, it manages the lifecycle of the container based on pre-planned specifications.\n\nSearch and enable Kubernetes Engine API in the GCP console, as shown in the following figure:\n\nFigure 12.6: Kubernetes Engine API\n\nNext, click on the Activate Cloud Shell icon in the top-right corner of the GCP account. It will open a cloud-based terminal:\n\ngcloud container clusters create mlkube --zone \"us-west1-b\" --machine- type \"n1-standard-1\" --num-nodes \"1\" --project mlops-54321\n\nFigure 12.7: Creating Kubernetes cluster with cloud shell",
      "content_length": 1468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "314  Machine Learning in Production\n\nAfter the successful creation of the Kubernetes cluster, its details can be checked using the GKE service, as shown in the following figure:\n\nFigure 12.8: Kubernetes cluster\n\nAll manifest files contain the apiVersion field. Each Kubernetes configuration file has three sections: metadata, specification, and status. Metadata and specification are provided by the developers in the configuration files; however, the status part is managed by Kubernetes. For instance, if 2 replicas are declared but only one replica is up and running, then Kubernetes scrutinize the status and creates one more replica to match the desired state and the actual state. As shown as follows, Kubernetes configuration files are placed in the k8s directory. The labels and selectors are responsible for connecting service and deployment files. The same labels should be used in the service.yaml and deployment.yaml files for successful communication between service and deployment.\n\nk8s\n\n├── deployment.yaml\n\n└── service.yaml\n\ndeployment.yaml\n\nThis file contains the configuration of Kubernetes deployment, such as the number of replicas to be created and the container image to be deployed. The role of deployment.yaml is to launch a Pod with a containerized app and ensure that the necessary number of replicas are always up and running in the Kubernetes cluster. In the following file, a template is a blueprint for a Pod. It has its own metadata and specification. In this case, the replicas field is set to 2, which means it will create two replicas of the Pods.\n\n1. apiVersion: apps/v1\n\n2. kind: Deployment\n\n3. metadata:",
      "content_length": 1641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "Deploying ML Models on Google Cloud Platform  315\n\n4. name: loan-prediction\n\n5. spec:\n\n6. replicas: 2\n\n7. selector:\n\n8. matchLabels:\n\n9. app: mlapp\n\n10. template:\n\n11. metadata:\n\n12. labels:\n\n13. app: mlapp\n\n14. spec:\n\n15. containers:\n\n16. - name: loan-prediction-app\n\n17. image: gcr.io/mlops-54321/mlapp:latest\n\n18. ports:\n\n19. - containerPort: 8000\n\nservice.yaml\n\nThis file connects the containerized application to the end user. The role of service. yaml is to expose an interface to a Pod created by deployment.yaml, which enables network access between cluster and service. It exposes the deployed application to the external world. Port 8000 of containerized application is mapped with port 80. In ports, you can specify more than one port. In this case, the type of service is LoadBalancer, which will expose the service through GCP’s (cloud provider’s) load balancer. Label selector helps to locate Pods. In this case, mlapp is a label.\n\n1. apiVersion: v1\n\n2. kind: Service\n\n3. metadata:\n\n4. name: mlapp\n\n5. spec:\n\n6. type: LoadBalancer\n\n7. selector:\n\n8. app: mlapp\n\n9. ports:",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "316  Machine Learning in Production\n\n10. - port: 80\n\n11. targetPort: 8000\n\nNote: In the targetPort field, the uppercase letter ‘P’ is used, as it is a key-value pair. In the key-value pair, the second word’s first character should be uppercase.\n\nDeployment using Cloud Shell – Manual Trigger First, open a cloud shell and clone the remote Git repository using the git clone command. Now, code files are accessible in the GCP cloud shell terminal. Run the cloudbuild.yaml file manually with the following command:\n\ngcloud builds submit --config cloudbuild.yaml --project=mlops-54321\n\nThis command will take a few minutes to execute. In this scenario, the pipeline is triggered manually, where a series of steps will be executed sequentially from the cloudbuild.yaml file.\n\nThe following figure shows the output of the preceding command, in which details of the deployed app using Kubernetes are given, such as the IP address of the deployed ML app, source, duration, status, and GKE URLs for workloads, services, configuration, and storage.\n\nFigure 12.9: Deploying ML app on GKE with manual trigger",
      "content_length": 1098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "Deploying ML Models on Google Cloud Platform  317\n\nCI/CD pipeline using Cloud Build GCP comes with a set of services for the CI/CD pipeline. In the previous section, the build was triggered manually with the cloud shell terminal. To automate the manual process, CI/CD pipeline needs to be built with an automated trigger. The following figure shows the list of CI/CD services provided by GCP:\n\nFigure 12.10: CI/CD services provided by GCP\n\nWhen an update is pushed to Cloud Source Repository (CSR), it will trigger the CI/CD pipeline. Cloud Build configuration file first runs the tests, and then builds and pushes the container image to the Container Registry. Simultaneously, it reads the Kubernetes manifest and deploys it on the Kubernetes engine. Kubernetes engine will pull the container image pushed by Cloud Build and eventually run the containerized ML app that is accessible to the end user.",
      "content_length": 902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "318  Machine Learning in Production\n\nThe following figure shows the automated CI/CD pipeline to deploy the containerized app on GKE:\n\nFigure 12.11: CI/CD pipeline to deploy ML app on GKE\n\nCreate a trigger in Cloud Build In this section, create a trigger in Cloud Build that will look for a Cloud Build configuration file, that is, a YAML file, to build and push the Docker image to the Container Registry. It is also responsible for applying the Kubernetes manifest in the Kubernetes engine.\n\nFirst, head to the Triggers section from the left side panel in Cloud Build. For the first time, it will show No triggers found in global, as shown in the following figure:\n\nFigure 12.12.: Creating trigger in Cloud Build\n\nCreate a new trigger using the CREATE TRIGGER button. Next, provide a unique name within the project’s region for triggers and select the region from the dropdown. Description and Tags are optional.\n\nIn this case, the Name is AutoDeploy, the Region is global(non-regional) and the Description is Mlapp, as shown in the following figure:",
      "content_length": 1052,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "Deploying ML Models on Google Cloud Platform  319\n\nFigure 12.13: Creating trigger in Cloud Build – providing a name and selecting a region\n\nFollowing that, select the repository event that will trigger the pipeline. It provides multiple options; however, Push to a branch is selected. So, any changes pushed to the master branch of the repository will trigger the pipeline. This will look for changes in the repository and clone the repository when the trigger is invoked.\n\nIn the following figure, the Push to a branch option is selected, along with the Cloud Source Repository and the master branch as a source:\n\nFigure 12.14: Creating trigger in Cloud Build – selecting an event and source",
      "content_length": 693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "320  Machine Learning in Production\n\nFinally, complete the process by selecting the configuration file type and location. In this case, the Cloud Build configuration file (YAML or JSON) option is selected for Configuration Type and the cloudbuild.yaml file location from the repository is provided, as shown in the following figure:\n\nFigure 12.15: Creating trigger in Cloud Build – selecting configuration file type and its location\n\nNow, any update pushed to the Cloud Source Repository will trigger the pipeline. To see the build history, go to the History section from the side panel or view it through the link from the dashboard.\n\nThe following figure shows the build history with Status, Build ID, the Source - loan_pred, that is, the Cloud Source Repository in this case, and Ref - the master branch of that repository in this case. It also shows the Trigger type, Trigger name, and Duration, that is, the time taken to complete the build.\n\nFigure 12.16: Build history",
      "content_length": 976,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "Deploying ML Models on Google Cloud Platform  321\n\nYou can see the live build logs for each step. It helps you to debug the errors that occurred during the build process. In the last step, you should see GKE deployment details, such as status, name, and URLs to access the app.\n\nThe following figure shows the build summary with the build ID and duration or time taken by each step:\n\nFigure 12.17: Build details – Build log\n\nThe dashboard of the Cloud Build display summarizes details, such as the status of the Latest Build, Duration, Build History of Pass-Fail builds, the Average Duration of the build, and Pass-Fail%, as shown in the following figure:\n\nFigure 12.18: Dashboard - Cloud Build",
      "content_length": 695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "322  Machine Learning in Production\n\nAfter successful deployment, you should see the ML app up and running in GKE, as shown in the following figure:\n\nFigure 12.19: ML app running in GKE\n\nGoogle Kubernetes also offers an app monitoring service. Go to the service ingress URL in GKE. This includes resource usage, details, events, and logs. The following figure shows the overview of the monitoring of deployed apps:\n\nFigure 12.20: Monitoring mlapp",
      "content_length": 447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "Deploying ML Models on Google Cloud Platform  323\n\nYou have now learned how to create a simple CI/CD pipeline using GitHub to deploy ML apps on the GCP platform. However, you can modify it as per business and application requirements.\n\nConclusion In this chapter, the ML app was deployed on Google Kubernetes Engine (GKE) with CI/CD pipeline. First off, you created a GCP account and created code files, including cloudbuild.yaml and Kubernetes manifest files for GCP deployment, and pushed them to Cloud Source Repositories (CSR). Then, you created a Container Registry to push-pull Docker container images and a Kubernetes cluster ml-kube to deploy an ML app. Finally, you enabled Cloud Build API and created an AutoDeploy trigger by integrating Cloud Source Repositories (CSR) to automate ML app deployment.\n\nIn the next chapter, you will learn to deploy an ML app on Amazon Web Services (AWS).\n\nPoints to remember\n\nKubernetes cluster can be shared among multiple projects. •\t Gmail email ID is required to create an account on GCP. •\t Cloud Build is a serverless platform that allows you to automate build, test, and deploy containers quickly.\n\nCloud builders are container images with common languages and tools installed in them.\n\nThe same labels should be used in service.yaml and deployment.yaml files for successful communication between service and deployment.\n\nMultiple choice questions\n\n1. _________ is CI/CD service provided by Google.\n\na) Cloud Build\n\nb) BigQuery\n\nc) Virtual Private Cloud (VPC)\n\nd) Pub/Sub",
      "content_length": 1522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "324  Machine Learning in Production\n\n2. Manifest file’s recommended file format is _________.\n\na)\n\n.git\n\nb) YAML\n\nc) Python\n\nd) Shell script\n\nAnswers 1. a\n\n2. b\n\nQuestions\n\n1. What is the location of the Cloud builder’s pre-built container images?\n\n2. What is the role of a Container Registry?\n\n3. What is container orchestration?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 532,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "Deploying ML Models on Amazon Web Services  325\n\nChapter 13 Deploying ML Models on Amazon Web Services\n\nIntroduction Cloud computing refers to storing and accessing data and applications over the internet. Usually, data is stored on a remote server. Simply put, you can access data and applications from anywhere on the internet without worrying about the physical or on-premises infrastructure, which makes cloud computing more popular among organizations.\n\nAmazon Web Services (AWS) helps solve on-premises infrastructure issues. AWS can spin up 100-1000 servers in a few minutes and, extra or unused servers will be removed. It is easy to scale applications with AWS. You can add more storage for applications or data. AWS helps focus on building and deploying applications on the cloud without worrying about setting up infrastructure from scratch.\n\nRefer to the previous chapters if you need to study the concepts discussed, such as packaging ML models, FastAPI, Docker, and CI/CD pipeline.\n\nStructure In this chapter, the following topics will be covered:\n\n\n\nIntroduction to Amazon Web Services (AWS)",
      "content_length": 1107,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "326  Machine Learning in Production\n\nAWS Elastic Container Registry (ECR) •\t AWS CodeCommit •\t Amazon Elastic Container Service (ECS) •\t AWS CodeBuild •\t Application Load Balancer (ALB) •\t Deploy web-based ML app to Elastic Container Service (ECS) with service and Application Load Balancer (ALB)\n\nAWS CodePipeline •\t Build an automated CI/CD CodePipeline to deploy a web-based ML app on Amazon Web Services (AWS)\n\nObjectives After studying this chapter, you should be able to deploy an ML model on Amazon Elastic Container Service (ECS) without the need to integrate any external tool, service, or platform except AWS. You will create a remote Git repository on AWS using AWS CodeCommit and Amazon Elastic Container Service (ECS) cluster to run scalable ML apps. You will also learn to integrate Application Load Balancer (ALB) with Amazon Elastic Container Service (ECS) for routing requests coming from the external world. Moving on, you will integrate the service port with the Docker port via port mapping, create a security group for Application Load Balancer (ALB) and learn to push the Docker container image to AWS Elastic Container Registry (ECR). You should be able to create a fully automated CI/CD pipeline with AWS CodePipeline, AWS CodeBuild, AWS Elastic Container Registry (ECR), Application Load Balancer (ALB), and Amazon Elastic Container Service (ECS) after completing this chapter.\n\nIntroduction to Amazon Web Services (AWS) Amazon Web Services (AWS) is a cloud infrastructure where you can host applications. In 2006, AWS started offering IT services to the market in the form of web services. AWS is one of the leading cloud service providers.\n\nAWS compute services Amazon Web Services (AWS) offers compute services for managing workloads that comprise many servers or instances.",
      "content_length": 1803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "Deploying ML Models on Amazon Web Services  327\n\nHere are some of the widely used compute services offered by AWS that can be used as per the business or application requirements. These services will be discussed later in the chapter.\n\nAmazon Elastic Compute Cloud (EC2) Amazon EC2 is a virtual machine that represents a remote server. Amazon EC2 service is grouped under Infrastructure-as-a-Service (IaaS). Amazon EC2 enables applications with resizable computing capacity, and these EC2 machines are known as instances. You can create multiple instances with different computing sizes. You can even upgrade the ram, vCPU, and so on after creation, so you do not need to recreate a new instance and configure it again. This is why the elastic term is used.\n\nAmazon Elastic Container Service (ECS) Amazon Elastic Container Service (ECS) enables you to deploy, scale, and manage containerized applications. It manages containers and enables developers to run containerized applications across the cluster of EC2 instances. However, it is not based on Kubernetes. Amazon ECS is free, meaning you don’t need to pay for the ECS cluster. However, additional charges are to be paid for EC2 instances running in ECS tasks. There are mainly two ways to launch an ECS clusters:\n\nFargate Launch •\t EC2 Launch\n\nAmazon ECS is a technology owned exclusively by AWS. ECS easily integrates with AWS Application Load Balancer (ALB) and Network Load Balancer (NLB).\n\nAmazon Elastic Kubernetes Service (EKS) Amazon Elastic Kubernetes Service (EKS) is a Kubernetes service backed by AWS, which enables you to build Kubernetes clusters on AWS without manually installing Kubernetes on EC2. Amazon Elastic Kubernetes Service (EKS) allows you to manage or orchestrate containers in a Kubernetes environment. You need to pay for the EKS cluster, with additional charges for EC2 instances running inside the Kubernetes pod. Amazon EKS service set up and manages the Kubernetes control plane for you.\n\nIt is a good choice if you are looking for multi-cloud functionality and additional features compared to the Amazon Elastic Container Service (ECS).\n\nAmazon Elastic Container Registry (ECR) Amazon Elastic Container Registry (ECR) allows developers to store, share and deploy Docker images on Amazon ECS. It provides security to images stored in",
      "content_length": 2322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "328  Machine Learning in Production\n\nit. Amazon Elastic Container Registry (ECR) is integrated with Amazon ECS. It is similar to the Docker Hub container registry but is managed by AWS. It allows you to store private Docker container images in it.\n\nAWS Fargate Simply put, AWS Fargate is a serverless compute for containers. You pay for the usage per minute for the resources used by containers like virtual CPU (vCPU) and memory. AWS Fargate is compatible with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\n\nAWS Lambda AWS Lambda is simple and less expensive. It is a serverless and event-driven compute service; it is a Function as a Service (FaaS). That means you don’t need to manage servers or clusters. Events could be any simple activity, such as a user clicking on links to get the latest news. The term Lambda is supposed to be borrowed from functions of lambda calculus and programming. Mostly, it comprises three components:\n\nA function: It is the actual code to perform the task. •\t A configuration: It dictates the execution of a function. •\t An event source: This is the event that triggers the function. However, this component is optional.\n\nAWS Lambda is a good choice for event-driven programming or when you need to access several services.\n\nAmazon SageMaker AWS also provides a machine learning platform as a service, that is, Amazon SageMaker. It removes the overhead of managing and maintaining servers manually. It also reduces the time and cost of machine learning model deployment on the AWS cloud.\n\nAmazon SageMaker is a cloud-based machine learning platform that enables data scientists and developers to build, train, tweak, and deploy machine learning models in production environments. It uses Amazon Simple Storage Service (S3) to store the data and comes with over 15 most commonly used built-in machine learning algorithms for training the data. It deploys the ML models to SageMaker endpoints. Amazon SageMaker uses the Amazon Elastic Container Registry (ECR) to store container images.",
      "content_length": 2063,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "Deploying ML Models on Amazon Web Services  329\n\nYou must have got the gist of AWS and its services. You are going to deploy a Machine Learning model to the Amazon Elastic Kubernetes Service (EKS). After this chapter, you will be comfortable working on AWS for ML model deployment.\n\nSet up an AWS account First of all, set up an AWS account; however, this is a one-time activity. It is ready to use immediately on log in.\n\nThe AWS Free Tier provides customers the ability to explore and try out AWS services free of charge up to specified limits for each service. The Free Tier consists of three different types of offerings, a 12-month Free Tier, an Always Free offer, and short-term trials.\n\nStep 1: Create or log in to the existing AWS account\n\nCreate an AWS account (if you don’t have one) and sign in. A free trial account can be created on the AWS platform at https://aws.amazon.com/free.\n\nFirst, click on the Create a Free Account button. Next, provide the login details, such as email ID, password, and AWS account name. Then, select the account type (Professional or Personal) and provide contact information. After that, issue PAN and payment details. Finally, complete the verification process on the identity verification page.\n\nAfter a few minutes, the account will be activated and ready to use. Select a plan as per your requirements.\n\nStep 2: Create an IAM user and provide the required permissions\n\nAfter signing in to the AWS Management Console, open the IAM console - https:// console.aws.amazon.com/iam/. IAM stands for Identity and Access management. Next, click on Users in the navigation pane and choose Add users. Then, provide the user name for that user and choose the access type. After that, select the existing policies from the attached existing policies or create a new one for that user. In this scenario, Administrator access is given to the IAM user. Tags are optional. Finally, review and complete the process. Do not forget to save the user details like Access key ID, Secret access key, and user name. In this case, an IAM user will be used to execute all the tasks. Hit the Create user button and complete the process.",
      "content_length": 2157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "330  Machine Learning in Production\n\nThe following figure shows that the IAM user is created:\n\nFigure 13.1: AWS IAM user\n\nThe Secret access key is available only when it is created, so you need to download and save it. If lost, then you have to create another one.\n\nCreating access keys for the AWS account root user is not recommended unless required. Rather, create one or more IAM users with the required permissions to execute the tasks.\n\nStep 3: Install and configure AWS CLI on the local machine\n\nAWS Command-Line Interface (CLI) is a tool that allows the management of several AWS services. It is an open-source tool that enables you to interact with AWS services through commands. First, download and install it, then configure it with AWS credentials. To get started with AWS CLI, follow this link https://docs.aws. amazon.com/cli/latest/userguide/getting-started-install.html.\n\nFor Linux, download the installation file using the curl command, where the -o option specifies the filename that the downloaded package is written to: curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n\nUnzip the installer as follows: unzip awscliv2.zip\n\nThe installation command uses a file name install from unzipped aws directory: sudo ./aws/install\n\nVerify the installation by checking the version of AWS CLI using the following command:",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "Deploying ML Models on Amazon Web Services  331\n\naws --version\n\nRestart the terminal if the aws command cannot be found.\n\nNext, configure the AWS CLI with AWS IAM credentials, as shown in the following figure:\n\nFigure 13.2: AWS CLI configuration\n\nAWS CodeCommit In this case, the Git repository, that is, AWS CodeCommit, is used; however, other remote Git repositories like GitHub can be used. AWS CodeCommit is a private Git repository hosted on AWS. It allows you to create multiple Git repositories and supports a standard set of Git commands, such as push, pull, clone, and log. AWS CodeCommit can be added to a local Git repository as a remote. It allows collaboration and provides security; however, it is a standard practice not to store any personal or sensitive data in it. The good part is that AWS CodeCommit is available to both new and existing users for free up to a certain limit. It does not expire even after 12 months of free tier usage. It is free for the first 5 active users, which includes 1,000 repositories per account, 50 GB of storage per month, and 10,000 Git requests per month.\n\nGo to CodeCommit from Services:\n\nServices | All Services | Developer Tools | CodeCommit\n\nCreate a new repository in CodeCommit and then provide the repository name; the description is optional. Repository names are included in the URL of that repository.\n\nOnce the repository is created, specific permissions need to be provided; then, create Git credentials to access the CodeCommit repository from the local machine. Go to the IAM console, select Users from the navigation pane and choose the user for which CodeCommit is to be configured. Then, attach the AWSCodeCommitPowerUser policy from the policies list and complete the process. In this case, the IAM user has admin privileges, so there is no need to attach the above-mentioned policy to the IAM user.\n\nChoose the same IAM user and locate HTTPS Git credentials for AWS CodeCommit. Next, select that user and click on the Generate credentials button. It will populate Git credentials, that is, username and password. Password can be seen and",
      "content_length": 2108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "332  Machine Learning in Production\n\ndownloaded, save it for future use. This password cannot be recovered later on; however, it can be regenerated.\n\nFigure 13.3: CodeCommit–Git credentials\n\nAfter creating the code commit repository, clone it to the local machine. Copy project code files into the directory on the local machine. Re-initialize the Git repository using the git init command. Finally, commit and push changes to the CodeCommit repository from the local machine.\n\nLet’s consider the scenario of loan prediction where the problem statement is to predict whether a customer’s loan will be approved. Feel free to implement hyperparameter tuning and tweak the model.\n\nFirst, create an installable package of ML code, and then build a web app using FastAPI; then, create the tests and dependencies file. After that, create Dockerfile and docker-compose.yml files. Finally, create the configuration file buildspec.yaml for AWS deployment.\n\nNote: For code files, refer to the previous chapters. You should create buildspec. yaml file and update the tox.ini file for the current scenario. You need to update src\\tests\\test_predict.py, src\\prediction_model\\predict.py, and start.sh for continuous training.\n\nYou access the code repository at https://github.com/suhas-ds/AWS-CICD",
      "content_length": 1284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "Deploying ML Models on Amazon Web Services  333\n\nThe following directory structure shows the code files for a CI/CD pipeline:\n\n.\n\n├─ src\n\n│ ├─ prediction_model\n\n│ │ ├─ config\n\n│ │ │ ├─ __init__.py\n\n│ │ │ └─ config.py\n\n│ │ ├─ datasets\n\n│ │ │ ├─ __init__.py\n\n│ │ │ ├─ test.csv\n\n│ │ │ └─ train.csv\n\n│ │ ├─ processing\n\n│ │ │ ├─ __init__.py\n\n│ │ │ ├─ data_management.py\n\n│ │ │ └─ preprocessors.py\n\n│ │ ├─ trained_models\n\n│ │ │ ├─ __init__.py\n\n│ │ │ └─ classification_v1.pkl\n\n│ │ ├─ VERSION\n\n│ │ ├─ __init__.py\n\n│ │ ├─ pipeline.py\n\n│ │ ├─ predict.py\n\n│ │ └─ train_pipeline.py\n\n│ ├─ tests\n\n│ │ ├─ pytest.ini\n\n│ │ └─ test_predict.py\n\n│ ├─ MANIFEST.in\n\n│ ├─ README.md\n\n│ ├─ requirements.txt\n\n│ ├─ setup.py\n\n│ └─ tox.ini\n\n├─ .gitignore\n\n├─ Dockerfile",
      "content_length": 741,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "334  Machine Learning in Production\n\n├─ README.md\n\n├─ buildspec.yaml\n\n├─ docker-compose.yml\n\n├─ main.py\n\n├─ pytest.ini\n\n├─ requirements.txt\n\n├─ runtime.txt\n\n├─ start.sh\n\n├─ test.py\n\n└─ tox.ini\n\ntox.ini\n\nThe tox is a free and open-source tool used for testing Python packages or applications. It creates the virtual environment and installs the required dependencies in it; finally, it runs the tests for that Python package. You need to update the tox.ini file as shown in the following code.\n\n1. [tox]\n\n2. envlist = my_env\n\n3. skipsdist=True\n\n4.\n\n5. [testenv]\n\n6. install_command = pip install {opts} {packages}\n\n7. deps =\n\n8. -r requirements.txt\n\n9.\n\n10. setenv =\n\n11. PYTHONPATH=src/\n\n12.\n\n13. commands=\n\n14. pip install requests\n\n15. pytest -v src/tests/ --junitxml=pytest_reports/Prediction_ test_report.xml\n\n16. pytest -v test.py --junitxml=pytest_reports/API_test_report. xml",
      "content_length": 883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "Deploying ML Models on Amazon Web Services  335\n\nThe skipsdist=True flag indicates not to perform a packaging operation. It means the package will not be installed in a virtual environment before performing any test. Set PYTHONPATH to the src/ directory where Python package files are placed. In the commands section, pytest commands will be executed, and the results of the tests will be exported in .xml files in the pytest_reports directory.\n\n.\\src\\prediction_model\\predict.py\n\nHere, add the train_accuracy function to the predict.py file that will return the accuracy score.\n\n1. def train_accuracy(input_data):\n\n2. \"\"\" Checking accuracy score of training data \"\"\"\n\n3.\n\n4. # Read Data\n\n5. data = pd.DataFrame(input_data)\n\n6. y_train = np.where(data['Loan_Status']=='Y', 1, 0).tolist()\n\n7.\n\n8. # Prediction\n\n9. prediction = _loan_pipe.predict(data[config.FEATURES])\n\n10. y_pred = prediction.tolist()\n\n11. score = accuracy_score(y_train,y_pred)*100\n\n12. return score\n\n.\\src\\tests\\test_predict.py\n\nNow, add the test_score function to the test_predict.py file that will test whether the training accuracy score is between 70 to 95. You can change this range.\n\n1. from prediction_model.predict import train_accuracy\n\n2.\n\n3. def test_score():\n\n4. ''' This function will check the accuracy score of training data '''\n\n5. data = load_dataset(file_name=config.TRAIN_FILE)\n\n6. score = train_accuracy(data)\n\n7. assert 70 <= score <= 95\n\nAfter a successful push from the local terminal, code files will be displayed in the cloud source repository.",
      "content_length": 1539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "336  Machine Learning in Production\n\nContinuous Training With a CI/CD pipeline, you can add Continuous Training (CT) stage. So when the CI/CD pipeline will run, it will also train the model on the latest available data as per the configuration settings.\n\nstart.sh\n\nTo train the model on the latest data, you need to add the following command to the start.sh file: python app/src/prediction_model/train_pipeline.py\n\nThis will run the train_pipeline.py file and generate the latest pickle file of the trained model. This file will first install the package, then train the model, and finally, run the FastAPI app.\n\n1. #!/bin/bash\n\n2.\n\n3. pip install app/src/\n\n4. python app/src/prediction_model/train_pipeline.py\n\n5. python app/main.py\n\nAmazon Elastic Container Registry (ECR) Amazon Elastic Container Registry (ECR) is a container registry service managed by AWS. It stores, manages, and provides security to private container images. If these container images are to be accessed through code or other services, such as CodeBuild, then container registry access needs to be provided to the specified service roles.\n\nGo to Elastic Container Registry from Services:\n\nServices | All Services | Containers | Elastic Container Registry\n\nCreate a repository with the name mlapp-cicd. By default, the visibility of the repository will be private; it means access will be managed by IAM and pre-defined permissions will be granted to the repository policy. The repository name should be concise yet meaningful, that is, it should be based on the content of the repository. The name must start with a letter and can only contain lowercase letters, numbers, hyphens, underscores, periods, and forward slashes.\n\nThen, go to the repository and click on the View push commands button in the top- right corner. It will display the push commands and instructions you have to follow while pushing a Docker container image from a local machine using AWS CLI. First,",
      "content_length": 1948,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "Deploying ML Models on Amazon Web Services  337\n\nyou need to log in to AWS ECR using the command given in the pop-up window. Next, build a Docker image on a local machine using the docker build command. If the Docker image is already built, the Docker build step can be skipped. Then, tag the Docker image and push the image to the AWS ECR repository using the commands given in the pop-up window.\n\nThe following figure shows the push commands for the mlapp-cicd repository:\n\nFigure 13.4: Amazon ECR – Push commands\n\nNote: It is recommended to install and configure the latest version of AWS CLI and Docker on a local machine.\n\nDocker Hub rate limit From November 20, 2020, anonymous and free user are limited to 100 and 200 container image pull requests every 6 hours. After the specified limit, it will throw an error that contains ERROR: toomanyrequests: Too Many Requests. or You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limits. To increase your pull rate limits, you can upgrade your account to a Docker Pro or Team subscription. Docker Pro and Docker Team accounts enable 5,000 pulls in 24 hours from Docker Hub.",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "338  Machine Learning in Production\n\nTo avoid this error, create a new repository on AWS ECR for the current scenario. Next, pull the base Python 3.7 image from the Docker Hub into the local machine or VM. Then, push that base Python 3.7 image to the newly created AWS ECR repository. In the current scenario, the base image is being maintained in a separate repository, as shown in the following figure:\n\nFigure 13.5: Amazon ECR–Repositories\n\nUse this image path in the Dockerfile so that when the docker build command runs for Dockerfile; it will pull the base Python 3.7 images from AWS. For this, you need to update Dockerfile as follows:\n\n1. # Pull base Python:3.7 image from AWS ECR repository\n\n2. ARG REPO=692601447418.dkr.ecr.us-west-2.amazonaws.com\n\n3.\n\n4. FROM ${REPO}/python:3.7\n\n5.\n\n6. COPY ./start.sh /start.sh\n\n7.\n\n8. RUN chmod +x /start.sh\n\n9.\n\n10. ENV PYTHONPATH \"${PYTHONPATH}:app/src/\"\n\n11.",
      "content_length": 909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "Deploying ML Models on Amazon Web Services  339\n\n12. COPY . /app\n\n13.\n\n14. RUN chmod +x /app\n\n15.\n\n16. # Exposing the port that uvicorn will run the app on\n\n17. ENV PORT=8000\n\n18. EXPOSE 8000\n\n19.\n\n20. RUN pip install --upgrade pip\n\n21.\n\n22. RUN pip install --no-cache-dir --upgrade -r app/requirements.txt\n\n23.\n\n24. CMD [\"./start.sh\"]\n\nIn the current scenario, the python:3.7-slim-buster image is built and tagged with 3.7, and then it is pushed to AWS ECR for reusability. This way, every time, the python:3.7-slim-buster image will be pulled from AWS ECR instead of the Docker Hub to avoid any rate limit error.\n\nAWS CodeBuild A CodeBuild is a fully managed Continuous Integration (CI) service backed by AWS infrastructure that allows developers to automate the build, test, and deploy containers or packages quickly. It works on an on-demand or pay-as-you-go model, that is, it charges users based on the minute for the compute resources they have used.\n\nThe CodeBuild config file type is YAML. This file contains a series of phases and commands to execute the build specified by the developer.\n\nbuildspec.yaml\n\nIn this case, the buildspec.yaml file first installs the tox package in the install phase. Next, it will run the tests using tox and log in to AWS ECR in the pre_build phase. Then, the buildspec.yaml file will build a Docker container image with the latest tag in the build phase and push the image to the AWS ECR repository in the post_build phase. Finally, specify the details for pytest reports, such as files, a base directory, and file format. Also, provide the filename to be stored as an artifact in the S3 bucket.",
      "content_length": 1638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "340  Machine Learning in Production\n\n1. version: 0.2\n\n2.\n\n3. phases:\n\n4. install:\n\n5. runtime-versions:\n\n6. python: 3.7\n\n7. commands:\n\n8. - pip install tox\n\n9. pre_build:\n\n10. commands:\n\n11. - echo Running test...\n\n12. - tox\n\n13. - echo Logging into Amazon ECR...\n\n14. - aws --version\n\n15. - aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 692601447418.dkr.ecr.us- west-2.amazonaws.com\n\n16. - REPOSITORY_URI=692601447418.dkr.ecr.us-west-2.amazonaws. com/mlapp-cicd\n\n17. - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)\n\n18. - IMAGE_TAG=${COMMIT_HASH:=latest}\n\n19. build:\n\n20. commands:\n\n21. - echo Build started on `date`\n\n22. - echo Building the Docker image...\n\n23. - docker build -t $REPOSITORY_URI:latest .\n\n24. - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_ TAG\n\n25. post_build:\n\n26. commands:\n\n27. - echo Build completed on `date`\n\n28. - echo Pushing the Docker images...",
      "content_length": 957,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "Deploying ML Models on Amazon Web Services  341\n\n29. - docker push $REPOSITORY_URI:latest\n\n30. - docker push $REPOSITORY_URI:$IMAGE_TAG\n\n31. - echo Writing container image definitions file...\n\n32. - printf '[{\"name\":\"mlapp-cicd\",\"imageUri\":\"%s\"}]' $REPOSITORY_URI:$IMAGE_TAG > imagedefinitions.json\n\n33. reports:\n\n34. pytest_reports:\n\n35. files:\n\n36. - Prediction_test_report.xml\n\n37. - API_test_report.xml\n\n38. base-directory: pytest_reports/\n\n39. file-format: JUNITXML\n\n40. artifacts:\n\n41. files: imagedefinitions.json\n\nIn the preceding file, mainly three supported builder images are used.\n\nYou can see, python:3.7 is the publicly available image used for testing the code using tox. If any image is to be used from the Docker Hub, then simply provide the image name in single quotes. However, if an image is from other registries, then the full registry path needs to be specified in single quotes. The args field of a build phase accepts a list of arguments and passes them to the image referenced by the name field.\n\nTo create a build, CodeBuild is used. Search and select CodeBuild from developer tools and choose Build projects from the Build section. Click on the Create build project button to create a new one. First off, under the Project configuration, provide the project name.",
      "content_length": 1292,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "342  Machine Learning in Production\n\nIn this case, the project name mlapp-cicd is provided, as shown in the following figure:\n\nFigure 13.6: CodeBuild–Project configuration\n\nNext, under the Source section, select the Source provider from the dropdown list and choose the subsequent details, such as Repository and Branch. Source providers contain details and code files to be used as input source code for the build project. In the current scenario, choose AWS CodeCommit as a Source provider, as shown in the following figure. However, other source providers can be selected instead of AWS CodeCommit, such as BitBucket, GitHub, and S3.",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "Deploying ML Models on Amazon Web Services  343\n\nFigure 13.7: CodeBuild–Source\n\nThen, under the Environment section, choose the Managed Image option and choose the operating system as Ubuntu from the dropdown. It provides other operating systems such as Windows Server and Amazon Linux. In the current scenario, aws/ CodeBuild/standard:4.0 image is chosen. The remaining selections can be kept as default. Make sure you select the Privileged checkbox, which allows you to build Docker images.",
      "content_length": 493,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "344  Machine Learning in Production\n\nThe following figure shows the selection made for the Environment section:\n\nFigure 13.8: CodeBuild–Environment\n\nAfter that, under the Buildspec section, choose Use a buildpsec file, as shown in the following figure:\n\nFigure 13.9: CodeBuild–Buildspec",
      "content_length": 287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "Deploying ML Models on Amazon Web Services  345\n\nFinally, under the Logs section, select Cloudwatch logs. This will upload build output logs to cloudwatch. This enables you to analyze the logs and output generated after building the project. However, this is optional. At the bottom of the page, click on the Create build project button and complete the configuration process.\n\nAttach container registry access to CodeBuild’s service role Now, you need to provide access to the service role of CodeBuild so that it can build the Docker container images. For this, go to IAM management console | Roles and select the CodeBuild service role. In the current scenario, it is CodeBuild- mlapp-cicd-service-role. Click on it, and the summary page will open. Click on Attach policies, and then search for EC2ContainerRegistry and select AmazonEC2ContainerRegistryFullAccess. It provides administrative access to Amazon ECR resources. If the required access is not provided, CodeBuild will be unable to build Docker images.\n\nThe following figure shows container registry access allowed for CodeBuild’s service role:\n\nFigure 13.10: CodeBuild–Service role\n\nNow, go to the main page of CodeBuild, and let’s manually execute the build phase by hitting the Start build button in the top-right corner. Build status will be displayed as In Progress. Logs are available under the Build logs tab. By default, it will show the last 1000 lines of the build log. After completion of the build phase, the latest container image should be available in Amazon Elastic Container Registry",
      "content_length": 1564,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "346  Machine Learning in Production\n\n(ECR). Also, the test report should be available under the Reports tab, as shown in the following figure:\n\nFigure 13.11: CodeBuild–Test report\n\nAmazon Elastic Container Service (ECS) Amazon Elastic Container Service (ECS) is a container orchestration tool used for managing and running Docker containers. Amazon Elastic Container Service (ECS) is a fully managed container management service offered by AWS. In the current scenario, ECS with the Fargate model will be used. It will take care of managing the cluster and load balancing. It will also make sure the application is up and running.\n\nLet’s understand the terms used in the Amazon Elastic Container Service (ECS):\n\nTask Definition: The task definition allows you to specify which Docker image to use, which ports to expose, how much CPU and memory to allot, how to collect logs, and how to define environment variables. This is a blueprint that dictates how a Docker container should launch.\n\nTask: A task resembles an instance of task definition. It can be created independently, that is, without an Amazon ECS cluster. This will spin up a container with an application running in it. It will not replace itself automatically if it stops or fails due to some error.",
      "content_length": 1264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "Deploying ML Models on Amazon Web Services  347\n\nService: A Service is responsible for creating and maintaining the desired number of tasks up and running all the time. If any task fails or is stopped due to some error, then the ECS Service will replace that task with a new one. It refers to the task definition file to create tasks. •\t Cluster: It is a logical group of container instances. •\t Container: This is the Docker container created during task instantiation.\n\nThe following figure resembles the Amazon Elastic Container Service (ECS) architecture.\n\nFigure 13.12: Amazon Elastic Container Service (ECS)–Architecture\n\nAWS ECS deployment models AWS ECS Deployment model can be chosen as per your requirement. Let’s look at the models AWS Elastic Container Service (ECS) mainly offers for cluster deployment.\n\nEC2 instance EC2 (Elastic Compute Cloud) is a virtual machine in the cloud. First, configure and deploy EC2 instances in the cluster to run the containers. It provides more granular control over the instances. You can choose the instances as per requirement.\n\nThis model is a better choice if you want to do the following:\n\nRun containerized applications continuously",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "348  Machine Learning in Production\n\nHave better control over the auto-scaling configuration •\t Use a Classic Load Balancer (CLB) to distribute workloads •\t Use Graphical Processing Unit (GPU) •\t Use Elastic Block Storage (EBS) •\t Deploy large and complex applications\n\nFargate This is a serverless pay-as-you-go model. You will be charged based on the computing capacity selected and the time of usage. ECS with Fargate is a container orchestration tool used for running Docker based containers without having to manage the underlying infrastructure.\n\nThis model is a better choice if you want to do the following:\n\nRun the task occasionally or for a short period •\t Containerized applications should be able to handle sudden spikes in incoming traffic\n\nUse application and network load balancers to distribute workloads •\t Save time from different configurations, regular maintenance, and security management\n\nNote: In this chapter, classic UI is used. By default, you might see a new UI; however; you can switch back to the classic UI.\n\nGo to Elastic Container Service from Services:\n\nServices | All Services | Containers | Elastic Container Service\n\nFirst off, go to the cluster page and create an ECS cluster by clicking on the Create Cluster button. AWS provides templates for creating clusters to simplify the process of cluster creation. In the current scenario, Networking only (AWS Fargate) is selected as the type of instance. On the next screen, under Cluster configurations, the Cluster name is to be provided. In this scenario, it is mlapp-cluster, as shown in the following figure:",
      "content_length": 1597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "Deploying ML Models on Amazon Web Services  349\n\nFigure 13.13: Amazon Elastic Container Service (ECS)–Cluster configuration\n\nThen, click on the Create button, and it will launch the ECS cluster. You will see ECS cluster is created with the type Fargate, but no instances are running.\n\nTask definition After that, create a task definition. Go to the Task Definition page and click on Create new Task Definition. Choose the type as Fargate in step 1. In step 2, provide the task definition name. In this case, it is mlapp-cicd. Select the Task role ecsTaskExecutionRole from the dropdown and select the Operating system family as Linux, as shown in the following figure:\n\nFigure 13.14: Amazon Elastic Container Service (ECS) –task definition",
      "content_length": 740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "350  Machine Learning in Production\n\nKeep the Task execution role as default, that is, ecsTaskExecutionRole. Under the Task size section, choose the required memory and CPU to be allotted for tasks based on application requirements or use cases. A Task memory (GB) of 1GB and Task CPU (vCPU) of 0.25 vCPU is chosen for the current scenario, as shown in the following figure:\n\nFigure 13.15: Task definition–Task Size\n\nFinally, add the container details, such as container name, image URI to be taken from AWS ECR, and memory limits (in MiB) for containers. Hard and soft limits correspond to the memory and memoryReservation parameters in task definitions. The Port mappings parameter is important. Here, you need to provide a port used by the container to run the application and expose it in the Dockerfile. In the current scenario, the container port is 8000. You can leave the rest of the configurations as they are and add a container.\n\nIn the end, click on Create and complete step 2 of the task definition.\n\nRunning task with the task definition The task can be run independently using the task definition created earlier.\n\nThe following figure shows the various options available for task definition. Now, to run the task, choose the first option, that is, Run Task.\n\nFigure 13.16: Running an independent task",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "Deploying ML Models on Amazon Web Services  351\n\nNext, choose the launch type. In the current scenario, FARGATE is chosen. Choose mlapp-cluster from the Cluster dropdown.\n\nAmazon VPC and subnets Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined. These virtual networks are similar to the traditional networks used in data centers. The merit of a virtual network is that it comes with scalable infrastructure managed by AWS. Each VPC network consists of one or more IP address ranges called subnets.\n\nNext, under VPC and security groups, choose cluster VPC and choose subnets. In the current scenario, choose 2a and 2b; however, the remaining 2c and 2d can also be chosen. Make sure the Auto-assign public IP is ENABLED. This will allow you to automatically assign available public IPs to access the ML app from anywhere.\n\nThen, edit the default security group, and the security groups window will show up. Here, create a new security group that will be used in a later stage. Choose Create new security group from Assigned security groups and provide the name for the security group as mlapp-sg, where sg stands for the security group. After that, two ports are allowed, that is, port 80 and port 8000, for the communication of containerized applications with the external world.\n\nAfter that, add HTTP port 80 and Custom TCP port 8000, on which the containerized application is running. Choose Anywhere in the source option for both ports.\n\nFinally, in the bottom-right corner, hit the Run Task button. It will take some time to get the task up and running. The task’s status will be shown as RUNNING in the Tasks tab, as shown in the following figure:\n\nFigure 13.17: Running an independent task–task created",
      "content_length": 1780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "352  Machine Learning in Production\n\nThe public IP of the task is 54.191.99.95. It means an ML app can be accessed with this public IP from anywhere on the internet.\n\nThe issue with running a task independently is that if the task fails or stops due to any reason, then the application also stops; this makes the application inaccessible to the external world. To overcome this issue, ECS service can be used. Don’t forget to stop the running task. With ECS service, multiple tasks can be created.\n\nLoad balancing In parallel computing, load balancing is a process of distributing application traffic across different available computes or resources in order to make overall request handling more efficient. The main objective of the technique is to avoid any downtime for end users or customers.\n\nThe following figure resembles the high-level architecture of an Application Load Balancer (ALB):\n\nFigure 13.18: Application Load Balancer (ALB)\n\nAn application load balancer can only be added to a service while creating a service. Next, go to the load balancing part and create the target groups, which takes care of dynamically adding the IP addresses that get created while creating the tasks.\n\nGo to load balancer from Services:\n\nServices | All Services | Developer Tools | Load balancing\n\nThe following figure shows the components of Load Balancing:",
      "content_length": 1353,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "Deploying ML Models on Amazon Web Services  353\n\nFigure 13.19: AWS-Load Balancing\n\nLet’s understand the role and configuration of the two components of Load Balancing.\n\nTarget group With a load balancer, a single DNS name is enough to access the containerized application. A load balancer will take the user requests on port 80 and route them to tasks with dynamic IP through the service. The target group will ensure connectivity between the load balancer and tasks with dynamic IP through service.\n\nGo to the target group and create a new target group by hitting the Create target group button.\n\nCreating a target is a three-step process:\n\nSpecify group details •\t Register targets •\t Review IP targets to include in your group\n\nStep 1: Specify group details\n\nA configuration setting cannot be changed after the creation of a target group. In this step, the target type is IP addresses; its features are listed in the following figure:\n\nFigure 13.20: Target Groups–Specify group details",
      "content_length": 989,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "354  Machine Learning in Production\n\nNext, provide the Target group name as mlapp-tg. The Target group name should not start or end with a hyphen (-). Then, provide port 8000 with HTTP type Protocol as the application is exposed to port 8000. Leave the rest of the selections as they are. This configuration is shown in the following figure:\n\nFigure 13.21: Target Groups–Basic configuration\n\nUnder the health check section, by default, the target group will check the root or index path (‘/’), or you can specify a custom path if preferred. The target group will periodically send the requests to this endpoint of the application to check whether it is healthy.\n\nStep 2: Register targets\n\nIn this step, IP addresses and ports can be specified manually from the selected network. By default, the VPC network will be selected. Remove the pre-selected IP, as this part is optional. This can be configured later on. The following figure shows this step:",
      "content_length": 950,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "Deploying ML Models on Amazon Web Services  355\n\nFigure 13.22: Target Groups–Register targets\n\nAfter that, specify the ports for routing to this target. In the current case, the application’s port 8000 is specified, as shown in the following figure:\n\nFigure 13.23: Target Groups – Review targets and ports for routing to target",
      "content_length": 328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "356  Machine Learning in Production\n\nStep 3: Review IP targets to include in your group\n\nAt this point, you can repeat steps 1 and 2 if you want to add additional IP targets. However, you have not specified any IP address is specified in step 2, so go ahead and hit the Create target group button at the bottom. It will take you back to the main page of the targets group and display a successful message, as shown in the following figure:\n\nFigure 13.24: Target Groups–mlapp-tg\n\nAmazon Resource Names (ARNs) are unique identifiers of AWS resources. AWS ARN format could be any one of the following formats:\n\narn:partition:service:region:account-id:resource-id •\t arn:partition:service:region:account-id:resource-type/resource-id •\t arn:partition:service:region:account-id:resource-type:resource-id\n\nA partition is a group of AWS regions, and each account has a limited scope: one partition.\n\nSecurity Groups A security group is a set of firewall rules that control the traffic toward the load balancer. It has not been created yet, so let’s go ahead and create a new security group. Follow these steps:\n\nServices | All Services | EC2 | Network & Security | Security Groups\n\nFirst off, specify basic details for the security group, such as name and description, as shown in the following figure. A Security group name is mlapp-lb-sg, where lb stands for the load balancer and sg stands for the security group.",
      "content_length": 1409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "Deploying ML Models on Amazon Web Services  357\n\nFigure 13.25: Security Group for load balancer–Create a security group\n\nNext, configure inbound rules for the security group. Hit the Add rule button. A load balancer should be able to access port 80 from anywhere, so specify port 80 under the Port range and choose Source type as anywhere, as shown in the following figure. More rules can be added through the Add rule button.\n\nFigure 13.26: Security Group for load balancer–Inbound rules",
      "content_length": 489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "358  Machine Learning in Production\n\nThen, hit the Create security group button in the bottom-right corner of the page and complete the process. The following figure shows the details of the security group after creation:\n\nFigure 13.27: Security Group for load balancer–mlapp-lb-sg\n\nThis security group, mlapp-lb-sg will be used for the load balancer.\n\nApplication Load Balancers (ALB) Application Load Balancer (ALB) accepts incoming requests and routes them to registered targets, such as EC2 instances. It also checks the health of its registered targets (by default, the root path is set to ‘/’; however, it can be changed to a different path of application). Application Load Balancer (ALB) is client-facing. It routes the traffic to healthy targets only. It can be easily integrated with Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Container Service for Kubernetes (Amazon EKS), AWS Fargate, and AWS Lambda.\n\nOnce security and target group is created, create a load balancer. Go to the Load balancing section again and choose Load Balancers.\n\nServices | All Services | EC2 | Load balancing | Load Balancers\n\nThen, hit the Create Load Balancer button. The following figure shows the main page of the load balancer:",
      "content_length": 1238,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "Deploying ML Models on Amazon Web Services  359\n\nFigure 13.28: Application Load Balancer-Create Load Balancer\n\nAfter clicking the Create Load Balancer button, it will show three options with brief descriptions and architecture to choose from, as shown in the following figure:\n\nFigure 13.29: Application Load Balancer–Select load balancer type\n\nThe application load balancer distributes the incoming requests across targets, such as containers, microservices, and EC2 instances, depending on the requests. Before passing the incoming requests to the targets, it evaluates whether the listeners’ rules are configured.\n\nIn the current scenario, choose the first type, which is Application Load Balancer, by hitting the Create button.\n\nNow, configure the load balancer. First, specify the name for the load balancer mlapp-lb, where lb stands for the load balancer. Next, choose the scheme as Internet-facing. The main difference between the Internet-facing and the Internal scheme is the internet. An Internet-facing load balancer routes the requests coming from clients over the internet to specified targets. On the other hand, an Internal load balancer will only route the request from clients with private IPs to specified",
      "content_length": 1224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "360  Machine Learning in Production\n\ntargets. Select the IP address type as IPv4, which is used by specified subnets. The following figure shows the basic configuration of the Application Load Balancer:\n\nFigure 13.30: Application Load Balancer–Basic configuration\n\nThen, configure the network mapping. Network mapping enables the load balancer to route incoming requests to specific subnets and IP addresses. For mapping, select at least two availability zones and one subnet per zone where the load balancer can route the traffic. By default, it will display the available zones for selection. A load balancer will route the requests to targets in these availability zones only. Choose the default VPC and two Subnet (2a and 2b) from the dropdown, as shown in the following figure:",
      "content_length": 783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "Deploying ML Models on Amazon Web Services  361\n\nFigure 13.31: Application Load Balancer–Network mapping\n\nAfter that, remove the default security group and choose the security group from the dropdown created in the previous section. A security group can also be created through the Create new security group link. It will take you to the main page of the security group only, as seen in the previous section. The following figure shows the security group mlapp-lb-sg chosen for the load balancer:\n\nFigure 13.32: Application Load Balancer–Security groups",
      "content_length": 554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "362  Machine Learning in Production\n\nFinally, choose the target group you created earlier, that is, mlapp-tg with port 80, as shown in the following figure:\n\nFigure 13.33: Application Load Balancer–Listeners and routing\n\nScroll down and hit the Create load balancer button in the bottom-right corner. The following figure shows the main page of the load balancer, which shows the newly created load balancer is created:\n\nFigure 13.34: Application Load Balancer–mlapp-lb",
      "content_length": 470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "Deploying ML Models on Amazon Web Services  363\n\nNow, first, delete the service if you created it earlier, as the load balancer setting cannot be added after the creation of the service. Next, create a service with a load balancer.\n\nService An AWS ECS service allows you to keep running a specified number of instances of a task definition simultaneously in an AWS ECS cluster. If the task(s) fails or stops due to any reason, the ECS service will launch another instance of task definition to ensure that the desired number of tasks are up and running. The benefit of this is that the application will be up and running despite the failure of any task.\n\nGo to the Services tab on the mlapp-cluster page and hit the Create button to create a new service.\n\nService creation involves four steps, as shown in the following figure. The configuration and setting for each step are discussed in the following figure:\n\nFigure 13.35: ECS Service creation steps\n\nStep 1: Configure service\n\nIn this step, select the launch type. As the ECS cluster is configured with FARGATE, choose the launch type as FARGATE. The operating system family will be Linux.",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "364  Machine Learning in Production\n\nChoose Task Definition as mlapp and Cluster as mlapp-cluster from the dropdown as shown in the following figure:\n\nFigure 13.36: ECS Service–Configure service\n\nNext, in the same step, issue the name for the service; in this scenario, it is mlapp- cicd, and Set the Number of tasks to 2, which refers to the desired number of tasks that will be influenced by the service. However, this count can be updated (1 and above) as per the requirement. The remaining options can be kept as it is. The following figure is the continuation of step 1:\n\nFigure 13.37: ECS Service–Configure service",
      "content_length": 621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "Deploying ML Models on Amazon Web Services  365\n\nStep 2: Configure network\n\nIn this step, the first choose Cluster VPC and the Subnets associated with it from the dropdown. Again, choose two subnets, that is, 2a and 2b, similar to standalone tasks but with additional configuration, as shown in the following figure. Make sure the Auto-assign public IP is ENABLED. This will allow you to assign available public IPs to access the ML app from anywhere.\n\nFigure 13.38: ECS Service–Configure network\n\nNext, click on the Edit button in the security groups. It will open a window to configure security groups. Here, the existing security group is chosen, which you created for running independent tasks. It has inbound rules defined for ports 80",
      "content_length": 741,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "366  Machine Learning in Production\n\nand 8000, as shown in the following figure. Save this configuration for the security group.\n\nFigure 13.39: ECS Service–Configure security groups\n\nThen, select Application Load Balancer as the load balancer type. This enables it to distribute across the tasks running in the service without letting the end user know. A load balancer of the type Application Load Balancer allows containers to use dynamic host port mapping, that is, multiple tasks are allowed per container instance. Multiple services can use the same listener port on a single load balancer with a rule-based routing path.\n\nFigure 13.40: ECS Service – Load balancer type",
      "content_length": 675,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "Deploying ML Models on Amazon Web Services  367\n\nAfter that, choose the Load balancer name from the dropdown as shown in the following figure. A load balancer will listen to HTTP port 80. Choose the Target group name as mlapp-tg from the dropdown, which you created and configured in the load balancer section.\n\nFigure 13.41: ECS Service–target and port\n\nStep 3: Set Auto Scaling (optional)\n\nAuto-scaling is optional. This will automatically update the service’s desired count within a specified range based on CloudWatch alarms. In the current scenario, the default is used, that is, Do not adjust the service’s desired count. Click on Next step.\n\nStep 4: Review\n\nIn this step, review the configuration for the service. Here, you can go to the previous step to update the configuration. If it looks fine, then go ahead and create a service by hitting the Create Service button, as shown in the following figure:\n\nFigure 13.42: ECS Service–Review\n\nAfter step 4, the service will be created; however, it will take a few minutes to create the desired number of tasks for that service. The progress of task creation can be",
      "content_length": 1120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "368  Machine Learning in Production\n\nchecked under the Events and Logs tab. After a few minutes, the tasks’ status will change to RUNNING, as shown in the following figure:\n\nFigure 13.43: ECS Service–mlapp-cicd\n\nNow, click on the first task, copy its public IP, and enter the public IP, followed by port 8000, in the browser. It should display a message, as shown in the following figure:\n\nFigure 13.44: ECS Service–Accessing ml app with a public IP of the first task\n\nGo back and click on the second task, copy its public IP, and enter the public IP followed by port 8000 in the browser. It should display a message, as shown in the following figure:\n\nFigure 13.45: ECS Service–Accessing ml app with a public IP of the second task",
      "content_length": 732,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "Deploying ML Models on Amazon Web Services  369\n\nThe good thing about service over standalone tasks is that if any one of the tasks fails or shuts down due to any reason, the service will replace it with a new task to maintain the desired number of tasks up and running. The issue with a service without a load balancer is that when it replaces a task or creates a new one, the public IP will get changed, which makes it a bit difficult to keep a track of the latest or running tasks for that service.\n\nThese public IPs can be accessed by the external world through the internet. However, you can restrict access to the public IP of tasks by updating inbound rules in the security group of service mlapp-sg. Go to the mlapp-sg security group, remove inbound rules (if they exist), add a new rule and specify container port, that is, 8000 in Port range, and choose the security group of the load balancer, that is, mlapp- lb-sg instead of 0.0.0.0/0 or anywhere in the source. Finally, you can save the rules. This will restrict the direct access to public IPs of tasks for the external world through the internet.\n\nLet’s check the deployed containerized app with an application load balancer. Go to the load balancer page, copy its DNS name, and run it on the browser (paste and enter). It should display a text message. This is the root page of the application.\n\nThen, pass the input data in <ALB DNS name>/docs and check the prediction output, as shown in the following figure. Here, you don’t need to pass the port and IP address as the load balancer is configured with port and target group, which will handle dynamic IPs of tasks.\n\nFigure 13.46: Application Load Balancer–Prediction response of ML app\n\nNote: A load balancing setting can only be set on service creation.\n\nCI/CD pipeline using CodePipeline An AWS comes with a set of tools and services for the CI/CD pipeline. In the previous section, the build was triggered manually with CodeBuild. To automate the manual process, you must build the CI/CD pipeline with an automated trigger.",
      "content_length": 2047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "370  Machine Learning in Production\n\nTo achieve this, AWS CodePipeline will be used. Before creating a CI/CD pipeline, make sure previous services and topics are studied and implemented on the AWS cloud. An AWS CodePipeline will connect those services to create an automated pipeline. AWS services like CodeCommit, ALB, ECS FARGATE cluster, and such need to be created and configured. Make sure you provide the necessary permissions to services wherever applicable. For the current scenario, any external service, such as GitHub, is not being used. However, you can integrate external services or third- party services into this process. Here, AWS services are leveraged to deploy an application.\n\nWhen there is any update pushed to CodeCommit, it will trigger the CI/CD pipeline. A CodeBuild configuration file buildspec.yaml will first run the tests. Next, log in to Amazon ECR, then build and push Docker container images to the Elastic Container Registry (ECR). It will also write a container image definitions file imagedefinitions. json, in which a repository, followed by an image tag, will be captured. This container image definitions file will be stored in an Amazon S3 bucket as an artifact. A CodeBuild will export the test reports (.xml) in the pytest_reports/ directory. These test results can be seen on the CodeBuild page. After that, in the deploy stage, specify the ECS cluster details. ECS cluster will pull the latest container image from Elastic Container Registry (ECR). Finally, the containerized app will be deployed into the ECS cluster. Logs will be captured in CloudWatch.\n\nWhen the client or end user will access this ML app through Domain Name System (DNS) name, DNS will translate the human-readable name to numerical IP addresses that the machine can understand. This request will go to Application Load Balancer (ALB). Next, the Application Load Balancer (ALB) will check the specified listener's rules, such as port numbers and IP addresses. Then, it will send incoming requests to a specific target group which will route requests toward running tasks located in the Elastic Container Service (ECS) FARGATE cluster. The application will process the request and send the prediction to the client or end user.\n\nThe following figure shows the architecture of the automated CI/CD pipeline to deploy the containerized app on Elastic Container Service (ECS):",
      "content_length": 2387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "Deploying ML Models on Amazon Web Services  371\n\nFigure 13.47: CI/CD pipeline with ECS reference architecture\n\nAWS CodePipeline A pipeline is a workflow that is responsible for automating the deployment or release of the application. A CodePipeline is a fully managed continuous delivery (CD) service in the AWS cloud. It enables faster and easy deployment of an application. It automates the different phases involved in the CI/CD process, such as build, test, and deploy. Each phase comprises a series of actions to be performed. AWS CodePipeline can easily integrate with third-party services like GitHub.",
      "content_length": 609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "372  Machine Learning in Production\n\nThe following figure shows the flow of CodePipeline and AWS services used to create a CI/CD pipeline.\n\nFigure 13.48: CI/CD pipeline using CodePipeline\n\nGo to CodePipeline from Services.\n\nServices | All Services | Developer Tools | CodePipeline\n\nLet’s start by creating a pipeline. Hit the Create pipeline button. It consists of 5 steps, namely:\n\nChoose pipeline settings •\t Add source stage •\t Add build stage •\t Add deploy stage •\t Review\n\nStep 1: Choose pipeline settings\n\nIn this step, you need to specify the Pipeline name as mlapp-cicd. A pipeline name cannot be changed after creation. Choose a New service role for CodePipeline. Check to Allow AWS CodePipeline to create a service role so it can be used with this new pipeline checkbox. The Role name should auto populate.",
      "content_length": 817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "Deploying ML Models on Amazon Web Services  373\n\nFigure 13.49: CodePipeline – Choose pipeline settings\n\nStep 2: Add source stage\n\nIn this step, you need to specify the details of the source from which the latest code and updates are to be pulled. Choose AWS CodeCommit as a Source provider. Next, choose the Repository name as loan_pred. Then, choose the main branch from which the latest code and updates are to be pulled. From the Change detection options, choose Amazon Cloudwatch Events (recommended) as shown in the",
      "content_length": 521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "374  Machine Learning in Production\n\nfollowing figure. This will trigger the pipeline as soon as it detects the changes in the source repository’s branch.\n\nFigure 13.50: CodePipeline–Add source stage\n\nStep 3: Add build stage\n\nHere, first off, you need to choose the AWS CodeBuild as the Build provider. Alternatively, Jenkins can be chosen as the Build provider. The region US West (Oregon) is chosen. Next, choose the build Project name that you have already created in the CodeBuild console, or create a build project in the CodeBuild console. The Create project link will redirect you to the CodeBuild console. In the Build type section, you need to choose Single build as it is expected to trigger a single build.",
      "content_length": 718,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "Deploying ML Models on Amazon Web Services  375\n\nFigure 13.51: CodePipeline–Add build stage\n\nStep 4: Add the deploy stage\n\nIn this step, provide deployment details where the application needs to deploy. This step depends on the previous step. Once the build is complete, this step will deploy the application to Elastic Container Service (ECS) cluster.\n\nFirst, choose the deploy provider as Amazon ECS. The region is US West (Oregon). Then, choose the cluster name that you have already created in the Amazon ECS console or create a cluster in the Amazon ECS console and complete this step. After",
      "content_length": 597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "376  Machine Learning in Production\n\nthat, specify the cluster and service name that was created in the previous section. Refer to the following figure for step 4 configuration.\n\nFigure 13.52: CodePipeline–Add deploy stage\n\nStep 5: Review\n\nIn this step, review the configuration of the previous steps.\n\nIf the configuration is fine, hit Create pipeline.\n\nRun CodePipeline By default, it will start building the pipeline for the first time soon after its creation. When any updates are pushed to the source, that is, AWS CodeCommit, pipeline execution will begin. You can see the progress and status of the pipeline on the main page of CodePipeline. Go to pipeline mlapp-cicd, and you can see the status of each phase. It displays the link to that service. For instance, the progress of the build can be seen through the link under the Build step.\n\nGo to the details in the Build section; you can see the source and submitter details. In the following figure, who started the build job and the source version are displayed. In the first row, the build job was started by CodePipeline, and in the next row, the",
      "content_length": 1109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "Deploying ML Models on Amazon Web Services  377\n\nbuild job was started manually by the IAM user.\n\nFigure 13.53: CodePipeline–Build status\n\nThe main page of CodePipeline is shown in the following figure. It displays details like the latest status of the pipeline and the latest source revision.\n\nFigure 13.54: Running a CodePipeline-Succeeded\n\nTo access the app, you can use the DNS name shown on the main page of the load balancer. A DNS name will not change even if a new task is created or if the public",
      "content_length": 506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "378  Machine Learning in Production\n\nIP of the task changes. The following figure shows the prediction after passing the input data using FastAPI UI.\n\nFigure 13.55: Running a CodePipeline–ML app prediction using DNS of ALB\n\nMonitoring Monitoring is an essential part of the deployment. It is recommended to monitor ECS clusters and other integrated services to ensure the high availability of applications. This enables you to check the overall health of ECS containers. First, create a plan for the resources and services that need to be monitored. Next, decide the action to be taken if a specified event is detected. Decide who should be notified of the event or error, where to monitor metrics and events, and so on.\n\nECS provides cluster-level statistics, such as:\n\nCPU Utilization–Current% of CPU utilized by the ECS cluster •\t Memory Utilization–Current% of Memory utilized by the ECS cluster\n\nAmazon ECS metric data is automatically sent to CloudWatch in 1-minute periods that get captured for 2 weeks. Amazon CloudWatch Container Insights allows you to aggregate and analyze metrics and logs from containerized applications. It helps to monitor utilization metrics such as CPU, memory, disk, and network. It also provides diagnostic information about container restart failures.\n\nYou can create an AWS Lambda function that will get triggered if a specified event is detected and will send the alert to the slack channel. This way, team members and developers will be notified about the alert.\n\nThus, you have learned to create a simple CI/CD pipeline using CodePipeline to deploy the ML app on the Amazon ECS Fargate cluster. However, you can modify it as per business and application requirements. Reference figures in this chapter are from classic UI. If you see a new UI for creating and managing services, you can switch back to the classic UI, or you can continue with the new UI. The underlying flow and services will remain the same for the new UI, but you might see selection",
      "content_length": 1993,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "Deploying ML Models on Amazon Web Services  379\n\nchanges. Alternatively, you can use AWS CloudFormation or Terraform to automate the creation of infrastructure or services.\n\nConclusion In this chapter, you created an Amazon Elastic Container Service (ECS) cluster with Fargate to deploy the ML app. First off, you created an AWS account and created a codebase including a buildspec.yaml file for Amazon Elastic Container Service (ECS) deployment. You added the Continuous Training stage in CI/CD pipeline and accuracy test cases in the pytest file. Next, you pushed the codebase and dependencies to AWS CodeCommit. After that, you created AWS Elastic Container Registry (ECR) to store and manage Docker container images, and you used AWS CodeBuild to build and push the Docker images to AWS Elastic Container Registry (ECR) using buildspec.yaml. You created an Application Load Balancer (ALB) with a security and target group to distribute incoming traffic across the Fargate tasks, created ECS Service with a security group, and added an Application Load Balancer (ALB) to it. Finally, you created AWS CodePipeline and integrated these services to automate ML app deployment.\n\nIn the next chapter, you will study the drift in ML models and the monitoring process for the deployed model.\n\nPoints to remember\n\nAmazon Elastic Container Service (ECS) offers deployment models: EC2 and Fargate.\n\nApplication Load Balancer (ALB) can be specified at the time of service creation only.\n\nApplication Load Balancer (ALB) checks the health of its registered targets (by default, the root path is set to ‘/’; however, it can be changed to a different path of application) and routes the traffic to healthy targets only.\n\n\n\nIt is recommended to install and configure the latest version of AWS CLI and Docker on the local machine.\n\nAmazon ECS cluster is a logical grouping of tasks or services. •\t Create one or more IAM users with the required permissions to run the services and avoid using root user to run the services directly.",
      "content_length": 2020,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "380  Machine Learning in Production\n\nMultiple choice questions\n\n1. _________ is serverless pay-as-you-go compute for containers.\n\na) EC2\n\nb) Fargate\n\nc) CloudWatch\n\nd) Amazon Lightsail\n\n2. The target group for load balancing _________\n\na) takes the request from Application Load Balancer (ALB)\n\nb) routes the request to specified targets\n\nc) handles dynamic IP of the tasks\n\nd) all of the above\n\nAnswers 1. b\n\n2. d\n\nQuestions\n\nWhat is a task in the Amazon ECS cluster? •\t What is the use of an Application Load Balancer (ALB)? •\t What are ECS services and their role?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "Monitoring and Debugging  381\n\nChapter 14 Monitoring and Debugging\n\nIntroduction So far, you have learned various techniques to deploy ML models in production. However, deploying a model into production is not the end; monitoring is the next important step. This chapter talks about concepts and techniques of model monitoring. Model monitoring is not limited to the final endpoint where the model is deployed; you should also integrate it into the intermediate stages wherever required. The static ML model is trained offline on historical data. However, this model may not deliver consistent performance forever. The main reasons could be changes in input data, business requirements, and model degradation over time.\n\nStructure The following topics will be covered in this chapter:\n\n\n\nImportance of Monitoring\n\nFundamentals of ML monitoring •\t Metrics for monitoring your ML system •\t Drift in ML - Types and detection techniques •\t Operational monitoring using Prometheus and Grafana •\t ML model monitoring with whylogs and WhyLabs",
      "content_length": 1036,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "382  Machine Learning in Production\n\nObjectives After studying this chapter, you can start monitoring ML models. You should be able to understand the importance and fundamentals of monitoring, decide on the metrics to monitor for your project and understand the concept of drift in ML, its types, and techniques to detect the drift. You should also be able to monitor drift in the data using whylogs, an open-source lightweight library, and WhyLabs, an observability platform. You should be able to integrate Prometheus and Grafana with FastAPI, and operational monitoring with open-source tools, that is, Prometheus and Grafana.\n\nImportance of monitoring Once an ML model is deployed in production, it is essential to monitor it in order to ensure that the model’s performance stays up to the mark and it continues to deliver reliable output seamlessly. As a matter of fact, there are many reasons for failure in ML apps or services, such as pipeline failure, model degradation over time, change in model input data, system or server failure, and change in the schema.\n\nMultiple teams are involved while deploying models to production, which usually includes a team of Data Scientists, Data Engineers, and DevOps. If prediction errors increase, who will be held responsible? Who is the owner of the model in production?\n\nDue to COVID-19, banks’ ML model's performance was heavily affected. The model’s predictions were far away from the actual figures. This is the case of a shift in input data. This should give you an overview of the challenges post-deploying models into production and the necessity of monitoring ML models.\n\nMonitoring is essential when it comes to comparing new and old models’ performance and their predictions over time. There could be latency issues, that is, delayed output. For tracking and investigating latency issues, efficient monitoring is needed. You need to monitor input data to ensure that production input data is being processed like the training data. To track and handle extreme values, out of the range values, or special cases before passing them to the models, monitoring is required. Last but not least is model security, that is, monitoring is required to track any external attack on the model or system.\n\nThe objectives of ML monitoring are as follows:\n\nTo detect the issues or failures at early stages so that necessary steps can be taken\n\nTo keep a track of resource usage and model prediction to evaluate model and system performance in the production environment",
      "content_length": 2515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "Monitoring and Debugging  383\n\nTo detect the change in distribution, schema, and anomalies in input data that may cause an error in the predictions\n\nTo ensure the availability of model predictions and that they are explainable •\t To track and store metrics in specified storage or database\n\nModel monitoring helps data scientists to reduce model failures, avoid downtime, and ensure reliable outcomes for users.\n\nFundamentals of ML monitoring ML monitoring refers to tracking the performance, errors, metrics, and such of deployed models and sending alerts (when required) to ensure models continue performing above an acceptable threshold. It is not limited to tracking input data or model degradation. However, it should take all the things into account that may affect model performance directly or indirectly. ML monitoring helps decide whether an existing model needs to be updated.\n\nThe following are the essential steps to consider:\n\nMonitoring is the key: Monitoring is essential once a model is deployed in production. Monitoring enables you to track and fix issues or errors in a faster manner. Monitoring can be broadly divided into two types: o Functional monitoring: In the case of ML, functional monitoring refers to tracking metrics, errors, and performance related to ML models, such as accuracy and outliers.\n\no Operational monitoring: Operational monitoring refers to tracking system-specific metrics, such as CPU and RAM utilization, uptime, and throughput.\n\nScalable integration: Monitoring should be easy to integrate with your existing infrastructure and workflow. The monitoring system should be seamless and scalable to integrate. It should be able to track multiple models if there is more than one model. The monitoring solution should be platform agnostic so that it can be used for different types of deployment, having different tech stacks.\n\nMetrics tracking: Tracking accuracy is not enough. Monitoring systems should be able to track all the metrics that can affect model and system performance over time. A centralized monitoring system should use multiple performance metrics to give the overall status of the solution. Use different metrics for different types of features. For instance, min, max, mean, standard deviation, and outliers can be used for numerical data. The metrics records and logs need to be stored in a database so that they can be analyzed later on.",
      "content_length": 2404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 410,
      "content": "384  Machine Learning in Production\n\nAlert system for important events: You cannot track 100+ metrics manually 24x7; hence, the alert system should be part of the monitoring solution. Not everything needs an alert. You may be tracking 20+ metrics; however, only a few metrics need high attention. For instance, if input data drift is exceeding the threshold, then the alert system should send the notification to the responsible team or Data Scientists. The whole purpose of the alert system is to notify concerned teams or individuals so that issues or errors can be avoided or can be solved at the earliest to ensure consistency in ML model performance.\n\nRoot cause analysis and debugging: Once you get the alert, you may need to act on it. You can start the root cause analysis to determine the issue and fix it by debugging. For instance, if model accuracy is going below the threshold, it could be because of a change in the distribution of input data or anomalies.\n\nTo design an efficient monitoring system, you need to consider existing pipelines and infrastructure. There are mainly three pillars of monitoring:\n\nProcessing and Storage: It processes and stores the critical metrics in a database or data source with a timestamp. You can query the metrics when required.\n\nGraphs and Dashboard: It will query or fetch the monitoring metrics from a connected database or data source. You can choose the visualization as per the type of metrics. Here, you have to decide the metrics that need to be displayed. It should summarize overall monitoring with intuitive graphs. •\t Alert: Finally, it should send an alert to the concerned team or data scientists to take immediate action, if required. This helps to prevent future failures or mitigate the impact.\n\nThe following figures show the three pillars of the monitoring system for effective monitoring:\n\nFigure 14.1: Pillars of the monitoring system",
      "content_length": 1905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "Monitoring and Debugging  385\n\nBefore setting up any monitoring system, here is the list of questions that need to be answered:\n\nWhat do you plan to monitor? •\t What tools and language are you using? •\t What platform or library will be used for monitoring? •\t How do you plan to integrate the monitoring setup with the existing environment and tools?\n\nWhat threshold is to be used for alerts? •\t What action should be taken after detecting an issue or failure?\n\nFinally, as per the model life cycle, monitoring should complete the feedback loop, that is, after detecting an issue or failure, it should send a notification to the concerned team or data scientists so that they can take necessary action, such as retraining the ML model.\n\nMetrics for monitoring your ML system You need to decide which type of metric to monitor. Operational metrics will enable developers to track system or server-related failures or warnings, such as high resource usage that can increase the latency, which will affect end-user experience. Tracking operational metrics is essential because the server should be in good condition to run the ML model and other tasks. Another factor is the cost. Operational teams have to set limits on the cost and need to track which resource or service causes higher costs.\n\nHere are a few important operational metrics:\n\nSystem or Server\n\no Resource usage\n\no Availability\n\no Latency\n\no Throughput\n\nCost o\n\nInfrastructure cost\n\no Storage cost\n\no Additional service (if any)\n\nOnce the server is up and running without any issues, you can focus on functional or ML model metrics. You will study the detection of model monitoring metrics and ways to address them in this chapter.",
      "content_length": 1695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "386  Machine Learning in Production\n\nThe following are a few important model metrics:\n\n\n\nInput data\n\no Model version\n\no Data drift\n\no Outliers\n\nML model\n\no Model version\n\no Model hyperparameters\n\no Metadata\n\no Predictions or Output\n\no Classification model evaluation metrics\n\n▪ Accuracy\n\n▪ Confusion Matrix\n\n▪ ROC-AUC Score\n\n▪ Precision and Recall Scores\n\n▪ F1-Score\n\no Regression model evaluation metrics\n\n▪ Root Mean Square Error (RMSE)\n\n▪ R-Squared and Adjusted R-Square\n\n▪ Mean Absolute Error (MAE)\n\n▪ Mean Absolute Percentage Error (MAPE)\n\nPrediction drift\n\nYou can decide which metrics to track closely and which metrics need alerts.\n\nDrift in ML Model drift means a change in the behavior of ML models or that the model is not performing as expected or per the Service Level Agreement (SLA). Model performance may degrade after deploying it to production, as the model can receive data that was not introduced during model training.\n\nTypes of drift in ML There are mainly three types of drift in ML:",
      "content_length": 1007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "Monitoring and Debugging  387\n\nData drift •\t Prediction drift •\t Concept shift\n\nData drift\n\nData drift is when the distribution or characteristics of input features change with reference to training data. It is also known as feature drift, population drift, or covariate shift. If the distribution of input data changes, then its predictions might get affected as the model is not prepared for it.\n\nIf a new category gets added to the feature post-deployment, then it may cause an error while making the prediction, as it was not there at the time of model training.\n\nAnother example could be, suppose there is a feature called Credit Rating in training data, and it has high weightage, which means a change in this feature may cause a major change in model output. Now, the business decided to go with Moody’s credit rating instead of S&P. This will cause a change in input data. For instance, S&P’s AA- is equivalent to Moody’s Aa3 rating.\n\nThe following equation shows that the distribution of training data does not match the distribution of reference (production) data.\n\nMathematically, data drift can be defined as follows:\n\nP(X) ≠ Pref(X)\n\nWhere P(X) denotes the input data probability distribution.\n\nPrediction drift\n\nAs input data changes with data drift, this may cause a change in the target or prediction variable. It refers to a change in predictions over time. It is also named a prior probability shift, label drift, or unconditional class shift. This can also occur due to the removal or addition of new classes. Retraining the model can help mitigate the model degradation owing to prediction drift.\n\nMathematically, prediction drift can be defined as follows:\n\nP(Y) ≠ Pref(Y)\n\nWhere P(Y) denotes the prior probability distribution of target labels.\n\nConcept shift\n\nA concept shift occurs when the relationship between independent variables and dependent or target variables changes. It is also known as posterior class shift, conditional change, or real concept drift. It refers to changes in the relationship",
      "content_length": 2028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "388  Machine Learning in Production\n\nbetween the input variables and the target variables. If you detect any significant concept shift, it is very likely that your model’s predictions are unreliable. A Concept in Concept shift refers to the relationship between independent and dependent variables.\n\nFor instance, a car insurance company changes its claim policy that a claim for a specific part of the car will be rejected. Suppose that part of the car got damaged in an accident, the customer’s claim for it will be rejected. In this scenario, mapping between the input feature and target feature changes even though the distribution of input data remains the same.\n\nMathematically, concept shift can be defined as follows:\n\nP(Y|X) ≠ Pref(Y|X)\n\nWhere P(Y|X) denotes the posterior probability distribution of the target labels.\n\nFollowing are the patterns of concept shift:\n\nFigure 14.2: Different patterns of concept shift\n\nTechniques to detect the drift in ML Statistical distance measurement using distance metrics between two distributions is useful for detecting drift in ML.\n\nIf there are many independent variables in the dataset, then you can use dimensionality reduction techniques, such as PCA. Tracking many features can increase the load on the monitoring system and sometimes it becomes difficult to mitigate drift by targeting specific features.\n\nBasic statistical metrics, such as mean value, standard deviation, correlation, and minimum and maximum values comparison, can be used for calculating the drift between training and current independent variables.\n\nDistance measures like Population Stability Index (PSI), Characteristic stability index (CSI), Kullback–Leibler divergence (KL-Divergence), Jensen–Shannon divergence (JS-Divergence), and Kolmogorov-Smirnov (KS) statistics can be used for continuous features.",
      "content_length": 1835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "Monitoring and Debugging  389\n\nThe cardinality checks, Chi-squared test, and entropy can be used for categorical variables.\n\nControl charts and histogram intersections can be used to detect a drift in data.\n\nFinally, there are several platforms for model monitoring, such as WhyLabs, and libraries, such as deepchecks, and alibi-detect. They come with easy integrations and a ready framework for drift detection. The best part about this is that most of them provide a drift detection framework, storage for logs and historical data, intuitive monitoring dashboards, and alert mechanisms for critical events.\n\nAddressing the drift in ML Once you detect the drift in the ML model, it can be addressed in the following ways:\n\nData quality issues\n\nIf there is an issue with input data, then it can be easily fixed. For instance, high- resolution images were provided for training face recognition models; however, low-resolution images were passed to the deployed model.\n\nRetraining the model\n\nAfter detecting the data or concept shift, retraining the model with recent data can improve its performance. Sometimes production data is not sufficient to train the model. In that case, you can combine historical data with recent production data and give more weight to recent data.\n\nHere are four strategies for retraining the model:\n\nPeriodically retraining: Scheduling it at a fixed time, for instance, every Monday at 10 PM\n\nData or event-driven: When new data is available •\t Model or metric driven: When accuracy is lower than a threshold or SLA •\t Online learning: Where the model continuously learns in real-time or near real-time on the latest data\n\nRebuilding or tuning the model\n\nIf retraining the model doesn’t work, then you may need to consider rebuilding it or tuning it on recent data. You can automate this using a pipeline.",
      "content_length": 1835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "390  Machine Learning in Production\n\nOperational monitoring with Prometheus and Grafana Prometheus is an open-source system used for event monitoring and alerting. It scrapes the real-time data from instrumented jobs and stores it with a timestamp in the database. The word instrument refers to the use of a client library that allows Prometheus to track and scrape its metrics and store them locally. Prometheus offers client libraries that can be used to instrument your application. In this scenario, you will be using the Prometheus Python client in the FastAPI application to expose its metrics that need to be tracked.\n\nA Prometheus server scrapes and stores metrics from instrumented jobs in time series format. This data can be fetched using PromQL - query language, and it can be used to visualize the metrics. It comes with an alert manager to handle alerts.\n\nAccording to GrafanaLabs, Grafana is an open-source interactive visualization and monitoring platform that enables users to visualize metrics, logs, and traces collected from deployed applications. Grafana is easy to integrate with most common databases, such as Prometheus, Influx DB, ElasticSearch, MySQL, and PostgreSQL.\n\nAs Grafana is an open source tool, hence you can write an integration plugin from scratch to connect with multiple data sources. The Grafana dashboard fetches the data from connected data sources and allows you to pick up the visualization type from plenty of visualization options, such as heat maps, bar charts, and line graphs. You can easily run the query, visualize metrics, and set up alerts for critical events.\n\nIn this scenario, you will be using Grafana and Prometheus. Both Prometheus and Grafana are open-source tools. As a matter of fact, Prometheus and Grafana are popular combinations in the industry for monitoring systems. Grafana dashboard will be used for visualization and alert management. It will fetch the data from the Prometheus database for querying metrics and will display the intuitive visualization on the dashboard.\n\nPrometheus and Grafana can be separately installed and configured on a local machine or a remote server. However, you will be using docker images of Prometheus and Grafana by executing the docker-compose.yaml file.\n\nTo maintain consistency, similar files from the previous chapters with minor modifications will be used. The files required for Prometheus and Grafana dashboards are added as shown in the following file structure:\n\n.\n\n├── .gitignore\n\n├── config.monitoring\n\n├── datasource.yml",
      "content_length": 2535,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "├── docker-compose.yaml\n\n├── Dockerfile\n\n├── fastapi-dashboard.json\n\n├── LICENSE\n\n├── main.py\n\n├── prometheus.yml\n\n├── pytest.ini\n\n├── README.md\n\n├── requirements.txt\n\n├── runtime.txt\n\n├── start.sh\n\n├── test.py\n\n├── tox.ini\n\n├── app\n\n└── src/\n\n├── MANIFEST.in\n\n├── README.md\n\n├── requirements.txt\n\n├── setup.py\n\n├── tox.ini\n\n└── prediction_model/\n\n├── pipeline.py\n\n├── predict.py\n\n├── train_pipeline.py\n\n├── VERSION\n\n├── __init__.py\n\n├── config/\n\n│ ├── config.py\n\n│ └── __init__.py\n\n├── datasets/\n\n│ ├── test.csv\n\n│ ├── train.csv\n\n│ └── __init__.py\n\nMonitoring and Debugging  391",
      "content_length": 580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "392  Machine Learning in Production\n\n├── processing/\n\n│ ├── data_management.py\n\n│ ├── preprocessors.py\n\n│ └── __init__.py\n\n├── trained_models/\n\n│ ├── classification_v1.pkl\n\n│ └── __init__.py\n\n└── tests/\n\n└── pytest.ini/\n\n└── test_predict.py\n\nconfig.monitoring\n\nThe extension of this file is .monitoring. In this file, configure the admin password and user permission for sign up.\n\n1. GF_SECURITY_ADMIN_PASSWORD=pass@123\n\n2. GF_USERS_ALLOW_SIGN_UP=false\n\nThe Docker container currently allows new Grafana users to sign up, so anyone can create an account and view the dashboard. Therefore, for security purposes, the admin password and allowing new users to sign up should be set to false.\n\nprometheus.yml\n\nThis file contains Prometheus configurations, such as scrape_configs and scrape_ intervals.\n\n1. # Global config\n\n2. global:\n\n3. scrape_interval: 15s\n\n4. evaluation_interval: 15s\n\n5. external_labels:\n\n6. monitor: \"app\"\n\n7.\n\n8. rule_files:\n\n9.\n\n10. scrape_configs:\n\n11. - job_name: \"prometheus\"",
      "content_length": 999,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "Monitoring and Debugging  393\n\n12.\n\n13. static_configs:\n\n14. - targets: [\"localhost:9090\"]\n\n15.\n\n16. - job_name: \"app\"\n\n17. dns_sd_configs:\n\n18. - names: [\"app\"]\n\n19. port: 8000\n\n20. type: A\n\n21. refresh_interval: 5s\n\nIn this file, the scrape interval is set to 15 seconds (15s) in the global config, which means it will scrape the metrics data after every 15 seconds. It is targeting an app, that is, the FastAPI app here. Port is the app’s port, that is, 8000, and the refresh interval is set to 5 seconds (5s).\n\nDockerfile\n\nThis Dockerfile will install the dependencies, set the working directory, expose ports, and finally, run the ML app.\n\n1. FROM python:3.7-slim-buster\n\n2.\n\n3. ENV PYTHONPATH \"${PYTHONPATH}:src/\"\n\n4.\n\n5. WORKDIR /app/\n\n6.\n\n7. COPY . .\n\n8.\n\n9. RUN chmod +x /app\n\n10.\n\n11. COPY ./start.sh /start.sh\n\n12.\n\n13. RUN chmod +x /start.sh\n\n14.\n\n15. # Expose the port that uvicorn will run the app on",
      "content_length": 915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "394  Machine Learning in Production\n\n16. ENV PORT=8000\n\n17. EXPOSE 8000\n\n18.\n\n19. RUN pip install --upgrade pip\n\n20.\n\n21. RUN pip install --upgrade -r requirements.txt --no-cache-dir\n\n22.\n\n23. CMD [\"/start.sh\"]\n\ndocker-compose.yaml\n\nThis docker-compose file will set up and run three services:\n\nThe app, that is, FastAPI app: For the ML model. It will run on port 8000. •\t Prometheus server: It will run on port 9090. •\t Grafana dashboard: It will run on port 3000.\n\nIn the following file, you can see that the Grafana service depends on the Prometheus service:\n\n1. version: \"2.2\"\n\n2.\n\n3. services:\n\n4. app:\n\n5. build: .\n\n6. restart: unless-stopped\n\n7. container_name: app\n\n8. ports:\n\n9. - 8000:8000\n\n10. networks:\n\n11. example-network:\n\n12. ipv4_address: 172.16.238.10\n\n13.\n\n14. prometheus:\n\n15. image: prom/prometheus:latest\n\n16. restart: unless-stopped",
      "content_length": 856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "Monitoring and Debugging  395\n\n17. container_name: prometheus\n\n18. ports:\n\n19. - 9090:9090\n\n20. volumes:\n\n21. - ./prometheus.yml:/etc/prometheus/prometheus.yml\n\n22. command:\n\n23. - \"--config.file=/etc/prometheus/prometheus.yml\"\n\n24. networks:\n\n25. example-network:\n\n26. ipv4_address: 172.16.238.11\n\n27.\n\n28. grafana:\n\n29. image: grafana/grafana:latest\n\n30. restart: unless-stopped\n\n31. user: \"472\"\n\n32. container_name: grafana\n\n33. depends_on:\n\n34. - prometheus\n\n35. ports:\n\n36. - 3000:3000\n\n37. volumes:\n\n38. - ./datasource.yml:/etc/grafana/provisioning/datasource.yml\n\n39. env_file:\n\n40. - ./config.monitoring\n\n41. networks:\n\n42. example-network:\n\n43. ipv4_address: 172.16.238.12\n\n44.\n\n45. networks:\n\n46. example-network:\n\n47. # name: example-network",
      "content_length": 753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "396  Machine Learning in Production\n\n48. driver: bridge\n\n49. ipam:\n\n50. driver: default\n\n51. config:\n\n52. - subnet: 172.16.238.0/24\n\nmain.py\n\nThis file contains code for the FastAPI app. In this code, add the following two lines to instrument the FastAPI app. In Prometheus, the instrumentation refers to the use of a library in application code (in this scenario, it is FastAPI) so that Prometheus can scrape metrics exposed by the app.\n\nFirst, add prometheus-fastapi-instrumentator with version 5.7.1 to the requirements.txt file. Then, import the Prometheus FastAPI instrumentator as follows:\n\n1. from prometheus_fastapi_instrumentator import Instrumentator\n\nThen, at the end of the file, add the following line to instrument FastAPI app to the Prometheus FastAPI instrumentator:\n\n1. Instrumentator().instrument(app).expose(app)\n\nWith this single line, FastAPI will be instrumented and all Prometheus metrics used in the FastAPI app can be scraped via the added /metrics endpoint.\n\nYou have already studied the rest of the files in the previous chapters. Refer to the previous chapters for files mentioned in the preceding directory structure.\n\nYou will need to run the docker containers by running the following docker-compose command in the terminal:\n\ndocker-compose up\n\nNow you have access to three containers and their respective ports:\n\nFastAPI: Running at http://localhost:8000/ •\t Prometheus: Running at http://localhost:9090/ •\t Grafana dashboard: Running at http://localhost:3000/\n\nOn the FastAPI app, you can access the /metrics endpoint to view the data that Prometheus is scraping from it, as shown in the following figure:",
      "content_length": 1639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "Monitoring and Debugging  397\n\nFigure 14.3: FastAPI metrics\n\nYou can access Prometheus UI on port 9000 and run the PromQL query. To get the total number of requests, execute prometheus_http_requests_total. You will get the HTTP status code, handler, instance, job name, and count, as shown in the following figure:\n\nFigure 14.4: Prometheus UI–Query execution",
      "content_length": 359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "398  Machine Learning in Production\n\nPrometheus allows you to view these query results in graphical format by switching to the Graph tab in Prometheus web UI.\n\nYou have seen Prometheus web UI, and PromQL is fetching the metric data as well. Now, in the next part, you will connect Prometheus to the Grafana dashboard as a data source. Metrics data fetched from the Prometheus server will be visualized on the Grafana dashboard. Go to Grafana dashboard web UI and click on the add your first data source option, as shown in the following figure:\n\nFigure 14.5: Grafana–Adding data source\n\nNext, select the Prometheus database from the Time series databases, as shown in the following figure:\n\nFigure 14.6: Grafana–Prometheus data source\n\nIn the settings, provide Prometheus as a name in the Name field. Then, mention http://prometheus:9090 in the URL field, as shown in the following figure, as Prometheus service is running on port 9090:",
      "content_length": 937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "Monitoring and Debugging  399\n\nFigure 14.7: Grafana–Prometheus configuration\n\nFinally, proceed with the basic configuration; however, you can also configure other fields. Scroll to the bottom of the page and hit the Save & test button, as shown in the following figure:\n\nFigure 14.8: Grafana–saving and testing Prometheus configuration\n\nIf it is configured properly, you should see the integration status success message after you save and test the configuration.\n\nOnce Prometheus integration is done, create a new dashboard with the New Dashboard option in Grafana web UI, as shown in the following figure:\n\nFigure 14.9: Grafana–creating a new dashboard",
      "content_length": 655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "400  Machine Learning in Production\n\nThe Grafana dashboards can be created from scratch or by importing JSON files from storage. Grafana also lets you import the dashboard via Grafana.com, as shown in the following figure:\n\nFigure 14.10: Grafana–uploading JSON file\n\nIn this scenario, import a JSON file from the directory. You can refer to one of the original public Grafana instances, hosted by Grafana Labs, using the following link:\n\nhttps://play.grafana.org/d/000000012/grafana-play-home?orgId=1\n\nFor various dashboard visualizations and sources, refer to the following link https:// grafana.com/grafana/dashboards/.\n\nNext, provide the dashboard name and Prometheus source, as shown in the following figure:\n\nFigure 14.11: Grafana–Prometheus as a data source",
      "content_length": 764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "Monitoring and Debugging  401\n\nFinally, open the newly created dashboard from the web UI. After importing the dashboard from the JSON file, you can customize the dashboard by changing the chart type, as shown in the following figure.\n\nFigure 14.12: Grafana–Dashboard\n\nThe next step is to send alerts for critical events. Grafana includes built-in support for the Prometheus Alert manager. Grafana provides mainly three strategies for setting alerts:\n\nCloud Alert manager: Cloud Alert manager runs in Grafana Cloud. It supports alerts from Grafana, Mimir, and Loki.\n\nGrafana Alert manager: Grafana Alert manager is an internal Alert manager. It is pre-configured and supports alerts from Grafana; however, it cannot receive alerts outside Grafana.\n\nAn external Alert manager: You can configure Grafana to use an external alert manager. This is useful when you are looking for a centralized alert manager that can receive alerts from different sources.",
      "content_length": 951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "402  Machine Learning in Production\n\nYou can add the alert rules, labels, and notification policy in Grafana. The following figure shows the Alert UI in Grafana:\n\nFigure 14.13: Grafana–Alerting UI\n\nML model monitoring with whylogs and WhyLabs WhyLabs is a model monitoring and observability platform built to monitor data drift, model performance degradation, data quality, model evaluation metrics, and sending alerts to teams or individuals. It is built on top of an open-source package whylogs. You can monitor whylogs profiles continuously with the WhyLabs Observability Platform.\n\nFollowing are the salient features of WhyLabs:\n\nMinimal setup time: You can start by installing lightweight open-source library whylogs with pip install whylogs. In no time, a summary of the dataset can be generated, that is, whylogs profiles, which helps you do the following: o Track changes in datasets\n\no Visualize the summary statistics of datasets\n\no Check the data quality by defining data constraints\n\nSeamless integration: It can be integrated with the existing data pipelines, model life cycle, model framework, and MLOps Ecosystem easily.\n\nData privacy: WhyLabs works with a statistical summary of the data created by the whylogs agent, so your actual data is not getting transferred or stored in the WhyLabs platform.\n\nScalability: It can be easily scaled up to handle large amounts of data and multiple projects.\n\nCentralized monitoring: With the WhyLabs platform, you can monitor multiple projects under a single umbrella. It allows you to create multiple isolated projects within a single account, and it also keeps track of input and output data, model performance, and such.",
      "content_length": 1678,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "Monitoring and Debugging  403\n\nwhylogs Without further delay, let’s start using the whylogs logging functionality. First, install whylogs using pip install whylogs if it’s not installed already. Next, import a loan prediction dataset and start logging it with whylogs.\n\n1. # Import packages\n\n2. import numpy as np\n\n3. import pandas as pd\n\n4. from sklearn.model_selection import train_test_split\n\n5. pd.set_option(\"display.max_columns\", None) #To show all columns in DataFrame\n\n6. import whylogs as why\n\n7.\n\n8. df = pd.read_csv(\"https://gist.githubusercontent. com/suhas-ds/a318d2b1dda8d8cbf2d6990a8f0b7e8a/ raw/9b548fab0952dd12b8fdf057188038c8950428f1/loan_dataset.csv\") 9.\n\n10. profile1 = why.log(df)\n\n11. profile_view1 = profile1.view()\n\n12. profile_view1.to_pandas()\n\nTo view the statistical summary, that is, the whylogs profile, convert it to the pandas DataFrame. The pandas DataFrame of the profile contains the following metrics:\n\n\n\ncardinality/est\n\n\n\ncardinality/lower_1\n\n\n\ncardinality/upper_1\n\n\n\ncounts/n\n\n\n\ncounts/null •\t distribution/max •\t distribution/mean •\t distribution/median •\t distribution/min •\t distribution/n •\t distribution/q_01",
      "content_length": 1153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "404  Machine Learning in Production\n\ndistribution/q_05 •\t distribution/q_10 •\t distribution/q_25 •\t distribution/q_75 •\t distribution/q_90 •\t distribution/q_95 •\t distribution/q_99 •\t distribution/stddev •\n\nfrequent_items/frequent_strings\n\n\n\nints/max\n\n\n\nints/min\n\n\n\ntype\n\n\n\ntypes/boolean\n\n\n\ntypes/fractional\n\n\n\ntypes/integral\n\n\n\ntypes/object\n\n\n\ntypes/string\n\nThis covers pretty much all the information required about the input data, such as cardinality, types, frequent_items, distribution, and counts.\n\nConstraints for data quality validation With whylogs, you can use constraints for performing data validation. This feature can be used in unit tests or CI/CD pipelines so that further errors can be avoided. You can set constraints on profile metrics, such as count, distribution, and type. The whylogs also supports user-defined custom metrics. You need a whylogs profile to set the constraints on data.\n\nSetting a new constraint is simple; you need to assign values to the following:\n\nname: It can be any string to describe the constraint. •\t condition: It is a lambda expression. •\t metric_selector: It is the type of metric to be measured or validated.\n\n1. from whylogs.core.constraints import (Constraints,\n\n2. ConstraintsBuilder,",
      "content_length": 1240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "Monitoring and Debugging  405\n\n3. MetricsSelector,\n\n4. MetricConstraint)\n\n5.\n\n6. # Function with Constraints for Data Quality Validation\n\n7. def validate_feat(profile_view, verbose=False, viz=False):\n\n8. builder = ConstraintsBuilder(profile_view)\n\n9. # Define a constraint for data validation\n\n10. builder.add_constraint(MetricConstraint(\n\n11. name=\"Credit_History == 0 or == 1\",\n\n12. condition=lambda x: x.min == 0 or x.max == 1,\n\n13. metric_selector=MetricsSelector(metric_name='distribution',\n\n14. column_name='Credit_History')\n\n15. ))\n\n16.\n\n17. # Build the constraints and return the report\n\n18. constraints: Constraints = builder.build()\n\n19.\n\n20. if verbose:\n\n21. print(constraints.report())\n\n22.\n\n23. # return constraints.report()\n\n24. return constraints\n\n25. # Call the function\n\n26. const = validate_feat(profile_view1, True)\n\nIn the current scenario, constraints are set to ensure that the Credit_History feature should contain either 0 or 1. If it receives any value other than 0 or 1, it will fail. You can add multiple constraints to validate multiple conditions at once. The following is the output of the preceding code:\n\n[('Credit_History == 0 or == 1', 1, 0)]\n\nThis can be read as [('Constraint Name', Pass, Fail)]. It can be visualized with the whylogs visualization functionality in the notebook.",
      "content_length": 1316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "406  Machine Learning in Production\n\n1. from whylogs.viz import NotebookProfileVisualizer\n\n2. visualization = NotebookProfileVisualizer()\n\n3. visualization.constraints_report(const, cell_height=300)\n\nFigure 14.14 depicts the constraints report, that is, the output of the preceding code, which visualizes the outcome of the constraint:\n\nFigure 14.14: whylogs–Constraints Report\n\nIn order to validate all constraints in one go, you can simply use validate() on top of the constraint's outcome.\n\n1. # check all constraints for passing:\n\n2. constraints_valid = const.validate()\n\n3. print(constraints_valid)\n\nAs it is validating only one condition, it will check whether it is true. As data contains 0 or 1 in the Credit_History feature, the test is passed and True is printed.\n\nTrue\n\nNext, with minimal code, you can generate a drift report. For this, you need two whylogs profiles of the data: the target profile and the reference profile.\n\n1. from whylogs.viz import NotebookProfileVisualizer\n\n2.\n\n3. visualization = NotebookProfileVisualizer()\n\n4. visualization.set_profiles(target_profile_view=profile_view1, reference_profile_view=profile_view2)\n\n5.\n\n6. visualization.profile_summary()\n\n7.\n\n8. visualization.summary_drift_report()",
      "content_length": 1233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "Monitoring and Debugging  407\n\nWith summary_drift_report(), you get an easy-to-read drift report. It is an interactive feature-by-feature comparison of two profiles. This report contains the following:\n\nTarget: Histogram of the target profile •\t Reference: Histogram of the reference profile •\t p-value: Ranges from 0 to 1 •\t Total count: Count of records or instances in the dataset •\t Mean: Mean of the numeric feature\n\nWhyLabs The next step is to start uploading these profiles to the WhyLabs platform.\n\nThe first step is to sign up for an account at https://whylabs.ai/whylabs-free-sign- up. If you have already done this, then log in to the account. Next, create a new project from the Project Management section. Depending on the requirement, there are different types of projects available to choose from, such as an ML model and a data pipeline. In this scenario, choose a Classification, that is, an ML project so that you can analyze the model’s performance and input-output data.\n\nAs shown in the following figure, provide the Project Name as ML_monitor (it can be updated later on) and Type as Classification:\n\nFigure 14.15: WhyLabs–Project Management",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 434,
      "content": "408  Machine Learning in Production\n\nThen, you need three things to start uploading your whylogs profiles to the WhyLabs platform:\n\nA WhyLabs API Key •\t The organization ID •\t A different Dataset or Model ID for each project\n\nCreate a new access token from the Access Tokens tab, as shown in the following figure. Save the access token and org ID as it can be used multiple times while accessing the project in WhyLabs.\n\nFigure 14.16: WhyLabs–Access Token\n\nWhyLabs allow you to set the notification workflow for important events via the following:\n\nEmails •\t Slack •\t PagerDuty\n\nThe following figure shows the notification options offered by WhyLabs under the Notifications page:\n\nFigure 14.17: WhyLabs–Notifications",
      "content_length": 717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "Monitoring and Debugging  409\n\nIn the User Management tab, you can see the team members and their roles. You can also control the privilege and access of the users. In this scenario, one user with an Admin role is shown in the following figure:\n\nFigure 14.18: WhyLabs–User management\n\nNext, ensure that the target project is selected from the Select project dropdown. You can update the default values and selections of the monitor. You can also set the threshold, trailing window, action, severity, and so on, as shown in the following figure:\n\nFigure 14.19: WhyLabs–Monitor Manager",
      "content_length": 584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "410  Machine Learning in Production\n\nWhyLabs offer preset monitors to target common detection. You can use these easy- to-use preset monitors for your project rather than setting them up from the scratch. Once you configure it, you can enable it with a single click. WhyLabs offers the following preset monitors:\n\nDrift\n\no Data drift in all discrete model inputs compared to a trailing 7 days\n\nbaseline\n\no Data drift in all non-discrete model inputs compared to a trailing 7\n\ndays baseline\n\nData quality\n\no Missing values\n\no Unique values\n\no Data types\n\no Percentage change\n\nModel performance o F1 score o Precision\n\no Recall\n\no Accuracy\n\n\n\nIntegration health\n\no Data availability\n\nThe following figure shows the presets available in WhyLabs to check input data quality:\n\nFigure 14.20: WhyLabs–Data quality presets",
      "content_length": 815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "Monitoring and Debugging  411\n\nNow, let’s install whylogs, if it’s not already installed, using the following command, or simply install whylogs using pip: pip install -q \"whylogs[whylabs]\"\n\nFirst, declare variables with org id, access key, and project id. Next, load the data and build the model.\n\n1. whylabs_org = 'org-ELqbpF'\n\n2. whylabs_key = '0Ji9w2gC7q. lQQ4pxgQBEUdx*********************D0vvDrL'\n\n3. whylabs_project = 'model-5'\n\n4.\n\n5. # Importing the required packages\n\n6. import numpy as np\n\n7. import pandas as pd\n\n8. from sklearn.linear_model import LogisticRegression\n\n9.\n\n10. from sklearn import preprocessing\n\n11. from sklearn.model_selection import train_test_split, GridSearchCV\n\n12. from sklearn import metrics\n\n13. import whylogs as why\n\n14. import os\n\n15. from whylogs.api.writer.whylabs import WhyLabsWriter\n\n16. import datetime as dt\n\n17. writer = WhyLabsWriter()\n\n18. os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = whylabs_org # ORG-ID is case sensitive\n\n19. os.environ[\"WHYLABS_API_KEY\"] = whylabs_key\n\n20. os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = whylabs_project\n\n21.\n\n22. # Read the data\n\n23. data = pd.read_csv(\"https://gist.githubusercontent. com/suhas-ds/a318d2b1dda8d8cbf2d6990a8f0b7e8a/ raw/9b548fab0952dd12b8fdf057188038c8950428f1/loan_dataset.csv\")",
      "content_length": 1275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "412  Machine Learning in Production\n\nHere, the baseline model is built as the focus is on model monitoring. After building the model, it delivered 80.9% accuracy on test data. Now, create additional columns in the existing DataFrame to store model predictions and scores.\n\n1. # Make Prediction\n\n2. y_pred = lr_model.predict(X)\n\n3. # Predict probability\n\n4. y_pred_prob = lr_model.predict_proba(X)\n\n5.\n\n6. # Get a maximum probability value of class\n\n7. y_pred_prob_max = [max(p) for p in y_pred_prob]\n\n8.\n\n9. # Output column name should contain the 'output' word\n\n10. data.rename(columns={'Loan_Status':'output_loan_status'}, inplace=True)\n\n11. data['output_prediction'] = y_pred\n\n12. data['output_score'] = y_pred_prob_max\n\nWith this, you should have a DataFrame with input data and output of the predictions. Here, consider the probability of the class that is predicted by the model. In order to show the output of daily batches, divide that DataFrame into four DataFrames. Subtract 1 day in every for-loop iteration and assign that time stamp to a given dataset.\n\n1. # Splitting the final DataFrame into four\n\n2. df1, df2, df3, df4 = np.array_split(data, 4)\n\n3.\n\n4. daily_batches = [df1, df2, df3, df4]\n\n5.\n\n6. for i, data_frame in enumerate(daily_batches):\n\n7. date_time = dt.datetime.now(tz=dt.timezone.utc) - dt.timedelta(days=i)\n\n8.\n\n9. df = data_frame\n\n10. print(\"logging data for date {}\".format(date_time))\n\n11. results = why.log_classification_metrics(",
      "content_length": 1464,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "Monitoring and Debugging  413\n\n12. df,\n\n13. target_column = \"output_loan_status\",\n\n14. prediction_column = \"output_prediction\",\n\n15. score_column=\"output_score\"\n\n16. )\n\n17.\n\n18. profile = results.profile()\n\n19. profile.set_dataset_timestamp(date_time)\n\n20.\n\n21. print(\"writing profiles to whylabs...\")\n\n22. results.writer(\"whylabs\").write()\n\nSince this is a classification example, use log_classification_metrics() to log the classification model metrics and pass the following four arguments as to why. log_classification_metrics():\n\ninput DataFrame: Entire DataFrame df •\t target_column: Actual target column •\t prediction_column: Predicted target column •\t score_column: score\n\nWhyLabs will identify a column in data as an output column if the column header contains output text.\n\nWhyLabsWriter() will publish the summary on the WhyLabs platform. Go to the Summary tab in WhyLabs. It offers a compact and single view for multiple widgets like Data profiles, Input Health, Model Health, Model Performance, and alerts, as shown in the following figure:\n\nFigure 14.21: WhyLabs–Summary",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "414  Machine Learning in Production\n\nUnder the Profiles tab, an overall data summary is displayed. It includes metrics like histogram, feature count, feature type, frequency of items, and null values, as shown in the following figure:\n\nFigure 14.22: WhyLabs–Profiles\n\nSimilarly, you can see input features metrics, such as data type, total project anomalies, and drift distance, in the Inputs tab, as shown in the following figure:\n\nFigure 14.23: WhyLabs–Inputs\n\nHere, you can see the total count over time, hence the line graph is displayed.",
      "content_length": 543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 441,
      "content": "Monitoring and Debugging  415\n\nUnder the Output tab, you can see similar metrics, but for output columns.\n\nFigure 14.24: WhyLabs–Outputs\n\nFinally, in the Performance tab, you can see the model metrics across different time periods. In the current scenario, you have passed the data on a daily basis. Therefore, you can see a change in accuracy on a daily basis. It also displays other model evaluation metrics for classification models, such as ROC, Precision-Recall, Confusion matrix, and f1 score.\n\nIn the following figure, you can see recent model accuracy with timestamps and plots for accuracy, ROC, Precision-Recall, and Confusion matrix:\n\nFigure 14.25: WhyLabs–Performance",
      "content_length": 680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "416  Machine Learning in Production\n\nThe Performance tab also displays the total input and output data count on a daily basis, as shown in the following figure:\n\nFigure 14.26: WhyLabs–Total input and output count\n\nYou can refer to the whylogs code in Google Colab notebook at the following link:\n\nhttps://colab.research.google.com/gist/suhas-ds/8bc0c895b7aa95839eaa5a5df549 2a42/whylogs-and-whylabs.ipynb\n\nOverall, WhyLabs supports integration with existing pipelines and tools in the MLOps environment. It requires low or zero maintenance as it runs on minimal configuration, and it is dynamic, meaning it can handle changes in data properties and characteristics.\n\nConclusion In this chapter, you learned the importance of monitoring in ML and the fundamentals of ML monitoring. You also studied various techniques to detect and address drift in ML, and you explored different metrics to be tracked for operational monitoring and functional monitoring. Next, you learned to integrate Prometheus and Grafana with FastAPI for operational monitoring. Further on in the chapter, you implemented an ML model monitoring solution with whylogs and WhyLabs. And finally, you covered the alert system to complete the feedback loop of the ML life cycle.\n\nIn the next chapter, you will study the steps to follow after deploying an ML model in production. You will also learn different types of ML attacks and model security.\n\nPoints to remember\n\nMake sure the column name contains the word output in order to show output or prediction data under the Output tab in the WhyLabs platform.",
      "content_length": 1576,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "Monitoring and Debugging  417\n\nML monitoring refers to tracking the performance, errors, metrics, and such of the deployed model and sending alerts (when required) to ensure that the model continues performing above the acceptable threshold.\n\nOnce an ML model is deployed into production, it is essential to monitor it in order to ensure that its performance is up to the mark and that it continues to deliver reliable output seamlessly.\n\nA concept shift occurs when the relationship between independent variables and dependent or target variables changes.\n\nMonitoring should complete the feedback loop, that is, after detecting an issue or failure, it should send a notification to the concerned team or data scientists so that they can take the necessary actions.\n\nMultiple choice questions\n\n1. A _________ occurs when the relationship between independent variables and dependent or target variables changes.\n\na) Concept shift\n\nb) Data drift\n\nc) Covariate Shift\n\nd) Label drift\n\n2. Monitoring infrastructure cost is a type of _________.\n\na) Model monitoring\n\nb) Recurring monitoring\n\nc) Operational monitoring\n\nd) Recurring monitoring\n\nAnswers 1. a\n\n2. c\n\nQuestions\n\n1. What is the concept of shift?\n\n2. What is the role of monitoring in the ML life cycle?\n\n3. What are the different techniques to detect a drift in ML?",
      "content_length": 1322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "418  Machine Learning in Production\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "Post-Productionizing ML Models  419\n\nChapter 15 Post-Productionizing ML Models\n\nIntroduction Till now, you have studied different stages of the ML life cycle and ways to package, deploy, and monitor ML models. You have also learned to implement tests to ensure the integrity and working of modules. You have created local applications out of ML models that can work on Windows and Android devices; deployed models in popular cloud platforms like Microsoft Azure, GCP, and AWS; and built monitoring solutions for operational and ML model monitoring.\n\nHowever, the next part is to get business value out of it. After deployment, when a new model is ready, you will have to decide whether the new model is outperforming the existing model in order to deliver reliable predictions. Model security is also essential after deploying models into production.\n\nStructure In this chapter, the following topics will be covered:\n\nBridging the gap between the ML models and the creation of business value •\t Model security •\t A/B testing",
      "content_length": 1025,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "420  Machine Learning in Production\n\nMLOps is the future\n\nObjectives After studying this chapter, you should be able to mitigate the risk of different types of attacks in the ML life cycle. In this chapter, you will learn the importance of converting MLOps solutions to business value and making better decisions while deploying an ML model by comparing two or more ML models with the help of A/B testing and Multi-Armed Bandit (MAB) testing. You will also study the future scope of MLOps in the industry.\n\nBridging the gap between the ML model and the creation of business value Business users leverage the ML model predictions to build the business strategy and make decisions. Achieving good accuracy doesn’t mean having a good business impact. Sometimes, less accurate models increase a company’s revenue. You need to fill the gap between building ML solutions and creating business value out of them. Data scientists should be able to convert model predictions into easy-to-understand actions. For instance, with a classification model, you can create five buckets based on the probability score of classes, and then rank them such that the highest probability will have the highest ranking.\n\nAfter deploying the ML model into the production environment with a monitoring system, the next step is to make sure the MLOps solution will benefit the business users. You can get feedback from the stakeholders, business users, or customers as they are consuming the model output. For instance, you can ask business users if any feature has become more important than the others after deploying the models so that you can give higher weightage to that feature. This way, you can tweak the model to deliver predictions that are more useful for business users.\n\nData scientists should be able to explain the working of the model (on a high level) to business users, as this will help them to trust the model outputs because many of them are not aware of ML terms. Business users or customers prefer to consume the output in a readable form, such as interactive dashboards or chatbots. You can also use Business Intelligence (BI) tools, such as Tableau or Power BI, to create a dashboard.\n\nModel security Model security is an essential part of MLOps. While processing the data, it might be important to protect the sensitive information it may contain. Attacks can take",
      "content_length": 2366,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "Post-Productionizing ML Models  421\n\nplace in the model training stage or the production stage. In this chapter, you will get familiar with different types of attacks so that you can implement adequate solutions to prevent them.\n\nFollowing are the terms related to model security:\n\nPoisoning: Passing malicious data to the training process aiming to change the model output\n\nExtraction: Reconstructing a new model from the target model that will function the same as the targeted model\n\nEvasion: Attempting to change the label to a particular class by making small variations in input\n\n\n\nInference: Figuring out whether a specific dataset was part of the training data\n\n\n\nInversion: Getting information about training data from the trained model by reverse engineering\n\nAdversarial attack It is a method of generating hostile data inputs. Attackers intentionally send malicious data to the model so that the model makes incorrect predictions. As the model learns the new data patterns, this attack causes an error in the predictions of the model. This malicious input data may seem normal to humans; however, small changes in input data may have a large impact on model predictions.\n\nAdversarial attacks can be classified into two main categories based on the goal of attackers:\n\nTargeted attacks: Attackers attempt to change the label to a particular target. For this, they can change the input data source to a specific target. It requires more time and effort.\n\nNon-targeted attacks: Attackers do not have any specific target that the model should predict. However, they change the label without any specific target. It requires less time and effort.\n\nAttackers can use the following methods for adversarial attacks:\n\nBlack-box method: In this method, attackers can send the input data to the model and get the output based on it.\n\nWhite-box method: In this method, the attacker is aware of almost everything about the ML model, such as training data and the weights assigned to features.",
      "content_length": 1992,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "422  Machine Learning in Production\n\nData poisoning attack In a data poisoning attack, attackers have access to input or source data. They change the input data in such a way that the model will make incorrect predictions that are unreliable for making any business decisions or taking any action. Attackers can add some noise to the original data to cause alterations in the prediction of the model. This attack targets the training data and modifies it intelligently.\n\nDistributed Denial of Service attack (DDoS) It refers to passing complex data to models that will take more time to make predictions. This type of attack limits the use of models for the users. For this, attackers can inject some malware to control the system or server.\n\nData privacy attack Data privacy refers to the confidentiality of Personally Identifiable Information (PII) and Personal Health Information (PHI). Attackers attempt to learn this type of sensitive information through this type of attack. It can be information about the model or training data. ML models like Support Vector Machines (SVMs) may leak the information, as support vectors are data points from training data.\n\nData privacy attacks can be broadly classified into the following categories:\n\nMembership inference attack: The goal of this attack is to determine whether input X is part of the training data. Mostly, shadow models are used in this type of privacy attack. Shadow model training uses a shadow dataset in order to imitate the target model. The output of the shadow models is then passed as an input to the meta-model. Finally, the output of the meta- model is used to extrapolate properties of training data or model. Over-fitted models are prone to data privacy attacks.\n\n\n\nInput inference attack: It is also known as model inversion or data extraction attack. It is the most common type of attack. The goal of this attack is to extract information from the training dataset by reverse engineering the model. It can also target learning the statistical properties of input data, such as probability distribution. Attackers can attempt to learn the features that are not encoded explicitly while training the model.\n\nModel extraction attack: It is also known as a parameter inference attack. The goal of this attack is to learn the hyper-parameters of the model and then reconstruct the model that behaves like the targeted model. Interestingly, over-fitted models are difficult to extract due to high prediction errors based on test data.",
      "content_length": 2504,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "Post-Productionizing ML Models  423\n\nMitigate the risk of model attacks The ML pipeline can be broadly divided into two phases: the training phase and the test phase. You can address various ML model attacks based on these phases.\n\nTraining phase A data scientist performs activities like data gathering, data cleansing, feature engineering, choosing suitable algorithms, hyperparameter tuning, and model building. Attackers target this phase via attacks like data poisoning. If a model is trained on poisoned data, you cannot rely on its predictions.\n\nYou can implement the following techniques to mitigate the risk of attacks during the training phase:\n\nData encryption •\t Protect the integrity of training data •\t Robust statistics •\t Data sanitization\n\nTest phase In this phase, attackers target ML models. Model extraction attacks are common among attackers. They attempt to determine whether input X is part of training data or try to steal model parameters defined during the training phase.\n\nThe following techniques can be implemented to mitigate the risk during the test phase:\n\nAdversarial training •\t Autoencoders •\t Distillation •\t Ensemble techniques •\t Limit the number of requests per user\n\nYou can use a Python library Adversarial Robustness Toolbox (ART) and a command-line tool Counterfit for ML model security. This library will defend against the most common types of ML attacks. It also supports popular ML frameworks, libraries, and data types.\n\nA/B testing A/B testing is widely used in marketing, website designs, and email campaigns to learn and understand user preferences. The goal of A/B testing is to increase the conversion rate, success rate, revenue, and so on.",
      "content_length": 1695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 450,
      "content": "424  Machine Learning in Production\n\nA/B testing involves splitting the audiences or customers from the population into equal sets. These sets will route to the control version and the experimental version. The control version is the existing version, whereas the experimental version is the new or challenger version. First off, define the problem statement for the A/B test, the null hypothesis, and the alternative hypothesis for the problem statement. Next, design the experiment to track and analyze the metric, and then run and validate the experiment. After that, compare the statistics of the output. Finally, make the decision based on the results. If the A/B test is not performed correctly, then its output will be unreliable.\n\nData scientists perform an offline evaluation of ML models by dividing data into training and validation sets. However, the A/B test allows you to perform the online evaluation of ML models by measuring business metrics or success rates. The A/B test can be implemented when you perform operations on training data, such as scaling and normalizing or applying different algorithms, and hyper-parameters while model building. An MLOps engineer or a data scientist can deploy multiple models simultaneously to test models in production. Google Cloud (GCP) and Amazon SageMaker allow the deployment of multiple models into the production behind the same endpoint to decide which model is performing better from the business point of view.\n\nA Multi-Armed Bandit (MAB) is an advanced version of A/B testing. It is more complex than the A/B test, as it uses ML algorithms while dynamically allocating more traffic to the better-performing version and less traffic to the underperforming version based on the data.\n\nMLOps is the future The field of Machine Learning is booming. Industries are making ML a critical part of the business development process, and many new challenges are being addressed by the ML system.\n\nMLOps can handle the complexity and scalability of ML models. Hence, the demand for MLOps is increasing across the industry. There is a shortage of MLOps engineers in the industry, as MLOps is an intersection of Machine Learning and software development. Companies need to set up separate teams for MLOps engineers, or they can upskill their data scientists to execute MLOps tasks. MLOps cut costs and reduce the manual efforts of the overall ML life cycle. This allows data scientists and ML engineers to focus on other productive tasks.\n\nMLOps is getting more popular than DevOps. Your model may be performing well in the local environment; however, if it is not reaching end users or customers, then its business impact is low. More than 85% of ML models are not deployed into the production environment. MLOps engineers can fill the gap between research",
      "content_length": 2809,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "Post-Productionizing ML Models  425\n\nenvironments and the production environment. This is a highly demanding field, as data scientists alone are not enough to convert ML models to business value.\n\nConclusion In this chapter, you learned the importance of model security and studied the various attacks in the ML life cycle. Building ML models and MLOps solutions is not enough; data scientists should work on delivering business value from them. A/B testing and Multi-Armed Bandit (MAB) play crucial roles while comparing existing models against newer models in order to improve the overall outcome and business metric. Finally, you learned how MLOps demand is increasing in the industry.\n\nPoints to remember\n\nModel security is an essential part of MLOps. •\t A/B testing enables you to perform an online evaluation of ML models. •\t Attackers can use the Black-box method or the White-box method while attempting an adversarial attack.\n\nA Python library Adversarial Robustness Toolbox (ART) and a command- line tool Counterfit can be used for ML model security.\n\nA Multi-Armed Bandit (MAB) is an advanced version of A/B testing. It is smart enough to decide which model should get more traffic by evaluating multiple models.\n\nMultiple choice questions\n\n1. In _________, attackers intentionally send malicious data to the model so that the model makes incorrect predictions.\n\na) phishing\n\nb) an adversarial attack\n\nc) shoulder surfing\n\nd) piggybacking",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "426  Machine Learning in Production\n\n2. _________ refers to getting information about training data from the trained model by reverse engineering. a) Extraction b) Evasion c) Inversion d) Monitoring\n\nAnswers 1. b\n\n2. c\n\nQuestions\n\n1. What is a data poisoning attack?\n\n2. What is A/B testing?\n\n3. What are the different techniques that can be implemented to mitigate the model security risk during the training phase?\n\nJoin our book's Discord space Join the book's Discord Workspace for Latest updates, Offers, Tech happenings around the world, New Release and Sessions with the Authors:\n\nhttps://discord.bpbonline.com",
      "content_length": 618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "Symbols\n\n__init__ method 9\n\nA\n\nA/B testing 423, 424\n\nActivate Cloud Shell icon 313\n\nAdversarial Robustness Toolbox\n\n(ART) 423\n\nAmazon CloudWatch Container\n\nInsights 378\n\nAmazon Elastic Compute Cloud\n\n(EC2) 327\n\nAmazon Elastic Container Registry\n\n(ECR) 327, 336, 337\n\nAmazon Elastic Container Service\n\n(ECS) 327, 346\n\ncluster 347\n\ncontainer 347\n\nservice 347\n\nIndex  427\n\nIndex\n\ntask 346\n\ntask definition 346, 349\n\nAmazon Elastic Kubernetes Service (EKS) 327\n\nAmazon SageMaker 328\n\nAmazon Simple Storage\n\nService (S3) 328\n\nAmazon Virtual Private Cloud (Amazon VPC) 351, 352\n\nAmazon Web Services (AWS) 326\n\nAnaconda\n\ninstalling 2\n\nApplication Load Balancer\n\n(ALB) 352, 358-362\n\narray 4\n\nAWS account\n\nsetting up 329, 330\n\nAWS Application Load Balancer\n\n(ALB) 327",
      "content_length": 759,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "428  Machine Learning in Production\n\nAWS CodeBuild 339-344\n\ncontainer registry access, attaching\n\nto service role 345, 346\n\nAWS CodeCommit 331-335\n\nAWS CodePipeline 371, 372\n\nbuild stage, adding 374\n\ndeploy stage, adding 375, 376\n\nmonitoring 378\n\nrunning 376-378\n\nsettings 372\n\nsource stage, adding 373, 374\n\nAWS Command-Line Interface\n\n(CLI) 330\n\nAWS compute services 326, 327\n\nAWS ECS deployment models 347\n\nEC2 instance 347, 348\n\nFargate 348, 349\n\nAWS Fargate 328\n\nAWS Lambda 328\n\nAzure 250, 251\n\ndeployment, with GitHub Actions 251-253\n\ninfrastructure setup 263\n\nAzure account\n\nsetting up 251\n\nAzure App Service\n\nconfiguring, for using GitHub Actions for CD 274-278\n\ncreating 267-270\n\nAzure Container Instance\n\n(ACI) 289, 292, 297\n\nAzure Container Registry (ACR) 252, 263\n\ncreating 264-267\n\nAzure Kubernetes Service (AKS) 250, 292\n\nAzure Machine Learning\n\n(AML) service 249, 279\n\nAzure Machine Learning Studio\n\nexperiments 280\n\nfeatures 279\n\nruns 280-289\n\nworkspace 280\n\nC\n\nCD pipeline\n\nconfiguring 292-298\n\nCI/CD pipeline\n\nbuilding 189\n\ncodebase, developing 189-196\n\ncreating, with Jenkins 202\n\nfor ML 184, 185\n\nGitHub Actions and Heroku, using\n\n227\n\nPersonal Access Token (PAT), creating\n\non GitHub 196\n\nusing, Cloud Build 317, 318\n\nusing, CodePipeline 369, 370\n\nwebhook, creating on GitHub repository 197, 198\n\nCI/CD pipeline, with Jenkins 202\n\ndeployment-status-email 210-217\n\nGitHub-to-container 203-205\n\ntesting 207-210\n\ntraining 205-207\n\nCI pipeline\n\nconfiguring 290-292\n\nclass 9\n\nCloud Alert manager 401\n\nCloud Build 309-311\n\ntrigger, creating 318-322\n\nCloud Source Repositories 304-308, 317\n\nCloudwatch logs 345",
      "content_length": 1625,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "CodeBuild 339\n\ncode editor\n\ninstalling 3\n\nContainer Registry 311, 312\n\nContinuous Delivery (CD) 186\n\nContinuous Deployment 186\n\nContinuous Integration (CI) 185, 186,\n\n339\n\nContinuous Training (CT) 186, 336\n\ncontrol statements and loops\n\nfor loop 6, 7\n\nif...else 6\n\npass statement 8\n\nwhile loop 7\n\nCORS (Cross-Origin Resource\n\nSharing) 191, 229, 254\n\nCSV files\n\nloading 15\n\nsaving 15\n\nD\n\ndata privacy attacks\n\ninput inference attack 422\n\nmembership inference attack 422\n\nmodel extraction attack 422\n\ndata structures 4\n\narray 4\n\ndictionary 4\n\nlist 4\n\nset 5\n\nstring 5\n\ntuple 5\n\ndictionary 4\n\nDistributed Version Control\n\nSystem (DVCS) 18\n\nDocker 112\n\ndetached mode 118\n\nIndex  429\n\nenvironment, setting up 113\n\nHello World example 116\n\ninstalling 113\n\nold versions, uninstalling 113, 114\n\nDocker commands 127\n\nDocker compose 115\n\nDocker container\n\nML model, deploying 122-127\n\nrunning 120, 121\n\nDocker Engine\n\ninstalling 114\n\nDockerfile\n\ncreating 119, 120\n\nDocker Hub rate limit 337-339\n\nDocker image\n\nbuilding 120\n\nDocker objects\n\nDocker container networking 118\n\nDocker containers 118\n\nDockerfile 117\n\nDocker image 117\n\nDomain Name System (DNS) 370\n\ndrift, in ML 386\n\naddressing 389\n\nconcept drift 387, 388\n\ndata drift 387\n\ndata quality issues 389\n\ndetecting techniques 388, 389\n\nmodel, rebuilding 389\n\nmodel, retaining 389\n\nmodel, tuning 389\n\ntypes 386\n\nDRY (Don’t Repeat Yourself) principles\n\n41\n\nE\n\nEC2 (Elastic Compute Cloud) 347",
      "content_length": 1433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "430  Machine Learning in Production\n\nECS service\n\nauto-scaling 367\n\nconfiguration, reviewing 367-369\n\nconfiguring 363, 364\n\ncreating 363\n\nnetwork, configuring 365-367\n\nElastic Container Registry (ECR) 370\n\nenvironment variables\n\npackage, building 71\n\npackage, installing 71, 72\n\npackage usage, with example 73, 74\n\npaths, adding 70\n\nsetting up 70\n\nExcel files\n\nloading 15\n\nsaving 15\n\nexternal Alert manager 401\n\nF\n\nFastAPI 131-139\n\nFlask 143-150\n\nfor loop 6, 7\n\nFunction as a Service (FaaS) 328\n\nfunctions 8\n\nG\n\nGCP account\n\nsetting up 303, 304\n\nGCP cloud shell\n\nusing, for deployment 316\n\nGCP platform\n\nURL 303\n\nGit 17, 18\n\nconfiguration 22\n\nfile, adding 23-26\n\nGUI clients 20\n\ninstalling 20\n\ninstalling, for all platforms 20\n\ninstalling, in Linux 20\n\nstatus, checking 22\n\nworkflow 19\n\nGit commands\n\nchanges 21, 22\n\nnew repository 21\n\nrevert 22\n\nsetup 21\n\nupdate 21\n\nGitHub 17, 19\n\nGitHub account\n\ncreating 20, 21\n\nGitHub Actions 225-270\n\nconfiguration 226\n\nservice principal 273\n\nGit repository\n\ninitializing 22\n\nGNU Privacy Guard (GPG) 114\n\nGoogle Cloud Platform (GCP) 302\n\nservices 302, 303\n\nGoogle Kubernetes Engine\n\n(GKE) 302, 313, 314\n\ndeployment.yaml 314\n\nservice.yaml 315\n\nGrafana 390\n\nGrafana Alert manager 401\n\nGraphical User Interface (GUI) 157\n\nGunicorn 150\n\nH\n\nHello World! example 3\n\nHeroku 219-221\n\ndeployment, with Container\n\nRegistry 225\n\ndeployment, with GitHub\n\nrepository integration 222",
      "content_length": 1409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "pipeline flow 224\n\nsetting up 221\n\nHeroku deployment\n\nPRODUCTION 224\n\nREVIEW APPS 223\n\nSTAGING 223\n\nHeroku Git\n\ndeploying with 222\n\nI\n\nIAM management console 345\n\nif...else statement 6\n\niloc 14, 15\n\ninstrument 390\n\nJ\n\nJenkins 187\n\nconfiguring 199-202\n\ninstallation 187-189\n\nK\n\nKivyMD app 173-178\n\nKubernetes 312\n\nKubernetes Engine API 313\n\nL\n\nlist 4\n\nload balancing 352\n\nApplication Load Balancer\n\n(ALB) 358-362\n\nsecurity group 356-358\n\ntarget group 353-356\n\nloc 14\n\nM\n\nmethod 9\n\nMICE algorithm 59\n\nMicrosoft Azure 250\n\nML as a Service (MLaaS) 250\n\nIndex  431\n\nML-based app\n\nbuilding, with kivy and kivyMD 170-173\n\nbuilding, with Tkinter 160-162\n\nMLflow 78\n\ncomponents 82\n\nenvironment, setting up 79\n\ninstalling 79\n\nMiniconda installation 79-81\n\nsceanarios 78, 79\n\nMLflow projects 94-97\n\nMLflow models 97-100\n\nMLflow registry 100, 101\n\nfeatures 102, 103\n\nMLflow server, starting 103-108\n\nMySQLdb module, installing 102\n\nMySQL server, setting 101\n\nMLflow tracking 83\n\nartifacts 84\n\nlog data, into run 84-94\n\nmetric 83\n\nparameters 83\n\nsource 83\n\nstart and end time 83\n\nML life cycle 30\n\nbusiness impact 31\n\ndata collection 31, 32\n\ndata preparation 32\n\nfeature engineering 32, 33\n\nmodel building 33\n\nmodel deployment 34\n\nmodel evaluation 34\n\nmodel testing 34\n\nmodel training 33\n\nmonitoring 35\n\noptimization 35",
      "content_length": 1307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "432  Machine Learning in Production\n\nML model\n\ndeployment, with Azure DevOps and Azure ML 278, 279\n\nde-serializing 48\n\ngap bridging between business\n\nvalue creation 420\n\npost-productionizing 419\n\nrequirements file 47, 48\n\nserializing 48\n\nvirtual environments 46, 47\n\nML model attacks\n\nrisk mitigation 423\n\nML monitoring\n\nconstraints, for data quality validation 404-407\n\nfunctional monitoring 383\n\nfundamentals 383-385\n\nmetrics 385, 386\n\nmetrics tracking 383, 384\n\noperational monitoring 383\n\nscalable integration 383\n\nwith whylogs and WhyLabs 402\n\nMLOps 39, 40\n\nautomation 41\n\nbenefits 40\n\nefficient management 41\n\nfeatures 424\n\nreproducibility 41\n\ntracking and feedback loop 42\n\nML packages\n\nbusiness problem 52\n\nclassification_v1.pkl 65\n\nconfiguration module 55, 56\n\ndata 52\n\ndata_management.py 57\n\ndeveloping 51, 53\n\n__init__.py module 54\n\nMANIFEST.in file 54, 55\n\nML model, building 53\n\npipeline.py 62\n\npredict.py module 63\n\npreprocessors.py 58-62\n\npytest.ini 68\n\nrequirements.txt 64\n\nsdist 70\n\nsetup.py 66\n\ntest_predict.py 68-70\n\ntrain_pipeline.py 65\n\nversion 68\n\nwheel 70\n\nmodel deployment 35\n\nbatch predictions 35\n\nmobile and edge devices 36, 37\n\nreal-time predictions 37\n\nweb service/REST API 36\n\nmodel deployment challenges, in\n\nproduction environment\n\ndata-related challenges 38\n\nportability 38\n\nrobustness 38, 39\n\nscalability 38\n\nsecurity 39\n\nteam coordination 37, 38\n\nmodel security 420, 421\n\nadversarial attack 421\n\ndata poisoning attack 422, 423\n\ndata privacy attack 422\n\nDistributed Denial of Service attack (DDoS) 422\n\nmonitoring 382, 383\n\nMulti-Armed Bandit (MAB) 424\n\nN\n\nNetwork Load Balancer (NLB) 327",
      "content_length": 1622,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "NGINX 150-154\n\nNumPy 10\n\nNumPy array 10\n\nreshaping 12\n\nO\n\nobject 9\n\nObject Oriented Programming (OOP) 9\n\nclass 9\n\n__init__ method 9\n\nmethod 9\n\nobject 9\n\nself 9\n\noperational monitoring\n\nwith Prometheus and Grafana 390-401\n\nP\n\nPandas 12\n\nPandas DataFrame\n\nusing 12, 13\n\npass statement 8\n\nPersonal Access Token (PAT) 196\n\nPersonal Health Information (PHI) 422\n\nPersonally Identifiable Information\n\n(PII) 422\n\nPlatform as a Service (PaaS) 219, 250\n\nprod.workflow.yml 270-272\n\nPrometheus 390\n\npytest fixtures 49\n\nPython\n\ninstalling 2\n\ninstalling, on Mac OS 2\n\ninstalling, on Linux 2\n\ninstalling on Windows 2\n\nPython 101 1\n\nPython app\n\nconverting, into Android app 179, 180\n\nIndex  433\n\nPython-based Tkinter app\n\nconverting, into Windows EXE file 167-169\n\nPython code\n\ntesting, with pytest 48, 49\n\nPython file\n\nexecuting 3\n\nPython Package Index (PyPI) 3\n\nPython packaging\n\nand dependency management 49\n\nmodular programming 50\n\nmodule 50\n\npackage 50, 51\n\nR\n\nRepresentational State Transfer (REST)\n\n130\n\nRest APIs 130\n\nrisk mitigation, ML model attacks\n\ntest phase 423\n\ntraining phase 423\n\nS\n\nService Principal (automatic) 282\n\nset 5\n\nsrc directory\n\ndocker-compose.yml 234, 259\n\nDockerfile 234, 259\n\n.gitignore 229, 253\n\nmain.py 229-232, 254-257\n\nproduction.yml 239-243\n\npytest.ini 235, 260\n\nrequirements.txt 258\n\nrequirements.txt file 233\n\nruntime.txt 235, 260\n\nstart.sh 235, 260\n\ntest.py 235, 236, 260, 261\n\ntox.ini 237, 262, 263",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "434  Machine Learning in Production\n\nworkflow.yml 238\n\nworkflow.yml workflow 244-246\n\nStaging 241\n\nStreamlit 139-143\n\nstring 5\n\nT\n\ntask definition 349, 350\n\ntask, running with 350\n\nTiny Machine Learning (TinyML) 36\n\nTkinter 158\n\nfeatures 160\n\nHello World app 159\n\nML-based app, building 160-162\n\nTkinter app 163-167\n\nTool Command Language (TCL) 158\n\ntuple 5\n\nV\n\nVirtual Machines (VM) 250\n\nVirtual Private Cloud (VPC) 303\n\nW\n\nWeb Server Gateway Interface (WSGI)\n\n143\n\nwhile loop 7\n\nWhyLabs 402, 407\n\naccess token 408\n\nfeatures 402\n\nNotifications page 408\n\noutput tab 415, 416\n\nprofile tab 414\n\nproject management 407, 408\n\nuser management 409-413\n\nwhylogs 403, 404",
      "content_length": 664,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": " i",
      "content_length": 3,
      "extraction_method": "Unstructured"
    }
  ]
}