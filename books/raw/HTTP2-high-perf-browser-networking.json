{
  "metadata": {
    "title": "HTTP2-high-perf-browser-networking",
    "author": "Ilya Grigorik",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 37,
    "conversion_date": "2025-12-19T18:46:42.674999",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "HTTP2-high-perf-browser-networking.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 2-12)",
      "start_page": 2,
      "end_page": 12,
      "detection_method": "topic_boundary",
      "content": "HTTP/2 A New Excerpt from High Performance Browser Networking\n\nIlya Grigorik\n\nHTTP/2: A New Excerpt from High Performance Browser Networking by Ilya Grigorik\n\nCopyright © 2015 Ilya Grigorik. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://safaribooksonline.com). For more sales department: 800-998-9938 or corporate@oreilly.com.\n\ninformation,\n\ncontact our\n\ncorporate/institutional\n\nEditor: Brian Anderson\n\nInterior Designer: David Futato Cover Designer: Karen Montgomery\n\nMay 2015:\n\nFirst Edition\n\nRevision History for the First Edition 2015-05-01: First Release\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. HTTP/2: A New Excerpt from High Performance Browser Networking and related trade dress are trademarks of O’Reilly Media, Inc.\n\nWhile the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limi‐ tation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsi‐ bility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-491-93248-3\n\n[LSI]\n\nTable of Contents\n\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nHTTP/2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Brief History of SPDY and HTTP/2 2 Design and Technical Goals 4 Brief Introduction to Binary Framing 23 Next steps with HTTP/2 29\n\nvii\n\nv\n\nPreface\n\nHTTP/2 is here. The standard is approved, all popular browsers have committed to support it, or have already enabled it for their users, and many popular sites are already leveraging HTTP/2 to deliver improved performance. In fact, in a short span of just a few months after the HTTP/2 and HPACK standards were approved in early 2015, their usage on the web has already surpassed that of SPDY! Which is to say, this is well tested and proven technology that is ready for production.\n\nSo, what’s new in HTTP/2, and why or how will your application benefit from it? To answer that we need to take an under the hood look at the new protocol, its features, and talk about its implications for how we design, deploy, and deliver our applications. Under‐ standing the design and technical goals of HTTP/2 will explain both how, and why, some of our existing best practices are no longer rele‐ vant—sometimes harmful, even—and what new capabilities we have at our disposal to further optimize our applications.\n\nWith that, there’s no time to waste, let’s dive in!\n\nvii\n\nHTTP/2\n\nHTTP/2 will make our applications faster, simpler, and more robust —a rare combination—by allowing us to undo many of the HTTP/1.1 workarounds previously done within our applications and address these concerns within the transport layer itself. Even better, it also opens up a number of entirely new opportunities to optimize our applications and improve performance!\n\nThe primary goals for HTTP/2 are to reduce latency by enabling full request and response multiplexing, minimize protocol overhead via efficient compression of HTTP header fields, and add support for request prioritization and server push. To implement these require‐ ments, there is a large supporting cast of other protocol enhance‐ ments, such as new flow control, error handling, and upgrade mech‐ anisms, but these are the most important features that every web developer should understand and leverage in their applications.\n\nHTTP/2 does not modify the application semantics of HTTP in any way. All of the core concepts, such as HTTP methods, status codes, URIs, and header fields, remain in place. Instead, HTTP/2 modifies how the data is formatted (framed) and transported between the cli‐ ent and server, both of whom manage the entire process, and hides all the complexity from our applications within the new framing layer. As a result, all existing applications can be delivered without modification. That’s the good news.\n\nHowever, we are not just interested in delivering a working applica‐ tion; our goal is to deliver the best performance! HTTP/2 enables a number of new optimizations that our applications can leverage,\n\n1\n\nwhich were previously not possible, and our job is to make the best of them. Let’s take a closer look under the hood.\n\nWhy not HTTP/1.2? To achieve the performance goals set by the HTTP Working Group, HTTP/2 introduces a new binary framing layer that is not back‐ ward compatible with previous HTTP/1.x servers and clients. Hence the major protocol version increment to HTTP/2.\n\nThat said, unless you are implementing a web server or a custom client by working with raw TCP sockets, you won’t see any differ‐ ence: all the new, low-level framing is performed by the client and server on your behalf. The only observable differences will be improved performance and availability of new capabilities like request prioritization, flow control, and server push!\n\nBrief History of SPDY and HTTP/2 SPDY was an experimental protocol, developed at Google and announced in mid-2009, whose primary goal was to try to reduce the load latency of web pages by addressing some of the well-known performance limitations of HTTP/1.1. Specifically, the outlined project goals were set as follows:\n\nTarget a 50% reduction in page load time (PLT).\n\nAvoid the need for any changes to content by website authors.\n\nMinimize deployment complexity, avoid changes in network infrastructure.\n\nDevelop this new protocol in partnership with the open-source community.\n\nGather real performance data to (in)validate the experimental protocol.\n\n2\n\n| HTTP/2\n\nTo achieve the 50% PLT improvement, SPDY aimed to make more efficient use of the underlying TCP con‐ nection by introducing a new binary framing layer to enable request and response multiplexing, prioritiza‐ tion, and header compression.1\n\nNot long after the initial announcement, Mike Belshe and Roberto Peon, both software engineers at Google, shared their first results, documentation, and source code for the experimental implementa‐ tion of the new SPDY protocol:\n\nSo far we have only tested SPDY in lab conditions. The initial results are very encouraging: when we download the top 25 web‐ sites over simulated home network connections, we see a signifi‐ cant improvement in performance—pages loaded up to 55% faster.\n\n— Chromium Blog A 2x Faster Web\n\nFast-forward to 2012 and the new experimental protocol was sup‐ ported in Chrome, Firefox, and Opera, and a rapidly growing num‐ ber of sites, both large (e.g., Google, Twitter, Facebook) and small, were deploying SPDY within their infrastructure. In effect, SPDY was on track to become a de facto standard through growing indus‐ try adoption.\n\nObserving this trend, the HTTP Working Group (HTTP-WG) kicked off a new effort to take the lessons learned from SPDY, build and improve on them, and deliver an official “HTTP/2” standard: a new charter was drafted, an open call for HTTP/2 proposals was made, and after a lot of discussion within the working group, the SPDY specification was adopted as a starting point for the new HTTP/2 protocol.\n\nOver the next few years, SPDY and HTTP/2 would continue to coevolve in parallel, with SPDY acting as an experimental branch that was used to test new features and proposals for the HTTP/2 standard: what looks good on paper may not work in practice, and vice versa, and SPDY offered a route to test and evaluate each pro‐ posal before its inclusion in the HTTP/2 standard. In the end, this process spanned three years and resulted in a over a dozen inter‐ mediate drafts:\n\nMar, 2012: Call for proposals for HTTP/2\n\n1 See “Latency as a Performance Bottleneck” at http://hpbn.co/latency-bottleneck\n\nBrief History of SPDY and HTTP/2\n\n|\n\n3\n\nNov, 2012: First draft of HTTP/2 (based on SPDY)\n\nAug, 2014: HTTP/2 draft-17 and HPACK draft-12 are published\n\nAug, 2014: Working Group last call for HTTP/2\n\nFeb, 2015: IESG approved HTTP/2\n\nMay, 2015: HTTP/2 and HPACK RFC’s (7540, 7541) are pub‐ lished\n\nIn early 2015 the IESG reviewed and approved the new HTTP/2 standard for publication. Shortly after that, the Google Chrome team announced their schedule to deprecate SPDY and NPN exten‐ sion for TLS:\n\nHTTP/2’s primary changes from HTTP/1.1 focus on improved per‐ formance. Some key features such as multiplexing, header com‐ pression, prioritization and protocol negotiation evolved from work done in an earlier open, but non-standard protocol named SPDY. Chrome has supported SPDY since Chrome 6, but since most of the benefits are present in HTTP/2, it’s time to say good‐ bye. We plan to remove support for SPDY in early 2016, and to also remove support for the TLS extension named NPN in favor of ALPN in Chrome at the same time. Server developers are strongly encouraged to move to HTTP/2 and ALPN.\n\nWe’re happy to have contributed to the open standards process that led to HTTP/2, and hope to see wide adoption given the broad industry engagement on standardization and implementation.\n\n— Chromium Blog Hello HTTP/2, Goodbye SPDY\n\nThe coevolution of SPDY and HTTP/2 enabled server, browser, and site developers to gain real-world experience with the new protocol as it was being developed. As a result, the HTTP/2 standard is one of the best and most extensively tested standards right out of the gate. By the time HTTP/2 was approved by the IESG, there were dozens of thoroughly tested and production-ready client and server implementations. In fact, just weeks after the final protocol was approved, many users were already enjoying its benefits as several popular browsers, and many sites, deployed full HTTP/2 support.\n\nDesign and Technical Goals First versions of the HTTP protocol were intentionally designed for simplicity of implementation: HTTP/0.9 was a one-line protocol to bootstrap the World Wide Web; HTTP/1.0 documented the popular\n\n4\n\n| HTTP/2\n\nextensions to HTTP/0.9 in an informational standard; HTTP/1.1 introduced an official IETF standard2. As such, HTTP/0.9-1.x deliv‐ ered exactly what it set out to do: HTTP is one of the most ubiqui‐ tous and widely adopted application protocols on the Internet.\n\nUnfortunately, implementation simplicity also came at the cost of application performance: HTTP/1.x clients need to use multiple connections to achieve concurrency and reduce latency; HTTP/1.x does not compress request and response headers, causing unneces‐ sary network traffic; HTTP/1.x does not allow effective resource pri‐ oritization, resulting in poor use of the underlying TCP connection; and so on.\n\nThese limitations were not fatal, but as the web applications contin‐ ued to grow in their scope, complexity, and importance in our everyday lives, they imposed a growing burden on both the develop‐ ers and users of the Web, which is the exact gap that HTTP/2 was designed to address:\n\nHTTP/2 enables a more efficient use of network resources and a reduced perception of latency by introducing header field compres‐ sion and allowing multiple concurrent exchanges on the same con‐ nection…. Specifically, it allows interleaving of request and response messages on the same connection and uses an efficient coding for HTTP header fields. It also allows prioritization of requests, letting more important requests complete more quickly, further improving performance.\n\nThe resulting protocol is more friendly to the network, because fewer TCP connections can be used in comparison to HTTP/1.x. This means less competition with other flows, and longer-lived connections, which in turn leads to better utilization of available network capacity. Finally, HTTP/2 also enables more efficient pro‐ cessing of messages through use of binary message framing.\n\n— Draft 17 Hypertext Transfer Protocol version 2\n\nIt is important to note that HTTP/2 is extending, not replacing, the previous HTTP standards. The application semantics of HTTP are the same, and no changes were made to the offered functionality or core concepts such as HTTP methods, status codes, URIs, and header fields—these changes were explicitly out of scope for the HTTP/2 effort. That said, while the high-level API remains the same, it is important to understand how the low-level changes\n\n2 See “Brief History of HTTP” at http://hpbn.co/http-history\n\nDesign and Technical Goals\n\n|\n\n5\n\naddress the performance limitations of the previous protocols. Let’s take a brief tour of the binary framing layer and its features.\n\nBinary Framing Layer At the core of all of the performance enhancements of HTTP/2 is the new binary framing layer (Figure 1-1), which dictates how the HTTP messages are encapsulated and transferred between the client and server.\n\nFigure 1-1. HTTP/2 binary framing layer\n\nThe “layer” refers to a design choice to introduce a new optimized encoding mechanism between the socket interface and the higher HTTP API exposed to our applications: the HTTP semantics, such as verbs, methods, and headers, are unaffected, but the way they are encoded while in transit is what’s different. Unlike the newline delimited plaintext HTTP/1.x protocol, all HTTP/2 communication is split into smaller messages and frames, each of which is encoded in binary format.\n\nAs a result, both client and server must use the new binary encoding mechanism to understand each other: an HTTP/1.x client won’t understand an HTTP/2 only server, and vice versa. Thankfully, our applications remain blissfully unaware of all these changes, as the\n\n6\n\n| HTTP/2\n\nclient and server perform all the necessary framing work on our behalf.\n\nThe Pros and Cons of Binary Protocols ASCII protocols are easy to inspect and get started with. However, they are not as efficient and are typically harder to implement cor‐ rectly: optional whitespace, varying termination sequences, and other quirks make it hard to distinguish the protocol from the pay‐ load and lead to parsing and security errors. By contrast, while binary protocols may take more effort to get started with, they tend to lead to more performant, robust, and provably correct imple‐ mentations.\n\nHTTP/2 uses binary framing. As a result, you will need a tool that understands it to inspect and debug the protocol—e.g., Wireshark or equivalent. In practice, this is less of an issue than it seems, since you would have to use the same tools to inspect the encrypted TLS flows—which also rely on binary framing3—carrying HTTP/1.x and HTTP/2 data.\n\nStreams, Messages, and Frames The introduction of the new binary framing mechanism changes how the data is exchanged (Figure 1-2) between the client and server. To describe this process, let’s familiarize ourselves with the HTTP/2 terminology:\n\nStream\n\nA bidirectional flow of bytes within an established connection, which may carry one or more messages.\n\nMessage\n\nA complete sequence of frames that map to a logical request or response message.\n\nFrame\n\nThe smallest unit of communication in HTTP/2, each contain‐ ing a frame header, which at a minimum identifies the stream to which the frame belongs.\n\n3 See “TLS Record Protocol” at http://hpbn.co/tls-record\n\nDesign and Technical Goals\n\n|\n\n7",
      "page_number": 2
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 13-20)",
      "start_page": 13,
      "end_page": 20,
      "detection_method": "topic_boundary",
      "content": "Figure 1-2. HTTP/2 Streams, messages, and frames\n\nAll communication is performed over a single TCP connection that can carry any number of bidirectional streams.\n\nEach stream has a unique identifier and optional priority infor‐ mation that is used to carry bidirectional messages.\n\nEach message is a logical HTTP message, such as a request, or response, which consists of one or more frames.\n\nThe frame is the smallest unit of communication that carries a specific type of data—e.g., HTTP headers, message payload, and so on. Frames from different streams may be interleaved and then reassembled via the embedded stream identifier in the header of each frame.\n\nIn short, HTTP/2 breaks down the HTTP protocol communication into an exchange of binary-encoded frames, which are then mapped to messages that belong to a particular stream, all of which are mul‐ tiplexed within a single TCP connection. This is the foundation that enables all other features and performance optimizations provided by the HTTP/2 protocol.\n\nRequest and Response Multiplexing With HTTP/1.x, if the client wants to make multiple parallel requests to improve performance, then multiple TCP connections must be used4. This behavior is a direct consequence of the\n\n4 See “Using Multiple TCP Connections” at http://hpbn.co/http-multiple-connections\n\n8\n\n| HTTP/2\n\nHTTP/1.x delivery model, which ensures that only one response can be delivered at a time (response queuing) per connection. Worse, this also results in head-of-line blocking and inefficient use of the underlying TCP connection.\n\nThe new binary framing layer in HTTP/2 removes these limitations, and enables full request and response multiplexing, by allowing the client and server to break down an HTTP message into independent frames (Figure 1-3), interleave them, and then reassemble them on the other end.\n\nFigure 1-3. HTTP/2 request and response multiplexing within a shared connection\n\nThe snapshot in Figure 1-3 captures multiple streams in flight within the same connection: the client is transmitting a DATA frame (stream 5) to the server, while the server is transmitting an inter‐ leaved sequence of frames to the client for streams 1 and 3. As a result, there are three parallel streams in flight!\n\nThe ability to break down an HTTP message into independent frames, interleave them, and then reassemble them on the other end is the single most important enhancement of HTTP/2. In fact, it introduces a ripple effect of numerous performance benefits across the entire stack of all web technologies, enabling us to:\n\nInterleave multiple requests in parallel without blocking on any one\n\nInterleave multiple responses in parallel without blocking on any one\n\nUse a single connection to deliver multiple requests and respon‐ ses in parallel\n\nDesign and Technical Goals\n\n|\n\n9\n\nRemove unnecessary HTTP/1.x workarounds5, such as con‐ catenated files, image sprites, and domain sharding\n\nDeliver lower page load times by eliminating unnecessary latency and improving utilization of available network capacity\n\nAnd much more…\n\nThe new binary framing layer in HTTP/2 resolves the head-of-line blocking problem found in HTTP/1.x and eliminates the need for multiple connections to enable parallel processing and delivery of requests and responses. As a result, this makes our applications faster, simpler, and cheaper to deploy.\n\nStream Prioritization Once an HTTP message can be split into many individual frames, and we allow for frames from multiple streams to be multiplexed, the order in which the frames are interleaved and delivered both by the client and server becomes a critical performance consideration. To facilitate this, the HTTP/2 standard allows each stream to have an associated weight and dependency:\n\nEach stream may be assigned an integer weight between 1 and 256\n\nEach stream may be given an explicit dependency on another stream\n\nThe combination of stream dependencies and weights allows the cli‐ ent tree” to construct and communicate a “prioritization (Figure 1-4) that expresses how it would prefer to receive the responses. In turn, the server can use this information to prioritize stream processing by controlling the allocation of CPU, memory, and other resources, and once the response data is available, alloca‐ tion of bandwidth to ensure optimal delivery of high-priority responses to the client.\n\n5 See “Optimizing for HTTP/1.x” at http://hpbn.co/optimizing-http1x\n\n10\n\n| HTTP/2\n\nFigure 1-4. HTTP/2 stream dependencies and weights\n\nA stream dependency within HTTP/2 is declared by referencing the unique identifier of another stream as its parent; if omitted the stream is said to be dependent on the “root stream.” Declaring a stream dependency indicates that, if possible, the parent stream should be allocated resources ahead of its dependencies—e.g., please process and deliver response D before response C.\n\nStreams that share the same parent can be prioritized with respect to each other by assigning a weight to each stream: the relative priority of the stream is proportional to its weight as compared to its siblings —e.g., resource A has a weight of 12, and B a weight of 4; A should receive two-thirds of available resources.\n\nLet’s work through a few hands-on examples in Figure 1-4. From left to right:\n\n1. Neither stream A nor B specify a parent dependency and are said to be dependent on the implicit “root stream”; A has a weight of 12, and B has a weight of 4. Thus, based on propor‐ tional weights: A should be assigned two-thirds of available resources, and B should receive the remaining one-third.\n\n2. D is dependent on the root stream; C is dependent on D. Thus, D should receive full allocation of resources ahead of C. The weights are inconsequential because C’s dependency communi‐ cates a stronger preference.\n\n3. D should receive full allocation of resources ahead of C; C should receive full allocation of resources ahead of A and B; A should receive two-thirds of available resources, and B should receive the remaining one-third.\n\nDesign and Technical Goals\n\n|\n\n11\n\n4. D should receive full allocation of resources ahead of E and C; E and C should receive equal allocation ahead of A and B; A and B should receive proportional allocation based on their weights.\n\nAs the above examples illustrate, the combination of stream depen‐ dencies and weights provides an expressive language for resource prioritization, which is a critical feature for improving browsing performance where we have many resource types with different dependencies and weights. Even better, the HTTP/2 protocol also allows the client to update these preferences at any point, which ena‐ bles further optimizations in the browser—e.g., we can change dependencies and reallocate weights in response to user interaction and other signals.\n\nStream dependencies and weights express a transport preference, not a requirement, and as such do not guar‐ antee a particular processing or transmission order. That is, the client cannot force the server to process the stream in particular order using stream prioritization. While this may seem counter-intuitive, it is, in fact, the desired behavior: we do not want to block the server from making progress on a lower-priority resource if a higher-priority resource is blocked.\n\nBrowser Request Prioritization and HTTP/2 Not all resources have equal priority when rendering a page in the browser: the HTML document itself is critical to construct the DOM; the CSS is required to construct the CSSOM; both DOM and CSSOM construction can be blocked on JavaScript resources6; and remaining resources, such as images, are often fetched with lower priority.\n\nTo accelerate the load time of the page, all modern browsers priori‐ tize requests based on type of asset, their location on the page, and even learned priority from previous visits—e.g., if the rendering was blocked on a certain asset in a previous visit, then the same asset may be prioritized higher in the future.\n\nWith HTTP/1.x, the browser has limited ability to leverage above priority data: the protocol does not support multiplexing, and there\n\n6 See “DOM, CSSOM, and JavaScript” at http://hpbn.co/dom-cssom-\n\njavascript\n\n12\n\n| HTTP/2\n\nis no way to communicate request priority to the server. Instead, it must rely on the use of parallel connections, which enables limited parallelism of up to six requests per origin. As a result, requests are queued on the client until a connection is available, which adds unnecessary network latency. In theory, “HTTP Pipelining” in High Performance Browser Networking tried to partially address this problem, but in practice it has failed to gain adoption.\n\nHTTP/2 resolves these inefficiencies: request queuing and head-of- line blocking is eliminated because the browser can dispatch all requests the moment they are discovered, and the browser can communicate its stream prioritization preference via stream depen‐ dencies and weights, allowing the server to further optimize response delivery.\n\nOne Connection Per Origin With the new binary framing mechanism in place, HTTP/2 no longer needs multiple TCP connections to multiplex streams in par‐ allel; each stream is split into many frames, which can be interleaved and prioritized. As a result, all HTTP/2 connections are persistent, and only one connection per origin is required, which offers numer‐ ous performance benefits.\n\nFor both SPDY and HTTP/2, the killer feature is arbitrary multi‐ plexing on a single well congestion controlled channel. It amazes me how important this is and how well it works. One great metric around that which I enjoy is the fraction of connections created that carry just a single HTTP transaction (and thus make that transaction bear all the overhead). For HTTP/1, 74% of our active connections carry just a single transaction—persistent connections just aren’t as helpful as we all want. But in HTTP/2 that number plummets to 25%. That’s a huge win for overhead reduction.\n\n— Patrick McManus HTTP/2 is Live in Firefox\n\nMost HTTP transfers are short and bursty, whereas TCP is opti‐ mized for long-lived, bulk data transfers. By reusing the same con‐ nection, HTTP/2 is able to both make more efficient use of each TCP connection and also significantly reduce the overall protocol overhead. Further, the use of fewer connections reduces the memory and processing footprint along the full connection path (i.e., client, intermediaries, and origin servers), which reduces the overall opera‐ tional costs and improves network utilization and capacity. As a result, the move to HTTP/2 should not only reduce the network\n\nDesign and Technical Goals\n\n|\n\n13\n\nlatency, but also help improve throughput and reduce the opera‐ tional costs.\n\nReduced number of connections is a particularly important feature for improving performance of HTTPS deployments: this translates to fewer expensive TLS handshakes, better session re-use, and an overall reduction in required client and server resources.\n\nPacket Loss, High-RTT Links, and HTTP/2 Performance Wait, I hear you say, we listed the benefits of using one TCP con‐ nection per origin but aren’t there some potential downsides? Yes, there are.\n\nWe have eliminated head-of-line blocking from HTTP, but there is still head-of-line blocking at the TCP level7.\n\nEffects of bandwidth-delay product may limit connection throughput if TCP window scaling is disabled.\n\nWhen packet loss occurs, the TCP congestion window size is reduced8, which reduces the maximum throughput of the entire connection.\n\nEach of the items in this list may adversely affect both the through‐ put and latency performance of an HTTP/2 connection. However, despite these limitations, the move to multiple connections would result in its own performance tradeoffs:\n\nLess effective header compression due to distinct compression contexts\n\nLess effective request prioritization due to distinct TCP streams\n\nLess effective utilization of each TCP stream and higher likeli‐ hood of congestion due to more competing flows\n\nIncreased resource overhead due to more TCP flows\n\nThe above pros and cons are not an exhaustive list, and it is always possible to construct specific scenarios where either one or more\n\n7 See “Head-of-Line Blocking” at http://hpbn.co/tcp-hol\n\n8 See “Congestion Avoidance” at http://hpbn.co/congestion-avoidance\n\n14\n\n| HTTP/2\n\nconnections may prove to be beneficial. However, the experimental evidence of deploying HTTP/2 in the wild showed that a single connection is the preferred deployment strategy:\n\nIn tests so far, the negative effects of head-of-line blocking (espe‐ cially in the presence of packet loss) is outweighed by the benefits of compression and prioritization.\n\n— Draft 2 Hypertext Transfer Protocol version 2\n\nAs with all performance optimization processes, the moment you remove one performance bottleneck, you unlock the next one. In the case of HTTP/2, TCP may be it. Which is why, once again, a well-tuned TCP stack on the server is such a critical optimization criteria for HTTP/2.\n\nThere is ongoing research to address these concerns and to improve TCP performance in general: TCP Fast Open, Proportional Rate Reduction, increased initial congestion window, and more. Having said that, it is important to acknowledge that HTTP/2, like its pred‐ ecessors, does not mandate the use of TCP. Other transports, such as UDP, are not outside the realm of possibility as we look to the future.\n\nFlow Control Flow control is a mechanism to prevent the sender from over‐ whelming the receiver with data it may not want or be able to pro‐ cess: the receiver may be busy, under heavy load, or may only be willing to allocate a fixed amount of resources for a particular stream. For example, the client may have requested a large video stream with high priority, but the user has paused the video and the client now wants to pause or throttle its delivery from the server to avoid fetching and buffering unnecessary data. Alternatively, a proxy server may have a fast downstream and slow upstream con‐ nections and similarly wants to regulate how quickly the down‐ stream delivers data to match the speed of upstream to control its resource usage; and so on.\n\nDo the above requirements remind you of TCP flow control? They should, as the problem is effectively identical9. However, because the HTTP/2 streams are multiplexed within a single TCP connection, TCP flow control is both not granular enough, and does not provide\n\n9 See “Flow Control” at http://.co/flow-control\n\nDesign and Technical Goals\n\n|\n\n15",
      "page_number": 13
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 21-28)",
      "start_page": 21,
      "end_page": 28,
      "detection_method": "topic_boundary",
      "content": "the necessary application-level APIs to regulate the delivery of indi‐ vidual streams. To address this, HTTP/2 provides a set of simple building blocks that allow the client and server to implement their own stream- and connection-level flow control:\n\nFlow control is directional. Each receiver may choose to set any window size that it desires for each stream and the entire con‐ nection.\n\nFlow control is credit-based. Each receiver advertises its initial connection and stream flow control window (in bytes), which is reduced whenever the sender emits a DATA frame and incremen‐ ted via a WINDOW_UPDATE frame sent by the receiver.\n\nFlow control cannot be disabled. When the HTTP/2 connection is established the client and server exchange SETTINGS frames, which set the flow control window sizes in both directions. The default value of the flow control window is set to 65,535 bytes, but the receiver can set a large maximum window size (231 − 1 bytes) and maintain it by sending a WINDOW_UPDATE frame whenever any data is received.\n\nFlow control is hop-by-hop, not end-to-end. That is, an inter‐ mediary can use it to control resource use and implement resource allocation mechanisms based on own criteria and heu‐ ristics.\n\nHTTP/2 does not specify any particular algorithm for implementing flow control. Instead, it provides the simple building blocks and defers the implementation to the client and server, which can use it to implement custom strategies to regulate resource use and alloca‐ tion, as well as implement new delivery capabilities that may help improve both the real and perceived performance10 of our web applications.\n\nFor example, application-layer flow control allows the browser to fetch only a part of a particular resource, put the fetch on hold by reducing the stream flow control window down to zero, and then resume it later—e.g., fetch a preview or first scan of an image, dis‐ play it and allow other high priority fetches to proceed, and resume the fetch once more critical resources have finished loading.\n\n10 See “Speed, Performance, and Human Perception” at http://hpbn.co/human-perception\n\n16\n\n| HTTP/2\n\nServer Push Another powerful new feature of HTTP/2 is the ability of the server to send multiple responses for a single client request. That is, in addition to the response to the original request, the server can push additional resources to the client (Figure 1-5), without the client having to request each one explicitly!\n\nFigure 1-5. Server initiates new streams (promises) for push resources\n\nHTTP/2 breaks away from the strict request-response semantics and enables one-to-many and server- initiated push workflows that open up a world of new interaction possibilities both within and outside the browser. This is an enabling feature that will have important long-term consequences both for how we think about the protocol, and where and how it is used.\n\nWhy would we need such a mechanism in a browser? A typical web application consists of dozens of resources, all of which are discov‐ ered by the client by examining the document provided by the server. As a result, why not eliminate the extra latency and let the server push the associated resources ahead of time? The server already knows which resources the client will require; that’s server push.\n\nIn fact, if you have ever inlined a CSS, JavaScript, or any other asset via a data URI11, then you already have hands-on experience with server push! By manually inlining the resource into the document, we are, in effect, pushing that resource to the client, without waiting\n\n11 See “Resource Inlining” at http://hpbn.co/inlining\n\nDesign and Technical Goals\n\n|\n\n17\n\nfor the client to request it. With HTTP/2 we can achieve the same results, but with additional performance benefits:\n\nPushed resources can be cached by the client\n\nPushed resources can be reused across different pages\n\nPushed resources can be multiplexed alongside other resources\n\nPushed resources can be prioritized by the server\n\nPushed resources can be declined by the client\n\nEach pushed resource is a stream that, unlike an inlined resource, allows it to be individually multiplexed, prioritized, and processed by the client. The only security restriction, as enforced by the browser, is that pushed resources must obey the same-origin policy: the server must be authoritative for the provided content.\n\nPUSH_PROMISE 101 All server push streams are initiated via PUSH_PROMISE frames, which signal the server’s intent to push the described resources to the client, in addition to the response to the original request. The PUSH_PROMISE frames contain just the HTTP headers of the prom‐ ised resource and are required to be sent ahead of the response (i.e., DATA frames) for the original request. This order is important because it notifies the client of which resources the server intends to send prior to the client initiating a request for same resources.\n\nOnce the client receives a PUSH_PROMISE frame, it has the option to decline the stream (via a RST_STREAM frame) if it wants to (e.g., the resource is already in cache), which is an important improvement over HTTP/1.x. By contrast, the use of resource inlining, which is a popular “optimization” for HTTP/1.x, is equivalent to a “forced push”: the client cannot opt-out, cancel it, or process the inlined resource individually.\n\nWith HTTP/2 the client remains in full control of how server push is used. The client can limit the number of concurrently pushed streams; adjust the initial flow control window to control how much data is pushed when the stream is first opened; disable server push entirely. These preferences are communicated via the SET TINGS frames at the beginning of the HTTP/2 connection and may be updated at any time.\n\n18\n\n| HTTP/2\n\nHeader Compression Each HTTP transfer carries a set of headers that describe the trans‐ ferred resource and its properties. In HTTP/1.x, this metadata is always sent as plain text and adds anywhere from 500–800 bytes of overhead per transfer, and sometimes kilobytes more if HTTP cook‐ ies are being used12. To reduce this overhead and improve perfor‐ mance, HTTP/2 compresses request and response header metadata using the HPACK compression format that uses two simple but powerful techniques:\n\n1. It allows the transmitted header fields to be encoded via a static Huffman code, which reduces their individual transfer size.\n\n2. It requires that both the client and server maintain and update an indexed list of previously seen header fields (i.e., establishes a shared compression context), which is then used as a reference to efficiently encode previously transmitted values.\n\nHuffman coding allows the individual values to be compressed when transferred, and the indexed list of previously transferred val‐ ues allows us to encode duplicate values (Figure 1-6) by transferring index values that can be used to efficiently look up and reconstruct the full header keys and values.\n\nFigure 1-6. HPACK: Header Compression for HTTP/2\n\nAs one further optimization, the HPACK compression context con‐ sists of a static and dynamic tables: the static table is defined in the specification and provides a list of common HTTP header fields that all connections are likely to use (e.g., valid header names); the dynamic table is initially empty and is updated based on exchanged values within a particular connection. As a result, the size of each\n\n12 See “Measuring and Controlling Protocol Overhead” at http://hpbn.co/protocol-\n\noverhead\n\nDesign and Technical Goals\n\n|\n\n19\n\nrequest is reduced by using static Huffman coding for values that haven’t been seen before, and substitution of indexes for values that are already present in the static or dynamic tables on each side.\n\nThe definitions of the request and response header fields in HTTP/2 remain unchanged, with a few minor exceptions: all header field names are lowercase, and the individ‐ is now ual :method, :scheme, :authority, and :path pseudo- header fields.\n\nrequest\n\nline\n\nsplit\n\ninto\n\nSecurity and Performance of HPACK Early versions of HTTP/2 and SPDY used zlib, with a custom dic‐ tionary, to compress all HTTP headers, which delivered 85%–88% reduction in the size of the transferred header data, and a signifi‐ cant improvement in page load time latency:\n\nOn the lower-bandwidth DSL link, in which the upload link is only 375 Kbps, request header compression in particular led to significant page load time improvements for certain sites (i.e., those that issued large number of resource requests). We found a reduction of 45–1142 ms in page load time simply due to header compression.\n\n— chromium.org SPDY whitepaper\n\nHowever, in the summer of 2012, a “CRIME” security attack was published against TLS and SPDY compression algorithms, which could result in session hijacking. As a result, the zlib compression algorithm was replaced by HPACK, which was specifically designed to address the discovered security issues, be efficient and simple to implement correctly, and of course, enable good compression of HTTP header metadata.\n\nFor full details of the HPACK compression algorithm, see https:// tools.ietf.org/html/draft-ietf-httpbis-header-compression.\n\nUpgrading to HTTP/2 The switch to HTTP/2 cannot happen overnight: millions of servers must be updated to use the new binary framing, and billions of cli‐ ents must similarly update their networking libraries, browsers, and other applications.\n\n20\n\n| HTTP/2\n\nThe good news is, all modern browsers have committed to support‐ ing HTTP/2, and most modern browsers use efficient background update mechanisms, which have already enabled HTTP/2 support with minimal intervention for a large proportion of existing users. That said, some users will be stuck on legacy browsers, and servers and intermediaries will also have to be updated to support HTTP/2, which is a much longer, and labor- and capital-intensive, process.\n\nHTTP/1.x will be around for at least another decade, and most servers and clients will have to support both HTTP/1.x and HTTP/2 standards. As a result, an HTTP/2 client and server must be able to discover and negotiate which protocol will be used prior to exchang‐ ing application data. To address this, the HTTP/2 protocol defines the following mechanisms:\n\n1. Negotiating HTTP/2 via a secure connection with TLS and ALPN\n\n2. Upgrading a plaintext connection to HTTP/2 without prior knowledge\n\n3. Initiating a plaintext HTTP/2 connection with prior knowledge\n\nThe HTTP/2 standard does not require use of TLS, but in practice it is the most reliable way to deploy a new protocol in the presence of large number of existing intermediaries13. As a result, the use of TLS and ALPN is the recommended mechanism to deploy and negotiate HTTP/2: the client and server negotiate the desired protocol as part of the TLS handshake without adding any extra latency or round‐ trips14. Further, as an additional constraint, while all popular brows‐ ers have committed to supporting HTTP/2 over TLS, some have also indicated that they will only enable HTTP/2 over TLS—e.g., Firefox and Google Chrome. As a result, TLS with ALPN negotia‐ tion is a de-facto requirement for enabling HTTP/2 in the browser.\n\nEstablishing an HTTP/2 connection over a regular, non-encrypted channel is still possible, albeit perhaps not with a popular browser, and with some additional complexity. Because both HTTP/1.x and HTTP/2 run on the same port (80), in absence of any other infor‐\n\n13 See “Proxies, Intermediaries, TLS, and New Protocols on the Web” at http://hpbn.co/\n\nnew-protocols\n\n14 See “TLS Handshake” and “Application Layer Protocol Negotiation (ALPN)” at http://\n\nhpbn.co/tls-handshake, and http://hpbn.co/alpn\n\nDesign and Technical Goals\n\n|\n\n21\n\nmation about server support for HTTP/2, the client has to use the HTTP Upgrade mechanism to negotiate the appropriate protocol:\n\nGET /page HTTP/1.1 Host: server.example.com Connection: Upgrade, HTTP2-Settings Upgrade: h2c HTTP2-Settings: (SETTINGS payload)\n\nHTTP/1.1 200 OK Content-length: 243 Content-type: text/html\n\n(... HTTP/1.1 response ...)\n\n(or)\n\nHTTP/1.1 101 Switching Protocols Connection: Upgrade Upgrade: h2c\n\n(... HTTP/2 response ...)\n\nInitial HTTP/1.1 request with HTTP/2 upgrade header\n\nBase64 URL encoding of HTTP/2 SETTINGS payload\n\nServer declines upgrade, returns response via HTTP/1.1\n\nServer accepts HTTP/2 upgrade, switches to new framing\n\nUsing the preceding Upgrade flow, if the server does not support HTTP/2, then it can immediately respond to the request with HTTP/1.1 response. Alternatively, it can confirm the HTTP/2 upgrade by returning the 101 Switching Protocols response in HTTP/1.1 format and then immediately switch to HTTP/2 and return the response using the new binary framing protocol. In either case, no extra roundtrips are incurred.\n\nFinally, if the client chooses to, it may also remember or obtain the information about HTTP/2 support through some other means— e.g., DNS record, manual configuration, and so on—instead of hav‐ ing to rely on the Upgrade workflow. Armed with this knowledge, it may choose to send HTTP/2 frames right from the start, over an unencrypted channel, and hope for the best. In the worst case, the connection will fail, and the client will fall back to Upgrade work‐ flow or switch to a TLS tunnel with ALPN negotiation.\n\n22\n\n| HTTP/2\n\nSecure communication between client and server, server to server, and all other permutations, is a secu‐ rity best practice: all in-transit data should be encryp‐ ted, authenticated, and checked against tampering. In short, use TLS with ALPN negotiation to deploy HTTP/2.\n\nBrief Introduction to Binary Framing At the core of all HTTP/2 improvements is the new binary, length- prefixed framing layer. Compared with the newline-delimited plain‐ text HTTP/1.x protocol, binary framing offers more compact repre‐ sentation that is both more efficient to process and easier to imple‐ ment correctly.\n\nOnce an HTTP/2 connection is established, the client and server communicate by exchanging frames, which serve as the smallest unit of communication within the protocol. All frames share a common 9-byte header (Figure 1-7), which contains the length of the frame, its type, a bit field for flags, and a 31-bit stream identifier.\n\nFigure 1-7. Common 9-byte frame header\n\nThe 24-bit length field allows a single frame to carry up to 224 bytes of data.\n\nThe 8-bit type field determines the format and semantics of the frame.\n\nThe 8-bit flags field communicates frame-type specific boolean flags.\n\nThe 1-bit reserved field is always set to 0.\n\nThe 31-bit stream identifier uniquely identifies the HTTP/2 stream.\n\nBrief Introduction to Binary Framing\n\n|\n\n23",
      "page_number": 21
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 29-36)",
      "start_page": 29,
      "end_page": 36,
      "detection_method": "topic_boundary",
      "content": "Technically, the length field allows payloads of up to 224 bytes (~16MB) per frame. However, the HTTP/2 standard sets the default maximum payload size of DATA frames to 214 bytes (~16KB) per frame and allows the client and server to negotiate the higher value. Big‐ ger is not always better: smaller frame size enables effi‐ cient multiplexing and minimizes head-of-line block‐ ing.\n\nGiven this knowledge of the shared HTTP/2 frame header, we can now write a simple parser that can examine any HTTP/2 bytestream and identify different frame types, report their flags, and report the length of each by examining the first nine bytes of every frame. Fur‐ ther, because each frame is length-prefixed, the parser can skip ahead to the beginning of the next frame both quickly and efficiently —a big performance improvement over HTTP/1.x.\n\nOnce the frame type is known, the remainder of the frame can be interpreted by the parser. The HTTP/2 standard defines the follow‐ ing types:\n\nDATA\n\nUsed to transport HTTP message bodies\n\nHEADERS\n\nUsed to communicate header fields for a stream\n\nPRIORITY\n\nUsed to communicate sender-advised priority of a stream\n\nRST_STREAM\n\nUsed to signal termination of a stream\n\nSETTINGS\n\nUsed to communicate configuration parameters for the connec‐ tion\n\nPUSH_PROMISE\n\nUsed to signal a promise to serve the referenced resource\n\nPING\n\nUsed to measure the roundtrip time and perform “liveness” checks\n\n24\n\n| HTTP/2\n\nGOAWAY\n\nUsed to inform the peer to stop creating streams for current connection\n\nWINDOW_UPDATE\n\nUsed to implement flow stream and connection flow control\n\nCONTINUATION\n\nUsed to continue a sequence of header block fragments\n\nYou will need some tooling to inspect the low-level HTTP/2 frame exchange. Your favorite hex viewer is, of course, an option. Or, for a more human-friendly representation, you can use a tool like Wireshark, which understands the HTTP/2 protocol and can cap‐ ture, decode, and analyze the exchange.\n\nThe good news is that the exact semantics of the preceding taxon‐ omy of frames is mostly only relevant to server and client imple‐ menters, who will need to worry about the semantics of flow con‐ trol, error handling, connection termination, and other details. The application layer features and semantics of the HTTP protocol remain unchanged: the client and server take care of the framing, multiplexing, and other details, while the application can enjoy the benefits of faster and more efficient delivery.\n\nHaving said that, even though the framing layer is hidden from our applications, it is useful for us to go just one step further and look at the two most common workflows: initiating a new stream and exchanging application data. Having an intuition for how a request, or a response, is translated into individual frames will give you the necessary knowledge to debug and optimize your HTTP/2 deploy‐ ments. Let’s dig a little deeper.\n\nFixed vs. Variable Length Fields and HTTP/2 HTTP/2 uses fixed-length fields exclusively. The overhead of an HTTP/2 frame is low (9-byte header for a data frame), and variable-length encoding savings do not offset the required com‐ plexity for the parsers, nor do they have a significant impact on the used bandwidth or latency of the exchange.\n\nBrief Introduction to Binary Framing\n\n|\n\n25\n\nFor example, if variable-length encoding could reduce the overhead by 50%, for a 1,400-byte network packet, this would amount to just 4 saved bytes (0.3%) for a single frame.\n\nInitiating a New Stream Before any application data can be sent, a new stream must be cre‐ ated and the appropriate request metadata must be sent: optional stream dependency and weight, optional flags, and the HPACK- encoded HTTP request headers describing the request. The client initiates this process by sending a HEADERS frame (Figure 1-8) with all of the above.\n\nFigure 1-8. Decoded HEADERS frame in Wireshark\n\nWireshark decodes and displays the frame fields in the same order as encoded on the wire—e.g., compare the fields in the common frame header to the frame layout in Figure 1-7.\n\nThe HEADERS frame is used to declare and communicate metadata about the new request. The application payload, if available, is deliv‐ ered independently within the DATA frames. This separation allows the protocol to separate processing of “control traffic” from delivery\n\n26\n\n| HTTP/2\n\nof application data—e.g., flow control is applied only to DATA frames, and non-DATA frames are always processed with high priority.\n\nServer-Initiated Streams via PUSH_PROMISE HTTP/2 allows both client and server to initiate new streams. In the case of a server-initiated stream, a PUSH_PROMISE frame is used to declare the promise and communicate the HPACK-encoded response headers. The format of the frame is similar to HEADERS, except that it omits the optional stream dependency and weight, since the server is in full control of how the promised data is deliv‐ ered.\n\nTo eliminate stream ID collisions between client- and server- initiated streams, the counters are offset: client-initiated streams have odd-numbered stream IDs, and server-initiated streams have even-numbered stream IDs. As a result, because the stream ID in Figure 1-8 is set to “1”, we can infer that it is a client-initiated stream.\n\nSending Application Data Once a new stream is created, and the HTTP headers are sent, DATA frames (Figure 1-9) are used to send the application payload if one is present. The payload can be split between multiple DATA frames, with the last frame indicating the end of the message by toggling the END_STREAM flag in the header of the frame.\n\nFigure 1-9. DATA frame\n\nBrief Introduction to Binary Framing\n\n|\n\n27\n\nThe “End Stream” flag is set to “false” in Figure 1-9, indicating that the client has not finished transmitting the application payload; more DATA frames are coming.\n\nAside from the length and flags fields, there really isn’t much more to say about the DATA frame. The application payload may be split between multiple DATA frames to enable efficient multiplexing, but otherwise it is delivered exactly as provided by the application—i.e., the choice of the encoding mechanism (plain text, gzip, or other encoding formats) is deferred to the application.\n\nAnalyzing HTTP/2 Frame Data Flow Armed with knowledge of the different frame types, we can now revisit the diagram (Figure 1-10) we encountered earlier in “Request and Response Multiplexing” on page 8 and analyze the HTTP/2 exchange:\n\nFigure 1-10. HTTP/2 request and response multiplexing within a shared connection\n\nThere are three streams, with IDs set to 1, 3, and 5.\n\nAll three stream IDs are odd; all three are client-initiated streams.\n\nThere are no server-initiated (“push”) streams in this exchange. • The server is sending interleaved DATA frames for stream 1, which carry the application response to the client’s earlier request.\n\nThe server has interleaved the HEADERS and DATA frames for stream 3 between the DATA frames for stream 1—response mul‐ tiplexing in action!\n\n28\n\n| HTTP/2\n\nThe client is transferring a DATA frame for stream 5, which indi‐ cates that a HEADERS frame was transferred earlier.\n\nThe above analysis is, of course, based on a simplified representation of an actual HTTP/2 exchange, but it still illustrates many of the strengths and features of the new protocol. By this point, you should have the necessary knowledge to successfully record and analyze a real-world HTTP/2 trace—give it a try!\n\nNext steps with HTTP/2 As we said at the beginning of this excerpt, the good news is that all of our existing applications can be delivered with HTTP/2 without modification. The semantics and the core functionality of the HTTP protocol remain unchanged.\n\nHowever, we are not just interested in delivering a working applica‐ tion; our goal is to deliver the best performance! HTTP/2 enables a number of new optimizations that fundamentally change, or elimi‐ nate the need for, many of today’s “performance best practices”. We are no longer constrained by parallelism, requests are cheap, and we can finally step back and re-architect our applications to take advan‐ tage of granular caching, leverage server push, and so much more.\n\nSo, where to from here? A few resources to help you on your quest:\n\nHTTP/2 and HPACK specs\n\nHTTP/2 FAQ\n\nKnown implementations\n\nAnd, of course, I would be remiss if I didn’t mention the full version of “High Performance Browser Networking”! Pick up the print ver‐ sion, ebook, or check out the free online version for more hands-on performance tips and recommendations on optimizing your appli‐ cation and server infrastructure for HTTP/2.\n\nNext steps with HTTP/2\n\n|\n\n29",
      "page_number": 29
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 37-37)",
      "start_page": 37,
      "end_page": 37,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 37
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Short. Smart. Seriously useful.\n\nFree ebooks and reports from O’Reilly at oreil.ly/webperf\n\nKubernetes\n\nDevOps in Practice\n\nDocker Security\n\nUsing Containers Safely in Production\n\nScheduling the Future at Cloud Scale\n\nJ. Paul Reed\n\nAdrian Mouat\n\nDevOps for Finance\n\nHTTP/2\n\nReducing Risk Through Continuous Delivery\n\nA New Excerpt from High Performance Browser Networking\n\nDavid K. Rensin\n\nJim Bird\n\nIlya Grigorik\n\nGet even more insights from industry experts and stay current with the latest developments in web operations, DevOps, and web performance with free ebooks and reports from O’Reilly.\n\n©2016 O’Reilly Media, Inc. The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. D1710",
      "content_length": 698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "HTTP/2 A New Excerpt from High Performance Browser Networking\n\nIlya Grigorik",
      "content_length": 76,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "HTTP/2: A New Excerpt from High Performance Browser Networking by Ilya Grigorik\n\nCopyright © 2015 Ilya Grigorik. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://safaribooksonline.com). For more sales department: 800-998-9938 or corporate@oreilly.com.\n\ninformation,\n\ncontact our\n\ncorporate/institutional\n\nEditor: Brian Anderson\n\nInterior Designer: David Futato Cover Designer: Karen Montgomery\n\nMay 2015:\n\nFirst Edition\n\nRevision History for the First Edition 2015-05-01: First Release\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. HTTP/2: A New Excerpt from High Performance Browser Networking and related trade dress are trademarks of O’Reilly Media, Inc.\n\nWhile the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limi‐ tation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsi‐ bility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-491-93248-3\n\n[LSI]",
      "content_length": 1638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Table of Contents\n\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nHTTP/2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Brief History of SPDY and HTTP/2 2 Design and Technical Goals 4 Brief Introduction to Binary Framing 23 Next steps with HTTP/2 29\n\nvii\n\nv",
      "content_length": 380,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Preface\n\nHTTP/2 is here. The standard is approved, all popular browsers have committed to support it, or have already enabled it for their users, and many popular sites are already leveraging HTTP/2 to deliver improved performance. In fact, in a short span of just a few months after the HTTP/2 and HPACK standards were approved in early 2015, their usage on the web has already surpassed that of SPDY! Which is to say, this is well tested and proven technology that is ready for production.\n\nSo, what’s new in HTTP/2, and why or how will your application benefit from it? To answer that we need to take an under the hood look at the new protocol, its features, and talk about its implications for how we design, deploy, and deliver our applications. Under‐ standing the design and technical goals of HTTP/2 will explain both how, and why, some of our existing best practices are no longer rele‐ vant—sometimes harmful, even—and what new capabilities we have at our disposal to further optimize our applications.\n\nWith that, there’s no time to waste, let’s dive in!\n\nvii",
      "content_length": 1070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "HTTP/2\n\nHTTP/2 will make our applications faster, simpler, and more robust —a rare combination—by allowing us to undo many of the HTTP/1.1 workarounds previously done within our applications and address these concerns within the transport layer itself. Even better, it also opens up a number of entirely new opportunities to optimize our applications and improve performance!\n\nThe primary goals for HTTP/2 are to reduce latency by enabling full request and response multiplexing, minimize protocol overhead via efficient compression of HTTP header fields, and add support for request prioritization and server push. To implement these require‐ ments, there is a large supporting cast of other protocol enhance‐ ments, such as new flow control, error handling, and upgrade mech‐ anisms, but these are the most important features that every web developer should understand and leverage in their applications.\n\nHTTP/2 does not modify the application semantics of HTTP in any way. All of the core concepts, such as HTTP methods, status codes, URIs, and header fields, remain in place. Instead, HTTP/2 modifies how the data is formatted (framed) and transported between the cli‐ ent and server, both of whom manage the entire process, and hides all the complexity from our applications within the new framing layer. As a result, all existing applications can be delivered without modification. That’s the good news.\n\nHowever, we are not just interested in delivering a working applica‐ tion; our goal is to deliver the best performance! HTTP/2 enables a number of new optimizations that our applications can leverage,\n\n1",
      "content_length": 1615,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "which were previously not possible, and our job is to make the best of them. Let’s take a closer look under the hood.\n\nWhy not HTTP/1.2? To achieve the performance goals set by the HTTP Working Group, HTTP/2 introduces a new binary framing layer that is not back‐ ward compatible with previous HTTP/1.x servers and clients. Hence the major protocol version increment to HTTP/2.\n\nThat said, unless you are implementing a web server or a custom client by working with raw TCP sockets, you won’t see any differ‐ ence: all the new, low-level framing is performed by the client and server on your behalf. The only observable differences will be improved performance and availability of new capabilities like request prioritization, flow control, and server push!\n\nBrief History of SPDY and HTTP/2 SPDY was an experimental protocol, developed at Google and announced in mid-2009, whose primary goal was to try to reduce the load latency of web pages by addressing some of the well-known performance limitations of HTTP/1.1. Specifically, the outlined project goals were set as follows:\n\nTarget a 50% reduction in page load time (PLT).\n\nAvoid the need for any changes to content by website authors.\n\nMinimize deployment complexity, avoid changes in network infrastructure.\n\nDevelop this new protocol in partnership with the open-source community.\n\nGather real performance data to (in)validate the experimental protocol.\n\n2\n\n| HTTP/2",
      "content_length": 1425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "To achieve the 50% PLT improvement, SPDY aimed to make more efficient use of the underlying TCP con‐ nection by introducing a new binary framing layer to enable request and response multiplexing, prioritiza‐ tion, and header compression.1\n\nNot long after the initial announcement, Mike Belshe and Roberto Peon, both software engineers at Google, shared their first results, documentation, and source code for the experimental implementa‐ tion of the new SPDY protocol:\n\nSo far we have only tested SPDY in lab conditions. The initial results are very encouraging: when we download the top 25 web‐ sites over simulated home network connections, we see a signifi‐ cant improvement in performance—pages loaded up to 55% faster.\n\n— Chromium Blog A 2x Faster Web\n\nFast-forward to 2012 and the new experimental protocol was sup‐ ported in Chrome, Firefox, and Opera, and a rapidly growing num‐ ber of sites, both large (e.g., Google, Twitter, Facebook) and small, were deploying SPDY within their infrastructure. In effect, SPDY was on track to become a de facto standard through growing indus‐ try adoption.\n\nObserving this trend, the HTTP Working Group (HTTP-WG) kicked off a new effort to take the lessons learned from SPDY, build and improve on them, and deliver an official “HTTP/2” standard: a new charter was drafted, an open call for HTTP/2 proposals was made, and after a lot of discussion within the working group, the SPDY specification was adopted as a starting point for the new HTTP/2 protocol.\n\nOver the next few years, SPDY and HTTP/2 would continue to coevolve in parallel, with SPDY acting as an experimental branch that was used to test new features and proposals for the HTTP/2 standard: what looks good on paper may not work in practice, and vice versa, and SPDY offered a route to test and evaluate each pro‐ posal before its inclusion in the HTTP/2 standard. In the end, this process spanned three years and resulted in a over a dozen inter‐ mediate drafts:\n\nMar, 2012: Call for proposals for HTTP/2\n\n1 See “Latency as a Performance Bottleneck” at http://hpbn.co/latency-bottleneck\n\nBrief History of SPDY and HTTP/2\n\n|\n\n3",
      "content_length": 2137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Nov, 2012: First draft of HTTP/2 (based on SPDY)\n\nAug, 2014: HTTP/2 draft-17 and HPACK draft-12 are published\n\nAug, 2014: Working Group last call for HTTP/2\n\nFeb, 2015: IESG approved HTTP/2\n\nMay, 2015: HTTP/2 and HPACK RFC’s (7540, 7541) are pub‐ lished\n\nIn early 2015 the IESG reviewed and approved the new HTTP/2 standard for publication. Shortly after that, the Google Chrome team announced their schedule to deprecate SPDY and NPN exten‐ sion for TLS:\n\nHTTP/2’s primary changes from HTTP/1.1 focus on improved per‐ formance. Some key features such as multiplexing, header com‐ pression, prioritization and protocol negotiation evolved from work done in an earlier open, but non-standard protocol named SPDY. Chrome has supported SPDY since Chrome 6, but since most of the benefits are present in HTTP/2, it’s time to say good‐ bye. We plan to remove support for SPDY in early 2016, and to also remove support for the TLS extension named NPN in favor of ALPN in Chrome at the same time. Server developers are strongly encouraged to move to HTTP/2 and ALPN.\n\nWe’re happy to have contributed to the open standards process that led to HTTP/2, and hope to see wide adoption given the broad industry engagement on standardization and implementation.\n\n— Chromium Blog Hello HTTP/2, Goodbye SPDY\n\nThe coevolution of SPDY and HTTP/2 enabled server, browser, and site developers to gain real-world experience with the new protocol as it was being developed. As a result, the HTTP/2 standard is one of the best and most extensively tested standards right out of the gate. By the time HTTP/2 was approved by the IESG, there were dozens of thoroughly tested and production-ready client and server implementations. In fact, just weeks after the final protocol was approved, many users were already enjoying its benefits as several popular browsers, and many sites, deployed full HTTP/2 support.\n\nDesign and Technical Goals First versions of the HTTP protocol were intentionally designed for simplicity of implementation: HTTP/0.9 was a one-line protocol to bootstrap the World Wide Web; HTTP/1.0 documented the popular\n\n4\n\n| HTTP/2",
      "content_length": 2121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "extensions to HTTP/0.9 in an informational standard; HTTP/1.1 introduced an official IETF standard2. As such, HTTP/0.9-1.x deliv‐ ered exactly what it set out to do: HTTP is one of the most ubiqui‐ tous and widely adopted application protocols on the Internet.\n\nUnfortunately, implementation simplicity also came at the cost of application performance: HTTP/1.x clients need to use multiple connections to achieve concurrency and reduce latency; HTTP/1.x does not compress request and response headers, causing unneces‐ sary network traffic; HTTP/1.x does not allow effective resource pri‐ oritization, resulting in poor use of the underlying TCP connection; and so on.\n\nThese limitations were not fatal, but as the web applications contin‐ ued to grow in their scope, complexity, and importance in our everyday lives, they imposed a growing burden on both the develop‐ ers and users of the Web, which is the exact gap that HTTP/2 was designed to address:\n\nHTTP/2 enables a more efficient use of network resources and a reduced perception of latency by introducing header field compres‐ sion and allowing multiple concurrent exchanges on the same con‐ nection…. Specifically, it allows interleaving of request and response messages on the same connection and uses an efficient coding for HTTP header fields. It also allows prioritization of requests, letting more important requests complete more quickly, further improving performance.\n\nThe resulting protocol is more friendly to the network, because fewer TCP connections can be used in comparison to HTTP/1.x. This means less competition with other flows, and longer-lived connections, which in turn leads to better utilization of available network capacity. Finally, HTTP/2 also enables more efficient pro‐ cessing of messages through use of binary message framing.\n\n— Draft 17 Hypertext Transfer Protocol version 2\n\nIt is important to note that HTTP/2 is extending, not replacing, the previous HTTP standards. The application semantics of HTTP are the same, and no changes were made to the offered functionality or core concepts such as HTTP methods, status codes, URIs, and header fields—these changes were explicitly out of scope for the HTTP/2 effort. That said, while the high-level API remains the same, it is important to understand how the low-level changes\n\n2 See “Brief History of HTTP” at http://hpbn.co/http-history\n\nDesign and Technical Goals\n\n|\n\n5",
      "content_length": 2415,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "address the performance limitations of the previous protocols. Let’s take a brief tour of the binary framing layer and its features.\n\nBinary Framing Layer At the core of all of the performance enhancements of HTTP/2 is the new binary framing layer (Figure 1-1), which dictates how the HTTP messages are encapsulated and transferred between the client and server.\n\nFigure 1-1. HTTP/2 binary framing layer\n\nThe “layer” refers to a design choice to introduce a new optimized encoding mechanism between the socket interface and the higher HTTP API exposed to our applications: the HTTP semantics, such as verbs, methods, and headers, are unaffected, but the way they are encoded while in transit is what’s different. Unlike the newline delimited plaintext HTTP/1.x protocol, all HTTP/2 communication is split into smaller messages and frames, each of which is encoded in binary format.\n\nAs a result, both client and server must use the new binary encoding mechanism to understand each other: an HTTP/1.x client won’t understand an HTTP/2 only server, and vice versa. Thankfully, our applications remain blissfully unaware of all these changes, as the\n\n6\n\n| HTTP/2",
      "content_length": 1159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "client and server perform all the necessary framing work on our behalf.\n\nThe Pros and Cons of Binary Protocols ASCII protocols are easy to inspect and get started with. However, they are not as efficient and are typically harder to implement cor‐ rectly: optional whitespace, varying termination sequences, and other quirks make it hard to distinguish the protocol from the pay‐ load and lead to parsing and security errors. By contrast, while binary protocols may take more effort to get started with, they tend to lead to more performant, robust, and provably correct imple‐ mentations.\n\nHTTP/2 uses binary framing. As a result, you will need a tool that understands it to inspect and debug the protocol—e.g., Wireshark or equivalent. In practice, this is less of an issue than it seems, since you would have to use the same tools to inspect the encrypted TLS flows—which also rely on binary framing3—carrying HTTP/1.x and HTTP/2 data.\n\nStreams, Messages, and Frames The introduction of the new binary framing mechanism changes how the data is exchanged (Figure 1-2) between the client and server. To describe this process, let’s familiarize ourselves with the HTTP/2 terminology:\n\nStream\n\nA bidirectional flow of bytes within an established connection, which may carry one or more messages.\n\nMessage\n\nA complete sequence of frames that map to a logical request or response message.\n\nFrame\n\nThe smallest unit of communication in HTTP/2, each contain‐ ing a frame header, which at a minimum identifies the stream to which the frame belongs.\n\n3 See “TLS Record Protocol” at http://hpbn.co/tls-record\n\nDesign and Technical Goals\n\n|\n\n7",
      "content_length": 1633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Figure 1-2. HTTP/2 Streams, messages, and frames\n\nAll communication is performed over a single TCP connection that can carry any number of bidirectional streams.\n\nEach stream has a unique identifier and optional priority infor‐ mation that is used to carry bidirectional messages.\n\nEach message is a logical HTTP message, such as a request, or response, which consists of one or more frames.\n\nThe frame is the smallest unit of communication that carries a specific type of data—e.g., HTTP headers, message payload, and so on. Frames from different streams may be interleaved and then reassembled via the embedded stream identifier in the header of each frame.\n\nIn short, HTTP/2 breaks down the HTTP protocol communication into an exchange of binary-encoded frames, which are then mapped to messages that belong to a particular stream, all of which are mul‐ tiplexed within a single TCP connection. This is the foundation that enables all other features and performance optimizations provided by the HTTP/2 protocol.\n\nRequest and Response Multiplexing With HTTP/1.x, if the client wants to make multiple parallel requests to improve performance, then multiple TCP connections must be used4. This behavior is a direct consequence of the\n\n4 See “Using Multiple TCP Connections” at http://hpbn.co/http-multiple-connections\n\n8\n\n| HTTP/2",
      "content_length": 1331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "HTTP/1.x delivery model, which ensures that only one response can be delivered at a time (response queuing) per connection. Worse, this also results in head-of-line blocking and inefficient use of the underlying TCP connection.\n\nThe new binary framing layer in HTTP/2 removes these limitations, and enables full request and response multiplexing, by allowing the client and server to break down an HTTP message into independent frames (Figure 1-3), interleave them, and then reassemble them on the other end.\n\nFigure 1-3. HTTP/2 request and response multiplexing within a shared connection\n\nThe snapshot in Figure 1-3 captures multiple streams in flight within the same connection: the client is transmitting a DATA frame (stream 5) to the server, while the server is transmitting an inter‐ leaved sequence of frames to the client for streams 1 and 3. As a result, there are three parallel streams in flight!\n\nThe ability to break down an HTTP message into independent frames, interleave them, and then reassemble them on the other end is the single most important enhancement of HTTP/2. In fact, it introduces a ripple effect of numerous performance benefits across the entire stack of all web technologies, enabling us to:\n\nInterleave multiple requests in parallel without blocking on any one\n\nInterleave multiple responses in parallel without blocking on any one\n\nUse a single connection to deliver multiple requests and respon‐ ses in parallel\n\nDesign and Technical Goals\n\n|\n\n9",
      "content_length": 1481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Remove unnecessary HTTP/1.x workarounds5, such as con‐ catenated files, image sprites, and domain sharding\n\nDeliver lower page load times by eliminating unnecessary latency and improving utilization of available network capacity\n\nAnd much more…\n\nThe new binary framing layer in HTTP/2 resolves the head-of-line blocking problem found in HTTP/1.x and eliminates the need for multiple connections to enable parallel processing and delivery of requests and responses. As a result, this makes our applications faster, simpler, and cheaper to deploy.\n\nStream Prioritization Once an HTTP message can be split into many individual frames, and we allow for frames from multiple streams to be multiplexed, the order in which the frames are interleaved and delivered both by the client and server becomes a critical performance consideration. To facilitate this, the HTTP/2 standard allows each stream to have an associated weight and dependency:\n\nEach stream may be assigned an integer weight between 1 and 256\n\nEach stream may be given an explicit dependency on another stream\n\nThe combination of stream dependencies and weights allows the cli‐ ent tree” to construct and communicate a “prioritization (Figure 1-4) that expresses how it would prefer to receive the responses. In turn, the server can use this information to prioritize stream processing by controlling the allocation of CPU, memory, and other resources, and once the response data is available, alloca‐ tion of bandwidth to ensure optimal delivery of high-priority responses to the client.\n\n5 See “Optimizing for HTTP/1.x” at http://hpbn.co/optimizing-http1x\n\n10\n\n| HTTP/2",
      "content_length": 1630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Figure 1-4. HTTP/2 stream dependencies and weights\n\nA stream dependency within HTTP/2 is declared by referencing the unique identifier of another stream as its parent; if omitted the stream is said to be dependent on the “root stream.” Declaring a stream dependency indicates that, if possible, the parent stream should be allocated resources ahead of its dependencies—e.g., please process and deliver response D before response C.\n\nStreams that share the same parent can be prioritized with respect to each other by assigning a weight to each stream: the relative priority of the stream is proportional to its weight as compared to its siblings —e.g., resource A has a weight of 12, and B a weight of 4; A should receive two-thirds of available resources.\n\nLet’s work through a few hands-on examples in Figure 1-4. From left to right:\n\n1. Neither stream A nor B specify a parent dependency and are said to be dependent on the implicit “root stream”; A has a weight of 12, and B has a weight of 4. Thus, based on propor‐ tional weights: A should be assigned two-thirds of available resources, and B should receive the remaining one-third.\n\n2. D is dependent on the root stream; C is dependent on D. Thus, D should receive full allocation of resources ahead of C. The weights are inconsequential because C’s dependency communi‐ cates a stronger preference.\n\n3. D should receive full allocation of resources ahead of C; C should receive full allocation of resources ahead of A and B; A should receive two-thirds of available resources, and B should receive the remaining one-third.\n\nDesign and Technical Goals\n\n|\n\n11",
      "content_length": 1614,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "4. D should receive full allocation of resources ahead of E and C; E and C should receive equal allocation ahead of A and B; A and B should receive proportional allocation based on their weights.\n\nAs the above examples illustrate, the combination of stream depen‐ dencies and weights provides an expressive language for resource prioritization, which is a critical feature for improving browsing performance where we have many resource types with different dependencies and weights. Even better, the HTTP/2 protocol also allows the client to update these preferences at any point, which ena‐ bles further optimizations in the browser—e.g., we can change dependencies and reallocate weights in response to user interaction and other signals.\n\nStream dependencies and weights express a transport preference, not a requirement, and as such do not guar‐ antee a particular processing or transmission order. That is, the client cannot force the server to process the stream in particular order using stream prioritization. While this may seem counter-intuitive, it is, in fact, the desired behavior: we do not want to block the server from making progress on a lower-priority resource if a higher-priority resource is blocked.\n\nBrowser Request Prioritization and HTTP/2 Not all resources have equal priority when rendering a page in the browser: the HTML document itself is critical to construct the DOM; the CSS is required to construct the CSSOM; both DOM and CSSOM construction can be blocked on JavaScript resources6; and remaining resources, such as images, are often fetched with lower priority.\n\nTo accelerate the load time of the page, all modern browsers priori‐ tize requests based on type of asset, their location on the page, and even learned priority from previous visits—e.g., if the rendering was blocked on a certain asset in a previous visit, then the same asset may be prioritized higher in the future.\n\nWith HTTP/1.x, the browser has limited ability to leverage above priority data: the protocol does not support multiplexing, and there\n\n6 See “DOM, CSSOM, and JavaScript” at http://hpbn.co/dom-cssom-\n\njavascript\n\n12\n\n| HTTP/2",
      "content_length": 2141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "is no way to communicate request priority to the server. Instead, it must rely on the use of parallel connections, which enables limited parallelism of up to six requests per origin. As a result, requests are queued on the client until a connection is available, which adds unnecessary network latency. In theory, “HTTP Pipelining” in High Performance Browser Networking tried to partially address this problem, but in practice it has failed to gain adoption.\n\nHTTP/2 resolves these inefficiencies: request queuing and head-of- line blocking is eliminated because the browser can dispatch all requests the moment they are discovered, and the browser can communicate its stream prioritization preference via stream depen‐ dencies and weights, allowing the server to further optimize response delivery.\n\nOne Connection Per Origin With the new binary framing mechanism in place, HTTP/2 no longer needs multiple TCP connections to multiplex streams in par‐ allel; each stream is split into many frames, which can be interleaved and prioritized. As a result, all HTTP/2 connections are persistent, and only one connection per origin is required, which offers numer‐ ous performance benefits.\n\nFor both SPDY and HTTP/2, the killer feature is arbitrary multi‐ plexing on a single well congestion controlled channel. It amazes me how important this is and how well it works. One great metric around that which I enjoy is the fraction of connections created that carry just a single HTTP transaction (and thus make that transaction bear all the overhead). For HTTP/1, 74% of our active connections carry just a single transaction—persistent connections just aren’t as helpful as we all want. But in HTTP/2 that number plummets to 25%. That’s a huge win for overhead reduction.\n\n— Patrick McManus HTTP/2 is Live in Firefox\n\nMost HTTP transfers are short and bursty, whereas TCP is opti‐ mized for long-lived, bulk data transfers. By reusing the same con‐ nection, HTTP/2 is able to both make more efficient use of each TCP connection and also significantly reduce the overall protocol overhead. Further, the use of fewer connections reduces the memory and processing footprint along the full connection path (i.e., client, intermediaries, and origin servers), which reduces the overall opera‐ tional costs and improves network utilization and capacity. As a result, the move to HTTP/2 should not only reduce the network\n\nDesign and Technical Goals\n\n|\n\n13",
      "content_length": 2444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "latency, but also help improve throughput and reduce the opera‐ tional costs.\n\nReduced number of connections is a particularly important feature for improving performance of HTTPS deployments: this translates to fewer expensive TLS handshakes, better session re-use, and an overall reduction in required client and server resources.\n\nPacket Loss, High-RTT Links, and HTTP/2 Performance Wait, I hear you say, we listed the benefits of using one TCP con‐ nection per origin but aren’t there some potential downsides? Yes, there are.\n\nWe have eliminated head-of-line blocking from HTTP, but there is still head-of-line blocking at the TCP level7.\n\nEffects of bandwidth-delay product may limit connection throughput if TCP window scaling is disabled.\n\nWhen packet loss occurs, the TCP congestion window size is reduced8, which reduces the maximum throughput of the entire connection.\n\nEach of the items in this list may adversely affect both the through‐ put and latency performance of an HTTP/2 connection. However, despite these limitations, the move to multiple connections would result in its own performance tradeoffs:\n\nLess effective header compression due to distinct compression contexts\n\nLess effective request prioritization due to distinct TCP streams\n\nLess effective utilization of each TCP stream and higher likeli‐ hood of congestion due to more competing flows\n\nIncreased resource overhead due to more TCP flows\n\nThe above pros and cons are not an exhaustive list, and it is always possible to construct specific scenarios where either one or more\n\n7 See “Head-of-Line Blocking” at http://hpbn.co/tcp-hol\n\n8 See “Congestion Avoidance” at http://hpbn.co/congestion-avoidance\n\n14\n\n| HTTP/2",
      "content_length": 1698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "connections may prove to be beneficial. However, the experimental evidence of deploying HTTP/2 in the wild showed that a single connection is the preferred deployment strategy:\n\nIn tests so far, the negative effects of head-of-line blocking (espe‐ cially in the presence of packet loss) is outweighed by the benefits of compression and prioritization.\n\n— Draft 2 Hypertext Transfer Protocol version 2\n\nAs with all performance optimization processes, the moment you remove one performance bottleneck, you unlock the next one. In the case of HTTP/2, TCP may be it. Which is why, once again, a well-tuned TCP stack on the server is such a critical optimization criteria for HTTP/2.\n\nThere is ongoing research to address these concerns and to improve TCP performance in general: TCP Fast Open, Proportional Rate Reduction, increased initial congestion window, and more. Having said that, it is important to acknowledge that HTTP/2, like its pred‐ ecessors, does not mandate the use of TCP. Other transports, such as UDP, are not outside the realm of possibility as we look to the future.\n\nFlow Control Flow control is a mechanism to prevent the sender from over‐ whelming the receiver with data it may not want or be able to pro‐ cess: the receiver may be busy, under heavy load, or may only be willing to allocate a fixed amount of resources for a particular stream. For example, the client may have requested a large video stream with high priority, but the user has paused the video and the client now wants to pause or throttle its delivery from the server to avoid fetching and buffering unnecessary data. Alternatively, a proxy server may have a fast downstream and slow upstream con‐ nections and similarly wants to regulate how quickly the down‐ stream delivers data to match the speed of upstream to control its resource usage; and so on.\n\nDo the above requirements remind you of TCP flow control? They should, as the problem is effectively identical9. However, because the HTTP/2 streams are multiplexed within a single TCP connection, TCP flow control is both not granular enough, and does not provide\n\n9 See “Flow Control” at http://.co/flow-control\n\nDesign and Technical Goals\n\n|\n\n15",
      "content_length": 2192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "the necessary application-level APIs to regulate the delivery of indi‐ vidual streams. To address this, HTTP/2 provides a set of simple building blocks that allow the client and server to implement their own stream- and connection-level flow control:\n\nFlow control is directional. Each receiver may choose to set any window size that it desires for each stream and the entire con‐ nection.\n\nFlow control is credit-based. Each receiver advertises its initial connection and stream flow control window (in bytes), which is reduced whenever the sender emits a DATA frame and incremen‐ ted via a WINDOW_UPDATE frame sent by the receiver.\n\nFlow control cannot be disabled. When the HTTP/2 connection is established the client and server exchange SETTINGS frames, which set the flow control window sizes in both directions. The default value of the flow control window is set to 65,535 bytes, but the receiver can set a large maximum window size (231 − 1 bytes) and maintain it by sending a WINDOW_UPDATE frame whenever any data is received.\n\nFlow control is hop-by-hop, not end-to-end. That is, an inter‐ mediary can use it to control resource use and implement resource allocation mechanisms based on own criteria and heu‐ ristics.\n\nHTTP/2 does not specify any particular algorithm for implementing flow control. Instead, it provides the simple building blocks and defers the implementation to the client and server, which can use it to implement custom strategies to regulate resource use and alloca‐ tion, as well as implement new delivery capabilities that may help improve both the real and perceived performance10 of our web applications.\n\nFor example, application-layer flow control allows the browser to fetch only a part of a particular resource, put the fetch on hold by reducing the stream flow control window down to zero, and then resume it later—e.g., fetch a preview or first scan of an image, dis‐ play it and allow other high priority fetches to proceed, and resume the fetch once more critical resources have finished loading.\n\n10 See “Speed, Performance, and Human Perception” at http://hpbn.co/human-perception\n\n16\n\n| HTTP/2",
      "content_length": 2139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Server Push Another powerful new feature of HTTP/2 is the ability of the server to send multiple responses for a single client request. That is, in addition to the response to the original request, the server can push additional resources to the client (Figure 1-5), without the client having to request each one explicitly!\n\nFigure 1-5. Server initiates new streams (promises) for push resources\n\nHTTP/2 breaks away from the strict request-response semantics and enables one-to-many and server- initiated push workflows that open up a world of new interaction possibilities both within and outside the browser. This is an enabling feature that will have important long-term consequences both for how we think about the protocol, and where and how it is used.\n\nWhy would we need such a mechanism in a browser? A typical web application consists of dozens of resources, all of which are discov‐ ered by the client by examining the document provided by the server. As a result, why not eliminate the extra latency and let the server push the associated resources ahead of time? The server already knows which resources the client will require; that’s server push.\n\nIn fact, if you have ever inlined a CSS, JavaScript, or any other asset via a data URI11, then you already have hands-on experience with server push! By manually inlining the resource into the document, we are, in effect, pushing that resource to the client, without waiting\n\n11 See “Resource Inlining” at http://hpbn.co/inlining\n\nDesign and Technical Goals\n\n|\n\n17",
      "content_length": 1527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "for the client to request it. With HTTP/2 we can achieve the same results, but with additional performance benefits:\n\nPushed resources can be cached by the client\n\nPushed resources can be reused across different pages\n\nPushed resources can be multiplexed alongside other resources\n\nPushed resources can be prioritized by the server\n\nPushed resources can be declined by the client\n\nEach pushed resource is a stream that, unlike an inlined resource, allows it to be individually multiplexed, prioritized, and processed by the client. The only security restriction, as enforced by the browser, is that pushed resources must obey the same-origin policy: the server must be authoritative for the provided content.\n\nPUSH_PROMISE 101 All server push streams are initiated via PUSH_PROMISE frames, which signal the server’s intent to push the described resources to the client, in addition to the response to the original request. The PUSH_PROMISE frames contain just the HTTP headers of the prom‐ ised resource and are required to be sent ahead of the response (i.e., DATA frames) for the original request. This order is important because it notifies the client of which resources the server intends to send prior to the client initiating a request for same resources.\n\nOnce the client receives a PUSH_PROMISE frame, it has the option to decline the stream (via a RST_STREAM frame) if it wants to (e.g., the resource is already in cache), which is an important improvement over HTTP/1.x. By contrast, the use of resource inlining, which is a popular “optimization” for HTTP/1.x, is equivalent to a “forced push”: the client cannot opt-out, cancel it, or process the inlined resource individually.\n\nWith HTTP/2 the client remains in full control of how server push is used. The client can limit the number of concurrently pushed streams; adjust the initial flow control window to control how much data is pushed when the stream is first opened; disable server push entirely. These preferences are communicated via the SET TINGS frames at the beginning of the HTTP/2 connection and may be updated at any time.\n\n18\n\n| HTTP/2",
      "content_length": 2114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Header Compression Each HTTP transfer carries a set of headers that describe the trans‐ ferred resource and its properties. In HTTP/1.x, this metadata is always sent as plain text and adds anywhere from 500–800 bytes of overhead per transfer, and sometimes kilobytes more if HTTP cook‐ ies are being used12. To reduce this overhead and improve perfor‐ mance, HTTP/2 compresses request and response header metadata using the HPACK compression format that uses two simple but powerful techniques:\n\n1. It allows the transmitted header fields to be encoded via a static Huffman code, which reduces their individual transfer size.\n\n2. It requires that both the client and server maintain and update an indexed list of previously seen header fields (i.e., establishes a shared compression context), which is then used as a reference to efficiently encode previously transmitted values.\n\nHuffman coding allows the individual values to be compressed when transferred, and the indexed list of previously transferred val‐ ues allows us to encode duplicate values (Figure 1-6) by transferring index values that can be used to efficiently look up and reconstruct the full header keys and values.\n\nFigure 1-6. HPACK: Header Compression for HTTP/2\n\nAs one further optimization, the HPACK compression context con‐ sists of a static and dynamic tables: the static table is defined in the specification and provides a list of common HTTP header fields that all connections are likely to use (e.g., valid header names); the dynamic table is initially empty and is updated based on exchanged values within a particular connection. As a result, the size of each\n\n12 See “Measuring and Controlling Protocol Overhead” at http://hpbn.co/protocol-\n\noverhead\n\nDesign and Technical Goals\n\n|\n\n19",
      "content_length": 1768,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "request is reduced by using static Huffman coding for values that haven’t been seen before, and substitution of indexes for values that are already present in the static or dynamic tables on each side.\n\nThe definitions of the request and response header fields in HTTP/2 remain unchanged, with a few minor exceptions: all header field names are lowercase, and the individ‐ is now ual :method, :scheme, :authority, and :path pseudo- header fields.\n\nrequest\n\nline\n\nsplit\n\ninto\n\nSecurity and Performance of HPACK Early versions of HTTP/2 and SPDY used zlib, with a custom dic‐ tionary, to compress all HTTP headers, which delivered 85%–88% reduction in the size of the transferred header data, and a signifi‐ cant improvement in page load time latency:\n\nOn the lower-bandwidth DSL link, in which the upload link is only 375 Kbps, request header compression in particular led to significant page load time improvements for certain sites (i.e., those that issued large number of resource requests). We found a reduction of 45–1142 ms in page load time simply due to header compression.\n\n— chromium.org SPDY whitepaper\n\nHowever, in the summer of 2012, a “CRIME” security attack was published against TLS and SPDY compression algorithms, which could result in session hijacking. As a result, the zlib compression algorithm was replaced by HPACK, which was specifically designed to address the discovered security issues, be efficient and simple to implement correctly, and of course, enable good compression of HTTP header metadata.\n\nFor full details of the HPACK compression algorithm, see https:// tools.ietf.org/html/draft-ietf-httpbis-header-compression.\n\nUpgrading to HTTP/2 The switch to HTTP/2 cannot happen overnight: millions of servers must be updated to use the new binary framing, and billions of cli‐ ents must similarly update their networking libraries, browsers, and other applications.\n\n20\n\n| HTTP/2",
      "content_length": 1909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "The good news is, all modern browsers have committed to support‐ ing HTTP/2, and most modern browsers use efficient background update mechanisms, which have already enabled HTTP/2 support with minimal intervention for a large proportion of existing users. That said, some users will be stuck on legacy browsers, and servers and intermediaries will also have to be updated to support HTTP/2, which is a much longer, and labor- and capital-intensive, process.\n\nHTTP/1.x will be around for at least another decade, and most servers and clients will have to support both HTTP/1.x and HTTP/2 standards. As a result, an HTTP/2 client and server must be able to discover and negotiate which protocol will be used prior to exchang‐ ing application data. To address this, the HTTP/2 protocol defines the following mechanisms:\n\n1. Negotiating HTTP/2 via a secure connection with TLS and ALPN\n\n2. Upgrading a plaintext connection to HTTP/2 without prior knowledge\n\n3. Initiating a plaintext HTTP/2 connection with prior knowledge\n\nThe HTTP/2 standard does not require use of TLS, but in practice it is the most reliable way to deploy a new protocol in the presence of large number of existing intermediaries13. As a result, the use of TLS and ALPN is the recommended mechanism to deploy and negotiate HTTP/2: the client and server negotiate the desired protocol as part of the TLS handshake without adding any extra latency or round‐ trips14. Further, as an additional constraint, while all popular brows‐ ers have committed to supporting HTTP/2 over TLS, some have also indicated that they will only enable HTTP/2 over TLS—e.g., Firefox and Google Chrome. As a result, TLS with ALPN negotia‐ tion is a de-facto requirement for enabling HTTP/2 in the browser.\n\nEstablishing an HTTP/2 connection over a regular, non-encrypted channel is still possible, albeit perhaps not with a popular browser, and with some additional complexity. Because both HTTP/1.x and HTTP/2 run on the same port (80), in absence of any other infor‐\n\n13 See “Proxies, Intermediaries, TLS, and New Protocols on the Web” at http://hpbn.co/\n\nnew-protocols\n\n14 See “TLS Handshake” and “Application Layer Protocol Negotiation (ALPN)” at http://\n\nhpbn.co/tls-handshake, and http://hpbn.co/alpn\n\nDesign and Technical Goals\n\n|\n\n21",
      "content_length": 2284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "mation about server support for HTTP/2, the client has to use the HTTP Upgrade mechanism to negotiate the appropriate protocol:\n\nGET /page HTTP/1.1 Host: server.example.com Connection: Upgrade, HTTP2-Settings Upgrade: h2c HTTP2-Settings: (SETTINGS payload)\n\nHTTP/1.1 200 OK Content-length: 243 Content-type: text/html\n\n(... HTTP/1.1 response ...)\n\n(or)\n\nHTTP/1.1 101 Switching Protocols Connection: Upgrade Upgrade: h2c\n\n(... HTTP/2 response ...)\n\nInitial HTTP/1.1 request with HTTP/2 upgrade header\n\nBase64 URL encoding of HTTP/2 SETTINGS payload\n\nServer declines upgrade, returns response via HTTP/1.1\n\nServer accepts HTTP/2 upgrade, switches to new framing\n\nUsing the preceding Upgrade flow, if the server does not support HTTP/2, then it can immediately respond to the request with HTTP/1.1 response. Alternatively, it can confirm the HTTP/2 upgrade by returning the 101 Switching Protocols response in HTTP/1.1 format and then immediately switch to HTTP/2 and return the response using the new binary framing protocol. In either case, no extra roundtrips are incurred.\n\nFinally, if the client chooses to, it may also remember or obtain the information about HTTP/2 support through some other means— e.g., DNS record, manual configuration, and so on—instead of hav‐ ing to rely on the Upgrade workflow. Armed with this knowledge, it may choose to send HTTP/2 frames right from the start, over an unencrypted channel, and hope for the best. In the worst case, the connection will fail, and the client will fall back to Upgrade work‐ flow or switch to a TLS tunnel with ALPN negotiation.\n\n22\n\n| HTTP/2",
      "content_length": 1603,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Secure communication between client and server, server to server, and all other permutations, is a secu‐ rity best practice: all in-transit data should be encryp‐ ted, authenticated, and checked against tampering. In short, use TLS with ALPN negotiation to deploy HTTP/2.\n\nBrief Introduction to Binary Framing At the core of all HTTP/2 improvements is the new binary, length- prefixed framing layer. Compared with the newline-delimited plain‐ text HTTP/1.x protocol, binary framing offers more compact repre‐ sentation that is both more efficient to process and easier to imple‐ ment correctly.\n\nOnce an HTTP/2 connection is established, the client and server communicate by exchanging frames, which serve as the smallest unit of communication within the protocol. All frames share a common 9-byte header (Figure 1-7), which contains the length of the frame, its type, a bit field for flags, and a 31-bit stream identifier.\n\nFigure 1-7. Common 9-byte frame header\n\nThe 24-bit length field allows a single frame to carry up to 224 bytes of data.\n\nThe 8-bit type field determines the format and semantics of the frame.\n\nThe 8-bit flags field communicates frame-type specific boolean flags.\n\nThe 1-bit reserved field is always set to 0.\n\nThe 31-bit stream identifier uniquely identifies the HTTP/2 stream.\n\nBrief Introduction to Binary Framing\n\n|\n\n23",
      "content_length": 1347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Technically, the length field allows payloads of up to 224 bytes (~16MB) per frame. However, the HTTP/2 standard sets the default maximum payload size of DATA frames to 214 bytes (~16KB) per frame and allows the client and server to negotiate the higher value. Big‐ ger is not always better: smaller frame size enables effi‐ cient multiplexing and minimizes head-of-line block‐ ing.\n\nGiven this knowledge of the shared HTTP/2 frame header, we can now write a simple parser that can examine any HTTP/2 bytestream and identify different frame types, report their flags, and report the length of each by examining the first nine bytes of every frame. Fur‐ ther, because each frame is length-prefixed, the parser can skip ahead to the beginning of the next frame both quickly and efficiently —a big performance improvement over HTTP/1.x.\n\nOnce the frame type is known, the remainder of the frame can be interpreted by the parser. The HTTP/2 standard defines the follow‐ ing types:\n\nDATA\n\nUsed to transport HTTP message bodies\n\nHEADERS\n\nUsed to communicate header fields for a stream\n\nPRIORITY\n\nUsed to communicate sender-advised priority of a stream\n\nRST_STREAM\n\nUsed to signal termination of a stream\n\nSETTINGS\n\nUsed to communicate configuration parameters for the connec‐ tion\n\nPUSH_PROMISE\n\nUsed to signal a promise to serve the referenced resource\n\nPING\n\nUsed to measure the roundtrip time and perform “liveness” checks\n\n24\n\n| HTTP/2",
      "content_length": 1433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "GOAWAY\n\nUsed to inform the peer to stop creating streams for current connection\n\nWINDOW_UPDATE\n\nUsed to implement flow stream and connection flow control\n\nCONTINUATION\n\nUsed to continue a sequence of header block fragments\n\nYou will need some tooling to inspect the low-level HTTP/2 frame exchange. Your favorite hex viewer is, of course, an option. Or, for a more human-friendly representation, you can use a tool like Wireshark, which understands the HTTP/2 protocol and can cap‐ ture, decode, and analyze the exchange.\n\nThe good news is that the exact semantics of the preceding taxon‐ omy of frames is mostly only relevant to server and client imple‐ menters, who will need to worry about the semantics of flow con‐ trol, error handling, connection termination, and other details. The application layer features and semantics of the HTTP protocol remain unchanged: the client and server take care of the framing, multiplexing, and other details, while the application can enjoy the benefits of faster and more efficient delivery.\n\nHaving said that, even though the framing layer is hidden from our applications, it is useful for us to go just one step further and look at the two most common workflows: initiating a new stream and exchanging application data. Having an intuition for how a request, or a response, is translated into individual frames will give you the necessary knowledge to debug and optimize your HTTP/2 deploy‐ ments. Let’s dig a little deeper.\n\nFixed vs. Variable Length Fields and HTTP/2 HTTP/2 uses fixed-length fields exclusively. The overhead of an HTTP/2 frame is low (9-byte header for a data frame), and variable-length encoding savings do not offset the required com‐ plexity for the parsers, nor do they have a significant impact on the used bandwidth or latency of the exchange.\n\nBrief Introduction to Binary Framing\n\n|\n\n25",
      "content_length": 1858,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "For example, if variable-length encoding could reduce the overhead by 50%, for a 1,400-byte network packet, this would amount to just 4 saved bytes (0.3%) for a single frame.\n\nInitiating a New Stream Before any application data can be sent, a new stream must be cre‐ ated and the appropriate request metadata must be sent: optional stream dependency and weight, optional flags, and the HPACK- encoded HTTP request headers describing the request. The client initiates this process by sending a HEADERS frame (Figure 1-8) with all of the above.\n\nFigure 1-8. Decoded HEADERS frame in Wireshark\n\nWireshark decodes and displays the frame fields in the same order as encoded on the wire—e.g., compare the fields in the common frame header to the frame layout in Figure 1-7.\n\nThe HEADERS frame is used to declare and communicate metadata about the new request. The application payload, if available, is deliv‐ ered independently within the DATA frames. This separation allows the protocol to separate processing of “control traffic” from delivery\n\n26\n\n| HTTP/2",
      "content_length": 1053,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "of application data—e.g., flow control is applied only to DATA frames, and non-DATA frames are always processed with high priority.\n\nServer-Initiated Streams via PUSH_PROMISE HTTP/2 allows both client and server to initiate new streams. In the case of a server-initiated stream, a PUSH_PROMISE frame is used to declare the promise and communicate the HPACK-encoded response headers. The format of the frame is similar to HEADERS, except that it omits the optional stream dependency and weight, since the server is in full control of how the promised data is deliv‐ ered.\n\nTo eliminate stream ID collisions between client- and server- initiated streams, the counters are offset: client-initiated streams have odd-numbered stream IDs, and server-initiated streams have even-numbered stream IDs. As a result, because the stream ID in Figure 1-8 is set to “1”, we can infer that it is a client-initiated stream.\n\nSending Application Data Once a new stream is created, and the HTTP headers are sent, DATA frames (Figure 1-9) are used to send the application payload if one is present. The payload can be split between multiple DATA frames, with the last frame indicating the end of the message by toggling the END_STREAM flag in the header of the frame.\n\nFigure 1-9. DATA frame\n\nBrief Introduction to Binary Framing\n\n|\n\n27",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "The “End Stream” flag is set to “false” in Figure 1-9, indicating that the client has not finished transmitting the application payload; more DATA frames are coming.\n\nAside from the length and flags fields, there really isn’t much more to say about the DATA frame. The application payload may be split between multiple DATA frames to enable efficient multiplexing, but otherwise it is delivered exactly as provided by the application—i.e., the choice of the encoding mechanism (plain text, gzip, or other encoding formats) is deferred to the application.\n\nAnalyzing HTTP/2 Frame Data Flow Armed with knowledge of the different frame types, we can now revisit the diagram (Figure 1-10) we encountered earlier in “Request and Response Multiplexing” on page 8 and analyze the HTTP/2 exchange:\n\nFigure 1-10. HTTP/2 request and response multiplexing within a shared connection\n\nThere are three streams, with IDs set to 1, 3, and 5.\n\nAll three stream IDs are odd; all three are client-initiated streams.\n\nThere are no server-initiated (“push”) streams in this exchange. • The server is sending interleaved DATA frames for stream 1, which carry the application response to the client’s earlier request.\n\nThe server has interleaved the HEADERS and DATA frames for stream 3 between the DATA frames for stream 1—response mul‐ tiplexing in action!\n\n28\n\n| HTTP/2",
      "content_length": 1350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "The client is transferring a DATA frame for stream 5, which indi‐ cates that a HEADERS frame was transferred earlier.\n\nThe above analysis is, of course, based on a simplified representation of an actual HTTP/2 exchange, but it still illustrates many of the strengths and features of the new protocol. By this point, you should have the necessary knowledge to successfully record and analyze a real-world HTTP/2 trace—give it a try!\n\nNext steps with HTTP/2 As we said at the beginning of this excerpt, the good news is that all of our existing applications can be delivered with HTTP/2 without modification. The semantics and the core functionality of the HTTP protocol remain unchanged.\n\nHowever, we are not just interested in delivering a working applica‐ tion; our goal is to deliver the best performance! HTTP/2 enables a number of new optimizations that fundamentally change, or elimi‐ nate the need for, many of today’s “performance best practices”. We are no longer constrained by parallelism, requests are cheap, and we can finally step back and re-architect our applications to take advan‐ tage of granular caching, leverage server push, and so much more.\n\nSo, where to from here? A few resources to help you on your quest:\n\nHTTP/2 and HPACK specs\n\nHTTP/2 FAQ\n\nKnown implementations\n\nAnd, of course, I would be remiss if I didn’t mention the full version of “High Performance Browser Networking”! Pick up the print ver‐ sion, ebook, or check out the free online version for more hands-on performance tips and recommendations on optimizing your appli‐ cation and server infrastructure for HTTP/2.\n\nNext steps with HTTP/2\n\n|\n\n29",
      "content_length": 1634,
      "extraction_method": "Unstructured"
    }
  ]
}