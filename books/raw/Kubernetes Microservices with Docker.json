{
  "metadata": {
    "title": "Kubernetes Microservices with Docker",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 440,
    "conversion_date": "2025-12-19T17:31:51.732921",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Kubernetes Microservices with Docker.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-11)",
      "start_page": 1,
      "end_page": 11,
      "detection_method": "topic_boundary",
      "content": "Kubernetes \nMicroservices \nwith Docker\n—\nDeepak Vohra\nForeword by Massimo Nardone\nTHE EXPERT’S VOICE® IN OPEN SOURCE\n\n\n Kubernetes \nMicroservices with \nDocker \n Deepak Vohra \n\n\nKubernetes Microservices with Docker\nDeepak Vohra\t\n\t\n\t\n\t\nWhite Rock, British Columbia\t\t\n\t\n\t\n\t\nCanada\t \t\n\t\nISBN-13 (pbk): 978-1-4842-1906-5\t\n\t\nISBN-13 (electronic): 978-1-4842-1907-2\nDOI 10.1007/978-1-4842-1907-2 \nLibrary of Congress Control Number: 2016937418\nCopyright © 2016 by Deepak Vohra\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is \nconcerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction \non microfilms or in any other physical way, and transmission or information storage and retrieval, electronic \nadaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. Exempted \nfrom this legal reservation are brief excerpts in connection with reviews or scholarly analysis or material supplied \nspecifically for the purpose of being entered and executed on a computer system, for exclusive use by the purchaser \nof the work. Duplication of this publication or parts thereof is permitted only under the provisions of the Copyright \nLaw of the Publisher's location, in its current version, and permission for use must always be obtained from Springer. \nPermissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations are liable to \nprosecution under the respective Copyright Law.\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every \noccurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion \nand to the benefit of the trademark owner, with no intention of infringement of the trademark.\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified \nas such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.\nWhile the advice and information in this book are believed to be true and accurate at the date of publication, neither \nthe authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may \nbe made. The publisher makes no warranty, express or implied, with respect to the material contained herein.\nManaging Director: Welmoed Spahr\nLead Editor: Michelle Lowman\nTechnical Reviewer: Massimo Nardone\nEditorial Board: Steve Anglin, Pramila Balan, Louise Corrigan, Jonathan Gennick, Robert Hutchinson,  \nCelstin Suresh John, Michelle Lowman, James Markham, Susan McDermott, Matthew Moodie,  \nJeffrey Pepper, Douglas Pundick, Ben Renow-Clarke, Gwenan Spearing\nCoordinating Editor: Mark Powers\nCompositor: SPi Global\nIndexer: SPi Global\nArtist: SPi Global\nDistributed to the book trade worldwide by Springer Science+Business Media New York, 233 Spring Street,  \n6th Floor, New York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail orders-ny@springer-sbm.com, \nor visit www.springeronline.com. Apress Media, LLC is a California LLC and the sole member (owner) is Springer  \nScience + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware corporation.  \nFor information on translations, please e-mail rights@apress.com, or visit www.apress.com. \nApress and friends of ED books may be purchased in bulk for academic, corporate, or promotional use.  \neBook versions and licenses are also available for most titles. For more information, reference our Special Bulk \nSales–eBook Licensing web page at www.apress.com/bulk-sales.\nAny source code or other supplementary material referenced by the author in this text is available to readers at \nwww.apress.com/9781484219065. For additional information about how to locate and download your book’s \nsource code, go to www.apress.com/source-code/. Readers can also access source code at SpringerLink in the \nSupplementary Material section for each chapter.\nPrinted on acid-free paper\n\n\niii\nContents at a Glance\nAbout the Author ...................................................................................................xvii\nAbout the Technical Reviewer ................................................................................xix\nForeword ................................................................................................................xxi\n \n■Part I: Getting Started ......................................................................... 1\n \n■Chapter 1: Installing Kubernetes Using Docker ..................................................... 3\n \n■Chapter 2: Hello Kubernetes ................................................................................ 39\n \n■Chapter 3: Using Custom Commands and Environment Variables ...................... 77\n \n■Part II: Relational Databases ............................................................ 95\n \n■Chapter 4: Using MySQL Database ...................................................................... 97\n \n■Chapter 5: Using PostgreSQL Database ............................................................. 115\n \n■Chapter 6: Using Oracle Database ..................................................................... 141\n \n■Part III: NoSQL Database ................................................................. 165\n \n■Chapter 7: Using MongoDB Database ................................................................ 167\n \n■Chapter 8: Using Apache Cassandra Database .................................................. 201\n \n■Chapter 9: Using Couchbase .............................................................................. 231\n \n■Part IV: Apache Hadoop Ecosystem ................................................ 275\n \n■Chapter 10: Using Apache Hadoop Ecosystem .................................................. 277\n \n■Chapter 11: Using Apache Solr .......................................................................... 313\n \n■Chapter 12: Using Apache Kafka ....................................................................... 347\n\n\n ■ CONTENTS AT A GLANCE\niv\n \n■Part V: Multi Containers and Nodes ................................................ 373\n \n■Chapter 13: Creating a Multi-Container Pod ...................................................... 375\n \n■Chapter 14: Installing Kubernetes on a Multi-Node Cluster .............................. 399\nIndex ..................................................................................................................... 429\n\n\nv\nContents\nAbout the Author ...................................................................................................xvii\nAbout the Technical Reviewer ................................................................................xix\nForeword ................................................................................................................xxi\n \n■Part I: Getting Started ......................................................................... 1\n \n■Chapter 1: Installing Kubernetes Using Docker ..................................................... 3\nSetting the Environment ................................................................................................... 4\nInstalling Docker .............................................................................................................. 5\nInstalling Kubernetes ..................................................................................................... 15\nStarting etcd ................................................................................................................... 23\nStarting Kubernetes Master ........................................................................................... 25\nStarting Service Proxy .................................................................................................... 28\nListing the Kubernetes Docker Containers ..................................................................... 29\nInstalling kubectl ............................................................................................................ 32\nListing Services .............................................................................................................. 35\nListing Nodes .................................................................................................................. 36\nTesting the Kubernetes Installation ................................................................................ 36\nSummary ........................................................................................................................ 38\n \n■Chapter 2: Hello Kubernetes ................................................................................ 39\nOverview ........................................................................................................................ 39\nWhat Is a Node? ....................................................................................................................................39\nWhat Is a Cluster?.................................................................................................................................40\nWhat Is a Pod? ......................................................................................................................................40\n\n\n ■ CONTENTS\nvi\nWhat Is a Service? ................................................................................................................................40\nWhat Is a Replication Controller? .........................................................................................................40\nWhat Is a Label? ...................................................................................................................................41\nWhat Is a Selector? ...............................................................................................................................41\nWhat Is a Name? ..................................................................................................................................41\nWhat Is a Namespace? .........................................................................................................................41\nWhat Is a Volume? ................................................................................................................................41\nWhy Kubernetes?..................................................................................................................................41\nSetting the Environment ................................................................................................. 42\nCreating an Application Imperatively .............................................................................. 43\nCreating a Service ................................................................................................................................44\nDescribing a Pod ...................................................................................................................................46\nInvoking the Hello-World Application ....................................................................................................47\nScaling the Application .........................................................................................................................48\nDeleting a Replication Controller ..........................................................................................................52\nDeleting a Service ................................................................................................................................53\nCreating an Application Declaratively ............................................................................. 53\nCreating a Pod Deﬁ nition ......................................................................................................................54\nCreating a Service Deﬁ nition ................................................................................................................58\nCreating a Replication Controller Deﬁ nition..........................................................................................61\nInvoking the Hello-World Application ....................................................................................................64\nScaling the Application .........................................................................................................................68\nUsing JSON for the Resource Deﬁ nitions ....................................................................... 70\nSummary ........................................................................................................................ 76\n \n■Chapter 3: Using Custom Commands and Environment Variables ...................... 77\nSetting the Environment ................................................................................................. 77\nThe ENTRYPOINT and CMD Instructions ......................................................................... 78\nThe Command and Args Fields in a Pod Deﬁ nition......................................................... 79\nEnvironment Variables .................................................................................................... 80\n\n\n ■ CONTENTS\nvii\nUsing the Default ENTRYPOINT and CMD from a Docker Image ..................................... 81\nOverriding Both the ENTRYPOINT and CMD .................................................................... 84\nSpecifying both the Executable and the Parameters in the Command Mapping ........... 87\nSpecifying Both the Executable and the Parameters in the Args Mapping .................... 90\nSummary ........................................................................................................................ 93\n \n■Part II: Relational Databases ............................................................ 95\n \n■Chapter 4: Using MySQL Database ...................................................................... 97\nSetting the Environment ................................................................................................. 97\nCreating a Service .......................................................................................................... 99\nCreating a Replication Controller.................................................................................. 100\nListing the Pods ............................................................................................................ 104\nListing Logs .................................................................................................................. 104\nDescribing the Service ................................................................................................. 106\nStarting an Interactive Shell ......................................................................................... 107\nStarting the MySQL CLI ................................................................................................ 109\nCreating a Database Table ............................................................................................ 110\nExiting the MySQL CLI and Interactive Shell ................................................................. 111\nScaling the Replicas ..................................................................................................... 111\nDeleting the Replication Controller ............................................................................... 113\nSummary ...................................................................................................................... 114\n \n■Chapter 5: Using PostgreSQL Database ............................................................. 115\nSetting the Environment ............................................................................................... 115\nCreating a PostgreSQL Cluster Declaratively ............................................................... 117\nCreating a Service ..............................................................................................................................117\nCreating a Replication Controller ........................................................................................................119\nGetting the Pods .................................................................................................................................123\nStarting an Interactive Command Shell ..............................................................................................123\n\n\n ■ CONTENTS\nviii\nStarting the PostgreSQL SQL Terminal ...............................................................................................124\nCreating a Database Table ..................................................................................................................125\nExiting the Interactive Command Shell ...............................................................................................126\nScaling the PostgreSQL Cluster ..........................................................................................................127\nListing the Logs ..................................................................................................................................128\nDeleting the Replication Controller .....................................................................................................130\nStopping the Service ..........................................................................................................................131\nCreating a PostgreSQL Cluster Imperatively ................................................................ 131\nCreating a Replication Controller ........................................................................................................132\nGetting the Pods .................................................................................................................................132\nCreating a Service ..............................................................................................................................133\nCreating a Database Table ..................................................................................................................134\nScaling the PostgreSQL Cluster ..........................................................................................................137\nDeleting the Replication Controller .....................................................................................................138\nStopping the Service ..........................................................................................................................139\nSummary ...................................................................................................................... 139\n \n■Chapter 6: Using Oracle Database ..................................................................... 141\nSetting the Environment ............................................................................................... 141\nCreating an Oracle Database Instance Imperatively .................................................... 142\nListing Logs ........................................................................................................................................144\nCreating a Service ..............................................................................................................................145\nScaling the Database ..........................................................................................................................146\nDeleting the Replication Controller and Service .................................................................................147\nCreating an Oracle Database Instance Declaratively ................................................... 148\nCreating a Pod ....................................................................................................................................148\nCreating a Service ..............................................................................................................................150\nCreating a Replication Controller ........................................................................................................153\nKeeping the Replication Level ............................................................................................................156\nScaling the Database ..........................................................................................................................158\nStarting the Interactive Shell ..............................................................................................................159\n\n\n ■ CONTENTS\nix\nConnecting to Database......................................................................................................................160\nCreating a User ...................................................................................................................................161\nCreating a Database Table ..................................................................................................................162\nExiting the Interactive Shell ................................................................................................................163\nSummary ...................................................................................................................... 163\n \n■Part III: NoSQL Database ................................................................. 165\n \n■Chapter 7: Using MongoDB Database ................................................................ 167\nSetting the Environment ............................................................................................... 167\nCreating a MongoDB Cluster Declaratively ................................................................... 169\nCreating a Service ..............................................................................................................................169\nCreating a Replication Controller ........................................................................................................173\nCreating a Volume ...............................................................................................................................176\nListing the Logs ..................................................................................................................................178\nStarting the Interactive Shell for Docker Container ............................................................................180\nStarting a Mongo Shell .......................................................................................................................182\nCreating a Database ...........................................................................................................................182\nCreating a Collection ..........................................................................................................................183\nAdding Documents .............................................................................................................................184\nFinding Documents .............................................................................................................................186\nFinding a Single Document .................................................................................................................186\nFinding Speciﬁ c Fields in a Single Document ....................................................................................187\nDropping a Collection .........................................................................................................................188\nExiting Mongo Shell and Interactive Shell ..........................................................................................188\nScaling the Cluster .............................................................................................................................188\nDeleting the Replication Controller .....................................................................................................189\nDeleting the Service ...........................................................................................................................190\nUsing a Host Port ................................................................................................................................190\nCreating a MongoDB Cluster Imperatively .................................................................... 194\nCreating a Replication Controller ........................................................................................................194\n\n\n ■ CONTENTS\nx\nListing the Pods ..................................................................................................................................195\nListing the Logs ..................................................................................................................................196\nCreating a Service ..............................................................................................................................197\nScaling the Cluster .............................................................................................................................198\nDeleting the Service and Replication Controller .................................................................................200\nSummary ...................................................................................................................... 200\n \n■Chapter 8: Using Apache Cassandra Database .................................................. 201\nSetting the Environment ............................................................................................... 201\nCreating a Cassandra Cluster Declaratively ................................................................. 203\nCreating a Service ..............................................................................................................................203\nCreating a Replication Controller ........................................................................................................206\nScaling the Database ..........................................................................................................................211\nDescribing the Pod .............................................................................................................................212\nStarting an Interactive Shell ...............................................................................................................213\nStarting the CQL Shell.........................................................................................................................215\nCreating a Keyspace ...........................................................................................................................215\nAltering a Keyspace ............................................................................................................................215\nUsing a Keyspace ...............................................................................................................................216\nCreating a Table ..................................................................................................................................216\nDeleting from a Table ..........................................................................................................................217\nTruncating a Table ...............................................................................................................................218\nDropping a Table and Keyspace ..........................................................................................................218\nCreating a Volume ...............................................................................................................................219\nCreating a Cassandra Cluster Imperatively .................................................................. 225\nCreating a Replication Controller ........................................................................................................225\nCreating a Service ..............................................................................................................................227\nScaling the Database ..........................................................................................................................228\nDeleting the Replication Controller and Service .................................................................................229\nSummary ...................................................................................................................... 230\n",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 12-22)",
      "start_page": 12,
      "end_page": 22,
      "detection_method": "topic_boundary",
      "content": " ■ CONTENTS\nxi\n \n■Chapter 9: Using Couchbase .............................................................................. 231\nSetting the Environment ............................................................................................... 231\nCreating a Couchbase Cluster Declaratively ................................................................ 234\nCreating a Pod ....................................................................................................................................234\nCreating a Service ..............................................................................................................................237\nCreating a Replication Controller ........................................................................................................239\nListing the Pods ..................................................................................................................................243\nListing the Logs ..................................................................................................................................243\nDescribing the Service .......................................................................................................................244\nListing the Endpoints ..........................................................................................................................244\nSetting Port Forwarding ......................................................................................................................244\nLogging into Couchbase Web Console ................................................................................................246\nConﬁ guring Couchbase Server ...........................................................................................................247\nAdding Documents .............................................................................................................................255\nStarting an Interactive Shell ...............................................................................................................264\nUsing the cbtransfer Tool ....................................................................................................................265\nCreating a Couchbase Cluster Imperatively ................................................................. 266\nCreating a Replication Controller ........................................................................................................266\nListing the Pods ..................................................................................................................................266\nCreating a Service ..............................................................................................................................268\nScaling the Cluster .............................................................................................................................269\nKeeping the Replication Level ............................................................................................................270\nSetting Port Forwarding ......................................................................................................................272\nLogging in to Couchbase Admin Console ............................................................................................272\nSummary ...................................................................................................................... 273\n \n■Part IV: Apache Hadoop Ecosystem ................................................ 275\n \n■Chapter 10: Using Apache Hadoop Ecosystem .................................................. 277\nSetting the Environment ............................................................................................... 277\nCreating an Apache Hadoop Cluster Declaratively ....................................................... 278\nCreating a Service ..............................................................................................................................279\n\n\n ■ CONTENTS\nxii\nCreating a Replication Controller ........................................................................................................281\nListing the Pods ..................................................................................................................................283\nListing Logs ........................................................................................................................................284\nScaling a Cluster .................................................................................................................................285\nStarting an Interactive Shell ...............................................................................................................286\nRunning a MapReduce Application .....................................................................................................287\nRunning Hive ................................................................................................................ 296\nRunning HBase ............................................................................................................. 302\nDeleting the Replication Controller and Service ........................................................... 307\nCreating an Apache Hadoop Cluster Imperatively ........................................................ 307\nCreating a Replication Controller ........................................................................................................307\nListing the Pods ..................................................................................................................................308\nScaling a Cluster .................................................................................................................................309\nCreating a Service ..............................................................................................................................309\nStarting an Interactive Shell ...............................................................................................................310\nSummary ...................................................................................................................... 311\n \n■Chapter 11: Using Apache Solr .......................................................................... 313\nSetting the Environment ............................................................................................... 314\nCreating a Service ........................................................................................................ 315\nListing Service Endpoints ............................................................................................. 317\nDescribing the Service ................................................................................................. 317\nCreating a Replication Controller.................................................................................. 318\nListing the Pods ............................................................................................................ 321\nDescribing a Replication Controller .............................................................................. 322\nListing the Logs ............................................................................................................ 323\nStarting an Interactive Shell ......................................................................................... 325\nCreating a Solr Core ..................................................................................................... 328\nIndexing Documents ..................................................................................................... 329\nAccessing Solr on Command Line with a REST Client ................................................. 332\n\n\n ■ CONTENTS\nxiii\nSetting Port Forwarding ............................................................................................... 337\nAccessing Solr in Admin Console ................................................................................. 338\nScaling the Cluster ....................................................................................................... 344\nSummary ...................................................................................................................... 345\n \n■Chapter 12: Using Apache Kafka ....................................................................... 347\nSetting the Environment ............................................................................................... 348\nModifying the Docker Image ........................................................................................ 349\nCreating a Service ........................................................................................................ 355\nCreating a Replication Controller.................................................................................. 358\nListing the Pods ............................................................................................................ 361\nDescribing a Pod .......................................................................................................... 362\nStarting an Interactive Shell ......................................................................................... 363\nStarting the Kafka Server ............................................................................................. 364\nCreating a Topic ............................................................................................................ 366\nStarting a Kafka Producer ............................................................................................ 367\nStarting a Kafka Consumer .......................................................................................... 367\nProducing and Consuming Messages .......................................................................... 367\nScaling the Cluster ....................................................................................................... 368\nDeleting Replication Controller and Service ................................................................. 370\nSummary ...................................................................................................................... 371\n \n■Part V: Multi Containers and Nodes ................................................ 373\n \n■Chapter 13: Creating a Multi-Container Pod ...................................................... 375\nHow to ﬁ nd Number of Containers in a Pod? ............................................................... 376\nTypes of Applications Using a Multi-Container Pod ...................................................... 376\nSetting the Environment ............................................................................................... 377\nCreating a Service ........................................................................................................ 378\nDescribing a Service  ................................................................................................... 379\nCreating a Replication Container .................................................................................. 380\n\n\n ■ CONTENTS\nxiv\nListing the Pods ............................................................................................................ 382\nListing the Docker Containers ...................................................................................... 383\nDescribing the Service after Creating Replication Controller ....................................... 384\nInvoking the Hello World Application on Command Line .............................................. 385\nStarting the Interactive Shell ........................................................................................ 386\nStarting PostgreSQL Shell ............................................................................................ 387\nSetting Port Forwarding ............................................................................................... 387\nOpening the Hello World Application in a Browser ....................................................... 388\nScaling the Cluster ....................................................................................................... 389\nListing the Docker Containers ............................................................................................................391\nDescribing the Service after Scaling ..................................................................................................392\nSetting Port Forwarding ......................................................................................................................392\nOpening the Hello World Application in a Browser .............................................................................393\nInvoking the Hello World Application from Command Line .................................................................394\nDeleting the Replication Controller .....................................................................................................396\nDeleting the Service ...........................................................................................................................397\nSummary ...................................................................................................................... 397\n \n■Chapter 14: Installing Kubernetes on a Multi-Node Cluster .............................. 399\nComponents of a Multi-Node Cluster ........................................................................... 400\nSetting the Environment ............................................................................................... 400\nInstalling the Master Node ........................................................................................... 402\nSetting Up Flanneld and etcd  ............................................................................................................402\nStarting the Kubernetes Master .........................................................................................................409\nRunning the Service Proxy .................................................................................................................411\nTesting the One-Node Cluster ...................................................................................... 412\nAdding a Worker Node .................................................................................................. 412\nExporting the Master IP ......................................................................................................................412\nSetting Up Flanneld ............................................................................................................................413\nStarting Up Kubernetes on Worker Node ............................................................................................418\nRunning the Service Proxy .................................................................................................................419\n\n\n ■ CONTENTS\nxv\nTesting the Kubernetes Cluster .................................................................................... 419\nRunning an Application on the Cluster ......................................................................... 419\nExposing the Application as a Service ......................................................................... 420\nTesting the Application in a Browser ............................................................................ 422\nScaling the Application ................................................................................................. 423\nSummary ...................................................................................................................... 427\nIndex ..................................................................................................................... 429\n\n\nxvii\n About the Author \n Deepak  Vohra  is a consultant and a principal member of the NuBean.com \nsoftware company. Deepak is a Sun-certified Java programmer and Web \ncomponent developer. He has worked in the fields of XML, Java \nprogramming, and Java EE for over seven years. Deepak is the coauthor of \n Pro XML Development with Java Technology (Apress, 2006). Deepak is also \nthe author of the  JDBC 4.0 and  Oracle JDeveloper for J2EE Development, \nProcessing XML Documents with Oracle JDeveloper 11g, EJB 3.0 Database \nPersistence with Oracle Fusion Middleware 11g , and  Java EE Development \nin Eclipse IDE (Packt Publishing). He also served as the technical reviewer \non  WebLogic: The Definitive Guide (O’Reilly Media, 2004) and  Ruby \nProgramming for the Absolute Beginner (Cengage Learning PTR, 2007). \nDeepak is the author of  Pro Couchbase Development, Pro MongoDB \nDevelopment , and  Pro Docker , all published by Apress in 2015. \n \n\n\nxix\n About the Technical Reviewer \n Massimo  Nardone  holds a Master of Science degree in Computing \nScience from the University of Salerno, Italy. He has worked as a Project \nManager, Software Engineer, Research Engineer, Chief Security Architect, \nInformation Security Manager, PCI/SCADA Auditor, and Senior Lead IT \nSecurity/Cloud/SCADA Architect for many years. He currently works as \nChief Information Security Office (CISO) for Cargotec Oyj. He has more \nthan 22 years of work experience in IT including Security, SCADA, Cloud \nComputing, IT Infrastructure, Mobile, Security, and WWW technology \nareas for both national and international projects. He worked as a visiting \nlecturer and supervisor for exercises at the Networking Laboratory of the \nHelsinki University of Technology (Aalto University). He has been \nprogramming and teaching how to program with Android, Perl, PHP, Java, \nVB, Python, C/C++, and MySQL for more than 20 years. He holds four \ninternational patents (PKI, SIP, SAML, and Proxy areas). \n He is the coauthor of  Pro Android Games (Apress, 2015). \n Massimo dedicates his work on this book to his loving brothers Mario \nNardone and Roberto Nardone, who are always there when he needs them. \n \n \n\n\nxxi\n Foreword \n It is a great pleasure to provide the Foreword for this book, as I’ve been reading and following Deepak \nVohra’s work for some time. Deepak has been developing Web components and Java applications for many \nyears, and the scope of his expertise is reflected in the books he has written – as is his passion to share that \nknowledge with others. \n About a year ago, I was given the opportunity to perform a technical review on his Pro Couchbase \nDevelopment book, and we formed an immediate connection. Since then, I’ve served as technical reviewer \non several more of his books, including this one. The reason I keep coming back is simple – I always come \naway knowing more than I did before. \n Docker is a new container technology that has become very popular because it is great for building \nand sharing disk images and enables users to run different operating systems such as Ubuntu, Fedora, and \nCentos. Docker is often used when a version control framework is required for an application’s operating \nsystem, to distribute applications on different machines, or to run code on laptop in the same environment \nas on the server. In general, Docker will always run the same, regardless of the environment in which it will \nbe running. \n Kubernetes is an open source container cluster manager that complements and extends Docker’s \nsoftware encapsulation power and makes it easier to organize and schedule applications across a fleet \nof machines. It’s a lightweight, portable (suited for the cloud architecture) and modular tool that can be \nrun on almost any platform with different local machine solutions. Kubernetes offers a number of distinct \nadvantages, first and foremost being that it combines all necessary tools – orchestration, service discovery, \nand load balancing – together in one nice package for you. Kubernetes also boasts heavy involvement from \nthe developer community. \n Kubernetes Microservices with Docker will show you how to use these two powerful tools in unison to \nmanage complex big data and enterprise applications. Installing Kubernetes on single nodes and multi-\nnode clusters, creating multi-container pods, using Kubernetes with the Apache Hadoop Ecosystem and \nNoSQL Databases – it’s all here, and more. So sit back, and let Deepak be your guide.\n —Massimo Nardone \n Chief Security Information Officer (CISO), Cargotec Oyj \n\n\n PART I \n Getting Started \n  \n\n\n3\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_1\n CHAPTER 1 \n Installing Kubernetes \nUsing Docker \n Kubernetes is software for managing a cluster of Docker containers. Kubernetes orchestration includes \nscheduling, distributing workload, and scaling. Kubernetes takes the software encapsulation provided by \nDocker further by introducing Pods. A Pod is a collection of one or more Docker containers with single \ninterface features such as providing networking and filesystem at the Pod level rather than at the container \nlevel. Kubernetes also introduces “labels” using which services and replication controllers (replication \ncontroller is used to scale a cluster) identify or select the containers or pods they manage. Kubernetes is \nlightweight, portable (suited for the cloud architecture), and modular. \n Kubernetes may be run on almost any platform.  Local machine solutions include local Docker based, \nVagrant, and no-VM local cluster. Hosted solutions include Google Container Engine. Some of the other \nplatforms supported by Kubernetes are Fedora (Ansible and Manual), Amazon Web Services, Mesos, \nvSphere, and CoreOS. Kubernetes is an orchestration software for Docker containers; the recommended \nsolution for installation is to use the Docker Engine. In this chapter we shall install Kubernetes on Docker, \nwhich runs on Ubuntu. We shall use an Amazon EC2 instance hosting Ubuntu as the operating system. In \nthis chapter, a single node installation of Kubernetes is discussed. Multi-node installation of Kubernetes is \ndiscussed in chapter  14 . This chapter has the following sections.\n Setting the Environment \n Installing Docker \n Installing Kubernetes \n Starting etcd \n Starting Kubernetes Master \n Starting Service Proxy \n Listing the Kubernetes Docker Containers \n Installing kubectl \n Listing Services \n Listing Nodes \n Testing the Kubernetes Installation \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n4\n Setting the Environment \n The following software is required for this chapter.\n - Docker Engine (latest version) \n - Kubernetes (version 1.01) \n Linux is required to support 64-bit software. We have used an Amazon EC2 instance created from AMI \nUbuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-d05e75b8. An Amazon EC2 instance based on the \nUbuntu AMI is shown in Figure  1-1 . \n Figure 1-1.  Amazon EC2 Instance Based on Ubuntu AMI \n A different Ubuntu version may be used if the requirement of a 64-bit architecture is met. The minimum \nkernel version requirement is 3.10. The kernel version may be verified with the following command. \n uname –r \n The Public IP would be different for different users. Multiple Amazon EC2 instances and therefore \nmultiple Public IP addresses have been used in the book as a different Public IP is assigned each time an \nAmazon EC2 instance is started. The Private IP Address of an Amazon EC2 instance is the same across \nrestarts. SSH into an Ubuntu instance on Amazon EC2 (Public IP is 52.91.80.173 in following command). \n ssh -i \"docker.pem\" ubuntu@52.91.80.173 \n The Amazon EC2 instance  gets  logged in as shown in Figure  1-2 . The command prompt becomes \n“ubuntu@ip-172-30-1-190” instead of root@localhost. Ip 172.30.1.190 is the Private IP of the Amazon EC2 \ninstance and would also be different for different users. \n \n",
      "page_number": 12
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 23-30)",
      "start_page": 23,
      "end_page": 30,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n5\n In the next section we shall install Docker on Ubuntu hosted on an Amazon EC2 instance . \n Installing Docker \n Ubuntu uses apt for package management; apt stores a list of repositories in the /etc/apt/sources.list list. \nDocker’s apt repository is kept in the /etc/apt/sources.list.d/docker.list file. First, add the new repository key \n(gpg key) for the Docker repository with the following command. \n sudo apt-key adv --keyserver hkp://pgp.mit.edu:80 --recv-keys \n58118E89F3A912897C070ADBF76221572C52609D \n Figure 1-2.  Loging into an Amazon EC2 instance \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n6\n The new gpg key gets  added  as shown in Figure  1-3 . \n Next, update  the  apt sources for the Docker repository in the /etc/apt/sources.list.d/docker.list file \nbased on the Ubuntu distribution, which may be found with the following command. \n lsb_release –a \n For Ubuntu Trusty, add the following line to the /etc/apt/sources.list.d/docker.list file; the docker.list \nfile may be opened with sudo vi /etc/apt/sources.list.d/docker.list. \n deb https://apt.dockerproject.org/repo ubuntu-trusty main \n Create the /etc/apt/sources.list.d/docker.list file if the file does not already exist. The updated file is \nshown in Figure  1-4 .  Save the file with the :wq command if opened in the vi editor. \n Figure 1-3.  Adding a new gpg key \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n7\n The entry to be added would be different for  different  Ubuntu distributions as listed in Table  1-1 . \n Figure 1-4.  Creating the docker.list file \n Table 1-1.  The docker.list file Entry Based on Ubuntu Distribution \n Ubuntu Distribution \n Entry \n Ubuntu Precise 12.04 (LTS) \n deb  https://apt.dockerproject.org/repo ubuntu-precise main \n Ubuntu Trusty 14.04 (LTS) \n deb  https://apt.dockerproject.org/repo ubuntu-trusty main \n Ubuntu Vivid 15.04 \n deb  https://apt.dockerproject.org/repo ubuntu-vivid main \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n8\n Run the following commands after updating the /etc/apt/sources.list.d/docker.list file to update the apt \npackage index. \n sudo apt-get update \n Apt package index gets updated as shown in Figure  1-5 . \n Figure 1-5.  Updating  Ubuntu  Package List \n Purge the old repository if it exists with the following command. \n sudo apt-get purge lxc-docker* \n The output in Figure  1-6 indicates that the old  packages  lxc-docker and lxc-docker-virtual-package are \nnot installed and therefore not removed. \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n9\n Run the following command to verify that apt is pulling from the updated repository for Docker. \n sudo apt-cache policy docker-engine \n The output in Figure  1-7 indicates that the  new  repository ubuntu-trusty as specified in the /etc/apt/\nsources.list.d/docker.list is being used. \n Figure 1-6.  Purging the  Old  Repository \n Figure 1-7.  Using the  Updated  Repository verification \n \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n10\n Next, install the prerequisites for Ubuntu, but first update the package manager with the following \ncommand. \n sudo apt-get update \n The package manager gets  updated  as shown in Figure  1-8 . \n Figure 1-8.  Updating the Package Manager \n Install the  prerequisite  linux-image-extra package with the following command. \n sudo apt-get install linux-image-generic-lts-trusty \n When the preceding command is run, select Y if prompted with the following message.\n After this operation, 281 MB of additional disk space will be used. \n Do you want to continue? [Y/n] \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n11\n The message prompt is shown in Figure  1-9 . \n Figure 1-9.  Message Prompt to Continue \n Subsequently, before the command completes, a Package Configuration dialog might prompt with the \nfollowing message:\n A new version of /boot/grub/menu.lst is available, but the version installed currently has \nbeen locally modified. What would you like to do about menu.lst? \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n12\n Reboot the system with the following command. \n sudo reboot \n When the sudo reboot command is run the AmazonEC2 instance is exited. Reconnect with the Amazon \nEC2 Ubuntu instance with the same ssh command as before. \n ssh -i \"docker.pem\" ubuntu@52.91.80.173 \n After the host system reboots, update the package manager again with the following command. \n sudo apt-get update \n Figure 1-10.  Selecting the  Default  Package Configuration \n Select the default selection, which is “keep the local version currently installed” and click on Enter as \nshown in Figure  1-10 . \n \n",
      "page_number": 23
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 31-43)",
      "start_page": 31,
      "end_page": 43,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n13\n Package manager gets updated as shown in Figure  1-11 . \n Figure 1-11.  Updating Package  Manager List after Reboot \n Install Docker with the following command. \n sudo apt-get install docker-engine \n Select Y at the following prompt, if displayed, as shown in Figure  1-12 .\n After this operation, 60.3 MB of  additional  disk space will be used. \n Do you want to continue? [Y/n] \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n14\n The Docker engine gets installed as shown in Figure  1-13 . \n Figure 1-13.  Installing  the  Docker Engine \n Figure 1-12.  Message Prompt about the additional disk space being added \n \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n15\n Start the Docker service with the following command. \n sudo service docker start \n To verify the status of the Docker service, run the following command. \n sudo service docker status \n The output from the preceding commands is shown in Figure  1-14 . The docker engine is indicated as \nrunning as process 2697. \n Figure 1-14.  Starting  Docker and  verifying its Status \n Having installed Docker, next we shall install Kubernetes. \n Installing Kubernetes \n Kubernetes is an open source container cluster manager. The  main  components of Kubernetes are the \nfollowing:\n \n 1. \n etcd \n \n 2. \n Kubernetes master \n \n 3. \n Service proxy \n \n 4. \n kubelet \n etcd is a simple, secure, fast and reliable distributed key-value store. \n Kubernetes master exposes the Kubernetes API using which containers are run on nodes to handle tasks. \n kubelet is an agent that runs on each node to monitor the containers running on the node, restarting \nthem if required to keep the replication level. \n A service proxy runs on each node to provide the Kubernetes service interface for clients. A service is an \nabstraction for the logical set of pods represented by the service, and a service selector is used to select the \npods represented by the service.  The  service proxy routes the client traffic to a matching pod. Labels are used \nto match a service with a pod. \n Optionally create a directory (/kubernetes) to install Kubernetes and set its permissions to global (777). \n sudo mkdir /kubernetes \n sudo chmod -R 777 /kubernetes \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n16\n Change directory to the /kubernetes directory and start the Docker engine. \n cd /kubernetes \n sudo service docker start \n If the Docker Engine is not running, it gets started.  The  Docker Engine is shown as already running in \nFigure  1-16 . \n Figure 1-15.  Creating  a  Directory to install Kubernetes \n The output from the preceding commands is shown in Figure  1-15 . \n Figure 1-16.  Starting Docker if not already running \n As a prerequisite we need to set some Linux kernel parameters if not already set. Add support for \nmemory and swap accounting.  The  following configs should be turned on in the kernel. \n CONFIG_RESOURCE_COUNTERS=y \n CONFIG_MEMCG=y \n CONFIG_MEMCG_SWAP=y \n CONFIG_MEMCG_SWAP_ENABLED=y \n CONFIG_MEMCG_KMEM=y \n The kernel configs are enabled when the Ubuntu system boots and  the  kernel configuration file is in the \n/boot directory. Change directory (cd) to the /boot directory and list the files/directories. \n cd /boot \n ls –l \n The files in  the  /boot directory get listed as shown in Figure  1-17 . The kernel configs are configured in \nthe config-3.13.0-48-generic file. The kernel version could be different for different users; for example, the \nkernel config file could /boot/config-3.13.0-66-generic. \n \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n17\n Open the config-3.13.0-48-generic file in a vi editor. \n sudo vi /boot/config-3.13.0-48-generic \n The  kernel configuration parameters get listed as shown in Figure  1-18 . \n Figure 1-18.  Kernel Configuration Parameter \n Figure 1-17.  Listing the Files in the  /boot Directory \n \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n18\n Most of the configs listed earlier are already turned on as shown in Figure  1-19 . The CONFIG_MEMCG_\nSWAP_ENABLED config is not set. \n Figure 1-19.  Most of the Required  Kernel  Parameters are already Set \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n19\n Next, we need to add support for memory and swap accounting to the kernel. The command-line \nparameters provided to the kernel may be listed with the following command. \n cat /proc/cmdline \n As shown in Figure  1-21 memory and  swap accounting are not turned on. \n Figure 1-20.  Setting  the  CONFIG_MEMCG_SWAP_ENABLED Kernel Parameter \n Figure 1-21.  Listing  the  Command-Line Parameters \n Set CONFIG_MEMCG_SWAP_ENABLED =  y  and save the kernel configuration file as shown in \nFigure  1-20 . \n \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n20\n Grub 2 is the default boot loader for Ubuntu. To turn on memory and swap accounting, open the /etc/\ndefault/grub file in the vi editor.  The  GRUB_CMDLINE_LINUX is set to an empty string as shown in \nFigure  1-22 . \n Figure 1-22.  The /etc/default/grub  file \n Set the GRUB_CMDLINE_LINU as follows, which enables memory and swap accounting in the kernel \nat boot. \n GRUB_CMDLINE_LINUX=\"cgroup_enable=memory swapaccount=1\" \n The modified /etc/default/grub file  is  shown in Figure  1-23 . Save the file with the :wq command. \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n21\n Update the grub.cfg file with the following command. \n sudo update-grub \n Figure 1-23.  Modified /etc/default/grub  file \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n22\n Reboot the system. \n sudo reboot \n Connection to  the  Ubuntu Amazon EC2 instance gets closed as shown in Figure  1-25 . \n Figure 1-25.  Rebooting  Ubuntu Instance \n SSH log in back into the Ubuntu instance. Rerun the command to list the command-line kernel \nparameters. \n cat /proc/cmdline \n The cgroup_enable =  memory  swapaccount = 1 settings get output as shown in Figure  1-26 . \n The  grub configuration file gets generated as shown in Figure  1-24 . \n Figure 1-24.  Generating an  Updated  Grub Configuration file \n \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n23\n Having set the  prerequisite  kernel parameters, next we shall start the Kubernetes components etcd, \nmaster, and service proxy. \n Starting etcd \n Run etcd with the following docker run command. \n sudo docker run --net=host -d gcr.io/google_containers/etcd:2.0.12 /usr/local/bin/etcd \n-- addr=127.0.0.1:4001 --bind-addr=0.0.0.0:4001 --data-dir=/var/etcd/data \n Figure 1-26.  Updated  Settings \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n24\n The docker run command to start  etcd is  required to be run each time the Kubernetes cluster manager \nis to be started. Subsequent starts of etcd do not need to download the container image as shown in \nFigure  1-28 . \n The docker run command parameters are as follows (Table  1-2 ). \n Table 1-2.  The docker run Command Parameters to start etcd \n Parameter \n Description \n --net = host \n Connects the Docker container to a network \nmaking use of the host container network inside \nthe container \n -d \n Starts the container in the background \n gcr.io/google_containers/etcd:2.0.12 \n The container image \n /usr/local/bin/etcd --addr = 127.0.0.1:4001 \n--bind-addr = 0.0.0.0:4001 --data-dir=/var/etcd/data \n The command to run \n The output from the preceding command is shown in Figure  1-27 . \n Figure 1-27.  Starting etcd \n Figure 1-28.  Subsequent Start of etcd does not need to download the container Image again \n \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n25\n Starting Kubernetes Master \n The Kubernetes master is started using the kubelet, which also starts the other Master components \napiserver, scheduler, controller, and pause, which are discussed in Table  1-3 . \n Table 1-3.  The docker run  Command  Parameters to start etcd \n Master Component \n Description \n Apiserver \n The apiserver takes API requests, processes them, and stores the result in etcd if \nrequired and returns the result. \n Scheduler \n The scheduler monitors the API for unscheduled pods and schedules them on a \nnode to run and also notifies the about the same to the API. \n Controller \n The controller manages the replication level of the pods, starting new pods in a \nscale up event and stopping some of the pods in a scale down. \n Pause \n The pause keeps the port mappings of all the containers in the pod or the network \nendpoint of the pod. \n Run the Kubernetes master with the following command. \n sudo docker run \\ \n    --volume=/:/rootfs:ro \\ \n    --volume=/sys:/sys:ro \\ \n    --volume=/dev:/dev \\ \n    --volume=/var/lib/docker/:/var/lib/docker:ro \\ \n    --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\ \n    --volume=/var/run:/var/run:rw \\ \n    --net=host \\ \n    --pid=host \\ \n    --privileged=true \\ \n    -d \\ \n    gcr.io/google_containers/hyperkube:v1.0.1 \\ \n     /hyperkube kubelet --containerized --hostname-override=\"127.0.0.1\" \n--address=\"0.0.0.0\" --api- \n servers=http://localhost:8080 --config=/etc/kubernetes/manifests \n",
      "page_number": 31
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 44-52)",
      "start_page": 44,
      "end_page": 52,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n26\n The docker run command parameters are discussed in Table  1-4 . \n Table 1-4.  The docker run Command Parameters to start etcd \n Parameter \n Description \n --volume=/:/rootfs:ro \\ \n --volume=/sys:/sys:ro \\ \n --volume=/dev:/dev \\ \n --volume=/var/lib/docker/:/var/lib/docker:ro \\ \n --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\ \n --volume=/var/run:/var/run:rw \\ \n The Docker volumes to use \n --net = host \n Connects the Docker container to a network making \nuse of the host container network inside the container \n --pid = host \n Sets the pid namespace \n --privileged = true \n Provides access to most of the capabilities of the host \nmachine in terms of kernel features and host access \n -d \n Starts the container in the background \n gcr.io/google_containers/hyperkube:v1.0.1 \n The container image \n hyperkube kubelet \n--containerized \n--hostname-override = \"127.0.0.1\" \n--address = \"0.0.0.0\" \n--api- \n servers= http://localhost:8080 \n--config=/etc/kubernetes/manifests \n The command run \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n27\n Figure 1-29.  The docker run Command to start Kubernetes Master \n The output from the docker run command to start the master is shown in Figure  1-29 . \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n28\n Starting Service Proxy \n To start the service proxy, which is a proxy for the Kubernetes service providing a pod/s interface using a \nservice selector with labels, start the service proxy by running the following docker run command. \n sudo docker run -d --net=host --privileged gcr.io/google_containers/hyperkube:v1.0.1 \n/hyperkube proxy -- master=http://127.0.0.1:8080 --v=2 \n The command parameters for the preceding command are discussed in Table  1-5 . \n The Master is required to be started each time the Kubernetes cluster manager is to be started. \nThe container image is downloaded only the first time the command is run, and on subsequent runs the \nimage is not downloaded as shown in Figure  1-30 . \n Figure 1-30.  Subsequent starts  of  Kubernetes Master do not need to download Container image again \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n29\n The output from the  preceding  docker run command is shown in Figure  1-31 . \n Table 1-5.  The docker run Command Parameters to start service proxy \n Parameter \n Description \n -d \n Runs the container in the background \n --net = host \n Sets the network for the container to the host’s network \n --privileged \n Provides access to most of the capabilities of the host \nmachine in terms of kernel features and host access \n gcr.io/google_containers/hyperkube:v1.0.1  The container image \n hyperkube proxy -- master=\n http://127.0.0.1:8080 --v = 2 \n The command to run. The master url is set to \n http://127.0.0.1:8080 . \n Figure 1-31.  Starting the Service proxy \n Listing the Kubernetes Docker Containers \n The Docker containers started for a Kubernetes cluster manager may be listed with the following command. \n sudo docker ps \n The Docker containers listed include a container for the service proxy; a container for the kubelet; a \ncontainer for etcd; and containers each for the master scheduler, controller, and apiserver, and pause as \nshown in Figure  1-32 . \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n30\n The Docker container info may be found using the Docker container id. For example, obtain the \ncontainer id for the Docker container running the controller as shown in Figure  1-33 . \n Figure 1-32.  Listing  the  Docker Containers \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n31\n Run the following command to find the detail about the Docker container. \n sudo docker inspect 37971b53f2c1 \n Figure 1-33.  Obtaining  the Docker Container Id \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n32\n The detail such as the master ip and about the Docker container running the controller manager gets \noutput as shown in Figure  1-34 . \n Figure 1-34.  Listing  Docker  Container Information \n Installing kubectl \n The kubectl is used to control the Kubernetes cluster manager including running an image, getting the pods, \ngetting the replication controller, making an application available as a service exposed at a specified port, \nand scaling the cluster. Download Kubectl binaries with the following command. \n sudo wget https://storage.googleapis.com/kubernetes-release/release/v1.0.1/bin/linux/amd64/kubectl \n The kubectl binaries get downloaded as shown in Figure  1-35 . \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n33\n Make the kubectl application executable by applying the + x permissions. \n sudo chmod +x kubectl \n Move the kubectl binaries to the /usr/local/bin/ directory. \n sudo mv kubectl /usr/local/bin/ \n The output from the preceding commands is shown in Figure  1-36 . \n Figure 1-35.  Installing Kubectl \n Figure 1-36.  Moving and making kubectl Binaries executable \n \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n34\n Figure 1-37.  Kubectl Command Usage \n The kubectl command lists the usage as shown in Figure  1-37 . \n \n",
      "page_number": 44
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 53-62)",
      "start_page": 53,
      "end_page": 62,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n35\n Figure 1-39.  Listing the Kubernetes Service \n Figure 1-38.  Command Parameters for Kubect l \n The command parameters also  get  listed as shown in Figure  1-38 . \n Listing Services \n The following command should list the Kubernetes service. \n kubectl get services \n The  kubernetes  service gets listed as shown in Figure  1-39 . \n \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n36\n Listing Nodes \n The following command should list the Kubernetes node. \n kubectl  get  nodes \n The single node in the cluster gets listed as shown in Figure  1-40 . \n Figure 1-41.  Running he nginx Application on Kubernetes Cluster \n Figure 1-40.  Listing the Nodes \n Testing the Kubernetes Installation \n To test the Kubernetes cluster manager, run the nginx application using the following command. \n kubectl -s http://localhost:8080 run nginx --image=nginx --port=80 \n The output from the kubectl run command lists the replication controller, container/s, image/sm \nselector, and replicas as shown in Figure  1-41 . \n Expose the nginx application replication controller as a service with the kubectl expose command. \n kubectl expose rc nginx --port=80 \n The nginx Kubernetes service gets created running on port 80 as shown in Figure  1-42 . \n Figure 1-42.  Creating a Kubernetes Service for nginx Application \n \n \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n37\n List the detail about the nginx service with the kubectl get svc command. \n kubectl get svc nginx \n The nginx service detail gets listed as shown in Figure  1-43 . \n Figure 1-43.  Listing the Kubernetes Service nginx \n The cluster IP may be obtained with the following command. \n kubectl get svc nginx --template={{.spec.clusterIP}} \n The cluster ip is listed as 10.0.0.146 as shown in Figure  1-44 . \n Figure 1-44.  Listing the Cluster IP \n The web server may be called making use of the cluster ip with the following command. \n curl 10.0.0.146 \n \n \n\n\nCHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n38\n Figure 1-45.  Using curl  to invoke Application \n The html output as text gets output as shown in Figure  1-45 . \n Summary \n In this chapter we installed Kubernetes using Docker. An Amazon EC2 instance running Ubuntu is used to \ninstall Docker and Kubernetes. The nginx application is run only to test the installation of the Kubernetes \ncluster manager. The kubectl commands to create an application, replication controller, and service are \ndiscussed in more detail in the next chapter. \n \n\n\n39\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_2\n CHAPTER 2 \n Hello Kubernetes \n Kubernetes is a cluster manager for Linux containers. While Kubernetes supports other types of containers \nsuch as Rocket, and support for more types is to be added, we shall discuss Kubernetes in the context of \nDocker containers only. Docker is an open source container virtualization platform to build, package, and \nrun distributed applications in containers that are lightweight snapshots of the underlying OS. A Docker \nimage, which is application specific, encapsulates all the required software including dependencies for \nan application and is used to create Docker containers to run applications in the containers. The Docker \ncontainers are isolated from each other and have their own networking and filesystem and provide \nContainer as a Service (CaaS). Docker is similar to virtual machines based on virtualization platforms such \nas Oracle VirtualBox and VMWare Player in that it is a virtualization over the underlying OS, but is different \nin that while a virtual machine makes use of an entire operating system, multiple Docker containers share \nthe kernel and run in isolation on the host OS. Docker containers run on the Docker Engine, which runs on \nthe underlying OS kernel. \n In this chapter we shall introduce Kubernetes concepts using a Hello-World application. This chapter \nhas the following sections.\n Overview \n Why Kubernetes \n Setting the Environment \n Creating an Application Imperatively \n Creating an Application Declaratively \n Using JSON for the Resource Definitions \n Overview \n Kubernetes concepts include Pod, Service, and Replication controller and are defined in the following \nsubsections. \n What Is a Node? \n A  node is  a  machine (physical or virtual) running Kubernetes onto which Pods may be scheduled. The node \ncould be the  master node or one of the  worker nodes . In the preceding chapter on installing Kubernetes only \na single node was used. In a later chapter, Chapter  14 , we shall discuss creating a multi-node cluster with a \nmaster and worker node/s. \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n40\n What Is a Cluster? \n A  cluster is a collection of nodes including other resources such as storage to run Kubernetes applications. \nA cluster has a single Kubernetes master node and zero or more worker nodes. A highly available cluster \nconsists of multiple masters or master nodes. \n What Is a Pod? \n A  Pod is a  collection of containers that are collocated and form an atomic unit. Multiple applications may \nbe run within a Pod and though the different containers within a Pod could be for the same application, \ntypically the different containers are for different applications. A Pod is a higher level abstraction for \nmanaging a group of containers with shared volumes and network namespace. All the applications \n(containers) in a Pod share the same filesystem and IP address with the port on which each application is \nexposed being different. Applications running in a Pod may access each other at “localhost”. Scheduling \nand replication are performed at the Pod level rather than at the individual container level. For example \nif a Pod defines two containers for different applications and replication level is set at 1, a single replica \nof the Pod consists of two containers, one each for the two applications. Pods facilitate resource sharing \nand communication what would otherwise be implemented using --link in individually running Docker \ncontainers. A Pod consisting of multiple containers would typically be used for tightly coupled applications. \nFor example, if an  nginx application makes use of MySQL database, the two applications are able to interact \nby Kubernetes running containers for each in the same Pod. \n What Is a Service? \n A  Service is the  external  interface for one or more Pods providing endpoint/s at which the application/s \nrepresented by the Service may be invoked. A Service is hosted at a single IP address but provides zero or \nmore endpoints depending on the application/s interfaced by the Service. Services are connected to Pods \nusing label selectors. Pods have label/s on them and a Service with a selector expression the same as a Pod \nlabel represents the Pod to an external client. An external client does not know or need to know about the \nPods represented by a Service. An external client only needs to know the name of the Service and the port at \nwhich a particular application is exposed. The Service routes requests for an application based on a round-\nrobin manner to one of the Pods selected using a label selector/. Thus, a Service is a high level abstraction \nfor a collection of applications leaving the detail of which Pod to route a request to up to the Service. \nA Service could also be used for load balancing. \n What Is a Replication Controller? \n A  Replication Controller manages the replication level of Pods as specified by the “replicas” setting in a \nReplication Controller definition or on the command line with the  –replicas parameter.  A  Replication \nController ensures that the configured level of Pod replicas are running at any given time. If a replica fails or \nis stopped deliberately a new replica is started automatically. A Replication Controller is used for scaling the \nPods within a cluster. A replica is defined at the Pod level implying that if a Pod consists of two containers a \ngroup of the two configured containers constitute a replica. \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n41\n What Is a Label? \n A  Label is a key-value  pair  identifying a resource such as a Pod, Service, or Replication Controller: most \ncommonly a Pod. Labels are used to identify a group or subset of resources for tasks such as assigning them \nto a Service. Services use label selectors to select the Pods they manage. For example, if a Pod is labeled \n“app = helloApp” and a Service “selector” is set as “app = helloApp” the Pod is represented by the Service. \nService selectors are based on labels and not on the type of application they manage. For example, a Service \ncould be representing a Pod running a hello-world application container with a specific label. Another \nPod also running a hello-world container but with a label different than the Service selector expression \nwould not be represented by the Service. And a third Pod running an application that is not a hello-world \napplication but has the same label as the Service selector would also be represented by the same Service. \n What Is a Selector? \n A  selector is a key-value expression to identify resources using matching labels. As discussed in the \npreceding subsection a Service selector expression “app = helloApp” would select all Pods with the label \n“app = helloApp”. While typically a Service defines a selector to select Pods a Service could be defined to not \ninclude a selector and be defined to abstract other kinds of back ends. Two kinds of selectors are supported: \nequality-based and set-based. A selector could be made of multiple requirements implying that multiple \nexpressions (equality-based or set-based) separated by ',' could be specified. All of the requirements must \nbe met by a matching resource such as a Pod for the resource to be selected. A resource such as a Pod could \nhave additional labels, but the ones in the selector must be specified for the resource to be selected. The \nequality-based selector, which is more commonly used and also the one used in the book, supports =,!=,== \noperators, the = being synonymous to ==. \n What Is a Name? \n A  name is identifies a resource. A name is not the same as a label. For matching resources with a Service a \nlabel is used and not a name. \n What Is a Namespace? \n A  namespace is a level above the name to demarcate a group of resources for a project or team to prevent \nname collisions. Resources within different namespaces could have the same name, but resources within a \nnamespace have different names. \n What Is a Volume? \n A  volume is a directory within the filesystem of a container. A volume could be used to store data. Kubernetes \nvolumes evolve from Docker volumes. \n Why Kubernetes? \n Docker containers introduced a new level of modularity and fluidity for applications with the provision \nto package applications including dependencies, and transfer and run the applications across different \nenvironments. But with the use of Docker containers in production, practical problems became apparent \nsuch as which container to run on which node (scheduling), how to increase/decrease the number of \nrunning containers for an application (scaling), and how to communicate within containers. Kubernetes \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n42\nwas designed to overcome all these and other practical issues of container cluster management. Kubernetes \nprovides dynamic container cluster orchestration in real time.  Kubernetes as a  cluster manager provides the \nfollowing benefits.\n -Microservices by breaking an application into smaller, manageable, scalable \ncomponents that could be used by groups with different requirements. \n -Fault-tolerant cluster in which if a single Pod replica fails (due to node failure, \nfor example), another is started automatically. \n -Horizontal scaling in which additional or fewer replicas of a Pod could be run \nby just modifying the “replicas” setting in the Replication Controller or using the \n –replicas parameter in the  kubectl scale command. \n -Higher resource utilization and efficiency. \n -Separation of concerns. The Service development team does not need to \ninterface with the cluster infrastructure team . \n Setting the Environment \n The following software is required for this chapter.\n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n We have used an Amazon EC2 Linux instance created from AMI Ubuntu Server 14.04 LTS (HVM), SSD \nVolume Type - ami-d05e75b8. \n SSH Login to the Ubuntu  interface  (Public IP address would be different for different users and multiple \nIP Addresses may have been used in this chapter). \n ssh -i \"docker.pem\" ubuntu@54.152.82.142 \n Install Docker as discussed in Chapter  1 and start the Docker Engine and verify its status using the \nfollowing commands. \n sudo service docker start \n sudo service docker status \n Install kubectl and start the Kubernetes cluster manager as discussed in Chapter  1 . Output the \nKubernetes cluster information using the following command. \n kubectl cluster-info \n The Kubernetes Master is shown running on  http://localhost:8080 in Figure  2-1 . \n Figure 2-1.  Getting Cluster Info \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n43\n In the following sections we shall run a  hello-world application using the Kubernetes cluster manager. \nAn application may be run imperatively using the  kubectl tool on the command line or declaratively using \ndefinition files for a Pod, Replication Controller, and Service. We shall discuss each of these methods. The \nkubectl tool is used throughout this chapter and in subsequent chapters and a complete command reference \nis available at  https://cloud.google.com/container-engine/docs/kubectl/ . \n Creating an Application Imperatively \n With the Kubernetes master running on  http://localhost:8080 , as obtained in the preceding section, run \nthe following  kubectl run command to run a  hello-world application using the image  tutum/hello-world . \nThe  –s option specifies the Kubernetes API server host and port. The  –image command parameter specifies \nthe Docker image to run as  tutum/hello-world . The  –replicas parameter specifies the number of replicas \nto create as 1. A Replication Controller is created even if the  –replicas parameter is not specified. \nThe default number of replicas is 1. The  –port parameter specifies the container port the application is \nhosted at as 80. \n kubectl -s http://localhost:8080 run hello-world --image=tutum/hello-world --replicas=1 --port=80 \n A new application  container  called  hello-world gets created as shown in Figure  2-2 . A Replication \nController called “hello-world” also gets created. The Pod is created implicitly and label “run = hello-world” \nis added to the Pod. The number of replicas created is 1. The Replication Controller’s selector field is also set \nto “run=hello-world”. The Pods managed by a Replication Controller must specify a label that is the same as \nthe selector specified at the Replication Controller level. By default a Replication Controller selector is set to \nthe same expression as the Pod label. \n Figure 2-2.  Creating an Application including a Replication Controller and Pod Replica/s \n The Replication Controller created may be listed with the following command. \n kubectl get rc \n The  hello-world Replication Controller gets listed as shown in Figure  2-3 . \n Figure 2-3.  Listing the Replication Controllers \n \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n44\n The Pods created and started are listed with the following command. \n kubectl get pods \n The single Pod created gets listed as shown in Figure  2-4 . A Pod name is assigned automatically. A Pod \nSTATUS “Running” is listed, but the Pod may still not be ready and available. The READY column value of \n0/1 indicates that 0 of 1 containers in the Pod are ready, which implies that the Pod has been created and is \nrunning but not yet ready. It could take a few seconds for a Pod to become Ready. \n Figure 2-4.  Listing the Pods \n Figure 2-5.  Listing a Pod as ready with all containers in the Pod as ready \n Run the same command again after a few seconds or a minute. \n kubectl get pods \n The Pod gets listed as ready as indicated by 1/1 in the READY column in Figure  2-5 . A value of 1/1 in \nthe READY column indicates that 1 of 1 containers in the Pod are ready. The syntax for the READY column \nvalue is  nReady/nTotal , which implies that  nReady of the total  nTotal containers in the Pod are ready. The \nKubernetes Pod  k8s-master-127.0.0.1 , for example, has a READY column value of 3/3, which implies that \n3 of 3 containers in the Kubernetes Pod are ready. \n Running a Pod and a Replication Controller does not implicitly create a Service. In the next subsection \nwe shall create a Service for the  hello-world application. \n Creating a Service \n Create a Kubernetes Service using the  kubectl expose command, which creates a Service from a Pod, \nReplication Controller, or another Service. As we created a Replication Controller called  hello-world , create \na Service using the following command in which the port to expose the Service is set to 8080 and the Service \ntype is  LoadBalancer . \n kubectl expose rc hello-world --port=8080 --type=LoadBalancer \n \n \n",
      "page_number": 53
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 63-70)",
      "start_page": 63,
      "end_page": 70,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n45\n A Kubernetes Service  called  hello-world gets created as shown in Figure  2-6 . The Service labels and \nselector also get set. The Service selector, listed in Figure  2-6 , is set to the same expression  run=hello-world \nas the Replication Controller selector, which is shown in Figure  2-3 , which implies that the Service manages \nthe Pods in the Replication Controller  hello-world . \n Figure 2-6.  Creating a Kubernetes Service \n The different types of Services are ClusterIp, NodePort, and LoadBalancer with the default being \nClusterIP, as discussed in Table  2-1 . \n Table 2-1.  Types of Services \n Service Type \n Description \n ClusterIp \n Uses a cluster-internal IP only. \n NodePort \n In addition to a cluster IP exposes the Service on each node of the cluster. \n LoadBalancer \n In addition to exposing the Service on a cluster internal Ip and a port on each \nnode on the cluster, requests the cloud provider to provide a load balancer for \nthe Service. The load balancer balances the load between the Pods in the Service. \n List all the Kubernetes Services with the following command. \n kubectl get services \n In addition to the “kubernetes” Service for the Kubernetes cluster manager a “hello-world” Service gets \ncreated as shown in Figure  2-7 . \n Figure 2-7.  Listing the Services \n \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n46\n Describing a Pod \n Using the Pod name  hello-world-syrqz obtained from the NAME column in the result for the  kubectl get \npods command use the  kubectl describe pod command to list detailed information about the Pod. \n kubectl describe pod hello-world-syrqz \n Detailed information about the  Pod  including the IP address gets listed as shown in Figure  2-8 . The Pod \nhas a Label run=hello-world, which is the same as the replication controller  selector and also same as the \nservice  selector , which implies that the replication controller manages the Pod when scaling the cluster of \nPods for example, and the service represents the Pod to external clients. \n Figure 2-8.  Describing a Pod \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n47\n Next, we shall invoke the application using the IP Address  172.0.17.2 listed in the IP field. \n Invoking the Hello-World Application \n The  hello-world application may be invoked using the IP for the application as listed in Figure  2-8 with the \nfollowing  curl command. \n curl 172.17.0.2 \n The HTML output from the application is shown in Figure  2-9 . \n Figure 2-9.  Invoking a Application using Pod IP with curl \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n48\n To display the HTML output in a browser we need to invoke the application from a browser using URL \n 172.17.0.2:80 . If a browser is not available on the Amazon EC2 Ubuntu instance, as it is not by default, \nwe need to set up a SSH tunnel to the IP Address of the application using local port forwarding. Obtain the \nPublic DNS for the Amazon EC2 instance (ec2-52-91-200-41.compute-1.amazonaws.com in the example) \nand run the following command to set up a SSH tunnel to the  172.17.0.2:80 host:port from a local \nmachine. The  –L indicates that local port forwarding is used to forward local port 80 to  172.17.0.2:80 . \n ssh -i \"docker.pem\" -f -nNT -L 80:172.17.0.2:80 ubuntu@ec2-52-91-200-41.compute-1.amazonaws.com \n Invoke the URL  http://localhost in a browser on the local machine. The HTML output from the \n hello-world application gets displayed as shown in Figure  2-10 . The hostname is listed the same as the Pod \nname in Figure  2-5 . \n Figure 2-10.  Invoking  the  Hello-World Application in a Browser \n Scaling the Application \n A Replication Controller was created by default when we created the  hello-world application with replicas \nset as 1. Next, we shall scale up the number of Pods to 4. The  kubectl scale command is used to scale a \nReplication Controller. Run the following command to scale up the Replication Controller  hello-world to 4. \n kubectl scale rc hello-world --replicas=4 \n Subsequently, list the Pods using the following command. \n kubectl get pods \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n49\n The additional Pods get listed but some of the new Pods could be listed in various states such as \nrunning but not ready, or image ready and container creating as shown in Figure  2-11 . \n Figure 2-11.  Scaling the Cluster of Pods with the Replication Controller \n After a few seconds run the same command again to list the Pods. \n kubectl get pods \n If the Pods have started all the Pods are listed with STATUS- > Running and READY state 1/1 as shown in \nFigure  2-12 . Scaling to 4 replicas does not create 4 new Pods, but the total number of Pods is scaled to 4 and \nthe single Pod created initially is included in the new scaled replicas of 4. \n Figure 2-12.  Listing all the Pods as Running and Ready \n \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n50\n Describe the  hello-world Service using the following command. \n kubectl describe svc hello-world \n The Service name, label/s, selector, type, IP, and Endpoints get listed as shown in Figure  2-13 . \nThe Service may be invoked using the Endpoints for the various Pod replicas. \n Figure 2-13.  Describing the Service hello-world \n As discussed previously, set up SSH tunneling with port forwarding for the newly added endpoints. The \nfollowing command sets up a SSH tunnel with port forwarding from  localhost port 8081 to  172.17.0.3:80 \non the Amazon EC2 instance. \n ssh -i \"docker.pem\" -f -nNT -L 8081:172.17.0.3:80 ubuntu@ec2-52-91-200-41.compute-1.\namazonaws.com \n Subsequently invoke the  hello-world application in a browser on a local machine with url \n http://localhost:8081 to display the application output as shown in Figure  2-14 . \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n51\n Similarly the following command from a local machine sets up a SSH tunnel with port forwarding from \n localhost port 8082 to  172.17.0.4:80 on the Amazon EC2 instance. \n ssh -i \"docker.pem\" -f -nNT -L 8082:172.17.0.4:80 ubuntu@ec2-52-91-200-41.compute-1.\namazonaws.com \n Subsequently invoke the  hello-world application using url  http://localhost:8082 to display the \napplication output as shown in Figure  2-15 . \n Figure 2-14.  Invoking an Application in a Local Browser \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n52\n Deleting a Replication Controller \n The Replication Controller  hello-world may be deleted with the following command. \n kubectl delete rc hello-world \n The Replication Controller gets deleted as shown in Figure  2-16 . Subsequently invoke the following \ncommand to list the Replication Controllers. \n kubectl get rc \n The  hello-world Replication Controller does not get listed as shown in Figure  2-16 . \n Figure 2-16.  Deleting a Replication Controller \n Figure 2-15.  Invoking the  second  Service Endpoint in a Local Browser \n \n \n",
      "page_number": 63
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 71-87)",
      "start_page": 71,
      "end_page": 87,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n53\n Deleting a Replication Controller deletes the Replication Controller and the Pods associated with the \nReplication Controller but does not delete the Service representing the Replication Controller. The  kubectl \nget services command still lists the Service as shown in Figure  2-17 . \n Figure 2-17.  Deleting a  Replication  Controller does not delete the Service \n Figure 2-18.  Deleting the hello-world Service \n Deleting a Service \n To delete the  Service  hello-world run the following command. \n kubectl delete svc hello-world \n Subsequently invoke the following command to list the Services. \n kubectl get services \n The output from the preceding two commands is shown in Figure  2-18 and does not list the  hello-\nworld Service. \n Creating an Application Declaratively \n Next, we shall create the same hello-world application declaratively using definition files for a Pod, Service, \nand Replication Controller. The definition files may be configured in YAML or JSON. We have used YAML \ninitially and also discussed the JSON alternative later. \n \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n54\n Creating a Pod Definition \n Create a  hello-world.yaml file and specify a definition for a Pod in the file. For the  hello-world application \nthe following definition is used in which the  apiVersion mapping is for the API schema version ( v1 ),  kind \nmapping is the resource and set to  Pod . The metadata mapping specifies the Pod’s metadata and sets the \nname to  hello-world (arbitrary). The  spec mapping specifies the Pod behavior. The  spec - >  containers \nmapping specifies a collection of images to run. The  hello-world.yaml specifies a single container for \nimage  tutum/hello-world . Container name is set to  hello-world and container  ports mapping is a list of \nports with a single  containerPort mapping for 8080 port. \n apiVersion: v1 \n kind: Pod \n metadata: \n name: hello-world \n spec: \n  containers: \n    - \n      image: tutum/hello-world \n      name: hello-world \n      ports:         \n        -containerPort: 8080 \n The preceding is equivalent to the following command. \n kubectl run hello-world --image=tutum/hello-world --port=8080 \n Only a few of the schema elements have been used in the  hello-world.yaml . For the complete Pod \nschema refer  http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_pod . \n Next, create the  hello-world application using the  hello-world.yaml definition file with the following \n kubectl create command. The  –validate option validates the Pod definition file. A YAML lint validator \n( http://www.yamllint.com/ ) may be used to validate the YAML syntax in the  hello-world.yaml . The \nsyntax validation does not validate if the definition file conforms to the Pod schema. \n kubectl create -f hello-world.yaml --validate \n A Pod called  hello-world gets created as shown in Figure  2-19 . \n Figure 2-19.  Creating a Pod using a Definition File \n List the Pods with the following command, which is the same regardless of how a Pod has been created. \n kubectl get pods \n The  hello-world Pod gets listed as shown in Figure  2-20 . Initially, the Pod may not be READY- > 1/1. \nA READY column value of “0/1” implies that 0 of 1 containers in the Pod are ready. \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n55\n Run the same command again after a few more seconds. \n kubectl get pods \n The  hello-world Pod gets listed with STATUS as “Running” and READY state as “1/1,” which implies \nthat 1 of 1 containers in the Pod are ready, as shown in Figure  2-21 . \n Figure 2-20.  Listing the Pods soon after creating the Pods \n Figure 2-21.  Listing the Pod as Ready and Running \n Describe the  hello-world Pod with the following command. \n kubectl describe pod hello-world \n \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n56\n The output from the preceding command is shown in Figure  2-22 . \n Figure 2-22.  Describing the hello-world Pod \n Invoke the  hello-world Pod application using the IP  172.17.0.2 . \n curl 172.17.0.2 \n The HTML output from the  hello-world application gets listed as shown in Figure  2-23 . \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n57\n Set up port forwarding from a local machine to the IP address of the  hello-world Pod. \n ssh -i \"docker.pem\" -f -nNT -L 80:172.17.0.2:80 ubuntu@ec2-52-91-200-41.compute-1.amazonaws.com \n Subsequently invoke the url  http://localhost:80 in a browser on a local machine to display the \nHTML output from the application as shown in Figure  2-24 . The default Hypertext transfer protocol port \nbeing 80, has been be omitted from the URL, as shown in Figure  2-24 . \n Figure 2-23.  Invoking the hello-world Application with curl \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n58\n Creating a Service Definition \n We created a Pod definition file and started a single Pod, but the Pod is not associated with any Service \nor Replication Controller. External clients have to access the Pod directly and are not able to scale the \napplication with just a single unassociated Pod. Create a Service definition file  hello-world-service.yaml \nas listed below. If copying and pasting YAML files listed in this chapter and other chapters it is \nrecommended to use the YAML Lint ( http://www.yamllint.com/ ) to format the files before using in an \napplication. \n apiVersion: v1 \n kind: Service \n metadata: \n  labels: \n    app: hello-world \n  name: hello-world \n spec: \n  ports: \n    - \n      name: http \n      port: 80 \n      targetPort: http \n  selector: \n    app: hello-world \n  type: LoadBalancer \n Figure 2-24.  Invoking the hello-world Application in  a  Browser on a local machine \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n59\n The main mappings of the Service definition file are  kind ,  metadata , and  spec . The  kind is set to \n Service to indicate a Kubernetes Service. The label  app and the  name constitute the metadata. The  spec \nmapping includes a  ports mapping for port 80 with name  http . Optionally a  targetPort may be set, which \ndefaults to the same value as port. The  selector is the main mapping in the  spec and specifies a mapping to \nbe used for selecting the Pods to expose via the Service. The  app:hello-world selector implies that all Pods \nwith label  app=hello-world are selected. The definition file may be created in the vi editor and saved with \nthe  :wq command as shown in Figure  2-25 . \n Figure 2-25.  Service Definition File hello-world-service.yaml \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n60\n A complete reference to the Kubernetes Service schema is available at  http://kubernetes.io/v1.1/\ndocs/api-reference/v1/definitions.html#_v1_service . \n Create a Service using the definition file with the  kubectl create command. \n kubectl create -f hello-world-service.yaml \n The  hello-world Service gets created as shown in Figure  2-26 . \n Figure 2-26.  Creating the hello-world Service using the Definition File \n Figure 2-27.  Listing the hello-world Service \n List the Services with the following command. \n kubectl get services \n The  hello-world Service gets listed in addition to the  kubernetes Service as shown in Figure  2-27 . \n Describe the  hello-world Service with the following command. \n kubectl describe svc hello-world \n The Service name, namespace, labels, selector, type, Ip get listed as shown in Figure  2-28 . Because the \n hello-world Pod created using the Pod definition file does not include a label to match the Service selector, \nit is not managed by the Service. As the  hello-world Service is not managing any Pods, no endpoint gets \nlisted. \n \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n61\n Creating a Replication Controller Definition \n Next, we shall create a Replication Controller and label the Replication Controller to match the selector \nof the Service created previously. Create a Service definition file  hello-rc.yaml . The kind mapping of \na Replication Controller is  ReplicationController . The  replicas ’ sub-mapping in the  spec mapping \nis set to 2 to create two replicas from the Pod also specified in the  spec . At least one of the labels in the \ntemplate- > metadata- > labels must match the Service selector in the Service definition file for the Pod \nto be exposed by the Service. As the Service selector in the  hello-world Service is  app:hello-world add \nthe  app:hello-world label to the Replication Controller template. The app:hello-world setting in YAML \ntranslates to app=hello-world. The template may define one or more containers to be included in the Pod \ncreated from the Replication Controller. We have included container definition for only one container for \nimage  tutum/hello-world . The  hello-rc.yaml is listed below. A YAML lint ( http://www.yamllint.com/ ) \nmay be used to validate the YAML syntax. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  name: hello-world \n spec: \n  replicas: 2 \n  template: \n    metadata: \n      labels: \n        app: hello-world \n Figure 2-28.  Describing  the  hello-world Service \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n62\n    spec: \n      containers: \n        - \n          image: tutum/hello-world \n          name: hello-world \n          ports: \n            - \n              containerPort: 8080 \n              name: http \n A complete schema for the Replication Controller is available at  http://kubernetes.io/v1.1/docs/\napi-reference/v1/definitions.html#_v1_replicationcontroller . \n Create the Replication Controller using the definition file with the  kubectl create command, the same \ncommand that was used to create a Pod and a Service. \n kubectl create -f hello-rc.yaml \n Subsequently run the following command to list the Replication Controllers. \n kubectl get rc \n A  hello-world Replication Controller gets created and gets listed as shown in Figure  2-29 . The number \nof replicas are listed as 2 as specified in the definition file. \n Figure 2-29.  Creating a Replication Controller \n List the Pods created with the Replication Controller with the following command. \n kubectl get pods \n The two Pods created from the definition file get listed as shown in Figure  2-30 . The Pod created the Pod \ndefinition file also gets listed but is not associated with the Replication Controller. Initially some or all of the \nnew Pods may be listed as not ready as indicated by the 0/1 value in the READY column for one of the Pods \nin Figure  2-30 . \n Figure 2-30.  Listing the Pods soon after creating a Replication Controller \n \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n63\n Invoke the same command again to list the Pods after a few more seconds. \n kubectl get pods \n All the Pods get listed as READY- > 1/1 and Running as shown in Figure  2-31 . \n Figure 2-32.  Describing the Service hello-world \n Figure 2-31.  Listing all the Pods as Running and Ready \n To describe the  hello-world Service run the following command. \n kubectl describe service hello-world \n The Service detail including the Endpoints get listed as shown in Figure  2-32 . The service selector is \napp = hello-world and the service endpoints are 172.17.0.3:8080 and 172.17.0.4:8080. \n All the preceding commands to create the  hello-world Replication Controller, list its Pods and \nendpoints association with the  hello-world Service shown in Figure  2-33 . \n \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n64\n Invoking the Hello-World Application \n The Pods associated with the hello-world Replication Controller and Service by the same name may be \ninvoked using the Service endpoints as listed in the Service description in Figure  2-33 . For example, invoke \nthe  172.17.0.3 endpoint with the following  curl command. \n curl 172.17.0.3 \n The HTML output from the Pod gets output as shown in Figure  2-34 . \n Figure 2-33.  Summary of Commands  to create a Replication Controller \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n65\n Similarly, invoke the 172.17.0.4 endpoint with the following curl command. \n curl 172.17.0.4 \n The HTML output from the other Pod gets output as shown in Figure  2-35 . \n Figure 2-34.  HTML Output from invoking the hello-world Application with curl \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n66\n Figure 2-35.  Invoking another Service Endpoint with curl \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n67\n To invoke the Service endpoints in a browser on a local machine configure local port forwarding for the \nService endpoints. \n ssh -i \"docker.pem\" -f -nNT -L 8081:172.17.0.3:8080 ubuntu@ec2-52-91-200-41.compute-1.\namazonaws.com \n ssh -i \"docker.pem\" -f -nNT -L 8082:172.17.0.4:8080 ubuntu@ec2-52-91-200-41.compute-1.\namazonaws.com \n Subsequently invoke the  localhost:8081 URL in a browser on a local machine as shown in Figure  2-36 \nto display the HTML output from the Pod at endpoint  172.17.0.3:8080 . \n Figure 2-36.  Invoking the hello-world Application in a Local machine Browser with its Service Endpoint \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n68\n Similarly invoke the  localhost:8082 URL in a browser on a local machine as shown in Figure  2-37 to \ndisplay the HTML output from the Pod at endpoint  172.17.0.4:8080 . \n Figure 2-37.  Invoking  another  Service Endpoint in a Browser \n Scaling the Application \n To scale the  hello-world Replication Controller to 6 replicas, for example, run the following  kubectl scale \ncommand. \n kubectl scale rc hello-world --replicas=6 \n An output of “scaled” as shown in Figure  2-38 indicates the Replication Controller has been scaled. \n Figure 2-38.  Scaling an Application \n The number of Pods for the hello-world Replication Controller increases when the Replication \nController is scaled up to 6. To list the Pods run the following command. \n kubectl get pods \n \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n69\n Six Pods get listed in addition to the  hello-world Pod created initially using a Pod definition file as \nshown in Figure  2-39 . The preceding command may have to be run more than once to list all the Pods with \nSTATUS as Running and READY state as 1/1. The  hello-world Pod is not associated with the  hello-world \nReplication Controller as it does not include a label that matches the selector label (same as template label) \nin the Replication Controller. \n Figure 2-39.  Listing Pods after Scaling \n In the preceding example we scaled  up the Replication Controller, but the  kubectl scale command \nmay also be used to scale  down the Replication Controller. As an example, scale down the  hello-world \nReplication Controller to 2 replicas. \n kubectl scale rc hello-world --replicas=2 \n Subsequently list the Pods. \n kubectl get pods \n The number of replicas gets listed as 2 in addition to the  hello-world Pod as shown in Figure  2-40 . \n \n",
      "page_number": 71
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 88-95)",
      "start_page": 88,
      "end_page": 95,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n70\n Using JSON for the Resource Definitions \n In the preceding section we used the YAML format to create the Pod, Service, and Replication \nController definition files.  The  definition files may be developed in JSON format instead. The YAMLToJSON \nutility ( http://yamltojson.com/ ) may be used to convert from YAML to JSON and the JSON lint \n( http://jsonlint.com/ ) may be used to validate the JSON. A JSON to YAML utility is also available at \n http://jsontoyaml.com/ . The  JSON  definition file  hello-world-service.json for the  hello-world \nService is listed: \n { \n  \"apiVersion\": \"v1\", \n  \"kind\": \"Service\", \n  \"metadata\": { \n    \"name\": \"hello-world\", \n    \"labels\": { \n      \"app\": \"hello-world\" \n    } \n  }, \n  \"spec\": { \n    \"ports\": [ \n      { \n        \"name\": \"http\", \n        \"port\": 80, \n        \"targetPort\": \"http\" \n      } \n    ], \n    \"selector\": { \n      \"app\": \"hello-world\" \n    }, \n    \"type\": \"LoadBalancer\" \n  } \n } \n Create a  hello-world-service.json file using a vi editor and copy and paste the preceding listing to \nthe file. Save the file using :wq as shown in Figure  2-41 . \n Figure 2-40.  Scaling Down to 2 Replicas \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n71\n Delete the  hello-world Service and  hello-world Replication Controller created previously. Run the \nfollowing command to create a Service from the JSON format definition file. \n kubectl create –f hello-world-service.json \n The  hello-world Service gets created as shown in Figure  2-42 . \n Figure 2-41.  Service  Definition File in JSON Format \n Figure 2-42.  Creating a Service from the JSON Definition File \n \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n72\n Subsequently list all the Kubernetes Services. \n kubectl get services \n The  hello-world Service gets listed as shown in Figure  2-43 . \n Figure 2-43.  Listing the  Service s \n The JSON format version of  the  Replication Controller definition file,  hello-rc.json, is as follows. \n { \n  \"apiVersion\": \"v1\", \n  \"kind\": \"ReplicationController\", \n  \"metadata\": { \n    \"name\": \"hello-world\" \n  }, \n  \"spec\": { \n    \"replicas\": 2, \n    \"template\": { \n      \"metadata\": { \n        \"labels\": { \n          \"app\": \"hello-world\" \n        } \n      }, \n      \"spec\": { \n        \"containers\": [ \n          { \n            \"image\": \"tutum/hello-world\", \n            \"name\": \"hello-world\", \n            \"ports\": [ \n              { \n                \"containerPort\": 8080, \n                \"name\": \"http\" \n              } \n            ] \n          } \n        ] \n      } \n    } \n  } \n } \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n73\n Create  the  hello-rc.json file in a vi editor and save the file with :wq as shown in Figure  2-44 . \n Figure 2-44.  Creating  the  hello-rc.json File in vi Editor \n Delete all previously created Pods and Replication Controllers. Run the following command to create \nthe  hello-world Replication Controller. \n kubectl create –f hello-rc.json \n The  hello-world Replication Controller gets created as shown in Figure  2-45 . Subsequently run the \nfollowing command to list the Replication Controllers. \n kubectl get rc \n The  hello-world Replication Controller gets listed as shown in Figure  2-45 . List the Pods created by the \nReplication Controller using the following command. \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n74\n kubectl get pods \n Because  replicas is set as 2 two  Pods get listed as shown in Figure  2-45 . \n Figure 2-46.  Describing the hello-world Service \n Figure 2-45.  Creating a  Replication  Controller from the JSON format Definition File \n Describe the  hello-world Service with the following command. \n kubectl describe svc hello-world \n Because the label on the  hello-world Replication Controller matches the Service selector, the two Pods \ncreated using the Replication Controller are represented by the Service and have endpoints in the Service as \nshown in Figure  2-46 . \n \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n75\n Invoke a Service endpoint using a curl command as follows. \n curl 172.17.0.2 \n The HTML output from  the  curl command gets output as shown in Figure  2-47 . \n Figure 2-47.  Invoking the hello- world Application with curl \n \n\n\nCHAPTER 2 ■ HELLO KUBERNETES\n76\n Set up local port forwarding to a Service endpoint. \n ssh -i \"docker.pem\" -f -nNT -L 80:172.17.0.2:8080 ubuntu@ec2-52-91-200-41.compute-1.\namazonaws.com \n Subsequently invoke the Service endpoint in a browser in a local machine to display  the  HTML output \nas shown in Figure  2-48 . \n Figure 2-48.  Displaying hello- world  Application HTML in a Browser \n Summary \n In this chapter we introduced the Kubernetes concepts such as Pod, Service, Replication Controller, Labels, \nand Selector. We also developed a hello-world application both imperatively on the command line, and \ndeclaratively using definition files. We discussed two different supported formats for the definition files: \nYAML and JSON. In the next chapter we shall discuss using environment variables in Pod definitions. \n \n\n\n77\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_3\n CHAPTER 3 \n Using Custom Commands and \nEnvironment Variables \n Kubernetes orchestrates Docker containers, and the instructions to run for a Docker image are specified in \nthe  Dockerfile . The  ENTRYPOINT instruction specifies the command to run, and the  CMD instruction specifies \nthe default arguments for the  ENTRYPOINT command. Kubernetes provides two fields,  \"Command\" and  \"Args\" , \nto be specified for a container image in a Pod definition to override the default settings of  ENTRYPOINT and \n CMD . We shall discuss these fields in this chapter. We shall also discuss using environment variables in a Pod \ndefinition’s container mapping with the  \"env\" field mapping. \n This chapter has the following sections.\n Setting the Environment \n The ENTRYPOINT and CMD Instructions \n The Command and Args Fields in a Pod Definition \n Environment Variables \n Using the default ENTRYPOINT and CMD from a Docker Image \n Overriding Both the ENTRYPOINT and CMD in a Docker Image \n Specifying both the Executable and the Parameters in the Command Mapping \n Specifying both the Executable and the Parameters in the Args Mapping \n Setting the Environment \n The following  software is used in this chapter.\n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n Install Docker engine, Kubernetes, and Kubectl as discussed in chapter  1 . Start Docker Engine and verify \nits status with the following commands. \n sudo service docker start \n sudo service docker status \n",
      "page_number": 88
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 96-113)",
      "start_page": 96,
      "end_page": 113,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n78\n The output shown in Figure  3-1 indicates that Docker is running. \n The ENTRYPOINT and CMD Instructions \n The  ENTRYPOINT in a Docker image’s  Dockerfile specifies the command to run when the image is run. The \n ENTRYPOINT has two forms discussed in Table  3-1 . A  Dockerfile may have only one  ENTRYPOINT . If multiple \n ENTRYPOINT s are specified, the last  ENTRYPOINT entry is run. \n Figure 3-1.  Starting Docker  and  Verifying Its Status \n Table 3-1.  ENTRYPOINT  Forms \n Form \n Description \n Format \n Exec form \n Runs an executable using the specified parameters. \nThe exec form is the preferred form if environment \nvariable substitution is not used. But if environment \nvariable substitution is used the shell form must \nbe used. The exec form does not perform any \nenvironment variable substitution. \n ENTRYPOINT [“executable“”, \n“param1”, “param2”] \n Shell form \n Runs the command in a shell and prevents any \nCMD or run command-line arguments to be used \nin conjunction with ENTRYPOINT. The shell form \nstarts a shell with /bin/sh -c even though a shell is \nnot invoked explicitly. \n ENTRYPOINT command \nparam1 param2 \n The  CMD instruction specifies the args for the  ENTRYPOINT command in exec form. The  CMD has three \nforms as discussed in Table  3-2 . A Dockerfile may have only one  CMD entry. If multiple  CMD s are specified the \nlast  CMD entry is run.  The  CMD instruction may include an executable. \n \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n79\n If command-line args are provided to  the  docker run command those override the default args in  CMD \ninstruction. The  ENTRYPOINT instruction could also be used in combination with a helper script. Next, we \nshall discuss the two fields, “command” and “args” that could be used to override the  ENTRYPOINT and  CMD \ninstructions in a  Dockerfile respectively. \n The Command and Args Fields in a Pod Definition \n Kubernetes has the provision to override the  ENTRYPOINT (command) and  CMD (args) instructions specified \nin the Dockerfile. Two field mappings in a Pod’s definition file could be used to override the  ENTRYPOINT and \n CMD instructions. These fields are “Command” and “Args,” and they override the Dockerfile “ENTRYPOINT \n“and “CMD” instructions respectively. The overriding applies based on which of these instructions and \nfields are specified. Some examples of overriding are discussed in Table  3-3 .   \n Table 3-2.  CMD Forms \n Form \n Description \n Format \n Exec form \n The exec form specifies the command to \ninvoke and the command parameters in \nJSON array format. The exec form does not \nperform environment variable substitution. \nIf environment variable substitution is to be \nperformed, use the shell form or invoke the \nshell explicitly in the exec form. In JSONs array \nformat, double quotes “” must be used around \nnames. \n CMD [“executable”, “param1”, \n“param2”] \n Default parameters \nto ENTRYPOINT \n Specifies the default args to the ENTRYPOINT \ncommand. Both the ENTRYPOINT and CMD \nmust be specified. Both the ENTRYPOINT \nand CMD must be specified using JSON array \nformats. In JSONs array format, double quotes \n“” must be used around names. \n CMD [“param1”, ”param2”] \n Shell form \n Invokes a  shell  to invoke the specified \ncommand using the parameters. The command \nis invoked as a sub-command of /bin/sh –c. \n CMD command param1 param2 \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n80\n Environment Variables \n A Pod’s schema has the provision to specify environment variables. The environment variables are specified \nas “name” and “value” field mappings as a collection within a  container  definition’s “ env ” mapping. The \nformat for specifying environment variables is as follows. \n spec: \n  containers: \n    - \n    image: \"image name\" \n    name: \"container name \" \n    env: \n      - \n        name: \"env variable 1\" \n        value: \" env variable 1 value\" \n      - \n        name: \"env variable 2\" \n        value: \" env variable 2 value\" \n Table 3-3.  Examples of Overriding ENTRYPOINT  and CMD with Command and Args \n ENTRYPOINT \n CMD \n Command \n Args \n Used \n Example 1 \n yes \n yes \n yes \n yes \n The Command and Args field mappings \nin the Pod definition file override the \nENTRYPOINT and CMD instructions in \nDockerfile. \n Example 2 \n yes \n yes \n no \n no \n The Dockerfile ENTRYPOINT command \nand CMD args are used. \n Example 3 \n yes \n yes \n yes \n no \n Only the command in the Command is \nused and Dockerfile ENTRYPOINT and \nCMD instructions are ignored. \n Example 4 \n yes \n yes \n no \n yes \n The Docker image’s command as \nspecified in the ENTRYPOINT is used \nwith the args specified in the Pod \ndefinition’s Args. The args from the \nDockerfile’s CMD are ignored. \n Example 5 \n no \n yes \n no \n no \n The command and parameters from the \nCMD instruction are run. \n Example 6 \n no \n yes \n yes \n yes \n The Command and Args field mappings \nin the Pod definition file are used. \nThe CMD instruction in Dockerfile is \noverridden. \n Example 7 \n no \n yes \n no \n yes \n The Args field mapping in the Pod \ndefinition file is used. The CMD \ninstruction in Dockerfile is overridden. \n Example 8 \n no \n yes \n yes \n no \n The command in the Command mapping \nis used, and Dockerfile CMD instruction \nis ignored. \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n81\n The environment variables are added to the  docker run command using  –e when the Docker image is \nrun by Kubernetes. The environment variables may also be used in “command” and “args” mappings using \nthe environment variable substitution if a shell is used to run the Docker image command. A shell is invoked \nif one or more of the following is used:\n -The shell form of the ENTRYPOINT or CMD is used \n -The shell is invoked explicitly in the ENTRYPOINT or CMD instruction \n In the following sections we shall use the “ubuntu” Docker image to demonstrate overriding the default \n ENTRYPOINT command and the default  CMD args. We shall start with using the default  ENTRYPOINT and  CMD \ninstructions. \n Using the Default ENTRYPOINT and CMD from a Docker Image \n The  Dockerfile for the Ubuntu image does not provide an  ENTRYPOINT instruction but the  CMD instruction \nis set to  CMD [\"/bin/bash\"] . In the example in this section we shall create a Pod definition that does not \noverride the  ENTRYPOINT or  CMD instruction from the Docker image. Create a Pod definition file as follows \nwith the image as “ubuntu” and some environment variables set. \n apiVersion: v1 \n kind: Pod \n metadata: \n  name: \"hello-world\" \n  labels: \n    app: \"helloApp\" \n spec: \n  restartPolicy: Never \n  containers: \n    - \n      image: \"ubuntu\" \n      name: \"hello\" \n      ports: \n  containerPort: 8020 \n      env: \n        - \n          name: \"MESSAGE1\" \n          value: \"hello\" \n        - \n          name: \"MESSAGE2\" \n          value: \"kubernetes\" \n The  env.yaml file may be created in a vi editor and saved with the :wq command as shown in \nFigure  3-2 . \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n82\n Run the following command to create a Pod from the definition file  env.yaml . \n kubectl create –f env.yaml \n The  hello-world pod gets created as shown in Figure  3-3 . Run the following command to list the pods. \n Figure 3-2.  A Pod definition file env.yaml to demonstrate Environment Variables \n \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n83\n kubectl get pods \n The  hello-world pod gets created but the Docker container created is listed as “creating” as shown \nin Figure  3-3 . \n When the Docker container gets created the  STATUS column value transitions to “Running” and the \n READY column value becomes 1/1, which indicates that 1 of 1 containers in the Pod are ready and which \nis not shown in Figure  3-4 because the  READY state transitions to 0/1 quickly thereafter. After the Pod \ncommand/args have run the Pod terminates and  STATUS becomes  ExitCode:0 as shown in Figure  3-4 . \n Figure 3-3.  Creating and listing a Pod \n Figure 3-4.  After the Command/Args have run, a Pod terminates and the Pod’s Status becomes ExitCode:0 \n Run the following command to list the output from the Pod. \n kubectl logs hello-world \n As the default  CMD [\"/bin/bash\"] in the “Ubuntu” Docker image is just the invocation of the bash shell \nusing  /bin/bash, no output is generated as shown in Figure  3-5 . \n \n \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n84\n Overriding Both the ENTRYPOINT and CMD \n In the second example we shall override both the  ENTRYPOINT and  CMD in a  Dockerfile using  Command \nand  Args mappings in the Pod definition file.  Using in combination  ENTRYPOINT and CMD will help us to \nspecify the default executable for the image and also it will provide the default arguments to that executable. \nEnvironment variable substitution is used for the  MESSAGE1 and  MESSAGE2 environment variables with the \n $(VARIABLE_NAME) syntax. \n command: [\"/bin/echo\"] \n args: [\" $(MESSAGE1)\", \" $(MESSAGE2)\"] \n The  env.yaml Pod definition file is listed: \n apiVersion: v1 \n kind: Pod \n metadata: \n  name: \"hello-world\" \n  labels: \n    app: \"helloApp\" \n spec: \n  restartPolicy: Never \n  containers: \n    - \n      image: \"ubuntu\" \n      name: \"hello\" \n      ports: \n          - \n          containerPort: 8020 \n      env: \n        - \n          name: \"MESSAGE1\" \n          value: \"hello\" \n        - \n          name: \"MESSAGE2\" \n          value: \"kubernetes\" \n      command: [\"/bin/echo\"] \n      args: [\" $(MESSAGE1)\", \" $(MESSAGE2)\"] \n The  env.yaml file may be opened and modified in the vi editor and saved using the :wq command as \nshown in Figure  3-6 . \n Figure 3-5.  No output generated  with  Default CMD [“/bin/bash”] in “ubuntu” Docker Image \n \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n85\n First, we need to delete the  hello-world pod created in the first example with the following command. \n kubectl delete pod hello-world \n The  hello-world pod gets deleted as shown in Figure  3-7 . \n Figure 3-6.  Modifying env.yaml in a vi Editor \n Figure 3-7.  Deleting the hello-world Pod \n \n \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n86\n Run the  kubectl create command to create a Pod from the definition file  env.yaml . \n kubectl create –f env.yaml \n The  hello-world Pod gets created as shown in Figure  3-8 . \n Figure 3-10.  Outputting Message Generated from Environment Variables using Value Substitution \n Figure 3-8.  Creating the hello-world Pod from definition file env.yaml \n Figure 3-9.  Listing the Pods with transitioning STATUS value \n Run the  kubectl get command to list the pods. \n kubectl get pods \n The  hello-world pod gets listed as shown in Figure  3-9 . The Pod transitions quickly from the  STATUS of \n“Running” to  ExitCode:0 as shown in Figure  3-9 . \n Run the following command to list the output from the Pod. \n kubectl logs hello-world \n The message created from environment variables  MESSAGE1 and  MESSAGE2 using substitution gets listed \nas shown in Figure  3-10 . \n \n \n \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n87\n Specifying both the Executable and the Parameters in the \nCommand Mapping \n In the third example, specify that both the executable and the parameters are specified in the Command \nmapping in the Pod definition file. Environment variable substitution is used for the  MESSAGE1 and  MESSAGE2 \nenvironment variables. The shell is not required to be invoked/started explicitly if the environment variable \nsyntax  $(VARIABLE_NAME) is used, which is what we have used. \n command: [\"/bin/echo\", \" $(MESSAGE1)\", \" $(MESSAGE2)\"] \n The  env.yaml Pod definition file is listed: \n apiVersion: v1 \n kind: Pod \n metadata: \n  name: \"hello-world\" \n  labels: \n    app: \"helloApp\" \n spec: \n  restartPolicy: Never \n  containers: \n    - \n      image: \"ubuntu\" \n      name: \"hello\" \n      ports: \n          - \n          containerPort: 8020 \n      env: \n        - \n          name: \"MESSAGE1\" \n          value: \"hello\" \n        - \n          name: \"MESSAGE2\" \n          value: \"kubernetes\" \n      command: [\"/bin/echo\", \" $(MESSAGE1)\", \" $(MESSAGE2)\"] \n The  env.yaml file may be opened and modified in the vi editor and saved using the :wq command as \nshown in Figure  3-11 . \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n88\n Run the  kubectl create command to create a Pod from the definition file  env.yaml . \n kubectl create –f env.yaml \n The  hello-world pod gets created as shown in Figure  3-12 . Run the  kubectl get command to list the pods. \n Figure 3-11.  The Command mapping with both the Command Executable and the Parameters \n \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n89\n kubectl get pods \n The  hello-world pod gets listed though initially the Pod  STATUS is not listed as “Running” as shown \nin Figure  3-12 . The Pod transitions quickly to the  READY value of 1/1 and subsequently 0/1. The 1/1 READY \nvalue is not shown in Figure  3-12 as it transitions quickly to 0/1. After the command has run the Pod \nterminates and the  STATUS becomes  ExitCode:0 as shown in Figure  3-12 . \n Subsequently invoke the following command to list the output generated by the Pod. \n kubectl get logs \n The message created from environment variables  MESSAGE1 and  MESSAGE2 gets listed as shown in \nFigure  3-13 . \n Figure 3-12.  Creating and Listing the Pod with Definition file from Figure  3-11 \n \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n90\n Specifying Both the Executable and the Parameters in the \nArgs Mapping \n In the fourth example, specify both the executable and the parameters in the  Args mapping in the Pod \ndefinition file as a result overriding the  CMD instruction in the  Dockerfile . Environment variable substitution \nis used for the  MESSAGE1 and  MESSAGE2 environment variables with the environment variable syntax \n $(VARIABLE_NAME) . \n args: [\"/bin/echo\", \" $(MESSAGE1)\", \" $(MESSAGE2)\"] \n The  env.yaml Pod definition file is listed: \n apiVersion: v1 \n kind: Pod \n metadata: \n  name: \"hello-world\" \n  labels: \n    app: \"helloApp\" \n spec: \n  restartPolicy: Never \n  containers: \n    - \n      image: \"ubuntu\" \n      name: \"hello\" \n      ports: \n          - \n          containerPort: 8020 \n      env: \n        - \n          name: \"MESSAGE1\" \n          value: \"hello\" \n        - \n          name: \"MESSAGE2\" \n          value: \"kubernetes\" \n      args: [\"/bin/echo\", \" $(MESSAGE1)\", \" $(MESSAGE2)\"] \n The  env.yaml file may be opened and modified in the vi editor and saved using the :wq command as \nshown in Figure  3-14 . \n Figure 3-13.  Message output by  Pod  created in Figure  3-12 \n \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n91\n The  hello-world Pod created from the previous example must be deleted as otherwise the error shown \nin Figure  3-15 gets generated when the  kubectl create command is run. \n Figure 3-14.  The args Mapping in the Pod definition file specifies both the Command Executable and the \nParameters \n Figure 3-15.  Error Generated if hello-world Pod already exists \n \n \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n92\n Run the  kubectl create command to create a Pod from the definition file  env.yaml . \n kubectl create –f env.yaml \n The output from the command is shown in Figure  3-16 . \n Figure 3-16.  Creating a Pod from definition file in Figure  3-14 \n The  hello-world pod gets created as shown in Figure  3-17 . Run the  kubectl get command to list \nthe pods. \n Figure 3-18.  Outputting  the  Message Generated by Pod \n Figure 3-17.  The Pod terminates and its Status transitions to ExitCode:0 after the command has run \n kubectl get pods \n The  hello-world pod gets listed as shown in Figure  3-17 . The Pod transitions quickly to the READY \nvalue of 1/1 and subsequently 0/1. The 1/1 READY value is not shown in Figure  3-17 as it transitions quickly \nto 0/1. After the command has run the Pod terminates and the STATUS becomes ExitCode:0 as shown in \nFigure  3-17 . \n Subsequently invoke the following command to list the output generated by the Pod. \n kubectl get logs \n The message created with environment variables substitution from  MESSAGE1 and  MESSAGE2 gets listed \nas shown in Figure  3-18 . \n \n \n \n\n\nCHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n93\n Summary \n In this chapter we discussed the  ENTRYPOINT and  CMD instructions in a Docker image  Dockerfile : \ninstructions used to run the default command with the default parameters when the image is run in a \nKubernetes Pod. We also discussed the  Command and  Args mappings in a Pod definition file that could be \nused to override the  ENTRYPOINT and  CMD instructions. We discussed various examples of overriding the \ndefault instructions for the “ubuntu” Docker image with “command” and “args” field mappings in a Pod \ndefinition file. We also demonstrated the use of environment variables in a Pod definition file. In the next \nchapter we shall discuss using MySQL Database with Kubernetes. \n\n\n   PART II \n Relational Databases \n  \n\n\n97\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_4\n CHAPTER 4 \n Using MySQL Database \n MySQL database is the most commonly used open source database. The Docker image “mysql” could be \nused to create a Docker container running a MySQL database instance. Running Docker separately for a \nsingle MySQL instance or multiple instances lacks the features of scheduling multiple instances, scaling, and \nproviding a service for external clients. In this chapter we shall discuss how the Kubernetes container cluster \nmanager could be used to overcome all of those deficiencies. \n Setting the Environment \n Creating a Service \n Creating a Replication Controller \n Listing the Pods \n Listing Logs \n Describing the Service \n Starting an Interactive Shell \n Starting the MySQL CLI \n Creating a Database Table \n Exiting the MySQL CLI and Interactive Shell \n Scaling the Replicas \n Deleting the Replication Controller \n Setting the Environment \n The following software is required for this chapter.\n -Docker Engine (latest version) \n -Kubernetes Cluster Manager (version 1.01) \n -Kubectl (version 1.01) \n -Docker image “mysql” (latest version) \n",
      "page_number": 96
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 114-128)",
      "start_page": 114,
      "end_page": 128,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n98\n We have used an Amazon EC2 instance created from AMI Ubuntu Server 14-04 LTS (HVM), SSD \nVolume Type - ami-d05e75b8 to install the required software. The procedure to install Docker, Kubernetes, \nand Kubectl is discussed in chapter  1 . Obtain the Public IP address of the Amazon EC2 instance as shown in \nFigure  4-1 . \n SSH log in to the Ubuntu instance using the Public IP Address, which would be different for different \nusers. \n sh -i \"docker.pem\" ubuntu@52.90.43.0 \n Start the Docker engine and verify its status. \n sudo service docker start \n sudo service docker status \n The Docker Engine should be listed as “running” as shown in Figure  4-2 . \n Figure 4-1.  Obtaining the Public IP Address \n Figure 4-2.  Starting Docker  and Verifying Its Status \n \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n99\n Creating a Service \n In this section we shall create a Kubernetes service using a definition file. We have used the YAML format for \ndefinition files, but JSON could be used just as well. Create a service definition file called  mysql-service.yaml \nand copy the following listing to the file. Within the  spec field mapping for the service the “selector” expression \nis set to  app: \"mysql-app,\" which translates to service selector app=mysql-app and which implies that the \nservice routes traffic to Pods with the label  app=mysql-app . If the  selector expression is empty all Pods are \nselected. The port to expose the service is set to 3306 within the ports listing. And the service has a label \n app: \"mysql-app\" . The  kind field mapping must have value “Service.” \n apiVersion: v1 \n kind: Service \n metadata: \n name: \"mysql\" \n labels: \n  app: \"mysql-app\" \n spec: \n ports: \n  # the port that this service should serve on \n  - port: 3306 \n # label keys and values that must match in order to receive traffic for this service \n selector: \n  app: \"mysql-app\" \n The service schema is available at  http://kubernetes.io/v1.1/docs/api-reference/v1/\ndefinitions.html#_v1_service . Setting the  selector field in the YAML definition file to  app: \"mysql-app\" \nimplies that all Pods with the YAML definition file label setting  app: \"mysql-app\" are managed by the \nservice. Create the service using the definition file with the  kubectl create command. \n kubectl create -f mysql-service.yaml \n The  mysql service gets created and the output is “services/mysql” as shown in Figure  4-3 . \n List the service using  the  following command. \n kubectl get services \n Figure 4-3.  Creating a Service for MySQL Database \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n100\n Creating a Replication Controller \n In this section we shall create a replication controller managed by the service created in the previous section. \nCreate a replication controller definition file called  mysql-rc.yaml and copy the following/next listing to the \nfile. The  kind field mapping must have value “ReplicationController.” The replication controller has a label \n app: \"mysql-app\" in the  metadata field mapping. If the labels are empty they are defaulted to the labels of \nthe Pods the replication controller manages. The  \"spec\" field mapping defines the replication controller \nand includes the  \"replicas\" field mapping for the number of replicas to create. The  replicas is set to 1 \nin the following/next listing. The default number of replicas is also 1. The  spec includes a  selector field \nmapping called  app: \"mysql-app,\" which selects all Pods with label  app: \"mysql-app\" for the replication \ncontroller to manage and count toward the “replicas” setting. A Pod could have other labels in addition \nto the selector, but must include the selector expression/s of a replication controller to be managed by \nthe replication controller. Similarly, a replication controller could be managing Pods not started with the \nreplication controller definition file. \n Labels and selector expression settings in YAML definition files are not used as such, but are translated \nto a label/selector by replacing the ‘:’ with the ‘=’. For example, service/replication controller selector setting \napp: “mysql-app” becomes selector app = mysql-app selector and label setting app: “mysql-app” becomes \nlabel app = mysql-app. \n If a  selector is not specified the labels on the template are used to match the Pods and count toward \nthe “replicas” setting. The  \"template\" field mapping defines a Pod managed by the replication controller. \nThe  spec field mapping within the  template field specifies the behavior of the Pod. The  \"containers\" field \nmapping within the  \"spec\" field defines the collection/list of containers to create including the image, the \nenvironment variables if any, and the ports to use for each container. \n We need to use an environment variable for the MySQL database replication controller. The Docker \nimage “mysql” requires (is mandatory) the environment variable  MYSQL_ROOT_PASSWORD to run a Docker \ncontainer for MySQL database. The  MYSQL_ROOT_PASSWORD variable sets the password for the  root user. \nEnvironment variables are set with the  \"env\" mapping within a  containers field listing. An  env mapping \nconsists of a  name mapping and a  value mapping. The  MYSQL_ROOT_PASSWORD environment variable is set \nas shown in the following listing. The  \"ports\" field collection includes a  containerPort mapping for port \n3306. The indentations and hyphens in a YAML file must be well formatted and the following listing should \nbe copied and syntax validated in the YAML Lint ( http://www.yamllint.com/ ). The YAML lint only validates \nthe syntax and does not validate if the Pod definition field conforms to the schema for a pod. The Pod schema \nis available at  http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_podspec . \n Figure 4-4.  Listing the mysql Service \n The  mysql service gets listed as shown in Figure  4-4 . \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n101\n --- \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  labels: \n    app: \"mysql-app\" \n spec: \n  replicas: 1 \n  selector: \n    app: \"mysql-app\" \n  template: \n    metadata: \n      labels: \n        app: \"mysql-app\" \n    spec: \n      containers: \n      - \n        env: \n          - \n            name: \"MYSQL_ROOT_PASSWORD\" \n            value: \"mysql\" \n        image: \"mysql\" \n        name: \"mysql\" \n        ports: \n          - \n            containerPort: 3306 \n The  mysql-rc.yaml definition file may be created in the vi editor and saved with the :wq command as \nshown in Figure  4-5 . \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n102\n Create a replication controller from the service definition file with the  kubectl create command. \n kubectl create -f mysql-rc.yaml \n As the output in Figure  4-6 indicates, the  mysql replication controller gets created. \n Figure 4-5.  Definition File for Replication Controller \n Figure 4-6.  Creating a Replication Controller for MySQL Database \n \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n103\n List the replication with the following command. \n kubectl get rc \n The  mysql replication controller including the container name, image name, selector expression \n(app=mysql-app), and number of replicas get listed as shown in Figure  4-7 . \n To describe the  mysql replication controller run the following command. \n kubectl describe rc mysql \n The  replication controller  name, namespace, image, selector, labels, replicas, pod status, and events get \nlisted as shown in Figure  4-8 . \n Figure 4-7.  Listing the MySQL Replication Controller \n Figure 4-8.  Describing the MySQL Replication Controller \n \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n104\n Listing Logs \n List the  Pod  logs for a pod: for example, the  mysql-wuo7x pod, with the following command. \n kubectl logs mysql-wuo7x \n Figure 4-9.  Listing the Pod/s  for  MySQL Database \n Listing the Pods \n The  Pods created may be listed with the following command. \n kubectl get pods \n As shown in Figure  4-9 the 2 replicas created by the replication controller get listed. Initially the Pods \nmay not be listed as READY 1/1. Run the preceding command after a few seconds, multiple times if required, \nto list all the Pods as ready. \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n105\n Figure 4-10.  Listing the  Logs generated by the Pod for MySQL Database \n The Pod logs get listed as shown in Figure  4-10 . \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n106\n The MySQL Server is listed as started and “ready for connections” as shown in Figure  4-11 . \n Describing the Service \n To describe the  mysql  service run the following command. \n kubectl describe svc mysql \n The service name, namespace, labels, selector, type, Ip, port and endpoints get listed. Because the \nnumber of replicas is set to 1 only one endpoint is listed as shown in Figure  4-12 . \n Figure 4-11.  Listing mysqld  as  Ready for Connections \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n107\n Starting an Interactive Shell \n Bash is the free version of the Bourne shell distributed with Linux and GNU operating systems (OS). \nFor Docker images that have a Linux OS image as the base image as specified in the  FROM instruction in \nthe  Dockerfile , the software running in a Docker container may be accessed using the Bash shell. The \n \"mysql\" Docker image is based on the  \"debian\" image and as a result supports access to software running in \nthe Docker containers via a bash interactive shell. \n Next, we shall start an  interactive shell to start the MySQL CLI. But first we need to obtain the container \nid for one of the containers running MySQL. Run the following command to list the Docker containers. \n sudo docker ps \n Figure 4-12.  Describing  the  MySQL Service \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n108\n Using the Docker container id from the output from the preceding command, start an interactive shell. \n sudo docker exec -it 526f5d5f6c2e bash \n An interactive shell or tty gets started as shown in Figure  4-14 . \n Figure 4-13.  Listing the Docker Containers \n Figure 4-14.  Starting the Interactive Terminal \n The Docker container for the  mysql image is shown listed in Figure  4-13 . \n \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n109\n Starting the MySQL CLI \n Within the interactive shell run the following command to start the MySQL CLI as user root. \n mysql –u root –p \n When prompted with Password: set the password as the value of the environment variable \n MYSQL_ROOT_PASSWORD , which was set as “mysql” in the  mysql-rc.yaml definition file. The MySQL CLI gets \nstarted as shown in Figure  4-15 . \n List the databases with the following command. \n show databases; \n The default databases shown in Figure  4-16 include the  \"mysql\" database, which we shall use to create \na database table. The other databases are system databases and should not be used for user tables. \n Figure 4-15.  Starting the MySQL CLI Shell \n Figure 4-16.  Listing the Databases \n \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n110\n Set the database “mysql” as the current database with the following command. \n use mysql \n The database gets set as  mysql  as  indicated by the “Database changed” output in Figure  4-17 . \n Creating a Database Table \n Next, create a database table called  Catalog with the following SQL statement. \n CREATE TABLE Catalog(CatalogId INTEGER PRIMARY KEY,Journal VARCHAR(25),\nPublisher VARCHAR(25),Edition VARCHAR(25),Title VARCHAR(45),Author VARCHAR(25)); \n Add a row of data to the  Catalog table with the following SQL statement. \n INSERT INTO Catalog VALUES('1','Oracle Magazine','Oracle Publishing',\n'November December 2013','Engineering as a Service','David A. Kelly'); \n The  Catalog table gets created and a row of data gets added as shown in Figure  4-18 . \n Subsequently run the following  SQL  statement to query the database table  Catalog . \n SELECT * FROM Catalog; \n Figure 4-17.  Setting the Database \n Figure 4-18.  Creating a MySQL Database Table \n \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n111\n Exiting the MySQL CLI and Interactive Shell \n Exit the MySQL CLI with the “quit”  command. \n quit \n Exit the interactive terminal with the “exit” command. \n exit \n The output from the preceding commands is shown in Figure  4-20 . \n Scaling the Replicas \n One of the main benefits of Kubernetes is to be able to scale the number of MySQL instances in the cluster. \nRun the following  kubectl scale command to scale the replicas from 1 to 4. \n kubectl scale rc mysql --replicas=4 \n Subsequently run the following command to list the Pods. \n kubectl get pods \n Figure 4-19.  Querying the Database Table \n Figure 4-20.  Exiting the MySQL CLI Shell and Docker Container Interactive Shell \n The single row of data added gets listed as shown in Figure  4-19 . \n \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n112\n The number of Pods for the MySQL database gets increased to 4 as shown in Figure  4-21 . Some of the \nPods may be listed as READY- > 0/1, which implies the Pod/s are not ready yet. When READY becomes 1/1 \na Pod is ready to be accessed. The 0/1 value implies that 0 of the 1 Docker containers in the Pod are ready \nand similarly the 1/1 value implies that 1 of 1 containers is ready. The general syntax for the READY column \nvalue if all the n containers in the Pod are running is of the form n/n. The STATUS must be “Running” for a \nPod to be considered available. \n To describe the  mysql service, run the following command. \n kubectl describe svc mysql \n The service description is the same as before except that the number of endpoints has increased to 4 as \nshown in Figure  4-22 . \n Figure 4-21.  Scaling the Pod Replicas to Four \n \n",
      "page_number": 114
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 129-143)",
      "start_page": 129,
      "end_page": 143,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n113\n The command “scale” will also allow us to specify one or more preconditions for the scale actions \nneeded. The following (Table  4-1 ) preconditions are supported. \n Deleting the Replication Controller \n To delete the replication controller  mysql , run the following command. \n kubectl delete rc mysql \n The replication controller gets deleted as shown in Figure  4-23 . Whenever a  kubectl command output \nto create or delete an artifact (a Pod, service or replication controller) is of the form  artifact type/artifact \nname , it implies that the command has succeeded to create/delete the pod/service/replication controller. \n Figure 4-22.  Describing the MySQL Service  After  Scaling the Pod Replicas \n Figure 4-23.  Deleting the Replication Controller \n Subsequently run the following command to get the replication controllers. The  mysql rc does not get \nlisted as shown in Figure  4-24 . \n kubectl get rc \n Table 4-1.  Preconditions for the ‘kubernetes scale’ command \n Precondition \n Description \n --current-replicas \n The current number of replicas for the scale to be performed. \n --resource-version \n The resource version to match for the scale to be performed. \n \n \n\n\nCHAPTER 4 ■ USING MYSQL DATABASE\n114\n Describe the service  mysql again with the following command. \n kubectl describe svc mysql \n No “Endpoints” get listed as shown in Figure  4-24 because all the Pods get deleted when the replication \ncontroller managing them is deleted. \n Summary \n In this chapter we discussed orchestrating the MySQL database cluster using the Kubernetes cluster \nmanager. We created a Kubernetes service to represent a MySQL-based Pod. The “mysql” Docker image is \nused to create a Pod. We used a replication controller to create replicas for MySQL base Pods. Initially the \nnumber of replicas is set to 1. We used a Docker container running a MySQL instance to start the MySQL \nCLI and create a database table. Subsequently, we scaled the number of replicas to 4 using the replication \ncontroller. When scaled, the number of replicas and therefore the number of MySQL instances becomes 4. The \nreplication controller maintains the replication level through replica failure or replica shut down by a user. \nThis chapter also demonstrates the use of environment variables. The  MYSQL_ROOT_PASSWORD environment \nvariable is required to run a container for the Docker image “mysql” and we set the  MYSQL_ROOT_PASSWORD \nenvironment variable in the Pod spec in the replication controller. In the next chapter we shall discuss using \nanother open source database, the PostgreSQL database. \n Figure 4-24.  Describing the Service  after  Deleting the Replication Controllers \n \n\n\n115\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_5\n CHAPTER 5 \n Using PostgreSQL Database \n PostgreSQL is an open source object-relational database. PostgreSQL is scalable both in terms of the quantity \nof data and number of concurrent users. PostgreSQL is supported in several of Apache Hadoop ecosystem \nprojects such as Apache Sqoop and may be used for Apache Hive Metastore. PostgreSQL 9.5 offers several \nnew features such as support for  UPSERT ,  BRIN indexing, faster sorts, and the  TABLESAMPLE clause for getting a \nstatistical sample of a large table. In this chapter we shall discuss creating a PostgreSQL 9.5 cluster using the \nKubernetes cluster manager. We shall discuss both the imperative approach and the declarative approach for \ncreating and scaling a PostgreSQL cluster. This chapter has the following sections.\n Setting the Environment \n Creating a PostgreSQL Cluster Declaratively \n Creating a PostgreSQL Cluster Imperatively \n Setting the Environment \n We have used the same type of Amazon EC2 instance in this chapter as in other chapters, an instance based \non Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-d05e75b8 AMI. The following software is \nrequired for this chapter. \n -Docker Engine (latest version) \n -Kubernetes Cluster Manager (version 1.01) \n -Kubectl (version 1.01) \n -Docker Image “postgres” (latest version) \n The procedure to install the required software, start Docker engine and Kubernetes cluster manager, is \ndiscussed in chapter  1 . To install the software first we need to log in to the Amazon EC2 instance. Obtain the \nPublic IP Address of the Amazon EC2 instance as shown in Figure  5-1 . \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n116\n SSH Login to the  Ubuntu instance using the Public IP Address. \n ssh -i \"docker.pem\" ubuntu@52.91.60.182 \n Start the  Docker engine and verify its status. \n sudo service docker start \n sudo service docker status \n Docker should be indicated as “running” as shown in Figure  5-2 . \n List the services with the following command. \n kubectl get services \n Figure 5-1.  Obtaining the Public IP Address \n Figure 5-2.  Starting Docker \n \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n117\n The  kubernetes service should get listed as shown in Figure  5-3 . \n Creating a PostgreSQL Cluster Declaratively \n In the following subsections we shall create and manage a PostgreSQL  cluster declaratively , which implies \nwe shall use definition files. The definition files could be based on the YAML format or the JSON format. \nWe shall be using YAML format. It is recommended to create the service first so that any pods created \nsubsequently have a service available to represent them. If the RC (replication controller) is created first, the \npods are not usable until a service is created. \n Creating a Service \n Create a service definition  file  postgres-service.yaml and copy the following listing to the file. The  \"spec\" \nfield mapping for the service specifies the behavior of the service. The ports on which the service is exposed \nare defined in the  \"ports\" field mapping. Only the port 5432 is exposed because PostgreSQL runs on port 5432. \nThe  selector expression is set to  app: \"postgres\" . All Pods with the label  app=postgres are managed by \nthe service. \n apiVersion: v1 \n kind: Service \n metadata: \n  name: \"postgres\" \n  labels: \n    app: \"postgres\" \n spec: \n  ports: \n    - port: 5432 \n  selector: \n    app: \"postgres\" \n The  postgres-service.yaml file may be created using the vi editor and saved with the :wq command as \nshown in Figure  5-4 . \n Figure 5-3.  Listing the Kubernetes Services \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n118\n Create the service using the  kubectl create command with the  postgres-service.yaml definition file. \n kubectl create -f postgres-service.yaml \n Subsequently list the services. \n kubectl get services \n Also list the Pods. \n kubectl get pods \n An output of  services/postgres from the first command indicates that the service has been created. \nThe second command lists the  postgres service as running at port 5432 as shown in Figure  5-5 . The IP \nAddress of the service is also listed. Creating a service by itself does not create a Pod by itself and only the \nPod for the Kubernetes is listed. A service only manages or provides an interface for Pods with the label that \nmatches the  selector expression in the service. \n Figure 5-4.  Service Definition File postgres-service.yaml \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n119\n Describe the service  postgres with the following command. \n kubectl describe svc postgres \n The service name, namespace, labels, selector, type, IP address, Port exposed on, and Endpoints get listed. \nBecause no Pods are initially associated with the service, no endpoints are listed as shown in Figure  5-6 . \n Creating a Replication Controller \n In this section we shall create a definition file for a replication  controller . Create a definition file called \n postgres-rc.yaml . The definition file has the field discussed in Table  5-1 . \n Figure 5-5.  Creating a Service and listing the Service \n Figure 5-6.  Describing the postgres Service \n \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n120\n Optionally the replication controller’s  selector field mapping may be specified. The key:value \nmapping in the  selector must match a label in the template- > metadata field mapping for the replication \ncontroller to manage the Pod in the template. The  selector field mapping if not specified defaults to \nthe template- > metadata- >  labels field mapping. In the following listing the  selector is italicized and \nnot included in the definition file used. The Pod's template- > metadata- > labels field mapping specifies \nan expression  app: \"postgres\", which translates to Pod label app=postgres . The  labels field \nexpression must be the same as the  \"selector\" field expression in the service definition file, which was \ndiscussed in the previous section, for the service to manage the Pod. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  name: \"postgres\" \n Table 5-1.  Replication Controller Definition File postgres-rc.yaml \n Field \n Value \n Description \n apiVersion \n v1 \n The API version. \n kind \n ReplicationController \n Defines the file to be a replication \ncontroller. \n metadata \n Metadata for the replication controller. \n metadata- > name \n The name of the replication controller. \nEither the name or the generateName \nfield must be specified. The \ngenerateName field is the prefix to use in \nan automatically generated name. \n spec \n The specification for the replication \ncontroller. \n spec- > replicas \n 2 \n The number of Pod replicas to create. \n template \n Specifies the template for the Pod that \nthe replication controller manages. \n template- > metadata \n The metadata for the Pod including \nlabels. The label is used to select the Pods \nmanaged by the replication controller \nand must manage the selector expression \nin the service definition file if the service \nis to represent the Pod. \n template- > spec \n Pod specification or configuration. \n template- > spec- > containers \n The containers in a Pod. Multiple \ncontainers could be specified but in \nthis chapter only the container for \nPostgreSQL is specified. \n template- > spec- > containers- > image \n template- > spec- > containers- > name \n The Docker image to run in the container. \nFor PostgreSQL the image is “postgres.” \nThe name field specifies the container \nname. \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n121\n spec: \n  replicas: 2 \n   selector: \n     - app: \"postgres\" \n  template: \n    metadata: \n       labels: \n         app: \"postgres\" \n    spec: \n      containers: \n      - \n        image: \"postgres\" \n        name: \"postgres\" \n Copy the preceding listing to the  postgres-rc.yaml file. The  postgres-rc.yaml file may be opened in \nthe vi editor and saved with :wq as shown in Figure  5-7 . \n Figure 5-7.  Replication Controller Definition File \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n122\n Create a replication controller using the definition file  postgres-rc.yaml . \n kubectl create -f postgres-rc.yaml \n Subsequently list the replication controllers. \n kubectl get rc \n An output of  replicationcontrollers/postgres from the first command as shown in Figure  5-8 \nindicates that the replication controller  postgres has been created. The second command lists the  postgres \nreplication controller. As discussed before the Replication Controller SELECTOR column is set to the same \nvalue as the Pod label, app=postgres. \n Describe the replication controller  postgres with the following command. \n kubectl describe rc postgres \n The replication controller's name, namespace, image associated with the rc, selectors if any, labels, \nnumber of replicas, pod status, and events get listed as shown in Figure  5-9 . \n Figure 5-9.  Describing the Replication Controller for PostgreSQL Database \n Figure 5-8.  Creating and listing the Replication Controller for PostgreSQL Database \n \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n123\n Getting the Pods \n To get and list the  Pods run the following command. \n kubectl get pods \n The two Pods created by the replication controller get listed as shown in Figure  5-10 . The Pods should \nhave the Running STATUS and have the READY column value as 1/1. \n Figure 5-10.  Listing the Pods for PostgreSQL Database \n Starting an Interactive Command Shell \n To be able to create a PostgreSQL table we need to start an  interactive bash shell to access the PostgreSQL \nserver running in a Docker container, and start the psql SQL shell for PostgreSQL. But, first we need to find \nthe container id for a Docker container running the PostgreSQL database. Run the following command to list \nthe Docker containers. \n sudo docker ps \n Two of the Docker containers are based on the “postgres” image as shown in Figure  5-11 . Copy the \ncontainer id for the first Docker container for the  postgres image from the CONTAINER ID column. \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n124\n Using the container id start the interactive shell. \n sudo docker exec -it a786960b2cb6 bash \n The interactive shell gets started as shown in Figure  5-12 . \n Figure 5-11.  Listing the Docker Containers \n Figure 5-12.  Starting an Interactive Shell \n Starting the PostgreSQL SQL Terminal \n Next, start the psql SQL shell for PostgreSQL. Set the user as  postgres . \n su –l postgres \n \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n125\n Start the psql command line shell with the following command. \n psql postgres \n The psql shall get started as shown in Figure  5-13 . \n For the general command syntax for the  psql command refer  http://www.postgresql.org/docs/9.5/\nstatic/app-psql.html . \n Creating a Database Table \n In the psql shell run the following SQL statements to create a database  table called  wlslog and add data to \nthe table. \n CREATE TABLE wlslog(time_stamp VARCHAR(255) PRIMARY KEY,category VARCHAR(255),type \nVARCHAR(255),servername VARCHAR(255),code VARCHAR(255),msg VARCHAR(255)); \n INSERT INTO wlslog(time_stamp,category,type,servername,code,msg) VALUES('Apr-8-2014-7:\n06:16-PM-PDT','Notice','WebLogicServer','AdminServer','BEA-000365','Server state changed \nto STANDBY'); \n INSERT INTO wlslog(time_stamp,category,type,servername,code,msg) VALUES('Apr-8-2014-7:\n06:17-PM-PDT','Notice','WebLogicServer','AdminServer','BEA-000365','Server state changed \nto STARTING'); \n INSERT INTO wlslog(time_stamp,category,type,servername,code,msg) VALUES('Apr-8-2014-7:\n06:18-PM-PDT','Notice','WebLogicServer','AdminServer','BEA-000360','Server started in \nRUNNING mode'); \n Database table  wlslog gets created and a row of data gets added as shown in Figure  5-14 . \n Figure 5-13.  Starting the  psql CLI Shell \n Figure 5-14.  Creating a Database Table \n \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n126\n Run the following SQL statement to query the database table  wlslog . \n SELECT * FROM wlslog; \n The 3 rows of data added get listed as shown in Figure  5-15 . \n Exiting the Interactive Command Shell \n To exit the psql shell run the following command. \n \\q \n To exit the interactive terminal run the following command. \n exit \n The  psql shell and the interactive shell get exited as shown in Figure  5-16 . \n Figure 5-15.  Querying the Database Table \n Figure 5-16.  Exiting the psql Shell and Docker Container Interactive Shell \n \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n127\n Scaling the PostgreSQL Cluster \n One of the main benefits of the Kubernetes cluster manager is to be able to scale the cluster as required. \nInitially we created 2 replicas. For example, to scale up the number of PostgreSQL instances to 4 run the \nfollowing command. \n kubectl scale rc postgres --replicas=4 \n An output of “scaled” from the preceding command indicates that the cluster has been scaled as shown \nin Figure  5-17 . \n Subsequently list the pods with the following command. \n kubectl get pods \n The 4 Pods get listed as shown in Figure  5-18 . Initially some of the Pods could be listed as not “Running” \nand/or not in READY (1/1) state. \n Run the preceding command again after a few seconds. \n kubectl get pods \n The new Pods added to the cluster also get listed as “Running” and in READY state 1/1 as shown in \nFigure  5-19 . \n Figure 5-17.  Scaling the number of Pod Replicas to 4 \n Figure 5-18.  Listing the Pods after Scaling \n \n \n",
      "page_number": 129
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 144-152)",
      "start_page": 144,
      "end_page": 152,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n128\n Describe the  postgres service again. \n kubectl describe svc postgres \n Initially no Endpoint was listed as being associated with the service when the service was initially \nstarted. With 4 Pods running 4 Endpoints get listed as shown in Figure  5-20 . \n Listing the Logs \n To list the logs  data for a Pod, for example the postgres-v0k42 Pod, run the following command. \n kubectl logs postgres-v0k42 \n The output in Figure  5-21 lists the PostgreSQL starting. \n Figure 5-19.  Listing all the Pods as running and ready \n Figure 5-20.  Describing the postgres Service \n \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n129\n When the PostgreSQL database gets started completely the message “database system is ready to accept \nconnections” gets output as shown in Figure  5-22 . \n Figure 5-21.  Listing the Logs for a Pod running PostgreSQL Database \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n130\n Deleting the Replication Controller \n To delete the replication  controller  postgres and as a result delete all the Pods managed by the replication \ncontroller run the following command. \n kubectl delete rc postgres \n The  postgres replication controller gets deleted as indicated by the  replicationcontrollers/\npostgres output shown in Figure  5-23 . Subsequently, run the following command to list the replication \ncontrollers. \n kubectl get rc \n Figure 5-22.  PostgreSQL Database listed as Started and subsequently Shutdown in the Logs \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n131\n The  postgres replication controller does not get listed as shown in Figure  5-23 . Deleting the replication \ncontroller does not delete the service managing the replication controller. To demonstrate list the services. \n kubectl get services \n The  postgres service is still getting listed, as shown in Figure  5-23 . \n Stopping the Service \n To stop the service  postgres run the following  command . \n kubectl stop service postgres \n Subsequently run the following command again. \n kubectl get services \n The  postgres service does not get listed as shown in Figure  5-24 . \n Figure 5-24.  Stopping the postgres Service \n Figure 5-23.  Deleting a Replication Controller \n Creating a PostgreSQL Cluster Imperatively \n Using a declarative approach with definition files offers finer control over the service and replication \ncontroller. But a replication controller and service could also be created on the command line with  kubectl \ncommands. In the following subsections we shall create a replication controller and a service.  \n \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n132\n Creating a Replication Controller \n To create a replication  controller called  postgres for image “postgres” with number of replicas as 2 and Post \nas 5432 run the following command. \n kubectl run postgres --image=postgres --replicas=2 --port=5432 \n The  postgres replication controller with 2 replicas of Pod with image  postgres and  selector \nexpression  run=postgres gets created as shown in Figure  5-25 . \n List the replication controllers with the following command. \n kubectl get rc \n The  postgres replication controller gets listed as shown in Figure  5-26 . \n Getting the Pods \n To list the Pods managed by the replication controller run the following command. \n kubectl get pods \n The two  Pods get listed as shown in Figure  5-27 . Initially some of the Pods could be listed not Ready as \nindicated by the 0/1 READY column value. Run the preceding command again to list the Pods as ready with \nREADY column value as 1/1. \n Figure 5-25.  Creating a Replication Controller Imperatively \n Figure 5-27.  Listing the Pods \n Figure 5-26.  Listing the Replication Controllers \n \n \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n133\n Creating a Service \n To create a  service we need to run the  kubectl expose command. Initially only the  kubernetes service is \nrunning. To demonstrate, run the following command. \n kubectl get services \n As shown in Figure  5-28 only the  kubernetes service is listed. \n To create a service for the replication controller  \"postgres\" run the following command in which the \n –port parameter specifies the port at which the service is exposed. The service type is set as  LoadBalancer . \n kubectl expose rc postgres --port=5432 --type=LoadBalancer \n Subsequently list the services. \n kubectl get services \n The postgres service gets listed as shown in Figure  5-29 . \n Figure 5-28.  Listing the “kubernetes” Service \n Figure 5-29.  Creating a Service exposed at Port 5432 \n \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n134\n Creating a Database Table \n The procedure to create a database  table is the same as discussed previously for the declarative section and \nis discussed only briefly in this section. List the Docker containers with the following command. \n sudo docker ps \n Two of the  Docker containers are listed with image as  postgres in the IMAGE column as shown in \nFigure  5-30 . Copy the container id for one of these columns from the CONTAINER ID column. \n Figure 5-30.  Listing the Docker Containers \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n135\n Start the interactive shell with the following command. \n sudo docker exec -it af0ac629b0e7d bash \n The interactive terminal gets started as shown in Figure  5-31 . \n Set the user as  postgres . \n su –l postgres \n Start the  psql command line shell. \n psql postgres \n The  psql shell is shown in Figure  5-32 . \n Run the following SQL statements to create a database table called  wlslog and add data to the table. \n CREATE TABLE wlslog(time_stamp VARCHAR(255) PRIMARY KEY,category VARCHAR(255),type \nVARCHAR(255),servername VARCHAR(255),code VARCHAR(255),msg VARCHAR(255)); \n INSERT INTO wlslog(time_stamp,category,type,servername,code,msg) VALUES('Apr-8-2014-7:\n06:16-PM-PDT','Notice','WebLogicServer','AdminServer','BEA-000365','Server state changed \nto STANDBY'); \n INSERT INTO wlslog(time_stamp,category,type,servername,code,msg) VALUES('Apr-8-2014-7:\n06:17-PM-PDT','Notice','WebLogicServer','AdminServer','BEA-000365','Server state changed \nto STARTING'); \n INSERT INTO wlslog(time_stamp,category,type,servername,code,msg) VALUES('Apr-8-2014-7:\n06:18-PM-PDT','Notice','WebLogicServer','AdminServer','BEA-000360','Server started in \nRUNNING mode'); \n Figure 5-32.  Starting the psql Shell \n Figure 5-31.  Starting the TTY \n \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n136\n The database table  wlslog gets created as shown in Figure  5-33 . \n Run the following SQL statement to query the  wlslog table. \n SELECT * FROM wlslog; \n The three rows of data added get listed as shown in Figure  5-34 . \n To quit the  psql shell and the interactive shell for the Docker container running PostgreSQL, run the \nfollowing commands. \n \\q \n exit \n The psql shell and the tty get exited as shown in Figure  5-35 . \n Figure 5-33.  Creating a Database Table \n Figure 5-34.  Querying the wlslog Database Table \n \n \n",
      "page_number": 144
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 153-176)",
      "start_page": 153,
      "end_page": 176,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n137\n Scaling the PostgreSQL Cluster \n When we created the cluster initially we set the replicas to 2. List the pods as follows. \n kubectl get pods \n Only two Pods get listed as shown in Figure  5-36 . \n Scale the cluster to 4 replicas with the following command. \n kubectl scale rc postgres --replicas=4 \n An output of “scaled” as shown in Figure  5-37 indicates that the cluster has been scaled. \n Subsequently list the Pods. \n kubectl get pods \n The preceding command may have to be run multiple times to list all the Pods as “Running” and in \nREADY state 1/1 as shown in Figure  5-38 . \n Figure 5-36.  Listing the Pods \n Figure 5-37.  Scaling the Pod Replicas to 4 \n Figure 5-35.  Exiting the Shells \n \n \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n138\n Deleting the Replication Controller \n To  delete the replication controller run the following command. \n kubectl delete rc postgres \n List the Pods subsequent to deleting the rc. \n kubectl get pods \n List the services. \n kubectl get services \n The  postgres replication controller gets deleted and does not get listed subsequently as shown in \nFigure  5-39 . The  postgres service still gets listed also shown in Figure  5-39 . \n Figure 5-38.  Listing the Pods in various states of starting \n \n\n\nCHAPTER 5 ■ USING POSTGRESQL DATABASE\n139\n Stopping the Service \n To  stop the service run the following command. \n kubectl stop service postgres \n The  postgres service gets stopped as shown in Figure  5-40 . Subsequently run the following command. \n kubectl get services \n The  postgres service does not get listed as shown in Figure  5-40 also. \n Summary \n In this chapter we used the Kubernetes cluster manager to start and manage a PostgreSQL server cluster. \nWe demonstrated creating a cluster both imperatively on the command line and declaratively using \ndefinition files. We scaled the cluster using a replication controller and exposed a service for the cluster \nusing a Kubernetes service. In the next chapter we shall discuss creating and managing an Oracle \nDatabase cluster. \n Figure 5-40.  Stopping the Service \n Figure 5-39.  Deleting the Replication Controller \n \n \n\n\n141\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_6\n CHAPTER 6 \n Using Oracle Database \n Oracle Database is the most commonly used relational database (RDBMS). Installing and configuring \nOracle Database would usually involve downloading the software, setting the kernel parameters, installing \nand configuring the software, all of which is quite involved. Using Docker containers coordinated with \nKubernetes makes the task of installing, configuring, and orchestrating a Oracle Database cluster much \neasier. Oracle Database cluster consisting of multiple instances could benefit from the schedulability, \nscalability, distributedness, and failover features of the Kubernetes container cluster manager. In this \nchapter we shall install Oracle Database using a Docker image for the database. We shall create multiple \nreplicas of the database Pod using a replication controller and expose the database as a service. This chapter \nhas the following sections.\n Setting the Environment \n Creating an Oracle Database Instance Imperatively \n Creating an Oracle Database Instance Declaratively \n Keeping the Replication Level \n Scaling the Database \n Starting the Interactive Shell \n Connecting to Database \n Creating a User \n Creating a Database Table \n Exiting the Interactive Shell \n Setting the Environment \n The following software is required for this chapter. \n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n -Docker Image for Oracle Database (Oracle Database XE 11g) \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n142\n If not already installed, install Docker Engine, Kubernetes, and Kubectl as discussed in Chapter  1 . \nSSH login to the Ubuntu instance on Amazon EC2 using the Public IP Address of the EC2 instance. \n ssh -i \"docker.pem\" ubuntu@52.90.115.30 \n Start the Docker instance and verify its status with the following commands. \n sudo service docker start \n sudo service docker status \n Docker is indicated as running in Figure  6-1 . \n List the services running. \n kubectl get services \n Only the  kubernetes service is listed as running in Figure  6-2 . \n Creating an Oracle Database Instance Imperatively \n In this section we shall create an Oracle Database cluster using  kubectl on the command line. Several \nDocker images are available for Oracle Database and we shall be using the  sath89/oracle-xe-11g image \n( https://hub.docker.com/r/sath89/oracle-xe-11g/ ). Run the following  kubectl command to create an \nOracle Database cluster consisting of 2 replicas with port set as 1521.  \n kubectl run oradb --image=sath89/oracle-xe-11g --replicas=2 --port=1521 \n The output from the command in Figure  6-3 lists a replication controller called  oradb , a Docker \ncontainer called  oradb , a selector ( run=oradb ) to select Pods that comprise the replication controller \nreplicas, and the number of replicas (2). The Pod label is also set to  run=oradb . \n Figure 6-1.  Starting Docker and verifying its Status \n Figure 6-2.  Listing the Kubernetes Service \n \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n143\n List the replication  controller with the following command. \n kubectl get rc \n The  oradb replication controller shown in Figure  6-4 gets listed. \n Figure 6-3.  Creating a Replication Controller and Pod Replicas for Oracle Database \n Figure 6-4.  Listing the Replication Controllers \n List the Pods using the following command. \n kubectl get pods \n In addition to the Kubernetes Pod  k8s-master-127.0.0.1 two other pods get listed for Oracle Database \nas shown in Figure  6-5 . Initially the Pods could be listed as “not ready” as shown in Figure  6-5 also. Run the \npreceding command after a duration of a few seconds, multiple times if required, to list the two Pods are \nRunning and READY (1/1). \n Figure 6-5.  Listing the Pods in various stages of running \n \n \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n144\n Get the nodes with the following command. \n kubectl get nodes \n And get the Kubernetes services with the following command. \n kubectl get services \n Only the  kubernetes service gets listed as shown in Figure  6-6 because we have not yet created a service \nfor Oracle Database. \n Listing Logs \n List the  logs for one of the Pods using the following command. \n kubectl logs oradb-ea57r \n The logs generated by a started Oracle Database instance get output as shown in Figure  6-7 . Oracle Net \nListener is indicated as having been started. \n Figure 6-6.  Creating a Replication Controller does not create a Service \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n145\n Creating a Service \n Next, expose the replication controller  oradb as a Kubernetes service on port 1521. Subsequently list the \nKubernetes  services . \n kubectl expose rc oradb --port=1521 --type=LoadBalancer \n kubectl get services \n The first of the two preceding commands starts the  oradb service. Subsequently the service gets listed \nas shown in Figure  6-8 . The service selector is run=oradb, which is the same as the replication controller \nselector. \n Figure 6-7.  Listing Logs for a Pod \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n146\n Describe the service with the following command. \n kubectl describe svc oradb \n The service name, namespace, labels, selector, type, IP, Port,NodePort, and endpoints get listed as \nshown in Figure  6-9 . \n Scaling the Database \n Run the  kubectl scale command to scale the replicas. For example, reduce the number of replicas to 1. \n kubectl scale rc oradb --replicas=1 \n Figure 6-8.  Creating a Service Imperatively \n Figure 6-9.  Describing the oradb Service \n \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n147\n An output of “scaled” indicates that the replicas have been scaled as shown in Figure  6-10 . \n Subsequently list the running Pods. \n kubectl get pods \n Only one Oracle Database Pod gets listed as the other has been stopped to reduce the replication level \nto one as shown in Figure  6-11 . Subsequently, describe the service. \n kubectl describe svc oradb \n Because the cluster has been scaled down to one replica the number of endpoints also gets reduced to \none as shown in Figure  6-11 . \n Deleting the Replication Controller and Service \n In subsequent sections we shall be creating a cluster of Oracle Database instances declaratively using \ndefinition files. As we shall be using the same configuration parameters,  delete the  \"oradb\" replication \ncontroller and the  \"oradb\" service with the following commands. \n kubectl delete rc oradb \n kubectl delete svc oradb \n Figure 6-10.  Scaling the Replicas to 1 \n Figure 6-11.  Listing and Describing the Single Pod \n \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n148\n Both the replication controller and the service get deleted as shown in Figure  6-12 . \n Creating an Oracle Database Instance Declaratively \n In this section we shall create Oracle Database cluster  declaratively using definition files for a Pod, \nreplication controller, and service. We have used the YAML format in the definition files but the JSON format \nmay be used instead. \n Creating a Pod \n Create a definition file for a  Pod called  oradb.yaml . Copy the following listing, which defines a Pod named \n“oradb” with a label setting  name: \"oradb\" , which translates to Pod label name=oradb. The container image \nis set as “sath89/oracle-xe-11g” and the container port is set as 1521. \n apiVersion: v1 \n kind: Pod \n metadata: \n  name: \"oradb\" \n  labels: \n    name: \"oradb\" \n spec: \n  containers: \n    - \n      image: \"sath89/oracle-xe-11g\" \n      name: \"oradb\" \n      ports: \n        - \n          containerPort: 1521 \n  restartPolicy: Always \n The  oradb.yaml file may be created in the vi editor and saved with the :wq command as shown in \nFigure  6-13 . \n Figure 6-12.  Deleting the Replication Controller and Service \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n149\n Create a Pod using the definition file  oradb.yaml with the  kubectl create command. \n kubectl create -f oradb.yaml --validate \n An output of “pods/oradb” in Figure  6-14 indicates that the  oradb Pod has been created. \n Figure 6-13.  Pod Definition File \n Figure 6-14.  Creating a Pod from a Definition File \n \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n150\n Subsequently list the running Pods with the following command. \n kubectl get pods \n The single Pod  oradb gets listed as shown in Figure  6-15 . Initially, the  oradb Pod could be listed in \nvarious phases of starting such as Image “is ready, container is creating” or the READY value could be 0/1 \nindicating that the Pod is not ready yet. When the STATUS column becomes “Running” and the READY \ncolumn becomes 1/1 the Pod is started and ready. The preceding command may have to be run multiple \ntimes to list the Pod as Running and Ready. \n Creating a Service \n Next, create a service for an Oracle Database cluster. The service does not specify how many instances \n(replicas) of the Oracle Database image are running or should be running. The replicas are controlled by \nthe replication controller. The service only defines a port to expose the service at, a label for the service \nand a selector to match the Pods to be managed by the service. The selector setting is app: “oradb”, which \ntranslates to service selector app=oradb. Create a service definition file  oradb-service.yaml and copy the \nfollowing listing to the definition file. \n apiVersion: v1 \n kind: Service \n metadata: \n  name: \"oradb\" \n  labels: \n    app: \"oradb\" \n Figure 6-15.  Listing the Pod/s, which could initially be not Running and not Ready \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n151\n spec: \n  ports: \n    - \n      port: 1521 \n  selector: \n    app: \"oradb\" \n The  oradb-service.yaml definition file may be created in the vi editor and saved with :wq as shown in \nFigure  6-16 . \n Figure 6-16.  Service Definition File \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n152\n Run the following command to create a service from the service definition file. \n kubectl create -f oradb-service.yaml \n The  oradb service gets created as indicated by the “services/oradb” output in Figure  6-17 . Subsequently \nlist the services. \n kubectl get services \n The  oradb service gets listed as shown in Figure  6-17 . \n Describe the  oradb service with the following command. \n kubectl describe svc oradb \n No service endpoint gets listed as shown in Figure  6-18 because the service selector does not match \nthe label on the Pod already running. The service selector app=oradb has to match a Pod label for the \nservice to be able to manage the Pod. In the next section we shall create a replication controller with a \nmatching label. \n Figure 6-17.  Creating a Service from a Service Definition File \n Figure 6-18.  Describing a Service for Oracle Database \n \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n153\n Creating a Replication Controller \n Create a replication controller definition file called  oradb-rc.yaml and copy the following listing, \nwhich defines a replication controller, to the definition file. For the replication controller to manage the \nPods defined in the spec field the key:value expression of the selector in the replication controller has \nto match a label in the Pod template mapping. The  selector is omitted in the  oradb-rc.yaml but the \nspec- > template- > metadata- > labels must be specified. The selector defaults to the same setting as the \n spec->template->metadata->labels . The template- > spec- > containers mapping defines the containers in \nthe Pod. Only the Oracle Database container “sath89/oracle-xe-11g” is defined. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  name: \"oradb\" \n  labels: \n    app: \"oradb\" \n spec: \n  replicas: 2 \n  template: \n    metadata: \n      labels: \n        app: \"oradb\" \n    spec: \n      containers: \n        - \n          image: \"sath89/oracle-xe-11g\" \n          name: \"oradb\" \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n154\n The  oradb-rc.yaml file may be edited in the vi editor and saved with the :wq command as shown in \nFigure  6-19 . \n Next, run the following command to create a replication controller from the definition file  oradb-rc.yaml . \n kubectl create -f oradb-rc.yaml \n The replication controller gets created as shown in Figure  6-20 . List the replication controller with the \nfollowing command. \n kubectl get rc \n Figure 6-19.  Replication Controller Definition File \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n155\n The  oradb replication controller gets created as shown in Figure  6-20 . \n The Pods created by the replication controller are listed with the following command. \n kubectl get pods \n Three Oracle Database Pods get listed as shown in Figure  6-21 . Why do three Pods get listed even \nthough the replication controller replicas are set to 2? Because the Pod started using the Pod definition file \n oradb.yaml does not include a label that matches the selector in the replication controller. The replication \ncontroller selector is  app: \"oradb\" while the label on the Pod is  name: \"oradb\" . Two replicas are started by \nthe replication controller and one Pod was started earlier by the pod definition file. \n Describe the service  oradb with the following command. \n kubectl describe svc oradb \n The service endpoints get listed as shown in Figure  6-22 . Only two endpoints get listed because the \nservice selector app: “oradb” matches the Pod label in the replication controller with two replicas. The Pod \ncreated earlier does not include a label that matches the selector expression. \n Figure 6-20.  Creating and listing a Replication Controller from a Definition File \n Figure 6-21.  Listing the Pod Replicas \n \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n156\n Keeping the Replication Level \n The task of the replication controller is to maintain the replication level of the Pods. Because the  replicas \nfield mapping in the replication controller spec is 2, two replicas of the Pod configured in the Pod spec must \nbe running at all time while the replication controller is running. To demonstrate that the replication level is \nkept, delete a Pod. \n kubectl delete pod oradb-5ntnj \n Subsequently list the running Pods. \n kubectl get pods \n One of the two replicas got deleted with the  kubectl delete pod command but another replica is \nlisted as getting started in Figure  6-23 . It may take a few seconds for the replicas to reach the replication \nlevel. Run the preceding command multiple times to list the replicas as running. The number of replicas \ngets back to 2. \n Figure 6-22.  Describing the Service after creating the Replication Controller \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n157\n The  \"oradb\" Pod is not associated with the replication controller and therefore it is not counted as \none of the replicas managed by the replication controller. The  oradb Pod is not managed by the replication \ncontroller because, as discussed earlier, the label on the  oradb Pod does not match the label on the \nreplication controller. To demonstrate that the  oradb pod is not managed by the replication controller delete \nthe Pod. \n kubectl delete pod oradb \n Subsequently list the running Pods. \n kubectl get pods \n The  oradb Pod gets deleted and a replacement Pod does not get started and does not get listed in the \nrunning Pods as shown in Figure  6-24 . \n Figure 6-23.  Maintaining the Replication Level \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n158\n Scaling the Database \n The replication controller may be used to scale the number of Pods running for Oracle Database. As an \nexample scale up the number of Pod replicas to 3 from 2. \n kubectl scale rc oradb --replicas=3 \n The “scaled” output indicates that the replicas have been scaled. Subsequently run the following \ncommand, multiple times if required, to list the new Pod replica as running and ready. \n kubectl get pods \n Three replicas of the Pod get listed as shown in Figure  6-25 . \n Figure 6-25.  Scaling the Cluster to 3 Replicas \n Figure 6-24.  The oradb Pod is not managed by the Replication Controller \n \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n159\n Describe the service again. \n kubectl describe svc oradb \n Three endpoints get listed instead of two as shown in Figure  6-26 . The service has a single IP address. \n Starting the Interactive Shell \n In this section we shall start an interactive tty (shell) to connect to the software, which is Oracle Database, \nrunning in a Docker container started with and managed by Kubernetes. First, list the Docker containers \nwith the following command. \n sudo docker ps \n Copy the container id for one of the Docker containers for the  sath89/oracle-xe-11g image as shown \nin Figure  6-27 . \n Figure 6-26.  Listing the 3 Endpoints in the Service \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n160\n Figure 6-27.  Copying the Container Id for a Docker Container \n Using the container id start an interactive shell with the following command. \n sudo docker exec -it 9f74a82d4ea0 bash \n The interactive shell gets started as shown in Figure  6-28 . \n Figure 6-28.  Starting an Interactive Shell \n Connecting to Database \n In the interactive tty change the user to “oracle.” \n su -l oracle \n The difference between  su oracle and  su - oracle is that the latter logs in with the environment \nvariables of  oracle user and also sets the current directory to oracle home directory while the former logs in \nas  oracle but the environment variables and current directory remain unchanged. \n \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n161\n Subsequently start the SQL*Plus. Using the  /nolog option does not establish an initial connection with \nthe database. \n sqlplus /nolog \n Run the following command to connect  SYS as  SYSDBA . \n CONNECT SYS AS SYSDBA \n Specify the Password as “oracle” when prompted. The output from the preceding commands to start \nSQL*Plus and connect  SYS are shown in Figure  6-29 . A connection gets established. \n Creating a User \n To create a user called OE and grant  CONNECT and  RESOURCE roles to the user, run the following commands. \n CREATE USER OE QUOTA UNLIMITED ON SYSTEM IDENTIFIED BY OE; \n GRANT CONNECT, RESOURCE TO OE; \n The  OE user gets created and the roles get granted as shown in Figure  6-30 . \n Figure 6-29.  Starting SQL*Plus \n Figure 6-30.  Connecting as SYSDBA and creating a User \n \n \n",
      "page_number": 153
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 177-184)",
      "start_page": 177,
      "end_page": 184,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n162\n Creating a Database Table \n Create a database table called  OE.Catalog with the following SQL statement. \n CREATE TABLE OE.Catalog(CatalogId INTEGER PRIMARY KEY,Journal VARCHAR2(25),Publisher \nVARCHAR2(25),Edition VARCHAR2(25),Title VARCHAR2(45),Author VARCHAR2(25)); \n Add a row of data to the OE.Catalog table with the following SQL statement. \n INSERT INTO OE.Catalog VALUES('1','Oracle Magazine','Oracle Publishing',\n'November December 2013','Engineering as a Service','David A. Kelly'); \n The  OE.Catalog table gets created and a row of data gets added as shown in Figure  6-31 . \n Run the following SQL statement to query the  OE.CATALOG table. \n SELECT * FROM OE.CATALOG; \n The single row of data added gets listed as shown in Figure  6-32 . \n Figure 6-31.  Creating a Database Table \n \n\n\nCHAPTER 6 ■ USING ORACLE DATABASE\n163\n Figure 6-32.  Querying the Database Table \n Exiting the Interactive Shell \n Logout from SQL*Plus command with the “exit” command and exit the “oracle” user with the “exit” \ncommand and exit the interactive terminal with the “exit” command also as shown in Figure  6-33 . \n Summary \n In this chapter we used Kubernetes to create and orchestrate an Oracle Database cluster. We discussed both \nthe imperative and declarative approaches to creating and managing a cluster. Using the imperative method, \nthe  kubectl commands may be used directly without a definition file to create a replication controller and a \nservice. With the declarative method definition files for a Pod, replication controller and service have to be \nused. We demonstrated scaling a cluster. We also used a Docker container to log in to SQL*Plus and create a \ndatabase table. In the next chapter we shall discuss using MongoDB with Kubernetes. \n Figure 6-33.  Exiting the Interactive Shell \n \n \n\n\n   PART III \n NoSQL Database   \n\n\n167\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_7\n CHAPTER 7 \n Using MongoDB Database \n MongoDB is a flexible schema model NoSQL data store, the most commonly used NoSQL data store. \nMongoDB is based on the BSON (binary JSON) storage model. Documents are stored in collections. Being a \nschema-free data store, no two documents need to be alike in terms of the fields in a BSON document. In a \nlarge scale cluster several instances of MongoDB could be running and several issues could arise.\n -MongoDB instances scheduling \n -Scaling the MongoDB Cluster \n -Load Balancing \n -Providing MongoDB as a Service \n While Docker has made it feasible to provide Container as a Service (CaaS) it does not provide by itself \nany of the features listed previously. In this chapter we discuss using Kubernetes container cluster manager \nto manage and orchestrate a cluster of Docker containers running MongoDB. This chapter has the following \nsections.\n Setting the Environment \n Creating a MongoDB Cluster Declaratively \n Creating a MongoDB Cluster Imperatively \n Setting the Environment \n The following software is required for this chapter. \n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n -Docker image for MongoDB (latest version) \n Install the required software on an Amazon EC2 instance running Ubuntu 14; the same AMI is used \nas in the other chapters. SSH Login to the Ubuntu instance using the Public IP Address, which would be \ndifferent for different users. \n ssh -i \"docker.pem\" ubuntu@52.91.190.195 \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n168\n The  Ubuntu instance gets logged into as shown in Figure  7-1 . \n The procedure to install is discussed in chapter  1 . To verify that Docker is running run the following \ncommand. \n sudo service docker start \n Docker should be listed as running as shown in Figure  7-2 . \n Figure 7-1.  Logging into Ubuntu Instance on Amazon EC2 \n Figure 7-2.  Starting Docker \n List the Pods with the following command. \n kubectl get pods \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n169\n And list the nodes with the following command. \n kubectl get nodes \n The  Kubernetes Pod gets listed and the node also gets listed as shown in Figure  7-3 . \n To list the services run the following command. \n kubectl get services \n The “kubernetes” service gets listed as shown in Figure  7-4 . \n Creating a MongoDB Cluster Declaratively \n In the following subsections we shall create a Kubernetes service and replication controller for a MongoDB \ncluster. We shall scale the cluster and also demonstrate features such as using a volume and a host port. \nWe shall create a MongoDB collection and add documents to the collection in a Mongo shell running in a \nDocker container tty (interactive terminal or shell). \n Creating a Service \n Create a service definition file  mongo-service-yaml . Add the following (Table  7-1 ) field mappings in the \ndefinition file. \n Figure 7-3.  Listing Kubernetes Pod and the single Node \n Figure 7-4.  Listing the Kubernetes Service \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n170\n Copy the following listing to the  mongo-service.yaml . \n apiVersion: v1 \n kind: Service \n metadata: \n  labels: \n    name: mongo \n  name: mongo \n spec: \n  ports: \n    - port: 27017 \n      targetPort: 27017 \n  selector: \n    name: mongo \n The vi editor could be used to create the  mongo-service.yaml file and saved using the :wq command as \nshown in Figure  7-5 . \n Table 7-1.  Service Definition File mongo-service-yaml File Fields \n Field \n Value \n Description \n apiVersion \n v1 \n The API version. \n kind \n Service \n Specifies the definition file to be a service. \n metadata \n The service metadata. \n metadata- > labels \n name: mongo \n metadata- > name \n mongo \n A label mapping. A label may be added multiple times and \ndoes not generate an error and has no additional significance. \n spec \n The service specification. \n spec- > ports \n The port/s on which the service is exposed. \n spec- > ports- > port \n 27017 \n The port on which the service is hosted. \n spec- > ports- > targetPort  27017 \n The port that an incoming port is mapped to. The targetPort \nfield is optional and defaults to the same value as the port \nfield. The targetPort could be useful if the service is to evolve \nwithout breaking clients’ settings. For example, the targetPort \ncould be set to a string port name of a back-end Pod, which \nstays fixed. And the actual port number the back-end Pod \nexposes could be varied without affecting the clients’ settings. \n selector \n name: mongo \n The service selector used to select Pods. Pods with label \nexpression the same as the selector are managed by the \nservice. \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n171\n The default service type is  ClusterIp , which uses a cluster-internal IP only. The type could be set to \n LoadBalancer as shown in Figure  7-6 to also expose the service on each of the nodes in the cluster and also \nrequests the cloud provider to provision a load balancer. \n Figure 7-5.  Service Definition File in vi Editor \n \n",
      "page_number": 177
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 185-198)",
      "start_page": 185,
      "end_page": 198,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n172\n To create the service from the definition file run the following command. \n kubectl create -f mongo-service.yaml \n List the services with the following command. \n kubectl get services \n The  mongo service gets listed as shown in Figure  7-7 . \n Figure 7-6.  Setting the Service Type \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n173\n Creating a Replication Controller \n In this section we shall create a replication  controller . Create a definition file  mongo-rc.yaml . Add the \nfollowing (Table  7-2 ) field mappings to the definition file. \n Figure 7-7.  Creating the Service from the Definition File \n Table 7-2.  Replication Controller Definition File Fields \n Field \n Value \n Description \n apiVersion \n v1 \n The API version. \n kind \n ReplicationController \n Specifies the definition file to be \nfor a replication controller. \n metadata \n Specifies the metadata for the \nreplication controller. \n metadata - > labels \n name: mongo \n The labels mapping for the \nreplication controller. \n metadata - > name \n mongo-rc \n The replication controller name. \n spec \n The replication controller \nspecification. \n spec- > replicas \n 2 \n The number of replicas to keep \nat all times. \n spec- > template \n The template for a Pod. \n spec- > template- > metadata \n The metadata for the Pod. \n spec- > template- > metadata- > labels \n The Pod labels. The labels \nare used by the replication \ncontroller and service to select \nPods to manage. The selector \nin a replication controller and a \nservice must match a Pod label \nfor the replication controller and \nService to managed the Pod. \n spec- > template- > metadata- > labels- > name \n mongo \n A Pod label. \n spec- > template- > spec \n The specification for the Pod. \n(continued)\n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n174\n Each of the Pod, Service, and Replication Controllers are defined in a separate YAML mapping file. \nThe  mongo-rc.yaml is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  labels: \n    name: mongo \n  name: mongo-rc \n spec: \n  replicas: 2 \n  template: \n    metadata: \n      labels: \n        name: mongo \n    spec: \n      containers: \n        - \n          image: mongo \n          name: mongo \n          ports: \n            - \n              containerPort: 27017 \n              name: mongo \nTable 7-2. (continued)\n Field \n Value \n Description \n spec- > template- > spec- > containers \n The containers in a Pod. \nMultiple containers could \nbe specified but we have \nconfigured only one container. \n spec- > template- > spec- > containers - > image \n mongo \n The container for “mongo” \nDocker image. \n spec- > template- > spec- > containers - > name \n mongo \n The container name. \n spec- > template- > spec- > containers - > ports \n The container ports to reserve. \n spec- > template- > spec- > containers \n- > ports- > name \n mongo \n The port name. \n spec- > template- > spec- > containers - > ports \n- > containerPort \n 27017 \n The container port number. \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n175\n The  mongo-rc.yaml file may be edited in a vi editor and saved with :wq as shown in Figure  7-8 . \n To create a replication controller from the definition file, run the following command. \n kubectl create -f mongo-rc.yaml \n The  mongo-rc replication controller gets created as shown in Figure  7-9 . \n Run the following command to list the replication containers. \n kubectl get rc \n Figure 7-8.  Replication Controller Definition File \n Figure 7-9.  Creating the Replication Controller \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n176\n The  mongo-rc replication controller gets listed as shown in Figure  7-10 . \n Creating a Volume \n Kubernetes supports  volumes . A  volume is a directory in a Pod that is accessible to containers in the Pod that \nprovide a volume mount for the volume. Volumes persist as long as the Pod containing the volumes exists. \nVolumes are useful for the following purposes. \n -Persist data across container crash. When a container that mounts a volume \ncrashes, the data in the volume is not deleted as the volume is not on the \ncontainer but is on the Pod. \n -Data in a volume may be shared by multiple containers that mount the volume. \n A volume in a Pod is specified with the spec- > volume field. A container mounts a volume with the \n spec.containers.volumeMounts field. Several types of volumes are supported, some of which are discussed \nin Table  7-3 . \n Next, we shall add a volume of type  emptyDir to the replication controller definition file  mongo-rc.yaml . \nA modified version of  mongo-rc.yaml is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  labels: \n    name: mongo \n  name: mongo-rc \n Figure 7-10.  Creating and isting Replication Controllers \n Table 7-3.  Types of  Volumes \n Volume Type \n Description \n emptyDir \n An empty directory in the Pod that could be used to keep some files used \nby one or more containers. An empty directory could also be used for \ncheckpointing. \n hostPath \n Mounts a directory from the host node into the Pod. \n gcePersistentDisk \n Mounts a Google Compute Engine Persistent disk into a Pod. \n awsElasticBlockStore \n Mounts an Amazon Web Services EBS volume into a Pod. \n gitRepo \n Mounts a git repo into the pod. \n flocker \n Mounts a Flocker dataset into a pod. \n nfs \n Mounts a Network File System into a Pod. \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n177\n spec: \n  replicas: 2 \n  template: \n    metadata: \n      labels: \n        name: mongo \n    spec: \n      containers: \n        - \n          image: mongo \n          name: mongo \n          ports: \n            - \n              containerPort: 27017 \n              name: mongo \n          volumeMounts: \n            - \n              mountPath: /mongo/data/db \n              name: mongo-storage \n      volumes: \n        - \n          emptyDir: {} \n          name: mongo-storage \n The preceding definition file includes the following volume  configuration for a volume named \n mongo-storage of type  emptyDir . \n      volumes: \n        - \n          emptyDir: {} \n          name: mongo-storage \n The volume exists in the Pod and individual containers in the Pod may mount the volume using field \n spec->containers->volumeMounts . The modified  mongo-rc.yaml includes the following volume mount for \nthe  mongo container. \n volumeMounts: \n            - \n              mountPath: /mongo/data/db \n              name: mongo-storage \n The preceding configuration adds a volume mount for the  mongo-storage volume at mount path \nor directory path  /mongo/data/db in the container. Within a container the volume may be accessed at \n /mongo/data/db . For example, in an interactive terminal for a container change directory (cd) to the \n /mongo/data/db directory. \n cd /mongo/data/db \n List the files and directories in the in the  /mongo/data/db directory. \n ls -l \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n178\n The directory is  empty as it is supposed to be initially as shown in Figure  7-11 . \n The volume should not be confused with the data directory for the MongoDB server. The data directory \nis created at  /data/db by default and is created in each Docker container running a MongoDB server \ninstance. The  /mongo/data/db is common to all Docker containers while the  /data/db exists in each \nDocker container. \n Listing the Logs \n After having started a replication controller, list the Pods with the following command. \n kubectl get pods \n The two Pods get listed as shown in Figure  7-12 . \n The logs for a Pod, for example, the  mongo-rc-4t43s Pod, may be listed with the following command. \n kubectl logs mongo-rc-4t43s \n Figure 7-11.  Empty Directory \n Figure 7-12.  Listing the Pods \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n179\n The Pod logs show the MongoDB server starting as shown in Figure  7-13 . \n Figure 7-13.  Listing the Pod Logs \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n180\n Starting the Interactive Shell for Docker Container \n In this section we shall start an  interactive terminal or bash shell for MongoDB server for which we need the \ncontainer id of a Docker container running a MongoDB server. List the Docker containers. \n sudo docker ps \n Figure 7-14.  MongoDB Running on Port 27017 \n When the MongoDB server gets started, the message “waiting for connections on port 27017” gets \noutput as shown in Figure  7-14 . \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n181\n Because the “mongo” Docker image is based on the “debian” Docker image as speciifed in the FROM \ninstruction, we are able to start a bash shell to interact with the MongoDB server running in a Docker \ncontainer based on the “mongo” image. Start an interactive bash shell using the following command. \n sudo docker exec -it 00c829e0a89d bash \n An interactive shell gets started as shown in Figure  7-16 . \n Figure 7-15.  Copying Docker Container ID \n Figure 7-16.  Starting an Interactive Shell \n Copy the container id for a container with image as “mongo” as shown in Figure  7-15 . \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n182\n Starting a Mongo Shell \n Start the  Mongo shell with the following command. \n mongo \n Mongo shell gets started as shown in Figure  7-17 . \n Creating a Database \n List the  databases with the following command from the Mongo shell. \n show dbs \n A database gets created implicitly when a database is used or set. For example, set the database to use \nas  mongodb , which is not listed with  show dbs and does not exist yet. \n use mongodb \n But, setting the database to use as  mongodb does not create the database  mongodb till the database is \nused. Run the following command to list the databases. \n show dbs \n Figure 7-17.  Mongo Shell \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n183\n The  mongodb database does not get listed as shown in Figure  7-19 . To create the  mongodb database, \ninvoke some operation on the database such as create a collection called  catalog with the following \ncommand. \n db.createCollection(\"catalog\") \n Subsequently list the databases again. \n show dbs \n The  mongodb database gets listed as shown in Figure  7-18 . To list the collections run the following \ncommand. \n show collections \n The  catalog collection gets listed. \n Creating a Collection \n The  catalog collection was created using the  db.createCollection method in the previous section. As \nanother example, create a capped collection called  catalog_capped using the following command: a capped \ncollection is a fixed size collection that supports high throughput operations to add and get documents \nbased on insertion order. \n db.createCollection(\"catalog_capped\", {capped: true, autoIndexId: true, size: 64 * 1024, \nmax: 1000} ) \n A  capped collection gets added as shown in Figure  7-19 . Initially the collection is empty. Get the \ndocuments in the  catalog collection with the following command. \n db.catalog.count() \n The document count is listed as 0 as we have not yet added any documents. \n Figure 7-18.  Creating and Listing a MongoDB Database \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n184\n Adding Documents \n In this section we shall  add documents to the catalog collection. Specify the JSON for the documents to be \nadded. The  _id field is required in each document stored in MongoDB. The  _id field may be added explicitly \nas in the  doc2 document. If not provided in the document JSON the  _id is generated automatically. \n doc1 = {\"catalogId\" : \"catalog1\", \"journal\" : 'Oracle Magazine', \"publisher\" : \n'Oracle Publishing', \"edition\" : 'November December 2013',\"title\" : 'Engineering as a \nService',\"author\" : 'David A. Kelly'} \n doc2 = {\"_id\": ObjectId(\"507f191e810c19729de860ea\"), \"catalogId\" : \"catalog1\", \"journal\" \n: 'Oracle Magazine', \"publisher\" : 'Oracle Publishing', \"edition\" : 'November December \n2013',\"title\" : 'Engineering as a Service',\"author\" : 'David A. Kelly'}; \n The doc1 and doc2 are shown in Figure  7-20 . \n Figure 7-19.  Creating a Capped Collection \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n185\n To add the documents to the  catalog collection run the following command. \n db.catalog.insert([doc1, doc2], { writeConcern: { w: \"majority\", wtimeout: 5000 }, \nordered:true }) \n As indicated by the  nInserted field in the JSON result in Figure  7-21 documents get added. \n Figure 7-20.  Documents doc1 and doc2 \n \n",
      "page_number": 185
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 199-208)",
      "start_page": 199,
      "end_page": 208,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n186\n Finding Documents \n To query the catalog invoke the  find()  method . To list all documents in the  catalog collection run the \nfollowing command. \n db.catalog.find() \n The two documents added get listed as shown in Figure  7-22 . For one of the documents the  _id field is \ngenerated automatically. \n Finding a Single Document \n To find a single document from the  catalog collection run the following command to invoke the  findOne() \nmethod. \n db.catalog.findOne() \n Figure 7-21.  Adding Documents \n Figure 7-22.  Finding Documents \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n187\n A single document gets listed as shown in Figure  7-23 . \n Figure 7-23.  Finding a Single Document \n Figure 7-24.  Finding Selected Fields in a Document \n Finding Specific Fields in a Single Document \n To get only specific fields,  edition, title , and  author , for example, from a single document run the \nfollowing command. \n db.catalog.findOne(\n    { }, \n { edition: 1, title: 1, author: 1 } \n ) \n Only the specific fields in a single document get listed as shown in Figure  7-24 . The  _id field always \ngets listed. \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n188\n Dropping a Collection \n To  drop the  catalog collection run the following command. \n db.catalog.drop() \n Subsequently list the collections with the following command. \n show collections \n The  catalog collection does not get listed and only the  catalog_capped collection gets listed as shown \nin Figure  7-25 . \n Exiting Mongo Shell and Interactive Shell \n To  exit the Mongo shell run the following command. \n exit \n To exit the interactive terminal run the following command. \n exit \n The Mongo shell and the interactive terminal get exited as shown in Figure  7-26 . \n Scaling the Cluster \n To scale the Mongo cluster run the  kubectl scale command. For example, the following command scales \nthe cluster to 4 replicas. \n kubectl scale rc mongo --replicas=4 \n Figure 7-25.  Dropping the catalog Collection \n Figure 7-26.  Exiting the Shells \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n189\n An output of “scaled” as shown in Figure  7-27 scales the cluster to 4 replicas. \n List the Pods after scaling. \n kubectl get pods \n The four Pods get listed. Initially some of the Pods could be listed as not in READY (1/1) state. Run the \npreceding command multiple times to list all pods running and ready as shown in Figure  7-28 . \n Deleting the Replication Controller \n To  delete a replication controller  mongo-rc run the following command. \n kubectl delete replicationcontroller mongo-rc \n All the Pods managed by the replication controller also get deleted. Subsequently run the following \ncommand to list the Pods. \n kubectl get pods \n Figure 7-27.  Scaling a Replication Controller \n Figure 7-28.  Listing the Pods after Scaling \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n190\n The  mongo Pods do not get listed as shown in Figure  7-29 . \n Deleting the Service \n To delete the service called  mongo run the following command. \n kubectl delete service mongo \n The  mongo service does not get listed as shown in Figure  7-30 . \n Using a Host Port \n The container specification within a Pod has the provision to configure a host port. A  host port is a container \nport mapping to the host implying that the specified host port gets reserved for a single container The \n hostPort field should be used for a single machine container. Multiple containers of the type in which the \n hostPort is specified cannot be started because the host port can be reserved only by a single container. \nOther Pods that do not specify a  hostPort field could be run, however, on the same machine on which a \ncontainer with  hostPort field mapping is running. As a variation of the replication controller we used earlier \nadd a  hostPort field in the spec- > containers- > ports field. The modified  mongo-rc.yaml is listed. \n --- \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  labels: \n    name: mongo \n  name: mongo-rc \n spec: \n  replicas: 2 \n  template: \n    metadata: \n Figure 7-29.  Deleting a Replication Controller \n Figure 7-30.  Deleting the mongo Service \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n191\n      labels: \n        name: mongo \n    spec: \n      containers: \n        - \n          image: mongo \n          name: mongo \n          ports: \n            - \n              containerPort: 27017 \n              hostPort: 27017 \n              name: mongo \n Run the following command to create a replication controller. \n kubectl create -f mongo-rc.yaml \n List the replication controllers with the following command. \n kubectl get rc \n The  mongo-rc replication controller gets created and listed as shown in Figure  7-31 . \n List the Pods with the following command. \n kubectl get pods \n Only one of the two replicas is listed as Running and READY (1/1). Even if the preceding command is \nrun multiple times, only one replica is listed as running as shown in Figure  7-32 . \n Figure 7-31.  Creating a Replication Controller from a Definition File \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n192\n Scale the MongoDB cluster to 4 replicas with the following command. \n kubectl scale rc mongo --replicas=4 \n Though the output from the command is “Scaled” and 4 Pods get created but only one Pod is in \nREADY (1/1) state at any particular time as shown in Figure  7-33 . \n Figure 7-32.  Listing the Pods after creating a Replication Controller \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n193\n Even if the single running Pod is stopped only one new Pod gets started. To demonstrate, stop the single \nrunning Pod. \n kubectl stop pod mongo-rc-laqpl \n The Pod gets removed but a replacement Pod gets created to maintain the replication level of 1 as \nshown in Figure  7-34 . \n Figure 7-33.  Scaling the Replication Controller to 4 Replicas \n Figure 7-34.  Another Pod gets created when the single running Pod is stopped \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n194\n List the Pods again after a few seconds and only one Pod gets listed as shown in Figure  7-35 . \n Using the  hostPort field is not recommended unless a single container machine is to be used or only a \nsingle container is required to be mapped to the host port. \n Creating a MongoDB Cluster Imperatively \n In the following subsections we shall create a Kubernetes replication controller and service for a MongoDB \n cluster on the command line using kubectl. \n Creating a Replication Controller \n To create a replication  controller for the Docker image “mongo” with 2 replicas and port 27017 run the \nfollowing command. \n kubectl run mongo --image=mongo --replicas=2 --port=27017 \n The replication controller gets created as shown in Figure  7-36 . \n List the Pods with the following command. \n kubectl get rc \n Figure 7-36.  Creating a Replication Controller Imperatively \n Figure 7-35.  Only a single Pod is Running and Ready \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n195\n The  mongo -rc gets listed as shown in Figure  7-37 . \n Listing the Pods \n List the  Pods with the following command. \n kubectl get pods \n The two Pods started for MongoDB get listed as shown in Figure  7-38 . Initially the Pods could be listed \nas not running. \n Run the following preceding multiple times if required to list the Pods as running as shown in Figure  7-39 . \n Figure 7-37.  Listing the Replication Controllers \n Figure 7-38.  Listing the Pods with some of the pods not Running yet \n Figure 7-39.  Listing all the Pods as Running \n \n \n \n",
      "page_number": 199
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 209-216)",
      "start_page": 209,
      "end_page": 216,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n196\n Listing the Logs \n List the  logs for a Pod with the following command. The  mongo-56850 is the Pod name. \n kubectl logs mongo-56850 \n The Pod logs get listed as shown in Figure  7-40 . \n MongoDB is listed as started as shown in Figure  7-41 . Output on commands run on the server also \nget output. \n Figure 7-40.  Listing Pod Logs \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n197\n Creating a Service \n To create a  service for the  mongo replication controller run the following command to expose a service on \nport 27017 of type  LoadBalancer , which was discussed earlier. \n kubectl expose rc mongo --port=27017 --type=LoadBalancer \n Figure 7-41.  Listing MongoDB Server as running and waiting for connections on port 27017 \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n198\n The  mongo service gets created as shown in Figure  7-42 . \n List the services with the following command. \n kubectl get services \n The  mongo service is listed as running in Figure  7-43 . \n An interactive terminal and a Mongo shell may get started to create a MongoDB database and collection \nto add and query documents in the collection as discussed when creating a MongoDB cluster declaratively. \n Scaling the Cluster \n To scale the cluster to 4 replicas, for example, run the following command. \n kubectl scale rc mongo --replicas=4 \n An output of “scaled” indicates that the cluster has been scaled as shown in Figure  7-44 . \n Subsequently get the Pods. \n kubectl get pods \n Figure 7-42.  Creating a Service Imperatively \n Figure 7-43.  Listing the Services including the mongo Service \n Figure 7-44.  Scaling the Cluster created Imperatively \n \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n199\n Four pods get listed as shown in Figure  7-45 . Initially some of the Pods could be shown as not running \nor ready. \n To describe the  mongo service run the following command. \n kubectl describe svc mongo \n The service description includes the service label, selector in addition to the service endpoints, one for \neach of the four pods as shown in Figure  7-46 . \n Figure 7-45.  Listing Pods after Scaling \n Figure 7-46.  Describing the Service mongo after Scaling \n \n \n\n\nCHAPTER 7 ■ USING MONGODB DATABASE\n200\n Deleting the Service and Replication Controller \n The  mongo service and the  mongo replication controller may be  deleted with the following commands. \n kubectl delete service mongo \n kubectl delete rc mongo \n The “mongo” service and the “mongo” replication controller get deleted as shown in Figure  7-47 . \nDeleting one does not delete the other; the decoupling of the replication controller from the service is a \nfeature suitable to evolve one without having to modify the other. \n Summary \n In this chapter we used the Kubernetes cluster manager to create and orchestrate a MongoDB cluster. We \ncreated a replication controller and a service both imperatively and declaratively. We also demonstrated \nscaling a cluster. We introduced two other features of Kubernetes replication controllers: volumes and \nhost port. This chapter is about using Kubernetes with MongoDB and the emphasis is less on MongoDB; \nbut if MongoDB is to be explored in more detail, refer to the Apress book  Pro MongoDB Development \n( http://www.apress.com/9781484215999?gtmf=s ). In the next chapter we shall discuss another NoSQL \ndatabase, Apache Cassandra. \n Figure 7-47.  Deleting the Service and the Replication Controller \n \n\n\n201\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_8\n CHAPTER 8 \n Using Apache Cassandra Database \n Apache Cassandra is an open source wide column data store. Cassandra is a scalable, reliable, fault-tolerant, \nand highly available NoSQL database. Cassandra is based on a  flexible schema data model in which data is \nstored in rows in a table (also called column family) with a primary key identifying a row. The primary key \ncould be a single column or multiple column (compound) row key. A relational database also stores data in \ntable rows, but what makes Cassandra different is that the table rows do not have to follow a fixed schema. \nEach row in a table could have different columns or some of the columns could be the same as other rows. \nEach row does not have to include all the columns or any column data at all. In this regard Cassandra \nprovides a  dynamic column specification . A keyspace is a namespace container for the data stored in \nCassandra. In this chapter we shall discuss using Kubernetes cluster manager with Apache Cassandra. \nThis chapter has the following sections.\n Setting the Environment \n Creating a Cassandra Cluster Declaratively \n Creating a Cassandra Cluster Imperatively \n Setting the Environment \n The following software is required for this chapter.\n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n -Docker image for Apache Cassandra (latest version) \n Install the software on an  Amazon EC2 instance created from Ubuntu Server 14.04 LTS (HVM), SSD \nVolume Type - ami-d05e75b8 AMI as explained in chapter  1 . SSH Login to the Ubuntu instance using the \nPublic IP Address of the Amazon EC2 instance. \n ssh -i \"docker.pem\" ubuntu@52.23.160.7 \n Start the Docker engine and verify its status. \n sudo service docker start \n sudo service docker status \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n202\n The  Docker engine should be running as shown in Figure  8-1 . \n List the services. \n kubectl get services \n The “ kubernetes ” service should be listed as shown in Figure  8-2 . \n List the Pods and the nodes with the following commands. \n kubectl get pods \n kubectl get nodes \n Initially the only pod running is the Kubernetes pod as shown in Figure  8-3 . \n A Cassandra cluster may be created and managed both declaratively and imperatively, and we shall \ndiscuss both the options. \n Figure 8-1.  Starting Docker \n Figure 8-2.  Listing the “kubernetes” Service \n Figure 8-3.  Listing the Pod and Node for Kubernetes \n \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n203\n Creating a Cassandra Cluster Declaratively \n In the following subsections we have discussed creating a Cassandra cluster using definition files based on \nthe YAML format. First, create a service to represent a Cassandra cluster. A service is the external interface \nfor a cluster of Pods, Apache Cassandra pods in the context of this chapter. \n Creating a  Service \n Create a service definition file called  cassandra-service.yaml . Add the fields discussed in Table  8-1 . \n The  cassandra-service.yaml is listed below. Use the YAML Lint ( http://www.yamllint.com/ ) to \nvalidate the syntax. \n apiVersion: v1 \n kind: Service \n metadata: \n  name: cassandra \n  labels: \n    app: cassandra \n Table 8-1.  Fields in the Service Definition File \n Field \n Description \n Value \n apiVersion \n API Version. \n v1 \n kind \n Kind of the definition file. \n Service \n metadata \n Metadata of the service. \n metadata - > name \n Service name. Required field. \n cassandra \n metadata - > labels \n Service labels. A label could be any key- > value pair. A service \nlabel is set as app:cassandra. \n app:cassandra \n spec \n The service specification. \n spec - > labels \n The spec labels. A label could be any key- > value pair. The service \nlabel is set as app:Cassandra. \n app:cassandra \n spec - > selector \n Service selector. Used to select Pods to manage. Pods with a label \nthe same as the selector expression are selected or managed by \nthe service. The selector expression could be any key:value pair. \nOr, multiple requirements or expressions could be specified \nusing a ‘,’. The app:cassandra setting translates to service selector \napp = cassandra. \n app:cassandra \n spec - > ports \n The service ports. The ports field is required. \n spec - > ports - > port \n A single service port at which the service is exposed for access by \nexternal clients. \n 9042 \n spec - > type \n The service type. \n LoadBalancer \n",
      "page_number": 209
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 217-230)",
      "start_page": 217,
      "end_page": 230,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n204\n spec: \n  labels: \n    app: cassandra \n  selector: \n    app: cassandra \n  ports: \n    - \n      port: 9042 \n  type: LoadBalancer \n The  cassandra-service.yaml file may be created in a vi editor and saved using the :wq command as \nshown in Figure  8-4 . \n Figure 8-4.  Service Definition File in vi Editor \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n205\n To create a service run the following command. \n kubectl create -f cassandra-service.yaml \n Subsequently list the services. \n kubectl get services \n The  cassandra service gets listed as shown in Figure  8-5 . \n Describe the  cassandra service with the following command. \n kubectl describe svc cassandra \n The service name, namespace, labels, selector, type, IP, Port, NodePort and endpoints get listed as \nshown in Figure  8-6 . No service endpoint is listed initially because a Pod has not been created yet. \n Figure 8-5.  Creating and listing a Service for Apache Cassandra \n Figure 8-6.  Describing the Service for Apache Cassandra \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n206\n Creating a Replication Controller \n Next, we shall create a replication  controller for Cassandra. A replication controller defines the configuration \nfor the containers and their respective Docker images in the Pod. Create a definition file  cassandra-rc.yaml \nand add the following (Table  8-2 ) fields. \n Table 8-2.  Fields in the Replication Controller Definition File \n Field \n Description \n Value \n apiVersion \n The API Version. \n v1 \n kind \n Kind of definition file. \n ReplicationController \n metadata \n Replication controller metadata. \n metadata - > labels \n Replication controller labels. The key:value pair \napp:cassandra is set as a label on the replication \ncontroller. \n app:cassandra \n spec \n The replication controller specification. \n spec - > replicas \n The number of replicas. \n 1 \n spec- > selector \n The selector expression for the replication controller. \nMust be the same as one of the labels in the spec \n- > template - > metadata - > labels field. Required \nfield but not required to be set explicitly and defaults \nto the labels in spec - > template - > metadata \n- > labels field. If multiple requirements are set in \nthe selector multiple labels in the Pod template \nlabels must match. For example if the  selector is \n app=cassandra,name=cassandra the Pod template \nlabels spec - > template - > metadata - > labels must \ninclude both of these labels. \n spec - > template \n The Pod template. Required field. \n spec - > template \n- > metadata \n Template metadata. \n spec - > template \n- > metadata - > labels \n Template labels. The key:value pair app:cassandra is \nset as a label on the Pod. A label must be set on the \ntemplate. The label setting translates to Pod label \napp=cassandra. \n app:cassandra \n spec - > template - > spec  The container specification. \n spec - > template - > spec \n- > containers \n The containers in the Pod. \n spec - > template - > spec \n- > containers - > image \n The Docker image for a container. \n cassandra \n spec - > template - > spec \n- > containers - > name \n The container name. \n cassandra \n spec - > template - > spec \n- > containers - > ports \n The container ports. \n(continued)\n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n207\n The  cassandra-rc.yaml is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  name: cassandra-rc \n  labels: \n    app: cassandra \n spec: \n  replicas: 1 \n  template: \n    metadata: \n      labels: \n        app: cassandra \n    spec: \n      containers: \n        - \n        image: cassandra \n        name: cassandra \n        ports: \n          - \n            containerPort: 9042 \n            name: cql \n          - \n            containerPort: 9160 \n            name: thrift \n The  cassandra-rc.yaml field may be created in a vi editor and saved with the :wq command as shown \nin Figure  8-7 . \n Field \n Description \n Value \n spec - > template - > spec \n- > containers - > ports \n- > containerPort \n The container port for CQL command shell. \n 9042 \n spec - > template - > spec \n- > containers - > ports \n- > name \n The port name. \n cql \n spec - > template - > spec \n- > containers - > ports \n- > containerPort \n The container port for thrift clients. \n 9160 \n spec - > template - > spec \n- > containers - > ports \n- > name \n The port name. \n thrift \nTable 8-2. (continued)\n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n208\n Create a replication controller with the following command. \n kubectl create -f cassandra-rc.yaml \n Subsequently list the replication controllers. \n kubectl get rc \n The  cassandra-rc replication controller gets created and listed as shown in Figure  8-8 . \n Figure 8-7.  Replication Controller Definition File in vi Editor \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n209\n List the Pods created by the replication controller. \n kubectl get pods \n As the number of replicas is set to 1 in the replication controller definition file, one Pod gets created \nand is listed in Figure  8-9 . The preceding command may have to be run multiple times to list the Pod as \nrunning and ready. Alternatively run the command for the first time after a few seconds of having created the \nreplication controller; by a minute all Pods should have started. \n Describe the Cassandra service. \n kubectl describe svc cassandra \n An endpoint gets listed for the Pod as shown in Figure  8-10 . When the service description was listed \nbefore creating a replication controller, no endpoint got listed. \n Figure 8-8.  Creating a Replication Controller from Definition File \n Figure 8-9.  Listing Pod/s for Apache Cassandra \n Figure 8-10.  Describing the Service after creating the Replication Controller \n \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n210\n In the preceding example we created a replication controller with the number of replicas set as 1. \nThe replication controller does not have to create a replica to start with. To demonstrate we shall create \nthe replication controller again, but with a different replicas setting. Delete the replication controller \npreviously created. \n kubectl delete rc cassandra-rc \n Modify the  cassandra-rc.yaml to set replicas field to 0 as shown in Figure  8-11 . \n Create the replication controller again with the modified definition file. \n kubectl create -f cassandra-rc.yaml \n Figure 8-11.  Setting Replicas to 0 \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n211\n Subsequently list the replicas. \n kubectl get rc \n The  cassandra-rc replication controller gets created and gets listed as shown in Figure  8-12 . \n List the Pods. \n kubectl get pods \n Because the replicas field is set to 0 the REPLICAS get listed as 0 as shown in Figure  8-13 . \n Scaling the Database \n Starting with the replication controller with 0 replicas created we  shall  scale up the cluster to a single replica. \nRun the following command to scale the Pod cluster to 1 replica. \n kubectl scale rc cassandra-rc --replicas=1 \n Subsequently list the Pods. \n kubectl get pods \n The output from the preceding commands is shown in Figure  8-14 . A “scaled” output indicates that the \ncluster has been scaled. The single Pod could take a while (a few seconds) to get started and become ready. \n Figure 8-12.  Creating the Replication Controller with Modified Definition File \n Figure 8-13.  With Replicas as 0 no Pod gets created \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n212\n Describe the  cassandra service again. \n kubectl describe svc cassandra \n A single endpoint should get listed for the Pod added as shown in Figure  8-15 . \n Describing the Pod \n To describe the  Pod run the following command. \n kubectl describe pod cassandra-rc-tou4u \n Detailed information about the Pod such as name, namespace, image, node, labels, status, IP address, \nand events gets output as shown in Figure  8-16 . The Pod label is  app=cassandra as specified in the \nreplication controller definition file. \n Figure 8-14.  Scaling the Replication Controller to 1 Pod \n Figure 8-15.  Describing the Service after Scaling the Cluster \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n213\n Starting an Interactive Shell \n As the “cassandra” Docker image inherits from the “debian” Docker image an interactive bash shell may be \nused to access a Docker container based on the cassandra image. To start an  interactive bash shell to access \nthe Cassandra server running in a Docker container, we need to obtain the container id. List the running \ncontainers. \n sudo docker ps \n All the running containers get listed as shown in Figure  8-17 . Copy the container id for the container for \nthe  cassandra image. \n Figure 8-16.  Describing the single Pod \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n214\n Using the container id start an interactive bash shell. \n sudo docker exec -it e8fc5e8ddff57 bash \n An interactive shell gets started as shown in Figure  8-18 . \n Figure 8-17.  Listing the Docker Containers \n Figure 8-18.  Starting the Interactive Shell \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n215\n Starting the  CQL Shell \n Cassandra Query Language (CQL) is the query language for Apache Cassandra. In the following sections we \nshall run CQL commands to create a keyspace and a table. Start the CQL Shell with the following command. \n cqlsh \n CQL Shell 5.0.1 gets started as shown in Figure  8-19 . \n Creating a Keyspace \n Next, create a keyspace called  CatalogKeyspace using the replication class as SimpleStrategy and replication \nfactor as 3. \n CREATE KEYSPACE CatalogKeyspace \n            WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 3}; \n A keyspace gets created as shown in Figure  8-20 . \n Altering a Keyspace \n A keyspace may be altered with the ALTER KEYSPACE command. Run the following command to alter the \nkeyspace setting replication factor to 1. \n ALTER KEYSPACE CatalogKeyspace \n          WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 1}; \n Keyspace gets altered as shown in Figure  8-21 . \n Figure 8-21.  Altering a Keyspace \n Figure 8-19.  Starting the cqlsh Shell \n Figure 8-20.  Creating a Keyspace \n \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n216\n Using a Keyspace \n To use the  CatalogKeyspace keyspace run the following command. \n use CatalogKeyspace; \n Keyspace  CatalogKeyspace gets set as shown in Figure  8-22 . \n Creating a  Table \n A table is also called a column family. Both  CREATE TABLE and  CREATE COLUMN FAMILY clauses may be used \nto create a table (column family). Create a table called  catalog using the following CQL statement. \n CREATE TABLE catalog(catalog_id text,journal text,publisher text,edition text,title \ntext,author text,PRIMARY KEY (catalog_id)) WITH compaction = { 'class' : \n'LeveledCompactionStrategy' }; \n Add two rows of data to the table using the following CQL statements. \n INSERT INTO catalog (catalog_id, journal, publisher, edition,title,author) VALUES \n('catalog1','Oracle Magazine', 'Oracle Publishing', 'November-December 2013', 'Engineering \nas a Service','David A. Kelly') IF NOT EXISTS; \n INSERT INTO catalog (catalog_id, journal, publisher, edition,title,author) VALUES \n('catalog2','Oracle Magazine', 'Oracle Publishing', 'November-December 2013', \n'Quintessential and Collaborative','Tom Haunert') IF NOT EXISTS; \n Output from the preceding commands is shown in Figure  8-23 . A Cassandra table gets created and two \nrows of data get added. \n Figure 8-22.  Setting a Keyspace to be used \n Figure 8-23.  Creating an Apache Cassandra Table \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n217\n Run the following  CQL query statement to select data from the  catalog table. \n SELECT * FROM catalog; \n The two rows of data added gets listed as shown in Figure  8-24 . \n Deleting from a Table \n To delete row/s of data run a  DELETE CQL statement. The primary key column value cannot be deleted with \n DELETE . Delete the other column values for the row with  catalog_id as ‘catalog’ with the following CQL \nstatement. \n DELETE journal, publisher, edition, title, author from catalog WHERE catalog_id='catalog1'; \n Subsequently run the following CQL query to select data from the  catalog table. \n SELECT * FROM catalog; \n As shown in Figure  8-25 only one complete row of data gets output. The other row lists only the \n catalog_id column value, and all the other column values are  null . \n Figure 8-24.  Querying an Apache Cassandra Table \n \n",
      "page_number": 217
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 231-238)",
      "start_page": 231,
      "end_page": 238,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n218\n Truncating a Table \n Truncating a  table implies removing all table data including primary key column values. Run the following \n TRUNCATE CQL statement to remove all rows. \n TRUNCATE catalog; \n Subsequently run the CQL query statement again. \n SELECT * from catalog; \n No rows get listed as shown in Figure  8-26 ; not even null values are listed after running a  TRUNCATE \nstatement. \n Dropping a Table and Keyspace \n To drop a table run the CQL statement with the  DROP TABLE clause . The  IF EXISTS clause drops the table if it \nexists but does not return an error if the table does not exist. \n DROP TABLE IF EXISTS catalog; \n Figure 8-25.  Querying Table after deleting  Data from a Row \n Figure 8-26.  Querying a Table after Truncating a Table \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n219\n Drop the  CatalogKeyspace keyspace using the  DROP KEYSPACE clause statement. The  IF EXISTS clause \ndrops the keyspace if it exists but does not return an error if the keyspace does not exist. \n DROP KEYSPACE IF EXISTS CatalogKeyspace; \n To verify that the keyspace  CatalogKeyspace has been removed, run the following statement. \n use CatalogKeyspace; \n As the  CatalogKeyspace keyspace does not exist an error gets generated as shown in Figure  8-27 . \n Creating a Volume \n In chapter  7 we introduced  volumes , how they are mounted into a Pod using volume mounts, and how they \nare accessed within a container. We introduced various types of volumes and demonstrated the  emptyDir \ntype of volume. In this section we shall use another type of volume, the  hostPath volume. The  hostPath \nvolume mounts a directory from the host into the Pod. All containers in the Pod and all Pods based on a Pod \ntemplate using a  hostPath type of volume may access the directory on the host. As a modification of the \nreplication controller used earlier, we shall add a volume of type  hostPath to the  cassandra-rc.yaml file. \nFor example, if the host directory  /cassandra/data is to be mounted in a Pod add the following volume in \nthe spec- > template field. \n volumes: \n  - \n    hostPath: \n      path: /cassandra/data \n     name: cassandra-storage \n The volume is mounted in the Pod using the same fields as a  emptyDir volume. The modified \n cassandra-rc.yaml is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  name: cassandra-rc \n  labels: \n    app: cassandra \n Figure 8-27.  Dropping a Table \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n220\n spec: \n  replicas: 1 \n  template: \n    metadata: \n      labels: \n        app: cassandra \n    spec: \n      containers: \n        - \n        image: cassandra \n        name: cassandra \n        ports: \n          - \n            containerPort: 9042 \n            name: cql \n          - \n            containerPort: 9160 \n            name: thrift \n        volumeMounts: \n          - \n            mountPath: /cassandra/data \n            name: cassandra-storage \n      volumes: \n        - \n          hostPath: \n            path: /cassandra/data \n          name: cassandra-storage \n The  cassandra-rc.yaml definition file may be edited in vi editor and saved with the :wq command as \nshown in Figure  8-28 . It is recommended to add quotes in field values. \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n221\n The host directory that is mounted into a Pod has to pre-exist. Create the  /cassandra/data directory \nand set its permissions to global (777). \n sudo mkdir –p /cassandra/data \n sudo chmod –R 777 /cassandra/data \n The output from the preceding commands is shown in Figure  8-29 . The  /cassandra/data directory \ngets created. \n Figure 8-28.  Replication Controller Definition File with a Volume of type hostPath \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n222\n Change directory (cd) to the  /cassandra/data directory on the host. \n cd /cassandra/data \n List the files and directories in the  /cassandra/data directory. \n ls –l \n Initially the  /cassandra/data is empty as shown in Figure  8-30 . Add a sample file,  cassandra.txt , to \nthe directory with the vi editor. Subsequently list the directory files and directories again. \n vi cassandra.txt \n ls –l \n As shown in Figure  8-30 the  cassandra.txt file gets listed. What the  hostPath volume does is to make \nthe  /cassandra/data directory available to all containers in the Pod. \n Create a replication controller as discussed for the definition file used previously. One Pod should get \ncreated. List the Docker containers. \n sudo docker ps \n Copy the container id for the Docker container for image “cassandra” as shown in Figure  8-31 . \n Figure 8-30.  Adding a file in the hostPath Volume Directory \n Figure 8-29.  Creating the Directory for the Volume \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n223\n Using the container id start an interactive shell. \n sudo docker exec -it 11a4b26d9a09 bash \n The interactive shell gets started as shown in Figure  8-32 . \n Figure 8-31.  Listing the Docker Containers \n Figure 8-32.  Starting an Interactive Shell \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n224\n Change directory (cd) to the  /cassandra/data directory and list the files in the directory. \n cd /cassandra/data \n ls –l \n As shown in Figure  8-33 the cassandra.txt file gets listed. The /cassandra/data directory exists on the \nhost but is accessible from a container. \n Similarly volumes of other types could be created. Following is the  volumeMounts and  volumes fields \nsettings for a AWS Volume. The  volumeID field has the format  aws://zone/volume id. \n    volumeMounts: \n        - \n          mountPath: /aws-ebs \n          name: aws-volume \n  volumes: \n      - \n        name: aws-volume \n        awsElasticBlockStore: \n              volumeID: aws://us-east-ib/vol-428ba3ae \n              fsType: ext4 \n A more complete  cassandra-rc.yaml file is shown in Figure  8-34 . \n Figure 8-33.  Accessing the Volume in a Docker Container \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n225\n Creating a Cassandra Cluster Imperatively \n If the default settings for most of the fields are to be used, creating a replication controller imperatively is the \nbetter option. \n Creating a Replication Controller \n To create a replication  controller on the command line use the  kubectl run command. For a replication \ncontroller based on the Docker image “cassandra” run the following command in which replication \ncontroller name is “cassandra” and port is 9042. The replicas is set to 1, also the default value. \n kubectl run cassandra --image=cassandra --replicas=1 --port=9042 \n Figure 8-34.  Volume of type awsElasticBlockStore in a Replication Controller Definition File \n \n",
      "page_number": 231
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 239-246)",
      "start_page": 239,
      "end_page": 246,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n226\n Subsequently list the replication controllers. \n kubectl get rc \n The “cassandra” replication controller gets created and get listed as shown in Figure  8-35 . \n To list the Pods run the following command. \n kubectl get pods \n The single Pod created gets listed as shown in Figure  8-36 . \n To describe the replication controller run the following command. \n kubectl describe rc cassandra \n The replication controller’s name, namespace, image, selector, labels, replicas, pod status, and events \nget listed as shown in Figure  8-37 . The selector defaults to “run=cassandra” for the  cassandra replication \ncontroller. \n Figure 8-35.  Creating a Replication Controller Imperatively \n Figure 8-36.  Listing the single Pod \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n227\n Figure 8-37.  Describing the Replication Controller \n Figure 8-38.  Creating a Service for Apache Cassandra Imperatively \n Creating a Service \n To expose the replication controller  cassandra as a  service , run the  kubectl expose command. The port is \nrequired to be specified and is set to 9042 for the service. \n kubectl expose rc cassandra --port=9042 --type=LoadBalancer \n The  cassandra service gets created as shown in Figure  8-38 . \n Describe the service with the following command. \n kubectl describe service cassandra \n As shown in Figure  8-39 the service name, namespace, labels, selector, type, IP, Port, NodePort, and \nEndpoint get listed. The service selector run=cassandra must be the same as the label on the Pod to manage. \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n228\n Figure 8-39.  Describing the Service \n Figure 8-40.  Scaling Down the Database Cluster to 0 Replicas \n Figure 8-41.  Listing the Pods after Scaling Down \n Scaling the Database \n To  scale the cluster, run the  kubectl scale command. An important reason, to scale the Cassandra \nreplication controller is to run more Cassandra nodes and have them join the cluster, and we demonstrated \nscaling up a cluster. But it is not always necessary to scale up a cluster. A cluster may also be scaled down. To \nscale down the cluster to 0 replicas run the following command. \n kubectl scale rc cassandra --replicas=0 \n A output of “scaled” in Figure  8-40 indicates that the cluster has been scaled down. \n List the Pods. \n kubectl get pods \n No pod gets listed as shown in Figure  8-41 . \n \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n229\n List the services with the following command. \n kubectl get services \n Scaling the cluster to 0 replicas would leave no Pod for the service to manage but the service is still \nrunning as shown in Figure  8-42 . \n But the service does not have any endpoint associated with it as shown with the  kubectl describe \ncommand in Figure  8-43 . \n Deleting the Replication Controller and Service \n To delete the replication  controller “cassandra” run the following command. \n kubectl delete rc cassandra \n Subsequently list the replication controllers. \n kubectl get rc \n Figure 8-42.  Listing the Services after Scaling Down \n Figure 8-43.  Describing the Service after Scaling Down \n \n \n\n\nCHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n230\n To delete the service “cassandra” run the following command. \n kubectl delete service cassandra \n Subsequently list the services. \n kubectl get services \n The output from the preceding commands is shown in Figure  8-44 . The replication controller and \nservice get deleted and do not get listed. \n Summary \n In this chapter we used Kubernetes to create an Apache Cassandra cluster. We used both the declarative and \nimperative approaches. We introduced the volumes in the previous chapter and in this chapter we discussed \nusing two other types of volumes: hostPath and AWS Volume. We scaled the cluster not only up but also \ndown. We demonstrated that a replication controller does not require a Pod to be running and could specify \n0 replicas. In the next chapter we shall discuss using Kubernetes cluster manager with another NoSQL \ndatabase, Couchbase. \n Figure 8-44.  Deleting the Replication Controller and the  Service \n \n\n\n231\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_9\n CHAPTER 9 \n Using Couchbase \n Couchbase is a distributed NoSQL database based on the JSON data model. Couchbase is faster than \nMongoDB and Apache Cassandra. Couchbase offers some features not available in MongoDB and \nCassandra such as a Graphical User Interface (GUI), the Couchbase Web Console. Couchbase also provides \ncommand-line tools such as  couchbase-cli ,  cbbackup ,  cbrestore , and  cbtransfer . Couchbase, being a \ndistributed database, could benefit from the cluster management provided by Kubernetes cluster manager, \nwhich is what we shall discuss in this chapter. This chapter has the following sections.\n Setting the Environment \n Creating a Couchbase Cluster Declaratively \n Creating a Couchbase Cluster Imperatively \n Setting the Environment \n We have used an Ubuntu instance on Amazon EC2 created using the same AMI as used in the other \nchapters, the Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-d05e75b8. If an instance created \nfrom the AMI already exists the same may be used. The following software is required for this chapter. \n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n -Docker image for Couchbase (latest version) \n First, we need to log in to the Ubuntu instance. Obtain the Public IP Address of the Ubuntu instance \nfrom the Amazon EC2 instance console as shown in Figure  9-1 . \n\n\nCHAPTER 9 ■ USING COUCHBASE\n232\n Use the Public IP Address log in to the Ubuntu instance. \n ssh -i \"docker.pem\" ubuntu@54.172.55.212 \n The Ubuntu instance gets logged into as shown in Figure  9-2 . \n Figure 9-1.  Getting  Public IP Address \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n233\n Start the Docker Engine and verify its status. \n sudo service docker start \n sudo service docker status \n Docker engine should be listed as running as shown in Figure  9-3 . \n Figure 9-2.  Logging into  Ubuntu Instance on Amazon EC2 \n Figure 9-3.  Starting Docker Engine \n List the running services. \n kubectl get services \n \n \n",
      "page_number": 239
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 247-255)",
      "start_page": 247,
      "end_page": 255,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 9 ■ USING COUCHBASE\n234\n The  kubernetes service should be listed as running as shown in Figure  9-4 . \n Figure 9-5.  Listing the Single Node \n Figure 9-4.  Listing the “kubernetes” Service \n List the nodes. \n kubectl get nodes \n The node should be listed with STATUS “Ready” as shown in Figure  9-5 . \n Creating a Couchbase  Cluster Declaratively \n In the following subsections we shall create a Couchbase Pod, a replication controller, and a service all using \ndefinition files. \n Creating a Pod \n A  Pod definition file is used to create a single Pod. A Pod could have 0 or more container configurations. \nCreate a definition file  couchbase.yaml . Add the following (Table  9-1 ) fields to the definition file. \n \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n235\n The  couchbase.yaml definition file is listed. \n apiVersion: v1 \n kind: Pod \n metadata: \n  labels:  \n    app: couchbaseApp \n  name: couchbase \n spec:  \n  containers: \n    -  \n      image: couchbase \n      name: couchbase \n      ports:  \n        -  \n          containerPort: 8091 \n Table 9-1.  Pod Definition File Fields \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n Pod \n metadata \n The Pod metadata. \n metadata - > labels \n The Pod labels. A service selector makes use of \nthe labels to select the Pods to manage. \n app: couchbaseApp \n metadata - > name \n The Pod name. \n couchbase \n spec \n The Pod specification. \n spec - > containers \n The containers in the Pod. \n spec - > containers - > image \n A container image. For Couchbase server the \nimage is “couchbase.” \n couchbase \n spec - > containers - > name \n The container name. \n couchbase \n spec - > containers - > ports \n The container ports. \n spec - > containers - > ports \n- > containerPort \n A container port for Couchbase server. \n 8091 \n\n\nCHAPTER 9 ■ USING COUCHBASE\n236\n The  couchbase.yaml file could be created in the vi editor and saved with the :wq command as shown in \nFigure  9-6 . \n Figure 9-6.  Pod Definition file couchbase.yaml in vi Editor \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n237\n Figure 9-7.  Creating a Pod from the Definition File \n Figure 9-8.  Listing the couchbase Pod \n Run the following command to create a Pod from the definition file.  \n kubectl create -f couchbase.yaml  \n A Pod gets created as indicated by the “pods/couchbase” output in Figure  9-7 . \n Subsequently list the Pods. \n kubectl get pods \n A Pod called “couchbase” gets listed as shown in Figure  9-7 . Initially the STATUS could be different from \n“Running” and the READY column could be not ready; 1/1 is ready state and 0/1 is not ready. \n Run the following command again after a few more seconds. \n kubectl get pods \n The  couchbase Pod is listed as “Running” and READY- > 1/1 as shown in Figure  9-8 . \n Creating a Service \n In this section we shall create a service using a  service definition file. Create a  couchbase-service.yaml file \nand add the following (Table  9-2 ) fields to the file. \n \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n238\n The  couchbase-service.yaml is listed. \n apiVersion: v1 \n kind: Service \n metadata:  \n  labels:  \n    app: couchbaseApp \n  name: couchbase \n spec:  \n  ports:  \n    -  \n      port: 8091 \n      targetPort: 8091 \n  selector:  \n    app: couchbaseApp \n  type: LoadBalancer \n Table 9-2.  Service Definition File couchbase-service.yaml \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n Service \n metadata \n The service metadata. \n metadata - > labels \n The service labels. \n app: couchbaseApp \n metadata - > name \n The service name. \n couchbase \n spec \n The service specification. \n spec - > ports \n The ports exposed by the service. \n spec - > ports - > port \n A port exposed by the service. \n 8091 \n spec - > ports \n- > targetPort \n The target port for the service, which could be a port \nnumber or the name of a port on the backend. The target \nport setting adds flexibility as the port number could be \nmodified while the port name is kept fixed. \n 8091 \n spec - > selector \n The Pod selector, which could be one or more label \nkey:value expressions/labels. All of the key:value \nexpressions in a selector must match with a Pod’s labels \nfor the Pod to be selected by the service. A Pod could have \nadditional labels but must include labels in the selector \nto be selected by the service. Service routes traffic to the \nPods with label/s matching the selector expression/s. \nOnly a single selector expression is used in the example \nservice definition file. If the selector is empty all Pods \nare selected. The app: couchbaseApp setting defaults to \nselector app = couchbaseApp. \n app: couchbaseApp \n spec - > selector - > type \n The service type. \n LoadBalancer \n\n\nCHAPTER 9 ■ USING COUCHBASE\n239\n Figure 9-9.  Listing the couchbase Service \n Create a service from the definition file with the following command. \n kubectl create -f couchbase-service.yaml \n Subsequently list the running services. \n kubectl get services \n An output of “services/couchbase” as shown in Figure  9-9 indicates that the  couchbase service has been \ncreated. The “couchbase” service gets listed, also shown in Figure  9-9 . \n List the service endpoints with the following command. \n kubectl get endpoints \n The service endpoint for the  couchbase service gets listed as shown in Figure  9-10 . \n Figure 9-10.  Listing the Endpoints \n Creating a Replication Controller \n In this section we shall create a replication  controller using a definition file. Create a  couchbase-rc.yaml file \nand add the following (Table  9-3 ) fields to the file. \n \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n240\n Table 9-3.  Definition File for Replication Controller \n Field \n Description \n Value \n Required Field \n(includes default \nsettings) \n apiVersion \n v1 \n yes \n kind \n The kind of definition file. \n ReplicationController  yes \n metadata \n The replication controller metadata. \n yes \n metadata - > labels \n The replication controller labels. \n app: couchbaseApp \n no \n metadata - > name \n The replication controller name. \n couchbase \n yes \n spec \n The replication controller \nspecification. \n yes \n spec - > replicas \n The number of Pod replicas. \nDefaults to 1 replica. \n 2 \n yes \n spec - > selector \n One or more key:value expressions \nfor selecting the Pods to manage. \nPods that include label/s with the \nsame expression/s as the selector \nexpression/s are managed by the \nreplication controller. A Pod could \ninclude additional labels but must \ninclude the ones in the selector \nto be managed by the replication \ncontroller. The selector defaults to \nthe spec - > template - > metadata \n- > labels key:value expression/s \nif not specified. A setting of app: \ncouchbaseApp translates to selector \napp = couchbaseApp. \n app: couchbaseApp \n yes \n spec - > template \n The Pod template. \n yes \n spec - > template - > metadata  The Pod template metadata. \n yes \n spec - > template - > \nmetadata - > labels \n The Pod template labels. \n app: couchbaseApp \n yes \n spec - > template - > spec \n The Pod template specification. \n yes \n spec - > template - > spec - > \ncontainers \n The containers configuration for \nthe Pod template. \n yes \n spec - > template - > \nspec - > containers - > image \n The Docker image. \n couchbase \n yes \n spec - > template - > \nspec - > containers - > name \n The container name. \n couchbase \n yes \n spec - > template - > \nspec - > containers - > ports \n The container ports. \n no \n spec - > template - > \nspec - > containers - > \nports - > containerPort \n A container port. \n 8091 \n no \n\n\nCHAPTER 9 ■ USING COUCHBASE\n241\n The  couchbase-rc.yaml is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata:  \n  labels:  \n    app: couchbaseApp \n  name: couchbase \n spec:  \n  replicas: 2 \n  selector:  \n    app: couchbaseApp \n  template:  \n    metadata:  \n      labels:  \n        app: couchbaseApp \n    spec:  \n      containers:  \n        -  \n          image: couchbase \n          name: couchbase \n          ports:  \n          -  \n            containerPort: 8091 \n The  couchbase-rc.yaml may be created in vi editor as shown in Figure  9-11 . \n\n\nCHAPTER 9 ■ USING COUCHBASE\n242\n Create the replication controller with the following command. \n kubectl create -f couchbase-rc.yaml \n Subsequently, list the replication controllers. \n kubectl get rc \n Figure 9-11.  Replication Controller Definition File couchbase-rc.yaml in vi Editor \n \n",
      "page_number": 247
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 256-263)",
      "start_page": 256,
      "end_page": 263,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 9 ■ USING COUCHBASE\n243\n An output of “replicationcontrollers/couchbase” as shown in Figure  9-12 indicates that the “couchbase” \nreplication controller has been created. The “couchbase” replication controller gets listed with the second \ncommand. The REPLICAS is listed as 2, but it does not imply that the replication controller created two new \nreplicas. The replication controller manages Pods based on selector expression matching a Pod label. If \nsome other Pod with the matching label is already running it is counted toward the replicas setting. \n Figure 9-12.  Creating and listing a Replication Controller from the Definition File \n Listing the Pods \n To list the  Pods run the following command. \n kubectl get pods \n Two Pods get listed as shown in Figure  9-13 , and one of the Pods is the Pod created earlier using \na Pod definition file. The label in the Pod definition file was app: “couchbaseApp,” which is also the \nselector expression for the replication controller. The expression app: “couchbaseApp” translates to \napp= couchbaseApp. As a result only one new Pod gets created when the replication controller with replicas \nset to 2 is created. \n Figure 9-13.  Listing the Pods for Couchbase Server \n Listing the Logs \n To list the  logs for a Pod run the  kubectl logs command. The pod name may be copied from the preceding \nlisting of Pods. \n kubectl logs couchbase-0hglx \n The output is shown in Figure  9-14 . The output indicates that the WEB UI is available at \n http://<ip>:8091 . \n Figure 9-14.  Listing Pod Logs \n \n \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n244\n Describing the Service \n To describe the  couchbase service run the following command. \n kubectl describe svc couchbase \n The service name, namespace, labels, selector, type, IP, Port, NodePort, and endpoints get listed as \nshown in Figure  9-15 . The  selector is listed as  app=couchbaseApp . \n Figure 9-15.  Describing the Service for Couchbase \n Listing the Endpoints \n List the  endpoints again. \n kubectl get endpoints \n When the endpoints were listed earlier only one endpoint was listed because only one Pod was running. \nWith two Pods running two endpoints get listed as shown in Figure  9-16 . \n Figure 9-16.  Listing the Endpoints for Couchbase \n Setting Port Forwarding \n When we listed the logs for a Couchbase Pod the URL to invoke the web console was listed as \n http://<ip>:8091 . The  < ip > is the service endpoint of the Pod. The previous section listed two service \nendpoints. Invoking either of these on a host browser, for example,  http://172.17.0.2:8091 would open \nthe web console. An Amazon EC2 Ubuntu instance does not install a web browser by default. Alternatively, \nwe shall set port forwarding to a local machine and open the web console from a browser on a local machine, \n \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n245\nwhich is required to have a browser available. To set port forwarding we need to know the Public DNS of the \nAmazon EC2 instance running Kubernetes. The Public DNS may be obtained from the Amazon EC2 console \nas shown in Figure  9-17 . \n Figure 9-17.  Obtaining the Public DNS \n The ports to forward to on the local machine must be open and not already bound. As an example, \nbind one of the endpoints to port 8093 on  localhost and the other to port 8094 on the  localhost with the \nfollowing commands. \n ssh -i \"docker.pem\" -f -nNT -L 8093:172.17.0.3:8091 ubuntu@ec2-54-172-55-212.compute-1.\namazonaws.com \n ssh -i \"docker.pem\" -f -nNT -L 8094:172.17.0.2:8091 ubuntu@ec2-54-172-55-212.compute-1.\namazonaws.com \n The  port forwarding from the service endpoints to  localhost ports gets set as shown in Figure  9-18 . \n Figure 9-18.  Setting Port Forwarding to localhost:8093 and localhost:8094 \n \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n246\n Logging into Couchbase Web Console \n Two ports are available on the local machine to open the Couchbase  web console , 8093 and 8094. \nEither or both of these could be used to open a Couchbase web console. For example, open the URL \n http://localhost:8093 in a web browser. The Couchbase Console gets opened as shown in Figure  9-19 . \nClick on Setup to set up the Couchbase server. \n Figure 9-19.  Setting Up Couchbase Server \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n247\n Configuring Couchbase Server \n In this section we shall configure the  Couchbase server , which is not directly related to using Kubernetes \nbut is discussed for completeness. When the Setup button is clicked the CONFIGURE SERVER window gets \ndisplayed as shown in Figure  9-20 . \n Figure 9-20.  Configuring Server Disk Storage, Hostname \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n248\n Keep the default settings and scroll down to select Start a new cluster. The RAM settings may have to be \nreduced if sufficient RAM is not available. Click on Next as shown in Figure  9-21 . \n Figure 9-21.  Starting New Cluster \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n249\n Some sample buckets get listed but a sample bucket is not required to be selected. Click on Next as \nshown in Figure  9-22 . \n Figure 9-22.  Sample Buckets are not required to be selected \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n250\n The Create Default Bucket settings include the Bucket Type, which should be Couchbase as shown in \nFigure  9-23 . Replicas should be enabled with the “Enable” check box. \n Figure 9-23.  Configuring Default Bucket \n \n",
      "page_number": 256
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 264-271)",
      "start_page": 264,
      "end_page": 271,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 9 ■ USING COUCHBASE\n251\n Scroll down to enable the Flush mode with the “Enable” check box. Click on Next as shown in Figure  9-24 . \n Figure 9-24.  Enabling Flush Mode and completing Server Configuration \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n252\n Next, accept the terms and conditions as shown in Figure  9-25 and click on Next. \n Figure 9-25.  Accepting Terms and Conditions \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n253\n To secure the server specify a Password and specify the same password in the Verify Password field as \nshown in Figure  9-26 . \n Figure 9-26.  Securing the Server with Username and Password \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n254\n The Couchbase server gets configured. Select the Server Nodes tab and the Server Node Name is listed \nas shown in Figure  9-27 . The Server Node Name is one of the service endpoints. \n Figure 9-27.  Server Node Name is the same as a Service Endpoint \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n255\n Adding Documents \n Next, we shall add some documents to the Couchbase server. Select the Data Buckets tab as shown in \nFigure  9-28 . \n Figure 9-28.  Selecting  Data Buckets Tab \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n256\n The default bucket gets listed as shown in Figure  9-29 . Click on Documents. \n Figure 9-29.  Clicking on Documents Button for the default Bucket \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n257\n Initially the “default” bucket is empty as shown in Figure  9-30 . \n Figure 9-30.  Initially no Documents are present in the default Data Bucket \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n258\n Click on Create Document to add a document as shown in Figure  9-31 . \n Figure 9-31.  Clicking on Create Document \n \n",
      "page_number": 264
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 272-279)",
      "start_page": 272,
      "end_page": 279,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 9 ■ USING COUCHBASE\n259\n In the Create Document dialog specify a Document Id and click on Create as shown in Figure  9-32 . \n Figure 9-32.  Specifying Document ID \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n260\n Copy and paste the following JSON document into the  catalog1 document. \n { \n  \"journal\": \"Oracle Magazine\", \n  \"publisher\": \"Oracle Publishing\", \n  \"edition\": \"November-December 2013\", \n  \"title\": \"Quintessential and Collaborative\", \n  \"author\": \"Tom Haunert\" \n } \n Click on Save to update the  catalog1 document as shown in Figure  9-34 . \n A new JSON document with  default fields gets added as shown in Figure  9-33 . \n Figure 9-33.  The catalog1 Document gets created with Default Fields \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n261\n The  catalog1 document gets saved and gets listed when the Documents link for the “default” bucket is \nselected as shown in Figure  9-35 . \n Figure 9-34.  Saving a  JSON Document \n Figure 9-35.  The catalog1 Document in default Bucket \n \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n262\n Similarly add another document with Document ID as  catalog2 and copy and paste the following \nlisting to the document. \n { \n \"journal\": “Oracle Magazine”, \n \"publisher\": \"Oracle Publishing\", \n \"edition\": \"November December 2013\", \n \"title\": \"Engineering as a Service\", \n \"author\": \"David A. Kelly\", \n } \n The  catalog2 document is shown in Figure  9-36 . \n Figure 9-36.  Adding another Document catalog2 \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n263\n The Documents link for the “default” bucket links the two documents added as shown in Figure  9-37 . \n Figure 9-37.  Listing the two Documents in the default Bucket \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n264\n Starting an Interactive Shell \n Next, we shall start and interactive bash  shell to access Couchbase server from the command line. Obtain \nthe container id for one of the Docker containers based on the Docker image “couchbase” as shown in \nFigure  9-38 . \n Figure 9-38.  Obtaining the Container Id \n Using the container id, start an interactive shell. \n sudo docker exec -it e1b2fe2f24bd bash \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n265\n Figure 9-39.  Starting an Interactive Shell \n An interactive shell gets started as shown in Figure  9-39 . \n Using the cbtransfer Tool \n From the interactive shell command-line tools may be run to access the Couchbase server. As an example \nrun the  cbtransfer tool , which is used to transfer data between clusters and to/from files, to output the \ndocuments in the default bucket at server  http://172.17.0.3:8091 to  stdout . \n cbtransfer http://172.17.0.3:8091/ stdout: \n The two documents added from the web console get output as shown in Figure  9-40 . \n Figure 9-40.  Using the cbtransfer Tool \n In the next section we shall create a Couchbase cluster imperatively using Kubernetes on the command \nline. As we shall be using the same replication controller name and service name, delete the replication \ncontroller “couchbase” and also delete the service called “couchbase.” \n kubectl delete rc couchbase \n kubectl delete svc couchbase \n\n\nCHAPTER 9 ■ USING COUCHBASE\n266\n Creating a Couchbase  Cluster Imperatively \n In the following subsections we shall create a Couchbase cluster on the command line. \n Creating a Replication Controller \n Create a replication  controller called “couchbase” using the Docker image “couchbase” with two replicas \nand container port as 8091 with the following command. \n kubectl run couchbase --image=couchbase --replicas=2 --port=8091 \n The replication controller gets created as shown in Figure  9-41 . The default selector is “run=couchbase,” \nwhich implies that pods with the label “run=couchbase” shall be managed by the replication controller. \nThe Pod labels get set to “run=couchbase”. \n Figure 9-41.  Creating a Replication Controller Imperatively \n List the replication controllers with the following command. \n kubectl get rc \n The  couchbase replication controller gets listed as shown in Figure  9-42 . \n Figure 9-42.  Listing the Replication Controllers \n Listing the Pods \n To list the  Pods run the following command. \n kubectl get pods \n The two pods get listed as shown in Figure  9-43 . \n \n \n",
      "page_number": 272
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 280-287)",
      "start_page": 280,
      "end_page": 287,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 9 ■ USING COUCHBASE\n267\n Figure 9-43.  Listing the Pods \n To describe any particular Pod run the  kubectl describe pod command, for example, the Pod \n couchbase-rd44o is described with the following command. \n kubectl describe pod couchbase-rd44o \n The Pod detail gets output as shown in Figure  9-44 . The Pod label is listed as  run=couchbase . \n Figure 9-44.  Describing a Pod \n \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n268\n Creating a Service \n To create a  service from the replication controller exposed at port 8091, run the following command, which \nalso specified the service type. \n kubectl expose rc couchbase --port=8091 --type=LoadBalancer \n Subsequently list the services. \n kubectl get services \n The  couchbase service gets created and listed as shown in Figure  9-45 . \n Figure 9-45.  Creating a Service for Couchbase Imperatively \n To describe the  couchbase service run the following command. \n kubectl describe svc couchbase \n The service name, namespace, labels, selector, type, Ip, port, node port, and endpoints get listed as \nshown in Figure  9-46 . Two endpoints are listed because the service manages two pods. \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n269\n Figure 9-46.  Describing a Service \n Scaling the Cluster \n A Couchbase cluster may be scaled up or down using the Kubernetes cluster manager. For example, to scale \ndown the replication controller called “couchbase” to 1 replica, run the following  kubectl scale command. \n kubectl scale rc couchbase --replicas=1 \n An output of “scaled” indicates that the rc has been scaled. But the “scaled” output does not always \nimply that the scaled number of replicas are running and ready. Run the following command to list the Pods. \n kubectl get pods \n A single Couchbase Pod gets listed as shown in Figure  9-47 . \n Figure 9-47.  Scaling Down the Couchbase Cluster to a Single Pod \n Run the following command to list the replication controllers and the  couchbase rc is listed with \nreplicas as 1 as shown in Figure  9-48 . \n kubectl get rc \n To scale the rc back to 2 Pods run the following command. \n kubectl scale rc couchbase --replicas=2 \n \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n270\n Subsequently list the Pods. \n kubectl get pods \n Initially the new Pod to be added could be not running or not ready but after a few seconds two Pods get \nlisted as running and ready as shown in Figure  9-48 . \n Figure 9-48.  Scaling Up the Couchbase Cluster \n Keeping the Replication Level \n The main purpose of a replication  controller is to keep the number of replicas to the configured level. With \n2 replicas configured in the  couchbase rc the number of Pods is maintained at 2. As an example, delete one \nof the Pods. \n kubectl delete pod couchbase-4z3hx \n One pod gets deleted, but it takes the total number of pods to 1, which is below the number of \nconfigured replicas. As a result the replication controller starts a new replica. Subsequently list the pods. \n kubectl get pods \n Initially the new Pod could be not running and/or not ready but after a few seconds two pods are \nrunning and ready as shown in Figure  9-49 . \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n271\n Figure 9-49.  Running the kubectl get pods Command Multiple Times until all Pods are Running and Ready \n Describe the  couchbase service. \n kubectl describe svc couchbase \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n272\n Two endpoints get listed as shown in Figure  9-50 . \n Figure 9-50.  Describing the couchbase Service \n Setting Port Forwarding \n Set  port forwarding of a service endpoint to a  localhost port, for example, port 8095, as discussed earlier. \n ssh -i \"docker.pem\" -f -nNT -L 8095:172.17.0.2:8091 ubuntu@ec2-52-91-80-177.compute-1.\namazonaws.com \n The preceding command does not generate any output as shown in Figure  9-51 . \n Figure 9-51.  Setting Port Forwarding \n Logging in to Couchbase Admin Console \n Login to the Couchbase  Web Console using the forwarded port on  localhost . \n http://localhost:8095/index.html \n The Couchbase Web Console gets displayed as shown in Figure  9-52 . \n \n \n\n\nCHAPTER 9 ■ USING COUCHBASE\n273\n Figure 9-52.  Displaying the Couchbase Console \n Summary \n In this chapter we used Kubernetes cluster manager to create a Couchbase cluster. We discussed both the \ndeclarative and imperative approaches. The declarative approach makes use of definition files and the \nimperative approach makes use of command-line configuration parameters. We demonstrated accessing \nthe Couchbase Web Console from a localhost browser using port forwarding. We also used the cbtransfer \ntool in an interactive shell for a Docker container running Couchbase server. Docker image “couchbase” is \nused to create a Couchbase server. In the next chapter we shall discuss using Kubernetes cluster manager for \nan Apache Hadoop cluster. \n \n\n\n PART IV \n Apache Hadoop Ecosystem \n  \n",
      "page_number": 280
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 288-296)",
      "start_page": 288,
      "end_page": 296,
      "detection_method": "topic_boundary",
      "content": "277\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_10\n CHAPTER 10 \n Using Apache Hadoop Ecosystem \n Apache Hadoop has evolved to be the de facto framework for processing large quantities of data. Apache \nHadoop ecosystem consists of a several projects including Apache Hive and Apache HBase. The Docker \nimage “svds/cdh” is based on the latest CDH release and includes all the main frameworks in the Apache \nHadoop ecosystem. All the frameworks such as Apache Hadoop, Apache Hive, and Apache HBase are \ninstalled in the same Docker image as a result facilitating development of applications that make use of \nmultiple frameworks from the Apache Hadoop ecosystem. In this chapter we shall discuss using Kubernetes \ncluster manager to manage a cluster of Pods based on the svds/cdh image.\n Setting the Environment \n Creating an Apache Hadoop Cluster Declaratively \n Creating an Apache Hadoop Cluster Imperatively \n  Setting the Environment \n The following software is required to be installed for this chapter, which is the same as the software used in \nother chapters except for the Docker image. \n -Docker Engine (latest version) \n -Kubernetes Cluster Manager (version 1.01) \n -Kubectl (version 1.01) \n -Docker image svds/cdh (latest version) \n Install the software as discussed in  chapter 1 on an Ubuntu instance on Amazon EC2. SSH Login to the \nUbuntu instance. \n ssh -i \"docker.pem\" ubuntu@54.86.45.173 \n Start the Docker engine with the following command. \n sudo service docker start \n Subsequently run the following command to verify the status of Docker. \n sudo service docker status \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n278\n As shown in Figure  10-1 , Docker should be listed as “running.” \n Figure 10-1.  Starting Docker \n Figure 10-2.  Listing the “kubernetes” Service \n Figure 10-3.  Listing the Pod and Node for Kubernetes \n List the services with the following command. \n kubectl get services \n The  kubernetes service should be listed as running as shown in Figure  10-2 . \n List the Pods with the following command. \n kubectl get pods \n List the nodes with the following command. \n kubectl get nodes \n The only Pod that gets listed is for Kubernetes as shown in Figure  10-3 . The node 127.0.0.1 also gets listed.  \n  Creating an Apache Hadoop Cluster Declaratively \n In the following subsections we shall create a Kubernetes service and a Kubernetes replication controller \ndeclaratively using definition files. A service is the external interface for Pods and routes client requests to one of \nthe Pods. A replication controller manages the replication level of the Pods and maintains the number of replicas \nto the specified value in the definition file. The replication controller is also used to scale the cluster of Pods. \n \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n279\n Creating a Service \n To run a  service for the CDH Pods create a service definition file  cdh-service.yaml and add the following \n(Table  10-1 ) fields to the definition file. \n Table 10-1.  Service Definition File Fields \n Field \n Description \n Value \n Required Field \n(including defaults) \n apiVersion \n v1 \n yes \n kind \n The kind of definition file. \n Service \n yes \n metadata \n The service metadata. \n yes \n metadata - > labels \n The service labels. \n app: cdh \n no \n metadata - > name \n The service name. \n cdh \n yes \n spec \n The service specification. \n yes \n spec - > ports \n The ports exposed by the service. \n yes \n spec - > ports - > port \n A port exposed by the service. The \n50010 port is for the DataNode. \n 50010 \n spec - > ports - > port \n Another port exposed by the service. \nThe 8020 port is for the NameNode. \n 8020 \n spec - > selector \n The Pod selector. Service routes traffic \nto the Pods with a label matching the \nselector expression. \n app: cdh \n yes \n spec - > selector - > type \n The service type. \n LoadBalancer \n no \n The service definition file  cdh-service.yaml is listed: \n apiVersion: v1 \n kind: Service \n metadata:  \n   labels:  \n    app: cdh \n  name: cdh \n spec:  \n   ports:  \n    -  \n      port: 50010 \n    -  \n      port: 8020 \n  selector:  \n    app: cdh \n   type: LoadBalancer \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n280\n Figure 10-4.  Service Definition File in vi Editor \n Create a service from the definition file with the following command. \n kubectl create -f cdh-service.yaml \n Subsequently list the services. \n kubectl get services \n An output of “services/cdh” from the first command indicates that the service has been created as \nshown in Figure  10-5 . The second command lists the service called “cdh.” The service selector is listed as \napp = cdh in the SELECTOR column.  \n The service definition file may be created and saved in the vi editor as shown in Figure  10-4 . \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n281\n Figure 10-5.  Creating a Service from a Definition File \n Creating a Replication Controller \n In this section we shall create a replication  controller using a definition file. Create a cdh-rc.yaml file and \nadd the following (Table  10-2 ) fields to the file. \n Table 10-2.  Replication Controller Definition File Fields \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n ReplicationController \n metadata \n The replication controller metadata. \n metadata - > labels \n The replication controller labels. \n app: cdh \n metadata - > name \n The replication controller name. \n cdh-rc \n spec \n The replication controller specification. \n spec - > replicas \n The number of Pod replicas. \n 2 \n spec - > selector \n Selector key:value expression/s for \nselecting the Pods to manage. Pods \nwith label/s the same as the selector \nexpression/s are managed by the \nreplication controller. For a single selector \nexpression the selector expression must be \nthe same as a spec - > template - > \nmetadata - > labels label. The selector \ndefaults to the spec - > template - > \nmetadata - > labels if not specified. \n Not set. Defaults to the \nsame value as the key:value \npairs in spec - > template - > \nmetadata - > labels. \n spec - > template \n The Pod template. \n spec - > template- > metadata \n The Pod template metadata. \n spec \n- > template- > metadata- > labels \n The Pod template labels. \n app: cdh \n name: cdh \n spec - > template - > spec \n The Pod template specification \n(continued)\n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n282\n The definition file for the replication controller,  cdh-rc.yaml , is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata:  \n  labels:  \n    app: cdh \n  name: cdh-rc \n spec:  \n  replicas: 2 \n  template:  \n    metadata:  \n      labels:  \n      app: cdh \n      name: cdh \n    spec:  \n      containers:  \n      image: svds/cdh \n      name: cdh \n Run the following command to create a replication controller from the definition file. \n kubectl create -f cdh-rc.yaml \n List the replication controllers. \n kubectl get rc \n The first command outputs “replicationcontrollers/cdh,” which implies that an rc has been created \nsuccessfully. The second command lists the replication controllers. The replication controller “cdh” gets \nlisted as shown in Figure  10-6 . The  SELECTOR was not specified in the replication controller file and is \nlisted as the same two key:value pairs,  app=cdh,name=cdh , as the template labels. A Pod managed by the \nreplication controller must include both of these labels, and may include additional labels. The number of \nreplicas is set to 2. \n Field \n Description \n Value \n spec - > template \n- > spec- > containers \n The containers configuration for the \nPod template \n spec - > template \n- > spec- > containers - > image \n The Docker image \n svds/cdh \n spec - > template - > spec - > \ncontainers - > name \n The container name \n cdh \nTable 10-2. (continuted)\n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n283\n Listing the Pods \n To list the  Pods run the following command. \n kubectl get pods \n Two Pods get listed as shown in Figure  10-7 . Initially the Pods could be listed as not running or/and \nnot ready. A not ready pod is indicated by the 0/1 value in the READY column, which implies that 0 of 1 \ncontainers in the Pod are rready. \n Figure 10-6.  Creating a Replication Controller from a Definition File \n Figure 10-7.  Listing the Pods for CDH, created but not Ready \n Run the same command again to list the Pods. \n kubectl get pods \n The two Pods should get listed as STATUS- > Running and READY- > 1/1 as shown in Figure  10-8 . \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n284\n Listing Logs \n To list the  logs for a particular Pod, for example, the cdh-612pr Pod, run the following command. \n kubectl logs cdh-612pr \n The output from the command lists the logs, which indicate that the Hadoop datanode, namenode, \nsecondarynamenode, resourcemanager, and nodemanager have been started as shown in Figure  10-9 . \n Figure 10-8.  Listing the Pods as Ready \n Figure 10-9.  Listing Pod Logs \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n285\n Other components such as HBase are also started. \n Scaling a Cluster \n Initially the CDH cluster has 2 replicas. To scale the replicas to 4 run the following command. \n kubectl scale rc cdh --replicas=4 \n Subsequently list the Pods in the cluster. \n kubectl get pods \n After scaling up the cluster 4 Pods get listed instead of the 2 listed initially. Some of the Pods could be \nlisted as not running or not ready. Run the preceding command after a few seconds periodically, and all the \npods should get started as shown in Figure  10-10 . \n Figure 10-10.  Scaling the Pod Cluster \n \n",
      "page_number": 288
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 297-305)",
      "start_page": 297,
      "end_page": 305,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n286\n Starting an Interactive Shell \n As the “svds/cdh” Docker image is based on the Linux “ubuntu” Docker image an interactive bash shell may \nbe started to access Docker containers based on the svds/cdh Docker image. To start an  interactive bash \nshell for the cdh software we need to obtain the container id for a Docker container running the “cdh” image \nas shown in Figure  10-11 . \n Figure 10-11.  Copying the Docker Container Id \n Subsequently start the interactive shell using the container id. \n sudo docker exec -it f1efdb5937c6 bash \n The interactive shell gets started as shown in Figure  10-12 . \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n287\n Running a MapReduce Application \n In this section we shall run an example  MapReduce application in the interactive shell. The  hdfs command \nis used to run a MapReduce application. Invoke the  hdfs command in the interactive shell. \n hdfs \n The command usage should get displayed as shown in Figure  10-13 . \n Figure 10-13.  Command Usage for  hdfs Command \n Figure 10-12.  Starting an Interactive Shell \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n288\n To change user to “hdfs” run the following command. \n su –l hdfs \n The user becomes “hdfs” as shown in Figure  10-14 . \n Figure 10-15.  Creating the Input Directory \n Figure 10-14.  Setting User as hdfs \n Next, we shall run a  wordcount application. We shall get input from the  /input directory files and \noutput in the  /output directory. Create the  /input directory and set its permissions to global (777). \n hdfs dfs -mkdir /input \n hdfs dfs -chmod -R 777 /input \n The  /input directory gets created and its permissions get set to global as shown in Figure  10-15 . \n Create an input file  input.1.txt in the vi editor. \n sudo vi input1.txt \n Add the following text to input1.txt. \n Hello World Application for Apache Hadoop \n Hello World and Hello Apache Hadoop \n The  input1.txt is shown in the  vi editor in Figure  10-16 . \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n289\n Put the  input1.txt in the HDFS directory  /input with the following command, which should be run with \n sudo –u hdfs if run as  root user. If the user is already set to “hdfs” omit the “sudo –u hdfs” from the command. \n sudo -u hdfs hdfs dfs -put input1.txt /input \n The  input1.txt file gets added to the  /input directory and no output is generated from the command \nas shown in Figure  10-17 . \n Figure 10-16.  Creating an Input Text File \n Figure 10-17.  Putting the Input Text File in HDFS \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n290\n Similarly create another file input2.txt. \n sudo vi input2.txt \n Add the following text to input2.txt. \n Hello World \n Hello Apache Hadoop \n Save the  input2.txt with the : wq command in the vi editor as shown in Figure  10-18 . \n Figure 10-18.  Creating another Text File input2.txt \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n291\n Put the  input2.txt into the  /input directory. \n sudo -u hdfs hdfs dfs -put input2.txt /input \n The  input2.txt also gets added to the  /input directory as shown in Figure  10-19 . \n Figure 10-19.  Putting the input2.txt File into HDFS \n The files in the  /input directory in the HDFS may be listed with the following command. \n hdfs dfs -ls /input \n The two files added  input1.txt and  input2.txt get listed as shown in Figure  10-20 . \n Figure 10-20.  Listing the Files in HDFS \n Next, run the  wordcount example application with the following command in which the jar file \ncontaining the example application is specified with the jar parameter and the  /input and  /output \ndirectories are set as the last two command parameters for the input directory and the output directory \nrespectively. \n sudo -u hdfs hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples-2.6.0-\ncdh5.4.7.jar wordcount /input /output \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n292\n The MapReduce job completes to run the  wordcount application . The output from the  wordcount \nMapReduce job, not the word count result, is shown in Figure  10-22 . \n Figure 10-21.  Starting a YARN Application for Word Count Example \n A MapReduce job gets started as shown in Figure  10-21 . \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n293\n A more detailed output from the MapReduce application is listed: \n root@cdh-6l2pr:/# sudo -u hdfs hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-\nexamples-2.6.0-cdh5.4.7.jar wordcount /input /output \n 15/12/21 16:39:52 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 \n 15/12/21 16:39:53 INFO input.FileInputFormat: Total input paths to process : 2 \n 15/12/21 16:39:53 INFO mapreduce.JobSubmitter: number of splits:2 \n 15/12/21 16:39:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: \njob_1450714825612_0002 \n 15/12/21 16:39:53 INFO impl.YarnClientImpl: Submitted application \napplication_1450714825612_0002 \n 15/12/21 16:39:53 INFO mapreduce.Job: The url to track the job: http://cdh-6l2pr:8088/proxy/\napplication_1450714825612_0002/ \n 15/12/21 16:39:53 INFO mapreduce.Job: Running job: job_1450714825612_0002 \n Figure 10-22.  Output from the MapReduce Job \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n294\n 15/12/21 16:39:59 INFO mapreduce.Job: Job job_1450714825612_0002 running in uber mode : \nfalse \n 15/12/21 16:39:59 INFO mapreduce.Job: map 0 % reduce 0 % \n 15/12/21 16:40:04 INFO mapreduce.Job: map 100 % reduce 0 % \n 15/12/21 16:40:10 INFO mapreduce.Job: map 100 % reduce 100 % \n 15/12/21 16:40:10 INFO mapreduce.Job: Job job_1450714825612_0002 completed successfully \n 15/12/21 16:40:10 INFO mapreduce.Job: Counters: 49 \n       File System Counters \n           FILE: Number of bytes read=144 \n           FILE: Number of bytes written=332672 \n           FILE: Number of read operations=0 \n           FILE: Number of large read operations=0 \n           FILE: Number of write operations=0 \n           HDFS: Number of bytes read=317 \n           HDFS: Number of bytes written=60 \n           HDFS: Number of read operations=9 \n           HDFS: Number of large read operations=0 \n           HDFS: Number of write operations=2 \n       Job Counters  \n           Launched map tasks=2 \n           Launched reduce tasks=1 \n           Data-local map tasks=2 \n           Total time spent by all maps in occupied slots (ms)=4939 \n           Total time spent by all reduces in occupied slots (ms)=2615 \n           Total time spent by all map tasks (ms)=4939 \n           Total time spent by all reduce tasks (ms)=2615 \n           Total vcore-seconds taken by all map tasks=4939 \n           Total vcore-seconds taken by all reduce tasks=2615 \n           Total megabyte-seconds taken by all map tasks=5057536 \n           Total megabyte-seconds taken by all reduce tasks=2677760 \n       Map-Reduce Framework \n           Map input records=5 \n           Map output records=17 \n           Map output bytes=178 \n           Map output materialized bytes=150 \n           Input split bytes=206 \n           Combine input records=17 \n           Combine output records=11 \n           Reduce input groups=7 \n           Reduce shuffle bytes=150 \n           Reduce input records=11 \n           Reduce output records=7 \n           Spilled Records=22 \n           Shuffled Maps =2 \n           Failed Shuffles=0 \n           Merged Map outputs=2 \n           GC time elapsed (ms)=158 \n           CPU time spent (ms)=2880 \n           Physical memory (bytes) snapshot=1148145664 \n           Virtual memory (bytes) snapshot=5006991360 \n           Total committed heap usage (bytes)=2472542208 \n",
      "page_number": 297
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 306-315)",
      "start_page": 306,
      "end_page": 315,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n295\n        Shuffle Errors \n           BAD_ID=0 \n           CONNECTION=0 \n           IO_ERROR=0 \n           WRONG_LENGTH=0 \n           WRONG_MAP=0 \n           WRONG_REDUCE=0 \n        File Input Format Counters  \n           Bytes Read=111 \n        File Output Format Counters  \n           Bytes Written=60 \n root@cdh-6l2pr:/#  \n Subsequently, list the files in the  /output directory. \n bin/hdfs dfs -ls /output \n Two files get listed:  _SUCCESS and  part-r-00000 as shown in Figure  10-23 . The  _SUCCESS file is to \nindicate that the MapReduce command completed successfully and the  part-r-00000 command contains \nthe result of the word count. \n Figure 10-24.  The Word Count for the Input Files \n Figure 10-23.  Listing the Files generated by the MapReduce Job \n To list the result of the  wordcount application run the following command. \n hdfs dfs -cat /output/part-r-00000 \n The word count for each of the words in the input gets listed as shown in Figure  10-24 . \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n296\n  Running Hive \n Apache Hive is a data warehouse framework for storing, managing, and querying large data sets in HDFS. \nAs mentioned before all/most of the components of CDH get installed when the  svds/cdh image is run. In \nthis section we shall test the Apache Hive framework. The Hive configuration directory is in the Hive  conf \ndirectory, in the  /etc/hive directory. Change directory (cd) to the  /etc/hive directory. \n cd /etc/hive \n The  conf directory gets listed as shown in Figure  10-25 . \n Figure 10-26.  Listing the Hive Metastore Directory \n Figure 10-25.  Listing the Files and Directories in the Hive Root Directory \n The Hive metastore is kept in the  /var/lib/hive directory. Cd to the  /var/lib/hive directory. \n cd /var/lib/hive \n The  metastore directory gets listed as shown in Figure  10-26 . \n The Hive home directory is  /usr/lib/hive . Cd to the  /usr/lib/hive directory. Subsequently list the \nfiles and directories. \n cd /usr/lib/hive \n ls –l \n The  bin ,  conf, and  lib directories for Apache Hive get listed as shown in Figure  10-27 . The  bin \ndirectory contains the executables, the  conf directory the configuration files, and the  lib directory the \njar files. \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n297\n All the environment variables are preconfigured. Run the following command to start the Beeline CLI. \n beeline \n Beeline version 1.1.0-cdh5.4.7 gets started as shown in Figure  10-28 . \n Figure 10-27.  The Hive Home Directory \n Figure 10-28.  Starting Beeline CLI \n Initially no connection to the Apache Hive server is available. To demonstrate, run the following \ncommands to set the database as default and show the tables. \n use default; \n show tables; \n The message “No current connection” is displayed as shown in Figure  10-29 . \n Figure 10-29.  No Current Connection \n Connect with Hive2 server using the default settings for the driver, username, and password as \nindicated by the three empty “”. \n !connect jdbc:hive2://localhost:10000/default \"\" \"\" \"\" \n \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n298\n Apache Hive2 server gets connected to using the Apache Hive JDBC driver as shown in Figure  10-30 . \n Figure 10-30.  Connecting with Hive Server \n Run the commands to set the database to default and show the tables. \n use default; \n show tables; \n The database connected to is already default, and the first command essentially is redundant but \nwhat is to be noted is the error generated earlier is not generated. The second command lists the table \nand because initially the default database does not have any tables, none get listed. The output from the \npreceding commands is shown in Figure  10-31 . \n Figure 10-31.  Setting the database to Use and the listing to the Hive Tables \n Figure 10-32.  Setting Permissions on the Hive Warehouse Directory \n Before creating a Hive table we need to set the permissions for the  /user/hive/warehouse directory to \nglobal (777). \n sudo –u hdfs hdfs dfs –chmod –R 777 /user/hive/warehouse \n Permissions for the Hive warehouse directory get set as shown in Figure  10-32 . \n Create a table called  wlslog with the following HiveQL command. \n CREATE TABLE wlslog(time_stamp STRING,category STRING,type STRING,servername STRING,code \nSTRING,msg STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'; \n The  wlslog table gets created in the default database as shown in Figure  10-33 . \n \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n299\n Describe the wlslog table with the following command. \n desc wlslog; \n The table columns (name and data type) get listed as shown in Figure  10-34 . \n Figure 10-33.  Creating a Hive Table called wlslog \n Figure 10-34.  Describing the Hive Table wlslog \n Add 7 rows of data to the  wlslog table. \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:16-PM-PDT','Notice','WebLogicServer',\n'AdminServer,BEA-000365','Server state changed to STANDBY'); \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:17-PM-PDT','Notice','WebLogicServer',\n'AdminServer','BEA-000365','Server state changed to STARTING'); \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:18-PM-PDT','Notice','WebLogicServer',\n'AdminServer','BEA-000365','Server state changed to ADMIN'); \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:19-PM-PDT','Notice','WebLogicServer',\n'AdminServer','BEA-000365','Server state changed to RESUMING'); \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:20-PM-PDT','Notice','WebLogicServer',\n'AdminServer','BEA-000331','Started WebLogic AdminServer'); \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:21-PM-PDT','Notice','WebLogicServer',\n'AdminServer','BEA-000365','Server state changed to RUNNING'); \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:22-PM-PDT','Notice','WebLogicServer',\n'AdminServer','BEA-000360','Server started in RUNNING mode'); \n A MapReduce job runs for each  INSERT statement to add the data to Hive table  wlslog as shown in \nFigure  10-35 . \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n300\n Subsequently query the  wlslog table. \n select * from wlslog; \n The 7 rows of data added get listed as shown in Figure  10-36 . \n Figure 10-35.  Adding Data to Hive Table wlslog \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n301\n To quit the Beeline CLI run the following command. \n !q \n As shown in Figure  10-37 the Hive Beeline CLI gets exited. The interactive shell command prompt gets \ndisplayed. \n Figure 10-36.  Querying the Hive Table \n Figure 10-37.  Exiting the Beeline CLI \n From the interactive shell any of the frameworks in CDH may be run. Next, we shall run Apache HBase.  \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n302\n  Running HBase \n Apache HBase is the Apache Hadoop database, which also stores data in HDFS by default. To start the HBase \nshell run the following command from a bash shell for a Docker container based on the svds/cdh Docker image. \n hbase shell \n HBase shell gets started as shown in Figure  10-38 . \n Figure 10-38.  Starting HBase Shell \n Create a table called ‘wlslog’ with column family ‘log’. \n create 'wlslog' , 'log' \n The  wlslog table gets created as shown in Figure  10-39 . \n Figure 10-39.  Creating a HBase Table \n Put 7 rows of data into the  wlslog table. \n put 'wlslog', 'log1', 'log:time_stamp', 'Apr-8-2014-7:06:16-PM-PDT' \n put 'wlslog', 'log1', 'log:category', 'Notice' \n put 'wlslog', 'log1', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log1', 'log:servername', 'AdminServer' \n put 'wlslog', 'log1', 'log:code', 'BEA-000365' \n put 'wlslog', 'log1', 'log:msg', 'Server state changed to STANDBY' \n put 'wlslog', 'log2', 'log:time_stamp', 'Apr-8-2014-7:06:17-PM-PDT' \n put 'wlslog', 'log2', 'log:category', 'Notice' \n put 'wlslog', 'log2', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log2', 'log:servername', 'AdminServer' \n put 'wlslog', 'log2', 'log:code', 'BEA-000365' \n put 'wlslog', 'log2', 'log:msg', 'Server state changed to STARTING' \n put 'wlslog', 'log3', 'log:time_stamp', 'Apr-8-2014-7:06:18-PM-PDT' \n put 'wlslog', 'log3', 'log:category', 'Notice' \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n303\n put 'wlslog', 'log3', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log3', 'log:servername', 'AdminServer' \n put 'wlslog', 'log3', 'log:code', 'BEA-000365' \n put 'wlslog', 'log3', 'log:msg', 'Server state changed to ADMIN' \n put 'wlslog', 'log4', 'log:time_stamp', 'Apr-8-2014-7:06:19-PM-PDT' \n put 'wlslog', 'log4', 'log:category', 'Notice' \n put 'wlslog', 'log4', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log4', 'log:servername', 'AdminServer' \n put 'wlslog', 'log4', 'log:code', 'BEA-000365' \n put 'wlslog', 'log4', 'log:msg', 'Server state changed to RESUMING' \n put 'wlslog', 'log5', 'log:time_stamp', 'Apr-8-2014-7:06:20-PM-PDT' \n put 'wlslog', 'log5', 'log:category', 'Notice' \n put 'wlslog', 'log5', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log5', 'log:servername', 'AdminServer' \n put 'wlslog', 'log5', 'log:code', 'BEA-000331' \n put 'wlslog', 'log5', 'log:msg', 'Started Weblogic AdminServer' \n put 'wlslog', 'log6', 'log:time_stamp', 'Apr-8-2014-7:06:21-PM-PDT' \n put 'wlslog', 'log6', 'log:category', 'Notice' \n put 'wlslog', 'log6', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log6', 'log:servername', 'AdminServer' \n put 'wlslog', 'log6', 'log:code', 'BEA-000365' \n put 'wlslog', 'log6', 'log:msg', 'Server state changed to RUNNING' \n put 'wlslog', 'log7', 'log:time_stamp', 'Apr-8-2014-7:06:22-PM-PDT' \n put 'wlslog', 'log7', 'log:category', 'Notice' \n put 'wlslog', 'log7', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log7', 'log:servername', 'AdminServer' \n put 'wlslog', 'log7', 'log:code', 'BEA-000360' \n put 'wlslog', 'log7', 'log:msg', 'Server started in RUNNING mode' \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n304\n To list the tables run the following command. \n list \n The  wlslog table gets listed as shown in Figure  10-41 . \n Figure 10-40.  Putting Data into HBase Table \n The output from the put commands is shown in Figure  10-40 . \n Figure 10-41.  Listing HBase Tables \n \n \n",
      "page_number": 306
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 316-325)",
      "start_page": 316,
      "end_page": 325,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n305\n To get the data in row with row key ‘log1’ run the following command. \n get 'wlslog', 'log1' \n A single row of data gets listed as shown in Figure  10-42 . \n Figure 10-42.  Getting a Single Row of Data \n Figure 10-43.  Getting a Single Column Value in a Row \n Get the data in a single column, the  log.msg column from row with row key  log7 . A column is specified \nwith column family:column format. \n get 'wlslog', 'log7', {COLUMNS=>['log:msg']} \n The single column data gets output as shown in Figure  10-43 . \n Scan the  wlslog table with the  scan command. \n scan 'wlslog' \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n306\n The scan command is shown in Figure  10-44 . \n Figure 10-44.  Scanning a HBase Table \n All the data from the  wlslog table gets listed as shown in Figure  10-45 .  \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n307\n Deleting the Replication Controller and Service \n In the next section we shall create a cluster for the  svds/cdh image imperatively on the command line. \nDelete the replication controller and the service created declaratively. \n kubectl delete rc cdh \n kubectl delete service cdh \n Creating an Apache Hadoop Cluster Imperatively \n In the following subsections we shall create a CDH cluster from the  svds/cdh Docker image on the \ncommand line. First, we shall create a replication controller. \n Creating a Replication Controller \n Run the following command to create a replication controller called  cdh with 2 replicas. \n kubectl run cdh --image=svds/cdh --replicas=2  \n Figure 10-45.  The scan Command outputs 7 Rows of Data \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n308\n The  cdh controller gets created as shown in Figure  10-46 . The selector is set to  run=cdh by default. \n Figure 10-46.  Creating a Replication Controller Imperatively \n List the replication controllers. \n kubectl get rc \n The  cdh replication controller gets listed as shown in Figure  10-47 . \n Figure 10-47.  Getting the Replication Controller \n Figure 10-48.  Listing the Pods with some Pod/s not READY yet \n Listing the Pods \n To list the Pods in the cluster run the following command. \n kubectl get pods \n The two Pods get listed. Initially some or all of the Pods could be not “Running” or not in the READY \nstate 1/1 as shown in Figure  10-48 . \n Run the preceding command again after a few seconds. \n kubectl get pods \n All the pods should be listed with STATUS “Running” and READY state 1/1 as shown in Figure  10-49 . \n Figure 10-49.  Listing all Pods as Running and Ready \n \n \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n309\n Scaling a Cluster \n To scale the cluster to 4 replicas run the following command. \n kubectl scale rc cdh --replicas=4 \n Subsequently list the Pods. \n kubectl get pods \n An output of “scaled” from the first command indicates that the cluster got scaled. The second \ncommand lists 4 Pods instead of the 2 created initially as shown in Figure  10-50 . The second command may \nhave to be run multiple times to list all Pods with STATUS “Running” and READY state 1/1. \n Figure 10-50.  Scaling the CDH Cluster \n Creating a Service \n A service exposes the Pods managed by the replication controller at service endpoints, which are just \nhost:port settings at which external clients may invoke the application. Run the following command to \ncreate a service. \n kubectl expose rc cdh --type=LoadBalancer \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n310\n Subsequently list the services. \n kubectl get services \n The “cdh” service gets listed with default settings for SELECTOR and PORT as shown in Figure  10-51 . \nThe default service selector is  run=cdh , which has the default format run =<servicename>. The default port is \n8020. \n Figure 10-51.  Creating a Service \n Figure 10-52.  Starting an Interactive Shell \n Starting an Interactive Shell \n The interactive shell may be started just as for a CDH cluster started declaratively. Copy the container id for \na Docker container running the CDH image and run the following command, which includes the container \nid, to start an interactive bash shell. \n sudo docker exec -it 42f2d8f40f17 bash \n The interactive shell gets started as shown in Figure  10-52 . \n Run the  hdfs command. \n hdfs \n The  hdfs command usage gets output as shown in Figure  10-53 . \n \n \n\n\nCHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n311\n Figure 10-53.  Command Usage for hdfs Command \n Summary \n In this chapter we used the Kubernetes cluster manager to create a cluster of Pods based on the Docker \nimage  svds/cdh . We used both the declarative and imperative approaches to create the cluster. We scaled \nthe cluster using the kubectl scale command. We also demonstrated using some of the Apache Hadoop \nframeworks packaged in the  cdh image. We ran a MapReduce  wordcount example application. We also ran \nthe Apache Hive and Apache HBase tools. In the next chapter we shall discuss using Kubernetes with the \nindexing and storage framework Apache Solr. \n \n\n\n313\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_11\n CHAPTER 11 \n Using Apache Solr \n Apache Solr is an Apache Lucene-based enterprise search platform providing features such as full-text \nsearch, near real-time indexing, and database integration. Apache Solr runs as a full-text search server \nwithin a servlet container, the default being Jetty, which is included with the Solr installation. In this chapter \nwe shall discuss using Kubernetes cluster manager with Apache Solr. We shall be using only the declarative \napproach, which makes use of definition files, for creating and managing a Solr cluster. This chapter has the \nfollowing sections.\n Setting the Environment \n Creating a Service \n Listing Service Endpoints \n Describing the Service \n Creating a Replication Controller \n Listing the Pods \n Describing a Pod \n Listing the Logs \n Starting an Interactive Shell \n Creating a Solr Core \n Adding Documents \n Accessing Solr on Command Line with a REST Client \n Setting Port Forwarding \n Accessing Solr in Admin Console \n Scaling the Cluster \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n314\n Setting the Environment \n The following software is required for this chapter. \n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n -Docker image for Apache Solr (latest version) \n We have used the same Amazon EC2 instance AMI as in the other chapters. SSH login to the Ubuntu \ninstance from a local machine. \n ssh -i \"docker.pem\" ubuntu@54.152.82.142 \n Install the required software as discussed in chapter  1 . Start Docker and verify its status. \n sudo service docker start \n sudo service docker status \n As shown in Figure  11-1 Docker should be running. \n List the services. \n kubectl get services \n As shown in Figure  11-2 Kubernetes service should be running. \n To list the nodes run the following command. \n kubectl get nodes \n Figure 11-1.  Starting Docker and Verifying Status \n Figure 11-2.  Listing the “kubernetes” Service \n \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n315\n The 127.0.0.1 node gets listed as shown in Figure  11-3 . \n List the endpoints with the following command. \n kubectl get endpoints \n Initially only the endpoint for kubernetes is listed as shown in Figure  11-4 . \n  Creating a Service \n Create a definition file  solr-service.yaml and add the following (Table  11-1 ) fields to the definition file. \n Figure 11-3.  Listing a Single Node \n Figure 11-4.  Listing “kubernetes” Endpoint \n Table 11-1.  Service Definition File for Apache Solr \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n Service \n metadata \n The service metadata. \n metadata - > labels \n The service labels. Not required. \n app: solrApp \n metadata - > name \n The service name. Required. \n solr-service \n spec \n The service specification. \n spec - > ports \n The ports exposed by the service. \n spec - > ports- > port \n A port exposed by the service. \n 8983 \n spec - > ports- > targetPort \n The target port. \n 8983 \n spec - > selector \n The Pod selector. Service routes traffic to the Pods \nwith a label matching the selector expression. \n app: solrApp \n \n \n",
      "page_number": 316
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 326-341)",
      "start_page": 326,
      "end_page": 341,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n316\n The  solr-service.yaml is listed. \n apiVersion: v1 \n kind: Service \n metadata:  \n  labels:  \n    app: solrApp \n  name: solr-service \n spec:  \n  ports:  \n    -  \n      port: 8983 \n      targetPort: 8983\n  selector: \n    app: solrApp \n The  solr-service.yaml may be edited in the vi editor and saved with :wq as shown in Figure  11-5 . \n Figure 11-5.  Service Definition File in vi Editor \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n317\n Create a service from the definition file with the following command. \n kubectl create -f solr-service.yaml   \n Subsequently list the services. \n kubectl get services \n An output of “services/solr-service” as shown in Figure  11-6 indicates that the service has been created. \nSubsequently the  solr-service gets listed. The service has label  app=solrApp and selector  app=solrApp . \n Listing Service Endpoints \n To list the endpoints run the following command. \n kubectl get endpoints \n As the  solr-service is not managing any Pods initially, no endpoint gets listed as shown in Figure  11-7 . \n Figure 11-6.  Creating a Service from Definition File \n Figure 11-7.  Listing the Endpoint for the Solr Service \n Describing the Service \n To describe the  solr-service run the following command. \n kubectl describe service solr-service \n The service name, namespace, labels, selector, type, IP, Port, endpoints, and events get listed as shown \nin Figure  11-8 . \n \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n318\n  Creating a Replication Controller \n Create a definition file  solr-rc.yaml for the replication controller and add the following (Table  11-2 ) fields \nto the definition file. \n Figure 11-8.  Describing the Apache Solr Service \n Table 11-2.  Replication Controller Definition File Fields \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n Replication Controller \n metadata \n The replication controller metadata. \n metadata - > labels \n The replication controller labels. \n app: solrApp \n metadata - > name \n The replication controller name. \n solr-rc \n spec \n The replication controller specification. \n spec - > replicas \n The number of Pod replicas. \n 2 \n spec - > selector \n A key: value expression for selecting \nthe Pods to manage. Pods with a label \nthe same as the selector expression \nare managed by the replication \ncontroller. For a single label/\nselector expression Pod/Replication \nController combination the selector \nexpression must be the same as the \nspec- > template- > metadata- > labels \nexpression. The selector defaults to the \nspec- > template- > metadata- > labels \nnot specified. The app: solrApp setting \ntranslates to app=solrApp. \n app: solrApp \n spec - > template \n The Pod template. \n spec - > template - > metadata \n The Pod template metadata. \n(continued)\n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n319\n The  solr-rc.yaml is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata:  \n  labels:  \n    app: solrApp \n  name: solr-rc \n spec:  \n  replicas: 2 \n  selector:  \n    app: solrApp \n  template:  \n    metadata:  \n      labels:  \n        app: solrApp \n    spec:  \n      containers:  \n        -  \n          image: solr \n          name: solr \n          ports:  \n            -  \n              containerPort: 8983 \n              name: solrApp \n The  solr-rc.yaml definition file may be created and saved in vi editor as shown in Figure  11-9 . \n Field \n Description \n Value \n spec - > template- > metadata- > labels \n The Pod template labels. \n app: solrApp \n spec - > template - > spec \n The Pod template specification. \n spec - > template - > spec - > containers \n The containers configuration for the Pod \ntemplate. \n spec - > template - > spec - > containers \n- > image \n The Docker image. \n solr \n spec - > template - > spec - > containers \n- > name \n The container name. \n solr \n spec - > template - > spec - > containers \n- > ports \n Container ports. \n spec - > template - > spec - > containers \n- > ports - > containerPort \n Container port for Solr server. \n 8983 \n spec - > template - > spec - > containers \n- > ports - > name \n Solr port name. \n solrApp \nTable 11-2. (continued)\n\n\nCHAPTER 11 ■ USING APACHE SOLR\n320\n Run the following command to create a replication controller from the definition file. \n kubectl create -f solr-rc.yaml   \n The  solr-rc replication controller gets created as shown in Figure  11-10 . Subsequently list the \nreplication controllers. \n kubectl get rc \n The  solr-rc replication controller gets listed as shown in Figure  11-10 . \n Figure 11-9.  Replication Controller Definition File in vi Editor \n Figure 11-10.  Creating a Replication Controller from Definition File \n \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n321\n Listing the Pods \n List the Pods with the following command. \n kubectl get pods \n The two Pods created by the replication controller get listed as shown in Figure  11-11 . Initially some of \nthe Pods could be not running and not ready. \n Run the same command again after a few seconds to list the Pods again. \n kubectl get pods \n The Pods should get listed with STATUS “Running” and READY state 1/1 as shown in Figure  11-12 . \n To describe the  solr-service run the following command. \n kubectl describe svc solr-service \n The service description gets listed as shown in Figure  11-13 . The service endpoints for the two Pods are \nalso listed. A service is accessed at its endpoints. When described previously, before creating the replication \ncontroller, no service endpoints got listed as shown in Figure  11-8 . \n Figure 11-11.  Listing the Pods, all of them not yet Ready \n Figure 11-12.  Listing the Pods as Ready \n \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n322\n The endpoints may also be listed separately. \n kubectl get endpoints  \n The endpoints get listed as shown in Figure  11-14 . \n Describing a Replication Controller \n To describe the replication controller  solr-rc run the following command. \n kubectl describe rc solr-rc \n The replication controller description gets listed as shown in Figure  11-15 . \n Figure 11-13.  Describing the Solr Service including the Service Endpoints \n Figure 11-14.  Listing the Endpoints for Solr Service \n \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n323\n Listing the Logs \n To list the logs for a particular command run the  kubectl logs command. For example, logs for the \n solr-rc-s82ip Pod are listed with the following command. \n kubectl logs solr-rc-s82ip \n In the log output the Solr server is starting as shown in Figure  11-16 . \n Figure 11-15.  Describing the Replication Controller \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n324\n After the server has started the output “Server Started” gets output as shown in Figure  11-17 . \n Figure 11-16.  Listing Logs for the Pod \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n325\n  Starting an Interactive Shell \n As the “solr” Docker image inherits from the “java:openjdk-8-jre” Docker image, which further inherits from \nthe “buildpack-deps:jessie-curl” image, which inherits from Docker image “debian” for Linux an interactive \nbash shell may be started to access a Docker container based on the “solr” Docker image. To access the Solr \nsoftware we need to start an interactive bash shell for a Docker container running Solr. Obtain the container \nif for a Docker container running Solr with the following command. \n sudo docker ps \n The Docker containers get listed as shown in Figure  11-18 . \n Figure 11-17.  Listing the Solr Server as started \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n326\n Copy the container if and start an interactive shell. \n sudo docker exec -it 2d4d7d02c05f bash \n The interactive shell gets started as shown in Figure  11-19 . To list the status of the Solr server run the \nfollowing command. \n bin/solr status \n Figure 11-18.  Listing the Docker Container for Apache Solr \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n327\n One Solr node is found as shown in Figure  11-19 . \n Solr 5.x introduce  configsets . The configsets directory consists of example configurations that may be \nused as a base to create new Solr cores or collections. The configsets replace the  collection1 example core \nconfiguration in Solr 4.x. Cd (change directory) to the  configsets directory. \n cd /opt/solr/server/solr/configsets \n List the files and directories in the  configsets directory. \n ls –l \n Three example configurations get listed as shown in Figure  11-20 . \n When we create a Solr core later in the chapter we shall be using the basic_configs configuration. \nList the files in the  //configsets/ basic_configs/conf directory. \n cd conf \n ls –l \n Figure 11-19.  Listing the Solr Status in an Interactive Shell for the Docker Container \n Figure 11-20.  Listing the Example Configurations \n \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n328\n The configuration files for  basic_configs example get listed and include the  schema.xml and \nsolrconfig.xml as shown in Figure  11-21 . \n Creating a Solr Core \n A new Solr core may also be created from the command line. The  solr create command is used to create a \nnew core or a collection. As an example, create a core called  wlslog with the  solr create_core command. \nUse the configset  basic_configs with the  –d option. The default config set used if none is specified (with the  –d \noption) is  data_driven_schema_configs . Cd to the  /opt/solr directory and run the following command. \n bin/solr create_core -c wlslog -d /opt/solr/server/solr/configsets/basic_configs \n A Solr core called  wlslog gets created as shown in Figure  11-22 . \n Figure 11-21.  Listing the Configuration Files in the basic_configs Example Configuration \n Figure 11-22.  Creating a Solr Core called wlslog \n \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n329\n  Indexing Documents \n Apache Solr provides the  post tool for indexing documents from the command line. The  post tool supports \ndifferent input file formats such as XML, CSV and JSON. We shall index an XML format document Save the \nfollowing XML document to the  wlslog.xml file. \n <add> \n <doc> \n <field name=\"id\">wlslog1</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:16-PM-PDT</field> \n  <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code_s\">BEA-000365</field> \n  <field name=\"msg_s\">Server state changed to STANDBY</field> \n </doc> \n <doc> \n <field name=\"id\">wlslog2</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:17-PM-PDT</field> \n  <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code\">BEA-000365</field> \n  <field name=\"msg_s\">Server state changed to STARTING</field> \n </doc> \n <doc> \n <field name=\"id\">wlslog3</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:18-PM-PDT</field> \n  <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code\">BEA-000365</field> \n  <field name=\"msg_s\">Server state changed to ADMIN</field> \n </doc> \n <doc> \n <field name=\"id\">wlslog4</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:19-PM-PDT</field> \n  <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n330\n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code\">BEA-000365</field> \n  <field name=\"msg_s\">Server state changed to RESUMING</field> \n </doc> \n <doc> \n <field name=\"id\">wlslog5</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:20-PM-PDT</field> \n  <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code\">BEA-000331</field> \n  <field name=\"msg_s\">Started WebLogic AdminServer</field> \n </doc> \n <doc> \n <field name=\"id\">wlslog6</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:21-PM-PDT</field> \n   <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code\">BEA-000365</field> \n  <field name=\"msg_s\">Server state changed to RUNNING</field> \n </doc> \n <doc> \n <field name=\"id\">wlslog7</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:22-PM-PDT</field> \n  <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code\">BEA-000360</field> \n  <field name=\"msg_s\">Server started in RUNNING mode</field> \n </doc> \n </add> \n The  wlslog.xml file may be created in the vi editor and saved with the :wq command as shown in \nFigure  11-23 . \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n331\n Cd to the  /opt/solr directory and run the post tool to add the documents in the  wlslog.xml file to \nSolr server. \n bin/post -c wlslog ./wlslog.xml \n Figure 11-23.  The wlslog.xml File \n \n",
      "page_number": 326
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 342-358)",
      "start_page": 342,
      "end_page": 358,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n332\n One file gets indexed as shown in Figure  11-24 . \n  Accessing Solr on Command Line with a REST Client \n Solr request handler commands such as  /update ,  /select may be run using a REST client such as curl and \nwget. In this section we shall use the curl tool to run some of the  /select request handler commands. \nFor example, query all documents using the following curl command. \n curl http://localhost:8983/solr/wlslog/select?q=*%3A*&wt=json&indent=true  \n The curl command is shown in Figure  11-25 . \n Figure 11-24.  Posting the wlslog.xml File to the Solr Index \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n333\n The 7 documents added get listed as shown in Figure  11-26 . \n Figure 11-25.  Using curl to send a Request to Solr Server with Request Handler /select \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n334\n As another example run the  /select request handler to query for the document with id  wlslog7 . \n curl http://localhost:8983/solr/wlslog/select?q=id:wlslog7&wt=json&indent=true \n The document for id  wlslog7 gets listed as shown in Figure  11-27 . \n Figure 11-26.  Listing the Documents returned by the /select Request Handler \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n335\n Documents may be deleted with the  post tool. For example, delete a document with id  wlslog1 using \nthe following command. \n bin/post -c wlslog -d \"<delete><id>wlslog1</id></delete>\" \n The document with id  wlslog1 gets deleted as shown in Figure  11-28 . \n Subsequently run the following curl command to list the documents in the  wlslog index. \n curl http://localhost:8983/solr/wlslog/select?q=*%3A*&wt=json&indent=true \n The document with id  wlslog1 does not get listed as shown in Figure  11-29 . \n Figure 11-27.  Querying for a Single Document with id wlslog7 using /select Request Handler and curl \n Figure 11-28.  Deleting a Document using post Tool \n \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n336\n The  /update request handler may be used to delete documents as in the following curl command, \nwhich deletes all documents in the  wlslog core. \n curl http://localhost:8983/solr/wlslog/update --data '<delete><query>*:*</query></delete>' \n-H 'Content-type:text/xml; charset=utf-8' \n Figure 11-29.  Querying after Deleting a Document \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n337\n If auto commit has not been configured the following curl command must be run to commit the \nchanges. \n curl http://localhost:8983/solr/wlslog/update --data '<commit/>' -H 'Content-type:text/xml; \ncharset=utf-8' \n Subsequently run the curl command to invoke the  /select request handler. \n curl http://localhost:8983/solr/wlslog/select?q=*%3A*&wt=json&indent=true \n No document gets listed as all have been deleted as shown in Figure  11-30 . \n Setting Port Forwarding \n If we were running Kubernetes on a local machine we could have opened the Solr Admin Console with url \n http://localhost:8983 but because we are using Amazon EC2 instance we need to set port forwarding \non a local machine with a web browser from localhost:8983 to 172.17.0.2:8983. Set port forwarding from \n localhost port 8983 with the following command run from a local machine. \n ssh -i key-pair-file -f -nNT -L 8983:172.17.0.2:8983 ubuntu@ec2-54-152-82-142.compute-1.\namazonaws.com \n Figure 11-30.  Deleting all Documents in Solr Index with /update \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n338\n The preceding command forwards the  localhost:8983 URL to endpoint 172.17.0.2:8983 as shown \nin Figure  11-31 . \n Accessing Solr in Admin Console \n After port forwarding the Solr  Admin Console may be accessed from the local machine using the url \n http://localhost:8983 as shown in Figure  11-32 . Select the  wlslog core in the Core Selector as shown in \nFigure  11-32 . \n Select the Documents tab and set Document Type as XML for the  /update Request handler as shown in \nFigure  11-33 . Copy and paste the XML document wlslog.xml listed earlier in the Document (s) field and click \non Submit Document. \n Figure 11-31.  Setting Port Forwarding to localhost \n Figure 11-32.  Displaying the Solr Admin Console \n \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n339\n An output of “success” as shown in Figure  11-34 indicates that the documents got indexed. \n Figure 11-33.  Adding Document to the wlslog Core \n Figure 11-34.  Response from adding Documents \n \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n340\n Next, we shall query the  wlslog index. Select the Query tab as shown in Figure  11-35 . \n Figure 11-35.  Selecting the Query Tab \n With the Request Handler as  /select the query is “*:*” by default as shown in Figure  11-36 . \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n341\n Click on Execute Query as shown in Figure  11-37 . \n Figure 11-36.  Using the Request Handler /select to Query Solr index wlslog \n Figure 11-37.  Submitting a Query to select all Documents in the wlslog Index \n \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n342\n Because we have not set auto commit the documents added have not yet been indexed. As a result no \ndocument gets listed as shown in Figure  11-38 . \n We need to reload the core for the added documents to get indexed. Alternatively we could restart the \nSolr server but reloading the core is a quicker option. Select Core Admin and click on Reload as shown in \nFigure  11-39 . \n Figure 11-38.  Response from the Query \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n343\n Run the query again and as shown in Figure  11-40 the 7 documents added get listed. \n Figure 11-40.  Query Response with 7 Documents \n Figure 11-39.  Reloading the Core \n \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n344\n The  _version_ field has been added to each document automatically by the Solr server as shown in \nFigure  11-41 . \n Scaling the Cluster \n To  scale the Solr pod cluster run the  kubectl scale command. For example, to scale to 4 Pods set \nreplicas as 4. \n kubectl scale rc solr-rc --replicas=4 \n An output of “scaled” indicates that the Solr cluster has been scaled. Subsequently run the following \ncommand to list the Pods. \n kubectl get pods \n The number of Pods listed is 4 instead of the 2 to start with as shown in Figure  11-42 . Some of the Pods \ncould be not running or not ready initially. \n Figure 11-41.  The _version_ Field is added to each Document stored in Solr Index Automatically by the \nSolr Server \n \n\n\nCHAPTER 11 ■ USING APACHE SOLR\n345\n Summary \n Apache Solr is an indexing and search engine that makes use of the local filesystem to store data. In this \nchapter we used Docker image “solr” with Kubernetes cluster manage to create and manage a cluster of Solr \ninstances. We demonstrated accessing a Solr instance from an interactive shell for a Docker container and \nalso using the Admin Console. In the next chapter we shall use Kubernetes with Apache Kafka. \n Figure 11-42.  Scaling the Apache Solr Cluster to 4 Pods \n \n\n\n347\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_12\n CHAPTER 12 \n Using Apache Kafka \n Apache Kafka is publish-subscribe, high throughput, distributed messaging system. A single broker in Kafka \ncould handle 100s MB (Terabytes)/sec of reads & writes from multiple clients. Messages are replicated \nacross the cluster and persisted to disk. Kafka could be used for stream processing, web site activity tracking, \nmetrics collection, and monitoring and log aggregation. \n The main components of Kafka architecture are Producer, Broker, Topic, and Consumer. Kafka keeps \nfeeds of messages in topics. Producers send (or write) messages to topics and Consumers consume \n(or read) messages from topics. Messages are byte arrays of data and could be in any format with String, \nJSON, and Avro being the most common. Messages are retained for a specified amount of time. A Zookeeper \ncoordinates the Kafka cluster. In a single producer–consumer architecture, a single Producer sends messages \nto a Topic and a single Consumer consumes messages from the topic. \n Kafka is similar to Flume in that it streams messages, but Kafka is designed for a different purpose. \nWhile Flume is designed to stream messages to a sink such as HDFS or HBase, Kafka is designed for \nmessages to be consumed by multiple applications. \n In this chapter we shall discuss using Kubernetes cluster manager with Apache Kafka.\n Setting the Environment \n Modifying the Docker Image \n Creating a Service \n Creating a Replication Controller \n Listing the Pods \n Describing a Pod \n Starting an Interactive Shell \n Starting the Kafka Server \n Creating a Topic \n Starting a Kafka Producer \n Starting a Kafka Consumer \n Producing and Consuming Messages \n Scaling the Cluster \n Deleting Replication Controller and Service \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n348\n Setting the Environment \n We have used an Amazon EC2 instance created from AMI Ubuntu Server 14.04 LTS (HVM), SSD Volume \nType - ami-d05e75b8. The following software is required for this chapter. \n -Docker Engine (latest version) \n -Kubernetes Cluster Manager (version 1.01) \n -Kubectl (version 1.01) \n -Docker image dockerkafka/kafka (latest version) \n We have used the Docker image  dockerkafka/kafka in this chapter. The default settings of the \n dockerkafka/kafka image Dockerfile are not suitable for orchestration with Kubernetes. In the next section \nwe have modified and rebuilt the default Docker image. First, connect with the Ubuntu instance using the \nPublic IP Address for the Amazon EC2 instance. \n ssh -i \"docker.pem\" ubuntu@54.146.140.160 \n The Ubuntu instance gets connected to as shown in Figure  12-1 . \n Install the required software as discussed in chapter  1 . Start the Docker service and find its status. \n sudo service docker start \n sudo service docker status \n Figure 12-1.  Connecting to an Ubuntu Instance on Amazon EC2 \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n349\n Docker should be listed as running as shown in Figure  12-2 . \n List the Kubernetes services. \n kubectl get services \n The “kubernetes” service should be listed as shown in Figure  12-3 . \n  Modifying the Docker Image \n The procedure to start Apache Kafka involves the following sequence. \n \n 1. \n Start Zookeeper Server \n \n 2. \n Start Apache Kafka Server \n The Apache Kafka Server has a dependency on Zookeeper server and as a result requires the \nZookeeper server to be running before the Kafka server may be started. The Kafka server makes use of the \n server.properties configuration file when started. The default settings in the  server.properties file are \nnot suitable for the Kafka server to start based on a  Zookeeper server running at  localhost:2181 . We need \nto modify the connect url for Zookeeper in the  server.properties file. \n In this section we shall download the  dockerkafka/kafka image, modify the  server.properties and \nrebuild the Docker image. Download the source code for the  dockerkafka/kafka image with the following \ncommand. \n git clone https://github.com/DockerKafka/kafka-docker.git \n The source code for the  dockerkafka/kafka image gets downloaded as shown in Figure  12-4 . \n Figure 12-2.  Starting Docker \n Figure 12-3.  Listing the “kubernetes” Service \n \n \n",
      "page_number": 342
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 359-371)",
      "start_page": 359,
      "end_page": 371,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n350\n Change directory (cd) to the  kafka-docker directory and list the files/directories. \n cd kafka-docker \n ls –l \n The files/directories in the Docker image get listed as shown in Figure  12-5 . \n We need to modify the settings in the  server.properties file, which is in the  image/conf directory. \nCd to the  image/conf directory and list the directory’s file/directories. \n cd image/conf \n ls –l \n The  server.properties file gets listed as shown in Figure  12-6 . \n Figure 12-4.  Downloading the kafka-docker Docker Image Source Code \n Figure 12-5.  Listing the Dockerfile and Image Directory for the kafka-source Docker Image \n Figure 12-6.  Listing the Configuration Files for the Docker Image \n \n \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n351\n Open the  server.properties file in a vi editor. \n sudo vi server.properties \n The  server.properties file is shown in Figure  12-7 . Uncomment the line with the  host.name=localhost \nsetting. \n As shown in Figure  12-8 the default setting for the  zookeeper.connect is  zookeeper:2181 . \n Figure 12-7.  Uncommenting the host.name Property \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n352\n Modify the  zookeeper.connect setting to  localhost:2181 as shown in Figure  12-9 . Save the modified \nfile with :wq. We need to modify the setting because no such host as “zookeeper” exists by default. \n Figure 12-8.  The default setting for the zookeeper.connect Property \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n353\n Subsequently cd back to the root directory for the Docker image, the  kafka-docker directory, and run \nthe following command to rebuild the Docker image. \n sudo docker build -t dockerkafka/kafka:v2. \n The output from the command is shown in Figure  12-10 . \n Figure 12-9.  Setting zookeeper.connect to localhost: 2181 \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n354\n Figure 12-10.  Rebuilding the Docker Image for Kafka \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n355\n Docker image gets rebuilt as shown in Figure  12-11 . \n The Docker image we shall use subsequently is not  dockerkafka/kafka but is  dockerkafka/kafka:v2 .  \n Creating a Service \n Create a service definition file called  kafka-service.yaml and add the following (Table  12-1 ) fields to \nthe file. \n Figure 12-11.  Completing the Rebuild of the Docker Image \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n356\n The  kafka-service.yaml is listed. \n apiVersion: v1 \n kind: Service \n metadata:  \n  labels:  \n    app: kafkaApp \n  name: kafka \n spec:  \n  ports:  \n    -  \n      port: 9092 \n      targetPort: 9092 \n    -  \n      port: 2181 \n      targetPort: 2181 \n  selector:  \n    app: kafkaApp \n  type: LoadBalancer \n The  kafka-service.yaml may be created in vi editor and saved with :wq as shown in Figure  12-12 . \n Table 12-1.  The Fields in the Service Definition File \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n Service \n metadata \n The service metadata. \n metadata - > labels \n The service labels. Not required. \n app: kafkaApp \n metadata - > name \n The service name. Required. \n kafka \n spec \n The service specification. \n spec - > ports \n The ports exposed by the service. \n spec - > ports- > port \n A port exposed by the service. The 9092 port is used for the \nKafka server. \n port: 9092 \n targetPort: 9092 \n spec - > ports- > port \n Another port exposed by the service. The 2181 port is for the \nZookeeper. \n port: 2181 \n targetPort: 2181 \n spec - > selector \n The Pod selector. Service routes traffic to the Pods with label \nmatching the selector expression. \n app: kafkaApp \n spec \n- > selector- > type \n The service type. \n LoadBalancer \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n357\n Create the service from the definition file. \n kubectl create -f kafka-service.yaml  \n Subsequently list the services. \n kubectl get services \n The “kafka” service gets listed as shown in Figure  12-13 . The service selector is app = kafkaApp. \n Figure 12-12.  Service Definition File in vi Editor \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n358\n Creating a Replication Controller \n Create a definition file called  kafka-rc.yaml for the replication controller and add the following (Table  12-2 ) \nfields. \n Table 12-2.  Fields in the Replication Controller Definition File \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n ReplicationController \n metadata \n The replication controller metadata. \n metadata - > labels \n The replication controller labels. \n app: kafkaApp \n name: kafka-rc \n spec \n The replication controller specification. \n spec - > replicas \n The number of Pod replicas. \n 2 \n spec - > selector \n A key:value expression for selecting the Pods \nto manage. Pods with a label the same as the \nselector expression are managed by the replication \ncontroller. The selector expression must be the \nsame as the spec- > template- > metadata- > labels \nexpression. The selector defaults to the \nspec- > template- > metadata- > labels key:value \nexpression if not specified. \n app: kafkaApp \n spec - > template \n The Pod template. \n spec - > template- > metadata  The Pod template metadata. \n spec - > template - > metadata \n- > labels \n The Pod template labels. \n app: kafkaApp \n spec - > template - > spec \n The Pod template specification. \n spec - > template - > spec \n- > containers \n The containers configuration for the Pod template. \n Figure 12-13.  Creating a Service from the Definition File \n(continued)\n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n359\n The  kafka-rc.yaml is listed. \n ---  \n apiVersion: v1 \n kind: ReplicationController \n metadata:  \n  labels:  \n    app: kafkaApp \n  name: kafka-rc \n spec:  \n  replicas: 1 \n  selector:  \n    app: kafkaApp \n  template:  \n    metadata:  \n      labels:  \n        app: kafkaApp \n    spec:  \n      containers:  \n        -  \n          command:  \n            - zookeeper-server-start.sh \n            - /opt/kafka_2.10-0.8.2.1/config/zookeeper.properties \n          image: \"dockerkafka/kafka:v2\" \n          name: zookeeper \n          ports:  \n            -  \n              containerPort: 2181 \n Field \n Description \n Value \n spec - > template - > spec \n- > containers - > command \n The command/s to run for the Docker image. \nThe default command in the Dockerfile is CMD \n[“kafka-server-start.sh”, “/opt/kafka_2.10-0.8.2.1/\nconfig/server.properties”]. The default command \nstarts the Kakfa server, but we want the Zookeeper \nserver before the Kafka server as the Kafka server \nwon’t start unless the Zookeeper server is running. \nThe modified command starts only the Zookeeper \nserver. We shall start the Kafka server separately. \n - zookeeper-server-\nstart.sh \n - /opt/\nkafka_2.10-0.8.2.1/\nconfig/zookeeper.\nproperties \n spec - > template - > spec \n- > containers - > image \n The Docker image. \n dockerkafka/kafka:v2 \n spec - > template - > spec \n- > containers - > name \n The container name. \n zookeeper \n ports \n Specifies the container port/s. \n containerPort: 2181 \nTable 12-2. (continued)\n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n360\n The  kafka-rc.yaml file may be created and saved in the vi editor as shown in Figure  12-14 . \n Create the replication controller from the definition file. \n kubectl create -f kafka-rc.yaml   \n Subsequently list the replication controllers. \n kubectl get rc \n The replication controller gets created and listed as shown in Figure  12-15 . \n Figure 12-14.  Replication Controller Definition File in vi Editor \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n361\n To describe the  kafka-rc run the following command. \n kubectl describe rc kafka-rc \n The replication controller description gets listed as shown in Figure  12-16 . \n Listing the Pods \n To list the Pods run the following command. \n kubectl get pods \n The Pods get listed as shown in Figure  12-17 . \n Figure 12-15.  Creating the Replication Controller from the Definition File \n Figure 12-16.  Describing the Replication Controller \n Figure 12-17.  Listing the pods for Kafka \n \n \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n362\n Describing a Pod \n Only a single Pod is created because the “replicas” setting in the definition file  kafka-rc.yaml is 1. To describe \nthe Pod run the following command. \n kubectl describe pod kafka-rc-k8as1 \n The pod description gets listed as shown in Figure  12-18 . The Pod label  app=kafkaApp is the same as the \nservice selector and the replication controller selector which makes the Pod manageable by the service and \nthe replication controller. \n When the Pod is created and started, the Zookeeper server gets started as the command for the \nmodified Docker image is to start the Zookeeper server. Next we shall start the Kafka server from an \ninteractive shell for the Docker container for the modified Docker image. \n Figure 12-18.  Describing a pod for Kafka \n \n",
      "page_number": 359
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 372-383)",
      "start_page": 372,
      "end_page": 383,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n363\n Starting an Interactive Shell \n To be able to start an interactive bash shell to access the Kafka software installed we need to know the \ncontainer id for the Docker container running the modified Docker image. List the Docker containers with \nthe following command. \n sudo docker ps \n The Docker containers get listed as shown in Figure  12-19 . \n Figure 12-19.  Obtaining the Docker Container Id \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n364\n Copy the container id and start the interactive bash shell. \n sudo docker exec -it 939ae2cb4f86 bash \n The interactive shell gets started as shown in Figure  12-20 . \n Starting the Kafka Server \n The configuration properties for Kafka server are set in the  config/server.properties file, which we \nmodified when we rebuilt the Docker image. As the Zookeeper is already running, start the Kafka server with \nthe following command. \n kafka-server-start.sh /opt/kafka_2.10-0.8.2.1/config/server.properties \n The preceding command is shown in Figure  12-21 . \n Figure 12-20.  Starting the Interactive TTY for the Docker Container \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n365\n Kafka server gets started as shown in Figure  12-22 . \n Figure 12-21.  Starting the Kafka Server \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n366\n Creating a Topic \n Next, create a topic called ‘kafka-on-kubernetes’ with the following command. Set the number of partitions \nto 1 and replication factor to 1. The Zookeeper is set to  localhost:2181 . \n kafka-topics.sh --create --topic kafka-on-kubernetes --zookeeper localhost:2181 \n--replication-factor 1 --partitions 1 \n As shown in Figure  12-23 the  kafka-on-kubernetes topic gets created. \n Figure 12-22.  Kafka Server started at localhost:9092 \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n367\n Starting a Kafka Producer \n A Kafka producer is used to produce messages. After starting the ZooKeeper and the Kafka server, start \nthe Kafka producer. Specify the topic with the  –topic option as ‘kafka-on-kubernetes’. The  --broker-list \nspecifies the Kafka server as  localhost:9092 , which are the settings configured in  server.properties file. \n kafka-console-producer.sh --topic kafka-on-kubernetes --broker-list localhost:9092 \n As shown in Figure  12-24 the Kafka producer gets started. \n Starting a Kafka Consumer \n A Kafka consumer consumes messages. Start the Kafka consumer with the following command. Specify \nthe topic with the  –topic option as ‘kafka-on-kubernetes’. The  --zookeeper specifies the Zookeeper server \nas  localhost:2181 , which are the settings configured in  server.properties file. The  --from-beginning \noption specifies that messages from the beginning are to be consumed, not just the messages consumed \nafter the consumer was started. \n kafka-console-consumer.sh --topic kafka-on-kubernetes --from-beginning --zookeeper \nlocalhost:2181 \n As shown in Figure  12-25 the Kafka producer gets started. \n Producing and Consuming Messages \n Having started the Producer and the Consumer, we shall produce message/s at the Producer and consume \nmessage/s at the Consumer. At the Producer add a message, for example, “Message from Kafka Producer” as \nshown in Figure  12-26 and click on Enter button. The message gets sent. \n Figure 12-24.  Starting a Kafka Producer \n Figure 12-25.  Starting a Kafka Consumer \n Figure 12-23.  Creating a Kafka Topic \n \n \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n368\n At the Consumer the message gets consumed as shown in Figure  12-27 . \n Send more messages at the Producer as shown in Figure  12-28 . \n And the messages get consumed at the Consumer as shown in Figure  12-29 . \n Scaling the Cluster \n To scale the cluster to 4 Pods from 1 Pod run the following command. \n kubectl scale rc kafka-rc --replicas=4 \n Figure 12-28.  Producing More Messages at the Kafka Producer \n Figure 12-29.  Consuming More Messages at the Kafka Consumer \n Figure 12-27.  Consuming a Message at the Kafka Consumer \n Figure 12-26.  Producing a Message at the Kafka Producer \n \n \n \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n369\n Subsequently list the Pods. \n kubectl get pods \n An output of “scaled” indicates that the cluster has been scaled as shown in Figure  12-30 . Subsequently \nthe Pods get listed, also shown in Figure  12-30 . \n When the number of Pods are increased to 4, the service endpoints also increase to 4. Describe the \nservice  kafka . \n kubectl describe svc kafka \n As shown in Figure  12-31 , 4 endpoints are listed for each of the two services, one for Zookeeper server \nand the other for the Kafka server. \n Figure 12-30.  Scaling the Kafka Cluster \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n370\n Deleting Replication Controller and Service \n To delete the replication controller and service run the following commands. \n kubectl delete rc kafka-rc \n kubectl delete service kafka \n As shown in Figure  12-32 the replication controller and service get deleted. \n Figure 12-32.  Deleting the Kafka Replication Controller and Service \n Figure 12-31.  Describing the Kafka Service with 4 Endpoints \n \n \n\n\nCHAPTER 12 ■ USING APACHE KAFKA\n371\n Summary \n Apache Kafka is a producer–consumer-based messaging system. In this chapter we discussed managing \na Kafka cluster with Kubernetes. Managing the Kafka is different from some of the other applications as \ntwo servers have to be started: the Zookeeper server and the Kafka server. And the Kafka server has a \ndependency on the Zookeeper server, which implies that the Zookeeper must be started before the Kafka \nserver. We needed to modify the default image  dockerkafka/kafka for the zookeeper connect url. In the \nreplication controller definition file we used a custom command to run the modified Docker image to \nstart the Zookeeper server, the default settings in the Docker image being to start the Kafka server. All the \napplications we have run as yet were based on a single container Pod. In the next chapter we shall develop a \nmulti-container Pod. \n\n\n   PART V \n Multi Containers and Nodes   \n\n\n375\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_13\n CHAPTER 13 \n Creating a Multi-Container Pod \n A  Pod is the atomic unit of an application managed by Kubernetes. A Pod has a single filesystem and IP \nAddress; the containers in the Pod share the filesystem and networking IP. A Pod could consist of one or more \ncontainers. A Pod is defined in a definition file for a Pod or a replication controller using the specification \nfor a Pod ( http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_podspec ). A \nsingle container within a Pod is specified using the container specification ( http://kubernetes.io/v1.1/\ndocs/api-reference/v1/definitions.html#_v1_container ). In all of the applications discussed as yet, \nin preceding chapters, a single container Pod was used. In this chapter we shall develop a multi-container \nPod. We have used the  tutum/hello-world and  postgres Docker images for the multi-container Pod. Each \nof these images have been used in a single container Pods in preceding chapters. This chapter will cover the \nfollowing topics.\n How to Find Number of Containers in a Pod? \n Type of applications Using a Multi-Container Pod \n Setting the Environment \n Creating a Service \n Describing a Service \n Creating a Replication Container \n Listing the Pods \n Listing the Docker Containers \n Describing the Service after Creating Replication Controller \n Invoking the Hello World Application on Command Line \n Starting the Interactive Shell \n Starting PostgreSQL Shell \n Setting Port Forwarding \n Opening the Hello World Application in a Browser \n Scaling the Cluster\n Describing the Service after Scaling \n Describing a Pod \n Setting Port Forwarding \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n376\n Opening the Hello World Applications in a Browser \n Invoking the Hello World Application from Command Line \n Deleting the Replication Controller \n Deleting the Service \n How to find Number of Containers in a Pod? \n As discussed previously the Pods may be listed with the following command. \n kubectl get pods \n The Kubernetes  Pod  k8s-master-127.0.0.1 Pod has 3/3 in the READY column as shown in Figure  13-1 . \nThe 3/3 indicates that the Pod has 3 containers and all three containers are ready. The n/n in the READY \ncolumn for any Pod indicates the number of containers ready out of the total number of containers. All the \ncontainers are running on a single node as indicated by the subsequent listing of nodes. \n Types of Applications Using a Multi-Container Pod \n Various types of  applications could make use of a multi-container Pod. Some of the examples are as follows:\n -An Apache Sqoop application makes use of a CDH Docker image-based \ncontainer and a MySQL database Docker image-based container for bulk \ntransferring data from MySQL database into HDFS. \n -An Apache Flume application makes use of a CDH Docker image-based \ncontainer and a Kafka-based container for streaming data from a Kafka source \ninto HDFS. \n -An Apache Solr application makes use of a Oracle Database-based container \nand the Solr container for data import from Oracle Database into Solr. \n -An Apache Hive application makes use a CDH container and a MongoDB \ncontainer to create a Hive table using the MongoDB storage handler. \n -An Apache Solr container and a CDH container are required to store Solr data in \nHDFS instead of the local filesystem. \n Figure 13-1.  Listing the Pods and the Number of Containers in the Pods \n \n",
      "page_number": 372
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 384-401)",
      "start_page": 384,
      "end_page": 401,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n377\n Setting the Environment \n We have used an Amazon EC2 instance created from AMI Ubuntu Server 14.04 LTS (HVM), SSD Volume \nType - ami-d05e75b8 to install the following required software. \n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n -Docker image tutum/hello-world (latest version) \n -Docker image postgres (latest version) \n Install Docker, Kubernetes, and Kubectl as discussed in chapter 1. To log in to the Ubuntu instance the \nPublic IP Adress may be obtained from the Amazon EC2 console as shown in Figure  13-2 . \n SSH Login to the Ubuntu instance. \n ssh -i \"docker.pem\" ubuntu@52.90.62.35 \n After having installed Docker start Docker and verify its status. \n sudo service docker start \n sudo service docker status \n Figure 13-2.  Obtaining the Public IP Address \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n378\n Docker should be listed as being “running” as shown in Figure  13-3 . \n Creating a Service \n Create a service definition file  hello-postgres-service.yaml to configure the service ports. We shall \nbe configuring two service ports, one for the  hello-world application and the other for the  postgres \napplication. The fields in the service definition file are discussed in Table  13-1 . \n Figure 13-3.  Starting Docker \n Table 13-1.  Fields in the Service Definition File \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n Service \n metadata \n The service metadata. \n metadata - > labels \n The service labels. The setting translates to label \n app = MultiContainerApp \n app: MultiContainerApp \n metadata - > name \n The service name. \n hello-postgres \n spec \n The service specification. \n spec - > ports \n The ports exposed by the service. Two ports are \nexposed, one for the hello-world application and \nthe other for the postgres application. \n name: hello-world \n port: 8080 \n name: postgres \n port: 5432 \n spec - > selector \n The Pod selector. Service routes traffic to the Pods \nwith label matching the selector expression. The \nsetting translates to selector \n app = MultiContainerApp \n app: MultiContainerApp \n spec - > selector - > type \n The service type. \n LoadBalancer \n The  hello-postgres-service.yaml is listed: \n apiVersion: v1 \n kind: Service \n metadata:  \n  labels:  \n    app: MultiContainerApp \n  name: hello-postgres \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n379\n spec:  \n  ports:  \n    -  \n      name: hello-world \n      port: 8080 \n    -  \n      name: postgres \n      port: 5432 \n  selector:  \n    app: MultiContainerApp \n  type: LoadBalancer \n Create a service from the definition file. \n kubectl create -f hello-postgres-service.yaml  \n Subsequently list the services. \n kubectl get services \n The  hello-postgres service gets created and listed as shown in Figure  13-4 . \n Describing a Service  \n The  hello-postgres service may be described with the following command. \n kubectl describe service hello-postgres \n The service description includes the name, namespace, labels, selector, type, IP, ports, and endpoints as \nshown in Figure  13-5 . Initially the service is not managing any pods and as a result no endpoints are listed. \n Figure 13-4.  Creating a Service from the Definition File \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n380\n Creating a Replication Container \n Create a definition file  hello-postgres-rc.yaml for a replication controller. Add the following (Table  13-2 ) \nfields to the definition file. \n Table 13-2.  Fields in the Replication Controller Definition File \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n ReplicationController \n metadata \n The replication controller metadata. \n metadata - > labels \n The replication controller labels. \n app: \"MultiContainerApp\" \n metadata - > name \n The replication controller name. \n \"hello-postgres\" \n spec \n The replication controller specification. \n spec - > replicas \n The number of Pod replicas. \n 1 \n spec - > selector \n A key:value expression for selecting the \nPods to manage. Pods with a label the same \nas the selector expression is managed by \nthe replication controller. The selector \nexpression must be the same as the \nspec - > template - > metadata - > labels \nexpression. The selector defaults to the spec \n- > template - > metadata - > labels key: value \nexpression if not specified. \n app: \"MultiContainerApp\" \n Figure 13-5.  Describing the Service \n(continued)\n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n381\n The  hello-postgres-rc.yaml is listed: \n apiVersion: v1 \n kind: ReplicationController \n metadata:  \n  labels:  \n    app: \"MultiContainerApp\" \n  name: \"hello-postgres\" \n spec:  \n  replicas: 1 \n  selector:  \n    app: \"MultiContainerApp\" \n  template:  \n    metadata:  \n      labels:  \n        app: \"MultiContainerApp\" \n Field \n Description \n Value \n spec - > template \n The Pod template. \n spec - > template- > metadata \n The Pod template metadata. \n spec - > template - > metadata \n- > labels \n The Pod template labels. The selector if not \nspecified defaults to this setting. The service \nselector must be the same as one of the Pod \ntemplate labels for the service to represent \nthe Pod. The service selector does not \ndefault to the same value as the label and \nwe already set the service selector to app: \nMultiContainerApp. \n app: “MultiContainerApp” \n spec - > template - > spec \n The Pod template specification. \n spec - > template - > spec \n- > containers \n The containers configuration for the Pod \ntemplate. \n spec - > template - > spec \n- > containers - > image \n The Docker image for the hello-world \ncontainer. \n tutum/hello-world \n spec - > template - > spec \n- > containers - > name \n The container name for the hello-world \ncontainer. \n hello-world \n ports \n Specifies the container port for the hello-\nworld container. \n containerPort: 8080 \n spec - > template - > spec \n- > containers - > image \n The Docker image for the postgres container.  postgres \n spec - > template - > spec \n- > containers - > name \n The container name for the postgres \ncontainer. \n postgres \n ports \n Container port for postgres container. \n containerPort: 5432 \nTable 13-2. (continued)\n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n382\n    spec:  \n      containers:  \n        -  \n          image: \"tutum/hello-world\" \n          name: \"hello-world\" \n          ports:  \n            -  \n              containerPort: 8080 \n        -  \n          image: \"postgres\" \n          name: \"postgres\" \n          ports:  \n            -  \n              containerPort: 5432 \n Create a replication controller from the definition file. \n kubectl create -f hello-postgres-rc.yaml \n Subsequently list the replication controllers. \n kubectl get rc \n As shown in Figure  13-6 the  hello-postgres replication controller gets created and listed. \n Listing the Pods \n To list the Pods run the following command. \n kubectl get pods \n As  replicas field is set to 1 in the replication controller only one Pod gets created as shown in \nFigure  13-7 . The READY column lists 0/2, which indicates that 0 or none of the two containers in the pod are \nready. Initially the container could be listed as not running and creating. Run the preceding command after \na few seconds and the Pod STATUS should be “Running” and the READY state should be 2/2, implying that 2 \nof 2 containers are running. \n Figure 13-6.  Creating a Replication Controller from the Definition File \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n383\n Listing the Docker Containers \n To list the Docker containers started, run the following command. \n sudo docker ps \n Two of the listed containers, the container based on the  postgres image and the container based on \nthe  tutum/hello-world image, as shown in Figure  13-8 , are started with the replication controller \n hello-postgres . \n Figure 13-7.  Listing the Pods \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n384\n Describing the Service after Creating Replication Controller \n Before we had created the replication controller the service  hello-postgres was not associated with any \nendpoints. After creating the replication controller and the Pod/s, run the following command again to \ndescribe the service again. \n kubectl describe service hello-postgres \n An endpoint is listed for each of the ports exposed by the service as shown in Figure  13-9 . \n Figure 13-8.  Listing the Docker Containers \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n385\n Invoking the Hello World Application on Command Line \n Invoke the service endpoint  172.17.0.2 using curl as follows. \n curl 172.17.0.2 \n The HTML generated by the application gets output as shown in Figure  13-10 . \n Figure 13-9.  Describing the Service \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n386\n Starting the Interactive Shell \n To start an interactive shell for the software installed, either of the Docker containers, listed previously in \nFigure  13-8 , for the multi-container Pod may be used. Both the containers access the same filesystem and IP. \nUse the following command to start an  interactive shell. \n sudo docker exec -it 2e351a609b5b bash \n An interactive shell gets started as shown in Figure  13-11 . \n Figure 13-10.  Invoking an Endpoint for the Service \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n387\n Starting PostgreSQL Shell \n To start the  PostgreSQL command shell called  psql run the following command in the interactive shell. \n psql postgres \n The  psql gets started and the  postgres command prompt gets displayed as shown in Figure  13-12 . \n PostgreSQL with Kubernetes is discussed in chapter 5. \n Setting Port Forwarding \n We had earlier invoked the service endpoint to output the HTML generated using curl on the command line, \nbut HTML is best displayed in a browser. As an Amazon EC2 instance does not provide a browser by default, \nwe need to set port forwarding to a local machine to be able to access the service endpoint in a browser. Set \nthe  port forwarding for  172.17.0.2:80 to  localhost:80 with the following command. \n ssh -i \"docker.pem\" -f -nNT -L 80:172.17.0.2:80 ubuntu@ec2-52-90-62-35.compute-1.amazonaws.com \n The port forwarding to  localhost gets set as shown in Figure  13-13 . \n The Public DNS for the Amazon EC2 instance may be obtained from the Amazon EC2 console as shown \nin Figure  13-14 . \n Figure 13-12.  Starting psql Shell \n Figure 13-13.  Setting Port Forwarding \n Figure 13-11.  Starting an Interactive Shell \n \n \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n388\n Opening the Hello World Application in a Browser \n Having set port forwarding the application may be opened in a browser on a local machine with url \n http://localhost as shown in Figure  13-15 . In addition to the hostname the two ports at which the \n HELLO_POSTGRES is listening at get listed. \n Figure 13-14.  Obtaining Public DNS \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n389\n Scaling the Cluster \n To  scale the cluster to 3 replicas or Pods run the following command. \n kubectl scale rc hello-postgres --replicas=3 \n Subsequently list the Pods. \n kubectl get pods \n Three Pods get listed as shown in Figure  13-16 . Some of the Pods could be not running or not ready \ninitially. Run the preceding command again after a few seconds to list all the Pods with STATUS as \n“Running” and READY state as 2/2. \n Figure 13-15.  Invoking the Service  Endpoint in a Browser \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n390\n A Pod may be described using the  kubectl describe pod command. For example, describe the \n hello-postgres-jliem pod with the following command. \n kubectl describe pod hello-postgres-jliem \n As shown in Figure  13-17 the Pod description gets listed. \n Figure 13-16.  Scaling the Cluster to 3 Replicas \n Figure 13-17.  Describing a Pod \n \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n391\n Listing the Docker Containers \n As each Pod consists of two containers, scaling up the cluster to 3 Pods or replicas starts four new containers, \n2 containers for each of the two new Pods. After scaling up the cluster run the following command to list the \nrunning  Docker containers again using the default output format. \n sudo docker ps \n A total of 3 containers based on the  postgres image and 3 containers based on the  tutum/hello-world \nimage get listed as shown in Figure  13-18 . \n Figure 13-18.  Listing the Docker Containers \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n392\n Describing the Service after Scaling \n Describe the  service again after scaling up the cluster. \n kubectl describe service hello-postgres \n Each of the ports exposed by the service is associated with three endpoints because 3 Pods are running \nas shown in Figure  13-19 . \n Setting Port Forwarding \n To be able to open the application in a browser we need to set port forwarding to locahost. Set the port \nforwarding to ports not previously bound. The  localhost:80 beind address is already sued up in the port \nforwarding of the single Pod created earlier. To set  port forwarding for the two new Pods use ports 81 and 82 \non localhost. \n ssh -i \"docker.pem\" -f -nNT -L 81:172.17.0.3:80 ubuntu@ec2-52-90-62-35.compute-1.amazonaws.com \n ssh -i \"docker.pem\" -f -nNT -L 82:172.17.0.4:80 ubuntu@ec2-52-90-62-35.compute-1.amazonaws.com \n The preceding commands do not generate any output but the ports get forwarded to the  localhost as \nshown in Figure  13-20 . \n Figure 13-19.  Describing the Service including the Service Endpoints \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n393\n Opening the Hello World Application in a Browser \n The application may be opened in a  browser at each of the forwarded ports; for example, open a browser at \n http://localhost:81 . The application HTML gets displayed as shown in Figure  13-21 . The  HELLO_POSTGRES \nservice is listening at two ports 8020 and 5432. \n Similarly open the other service endpoint in a browser with url  http://localhost:82 . Different \nhostnames listening on the same port are forwarded to different ports on the  localhost . The service \nendpoint HTML gets output as shown in Figure  13-22 . \n Figure 13-20.  Setting Port Forwarding \n Figure 13-21.  Invoking a Service Endpoint in a Browser \n \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n394\n Invoking the Hello World Application from Command Line \n As for a single container Pod, each of the two new service endpoints may be invoked on the  command line . \nFor example, invoke the  172.17.0.3 endpoint with the following curl command. \n curl 172.17.0.3 \n The HTML for the service endpoint gets output as shown in Figure  13-23 . \n Figure 13-22.  Invoking another Service Endpoint in a Browser \n \n",
      "page_number": 384
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 402-416)",
      "start_page": 402,
      "end_page": 416,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n395\n Invoke the  172.17.0.4 endpoint with the following curl command. \n curl 172.17.0.4 \n The HTML for the service endpoint gets output as shown in Figure  13-24 . \n Figure 13-23.  Invoking a Service Endpoint with curl \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n396\n Deleting the Replication Controller \n To delete the  hello-postgres replication controller run the following command. \n kubectl delete rc hello-postgres \n Subsequently list the Pods with the following command. \n kubectl get pods \n The Pods for the  hello-postgres replication controller are not listed as shown in Figure  13-25 . \n Figure 13-24.  Invoking another Service Endpoint with curl \n \n\n\nCHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n397\n Deleting the Service \n To delete the  service  hello-postgres run the following command. \n kubectl delete service hello-postgres \n Subsequently run the following command to list the services. \n kubectl get services \n The  hello-postgres service is not listed as shown in Figure  13-26 . \n Summary \n In this chapter we discussed using multiple containers in a Pod. We discussed the use case for a multi-container \nPod and used the  tutum/hello-world and  postgres Docker images to create a multi-container Pod. A \nmulti-container pod starts multiple Docker containers for each Pod even though the Pod is the atomic unit. \nThe multiple containers in a Pod share the same IP address and filesystem. When a multi-container Pod is \nscaled, multiple containers are started for each of the new Pods. In the next chapter we shall discuss installing \nKubernetes on a multi-node cluster. \n Figure 13-25.  Deleting the Replication Controller \n Figure 13-26.  Deleting the Service \n \n \n\n\n399\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_14\n CHAPTER 14 \n Installing Kubernetes on \na Multi-Node Cluster \n In all of the preceding chapters in the book we have used a single-node cluster. For most small scale \napplications a single-node cluster should suffice. But, for relatively large scale, distributed applications a \n multi-node cluster is a more suitable option. In this chapter we shall install Kubernetes on a multi-node \n(2 nodes) cluster. This chapter has the following sections.\n Components of a Multi-Node Cluster \n Setting the Environment \n Installing the Master Node\n Setting Up Flanneld and etcd \n Starting the Kubernetes on Master Node \n Running the Service Proxy \n Testing the Master Node \n Adding a Worker Node\n Exporting the Master IP \n Setting Up Flanneld and etcd \n Starting Up Kubernetes on Worker Node \n Running the Service Proxy \n Testing the Kubernetes Cluster \n Running an Application on the Cluster \n Exposing the Application as a Service \n Testing the Application in a Browser \n Scaling the Application \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n400\n Components of a Multi-Node Cluster \n A multi-node cluster consists of the following main and ancillary  components. \n -Kubernetes Master Node \n -Kubernetes Worker Node/s \n -Etcd \n -Flannel \n -Service Proxy \n -Kubectl \n etcd, kubernetes master, and service proxy were discussed in chapter  1 . etcd as introduced in chapter  1 \nis a distributed, key-value store used by the Kubernetes cluster manager. We have installed etcd on the \nsame node as the Kubernetes master but in a production environment etcd would typically be installed as \nseparate cluster installed on nodes different than the Kubernetes master node. A commit to an etcd cluster \nis based on replication to a majority (quorum) of available nodes with provision for failure of one or more \nnodes. While the majority of a 1-node cluster is 1, the majority of a 3-node cluster is 2, majority of a 4-node \ncluster is 3, majority of a 5-node cluster is 3. A etcd cluster would typically have an odd number (>2) of \nnodes with tolerance for failure. For example, a 5-node etcd cluster could loose up to 2 nodes resulting in \na 3-node cluster in which the majority is still determinable. A 3-node cluster has a failure tolerance for one \nmore node. A 2-node etcd cluster does not have any failure tolerance and the majority of a 2-node cluster is \nconsidered as 2. The recommended etcd cluster size in production is 3,5, or 7. \n Flannel is a network fabric for containers. Flannel provides a subnet to each host that is used by \ncontainers at runtime. Actually, Flannel runs an agent called flanneld on each host that allocates subnets. \nFlannel sets up and manages the network that interconnects all the Docker containers created by \nKubernetes. Flannel is etcd backed and uses etcd to store the network configuration, allocated subnets, and \nauxiliary data such as the IP Address of the host. \n Setting the Environment \n We have used Amazon EC2 instances created from Ubuntu Server 14-04 LTS (HVM), SSD Volume Type - \nami-d05e75b8 AMI for this chapter. The following software is required to be installed for this chapter. \n -Docker Engine (latest version) \n -Kubernetes on Master Node (version 1.01) \n -Kubernetes on Worker Node (version 1.01) \n -Kubectl (version 1.01) \n Because we are creating a multi-node cluster we need to create multiple Amazon EC2 instances. For a \ntwo-node cluster create two Amazon EC2 instances – KubernetesMaster and KubernetesWorker – as shown \nin Figure  14-1 . \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n401\n SSH Login to each node separately. The Public IP Address for the Master Node may be obtained from \nthe Amazon EC2 console as shown in Figure  14-2 . \n Figure 14-1.  Creating two Ubuntu Instances for Kubernetes Master and Worker Nodes \n Figure 14-2.  Obtaining the Public IP Address for a Ubuntu Instance \n Log in to the Ubuntu instance for the Master node. \n ssh -i \"docker.pem\" ubuntu@52.91.243.99 \n Similarly, obtain the Public IP Address for the Ubuntu instance for the Worker node and log in to the \nUbuntu instance for the Worker node. \n ssh -i \"docker.pem\" ubuntu@52.23.236.15 \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n402\n Install Docker and Kubectl on each node as discussed in chapter  1 . Do not install Kubernetes just as \nchapter  1 because a multi-node configuration for Kubernetes is different than a single-node configuration. \n Start the Docker Engine and verify its status. \n sudo service docker start \n sudo service docker status \n Docker engine should be listed as “running” as shown in Figure  14-3 . \n Figure 14-3.  Starting Docker \n Installing the Master Node \n The Master node hosts the API server and assigns work to worker node/s. We need to run two Docker \ndaemons, a main Docker instance and a bootstrap Docker instance. The main Docker instance is used by \nthe Kubernetes and the bootstrap Docker instance is used by flannel, an etcd. The flannel daemon sets up \nand manages the network that interconnects all the Docker containers created by Kubernetes. \n Setting Up Flanneld and etcd  \n Setting Up Flanneld and etcd involves setting up a bootstrap instance for Docker, starting etcd for flannel \nand the API server, and setting up flannel on the master node. \n Setting up Bootstrap Instance of Docker \n Flannel, which sets up networking between Docker containers; and etcd on which flannel relies, run inside \nDocker containers themselves. A separate  bootstrap Docker is used because flannel is used for networking \nbetween Docker containers created by Kubernetes; and running flannel and Kubernetes in the same Docker \nengine could be problematic and is not recommended. Create a separate bootstrap instance of Docker for \nflannel and etcd. \n sudo sh -c 'docker daemon -H unix:///var/run/docker-bootstrap.sock -p /var/run/docker-\nbootstrap.pid --iptables=false --ip-masq=false --bridge=none --graph=/var/lib/docker-\nbootstrap 2> /var/log/docker-bootstrap.log 1> /dev/null &' \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n403\n The bootstrap Docker daemon gets started and the output from the preceding command is shown in \nFigure  14-4 . \n Figure 14-4.  Starting the Bootstrap Daemon on the Master Node \n Figure 14-5.  Setting up etcd on the Master Node \n The ‘–d’ option is completely removed in Docker 1.10 and replaced with ‘daemon’. If using the Docker version \nprior to Docker 1.10, for example Docker 1.9.1, replace 'daemon’ with '-d' in the preceding command to run the \ncommand as follows: \n sudo sh -c 'docker -d -H unix:///var/run/docker-bootstrap.sock -p /var/run/docker-bootstrap.\npid --iptables=false --ip-masq=false --bridge=none --graph=/var/lib/docker-bootstrap 2> /\nvar/log/docker- bootstrap.log 1> /dev/null &' \n Setting Up etcd \n Set up  etcd for the flannel and the API server with the following command. \n sudo docker -H unix:///var/run/docker-bootstrap.sock run --net=host -d gcr.io/google_\ncontainers/etcd:2.0.12 /usr/local/bin/etcd --addr=127.0.0.1:4001 --bind-addr=0.0.0.0:4001 \n--data-dir=/var/etcd/data \n The container for etcd gets downloaded and etcd gets installed as shown in Figure  14-5 . \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n404\n Set up a Classless Inter-Domain Routing (CIDR), which is an IP Addressing scheme that reduces the \nsize of routing tables and makes more addresses available, range for flannel. \n sudo docker -H unix:///var/run/docker-bootstrap.sock run --net=host gcr.io/google_\ncontainers/etcd:2.0.12 etcdctl set /coreos.com/network/config '{ \"Network\": \"10.1.0.0/16\" }' \n The preceding command does not generate any output as shown in Figure  14-6 . \n Figure 14-7.  Stopping Docker Temporarily \n Figure 14-6.  Setting Up CIDR on the Master Node \n Setting Up Flannel \n By default Docker does provide a networking between containers and Pods but the networking provided by \n Flannel is much more simplified. We shall be using Flannel for networking. First, we need to stop Docker. \n sudo service docker stop \n Docker gets stopped as shown in Figure  14-7 . \n Run flannel with the following command. \n sudo docker -H unix:///var/run/docker-bootstrap.sock run -d --net=host --privileged -v /dev/\nnet:/dev/net quay.io/coreos/flannel:0.5.0 \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n405\n Flannel gets installed as shown in Figure  14-8 . \n Figure 14-8.  Installing Flannel \n Figure 14-9.  Obtaining the Hash Generated by Flannel \n Flannel generates a hash as shown in Figure  14-9 . Copy the Hash. \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n406\n Make a note of the  FLANNEL_SUBNET and  FLANNEL_MTU values as we shall need these to edit the Docker \nconfiguration. Open the Docker configuration file in a vi editor. \n sudo vi /etc/default/docker \n The default settings in the docker configuration file are shown in Figure  14-11 . \n Copy and paste the hash into the following command, and run the command to obtain the subnet settings. \n sudo docker -H unix:///var/run/docker-bootstrap.sock exec <really-long-hash-from-above-here> \ncat /run/flannel/subnet.env \n The subnet settings get listed as shown in Figure  14-10 . \n Figure 14-10.  Listing the Subnet Settings \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n407\n To the  DOCKER_OPTS setting append the following parameters whose values are obtained from the \noutput in Figure  14-10 . \n --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} \n Figure 14-11.  Docker Configuration File Default Settings \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n408\n The modified docker configuration file is shown in Figure  14-12 . \n Figure 14-12.  Modified Docker Configuration File \n As mentioned before Docker provides its own networking with a Docker bridge called  docker0 . As we \nwon’t be using the default Docker bridge remove the default Docker bridge. For the brctl binaries first install \nthe bridge-utils package. \n sudo /sbin/ifconfig docker0 down \n sudo apt-get install bridge-utils \n sudo brctl delbr docker0 \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n409\n The output from installing the bridge-utils package and removing the docker0 bridge is shown in \nFigure  14-13 . \n Figure 14-14.  Restarting Docker \n Figure 14-13.  Removing docker0 bridge \n Restart Docker. \n sudo service docker start \n Docker gets restarted as shown in Figure  14-14 . \n Starting the Kubernetes Master \n Setting up flannel networking is the main difference between setting up a single-node cluster and a \nmulti-node cluster. Start the  Kubernetes master with the same command as used for a single-node cluster. \n sudo docker run \\ \n  --volume=/:/rootfs:ro \\ \n  --volume=/sys:/sys:ro \\ \n  --volume=/dev:/dev \\ \n  --volume=/var/lib/docker/:/var/lib/docker:rw \\ \n  --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\ \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n410\n  --volume=/var/run:/var/run:rw \\ \n  --net=host \\ \n  --privileged=true \\ \n  --pid=host \\  \n  -d \\ \n  gcr.io/google_containers/hyperkube:v1.0.1 /hyperkube kubelet --api-servers=\nhttp://localhost:8080 --v=2 --address=0.0.0.0 --enable-server --hostname-override=127.0.0.1 --config=/\netc/kubernetes/manifests-multi --cluster-dns=10.0.0.10 --cluster-domain=cluster.local \n The preceding command is run from the Master Node as shown in Figure  14-15 . \n Figure 14-15.  Starting Kubernetes on the Master Node \n \n",
      "page_number": 402
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 417-425)",
      "start_page": 417,
      "end_page": 425,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n411\n Kubernetes gets installed on the master node as shown in Figure  14-16 . \n Figure 14-16.  Kubernetes Started on Master Node \n Running the Service Proxy \n Run the  service proxy also using the same command as used for a single-node cluster. \n sudo docker run -d --net=host --privileged gcr.io/google_containers/hyperkube:v1.0.1 /\nhyperkube proxy --master=http://127.0.0.1:8080 --v=2 \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n412\n Service proxy gets installed as shown in Figure  14-17 . \n Figure 14-17.  Starting Service proxy on Master Node \n Figure 14-18.  Listing the Nodes, only the Master Node to start with \n Testing the One-Node Cluster \n To  test the master node run the following command, which lists the nodes in the cluster. \n kubectl get nodes \n The single node gets listed as shown in Figure  14-18 . \n Adding a Worker Node \n Setting up a  worker node is very similar to setting up the master node. Next, we shall set up a worker node. \nSSH login to the Ubuntu instance for the worker node. \n Exporting the Master IP \n First, we need to set the environment variable  MASTER_IP . Obtain the Public IP Address for the Ubuntu \ninstance running the master node as shown in Figure  14-19 . \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n413\n Export the environment variable  MASTER_IP using the Public IP Address. \n export MASTER_IP=52.91.243.99 \n Echo the  MASTER_IP environment variable. \n echo $MASTER_IP \n The output from the preceding command is shown in Figure  14-20 . \n Figure 14-19.  Obtaining the Master Node’s IP Address \n Figure 14-20.  Exporting the MASTER_IP Environment Variable on a Worker Node \n Setting Up Flanneld \n Start a bootstrap Docker daemon just for the  flannel networking. \n sudo sh -c 'docker daemon -H unix:///var/run/docker-bootstrap.sock -p /var/run/docker-\nbootstrap.pid --iptables=false --ip-masq=false --bridge=none --graph=/var/lib/docker-\nbootstrap 2> /var/log/docker-bootstrap.log 1> /dev/null &' \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n414\n Bootstrap Docker gets set up as shown in Figure  14-21 . \n Figure 14-21.  Starting Bootstrap Docker on the Worker Node \n The ‘–d’ option is completely removed in Docker 1.10 and replaced with ‘daemon’. If using the Docker version \nprior to Docker 1.10, for example Docker 1.9.1, replace 'daemon’ with '-d' in the preceding command to run the \ncommand as follows: \n sudo sh -c 'docker -d -H unix:///var/run/docker-bootstrap.sock -p /var/run/docker-bootstrap.\npid --iptables=false --ip-masq=false --bridge=none --graph=/var/lib/docker-bootstrap 2> /\nvar/log/docker- bootstrap.log 1> /dev/null &' \n To install Flannel, first we need to stop the Docker engine. \n sudo service docker stop \n Docker engine gets stopped as shown in Figure  14-22 . \n Figure 14-22.  Stopping Docker Temporarily on the Worker Node \n Next, install flannel on the worker node. The same etcd that is running on the master is used for \nthe flanneld on the worker node. The etcd instance includes the Master’s Ip using the  MASTER_IP \nenvironment variable. \n sudo docker -H unix:///var/run/docker-bootstrap.sock run -d --net=host --privileged \n-v /dev/net:/dev/net quay.io/coreos/flannel:0.5.0 /opt/bin/flanneld --etcd-\nendpoints=http://${MASTER_IP}:4001 \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n415\n Flannel gets set up on the worker node as shown in Figure  14-23 . \n Figure 14-23.  Installing Flannel on the Worker Node \n Copy the hash generated by the preceding command as shown in Figure  14-24 . \n Figure 14-24.  Obtaining the Hash geenrated by Flannel \n Using the hash value in the following command obtain the subnet settings from flannel. \n sudo docker -H unix:///var/run/docker-bootstrap.sock exec <really-long-hash-from-above-here> \ncat /run/flannel/subnet.env \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n416\n The subnet settings get output as shown in Figure  14-25 . \n Figure 14-25.  Listing the Subnet Settings on the Worker Node \n Using the subnet settings we need to edit the Docker configuration file. Open the Docker configuration \nfile in the vi editor. \n sudo /etc/default/docker \n Append the following parameters to the  DOCKER_OPTS setting. Substitute the values for  FLANNEL_SUBNET \nand  FLANNEL_MTU as obtained from Figure  14-25 . \n --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n417\n The modified Docker configuration file is shown in Figure  14-26 . \n Figure 14-26.  Modified Docker Configuration File \n Shut down and remove the existing Docker bridge  docker0 , which is used by default by Docker for \nnetworking between containers and Pods. The bridge-utils package is needed to be installed as it is not \navailable by default on an Ubuntu instance on Amazon EC2. \n sudo /sbin/ifconfig docker0 down \n sudo apt-get install bridge-utils \n sudo brctl delbr docker0 \n Restart Docker. \n sudo service docker start \n The Docker engine gets started as shown in Figure  14-27 . \n Figure 14-27.  Restarting Docker \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n418\n Starting Up Kubernetes on Worker Node \n Start up Kubernetes on the  worker node with the same command as used in the Master node with the \ndifference that instead of setting the  --api-servers to  http://localhost:8080 set the --api-servers to the \n http://${MASTER_IP}:8080 . \n sudo docker run \\ \n  --volume=/:/rootfs:ro \\ \n  --volume=/sys:/sys:ro \\ \n  --volume=/dev:/dev \\ \n  --volume=/var/lib/docker/:/var/lib/docker:rw \\ \n  --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\ \n  --volume=/var/run:/var/run:rw \\ \n  --net=host \\ \n  --privileged=true \\ \n  --pid=host \\  \n  -d \\ \n  gcr.io/google_containers/hyperkube:v1.0.1 /hyperkube kubelet --api-\nservers=http://${MASTER_IP}:8080 --v=2 --address=0.0.0.0 --enable-server --hostname-\noverride=$(hostname -i) --cluster-dns=10.0.0.10 --cluster-domain=cluster.local \n The preceding command is to be run on the worker node as shown in Figure  14-28 . \n Figure 14-28.  Starting Kubernetes on the Worker Node \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n419\n Running the Service Proxy \n The  service proxy on the worker node is also run with the same command as for the master node \nexcept that the Master’s Ip parameter  -- master= http://127.0.0.1:8080 should be replaced with \n --master=http://${MASTER_IP}:8080 . \n sudo docker run -d --net=host --privileged gcr.io/google_containers/hyperkube:v1.0.1 /\nhyperkube proxy --master=http://${MASTER_IP}:8080 --v=2 \n The service proxy gets started as shown in Figure  14-29 . \n Figure 14-29.  Starting Service Proxy on the Worker Node \n Testing the Kubernetes Cluster \n From the Master node, not the  worker node that was being configured in the preceding commands, list the \nnodes in the cluster. \n kubectl get nodes \n Two nodes get listed as shown in Figure  14-30 : the master node and the worker node. \n Figure 14-30.  Listing a Two-Node Cluster \n Add more nodes as required using the same procedure as discussed in this section Adding a Worker Node. \n Running an Application on the Cluster \n To test the cluster run an application on the command line using kubectl. As an example, run the Docker \nimage “nginx” with the following command. \n kubectl -s http://localhost:8080 run nginx --image=nginx --port=80 \n Subsequently list the Pods. \n kubectl get pods \n \n \n",
      "page_number": 417
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 426-435)",
      "start_page": 426,
      "end_page": 435,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n420\n Figure 14-31.  Installing an Application on the Cluster \n Figure 14-32.  Creating a Service \n The nginx application container is created and the nginx replication controller is created with default of \n1 replicas as shown in Figure  14-31 . One pod gets listed, also shown in Figure  14-31 . Initially the Pod could \nbe listed as Pending status. Run the preceding command after a few seconds to list the Pod as running and \nready. To find on which instance/s (node/s) in the cluster the Pod/s is/are running on, run the command. \n kubectl get pods -o wide. \n  Exposing the Application as a Service \n To expose the replication controller nginx as a service run the following command. \n kubectl expose rc nginx --port=80 \n The nginx service gets created as shown in Figure  14-32 . \n List the services with the following command. \n kubectl get services \n To be able to invoke the service obtain the first cluster Ip with the following command as shown in \nFigure  14-33 . \n kubectl get svc nginx --template={{.spec.clusterIP}} \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n421\n Figure 14-33.  Invoking a Web Server with Curl \n The HTML returned from the nginx application is output as shown in Figure  14-34 . \n Invoke the web server using the cluster Ip returned, 10.0.0.99. \n curl 10.0.0.99 \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n422\n  Testing the Application in a Browser \n To invoke the service endpoint in a browser, set port forwarding from  10.0.0.99:80 endpoint to \n localhost:80 . \n ssh -i docker.pem -f -nNT -L 80:10.0.0.99:80 ubuntu@ec2-52-91-243-99.compute-1.amazonaws.com \n Port forwarding gets set as shown in Figure  14-35 . \n Figure 14-35.  Setting Port Forwarding \n Figure 14-34.  The HTML generated by the Application \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n423\n Invoke the nginx application in a local browser with url  http://localhost as shown in Figure  14-36 . \n Figure 14-36.  Invoking a Service Endpoint in a Browser \n  Scaling the Application \n Scaling is a common usage pattern of Replication Controller. The nginx replication controller may be scaled \nwith the  kubectl scale command. As an example, scale to 3 replicas. \n kubectl scale rc nginx --replicas=3 \n Subsequently list the Pods. \n kubectl get pods \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n424\n Figure 14-37.  Listing the Pods \n Describe the service with the following command. \n kubectl describe svc nginx \n Three service endpoints get listed as shown in Figure  14-38 . \n Figure 14-38.  Describing the Service \n To be able to invoke each of the service endpoints in a browser on a local machine, set the port forwarding. \n ssh -i docker.pem -f -nNT -L 8081:10.1.34.2:80 ubuntu@ec2-52-91-243-99.compute-1.amazonaws.com \n ssh -i docker.pem -f -nNT -L 8082:10.1.35.2:80 ubuntu@ec2-52-91-243-99.compute-1.amazonaws.com \n ssh -i docker.pem -f -nNT -L 8083:10.1.35.3:80 ubuntu@ec2-52-91-243-99.compute-1.amazonaws.com \n An output of “scaled” indicates that the replication controller has been scaled. Three Pods get listed as \nshown in Figure  14-37 . \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n425\n Port forwarding gets set as shown in Figure  14-39 . \n Figure 14-39.  Setting port Forwarding for the additional Service Endpoints \n Figure 14-40.  Invoking a Service Endpoint in a Browser \n The service endpoints may be invoked in a local browser. For example the url  http://localhost:8081 \ninvokes one of the service endpoints as shown in Figure  14-40 . \n \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n426\n Similarly, the url  http://localhost:8082 invokes another service endpoint as shown in Figure  14-41 . \n Figure 14-41.  Invoking another Service Endpoint in a Browser \n Similarly, the url  http://localhost:8083 invokes the third service endpoint as shown in Figure  14-42 . \n \n\n\nCHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n427\n Summary \n In this chapter we installed Kubernetes on a multi-node cluster. The multi-node configuration makes \nuse of flannel for networking instead of the default networking provided by Docker. First, we installed \nKubernetes on the master node. Using the Master’s Ip Address we installed Kubernetes on a worker node, \nas a result creating a two-node cluster. As many worker nodes as required may be added using the same \nprocedure. We created an application using the nginx Docker image and invoked the application on the \ncommand line using curl, and in a local browser using port forwarding. We also scaled the application. \nIn a single-node cluster an application runs on the master node itself. In a multi-node cluster an application \nruns on both the worker nodes and the master node. This chapter concludes the book on Kubernetes \nMicroservices with Docker. \n Figure 14-42.  Invoking a Third Service Endpoint in a Browser \n \n\n\n429\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2\n\n\n\n A, B \n Apache Cassandra \n Amazon EC2 instance , 201 \n cluster declaratively \n CatalogKeyspace , 215, 219 \n CQL shell , 215 \n data deletion , 218 \n DROP TABLE clause , 218 \n interactive shell , 213 \n Pod , 212 \n replication controller creation , 206 \n scaling database , 211 \n service creation , 203 \n table creation , 216 \n truncate table , 218 \n volume creation , 219 \n cluster imperatively \n replication controller creation , 225 \n replication controller deletion , 229 \n scaling database , 228 \n service creation , 227 \n service deletion , 230 \n Docker engine , 202 \n dynamic column \nspecifi cation , 201 \n fl exible schema \ndata model , 201 \n kubectl , 202 \n Kubernetes , 202 \n Apache Hadoop , 277 \n cluster declaratively , 278 \n interactive shell , 286 \n logs list , 284 \n MapReduceapplication \n(see  MapReduce application ) \n Pods list , 283 \n replication controller creation , 281 \n scaling , 285 \n service creation , 279 \n cluster imperatively , 307 \n interactive shell , 310 \n Pods list , 308 \n replication controller creation , 307 \n scaling , 309 \n service creation , 309 \n environment settings , 277 \n Apache HBase , 302 \n Apache Hive , 296 \n Apache Kafka , 347 \n confi guration properties , 364 \n consumer , 367 \n docker image, modifi cation , 349 \n environment settings , 348 \n interactive shell , 363 \n messages , 367 \n pod describing , 362 \n pods list , 361 \n producer , 367 \n replication controller creation , 358 \n replication controller deletion , 370 \n scaling , 368 \n service creation , 355 \n topic creation , 366 \n Apache Solr , 313 \n Admin Console , 338 \n core creation , 328 \n environment settings , 314 \n indexing documents , 329 \n interactive shell , 325 \n logs list , 323 \n pods list , 321 \n port forwarding , 337 \n replication controller creation , 318 \n replication controller describing , 322 \n scaling , 344 \n service creation , 315 \n service describing , 317 \n service endpoints , 317 \n using REST client , 332 \n Index \n\n\n■ INDEX\n430\n\n\n\n C, D \n Cassandra Query Language (CQL) , 215, 217 \n cbtransfer tool , 265 \n Couchbase , 231 \n cluster declaratively , 234 \n catalog2 document , 262 \n Data Buckets Tab , 255 \n default fi elds , 260 \n endpoints , 244 \n interactive shell , 264 \n JSON document , 261 \n logs list , 243 \n Pod , 234, 243 \n port forwarding , 245 \n replication controller creation , 239 \n server confi guration , 247 \n service creation , 237 \n service describing , 244 \n web console , 246 \n cluster imperatively , 266 \n Pods list , 266 \n port forwarding , 272 \n replication controller creation , 266 \n replication controller deletion , 270 \n scaling , 269 \n service creation , 268 \n web console , 272 \n environment settings , 231 \n Docker engine , 233 \n Kubernetes service , 234 \n Public IP Address , 232 \n Ubuntu instance , 233 \n Custom command \n Args fi elds , 80 \n CMD instruction , 78–79 \n ENTRYPOINT entry , 78 \n environment setting , 77–78 \n\n\n\n E, F, G, H, I, J \n Environment variables \n Args mapping , 92 \n command mapping , 84, 89 \n defi nition , 80 \n Docker image , 83 \n ENTRYPOINT , 84 \n\n\n\n K, L \n Kubernetes \n application creation \n cluster , 40 \n hello-world application , 48, 68 \n label , 41 \n namespace , 41 \n nodes , 39 \n Pod , 40, 46, 58 \n replication controller , 40, 43, 53, 64 \n scaling , 52, 70 \n selector , 41 \n service , 40, 45, 53, 61 \n volume , 41 \n benefi ts , 42 \n Docker \n adding gpg key , 6 \n apt package index , 8 \n apt sources , 6 \n containers , 30–32 \n Default Package Confi guration , 12 \n docker.list fi le , 6 \n engine installation , 14–15 \n linux-image-extra package , 10 \n lxc-docker and \nlxc-docker-virtual-package , 8–9 \n message prompt , 11, 13 \n package manager , 10 \n repository verifi cation , 9 \n sudo apt-get update , 13 \n Ubuntu distribution , 7 \n environment setting , 4, 42 \n etcd , 24–25 \n installation \n /boot directory , 16–17 \n command-line parameters , 19 \n components , 15 \n CONFIG_MEMCG_\nSWAP_ENABLED setting , 19 \n directory creation , 16 \n Docker engine , 16 \n GRUB_CMDLINE_LINUX , 20–21 \n grub confi guration fi le , 22 \n kernel confi guration , 16–18 \n service proxy , 15 \n settings, updation , 22–23 \n testing , 38 \n Ubuntu Amazon EC2 instance , 22 \n JSON \n curl command , 75 \n defi nition , 70 \n hello-rc.json fi le , 73 \n hello-world replication controller , 73–74 \n hello-world-service.json fi le , 70–72 \n HTML output , 76 \n replication controller defi nition fi le , 72 \n kubectl , 35 \n Kubernetes master , 28 \n local machine solutions , 3 \n nodes , 36 \n service proxy , 29, 35 \n",
      "page_number": 426
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 436-440)",
      "start_page": 436,
      "end_page": 440,
      "detection_method": "topic_boundary",
      "content": "■ INDEX\n431\n\n\n\n M, N \n MapReduce application , 287 \n hdfs command , 287 \n input directory , 288 \n vi editor , 288 \n wordcount application , 292, 295 \n wq command , 290 \n Master node \n bootstrap Docker , 402 \n etcd set up , 403 \n Flannel set up , 404 \n Kubernetes , 409, 411 \n testing , 412 \n MongoDB database , 167 \n cluster declaratively , 169 \n adding documents , 184 \n capped collection , 183 \n catalog collection , 183 \n database creation , 182 \n Docker , 168 \n drop command , 188 \n exit command , 188 \n fi nding documents , 186 \n host port , 190 \n interactive shell , 180 \n Kubernetes Pod , 169 \n logs list , 178 \n Mongo shell , 182 \n replication controller creation , 173 \n replication controller deletion , 189 \n scaling , 189 \n service defi nition fi le , 169 \n service deletion , 190 \n Ubuntu instance , 168 \n volume (see  Volume ) \n cluster imperatively , 194 \n logs list , 196 \n Pods , 195 \n replication controller creation , 194 \n replication controller deletion , 200 \n scaling , 198 \n service creation , 197 \n environment settings , 167 \n Multi-node cluster , 399 \n components , 400 \n environment settings , 400 \n execution , 419 \n exposing , 420 \n masternode (see  Master node ) \n scaling , 423 \n testing , 422 \n workernode (see  Worker node ) \n MySQL database \n database table , 110 \n environment setting , 98 \n interactive shell , 107, 111 \n logs , 104–106 \n MySQL CLI , 110–111 \n Pods , 104 \n replication controller , 103, 114 \n scaling , 113 \n service , 99, 106–107 \n\n\n\n O \n Oracle database , 141 \n environment settings , 141 \n instance declaratively , 148 \n database connection , 160 \n exit command , 163 \n interactive shell , 159 \n Pod creation , 148 \n replication controller creation , 153 \n replication level , 156 \n scaling , 158 \n service creation , 150 \n table creation , 162 \n user creation , 161 \n instance imperatively , 142 \n logs list , 144 \n replication controller creation , 143 \n replication controller deletion , 147 \n scaling , 147 \n service creation , 145 \n service deletion , 147 \n\n\n\n P, Q, R \n Pod \n application types , 376 \n docker containers , 383 \n environment settings , 377 \n Hello world application \n browser , 389 \n command line , 385 \n interactive shell , 386 \n lists , 382 \n number of containers , 376 \n overview , 375 \n port forwarding , 387 \n PostgreSQL command , 387 \n\n\n■ INDEX\n432\n replication controller creation , 380 \n scaling (see  Scaling ) \n service creation , 378 \n service describing , 379, 384 \n PostgreSQL database , 115 \n cluster declaratively , 117 \n interactive shell , 123 \n logs list , 128 \n Pods , 123 \n psql CLI Shell , 125–126 \n replication controller creation , 119 \n replication controller deletion , 130 \n scaling , 127 \n service creation , 117 \n stop command , 131 \n table creation , 125 \n cluster imperatively , 131 \n Docker containers , 134 \n Pods , 132 \n psql shell , 135 \n replication controller creation , 132 \n replication \ncontroller deletion , 138 \n scaling , 137 \n service creation , 133 \n stop command , 139 \n table creation , 134 \n environment settings , 115 \n Docker engine , 116 \n Kubernetes service , 117 \n Ubuntu instance , 116 \n\n\n\n S, T, U \n Scaling \n Docker containers , 391 \n Hello world application \n browser , 393 \n command line , 394 \n replication controller deletion , 396 \n service describing , 392 \n 3 replicas , 390 \n\n\n\n V \n Volume \n confi guration , 177 \n defi nition , 176 \n empty directory , 178 \n types , 176 \n usages , 176 \n\n\n\n W, X, Y \n Worker node , 412 \n fl annel set up , 413 \n Kubernetes , 418 \n MASTER_IP , 412 \n service proxy , 419 \n testing , 419 \n\n\n\n Z \n Zookeeper server , 349 \nPod (cont.)\n",
      "page_number": 436
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "Kubernetes \nMicroservices \nwith Docker\n—\nDeepak Vohra\nForeword by Massimo Nardone\nTHE EXPERT’S VOICE® IN OPEN SOURCE\n",
      "content_length": 117,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 2,
      "content": " Kubernetes \nMicroservices with \nDocker \n Deepak Vohra \n",
      "content_length": 56,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "Kubernetes Microservices with Docker\nDeepak Vohra\t\n\t\n\t\n\t\nWhite Rock, British Columbia\t\t\n\t\n\t\n\t\nCanada\t \t\n\t\nISBN-13 (pbk): 978-1-4842-1906-5\t\n\t\nISBN-13 (electronic): 978-1-4842-1907-2\nDOI 10.1007/978-1-4842-1907-2 \nLibrary of Congress Control Number: 2016937418\nCopyright © 2016 by Deepak Vohra\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is \nconcerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction \non microfilms or in any other physical way, and transmission or information storage and retrieval, electronic \nadaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. Exempted \nfrom this legal reservation are brief excerpts in connection with reviews or scholarly analysis or material supplied \nspecifically for the purpose of being entered and executed on a computer system, for exclusive use by the purchaser \nof the work. Duplication of this publication or parts thereof is permitted only under the provisions of the Copyright \nLaw of the Publisher's location, in its current version, and permission for use must always be obtained from Springer. \nPermissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations are liable to \nprosecution under the respective Copyright Law.\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every \noccurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion \nand to the benefit of the trademark owner, with no intention of infringement of the trademark.\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified \nas such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.\nWhile the advice and information in this book are believed to be true and accurate at the date of publication, neither \nthe authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may \nbe made. The publisher makes no warranty, express or implied, with respect to the material contained herein.\nManaging Director: Welmoed Spahr\nLead Editor: Michelle Lowman\nTechnical Reviewer: Massimo Nardone\nEditorial Board: Steve Anglin, Pramila Balan, Louise Corrigan, Jonathan Gennick, Robert Hutchinson,  \nCelstin Suresh John, Michelle Lowman, James Markham, Susan McDermott, Matthew Moodie,  \nJeffrey Pepper, Douglas Pundick, Ben Renow-Clarke, Gwenan Spearing\nCoordinating Editor: Mark Powers\nCompositor: SPi Global\nIndexer: SPi Global\nArtist: SPi Global\nDistributed to the book trade worldwide by Springer Science+Business Media New York, 233 Spring Street,  \n6th Floor, New York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail orders-ny@springer-sbm.com, \nor visit www.springeronline.com. Apress Media, LLC is a California LLC and the sole member (owner) is Springer  \nScience + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a Delaware corporation.  \nFor information on translations, please e-mail rights@apress.com, or visit www.apress.com. \nApress and friends of ED books may be purchased in bulk for academic, corporate, or promotional use.  \neBook versions and licenses are also available for most titles. For more information, reference our Special Bulk \nSales–eBook Licensing web page at www.apress.com/bulk-sales.\nAny source code or other supplementary material referenced by the author in this text is available to readers at \nwww.apress.com/9781484219065. For additional information about how to locate and download your book’s \nsource code, go to www.apress.com/source-code/. Readers can also access source code at SpringerLink in the \nSupplementary Material section for each chapter.\nPrinted on acid-free paper\n",
      "content_length": 3967,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "iii\nContents at a Glance\nAbout the Author ...................................................................................................xvii\nAbout the Technical Reviewer ................................................................................xix\nForeword ................................................................................................................xxi\n \n■Part I: Getting Started ......................................................................... 1\n \n■Chapter 1: Installing Kubernetes Using Docker ..................................................... 3\n \n■Chapter 2: Hello Kubernetes ................................................................................ 39\n \n■Chapter 3: Using Custom Commands and Environment Variables ...................... 77\n \n■Part II: Relational Databases ............................................................ 95\n \n■Chapter 4: Using MySQL Database ...................................................................... 97\n \n■Chapter 5: Using PostgreSQL Database ............................................................. 115\n \n■Chapter 6: Using Oracle Database ..................................................................... 141\n \n■Part III: NoSQL Database ................................................................. 165\n \n■Chapter 7: Using MongoDB Database ................................................................ 167\n \n■Chapter 8: Using Apache Cassandra Database .................................................. 201\n \n■Chapter 9: Using Couchbase .............................................................................. 231\n \n■Part IV: Apache Hadoop Ecosystem ................................................ 275\n \n■Chapter 10: Using Apache Hadoop Ecosystem .................................................. 277\n \n■Chapter 11: Using Apache Solr .......................................................................... 313\n \n■Chapter 12: Using Apache Kafka ....................................................................... 347\n",
      "content_length": 2046,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": " ■ CONTENTS AT A GLANCE\niv\n \n■Part V: Multi Containers and Nodes ................................................ 373\n \n■Chapter 13: Creating a Multi-Container Pod ...................................................... 375\n \n■Chapter 14: Installing Kubernetes on a Multi-Node Cluster .............................. 399\nIndex ..................................................................................................................... 429\n",
      "content_length": 447,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "v\nContents\nAbout the Author ...................................................................................................xvii\nAbout the Technical Reviewer ................................................................................xix\nForeword ................................................................................................................xxi\n \n■Part I: Getting Started ......................................................................... 1\n \n■Chapter 1: Installing Kubernetes Using Docker ..................................................... 3\nSetting the Environment ................................................................................................... 4\nInstalling Docker .............................................................................................................. 5\nInstalling Kubernetes ..................................................................................................... 15\nStarting etcd ................................................................................................................... 23\nStarting Kubernetes Master ........................................................................................... 25\nStarting Service Proxy .................................................................................................... 28\nListing the Kubernetes Docker Containers ..................................................................... 29\nInstalling kubectl ............................................................................................................ 32\nListing Services .............................................................................................................. 35\nListing Nodes .................................................................................................................. 36\nTesting the Kubernetes Installation ................................................................................ 36\nSummary ........................................................................................................................ 38\n \n■Chapter 2: Hello Kubernetes ................................................................................ 39\nOverview ........................................................................................................................ 39\nWhat Is a Node? ....................................................................................................................................39\nWhat Is a Cluster?.................................................................................................................................40\nWhat Is a Pod? ......................................................................................................................................40\n",
      "content_length": 2805,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": " ■ CONTENTS\nvi\nWhat Is a Service? ................................................................................................................................40\nWhat Is a Replication Controller? .........................................................................................................40\nWhat Is a Label? ...................................................................................................................................41\nWhat Is a Selector? ...............................................................................................................................41\nWhat Is a Name? ..................................................................................................................................41\nWhat Is a Namespace? .........................................................................................................................41\nWhat Is a Volume? ................................................................................................................................41\nWhy Kubernetes?..................................................................................................................................41\nSetting the Environment ................................................................................................. 42\nCreating an Application Imperatively .............................................................................. 43\nCreating a Service ................................................................................................................................44\nDescribing a Pod ...................................................................................................................................46\nInvoking the Hello-World Application ....................................................................................................47\nScaling the Application .........................................................................................................................48\nDeleting a Replication Controller ..........................................................................................................52\nDeleting a Service ................................................................................................................................53\nCreating an Application Declaratively ............................................................................. 53\nCreating a Pod Deﬁ nition ......................................................................................................................54\nCreating a Service Deﬁ nition ................................................................................................................58\nCreating a Replication Controller Deﬁ nition..........................................................................................61\nInvoking the Hello-World Application ....................................................................................................64\nScaling the Application .........................................................................................................................68\nUsing JSON for the Resource Deﬁ nitions ....................................................................... 70\nSummary ........................................................................................................................ 76\n \n■Chapter 3: Using Custom Commands and Environment Variables ...................... 77\nSetting the Environment ................................................................................................. 77\nThe ENTRYPOINT and CMD Instructions ......................................................................... 78\nThe Command and Args Fields in a Pod Deﬁ nition......................................................... 79\nEnvironment Variables .................................................................................................... 80\n",
      "content_length": 3968,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": " ■ CONTENTS\nvii\nUsing the Default ENTRYPOINT and CMD from a Docker Image ..................................... 81\nOverriding Both the ENTRYPOINT and CMD .................................................................... 84\nSpecifying both the Executable and the Parameters in the Command Mapping ........... 87\nSpecifying Both the Executable and the Parameters in the Args Mapping .................... 90\nSummary ........................................................................................................................ 93\n \n■Part II: Relational Databases ............................................................ 95\n \n■Chapter 4: Using MySQL Database ...................................................................... 97\nSetting the Environment ................................................................................................. 97\nCreating a Service .......................................................................................................... 99\nCreating a Replication Controller.................................................................................. 100\nListing the Pods ............................................................................................................ 104\nListing Logs .................................................................................................................. 104\nDescribing the Service ................................................................................................. 106\nStarting an Interactive Shell ......................................................................................... 107\nStarting the MySQL CLI ................................................................................................ 109\nCreating a Database Table ............................................................................................ 110\nExiting the MySQL CLI and Interactive Shell ................................................................. 111\nScaling the Replicas ..................................................................................................... 111\nDeleting the Replication Controller ............................................................................... 113\nSummary ...................................................................................................................... 114\n \n■Chapter 5: Using PostgreSQL Database ............................................................. 115\nSetting the Environment ............................................................................................... 115\nCreating a PostgreSQL Cluster Declaratively ............................................................... 117\nCreating a Service ..............................................................................................................................117\nCreating a Replication Controller ........................................................................................................119\nGetting the Pods .................................................................................................................................123\nStarting an Interactive Command Shell ..............................................................................................123\n",
      "content_length": 3288,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": " ■ CONTENTS\nviii\nStarting the PostgreSQL SQL Terminal ...............................................................................................124\nCreating a Database Table ..................................................................................................................125\nExiting the Interactive Command Shell ...............................................................................................126\nScaling the PostgreSQL Cluster ..........................................................................................................127\nListing the Logs ..................................................................................................................................128\nDeleting the Replication Controller .....................................................................................................130\nStopping the Service ..........................................................................................................................131\nCreating a PostgreSQL Cluster Imperatively ................................................................ 131\nCreating a Replication Controller ........................................................................................................132\nGetting the Pods .................................................................................................................................132\nCreating a Service ..............................................................................................................................133\nCreating a Database Table ..................................................................................................................134\nScaling the PostgreSQL Cluster ..........................................................................................................137\nDeleting the Replication Controller .....................................................................................................138\nStopping the Service ..........................................................................................................................139\nSummary ...................................................................................................................... 139\n \n■Chapter 6: Using Oracle Database ..................................................................... 141\nSetting the Environment ............................................................................................... 141\nCreating an Oracle Database Instance Imperatively .................................................... 142\nListing Logs ........................................................................................................................................144\nCreating a Service ..............................................................................................................................145\nScaling the Database ..........................................................................................................................146\nDeleting the Replication Controller and Service .................................................................................147\nCreating an Oracle Database Instance Declaratively ................................................... 148\nCreating a Pod ....................................................................................................................................148\nCreating a Service ..............................................................................................................................150\nCreating a Replication Controller ........................................................................................................153\nKeeping the Replication Level ............................................................................................................156\nScaling the Database ..........................................................................................................................158\nStarting the Interactive Shell ..............................................................................................................159\n",
      "content_length": 4177,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": " ■ CONTENTS\nix\nConnecting to Database......................................................................................................................160\nCreating a User ...................................................................................................................................161\nCreating a Database Table ..................................................................................................................162\nExiting the Interactive Shell ................................................................................................................163\nSummary ...................................................................................................................... 163\n \n■Part III: NoSQL Database ................................................................. 165\n \n■Chapter 7: Using MongoDB Database ................................................................ 167\nSetting the Environment ............................................................................................... 167\nCreating a MongoDB Cluster Declaratively ................................................................... 169\nCreating a Service ..............................................................................................................................169\nCreating a Replication Controller ........................................................................................................173\nCreating a Volume ...............................................................................................................................176\nListing the Logs ..................................................................................................................................178\nStarting the Interactive Shell for Docker Container ............................................................................180\nStarting a Mongo Shell .......................................................................................................................182\nCreating a Database ...........................................................................................................................182\nCreating a Collection ..........................................................................................................................183\nAdding Documents .............................................................................................................................184\nFinding Documents .............................................................................................................................186\nFinding a Single Document .................................................................................................................186\nFinding Speciﬁ c Fields in a Single Document ....................................................................................187\nDropping a Collection .........................................................................................................................188\nExiting Mongo Shell and Interactive Shell ..........................................................................................188\nScaling the Cluster .............................................................................................................................188\nDeleting the Replication Controller .....................................................................................................189\nDeleting the Service ...........................................................................................................................190\nUsing a Host Port ................................................................................................................................190\nCreating a MongoDB Cluster Imperatively .................................................................... 194\nCreating a Replication Controller ........................................................................................................194\n",
      "content_length": 4031,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": " ■ CONTENTS\nx\nListing the Pods ..................................................................................................................................195\nListing the Logs ..................................................................................................................................196\nCreating a Service ..............................................................................................................................197\nScaling the Cluster .............................................................................................................................198\nDeleting the Service and Replication Controller .................................................................................200\nSummary ...................................................................................................................... 200\n \n■Chapter 8: Using Apache Cassandra Database .................................................. 201\nSetting the Environment ............................................................................................... 201\nCreating a Cassandra Cluster Declaratively ................................................................. 203\nCreating a Service ..............................................................................................................................203\nCreating a Replication Controller ........................................................................................................206\nScaling the Database ..........................................................................................................................211\nDescribing the Pod .............................................................................................................................212\nStarting an Interactive Shell ...............................................................................................................213\nStarting the CQL Shell.........................................................................................................................215\nCreating a Keyspace ...........................................................................................................................215\nAltering a Keyspace ............................................................................................................................215\nUsing a Keyspace ...............................................................................................................................216\nCreating a Table ..................................................................................................................................216\nDeleting from a Table ..........................................................................................................................217\nTruncating a Table ...............................................................................................................................218\nDropping a Table and Keyspace ..........................................................................................................218\nCreating a Volume ...............................................................................................................................219\nCreating a Cassandra Cluster Imperatively .................................................................. 225\nCreating a Replication Controller ........................................................................................................225\nCreating a Service ..............................................................................................................................227\nScaling the Database ..........................................................................................................................228\nDeleting the Replication Controller and Service .................................................................................229\nSummary ...................................................................................................................... 230\n",
      "content_length": 4090,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": " ■ CONTENTS\nxi\n \n■Chapter 9: Using Couchbase .............................................................................. 231\nSetting the Environment ............................................................................................... 231\nCreating a Couchbase Cluster Declaratively ................................................................ 234\nCreating a Pod ....................................................................................................................................234\nCreating a Service ..............................................................................................................................237\nCreating a Replication Controller ........................................................................................................239\nListing the Pods ..................................................................................................................................243\nListing the Logs ..................................................................................................................................243\nDescribing the Service .......................................................................................................................244\nListing the Endpoints ..........................................................................................................................244\nSetting Port Forwarding ......................................................................................................................244\nLogging into Couchbase Web Console ................................................................................................246\nConﬁ guring Couchbase Server ...........................................................................................................247\nAdding Documents .............................................................................................................................255\nStarting an Interactive Shell ...............................................................................................................264\nUsing the cbtransfer Tool ....................................................................................................................265\nCreating a Couchbase Cluster Imperatively ................................................................. 266\nCreating a Replication Controller ........................................................................................................266\nListing the Pods ..................................................................................................................................266\nCreating a Service ..............................................................................................................................268\nScaling the Cluster .............................................................................................................................269\nKeeping the Replication Level ............................................................................................................270\nSetting Port Forwarding ......................................................................................................................272\nLogging in to Couchbase Admin Console ............................................................................................272\nSummary ...................................................................................................................... 273\n \n■Part IV: Apache Hadoop Ecosystem ................................................ 275\n \n■Chapter 10: Using Apache Hadoop Ecosystem .................................................. 277\nSetting the Environment ............................................................................................... 277\nCreating an Apache Hadoop Cluster Declaratively ....................................................... 278\nCreating a Service ..............................................................................................................................279\n",
      "content_length": 4086,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": " ■ CONTENTS\nxii\nCreating a Replication Controller ........................................................................................................281\nListing the Pods ..................................................................................................................................283\nListing Logs ........................................................................................................................................284\nScaling a Cluster .................................................................................................................................285\nStarting an Interactive Shell ...............................................................................................................286\nRunning a MapReduce Application .....................................................................................................287\nRunning Hive ................................................................................................................ 296\nRunning HBase ............................................................................................................. 302\nDeleting the Replication Controller and Service ........................................................... 307\nCreating an Apache Hadoop Cluster Imperatively ........................................................ 307\nCreating a Replication Controller ........................................................................................................307\nListing the Pods ..................................................................................................................................308\nScaling a Cluster .................................................................................................................................309\nCreating a Service ..............................................................................................................................309\nStarting an Interactive Shell ...............................................................................................................310\nSummary ...................................................................................................................... 311\n \n■Chapter 11: Using Apache Solr .......................................................................... 313\nSetting the Environment ............................................................................................... 314\nCreating a Service ........................................................................................................ 315\nListing Service Endpoints ............................................................................................. 317\nDescribing the Service ................................................................................................. 317\nCreating a Replication Controller.................................................................................. 318\nListing the Pods ............................................................................................................ 321\nDescribing a Replication Controller .............................................................................. 322\nListing the Logs ............................................................................................................ 323\nStarting an Interactive Shell ......................................................................................... 325\nCreating a Solr Core ..................................................................................................... 328\nIndexing Documents ..................................................................................................... 329\nAccessing Solr on Command Line with a REST Client ................................................. 332\n",
      "content_length": 3834,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": " ■ CONTENTS\nxiii\nSetting Port Forwarding ............................................................................................... 337\nAccessing Solr in Admin Console ................................................................................. 338\nScaling the Cluster ....................................................................................................... 344\nSummary ...................................................................................................................... 345\n \n■Chapter 12: Using Apache Kafka ....................................................................... 347\nSetting the Environment ............................................................................................... 348\nModifying the Docker Image ........................................................................................ 349\nCreating a Service ........................................................................................................ 355\nCreating a Replication Controller.................................................................................. 358\nListing the Pods ............................................................................................................ 361\nDescribing a Pod .......................................................................................................... 362\nStarting an Interactive Shell ......................................................................................... 363\nStarting the Kafka Server ............................................................................................. 364\nCreating a Topic ............................................................................................................ 366\nStarting a Kafka Producer ............................................................................................ 367\nStarting a Kafka Consumer .......................................................................................... 367\nProducing and Consuming Messages .......................................................................... 367\nScaling the Cluster ....................................................................................................... 368\nDeleting Replication Controller and Service ................................................................. 370\nSummary ...................................................................................................................... 371\n \n■Part V: Multi Containers and Nodes ................................................ 373\n \n■Chapter 13: Creating a Multi-Container Pod ...................................................... 375\nHow to ﬁ nd Number of Containers in a Pod? ............................................................... 376\nTypes of Applications Using a Multi-Container Pod ...................................................... 376\nSetting the Environment ............................................................................................... 377\nCreating a Service ........................................................................................................ 378\nDescribing a Service  ................................................................................................... 379\nCreating a Replication Container .................................................................................. 380\n",
      "content_length": 3399,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": " ■ CONTENTS\nxiv\nListing the Pods ............................................................................................................ 382\nListing the Docker Containers ...................................................................................... 383\nDescribing the Service after Creating Replication Controller ....................................... 384\nInvoking the Hello World Application on Command Line .............................................. 385\nStarting the Interactive Shell ........................................................................................ 386\nStarting PostgreSQL Shell ............................................................................................ 387\nSetting Port Forwarding ............................................................................................... 387\nOpening the Hello World Application in a Browser ....................................................... 388\nScaling the Cluster ....................................................................................................... 389\nListing the Docker Containers ............................................................................................................391\nDescribing the Service after Scaling ..................................................................................................392\nSetting Port Forwarding ......................................................................................................................392\nOpening the Hello World Application in a Browser .............................................................................393\nInvoking the Hello World Application from Command Line .................................................................394\nDeleting the Replication Controller .....................................................................................................396\nDeleting the Service ...........................................................................................................................397\nSummary ...................................................................................................................... 397\n \n■Chapter 14: Installing Kubernetes on a Multi-Node Cluster .............................. 399\nComponents of a Multi-Node Cluster ........................................................................... 400\nSetting the Environment ............................................................................................... 400\nInstalling the Master Node ........................................................................................... 402\nSetting Up Flanneld and etcd  ............................................................................................................402\nStarting the Kubernetes Master .........................................................................................................409\nRunning the Service Proxy .................................................................................................................411\nTesting the One-Node Cluster ...................................................................................... 412\nAdding a Worker Node .................................................................................................. 412\nExporting the Master IP ......................................................................................................................412\nSetting Up Flanneld ............................................................................................................................413\nStarting Up Kubernetes on Worker Node ............................................................................................418\nRunning the Service Proxy .................................................................................................................419\n",
      "content_length": 3883,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": " ■ CONTENTS\nxv\nTesting the Kubernetes Cluster .................................................................................... 419\nRunning an Application on the Cluster ......................................................................... 419\nExposing the Application as a Service ......................................................................... 420\nTesting the Application in a Browser ............................................................................ 422\nScaling the Application ................................................................................................. 423\nSummary ...................................................................................................................... 427\nIndex ..................................................................................................................... 429\n",
      "content_length": 870,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "xvii\n About the Author \n Deepak  Vohra  is a consultant and a principal member of the NuBean.com \nsoftware company. Deepak is a Sun-certified Java programmer and Web \ncomponent developer. He has worked in the fields of XML, Java \nprogramming, and Java EE for over seven years. Deepak is the coauthor of \n Pro XML Development with Java Technology (Apress, 2006). Deepak is also \nthe author of the  JDBC 4.0 and  Oracle JDeveloper for J2EE Development, \nProcessing XML Documents with Oracle JDeveloper 11g, EJB 3.0 Database \nPersistence with Oracle Fusion Middleware 11g , and  Java EE Development \nin Eclipse IDE (Packt Publishing). He also served as the technical reviewer \non  WebLogic: The Definitive Guide (O’Reilly Media, 2004) and  Ruby \nProgramming for the Absolute Beginner (Cengage Learning PTR, 2007). \nDeepak is the author of  Pro Couchbase Development, Pro MongoDB \nDevelopment , and  Pro Docker , all published by Apress in 2015. \n \n",
      "content_length": 945,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "xix\n About the Technical Reviewer \n Massimo  Nardone  holds a Master of Science degree in Computing \nScience from the University of Salerno, Italy. He has worked as a Project \nManager, Software Engineer, Research Engineer, Chief Security Architect, \nInformation Security Manager, PCI/SCADA Auditor, and Senior Lead IT \nSecurity/Cloud/SCADA Architect for many years. He currently works as \nChief Information Security Office (CISO) for Cargotec Oyj. He has more \nthan 22 years of work experience in IT including Security, SCADA, Cloud \nComputing, IT Infrastructure, Mobile, Security, and WWW technology \nareas for both national and international projects. He worked as a visiting \nlecturer and supervisor for exercises at the Networking Laboratory of the \nHelsinki University of Technology (Aalto University). He has been \nprogramming and teaching how to program with Android, Perl, PHP, Java, \nVB, Python, C/C++, and MySQL for more than 20 years. He holds four \ninternational patents (PKI, SIP, SAML, and Proxy areas). \n He is the coauthor of  Pro Android Games (Apress, 2015). \n Massimo dedicates his work on this book to his loving brothers Mario \nNardone and Roberto Nardone, who are always there when he needs them. \n \n \n",
      "content_length": 1224,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "xxi\n Foreword \n It is a great pleasure to provide the Foreword for this book, as I’ve been reading and following Deepak \nVohra’s work for some time. Deepak has been developing Web components and Java applications for many \nyears, and the scope of his expertise is reflected in the books he has written – as is his passion to share that \nknowledge with others. \n About a year ago, I was given the opportunity to perform a technical review on his Pro Couchbase \nDevelopment book, and we formed an immediate connection. Since then, I’ve served as technical reviewer \non several more of his books, including this one. The reason I keep coming back is simple – I always come \naway knowing more than I did before. \n Docker is a new container technology that has become very popular because it is great for building \nand sharing disk images and enables users to run different operating systems such as Ubuntu, Fedora, and \nCentos. Docker is often used when a version control framework is required for an application’s operating \nsystem, to distribute applications on different machines, or to run code on laptop in the same environment \nas on the server. In general, Docker will always run the same, regardless of the environment in which it will \nbe running. \n Kubernetes is an open source container cluster manager that complements and extends Docker’s \nsoftware encapsulation power and makes it easier to organize and schedule applications across a fleet \nof machines. It’s a lightweight, portable (suited for the cloud architecture) and modular tool that can be \nrun on almost any platform with different local machine solutions. Kubernetes offers a number of distinct \nadvantages, first and foremost being that it combines all necessary tools – orchestration, service discovery, \nand load balancing – together in one nice package for you. Kubernetes also boasts heavy involvement from \nthe developer community. \n Kubernetes Microservices with Docker will show you how to use these two powerful tools in unison to \nmanage complex big data and enterprise applications. Installing Kubernetes on single nodes and multi-\nnode clusters, creating multi-container pods, using Kubernetes with the Apache Hadoop Ecosystem and \nNoSQL Databases – it’s all here, and more. So sit back, and let Deepak be your guide.\n —Massimo Nardone \n Chief Security Information Officer (CISO), Cargotec Oyj \n",
      "content_length": 2378,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": " PART I \n Getting Started \n  \n",
      "content_length": 30,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "3\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_1\n CHAPTER 1 \n Installing Kubernetes \nUsing Docker \n Kubernetes is software for managing a cluster of Docker containers. Kubernetes orchestration includes \nscheduling, distributing workload, and scaling. Kubernetes takes the software encapsulation provided by \nDocker further by introducing Pods. A Pod is a collection of one or more Docker containers with single \ninterface features such as providing networking and filesystem at the Pod level rather than at the container \nlevel. Kubernetes also introduces “labels” using which services and replication controllers (replication \ncontroller is used to scale a cluster) identify or select the containers or pods they manage. Kubernetes is \nlightweight, portable (suited for the cloud architecture), and modular. \n Kubernetes may be run on almost any platform.  Local machine solutions include local Docker based, \nVagrant, and no-VM local cluster. Hosted solutions include Google Container Engine. Some of the other \nplatforms supported by Kubernetes are Fedora (Ansible and Manual), Amazon Web Services, Mesos, \nvSphere, and CoreOS. Kubernetes is an orchestration software for Docker containers; the recommended \nsolution for installation is to use the Docker Engine. In this chapter we shall install Kubernetes on Docker, \nwhich runs on Ubuntu. We shall use an Amazon EC2 instance hosting Ubuntu as the operating system. In \nthis chapter, a single node installation of Kubernetes is discussed. Multi-node installation of Kubernetes is \ndiscussed in chapter  14 . This chapter has the following sections.\n Setting the Environment \n Installing Docker \n Installing Kubernetes \n Starting etcd \n Starting Kubernetes Master \n Starting Service Proxy \n Listing the Kubernetes Docker Containers \n Installing kubectl \n Listing Services \n Listing Nodes \n Testing the Kubernetes Installation \n",
      "content_length": 1934,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n4\n Setting the Environment \n The following software is required for this chapter.\n - Docker Engine (latest version) \n - Kubernetes (version 1.01) \n Linux is required to support 64-bit software. We have used an Amazon EC2 instance created from AMI \nUbuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-d05e75b8. An Amazon EC2 instance based on the \nUbuntu AMI is shown in Figure  1-1 . \n Figure 1-1.  Amazon EC2 Instance Based on Ubuntu AMI \n A different Ubuntu version may be used if the requirement of a 64-bit architecture is met. The minimum \nkernel version requirement is 3.10. The kernel version may be verified with the following command. \n uname –r \n The Public IP would be different for different users. Multiple Amazon EC2 instances and therefore \nmultiple Public IP addresses have been used in the book as a different Public IP is assigned each time an \nAmazon EC2 instance is started. The Private IP Address of an Amazon EC2 instance is the same across \nrestarts. SSH into an Ubuntu instance on Amazon EC2 (Public IP is 52.91.80.173 in following command). \n ssh -i \"docker.pem\" ubuntu@52.91.80.173 \n The Amazon EC2 instance  gets  logged in as shown in Figure  1-2 . The command prompt becomes \n“ubuntu@ip-172-30-1-190” instead of root@localhost. Ip 172.30.1.190 is the Private IP of the Amazon EC2 \ninstance and would also be different for different users. \n \n",
      "content_length": 1420,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n5\n In the next section we shall install Docker on Ubuntu hosted on an Amazon EC2 instance . \n Installing Docker \n Ubuntu uses apt for package management; apt stores a list of repositories in the /etc/apt/sources.list list. \nDocker’s apt repository is kept in the /etc/apt/sources.list.d/docker.list file. First, add the new repository key \n(gpg key) for the Docker repository with the following command. \n sudo apt-key adv --keyserver hkp://pgp.mit.edu:80 --recv-keys \n58118E89F3A912897C070ADBF76221572C52609D \n Figure 1-2.  Loging into an Amazon EC2 instance \n \n",
      "content_length": 611,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n6\n The new gpg key gets  added  as shown in Figure  1-3 . \n Next, update  the  apt sources for the Docker repository in the /etc/apt/sources.list.d/docker.list file \nbased on the Ubuntu distribution, which may be found with the following command. \n lsb_release –a \n For Ubuntu Trusty, add the following line to the /etc/apt/sources.list.d/docker.list file; the docker.list \nfile may be opened with sudo vi /etc/apt/sources.list.d/docker.list. \n deb https://apt.dockerproject.org/repo ubuntu-trusty main \n Create the /etc/apt/sources.list.d/docker.list file if the file does not already exist. The updated file is \nshown in Figure  1-4 .  Save the file with the :wq command if opened in the vi editor. \n Figure 1-3.  Adding a new gpg key \n \n",
      "content_length": 788,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n7\n The entry to be added would be different for  different  Ubuntu distributions as listed in Table  1-1 . \n Figure 1-4.  Creating the docker.list file \n Table 1-1.  The docker.list file Entry Based on Ubuntu Distribution \n Ubuntu Distribution \n Entry \n Ubuntu Precise 12.04 (LTS) \n deb  https://apt.dockerproject.org/repo ubuntu-precise main \n Ubuntu Trusty 14.04 (LTS) \n deb  https://apt.dockerproject.org/repo ubuntu-trusty main \n Ubuntu Vivid 15.04 \n deb  https://apt.dockerproject.org/repo ubuntu-vivid main \n \n",
      "content_length": 564,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n8\n Run the following commands after updating the /etc/apt/sources.list.d/docker.list file to update the apt \npackage index. \n sudo apt-get update \n Apt package index gets updated as shown in Figure  1-5 . \n Figure 1-5.  Updating  Ubuntu  Package List \n Purge the old repository if it exists with the following command. \n sudo apt-get purge lxc-docker* \n The output in Figure  1-6 indicates that the old  packages  lxc-docker and lxc-docker-virtual-package are \nnot installed and therefore not removed. \n \n",
      "content_length": 553,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n9\n Run the following command to verify that apt is pulling from the updated repository for Docker. \n sudo apt-cache policy docker-engine \n The output in Figure  1-7 indicates that the  new  repository ubuntu-trusty as specified in the /etc/apt/\nsources.list.d/docker.list is being used. \n Figure 1-6.  Purging the  Old  Repository \n Figure 1-7.  Using the  Updated  Repository verification \n \n \n",
      "content_length": 443,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n10\n Next, install the prerequisites for Ubuntu, but first update the package manager with the following \ncommand. \n sudo apt-get update \n The package manager gets  updated  as shown in Figure  1-8 . \n Figure 1-8.  Updating the Package Manager \n Install the  prerequisite  linux-image-extra package with the following command. \n sudo apt-get install linux-image-generic-lts-trusty \n When the preceding command is run, select Y if prompted with the following message.\n After this operation, 281 MB of additional disk space will be used. \n Do you want to continue? [Y/n] \n \n",
      "content_length": 619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n11\n The message prompt is shown in Figure  1-9 . \n Figure 1-9.  Message Prompt to Continue \n Subsequently, before the command completes, a Package Configuration dialog might prompt with the \nfollowing message:\n A new version of /boot/grub/menu.lst is available, but the version installed currently has \nbeen locally modified. What would you like to do about menu.lst? \n \n",
      "content_length": 419,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n12\n Reboot the system with the following command. \n sudo reboot \n When the sudo reboot command is run the AmazonEC2 instance is exited. Reconnect with the Amazon \nEC2 Ubuntu instance with the same ssh command as before. \n ssh -i \"docker.pem\" ubuntu@52.91.80.173 \n After the host system reboots, update the package manager again with the following command. \n sudo apt-get update \n Figure 1-10.  Selecting the  Default  Package Configuration \n Select the default selection, which is “keep the local version currently installed” and click on Enter as \nshown in Figure  1-10 . \n \n",
      "content_length": 624,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n13\n Package manager gets updated as shown in Figure  1-11 . \n Figure 1-11.  Updating Package  Manager List after Reboot \n Install Docker with the following command. \n sudo apt-get install docker-engine \n Select Y at the following prompt, if displayed, as shown in Figure  1-12 .\n After this operation, 60.3 MB of  additional  disk space will be used. \n Do you want to continue? [Y/n] \n \n",
      "content_length": 435,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n14\n The Docker engine gets installed as shown in Figure  1-13 . \n Figure 1-13.  Installing  the  Docker Engine \n Figure 1-12.  Message Prompt about the additional disk space being added \n \n \n",
      "content_length": 239,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n15\n Start the Docker service with the following command. \n sudo service docker start \n To verify the status of the Docker service, run the following command. \n sudo service docker status \n The output from the preceding commands is shown in Figure  1-14 . The docker engine is indicated as \nrunning as process 2697. \n Figure 1-14.  Starting  Docker and  verifying its Status \n Having installed Docker, next we shall install Kubernetes. \n Installing Kubernetes \n Kubernetes is an open source container cluster manager. The  main  components of Kubernetes are the \nfollowing:\n \n 1. \n etcd \n \n 2. \n Kubernetes master \n \n 3. \n Service proxy \n \n 4. \n kubelet \n etcd is a simple, secure, fast and reliable distributed key-value store. \n Kubernetes master exposes the Kubernetes API using which containers are run on nodes to handle tasks. \n kubelet is an agent that runs on each node to monitor the containers running on the node, restarting \nthem if required to keep the replication level. \n A service proxy runs on each node to provide the Kubernetes service interface for clients. A service is an \nabstraction for the logical set of pods represented by the service, and a service selector is used to select the \npods represented by the service.  The  service proxy routes the client traffic to a matching pod. Labels are used \nto match a service with a pod. \n Optionally create a directory (/kubernetes) to install Kubernetes and set its permissions to global (777). \n sudo mkdir /kubernetes \n sudo chmod -R 777 /kubernetes \n \n",
      "content_length": 1571,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n16\n Change directory to the /kubernetes directory and start the Docker engine. \n cd /kubernetes \n sudo service docker start \n If the Docker Engine is not running, it gets started.  The  Docker Engine is shown as already running in \nFigure  1-16 . \n Figure 1-15.  Creating  a  Directory to install Kubernetes \n The output from the preceding commands is shown in Figure  1-15 . \n Figure 1-16.  Starting Docker if not already running \n As a prerequisite we need to set some Linux kernel parameters if not already set. Add support for \nmemory and swap accounting.  The  following configs should be turned on in the kernel. \n CONFIG_RESOURCE_COUNTERS=y \n CONFIG_MEMCG=y \n CONFIG_MEMCG_SWAP=y \n CONFIG_MEMCG_SWAP_ENABLED=y \n CONFIG_MEMCG_KMEM=y \n The kernel configs are enabled when the Ubuntu system boots and  the  kernel configuration file is in the \n/boot directory. Change directory (cd) to the /boot directory and list the files/directories. \n cd /boot \n ls –l \n The files in  the  /boot directory get listed as shown in Figure  1-17 . The kernel configs are configured in \nthe config-3.13.0-48-generic file. The kernel version could be different for different users; for example, the \nkernel config file could /boot/config-3.13.0-66-generic. \n \n \n",
      "content_length": 1296,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n17\n Open the config-3.13.0-48-generic file in a vi editor. \n sudo vi /boot/config-3.13.0-48-generic \n The  kernel configuration parameters get listed as shown in Figure  1-18 . \n Figure 1-18.  Kernel Configuration Parameter \n Figure 1-17.  Listing the Files in the  /boot Directory \n \n \n",
      "content_length": 335,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n18\n Most of the configs listed earlier are already turned on as shown in Figure  1-19 . The CONFIG_MEMCG_\nSWAP_ENABLED config is not set. \n Figure 1-19.  Most of the Required  Kernel  Parameters are already Set \n \n",
      "content_length": 262,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n19\n Next, we need to add support for memory and swap accounting to the kernel. The command-line \nparameters provided to the kernel may be listed with the following command. \n cat /proc/cmdline \n As shown in Figure  1-21 memory and  swap accounting are not turned on. \n Figure 1-20.  Setting  the  CONFIG_MEMCG_SWAP_ENABLED Kernel Parameter \n Figure 1-21.  Listing  the  Command-Line Parameters \n Set CONFIG_MEMCG_SWAP_ENABLED =  y  and save the kernel configuration file as shown in \nFigure  1-20 . \n \n \n",
      "content_length": 552,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n20\n Grub 2 is the default boot loader for Ubuntu. To turn on memory and swap accounting, open the /etc/\ndefault/grub file in the vi editor.  The  GRUB_CMDLINE_LINUX is set to an empty string as shown in \nFigure  1-22 . \n Figure 1-22.  The /etc/default/grub  file \n Set the GRUB_CMDLINE_LINU as follows, which enables memory and swap accounting in the kernel \nat boot. \n GRUB_CMDLINE_LINUX=\"cgroup_enable=memory swapaccount=1\" \n The modified /etc/default/grub file  is  shown in Figure  1-23 . Save the file with the :wq command. \n \n",
      "content_length": 580,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n21\n Update the grub.cfg file with the following command. \n sudo update-grub \n Figure 1-23.  Modified /etc/default/grub  file \n \n",
      "content_length": 176,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n22\n Reboot the system. \n sudo reboot \n Connection to  the  Ubuntu Amazon EC2 instance gets closed as shown in Figure  1-25 . \n Figure 1-25.  Rebooting  Ubuntu Instance \n SSH log in back into the Ubuntu instance. Rerun the command to list the command-line kernel \nparameters. \n cat /proc/cmdline \n The cgroup_enable =  memory  swapaccount = 1 settings get output as shown in Figure  1-26 . \n The  grub configuration file gets generated as shown in Figure  1-24 . \n Figure 1-24.  Generating an  Updated  Grub Configuration file \n \n \n",
      "content_length": 579,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n23\n Having set the  prerequisite  kernel parameters, next we shall start the Kubernetes components etcd, \nmaster, and service proxy. \n Starting etcd \n Run etcd with the following docker run command. \n sudo docker run --net=host -d gcr.io/google_containers/etcd:2.0.12 /usr/local/bin/etcd \n-- addr=127.0.0.1:4001 --bind-addr=0.0.0.0:4001 --data-dir=/var/etcd/data \n Figure 1-26.  Updated  Settings \n \n",
      "content_length": 448,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n24\n The docker run command to start  etcd is  required to be run each time the Kubernetes cluster manager \nis to be started. Subsequent starts of etcd do not need to download the container image as shown in \nFigure  1-28 . \n The docker run command parameters are as follows (Table  1-2 ). \n Table 1-2.  The docker run Command Parameters to start etcd \n Parameter \n Description \n --net = host \n Connects the Docker container to a network \nmaking use of the host container network inside \nthe container \n -d \n Starts the container in the background \n gcr.io/google_containers/etcd:2.0.12 \n The container image \n /usr/local/bin/etcd --addr = 127.0.0.1:4001 \n--bind-addr = 0.0.0.0:4001 --data-dir=/var/etcd/data \n The command to run \n The output from the preceding command is shown in Figure  1-27 . \n Figure 1-27.  Starting etcd \n Figure 1-28.  Subsequent Start of etcd does not need to download the container Image again \n \n \n",
      "content_length": 972,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n25\n Starting Kubernetes Master \n The Kubernetes master is started using the kubelet, which also starts the other Master components \napiserver, scheduler, controller, and pause, which are discussed in Table  1-3 . \n Table 1-3.  The docker run  Command  Parameters to start etcd \n Master Component \n Description \n Apiserver \n The apiserver takes API requests, processes them, and stores the result in etcd if \nrequired and returns the result. \n Scheduler \n The scheduler monitors the API for unscheduled pods and schedules them on a \nnode to run and also notifies the about the same to the API. \n Controller \n The controller manages the replication level of the pods, starting new pods in a \nscale up event and stopping some of the pods in a scale down. \n Pause \n The pause keeps the port mappings of all the containers in the pod or the network \nendpoint of the pod. \n Run the Kubernetes master with the following command. \n sudo docker run \\ \n    --volume=/:/rootfs:ro \\ \n    --volume=/sys:/sys:ro \\ \n    --volume=/dev:/dev \\ \n    --volume=/var/lib/docker/:/var/lib/docker:ro \\ \n    --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\ \n    --volume=/var/run:/var/run:rw \\ \n    --net=host \\ \n    --pid=host \\ \n    --privileged=true \\ \n    -d \\ \n    gcr.io/google_containers/hyperkube:v1.0.1 \\ \n     /hyperkube kubelet --containerized --hostname-override=\"127.0.0.1\" \n--address=\"0.0.0.0\" --api- \n servers=http://localhost:8080 --config=/etc/kubernetes/manifests \n",
      "content_length": 1506,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n26\n The docker run command parameters are discussed in Table  1-4 . \n Table 1-4.  The docker run Command Parameters to start etcd \n Parameter \n Description \n --volume=/:/rootfs:ro \\ \n --volume=/sys:/sys:ro \\ \n --volume=/dev:/dev \\ \n --volume=/var/lib/docker/:/var/lib/docker:ro \\ \n --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\ \n --volume=/var/run:/var/run:rw \\ \n The Docker volumes to use \n --net = host \n Connects the Docker container to a network making \nuse of the host container network inside the container \n --pid = host \n Sets the pid namespace \n --privileged = true \n Provides access to most of the capabilities of the host \nmachine in terms of kernel features and host access \n -d \n Starts the container in the background \n gcr.io/google_containers/hyperkube:v1.0.1 \n The container image \n hyperkube kubelet \n--containerized \n--hostname-override = \"127.0.0.1\" \n--address = \"0.0.0.0\" \n--api- \n servers= http://localhost:8080 \n--config=/etc/kubernetes/manifests \n The command run \n",
      "content_length": 1040,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n27\n Figure 1-29.  The docker run Command to start Kubernetes Master \n The output from the docker run command to start the master is shown in Figure  1-29 . \n \n",
      "content_length": 207,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n28\n Starting Service Proxy \n To start the service proxy, which is a proxy for the Kubernetes service providing a pod/s interface using a \nservice selector with labels, start the service proxy by running the following docker run command. \n sudo docker run -d --net=host --privileged gcr.io/google_containers/hyperkube:v1.0.1 \n/hyperkube proxy -- master=http://127.0.0.1:8080 --v=2 \n The command parameters for the preceding command are discussed in Table  1-5 . \n The Master is required to be started each time the Kubernetes cluster manager is to be started. \nThe container image is downloaded only the first time the command is run, and on subsequent runs the \nimage is not downloaded as shown in Figure  1-30 . \n Figure 1-30.  Subsequent starts  of  Kubernetes Master do not need to download Container image again \n \n",
      "content_length": 867,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n29\n The output from the  preceding  docker run command is shown in Figure  1-31 . \n Table 1-5.  The docker run Command Parameters to start service proxy \n Parameter \n Description \n -d \n Runs the container in the background \n --net = host \n Sets the network for the container to the host’s network \n --privileged \n Provides access to most of the capabilities of the host \nmachine in terms of kernel features and host access \n gcr.io/google_containers/hyperkube:v1.0.1  The container image \n hyperkube proxy -- master=\n http://127.0.0.1:8080 --v = 2 \n The command to run. The master url is set to \n http://127.0.0.1:8080 . \n Figure 1-31.  Starting the Service proxy \n Listing the Kubernetes Docker Containers \n The Docker containers started for a Kubernetes cluster manager may be listed with the following command. \n sudo docker ps \n The Docker containers listed include a container for the service proxy; a container for the kubelet; a \ncontainer for etcd; and containers each for the master scheduler, controller, and apiserver, and pause as \nshown in Figure  1-32 . \n",
      "content_length": 1117,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n30\n The Docker container info may be found using the Docker container id. For example, obtain the \ncontainer id for the Docker container running the controller as shown in Figure  1-33 . \n Figure 1-32.  Listing  the  Docker Containers \n \n",
      "content_length": 286,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n31\n Run the following command to find the detail about the Docker container. \n sudo docker inspect 37971b53f2c1 \n Figure 1-33.  Obtaining  the Docker Container Id \n \n",
      "content_length": 214,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n32\n The detail such as the master ip and about the Docker container running the controller manager gets \noutput as shown in Figure  1-34 . \n Figure 1-34.  Listing  Docker  Container Information \n Installing kubectl \n The kubectl is used to control the Kubernetes cluster manager including running an image, getting the pods, \ngetting the replication controller, making an application available as a service exposed at a specified port, \nand scaling the cluster. Download Kubectl binaries with the following command. \n sudo wget https://storage.googleapis.com/kubernetes-release/release/v1.0.1/bin/linux/amd64/kubectl \n The kubectl binaries get downloaded as shown in Figure  1-35 . \n \n",
      "content_length": 733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n33\n Make the kubectl application executable by applying the + x permissions. \n sudo chmod +x kubectl \n Move the kubectl binaries to the /usr/local/bin/ directory. \n sudo mv kubectl /usr/local/bin/ \n The output from the preceding commands is shown in Figure  1-36 . \n Figure 1-35.  Installing Kubectl \n Figure 1-36.  Moving and making kubectl Binaries executable \n \n \n",
      "content_length": 415,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n34\n Figure 1-37.  Kubectl Command Usage \n The kubectl command lists the usage as shown in Figure  1-37 . \n \n",
      "content_length": 156,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n35\n Figure 1-39.  Listing the Kubernetes Service \n Figure 1-38.  Command Parameters for Kubect l \n The command parameters also  get  listed as shown in Figure  1-38 . \n Listing Services \n The following command should list the Kubernetes service. \n kubectl get services \n The  kubernetes  service gets listed as shown in Figure  1-39 . \n \n \n",
      "content_length": 388,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n36\n Listing Nodes \n The following command should list the Kubernetes node. \n kubectl  get  nodes \n The single node in the cluster gets listed as shown in Figure  1-40 . \n Figure 1-41.  Running he nginx Application on Kubernetes Cluster \n Figure 1-40.  Listing the Nodes \n Testing the Kubernetes Installation \n To test the Kubernetes cluster manager, run the nginx application using the following command. \n kubectl -s http://localhost:8080 run nginx --image=nginx --port=80 \n The output from the kubectl run command lists the replication controller, container/s, image/sm \nselector, and replicas as shown in Figure  1-41 . \n Expose the nginx application replication controller as a service with the kubectl expose command. \n kubectl expose rc nginx --port=80 \n The nginx Kubernetes service gets created running on port 80 as shown in Figure  1-42 . \n Figure 1-42.  Creating a Kubernetes Service for nginx Application \n \n \n \n",
      "content_length": 972,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n37\n List the detail about the nginx service with the kubectl get svc command. \n kubectl get svc nginx \n The nginx service detail gets listed as shown in Figure  1-43 . \n Figure 1-43.  Listing the Kubernetes Service nginx \n The cluster IP may be obtained with the following command. \n kubectl get svc nginx --template={{.spec.clusterIP}} \n The cluster ip is listed as 10.0.0.146 as shown in Figure  1-44 . \n Figure 1-44.  Listing the Cluster IP \n The web server may be called making use of the cluster ip with the following command. \n curl 10.0.0.146 \n \n \n",
      "content_length": 603,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "CHAPTER 1 ■ INSTALLING KUBERNETES USING DOCKER \n38\n Figure 1-45.  Using curl  to invoke Application \n The html output as text gets output as shown in Figure  1-45 . \n Summary \n In this chapter we installed Kubernetes using Docker. An Amazon EC2 instance running Ubuntu is used to \ninstall Docker and Kubernetes. The nginx application is run only to test the installation of the Kubernetes \ncluster manager. The kubectl commands to create an application, replication controller, and service are \ndiscussed in more detail in the next chapter. \n \n",
      "content_length": 544,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "39\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_2\n CHAPTER 2 \n Hello Kubernetes \n Kubernetes is a cluster manager for Linux containers. While Kubernetes supports other types of containers \nsuch as Rocket, and support for more types is to be added, we shall discuss Kubernetes in the context of \nDocker containers only. Docker is an open source container virtualization platform to build, package, and \nrun distributed applications in containers that are lightweight snapshots of the underlying OS. A Docker \nimage, which is application specific, encapsulates all the required software including dependencies for \nan application and is used to create Docker containers to run applications in the containers. The Docker \ncontainers are isolated from each other and have their own networking and filesystem and provide \nContainer as a Service (CaaS). Docker is similar to virtual machines based on virtualization platforms such \nas Oracle VirtualBox and VMWare Player in that it is a virtualization over the underlying OS, but is different \nin that while a virtual machine makes use of an entire operating system, multiple Docker containers share \nthe kernel and run in isolation on the host OS. Docker containers run on the Docker Engine, which runs on \nthe underlying OS kernel. \n In this chapter we shall introduce Kubernetes concepts using a Hello-World application. This chapter \nhas the following sections.\n Overview \n Why Kubernetes \n Setting the Environment \n Creating an Application Imperatively \n Creating an Application Declaratively \n Using JSON for the Resource Definitions \n Overview \n Kubernetes concepts include Pod, Service, and Replication controller and are defined in the following \nsubsections. \n What Is a Node? \n A  node is  a  machine (physical or virtual) running Kubernetes onto which Pods may be scheduled. The node \ncould be the  master node or one of the  worker nodes . In the preceding chapter on installing Kubernetes only \na single node was used. In a later chapter, Chapter  14 , we shall discuss creating a multi-node cluster with a \nmaster and worker node/s. \n",
      "content_length": 2147,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n40\n What Is a Cluster? \n A  cluster is a collection of nodes including other resources such as storage to run Kubernetes applications. \nA cluster has a single Kubernetes master node and zero or more worker nodes. A highly available cluster \nconsists of multiple masters or master nodes. \n What Is a Pod? \n A  Pod is a  collection of containers that are collocated and form an atomic unit. Multiple applications may \nbe run within a Pod and though the different containers within a Pod could be for the same application, \ntypically the different containers are for different applications. A Pod is a higher level abstraction for \nmanaging a group of containers with shared volumes and network namespace. All the applications \n(containers) in a Pod share the same filesystem and IP address with the port on which each application is \nexposed being different. Applications running in a Pod may access each other at “localhost”. Scheduling \nand replication are performed at the Pod level rather than at the individual container level. For example \nif a Pod defines two containers for different applications and replication level is set at 1, a single replica \nof the Pod consists of two containers, one each for the two applications. Pods facilitate resource sharing \nand communication what would otherwise be implemented using --link in individually running Docker \ncontainers. A Pod consisting of multiple containers would typically be used for tightly coupled applications. \nFor example, if an  nginx application makes use of MySQL database, the two applications are able to interact \nby Kubernetes running containers for each in the same Pod. \n What Is a Service? \n A  Service is the  external  interface for one or more Pods providing endpoint/s at which the application/s \nrepresented by the Service may be invoked. A Service is hosted at a single IP address but provides zero or \nmore endpoints depending on the application/s interfaced by the Service. Services are connected to Pods \nusing label selectors. Pods have label/s on them and a Service with a selector expression the same as a Pod \nlabel represents the Pod to an external client. An external client does not know or need to know about the \nPods represented by a Service. An external client only needs to know the name of the Service and the port at \nwhich a particular application is exposed. The Service routes requests for an application based on a round-\nrobin manner to one of the Pods selected using a label selector/. Thus, a Service is a high level abstraction \nfor a collection of applications leaving the detail of which Pod to route a request to up to the Service. \nA Service could also be used for load balancing. \n What Is a Replication Controller? \n A  Replication Controller manages the replication level of Pods as specified by the “replicas” setting in a \nReplication Controller definition or on the command line with the  –replicas parameter.  A  Replication \nController ensures that the configured level of Pod replicas are running at any given time. If a replica fails or \nis stopped deliberately a new replica is started automatically. A Replication Controller is used for scaling the \nPods within a cluster. A replica is defined at the Pod level implying that if a Pod consists of two containers a \ngroup of the two configured containers constitute a replica. \n",
      "content_length": 3376,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n41\n What Is a Label? \n A  Label is a key-value  pair  identifying a resource such as a Pod, Service, or Replication Controller: most \ncommonly a Pod. Labels are used to identify a group or subset of resources for tasks such as assigning them \nto a Service. Services use label selectors to select the Pods they manage. For example, if a Pod is labeled \n“app = helloApp” and a Service “selector” is set as “app = helloApp” the Pod is represented by the Service. \nService selectors are based on labels and not on the type of application they manage. For example, a Service \ncould be representing a Pod running a hello-world application container with a specific label. Another \nPod also running a hello-world container but with a label different than the Service selector expression \nwould not be represented by the Service. And a third Pod running an application that is not a hello-world \napplication but has the same label as the Service selector would also be represented by the same Service. \n What Is a Selector? \n A  selector is a key-value expression to identify resources using matching labels. As discussed in the \npreceding subsection a Service selector expression “app = helloApp” would select all Pods with the label \n“app = helloApp”. While typically a Service defines a selector to select Pods a Service could be defined to not \ninclude a selector and be defined to abstract other kinds of back ends. Two kinds of selectors are supported: \nequality-based and set-based. A selector could be made of multiple requirements implying that multiple \nexpressions (equality-based or set-based) separated by ',' could be specified. All of the requirements must \nbe met by a matching resource such as a Pod for the resource to be selected. A resource such as a Pod could \nhave additional labels, but the ones in the selector must be specified for the resource to be selected. The \nequality-based selector, which is more commonly used and also the one used in the book, supports =,!=,== \noperators, the = being synonymous to ==. \n What Is a Name? \n A  name is identifies a resource. A name is not the same as a label. For matching resources with a Service a \nlabel is used and not a name. \n What Is a Namespace? \n A  namespace is a level above the name to demarcate a group of resources for a project or team to prevent \nname collisions. Resources within different namespaces could have the same name, but resources within a \nnamespace have different names. \n What Is a Volume? \n A  volume is a directory within the filesystem of a container. A volume could be used to store data. Kubernetes \nvolumes evolve from Docker volumes. \n Why Kubernetes? \n Docker containers introduced a new level of modularity and fluidity for applications with the provision \nto package applications including dependencies, and transfer and run the applications across different \nenvironments. But with the use of Docker containers in production, practical problems became apparent \nsuch as which container to run on which node (scheduling), how to increase/decrease the number of \nrunning containers for an application (scaling), and how to communicate within containers. Kubernetes \n",
      "content_length": 3193,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n42\nwas designed to overcome all these and other practical issues of container cluster management. Kubernetes \nprovides dynamic container cluster orchestration in real time.  Kubernetes as a  cluster manager provides the \nfollowing benefits.\n -Microservices by breaking an application into smaller, manageable, scalable \ncomponents that could be used by groups with different requirements. \n -Fault-tolerant cluster in which if a single Pod replica fails (due to node failure, \nfor example), another is started automatically. \n -Horizontal scaling in which additional or fewer replicas of a Pod could be run \nby just modifying the “replicas” setting in the Replication Controller or using the \n –replicas parameter in the  kubectl scale command. \n -Higher resource utilization and efficiency. \n -Separation of concerns. The Service development team does not need to \ninterface with the cluster infrastructure team . \n Setting the Environment \n The following software is required for this chapter.\n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n We have used an Amazon EC2 Linux instance created from AMI Ubuntu Server 14.04 LTS (HVM), SSD \nVolume Type - ami-d05e75b8. \n SSH Login to the Ubuntu  interface  (Public IP address would be different for different users and multiple \nIP Addresses may have been used in this chapter). \n ssh -i \"docker.pem\" ubuntu@54.152.82.142 \n Install Docker as discussed in Chapter  1 and start the Docker Engine and verify its status using the \nfollowing commands. \n sudo service docker start \n sudo service docker status \n Install kubectl and start the Kubernetes cluster manager as discussed in Chapter  1 . Output the \nKubernetes cluster information using the following command. \n kubectl cluster-info \n The Kubernetes Master is shown running on  http://localhost:8080 in Figure  2-1 . \n Figure 2-1.  Getting Cluster Info \n \n",
      "content_length": 1929,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n43\n In the following sections we shall run a  hello-world application using the Kubernetes cluster manager. \nAn application may be run imperatively using the  kubectl tool on the command line or declaratively using \ndefinition files for a Pod, Replication Controller, and Service. We shall discuss each of these methods. The \nkubectl tool is used throughout this chapter and in subsequent chapters and a complete command reference \nis available at  https://cloud.google.com/container-engine/docs/kubectl/ . \n Creating an Application Imperatively \n With the Kubernetes master running on  http://localhost:8080 , as obtained in the preceding section, run \nthe following  kubectl run command to run a  hello-world application using the image  tutum/hello-world . \nThe  –s option specifies the Kubernetes API server host and port. The  –image command parameter specifies \nthe Docker image to run as  tutum/hello-world . The  –replicas parameter specifies the number of replicas \nto create as 1. A Replication Controller is created even if the  –replicas parameter is not specified. \nThe default number of replicas is 1. The  –port parameter specifies the container port the application is \nhosted at as 80. \n kubectl -s http://localhost:8080 run hello-world --image=tutum/hello-world --replicas=1 --port=80 \n A new application  container  called  hello-world gets created as shown in Figure  2-2 . A Replication \nController called “hello-world” also gets created. The Pod is created implicitly and label “run = hello-world” \nis added to the Pod. The number of replicas created is 1. The Replication Controller’s selector field is also set \nto “run=hello-world”. The Pods managed by a Replication Controller must specify a label that is the same as \nthe selector specified at the Replication Controller level. By default a Replication Controller selector is set to \nthe same expression as the Pod label. \n Figure 2-2.  Creating an Application including a Replication Controller and Pod Replica/s \n The Replication Controller created may be listed with the following command. \n kubectl get rc \n The  hello-world Replication Controller gets listed as shown in Figure  2-3 . \n Figure 2-3.  Listing the Replication Controllers \n \n \n",
      "content_length": 2252,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n44\n The Pods created and started are listed with the following command. \n kubectl get pods \n The single Pod created gets listed as shown in Figure  2-4 . A Pod name is assigned automatically. A Pod \nSTATUS “Running” is listed, but the Pod may still not be ready and available. The READY column value of \n0/1 indicates that 0 of 1 containers in the Pod are ready, which implies that the Pod has been created and is \nrunning but not yet ready. It could take a few seconds for a Pod to become Ready. \n Figure 2-4.  Listing the Pods \n Figure 2-5.  Listing a Pod as ready with all containers in the Pod as ready \n Run the same command again after a few seconds or a minute. \n kubectl get pods \n The Pod gets listed as ready as indicated by 1/1 in the READY column in Figure  2-5 . A value of 1/1 in \nthe READY column indicates that 1 of 1 containers in the Pod are ready. The syntax for the READY column \nvalue is  nReady/nTotal , which implies that  nReady of the total  nTotal containers in the Pod are ready. The \nKubernetes Pod  k8s-master-127.0.0.1 , for example, has a READY column value of 3/3, which implies that \n3 of 3 containers in the Kubernetes Pod are ready. \n Running a Pod and a Replication Controller does not implicitly create a Service. In the next subsection \nwe shall create a Service for the  hello-world application. \n Creating a Service \n Create a Kubernetes Service using the  kubectl expose command, which creates a Service from a Pod, \nReplication Controller, or another Service. As we created a Replication Controller called  hello-world , create \na Service using the following command in which the port to expose the Service is set to 8080 and the Service \ntype is  LoadBalancer . \n kubectl expose rc hello-world --port=8080 --type=LoadBalancer \n \n \n",
      "content_length": 1803,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n45\n A Kubernetes Service  called  hello-world gets created as shown in Figure  2-6 . The Service labels and \nselector also get set. The Service selector, listed in Figure  2-6 , is set to the same expression  run=hello-world \nas the Replication Controller selector, which is shown in Figure  2-3 , which implies that the Service manages \nthe Pods in the Replication Controller  hello-world . \n Figure 2-6.  Creating a Kubernetes Service \n The different types of Services are ClusterIp, NodePort, and LoadBalancer with the default being \nClusterIP, as discussed in Table  2-1 . \n Table 2-1.  Types of Services \n Service Type \n Description \n ClusterIp \n Uses a cluster-internal IP only. \n NodePort \n In addition to a cluster IP exposes the Service on each node of the cluster. \n LoadBalancer \n In addition to exposing the Service on a cluster internal Ip and a port on each \nnode on the cluster, requests the cloud provider to provide a load balancer for \nthe Service. The load balancer balances the load between the Pods in the Service. \n List all the Kubernetes Services with the following command. \n kubectl get services \n In addition to the “kubernetes” Service for the Kubernetes cluster manager a “hello-world” Service gets \ncreated as shown in Figure  2-7 . \n Figure 2-7.  Listing the Services \n \n \n",
      "content_length": 1333,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n46\n Describing a Pod \n Using the Pod name  hello-world-syrqz obtained from the NAME column in the result for the  kubectl get \npods command use the  kubectl describe pod command to list detailed information about the Pod. \n kubectl describe pod hello-world-syrqz \n Detailed information about the  Pod  including the IP address gets listed as shown in Figure  2-8 . The Pod \nhas a Label run=hello-world, which is the same as the replication controller  selector and also same as the \nservice  selector , which implies that the replication controller manages the Pod when scaling the cluster of \nPods for example, and the service represents the Pod to external clients. \n Figure 2-8.  Describing a Pod \n \n",
      "content_length": 732,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n47\n Next, we shall invoke the application using the IP Address  172.0.17.2 listed in the IP field. \n Invoking the Hello-World Application \n The  hello-world application may be invoked using the IP for the application as listed in Figure  2-8 with the \nfollowing  curl command. \n curl 172.17.0.2 \n The HTML output from the application is shown in Figure  2-9 . \n Figure 2-9.  Invoking a Application using Pod IP with curl \n \n",
      "content_length": 453,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n48\n To display the HTML output in a browser we need to invoke the application from a browser using URL \n 172.17.0.2:80 . If a browser is not available on the Amazon EC2 Ubuntu instance, as it is not by default, \nwe need to set up a SSH tunnel to the IP Address of the application using local port forwarding. Obtain the \nPublic DNS for the Amazon EC2 instance (ec2-52-91-200-41.compute-1.amazonaws.com in the example) \nand run the following command to set up a SSH tunnel to the  172.17.0.2:80 host:port from a local \nmachine. The  –L indicates that local port forwarding is used to forward local port 80 to  172.17.0.2:80 . \n ssh -i \"docker.pem\" -f -nNT -L 80:172.17.0.2:80 ubuntu@ec2-52-91-200-41.compute-1.amazonaws.com \n Invoke the URL  http://localhost in a browser on the local machine. The HTML output from the \n hello-world application gets displayed as shown in Figure  2-10 . The hostname is listed the same as the Pod \nname in Figure  2-5 . \n Figure 2-10.  Invoking  the  Hello-World Application in a Browser \n Scaling the Application \n A Replication Controller was created by default when we created the  hello-world application with replicas \nset as 1. Next, we shall scale up the number of Pods to 4. The  kubectl scale command is used to scale a \nReplication Controller. Run the following command to scale up the Replication Controller  hello-world to 4. \n kubectl scale rc hello-world --replicas=4 \n Subsequently, list the Pods using the following command. \n kubectl get pods \n \n",
      "content_length": 1524,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n49\n The additional Pods get listed but some of the new Pods could be listed in various states such as \nrunning but not ready, or image ready and container creating as shown in Figure  2-11 . \n Figure 2-11.  Scaling the Cluster of Pods with the Replication Controller \n After a few seconds run the same command again to list the Pods. \n kubectl get pods \n If the Pods have started all the Pods are listed with STATUS- > Running and READY state 1/1 as shown in \nFigure  2-12 . Scaling to 4 replicas does not create 4 new Pods, but the total number of Pods is scaled to 4 and \nthe single Pod created initially is included in the new scaled replicas of 4. \n Figure 2-12.  Listing all the Pods as Running and Ready \n \n \n",
      "content_length": 744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n50\n Describe the  hello-world Service using the following command. \n kubectl describe svc hello-world \n The Service name, label/s, selector, type, IP, and Endpoints get listed as shown in Figure  2-13 . \nThe Service may be invoked using the Endpoints for the various Pod replicas. \n Figure 2-13.  Describing the Service hello-world \n As discussed previously, set up SSH tunneling with port forwarding for the newly added endpoints. The \nfollowing command sets up a SSH tunnel with port forwarding from  localhost port 8081 to  172.17.0.3:80 \non the Amazon EC2 instance. \n ssh -i \"docker.pem\" -f -nNT -L 8081:172.17.0.3:80 ubuntu@ec2-52-91-200-41.compute-1.\namazonaws.com \n Subsequently invoke the  hello-world application in a browser on a local machine with url \n http://localhost:8081 to display the application output as shown in Figure  2-14 . \n \n",
      "content_length": 880,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n51\n Similarly the following command from a local machine sets up a SSH tunnel with port forwarding from \n localhost port 8082 to  172.17.0.4:80 on the Amazon EC2 instance. \n ssh -i \"docker.pem\" -f -nNT -L 8082:172.17.0.4:80 ubuntu@ec2-52-91-200-41.compute-1.\namazonaws.com \n Subsequently invoke the  hello-world application using url  http://localhost:8082 to display the \napplication output as shown in Figure  2-15 . \n Figure 2-14.  Invoking an Application in a Local Browser \n \n",
      "content_length": 510,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n52\n Deleting a Replication Controller \n The Replication Controller  hello-world may be deleted with the following command. \n kubectl delete rc hello-world \n The Replication Controller gets deleted as shown in Figure  2-16 . Subsequently invoke the following \ncommand to list the Replication Controllers. \n kubectl get rc \n The  hello-world Replication Controller does not get listed as shown in Figure  2-16 . \n Figure 2-16.  Deleting a Replication Controller \n Figure 2-15.  Invoking the  second  Service Endpoint in a Local Browser \n \n \n",
      "content_length": 568,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n53\n Deleting a Replication Controller deletes the Replication Controller and the Pods associated with the \nReplication Controller but does not delete the Service representing the Replication Controller. The  kubectl \nget services command still lists the Service as shown in Figure  2-17 . \n Figure 2-17.  Deleting a  Replication  Controller does not delete the Service \n Figure 2-18.  Deleting the hello-world Service \n Deleting a Service \n To delete the  Service  hello-world run the following command. \n kubectl delete svc hello-world \n Subsequently invoke the following command to list the Services. \n kubectl get services \n The output from the preceding two commands is shown in Figure  2-18 and does not list the  hello-\nworld Service. \n Creating an Application Declaratively \n Next, we shall create the same hello-world application declaratively using definition files for a Pod, Service, \nand Replication Controller. The definition files may be configured in YAML or JSON. We have used YAML \ninitially and also discussed the JSON alternative later. \n \n \n",
      "content_length": 1090,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n54\n Creating a Pod Definition \n Create a  hello-world.yaml file and specify a definition for a Pod in the file. For the  hello-world application \nthe following definition is used in which the  apiVersion mapping is for the API schema version ( v1 ),  kind \nmapping is the resource and set to  Pod . The metadata mapping specifies the Pod’s metadata and sets the \nname to  hello-world (arbitrary). The  spec mapping specifies the Pod behavior. The  spec - >  containers \nmapping specifies a collection of images to run. The  hello-world.yaml specifies a single container for \nimage  tutum/hello-world . Container name is set to  hello-world and container  ports mapping is a list of \nports with a single  containerPort mapping for 8080 port. \n apiVersion: v1 \n kind: Pod \n metadata: \n name: hello-world \n spec: \n  containers: \n    - \n      image: tutum/hello-world \n      name: hello-world \n      ports:         \n        -containerPort: 8080 \n The preceding is equivalent to the following command. \n kubectl run hello-world --image=tutum/hello-world --port=8080 \n Only a few of the schema elements have been used in the  hello-world.yaml . For the complete Pod \nschema refer  http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_pod . \n Next, create the  hello-world application using the  hello-world.yaml definition file with the following \n kubectl create command. The  –validate option validates the Pod definition file. A YAML lint validator \n( http://www.yamllint.com/ ) may be used to validate the YAML syntax in the  hello-world.yaml . The \nsyntax validation does not validate if the definition file conforms to the Pod schema. \n kubectl create -f hello-world.yaml --validate \n A Pod called  hello-world gets created as shown in Figure  2-19 . \n Figure 2-19.  Creating a Pod using a Definition File \n List the Pods with the following command, which is the same regardless of how a Pod has been created. \n kubectl get pods \n The  hello-world Pod gets listed as shown in Figure  2-20 . Initially, the Pod may not be READY- > 1/1. \nA READY column value of “0/1” implies that 0 of 1 containers in the Pod are ready. \n \n",
      "content_length": 2167,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n55\n Run the same command again after a few more seconds. \n kubectl get pods \n The  hello-world Pod gets listed with STATUS as “Running” and READY state as “1/1,” which implies \nthat 1 of 1 containers in the Pod are ready, as shown in Figure  2-21 . \n Figure 2-20.  Listing the Pods soon after creating the Pods \n Figure 2-21.  Listing the Pod as Ready and Running \n Describe the  hello-world Pod with the following command. \n kubectl describe pod hello-world \n \n \n",
      "content_length": 493,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n56\n The output from the preceding command is shown in Figure  2-22 . \n Figure 2-22.  Describing the hello-world Pod \n Invoke the  hello-world Pod application using the IP  172.17.0.2 . \n curl 172.17.0.2 \n The HTML output from the  hello-world application gets listed as shown in Figure  2-23 . \n \n",
      "content_length": 326,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n57\n Set up port forwarding from a local machine to the IP address of the  hello-world Pod. \n ssh -i \"docker.pem\" -f -nNT -L 80:172.17.0.2:80 ubuntu@ec2-52-91-200-41.compute-1.amazonaws.com \n Subsequently invoke the url  http://localhost:80 in a browser on a local machine to display the \nHTML output from the application as shown in Figure  2-24 . The default Hypertext transfer protocol port \nbeing 80, has been be omitted from the URL, as shown in Figure  2-24 . \n Figure 2-23.  Invoking the hello-world Application with curl \n \n",
      "content_length": 560,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n58\n Creating a Service Definition \n We created a Pod definition file and started a single Pod, but the Pod is not associated with any Service \nor Replication Controller. External clients have to access the Pod directly and are not able to scale the \napplication with just a single unassociated Pod. Create a Service definition file  hello-world-service.yaml \nas listed below. If copying and pasting YAML files listed in this chapter and other chapters it is \nrecommended to use the YAML Lint ( http://www.yamllint.com/ ) to format the files before using in an \napplication. \n apiVersion: v1 \n kind: Service \n metadata: \n  labels: \n    app: hello-world \n  name: hello-world \n spec: \n  ports: \n    - \n      name: http \n      port: 80 \n      targetPort: http \n  selector: \n    app: hello-world \n  type: LoadBalancer \n Figure 2-24.  Invoking the hello-world Application in  a  Browser on a local machine \n \n",
      "content_length": 932,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n59\n The main mappings of the Service definition file are  kind ,  metadata , and  spec . The  kind is set to \n Service to indicate a Kubernetes Service. The label  app and the  name constitute the metadata. The  spec \nmapping includes a  ports mapping for port 80 with name  http . Optionally a  targetPort may be set, which \ndefaults to the same value as port. The  selector is the main mapping in the  spec and specifies a mapping to \nbe used for selecting the Pods to expose via the Service. The  app:hello-world selector implies that all Pods \nwith label  app=hello-world are selected. The definition file may be created in the vi editor and saved with \nthe  :wq command as shown in Figure  2-25 . \n Figure 2-25.  Service Definition File hello-world-service.yaml \n \n",
      "content_length": 799,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n60\n A complete reference to the Kubernetes Service schema is available at  http://kubernetes.io/v1.1/\ndocs/api-reference/v1/definitions.html#_v1_service . \n Create a Service using the definition file with the  kubectl create command. \n kubectl create -f hello-world-service.yaml \n The  hello-world Service gets created as shown in Figure  2-26 . \n Figure 2-26.  Creating the hello-world Service using the Definition File \n Figure 2-27.  Listing the hello-world Service \n List the Services with the following command. \n kubectl get services \n The  hello-world Service gets listed in addition to the  kubernetes Service as shown in Figure  2-27 . \n Describe the  hello-world Service with the following command. \n kubectl describe svc hello-world \n The Service name, namespace, labels, selector, type, Ip get listed as shown in Figure  2-28 . Because the \n hello-world Pod created using the Pod definition file does not include a label to match the Service selector, \nit is not managed by the Service. As the  hello-world Service is not managing any Pods, no endpoint gets \nlisted. \n \n \n",
      "content_length": 1113,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n61\n Creating a Replication Controller Definition \n Next, we shall create a Replication Controller and label the Replication Controller to match the selector \nof the Service created previously. Create a Service definition file  hello-rc.yaml . The kind mapping of \na Replication Controller is  ReplicationController . The  replicas ’ sub-mapping in the  spec mapping \nis set to 2 to create two replicas from the Pod also specified in the  spec . At least one of the labels in the \ntemplate- > metadata- > labels must match the Service selector in the Service definition file for the Pod \nto be exposed by the Service. As the Service selector in the  hello-world Service is  app:hello-world add \nthe  app:hello-world label to the Replication Controller template. The app:hello-world setting in YAML \ntranslates to app=hello-world. The template may define one or more containers to be included in the Pod \ncreated from the Replication Controller. We have included container definition for only one container for \nimage  tutum/hello-world . The  hello-rc.yaml is listed below. A YAML lint ( http://www.yamllint.com/ ) \nmay be used to validate the YAML syntax. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  name: hello-world \n spec: \n  replicas: 2 \n  template: \n    metadata: \n      labels: \n        app: hello-world \n Figure 2-28.  Describing  the  hello-world Service \n \n",
      "content_length": 1413,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n62\n    spec: \n      containers: \n        - \n          image: tutum/hello-world \n          name: hello-world \n          ports: \n            - \n              containerPort: 8080 \n              name: http \n A complete schema for the Replication Controller is available at  http://kubernetes.io/v1.1/docs/\napi-reference/v1/definitions.html#_v1_replicationcontroller . \n Create the Replication Controller using the definition file with the  kubectl create command, the same \ncommand that was used to create a Pod and a Service. \n kubectl create -f hello-rc.yaml \n Subsequently run the following command to list the Replication Controllers. \n kubectl get rc \n A  hello-world Replication Controller gets created and gets listed as shown in Figure  2-29 . The number \nof replicas are listed as 2 as specified in the definition file. \n Figure 2-29.  Creating a Replication Controller \n List the Pods created with the Replication Controller with the following command. \n kubectl get pods \n The two Pods created from the definition file get listed as shown in Figure  2-30 . The Pod created the Pod \ndefinition file also gets listed but is not associated with the Replication Controller. Initially some or all of the \nnew Pods may be listed as not ready as indicated by the 0/1 value in the READY column for one of the Pods \nin Figure  2-30 . \n Figure 2-30.  Listing the Pods soon after creating a Replication Controller \n \n \n",
      "content_length": 1444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n63\n Invoke the same command again to list the Pods after a few more seconds. \n kubectl get pods \n All the Pods get listed as READY- > 1/1 and Running as shown in Figure  2-31 . \n Figure 2-32.  Describing the Service hello-world \n Figure 2-31.  Listing all the Pods as Running and Ready \n To describe the  hello-world Service run the following command. \n kubectl describe service hello-world \n The Service detail including the Endpoints get listed as shown in Figure  2-32 . The service selector is \napp = hello-world and the service endpoints are 172.17.0.3:8080 and 172.17.0.4:8080. \n All the preceding commands to create the  hello-world Replication Controller, list its Pods and \nendpoints association with the  hello-world Service shown in Figure  2-33 . \n \n \n",
      "content_length": 793,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n64\n Invoking the Hello-World Application \n The Pods associated with the hello-world Replication Controller and Service by the same name may be \ninvoked using the Service endpoints as listed in the Service description in Figure  2-33 . For example, invoke \nthe  172.17.0.3 endpoint with the following  curl command. \n curl 172.17.0.3 \n The HTML output from the Pod gets output as shown in Figure  2-34 . \n Figure 2-33.  Summary of Commands  to create a Replication Controller \n \n",
      "content_length": 507,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n65\n Similarly, invoke the 172.17.0.4 endpoint with the following curl command. \n curl 172.17.0.4 \n The HTML output from the other Pod gets output as shown in Figure  2-35 . \n Figure 2-34.  HTML Output from invoking the hello-world Application with curl \n \n",
      "content_length": 285,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n66\n Figure 2-35.  Invoking another Service Endpoint with curl \n \n",
      "content_length": 94,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n67\n To invoke the Service endpoints in a browser on a local machine configure local port forwarding for the \nService endpoints. \n ssh -i \"docker.pem\" -f -nNT -L 8081:172.17.0.3:8080 ubuntu@ec2-52-91-200-41.compute-1.\namazonaws.com \n ssh -i \"docker.pem\" -f -nNT -L 8082:172.17.0.4:8080 ubuntu@ec2-52-91-200-41.compute-1.\namazonaws.com \n Subsequently invoke the  localhost:8081 URL in a browser on a local machine as shown in Figure  2-36 \nto display the HTML output from the Pod at endpoint  172.17.0.3:8080 . \n Figure 2-36.  Invoking the hello-world Application in a Local machine Browser with its Service Endpoint \n \n",
      "content_length": 647,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n68\n Similarly invoke the  localhost:8082 URL in a browser on a local machine as shown in Figure  2-37 to \ndisplay the HTML output from the Pod at endpoint  172.17.0.4:8080 . \n Figure 2-37.  Invoking  another  Service Endpoint in a Browser \n Scaling the Application \n To scale the  hello-world Replication Controller to 6 replicas, for example, run the following  kubectl scale \ncommand. \n kubectl scale rc hello-world --replicas=6 \n An output of “scaled” as shown in Figure  2-38 indicates the Replication Controller has been scaled. \n Figure 2-38.  Scaling an Application \n The number of Pods for the hello-world Replication Controller increases when the Replication \nController is scaled up to 6. To list the Pods run the following command. \n kubectl get pods \n \n \n",
      "content_length": 796,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n69\n Six Pods get listed in addition to the  hello-world Pod created initially using a Pod definition file as \nshown in Figure  2-39 . The preceding command may have to be run more than once to list all the Pods with \nSTATUS as Running and READY state as 1/1. The  hello-world Pod is not associated with the  hello-world \nReplication Controller as it does not include a label that matches the selector label (same as template label) \nin the Replication Controller. \n Figure 2-39.  Listing Pods after Scaling \n In the preceding example we scaled  up the Replication Controller, but the  kubectl scale command \nmay also be used to scale  down the Replication Controller. As an example, scale down the  hello-world \nReplication Controller to 2 replicas. \n kubectl scale rc hello-world --replicas=2 \n Subsequently list the Pods. \n kubectl get pods \n The number of replicas gets listed as 2 in addition to the  hello-world Pod as shown in Figure  2-40 . \n \n",
      "content_length": 980,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n70\n Using JSON for the Resource Definitions \n In the preceding section we used the YAML format to create the Pod, Service, and Replication \nController definition files.  The  definition files may be developed in JSON format instead. The YAMLToJSON \nutility ( http://yamltojson.com/ ) may be used to convert from YAML to JSON and the JSON lint \n( http://jsonlint.com/ ) may be used to validate the JSON. A JSON to YAML utility is also available at \n http://jsontoyaml.com/ . The  JSON  definition file  hello-world-service.json for the  hello-world \nService is listed: \n { \n  \"apiVersion\": \"v1\", \n  \"kind\": \"Service\", \n  \"metadata\": { \n    \"name\": \"hello-world\", \n    \"labels\": { \n      \"app\": \"hello-world\" \n    } \n  }, \n  \"spec\": { \n    \"ports\": [ \n      { \n        \"name\": \"http\", \n        \"port\": 80, \n        \"targetPort\": \"http\" \n      } \n    ], \n    \"selector\": { \n      \"app\": \"hello-world\" \n    }, \n    \"type\": \"LoadBalancer\" \n  } \n } \n Create a  hello-world-service.json file using a vi editor and copy and paste the preceding listing to \nthe file. Save the file using :wq as shown in Figure  2-41 . \n Figure 2-40.  Scaling Down to 2 Replicas \n \n",
      "content_length": 1184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n71\n Delete the  hello-world Service and  hello-world Replication Controller created previously. Run the \nfollowing command to create a Service from the JSON format definition file. \n kubectl create –f hello-world-service.json \n The  hello-world Service gets created as shown in Figure  2-42 . \n Figure 2-41.  Service  Definition File in JSON Format \n Figure 2-42.  Creating a Service from the JSON Definition File \n \n \n",
      "content_length": 448,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n72\n Subsequently list all the Kubernetes Services. \n kubectl get services \n The  hello-world Service gets listed as shown in Figure  2-43 . \n Figure 2-43.  Listing the  Service s \n The JSON format version of  the  Replication Controller definition file,  hello-rc.json, is as follows. \n { \n  \"apiVersion\": \"v1\", \n  \"kind\": \"ReplicationController\", \n  \"metadata\": { \n    \"name\": \"hello-world\" \n  }, \n  \"spec\": { \n    \"replicas\": 2, \n    \"template\": { \n      \"metadata\": { \n        \"labels\": { \n          \"app\": \"hello-world\" \n        } \n      }, \n      \"spec\": { \n        \"containers\": [ \n          { \n            \"image\": \"tutum/hello-world\", \n            \"name\": \"hello-world\", \n            \"ports\": [ \n              { \n                \"containerPort\": 8080, \n                \"name\": \"http\" \n              } \n            ] \n          } \n        ] \n      } \n    } \n  } \n } \n \n",
      "content_length": 905,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n73\n Create  the  hello-rc.json file in a vi editor and save the file with :wq as shown in Figure  2-44 . \n Figure 2-44.  Creating  the  hello-rc.json File in vi Editor \n Delete all previously created Pods and Replication Controllers. Run the following command to create \nthe  hello-world Replication Controller. \n kubectl create –f hello-rc.json \n The  hello-world Replication Controller gets created as shown in Figure  2-45 . Subsequently run the \nfollowing command to list the Replication Controllers. \n kubectl get rc \n The  hello-world Replication Controller gets listed as shown in Figure  2-45 . List the Pods created by the \nReplication Controller using the following command. \n \n",
      "content_length": 717,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n74\n kubectl get pods \n Because  replicas is set as 2 two  Pods get listed as shown in Figure  2-45 . \n Figure 2-46.  Describing the hello-world Service \n Figure 2-45.  Creating a  Replication  Controller from the JSON format Definition File \n Describe the  hello-world Service with the following command. \n kubectl describe svc hello-world \n Because the label on the  hello-world Replication Controller matches the Service selector, the two Pods \ncreated using the Replication Controller are represented by the Service and have endpoints in the Service as \nshown in Figure  2-46 . \n \n \n",
      "content_length": 615,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n75\n Invoke a Service endpoint using a curl command as follows. \n curl 172.17.0.2 \n The HTML output from  the  curl command gets output as shown in Figure  2-47 . \n Figure 2-47.  Invoking the hello- world Application with curl \n \n",
      "content_length": 258,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "CHAPTER 2 ■ HELLO KUBERNETES\n76\n Set up local port forwarding to a Service endpoint. \n ssh -i \"docker.pem\" -f -nNT -L 80:172.17.0.2:8080 ubuntu@ec2-52-91-200-41.compute-1.\namazonaws.com \n Subsequently invoke the Service endpoint in a browser in a local machine to display  the  HTML output \nas shown in Figure  2-48 . \n Figure 2-48.  Displaying hello- world  Application HTML in a Browser \n Summary \n In this chapter we introduced the Kubernetes concepts such as Pod, Service, Replication Controller, Labels, \nand Selector. We also developed a hello-world application both imperatively on the command line, and \ndeclaratively using definition files. We discussed two different supported formats for the definition files: \nYAML and JSON. In the next chapter we shall discuss using environment variables in Pod definitions. \n \n",
      "content_length": 825,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "77\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_3\n CHAPTER 3 \n Using Custom Commands and \nEnvironment Variables \n Kubernetes orchestrates Docker containers, and the instructions to run for a Docker image are specified in \nthe  Dockerfile . The  ENTRYPOINT instruction specifies the command to run, and the  CMD instruction specifies \nthe default arguments for the  ENTRYPOINT command. Kubernetes provides two fields,  \"Command\" and  \"Args\" , \nto be specified for a container image in a Pod definition to override the default settings of  ENTRYPOINT and \n CMD . We shall discuss these fields in this chapter. We shall also discuss using environment variables in a Pod \ndefinition’s container mapping with the  \"env\" field mapping. \n This chapter has the following sections.\n Setting the Environment \n The ENTRYPOINT and CMD Instructions \n The Command and Args Fields in a Pod Definition \n Environment Variables \n Using the default ENTRYPOINT and CMD from a Docker Image \n Overriding Both the ENTRYPOINT and CMD in a Docker Image \n Specifying both the Executable and the Parameters in the Command Mapping \n Specifying both the Executable and the Parameters in the Args Mapping \n Setting the Environment \n The following  software is used in this chapter.\n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n Install Docker engine, Kubernetes, and Kubectl as discussed in chapter  1 . Start Docker Engine and verify \nits status with the following commands. \n sudo service docker start \n sudo service docker status \n",
      "content_length": 1602,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n78\n The output shown in Figure  3-1 indicates that Docker is running. \n The ENTRYPOINT and CMD Instructions \n The  ENTRYPOINT in a Docker image’s  Dockerfile specifies the command to run when the image is run. The \n ENTRYPOINT has two forms discussed in Table  3-1 . A  Dockerfile may have only one  ENTRYPOINT . If multiple \n ENTRYPOINT s are specified, the last  ENTRYPOINT entry is run. \n Figure 3-1.  Starting Docker  and  Verifying Its Status \n Table 3-1.  ENTRYPOINT  Forms \n Form \n Description \n Format \n Exec form \n Runs an executable using the specified parameters. \nThe exec form is the preferred form if environment \nvariable substitution is not used. But if environment \nvariable substitution is used the shell form must \nbe used. The exec form does not perform any \nenvironment variable substitution. \n ENTRYPOINT [“executable“”, \n“param1”, “param2”] \n Shell form \n Runs the command in a shell and prevents any \nCMD or run command-line arguments to be used \nin conjunction with ENTRYPOINT. The shell form \nstarts a shell with /bin/sh -c even though a shell is \nnot invoked explicitly. \n ENTRYPOINT command \nparam1 param2 \n The  CMD instruction specifies the args for the  ENTRYPOINT command in exec form. The  CMD has three \nforms as discussed in Table  3-2 . A Dockerfile may have only one  CMD entry. If multiple  CMD s are specified the \nlast  CMD entry is run.  The  CMD instruction may include an executable. \n \n",
      "content_length": 1490,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n79\n If command-line args are provided to  the  docker run command those override the default args in  CMD \ninstruction. The  ENTRYPOINT instruction could also be used in combination with a helper script. Next, we \nshall discuss the two fields, “command” and “args” that could be used to override the  ENTRYPOINT and  CMD \ninstructions in a  Dockerfile respectively. \n The Command and Args Fields in a Pod Definition \n Kubernetes has the provision to override the  ENTRYPOINT (command) and  CMD (args) instructions specified \nin the Dockerfile. Two field mappings in a Pod’s definition file could be used to override the  ENTRYPOINT and \n CMD instructions. These fields are “Command” and “Args,” and they override the Dockerfile “ENTRYPOINT \n“and “CMD” instructions respectively. The overriding applies based on which of these instructions and \nfields are specified. Some examples of overriding are discussed in Table  3-3 .   \n Table 3-2.  CMD Forms \n Form \n Description \n Format \n Exec form \n The exec form specifies the command to \ninvoke and the command parameters in \nJSON array format. The exec form does not \nperform environment variable substitution. \nIf environment variable substitution is to be \nperformed, use the shell form or invoke the \nshell explicitly in the exec form. In JSONs array \nformat, double quotes “” must be used around \nnames. \n CMD [“executable”, “param1”, \n“param2”] \n Default parameters \nto ENTRYPOINT \n Specifies the default args to the ENTRYPOINT \ncommand. Both the ENTRYPOINT and CMD \nmust be specified. Both the ENTRYPOINT \nand CMD must be specified using JSON array \nformats. In JSONs array format, double quotes \n“” must be used around names. \n CMD [“param1”, ”param2”] \n Shell form \n Invokes a  shell  to invoke the specified \ncommand using the parameters. The command \nis invoked as a sub-command of /bin/sh –c. \n CMD command param1 param2 \n",
      "content_length": 1940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n80\n Environment Variables \n A Pod’s schema has the provision to specify environment variables. The environment variables are specified \nas “name” and “value” field mappings as a collection within a  container  definition’s “ env ” mapping. The \nformat for specifying environment variables is as follows. \n spec: \n  containers: \n    - \n    image: \"image name\" \n    name: \"container name \" \n    env: \n      - \n        name: \"env variable 1\" \n        value: \" env variable 1 value\" \n      - \n        name: \"env variable 2\" \n        value: \" env variable 2 value\" \n Table 3-3.  Examples of Overriding ENTRYPOINT  and CMD with Command and Args \n ENTRYPOINT \n CMD \n Command \n Args \n Used \n Example 1 \n yes \n yes \n yes \n yes \n The Command and Args field mappings \nin the Pod definition file override the \nENTRYPOINT and CMD instructions in \nDockerfile. \n Example 2 \n yes \n yes \n no \n no \n The Dockerfile ENTRYPOINT command \nand CMD args are used. \n Example 3 \n yes \n yes \n yes \n no \n Only the command in the Command is \nused and Dockerfile ENTRYPOINT and \nCMD instructions are ignored. \n Example 4 \n yes \n yes \n no \n yes \n The Docker image’s command as \nspecified in the ENTRYPOINT is used \nwith the args specified in the Pod \ndefinition’s Args. The args from the \nDockerfile’s CMD are ignored. \n Example 5 \n no \n yes \n no \n no \n The command and parameters from the \nCMD instruction are run. \n Example 6 \n no \n yes \n yes \n yes \n The Command and Args field mappings \nin the Pod definition file are used. \nThe CMD instruction in Dockerfile is \noverridden. \n Example 7 \n no \n yes \n no \n yes \n The Args field mapping in the Pod \ndefinition file is used. The CMD \ninstruction in Dockerfile is overridden. \n Example 8 \n no \n yes \n yes \n no \n The command in the Command mapping \nis used, and Dockerfile CMD instruction \nis ignored. \n",
      "content_length": 1879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n81\n The environment variables are added to the  docker run command using  –e when the Docker image is \nrun by Kubernetes. The environment variables may also be used in “command” and “args” mappings using \nthe environment variable substitution if a shell is used to run the Docker image command. A shell is invoked \nif one or more of the following is used:\n -The shell form of the ENTRYPOINT or CMD is used \n -The shell is invoked explicitly in the ENTRYPOINT or CMD instruction \n In the following sections we shall use the “ubuntu” Docker image to demonstrate overriding the default \n ENTRYPOINT command and the default  CMD args. We shall start with using the default  ENTRYPOINT and  CMD \ninstructions. \n Using the Default ENTRYPOINT and CMD from a Docker Image \n The  Dockerfile for the Ubuntu image does not provide an  ENTRYPOINT instruction but the  CMD instruction \nis set to  CMD [\"/bin/bash\"] . In the example in this section we shall create a Pod definition that does not \noverride the  ENTRYPOINT or  CMD instruction from the Docker image. Create a Pod definition file as follows \nwith the image as “ubuntu” and some environment variables set. \n apiVersion: v1 \n kind: Pod \n metadata: \n  name: \"hello-world\" \n  labels: \n    app: \"helloApp\" \n spec: \n  restartPolicy: Never \n  containers: \n    - \n      image: \"ubuntu\" \n      name: \"hello\" \n      ports: \n  containerPort: 8020 \n      env: \n        - \n          name: \"MESSAGE1\" \n          value: \"hello\" \n        - \n          name: \"MESSAGE2\" \n          value: \"kubernetes\" \n The  env.yaml file may be created in a vi editor and saved with the :wq command as shown in \nFigure  3-2 . \n",
      "content_length": 1703,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n82\n Run the following command to create a Pod from the definition file  env.yaml . \n kubectl create –f env.yaml \n The  hello-world pod gets created as shown in Figure  3-3 . Run the following command to list the pods. \n Figure 3-2.  A Pod definition file env.yaml to demonstrate Environment Variables \n \n",
      "content_length": 364,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n83\n kubectl get pods \n The  hello-world pod gets created but the Docker container created is listed as “creating” as shown \nin Figure  3-3 . \n When the Docker container gets created the  STATUS column value transitions to “Running” and the \n READY column value becomes 1/1, which indicates that 1 of 1 containers in the Pod are ready and which \nis not shown in Figure  3-4 because the  READY state transitions to 0/1 quickly thereafter. After the Pod \ncommand/args have run the Pod terminates and  STATUS becomes  ExitCode:0 as shown in Figure  3-4 . \n Figure 3-3.  Creating and listing a Pod \n Figure 3-4.  After the Command/Args have run, a Pod terminates and the Pod’s Status becomes ExitCode:0 \n Run the following command to list the output from the Pod. \n kubectl logs hello-world \n As the default  CMD [\"/bin/bash\"] in the “Ubuntu” Docker image is just the invocation of the bash shell \nusing  /bin/bash, no output is generated as shown in Figure  3-5 . \n \n \n",
      "content_length": 1025,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n84\n Overriding Both the ENTRYPOINT and CMD \n In the second example we shall override both the  ENTRYPOINT and  CMD in a  Dockerfile using  Command \nand  Args mappings in the Pod definition file.  Using in combination  ENTRYPOINT and CMD will help us to \nspecify the default executable for the image and also it will provide the default arguments to that executable. \nEnvironment variable substitution is used for the  MESSAGE1 and  MESSAGE2 environment variables with the \n $(VARIABLE_NAME) syntax. \n command: [\"/bin/echo\"] \n args: [\" $(MESSAGE1)\", \" $(MESSAGE2)\"] \n The  env.yaml Pod definition file is listed: \n apiVersion: v1 \n kind: Pod \n metadata: \n  name: \"hello-world\" \n  labels: \n    app: \"helloApp\" \n spec: \n  restartPolicy: Never \n  containers: \n    - \n      image: \"ubuntu\" \n      name: \"hello\" \n      ports: \n          - \n          containerPort: 8020 \n      env: \n        - \n          name: \"MESSAGE1\" \n          value: \"hello\" \n        - \n          name: \"MESSAGE2\" \n          value: \"kubernetes\" \n      command: [\"/bin/echo\"] \n      args: [\" $(MESSAGE1)\", \" $(MESSAGE2)\"] \n The  env.yaml file may be opened and modified in the vi editor and saved using the :wq command as \nshown in Figure  3-6 . \n Figure 3-5.  No output generated  with  Default CMD [“/bin/bash”] in “ubuntu” Docker Image \n \n",
      "content_length": 1367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n85\n First, we need to delete the  hello-world pod created in the first example with the following command. \n kubectl delete pod hello-world \n The  hello-world pod gets deleted as shown in Figure  3-7 . \n Figure 3-6.  Modifying env.yaml in a vi Editor \n Figure 3-7.  Deleting the hello-world Pod \n \n \n",
      "content_length": 360,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n86\n Run the  kubectl create command to create a Pod from the definition file  env.yaml . \n kubectl create –f env.yaml \n The  hello-world Pod gets created as shown in Figure  3-8 . \n Figure 3-10.  Outputting Message Generated from Environment Variables using Value Substitution \n Figure 3-8.  Creating the hello-world Pod from definition file env.yaml \n Figure 3-9.  Listing the Pods with transitioning STATUS value \n Run the  kubectl get command to list the pods. \n kubectl get pods \n The  hello-world pod gets listed as shown in Figure  3-9 . The Pod transitions quickly from the  STATUS of \n“Running” to  ExitCode:0 as shown in Figure  3-9 . \n Run the following command to list the output from the Pod. \n kubectl logs hello-world \n The message created from environment variables  MESSAGE1 and  MESSAGE2 using substitution gets listed \nas shown in Figure  3-10 . \n \n \n \n",
      "content_length": 931,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n87\n Specifying both the Executable and the Parameters in the \nCommand Mapping \n In the third example, specify that both the executable and the parameters are specified in the Command \nmapping in the Pod definition file. Environment variable substitution is used for the  MESSAGE1 and  MESSAGE2 \nenvironment variables. The shell is not required to be invoked/started explicitly if the environment variable \nsyntax  $(VARIABLE_NAME) is used, which is what we have used. \n command: [\"/bin/echo\", \" $(MESSAGE1)\", \" $(MESSAGE2)\"] \n The  env.yaml Pod definition file is listed: \n apiVersion: v1 \n kind: Pod \n metadata: \n  name: \"hello-world\" \n  labels: \n    app: \"helloApp\" \n spec: \n  restartPolicy: Never \n  containers: \n    - \n      image: \"ubuntu\" \n      name: \"hello\" \n      ports: \n          - \n          containerPort: 8020 \n      env: \n        - \n          name: \"MESSAGE1\" \n          value: \"hello\" \n        - \n          name: \"MESSAGE2\" \n          value: \"kubernetes\" \n      command: [\"/bin/echo\", \" $(MESSAGE1)\", \" $(MESSAGE2)\"] \n The  env.yaml file may be opened and modified in the vi editor and saved using the :wq command as \nshown in Figure  3-11 . \n",
      "content_length": 1219,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n88\n Run the  kubectl create command to create a Pod from the definition file  env.yaml . \n kubectl create –f env.yaml \n The  hello-world pod gets created as shown in Figure  3-12 . Run the  kubectl get command to list the pods. \n Figure 3-11.  The Command mapping with both the Command Executable and the Parameters \n \n",
      "content_length": 379,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n89\n kubectl get pods \n The  hello-world pod gets listed though initially the Pod  STATUS is not listed as “Running” as shown \nin Figure  3-12 . The Pod transitions quickly to the  READY value of 1/1 and subsequently 0/1. The 1/1 READY \nvalue is not shown in Figure  3-12 as it transitions quickly to 0/1. After the command has run the Pod \nterminates and the  STATUS becomes  ExitCode:0 as shown in Figure  3-12 . \n Subsequently invoke the following command to list the output generated by the Pod. \n kubectl get logs \n The message created from environment variables  MESSAGE1 and  MESSAGE2 gets listed as shown in \nFigure  3-13 . \n Figure 3-12.  Creating and Listing the Pod with Definition file from Figure  3-11 \n \n",
      "content_length": 778,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n90\n Specifying Both the Executable and the Parameters in the \nArgs Mapping \n In the fourth example, specify both the executable and the parameters in the  Args mapping in the Pod \ndefinition file as a result overriding the  CMD instruction in the  Dockerfile . Environment variable substitution \nis used for the  MESSAGE1 and  MESSAGE2 environment variables with the environment variable syntax \n $(VARIABLE_NAME) . \n args: [\"/bin/echo\", \" $(MESSAGE1)\", \" $(MESSAGE2)\"] \n The  env.yaml Pod definition file is listed: \n apiVersion: v1 \n kind: Pod \n metadata: \n  name: \"hello-world\" \n  labels: \n    app: \"helloApp\" \n spec: \n  restartPolicy: Never \n  containers: \n    - \n      image: \"ubuntu\" \n      name: \"hello\" \n      ports: \n          - \n          containerPort: 8020 \n      env: \n        - \n          name: \"MESSAGE1\" \n          value: \"hello\" \n        - \n          name: \"MESSAGE2\" \n          value: \"kubernetes\" \n      args: [\"/bin/echo\", \" $(MESSAGE1)\", \" $(MESSAGE2)\"] \n The  env.yaml file may be opened and modified in the vi editor and saved using the :wq command as \nshown in Figure  3-14 . \n Figure 3-13.  Message output by  Pod  created in Figure  3-12 \n \n",
      "content_length": 1227,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n91\n The  hello-world Pod created from the previous example must be deleted as otherwise the error shown \nin Figure  3-15 gets generated when the  kubectl create command is run. \n Figure 3-14.  The args Mapping in the Pod definition file specifies both the Command Executable and the \nParameters \n Figure 3-15.  Error Generated if hello-world Pod already exists \n \n \n",
      "content_length": 426,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n92\n Run the  kubectl create command to create a Pod from the definition file  env.yaml . \n kubectl create –f env.yaml \n The output from the command is shown in Figure  3-16 . \n Figure 3-16.  Creating a Pod from definition file in Figure  3-14 \n The  hello-world pod gets created as shown in Figure  3-17 . Run the  kubectl get command to list \nthe pods. \n Figure 3-18.  Outputting  the  Message Generated by Pod \n Figure 3-17.  The Pod terminates and its Status transitions to ExitCode:0 after the command has run \n kubectl get pods \n The  hello-world pod gets listed as shown in Figure  3-17 . The Pod transitions quickly to the READY \nvalue of 1/1 and subsequently 0/1. The 1/1 READY value is not shown in Figure  3-17 as it transitions quickly \nto 0/1. After the command has run the Pod terminates and the STATUS becomes ExitCode:0 as shown in \nFigure  3-17 . \n Subsequently invoke the following command to list the output generated by the Pod. \n kubectl get logs \n The message created with environment variables substitution from  MESSAGE1 and  MESSAGE2 gets listed \nas shown in Figure  3-18 . \n \n \n \n",
      "content_length": 1165,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "CHAPTER 3 ■ USING CUSTOM COMMANDS AND ENVIRONMENT VARIABLES\n93\n Summary \n In this chapter we discussed the  ENTRYPOINT and  CMD instructions in a Docker image  Dockerfile : \ninstructions used to run the default command with the default parameters when the image is run in a \nKubernetes Pod. We also discussed the  Command and  Args mappings in a Pod definition file that could be \nused to override the  ENTRYPOINT and  CMD instructions. We discussed various examples of overriding the \ndefault instructions for the “ubuntu” Docker image with “command” and “args” field mappings in a Pod \ndefinition file. We also demonstrated the use of environment variables in a Pod definition file. In the next \nchapter we shall discuss using MySQL Database with Kubernetes. \n",
      "content_length": 762,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "   PART II \n Relational Databases \n  \n",
      "content_length": 38,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "97\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_4\n CHAPTER 4 \n Using MySQL Database \n MySQL database is the most commonly used open source database. The Docker image “mysql” could be \nused to create a Docker container running a MySQL database instance. Running Docker separately for a \nsingle MySQL instance or multiple instances lacks the features of scheduling multiple instances, scaling, and \nproviding a service for external clients. In this chapter we shall discuss how the Kubernetes container cluster \nmanager could be used to overcome all of those deficiencies. \n Setting the Environment \n Creating a Service \n Creating a Replication Controller \n Listing the Pods \n Listing Logs \n Describing the Service \n Starting an Interactive Shell \n Starting the MySQL CLI \n Creating a Database Table \n Exiting the MySQL CLI and Interactive Shell \n Scaling the Replicas \n Deleting the Replication Controller \n Setting the Environment \n The following software is required for this chapter.\n -Docker Engine (latest version) \n -Kubernetes Cluster Manager (version 1.01) \n -Kubectl (version 1.01) \n -Docker image “mysql” (latest version) \n",
      "content_length": 1186,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n98\n We have used an Amazon EC2 instance created from AMI Ubuntu Server 14-04 LTS (HVM), SSD \nVolume Type - ami-d05e75b8 to install the required software. The procedure to install Docker, Kubernetes, \nand Kubectl is discussed in chapter  1 . Obtain the Public IP address of the Amazon EC2 instance as shown in \nFigure  4-1 . \n SSH log in to the Ubuntu instance using the Public IP Address, which would be different for different \nusers. \n sh -i \"docker.pem\" ubuntu@52.90.43.0 \n Start the Docker engine and verify its status. \n sudo service docker start \n sudo service docker status \n The Docker Engine should be listed as “running” as shown in Figure  4-2 . \n Figure 4-1.  Obtaining the Public IP Address \n Figure 4-2.  Starting Docker  and Verifying Its Status \n \n \n",
      "content_length": 799,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n99\n Creating a Service \n In this section we shall create a Kubernetes service using a definition file. We have used the YAML format for \ndefinition files, but JSON could be used just as well. Create a service definition file called  mysql-service.yaml \nand copy the following listing to the file. Within the  spec field mapping for the service the “selector” expression \nis set to  app: \"mysql-app,\" which translates to service selector app=mysql-app and which implies that the \nservice routes traffic to Pods with the label  app=mysql-app . If the  selector expression is empty all Pods are \nselected. The port to expose the service is set to 3306 within the ports listing. And the service has a label \n app: \"mysql-app\" . The  kind field mapping must have value “Service.” \n apiVersion: v1 \n kind: Service \n metadata: \n name: \"mysql\" \n labels: \n  app: \"mysql-app\" \n spec: \n ports: \n  # the port that this service should serve on \n  - port: 3306 \n # label keys and values that must match in order to receive traffic for this service \n selector: \n  app: \"mysql-app\" \n The service schema is available at  http://kubernetes.io/v1.1/docs/api-reference/v1/\ndefinitions.html#_v1_service . Setting the  selector field in the YAML definition file to  app: \"mysql-app\" \nimplies that all Pods with the YAML definition file label setting  app: \"mysql-app\" are managed by the \nservice. Create the service using the definition file with the  kubectl create command. \n kubectl create -f mysql-service.yaml \n The  mysql service gets created and the output is “services/mysql” as shown in Figure  4-3 . \n List the service using  the  following command. \n kubectl get services \n Figure 4-3.  Creating a Service for MySQL Database \n \n",
      "content_length": 1750,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n100\n Creating a Replication Controller \n In this section we shall create a replication controller managed by the service created in the previous section. \nCreate a replication controller definition file called  mysql-rc.yaml and copy the following/next listing to the \nfile. The  kind field mapping must have value “ReplicationController.” The replication controller has a label \n app: \"mysql-app\" in the  metadata field mapping. If the labels are empty they are defaulted to the labels of \nthe Pods the replication controller manages. The  \"spec\" field mapping defines the replication controller \nand includes the  \"replicas\" field mapping for the number of replicas to create. The  replicas is set to 1 \nin the following/next listing. The default number of replicas is also 1. The  spec includes a  selector field \nmapping called  app: \"mysql-app,\" which selects all Pods with label  app: \"mysql-app\" for the replication \ncontroller to manage and count toward the “replicas” setting. A Pod could have other labels in addition \nto the selector, but must include the selector expression/s of a replication controller to be managed by \nthe replication controller. Similarly, a replication controller could be managing Pods not started with the \nreplication controller definition file. \n Labels and selector expression settings in YAML definition files are not used as such, but are translated \nto a label/selector by replacing the ‘:’ with the ‘=’. For example, service/replication controller selector setting \napp: “mysql-app” becomes selector app = mysql-app selector and label setting app: “mysql-app” becomes \nlabel app = mysql-app. \n If a  selector is not specified the labels on the template are used to match the Pods and count toward \nthe “replicas” setting. The  \"template\" field mapping defines a Pod managed by the replication controller. \nThe  spec field mapping within the  template field specifies the behavior of the Pod. The  \"containers\" field \nmapping within the  \"spec\" field defines the collection/list of containers to create including the image, the \nenvironment variables if any, and the ports to use for each container. \n We need to use an environment variable for the MySQL database replication controller. The Docker \nimage “mysql” requires (is mandatory) the environment variable  MYSQL_ROOT_PASSWORD to run a Docker \ncontainer for MySQL database. The  MYSQL_ROOT_PASSWORD variable sets the password for the  root user. \nEnvironment variables are set with the  \"env\" mapping within a  containers field listing. An  env mapping \nconsists of a  name mapping and a  value mapping. The  MYSQL_ROOT_PASSWORD environment variable is set \nas shown in the following listing. The  \"ports\" field collection includes a  containerPort mapping for port \n3306. The indentations and hyphens in a YAML file must be well formatted and the following listing should \nbe copied and syntax validated in the YAML Lint ( http://www.yamllint.com/ ). The YAML lint only validates \nthe syntax and does not validate if the Pod definition field conforms to the schema for a pod. The Pod schema \nis available at  http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_podspec . \n Figure 4-4.  Listing the mysql Service \n The  mysql service gets listed as shown in Figure  4-4 . \n \n",
      "content_length": 3325,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n101\n --- \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  labels: \n    app: \"mysql-app\" \n spec: \n  replicas: 1 \n  selector: \n    app: \"mysql-app\" \n  template: \n    metadata: \n      labels: \n        app: \"mysql-app\" \n    spec: \n      containers: \n      - \n        env: \n          - \n            name: \"MYSQL_ROOT_PASSWORD\" \n            value: \"mysql\" \n        image: \"mysql\" \n        name: \"mysql\" \n        ports: \n          - \n            containerPort: 3306 \n The  mysql-rc.yaml definition file may be created in the vi editor and saved with the :wq command as \nshown in Figure  4-5 . \n",
      "content_length": 633,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n102\n Create a replication controller from the service definition file with the  kubectl create command. \n kubectl create -f mysql-rc.yaml \n As the output in Figure  4-6 indicates, the  mysql replication controller gets created. \n Figure 4-5.  Definition File for Replication Controller \n Figure 4-6.  Creating a Replication Controller for MySQL Database \n \n \n",
      "content_length": 392,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n103\n List the replication with the following command. \n kubectl get rc \n The  mysql replication controller including the container name, image name, selector expression \n(app=mysql-app), and number of replicas get listed as shown in Figure  4-7 . \n To describe the  mysql replication controller run the following command. \n kubectl describe rc mysql \n The  replication controller  name, namespace, image, selector, labels, replicas, pod status, and events get \nlisted as shown in Figure  4-8 . \n Figure 4-7.  Listing the MySQL Replication Controller \n Figure 4-8.  Describing the MySQL Replication Controller \n \n \n",
      "content_length": 647,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n104\n Listing Logs \n List the  Pod  logs for a pod: for example, the  mysql-wuo7x pod, with the following command. \n kubectl logs mysql-wuo7x \n Figure 4-9.  Listing the Pod/s  for  MySQL Database \n Listing the Pods \n The  Pods created may be listed with the following command. \n kubectl get pods \n As shown in Figure  4-9 the 2 replicas created by the replication controller get listed. Initially the Pods \nmay not be listed as READY 1/1. Run the preceding command after a few seconds, multiple times if required, \nto list all the Pods as ready. \n \n",
      "content_length": 581,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n105\n Figure 4-10.  Listing the  Logs generated by the Pod for MySQL Database \n The Pod logs get listed as shown in Figure  4-10 . \n \n",
      "content_length": 166,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n106\n The MySQL Server is listed as started and “ready for connections” as shown in Figure  4-11 . \n Describing the Service \n To describe the  mysql  service run the following command. \n kubectl describe svc mysql \n The service name, namespace, labels, selector, type, Ip, port and endpoints get listed. Because the \nnumber of replicas is set to 1 only one endpoint is listed as shown in Figure  4-12 . \n Figure 4-11.  Listing mysqld  as  Ready for Connections \n \n",
      "content_length": 496,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n107\n Starting an Interactive Shell \n Bash is the free version of the Bourne shell distributed with Linux and GNU operating systems (OS). \nFor Docker images that have a Linux OS image as the base image as specified in the  FROM instruction in \nthe  Dockerfile , the software running in a Docker container may be accessed using the Bash shell. The \n \"mysql\" Docker image is based on the  \"debian\" image and as a result supports access to software running in \nthe Docker containers via a bash interactive shell. \n Next, we shall start an  interactive shell to start the MySQL CLI. But first we need to obtain the container \nid for one of the containers running MySQL. Run the following command to list the Docker containers. \n sudo docker ps \n Figure 4-12.  Describing  the  MySQL Service \n \n",
      "content_length": 822,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n108\n Using the Docker container id from the output from the preceding command, start an interactive shell. \n sudo docker exec -it 526f5d5f6c2e bash \n An interactive shell or tty gets started as shown in Figure  4-14 . \n Figure 4-13.  Listing the Docker Containers \n Figure 4-14.  Starting the Interactive Terminal \n The Docker container for the  mysql image is shown listed in Figure  4-13 . \n \n \n",
      "content_length": 430,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n109\n Starting the MySQL CLI \n Within the interactive shell run the following command to start the MySQL CLI as user root. \n mysql –u root –p \n When prompted with Password: set the password as the value of the environment variable \n MYSQL_ROOT_PASSWORD , which was set as “mysql” in the  mysql-rc.yaml definition file. The MySQL CLI gets \nstarted as shown in Figure  4-15 . \n List the databases with the following command. \n show databases; \n The default databases shown in Figure  4-16 include the  \"mysql\" database, which we shall use to create \na database table. The other databases are system databases and should not be used for user tables. \n Figure 4-15.  Starting the MySQL CLI Shell \n Figure 4-16.  Listing the Databases \n \n \n",
      "content_length": 767,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n110\n Set the database “mysql” as the current database with the following command. \n use mysql \n The database gets set as  mysql  as  indicated by the “Database changed” output in Figure  4-17 . \n Creating a Database Table \n Next, create a database table called  Catalog with the following SQL statement. \n CREATE TABLE Catalog(CatalogId INTEGER PRIMARY KEY,Journal VARCHAR(25),\nPublisher VARCHAR(25),Edition VARCHAR(25),Title VARCHAR(45),Author VARCHAR(25)); \n Add a row of data to the  Catalog table with the following SQL statement. \n INSERT INTO Catalog VALUES('1','Oracle Magazine','Oracle Publishing',\n'November December 2013','Engineering as a Service','David A. Kelly'); \n The  Catalog table gets created and a row of data gets added as shown in Figure  4-18 . \n Subsequently run the following  SQL  statement to query the database table  Catalog . \n SELECT * FROM Catalog; \n Figure 4-17.  Setting the Database \n Figure 4-18.  Creating a MySQL Database Table \n \n \n",
      "content_length": 1004,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n111\n Exiting the MySQL CLI and Interactive Shell \n Exit the MySQL CLI with the “quit”  command. \n quit \n Exit the interactive terminal with the “exit” command. \n exit \n The output from the preceding commands is shown in Figure  4-20 . \n Scaling the Replicas \n One of the main benefits of Kubernetes is to be able to scale the number of MySQL instances in the cluster. \nRun the following  kubectl scale command to scale the replicas from 1 to 4. \n kubectl scale rc mysql --replicas=4 \n Subsequently run the following command to list the Pods. \n kubectl get pods \n Figure 4-19.  Querying the Database Table \n Figure 4-20.  Exiting the MySQL CLI Shell and Docker Container Interactive Shell \n The single row of data added gets listed as shown in Figure  4-19 . \n \n \n",
      "content_length": 796,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n112\n The number of Pods for the MySQL database gets increased to 4 as shown in Figure  4-21 . Some of the \nPods may be listed as READY- > 0/1, which implies the Pod/s are not ready yet. When READY becomes 1/1 \na Pod is ready to be accessed. The 0/1 value implies that 0 of the 1 Docker containers in the Pod are ready \nand similarly the 1/1 value implies that 1 of 1 containers is ready. The general syntax for the READY column \nvalue if all the n containers in the Pod are running is of the form n/n. The STATUS must be “Running” for a \nPod to be considered available. \n To describe the  mysql service, run the following command. \n kubectl describe svc mysql \n The service description is the same as before except that the number of endpoints has increased to 4 as \nshown in Figure  4-22 . \n Figure 4-21.  Scaling the Pod Replicas to Four \n \n",
      "content_length": 876,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n113\n The command “scale” will also allow us to specify one or more preconditions for the scale actions \nneeded. The following (Table  4-1 ) preconditions are supported. \n Deleting the Replication Controller \n To delete the replication controller  mysql , run the following command. \n kubectl delete rc mysql \n The replication controller gets deleted as shown in Figure  4-23 . Whenever a  kubectl command output \nto create or delete an artifact (a Pod, service or replication controller) is of the form  artifact type/artifact \nname , it implies that the command has succeeded to create/delete the pod/service/replication controller. \n Figure 4-22.  Describing the MySQL Service  After  Scaling the Pod Replicas \n Figure 4-23.  Deleting the Replication Controller \n Subsequently run the following command to get the replication controllers. The  mysql rc does not get \nlisted as shown in Figure  4-24 . \n kubectl get rc \n Table 4-1.  Preconditions for the ‘kubernetes scale’ command \n Precondition \n Description \n --current-replicas \n The current number of replicas for the scale to be performed. \n --resource-version \n The resource version to match for the scale to be performed. \n \n \n",
      "content_length": 1219,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "CHAPTER 4 ■ USING MYSQL DATABASE\n114\n Describe the service  mysql again with the following command. \n kubectl describe svc mysql \n No “Endpoints” get listed as shown in Figure  4-24 because all the Pods get deleted when the replication \ncontroller managing them is deleted. \n Summary \n In this chapter we discussed orchestrating the MySQL database cluster using the Kubernetes cluster \nmanager. We created a Kubernetes service to represent a MySQL-based Pod. The “mysql” Docker image is \nused to create a Pod. We used a replication controller to create replicas for MySQL base Pods. Initially the \nnumber of replicas is set to 1. We used a Docker container running a MySQL instance to start the MySQL \nCLI and create a database table. Subsequently, we scaled the number of replicas to 4 using the replication \ncontroller. When scaled, the number of replicas and therefore the number of MySQL instances becomes 4. The \nreplication controller maintains the replication level through replica failure or replica shut down by a user. \nThis chapter also demonstrates the use of environment variables. The  MYSQL_ROOT_PASSWORD environment \nvariable is required to run a container for the Docker image “mysql” and we set the  MYSQL_ROOT_PASSWORD \nenvironment variable in the Pod spec in the replication controller. In the next chapter we shall discuss using \nanother open source database, the PostgreSQL database. \n Figure 4-24.  Describing the Service  after  Deleting the Replication Controllers \n \n",
      "content_length": 1493,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "115\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_5\n CHAPTER 5 \n Using PostgreSQL Database \n PostgreSQL is an open source object-relational database. PostgreSQL is scalable both in terms of the quantity \nof data and number of concurrent users. PostgreSQL is supported in several of Apache Hadoop ecosystem \nprojects such as Apache Sqoop and may be used for Apache Hive Metastore. PostgreSQL 9.5 offers several \nnew features such as support for  UPSERT ,  BRIN indexing, faster sorts, and the  TABLESAMPLE clause for getting a \nstatistical sample of a large table. In this chapter we shall discuss creating a PostgreSQL 9.5 cluster using the \nKubernetes cluster manager. We shall discuss both the imperative approach and the declarative approach for \ncreating and scaling a PostgreSQL cluster. This chapter has the following sections.\n Setting the Environment \n Creating a PostgreSQL Cluster Declaratively \n Creating a PostgreSQL Cluster Imperatively \n Setting the Environment \n We have used the same type of Amazon EC2 instance in this chapter as in other chapters, an instance based \non Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-d05e75b8 AMI. The following software is \nrequired for this chapter. \n -Docker Engine (latest version) \n -Kubernetes Cluster Manager (version 1.01) \n -Kubectl (version 1.01) \n -Docker Image “postgres” (latest version) \n The procedure to install the required software, start Docker engine and Kubernetes cluster manager, is \ndiscussed in chapter  1 . To install the software first we need to log in to the Amazon EC2 instance. Obtain the \nPublic IP Address of the Amazon EC2 instance as shown in Figure  5-1 . \n",
      "content_length": 1703,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n116\n SSH Login to the  Ubuntu instance using the Public IP Address. \n ssh -i \"docker.pem\" ubuntu@52.91.60.182 \n Start the  Docker engine and verify its status. \n sudo service docker start \n sudo service docker status \n Docker should be indicated as “running” as shown in Figure  5-2 . \n List the services with the following command. \n kubectl get services \n Figure 5-1.  Obtaining the Public IP Address \n Figure 5-2.  Starting Docker \n \n \n",
      "content_length": 477,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n117\n The  kubernetes service should get listed as shown in Figure  5-3 . \n Creating a PostgreSQL Cluster Declaratively \n In the following subsections we shall create and manage a PostgreSQL  cluster declaratively , which implies \nwe shall use definition files. The definition files could be based on the YAML format or the JSON format. \nWe shall be using YAML format. It is recommended to create the service first so that any pods created \nsubsequently have a service available to represent them. If the RC (replication controller) is created first, the \npods are not usable until a service is created. \n Creating a Service \n Create a service definition  file  postgres-service.yaml and copy the following listing to the file. The  \"spec\" \nfield mapping for the service specifies the behavior of the service. The ports on which the service is exposed \nare defined in the  \"ports\" field mapping. Only the port 5432 is exposed because PostgreSQL runs on port 5432. \nThe  selector expression is set to  app: \"postgres\" . All Pods with the label  app=postgres are managed by \nthe service. \n apiVersion: v1 \n kind: Service \n metadata: \n  name: \"postgres\" \n  labels: \n    app: \"postgres\" \n spec: \n  ports: \n    - port: 5432 \n  selector: \n    app: \"postgres\" \n The  postgres-service.yaml file may be created using the vi editor and saved with the :wq command as \nshown in Figure  5-4 . \n Figure 5-3.  Listing the Kubernetes Services \n \n",
      "content_length": 1467,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n118\n Create the service using the  kubectl create command with the  postgres-service.yaml definition file. \n kubectl create -f postgres-service.yaml \n Subsequently list the services. \n kubectl get services \n Also list the Pods. \n kubectl get pods \n An output of  services/postgres from the first command indicates that the service has been created. \nThe second command lists the  postgres service as running at port 5432 as shown in Figure  5-5 . The IP \nAddress of the service is also listed. Creating a service by itself does not create a Pod by itself and only the \nPod for the Kubernetes is listed. A service only manages or provides an interface for Pods with the label that \nmatches the  selector expression in the service. \n Figure 5-4.  Service Definition File postgres-service.yaml \n \n",
      "content_length": 832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n119\n Describe the service  postgres with the following command. \n kubectl describe svc postgres \n The service name, namespace, labels, selector, type, IP address, Port exposed on, and Endpoints get listed. \nBecause no Pods are initially associated with the service, no endpoints are listed as shown in Figure  5-6 . \n Creating a Replication Controller \n In this section we shall create a definition file for a replication  controller . Create a definition file called \n postgres-rc.yaml . The definition file has the field discussed in Table  5-1 . \n Figure 5-5.  Creating a Service and listing the Service \n Figure 5-6.  Describing the postgres Service \n \n \n",
      "content_length": 697,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n120\n Optionally the replication controller’s  selector field mapping may be specified. The key:value \nmapping in the  selector must match a label in the template- > metadata field mapping for the replication \ncontroller to manage the Pod in the template. The  selector field mapping if not specified defaults to \nthe template- > metadata- >  labels field mapping. In the following listing the  selector is italicized and \nnot included in the definition file used. The Pod's template- > metadata- > labels field mapping specifies \nan expression  app: \"postgres\", which translates to Pod label app=postgres . The  labels field \nexpression must be the same as the  \"selector\" field expression in the service definition file, which was \ndiscussed in the previous section, for the service to manage the Pod. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  name: \"postgres\" \n Table 5-1.  Replication Controller Definition File postgres-rc.yaml \n Field \n Value \n Description \n apiVersion \n v1 \n The API version. \n kind \n ReplicationController \n Defines the file to be a replication \ncontroller. \n metadata \n Metadata for the replication controller. \n metadata- > name \n The name of the replication controller. \nEither the name or the generateName \nfield must be specified. The \ngenerateName field is the prefix to use in \nan automatically generated name. \n spec \n The specification for the replication \ncontroller. \n spec- > replicas \n 2 \n The number of Pod replicas to create. \n template \n Specifies the template for the Pod that \nthe replication controller manages. \n template- > metadata \n The metadata for the Pod including \nlabels. The label is used to select the Pods \nmanaged by the replication controller \nand must manage the selector expression \nin the service definition file if the service \nis to represent the Pod. \n template- > spec \n Pod specification or configuration. \n template- > spec- > containers \n The containers in a Pod. Multiple \ncontainers could be specified but in \nthis chapter only the container for \nPostgreSQL is specified. \n template- > spec- > containers- > image \n template- > spec- > containers- > name \n The Docker image to run in the container. \nFor PostgreSQL the image is “postgres.” \nThe name field specifies the container \nname. \n",
      "content_length": 2315,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n121\n spec: \n  replicas: 2 \n   selector: \n     - app: \"postgres\" \n  template: \n    metadata: \n       labels: \n         app: \"postgres\" \n    spec: \n      containers: \n      - \n        image: \"postgres\" \n        name: \"postgres\" \n Copy the preceding listing to the  postgres-rc.yaml file. The  postgres-rc.yaml file may be opened in \nthe vi editor and saved with :wq as shown in Figure  5-7 . \n Figure 5-7.  Replication Controller Definition File \n \n",
      "content_length": 485,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n122\n Create a replication controller using the definition file  postgres-rc.yaml . \n kubectl create -f postgres-rc.yaml \n Subsequently list the replication controllers. \n kubectl get rc \n An output of  replicationcontrollers/postgres from the first command as shown in Figure  5-8 \nindicates that the replication controller  postgres has been created. The second command lists the  postgres \nreplication controller. As discussed before the Replication Controller SELECTOR column is set to the same \nvalue as the Pod label, app=postgres. \n Describe the replication controller  postgres with the following command. \n kubectl describe rc postgres \n The replication controller's name, namespace, image associated with the rc, selectors if any, labels, \nnumber of replicas, pod status, and events get listed as shown in Figure  5-9 . \n Figure 5-9.  Describing the Replication Controller for PostgreSQL Database \n Figure 5-8.  Creating and listing the Replication Controller for PostgreSQL Database \n \n \n",
      "content_length": 1036,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n123\n Getting the Pods \n To get and list the  Pods run the following command. \n kubectl get pods \n The two Pods created by the replication controller get listed as shown in Figure  5-10 . The Pods should \nhave the Running STATUS and have the READY column value as 1/1. \n Figure 5-10.  Listing the Pods for PostgreSQL Database \n Starting an Interactive Command Shell \n To be able to create a PostgreSQL table we need to start an  interactive bash shell to access the PostgreSQL \nserver running in a Docker container, and start the psql SQL shell for PostgreSQL. But, first we need to find \nthe container id for a Docker container running the PostgreSQL database. Run the following command to list \nthe Docker containers. \n sudo docker ps \n Two of the Docker containers are based on the “postgres” image as shown in Figure  5-11 . Copy the \ncontainer id for the first Docker container for the  postgres image from the CONTAINER ID column. \n \n",
      "content_length": 977,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n124\n Using the container id start the interactive shell. \n sudo docker exec -it a786960b2cb6 bash \n The interactive shell gets started as shown in Figure  5-12 . \n Figure 5-11.  Listing the Docker Containers \n Figure 5-12.  Starting an Interactive Shell \n Starting the PostgreSQL SQL Terminal \n Next, start the psql SQL shell for PostgreSQL. Set the user as  postgres . \n su –l postgres \n \n \n",
      "content_length": 430,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n125\n Start the psql command line shell with the following command. \n psql postgres \n The psql shall get started as shown in Figure  5-13 . \n For the general command syntax for the  psql command refer  http://www.postgresql.org/docs/9.5/\nstatic/app-psql.html . \n Creating a Database Table \n In the psql shell run the following SQL statements to create a database  table called  wlslog and add data to \nthe table. \n CREATE TABLE wlslog(time_stamp VARCHAR(255) PRIMARY KEY,category VARCHAR(255),type \nVARCHAR(255),servername VARCHAR(255),code VARCHAR(255),msg VARCHAR(255)); \n INSERT INTO wlslog(time_stamp,category,type,servername,code,msg) VALUES('Apr-8-2014-7:\n06:16-PM-PDT','Notice','WebLogicServer','AdminServer','BEA-000365','Server state changed \nto STANDBY'); \n INSERT INTO wlslog(time_stamp,category,type,servername,code,msg) VALUES('Apr-8-2014-7:\n06:17-PM-PDT','Notice','WebLogicServer','AdminServer','BEA-000365','Server state changed \nto STARTING'); \n INSERT INTO wlslog(time_stamp,category,type,servername,code,msg) VALUES('Apr-8-2014-7:\n06:18-PM-PDT','Notice','WebLogicServer','AdminServer','BEA-000360','Server started in \nRUNNING mode'); \n Database table  wlslog gets created and a row of data gets added as shown in Figure  5-14 . \n Figure 5-13.  Starting the  psql CLI Shell \n Figure 5-14.  Creating a Database Table \n \n \n",
      "content_length": 1375,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n126\n Run the following SQL statement to query the database table  wlslog . \n SELECT * FROM wlslog; \n The 3 rows of data added get listed as shown in Figure  5-15 . \n Exiting the Interactive Command Shell \n To exit the psql shell run the following command. \n \\q \n To exit the interactive terminal run the following command. \n exit \n The  psql shell and the interactive shell get exited as shown in Figure  5-16 . \n Figure 5-15.  Querying the Database Table \n Figure 5-16.  Exiting the psql Shell and Docker Container Interactive Shell \n \n \n",
      "content_length": 577,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n127\n Scaling the PostgreSQL Cluster \n One of the main benefits of the Kubernetes cluster manager is to be able to scale the cluster as required. \nInitially we created 2 replicas. For example, to scale up the number of PostgreSQL instances to 4 run the \nfollowing command. \n kubectl scale rc postgres --replicas=4 \n An output of “scaled” from the preceding command indicates that the cluster has been scaled as shown \nin Figure  5-17 . \n Subsequently list the pods with the following command. \n kubectl get pods \n The 4 Pods get listed as shown in Figure  5-18 . Initially some of the Pods could be listed as not “Running” \nand/or not in READY (1/1) state. \n Run the preceding command again after a few seconds. \n kubectl get pods \n The new Pods added to the cluster also get listed as “Running” and in READY state 1/1 as shown in \nFigure  5-19 . \n Figure 5-17.  Scaling the number of Pod Replicas to 4 \n Figure 5-18.  Listing the Pods after Scaling \n \n \n",
      "content_length": 992,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n128\n Describe the  postgres service again. \n kubectl describe svc postgres \n Initially no Endpoint was listed as being associated with the service when the service was initially \nstarted. With 4 Pods running 4 Endpoints get listed as shown in Figure  5-20 . \n Listing the Logs \n To list the logs  data for a Pod, for example the postgres-v0k42 Pod, run the following command. \n kubectl logs postgres-v0k42 \n The output in Figure  5-21 lists the PostgreSQL starting. \n Figure 5-19.  Listing all the Pods as running and ready \n Figure 5-20.  Describing the postgres Service \n \n \n",
      "content_length": 615,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n129\n When the PostgreSQL database gets started completely the message “database system is ready to accept \nconnections” gets output as shown in Figure  5-22 . \n Figure 5-21.  Listing the Logs for a Pod running PostgreSQL Database \n \n",
      "content_length": 271,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n130\n Deleting the Replication Controller \n To delete the replication  controller  postgres and as a result delete all the Pods managed by the replication \ncontroller run the following command. \n kubectl delete rc postgres \n The  postgres replication controller gets deleted as indicated by the  replicationcontrollers/\npostgres output shown in Figure  5-23 . Subsequently, run the following command to list the replication \ncontrollers. \n kubectl get rc \n Figure 5-22.  PostgreSQL Database listed as Started and subsequently Shutdown in the Logs \n \n",
      "content_length": 587,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n131\n The  postgres replication controller does not get listed as shown in Figure  5-23 . Deleting the replication \ncontroller does not delete the service managing the replication controller. To demonstrate list the services. \n kubectl get services \n The  postgres service is still getting listed, as shown in Figure  5-23 . \n Stopping the Service \n To stop the service  postgres run the following  command . \n kubectl stop service postgres \n Subsequently run the following command again. \n kubectl get services \n The  postgres service does not get listed as shown in Figure  5-24 . \n Figure 5-24.  Stopping the postgres Service \n Figure 5-23.  Deleting a Replication Controller \n Creating a PostgreSQL Cluster Imperatively \n Using a declarative approach with definition files offers finer control over the service and replication \ncontroller. But a replication controller and service could also be created on the command line with  kubectl \ncommands. In the following subsections we shall create a replication controller and a service.  \n \n \n",
      "content_length": 1080,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n132\n Creating a Replication Controller \n To create a replication  controller called  postgres for image “postgres” with number of replicas as 2 and Post \nas 5432 run the following command. \n kubectl run postgres --image=postgres --replicas=2 --port=5432 \n The  postgres replication controller with 2 replicas of Pod with image  postgres and  selector \nexpression  run=postgres gets created as shown in Figure  5-25 . \n List the replication controllers with the following command. \n kubectl get rc \n The  postgres replication controller gets listed as shown in Figure  5-26 . \n Getting the Pods \n To list the Pods managed by the replication controller run the following command. \n kubectl get pods \n The two  Pods get listed as shown in Figure  5-27 . Initially some of the Pods could be listed not Ready as \nindicated by the 0/1 READY column value. Run the preceding command again to list the Pods as ready with \nREADY column value as 1/1. \n Figure 5-25.  Creating a Replication Controller Imperatively \n Figure 5-27.  Listing the Pods \n Figure 5-26.  Listing the Replication Controllers \n \n \n \n",
      "content_length": 1133,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n133\n Creating a Service \n To create a  service we need to run the  kubectl expose command. Initially only the  kubernetes service is \nrunning. To demonstrate, run the following command. \n kubectl get services \n As shown in Figure  5-28 only the  kubernetes service is listed. \n To create a service for the replication controller  \"postgres\" run the following command in which the \n –port parameter specifies the port at which the service is exposed. The service type is set as  LoadBalancer . \n kubectl expose rc postgres --port=5432 --type=LoadBalancer \n Subsequently list the services. \n kubectl get services \n The postgres service gets listed as shown in Figure  5-29 . \n Figure 5-28.  Listing the “kubernetes” Service \n Figure 5-29.  Creating a Service exposed at Port 5432 \n \n \n",
      "content_length": 821,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n134\n Creating a Database Table \n The procedure to create a database  table is the same as discussed previously for the declarative section and \nis discussed only briefly in this section. List the Docker containers with the following command. \n sudo docker ps \n Two of the  Docker containers are listed with image as  postgres in the IMAGE column as shown in \nFigure  5-30 . Copy the container id for one of these columns from the CONTAINER ID column. \n Figure 5-30.  Listing the Docker Containers \n \n",
      "content_length": 538,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n135\n Start the interactive shell with the following command. \n sudo docker exec -it af0ac629b0e7d bash \n The interactive terminal gets started as shown in Figure  5-31 . \n Set the user as  postgres . \n su –l postgres \n Start the  psql command line shell. \n psql postgres \n The  psql shell is shown in Figure  5-32 . \n Run the following SQL statements to create a database table called  wlslog and add data to the table. \n CREATE TABLE wlslog(time_stamp VARCHAR(255) PRIMARY KEY,category VARCHAR(255),type \nVARCHAR(255),servername VARCHAR(255),code VARCHAR(255),msg VARCHAR(255)); \n INSERT INTO wlslog(time_stamp,category,type,servername,code,msg) VALUES('Apr-8-2014-7:\n06:16-PM-PDT','Notice','WebLogicServer','AdminServer','BEA-000365','Server state changed \nto STANDBY'); \n INSERT INTO wlslog(time_stamp,category,type,servername,code,msg) VALUES('Apr-8-2014-7:\n06:17-PM-PDT','Notice','WebLogicServer','AdminServer','BEA-000365','Server state changed \nto STARTING'); \n INSERT INTO wlslog(time_stamp,category,type,servername,code,msg) VALUES('Apr-8-2014-7:\n06:18-PM-PDT','Notice','WebLogicServer','AdminServer','BEA-000360','Server started in \nRUNNING mode'); \n Figure 5-32.  Starting the psql Shell \n Figure 5-31.  Starting the TTY \n \n \n",
      "content_length": 1275,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n136\n The database table  wlslog gets created as shown in Figure  5-33 . \n Run the following SQL statement to query the  wlslog table. \n SELECT * FROM wlslog; \n The three rows of data added get listed as shown in Figure  5-34 . \n To quit the  psql shell and the interactive shell for the Docker container running PostgreSQL, run the \nfollowing commands. \n \\q \n exit \n The psql shell and the tty get exited as shown in Figure  5-35 . \n Figure 5-33.  Creating a Database Table \n Figure 5-34.  Querying the wlslog Database Table \n \n \n",
      "content_length": 568,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n137\n Scaling the PostgreSQL Cluster \n When we created the cluster initially we set the replicas to 2. List the pods as follows. \n kubectl get pods \n Only two Pods get listed as shown in Figure  5-36 . \n Scale the cluster to 4 replicas with the following command. \n kubectl scale rc postgres --replicas=4 \n An output of “scaled” as shown in Figure  5-37 indicates that the cluster has been scaled. \n Subsequently list the Pods. \n kubectl get pods \n The preceding command may have to be run multiple times to list all the Pods as “Running” and in \nREADY state 1/1 as shown in Figure  5-38 . \n Figure 5-36.  Listing the Pods \n Figure 5-37.  Scaling the Pod Replicas to 4 \n Figure 5-35.  Exiting the Shells \n \n \n \n",
      "content_length": 748,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n138\n Deleting the Replication Controller \n To  delete the replication controller run the following command. \n kubectl delete rc postgres \n List the Pods subsequent to deleting the rc. \n kubectl get pods \n List the services. \n kubectl get services \n The  postgres replication controller gets deleted and does not get listed subsequently as shown in \nFigure  5-39 . The  postgres service still gets listed also shown in Figure  5-39 . \n Figure 5-38.  Listing the Pods in various states of starting \n \n",
      "content_length": 537,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "CHAPTER 5 ■ USING POSTGRESQL DATABASE\n139\n Stopping the Service \n To  stop the service run the following command. \n kubectl stop service postgres \n The  postgres service gets stopped as shown in Figure  5-40 . Subsequently run the following command. \n kubectl get services \n The  postgres service does not get listed as shown in Figure  5-40 also. \n Summary \n In this chapter we used the Kubernetes cluster manager to start and manage a PostgreSQL server cluster. \nWe demonstrated creating a cluster both imperatively on the command line and declaratively using \ndefinition files. We scaled the cluster using a replication controller and exposed a service for the cluster \nusing a Kubernetes service. In the next chapter we shall discuss creating and managing an Oracle \nDatabase cluster. \n Figure 5-40.  Stopping the Service \n Figure 5-39.  Deleting the Replication Controller \n \n \n",
      "content_length": 883,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "141\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_6\n CHAPTER 6 \n Using Oracle Database \n Oracle Database is the most commonly used relational database (RDBMS). Installing and configuring \nOracle Database would usually involve downloading the software, setting the kernel parameters, installing \nand configuring the software, all of which is quite involved. Using Docker containers coordinated with \nKubernetes makes the task of installing, configuring, and orchestrating a Oracle Database cluster much \neasier. Oracle Database cluster consisting of multiple instances could benefit from the schedulability, \nscalability, distributedness, and failover features of the Kubernetes container cluster manager. In this \nchapter we shall install Oracle Database using a Docker image for the database. We shall create multiple \nreplicas of the database Pod using a replication controller and expose the database as a service. This chapter \nhas the following sections.\n Setting the Environment \n Creating an Oracle Database Instance Imperatively \n Creating an Oracle Database Instance Declaratively \n Keeping the Replication Level \n Scaling the Database \n Starting the Interactive Shell \n Connecting to Database \n Creating a User \n Creating a Database Table \n Exiting the Interactive Shell \n Setting the Environment \n The following software is required for this chapter. \n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n -Docker Image for Oracle Database (Oracle Database XE 11g) \n",
      "content_length": 1566,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n142\n If not already installed, install Docker Engine, Kubernetes, and Kubectl as discussed in Chapter  1 . \nSSH login to the Ubuntu instance on Amazon EC2 using the Public IP Address of the EC2 instance. \n ssh -i \"docker.pem\" ubuntu@52.90.115.30 \n Start the Docker instance and verify its status with the following commands. \n sudo service docker start \n sudo service docker status \n Docker is indicated as running in Figure  6-1 . \n List the services running. \n kubectl get services \n Only the  kubernetes service is listed as running in Figure  6-2 . \n Creating an Oracle Database Instance Imperatively \n In this section we shall create an Oracle Database cluster using  kubectl on the command line. Several \nDocker images are available for Oracle Database and we shall be using the  sath89/oracle-xe-11g image \n( https://hub.docker.com/r/sath89/oracle-xe-11g/ ). Run the following  kubectl command to create an \nOracle Database cluster consisting of 2 replicas with port set as 1521.  \n kubectl run oradb --image=sath89/oracle-xe-11g --replicas=2 --port=1521 \n The output from the command in Figure  6-3 lists a replication controller called  oradb , a Docker \ncontainer called  oradb , a selector ( run=oradb ) to select Pods that comprise the replication controller \nreplicas, and the number of replicas (2). The Pod label is also set to  run=oradb . \n Figure 6-1.  Starting Docker and verifying its Status \n Figure 6-2.  Listing the Kubernetes Service \n \n \n",
      "content_length": 1497,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n143\n List the replication  controller with the following command. \n kubectl get rc \n The  oradb replication controller shown in Figure  6-4 gets listed. \n Figure 6-3.  Creating a Replication Controller and Pod Replicas for Oracle Database \n Figure 6-4.  Listing the Replication Controllers \n List the Pods using the following command. \n kubectl get pods \n In addition to the Kubernetes Pod  k8s-master-127.0.0.1 two other pods get listed for Oracle Database \nas shown in Figure  6-5 . Initially the Pods could be listed as “not ready” as shown in Figure  6-5 also. Run the \npreceding command after a duration of a few seconds, multiple times if required, to list the two Pods are \nRunning and READY (1/1). \n Figure 6-5.  Listing the Pods in various stages of running \n \n \n \n",
      "content_length": 808,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n144\n Get the nodes with the following command. \n kubectl get nodes \n And get the Kubernetes services with the following command. \n kubectl get services \n Only the  kubernetes service gets listed as shown in Figure  6-6 because we have not yet created a service \nfor Oracle Database. \n Listing Logs \n List the  logs for one of the Pods using the following command. \n kubectl logs oradb-ea57r \n The logs generated by a started Oracle Database instance get output as shown in Figure  6-7 . Oracle Net \nListener is indicated as having been started. \n Figure 6-6.  Creating a Replication Controller does not create a Service \n \n",
      "content_length": 657,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n145\n Creating a Service \n Next, expose the replication controller  oradb as a Kubernetes service on port 1521. Subsequently list the \nKubernetes  services . \n kubectl expose rc oradb --port=1521 --type=LoadBalancer \n kubectl get services \n The first of the two preceding commands starts the  oradb service. Subsequently the service gets listed \nas shown in Figure  6-8 . The service selector is run=oradb, which is the same as the replication controller \nselector. \n Figure 6-7.  Listing Logs for a Pod \n \n",
      "content_length": 540,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n146\n Describe the service with the following command. \n kubectl describe svc oradb \n The service name, namespace, labels, selector, type, IP, Port,NodePort, and endpoints get listed as \nshown in Figure  6-9 . \n Scaling the Database \n Run the  kubectl scale command to scale the replicas. For example, reduce the number of replicas to 1. \n kubectl scale rc oradb --replicas=1 \n Figure 6-8.  Creating a Service Imperatively \n Figure 6-9.  Describing the oradb Service \n \n \n",
      "content_length": 505,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n147\n An output of “scaled” indicates that the replicas have been scaled as shown in Figure  6-10 . \n Subsequently list the running Pods. \n kubectl get pods \n Only one Oracle Database Pod gets listed as the other has been stopped to reduce the replication level \nto one as shown in Figure  6-11 . Subsequently, describe the service. \n kubectl describe svc oradb \n Because the cluster has been scaled down to one replica the number of endpoints also gets reduced to \none as shown in Figure  6-11 . \n Deleting the Replication Controller and Service \n In subsequent sections we shall be creating a cluster of Oracle Database instances declaratively using \ndefinition files. As we shall be using the same configuration parameters,  delete the  \"oradb\" replication \ncontroller and the  \"oradb\" service with the following commands. \n kubectl delete rc oradb \n kubectl delete svc oradb \n Figure 6-10.  Scaling the Replicas to 1 \n Figure 6-11.  Listing and Describing the Single Pod \n \n \n",
      "content_length": 1013,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n148\n Both the replication controller and the service get deleted as shown in Figure  6-12 . \n Creating an Oracle Database Instance Declaratively \n In this section we shall create Oracle Database cluster  declaratively using definition files for a Pod, \nreplication controller, and service. We have used the YAML format in the definition files but the JSON format \nmay be used instead. \n Creating a Pod \n Create a definition file for a  Pod called  oradb.yaml . Copy the following listing, which defines a Pod named \n“oradb” with a label setting  name: \"oradb\" , which translates to Pod label name=oradb. The container image \nis set as “sath89/oracle-xe-11g” and the container port is set as 1521. \n apiVersion: v1 \n kind: Pod \n metadata: \n  name: \"oradb\" \n  labels: \n    name: \"oradb\" \n spec: \n  containers: \n    - \n      image: \"sath89/oracle-xe-11g\" \n      name: \"oradb\" \n      ports: \n        - \n          containerPort: 1521 \n  restartPolicy: Always \n The  oradb.yaml file may be created in the vi editor and saved with the :wq command as shown in \nFigure  6-13 . \n Figure 6-12.  Deleting the Replication Controller and Service \n \n",
      "content_length": 1169,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n149\n Create a Pod using the definition file  oradb.yaml with the  kubectl create command. \n kubectl create -f oradb.yaml --validate \n An output of “pods/oradb” in Figure  6-14 indicates that the  oradb Pod has been created. \n Figure 6-13.  Pod Definition File \n Figure 6-14.  Creating a Pod from a Definition File \n \n \n",
      "content_length": 353,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n150\n Subsequently list the running Pods with the following command. \n kubectl get pods \n The single Pod  oradb gets listed as shown in Figure  6-15 . Initially, the  oradb Pod could be listed in \nvarious phases of starting such as Image “is ready, container is creating” or the READY value could be 0/1 \nindicating that the Pod is not ready yet. When the STATUS column becomes “Running” and the READY \ncolumn becomes 1/1 the Pod is started and ready. The preceding command may have to be run multiple \ntimes to list the Pod as Running and Ready. \n Creating a Service \n Next, create a service for an Oracle Database cluster. The service does not specify how many instances \n(replicas) of the Oracle Database image are running or should be running. The replicas are controlled by \nthe replication controller. The service only defines a port to expose the service at, a label for the service \nand a selector to match the Pods to be managed by the service. The selector setting is app: “oradb”, which \ntranslates to service selector app=oradb. Create a service definition file  oradb-service.yaml and copy the \nfollowing listing to the definition file. \n apiVersion: v1 \n kind: Service \n metadata: \n  name: \"oradb\" \n  labels: \n    app: \"oradb\" \n Figure 6-15.  Listing the Pod/s, which could initially be not Running and not Ready \n \n",
      "content_length": 1363,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n151\n spec: \n  ports: \n    - \n      port: 1521 \n  selector: \n    app: \"oradb\" \n The  oradb-service.yaml definition file may be created in the vi editor and saved with :wq as shown in \nFigure  6-16 . \n Figure 6-16.  Service Definition File \n \n",
      "content_length": 275,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n152\n Run the following command to create a service from the service definition file. \n kubectl create -f oradb-service.yaml \n The  oradb service gets created as indicated by the “services/oradb” output in Figure  6-17 . Subsequently \nlist the services. \n kubectl get services \n The  oradb service gets listed as shown in Figure  6-17 . \n Describe the  oradb service with the following command. \n kubectl describe svc oradb \n No service endpoint gets listed as shown in Figure  6-18 because the service selector does not match \nthe label on the Pod already running. The service selector app=oradb has to match a Pod label for the \nservice to be able to manage the Pod. In the next section we shall create a replication controller with a \nmatching label. \n Figure 6-17.  Creating a Service from a Service Definition File \n Figure 6-18.  Describing a Service for Oracle Database \n \n \n",
      "content_length": 915,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n153\n Creating a Replication Controller \n Create a replication controller definition file called  oradb-rc.yaml and copy the following listing, \nwhich defines a replication controller, to the definition file. For the replication controller to manage the \nPods defined in the spec field the key:value expression of the selector in the replication controller has \nto match a label in the Pod template mapping. The  selector is omitted in the  oradb-rc.yaml but the \nspec- > template- > metadata- > labels must be specified. The selector defaults to the same setting as the \n spec->template->metadata->labels . The template- > spec- > containers mapping defines the containers in \nthe Pod. Only the Oracle Database container “sath89/oracle-xe-11g” is defined. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  name: \"oradb\" \n  labels: \n    app: \"oradb\" \n spec: \n  replicas: 2 \n  template: \n    metadata: \n      labels: \n        app: \"oradb\" \n    spec: \n      containers: \n        - \n          image: \"sath89/oracle-xe-11g\" \n          name: \"oradb\" \n",
      "content_length": 1091,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n154\n The  oradb-rc.yaml file may be edited in the vi editor and saved with the :wq command as shown in \nFigure  6-19 . \n Next, run the following command to create a replication controller from the definition file  oradb-rc.yaml . \n kubectl create -f oradb-rc.yaml \n The replication controller gets created as shown in Figure  6-20 . List the replication controller with the \nfollowing command. \n kubectl get rc \n Figure 6-19.  Replication Controller Definition File \n \n",
      "content_length": 503,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n155\n The  oradb replication controller gets created as shown in Figure  6-20 . \n The Pods created by the replication controller are listed with the following command. \n kubectl get pods \n Three Oracle Database Pods get listed as shown in Figure  6-21 . Why do three Pods get listed even \nthough the replication controller replicas are set to 2? Because the Pod started using the Pod definition file \n oradb.yaml does not include a label that matches the selector in the replication controller. The replication \ncontroller selector is  app: \"oradb\" while the label on the Pod is  name: \"oradb\" . Two replicas are started by \nthe replication controller and one Pod was started earlier by the pod definition file. \n Describe the service  oradb with the following command. \n kubectl describe svc oradb \n The service endpoints get listed as shown in Figure  6-22 . Only two endpoints get listed because the \nservice selector app: “oradb” matches the Pod label in the replication controller with two replicas. The Pod \ncreated earlier does not include a label that matches the selector expression. \n Figure 6-20.  Creating and listing a Replication Controller from a Definition File \n Figure 6-21.  Listing the Pod Replicas \n \n \n",
      "content_length": 1257,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n156\n Keeping the Replication Level \n The task of the replication controller is to maintain the replication level of the Pods. Because the  replicas \nfield mapping in the replication controller spec is 2, two replicas of the Pod configured in the Pod spec must \nbe running at all time while the replication controller is running. To demonstrate that the replication level is \nkept, delete a Pod. \n kubectl delete pod oradb-5ntnj \n Subsequently list the running Pods. \n kubectl get pods \n One of the two replicas got deleted with the  kubectl delete pod command but another replica is \nlisted as getting started in Figure  6-23 . It may take a few seconds for the replicas to reach the replication \nlevel. Run the preceding command multiple times to list the replicas as running. The number of replicas \ngets back to 2. \n Figure 6-22.  Describing the Service after creating the Replication Controller \n \n",
      "content_length": 936,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n157\n The  \"oradb\" Pod is not associated with the replication controller and therefore it is not counted as \none of the replicas managed by the replication controller. The  oradb Pod is not managed by the replication \ncontroller because, as discussed earlier, the label on the  oradb Pod does not match the label on the \nreplication controller. To demonstrate that the  oradb pod is not managed by the replication controller delete \nthe Pod. \n kubectl delete pod oradb \n Subsequently list the running Pods. \n kubectl get pods \n The  oradb Pod gets deleted and a replacement Pod does not get started and does not get listed in the \nrunning Pods as shown in Figure  6-24 . \n Figure 6-23.  Maintaining the Replication Level \n \n",
      "content_length": 757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n158\n Scaling the Database \n The replication controller may be used to scale the number of Pods running for Oracle Database. As an \nexample scale up the number of Pod replicas to 3 from 2. \n kubectl scale rc oradb --replicas=3 \n The “scaled” output indicates that the replicas have been scaled. Subsequently run the following \ncommand, multiple times if required, to list the new Pod replica as running and ready. \n kubectl get pods \n Three replicas of the Pod get listed as shown in Figure  6-25 . \n Figure 6-25.  Scaling the Cluster to 3 Replicas \n Figure 6-24.  The oradb Pod is not managed by the Replication Controller \n \n \n",
      "content_length": 662,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n159\n Describe the service again. \n kubectl describe svc oradb \n Three endpoints get listed instead of two as shown in Figure  6-26 . The service has a single IP address. \n Starting the Interactive Shell \n In this section we shall start an interactive tty (shell) to connect to the software, which is Oracle Database, \nrunning in a Docker container started with and managed by Kubernetes. First, list the Docker containers \nwith the following command. \n sudo docker ps \n Copy the container id for one of the Docker containers for the  sath89/oracle-xe-11g image as shown \nin Figure  6-27 . \n Figure 6-26.  Listing the 3 Endpoints in the Service \n \n",
      "content_length": 681,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n160\n Figure 6-27.  Copying the Container Id for a Docker Container \n Using the container id start an interactive shell with the following command. \n sudo docker exec -it 9f74a82d4ea0 bash \n The interactive shell gets started as shown in Figure  6-28 . \n Figure 6-28.  Starting an Interactive Shell \n Connecting to Database \n In the interactive tty change the user to “oracle.” \n su -l oracle \n The difference between  su oracle and  su - oracle is that the latter logs in with the environment \nvariables of  oracle user and also sets the current directory to oracle home directory while the former logs in \nas  oracle but the environment variables and current directory remain unchanged. \n \n \n",
      "content_length": 727,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n161\n Subsequently start the SQL*Plus. Using the  /nolog option does not establish an initial connection with \nthe database. \n sqlplus /nolog \n Run the following command to connect  SYS as  SYSDBA . \n CONNECT SYS AS SYSDBA \n Specify the Password as “oracle” when prompted. The output from the preceding commands to start \nSQL*Plus and connect  SYS are shown in Figure  6-29 . A connection gets established. \n Creating a User \n To create a user called OE and grant  CONNECT and  RESOURCE roles to the user, run the following commands. \n CREATE USER OE QUOTA UNLIMITED ON SYSTEM IDENTIFIED BY OE; \n GRANT CONNECT, RESOURCE TO OE; \n The  OE user gets created and the roles get granted as shown in Figure  6-30 . \n Figure 6-29.  Starting SQL*Plus \n Figure 6-30.  Connecting as SYSDBA and creating a User \n \n \n",
      "content_length": 838,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n162\n Creating a Database Table \n Create a database table called  OE.Catalog with the following SQL statement. \n CREATE TABLE OE.Catalog(CatalogId INTEGER PRIMARY KEY,Journal VARCHAR2(25),Publisher \nVARCHAR2(25),Edition VARCHAR2(25),Title VARCHAR2(45),Author VARCHAR2(25)); \n Add a row of data to the OE.Catalog table with the following SQL statement. \n INSERT INTO OE.Catalog VALUES('1','Oracle Magazine','Oracle Publishing',\n'November December 2013','Engineering as a Service','David A. Kelly'); \n The  OE.Catalog table gets created and a row of data gets added as shown in Figure  6-31 . \n Run the following SQL statement to query the  OE.CATALOG table. \n SELECT * FROM OE.CATALOG; \n The single row of data added gets listed as shown in Figure  6-32 . \n Figure 6-31.  Creating a Database Table \n \n",
      "content_length": 833,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "CHAPTER 6 ■ USING ORACLE DATABASE\n163\n Figure 6-32.  Querying the Database Table \n Exiting the Interactive Shell \n Logout from SQL*Plus command with the “exit” command and exit the “oracle” user with the “exit” \ncommand and exit the interactive terminal with the “exit” command also as shown in Figure  6-33 . \n Summary \n In this chapter we used Kubernetes to create and orchestrate an Oracle Database cluster. We discussed both \nthe imperative and declarative approaches to creating and managing a cluster. Using the imperative method, \nthe  kubectl commands may be used directly without a definition file to create a replication controller and a \nservice. With the declarative method definition files for a Pod, replication controller and service have to be \nused. We demonstrated scaling a cluster. We also used a Docker container to log in to SQL*Plus and create a \ndatabase table. In the next chapter we shall discuss using MongoDB with Kubernetes. \n Figure 6-33.  Exiting the Interactive Shell \n \n \n",
      "content_length": 1005,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "   PART III \n NoSQL Database   \n",
      "content_length": 32,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "167\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_7\n CHAPTER 7 \n Using MongoDB Database \n MongoDB is a flexible schema model NoSQL data store, the most commonly used NoSQL data store. \nMongoDB is based on the BSON (binary JSON) storage model. Documents are stored in collections. Being a \nschema-free data store, no two documents need to be alike in terms of the fields in a BSON document. In a \nlarge scale cluster several instances of MongoDB could be running and several issues could arise.\n -MongoDB instances scheduling \n -Scaling the MongoDB Cluster \n -Load Balancing \n -Providing MongoDB as a Service \n While Docker has made it feasible to provide Container as a Service (CaaS) it does not provide by itself \nany of the features listed previously. In this chapter we discuss using Kubernetes container cluster manager \nto manage and orchestrate a cluster of Docker containers running MongoDB. This chapter has the following \nsections.\n Setting the Environment \n Creating a MongoDB Cluster Declaratively \n Creating a MongoDB Cluster Imperatively \n Setting the Environment \n The following software is required for this chapter. \n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n -Docker image for MongoDB (latest version) \n Install the required software on an Amazon EC2 instance running Ubuntu 14; the same AMI is used \nas in the other chapters. SSH Login to the Ubuntu instance using the Public IP Address, which would be \ndifferent for different users. \n ssh -i \"docker.pem\" ubuntu@52.91.190.195 \n",
      "content_length": 1598,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n168\n The  Ubuntu instance gets logged into as shown in Figure  7-1 . \n The procedure to install is discussed in chapter  1 . To verify that Docker is running run the following \ncommand. \n sudo service docker start \n Docker should be listed as running as shown in Figure  7-2 . \n Figure 7-1.  Logging into Ubuntu Instance on Amazon EC2 \n Figure 7-2.  Starting Docker \n List the Pods with the following command. \n kubectl get pods \n \n \n",
      "content_length": 469,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n169\n And list the nodes with the following command. \n kubectl get nodes \n The  Kubernetes Pod gets listed and the node also gets listed as shown in Figure  7-3 . \n To list the services run the following command. \n kubectl get services \n The “kubernetes” service gets listed as shown in Figure  7-4 . \n Creating a MongoDB Cluster Declaratively \n In the following subsections we shall create a Kubernetes service and replication controller for a MongoDB \ncluster. We shall scale the cluster and also demonstrate features such as using a volume and a host port. \nWe shall create a MongoDB collection and add documents to the collection in a Mongo shell running in a \nDocker container tty (interactive terminal or shell). \n Creating a Service \n Create a service definition file  mongo-service-yaml . Add the following (Table  7-1 ) field mappings in the \ndefinition file. \n Figure 7-3.  Listing Kubernetes Pod and the single Node \n Figure 7-4.  Listing the Kubernetes Service \n \n \n",
      "content_length": 1012,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n170\n Copy the following listing to the  mongo-service.yaml . \n apiVersion: v1 \n kind: Service \n metadata: \n  labels: \n    name: mongo \n  name: mongo \n spec: \n  ports: \n    - port: 27017 \n      targetPort: 27017 \n  selector: \n    name: mongo \n The vi editor could be used to create the  mongo-service.yaml file and saved using the :wq command as \nshown in Figure  7-5 . \n Table 7-1.  Service Definition File mongo-service-yaml File Fields \n Field \n Value \n Description \n apiVersion \n v1 \n The API version. \n kind \n Service \n Specifies the definition file to be a service. \n metadata \n The service metadata. \n metadata- > labels \n name: mongo \n metadata- > name \n mongo \n A label mapping. A label may be added multiple times and \ndoes not generate an error and has no additional significance. \n spec \n The service specification. \n spec- > ports \n The port/s on which the service is exposed. \n spec- > ports- > port \n 27017 \n The port on which the service is hosted. \n spec- > ports- > targetPort  27017 \n The port that an incoming port is mapped to. The targetPort \nfield is optional and defaults to the same value as the port \nfield. The targetPort could be useful if the service is to evolve \nwithout breaking clients’ settings. For example, the targetPort \ncould be set to a string port name of a back-end Pod, which \nstays fixed. And the actual port number the back-end Pod \nexposes could be varied without affecting the clients’ settings. \n selector \n name: mongo \n The service selector used to select Pods. Pods with label \nexpression the same as the selector are managed by the \nservice. \n",
      "content_length": 1629,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n171\n The default service type is  ClusterIp , which uses a cluster-internal IP only. The type could be set to \n LoadBalancer as shown in Figure  7-6 to also expose the service on each of the nodes in the cluster and also \nrequests the cloud provider to provision a load balancer. \n Figure 7-5.  Service Definition File in vi Editor \n \n",
      "content_length": 370,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n172\n To create the service from the definition file run the following command. \n kubectl create -f mongo-service.yaml \n List the services with the following command. \n kubectl get services \n The  mongo service gets listed as shown in Figure  7-7 . \n Figure 7-6.  Setting the Service Type \n \n",
      "content_length": 326,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n173\n Creating a Replication Controller \n In this section we shall create a replication  controller . Create a definition file  mongo-rc.yaml . Add the \nfollowing (Table  7-2 ) field mappings to the definition file. \n Figure 7-7.  Creating the Service from the Definition File \n Table 7-2.  Replication Controller Definition File Fields \n Field \n Value \n Description \n apiVersion \n v1 \n The API version. \n kind \n ReplicationController \n Specifies the definition file to be \nfor a replication controller. \n metadata \n Specifies the metadata for the \nreplication controller. \n metadata - > labels \n name: mongo \n The labels mapping for the \nreplication controller. \n metadata - > name \n mongo-rc \n The replication controller name. \n spec \n The replication controller \nspecification. \n spec- > replicas \n 2 \n The number of replicas to keep \nat all times. \n spec- > template \n The template for a Pod. \n spec- > template- > metadata \n The metadata for the Pod. \n spec- > template- > metadata- > labels \n The Pod labels. The labels \nare used by the replication \ncontroller and service to select \nPods to manage. The selector \nin a replication controller and a \nservice must match a Pod label \nfor the replication controller and \nService to managed the Pod. \n spec- > template- > metadata- > labels- > name \n mongo \n A Pod label. \n spec- > template- > spec \n The specification for the Pod. \n(continued)\n \n",
      "content_length": 1432,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n174\n Each of the Pod, Service, and Replication Controllers are defined in a separate YAML mapping file. \nThe  mongo-rc.yaml is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  labels: \n    name: mongo \n  name: mongo-rc \n spec: \n  replicas: 2 \n  template: \n    metadata: \n      labels: \n        name: mongo \n    spec: \n      containers: \n        - \n          image: mongo \n          name: mongo \n          ports: \n            - \n              containerPort: 27017 \n              name: mongo \nTable 7-2. (continued)\n Field \n Value \n Description \n spec- > template- > spec- > containers \n The containers in a Pod. \nMultiple containers could \nbe specified but we have \nconfigured only one container. \n spec- > template- > spec- > containers - > image \n mongo \n The container for “mongo” \nDocker image. \n spec- > template- > spec- > containers - > name \n mongo \n The container name. \n spec- > template- > spec- > containers - > ports \n The container ports to reserve. \n spec- > template- > spec- > containers \n- > ports- > name \n mongo \n The port name. \n spec- > template- > spec- > containers - > ports \n- > containerPort \n 27017 \n The container port number. \n",
      "content_length": 1212,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n175\n The  mongo-rc.yaml file may be edited in a vi editor and saved with :wq as shown in Figure  7-8 . \n To create a replication controller from the definition file, run the following command. \n kubectl create -f mongo-rc.yaml \n The  mongo-rc replication controller gets created as shown in Figure  7-9 . \n Run the following command to list the replication containers. \n kubectl get rc \n Figure 7-8.  Replication Controller Definition File \n Figure 7-9.  Creating the Replication Controller \n \n \n",
      "content_length": 531,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n176\n The  mongo-rc replication controller gets listed as shown in Figure  7-10 . \n Creating a Volume \n Kubernetes supports  volumes . A  volume is a directory in a Pod that is accessible to containers in the Pod that \nprovide a volume mount for the volume. Volumes persist as long as the Pod containing the volumes exists. \nVolumes are useful for the following purposes. \n -Persist data across container crash. When a container that mounts a volume \ncrashes, the data in the volume is not deleted as the volume is not on the \ncontainer but is on the Pod. \n -Data in a volume may be shared by multiple containers that mount the volume. \n A volume in a Pod is specified with the spec- > volume field. A container mounts a volume with the \n spec.containers.volumeMounts field. Several types of volumes are supported, some of which are discussed \nin Table  7-3 . \n Next, we shall add a volume of type  emptyDir to the replication controller definition file  mongo-rc.yaml . \nA modified version of  mongo-rc.yaml is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  labels: \n    name: mongo \n  name: mongo-rc \n Figure 7-10.  Creating and isting Replication Controllers \n Table 7-3.  Types of  Volumes \n Volume Type \n Description \n emptyDir \n An empty directory in the Pod that could be used to keep some files used \nby one or more containers. An empty directory could also be used for \ncheckpointing. \n hostPath \n Mounts a directory from the host node into the Pod. \n gcePersistentDisk \n Mounts a Google Compute Engine Persistent disk into a Pod. \n awsElasticBlockStore \n Mounts an Amazon Web Services EBS volume into a Pod. \n gitRepo \n Mounts a git repo into the pod. \n flocker \n Mounts a Flocker dataset into a pod. \n nfs \n Mounts a Network File System into a Pod. \n \n",
      "content_length": 1820,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n177\n spec: \n  replicas: 2 \n  template: \n    metadata: \n      labels: \n        name: mongo \n    spec: \n      containers: \n        - \n          image: mongo \n          name: mongo \n          ports: \n            - \n              containerPort: 27017 \n              name: mongo \n          volumeMounts: \n            - \n              mountPath: /mongo/data/db \n              name: mongo-storage \n      volumes: \n        - \n          emptyDir: {} \n          name: mongo-storage \n The preceding definition file includes the following volume  configuration for a volume named \n mongo-storage of type  emptyDir . \n      volumes: \n        - \n          emptyDir: {} \n          name: mongo-storage \n The volume exists in the Pod and individual containers in the Pod may mount the volume using field \n spec->containers->volumeMounts . The modified  mongo-rc.yaml includes the following volume mount for \nthe  mongo container. \n volumeMounts: \n            - \n              mountPath: /mongo/data/db \n              name: mongo-storage \n The preceding configuration adds a volume mount for the  mongo-storage volume at mount path \nor directory path  /mongo/data/db in the container. Within a container the volume may be accessed at \n /mongo/data/db . For example, in an interactive terminal for a container change directory (cd) to the \n /mongo/data/db directory. \n cd /mongo/data/db \n List the files and directories in the in the  /mongo/data/db directory. \n ls -l \n",
      "content_length": 1486,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n178\n The directory is  empty as it is supposed to be initially as shown in Figure  7-11 . \n The volume should not be confused with the data directory for the MongoDB server. The data directory \nis created at  /data/db by default and is created in each Docker container running a MongoDB server \ninstance. The  /mongo/data/db is common to all Docker containers while the  /data/db exists in each \nDocker container. \n Listing the Logs \n After having started a replication controller, list the Pods with the following command. \n kubectl get pods \n The two Pods get listed as shown in Figure  7-12 . \n The logs for a Pod, for example, the  mongo-rc-4t43s Pod, may be listed with the following command. \n kubectl logs mongo-rc-4t43s \n Figure 7-11.  Empty Directory \n Figure 7-12.  Listing the Pods \n \n \n",
      "content_length": 833,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n179\n The Pod logs show the MongoDB server starting as shown in Figure  7-13 . \n Figure 7-13.  Listing the Pod Logs \n \n",
      "content_length": 153,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n180\n Starting the Interactive Shell for Docker Container \n In this section we shall start an  interactive terminal or bash shell for MongoDB server for which we need the \ncontainer id of a Docker container running a MongoDB server. List the Docker containers. \n sudo docker ps \n Figure 7-14.  MongoDB Running on Port 27017 \n When the MongoDB server gets started, the message “waiting for connections on port 27017” gets \noutput as shown in Figure  7-14 . \n \n",
      "content_length": 493,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n181\n Because the “mongo” Docker image is based on the “debian” Docker image as speciifed in the FROM \ninstruction, we are able to start a bash shell to interact with the MongoDB server running in a Docker \ncontainer based on the “mongo” image. Start an interactive bash shell using the following command. \n sudo docker exec -it 00c829e0a89d bash \n An interactive shell gets started as shown in Figure  7-16 . \n Figure 7-15.  Copying Docker Container ID \n Figure 7-16.  Starting an Interactive Shell \n Copy the container id for a container with image as “mongo” as shown in Figure  7-15 . \n \n \n",
      "content_length": 628,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n182\n Starting a Mongo Shell \n Start the  Mongo shell with the following command. \n mongo \n Mongo shell gets started as shown in Figure  7-17 . \n Creating a Database \n List the  databases with the following command from the Mongo shell. \n show dbs \n A database gets created implicitly when a database is used or set. For example, set the database to use \nas  mongodb , which is not listed with  show dbs and does not exist yet. \n use mongodb \n But, setting the database to use as  mongodb does not create the database  mongodb till the database is \nused. Run the following command to list the databases. \n show dbs \n Figure 7-17.  Mongo Shell \n \n",
      "content_length": 680,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n183\n The  mongodb database does not get listed as shown in Figure  7-19 . To create the  mongodb database, \ninvoke some operation on the database such as create a collection called  catalog with the following \ncommand. \n db.createCollection(\"catalog\") \n Subsequently list the databases again. \n show dbs \n The  mongodb database gets listed as shown in Figure  7-18 . To list the collections run the following \ncommand. \n show collections \n The  catalog collection gets listed. \n Creating a Collection \n The  catalog collection was created using the  db.createCollection method in the previous section. As \nanother example, create a capped collection called  catalog_capped using the following command: a capped \ncollection is a fixed size collection that supports high throughput operations to add and get documents \nbased on insertion order. \n db.createCollection(\"catalog_capped\", {capped: true, autoIndexId: true, size: 64 * 1024, \nmax: 1000} ) \n A  capped collection gets added as shown in Figure  7-19 . Initially the collection is empty. Get the \ndocuments in the  catalog collection with the following command. \n db.catalog.count() \n The document count is listed as 0 as we have not yet added any documents. \n Figure 7-18.  Creating and Listing a MongoDB Database \n \n",
      "content_length": 1309,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n184\n Adding Documents \n In this section we shall  add documents to the catalog collection. Specify the JSON for the documents to be \nadded. The  _id field is required in each document stored in MongoDB. The  _id field may be added explicitly \nas in the  doc2 document. If not provided in the document JSON the  _id is generated automatically. \n doc1 = {\"catalogId\" : \"catalog1\", \"journal\" : 'Oracle Magazine', \"publisher\" : \n'Oracle Publishing', \"edition\" : 'November December 2013',\"title\" : 'Engineering as a \nService',\"author\" : 'David A. Kelly'} \n doc2 = {\"_id\": ObjectId(\"507f191e810c19729de860ea\"), \"catalogId\" : \"catalog1\", \"journal\" \n: 'Oracle Magazine', \"publisher\" : 'Oracle Publishing', \"edition\" : 'November December \n2013',\"title\" : 'Engineering as a Service',\"author\" : 'David A. Kelly'}; \n The doc1 and doc2 are shown in Figure  7-20 . \n Figure 7-19.  Creating a Capped Collection \n \n",
      "content_length": 934,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n185\n To add the documents to the  catalog collection run the following command. \n db.catalog.insert([doc1, doc2], { writeConcern: { w: \"majority\", wtimeout: 5000 }, \nordered:true }) \n As indicated by the  nInserted field in the JSON result in Figure  7-21 documents get added. \n Figure 7-20.  Documents doc1 and doc2 \n \n",
      "content_length": 355,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n186\n Finding Documents \n To query the catalog invoke the  find()  method . To list all documents in the  catalog collection run the \nfollowing command. \n db.catalog.find() \n The two documents added get listed as shown in Figure  7-22 . For one of the documents the  _id field is \ngenerated automatically. \n Finding a Single Document \n To find a single document from the  catalog collection run the following command to invoke the  findOne() \nmethod. \n db.catalog.findOne() \n Figure 7-21.  Adding Documents \n Figure 7-22.  Finding Documents \n \n \n",
      "content_length": 580,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n187\n A single document gets listed as shown in Figure  7-23 . \n Figure 7-23.  Finding a Single Document \n Figure 7-24.  Finding Selected Fields in a Document \n Finding Specific Fields in a Single Document \n To get only specific fields,  edition, title , and  author , for example, from a single document run the \nfollowing command. \n db.catalog.findOne(\n    { }, \n { edition: 1, title: 1, author: 1 } \n ) \n Only the specific fields in a single document get listed as shown in Figure  7-24 . The  _id field always \ngets listed. \n \n \n",
      "content_length": 567,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n188\n Dropping a Collection \n To  drop the  catalog collection run the following command. \n db.catalog.drop() \n Subsequently list the collections with the following command. \n show collections \n The  catalog collection does not get listed and only the  catalog_capped collection gets listed as shown \nin Figure  7-25 . \n Exiting Mongo Shell and Interactive Shell \n To  exit the Mongo shell run the following command. \n exit \n To exit the interactive terminal run the following command. \n exit \n The Mongo shell and the interactive terminal get exited as shown in Figure  7-26 . \n Scaling the Cluster \n To scale the Mongo cluster run the  kubectl scale command. For example, the following command scales \nthe cluster to 4 replicas. \n kubectl scale rc mongo --replicas=4 \n Figure 7-25.  Dropping the catalog Collection \n Figure 7-26.  Exiting the Shells \n \n \n",
      "content_length": 891,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n189\n An output of “scaled” as shown in Figure  7-27 scales the cluster to 4 replicas. \n List the Pods after scaling. \n kubectl get pods \n The four Pods get listed. Initially some of the Pods could be listed as not in READY (1/1) state. Run the \npreceding command multiple times to list all pods running and ready as shown in Figure  7-28 . \n Deleting the Replication Controller \n To  delete a replication controller  mongo-rc run the following command. \n kubectl delete replicationcontroller mongo-rc \n All the Pods managed by the replication controller also get deleted. Subsequently run the following \ncommand to list the Pods. \n kubectl get pods \n Figure 7-27.  Scaling a Replication Controller \n Figure 7-28.  Listing the Pods after Scaling \n \n \n",
      "content_length": 785,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n190\n The  mongo Pods do not get listed as shown in Figure  7-29 . \n Deleting the Service \n To delete the service called  mongo run the following command. \n kubectl delete service mongo \n The  mongo service does not get listed as shown in Figure  7-30 . \n Using a Host Port \n The container specification within a Pod has the provision to configure a host port. A  host port is a container \nport mapping to the host implying that the specified host port gets reserved for a single container The \n hostPort field should be used for a single machine container. Multiple containers of the type in which the \n hostPort is specified cannot be started because the host port can be reserved only by a single container. \nOther Pods that do not specify a  hostPort field could be run, however, on the same machine on which a \ncontainer with  hostPort field mapping is running. As a variation of the replication controller we used earlier \nadd a  hostPort field in the spec- > containers- > ports field. The modified  mongo-rc.yaml is listed. \n --- \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  labels: \n    name: mongo \n  name: mongo-rc \n spec: \n  replicas: 2 \n  template: \n    metadata: \n Figure 7-29.  Deleting a Replication Controller \n Figure 7-30.  Deleting the mongo Service \n \n \n",
      "content_length": 1326,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n191\n      labels: \n        name: mongo \n    spec: \n      containers: \n        - \n          image: mongo \n          name: mongo \n          ports: \n            - \n              containerPort: 27017 \n              hostPort: 27017 \n              name: mongo \n Run the following command to create a replication controller. \n kubectl create -f mongo-rc.yaml \n List the replication controllers with the following command. \n kubectl get rc \n The  mongo-rc replication controller gets created and listed as shown in Figure  7-31 . \n List the Pods with the following command. \n kubectl get pods \n Only one of the two replicas is listed as Running and READY (1/1). Even if the preceding command is \nrun multiple times, only one replica is listed as running as shown in Figure  7-32 . \n Figure 7-31.  Creating a Replication Controller from a Definition File \n \n",
      "content_length": 884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n192\n Scale the MongoDB cluster to 4 replicas with the following command. \n kubectl scale rc mongo --replicas=4 \n Though the output from the command is “Scaled” and 4 Pods get created but only one Pod is in \nREADY (1/1) state at any particular time as shown in Figure  7-33 . \n Figure 7-32.  Listing the Pods after creating a Replication Controller \n \n",
      "content_length": 386,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n193\n Even if the single running Pod is stopped only one new Pod gets started. To demonstrate, stop the single \nrunning Pod. \n kubectl stop pod mongo-rc-laqpl \n The Pod gets removed but a replacement Pod gets created to maintain the replication level of 1 as \nshown in Figure  7-34 . \n Figure 7-33.  Scaling the Replication Controller to 4 Replicas \n Figure 7-34.  Another Pod gets created when the single running Pod is stopped \n \n \n",
      "content_length": 468,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n194\n List the Pods again after a few seconds and only one Pod gets listed as shown in Figure  7-35 . \n Using the  hostPort field is not recommended unless a single container machine is to be used or only a \nsingle container is required to be mapped to the host port. \n Creating a MongoDB Cluster Imperatively \n In the following subsections we shall create a Kubernetes replication controller and service for a MongoDB \n cluster on the command line using kubectl. \n Creating a Replication Controller \n To create a replication  controller for the Docker image “mongo” with 2 replicas and port 27017 run the \nfollowing command. \n kubectl run mongo --image=mongo --replicas=2 --port=27017 \n The replication controller gets created as shown in Figure  7-36 . \n List the Pods with the following command. \n kubectl get rc \n Figure 7-36.  Creating a Replication Controller Imperatively \n Figure 7-35.  Only a single Pod is Running and Ready \n \n \n",
      "content_length": 973,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n195\n The  mongo -rc gets listed as shown in Figure  7-37 . \n Listing the Pods \n List the  Pods with the following command. \n kubectl get pods \n The two Pods started for MongoDB get listed as shown in Figure  7-38 . Initially the Pods could be listed \nas not running. \n Run the following preceding multiple times if required to list the Pods as running as shown in Figure  7-39 . \n Figure 7-37.  Listing the Replication Controllers \n Figure 7-38.  Listing the Pods with some of the pods not Running yet \n Figure 7-39.  Listing all the Pods as Running \n \n \n \n",
      "content_length": 592,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n196\n Listing the Logs \n List the  logs for a Pod with the following command. The  mongo-56850 is the Pod name. \n kubectl logs mongo-56850 \n The Pod logs get listed as shown in Figure  7-40 . \n MongoDB is listed as started as shown in Figure  7-41 . Output on commands run on the server also \nget output. \n Figure 7-40.  Listing Pod Logs \n \n",
      "content_length": 375,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n197\n Creating a Service \n To create a  service for the  mongo replication controller run the following command to expose a service on \nport 27017 of type  LoadBalancer , which was discussed earlier. \n kubectl expose rc mongo --port=27017 --type=LoadBalancer \n Figure 7-41.  Listing MongoDB Server as running and waiting for connections on port 27017 \n \n",
      "content_length": 388,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n198\n The  mongo service gets created as shown in Figure  7-42 . \n List the services with the following command. \n kubectl get services \n The  mongo service is listed as running in Figure  7-43 . \n An interactive terminal and a Mongo shell may get started to create a MongoDB database and collection \nto add and query documents in the collection as discussed when creating a MongoDB cluster declaratively. \n Scaling the Cluster \n To scale the cluster to 4 replicas, for example, run the following command. \n kubectl scale rc mongo --replicas=4 \n An output of “scaled” indicates that the cluster has been scaled as shown in Figure  7-44 . \n Subsequently get the Pods. \n kubectl get pods \n Figure 7-42.  Creating a Service Imperatively \n Figure 7-43.  Listing the Services including the mongo Service \n Figure 7-44.  Scaling the Cluster created Imperatively \n \n \n \n",
      "content_length": 897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n199\n Four pods get listed as shown in Figure  7-45 . Initially some of the Pods could be shown as not running \nor ready. \n To describe the  mongo service run the following command. \n kubectl describe svc mongo \n The service description includes the service label, selector in addition to the service endpoints, one for \neach of the four pods as shown in Figure  7-46 . \n Figure 7-45.  Listing Pods after Scaling \n Figure 7-46.  Describing the Service mongo after Scaling \n \n \n",
      "content_length": 511,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "CHAPTER 7 ■ USING MONGODB DATABASE\n200\n Deleting the Service and Replication Controller \n The  mongo service and the  mongo replication controller may be  deleted with the following commands. \n kubectl delete service mongo \n kubectl delete rc mongo \n The “mongo” service and the “mongo” replication controller get deleted as shown in Figure  7-47 . \nDeleting one does not delete the other; the decoupling of the replication controller from the service is a \nfeature suitable to evolve one without having to modify the other. \n Summary \n In this chapter we used the Kubernetes cluster manager to create and orchestrate a MongoDB cluster. We \ncreated a replication controller and a service both imperatively and declaratively. We also demonstrated \nscaling a cluster. We introduced two other features of Kubernetes replication controllers: volumes and \nhost port. This chapter is about using Kubernetes with MongoDB and the emphasis is less on MongoDB; \nbut if MongoDB is to be explored in more detail, refer to the Apress book  Pro MongoDB Development \n( http://www.apress.com/9781484215999?gtmf=s ). In the next chapter we shall discuss another NoSQL \ndatabase, Apache Cassandra. \n Figure 7-47.  Deleting the Service and the Replication Controller \n \n",
      "content_length": 1251,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "201\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_8\n CHAPTER 8 \n Using Apache Cassandra Database \n Apache Cassandra is an open source wide column data store. Cassandra is a scalable, reliable, fault-tolerant, \nand highly available NoSQL database. Cassandra is based on a  flexible schema data model in which data is \nstored in rows in a table (also called column family) with a primary key identifying a row. The primary key \ncould be a single column or multiple column (compound) row key. A relational database also stores data in \ntable rows, but what makes Cassandra different is that the table rows do not have to follow a fixed schema. \nEach row in a table could have different columns or some of the columns could be the same as other rows. \nEach row does not have to include all the columns or any column data at all. In this regard Cassandra \nprovides a  dynamic column specification . A keyspace is a namespace container for the data stored in \nCassandra. In this chapter we shall discuss using Kubernetes cluster manager with Apache Cassandra. \nThis chapter has the following sections.\n Setting the Environment \n Creating a Cassandra Cluster Declaratively \n Creating a Cassandra Cluster Imperatively \n Setting the Environment \n The following software is required for this chapter.\n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n -Docker image for Apache Cassandra (latest version) \n Install the software on an  Amazon EC2 instance created from Ubuntu Server 14.04 LTS (HVM), SSD \nVolume Type - ami-d05e75b8 AMI as explained in chapter  1 . SSH Login to the Ubuntu instance using the \nPublic IP Address of the Amazon EC2 instance. \n ssh -i \"docker.pem\" ubuntu@52.23.160.7 \n Start the Docker engine and verify its status. \n sudo service docker start \n sudo service docker status \n",
      "content_length": 1883,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n202\n The  Docker engine should be running as shown in Figure  8-1 . \n List the services. \n kubectl get services \n The “ kubernetes ” service should be listed as shown in Figure  8-2 . \n List the Pods and the nodes with the following commands. \n kubectl get pods \n kubectl get nodes \n Initially the only pod running is the Kubernetes pod as shown in Figure  8-3 . \n A Cassandra cluster may be created and managed both declaratively and imperatively, and we shall \ndiscuss both the options. \n Figure 8-1.  Starting Docker \n Figure 8-2.  Listing the “kubernetes” Service \n Figure 8-3.  Listing the Pod and Node for Kubernetes \n \n \n \n",
      "content_length": 674,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n203\n Creating a Cassandra Cluster Declaratively \n In the following subsections we have discussed creating a Cassandra cluster using definition files based on \nthe YAML format. First, create a service to represent a Cassandra cluster. A service is the external interface \nfor a cluster of Pods, Apache Cassandra pods in the context of this chapter. \n Creating a  Service \n Create a service definition file called  cassandra-service.yaml . Add the fields discussed in Table  8-1 . \n The  cassandra-service.yaml is listed below. Use the YAML Lint ( http://www.yamllint.com/ ) to \nvalidate the syntax. \n apiVersion: v1 \n kind: Service \n metadata: \n  name: cassandra \n  labels: \n    app: cassandra \n Table 8-1.  Fields in the Service Definition File \n Field \n Description \n Value \n apiVersion \n API Version. \n v1 \n kind \n Kind of the definition file. \n Service \n metadata \n Metadata of the service. \n metadata - > name \n Service name. Required field. \n cassandra \n metadata - > labels \n Service labels. A label could be any key- > value pair. A service \nlabel is set as app:cassandra. \n app:cassandra \n spec \n The service specification. \n spec - > labels \n The spec labels. A label could be any key- > value pair. The service \nlabel is set as app:Cassandra. \n app:cassandra \n spec - > selector \n Service selector. Used to select Pods to manage. Pods with a label \nthe same as the selector expression are selected or managed by \nthe service. The selector expression could be any key:value pair. \nOr, multiple requirements or expressions could be specified \nusing a ‘,’. The app:cassandra setting translates to service selector \napp = cassandra. \n app:cassandra \n spec - > ports \n The service ports. The ports field is required. \n spec - > ports - > port \n A single service port at which the service is exposed for access by \nexternal clients. \n 9042 \n spec - > type \n The service type. \n LoadBalancer \n",
      "content_length": 1940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n204\n spec: \n  labels: \n    app: cassandra \n  selector: \n    app: cassandra \n  ports: \n    - \n      port: 9042 \n  type: LoadBalancer \n The  cassandra-service.yaml file may be created in a vi editor and saved using the :wq command as \nshown in Figure  8-4 . \n Figure 8-4.  Service Definition File in vi Editor \n \n",
      "content_length": 355,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n205\n To create a service run the following command. \n kubectl create -f cassandra-service.yaml \n Subsequently list the services. \n kubectl get services \n The  cassandra service gets listed as shown in Figure  8-5 . \n Describe the  cassandra service with the following command. \n kubectl describe svc cassandra \n The service name, namespace, labels, selector, type, IP, Port, NodePort and endpoints get listed as \nshown in Figure  8-6 . No service endpoint is listed initially because a Pod has not been created yet. \n Figure 8-5.  Creating and listing a Service for Apache Cassandra \n Figure 8-6.  Describing the Service for Apache Cassandra \n \n \n",
      "content_length": 691,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n206\n Creating a Replication Controller \n Next, we shall create a replication  controller for Cassandra. A replication controller defines the configuration \nfor the containers and their respective Docker images in the Pod. Create a definition file  cassandra-rc.yaml \nand add the following (Table  8-2 ) fields. \n Table 8-2.  Fields in the Replication Controller Definition File \n Field \n Description \n Value \n apiVersion \n The API Version. \n v1 \n kind \n Kind of definition file. \n ReplicationController \n metadata \n Replication controller metadata. \n metadata - > labels \n Replication controller labels. The key:value pair \napp:cassandra is set as a label on the replication \ncontroller. \n app:cassandra \n spec \n The replication controller specification. \n spec - > replicas \n The number of replicas. \n 1 \n spec- > selector \n The selector expression for the replication controller. \nMust be the same as one of the labels in the spec \n- > template - > metadata - > labels field. Required \nfield but not required to be set explicitly and defaults \nto the labels in spec - > template - > metadata \n- > labels field. If multiple requirements are set in \nthe selector multiple labels in the Pod template \nlabels must match. For example if the  selector is \n app=cassandra,name=cassandra the Pod template \nlabels spec - > template - > metadata - > labels must \ninclude both of these labels. \n spec - > template \n The Pod template. Required field. \n spec - > template \n- > metadata \n Template metadata. \n spec - > template \n- > metadata - > labels \n Template labels. The key:value pair app:cassandra is \nset as a label on the Pod. A label must be set on the \ntemplate. The label setting translates to Pod label \napp=cassandra. \n app:cassandra \n spec - > template - > spec  The container specification. \n spec - > template - > spec \n- > containers \n The containers in the Pod. \n spec - > template - > spec \n- > containers - > image \n The Docker image for a container. \n cassandra \n spec - > template - > spec \n- > containers - > name \n The container name. \n cassandra \n spec - > template - > spec \n- > containers - > ports \n The container ports. \n(continued)\n",
      "content_length": 2195,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n207\n The  cassandra-rc.yaml is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  name: cassandra-rc \n  labels: \n    app: cassandra \n spec: \n  replicas: 1 \n  template: \n    metadata: \n      labels: \n        app: cassandra \n    spec: \n      containers: \n        - \n        image: cassandra \n        name: cassandra \n        ports: \n          - \n            containerPort: 9042 \n            name: cql \n          - \n            containerPort: 9160 \n            name: thrift \n The  cassandra-rc.yaml field may be created in a vi editor and saved with the :wq command as shown \nin Figure  8-7 . \n Field \n Description \n Value \n spec - > template - > spec \n- > containers - > ports \n- > containerPort \n The container port for CQL command shell. \n 9042 \n spec - > template - > spec \n- > containers - > ports \n- > name \n The port name. \n cql \n spec - > template - > spec \n- > containers - > ports \n- > containerPort \n The container port for thrift clients. \n 9160 \n spec - > template - > spec \n- > containers - > ports \n- > name \n The port name. \n thrift \nTable 8-2. (continued)\n",
      "content_length": 1133,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n208\n Create a replication controller with the following command. \n kubectl create -f cassandra-rc.yaml \n Subsequently list the replication controllers. \n kubectl get rc \n The  cassandra-rc replication controller gets created and listed as shown in Figure  8-8 . \n Figure 8-7.  Replication Controller Definition File in vi Editor \n \n",
      "content_length": 376,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n209\n List the Pods created by the replication controller. \n kubectl get pods \n As the number of replicas is set to 1 in the replication controller definition file, one Pod gets created \nand is listed in Figure  8-9 . The preceding command may have to be run multiple times to list the Pod as \nrunning and ready. Alternatively run the command for the first time after a few seconds of having created the \nreplication controller; by a minute all Pods should have started. \n Describe the Cassandra service. \n kubectl describe svc cassandra \n An endpoint gets listed for the Pod as shown in Figure  8-10 . When the service description was listed \nbefore creating a replication controller, no endpoint got listed. \n Figure 8-8.  Creating a Replication Controller from Definition File \n Figure 8-9.  Listing Pod/s for Apache Cassandra \n Figure 8-10.  Describing the Service after creating the Replication Controller \n \n \n \n",
      "content_length": 961,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n210\n In the preceding example we created a replication controller with the number of replicas set as 1. \nThe replication controller does not have to create a replica to start with. To demonstrate we shall create \nthe replication controller again, but with a different replicas setting. Delete the replication controller \npreviously created. \n kubectl delete rc cassandra-rc \n Modify the  cassandra-rc.yaml to set replicas field to 0 as shown in Figure  8-11 . \n Create the replication controller again with the modified definition file. \n kubectl create -f cassandra-rc.yaml \n Figure 8-11.  Setting Replicas to 0 \n \n",
      "content_length": 660,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n211\n Subsequently list the replicas. \n kubectl get rc \n The  cassandra-rc replication controller gets created and gets listed as shown in Figure  8-12 . \n List the Pods. \n kubectl get pods \n Because the replicas field is set to 0 the REPLICAS get listed as 0 as shown in Figure  8-13 . \n Scaling the Database \n Starting with the replication controller with 0 replicas created we  shall  scale up the cluster to a single replica. \nRun the following command to scale the Pod cluster to 1 replica. \n kubectl scale rc cassandra-rc --replicas=1 \n Subsequently list the Pods. \n kubectl get pods \n The output from the preceding commands is shown in Figure  8-14 . A “scaled” output indicates that the \ncluster has been scaled. The single Pod could take a while (a few seconds) to get started and become ready. \n Figure 8-12.  Creating the Replication Controller with Modified Definition File \n Figure 8-13.  With Replicas as 0 no Pod gets created \n \n \n",
      "content_length": 989,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n212\n Describe the  cassandra service again. \n kubectl describe svc cassandra \n A single endpoint should get listed for the Pod added as shown in Figure  8-15 . \n Describing the Pod \n To describe the  Pod run the following command. \n kubectl describe pod cassandra-rc-tou4u \n Detailed information about the Pod such as name, namespace, image, node, labels, status, IP address, \nand events gets output as shown in Figure  8-16 . The Pod label is  app=cassandra as specified in the \nreplication controller definition file. \n Figure 8-14.  Scaling the Replication Controller to 1 Pod \n Figure 8-15.  Describing the Service after Scaling the Cluster \n \n \n",
      "content_length": 694,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n213\n Starting an Interactive Shell \n As the “cassandra” Docker image inherits from the “debian” Docker image an interactive bash shell may be \nused to access a Docker container based on the cassandra image. To start an  interactive bash shell to access \nthe Cassandra server running in a Docker container, we need to obtain the container id. List the running \ncontainers. \n sudo docker ps \n All the running containers get listed as shown in Figure  8-17 . Copy the container id for the container for \nthe  cassandra image. \n Figure 8-16.  Describing the single Pod \n \n",
      "content_length": 612,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n214\n Using the container id start an interactive bash shell. \n sudo docker exec -it e8fc5e8ddff57 bash \n An interactive shell gets started as shown in Figure  8-18 . \n Figure 8-17.  Listing the Docker Containers \n Figure 8-18.  Starting the Interactive Shell \n \n \n",
      "content_length": 308,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n215\n Starting the  CQL Shell \n Cassandra Query Language (CQL) is the query language for Apache Cassandra. In the following sections we \nshall run CQL commands to create a keyspace and a table. Start the CQL Shell with the following command. \n cqlsh \n CQL Shell 5.0.1 gets started as shown in Figure  8-19 . \n Creating a Keyspace \n Next, create a keyspace called  CatalogKeyspace using the replication class as SimpleStrategy and replication \nfactor as 3. \n CREATE KEYSPACE CatalogKeyspace \n            WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 3}; \n A keyspace gets created as shown in Figure  8-20 . \n Altering a Keyspace \n A keyspace may be altered with the ALTER KEYSPACE command. Run the following command to alter the \nkeyspace setting replication factor to 1. \n ALTER KEYSPACE CatalogKeyspace \n          WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 1}; \n Keyspace gets altered as shown in Figure  8-21 . \n Figure 8-21.  Altering a Keyspace \n Figure 8-19.  Starting the cqlsh Shell \n Figure 8-20.  Creating a Keyspace \n \n \n \n",
      "content_length": 1127,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n216\n Using a Keyspace \n To use the  CatalogKeyspace keyspace run the following command. \n use CatalogKeyspace; \n Keyspace  CatalogKeyspace gets set as shown in Figure  8-22 . \n Creating a  Table \n A table is also called a column family. Both  CREATE TABLE and  CREATE COLUMN FAMILY clauses may be used \nto create a table (column family). Create a table called  catalog using the following CQL statement. \n CREATE TABLE catalog(catalog_id text,journal text,publisher text,edition text,title \ntext,author text,PRIMARY KEY (catalog_id)) WITH compaction = { 'class' : \n'LeveledCompactionStrategy' }; \n Add two rows of data to the table using the following CQL statements. \n INSERT INTO catalog (catalog_id, journal, publisher, edition,title,author) VALUES \n('catalog1','Oracle Magazine', 'Oracle Publishing', 'November-December 2013', 'Engineering \nas a Service','David A. Kelly') IF NOT EXISTS; \n INSERT INTO catalog (catalog_id, journal, publisher, edition,title,author) VALUES \n('catalog2','Oracle Magazine', 'Oracle Publishing', 'November-December 2013', \n'Quintessential and Collaborative','Tom Haunert') IF NOT EXISTS; \n Output from the preceding commands is shown in Figure  8-23 . A Cassandra table gets created and two \nrows of data get added. \n Figure 8-22.  Setting a Keyspace to be used \n Figure 8-23.  Creating an Apache Cassandra Table \n \n \n",
      "content_length": 1395,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n217\n Run the following  CQL query statement to select data from the  catalog table. \n SELECT * FROM catalog; \n The two rows of data added gets listed as shown in Figure  8-24 . \n Deleting from a Table \n To delete row/s of data run a  DELETE CQL statement. The primary key column value cannot be deleted with \n DELETE . Delete the other column values for the row with  catalog_id as ‘catalog’ with the following CQL \nstatement. \n DELETE journal, publisher, edition, title, author from catalog WHERE catalog_id='catalog1'; \n Subsequently run the following CQL query to select data from the  catalog table. \n SELECT * FROM catalog; \n As shown in Figure  8-25 only one complete row of data gets output. The other row lists only the \n catalog_id column value, and all the other column values are  null . \n Figure 8-24.  Querying an Apache Cassandra Table \n \n",
      "content_length": 897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n218\n Truncating a Table \n Truncating a  table implies removing all table data including primary key column values. Run the following \n TRUNCATE CQL statement to remove all rows. \n TRUNCATE catalog; \n Subsequently run the CQL query statement again. \n SELECT * from catalog; \n No rows get listed as shown in Figure  8-26 ; not even null values are listed after running a  TRUNCATE \nstatement. \n Dropping a Table and Keyspace \n To drop a table run the CQL statement with the  DROP TABLE clause . The  IF EXISTS clause drops the table if it \nexists but does not return an error if the table does not exist. \n DROP TABLE IF EXISTS catalog; \n Figure 8-25.  Querying Table after deleting  Data from a Row \n Figure 8-26.  Querying a Table after Truncating a Table \n \n \n",
      "content_length": 805,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n219\n Drop the  CatalogKeyspace keyspace using the  DROP KEYSPACE clause statement. The  IF EXISTS clause \ndrops the keyspace if it exists but does not return an error if the keyspace does not exist. \n DROP KEYSPACE IF EXISTS CatalogKeyspace; \n To verify that the keyspace  CatalogKeyspace has been removed, run the following statement. \n use CatalogKeyspace; \n As the  CatalogKeyspace keyspace does not exist an error gets generated as shown in Figure  8-27 . \n Creating a Volume \n In chapter  7 we introduced  volumes , how they are mounted into a Pod using volume mounts, and how they \nare accessed within a container. We introduced various types of volumes and demonstrated the  emptyDir \ntype of volume. In this section we shall use another type of volume, the  hostPath volume. The  hostPath \nvolume mounts a directory from the host into the Pod. All containers in the Pod and all Pods based on a Pod \ntemplate using a  hostPath type of volume may access the directory on the host. As a modification of the \nreplication controller used earlier, we shall add a volume of type  hostPath to the  cassandra-rc.yaml file. \nFor example, if the host directory  /cassandra/data is to be mounted in a Pod add the following volume in \nthe spec- > template field. \n volumes: \n  - \n    hostPath: \n      path: /cassandra/data \n     name: cassandra-storage \n The volume is mounted in the Pod using the same fields as a  emptyDir volume. The modified \n cassandra-rc.yaml is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata: \n  name: cassandra-rc \n  labels: \n    app: cassandra \n Figure 8-27.  Dropping a Table \n \n",
      "content_length": 1664,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n220\n spec: \n  replicas: 1 \n  template: \n    metadata: \n      labels: \n        app: cassandra \n    spec: \n      containers: \n        - \n        image: cassandra \n        name: cassandra \n        ports: \n          - \n            containerPort: 9042 \n            name: cql \n          - \n            containerPort: 9160 \n            name: thrift \n        volumeMounts: \n          - \n            mountPath: /cassandra/data \n            name: cassandra-storage \n      volumes: \n        - \n          hostPath: \n            path: /cassandra/data \n          name: cassandra-storage \n The  cassandra-rc.yaml definition file may be edited in vi editor and saved with the :wq command as \nshown in Figure  8-28 . It is recommended to add quotes in field values. \n",
      "content_length": 794,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n221\n The host directory that is mounted into a Pod has to pre-exist. Create the  /cassandra/data directory \nand set its permissions to global (777). \n sudo mkdir –p /cassandra/data \n sudo chmod –R 777 /cassandra/data \n The output from the preceding commands is shown in Figure  8-29 . The  /cassandra/data directory \ngets created. \n Figure 8-28.  Replication Controller Definition File with a Volume of type hostPath \n \n",
      "content_length": 464,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n222\n Change directory (cd) to the  /cassandra/data directory on the host. \n cd /cassandra/data \n List the files and directories in the  /cassandra/data directory. \n ls –l \n Initially the  /cassandra/data is empty as shown in Figure  8-30 . Add a sample file,  cassandra.txt , to \nthe directory with the vi editor. Subsequently list the directory files and directories again. \n vi cassandra.txt \n ls –l \n As shown in Figure  8-30 the  cassandra.txt file gets listed. What the  hostPath volume does is to make \nthe  /cassandra/data directory available to all containers in the Pod. \n Create a replication controller as discussed for the definition file used previously. One Pod should get \ncreated. List the Docker containers. \n sudo docker ps \n Copy the container id for the Docker container for image “cassandra” as shown in Figure  8-31 . \n Figure 8-30.  Adding a file in the hostPath Volume Directory \n Figure 8-29.  Creating the Directory for the Volume \n \n \n",
      "content_length": 1006,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n223\n Using the container id start an interactive shell. \n sudo docker exec -it 11a4b26d9a09 bash \n The interactive shell gets started as shown in Figure  8-32 . \n Figure 8-31.  Listing the Docker Containers \n Figure 8-32.  Starting an Interactive Shell \n \n \n",
      "content_length": 302,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n224\n Change directory (cd) to the  /cassandra/data directory and list the files in the directory. \n cd /cassandra/data \n ls –l \n As shown in Figure  8-33 the cassandra.txt file gets listed. The /cassandra/data directory exists on the \nhost but is accessible from a container. \n Similarly volumes of other types could be created. Following is the  volumeMounts and  volumes fields \nsettings for a AWS Volume. The  volumeID field has the format  aws://zone/volume id. \n    volumeMounts: \n        - \n          mountPath: /aws-ebs \n          name: aws-volume \n  volumes: \n      - \n        name: aws-volume \n        awsElasticBlockStore: \n              volumeID: aws://us-east-ib/vol-428ba3ae \n              fsType: ext4 \n A more complete  cassandra-rc.yaml file is shown in Figure  8-34 . \n Figure 8-33.  Accessing the Volume in a Docker Container \n \n",
      "content_length": 891,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n225\n Creating a Cassandra Cluster Imperatively \n If the default settings for most of the fields are to be used, creating a replication controller imperatively is the \nbetter option. \n Creating a Replication Controller \n To create a replication  controller on the command line use the  kubectl run command. For a replication \ncontroller based on the Docker image “cassandra” run the following command in which replication \ncontroller name is “cassandra” and port is 9042. The replicas is set to 1, also the default value. \n kubectl run cassandra --image=cassandra --replicas=1 --port=9042 \n Figure 8-34.  Volume of type awsElasticBlockStore in a Replication Controller Definition File \n \n",
      "content_length": 731,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n226\n Subsequently list the replication controllers. \n kubectl get rc \n The “cassandra” replication controller gets created and get listed as shown in Figure  8-35 . \n To list the Pods run the following command. \n kubectl get pods \n The single Pod created gets listed as shown in Figure  8-36 . \n To describe the replication controller run the following command. \n kubectl describe rc cassandra \n The replication controller’s name, namespace, image, selector, labels, replicas, pod status, and events \nget listed as shown in Figure  8-37 . The selector defaults to “run=cassandra” for the  cassandra replication \ncontroller. \n Figure 8-35.  Creating a Replication Controller Imperatively \n Figure 8-36.  Listing the single Pod \n \n \n",
      "content_length": 775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n227\n Figure 8-37.  Describing the Replication Controller \n Figure 8-38.  Creating a Service for Apache Cassandra Imperatively \n Creating a Service \n To expose the replication controller  cassandra as a  service , run the  kubectl expose command. The port is \nrequired to be specified and is set to 9042 for the service. \n kubectl expose rc cassandra --port=9042 --type=LoadBalancer \n The  cassandra service gets created as shown in Figure  8-38 . \n Describe the service with the following command. \n kubectl describe service cassandra \n As shown in Figure  8-39 the service name, namespace, labels, selector, type, IP, Port, NodePort, and \nEndpoint get listed. The service selector run=cassandra must be the same as the label on the Pod to manage. \n \n \n",
      "content_length": 797,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n228\n Figure 8-39.  Describing the Service \n Figure 8-40.  Scaling Down the Database Cluster to 0 Replicas \n Figure 8-41.  Listing the Pods after Scaling Down \n Scaling the Database \n To  scale the cluster, run the  kubectl scale command. An important reason, to scale the Cassandra \nreplication controller is to run more Cassandra nodes and have them join the cluster, and we demonstrated \nscaling up a cluster. But it is not always necessary to scale up a cluster. A cluster may also be scaled down. To \nscale down the cluster to 0 replicas run the following command. \n kubectl scale rc cassandra --replicas=0 \n A output of “scaled” in Figure  8-40 indicates that the cluster has been scaled down. \n List the Pods. \n kubectl get pods \n No pod gets listed as shown in Figure  8-41 . \n \n \n \n",
      "content_length": 834,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n229\n List the services with the following command. \n kubectl get services \n Scaling the cluster to 0 replicas would leave no Pod for the service to manage but the service is still \nrunning as shown in Figure  8-42 . \n But the service does not have any endpoint associated with it as shown with the  kubectl describe \ncommand in Figure  8-43 . \n Deleting the Replication Controller and Service \n To delete the replication  controller “cassandra” run the following command. \n kubectl delete rc cassandra \n Subsequently list the replication controllers. \n kubectl get rc \n Figure 8-42.  Listing the Services after Scaling Down \n Figure 8-43.  Describing the Service after Scaling Down \n \n \n",
      "content_length": 731,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "CHAPTER 8 ■ USING APACHE CASSANDRA DATABASE\n230\n To delete the service “cassandra” run the following command. \n kubectl delete service cassandra \n Subsequently list the services. \n kubectl get services \n The output from the preceding commands is shown in Figure  8-44 . The replication controller and \nservice get deleted and do not get listed. \n Summary \n In this chapter we used Kubernetes to create an Apache Cassandra cluster. We used both the declarative and \nimperative approaches. We introduced the volumes in the previous chapter and in this chapter we discussed \nusing two other types of volumes: hostPath and AWS Volume. We scaled the cluster not only up but also \ndown. We demonstrated that a replication controller does not require a Pod to be running and could specify \n0 replicas. In the next chapter we shall discuss using Kubernetes cluster manager with another NoSQL \ndatabase, Couchbase. \n Figure 8-44.  Deleting the Replication Controller and the  Service \n \n",
      "content_length": 978,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "231\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_9\n CHAPTER 9 \n Using Couchbase \n Couchbase is a distributed NoSQL database based on the JSON data model. Couchbase is faster than \nMongoDB and Apache Cassandra. Couchbase offers some features not available in MongoDB and \nCassandra such as a Graphical User Interface (GUI), the Couchbase Web Console. Couchbase also provides \ncommand-line tools such as  couchbase-cli ,  cbbackup ,  cbrestore , and  cbtransfer . Couchbase, being a \ndistributed database, could benefit from the cluster management provided by Kubernetes cluster manager, \nwhich is what we shall discuss in this chapter. This chapter has the following sections.\n Setting the Environment \n Creating a Couchbase Cluster Declaratively \n Creating a Couchbase Cluster Imperatively \n Setting the Environment \n We have used an Ubuntu instance on Amazon EC2 created using the same AMI as used in the other \nchapters, the Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-d05e75b8. If an instance created \nfrom the AMI already exists the same may be used. The following software is required for this chapter. \n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n -Docker image for Couchbase (latest version) \n First, we need to log in to the Ubuntu instance. Obtain the Public IP Address of the Ubuntu instance \nfrom the Amazon EC2 instance console as shown in Figure  9-1 . \n",
      "content_length": 1475,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n232\n Use the Public IP Address log in to the Ubuntu instance. \n ssh -i \"docker.pem\" ubuntu@54.172.55.212 \n The Ubuntu instance gets logged into as shown in Figure  9-2 . \n Figure 9-1.  Getting  Public IP Address \n \n",
      "content_length": 243,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n233\n Start the Docker Engine and verify its status. \n sudo service docker start \n sudo service docker status \n Docker engine should be listed as running as shown in Figure  9-3 . \n Figure 9-2.  Logging into  Ubuntu Instance on Amazon EC2 \n Figure 9-3.  Starting Docker Engine \n List the running services. \n kubectl get services \n \n \n",
      "content_length": 361,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n234\n The  kubernetes service should be listed as running as shown in Figure  9-4 . \n Figure 9-5.  Listing the Single Node \n Figure 9-4.  Listing the “kubernetes” Service \n List the nodes. \n kubectl get nodes \n The node should be listed with STATUS “Ready” as shown in Figure  9-5 . \n Creating a Couchbase  Cluster Declaratively \n In the following subsections we shall create a Couchbase Pod, a replication controller, and a service all using \ndefinition files. \n Creating a Pod \n A  Pod definition file is used to create a single Pod. A Pod could have 0 or more container configurations. \nCreate a definition file  couchbase.yaml . Add the following (Table  9-1 ) fields to the definition file. \n \n \n",
      "content_length": 728,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n235\n The  couchbase.yaml definition file is listed. \n apiVersion: v1 \n kind: Pod \n metadata: \n  labels:  \n    app: couchbaseApp \n  name: couchbase \n spec:  \n  containers: \n    -  \n      image: couchbase \n      name: couchbase \n      ports:  \n        -  \n          containerPort: 8091 \n Table 9-1.  Pod Definition File Fields \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n Pod \n metadata \n The Pod metadata. \n metadata - > labels \n The Pod labels. A service selector makes use of \nthe labels to select the Pods to manage. \n app: couchbaseApp \n metadata - > name \n The Pod name. \n couchbase \n spec \n The Pod specification. \n spec - > containers \n The containers in the Pod. \n spec - > containers - > image \n A container image. For Couchbase server the \nimage is “couchbase.” \n couchbase \n spec - > containers - > name \n The container name. \n couchbase \n spec - > containers - > ports \n The container ports. \n spec - > containers - > ports \n- > containerPort \n A container port for Couchbase server. \n 8091 \n",
      "content_length": 1074,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n236\n The  couchbase.yaml file could be created in the vi editor and saved with the :wq command as shown in \nFigure  9-6 . \n Figure 9-6.  Pod Definition file couchbase.yaml in vi Editor \n \n",
      "content_length": 216,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n237\n Figure 9-7.  Creating a Pod from the Definition File \n Figure 9-8.  Listing the couchbase Pod \n Run the following command to create a Pod from the definition file.  \n kubectl create -f couchbase.yaml  \n A Pod gets created as indicated by the “pods/couchbase” output in Figure  9-7 . \n Subsequently list the Pods. \n kubectl get pods \n A Pod called “couchbase” gets listed as shown in Figure  9-7 . Initially the STATUS could be different from \n“Running” and the READY column could be not ready; 1/1 is ready state and 0/1 is not ready. \n Run the following command again after a few more seconds. \n kubectl get pods \n The  couchbase Pod is listed as “Running” and READY- > 1/1 as shown in Figure  9-8 . \n Creating a Service \n In this section we shall create a service using a  service definition file. Create a  couchbase-service.yaml file \nand add the following (Table  9-2 ) fields to the file. \n \n \n",
      "content_length": 933,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n238\n The  couchbase-service.yaml is listed. \n apiVersion: v1 \n kind: Service \n metadata:  \n  labels:  \n    app: couchbaseApp \n  name: couchbase \n spec:  \n  ports:  \n    -  \n      port: 8091 \n      targetPort: 8091 \n  selector:  \n    app: couchbaseApp \n  type: LoadBalancer \n Table 9-2.  Service Definition File couchbase-service.yaml \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n Service \n metadata \n The service metadata. \n metadata - > labels \n The service labels. \n app: couchbaseApp \n metadata - > name \n The service name. \n couchbase \n spec \n The service specification. \n spec - > ports \n The ports exposed by the service. \n spec - > ports - > port \n A port exposed by the service. \n 8091 \n spec - > ports \n- > targetPort \n The target port for the service, which could be a port \nnumber or the name of a port on the backend. The target \nport setting adds flexibility as the port number could be \nmodified while the port name is kept fixed. \n 8091 \n spec - > selector \n The Pod selector, which could be one or more label \nkey:value expressions/labels. All of the key:value \nexpressions in a selector must match with a Pod’s labels \nfor the Pod to be selected by the service. A Pod could have \nadditional labels but must include labels in the selector \nto be selected by the service. Service routes traffic to the \nPods with label/s matching the selector expression/s. \nOnly a single selector expression is used in the example \nservice definition file. If the selector is empty all Pods \nare selected. The app: couchbaseApp setting defaults to \nselector app = couchbaseApp. \n app: couchbaseApp \n spec - > selector - > type \n The service type. \n LoadBalancer \n",
      "content_length": 1732,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n239\n Figure 9-9.  Listing the couchbase Service \n Create a service from the definition file with the following command. \n kubectl create -f couchbase-service.yaml \n Subsequently list the running services. \n kubectl get services \n An output of “services/couchbase” as shown in Figure  9-9 indicates that the  couchbase service has been \ncreated. The “couchbase” service gets listed, also shown in Figure  9-9 . \n List the service endpoints with the following command. \n kubectl get endpoints \n The service endpoint for the  couchbase service gets listed as shown in Figure  9-10 . \n Figure 9-10.  Listing the Endpoints \n Creating a Replication Controller \n In this section we shall create a replication  controller using a definition file. Create a  couchbase-rc.yaml file \nand add the following (Table  9-3 ) fields to the file. \n \n \n",
      "content_length": 862,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n240\n Table 9-3.  Definition File for Replication Controller \n Field \n Description \n Value \n Required Field \n(includes default \nsettings) \n apiVersion \n v1 \n yes \n kind \n The kind of definition file. \n ReplicationController  yes \n metadata \n The replication controller metadata. \n yes \n metadata - > labels \n The replication controller labels. \n app: couchbaseApp \n no \n metadata - > name \n The replication controller name. \n couchbase \n yes \n spec \n The replication controller \nspecification. \n yes \n spec - > replicas \n The number of Pod replicas. \nDefaults to 1 replica. \n 2 \n yes \n spec - > selector \n One or more key:value expressions \nfor selecting the Pods to manage. \nPods that include label/s with the \nsame expression/s as the selector \nexpression/s are managed by the \nreplication controller. A Pod could \ninclude additional labels but must \ninclude the ones in the selector \nto be managed by the replication \ncontroller. The selector defaults to \nthe spec - > template - > metadata \n- > labels key:value expression/s \nif not specified. A setting of app: \ncouchbaseApp translates to selector \napp = couchbaseApp. \n app: couchbaseApp \n yes \n spec - > template \n The Pod template. \n yes \n spec - > template - > metadata  The Pod template metadata. \n yes \n spec - > template - > \nmetadata - > labels \n The Pod template labels. \n app: couchbaseApp \n yes \n spec - > template - > spec \n The Pod template specification. \n yes \n spec - > template - > spec - > \ncontainers \n The containers configuration for \nthe Pod template. \n yes \n spec - > template - > \nspec - > containers - > image \n The Docker image. \n couchbase \n yes \n spec - > template - > \nspec - > containers - > name \n The container name. \n couchbase \n yes \n spec - > template - > \nspec - > containers - > ports \n The container ports. \n no \n spec - > template - > \nspec - > containers - > \nports - > containerPort \n A container port. \n 8091 \n no \n",
      "content_length": 1939,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n241\n The  couchbase-rc.yaml is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata:  \n  labels:  \n    app: couchbaseApp \n  name: couchbase \n spec:  \n  replicas: 2 \n  selector:  \n    app: couchbaseApp \n  template:  \n    metadata:  \n      labels:  \n        app: couchbaseApp \n    spec:  \n      containers:  \n        -  \n          image: couchbase \n          name: couchbase \n          ports:  \n          -  \n            containerPort: 8091 \n The  couchbase-rc.yaml may be created in vi editor as shown in Figure  9-11 . \n",
      "content_length": 561,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n242\n Create the replication controller with the following command. \n kubectl create -f couchbase-rc.yaml \n Subsequently, list the replication controllers. \n kubectl get rc \n Figure 9-11.  Replication Controller Definition File couchbase-rc.yaml in vi Editor \n \n",
      "content_length": 289,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n243\n An output of “replicationcontrollers/couchbase” as shown in Figure  9-12 indicates that the “couchbase” \nreplication controller has been created. The “couchbase” replication controller gets listed with the second \ncommand. The REPLICAS is listed as 2, but it does not imply that the replication controller created two new \nreplicas. The replication controller manages Pods based on selector expression matching a Pod label. If \nsome other Pod with the matching label is already running it is counted toward the replicas setting. \n Figure 9-12.  Creating and listing a Replication Controller from the Definition File \n Listing the Pods \n To list the  Pods run the following command. \n kubectl get pods \n Two Pods get listed as shown in Figure  9-13 , and one of the Pods is the Pod created earlier using \na Pod definition file. The label in the Pod definition file was app: “couchbaseApp,” which is also the \nselector expression for the replication controller. The expression app: “couchbaseApp” translates to \napp= couchbaseApp. As a result only one new Pod gets created when the replication controller with replicas \nset to 2 is created. \n Figure 9-13.  Listing the Pods for Couchbase Server \n Listing the Logs \n To list the  logs for a Pod run the  kubectl logs command. The pod name may be copied from the preceding \nlisting of Pods. \n kubectl logs couchbase-0hglx \n The output is shown in Figure  9-14 . The output indicates that the WEB UI is available at \n http://<ip>:8091 . \n Figure 9-14.  Listing Pod Logs \n \n \n \n",
      "content_length": 1555,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n244\n Describing the Service \n To describe the  couchbase service run the following command. \n kubectl describe svc couchbase \n The service name, namespace, labels, selector, type, IP, Port, NodePort, and endpoints get listed as \nshown in Figure  9-15 . The  selector is listed as  app=couchbaseApp . \n Figure 9-15.  Describing the Service for Couchbase \n Listing the Endpoints \n List the  endpoints again. \n kubectl get endpoints \n When the endpoints were listed earlier only one endpoint was listed because only one Pod was running. \nWith two Pods running two endpoints get listed as shown in Figure  9-16 . \n Figure 9-16.  Listing the Endpoints for Couchbase \n Setting Port Forwarding \n When we listed the logs for a Couchbase Pod the URL to invoke the web console was listed as \n http://<ip>:8091 . The  < ip > is the service endpoint of the Pod. The previous section listed two service \nendpoints. Invoking either of these on a host browser, for example,  http://172.17.0.2:8091 would open \nthe web console. An Amazon EC2 Ubuntu instance does not install a web browser by default. Alternatively, \nwe shall set port forwarding to a local machine and open the web console from a browser on a local machine, \n \n \n",
      "content_length": 1242,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n245\nwhich is required to have a browser available. To set port forwarding we need to know the Public DNS of the \nAmazon EC2 instance running Kubernetes. The Public DNS may be obtained from the Amazon EC2 console \nas shown in Figure  9-17 . \n Figure 9-17.  Obtaining the Public DNS \n The ports to forward to on the local machine must be open and not already bound. As an example, \nbind one of the endpoints to port 8093 on  localhost and the other to port 8094 on the  localhost with the \nfollowing commands. \n ssh -i \"docker.pem\" -f -nNT -L 8093:172.17.0.3:8091 ubuntu@ec2-54-172-55-212.compute-1.\namazonaws.com \n ssh -i \"docker.pem\" -f -nNT -L 8094:172.17.0.2:8091 ubuntu@ec2-54-172-55-212.compute-1.\namazonaws.com \n The  port forwarding from the service endpoints to  localhost ports gets set as shown in Figure  9-18 . \n Figure 9-18.  Setting Port Forwarding to localhost:8093 and localhost:8094 \n \n \n",
      "content_length": 932,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n246\n Logging into Couchbase Web Console \n Two ports are available on the local machine to open the Couchbase  web console , 8093 and 8094. \nEither or both of these could be used to open a Couchbase web console. For example, open the URL \n http://localhost:8093 in a web browser. The Couchbase Console gets opened as shown in Figure  9-19 . \nClick on Setup to set up the Couchbase server. \n Figure 9-19.  Setting Up Couchbase Server \n \n",
      "content_length": 463,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n247\n Configuring Couchbase Server \n In this section we shall configure the  Couchbase server , which is not directly related to using Kubernetes \nbut is discussed for completeness. When the Setup button is clicked the CONFIGURE SERVER window gets \ndisplayed as shown in Figure  9-20 . \n Figure 9-20.  Configuring Server Disk Storage, Hostname \n \n",
      "content_length": 374,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n248\n Keep the default settings and scroll down to select Start a new cluster. The RAM settings may have to be \nreduced if sufficient RAM is not available. Click on Next as shown in Figure  9-21 . \n Figure 9-21.  Starting New Cluster \n \n",
      "content_length": 264,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n249\n Some sample buckets get listed but a sample bucket is not required to be selected. Click on Next as \nshown in Figure  9-22 . \n Figure 9-22.  Sample Buckets are not required to be selected \n \n",
      "content_length": 224,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n250\n The Create Default Bucket settings include the Bucket Type, which should be Couchbase as shown in \nFigure  9-23 . Replicas should be enabled with the “Enable” check box. \n Figure 9-23.  Configuring Default Bucket \n \n",
      "content_length": 249,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n251\n Scroll down to enable the Flush mode with the “Enable” check box. Click on Next as shown in Figure  9-24 . \n Figure 9-24.  Enabling Flush Mode and completing Server Configuration \n \n",
      "content_length": 215,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n252\n Next, accept the terms and conditions as shown in Figure  9-25 and click on Next. \n Figure 9-25.  Accepting Terms and Conditions \n \n",
      "content_length": 165,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n253\n To secure the server specify a Password and specify the same password in the Verify Password field as \nshown in Figure  9-26 . \n Figure 9-26.  Securing the Server with Username and Password \n \n",
      "content_length": 226,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n254\n The Couchbase server gets configured. Select the Server Nodes tab and the Server Node Name is listed \nas shown in Figure  9-27 . The Server Node Name is one of the service endpoints. \n Figure 9-27.  Server Node Name is the same as a Service Endpoint \n \n",
      "content_length": 286,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n255\n Adding Documents \n Next, we shall add some documents to the Couchbase server. Select the Data Buckets tab as shown in \nFigure  9-28 . \n Figure 9-28.  Selecting  Data Buckets Tab \n \n",
      "content_length": 214,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n256\n The default bucket gets listed as shown in Figure  9-29 . Click on Documents. \n Figure 9-29.  Clicking on Documents Button for the default Bucket \n \n",
      "content_length": 182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n257\n Initially the “default” bucket is empty as shown in Figure  9-30 . \n Figure 9-30.  Initially no Documents are present in the default Data Bucket \n \n",
      "content_length": 181,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n258\n Click on Create Document to add a document as shown in Figure  9-31 . \n Figure 9-31.  Clicking on Create Document \n \n",
      "content_length": 150,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n259\n In the Create Document dialog specify a Document Id and click on Create as shown in Figure  9-32 . \n Figure 9-32.  Specifying Document ID \n \n",
      "content_length": 174,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n260\n Copy and paste the following JSON document into the  catalog1 document. \n { \n  \"journal\": \"Oracle Magazine\", \n  \"publisher\": \"Oracle Publishing\", \n  \"edition\": \"November-December 2013\", \n  \"title\": \"Quintessential and Collaborative\", \n  \"author\": \"Tom Haunert\" \n } \n Click on Save to update the  catalog1 document as shown in Figure  9-34 . \n A new JSON document with  default fields gets added as shown in Figure  9-33 . \n Figure 9-33.  The catalog1 Document gets created with Default Fields \n \n",
      "content_length": 529,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n261\n The  catalog1 document gets saved and gets listed when the Documents link for the “default” bucket is \nselected as shown in Figure  9-35 . \n Figure 9-34.  Saving a  JSON Document \n Figure 9-35.  The catalog1 Document in default Bucket \n \n \n",
      "content_length": 273,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n262\n Similarly add another document with Document ID as  catalog2 and copy and paste the following \nlisting to the document. \n { \n \"journal\": “Oracle Magazine”, \n \"publisher\": \"Oracle Publishing\", \n \"edition\": \"November December 2013\", \n \"title\": \"Engineering as a Service\", \n \"author\": \"David A. Kelly\", \n } \n The  catalog2 document is shown in Figure  9-36 . \n Figure 9-36.  Adding another Document catalog2 \n \n",
      "content_length": 441,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n263\n The Documents link for the “default” bucket links the two documents added as shown in Figure  9-37 . \n Figure 9-37.  Listing the two Documents in the default Bucket \n \n",
      "content_length": 201,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n264\n Starting an Interactive Shell \n Next, we shall start and interactive bash  shell to access Couchbase server from the command line. Obtain \nthe container id for one of the Docker containers based on the Docker image “couchbase” as shown in \nFigure  9-38 . \n Figure 9-38.  Obtaining the Container Id \n Using the container id, start an interactive shell. \n sudo docker exec -it e1b2fe2f24bd bash \n \n",
      "content_length": 429,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n265\n Figure 9-39.  Starting an Interactive Shell \n An interactive shell gets started as shown in Figure  9-39 . \n Using the cbtransfer Tool \n From the interactive shell command-line tools may be run to access the Couchbase server. As an example \nrun the  cbtransfer tool , which is used to transfer data between clusters and to/from files, to output the \ndocuments in the default bucket at server  http://172.17.0.3:8091 to  stdout . \n cbtransfer http://172.17.0.3:8091/ stdout: \n The two documents added from the web console get output as shown in Figure  9-40 . \n Figure 9-40.  Using the cbtransfer Tool \n In the next section we shall create a Couchbase cluster imperatively using Kubernetes on the command \nline. As we shall be using the same replication controller name and service name, delete the replication \ncontroller “couchbase” and also delete the service called “couchbase.” \n kubectl delete rc couchbase \n kubectl delete svc couchbase \n",
      "content_length": 977,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n266\n Creating a Couchbase  Cluster Imperatively \n In the following subsections we shall create a Couchbase cluster on the command line. \n Creating a Replication Controller \n Create a replication  controller called “couchbase” using the Docker image “couchbase” with two replicas \nand container port as 8091 with the following command. \n kubectl run couchbase --image=couchbase --replicas=2 --port=8091 \n The replication controller gets created as shown in Figure  9-41 . The default selector is “run=couchbase,” \nwhich implies that pods with the label “run=couchbase” shall be managed by the replication controller. \nThe Pod labels get set to “run=couchbase”. \n Figure 9-41.  Creating a Replication Controller Imperatively \n List the replication controllers with the following command. \n kubectl get rc \n The  couchbase replication controller gets listed as shown in Figure  9-42 . \n Figure 9-42.  Listing the Replication Controllers \n Listing the Pods \n To list the  Pods run the following command. \n kubectl get pods \n The two pods get listed as shown in Figure  9-43 . \n \n \n",
      "content_length": 1105,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n267\n Figure 9-43.  Listing the Pods \n To describe any particular Pod run the  kubectl describe pod command, for example, the Pod \n couchbase-rd44o is described with the following command. \n kubectl describe pod couchbase-rd44o \n The Pod detail gets output as shown in Figure  9-44 . The Pod label is listed as  run=couchbase . \n Figure 9-44.  Describing a Pod \n \n \n",
      "content_length": 393,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n268\n Creating a Service \n To create a  service from the replication controller exposed at port 8091, run the following command, which \nalso specified the service type. \n kubectl expose rc couchbase --port=8091 --type=LoadBalancer \n Subsequently list the services. \n kubectl get services \n The  couchbase service gets created and listed as shown in Figure  9-45 . \n Figure 9-45.  Creating a Service for Couchbase Imperatively \n To describe the  couchbase service run the following command. \n kubectl describe svc couchbase \n The service name, namespace, labels, selector, type, Ip, port, node port, and endpoints get listed as \nshown in Figure  9-46 . Two endpoints are listed because the service manages two pods. \n \n",
      "content_length": 745,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n269\n Figure 9-46.  Describing a Service \n Scaling the Cluster \n A Couchbase cluster may be scaled up or down using the Kubernetes cluster manager. For example, to scale \ndown the replication controller called “couchbase” to 1 replica, run the following  kubectl scale command. \n kubectl scale rc couchbase --replicas=1 \n An output of “scaled” indicates that the rc has been scaled. But the “scaled” output does not always \nimply that the scaled number of replicas are running and ready. Run the following command to list the Pods. \n kubectl get pods \n A single Couchbase Pod gets listed as shown in Figure  9-47 . \n Figure 9-47.  Scaling Down the Couchbase Cluster to a Single Pod \n Run the following command to list the replication controllers and the  couchbase rc is listed with \nreplicas as 1 as shown in Figure  9-48 . \n kubectl get rc \n To scale the rc back to 2 Pods run the following command. \n kubectl scale rc couchbase --replicas=2 \n \n \n",
      "content_length": 976,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n270\n Subsequently list the Pods. \n kubectl get pods \n Initially the new Pod to be added could be not running or not ready but after a few seconds two Pods get \nlisted as running and ready as shown in Figure  9-48 . \n Figure 9-48.  Scaling Up the Couchbase Cluster \n Keeping the Replication Level \n The main purpose of a replication  controller is to keep the number of replicas to the configured level. With \n2 replicas configured in the  couchbase rc the number of Pods is maintained at 2. As an example, delete one \nof the Pods. \n kubectl delete pod couchbase-4z3hx \n One pod gets deleted, but it takes the total number of pods to 1, which is below the number of \nconfigured replicas. As a result the replication controller starts a new replica. Subsequently list the pods. \n kubectl get pods \n Initially the new Pod could be not running and/or not ready but after a few seconds two pods are \nrunning and ready as shown in Figure  9-49 . \n \n",
      "content_length": 971,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n271\n Figure 9-49.  Running the kubectl get pods Command Multiple Times until all Pods are Running and Ready \n Describe the  couchbase service. \n kubectl describe svc couchbase \n \n",
      "content_length": 207,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n272\n Two endpoints get listed as shown in Figure  9-50 . \n Figure 9-50.  Describing the couchbase Service \n Setting Port Forwarding \n Set  port forwarding of a service endpoint to a  localhost port, for example, port 8095, as discussed earlier. \n ssh -i \"docker.pem\" -f -nNT -L 8095:172.17.0.2:8091 ubuntu@ec2-52-91-80-177.compute-1.\namazonaws.com \n The preceding command does not generate any output as shown in Figure  9-51 . \n Figure 9-51.  Setting Port Forwarding \n Logging in to Couchbase Admin Console \n Login to the Couchbase  Web Console using the forwarded port on  localhost . \n http://localhost:8095/index.html \n The Couchbase Web Console gets displayed as shown in Figure  9-52 . \n \n \n",
      "content_length": 725,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "CHAPTER 9 ■ USING COUCHBASE\n273\n Figure 9-52.  Displaying the Couchbase Console \n Summary \n In this chapter we used Kubernetes cluster manager to create a Couchbase cluster. We discussed both the \ndeclarative and imperative approaches. The declarative approach makes use of definition files and the \nimperative approach makes use of command-line configuration parameters. We demonstrated accessing \nthe Couchbase Web Console from a localhost browser using port forwarding. We also used the cbtransfer \ntool in an interactive shell for a Docker container running Couchbase server. Docker image “couchbase” is \nused to create a Couchbase server. In the next chapter we shall discuss using Kubernetes cluster manager for \nan Apache Hadoop cluster. \n \n",
      "content_length": 748,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": " PART IV \n Apache Hadoop Ecosystem \n  \n",
      "content_length": 39,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "277\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_10\n CHAPTER 10 \n Using Apache Hadoop Ecosystem \n Apache Hadoop has evolved to be the de facto framework for processing large quantities of data. Apache \nHadoop ecosystem consists of a several projects including Apache Hive and Apache HBase. The Docker \nimage “svds/cdh” is based on the latest CDH release and includes all the main frameworks in the Apache \nHadoop ecosystem. All the frameworks such as Apache Hadoop, Apache Hive, and Apache HBase are \ninstalled in the same Docker image as a result facilitating development of applications that make use of \nmultiple frameworks from the Apache Hadoop ecosystem. In this chapter we shall discuss using Kubernetes \ncluster manager to manage a cluster of Pods based on the svds/cdh image.\n Setting the Environment \n Creating an Apache Hadoop Cluster Declaratively \n Creating an Apache Hadoop Cluster Imperatively \n  Setting the Environment \n The following software is required to be installed for this chapter, which is the same as the software used in \nother chapters except for the Docker image. \n -Docker Engine (latest version) \n -Kubernetes Cluster Manager (version 1.01) \n -Kubectl (version 1.01) \n -Docker image svds/cdh (latest version) \n Install the software as discussed in  chapter 1 on an Ubuntu instance on Amazon EC2. SSH Login to the \nUbuntu instance. \n ssh -i \"docker.pem\" ubuntu@54.86.45.173 \n Start the Docker engine with the following command. \n sudo service docker start \n Subsequently run the following command to verify the status of Docker. \n sudo service docker status \n",
      "content_length": 1644,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n278\n As shown in Figure  10-1 , Docker should be listed as “running.” \n Figure 10-1.  Starting Docker \n Figure 10-2.  Listing the “kubernetes” Service \n Figure 10-3.  Listing the Pod and Node for Kubernetes \n List the services with the following command. \n kubectl get services \n The  kubernetes service should be listed as running as shown in Figure  10-2 . \n List the Pods with the following command. \n kubectl get pods \n List the nodes with the following command. \n kubectl get nodes \n The only Pod that gets listed is for Kubernetes as shown in Figure  10-3 . The node 127.0.0.1 also gets listed.  \n  Creating an Apache Hadoop Cluster Declaratively \n In the following subsections we shall create a Kubernetes service and a Kubernetes replication controller \ndeclaratively using definition files. A service is the external interface for Pods and routes client requests to one of \nthe Pods. A replication controller manages the replication level of the Pods and maintains the number of replicas \nto the specified value in the definition file. The replication controller is also used to scale the cluster of Pods. \n \n \n \n",
      "content_length": 1165,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n279\n Creating a Service \n To run a  service for the CDH Pods create a service definition file  cdh-service.yaml and add the following \n(Table  10-1 ) fields to the definition file. \n Table 10-1.  Service Definition File Fields \n Field \n Description \n Value \n Required Field \n(including defaults) \n apiVersion \n v1 \n yes \n kind \n The kind of definition file. \n Service \n yes \n metadata \n The service metadata. \n yes \n metadata - > labels \n The service labels. \n app: cdh \n no \n metadata - > name \n The service name. \n cdh \n yes \n spec \n The service specification. \n yes \n spec - > ports \n The ports exposed by the service. \n yes \n spec - > ports - > port \n A port exposed by the service. The \n50010 port is for the DataNode. \n 50010 \n spec - > ports - > port \n Another port exposed by the service. \nThe 8020 port is for the NameNode. \n 8020 \n spec - > selector \n The Pod selector. Service routes traffic \nto the Pods with a label matching the \nselector expression. \n app: cdh \n yes \n spec - > selector - > type \n The service type. \n LoadBalancer \n no \n The service definition file  cdh-service.yaml is listed: \n apiVersion: v1 \n kind: Service \n metadata:  \n   labels:  \n    app: cdh \n  name: cdh \n spec:  \n   ports:  \n    -  \n      port: 50010 \n    -  \n      port: 8020 \n  selector:  \n    app: cdh \n   type: LoadBalancer \n",
      "content_length": 1364,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n280\n Figure 10-4.  Service Definition File in vi Editor \n Create a service from the definition file with the following command. \n kubectl create -f cdh-service.yaml \n Subsequently list the services. \n kubectl get services \n An output of “services/cdh” from the first command indicates that the service has been created as \nshown in Figure  10-5 . The second command lists the service called “cdh.” The service selector is listed as \napp = cdh in the SELECTOR column.  \n The service definition file may be created and saved in the vi editor as shown in Figure  10-4 . \n \n",
      "content_length": 613,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n281\n Figure 10-5.  Creating a Service from a Definition File \n Creating a Replication Controller \n In this section we shall create a replication  controller using a definition file. Create a cdh-rc.yaml file and \nadd the following (Table  10-2 ) fields to the file. \n Table 10-2.  Replication Controller Definition File Fields \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n ReplicationController \n metadata \n The replication controller metadata. \n metadata - > labels \n The replication controller labels. \n app: cdh \n metadata - > name \n The replication controller name. \n cdh-rc \n spec \n The replication controller specification. \n spec - > replicas \n The number of Pod replicas. \n 2 \n spec - > selector \n Selector key:value expression/s for \nselecting the Pods to manage. Pods \nwith label/s the same as the selector \nexpression/s are managed by the \nreplication controller. For a single selector \nexpression the selector expression must be \nthe same as a spec - > template - > \nmetadata - > labels label. The selector \ndefaults to the spec - > template - > \nmetadata - > labels if not specified. \n Not set. Defaults to the \nsame value as the key:value \npairs in spec - > template - > \nmetadata - > labels. \n spec - > template \n The Pod template. \n spec - > template- > metadata \n The Pod template metadata. \n spec \n- > template- > metadata- > labels \n The Pod template labels. \n app: cdh \n name: cdh \n spec - > template - > spec \n The Pod template specification \n(continued)\n \n",
      "content_length": 1563,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n282\n The definition file for the replication controller,  cdh-rc.yaml , is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata:  \n  labels:  \n    app: cdh \n  name: cdh-rc \n spec:  \n  replicas: 2 \n  template:  \n    metadata:  \n      labels:  \n      app: cdh \n      name: cdh \n    spec:  \n      containers:  \n      image: svds/cdh \n      name: cdh \n Run the following command to create a replication controller from the definition file. \n kubectl create -f cdh-rc.yaml \n List the replication controllers. \n kubectl get rc \n The first command outputs “replicationcontrollers/cdh,” which implies that an rc has been created \nsuccessfully. The second command lists the replication controllers. The replication controller “cdh” gets \nlisted as shown in Figure  10-6 . The  SELECTOR was not specified in the replication controller file and is \nlisted as the same two key:value pairs,  app=cdh,name=cdh , as the template labels. A Pod managed by the \nreplication controller must include both of these labels, and may include additional labels. The number of \nreplicas is set to 2. \n Field \n Description \n Value \n spec - > template \n- > spec- > containers \n The containers configuration for the \nPod template \n spec - > template \n- > spec- > containers - > image \n The Docker image \n svds/cdh \n spec - > template - > spec - > \ncontainers - > name \n The container name \n cdh \nTable 10-2. (continuted)\n",
      "content_length": 1448,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n283\n Listing the Pods \n To list the  Pods run the following command. \n kubectl get pods \n Two Pods get listed as shown in Figure  10-7 . Initially the Pods could be listed as not running or/and \nnot ready. A not ready pod is indicated by the 0/1 value in the READY column, which implies that 0 of 1 \ncontainers in the Pod are rready. \n Figure 10-6.  Creating a Replication Controller from a Definition File \n Figure 10-7.  Listing the Pods for CDH, created but not Ready \n Run the same command again to list the Pods. \n kubectl get pods \n The two Pods should get listed as STATUS- > Running and READY- > 1/1 as shown in Figure  10-8 . \n \n \n",
      "content_length": 683,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n284\n Listing Logs \n To list the  logs for a particular Pod, for example, the cdh-612pr Pod, run the following command. \n kubectl logs cdh-612pr \n The output from the command lists the logs, which indicate that the Hadoop datanode, namenode, \nsecondarynamenode, resourcemanager, and nodemanager have been started as shown in Figure  10-9 . \n Figure 10-8.  Listing the Pods as Ready \n Figure 10-9.  Listing Pod Logs \n \n \n",
      "content_length": 462,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n285\n Other components such as HBase are also started. \n Scaling a Cluster \n Initially the CDH cluster has 2 replicas. To scale the replicas to 4 run the following command. \n kubectl scale rc cdh --replicas=4 \n Subsequently list the Pods in the cluster. \n kubectl get pods \n After scaling up the cluster 4 Pods get listed instead of the 2 listed initially. Some of the Pods could be \nlisted as not running or not ready. Run the preceding command after a few seconds periodically, and all the \npods should get started as shown in Figure  10-10 . \n Figure 10-10.  Scaling the Pod Cluster \n \n",
      "content_length": 631,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n286\n Starting an Interactive Shell \n As the “svds/cdh” Docker image is based on the Linux “ubuntu” Docker image an interactive bash shell may \nbe started to access Docker containers based on the svds/cdh Docker image. To start an  interactive bash \nshell for the cdh software we need to obtain the container id for a Docker container running the “cdh” image \nas shown in Figure  10-11 . \n Figure 10-11.  Copying the Docker Container Id \n Subsequently start the interactive shell using the container id. \n sudo docker exec -it f1efdb5937c6 bash \n The interactive shell gets started as shown in Figure  10-12 . \n \n",
      "content_length": 655,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n287\n Running a MapReduce Application \n In this section we shall run an example  MapReduce application in the interactive shell. The  hdfs command \nis used to run a MapReduce application. Invoke the  hdfs command in the interactive shell. \n hdfs \n The command usage should get displayed as shown in Figure  10-13 . \n Figure 10-13.  Command Usage for  hdfs Command \n Figure 10-12.  Starting an Interactive Shell \n \n \n",
      "content_length": 458,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n288\n To change user to “hdfs” run the following command. \n su –l hdfs \n The user becomes “hdfs” as shown in Figure  10-14 . \n Figure 10-15.  Creating the Input Directory \n Figure 10-14.  Setting User as hdfs \n Next, we shall run a  wordcount application. We shall get input from the  /input directory files and \noutput in the  /output directory. Create the  /input directory and set its permissions to global (777). \n hdfs dfs -mkdir /input \n hdfs dfs -chmod -R 777 /input \n The  /input directory gets created and its permissions get set to global as shown in Figure  10-15 . \n Create an input file  input.1.txt in the vi editor. \n sudo vi input1.txt \n Add the following text to input1.txt. \n Hello World Application for Apache Hadoop \n Hello World and Hello Apache Hadoop \n The  input1.txt is shown in the  vi editor in Figure  10-16 . \n \n \n",
      "content_length": 885,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n289\n Put the  input1.txt in the HDFS directory  /input with the following command, which should be run with \n sudo –u hdfs if run as  root user. If the user is already set to “hdfs” omit the “sudo –u hdfs” from the command. \n sudo -u hdfs hdfs dfs -put input1.txt /input \n The  input1.txt file gets added to the  /input directory and no output is generated from the command \nas shown in Figure  10-17 . \n Figure 10-16.  Creating an Input Text File \n Figure 10-17.  Putting the Input Text File in HDFS \n \n \n",
      "content_length": 549,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n290\n Similarly create another file input2.txt. \n sudo vi input2.txt \n Add the following text to input2.txt. \n Hello World \n Hello Apache Hadoop \n Save the  input2.txt with the : wq command in the vi editor as shown in Figure  10-18 . \n Figure 10-18.  Creating another Text File input2.txt \n \n",
      "content_length": 335,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n291\n Put the  input2.txt into the  /input directory. \n sudo -u hdfs hdfs dfs -put input2.txt /input \n The  input2.txt also gets added to the  /input directory as shown in Figure  10-19 . \n Figure 10-19.  Putting the input2.txt File into HDFS \n The files in the  /input directory in the HDFS may be listed with the following command. \n hdfs dfs -ls /input \n The two files added  input1.txt and  input2.txt get listed as shown in Figure  10-20 . \n Figure 10-20.  Listing the Files in HDFS \n Next, run the  wordcount example application with the following command in which the jar file \ncontaining the example application is specified with the jar parameter and the  /input and  /output \ndirectories are set as the last two command parameters for the input directory and the output directory \nrespectively. \n sudo -u hdfs hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples-2.6.0-\ncdh5.4.7.jar wordcount /input /output \n \n \n",
      "content_length": 975,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n292\n The MapReduce job completes to run the  wordcount application . The output from the  wordcount \nMapReduce job, not the word count result, is shown in Figure  10-22 . \n Figure 10-21.  Starting a YARN Application for Word Count Example \n A MapReduce job gets started as shown in Figure  10-21 . \n \n",
      "content_length": 344,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n293\n A more detailed output from the MapReduce application is listed: \n root@cdh-6l2pr:/# sudo -u hdfs hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-\nexamples-2.6.0-cdh5.4.7.jar wordcount /input /output \n 15/12/21 16:39:52 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 \n 15/12/21 16:39:53 INFO input.FileInputFormat: Total input paths to process : 2 \n 15/12/21 16:39:53 INFO mapreduce.JobSubmitter: number of splits:2 \n 15/12/21 16:39:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: \njob_1450714825612_0002 \n 15/12/21 16:39:53 INFO impl.YarnClientImpl: Submitted application \napplication_1450714825612_0002 \n 15/12/21 16:39:53 INFO mapreduce.Job: The url to track the job: http://cdh-6l2pr:8088/proxy/\napplication_1450714825612_0002/ \n 15/12/21 16:39:53 INFO mapreduce.Job: Running job: job_1450714825612_0002 \n Figure 10-22.  Output from the MapReduce Job \n \n",
      "content_length": 944,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n294\n 15/12/21 16:39:59 INFO mapreduce.Job: Job job_1450714825612_0002 running in uber mode : \nfalse \n 15/12/21 16:39:59 INFO mapreduce.Job: map 0 % reduce 0 % \n 15/12/21 16:40:04 INFO mapreduce.Job: map 100 % reduce 0 % \n 15/12/21 16:40:10 INFO mapreduce.Job: map 100 % reduce 100 % \n 15/12/21 16:40:10 INFO mapreduce.Job: Job job_1450714825612_0002 completed successfully \n 15/12/21 16:40:10 INFO mapreduce.Job: Counters: 49 \n       File System Counters \n           FILE: Number of bytes read=144 \n           FILE: Number of bytes written=332672 \n           FILE: Number of read operations=0 \n           FILE: Number of large read operations=0 \n           FILE: Number of write operations=0 \n           HDFS: Number of bytes read=317 \n           HDFS: Number of bytes written=60 \n           HDFS: Number of read operations=9 \n           HDFS: Number of large read operations=0 \n           HDFS: Number of write operations=2 \n       Job Counters  \n           Launched map tasks=2 \n           Launched reduce tasks=1 \n           Data-local map tasks=2 \n           Total time spent by all maps in occupied slots (ms)=4939 \n           Total time spent by all reduces in occupied slots (ms)=2615 \n           Total time spent by all map tasks (ms)=4939 \n           Total time spent by all reduce tasks (ms)=2615 \n           Total vcore-seconds taken by all map tasks=4939 \n           Total vcore-seconds taken by all reduce tasks=2615 \n           Total megabyte-seconds taken by all map tasks=5057536 \n           Total megabyte-seconds taken by all reduce tasks=2677760 \n       Map-Reduce Framework \n           Map input records=5 \n           Map output records=17 \n           Map output bytes=178 \n           Map output materialized bytes=150 \n           Input split bytes=206 \n           Combine input records=17 \n           Combine output records=11 \n           Reduce input groups=7 \n           Reduce shuffle bytes=150 \n           Reduce input records=11 \n           Reduce output records=7 \n           Spilled Records=22 \n           Shuffled Maps =2 \n           Failed Shuffles=0 \n           Merged Map outputs=2 \n           GC time elapsed (ms)=158 \n           CPU time spent (ms)=2880 \n           Physical memory (bytes) snapshot=1148145664 \n           Virtual memory (bytes) snapshot=5006991360 \n           Total committed heap usage (bytes)=2472542208 \n",
      "content_length": 2401,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n295\n        Shuffle Errors \n           BAD_ID=0 \n           CONNECTION=0 \n           IO_ERROR=0 \n           WRONG_LENGTH=0 \n           WRONG_MAP=0 \n           WRONG_REDUCE=0 \n        File Input Format Counters  \n           Bytes Read=111 \n        File Output Format Counters  \n           Bytes Written=60 \n root@cdh-6l2pr:/#  \n Subsequently, list the files in the  /output directory. \n bin/hdfs dfs -ls /output \n Two files get listed:  _SUCCESS and  part-r-00000 as shown in Figure  10-23 . The  _SUCCESS file is to \nindicate that the MapReduce command completed successfully and the  part-r-00000 command contains \nthe result of the word count. \n Figure 10-24.  The Word Count for the Input Files \n Figure 10-23.  Listing the Files generated by the MapReduce Job \n To list the result of the  wordcount application run the following command. \n hdfs dfs -cat /output/part-r-00000 \n The word count for each of the words in the input gets listed as shown in Figure  10-24 . \n \n \n",
      "content_length": 1019,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n296\n  Running Hive \n Apache Hive is a data warehouse framework for storing, managing, and querying large data sets in HDFS. \nAs mentioned before all/most of the components of CDH get installed when the  svds/cdh image is run. In \nthis section we shall test the Apache Hive framework. The Hive configuration directory is in the Hive  conf \ndirectory, in the  /etc/hive directory. Change directory (cd) to the  /etc/hive directory. \n cd /etc/hive \n The  conf directory gets listed as shown in Figure  10-25 . \n Figure 10-26.  Listing the Hive Metastore Directory \n Figure 10-25.  Listing the Files and Directories in the Hive Root Directory \n The Hive metastore is kept in the  /var/lib/hive directory. Cd to the  /var/lib/hive directory. \n cd /var/lib/hive \n The  metastore directory gets listed as shown in Figure  10-26 . \n The Hive home directory is  /usr/lib/hive . Cd to the  /usr/lib/hive directory. Subsequently list the \nfiles and directories. \n cd /usr/lib/hive \n ls –l \n The  bin ,  conf, and  lib directories for Apache Hive get listed as shown in Figure  10-27 . The  bin \ndirectory contains the executables, the  conf directory the configuration files, and the  lib directory the \njar files. \n \n \n",
      "content_length": 1252,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n297\n All the environment variables are preconfigured. Run the following command to start the Beeline CLI. \n beeline \n Beeline version 1.1.0-cdh5.4.7 gets started as shown in Figure  10-28 . \n Figure 10-27.  The Hive Home Directory \n Figure 10-28.  Starting Beeline CLI \n Initially no connection to the Apache Hive server is available. To demonstrate, run the following \ncommands to set the database as default and show the tables. \n use default; \n show tables; \n The message “No current connection” is displayed as shown in Figure  10-29 . \n Figure 10-29.  No Current Connection \n Connect with Hive2 server using the default settings for the driver, username, and password as \nindicated by the three empty “”. \n !connect jdbc:hive2://localhost:10000/default \"\" \"\" \"\" \n \n \n \n",
      "content_length": 817,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n298\n Apache Hive2 server gets connected to using the Apache Hive JDBC driver as shown in Figure  10-30 . \n Figure 10-30.  Connecting with Hive Server \n Run the commands to set the database to default and show the tables. \n use default; \n show tables; \n The database connected to is already default, and the first command essentially is redundant but \nwhat is to be noted is the error generated earlier is not generated. The second command lists the table \nand because initially the default database does not have any tables, none get listed. The output from the \npreceding commands is shown in Figure  10-31 . \n Figure 10-31.  Setting the database to Use and the listing to the Hive Tables \n Figure 10-32.  Setting Permissions on the Hive Warehouse Directory \n Before creating a Hive table we need to set the permissions for the  /user/hive/warehouse directory to \nglobal (777). \n sudo –u hdfs hdfs dfs –chmod –R 777 /user/hive/warehouse \n Permissions for the Hive warehouse directory get set as shown in Figure  10-32 . \n Create a table called  wlslog with the following HiveQL command. \n CREATE TABLE wlslog(time_stamp STRING,category STRING,type STRING,servername STRING,code \nSTRING,msg STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'; \n The  wlslog table gets created in the default database as shown in Figure  10-33 . \n \n \n \n",
      "content_length": 1406,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n299\n Describe the wlslog table with the following command. \n desc wlslog; \n The table columns (name and data type) get listed as shown in Figure  10-34 . \n Figure 10-33.  Creating a Hive Table called wlslog \n Figure 10-34.  Describing the Hive Table wlslog \n Add 7 rows of data to the  wlslog table. \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:16-PM-PDT','Notice','WebLogicServer',\n'AdminServer,BEA-000365','Server state changed to STANDBY'); \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:17-PM-PDT','Notice','WebLogicServer',\n'AdminServer','BEA-000365','Server state changed to STARTING'); \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:18-PM-PDT','Notice','WebLogicServer',\n'AdminServer','BEA-000365','Server state changed to ADMIN'); \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:19-PM-PDT','Notice','WebLogicServer',\n'AdminServer','BEA-000365','Server state changed to RESUMING'); \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:20-PM-PDT','Notice','WebLogicServer',\n'AdminServer','BEA-000331','Started WebLogic AdminServer'); \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:21-PM-PDT','Notice','WebLogicServer',\n'AdminServer','BEA-000365','Server state changed to RUNNING'); \n INSERT INTO TABLE wlslog VALUES ('Apr-8-2014-7:06:22-PM-PDT','Notice','WebLogicServer',\n'AdminServer','BEA-000360','Server started in RUNNING mode'); \n A MapReduce job runs for each  INSERT statement to add the data to Hive table  wlslog as shown in \nFigure  10-35 . \n \n \n",
      "content_length": 1530,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n300\n Subsequently query the  wlslog table. \n select * from wlslog; \n The 7 rows of data added get listed as shown in Figure  10-36 . \n Figure 10-35.  Adding Data to Hive Table wlslog \n \n",
      "content_length": 229,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n301\n To quit the Beeline CLI run the following command. \n !q \n As shown in Figure  10-37 the Hive Beeline CLI gets exited. The interactive shell command prompt gets \ndisplayed. \n Figure 10-36.  Querying the Hive Table \n Figure 10-37.  Exiting the Beeline CLI \n From the interactive shell any of the frameworks in CDH may be run. Next, we shall run Apache HBase.  \n \n \n",
      "content_length": 411,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 316,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n302\n  Running HBase \n Apache HBase is the Apache Hadoop database, which also stores data in HDFS by default. To start the HBase \nshell run the following command from a bash shell for a Docker container based on the svds/cdh Docker image. \n hbase shell \n HBase shell gets started as shown in Figure  10-38 . \n Figure 10-38.  Starting HBase Shell \n Create a table called ‘wlslog’ with column family ‘log’. \n create 'wlslog' , 'log' \n The  wlslog table gets created as shown in Figure  10-39 . \n Figure 10-39.  Creating a HBase Table \n Put 7 rows of data into the  wlslog table. \n put 'wlslog', 'log1', 'log:time_stamp', 'Apr-8-2014-7:06:16-PM-PDT' \n put 'wlslog', 'log1', 'log:category', 'Notice' \n put 'wlslog', 'log1', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log1', 'log:servername', 'AdminServer' \n put 'wlslog', 'log1', 'log:code', 'BEA-000365' \n put 'wlslog', 'log1', 'log:msg', 'Server state changed to STANDBY' \n put 'wlslog', 'log2', 'log:time_stamp', 'Apr-8-2014-7:06:17-PM-PDT' \n put 'wlslog', 'log2', 'log:category', 'Notice' \n put 'wlslog', 'log2', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log2', 'log:servername', 'AdminServer' \n put 'wlslog', 'log2', 'log:code', 'BEA-000365' \n put 'wlslog', 'log2', 'log:msg', 'Server state changed to STARTING' \n put 'wlslog', 'log3', 'log:time_stamp', 'Apr-8-2014-7:06:18-PM-PDT' \n put 'wlslog', 'log3', 'log:category', 'Notice' \n \n \n",
      "content_length": 1436,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n303\n put 'wlslog', 'log3', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log3', 'log:servername', 'AdminServer' \n put 'wlslog', 'log3', 'log:code', 'BEA-000365' \n put 'wlslog', 'log3', 'log:msg', 'Server state changed to ADMIN' \n put 'wlslog', 'log4', 'log:time_stamp', 'Apr-8-2014-7:06:19-PM-PDT' \n put 'wlslog', 'log4', 'log:category', 'Notice' \n put 'wlslog', 'log4', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log4', 'log:servername', 'AdminServer' \n put 'wlslog', 'log4', 'log:code', 'BEA-000365' \n put 'wlslog', 'log4', 'log:msg', 'Server state changed to RESUMING' \n put 'wlslog', 'log5', 'log:time_stamp', 'Apr-8-2014-7:06:20-PM-PDT' \n put 'wlslog', 'log5', 'log:category', 'Notice' \n put 'wlslog', 'log5', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log5', 'log:servername', 'AdminServer' \n put 'wlslog', 'log5', 'log:code', 'BEA-000331' \n put 'wlslog', 'log5', 'log:msg', 'Started Weblogic AdminServer' \n put 'wlslog', 'log6', 'log:time_stamp', 'Apr-8-2014-7:06:21-PM-PDT' \n put 'wlslog', 'log6', 'log:category', 'Notice' \n put 'wlslog', 'log6', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log6', 'log:servername', 'AdminServer' \n put 'wlslog', 'log6', 'log:code', 'BEA-000365' \n put 'wlslog', 'log6', 'log:msg', 'Server state changed to RUNNING' \n put 'wlslog', 'log7', 'log:time_stamp', 'Apr-8-2014-7:06:22-PM-PDT' \n put 'wlslog', 'log7', 'log:category', 'Notice' \n put 'wlslog', 'log7', 'log:type', 'WeblogicServer' \n put 'wlslog', 'log7', 'log:servername', 'AdminServer' \n put 'wlslog', 'log7', 'log:code', 'BEA-000360' \n put 'wlslog', 'log7', 'log:msg', 'Server started in RUNNING mode' \n",
      "content_length": 1653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n304\n To list the tables run the following command. \n list \n The  wlslog table gets listed as shown in Figure  10-41 . \n Figure 10-40.  Putting Data into HBase Table \n The output from the put commands is shown in Figure  10-40 . \n Figure 10-41.  Listing HBase Tables \n \n \n",
      "content_length": 314,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n305\n To get the data in row with row key ‘log1’ run the following command. \n get 'wlslog', 'log1' \n A single row of data gets listed as shown in Figure  10-42 . \n Figure 10-42.  Getting a Single Row of Data \n Figure 10-43.  Getting a Single Column Value in a Row \n Get the data in a single column, the  log.msg column from row with row key  log7 . A column is specified \nwith column family:column format. \n get 'wlslog', 'log7', {COLUMNS=>['log:msg']} \n The single column data gets output as shown in Figure  10-43 . \n Scan the  wlslog table with the  scan command. \n scan 'wlslog' \n \n \n",
      "content_length": 630,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n306\n The scan command is shown in Figure  10-44 . \n Figure 10-44.  Scanning a HBase Table \n All the data from the  wlslog table gets listed as shown in Figure  10-45 .  \n \n",
      "content_length": 215,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n307\n Deleting the Replication Controller and Service \n In the next section we shall create a cluster for the  svds/cdh image imperatively on the command line. \nDelete the replication controller and the service created declaratively. \n kubectl delete rc cdh \n kubectl delete service cdh \n Creating an Apache Hadoop Cluster Imperatively \n In the following subsections we shall create a CDH cluster from the  svds/cdh Docker image on the \ncommand line. First, we shall create a replication controller. \n Creating a Replication Controller \n Run the following command to create a replication controller called  cdh with 2 replicas. \n kubectl run cdh --image=svds/cdh --replicas=2  \n Figure 10-45.  The scan Command outputs 7 Rows of Data \n \n",
      "content_length": 779,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n308\n The  cdh controller gets created as shown in Figure  10-46 . The selector is set to  run=cdh by default. \n Figure 10-46.  Creating a Replication Controller Imperatively \n List the replication controllers. \n kubectl get rc \n The  cdh replication controller gets listed as shown in Figure  10-47 . \n Figure 10-47.  Getting the Replication Controller \n Figure 10-48.  Listing the Pods with some Pod/s not READY yet \n Listing the Pods \n To list the Pods in the cluster run the following command. \n kubectl get pods \n The two Pods get listed. Initially some or all of the Pods could be not “Running” or not in the READY \nstate 1/1 as shown in Figure  10-48 . \n Run the preceding command again after a few seconds. \n kubectl get pods \n All the pods should be listed with STATUS “Running” and READY state 1/1 as shown in Figure  10-49 . \n Figure 10-49.  Listing all Pods as Running and Ready \n \n \n \n \n",
      "content_length": 942,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n309\n Scaling a Cluster \n To scale the cluster to 4 replicas run the following command. \n kubectl scale rc cdh --replicas=4 \n Subsequently list the Pods. \n kubectl get pods \n An output of “scaled” from the first command indicates that the cluster got scaled. The second \ncommand lists 4 Pods instead of the 2 created initially as shown in Figure  10-50 . The second command may \nhave to be run multiple times to list all Pods with STATUS “Running” and READY state 1/1. \n Figure 10-50.  Scaling the CDH Cluster \n Creating a Service \n A service exposes the Pods managed by the replication controller at service endpoints, which are just \nhost:port settings at which external clients may invoke the application. Run the following command to \ncreate a service. \n kubectl expose rc cdh --type=LoadBalancer \n \n",
      "content_length": 846,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n310\n Subsequently list the services. \n kubectl get services \n The “cdh” service gets listed with default settings for SELECTOR and PORT as shown in Figure  10-51 . \nThe default service selector is  run=cdh , which has the default format run =<servicename>. The default port is \n8020. \n Figure 10-51.  Creating a Service \n Figure 10-52.  Starting an Interactive Shell \n Starting an Interactive Shell \n The interactive shell may be started just as for a CDH cluster started declaratively. Copy the container id for \na Docker container running the CDH image and run the following command, which includes the container \nid, to start an interactive bash shell. \n sudo docker exec -it 42f2d8f40f17 bash \n The interactive shell gets started as shown in Figure  10-52 . \n Run the  hdfs command. \n hdfs \n The  hdfs command usage gets output as shown in Figure  10-53 . \n \n \n",
      "content_length": 908,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "CHAPTER 10 ■ USING APACHE HADOOP ECOSYSTEM\n311\n Figure 10-53.  Command Usage for hdfs Command \n Summary \n In this chapter we used the Kubernetes cluster manager to create a cluster of Pods based on the Docker \nimage  svds/cdh . We used both the declarative and imperative approaches to create the cluster. We scaled \nthe cluster using the kubectl scale command. We also demonstrated using some of the Apache Hadoop \nframeworks packaged in the  cdh image. We ran a MapReduce  wordcount example application. We also ran \nthe Apache Hive and Apache HBase tools. In the next chapter we shall discuss using Kubernetes with the \nindexing and storage framework Apache Solr. \n \n",
      "content_length": 670,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 326,
      "content": "313\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_11\n CHAPTER 11 \n Using Apache Solr \n Apache Solr is an Apache Lucene-based enterprise search platform providing features such as full-text \nsearch, near real-time indexing, and database integration. Apache Solr runs as a full-text search server \nwithin a servlet container, the default being Jetty, which is included with the Solr installation. In this chapter \nwe shall discuss using Kubernetes cluster manager with Apache Solr. We shall be using only the declarative \napproach, which makes use of definition files, for creating and managing a Solr cluster. This chapter has the \nfollowing sections.\n Setting the Environment \n Creating a Service \n Listing Service Endpoints \n Describing the Service \n Creating a Replication Controller \n Listing the Pods \n Describing a Pod \n Listing the Logs \n Starting an Interactive Shell \n Creating a Solr Core \n Adding Documents \n Accessing Solr on Command Line with a REST Client \n Setting Port Forwarding \n Accessing Solr in Admin Console \n Scaling the Cluster \n",
      "content_length": 1105,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n314\n Setting the Environment \n The following software is required for this chapter. \n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n -Docker image for Apache Solr (latest version) \n We have used the same Amazon EC2 instance AMI as in the other chapters. SSH login to the Ubuntu \ninstance from a local machine. \n ssh -i \"docker.pem\" ubuntu@54.152.82.142 \n Install the required software as discussed in chapter  1 . Start Docker and verify its status. \n sudo service docker start \n sudo service docker status \n As shown in Figure  11-1 Docker should be running. \n List the services. \n kubectl get services \n As shown in Figure  11-2 Kubernetes service should be running. \n To list the nodes run the following command. \n kubectl get nodes \n Figure 11-1.  Starting Docker and Verifying Status \n Figure 11-2.  Listing the “kubernetes” Service \n \n \n",
      "content_length": 916,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n315\n The 127.0.0.1 node gets listed as shown in Figure  11-3 . \n List the endpoints with the following command. \n kubectl get endpoints \n Initially only the endpoint for kubernetes is listed as shown in Figure  11-4 . \n  Creating a Service \n Create a definition file  solr-service.yaml and add the following (Table  11-1 ) fields to the definition file. \n Figure 11-3.  Listing a Single Node \n Figure 11-4.  Listing “kubernetes” Endpoint \n Table 11-1.  Service Definition File for Apache Solr \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n Service \n metadata \n The service metadata. \n metadata - > labels \n The service labels. Not required. \n app: solrApp \n metadata - > name \n The service name. Required. \n solr-service \n spec \n The service specification. \n spec - > ports \n The ports exposed by the service. \n spec - > ports- > port \n A port exposed by the service. \n 8983 \n spec - > ports- > targetPort \n The target port. \n 8983 \n spec - > selector \n The Pod selector. Service routes traffic to the Pods \nwith a label matching the selector expression. \n app: solrApp \n \n \n",
      "content_length": 1147,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n316\n The  solr-service.yaml is listed. \n apiVersion: v1 \n kind: Service \n metadata:  \n  labels:  \n    app: solrApp \n  name: solr-service \n spec:  \n  ports:  \n    -  \n      port: 8983 \n      targetPort: 8983\n  selector: \n    app: solrApp \n The  solr-service.yaml may be edited in the vi editor and saved with :wq as shown in Figure  11-5 . \n Figure 11-5.  Service Definition File in vi Editor \n \n",
      "content_length": 426,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n317\n Create a service from the definition file with the following command. \n kubectl create -f solr-service.yaml   \n Subsequently list the services. \n kubectl get services \n An output of “services/solr-service” as shown in Figure  11-6 indicates that the service has been created. \nSubsequently the  solr-service gets listed. The service has label  app=solrApp and selector  app=solrApp . \n Listing Service Endpoints \n To list the endpoints run the following command. \n kubectl get endpoints \n As the  solr-service is not managing any Pods initially, no endpoint gets listed as shown in Figure  11-7 . \n Figure 11-6.  Creating a Service from Definition File \n Figure 11-7.  Listing the Endpoint for the Solr Service \n Describing the Service \n To describe the  solr-service run the following command. \n kubectl describe service solr-service \n The service name, namespace, labels, selector, type, IP, Port, endpoints, and events get listed as shown \nin Figure  11-8 . \n \n \n",
      "content_length": 1002,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n318\n  Creating a Replication Controller \n Create a definition file  solr-rc.yaml for the replication controller and add the following (Table  11-2 ) fields \nto the definition file. \n Figure 11-8.  Describing the Apache Solr Service \n Table 11-2.  Replication Controller Definition File Fields \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n Replication Controller \n metadata \n The replication controller metadata. \n metadata - > labels \n The replication controller labels. \n app: solrApp \n metadata - > name \n The replication controller name. \n solr-rc \n spec \n The replication controller specification. \n spec - > replicas \n The number of Pod replicas. \n 2 \n spec - > selector \n A key: value expression for selecting \nthe Pods to manage. Pods with a label \nthe same as the selector expression \nare managed by the replication \ncontroller. For a single label/\nselector expression Pod/Replication \nController combination the selector \nexpression must be the same as the \nspec- > template- > metadata- > labels \nexpression. The selector defaults to the \nspec- > template- > metadata- > labels \nnot specified. The app: solrApp setting \ntranslates to app=solrApp. \n app: solrApp \n spec - > template \n The Pod template. \n spec - > template - > metadata \n The Pod template metadata. \n(continued)\n \n",
      "content_length": 1362,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n319\n The  solr-rc.yaml is listed. \n apiVersion: v1 \n kind: ReplicationController \n metadata:  \n  labels:  \n    app: solrApp \n  name: solr-rc \n spec:  \n  replicas: 2 \n  selector:  \n    app: solrApp \n  template:  \n    metadata:  \n      labels:  \n        app: solrApp \n    spec:  \n      containers:  \n        -  \n          image: solr \n          name: solr \n          ports:  \n            -  \n              containerPort: 8983 \n              name: solrApp \n The  solr-rc.yaml definition file may be created and saved in vi editor as shown in Figure  11-9 . \n Field \n Description \n Value \n spec - > template- > metadata- > labels \n The Pod template labels. \n app: solrApp \n spec - > template - > spec \n The Pod template specification. \n spec - > template - > spec - > containers \n The containers configuration for the Pod \ntemplate. \n spec - > template - > spec - > containers \n- > image \n The Docker image. \n solr \n spec - > template - > spec - > containers \n- > name \n The container name. \n solr \n spec - > template - > spec - > containers \n- > ports \n Container ports. \n spec - > template - > spec - > containers \n- > ports - > containerPort \n Container port for Solr server. \n 8983 \n spec - > template - > spec - > containers \n- > ports - > name \n Solr port name. \n solrApp \nTable 11-2. (continued)\n",
      "content_length": 1330,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n320\n Run the following command to create a replication controller from the definition file. \n kubectl create -f solr-rc.yaml   \n The  solr-rc replication controller gets created as shown in Figure  11-10 . Subsequently list the \nreplication controllers. \n kubectl get rc \n The  solr-rc replication controller gets listed as shown in Figure  11-10 . \n Figure 11-9.  Replication Controller Definition File in vi Editor \n Figure 11-10.  Creating a Replication Controller from Definition File \n \n \n",
      "content_length": 525,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n321\n Listing the Pods \n List the Pods with the following command. \n kubectl get pods \n The two Pods created by the replication controller get listed as shown in Figure  11-11 . Initially some of \nthe Pods could be not running and not ready. \n Run the same command again after a few seconds to list the Pods again. \n kubectl get pods \n The Pods should get listed with STATUS “Running” and READY state 1/1 as shown in Figure  11-12 . \n To describe the  solr-service run the following command. \n kubectl describe svc solr-service \n The service description gets listed as shown in Figure  11-13 . The service endpoints for the two Pods are \nalso listed. A service is accessed at its endpoints. When described previously, before creating the replication \ncontroller, no service endpoints got listed as shown in Figure  11-8 . \n Figure 11-11.  Listing the Pods, all of them not yet Ready \n Figure 11-12.  Listing the Pods as Ready \n \n \n",
      "content_length": 961,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n322\n The endpoints may also be listed separately. \n kubectl get endpoints  \n The endpoints get listed as shown in Figure  11-14 . \n Describing a Replication Controller \n To describe the replication controller  solr-rc run the following command. \n kubectl describe rc solr-rc \n The replication controller description gets listed as shown in Figure  11-15 . \n Figure 11-13.  Describing the Solr Service including the Service Endpoints \n Figure 11-14.  Listing the Endpoints for Solr Service \n \n \n",
      "content_length": 525,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n323\n Listing the Logs \n To list the logs for a particular command run the  kubectl logs command. For example, logs for the \n solr-rc-s82ip Pod are listed with the following command. \n kubectl logs solr-rc-s82ip \n In the log output the Solr server is starting as shown in Figure  11-16 . \n Figure 11-15.  Describing the Replication Controller \n \n",
      "content_length": 376,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n324\n After the server has started the output “Server Started” gets output as shown in Figure  11-17 . \n Figure 11-16.  Listing Logs for the Pod \n \n",
      "content_length": 178,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 338,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n325\n  Starting an Interactive Shell \n As the “solr” Docker image inherits from the “java:openjdk-8-jre” Docker image, which further inherits from \nthe “buildpack-deps:jessie-curl” image, which inherits from Docker image “debian” for Linux an interactive \nbash shell may be started to access a Docker container based on the “solr” Docker image. To access the Solr \nsoftware we need to start an interactive bash shell for a Docker container running Solr. Obtain the container \nif for a Docker container running Solr with the following command. \n sudo docker ps \n The Docker containers get listed as shown in Figure  11-18 . \n Figure 11-17.  Listing the Solr Server as started \n \n",
      "content_length": 708,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 339,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n326\n Copy the container if and start an interactive shell. \n sudo docker exec -it 2d4d7d02c05f bash \n The interactive shell gets started as shown in Figure  11-19 . To list the status of the Solr server run the \nfollowing command. \n bin/solr status \n Figure 11-18.  Listing the Docker Container for Apache Solr \n \n",
      "content_length": 345,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 340,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n327\n One Solr node is found as shown in Figure  11-19 . \n Solr 5.x introduce  configsets . The configsets directory consists of example configurations that may be \nused as a base to create new Solr cores or collections. The configsets replace the  collection1 example core \nconfiguration in Solr 4.x. Cd (change directory) to the  configsets directory. \n cd /opt/solr/server/solr/configsets \n List the files and directories in the  configsets directory. \n ls –l \n Three example configurations get listed as shown in Figure  11-20 . \n When we create a Solr core later in the chapter we shall be using the basic_configs configuration. \nList the files in the  //configsets/ basic_configs/conf directory. \n cd conf \n ls –l \n Figure 11-19.  Listing the Solr Status in an Interactive Shell for the Docker Container \n Figure 11-20.  Listing the Example Configurations \n \n \n",
      "content_length": 897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 341,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n328\n The configuration files for  basic_configs example get listed and include the  schema.xml and \nsolrconfig.xml as shown in Figure  11-21 . \n Creating a Solr Core \n A new Solr core may also be created from the command line. The  solr create command is used to create a \nnew core or a collection. As an example, create a core called  wlslog with the  solr create_core command. \nUse the configset  basic_configs with the  –d option. The default config set used if none is specified (with the  –d \noption) is  data_driven_schema_configs . Cd to the  /opt/solr directory and run the following command. \n bin/solr create_core -c wlslog -d /opt/solr/server/solr/configsets/basic_configs \n A Solr core called  wlslog gets created as shown in Figure  11-22 . \n Figure 11-21.  Listing the Configuration Files in the basic_configs Example Configuration \n Figure 11-22.  Creating a Solr Core called wlslog \n \n \n",
      "content_length": 934,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 342,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n329\n  Indexing Documents \n Apache Solr provides the  post tool for indexing documents from the command line. The  post tool supports \ndifferent input file formats such as XML, CSV and JSON. We shall index an XML format document Save the \nfollowing XML document to the  wlslog.xml file. \n <add> \n <doc> \n <field name=\"id\">wlslog1</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:16-PM-PDT</field> \n  <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code_s\">BEA-000365</field> \n  <field name=\"msg_s\">Server state changed to STANDBY</field> \n </doc> \n <doc> \n <field name=\"id\">wlslog2</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:17-PM-PDT</field> \n  <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code\">BEA-000365</field> \n  <field name=\"msg_s\">Server state changed to STARTING</field> \n </doc> \n <doc> \n <field name=\"id\">wlslog3</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:18-PM-PDT</field> \n  <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code\">BEA-000365</field> \n  <field name=\"msg_s\">Server state changed to ADMIN</field> \n </doc> \n <doc> \n <field name=\"id\">wlslog4</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:19-PM-PDT</field> \n  <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n",
      "content_length": 1604,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 343,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n330\n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code\">BEA-000365</field> \n  <field name=\"msg_s\">Server state changed to RESUMING</field> \n </doc> \n <doc> \n <field name=\"id\">wlslog5</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:20-PM-PDT</field> \n  <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code\">BEA-000331</field> \n  <field name=\"msg_s\">Started WebLogic AdminServer</field> \n </doc> \n <doc> \n <field name=\"id\">wlslog6</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:21-PM-PDT</field> \n   <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code\">BEA-000365</field> \n  <field name=\"msg_s\">Server state changed to RUNNING</field> \n </doc> \n <doc> \n <field name=\"id\">wlslog7</field> \n  <field name=\"time_stamp_s\">Apr-8-2014-7:06:22-PM-PDT</field> \n  <field name=\"category_s\">Notice</field> \n  <field name=\"type_s\">WebLogicServer</field> \n  <field name=\"servername_s\">AdminServer</field> \n  <field name=\"code\">BEA-000360</field> \n  <field name=\"msg_s\">Server started in RUNNING mode</field> \n </doc> \n </add> \n The  wlslog.xml file may be created in the vi editor and saved with the :wq command as shown in \nFigure  11-23 . \n",
      "content_length": 1400,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 344,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n331\n Cd to the  /opt/solr directory and run the post tool to add the documents in the  wlslog.xml file to \nSolr server. \n bin/post -c wlslog ./wlslog.xml \n Figure 11-23.  The wlslog.xml File \n \n",
      "content_length": 225,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 345,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n332\n One file gets indexed as shown in Figure  11-24 . \n  Accessing Solr on Command Line with a REST Client \n Solr request handler commands such as  /update ,  /select may be run using a REST client such as curl and \nwget. In this section we shall use the curl tool to run some of the  /select request handler commands. \nFor example, query all documents using the following curl command. \n curl http://localhost:8983/solr/wlslog/select?q=*%3A*&wt=json&indent=true  \n The curl command is shown in Figure  11-25 . \n Figure 11-24.  Posting the wlslog.xml File to the Solr Index \n \n",
      "content_length": 609,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 346,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n333\n The 7 documents added get listed as shown in Figure  11-26 . \n Figure 11-25.  Using curl to send a Request to Solr Server with Request Handler /select \n \n",
      "content_length": 190,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 347,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n334\n As another example run the  /select request handler to query for the document with id  wlslog7 . \n curl http://localhost:8983/solr/wlslog/select?q=id:wlslog7&wt=json&indent=true \n The document for id  wlslog7 gets listed as shown in Figure  11-27 . \n Figure 11-26.  Listing the Documents returned by the /select Request Handler \n \n",
      "content_length": 367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 348,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n335\n Documents may be deleted with the  post tool. For example, delete a document with id  wlslog1 using \nthe following command. \n bin/post -c wlslog -d \"<delete><id>wlslog1</id></delete>\" \n The document with id  wlslog1 gets deleted as shown in Figure  11-28 . \n Subsequently run the following curl command to list the documents in the  wlslog index. \n curl http://localhost:8983/solr/wlslog/select?q=*%3A*&wt=json&indent=true \n The document with id  wlslog1 does not get listed as shown in Figure  11-29 . \n Figure 11-27.  Querying for a Single Document with id wlslog7 using /select Request Handler and curl \n Figure 11-28.  Deleting a Document using post Tool \n \n \n",
      "content_length": 700,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 349,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n336\n The  /update request handler may be used to delete documents as in the following curl command, \nwhich deletes all documents in the  wlslog core. \n curl http://localhost:8983/solr/wlslog/update --data '<delete><query>*:*</query></delete>' \n-H 'Content-type:text/xml; charset=utf-8' \n Figure 11-29.  Querying after Deleting a Document \n \n",
      "content_length": 372,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 350,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n337\n If auto commit has not been configured the following curl command must be run to commit the \nchanges. \n curl http://localhost:8983/solr/wlslog/update --data '<commit/>' -H 'Content-type:text/xml; \ncharset=utf-8' \n Subsequently run the curl command to invoke the  /select request handler. \n curl http://localhost:8983/solr/wlslog/select?q=*%3A*&wt=json&indent=true \n No document gets listed as all have been deleted as shown in Figure  11-30 . \n Setting Port Forwarding \n If we were running Kubernetes on a local machine we could have opened the Solr Admin Console with url \n http://localhost:8983 but because we are using Amazon EC2 instance we need to set port forwarding \non a local machine with a web browser from localhost:8983 to 172.17.0.2:8983. Set port forwarding from \n localhost port 8983 with the following command run from a local machine. \n ssh -i key-pair-file -f -nNT -L 8983:172.17.0.2:8983 ubuntu@ec2-54-152-82-142.compute-1.\namazonaws.com \n Figure 11-30.  Deleting all Documents in Solr Index with /update \n",
      "content_length": 1061,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 351,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n338\n The preceding command forwards the  localhost:8983 URL to endpoint 172.17.0.2:8983 as shown \nin Figure  11-31 . \n Accessing Solr in Admin Console \n After port forwarding the Solr  Admin Console may be accessed from the local machine using the url \n http://localhost:8983 as shown in Figure  11-32 . Select the  wlslog core in the Core Selector as shown in \nFigure  11-32 . \n Select the Documents tab and set Document Type as XML for the  /update Request handler as shown in \nFigure  11-33 . Copy and paste the XML document wlslog.xml listed earlier in the Document (s) field and click \non Submit Document. \n Figure 11-31.  Setting Port Forwarding to localhost \n Figure 11-32.  Displaying the Solr Admin Console \n \n \n",
      "content_length": 752,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 352,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n339\n An output of “success” as shown in Figure  11-34 indicates that the documents got indexed. \n Figure 11-33.  Adding Document to the wlslog Core \n Figure 11-34.  Response from adding Documents \n \n \n",
      "content_length": 232,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 353,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n340\n Next, we shall query the  wlslog index. Select the Query tab as shown in Figure  11-35 . \n Figure 11-35.  Selecting the Query Tab \n With the Request Handler as  /select the query is “*:*” by default as shown in Figure  11-36 . \n \n",
      "content_length": 266,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 354,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n341\n Click on Execute Query as shown in Figure  11-37 . \n Figure 11-36.  Using the Request Handler /select to Query Solr index wlslog \n Figure 11-37.  Submitting a Query to select all Documents in the wlslog Index \n \n \n",
      "content_length": 250,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 355,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n342\n Because we have not set auto commit the documents added have not yet been indexed. As a result no \ndocument gets listed as shown in Figure  11-38 . \n We need to reload the core for the added documents to get indexed. Alternatively we could restart the \nSolr server but reloading the core is a quicker option. Select Core Admin and click on Reload as shown in \nFigure  11-39 . \n Figure 11-38.  Response from the Query \n \n",
      "content_length": 456,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 356,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n343\n Run the query again and as shown in Figure  11-40 the 7 documents added get listed. \n Figure 11-40.  Query Response with 7 Documents \n Figure 11-39.  Reloading the Core \n \n \n",
      "content_length": 210,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 357,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n344\n The  _version_ field has been added to each document automatically by the Solr server as shown in \nFigure  11-41 . \n Scaling the Cluster \n To  scale the Solr pod cluster run the  kubectl scale command. For example, to scale to 4 Pods set \nreplicas as 4. \n kubectl scale rc solr-rc --replicas=4 \n An output of “scaled” indicates that the Solr cluster has been scaled. Subsequently run the following \ncommand to list the Pods. \n kubectl get pods \n The number of Pods listed is 4 instead of the 2 to start with as shown in Figure  11-42 . Some of the Pods \ncould be not running or not ready initially. \n Figure 11-41.  The _version_ Field is added to each Document stored in Solr Index Automatically by the \nSolr Server \n \n",
      "content_length": 756,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 358,
      "content": "CHAPTER 11 ■ USING APACHE SOLR\n345\n Summary \n Apache Solr is an indexing and search engine that makes use of the local filesystem to store data. In this \nchapter we used Docker image “solr” with Kubernetes cluster manage to create and manage a cluster of Solr \ninstances. We demonstrated accessing a Solr instance from an interactive shell for a Docker container and \nalso using the Admin Console. In the next chapter we shall use Kubernetes with Apache Kafka. \n Figure 11-42.  Scaling the Apache Solr Cluster to 4 Pods \n \n",
      "content_length": 523,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 359,
      "content": "347\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_12\n CHAPTER 12 \n Using Apache Kafka \n Apache Kafka is publish-subscribe, high throughput, distributed messaging system. A single broker in Kafka \ncould handle 100s MB (Terabytes)/sec of reads & writes from multiple clients. Messages are replicated \nacross the cluster and persisted to disk. Kafka could be used for stream processing, web site activity tracking, \nmetrics collection, and monitoring and log aggregation. \n The main components of Kafka architecture are Producer, Broker, Topic, and Consumer. Kafka keeps \nfeeds of messages in topics. Producers send (or write) messages to topics and Consumers consume \n(or read) messages from topics. Messages are byte arrays of data and could be in any format with String, \nJSON, and Avro being the most common. Messages are retained for a specified amount of time. A Zookeeper \ncoordinates the Kafka cluster. In a single producer–consumer architecture, a single Producer sends messages \nto a Topic and a single Consumer consumes messages from the topic. \n Kafka is similar to Flume in that it streams messages, but Kafka is designed for a different purpose. \nWhile Flume is designed to stream messages to a sink such as HDFS or HBase, Kafka is designed for \nmessages to be consumed by multiple applications. \n In this chapter we shall discuss using Kubernetes cluster manager with Apache Kafka.\n Setting the Environment \n Modifying the Docker Image \n Creating a Service \n Creating a Replication Controller \n Listing the Pods \n Describing a Pod \n Starting an Interactive Shell \n Starting the Kafka Server \n Creating a Topic \n Starting a Kafka Producer \n Starting a Kafka Consumer \n Producing and Consuming Messages \n Scaling the Cluster \n Deleting Replication Controller and Service \n",
      "content_length": 1835,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 360,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n348\n Setting the Environment \n We have used an Amazon EC2 instance created from AMI Ubuntu Server 14.04 LTS (HVM), SSD Volume \nType - ami-d05e75b8. The following software is required for this chapter. \n -Docker Engine (latest version) \n -Kubernetes Cluster Manager (version 1.01) \n -Kubectl (version 1.01) \n -Docker image dockerkafka/kafka (latest version) \n We have used the Docker image  dockerkafka/kafka in this chapter. The default settings of the \n dockerkafka/kafka image Dockerfile are not suitable for orchestration with Kubernetes. In the next section \nwe have modified and rebuilt the default Docker image. First, connect with the Ubuntu instance using the \nPublic IP Address for the Amazon EC2 instance. \n ssh -i \"docker.pem\" ubuntu@54.146.140.160 \n The Ubuntu instance gets connected to as shown in Figure  12-1 . \n Install the required software as discussed in chapter  1 . Start the Docker service and find its status. \n sudo service docker start \n sudo service docker status \n Figure 12-1.  Connecting to an Ubuntu Instance on Amazon EC2 \n \n",
      "content_length": 1089,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 361,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n349\n Docker should be listed as running as shown in Figure  12-2 . \n List the Kubernetes services. \n kubectl get services \n The “kubernetes” service should be listed as shown in Figure  12-3 . \n  Modifying the Docker Image \n The procedure to start Apache Kafka involves the following sequence. \n \n 1. \n Start Zookeeper Server \n \n 2. \n Start Apache Kafka Server \n The Apache Kafka Server has a dependency on Zookeeper server and as a result requires the \nZookeeper server to be running before the Kafka server may be started. The Kafka server makes use of the \n server.properties configuration file when started. The default settings in the  server.properties file are \nnot suitable for the Kafka server to start based on a  Zookeeper server running at  localhost:2181 . We need \nto modify the connect url for Zookeeper in the  server.properties file. \n In this section we shall download the  dockerkafka/kafka image, modify the  server.properties and \nrebuild the Docker image. Download the source code for the  dockerkafka/kafka image with the following \ncommand. \n git clone https://github.com/DockerKafka/kafka-docker.git \n The source code for the  dockerkafka/kafka image gets downloaded as shown in Figure  12-4 . \n Figure 12-2.  Starting Docker \n Figure 12-3.  Listing the “kubernetes” Service \n \n \n",
      "content_length": 1337,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 362,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n350\n Change directory (cd) to the  kafka-docker directory and list the files/directories. \n cd kafka-docker \n ls –l \n The files/directories in the Docker image get listed as shown in Figure  12-5 . \n We need to modify the settings in the  server.properties file, which is in the  image/conf directory. \nCd to the  image/conf directory and list the directory’s file/directories. \n cd image/conf \n ls –l \n The  server.properties file gets listed as shown in Figure  12-6 . \n Figure 12-4.  Downloading the kafka-docker Docker Image Source Code \n Figure 12-5.  Listing the Dockerfile and Image Directory for the kafka-source Docker Image \n Figure 12-6.  Listing the Configuration Files for the Docker Image \n \n \n \n",
      "content_length": 742,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 363,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n351\n Open the  server.properties file in a vi editor. \n sudo vi server.properties \n The  server.properties file is shown in Figure  12-7 . Uncomment the line with the  host.name=localhost \nsetting. \n As shown in Figure  12-8 the default setting for the  zookeeper.connect is  zookeeper:2181 . \n Figure 12-7.  Uncommenting the host.name Property \n \n",
      "content_length": 380,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 364,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n352\n Modify the  zookeeper.connect setting to  localhost:2181 as shown in Figure  12-9 . Save the modified \nfile with :wq. We need to modify the setting because no such host as “zookeeper” exists by default. \n Figure 12-8.  The default setting for the zookeeper.connect Property \n \n",
      "content_length": 314,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 365,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n353\n Subsequently cd back to the root directory for the Docker image, the  kafka-docker directory, and run \nthe following command to rebuild the Docker image. \n sudo docker build -t dockerkafka/kafka:v2. \n The output from the command is shown in Figure  12-10 . \n Figure 12-9.  Setting zookeeper.connect to localhost: 2181 \n \n",
      "content_length": 358,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 366,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n354\n Figure 12-10.  Rebuilding the Docker Image for Kafka \n \n",
      "content_length": 93,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 367,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n355\n Docker image gets rebuilt as shown in Figure  12-11 . \n The Docker image we shall use subsequently is not  dockerkafka/kafka but is  dockerkafka/kafka:v2 .  \n Creating a Service \n Create a service definition file called  kafka-service.yaml and add the following (Table  12-1 ) fields to \nthe file. \n Figure 12-11.  Completing the Rebuild of the Docker Image \n \n",
      "content_length": 398,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 368,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n356\n The  kafka-service.yaml is listed. \n apiVersion: v1 \n kind: Service \n metadata:  \n  labels:  \n    app: kafkaApp \n  name: kafka \n spec:  \n  ports:  \n    -  \n      port: 9092 \n      targetPort: 9092 \n    -  \n      port: 2181 \n      targetPort: 2181 \n  selector:  \n    app: kafkaApp \n  type: LoadBalancer \n The  kafka-service.yaml may be created in vi editor and saved with :wq as shown in Figure  12-12 . \n Table 12-1.  The Fields in the Service Definition File \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n Service \n metadata \n The service metadata. \n metadata - > labels \n The service labels. Not required. \n app: kafkaApp \n metadata - > name \n The service name. Required. \n kafka \n spec \n The service specification. \n spec - > ports \n The ports exposed by the service. \n spec - > ports- > port \n A port exposed by the service. The 9092 port is used for the \nKafka server. \n port: 9092 \n targetPort: 9092 \n spec - > ports- > port \n Another port exposed by the service. The 2181 port is for the \nZookeeper. \n port: 2181 \n targetPort: 2181 \n spec - > selector \n The Pod selector. Service routes traffic to the Pods with label \nmatching the selector expression. \n app: kafkaApp \n spec \n- > selector- > type \n The service type. \n LoadBalancer \n",
      "content_length": 1319,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 369,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n357\n Create the service from the definition file. \n kubectl create -f kafka-service.yaml  \n Subsequently list the services. \n kubectl get services \n The “kafka” service gets listed as shown in Figure  12-13 . The service selector is app = kafkaApp. \n Figure 12-12.  Service Definition File in vi Editor \n \n",
      "content_length": 338,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 370,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n358\n Creating a Replication Controller \n Create a definition file called  kafka-rc.yaml for the replication controller and add the following (Table  12-2 ) \nfields. \n Table 12-2.  Fields in the Replication Controller Definition File \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n ReplicationController \n metadata \n The replication controller metadata. \n metadata - > labels \n The replication controller labels. \n app: kafkaApp \n name: kafka-rc \n spec \n The replication controller specification. \n spec - > replicas \n The number of Pod replicas. \n 2 \n spec - > selector \n A key:value expression for selecting the Pods \nto manage. Pods with a label the same as the \nselector expression are managed by the replication \ncontroller. The selector expression must be the \nsame as the spec- > template- > metadata- > labels \nexpression. The selector defaults to the \nspec- > template- > metadata- > labels key:value \nexpression if not specified. \n app: kafkaApp \n spec - > template \n The Pod template. \n spec - > template- > metadata  The Pod template metadata. \n spec - > template - > metadata \n- > labels \n The Pod template labels. \n app: kafkaApp \n spec - > template - > spec \n The Pod template specification. \n spec - > template - > spec \n- > containers \n The containers configuration for the Pod template. \n Figure 12-13.  Creating a Service from the Definition File \n(continued)\n \n",
      "content_length": 1452,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 371,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n359\n The  kafka-rc.yaml is listed. \n ---  \n apiVersion: v1 \n kind: ReplicationController \n metadata:  \n  labels:  \n    app: kafkaApp \n  name: kafka-rc \n spec:  \n  replicas: 1 \n  selector:  \n    app: kafkaApp \n  template:  \n    metadata:  \n      labels:  \n        app: kafkaApp \n    spec:  \n      containers:  \n        -  \n          command:  \n            - zookeeper-server-start.sh \n            - /opt/kafka_2.10-0.8.2.1/config/zookeeper.properties \n          image: \"dockerkafka/kafka:v2\" \n          name: zookeeper \n          ports:  \n            -  \n              containerPort: 2181 \n Field \n Description \n Value \n spec - > template - > spec \n- > containers - > command \n The command/s to run for the Docker image. \nThe default command in the Dockerfile is CMD \n[“kafka-server-start.sh”, “/opt/kafka_2.10-0.8.2.1/\nconfig/server.properties”]. The default command \nstarts the Kakfa server, but we want the Zookeeper \nserver before the Kafka server as the Kafka server \nwon’t start unless the Zookeeper server is running. \nThe modified command starts only the Zookeeper \nserver. We shall start the Kafka server separately. \n - zookeeper-server-\nstart.sh \n - /opt/\nkafka_2.10-0.8.2.1/\nconfig/zookeeper.\nproperties \n spec - > template - > spec \n- > containers - > image \n The Docker image. \n dockerkafka/kafka:v2 \n spec - > template - > spec \n- > containers - > name \n The container name. \n zookeeper \n ports \n Specifies the container port/s. \n containerPort: 2181 \nTable 12-2. (continued)\n",
      "content_length": 1522,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 372,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n360\n The  kafka-rc.yaml file may be created and saved in the vi editor as shown in Figure  12-14 . \n Create the replication controller from the definition file. \n kubectl create -f kafka-rc.yaml   \n Subsequently list the replication controllers. \n kubectl get rc \n The replication controller gets created and listed as shown in Figure  12-15 . \n Figure 12-14.  Replication Controller Definition File in vi Editor \n \n",
      "content_length": 448,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 373,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n361\n To describe the  kafka-rc run the following command. \n kubectl describe rc kafka-rc \n The replication controller description gets listed as shown in Figure  12-16 . \n Listing the Pods \n To list the Pods run the following command. \n kubectl get pods \n The Pods get listed as shown in Figure  12-17 . \n Figure 12-15.  Creating the Replication Controller from the Definition File \n Figure 12-16.  Describing the Replication Controller \n Figure 12-17.  Listing the pods for Kafka \n \n \n \n",
      "content_length": 520,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 374,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n362\n Describing a Pod \n Only a single Pod is created because the “replicas” setting in the definition file  kafka-rc.yaml is 1. To describe \nthe Pod run the following command. \n kubectl describe pod kafka-rc-k8as1 \n The pod description gets listed as shown in Figure  12-18 . The Pod label  app=kafkaApp is the same as the \nservice selector and the replication controller selector which makes the Pod manageable by the service and \nthe replication controller. \n When the Pod is created and started, the Zookeeper server gets started as the command for the \nmodified Docker image is to start the Zookeeper server. Next we shall start the Kafka server from an \ninteractive shell for the Docker container for the modified Docker image. \n Figure 12-18.  Describing a pod for Kafka \n \n",
      "content_length": 812,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 375,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n363\n Starting an Interactive Shell \n To be able to start an interactive bash shell to access the Kafka software installed we need to know the \ncontainer id for the Docker container running the modified Docker image. List the Docker containers with \nthe following command. \n sudo docker ps \n The Docker containers get listed as shown in Figure  12-19 . \n Figure 12-19.  Obtaining the Docker Container Id \n \n",
      "content_length": 438,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 376,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n364\n Copy the container id and start the interactive bash shell. \n sudo docker exec -it 939ae2cb4f86 bash \n The interactive shell gets started as shown in Figure  12-20 . \n Starting the Kafka Server \n The configuration properties for Kafka server are set in the  config/server.properties file, which we \nmodified when we rebuilt the Docker image. As the Zookeeper is already running, start the Kafka server with \nthe following command. \n kafka-server-start.sh /opt/kafka_2.10-0.8.2.1/config/server.properties \n The preceding command is shown in Figure  12-21 . \n Figure 12-20.  Starting the Interactive TTY for the Docker Container \n \n",
      "content_length": 667,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 377,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n365\n Kafka server gets started as shown in Figure  12-22 . \n Figure 12-21.  Starting the Kafka Server \n \n",
      "content_length": 137,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 378,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n366\n Creating a Topic \n Next, create a topic called ‘kafka-on-kubernetes’ with the following command. Set the number of partitions \nto 1 and replication factor to 1. The Zookeeper is set to  localhost:2181 . \n kafka-topics.sh --create --topic kafka-on-kubernetes --zookeeper localhost:2181 \n--replication-factor 1 --partitions 1 \n As shown in Figure  12-23 the  kafka-on-kubernetes topic gets created. \n Figure 12-22.  Kafka Server started at localhost:9092 \n \n",
      "content_length": 493,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 379,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n367\n Starting a Kafka Producer \n A Kafka producer is used to produce messages. After starting the ZooKeeper and the Kafka server, start \nthe Kafka producer. Specify the topic with the  –topic option as ‘kafka-on-kubernetes’. The  --broker-list \nspecifies the Kafka server as  localhost:9092 , which are the settings configured in  server.properties file. \n kafka-console-producer.sh --topic kafka-on-kubernetes --broker-list localhost:9092 \n As shown in Figure  12-24 the Kafka producer gets started. \n Starting a Kafka Consumer \n A Kafka consumer consumes messages. Start the Kafka consumer with the following command. Specify \nthe topic with the  –topic option as ‘kafka-on-kubernetes’. The  --zookeeper specifies the Zookeeper server \nas  localhost:2181 , which are the settings configured in  server.properties file. The  --from-beginning \noption specifies that messages from the beginning are to be consumed, not just the messages consumed \nafter the consumer was started. \n kafka-console-consumer.sh --topic kafka-on-kubernetes --from-beginning --zookeeper \nlocalhost:2181 \n As shown in Figure  12-25 the Kafka producer gets started. \n Producing and Consuming Messages \n Having started the Producer and the Consumer, we shall produce message/s at the Producer and consume \nmessage/s at the Consumer. At the Producer add a message, for example, “Message from Kafka Producer” as \nshown in Figure  12-26 and click on Enter button. The message gets sent. \n Figure 12-24.  Starting a Kafka Producer \n Figure 12-25.  Starting a Kafka Consumer \n Figure 12-23.  Creating a Kafka Topic \n \n \n \n",
      "content_length": 1622,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 380,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n368\n At the Consumer the message gets consumed as shown in Figure  12-27 . \n Send more messages at the Producer as shown in Figure  12-28 . \n And the messages get consumed at the Consumer as shown in Figure  12-29 . \n Scaling the Cluster \n To scale the cluster to 4 Pods from 1 Pod run the following command. \n kubectl scale rc kafka-rc --replicas=4 \n Figure 12-28.  Producing More Messages at the Kafka Producer \n Figure 12-29.  Consuming More Messages at the Kafka Consumer \n Figure 12-27.  Consuming a Message at the Kafka Consumer \n Figure 12-26.  Producing a Message at the Kafka Producer \n \n \n \n \n",
      "content_length": 635,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 381,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n369\n Subsequently list the Pods. \n kubectl get pods \n An output of “scaled” indicates that the cluster has been scaled as shown in Figure  12-30 . Subsequently \nthe Pods get listed, also shown in Figure  12-30 . \n When the number of Pods are increased to 4, the service endpoints also increase to 4. Describe the \nservice  kafka . \n kubectl describe svc kafka \n As shown in Figure  12-31 , 4 endpoints are listed for each of the two services, one for Zookeeper server \nand the other for the Kafka server. \n Figure 12-30.  Scaling the Kafka Cluster \n \n",
      "content_length": 583,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 382,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n370\n Deleting Replication Controller and Service \n To delete the replication controller and service run the following commands. \n kubectl delete rc kafka-rc \n kubectl delete service kafka \n As shown in Figure  12-32 the replication controller and service get deleted. \n Figure 12-32.  Deleting the Kafka Replication Controller and Service \n Figure 12-31.  Describing the Kafka Service with 4 Endpoints \n \n \n",
      "content_length": 439,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 383,
      "content": "CHAPTER 12 ■ USING APACHE KAFKA\n371\n Summary \n Apache Kafka is a producer–consumer-based messaging system. In this chapter we discussed managing \na Kafka cluster with Kubernetes. Managing the Kafka is different from some of the other applications as \ntwo servers have to be started: the Zookeeper server and the Kafka server. And the Kafka server has a \ndependency on the Zookeeper server, which implies that the Zookeeper must be started before the Kafka \nserver. We needed to modify the default image  dockerkafka/kafka for the zookeeper connect url. In the \nreplication controller definition file we used a custom command to run the modified Docker image to \nstart the Zookeeper server, the default settings in the Docker image being to start the Kafka server. All the \napplications we have run as yet were based on a single container Pod. In the next chapter we shall develop a \nmulti-container Pod. \n",
      "content_length": 905,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 384,
      "content": "   PART V \n Multi Containers and Nodes   \n",
      "content_length": 42,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 385,
      "content": "375\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_13\n CHAPTER 13 \n Creating a Multi-Container Pod \n A  Pod is the atomic unit of an application managed by Kubernetes. A Pod has a single filesystem and IP \nAddress; the containers in the Pod share the filesystem and networking IP. A Pod could consist of one or more \ncontainers. A Pod is defined in a definition file for a Pod or a replication controller using the specification \nfor a Pod ( http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_podspec ). A \nsingle container within a Pod is specified using the container specification ( http://kubernetes.io/v1.1/\ndocs/api-reference/v1/definitions.html#_v1_container ). In all of the applications discussed as yet, \nin preceding chapters, a single container Pod was used. In this chapter we shall develop a multi-container \nPod. We have used the  tutum/hello-world and  postgres Docker images for the multi-container Pod. Each \nof these images have been used in a single container Pods in preceding chapters. This chapter will cover the \nfollowing topics.\n How to Find Number of Containers in a Pod? \n Type of applications Using a Multi-Container Pod \n Setting the Environment \n Creating a Service \n Describing a Service \n Creating a Replication Container \n Listing the Pods \n Listing the Docker Containers \n Describing the Service after Creating Replication Controller \n Invoking the Hello World Application on Command Line \n Starting the Interactive Shell \n Starting PostgreSQL Shell \n Setting Port Forwarding \n Opening the Hello World Application in a Browser \n Scaling the Cluster\n Describing the Service after Scaling \n Describing a Pod \n Setting Port Forwarding \n",
      "content_length": 1738,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 386,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n376\n Opening the Hello World Applications in a Browser \n Invoking the Hello World Application from Command Line \n Deleting the Replication Controller \n Deleting the Service \n How to find Number of Containers in a Pod? \n As discussed previously the Pods may be listed with the following command. \n kubectl get pods \n The Kubernetes  Pod  k8s-master-127.0.0.1 Pod has 3/3 in the READY column as shown in Figure  13-1 . \nThe 3/3 indicates that the Pod has 3 containers and all three containers are ready. The n/n in the READY \ncolumn for any Pod indicates the number of containers ready out of the total number of containers. All the \ncontainers are running on a single node as indicated by the subsequent listing of nodes. \n Types of Applications Using a Multi-Container Pod \n Various types of  applications could make use of a multi-container Pod. Some of the examples are as follows:\n -An Apache Sqoop application makes use of a CDH Docker image-based \ncontainer and a MySQL database Docker image-based container for bulk \ntransferring data from MySQL database into HDFS. \n -An Apache Flume application makes use of a CDH Docker image-based \ncontainer and a Kafka-based container for streaming data from a Kafka source \ninto HDFS. \n -An Apache Solr application makes use of a Oracle Database-based container \nand the Solr container for data import from Oracle Database into Solr. \n -An Apache Hive application makes use a CDH container and a MongoDB \ncontainer to create a Hive table using the MongoDB storage handler. \n -An Apache Solr container and a CDH container are required to store Solr data in \nHDFS instead of the local filesystem. \n Figure 13-1.  Listing the Pods and the Number of Containers in the Pods \n \n",
      "content_length": 1762,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 387,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n377\n Setting the Environment \n We have used an Amazon EC2 instance created from AMI Ubuntu Server 14.04 LTS (HVM), SSD Volume \nType - ami-d05e75b8 to install the following required software. \n -Docker Engine (latest version) \n -Kubernetes (version 1.01) \n -Kubectl (version 1.01) \n -Docker image tutum/hello-world (latest version) \n -Docker image postgres (latest version) \n Install Docker, Kubernetes, and Kubectl as discussed in chapter 1. To log in to the Ubuntu instance the \nPublic IP Adress may be obtained from the Amazon EC2 console as shown in Figure  13-2 . \n SSH Login to the Ubuntu instance. \n ssh -i \"docker.pem\" ubuntu@52.90.62.35 \n After having installed Docker start Docker and verify its status. \n sudo service docker start \n sudo service docker status \n Figure 13-2.  Obtaining the Public IP Address \n \n",
      "content_length": 865,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 388,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n378\n Docker should be listed as being “running” as shown in Figure  13-3 . \n Creating a Service \n Create a service definition file  hello-postgres-service.yaml to configure the service ports. We shall \nbe configuring two service ports, one for the  hello-world application and the other for the  postgres \napplication. The fields in the service definition file are discussed in Table  13-1 . \n Figure 13-3.  Starting Docker \n Table 13-1.  Fields in the Service Definition File \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n Service \n metadata \n The service metadata. \n metadata - > labels \n The service labels. The setting translates to label \n app = MultiContainerApp \n app: MultiContainerApp \n metadata - > name \n The service name. \n hello-postgres \n spec \n The service specification. \n spec - > ports \n The ports exposed by the service. Two ports are \nexposed, one for the hello-world application and \nthe other for the postgres application. \n name: hello-world \n port: 8080 \n name: postgres \n port: 5432 \n spec - > selector \n The Pod selector. Service routes traffic to the Pods \nwith label matching the selector expression. The \nsetting translates to selector \n app = MultiContainerApp \n app: MultiContainerApp \n spec - > selector - > type \n The service type. \n LoadBalancer \n The  hello-postgres-service.yaml is listed: \n apiVersion: v1 \n kind: Service \n metadata:  \n  labels:  \n    app: MultiContainerApp \n  name: hello-postgres \n \n",
      "content_length": 1524,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 389,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n379\n spec:  \n  ports:  \n    -  \n      name: hello-world \n      port: 8080 \n    -  \n      name: postgres \n      port: 5432 \n  selector:  \n    app: MultiContainerApp \n  type: LoadBalancer \n Create a service from the definition file. \n kubectl create -f hello-postgres-service.yaml  \n Subsequently list the services. \n kubectl get services \n The  hello-postgres service gets created and listed as shown in Figure  13-4 . \n Describing a Service  \n The  hello-postgres service may be described with the following command. \n kubectl describe service hello-postgres \n The service description includes the name, namespace, labels, selector, type, IP, ports, and endpoints as \nshown in Figure  13-5 . Initially the service is not managing any pods and as a result no endpoints are listed. \n Figure 13-4.  Creating a Service from the Definition File \n \n",
      "content_length": 887,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 390,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n380\n Creating a Replication Container \n Create a definition file  hello-postgres-rc.yaml for a replication controller. Add the following (Table  13-2 ) \nfields to the definition file. \n Table 13-2.  Fields in the Replication Controller Definition File \n Field \n Description \n Value \n apiVersion \n v1 \n kind \n The kind of definition file. \n ReplicationController \n metadata \n The replication controller metadata. \n metadata - > labels \n The replication controller labels. \n app: \"MultiContainerApp\" \n metadata - > name \n The replication controller name. \n \"hello-postgres\" \n spec \n The replication controller specification. \n spec - > replicas \n The number of Pod replicas. \n 1 \n spec - > selector \n A key:value expression for selecting the \nPods to manage. Pods with a label the same \nas the selector expression is managed by \nthe replication controller. The selector \nexpression must be the same as the \nspec - > template - > metadata - > labels \nexpression. The selector defaults to the spec \n- > template - > metadata - > labels key: value \nexpression if not specified. \n app: \"MultiContainerApp\" \n Figure 13-5.  Describing the Service \n(continued)\n \n",
      "content_length": 1198,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 391,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n381\n The  hello-postgres-rc.yaml is listed: \n apiVersion: v1 \n kind: ReplicationController \n metadata:  \n  labels:  \n    app: \"MultiContainerApp\" \n  name: \"hello-postgres\" \n spec:  \n  replicas: 1 \n  selector:  \n    app: \"MultiContainerApp\" \n  template:  \n    metadata:  \n      labels:  \n        app: \"MultiContainerApp\" \n Field \n Description \n Value \n spec - > template \n The Pod template. \n spec - > template- > metadata \n The Pod template metadata. \n spec - > template - > metadata \n- > labels \n The Pod template labels. The selector if not \nspecified defaults to this setting. The service \nselector must be the same as one of the Pod \ntemplate labels for the service to represent \nthe Pod. The service selector does not \ndefault to the same value as the label and \nwe already set the service selector to app: \nMultiContainerApp. \n app: “MultiContainerApp” \n spec - > template - > spec \n The Pod template specification. \n spec - > template - > spec \n- > containers \n The containers configuration for the Pod \ntemplate. \n spec - > template - > spec \n- > containers - > image \n The Docker image for the hello-world \ncontainer. \n tutum/hello-world \n spec - > template - > spec \n- > containers - > name \n The container name for the hello-world \ncontainer. \n hello-world \n ports \n Specifies the container port for the hello-\nworld container. \n containerPort: 8080 \n spec - > template - > spec \n- > containers - > image \n The Docker image for the postgres container.  postgres \n spec - > template - > spec \n- > containers - > name \n The container name for the postgres \ncontainer. \n postgres \n ports \n Container port for postgres container. \n containerPort: 5432 \nTable 13-2. (continued)\n",
      "content_length": 1728,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 392,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n382\n    spec:  \n      containers:  \n        -  \n          image: \"tutum/hello-world\" \n          name: \"hello-world\" \n          ports:  \n            -  \n              containerPort: 8080 \n        -  \n          image: \"postgres\" \n          name: \"postgres\" \n          ports:  \n            -  \n              containerPort: 5432 \n Create a replication controller from the definition file. \n kubectl create -f hello-postgres-rc.yaml \n Subsequently list the replication controllers. \n kubectl get rc \n As shown in Figure  13-6 the  hello-postgres replication controller gets created and listed. \n Listing the Pods \n To list the Pods run the following command. \n kubectl get pods \n As  replicas field is set to 1 in the replication controller only one Pod gets created as shown in \nFigure  13-7 . The READY column lists 0/2, which indicates that 0 or none of the two containers in the pod are \nready. Initially the container could be listed as not running and creating. Run the preceding command after \na few seconds and the Pod STATUS should be “Running” and the READY state should be 2/2, implying that 2 \nof 2 containers are running. \n Figure 13-6.  Creating a Replication Controller from the Definition File \n \n",
      "content_length": 1252,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 393,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n383\n Listing the Docker Containers \n To list the Docker containers started, run the following command. \n sudo docker ps \n Two of the listed containers, the container based on the  postgres image and the container based on \nthe  tutum/hello-world image, as shown in Figure  13-8 , are started with the replication controller \n hello-postgres . \n Figure 13-7.  Listing the Pods \n \n",
      "content_length": 423,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 394,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n384\n Describing the Service after Creating Replication Controller \n Before we had created the replication controller the service  hello-postgres was not associated with any \nendpoints. After creating the replication controller and the Pod/s, run the following command again to \ndescribe the service again. \n kubectl describe service hello-postgres \n An endpoint is listed for each of the ports exposed by the service as shown in Figure  13-9 . \n Figure 13-8.  Listing the Docker Containers \n \n",
      "content_length": 537,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 395,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n385\n Invoking the Hello World Application on Command Line \n Invoke the service endpoint  172.17.0.2 using curl as follows. \n curl 172.17.0.2 \n The HTML generated by the application gets output as shown in Figure  13-10 . \n Figure 13-9.  Describing the Service \n \n",
      "content_length": 307,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 396,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n386\n Starting the Interactive Shell \n To start an interactive shell for the software installed, either of the Docker containers, listed previously in \nFigure  13-8 , for the multi-container Pod may be used. Both the containers access the same filesystem and IP. \nUse the following command to start an  interactive shell. \n sudo docker exec -it 2e351a609b5b bash \n An interactive shell gets started as shown in Figure  13-11 . \n Figure 13-10.  Invoking an Endpoint for the Service \n \n",
      "content_length": 527,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 397,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n387\n Starting PostgreSQL Shell \n To start the  PostgreSQL command shell called  psql run the following command in the interactive shell. \n psql postgres \n The  psql gets started and the  postgres command prompt gets displayed as shown in Figure  13-12 . \n PostgreSQL with Kubernetes is discussed in chapter 5. \n Setting Port Forwarding \n We had earlier invoked the service endpoint to output the HTML generated using curl on the command line, \nbut HTML is best displayed in a browser. As an Amazon EC2 instance does not provide a browser by default, \nwe need to set port forwarding to a local machine to be able to access the service endpoint in a browser. Set \nthe  port forwarding for  172.17.0.2:80 to  localhost:80 with the following command. \n ssh -i \"docker.pem\" -f -nNT -L 80:172.17.0.2:80 ubuntu@ec2-52-90-62-35.compute-1.amazonaws.com \n The port forwarding to  localhost gets set as shown in Figure  13-13 . \n The Public DNS for the Amazon EC2 instance may be obtained from the Amazon EC2 console as shown \nin Figure  13-14 . \n Figure 13-12.  Starting psql Shell \n Figure 13-13.  Setting Port Forwarding \n Figure 13-11.  Starting an Interactive Shell \n \n \n \n",
      "content_length": 1211,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 398,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n388\n Opening the Hello World Application in a Browser \n Having set port forwarding the application may be opened in a browser on a local machine with url \n http://localhost as shown in Figure  13-15 . In addition to the hostname the two ports at which the \n HELLO_POSTGRES is listening at get listed. \n Figure 13-14.  Obtaining Public DNS \n \n",
      "content_length": 386,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 399,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n389\n Scaling the Cluster \n To  scale the cluster to 3 replicas or Pods run the following command. \n kubectl scale rc hello-postgres --replicas=3 \n Subsequently list the Pods. \n kubectl get pods \n Three Pods get listed as shown in Figure  13-16 . Some of the Pods could be not running or not ready \ninitially. Run the preceding command again after a few seconds to list all the Pods with STATUS as \n“Running” and READY state as 2/2. \n Figure 13-15.  Invoking the Service  Endpoint in a Browser \n \n",
      "content_length": 540,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 400,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n390\n A Pod may be described using the  kubectl describe pod command. For example, describe the \n hello-postgres-jliem pod with the following command. \n kubectl describe pod hello-postgres-jliem \n As shown in Figure  13-17 the Pod description gets listed. \n Figure 13-16.  Scaling the Cluster to 3 Replicas \n Figure 13-17.  Describing a Pod \n \n \n",
      "content_length": 389,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 401,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n391\n Listing the Docker Containers \n As each Pod consists of two containers, scaling up the cluster to 3 Pods or replicas starts four new containers, \n2 containers for each of the two new Pods. After scaling up the cluster run the following command to list the \nrunning  Docker containers again using the default output format. \n sudo docker ps \n A total of 3 containers based on the  postgres image and 3 containers based on the  tutum/hello-world \nimage get listed as shown in Figure  13-18 . \n Figure 13-18.  Listing the Docker Containers \n \n",
      "content_length": 589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 402,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n392\n Describing the Service after Scaling \n Describe the  service again after scaling up the cluster. \n kubectl describe service hello-postgres \n Each of the ports exposed by the service is associated with three endpoints because 3 Pods are running \nas shown in Figure  13-19 . \n Setting Port Forwarding \n To be able to open the application in a browser we need to set port forwarding to locahost. Set the port \nforwarding to ports not previously bound. The  localhost:80 beind address is already sued up in the port \nforwarding of the single Pod created earlier. To set  port forwarding for the two new Pods use ports 81 and 82 \non localhost. \n ssh -i \"docker.pem\" -f -nNT -L 81:172.17.0.3:80 ubuntu@ec2-52-90-62-35.compute-1.amazonaws.com \n ssh -i \"docker.pem\" -f -nNT -L 82:172.17.0.4:80 ubuntu@ec2-52-90-62-35.compute-1.amazonaws.com \n The preceding commands do not generate any output but the ports get forwarded to the  localhost as \nshown in Figure  13-20 . \n Figure 13-19.  Describing the Service including the Service Endpoints \n \n",
      "content_length": 1084,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 403,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n393\n Opening the Hello World Application in a Browser \n The application may be opened in a  browser at each of the forwarded ports; for example, open a browser at \n http://localhost:81 . The application HTML gets displayed as shown in Figure  13-21 . The  HELLO_POSTGRES \nservice is listening at two ports 8020 and 5432. \n Similarly open the other service endpoint in a browser with url  http://localhost:82 . Different \nhostnames listening on the same port are forwarded to different ports on the  localhost . The service \nendpoint HTML gets output as shown in Figure  13-22 . \n Figure 13-20.  Setting Port Forwarding \n Figure 13-21.  Invoking a Service Endpoint in a Browser \n \n \n",
      "content_length": 726,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 404,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n394\n Invoking the Hello World Application from Command Line \n As for a single container Pod, each of the two new service endpoints may be invoked on the  command line . \nFor example, invoke the  172.17.0.3 endpoint with the following curl command. \n curl 172.17.0.3 \n The HTML for the service endpoint gets output as shown in Figure  13-23 . \n Figure 13-22.  Invoking another Service Endpoint in a Browser \n \n",
      "content_length": 453,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 405,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n395\n Invoke the  172.17.0.4 endpoint with the following curl command. \n curl 172.17.0.4 \n The HTML for the service endpoint gets output as shown in Figure  13-24 . \n Figure 13-23.  Invoking a Service Endpoint with curl \n \n",
      "content_length": 266,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 406,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n396\n Deleting the Replication Controller \n To delete the  hello-postgres replication controller run the following command. \n kubectl delete rc hello-postgres \n Subsequently list the Pods with the following command. \n kubectl get pods \n The Pods for the  hello-postgres replication controller are not listed as shown in Figure  13-25 . \n Figure 13-24.  Invoking another Service Endpoint with curl \n \n",
      "content_length": 443,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 407,
      "content": "CHAPTER 13 ■ CREATING A MULTI-CONTAINER POD\n397\n Deleting the Service \n To delete the  service  hello-postgres run the following command. \n kubectl delete service hello-postgres \n Subsequently run the following command to list the services. \n kubectl get services \n The  hello-postgres service is not listed as shown in Figure  13-26 . \n Summary \n In this chapter we discussed using multiple containers in a Pod. We discussed the use case for a multi-container \nPod and used the  tutum/hello-world and  postgres Docker images to create a multi-container Pod. A \nmulti-container pod starts multiple Docker containers for each Pod even though the Pod is the atomic unit. \nThe multiple containers in a Pod share the same IP address and filesystem. When a multi-container Pod is \nscaled, multiple containers are started for each of the new Pods. In the next chapter we shall discuss installing \nKubernetes on a multi-node cluster. \n Figure 13-25.  Deleting the Replication Controller \n Figure 13-26.  Deleting the Service \n \n \n",
      "content_length": 1023,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 408,
      "content": "399\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2_14\n CHAPTER 14 \n Installing Kubernetes on \na Multi-Node Cluster \n In all of the preceding chapters in the book we have used a single-node cluster. For most small scale \napplications a single-node cluster should suffice. But, for relatively large scale, distributed applications a \n multi-node cluster is a more suitable option. In this chapter we shall install Kubernetes on a multi-node \n(2 nodes) cluster. This chapter has the following sections.\n Components of a Multi-Node Cluster \n Setting the Environment \n Installing the Master Node\n Setting Up Flanneld and etcd \n Starting the Kubernetes on Master Node \n Running the Service Proxy \n Testing the Master Node \n Adding a Worker Node\n Exporting the Master IP \n Setting Up Flanneld and etcd \n Starting Up Kubernetes on Worker Node \n Running the Service Proxy \n Testing the Kubernetes Cluster \n Running an Application on the Cluster \n Exposing the Application as a Service \n Testing the Application in a Browser \n Scaling the Application \n",
      "content_length": 1094,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 409,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n400\n Components of a Multi-Node Cluster \n A multi-node cluster consists of the following main and ancillary  components. \n -Kubernetes Master Node \n -Kubernetes Worker Node/s \n -Etcd \n -Flannel \n -Service Proxy \n -Kubectl \n etcd, kubernetes master, and service proxy were discussed in chapter  1 . etcd as introduced in chapter  1 \nis a distributed, key-value store used by the Kubernetes cluster manager. We have installed etcd on the \nsame node as the Kubernetes master but in a production environment etcd would typically be installed as \nseparate cluster installed on nodes different than the Kubernetes master node. A commit to an etcd cluster \nis based on replication to a majority (quorum) of available nodes with provision for failure of one or more \nnodes. While the majority of a 1-node cluster is 1, the majority of a 3-node cluster is 2, majority of a 4-node \ncluster is 3, majority of a 5-node cluster is 3. A etcd cluster would typically have an odd number (>2) of \nnodes with tolerance for failure. For example, a 5-node etcd cluster could loose up to 2 nodes resulting in \na 3-node cluster in which the majority is still determinable. A 3-node cluster has a failure tolerance for one \nmore node. A 2-node etcd cluster does not have any failure tolerance and the majority of a 2-node cluster is \nconsidered as 2. The recommended etcd cluster size in production is 3,5, or 7. \n Flannel is a network fabric for containers. Flannel provides a subnet to each host that is used by \ncontainers at runtime. Actually, Flannel runs an agent called flanneld on each host that allocates subnets. \nFlannel sets up and manages the network that interconnects all the Docker containers created by \nKubernetes. Flannel is etcd backed and uses etcd to store the network configuration, allocated subnets, and \nauxiliary data such as the IP Address of the host. \n Setting the Environment \n We have used Amazon EC2 instances created from Ubuntu Server 14-04 LTS (HVM), SSD Volume Type - \nami-d05e75b8 AMI for this chapter. The following software is required to be installed for this chapter. \n -Docker Engine (latest version) \n -Kubernetes on Master Node (version 1.01) \n -Kubernetes on Worker Node (version 1.01) \n -Kubectl (version 1.01) \n Because we are creating a multi-node cluster we need to create multiple Amazon EC2 instances. For a \ntwo-node cluster create two Amazon EC2 instances – KubernetesMaster and KubernetesWorker – as shown \nin Figure  14-1 . \n",
      "content_length": 2518,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 410,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n401\n SSH Login to each node separately. The Public IP Address for the Master Node may be obtained from \nthe Amazon EC2 console as shown in Figure  14-2 . \n Figure 14-1.  Creating two Ubuntu Instances for Kubernetes Master and Worker Nodes \n Figure 14-2.  Obtaining the Public IP Address for a Ubuntu Instance \n Log in to the Ubuntu instance for the Master node. \n ssh -i \"docker.pem\" ubuntu@52.91.243.99 \n Similarly, obtain the Public IP Address for the Ubuntu instance for the Worker node and log in to the \nUbuntu instance for the Worker node. \n ssh -i \"docker.pem\" ubuntu@52.23.236.15 \n \n \n",
      "content_length": 653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 411,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n402\n Install Docker and Kubectl on each node as discussed in chapter  1 . Do not install Kubernetes just as \nchapter  1 because a multi-node configuration for Kubernetes is different than a single-node configuration. \n Start the Docker Engine and verify its status. \n sudo service docker start \n sudo service docker status \n Docker engine should be listed as “running” as shown in Figure  14-3 . \n Figure 14-3.  Starting Docker \n Installing the Master Node \n The Master node hosts the API server and assigns work to worker node/s. We need to run two Docker \ndaemons, a main Docker instance and a bootstrap Docker instance. The main Docker instance is used by \nthe Kubernetes and the bootstrap Docker instance is used by flannel, an etcd. The flannel daemon sets up \nand manages the network that interconnects all the Docker containers created by Kubernetes. \n Setting Up Flanneld and etcd  \n Setting Up Flanneld and etcd involves setting up a bootstrap instance for Docker, starting etcd for flannel \nand the API server, and setting up flannel on the master node. \n Setting up Bootstrap Instance of Docker \n Flannel, which sets up networking between Docker containers; and etcd on which flannel relies, run inside \nDocker containers themselves. A separate  bootstrap Docker is used because flannel is used for networking \nbetween Docker containers created by Kubernetes; and running flannel and Kubernetes in the same Docker \nengine could be problematic and is not recommended. Create a separate bootstrap instance of Docker for \nflannel and etcd. \n sudo sh -c 'docker daemon -H unix:///var/run/docker-bootstrap.sock -p /var/run/docker-\nbootstrap.pid --iptables=false --ip-masq=false --bridge=none --graph=/var/lib/docker-\nbootstrap 2> /var/log/docker-bootstrap.log 1> /dev/null &' \n \n",
      "content_length": 1845,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 412,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n403\n The bootstrap Docker daemon gets started and the output from the preceding command is shown in \nFigure  14-4 . \n Figure 14-4.  Starting the Bootstrap Daemon on the Master Node \n Figure 14-5.  Setting up etcd on the Master Node \n The ‘–d’ option is completely removed in Docker 1.10 and replaced with ‘daemon’. If using the Docker version \nprior to Docker 1.10, for example Docker 1.9.1, replace 'daemon’ with '-d' in the preceding command to run the \ncommand as follows: \n sudo sh -c 'docker -d -H unix:///var/run/docker-bootstrap.sock -p /var/run/docker-bootstrap.\npid --iptables=false --ip-masq=false --bridge=none --graph=/var/lib/docker-bootstrap 2> /\nvar/log/docker- bootstrap.log 1> /dev/null &' \n Setting Up etcd \n Set up  etcd for the flannel and the API server with the following command. \n sudo docker -H unix:///var/run/docker-bootstrap.sock run --net=host -d gcr.io/google_\ncontainers/etcd:2.0.12 /usr/local/bin/etcd --addr=127.0.0.1:4001 --bind-addr=0.0.0.0:4001 \n--data-dir=/var/etcd/data \n The container for etcd gets downloaded and etcd gets installed as shown in Figure  14-5 . \n \n \n",
      "content_length": 1165,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 413,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n404\n Set up a Classless Inter-Domain Routing (CIDR), which is an IP Addressing scheme that reduces the \nsize of routing tables and makes more addresses available, range for flannel. \n sudo docker -H unix:///var/run/docker-bootstrap.sock run --net=host gcr.io/google_\ncontainers/etcd:2.0.12 etcdctl set /coreos.com/network/config '{ \"Network\": \"10.1.0.0/16\" }' \n The preceding command does not generate any output as shown in Figure  14-6 . \n Figure 14-7.  Stopping Docker Temporarily \n Figure 14-6.  Setting Up CIDR on the Master Node \n Setting Up Flannel \n By default Docker does provide a networking between containers and Pods but the networking provided by \n Flannel is much more simplified. We shall be using Flannel for networking. First, we need to stop Docker. \n sudo service docker stop \n Docker gets stopped as shown in Figure  14-7 . \n Run flannel with the following command. \n sudo docker -H unix:///var/run/docker-bootstrap.sock run -d --net=host --privileged -v /dev/\nnet:/dev/net quay.io/coreos/flannel:0.5.0 \n \n \n",
      "content_length": 1089,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 414,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n405\n Flannel gets installed as shown in Figure  14-8 . \n Figure 14-8.  Installing Flannel \n Figure 14-9.  Obtaining the Hash Generated by Flannel \n Flannel generates a hash as shown in Figure  14-9 . Copy the Hash. \n \n \n",
      "content_length": 280,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 415,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n406\n Make a note of the  FLANNEL_SUBNET and  FLANNEL_MTU values as we shall need these to edit the Docker \nconfiguration. Open the Docker configuration file in a vi editor. \n sudo vi /etc/default/docker \n The default settings in the docker configuration file are shown in Figure  14-11 . \n Copy and paste the hash into the following command, and run the command to obtain the subnet settings. \n sudo docker -H unix:///var/run/docker-bootstrap.sock exec <really-long-hash-from-above-here> \ncat /run/flannel/subnet.env \n The subnet settings get listed as shown in Figure  14-10 . \n Figure 14-10.  Listing the Subnet Settings \n \n",
      "content_length": 686,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 416,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n407\n To the  DOCKER_OPTS setting append the following parameters whose values are obtained from the \noutput in Figure  14-10 . \n --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} \n Figure 14-11.  Docker Configuration File Default Settings \n \n",
      "content_length": 297,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 417,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n408\n The modified docker configuration file is shown in Figure  14-12 . \n Figure 14-12.  Modified Docker Configuration File \n As mentioned before Docker provides its own networking with a Docker bridge called  docker0 . As we \nwon’t be using the default Docker bridge remove the default Docker bridge. For the brctl binaries first install \nthe bridge-utils package. \n sudo /sbin/ifconfig docker0 down \n sudo apt-get install bridge-utils \n sudo brctl delbr docker0 \n \n",
      "content_length": 527,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 418,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n409\n The output from installing the bridge-utils package and removing the docker0 bridge is shown in \nFigure  14-13 . \n Figure 14-14.  Restarting Docker \n Figure 14-13.  Removing docker0 bridge \n Restart Docker. \n sudo service docker start \n Docker gets restarted as shown in Figure  14-14 . \n Starting the Kubernetes Master \n Setting up flannel networking is the main difference between setting up a single-node cluster and a \nmulti-node cluster. Start the  Kubernetes master with the same command as used for a single-node cluster. \n sudo docker run \\ \n  --volume=/:/rootfs:ro \\ \n  --volume=/sys:/sys:ro \\ \n  --volume=/dev:/dev \\ \n  --volume=/var/lib/docker/:/var/lib/docker:rw \\ \n  --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\ \n \n \n",
      "content_length": 799,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 419,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n410\n  --volume=/var/run:/var/run:rw \\ \n  --net=host \\ \n  --privileged=true \\ \n  --pid=host \\  \n  -d \\ \n  gcr.io/google_containers/hyperkube:v1.0.1 /hyperkube kubelet --api-servers=\nhttp://localhost:8080 --v=2 --address=0.0.0.0 --enable-server --hostname-override=127.0.0.1 --config=/\netc/kubernetes/manifests-multi --cluster-dns=10.0.0.10 --cluster-domain=cluster.local \n The preceding command is run from the Master Node as shown in Figure  14-15 . \n Figure 14-15.  Starting Kubernetes on the Master Node \n \n",
      "content_length": 569,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 420,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n411\n Kubernetes gets installed on the master node as shown in Figure  14-16 . \n Figure 14-16.  Kubernetes Started on Master Node \n Running the Service Proxy \n Run the  service proxy also using the same command as used for a single-node cluster. \n sudo docker run -d --net=host --privileged gcr.io/google_containers/hyperkube:v1.0.1 /\nhyperkube proxy --master=http://127.0.0.1:8080 --v=2 \n \n",
      "content_length": 450,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 421,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n412\n Service proxy gets installed as shown in Figure  14-17 . \n Figure 14-17.  Starting Service proxy on Master Node \n Figure 14-18.  Listing the Nodes, only the Master Node to start with \n Testing the One-Node Cluster \n To  test the master node run the following command, which lists the nodes in the cluster. \n kubectl get nodes \n The single node gets listed as shown in Figure  14-18 . \n Adding a Worker Node \n Setting up a  worker node is very similar to setting up the master node. Next, we shall set up a worker node. \nSSH login to the Ubuntu instance for the worker node. \n Exporting the Master IP \n First, we need to set the environment variable  MASTER_IP . Obtain the Public IP Address for the Ubuntu \ninstance running the master node as shown in Figure  14-19 . \n \n \n",
      "content_length": 838,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 422,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n413\n Export the environment variable  MASTER_IP using the Public IP Address. \n export MASTER_IP=52.91.243.99 \n Echo the  MASTER_IP environment variable. \n echo $MASTER_IP \n The output from the preceding command is shown in Figure  14-20 . \n Figure 14-19.  Obtaining the Master Node’s IP Address \n Figure 14-20.  Exporting the MASTER_IP Environment Variable on a Worker Node \n Setting Up Flanneld \n Start a bootstrap Docker daemon just for the  flannel networking. \n sudo sh -c 'docker daemon -H unix:///var/run/docker-bootstrap.sock -p /var/run/docker-\nbootstrap.pid --iptables=false --ip-masq=false --bridge=none --graph=/var/lib/docker-\nbootstrap 2> /var/log/docker-bootstrap.log 1> /dev/null &' \n \n \n",
      "content_length": 763,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 423,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n414\n Bootstrap Docker gets set up as shown in Figure  14-21 . \n Figure 14-21.  Starting Bootstrap Docker on the Worker Node \n The ‘–d’ option is completely removed in Docker 1.10 and replaced with ‘daemon’. If using the Docker version \nprior to Docker 1.10, for example Docker 1.9.1, replace 'daemon’ with '-d' in the preceding command to run the \ncommand as follows: \n sudo sh -c 'docker -d -H unix:///var/run/docker-bootstrap.sock -p /var/run/docker-bootstrap.\npid --iptables=false --ip-masq=false --bridge=none --graph=/var/lib/docker-bootstrap 2> /\nvar/log/docker- bootstrap.log 1> /dev/null &' \n To install Flannel, first we need to stop the Docker engine. \n sudo service docker stop \n Docker engine gets stopped as shown in Figure  14-22 . \n Figure 14-22.  Stopping Docker Temporarily on the Worker Node \n Next, install flannel on the worker node. The same etcd that is running on the master is used for \nthe flanneld on the worker node. The etcd instance includes the Master’s Ip using the  MASTER_IP \nenvironment variable. \n sudo docker -H unix:///var/run/docker-bootstrap.sock run -d --net=host --privileged \n-v /dev/net:/dev/net quay.io/coreos/flannel:0.5.0 /opt/bin/flanneld --etcd-\nendpoints=http://${MASTER_IP}:4001 \n \n \n",
      "content_length": 1294,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 424,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n415\n Flannel gets set up on the worker node as shown in Figure  14-23 . \n Figure 14-23.  Installing Flannel on the Worker Node \n Copy the hash generated by the preceding command as shown in Figure  14-24 . \n Figure 14-24.  Obtaining the Hash geenrated by Flannel \n Using the hash value in the following command obtain the subnet settings from flannel. \n sudo docker -H unix:///var/run/docker-bootstrap.sock exec <really-long-hash-from-above-here> \ncat /run/flannel/subnet.env \n \n \n",
      "content_length": 541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 425,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n416\n The subnet settings get output as shown in Figure  14-25 . \n Figure 14-25.  Listing the Subnet Settings on the Worker Node \n Using the subnet settings we need to edit the Docker configuration file. Open the Docker configuration \nfile in the vi editor. \n sudo /etc/default/docker \n Append the following parameters to the  DOCKER_OPTS setting. Substitute the values for  FLANNEL_SUBNET \nand  FLANNEL_MTU as obtained from Figure  14-25 . \n --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} \n \n",
      "content_length": 550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 426,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n417\n The modified Docker configuration file is shown in Figure  14-26 . \n Figure 14-26.  Modified Docker Configuration File \n Shut down and remove the existing Docker bridge  docker0 , which is used by default by Docker for \nnetworking between containers and Pods. The bridge-utils package is needed to be installed as it is not \navailable by default on an Ubuntu instance on Amazon EC2. \n sudo /sbin/ifconfig docker0 down \n sudo apt-get install bridge-utils \n sudo brctl delbr docker0 \n Restart Docker. \n sudo service docker start \n The Docker engine gets started as shown in Figure  14-27 . \n Figure 14-27.  Restarting Docker \n \n \n",
      "content_length": 693,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 427,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n418\n Starting Up Kubernetes on Worker Node \n Start up Kubernetes on the  worker node with the same command as used in the Master node with the \ndifference that instead of setting the  --api-servers to  http://localhost:8080 set the --api-servers to the \n http://${MASTER_IP}:8080 . \n sudo docker run \\ \n  --volume=/:/rootfs:ro \\ \n  --volume=/sys:/sys:ro \\ \n  --volume=/dev:/dev \\ \n  --volume=/var/lib/docker/:/var/lib/docker:rw \\ \n  --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \\ \n  --volume=/var/run:/var/run:rw \\ \n  --net=host \\ \n  --privileged=true \\ \n  --pid=host \\  \n  -d \\ \n  gcr.io/google_containers/hyperkube:v1.0.1 /hyperkube kubelet --api-\nservers=http://${MASTER_IP}:8080 --v=2 --address=0.0.0.0 --enable-server --hostname-\noverride=$(hostname -i) --cluster-dns=10.0.0.10 --cluster-domain=cluster.local \n The preceding command is to be run on the worker node as shown in Figure  14-28 . \n Figure 14-28.  Starting Kubernetes on the Worker Node \n \n",
      "content_length": 1019,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 428,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n419\n Running the Service Proxy \n The  service proxy on the worker node is also run with the same command as for the master node \nexcept that the Master’s Ip parameter  -- master= http://127.0.0.1:8080 should be replaced with \n --master=http://${MASTER_IP}:8080 . \n sudo docker run -d --net=host --privileged gcr.io/google_containers/hyperkube:v1.0.1 /\nhyperkube proxy --master=http://${MASTER_IP}:8080 --v=2 \n The service proxy gets started as shown in Figure  14-29 . \n Figure 14-29.  Starting Service Proxy on the Worker Node \n Testing the Kubernetes Cluster \n From the Master node, not the  worker node that was being configured in the preceding commands, list the \nnodes in the cluster. \n kubectl get nodes \n Two nodes get listed as shown in Figure  14-30 : the master node and the worker node. \n Figure 14-30.  Listing a Two-Node Cluster \n Add more nodes as required using the same procedure as discussed in this section Adding a Worker Node. \n Running an Application on the Cluster \n To test the cluster run an application on the command line using kubectl. As an example, run the Docker \nimage “nginx” with the following command. \n kubectl -s http://localhost:8080 run nginx --image=nginx --port=80 \n Subsequently list the Pods. \n kubectl get pods \n \n \n",
      "content_length": 1320,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 429,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n420\n Figure 14-31.  Installing an Application on the Cluster \n Figure 14-32.  Creating a Service \n The nginx application container is created and the nginx replication controller is created with default of \n1 replicas as shown in Figure  14-31 . One pod gets listed, also shown in Figure  14-31 . Initially the Pod could \nbe listed as Pending status. Run the preceding command after a few seconds to list the Pod as running and \nready. To find on which instance/s (node/s) in the cluster the Pod/s is/are running on, run the command. \n kubectl get pods -o wide. \n  Exposing the Application as a Service \n To expose the replication controller nginx as a service run the following command. \n kubectl expose rc nginx --port=80 \n The nginx service gets created as shown in Figure  14-32 . \n List the services with the following command. \n kubectl get services \n To be able to invoke the service obtain the first cluster Ip with the following command as shown in \nFigure  14-33 . \n kubectl get svc nginx --template={{.spec.clusterIP}} \n \n \n",
      "content_length": 1095,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 430,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n421\n Figure 14-33.  Invoking a Web Server with Curl \n The HTML returned from the nginx application is output as shown in Figure  14-34 . \n Invoke the web server using the cluster Ip returned, 10.0.0.99. \n curl 10.0.0.99 \n \n",
      "content_length": 283,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 431,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n422\n  Testing the Application in a Browser \n To invoke the service endpoint in a browser, set port forwarding from  10.0.0.99:80 endpoint to \n localhost:80 . \n ssh -i docker.pem -f -nNT -L 80:10.0.0.99:80 ubuntu@ec2-52-91-243-99.compute-1.amazonaws.com \n Port forwarding gets set as shown in Figure  14-35 . \n Figure 14-35.  Setting Port Forwarding \n Figure 14-34.  The HTML generated by the Application \n \n \n",
      "content_length": 469,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 432,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n423\n Invoke the nginx application in a local browser with url  http://localhost as shown in Figure  14-36 . \n Figure 14-36.  Invoking a Service Endpoint in a Browser \n  Scaling the Application \n Scaling is a common usage pattern of Replication Controller. The nginx replication controller may be scaled \nwith the  kubectl scale command. As an example, scale to 3 replicas. \n kubectl scale rc nginx --replicas=3 \n Subsequently list the Pods. \n kubectl get pods \n \n",
      "content_length": 523,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 433,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n424\n Figure 14-37.  Listing the Pods \n Describe the service with the following command. \n kubectl describe svc nginx \n Three service endpoints get listed as shown in Figure  14-38 . \n Figure 14-38.  Describing the Service \n To be able to invoke each of the service endpoints in a browser on a local machine, set the port forwarding. \n ssh -i docker.pem -f -nNT -L 8081:10.1.34.2:80 ubuntu@ec2-52-91-243-99.compute-1.amazonaws.com \n ssh -i docker.pem -f -nNT -L 8082:10.1.35.2:80 ubuntu@ec2-52-91-243-99.compute-1.amazonaws.com \n ssh -i docker.pem -f -nNT -L 8083:10.1.35.3:80 ubuntu@ec2-52-91-243-99.compute-1.amazonaws.com \n An output of “scaled” indicates that the replication controller has been scaled. Three Pods get listed as \nshown in Figure  14-37 . \n \n \n",
      "content_length": 823,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 434,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n425\n Port forwarding gets set as shown in Figure  14-39 . \n Figure 14-39.  Setting port Forwarding for the additional Service Endpoints \n Figure 14-40.  Invoking a Service Endpoint in a Browser \n The service endpoints may be invoked in a local browser. For example the url  http://localhost:8081 \ninvokes one of the service endpoints as shown in Figure  14-40 . \n \n \n",
      "content_length": 427,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 435,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n426\n Similarly, the url  http://localhost:8082 invokes another service endpoint as shown in Figure  14-41 . \n Figure 14-41.  Invoking another Service Endpoint in a Browser \n Similarly, the url  http://localhost:8083 invokes the third service endpoint as shown in Figure  14-42 . \n \n",
      "content_length": 342,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 436,
      "content": "CHAPTER 14 ■ INSTALLING KUBERNETES ON A MULTI-NODE CLUSTER \n427\n Summary \n In this chapter we installed Kubernetes on a multi-node cluster. The multi-node configuration makes \nuse of flannel for networking instead of the default networking provided by Docker. First, we installed \nKubernetes on the master node. Using the Master’s Ip Address we installed Kubernetes on a worker node, \nas a result creating a two-node cluster. As many worker nodes as required may be added using the same \nprocedure. We created an application using the nginx Docker image and invoked the application on the \ncommand line using curl, and in a local browser using port forwarding. We also scaled the application. \nIn a single-node cluster an application runs on the master node itself. In a multi-node cluster an application \nruns on both the worker nodes and the master node. This chapter concludes the book on Kubernetes \nMicroservices with Docker. \n Figure 14-42.  Invoking a Third Service Endpoint in a Browser \n \n",
      "content_length": 998,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 437,
      "content": "429\n© Deepak Vohra 2016 \nD. Vohra, Kubernetes Microservices with Docker, DOI 10.1007/978-1-4842-1907-2\n\n\n\n A, B \n Apache Cassandra \n Amazon EC2 instance , 201 \n cluster declaratively \n CatalogKeyspace , 215, 219 \n CQL shell , 215 \n data deletion , 218 \n DROP TABLE clause , 218 \n interactive shell , 213 \n Pod , 212 \n replication controller creation , 206 \n scaling database , 211 \n service creation , 203 \n table creation , 216 \n truncate table , 218 \n volume creation , 219 \n cluster imperatively \n replication controller creation , 225 \n replication controller deletion , 229 \n scaling database , 228 \n service creation , 227 \n service deletion , 230 \n Docker engine , 202 \n dynamic column \nspecifi cation , 201 \n fl exible schema \ndata model , 201 \n kubectl , 202 \n Kubernetes , 202 \n Apache Hadoop , 277 \n cluster declaratively , 278 \n interactive shell , 286 \n logs list , 284 \n MapReduceapplication \n(see  MapReduce application ) \n Pods list , 283 \n replication controller creation , 281 \n scaling , 285 \n service creation , 279 \n cluster imperatively , 307 \n interactive shell , 310 \n Pods list , 308 \n replication controller creation , 307 \n scaling , 309 \n service creation , 309 \n environment settings , 277 \n Apache HBase , 302 \n Apache Hive , 296 \n Apache Kafka , 347 \n confi guration properties , 364 \n consumer , 367 \n docker image, modifi cation , 349 \n environment settings , 348 \n interactive shell , 363 \n messages , 367 \n pod describing , 362 \n pods list , 361 \n producer , 367 \n replication controller creation , 358 \n replication controller deletion , 370 \n scaling , 368 \n service creation , 355 \n topic creation , 366 \n Apache Solr , 313 \n Admin Console , 338 \n core creation , 328 \n environment settings , 314 \n indexing documents , 329 \n interactive shell , 325 \n logs list , 323 \n pods list , 321 \n port forwarding , 337 \n replication controller creation , 318 \n replication controller describing , 322 \n scaling , 344 \n service creation , 315 \n service describing , 317 \n service endpoints , 317 \n using REST client , 332 \n Index \n",
      "content_length": 2063,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 438,
      "content": "■ INDEX\n430\n\n\n\n C, D \n Cassandra Query Language (CQL) , 215, 217 \n cbtransfer tool , 265 \n Couchbase , 231 \n cluster declaratively , 234 \n catalog2 document , 262 \n Data Buckets Tab , 255 \n default fi elds , 260 \n endpoints , 244 \n interactive shell , 264 \n JSON document , 261 \n logs list , 243 \n Pod , 234, 243 \n port forwarding , 245 \n replication controller creation , 239 \n server confi guration , 247 \n service creation , 237 \n service describing , 244 \n web console , 246 \n cluster imperatively , 266 \n Pods list , 266 \n port forwarding , 272 \n replication controller creation , 266 \n replication controller deletion , 270 \n scaling , 269 \n service creation , 268 \n web console , 272 \n environment settings , 231 \n Docker engine , 233 \n Kubernetes service , 234 \n Public IP Address , 232 \n Ubuntu instance , 233 \n Custom command \n Args fi elds , 80 \n CMD instruction , 78–79 \n ENTRYPOINT entry , 78 \n environment setting , 77–78 \n\n\n\n E, F, G, H, I, J \n Environment variables \n Args mapping , 92 \n command mapping , 84, 89 \n defi nition , 80 \n Docker image , 83 \n ENTRYPOINT , 84 \n\n\n\n K, L \n Kubernetes \n application creation \n cluster , 40 \n hello-world application , 48, 68 \n label , 41 \n namespace , 41 \n nodes , 39 \n Pod , 40, 46, 58 \n replication controller , 40, 43, 53, 64 \n scaling , 52, 70 \n selector , 41 \n service , 40, 45, 53, 61 \n volume , 41 \n benefi ts , 42 \n Docker \n adding gpg key , 6 \n apt package index , 8 \n apt sources , 6 \n containers , 30–32 \n Default Package Confi guration , 12 \n docker.list fi le , 6 \n engine installation , 14–15 \n linux-image-extra package , 10 \n lxc-docker and \nlxc-docker-virtual-package , 8–9 \n message prompt , 11, 13 \n package manager , 10 \n repository verifi cation , 9 \n sudo apt-get update , 13 \n Ubuntu distribution , 7 \n environment setting , 4, 42 \n etcd , 24–25 \n installation \n /boot directory , 16–17 \n command-line parameters , 19 \n components , 15 \n CONFIG_MEMCG_\nSWAP_ENABLED setting , 19 \n directory creation , 16 \n Docker engine , 16 \n GRUB_CMDLINE_LINUX , 20–21 \n grub confi guration fi le , 22 \n kernel confi guration , 16–18 \n service proxy , 15 \n settings, updation , 22–23 \n testing , 38 \n Ubuntu Amazon EC2 instance , 22 \n JSON \n curl command , 75 \n defi nition , 70 \n hello-rc.json fi le , 73 \n hello-world replication controller , 73–74 \n hello-world-service.json fi le , 70–72 \n HTML output , 76 \n replication controller defi nition fi le , 72 \n kubectl , 35 \n Kubernetes master , 28 \n local machine solutions , 3 \n nodes , 36 \n service proxy , 29, 35 \n",
      "content_length": 2545,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 439,
      "content": "■ INDEX\n431\n\n\n\n M, N \n MapReduce application , 287 \n hdfs command , 287 \n input directory , 288 \n vi editor , 288 \n wordcount application , 292, 295 \n wq command , 290 \n Master node \n bootstrap Docker , 402 \n etcd set up , 403 \n Flannel set up , 404 \n Kubernetes , 409, 411 \n testing , 412 \n MongoDB database , 167 \n cluster declaratively , 169 \n adding documents , 184 \n capped collection , 183 \n catalog collection , 183 \n database creation , 182 \n Docker , 168 \n drop command , 188 \n exit command , 188 \n fi nding documents , 186 \n host port , 190 \n interactive shell , 180 \n Kubernetes Pod , 169 \n logs list , 178 \n Mongo shell , 182 \n replication controller creation , 173 \n replication controller deletion , 189 \n scaling , 189 \n service defi nition fi le , 169 \n service deletion , 190 \n Ubuntu instance , 168 \n volume (see  Volume ) \n cluster imperatively , 194 \n logs list , 196 \n Pods , 195 \n replication controller creation , 194 \n replication controller deletion , 200 \n scaling , 198 \n service creation , 197 \n environment settings , 167 \n Multi-node cluster , 399 \n components , 400 \n environment settings , 400 \n execution , 419 \n exposing , 420 \n masternode (see  Master node ) \n scaling , 423 \n testing , 422 \n workernode (see  Worker node ) \n MySQL database \n database table , 110 \n environment setting , 98 \n interactive shell , 107, 111 \n logs , 104–106 \n MySQL CLI , 110–111 \n Pods , 104 \n replication controller , 103, 114 \n scaling , 113 \n service , 99, 106–107 \n\n\n\n O \n Oracle database , 141 \n environment settings , 141 \n instance declaratively , 148 \n database connection , 160 \n exit command , 163 \n interactive shell , 159 \n Pod creation , 148 \n replication controller creation , 153 \n replication level , 156 \n scaling , 158 \n service creation , 150 \n table creation , 162 \n user creation , 161 \n instance imperatively , 142 \n logs list , 144 \n replication controller creation , 143 \n replication controller deletion , 147 \n scaling , 147 \n service creation , 145 \n service deletion , 147 \n\n\n\n P, Q, R \n Pod \n application types , 376 \n docker containers , 383 \n environment settings , 377 \n Hello world application \n browser , 389 \n command line , 385 \n interactive shell , 386 \n lists , 382 \n number of containers , 376 \n overview , 375 \n port forwarding , 387 \n PostgreSQL command , 387 \n",
      "content_length": 2331,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 440,
      "content": "■ INDEX\n432\n replication controller creation , 380 \n scaling (see  Scaling ) \n service creation , 378 \n service describing , 379, 384 \n PostgreSQL database , 115 \n cluster declaratively , 117 \n interactive shell , 123 \n logs list , 128 \n Pods , 123 \n psql CLI Shell , 125–126 \n replication controller creation , 119 \n replication controller deletion , 130 \n scaling , 127 \n service creation , 117 \n stop command , 131 \n table creation , 125 \n cluster imperatively , 131 \n Docker containers , 134 \n Pods , 132 \n psql shell , 135 \n replication controller creation , 132 \n replication \ncontroller deletion , 138 \n scaling , 137 \n service creation , 133 \n stop command , 139 \n table creation , 134 \n environment settings , 115 \n Docker engine , 116 \n Kubernetes service , 117 \n Ubuntu instance , 116 \n\n\n\n S, T, U \n Scaling \n Docker containers , 391 \n Hello world application \n browser , 393 \n command line , 394 \n replication controller deletion , 396 \n service describing , 392 \n 3 replicas , 390 \n\n\n\n V \n Volume \n confi guration , 177 \n defi nition , 176 \n empty directory , 178 \n types , 176 \n usages , 176 \n\n\n\n W, X, Y \n Worker node , 412 \n fl annel set up , 413 \n Kubernetes , 418 \n MASTER_IP , 412 \n service proxy , 419 \n testing , 419 \n\n\n\n Z \n Zookeeper server , 349 \nPod (cont.)\n",
      "content_length": 1299,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}