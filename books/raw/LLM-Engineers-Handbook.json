{
  "metadata": {
    "title": "LLM-Engineers-Handbook",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 523,
    "conversion_date": "2025-11-28T11:59:41.862820",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "LLM-Engineers-Handbook.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 2-10)",
      "start_page": 2,
      "end_page": 10,
      "detection_method": "topic_boundary",
      "content": "LLM Engineer’s Handbook\nCopyright © 2024 Packt Publishing\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in \nany form or by any means, without the prior written permission of the publisher, except in the case of brief \nquotations embedded in critical articles or reviews.\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information \npresented. However, the information contained in this book is sold without warranty, either express or \nimplied. Neither the authors, nor Packt Publishing or its dealers and distributors, will be held liable for any \ndamages caused or alleged to have been caused directly or indirectly by this book.\nPackt Publishing has endeavored to provide trademark information about all of the companies and products \nmentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee \nthe accuracy of this information.\nSenior Publishing Product Manager: Gebin George\nAcquisition Editor – Peer Reviews: Swaroop Singh\nProject Editor: Amisha Vathare\nContent Development Editor: Tanya D’cruz\nCopy Editor: Safis Editing\nTechnical Editor: Karan Sonawane\nProofreader: Safis Editing\nIndexer: Manju Arasan\nPresentation Designer: Rajesh Shirsath\nDeveloper Relations Marketing Executive: Anamika Singh\nFirst published: October 2024\nProduction reference: 2171024\nPublished by Packt Publishing Ltd. \nGrosvenor House\n11 St Paul’s Square\nBirmingham \nB3 1RB, UK.\nISBN 978-1-83620-007-9\nwww.packt.com\n\n\nForewords\nAs my co-founder at Hugging Face, Clement Delangue, and I often say, AI is becoming the default \nway of building technology.\nOver the past 3 years, LLMs have already had a profound impact on technology, and they are \nbound to have an even greater impact in the coming 5 years. They will be embedded in more and \nmore products and, I believe, at the center of any human activity based on knowledge or creativity.\nFor instance, coders are already leveraging LLMs and changing the way they work, focusing on \nhigher-order thinking and tasks while collaborating with machines. Studio musicians rely on \nAI-powered tools to explore the musical creativity space faster. Lawyers are increasing their impact \nthrough retrieval-augmented generation (RAG) and large databases of case law.\nAt Hugging Face, we’ve always advocated for a future where not just one company or a small \nnumber of scientists control the AI models used by the rest of the population, but instead for a \nfuture where as many people as possible—from as many different backgrounds as possible—are \ncapable of diving into how cutting-edge machine learning models actually work.\nMaxime Labonne and Paul Iusztin have been instrumental in this movement to democratize \nLLMs by writing this book and making sure that as many people as possible can not only use \nthem but also adapt them, fine-tune them, quantize them, and make them efficient enough to \nactually deploy in the real world.\nTheir work is essential, and I’m glad they are making this resource available to the community. \nThis expands the convex hull of human knowledge.\nJulien Chaumond\nCo-founder and CTO, Hugging Face\n\n\nAs someone deeply immersed in the world of machine learning operations, I’m thrilled to en-\ndorse The LLM Engineer’s Handbook. This comprehensive guide arrives at a crucial time when the \ndemand for LLM expertise is skyrocketing across industries.\nWhat sets this book apart is its practical, end-to-end approach. By walking readers through the \ncreation of an LLM Twin, it bridges the often daunting gap between theory and real-world ap-\nplication. From data engineering and model fine-tuning to advanced topics like RAG pipelines \nand inference optimization, the authors leave no stone unturned.\nI’m particularly impressed by the emphasis on MLOps and LLMOps principles. As organizations \nincreasingly rely on LLMs, understanding how to build scalable, reproducible, and robust systems \nis paramount. The inclusion of orchestration strategies and cloud integration showcases the \nauthors’ commitment to equipping readers with truly production-ready skills.\nWhether you’re a seasoned ML practitioner looking to specialize in LLMs or a software engineer \naiming to break into this exciting field, this handbook provides the perfect blend of foundational \nknowledge and cutting-edge techniques. The clear explanations, practical examples, and focus on \nbest practices make it an invaluable resource for anyone serious about mastering LLM engineering.\nIn an era where AI is reshaping industries at breakneck speed, The LLM Engineer’s Handbook stands \nout as an essential guide for navigating the complexities of large language models. It’s not just \na book; it’s a roadmap to becoming a proficient LLM engineer in today’s AI-driven landscape.\nHamza Tahir\nCo-founder and CTO, ZenML\n\n\nContributors\nAbout the authors\nPaul Iusztin is a senior ML and MLOps engineer with over seven years of experience building \nGenAI, Computer Vision and MLOps solutions. His latest contribution was at Metaphysic, where he \nserved as one of their core engineers in taking large neural networks to production. He previously \nworked at CoreAI, Everseen, and Continental. He is the Founder of Decoding ML, an educational \nchannel on production-grade ML that provides posts, articles, and open-source courses to help \nothers build real-world ML systems. \nMaxime Labonne is the Head of Post-Training at Liquid AI. He holds a PhD. in ML from the \nPolytechnic Institute of Paris and is recognized as a Google Developer Expert in AI/ML. As an \nactive blogger, he has made significant contributions to the open-source community, including \nthe LLM Course on GitHub, tools such as LLM AutoEval, and several state-of-the-art models like \nNeuralDaredevil. He is the author of the best-selling book Hands-On Graph Neural Networks Using \nPython, published by Packt.\nI want to thank my family and partner. Your unwavering support and patience made this book possible.\n\n\nAbout the reviewer\nRany ElHousieny is an AI solutions architect and AI engineering manager with over two decades \nof experience in AI, NLP, and ML. Throughout his career, he has focused on the development and \ndeployment of AI models, authoring multiple articles on AI systems architecture and ethical AI de-\nployment. He has led groundbreaking projects at companies like Microsoft, where he spearheaded \nadvancements in NLP and the Language Understanding Intelligent Service (LUIS). Currently, he \nplays a pivotal role at Clearwater Analytics, driving innovation in GenAI and AI-driven financial \nand investment management solutions.\nI would like to thank Clearwater Analytics for providing a supportive and learning environment that fosters \ngrowth and innovation. The vision of our leaders, always staying ahead with the latest technologies, has been \na constant source of inspiration. Their commitment to AI advancements made my experience of reviewing \nthis book insightful and enriching. Special thanks to my family for their ongoing encouragement throughout \nthis journey.\n\n\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n\n\nTable of Contents\nPreface  \n xxi\nChapter 1: Understanding the LLM Twin Concept and Architecture  \n 1\nUnderstanding the LLM Twin concept                                                                                  2\nWhat is an LLM Twin? • 2\nWhy building an LLM Twin matters • 3\nWhy not use ChatGPT (or another similar chatbot)? • 5\nPlanning the MVP of the LLM Twin product                                                                          6\nWhat is an MVP? • 6\nDefining the LLM Twin MVP • 7\nBuilding ML systems with feature/training/inference pipelines                                            8\nThe problem with building ML systems • 8\nThe issue with previous solutions • 10\nThe solution – ML pipelines for ML systems • 13\nThe feature pipeline • 14\nThe training pipeline • 14\nThe inference pipeline • 14\nBenefits of the FTI architecture • 15\nDesigning the system architecture of the LLM Twin                                                            16\nListing the technical details of the LLM Twin architecture • 16\nHow to design the LLM Twin architecture using the FTI pipeline design • 17\nData collection pipeline • 19\n\n\nTable of Contents\nx\nFeature pipeline • 19\nTraining pipeline • 21\nInference pipeline • 22\nFinal thoughts on the FTI design and the LLM Twin architecture • 22\nSummary                                                                                                                             23\nReferences                                                                                                                           23\nChapter 2: Tooling and Installation  \n 25\nPython ecosystem and project installation                                                                          26\nPoetry: dependency and virtual environment management • 27\nPoe the Poet: task execution tool • 29\nMLOps and LLMOps tooling                                                                                                30\nHugging Face: model registry • 31\nZenML: orchestrator, artifacts, and metadata • 32\nOrchestrator • 33\nArtifacts and metadata • 39\nHow to run and configure a ZenML pipeline • 43\nComet ML: experiment tracker • 45\nOpik: prompt monitoring • 46\nDatabases for storing unstructured and vector data                                                            47\nMongoDB: NoSQL database • 47\nQdrant: vector database • 47\nPreparing for AWS                                                                                                               48\nSetting up an AWS account, an access key, and the CLI • 48\nSageMaker: training and inference compute • 50\nWhy AWS SageMaker? • 51\nSummary                                                                                                                             52\nReferences                                                                                                                           53\nChapter 3: Data Engineering  \n 55\nDesigning the LLM Twin’s data collection pipeline                                                             56\n\n\nTable of Contents\nxi\nImplementing the LLM Twin’s data collection pipeline • 61\nZenML pipeline and steps • 61\nThe dispatcher: How do you instantiate the right crawler? • 66\nThe crawlers • 69\nBase classes • 69\nGitHubCrawler class • 73\nCustomArticleCrawler class • 75\nMediumCrawler class • 77\nThe NoSQL data warehouse documents • 79\nThe ORM and ODM software patterns • 80\nImplementing the ODM class • 82\nData categories and user document classes • 87\nGathering raw data into the data warehouse                                                                       89\nTroubleshooting • 94\nSelenium issues • 95\nImport our backed-up data • 95\nSummary                                                                                                                             96\nReferences                                                                                                                           96\nChapter 4: RAG Feature Pipeline  \n 99\nUnderstanding RAG                                                                                                           100\nWhy use RAG? • 100\nHallucinations • 101\nOld information • 101\nThe vanilla RAG framework • 101\nIngestion pipeline • 104\nRetrieval pipeline • 105\nGeneration pipeline • 105\nWhat are embeddings? • 107\nWhy embeddings are so powerful • 109\n",
      "page_number": 2
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 11-18)",
      "start_page": 11,
      "end_page": 18,
      "detection_method": "topic_boundary",
      "content": "Table of Contents\nxii\nHow are embeddings created? • 111\nApplications of embeddings • 114\nMore on vector DBs • 115\nHow does a vector DB work? • 115\nAlgorithms for creating the vector index • 116\nDB operations • 116\nAn overview of advanced RAG                                                                                             117\nPre-retrieval • 119\nRetrieval • 122\nPost-retrieval • 124\nExploring the LLM Twin’s RAG feature pipeline architecture                                             127\nThe problem we are solving • 127\nThe feature store • 128\nWhere does the raw data come from? • 128\nDesigning the architecture of the RAG feature pipeline • 129\nBatch pipelines • 130\nBatch versus streaming pipelines • 130\nCore steps • 134\nChange data capture: syncing the data warehouse and feature store • 136\nWhy is the data stored in two snapshots? • 138\nOrchestration • 138\nImplementing the LLM Twin’s RAG feature pipeline                                                          139\nSettings • 139\nZenML pipeline and steps • 140\nQuerying the data warehouse • 143\nCleaning the documents • 146\nChunk and embed the cleaned documents • 147\nLoading the documents to the vector DB • 150\nPydantic domain entities • 150\nOVM • 154\nThe dispatcher layer • 160\n\n\nTable of Contents\nxiii\nThe handlers • 162\nThe cleaning handlers • 163\nThe chunking handlers • 165\nThe embedding handlers • 169\nSummary                                                                                                                            173\nReferences                                                                                                                          174\nChapter 5: Supervised Fine-Tuning  \n 177\nCreating an instruction dataset                                                                                         178\nGeneral framework • 178\nData quantity • 180\nData curation • 182\nRule-based filtering • 182\nData deduplication • 184\nData decontamination • 185\nData quality evaluation • 186\nData exploration • 189\nData generation • 191\nData augmentation • 193\nCreating our own instruction dataset                                                                                196\nExploring SFT and its techniques                                                                                       206\nWhen to fine-tune • 206\nInstruction dataset formats • 208\nChat templates • 208\nParameter-efficient fine-tuning techniques • 211\nFull fine-tuning • 211\nLoRA • 213\nQLoRA • 215\nTraining parameters • 216\nLearning rate and scheduler • 216\nBatch size • 216\n\n\nTable of Contents\nxiv\nMaximum length and packing • 217\nNumber of epochs • 218\nOptimizers • 218\nWeight decay • 219\nGradient checkpointing • 219\nFine-tuning in practice                                                                                                       219\nSummary                                                                                                                           226\nReferences                                                                                                                         227\nChapter 6: Fine-Tuning with Preference Alignment  \n 229\nUnderstanding preference datasets                                                                                   230\nPreference data • 230\nData quantity • 232\nData generation and evaluation • 233\nGenerating preferences • 233\nTips for data generation • 234\nEvaluating preferences • 235\nCreating our own preference dataset                                                                                 237\nPreference alignment                                                                                                         245\nReinforcement Learning from Human Feedback • 246\nDirect Preference Optimization • 248\nImplementing DPO                                                                                                            250\nSummary                                                                                                                           257\nReferences                                                                                                                         258\nChapter 7: Evaluating LLMs  \n 261\nModel evaluation                                                                                                                261\nComparing ML and LLM evaluation • 262\nGeneral-purpose LLM evaluations • 263\nDomain-specific LLM evaluations • 265\nTask-specific LLM evaluations • 267\n\n\nTable of Contents\nxv\nRAG evaluation                                                                                                                   271\nRagas • 272\nARES • 274\nEvaluating TwinLlama-3 1-8B                                                                                           275\nGenerating answers • 276\nEvaluating answers • 278\nAnalyzing results • 283\nSummary                                                                                                                           286\nReferences                                                                                                                         287\nChapter 8: Inference Optimization  \n 289\nModel optimization strategies                                                                                           290\nKV cache • 291\nContinuous batching • 294\nSpeculative decoding • 295\nOptimized attention mechanisms • 297\nModel parallelism                                                                                                              298\nData parallelism • 299\nPipeline parallelism • 300\nTensor parallelism • 301\nCombining approaches • 303\nModel quantization                                                                                                           303\nIntroduction to quantization • 304\nQuantization with GGUF and llama.cpp • 309\nQuantization with GPTQ and EXL2 • 311\nOther quantization techniques • 313\nSummary                                                                                                                            314\nReferences                                                                                                                          315\nChapter 9: RAG Inference Pipeline  \n 317\nUnderstanding the LLM Twin’s RAG inference pipeline                                                    318\n\n\nTable of Contents\nxvi\nExploring the LLM Twin’s advanced RAG techniques                                                         321\nAdvanced RAG pre-retrieval optimizations: query expansion and self-querying • 324\nQuery expansion • 324\nSelf-querying • 328\nAdvanced RAG retrieval optimization: filtered vector search • 332\nAdvanced RAG post-retrieval optimization: reranking • 334\nImplementing the LLM Twin’s RAG inference pipeline                                                      338\nImplementing the retrieval module • 339\nBringing everything together into the RAG inference pipeline • 346\nSummary                                                                                                                            351\nReferences                                                                                                                          351\nChapter 10: Inference Pipeline Deployment  \n 355\nCriteria for choosing deployment types                                                                             356\nThroughput and latency • 356\nData • 357\nUnderstanding inference deployment types                                                                      359\nOnline real-time inference • 360\nAsynchronous inference • 361\nOffline batch transform • 362\nMonolithic versus microservices architecture in model serving                                        363\nMonolithic architecture • 365\nMicroservices architecture • 365\nChoosing between monolithic and microservices architectures • 367\nExploring the LLM Twin’s inference pipeline deployment strategy                                   368\nThe training versus the inference pipeline • 371\nDeploying the LLM Twin service                                                                                       372\nImplementing the LLM microservice using AWS SageMaker • 373\nWhat are Hugging Face’s DLCs? • 373\nConfiguring SageMaker roles • 374\n\n\nTable of Contents\nxvii\nDeploying the LLM Twin model to AWS SageMaker • 375\nCalling the AWS SageMaker Inference endpoint • 386\nBuilding the business microservice using FastAPI • 390\nAutoscaling capabilities to handle spikes in usage                                                             393\nRegistering a scalable target • 396\nCreating a scalable policy • 397\nMinimum and maximum scaling limits • 398\nCooldown period • 398\nSummary                                                                                                                           399\nReferences                                                                                                                        400\nChapter 11: MLOps and LLMOps  \n 401\nThe path to LLMOps: Understanding its roots in DevOps and MLOps                               402\nDevOps • 403\nThe DevOps lifecycle • 403\nThe core DevOps concepts • 404\nMLOps • 405\nMLOps core components • 407\nMLOps principles • 407\nML vs. MLOps engineering • 409\nLLMOps • 410\nHuman feedback • 411\nGuardrails • 411\nPrompt monitoring • 413\nDeploying the LLM Twin’s pipelines to the cloud                                                               415\nUnderstanding the infrastructure • 416\nSetting up MongoDB • 418\nSetting up Qdrant • 419\nSetting up the ZenML cloud • 421\nContainerize the code using Docker • 424\n\n\nTable of Contents\nxviii\nRun the pipelines on AWS • 428\nTroubleshooting the ResourceLimitExceeded error after running a ZenML pipeline \non SageMaker • 432\nAdding LLMOps to the LLM Twin                                                                                      434\nLLM Twin’s CI/CD pipeline flow • 434\nMore on formatting errors • 436\nMore on linting errors • 436\nQuick overview of GitHub Actions • 437\nThe CI pipeline • 438\nGitHub Actions CI YAML file • 438\nThe CD pipeline • 442\nTest out the CI/CD pipeline • 445\nThe CT pipeline • 446\nInitial triggers • 448\nTrigger downstream pipelines • 449\nPrompt monitoring • 451\nAlerting • 457\nSummary                                                                                                                           458\nReferences                                                                                                                         459\nAppendix: MLOps Principles  \n 461\n1  Automation or operationalization                                                                                  461\n2  Versioning                                                                                                                      463\n3  Experiment tracking                                                                                                      464\n4  Testing                                                                                                                           464\nTest types • 464\nWhat do we test? • 465\nTest examples • 465\n5  Monitoring                                                                                                                     468\nLogs • 468\nMetrics • 468\n\n\nTable of Contents\nxix\nSystem metrics • 469\nModel metrics • 469\nDrifts • 469\nMonitoring vs. observability • 472\nAlerts • 473\n6  Reproducibility                                                                                                              473\nOther Books You May Enjoy  \n 477\nIndex  \n 481\n",
      "page_number": 11
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 19-27)",
      "start_page": 19,
      "end_page": 27,
      "detection_method": "topic_boundary",
      "content": "Preface\nThe field of LLM engineering has rapidly emerged as a critical area in artificial intelligence and \nmachine learning. As LLMs continue to revolutionize natural language processing and genera-\ntion, the demand for professionals who can effectively implement, optimize, and deploy these \nmodels in real-world scenarios has grown exponentially. LLM engineering encompasses a wide \nrange of disciplines, from data preparation and model fine-tuning to inference optimization and \nproduction deployment, requiring a unique blend of software engineering, machine learning \nexpertise, and domain knowledge.\nMachine Learning Operations (MLOps) plays a crucial role in the successful implementation of \nLLMs in production environments. MLOps extends the principles of DevOps to machine learning \nprojects, focusing on automating and streamlining the entire ML lifecycle. For LLMs, MLOps is \nparticularly important due to the complexity and scale of these models. It addresses challeng-\nes such as managing large datasets, handling model versioning, ensuring reproducibility, and \nmaintaining model performance over time. By incorporating MLOps practices, LLM projects can \nachieve greater efficiency, reliability, and scalability, ultimately leading to more successful and \nimpactful deployments.\nThe LLM Engineer’s Handbook is a comprehensive guide to applying best practices to the new \nfield of LLM engineering. Throughout the chapters, readers will find simplified key concepts, \npractical techniques, and experts tips for every stage of the LLM lifecycle. The book covers topics \nsuch as data engineering, supervised fine-tuning, model evaluation, inference optimization, and \nRetrieval-Augmented Generation (RAG) pipeline development.\nTo illustrate these concepts in action, an end-to-end project called the LLM Twin will be developed \nthroughout the book., with the goal of imitating someone’s writing style and personality. This \nuse case will demonstrate how to build a minimum viable product to solve a specific problem, \nusing various aspects of LLM engineering and MLOps.\n\n\nPreface\nxxii\nReaders can expect to gain a deeper understanding of how to collect and prepare data for LLMs, \nfine-tune models for specific tasks, optimize inference performance, and implement RAG pipelines. \nThey will learn how to evaluate LLM performance, align models with human preferences, and \ndeploy LLM-based applications. The book also covers essential MLOps principles and practices, \nenabling readers to build scalable, reproducible, and robust LLM applications.\nWho this book is for\nThis book is intended for a wide range of technology professionals and enthusiasts interested \nin the practical applications of LLMs. It’s ideal for software engineers aiming to transition into \nAI projects. While some familiarity with software development is beneficial, the book explains \nmany concepts from the ground up, making it accessible even to those who are new to AI and \nmachine learning.\nFor those already working with machine learning , this book will enhance your skills in imple-\nmenting and deploying LLM-based systems. We provide a deep dive into the fundamentals of \nMLOps, guiding you through the process of creating a minimum viable product using an open-\nsource LLM to solve real-world problems.\nWhat this book covers\nChapter 1, Understanding the LLM Twin Concept and Architecture, introduces the LLM Twin project, \nwhich is used throughout the book as an end-to-end example of a production-level LLM appli-\ncation, and defines the FTI architecture for building scalable ML systems and applies it to the \nLLM Twin use case.\nChapter 2, Tooling and Installation, presents Python, MLOps, and cloud tools used to build re-\nal-world LLM applications, such as an orchestrator, experiment tracker, prompt monitoring and \nLLM evaluation tool. It shows how to use and install them locally for testing and development.\nChapter 3, Data Engineering, shows the implementation of a data collection pipeline that scrapes \nmultiple sites, such as Medium, GitHub and Substack and stores the raw data in a data warehouse. \nIt emphasizes collecting raw data from dynamic sources over static datasets for real-world ML \napplications. \nChapter 4, RAG Feature Pipeline, introduces RAG fundamental concepts, such as embeddings, the \nvanilla RAG framework, vector databases, and how to optimize RAG applications. It applies the \nRAG theory by architecting and implementing LLM Twin’s RAG feature pipeline using software \nbest practices.\n\n\nPreface\nxxiii\nChapter 5, Supervised Fine-Tuning, explores the process of refining pre-trained language models \nfor specific tasks using instruction-answer pairs. It covers creating high-quality datasets, imple-\nmenting fine-tuning techniques like full fine-tuning, LoRA, and QLoRA, and provides a practical \ndemonstration of fine-tuning a Llama 3.1 8B model on a custom dataset.\nChapter 6, Fine-Tuning with Preference Alignment, introduces techniques for aligning language \nmodels with human preferences, focusing on Direct Preference Optimization (DPO). It covers \ncreating custom preference datasets, implementing DPO, and provides a practical demonstration \nof aligning the TwinLlama-3.1-8B model using the Unsloth library.\nChapter 7, Evaluating LLMs, details various methods for assessing the performance of language \nmodels and LLM systems. It introduces general-purpose and domain-specific evaluations and dis-\ncusses popular benchmarks. The chapter includes a practical evaluation of the TwinLlama-3.1-8B \nmodel using multiple criteria.\nChapter 8, Inference Optimization, covers key optimization strategies such as speculative decoding, \nmodel parallelism, and weight quantization. It discusses how to improve inference speed, reduce \nlatency, and minimize memory usage, introducing popular inference engines and comparing \ntheir features.\nChapter 9, RAG Inference Pipeline, explores advanced RAG techniques by implementing methods \nsuch as self-query, reranking, and filtered vector search from scratch. It covers designing and \nimplementing the LLM Twin’s RAG inference pipeline and a custom retrieval module similar to \nwhat you see in popular frameworks such as LangChain.\nChapter 10, Inference Pipeline Deployment, introduces ML deployment strategies, such as online, \nasynchronous and batch inference, which will help in architecting and deploying the LLM Twin \nfine-tuned model to AWS SageMaker and building a FastAPI microservice to expose the RAG \ninference pipeline as a RESTful API.\nChapter 11, MLOps and LLMOps, presents what LLMOps is, starting with its roots in DevOps and \nMLOps. This chapter explains how to deploy the LLM Twin project to the cloud, such as the ML \npipelines to AWS and shows how to containerize the code using Docker and build a CI/CD/CT \npipeline. It also adds a prompt monitoring layer on top of LLM Twin’s inference pipeline.\nAppendix, MLOps Principles, covers the six MLOps principles used to build scalable, reproducible, \nand robust ML applications.\n\n\nPreface\nxxiv\nTo get the most out of this book\nTo maximize your learning experience, you are expected to have, at the very least, a foundational \nunderstanding of software development principles and practices. Familiarity with Python pro-\ngramming is particularly beneficial, as the book’s examples and code snippets are predominantly \nin Python. While prior experience with machine learning concepts is advantageous, it is not \nstrictly necessary, as the book provides explanations for many fundamental AI and ML concepts. \nHowever, you should be comfortable with basic data structures, algorithms, and have some ex-\nperience working with APIs and cloud services. \nFamiliarity with version control systems like Git is assumed, as this book has a GitHub reposi-\ntory for code examples. While this book is designed to be accessible to those who are new to AI \nand LLMs, if you have some background in these areas, you will find it easier to grasp the more \nadvanced concepts and techniques we present.\nDownload the example code files\nThe code bundle for the book is hosted on GitHub at https://github.com/PacktPublishing/\nLLM-Engineers-Handbook. We also have other code bundles from our rich catalog of books and \nvideos available at https://github.com/PacktPublishing/. Check them out!\nDownload the color images\nWe also provide a PDF file that has color images of the screenshots/diagrams used in this book. \nYou can download it here: https://packt.link/gbp/9781836200079.\nConventions used\nThere are a number of text conventions used throughout this book.\nCodeInText: Indicates code words in text, database table names, folder names, filenames, file \nextensions, pathnames, dummy URLs, user input, and Twitter handles. For example: “In the \nformat_samples function, we apply the Alpaca chat template to each individual message.”\nA block of code is set as follows:\ndef format_samples(example):\n    example[\"prompt\"] = alpaca_template.format(example[\"prompt\"])\n    example[\"chosen\"] = example['chosen'] + EOS_TOKEN\n    example[\"rejected\"] = example['rejected'] + EOS_TOKEN\n    return {\"prompt\": example[\"prompt\"], \"chosen\": example[\"chosen\"], \n\"rejected\": example[\"rejected\"]}\n\n\nPreface\nxxv\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines or \nitems are set in bold:\ndef format_samples(example):\n    example[\"prompt\"] = alpaca_template.format(example[\"prompt\"])\n    example[\"chosen\"] = example['chosen'] + EOS_TOKEN\n    example[\"rejected\"] = example['rejected'] + EOS_TOKEN\n    return {\"prompt\": example[\"prompt\"], \"chosen\": example[\"chosen\"], \n\"rejected\": example[\"rejected\"]}\nAny command-line input or output is written as follows:\npoetry install --without aws\nBold: Indicates a new term, an important word, or words that you see on the screen. For instance, \nwords in menus or dialog boxes appear in the text like this. For example: “To do so, go to the Set-\ntings tab at the top of the forked repository in GitHub. In the left panel, in the Security section, \nclick on the Secrets and Variables toggle and, finally, click on Actions.”\nGet in touch\nFeedback from our readers is always welcome.\nGeneral feedback: Email feedback@packtpub.com and mention the book’s title in the subject of \nyour message. If you have questions about any aspect of this book, please email us at questions@\npacktpub.com.\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do \nhappen. If you have found a mistake in this book, we would be grateful if you reported this to us. \nPlease visit http://www.packtpub.com/submit-errata, click Submit Errata, and fill in the form.\nWarnings or important notes appear like this.\nTips and tricks appear like this.\n\n\nPreface\nxxvi\nPiracy: If you come across any illegal copies of our works in any form on the internet, we would \nbe grateful if you would provide us with the location address or website name. Please contact us \nat copyright@packtpub.com with a link to the material.\nIf you are interested in becoming an author: If there is a topic that you have expertise in and you \nare interested in either writing or contributing to a book, please visit http://authors.packtpub.\ncom.\nShare your thoughts\nOnce you’ve read LLM Engineer’s Handbook, First Edition, we’d love to hear your thoughts! Please \nclick here to go straight to the Amazon review page for this book and share your feedback.\nYour review is important to us and the tech community and will help us make sure we’re deliv-\nering excellent quality content.\n\n\nDownload a free PDF copy of this book\nThanks for purchasing this book!\nDo you like to read on the go but are unable to carry your print books everywhere?\nIs your eBook purchase not compatible with the device of your choice?\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical \nbooks directly into your application.\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free \ncontent in your inbox daily.\nFollow these simple steps to get the benefits:\n1. \nScan the QR code or visit the link below:\nhttps://packt.link/free-ebook/9781836200079\n2. \nSubmit your proof of purchase.\n3. \nThat’s it! We’ll send your free PDF and other benefits to your email directly.\n\n\n1\nUnderstanding the LLM Twin \nConcept and Architecture\nBy the end of this book, we will have walked you through the journey of building an end-to-end \nlarge language model (LLM) product. We firmly believe that the best way to learn about LLMs \nand production machine learning (ML) is to get your hands dirty and build systems. This book \nwill show you how to build an LLM Twin, an AI character that learns to write like a particular \nperson by incorporating its style, voice, and personality into an LLM. Using this example, we will \nwalk you through the complete ML life cycle, from data gathering to deployment and monitoring. \nMost of the concepts learned while implementing your LLM Twin can be applied in other LLM-\nbased or ML applications.\nWhen starting to implement a new product, from an engineering point of view, there are three \nplanning steps we must go through before we start building. First, it is critical to understand the \nproblem we are trying to solve and what we want to build. In our case, what exactly is an LLM Twin, \nand why build it? This step is where we must dream and focus on the “Why.” Secondly, to reflect \na real-world scenario, we will design the first iteration of a product with minimum functionality. \nHere, we must clearly define the core features required to create a working and valuable product. \nThe choices are made based on the timeline, resources, and team’s knowledge. This is where we \nbridge the gap between dreaming and focusing on what is realistic and eventually answer the \nfollowing question: “What are we going to build?”.\nFinally, we will go through a system design step, laying out the core architecture and design choices \nused to build the LLM system. Note that the first two components are primarily product-related, \nwhile the last one is technical and focuses on the “How.”\n\n\nUnderstanding the LLM Twin Concept and Architecture\n2\nThese three steps are natural in building a real-world product. Even if the first two do not require \nmuch ML knowledge, it is critical to go through them to understand “how” to build the product \nwith a clear vision. In a nutshell, this chapter covers the following topics:\n•\nUnderstanding the LLM Twin concept\n•\nPlanning the MVP of the LLM Twin product\n•\nBuilding ML systems with feature/training/inference pipelines\n•\nDesigning the system architecture of the LLM Twin\nBy the end of this chapter, you will have a clear picture of what you will learn to build throughout \nthe book.\nUnderstanding the LLM Twin concept\nThe first step is to have a clear vision of what we want to create and why it’s valuable to build it. \nThe concept of an LLM Twin is new. Thus, before diving into the technical details, it is essential \nto understand what it is, what we should expect from it, and how it should work. Having a solid \nintuition of your end goal makes it much easier to digest the theory, code, and infrastructure \npresented in this book.\nWhat is an LLM Twin?\nIn a few words, an LLM Twin is an AI character that incorporates your writing style, voice, and \npersonality into an LLM, which is a complex AI model. It is a digital version of yourself projected\ninto an LLM. Instead of a generic LLM trained on the whole internet, an LLM Twin is fine-tuned \non yourself. Naturally, as an ML model reflects the data it is trained on, this LLM will incorporate \nyour writing style, voice, and personality. We intentionally used the word “projected.” As with \nany other projection, you lose a lot of information along the way. Thus, this LLM will not be you; \nit will copy the side of you reflected in the data it was trained on.\nIt is essential to understand that an LLM reflects the data it was trained on. If you feed it Shake-\nspeare, it will start writing like him. If you train it on Billie Eilish, it will start writing songs in \nher style. This is also known as style transfer. This concept is prevalent in generating images, too. \nFor example, let’s say you want to create a cat image using Van Gogh’s style. We will leverage the \nstyle transfer strategy, but instead of choosing a personality, we will do it on our own persona.\nTo adjust the LLM to a given style and voice along with fine-tuning, we will also leverage various \nadvanced retrieval-augmented generation (RAG) techniques to condition the autoregressive \nprocess with previous embeddings of ourselves. \n",
      "page_number": 19
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 28-36)",
      "start_page": 28,
      "end_page": 36,
      "detection_method": "topic_boundary",
      "content": "Chapter 1\n3\nWe will explore the details in Chapter 5 on fine-tuning and Chapters 4 and 9 on RAG, but for now, \nlet’s look at a few examples to intuitively understand what we stated previously.\nHere are some scenarios of what you can fine-tune an LLM on to become your twin:\n• \nLinkedIn posts and X threads: Specialize the LLM in writing social media content.\n• \nMessages with your friends and family: Adapt the LLM to an unfiltered version of yourself.\n• \nAcademic papers and articles: Calibrate the LLM in writing formal and educative content.\n• \nCode: Specialize the LLM in implementing code as you would.\nAll the preceding scenarios can be reduced to one core strategy: collecting your digital data (or \nsome parts of it) and feeding it to an LLM using different algorithms. Ultimately, the LLM reflects \nthe voice and style of the collected data. Easy, right?\nUnfortunately, this raises many technical and moral issues. First, on the technical side, how can \nwe access this data? Do we have enough digital data to project ourselves into an LLM? What kind \nof data would be valuable? Secondly, on the moral side, is it OK to do this in the first place? Do \nwe want to create a copycat of ourselves? Will it write using our voice and personality, or just try \nto replicate it?\nRemember that the role of this section is not to bother with the “What” and “How” but with the \n“Why.” Let’s understand why it makes sense to have your LLM Twin, why it can be valuable, and \nwhy it is morally correct if we frame the problem correctly.\nWhy building an LLM Twin matters\nAs an engineer (or any other professional career), building a personal brand is more valuable than \na standard CV. The biggest issue with creating a personal brand is that writing content on plat-\nforms such as LinkedIn, X, or Medium takes a lot of time. Even if you enjoy writing and creating \ncontent, you will eventually run out of inspiration or time and feel like you need assistance. We \ndon’t want to transform this section into a pitch, but we have to understand the scope of this \nproduct/project clearly.\nWe want to build an LLM Twin to write personalized content on LinkedIn, X, Instagram, Sub-\nstack, and Medium (or other blogs) using our style and voice. It will not be used in any immoral \nscenarios, but it will act as your writing co-pilot. Based on what we will teach you in this book, \nyou can get creative and adapt it to various use cases, but we will focus on the niche of generating \nsocial media content and articles. Thus, instead of writing the content from scratch, we can feed \nthe skeleton of our main idea to the LLM Twin and let it do the grunt work. \n\n\nUnderstanding the LLM Twin Concept and Architecture\n4\nUltimately, we will have to check whether everything is correct and format it to our liking (more \non the concrete features in the Planning the MVP of the LLM Twin product section). Hence, we proj-\nect ourselves into a content-writing LLM Twin that will help us automate our writing process. It \nwill likely fail if we try to use this particular LLM in a different scenario, as this is where we will \nspecialize the LLM through fine-tuning, prompt engineering, and RAG.\nSo, why does building an LLM Twin matter? It helps you do the following:\n•\nCreate your brand\n•\nAutomate the writing process\n•\nBrainstorm new creative ideas\nAlso, it is critical to understand that building an LLM Twin is entirely moral. The LLM will be \nfine-tuned only on our personal digital data. We won’t collect and use other people’s data to try \nto impersonate anyone’s identity. We have a clear goal in mind: creating our personalized writing \ncopycat. Everyone will have their own LLM Twin with restricted access.\nOf course, many security concerns are involved, but we won’t go into that here as it could be a \nbook in itself.\nWhat’s the difference between a co-pilot and an LLM Twin?\nA co-pilot and digital twin are two different concepts that work together and can be \ncombined into a powerful solution:\n•\nThe co-pilot is an AI assistant or tool that augments human users in various \nprogramming, writing, or content creation tasks.\n•\nThe twin serves as a 1:1 digital representation of a real-world entity, often \nusing AI to bridge the gap between the physical and digital worlds. For in-\nstance, an LLM Twin is an LLM that learns to mimic your voice, personality, \nand writing style.\nWith these definitions in mind, a writing and content creation AI assistant who \nwrites like you is your LLM Twin co-pilot.\n\n\nChapter 1\n5\nWhy not use ChatGPT (or another similar chatbot)?\nWe have already provided the answer. ChatGPT is not personalized to your writing style and voice. \nInstead, it is very generic, unarticulated, and wordy. Maintaining an original voice is critical for \nlong-term success when building your brand. Thus, directly using ChatGPT or Gemini will not \nyield the most optimal results. Even if you are OK with sharing impersonalized content, mindlessly \nusing ChatGPT can result in the following:\n•\nMisinformation due to hallucination: Manually checking the results for hallucinations or \nusing third-party tools to evaluate your results is a tedious and unproductive experience.\n•\nTedious manual prompting: You must manually craft your prompts and inject external \ninformation, which is a tiresome experience. Also, the generated answers will be hard to \nreplicate between multiple sessions as you don’t have complete control over your prompts \nand injected data. You can solve part of this problem using an API and a tool such as \nLangChain, but you need programming experience to do so.\nFrom our experience, if you want high-quality content that provides real value, you will spend \nmore time debugging the generated text than writing it yourself.\nThe key of the LLM Twin stands in the following:\n• \nWhat data we collect\n•\nHow we preprocess the data\n•\nHow we feed the data into the LLM\n•\nHow we chain multiple prompts for the desired results\n•\nHow we evaluate the generated content\nThe LLM itself is important, but we want to highlight that using ChatGPT’s web interface is \nexceptionally tedious in managing and injecting various data sources or evaluating the outputs. \nThe solution is to build an LLM system that encapsulates and automates all the following steps \n(manually replicating them each time is not a long-term and feasible solution):\n• \nData collection\n• \nData preprocessing\nThis subsection will refer to using ChatGPT (or another similar chatbot) just in the \ncontext of generating personalized content.\n\n\nUnderstanding the LLM Twin Concept and Architecture\n6\n•\nData storage, versioning, and retrieval\n•\nLLM fine-tuning\n•\nRAG\n•\nContent generation evaluation\nNote that we never said not to use OpenAI’s GPT API, just that the LLM framework we will pres-\nent is LLM-agnostic. Thus, if it can be manipulated programmatically and exposes a fine-tuning \ninterface, it can be integrated into the LLM Twin system we will learn to build. The key to most \nsuccessful ML products is to be data-centric and make your architecture model-agnostic. Thus, \nyou can quickly experiment with multiple models on your specific data.\nPlanning the MVP of the LLM Twin product\nNow that we understand what an LLM Twin is and why we want to build it, we must clearly define \nthe product’s features. In this book, we will focus on the first iteration, often labeled the minimum \nviable product (MVP), to follow the natural cycle of most products. Here, the main objective is \nto align our ideas with realistic and doable business objectives using the available resources to \nproduce the product. Even as an engineer, as you grow up in responsibilities, you must go through \nthese steps to bridge the gap between the business needs and what can be implemented.\nWhat is an MVP?\nAn MVP is a version of a product that includes just enough features to draw in early users and test \nthe viability of the product concept in the initial stages of development. Usually, the purpose of \nthe MVP is to gather insights from the market with minimal effort.\nAn MVP is a powerful strategy because of the following reasons:\n• \nAccelerated time-to-market: Launch a product quickly to gain early traction\n• \nIdea validation: Test it with real users before investing in the full development of the \nproduct\n• \nMarket research: Gain insights into what resonates with the target audience\n• \nRisk minimization: Reduces the time and resources needed for a product that might not \nachieve market success\nSticking to the V in MVP is essential, meaning the product must be viable. The product must \nprovide an end-to-end user journey without half-implemented features, even if the product is \nminimal. It must be a working product with a good user experience that people will love and \nwant to keep using to see how it evolves to its full potential.\n\n\nChapter 1\n7\nDefining the LLM Twin MVP\nAs a thought experiment, let’s assume that instead of building this project for this book, we want \nto make a real product. In that case, what are our resources? Well, unfortunately, not many:\n• \nWe are a team of three people with two ML engineers and one ML researcher\n•\nOur laptops\n•\nPersonal funding for computing, such as training LLMs\n•\nOur enthusiasm\nAs you can see, we don’t have many resources. Even if this is just a thought experiment, it reflects \nthe reality for most start-ups at the beginning of their journey. Thus, we must be very strategic \nin defining our LLM Twin MVP and what features we want to pick. Our goal is simple: we want \nto maximize the product’s value relative to the effort and resources poured into it.\nTo keep it simple, we will build the features that can do the following for the LLM Twin:\n•\nCollect data from your LinkedIn, Medium, Substack, and GitHub profiles\n•\nFine-tune an open-source LLM using the collected data\n•\nPopulate a vector database (DB) using our digital data for RAG\n•\nCreate LinkedIn posts leveraging the following:\n•\nUser prompts\n•\nRAG to reuse and reference old content\n•\nNew posts, articles, or papers as additional knowledge to the LLM\n• \nHave a simple web interface to interact with the LLM Twin and be able to do the following:\n• \nConfigure your social media links and trigger the collection step\n• \nSend prompts or links to external resources\nThat will be the LLM Twin MVP. Even if it doesn’t sound like much, remember that we must make \nthis system cost effective, scalable, and modular.\nEven if we focus only on the core features of the LLM Twin defined in this section, we \nwill build the product with the latest LLM research and best software engineering \nand MLOps practices in mind. We aim to show you how to engineer a cost-effective \nand scalable LLM application.\n\n\nUnderstanding the LLM Twin Concept and Architecture\n8\nUntil now, we have examined the LLM Twin from the users’ and businesses’ perspectives. The last \nstep is to examine it from an engineering perspective and define a development plan to under-\nstand how to solve it technically. From now on, the book’s focus will be on the implementation \nof the LLM Twin.\nBuilding ML systems with feature/training/inference \npipelines\nBefore diving into the specifics of the LLM Twin architecture, we must understand an ML system \npattern at the core of the architecture, known as the feature/training/inference (FTI) architecture. \nThis section will present a general overview of the FTI pipeline design and how it can structure \nan ML application.\nLet’s see how we can apply the FTI pipelines to the LLM Twin architecture.\nThe problem with building ML systems\nBuilding production-ready ML systems is much more than just training a model. From an en-\ngineering point of view, training the model is the most straightforward step in most use cases. \nHowever, training a model becomes complex when deciding on the correct architecture and \nhyperparameters. That’s not an engineering problem but a research problem.\nAt this point, we want to focus on how to design a production-ready architecture. Training a \nmodel with high accuracy is extremely valuable, but just by training it on a static dataset, you \nare far from deploying it robustly. We have to consider how to do the following:\n•\nIngest, clean, and validate fresh data\n•\nTraining versus inference setups\n•\nCompute and serve features in the right environment\n•\nServe the model in a cost-effective way\n• \nVersion, track, and share the datasets and models\n• \nMonitor your infrastructure and models\n• \nDeploy the model on a scalable infrastructure\n• \nAutomate the deployments and training\nThese are the types of problems an ML or MLOps engineer must consider, while the research or \ndata science team is often responsible for training the model.\n\n\nChapter 1\n9\nFigure 1.1: Common elements from an ML system\nThe preceding figure shows all the components the Google Cloud team suggests that a mature ML \nand MLOps system requires. Along with the ML code, there are many moving pieces. The rest of \nthe system comprises configuration, automation, data collection, data verification, testing and \ndebugging, resource management, model analysis, process and metadata management, serving \ninfrastructure, and monitoring. The point is that there are many components we must consider \nwhen productionizing an ML model.\nThus, the critical question is this: How do we connect all these components into a single homog-\nenous system? We must create a boilerplate for clearly designing ML systems to answer that \nquestion.\nSimilar solutions exist for classic software. For example, if you zoom out, most software appli-\ncations can be split between a DB, business logic, and UI layer. Every layer can be as complex as \nneeded, but at a high-level overview, the architecture of standard software can be boiled down \nto the previous three components.\nDo we have something similar for ML applications? The first step is to examine previous solutions \nand why they are unsuitable for building scalable ML systems.\n\n\nUnderstanding the LLM Twin Concept and Architecture\n10\nThe issue with previous solutions\nIn Figure 1.2, you can observe the typical architecture present in most ML applications. It is based \non a monolithic batch architecture that couples the feature creation, model training, and infer-\nence into the same component. By taking this approach, you quickly solve one critical problem in \nthe ML world: the training-serving skew. The training-serving skew happens when the features \npassed to the model are computed differently at training and inference time.\nIn this architecture, the features are created using the same code. Hence, the training-serving \nskew issue is solved by default. This pattern works fine when working with small data. The \npipeline runs on a schedule in batch mode, and the predictions are consumed by a third-party \napplication such as a dashboard.\nFigure 1.2: Monolithic batch pipeline architecture\nUnfortunately, building a monolithic batch system raises many other issues, such as the following:\n•\nFeatures are not reusable (by your system or others)\n•\nIf the data increases, you have to refactor the whole code to support PySpark or Ray\n•\nIt’s hard to rewrite the prediction module in a more efficient language such as C++, Java, \nor Rust\n\n\nChapter 1\n11\n•\nIt’s hard to share the work between multiple teams between the features, training, and \nprediction modules\n•\nIt’s impossible to switch to streaming technology for real-time training\nIn Figure 1.3, we can see a similar scenario for a real-time system. This use case introduces an-\nother issue in addition to what we listed before. To make the predictions, we have to transfer the \nwhole state through the client request so the features can be computed and passed to the model.\nConsider the scenario of computing movie recommendations for a user. Instead of simply pass-\ning the user ID, we must transmit the entire user state, including their name, age, gender, movie \nhistory, and more. This approach is fraught with potential errors, as the client must understand \nhow to access this state, and it’s tightly coupled with the model service.\nAnother example would be when implementing an LLM with RAG support. The documents we add \nas context along the query represent our external state. If we didn’t store the records in a vector \nDB, we would have to pass them with the user query. To do so, the client must know how to query \nand retrieve the documents, which is not feasible. It is an antipattern for the client application \nto know how to access or compute the features. If you don’t understand how RAG works, we will \nexplain it in detail in Chapters 8 and 9.\nFigure 1.3: Stateless real-time architecture\n",
      "page_number": 28
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 37-46)",
      "start_page": 37,
      "end_page": 46,
      "detection_method": "topic_boundary",
      "content": "Understanding the LLM Twin Concept and Architecture\n12\nIn conclusion, our problem is accessing the features to make predictions without passing them at \nthe client’s request. For example, based on our first user movie recommendation example, how \ncan we predict the recommendations solely based on the user’s ID? Remember these questions, \nas we will answer them shortly.\nUltimately, on the other spectrum, Google Cloud provides a production-ready architecture, as \nshown in Figure 1.4. Unfortunately, even if it’s a feasible solution, it’s very complex and not intu-\nitive. You will have difficulty understanding this if you are not highly experienced in deploying \nand keeping ML models in production. Also, it is not straightforward to understand how to start \nsmall and grow the system in time.\nThe following image is reproduced from work created and shared by Google and used according \nto terms described in the Creative Commons 4.0 Attribution License:\nFigure 1.4: ML pipeline automation for CT (source: https://cloud.google.com/architecture/\nmlops-continuous-delivery-and-automation-pipelines-in-machine-learning)\n\n\nChapter 1\n13\nBut here is where the FTI pipeline architectures kick in. The following section will show you how \nto solve these fundamental issues using an intuitive ML design.\nThe solution – ML pipelines for ML systems\nThe solution is based on creating a clear and straightforward mind map that any team or person \ncan follow to compute the features, train the model, and make predictions. Based on these three \ncritical steps that any ML system requires, the pattern is known as the FTI pipeline. So, how does \nthis differ from what we presented before?\nThe pattern suggests that any ML system can be boiled down to these three pipelines: feature, \ntraining, and inference (similar to the DB, business logic, and UI layers from classic software). \nThis is powerful, as we can clearly define the scope and interface of each pipeline. Also, it’s easier \nto understand how the three components interact. Ultimately, we have just three instead of 20 \nmoving pieces, as suggested in Figure 1.4, which is much easier to work with and define.\nAs shown in Figure 1.5, we have the feature, training, and inference pipelines. We will zoom in on \neach of them and understand their scope and interface.\nFigure 1.5: FTI pipelines architecture\n\n\nUnderstanding the LLM Twin Concept and Architecture\n14\nBefore going into the details, it is essential to understand that each pipeline is a different com-\nponent that can run on a different process or hardware. Thus, each pipeline can be written using \na different technology, by a different team, or scaled differently. The key idea is that the design \nis very flexible to the needs of your team. It acts as a mind map for structuring your architecture.\nThe feature pipeline\nThe feature pipeline takes raw data as input, processes it, and outputs the features and labels \nrequired by the model for training or inference. Instead of directly passing them to the model, the \nfeatures and labels are stored inside a feature store. Its responsibility is to store, version, track, and \nshare the features. By saving the features in a feature store, we always have a state of our features. \nThus, we can easily send the features to the training and inference pipelines.\nAs the data is versioned, we can always ensure that the training and inference time features match. \nThus, we avoid the training-serving skew problem.\nThe training pipeline\nThe training pipeline takes the features and labels from the features stored as input and outputs \na train model or models. The models are stored in a model registry. Its role is similar to that of \nfeature stores, but this time, the model is the first-class citizen. Thus, the model registry will store, \nversion, track, and share the model with the inference pipeline.\nAlso, most modern model registries support a metadata store that allows you to specify essential \naspects of how the model was trained. The most important are the features, labels, and their \nversion used to train the model. Thus, we will always know what data the model was trained on.\nThe inference pipeline\nThe inference pipeline takes as input the features and labels from the feature store and the trained \nmodel from the model registry. With these two, predictions can be easily made in either batch \nor real-time mode.\nAs this is a versatile pattern, it is up to you to decide what you do with your predictions. If it’s a \nbatch system, they will probably be stored in a DB. If it’s a real-time system, the predictions will \nbe served to the client who requested them. Additionally, the features, labels, and models are \nversioned. We can easily upgrade or roll back the deployment of the model. For example, we will \nalways know that model v1 uses features F1, F2, and F3, and model v2 uses F2, F3, and F4. Thus, \nwe can quickly change the connections between the model and features.\n\n\nChapter 1\n15\nBenefits of the FTI architecture\nTo conclude, the most important thing you must remember about the FTI pipelines is their in-\nterface:\n• \nThe feature pipeline takes in data and outputs the features and labels saved to the feature \nstore.\n• \nThe training pipeline queries the features store for features and labels and outputs a \nmodel to the model registry.\n• \nThe inference pipeline uses the features from the feature store and the model from the \nmodel registry to make predictions.\nIt doesn’t matter how complex your ML system gets, these interfaces will remain the same.\nNow that we understand better how the pattern works, we want to highlight the main benefits \nof using this pattern:\n• \nAs you have just three components, it is intuitive to use and easy to understand.\n• \nEach component can be written into its tech stack, so we can quickly adapt them to specific \nneeds, such as big or streaming data. Also, it allows us to pick the best tools for the job.\n• \nAs there is a transparent interface between the three components, each one can be de-\nveloped by a different team (if necessary), making the development more manageable \nand scalable.\n• \nEvery component can be deployed, scaled, and monitored independently.\nThe final thing you must understand about the FTI pattern is that the system doesn’t have to \ncontain only three pipelines. In most cases, it will include more. For example, the feature pipeline \ncan be composed of a service that computes the features and one that validates the data. Also, the \ntraining pipeline can be composed of the training and evaluation components.\nThe FTI pipelines act as logical layers. Thus, it is perfectly fine for each to be complex and contain \nmultiple services. However, what is essential is to stick to the same interface on how the FTI pipe-\nlines interact with each other through the feature store and model registries. By doing so, each \nFTI component can evolve differently, without knowing the details of each other and without \nbreaking the system on new changes.\n\n\nUnderstanding the LLM Twin Concept and Architecture\n16\nNow that we understand the FTI pipeline architecture, the final step of this chapter is to see how \nit can be applied to the LLM Twin use case.\nDesigning the system architecture of the LLM Twin\nIn this section, we will list the concrete technical details of the LLM Twin application and under-\nstand how we can solve them by designing our LLM system using the FTI architecture. However, \nbefore diving into the pipelines, we want to highlight that we won’t focus on the tooling or the \ntech stack at this step. We only want to define a high-level architecture of the system, which is \nlanguage-, framework-, platform-, and infrastructure-agnostic at this point. We will focus on \neach component’s scope, interface, and interconnectivity. In future chapters, we will cover the \nimplementation details and tech stack.\nListing the technical details of the LLM Twin architecture\nUntil now, we defined what the LLM Twin should support from the user’s point of view. Now, \nlet’s clarify the requirements of the ML system from a purely technical perspective:\n• \nOn the data side, we have to do the following:\n• \nCollect data from LinkedIn, Medium, Substack, and GitHub completely autono-\nmously and on a schedule\n•\nStandardize the crawled data and store it in a data warehouse\n•\nClean the raw data\n•\nCreate instruct datasets for fine-tuning an LLM\n•\nChunk and embed the cleaned data. Store the vectorized data into a vector DB \nfor RAG.\n• \nFor training, we have to do the following:\n•\nFine-tune LLMs of various sizes (7B, 14B, 30B, or 70B parameters)\n•\nFine-tune on instruction datasets of multiple sizes\n•\nSwitch between LLM types (for example, between Mistral, Llama, and GPT)\n•\nTrack and compare experiments\nTo learn more about the FTI pipeline pattern, consider reading From MLOps to ML \nSystems with Feature/Training/Inference Pipelines by Jim Dowling, CEO and co-founder \nof Hopsworks: https://www.hopsworks.ai/post/mlops-to-ml-systems-with-\nfti-pipelines. His article inspired this section.\n\n\nChapter 1\n17\n• \nTest potential production LLM candidates before deploying them\n• \nAutomatically start the training when new instruction datasets are available.\n• \nThe inference code will have the following properties:\n• \nA REST API interface for clients to interact with the LLM Twin\n• \nAccess to the vector DB in real time for RAG\n• \nInference with LLMs of various sizes\n• \nAutoscaling based on user requests\n• \nAutomatically deploy the LLMs that pass the evaluation step.\n• \nThe system will support the following LLMOps features:\n•\nInstruction dataset versioning, lineage, and reusability\n•\nModel versioning, lineage, and reusability\n•\nExperiment tracking\n•\nContinuous training, continuous integration, and continuous delivery (CT/\nCI/CD)\n•\nPrompt and system monitoring\nThe preceding list is quite comprehensive. We could have detailed it even more, but at this point, \nwe want to focus on the core functionality. When implementing each component, we will look \ninto all the little details. But for now, the fundamental question we must ask ourselves is this: \nHow can we apply the FTI pipeline design to implement the preceding list of requirements?\nHow to design the LLM Twin architecture using the FTI \npipeline design\nWe will split the system into four core components. You will ask yourself this: “Four? Why not \nthree, as the FTI pipeline design clearly states?” That is a great question. Fortunately, the answer \nis simple. We must also implement the data pipeline along the three feature/training/inference \npipelines. According to best practices:\n• \nThe data engineering team owns the data pipeline\n• \nThe ML engineering team owns the FTI pipelines.\nIf any technical requirement doesn’t make sense now, bear with us. To avoid repe-\ntition, we will examine the details in their specific chapter.\n\n\nUnderstanding the LLM Twin Concept and Architecture\n18\nGiven our goal of building an MVP with a small team, we must implement the entire application. \nThis includes defining the data collection and FTI pipelines. Tackling a problem end to end is \noften encountered in start-ups that can’t afford dedicated teams. Thus, engineers have to wear \nmany hats, depending on the state of the product. Nevertheless, in any scenario, knowing how \nan end-to-end ML system works is valuable for better understanding other people’s work.\nFigure 1.6 shows the LLM system architecture. The best way to understand it is to review the four \ncomponents individually and explain how they work.\nFigure 1.6: LLM Twin high-level architecture\n\n\nChapter 1\n19\nData collection pipeline\nThe data collection pipeline involves crawling your personal data from Medium, Substack, Linke-\ndIn, and GitHub. As a data pipeline, we will use the extract, load, transform (ETL) pattern to \nextract data from social media platforms, standardize it, and load it into a data warehouse.\nThe output of this component will be a NoSQL DB, which will act as our data warehouse. As we \nwork with text data, which is naturally unstructured, a NoSQL DB fits like a glove.\nEven though a NoSQL DB, such as MongoDB, is not labeled as a data warehouse, from our point \nof view, it will act as one. Why? Because it stores standardized raw data gathered by various ETL \npipelines that are ready to be ingested into an ML system.\nThe collected digital data is binned into three categories:\n• \nArticles (Medium, Substack)\n• \nPosts (LinkedIn)\n• \nCode (GitHub)\nWe want to abstract away the platform where the data was crawled. For example, when feeding \nan article to the LLM, knowing it came from Medium or Substack is not essential. We can keep \nthe source URL as metadata to give references. However, from the processing, fine-tuning, and \nRAG points of view, it is vital to know what type of data we ingested, as each category must be \nprocessed differently. For example, the chunking strategy between a post, article, and piece of \ncode will look different.\nAlso, by grouping the data by category, not the source, we can quickly plug data from other plat-\nforms, such as X into the posts or GitLab into the code collection. As a modular system, we must \nattach an additional ETL in the data collection pipeline, and everything else will work without \nfurther code modifications.\nFeature pipeline\nThe feature pipeline’s role is to take raw articles, posts, and code data points from the data ware-\nhouse, process them, and load them into the feature store. \nIt is critical to highlight that the data collection pipeline is designed to crawl data \nonly from your social media platform. It will not have access to other people. As an \nexample for this book, we agreed to make our collected data available for learning \npurposes. Otherwise, using other people’s data without their consent is not moral.\n\n\nUnderstanding the LLM Twin Concept and Architecture\n20\nThe characteristics of the FTI pattern are already present.\nHere are some custom properties of the LLM Twin’s feature pipeline:\n•\nIt processes three types of data differently: articles, posts, and code\n•\nIt contains three main processing steps necessary for fine-tuning and RAG: cleaning, \nchunking, and embedding\n•\nIt creates two snapshots of the digital data, one after cleaning (used for fine-tuning) and \none after embedding (used for RAG)\n•\nIt uses a logical feature store instead of a specialized feature store\nLet’s zoom in on the logical feature store part a bit. As with any RAG-based system, one of the \ncentral pieces of the infrastructure is a vector DB. Instead of integrating another DB, more con-\ncretely, a specialized feature store, we used the vector DB, plus some additional logic to check all \nthe properties of a feature store our system needs.\nThe vector DB doesn’t offer the concept of a training dataset, but it can be used as a NoSQL DB. \nThis means we can access data points using their ID and collection name. Thus, we can easily \nquery the vector DB for new data points without any vector search logic. Ultimately, we will \nwrap the retrieved data into a versioned, tracked, and shareable artifact—more on artifacts in \nChapter 2. For now, you must know it is an MLOps concept used to wrap data and enrich it with \nthe properties listed before.\nHow will the rest of the system access the logical feature store? The training pipeline will use the \ninstruct datasets as artifacts, and the inference pipeline will query the vector DB for additional \ncontext using vector search techniques.\nFor our use case, this is more than enough because of the following reasons:\n• \nThe artifacts work great for offline use cases such as training\n• \nThe vector DB is built for online access, which we require for inference.\nIn future chapters, however, we will explain how the three data categories (articles, posts, and \ncode) are cleaned, chunked, and embedded.\nTo conclude, we take in raw article, post, or code data points, process them, and store them in \na feature store to make them accessible to the training and inference pipelines. Note that trim-\nming all the complexity away and focusing only on the interface is a perfect match with the FTI \npattern. Beautiful, right?\n\n\nChapter 1\n21\nTraining pipeline\nThe training pipeline consumes instruct datasets from the feature store, fine-tunes an LLM with \nit, and stores the tuned LLM weights in a model registry. More concretely, when a new instruct \ndataset is available in the logical feature store, we will trigger the training pipeline, consume the \nartifact, and fine-tune the LLM.\nIn the initial stages, the data science team owns this step. They run multiple experiments to find \nthe best model and hyperparameters for the job, either through automatic hyperparameter tuning \nor manually. To compare and pick the best set of hyperparameters, we will use an experiment \ntracker to log everything of value and compare it between experiments. Ultimately, they will pick \nthe best hyperparameters and fine-tuned LLM and propose it as the LLM production candidate. \nThe proposed LLM is then stored in the model registry. After the experimentation phase is over, \nwe store and reuse the best hyperparameters found to eliminate the manual restrictions of the \nprocess. Now, we can completely automate the training process, known as continuous training.\nThe testing pipeline is triggered for a more detailed analysis than during fine-tuning. Before \npushing the new model to production, assessing it against a stricter set of tests is critical to see \nthat the latest candidate is better than what is currently in production. If this step passes, the \nmodel is ultimately tagged as accepted and deployed to the production inference pipeline. Even \nin a fully automated ML system, it is recommended to have a manual step before accepting a new \nproduction model. It is like pushing the red button before a significant action with high conse-\nquences. Thus, at this stage, an expert looks at a report generated by the testing component. If \neverything looks good, it approves the model, and the automation can continue.\nThe particularities of this component will be on LLM aspects, such as the following:\n• \nHow do you implement an LLM agnostic pipeline?\n• \nWhat fine-tuning techniques should you use?\n•\nHow do you scale the fine-tuning algorithm on LLMs and datasets of various sizes?\n•\nHow do you pick an LLM production candidate from multiple experiments?\n•\nHow do you test the LLM to decide whether to push it to production or not?\nBy the end of this book, you will know how to answer all these questions.\nOne last aspect we want to clarify is CT. Our modular design allows us to quickly leverage an ML \norchestrator to schedule and trigger different system parts. For example, we can schedule the \ndata collection pipeline to crawl data every week. \n",
      "page_number": 37
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 47-58)",
      "start_page": 47,
      "end_page": 58,
      "detection_method": "topic_boundary",
      "content": "Understanding the LLM Twin Concept and Architecture\n22\nThen, we can trigger the feature pipeline when new data is available in the data warehouse and \nthe training pipeline when new instruction datasets are available.\nInference pipeline\nThe inference pipeline is the last piece of the puzzle. It is connected to the model registry and log-\nical feature store. It loads a fine-tuned LLM from the model registry, and from the logical feature \nstore, it accesses the vector DB for RAG. It takes in client requests through a REST API as queries. \nIt uses the fine-tuned LLM and access to the vector DB to carry out RAG and answer the queries.\nAll the client queries, enriched prompts using RAG, and generated answers are sent to a prompt \nmonitoring system to analyze, debug, and better understand the system. Based on specific require-\nments, the monitoring system can trigger alarms to take action either manually or automatically.\nAt the interface level, this component follows exactly the FTI architecture, but when zooming in, \nwe can observe unique characteristics of an LLM and RAG system, such as the following:\n• \nA retrieval client used to do vector searches for RAG\n• \nPrompt templates used to map user queries and external information to LLM inputs\n• \nSpecial tools for prompt monitoring\nFinal thoughts on the FTI design and the LLM Twin \narchitecture\nWe don’t have to be highly rigid about the FTI pattern. It is a tool used to clarify how to design \nML systems. For example, instead of using a dedicated features store just because that is how \nit is done, in our system, it is easier and cheaper to use a logical feature store based on a vector \nDB and artifacts. What was important to focus on were the required properties a feature store \nprovides, such as a versioned and reusable training dataset.\nUltimately, we will explain the computing requirements of each component briefly. The data \ncollection and feature pipeline are mostly CPU-based and do not require powerful machines. The \ntraining pipeline requires powerful GPU-based machines that could load an LLM and fine-tune it. \nThe inference pipeline is somewhere in the middle. It still needs a powerful machine but is less \ncompute-intensive than the training step. However, it must be tested carefully, as the inference \npipeline directly interfaces with the user. Thus, we want the latency to be within the required \nparameters for a good user experience. However, using the FTI design is not an issue. We can pick \nthe proper computing requirements for each component.\n\n\nChapter 1\n23\nAlso, each pipeline will be scaled differently. The data and feature pipelines will be scaled horizon-\ntally based on the CPU and RAM load. The training pipeline will be scaled vertically by adding more \nGPUs. The inference pipeline will be scaled horizontally based on the number of client requests.\nTo conclude, the presented LLM architecture checks all the technical requirements listed at the \nbeginning of the section. It processes the data as requested, and the training is modular and can \nbe quickly adapted to different LLMs, datasets, or fine-tuning techniques. The inference pipeline \nsupports RAG and is exposed as a REST API. On the LLMOps side, the system supports dataset and \nmodel versioning, lineage, and reusability. The system has a monitoring service, and the whole \nML architecture is designed with CT/CI/CD in mind.\nThis concludes the high-level overview of the LLM Twin architecture.\nSummary\nThis first chapter was critical to understanding the book’s goal. As a product-oriented book that \nwill walk you through building an end-to-end ML system, it was essential to understand the \nconcept of an LLM Twin initially. Afterward, we walked you through what an MVP is and how \nto plan our LLM Twin MVP based on our available resources. Following this, we translated our \nconcept into a practical technical solution with specific requirements. In this context, we intro-\nduced the FTI design pattern and showcased its real-world application in designing systems that \nare both modular and scalable. Ultimately, we successfully applied the FTI pattern to design the \narchitecture of the LLM Twin to fit all our technical requirements.\nHaving a clear vision of the big picture is essential when building systems. Understanding how \na single component will be integrated into the rest of the application can be very valuable when \nworking on it. We started with a more abstract presentation of the LLM Twin architecture, fo-\ncusing on each component’s scope, interface, and interconnectivity.\nThe following chapters will explore how to implement and deploy each component. On the \nMLOps side, we will walk you through using a computing platform, orchestrator, model registry, \nartifacts, and other tools and concepts to support all MLOps best practices.\nReferences\n•\nDowling, J. (2024a, July 11). From MLOps to ML Systems with Feature/Training/Inference \nPipelines. Hopsworks. https://www.hopsworks.ai/post/mlops-to-ml-systems-with-\nfti-pipelines\n\n\nUnderstanding the LLM Twin Concept and Architecture\n24\n• \nDowling, J. (2024b, August 5). Modularity and Composability for AI Systems with AI Pipe-\nlines and Shared Storage. Hopsworks. https://www.hopsworks.ai/post/modularity-and-\ncomposability-for-ai-systems-with-ai-pipelines-and-shared-storage\n• \nJoseph, M. (2024, August 23). The Taxonomy for Data Transformations in AI Systems. Hop-\nsworks. https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-\nin-ai-systems\n•\nMLOps: Continuous delivery and automation pipelines in machine learning. (2024, August \n28). Google Cloud. https://cloud.google.com/architecture/mlops-continuous-\ndelivery-and-automation-pipelines-in-machine-learning\n•\nQwak. (2024a, June 2). CI/CD for Machine Learning in 2024: Best Practices to build, test, \nand Deploy | Infer. Medium. https://medium.com/infer-qwak/ci-cd-for-machine-\nlearning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2\n• \nQwak. (2024b, July 23). 5 Best Open Source Tools to build End-to-End MLOPs Pipeline in 2024. \nMedium. https://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-\nwith-open-source-tools-d8bacbf4184f\n•\nSalama, K., Kazmierczak, J., & Schut, D. (2021). Practitioners guide to MLOPs: A framework \nfor continuous delivery and automation of machine learning (1\nst ed.) [PDF]. Google Cloud. \nhttps://services.google.com/fh/files/misc/practitioners_guide_to_mlops_\nwhitepaper.pdf\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n\n\n2\nTooling and Installation\nThis chapter presents all the essential tools that will be used throughout the book, especially in \nimplementing and deploying the LLM Twin project. At this point in the book, we don’t plan to \npresent in-depth LLM, RAG, MLOps, or LLMOps concepts. We will quickly walk you through our \ntech stack and prerequisites to avoid repeating ourselves throughout the book on how to set up \na particular tool and why we chose it. Starting with Chapter 3, we will begin exploring our LLM \nTwin use case by implementing a data collection ETL that crawls data from the internet.\nIn the first part of the chapter, we will present the tools within the Python ecosystem to manage \nmultiple Python versions, create a virtual environment, and install the pinned dependencies re-\nquired for our project to run. Alongside presenting these tools, we will also show how to install \nthe LLM-Engineers-Handbook repository on your local machine (in case you want to try out the \ncode yourself): https://github.com/PacktPublishing/LLM-Engineers-Handbook.\nNext, we will explore all the MLOps and LLMOps tools we will use, starting with more generic tools, \nsuch as a model registry, and moving on to more LLM-oriented tools, such as LLM evaluation and \nprompt monitoring tools. We will also understand how to manage a project with multiple ML \npipelines using ZenML, an orchestrator bridging the gap between ML and MLOps. Also, we will \nquickly explore what databases we will use for NoSQL and vector storage. We will show you how \nto run all these components on your local machine using Docker. Lastly, we will quickly review \nAWS and show you how to create an AWS user and access keys and install and configure the AWS \nCLI to manipulate your cloud resources programmatically. We will also explore SageMaker and \nwhy we use it to train and deploy our open-source LLMs.\n\n\nTooling and Installation\n26\nIf you are familiar with these tools, you can safely skip this chapter. We also explain how to in-\nstall the project and set up all the necessary components in the repository’s README. Thus, you \nalso have the option to use that as more concise documentation if you plan to run the code while \nreading the book.\nTo sum all that up, in this chapter, we will explore the following topics:\n•\nPython ecosystem and project installation\n•\nMLOps and LLMOps tooling\n•\nDatabases for storing unstructured and vector data\n•\nPreparing for AWS\nBy the end of this chapter, you will be aware of all the tools we will use across the book. Also, you \nwill have learned how to install the LLM-Engineers-Handbook repository, set up the rest of the \ntools, and use them if you run the code while reading the book.\nPython ecosystem and project installation\nAny Python project needs three fundamental tools: the Python interpreter, dependency manage-\nment, and a task execution tool. The Python interpreter executes your Python project as expected. \nAll the code within the book is tested with Python 3.11.8. You can download the Python interpreter \nfrom here: https://www.python.org/downloads/. We recommend installing the exact Python \nversion (Python 3.11.8) to run the LLM Twin project using pyenv, making the installation process \nstraightforward.\nInstead of installing multiple global Python versions, we recommend managing them using pyenv, \na Python version management tool that lets you manage multiple Python versions between \nprojects. You can install it using this link: https://github.com/pyenv/pyenv?tab=readme-ov-\nfile#installation.\nAfter you have installed pyenv, you can install the latest version of Python 3.11, using pyenv, as \nfollows:\npyenv install 3.11.8\nNow list all installed Python versions to see that it was installed correctly:\npyenv versions\nYou should see something like this:\n# * system\n\n\nChapter 2\n27\n#   3.11.8\nTo make Python 3.11.8 the default version across your entire system (whenever you open a new \nterminal), use the following command:\npyenv global 3.11.8\nHowever, we aim to use Python 3.11.8 locally only in our repository. To achieve that, first, we have \nto clone the repository and navigate to it:\ngit clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.git \ncd LLM-Engineers-Handbook\nBecause we defined a .python-version file within the repository, pyenv will know to pick up \nthe version from that file and use it locally whenever you are working within that folder. To \ndouble-check that, run the following command while you are in the repository:\npython --version\nIt should output:\n# Python 3.11.8\nTo create the .python-version file, you must run pyenv local 3.11.8 once. Then, pyenv will \nalways know to use that Python version while working within a specific directory.\nNow that we have installed the correct Python version using pyenv, let’s move on to Poetry, which \nwe will use as our dependency and virtual environment manager.\nPoetry: dependency and virtual environment management\nPoetry is one of the most popular dependency and virtual environment managers within the \nPython ecosystem. But let’s start by clarifying what a dependency manager is. In Python, a depen-\ndency manager allows you to specify, install, update, and manage external libraries or packages \n(dependencies) that a project relies on. For example, this is a simple Poetry requirements file that \nuses Python 3.11 and the requests and numpy Python packages.\n[tool.poetry.dependencies]\npython = \"^3.11\"\nrequests = \"^2.25.1\"\nnumpy = \"^1.19.5\"\n[build-system]\n\n\nTooling and Installation\n28\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\nBy using Poetry to pin your dependencies, you always ensure that you install the correct version \nof the dependencies that your projects work with. Poetry, by default, saves all its requirements in \npyproject.toml files, which are stored at the root of your repository, as you can see in the cloned \nLLM-Engineers-Handbook repository.\nAnother massive advantage of using Poetry is that it creates a new Python virtual environment in \nwhich it installs the specified Python version and requirements. A virtual environment allows you \nto isolate your project’s dependencies from your global Python dependencies and other projects. \nBy doing so, you ensure there are no version clashes between projects. For example, let’s assume \nthat Project A needs numpy == 1.19.5, and Project B needs numpy == 1.26.0. If you keep both \nprojects in the global Python environment, that will not work, as Project B will override Project A’s \nnumpy installation, which will corrupt Project A and stop it from working. Using Poetry, you can \nisolate each project in its own Python environment with its own Python dependencies, avoiding \nany dependency clashes.\nYou can install Poetry from here: https://python-poetry.org/docs/. We use Poetry 1.8.3 \nthroughout the book. Once Poetry is installed, navigate to your cloned LLM-Engineers-Hand-\nbook repository and run the following command to install all the necessary Python dependencies:\npoetry install --without aws\nThis command knows to pick up all the dependencies from your repository that are listed in \nthe pyproject.toml and poetry.lock files. After the installation, you can activate your Poetry \nenvironment by running poetry shell in your terminal or by prefixing all your CLI commands \nas follows: poetry run <your command>.\nOne final note on Poetry is that it locks down the exact versions of the dependency tree in the \npoetry.lock file based on the definitions added to the project.toml file. While the pyproject.\ntoml file may specify version ranges (e.g., requests = \"^2.25.1\"), the poetry.lock file records \nthe exact version (e.g., requests = \"2.25.1\") that was installed. It also locks the versions of \nsub-dependencies (dependencies of your dependencies), which may not be explicitly listed in \nyour pyproject.toml file. By locking all the dependencies and sub-dependencies to specific \nversions, the poetry.lock file ensures that all project installations use the same versions of each \npackage. This consistency leads to predictable behavior, reducing the likelihood of encountering \n“works on my machine” issues.\n\n\nChapter 2\n29\nOther tools similar to Poetry are Venv and Conda for creating virtual environments. Still, they lack \nthe dependency management option. Thus, you must do it through Python’s default requirements.\ntxt files, which are less powerful than Poetry’s lock files. Another option is Pipenv, which fea-\nture-wise is more like Poetry but slower, and uv, which is a replacement for Poetry built in Rust, \nmaking it blazing fast. uv has lots of potential to replace Poetry, making it worthwhile to test out: \nhttps://github.com/astral-sh/uv.\nThe final piece of the puzzle is to look at the task execution tool we used to manage all our CLI \ncommands.\nPoe the Poet: task execution tool\nPoe the Poet is a plugin on top of Poetry that is used to manage and execute all the CLI commands \nrequired to interact with the project. It helps you define and run tasks within your Python proj-\nect, simplifying automation and script execution. Other popular options are Makefile, Invoke, or \nshell scripts, but Poe the Poet eliminates the need to write separate shell scripts or Makefiles for \nmanaging project tasks, making it an elegant way to manage tasks using the same configuration \nfile that Poetry already uses for dependencies.\nWhen working with Poe the Poet, instead of having all your commands documented in a README \nfile or other document, you can add them directly to your pyproject.toml file and execute them \nin the command line with an alias. For example, using Poe the Poet, we can define the following \ntasks in a pyproject.toml file:\n[tool.poe.tasks]\ntest = \"pytest\"\nformat = \"black .\"\nstart = \"python main.py\"\nYou can then run these tasks using the poe command:\npoetry poe test\npoetry poe format\npoetry poe start\nYou can install Poe the Poet as a Poetry plugin, as follows:\npoetry self add 'poethepoet[poetry_plugin]'\n\n\nTooling and Installation\n30\nTo conclude, using a tool as a façade over all your CLI commands is necessary to run your appli-\ncation. It significantly simplifies the application’s complexity and enhances collaboration as it \nacts as out-of-the-box documentation.\nAssuming you have pyenv and Poetry installed, here are all the commands you need to run to \nclone the repository and install the dependencies and Poe the Poet as a Poetry plugin:\ngit clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.gitcd \nLLM-Engineers-Handbook\npoetry install --without aws\npoetry self add 'poethepoet[poetry_plugin]'\nTo make the project fully operational, there are still a few steps to follow, such as filling out a \n.env file with your credentials and getting tokens from OpenAI and Hugging Face. But this book \nisn’t an installation guide, so we’ve moved all these details into the repository’s README as \nthey are useful only if you plan to run the repository: https://github.com/PacktPublishing/\nLLM-Engineers-Handbook.\nNow that we have installed our Python project, let’s present the MLOps tools we will use in the \nbook. If you are already familiar with these tools, you can safely skip the following tooling section \nand move on to the Databases for storing unstructured and vector data section.\nMLOps and LLMOps tooling\nThis section will quickly present all the MLOps and LLMOps tools we will use throughout the \nbook and their role in building ML systems using MLOps best practices. At this point in the \nbook, we don’t aim to detail all the MLOps components we will use to implement the LLM Twin \nuse case, such as model registries and orchestrators, but only provide a quick idea of what they \nare and how to use them. As we develop the LLM Twin project throughout the book, you will \nsee hands-on examples of how we use all these tools. In Chapter 11, we will dive deeply into the \ntheory of MLOps and LLMOps and connect all the dots. As the MLOps and LLMOps fields are \nhighly practical, we will leave the theory of these aspects to the end, as it will be much easier to \nunderstand it after you go through the LLM Twin use case implementation.\nAlso, this section is not dedicated to showing you how to set up each tool. It focuses primarily on \nwhat each tool is used for and highlights the core features used throughout this book.\nStill, using Docker, you can quickly run the whole infrastructure locally. If you want to run the \nsteps within the book yourself, you can host the application locally with these three simple steps:\n1. \nHave Docker 27.1.1 (or higher) installed.\n\n\nChapter 2\n31\n2.\nFill your .env file with all the necessary credentials as explained in the repository README.\n3.\nRun poetry poe local-infrastructure-up to locally spin up ZenML (http://127.0.0.1:8237/) \nand the MongoDB and Qdrant databases.\nYou can read more details on how to run everything locally in the LLM-Engineers-Handbook re-\npository README: https://github.com/PacktPublishing/LLM-Engineers-Handbook. Within \nthe book, we will also show you how to deploy each component to the cloud.\nHugging Face: model registry\nA model registry is a centralized repository that manages ML models throughout their lifecycle. \nIt stores models along with their metadata, version history, and performance metrics, serving \nas a single source of truth. In MLOps, a model registry is crucial for tracking, sharing, and docu-\nmenting model versions, facilitating team collaboration. Also, it is a fundamental element in the \ndeployment process as it integrates with continuous integration and continuous deployment\n(CI/CD) pipelines.\nWe used Hugging Face as our model registry, as we can leverage its ecosystem to easily share our \nfine-tuned LLM Twin models with anyone who reads the book. Also, by following the Hugging \nFace model registry interface, we can easily integrate the model with all the frameworks around \nthe LLMs ecosystem, such as Unsloth for fine-tuning and SageMaker for inference.\nOur fine-tuned LLMs are available on Hugging Face at:\n• \nTwinLlama 3 1 8B (after fine-tuning): https://huggingface.co/mlabonne/TwinLlama-\n3.1-8B\n• \nTwinLlama 3 1 8B DPO (after preference alignment): https://huggingface.co/\nmlabonne/TwinLlama-3.1-8B-DPO\nFigure 2.1: Hugging Face model registry example\n\n\nTooling and Installation\n32\nFor a quick demo, we have them available on Hugging Face Spaces:\n•\nTwinLlama 3 1 8B: https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B\n•\nTwinLlama 3 1 8B DPO: https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-\n8B-DPO\nMost ML tools provide model registry features. For example, ZenML, Comet, and SageMaker, which \nwe will present in future sections, also offer their own model registries. They are good options, \nbut we picked Hugging Face solely because of its ecosystem, which provides easy shareability and \nintegration throughout the open-source environment. Thus, you will usually select the model \nregistry that integrates the most with your project’s tooling and requirements.\nZenML: orchestrator, artifacts, and metadata\nZenML acts as the bridge between ML and MLOps. Thus, it offers multiple MLOps features that \nmake your ML pipeline traceability, reproducibility, deployment, and maintainability easier. At \nits core, it is designed to create reproducible workflows in machine learning. It addresses the \nchallenge of transitioning from exploratory research in Jupyter notebooks to a production-ready \nML environment. It tackles production-based replication issues, such as versioning difficulties, \nreproducing experiments, organizing complex ML workflows, bridging the gap between train-\ning and deployment, and tracking metadata. Thus, ZenML’s main features are orchestrating ML \npipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts \nfor better observability.\nInstead of being another ML platform, ZenML introduced the concept of a stack, which allows \nyou to run ZenML on multiple infrastructure options. A stack will enable you to connect ZenML \nto different cloud services, such as:\n• \nAn orchestrator and compute engine (for example, AWS SageMaker or Vertex AI)\n• \nRemote storage (for instance, AWS S3 or Google Cloud Storage buckets)\n• \nA container registry (for example, Docker Registry or AWS ECR)\nThus, ZenML acts as a glue that brings all your infrastructure and tools together in one place \nthrough its stack feature, allowing you to quickly iterate through your development processes and \neasily monitor your entire ML system. The beauty of this is that ZenML doesn’t vendor-lock you \ninto any cloud platform. It completely abstracts away the implementation of your Python code \nfrom the infrastructure it runs on. For example, in our LLM Twin use case, we used the AWS stack:\n• \nSageMaker as our orchestrator and compute\n\n\nChapter 2\n33\n• \nS3 as our remote storage used to store and track artifacts\n• \nECR as our container registry\nHowever, the Python code contains no S3 or ECR particularities, as ZenML takes care of them. \nThus, we can easily switch to other providers, such as Google Cloud Storage or Azure. For more \ndetails on ZenML stacks, you can start here: https://docs.zenml.io/user-guide/production-\nguide/understand-stacks.\nThe local version of the ZenML server comes installed as a Python package. Thus, when running \npoetry install, it installs a ZenML debugging server that you can use locally. In Chapter 11, we \nwill show you how to use their cloud serverless option to deploy the ML pipelines to AWS.\nOrchestrator\nAn orchestrator is a system that automates, schedules, and coordinates all your ML pipelines. It \nensures that each pipeline—such as data ingestion, preprocessing, model training, and deploy-\nment—executes in the correct order and handles dependencies efficiently. By managing these \nprocesses, an orchestrator optimizes resource utilization, handles failures gracefully, and enhances \nscalability, making complex ML pipelines more reliable and easier to manage.\nHow does ZenML work as an orchestrator? It works with pipelines and steps. A pipeline is a \nhigh-level object that contains multiple steps. A function becomes a ZenML pipeline by being \ndecorated with @pipeline, and a step when decorated with @step. This is a standard pattern \nwhen using orchestrators: you have a high-level function, often called a pipeline, that calls mul-\ntiple units/steps/tasks.\nLet’s explore how we can implement a ZenML pipeline with one of the ML pipelines implemented \nfor the LLM Twin project. In the code snippet below, we defined a ZenML pipeline that queries \nthe database for a user based on its full name and crawls all the provided links under that user:\nfrom zenml import pipeline\nfrom steps.etl import crawl_links, get_or_create_user\n@pipeline\nWe will focus only on the ZenML features used throughout the book, such as orches-\ntrating, artifacts, and metadata. For more details on ZenML, check out their starter \nguide: https://docs.zenml.io/user-guide/starter-guide.\n",
      "page_number": 47
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 59-66)",
      "start_page": 59,
      "end_page": 66,
      "detection_method": "topic_boundary",
      "content": "Tooling and Installation\n34\ndef digital_data_etl(user_full_name: str, links: list[str]) -> None:\n    user = get_or_create_user(user_full_name)\n    crawl_links(user=user, links=links)\nYou can run the pipeline with the following CLI command: poetry poe run-digital-data-etl. \nTo visualize the pipeline run, you can go to your ZenML dashboard (at http://127.0.0.1:8237/) \nand, on the left panel, click on the Pipelines tab and then on the digital_data_etl pipeline, as \nillustrated in Figure 2.2:\nFigure 2.2: ZenML Pipelines dashboard\nAfter clicking on the digital_data_etl pipeline, you can visualize all the previous and current \npipeline runs, as seen in Figure 2.3. You can see which one succeeded, failed, or is still running. \nAlso, you can see the stack used to run the pipeline, where the default stack is the one used to \nrun your ML pipelines locally.\n\n\nChapter 2\n35\nFigure 2.3: ZenML digital_data_etl pipeline dashboard. Example of a specific pipeline\nNow, after clicking on the latest digital_data_etl pipeline run (or any other run that succeeded or \nis still running), we can visualize the pipeline’s steps, outputs, and insights, as illustrated in Figure \n2.4. This structure is often called a directed acyclic graph (DAG). More on DAGs in Chapter 11.\nFigure 2.4: ZenML digital_data_etl pipeline run dashboard (example of a specific pipeline run)\n\n\nTooling and Installation\n36\nBy clicking on a specific step, you can get more insights into its code and configuration. It even \naggregates the logs output by that specific step to avoid switching between tools, as shown in \nFigure 2.5.\nFigure 2.5: Example of insights from a specific step of the digital_data_etl pipeline run\nNow that we understand how to define a ZenML pipeline and how to look it up in the dashboard, \nlet’s quickly look at how to define a ZenML step. In the code snippet below, we defined the get_\nor_create_user() step, which works just like a normal Python function but is decorated with \n@step. We won’t go into the details of the logic, as we will cover the ETL logic in Chapter 3. For \nnow, we will focus only on the ZenML functionality.\nfrom loguru import logger\nfrom typing_extensions import Annotated\nfrom zenml import get_step_context, step\nfrom llm_engineering.application import utils\n\n\nChapter 2\n37\nfrom llm_engineering.domain.documents import UserDocument\n@step\ndef get_or_create_user(user_full_name: str) -> Annotated[UserDocument, \n\"user\"]:\n    logger.info(f\"Getting or creating user: {user_full_name}\")\n    first_name, last_name = utils.split_user_full_name(user_full_name)\n    user = UserDocument.get_or_create(first_name=first_name, last_\nname=last_name)\n    return user\nWithin a ZenML step, you can define any Python logic your use case needs. In this simple example, \nwe are just creating or retrieving a user, but we could replace that code with anything, starting \nfrom data collection to feature engineering and training. What is essential to notice is that to \nintegrate ZenML with your code, you have to write modular code, where each function does just \none thing. The modularity of your code makes it easy to decorate your functions with @step and \nthen glue multiple steps together within a main function decorated with @pipeline. One design \nchoice that will impact your application is deciding the granularity of each step, as each will run \nas a different unit on a different machine when deployed in the cloud.\nTo decouple our code from ZenML, we encapsulated all the application and domain logic into \nthe llm_engineering Python module. We also defined the pipelines and steps folders, where \nwe defined our ZenML logic. Within the steps module, we only used what we needed from the \nllm_engineering Python module (similar to how you use a Python package). In the pipelines\nmodule, we only aggregated ZenML steps to glue them into the final pipeline. Using this de-\nsign, we can easily swap ZenML with another orchestrator or use our application logic in other \nuse cases, such as a REST API. We only have to replace the ZenML code without touching the \nllm_engineering module where all our logic resides. \n\n\nTooling and Installation\n38\nThis folder structure is reflected at the root of the LLM-Engineers-Handbook repository, as il-\nlustrated in Figure 2.6:\nFigure 2.6: LLM-Engineers-Handbook repository folder structure\nOne last thing to consider when writing ZenML steps is that if you return a value, it should be se-\nrializable. ZenML can serialize most objects that can be reduced to primitive data types, but there \nare a few exceptions. For example, we used UUID types as IDs throughout the code, which aren’t \nnatively supported by ZenML. Thus, we had to extend ZenML’s materializer to support UUIDs. \nWe raised this issue to ZenML. Hence, in future ZenML versions, UUIDs will be supported, but it \nwas an excellent example of the serialization aspect of transforming function outputs in artifacts.\n\n\nChapter 2\n39\nArtifacts and metadata\nAs mentioned in the previous section, ZenML transforms any step output into an artifact. First, \nlet’s quickly understand what an artifact is. In MLOps, an artifact is any file(s) produced during \nthe machine learning lifecycle, such as datasets, trained models, checkpoints, or logs. Artifacts \nare crucial for reproducing experiments and deploying models. We can transform anything into \nan artifact. For example, the model registry is a particular use case for an artifact. Thus, artifacts \nhave these unique properties: they are versioned, sharable, and have metadata attached to them \nto understand what’s inside quickly. For example, when wrapping your dataset with an artifact, \nyou can add to its metadata the size of the dataset, the train-test split ratio, the size, types of labels, \nand anything else useful to understand what’s inside the dataset without actually downloading it.\nLet’s circle back to our digital_data_etl pipeline example, where we had as a step output an ar-\ntifact, the crawled links, which are an artifact, as seen in Figure 2.7\nFigure 2.7: ZenML artifact example using the digital_data_etl pipeline as an example\n\n\nTooling and Installation\n40\nBy clicking on the crawled_links artifact and navigating to the Metadata tab, we can quickly \nsee all the domains we crawled for a particular author, the number of links we crawled for each \ndomain, and how many were successful, as illustrated in Figure 2.8:\nFigure 2.8: ZenML metadata example using the digital_data_etl pipeline as an example\nA more interesting example of an artifact and its metadata is the generated dataset artifact. In \nFigure 2.9, we can visualize the metadata of the instruct_datasets artifact, which was auto-\nmatically generated and will be used to fine-tune the LLM Twin model. More details on the \ninstruction datasets are in Chapter 5. For now, we want to highlight that within the dataset’s \nmetadata, we have precomputed a lot of helpful information about it, such as how many data \ncategories it contains, its storage size, and the number of samples per training and testing split. \n\n\nChapter 2\n41\nFigure 2.9: ZenML metadata example for the instruct_datasets artifact\nThe metadata is manually added to the artifact, as shown in the code snippet below. Thus, you \ncan precompute and attach to the artifact’s metadata anything you consider helpful for dataset \ndiscovery across your business and projects:\n… # More imports\nfrom zenml import ArtifactConfig, get_step_context, step\n@step\ndef generate_intruction_dataset(\n    prompts: Annotated[dict[DataCategory, \nlist[GenerateDatasetSamplesPrompt]], \"prompts\"]) -> Annotated[\n",
      "page_number": 59
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 67-75)",
      "start_page": 67,
      "end_page": 75,
      "detection_method": "topic_boundary",
      "content": "Tooling and Installation\n42\n    InstructTrainTestSplit,\n    ArtifactConfig(\n        name=\"instruct_datasets\",\n        tags=[\"dataset\", \"instruct\", \"cleaned\"],\n    ),\n]:\n    datasets = … # Generate datasets\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"instruct_datasets\", \nmetadata=_get_metadata_instruct_dataset(datasets))\n    return datasets\ndef _get_metadata_instruct_dataset(datasets: InstructTrainTestSplit) -> \ndict[str, Any]:\n    instruct_dataset_categories = list(datasets.train.keys())\n    train_num_samples = {\n        category: instruct_dataset.num_samples for category, instruct_\ndataset in datasets.train.items()\n    }\n    test_num_samples = {category: instruct_dataset.num_samples for \ncategory, instruct_dataset in datasets.test.items()}\n    return {\n        \"data_categories\": instruct_dataset_categories,\n        \"test_split_size\": datasets.test_split_size,\n        \"train_num_samples_per_category\": train_num_samples,\n        \"test_num_samples_per_category\": test_num_samples,\n    }\nAlso, you can easily download and access a specific version of the dataset using its Universally \nUnique Identifier (UUID), which you can find using the ZenML dashboard or CLI:\nfrom zenml.client import Client\nartifact = Client().get_artifact_version('8bba35c4-8ff9-4d8f-a039-\n08046efc9fdc')\nloaded_artifact = artifact.load()\n\n\nChapter 2\n43\nThe last step in exploring ZenML is understanding how to run and configure a ZenML pipeline.\nHow to run and configure a ZenML pipeline\nAll the ZenML pipelines can be called from the run.py file, accessed at tools/run.py in our GitHub \nrepository. Within the run.py file, we implemented a simple CLI that allows you to specify what \npipeline to run. For example, to call the digital_data_etl pipeline to crawl Maxime’s content, \nyou have to run:\npython -m tools.run --run-etl --no-cache --etl-config-filename digital_\ndata_etl_maxime_labonne.yaml\nOr, to crawl Paul’s content, you can run:\npython -m tools.run --run-etl --no-cache --etl-config-filename digital_\ndata_etl_paul_iusztin.yaml\nAs explained when introducing Poe the Poet, all our CLI commands used to interact with the proj-\nect will be executed through Poe to simplify and standardize the project. Thus, we encapsulated \nthese Python calls under the following poe CLI commands:\npoetry poe run-digital-data-etl-maxime\npoetry poe run-digital-data-etl-paul\nWe only change the ETL config file name when scraping content for different people. ZenML \nallows us to inject specific configuration files at runtime as follows:\nconfig_path = root_dir / \"configs\" / etl_config_filename\nassert config_path.exists(), f\"Config file not found: { config_path }\"\nrun_args_etl = {\n\"config_path\": config_path,\n\"run_name\": f\"digital_data_etl_run_{dt.now().\nstrftime('%Y_%m_%d_%H_%M_%S')}\"\n}\n digital_data_etl.with_options()(**run_args_etl)\nIn the config file, we specify all the parameters that will input the pipeline as parameters. For ex-\nample, the configs/digital_data_etl_maxime_labonne.yaml configuration file looks as follows:\nparameters:\n  user_full_name: Maxime Labonne # [First Name(s)] [Last Name]\n  links:\n    # Personal Blog\n\n\nTooling and Installation\n44\n    - https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.\nhtml\n    - https://mlabonne.github.io/blog/posts/2024-07-15_The_Rise_of_\nAgentic_Data_Generation.html\n    # Substack\n    - https://maximelabonne.substack.com/p/uncensor-any-llm-with-\nabliteration-d30148b7d43e\n    … # More links\nWhere the digital_data_etl function signature looks like this:\n@pipeline\ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\nThis approach allows us to configure each pipeline at runtime without modifying the code. We \ncan also clearly track the inputs for all our pipelines, ensuring reproducibility. As seen in Figure \n2.10, we have one or more configs for each pipeline.\nFigure 2.10: ZenML pipeline configs\n\n\nChapter 2\n45\nOther popular orchestrators similar to ZenML that we’ve personally tested and consider powerful \nare Airflow, Prefect, Metaflow, and Dagster. Also, if you are a heavy user of Kubernetes, you can \nopt for Agro Workflows or Kubeflow, the latter of which works only on top of Kubernetes. We still \nconsider ZenML the best trade-off between ease of use, features, and costs. Also, none of these \ntools offer the stack feature that is offered by ZenML, which allows it to avoid vendor-locking \nyou in to any cloud ecosystem.\nIn Chapter 11, we will explore in more depth how to leverage an orchestrator to implement MLOps \nbest practices. But now that we understand ZenML, what it is helpful for, and how to use it, let’s \nmove on to the experiment tracker.\nComet ML: experiment tracker\nTraining ML models is an entirely iterative and experimental process. Unlike traditional software \ndevelopment, it involves running multiple parallel experiments, comparing them based on pre-\ndefined metrics, and deciding which one should advance to production. An experiment tracking \ntool allows you to log all the necessary information, such as metrics and visual representations \nof your model predictions, to compare all your experiments and quickly select the best model. \nOur LLM project is no exception.\nAs illustrated in Figure 2.11, we used Comet to track metrics such as training and evaluation loss \nor the value of the gradient norm across all our experiments.\nFigure 2.11: Comet ML training metrics example\n\n\nTooling and Installation\n46\nUsing an experiment tracker, you can go beyond training and evaluation metrics and log your \ntraining hyperparameters to track different configurations between experiments. \nIt also logs out-of-the-box system metrics such as GPU, CPU, or memory utilization to give you \na clear picture of what resources you need during training and where potential bottlenecks slow \ndown your training, as seen in Figure 2.12.\nFigure 2.12: Comet ML system metrics example\nYou don’t have to set up Comet locally. We will use their online version for free without any \nconstraints throughout this book. Also, if you want to look more in-depth into the Comet ML \nexperiment tracker, we made the training experiments tracked with Comet ML public while \nfine-tuning our LLM Twin models. You can access them here: https://www.comet.com/mlabonne/\nllm-twin-training/view/new/panels.\nOther popular experiment trackers are W&B, MLflow, and Neptune. We’ve worked with all of \nthem and can state that they all have mostly the same features, but Comet ML differentiates it-\nself through its ease of use and intuitive interface. Let’s move on to the final piece of the MLOps \npuzzle: Opik for prompt monitoring.\nOpik: prompt monitoring\nYou cannot use standard tools and techniques when logging and monitoring prompts. The reason \nfor this is complicated. We will dig into it in Chapter 11. However, to quickly give you some under-\nstanding, you cannot use standard logging tools as prompts are complex and unstructured chains. \n\n\nChapter 2\n47\nWhen interacting with an LLM application, you chain multiple input prompts and the generated \noutput into a trace, where one prompt depends on previous prompts. \nThus, instead of plain text logs, you need an intuitive way to group these traces into a specialized \ndashboard that makes debugging and monitoring traces of prompts easier.\nWe used Opik, an open-source tool made by Comet, as our prompt monitoring tool because it fol-\nlows Comet’s philosophy of simplicity and ease of use, which is currently relatively rare in the LLM \nlandscape. Other options offering similar features are Langfuse (open source, https://langfuse.\ncom), Galileo (not open source, rungalileo.io), and LangSmith (not open source, https://www.\nlangchain.com/langsmith), but we found their solutions more cumbersome to use and imple-\nment. Opik, along with its serverless option, also provides a free open-source version that you \nhave complete control over. You can read more on Opik at https://github.com/comet-ml/opik.\nDatabases for storing unstructured and vector data\nWe also want to present the NoSQL and vector databases we will use within our examples. When \nworking locally, they are already integrated through Docker. Thus, when running poetry poe \nlocal-infrastructure-up, as instructed a few sections above, local images of Docker for both \ndatabases will be pulled and run on your machine. Also, when deploying the project, we will \nshow you how to use their serverless option and integrate it with the rest of the LLM Twin project.\nMongoDB: NoSQL database\nMongoDB is one of today’s most popular, robust, fast, and feature-rich NoSQL databases. It \nintegrates well with most cloud ecosystems, such as AWS, Google Cloud, Azure, and Databricks. \nThus, using MongoDB as our NoSQL database was a no-brainer.\nWhen we wrote this book, MongoDB was used by big players such as Novo Nordisk, Delivery \nHero, Okta, and Volvo. This widespread adoption suggests that MongoDB will remain a leading \nNoSQL database for a long time.\nWe use MongoDB as a NoSQL database to store the raw data we collect from the internet before \nprocessing it and pushing it into the vector database. As we work with unstructured text data, \nthe flexibility of the NoSQL database fits like a charm.\nQdrant: vector database\nQdrant (https://qdrant.tech/) is one of the most popular, robust, and feature-rich vector \ndatabases. We could have used almost any vector database for our small MVP, but we wanted to \npick something light and likely to be used in the industry for many years to come. \n\n\nTooling and Installation\n48\nWe will use Qdrant to store the data from MongoDB after it’s processed and transformed for \nGenAI usability.\nQdrant is used by big players such as X (formerly Twitter), Disney, Microsoft, Discord, and John-\nson & Johnson. Thus, it is highly probable that Qdrant will remain in the vector database game \nfor a long time.\nWhile writing the book, other popular options were Milvus, Redis, Weaviate, Pinecone, Chroma, \nand pgvector (a PostgreSQL plugin for vector indexes). We found that Qdrant offers the best \ntrade-off between RPS, latency, and index time, making it a solid choice for many generative AI \napplications.\nComparing all the vector databases in detail could be a chapter in itself. We don’t want to do \nthat here. Still, if curious, you can check the Vector DB Comparison resource from Superlinked at \nhttps://superlinked.com/vector-db-comparison, which compares all the top vector databases \nin terms of everything you can think about, from the license and release year to database features, \nembedding models, and frameworks supported.\nPreparing for AWS\nThis last part of the chapter will focus on setting up an AWS account (if you don’t already have \none), an AWS access key, and the CLI. Also, we will look into what SageMaker is and why we use it.\nWe picked AWS as our cloud provider because it’s the most popular out there and the cloud in \nwhich we (the writers) have the most experience. The reality is that other big cloud providers, \nsuch as GCP or Azure, offer similar services. Thus, depending on your specific application, there is \nalways a trade-off between development time (in which you have the most experience), features, \nand costs. But for our MVP, AWS, it’s the perfect option as it provides robust features for every-\nthing we need, such as S3 (object storage), ECR (container registry), and SageMaker (compute \nfor training and inference).\nSetting up an AWS account, an access key, and the CLI\nAs AWS could change its UI/UX, the best way to instruct you on how to create an AWS account is \nby redirecting you to their official tutorial: https://docs.aws.amazon.com/accounts/latest/\nreference/manage-acct-creating.html.\nAfter successfully creating an AWS account, you can access the AWS console at http://console.\naws.amazon.com. Select Sign in using root user email (found under the Sign in button), then \nenter your account’s email address and password.\n\n\nChapter 2\n49\nNext, we must generate access keys to access AWS programmatically. The best option to do so is \nfirst to create an IAM user with administrative access as described in this AWS official tutorial: \nhttps://docs.aws.amazon.com/streams/latest/dev/setting-up.html\nFor production accounts, it is best practice to grant permissions with a policy of least privilege, \ngiving each user only the permissions they require to perform their role. However, to simplify the \nsetup of our test account, we will use the AdministratorAccess managed policy, which gives our \nuser full access, as explained in the tutorial above and illustrated in Figure 2.13.\nFigure 2.13: IAM user permission policies example\nNext, you have to create an access key for the IAM user you just created using the following tutorial: \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html.\nThe access keys will look as follows:\naws_access_key_id = <your_access_key_id>\naws_secret_access_key = <your_secret_access_key>\nJust be careful to store them somewhere safe, as you won’t be able to access them after you cre-\nate them. Also, be cautious with who you share them, as they could be used to access your AWS \naccount and manipulate various AWS resources.\nThe last step is to install the AWS CLI and configure it with your newly created access keys. You \ncan install the AWS CLI using the following link: https://docs.aws.amazon.com/cli/latest/\nuserguide/getting-started-install.html.\nAfter installing the AWS CLI, you can configure it by running aws configure. Here is an example \nof our AWS configuration:\n[default]\naws_access_key_id = *************\naws_secret_access_key = ************\n\n\nTooling and Installation\n50\nregion = eu-central-1\noutput = json\nFor more details on how to configure the AWS CLI, check out the following tutorial: https://\ndocs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html.\nAlso, to configure the project with your AWS credentials, you must fill in the following variables \nwithin your .env file:\nAWS_REGION=\"eu-central-1\" # Change it with your AWS region. By default, we \nuse \"eu-central-1\".\nAWS_ACCESS_KEY=\"<your_aws_access_key>\"\nAWS_SECRET_KEY=\"<your_aws_secret_key>\"\nSageMaker: training and inference compute\nThe last topic of this chapter is understanding SageMaker and why we decided to use it. SageMaker \nis an ML platform used to train and deploy ML models. An official definition is as follows: AWS \nSageMaker is a fully managed machine learning service by AWS that enables developers and data \nscientists to build, train, and deploy machine learning models at scale. It simplifies the process \nby handling the underlying infrastructure, allowing users to focus on developing high-quality \nmodels efficiently.\nAn important note about costs associated with hands-on tasks in this book\nAll the cloud services used across the book stick to their freemium option, except AWS. \nThus, if you use a personal AWS account, you will be responsible for AWS costs as you \nfollow along in this book. While some services may fall under AWS Free Tier usage, \nothers will not. Thus, you are responsible for checking your billing console regularly.\nMost of the costs will come when testing SageMaker for training and inference. Based \non our tests, the AWS costs can vary between $50 and $100 using the specifications \nprovided in this book and repository.\nSee the AWS documentation on setting up billing alarms to monitor your costs \nat https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/\nmonitor_estimated_charges_with_cloudwatch.html.\n",
      "page_number": 67
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 76-83)",
      "start_page": 76,
      "end_page": 83,
      "detection_method": "topic_boundary",
      "content": "Chapter 2\n51\nWe will use SageMaker to fine-tune and operationalize our training pipeline on clusters of GPUs \nand to deploy our custom LLM Twin model as a REST API that can be accessed in real time from \nanywhere in the world.\nWhy AWS SageMaker?\nWe must also discuss why we chose AWS SageMaker over simpler and more cost-effective options, \nsuch as AWS Bedrock. First, let’s explain Bedrock and its benefits.\nAmazon Bedrock is a serverless solution for deploying LLMs. Serverless means that there are no \nservers or infrastructure to manage. It provides pre-trained models, which you can access directly \nthrough API calls. When we wrote this book, they provided support only for Mistral, Flan, Llama \n2, and Llama 3 (quite a limited list of options). You can send input data and receive predictions \nfrom the models without managing the underlying infrastructure or software. This approach sig-\nnificantly reduces the complexity and time required to integrate AI capabilities into applications, \nmaking it more accessible to developers with limited machine learning expertise. However, this \nease of integration comes at the cost of limited customization options, as you’re restricted to the \npre-trained models and APIs provided by Amazon Bedrock. In terms of pricing, Bedrock uses a \nsimple pricing model based on the number of API calls. This straightforward pricing structure \nmakes it more efficient to estimate and control costs.\nMeanwhile, SageMaker provides a comprehensive platform for building, training, and deploying \nmachine learning models. It allows you to customize your ML processes entirely or even use \nthe platform for research. That’s why SageMaker is mainly used by data scientists and machine \nlearning experts who know how to program, understand machine learning concepts, and are \ncomfortable working with cloud platforms such as AWS. SageMaker is a double-edged sword \nregarding costs, following a pay-as-you-go pricing model similar to most AWS services. This \nmeans you have to pay for the usage of computing resources, storage, and any other services \nrequired to build your applications.\nIn contrast to Bedrock, even if the SageMaker endpoint is not used, you will still pay for the \ndeployed resources on AWS, such as online EC2 instances. Thus, you have to design autoscaling \nsystems that delete unused resources. To conclude, Bedrock offers an out-of-the-box solution \nthat allows you to quickly deploy an API endpoint powered by one of the available foundation \nmodels. Meanwhile, SageMaker is a multi-functional platform enabling you to customize your \nML logic fully.\n\n\nTooling and Installation\n52\nSo why did we choose SageMaker over Bedrock? Bedrock would have been an excellent solution \nfor quickly prototyping something, but this is a book on LLM engineering, and our goal is to dig \ninto all the engineering aspects that Bedrock tries to mask away. Thus, we chose SageMaker \nbecause of its high level of customizability, allowing us to show you all the engineering required \nto deploy a model.\nIn reality, even SageMaker isn’t fully customizable. If you want complete control over your de-\nployment, use EKS, AWS’s Kubernetes self-managed service. In this case, you have direct access \nto the virtual machines, allowing you to fully customize how you build your ML pipelines, how \nthey interact, and how you manage your resources. You could do the same thing with AWS ECS, \nAWS’s version of Kubernetes. Using EKS or ECS, you could also reduce the costs, as these services \ncost considerably less.\nTo conclude, SageMaker strikes a balance between complete control and customization and a fully \nmanaged service that hides all the engineering complexity behind the scenes. This balance ensures \nthat you have the control you need while also benefiting from the managed service’s convenience.\nSummary\nIn this chapter, we reviewed the core tools used across the book. First, we understood how to \ninstall the correct version of Python that supports our repository. Then, we looked over how to \ncreate a virtual environment and install all the dependencies using Poetry. Finally, we understood \nhow to use a task execution tool like Poe the Poet to aggregate all the commands required to run \nthe application.\nThe next step was to review all the tools used to ensure MLOps best practices, such as a model \nregistry to share our models, an experiment tracker to manage our training experiments, an \norchestrator to manage all our ML pipelines and artifacts, and metadata to manage all our files \nand datasets. We also understood what type of databases we need to implement the LLM Twin \nuse case. Finally, we explored the process of setting up an AWS account, generating an access \nkey, and configuring the AWS CLI for programmatic access to the AWS cloud. We also gained a \ndeep understanding of AWS SageMaker and the reasons behind choosing it to build our LLM \nTwin application.\nIn the next chapter, we will explore the implementation of the LLM Twin project by starting with \nthe data collection ETL that scrapes posts, articles, and repositories from the internet and stores \nthem in a data warehouse.\n\n\nChapter 2\n53\nReferences\n• \nAcsany, P. (2024, February 19). Dependency Management With Python Poetry. https://\nrealpython.com/dependency-management-python-poetry/\n•\nComet.ml. (n.d.). comet-ml/opik: Open-source end-to-end LLM Development Platform. GitHub. \nhttps://github.com/comet-ml/opik\n•\nCzakon, J. (2024, September 25). ML Experiment Tracking: What It Is, Why It Matters, and \nHow to Implement It. neptune.ai. https://neptune.ai/blog/ml-experiment-tracking\n•\nHopsworks. (n.d.). ML Artifacts (ML Assets)? Hopsworks. https://www.hopsworks.ai/\ndictionary/ml-artifacts\n•\nIntroduction | Documentation | Poetry – Python dependency management and packaging made \neasy. (n.d.). https://python-poetry.org/docs\n•\nJones, L. (2024, March 21). Managing Multiple Python Versions With pyenv. https://\nrealpython.com/intro-to-pyenv/\n•\nKaewsanmua, K. (2024, January 3). Best Machine Learning Workflow and Pipeline Orches-\ntration Tools. neptune.ai. https://neptune.ai/blog/best-workflow-and-pipeline-\norchestration-tools\n•\nMongoDB. (n.d.). What is NoSQL? NoSQL databases explained. https://www.mongodb.\ncom/resources/basics/databases/nosql-explained\n•\nNat-N. (n.d.). nat-n/poethepoet: A task runner that works well with poetry. GitHub. https://\ngithub.com/nat-n/poethepoet\n•\nOladele, S. (2024, August 29). ML Model Registry: The Ultimate Guide. neptune.ai. https://\nneptune.ai/blog/ml-model-registry\n•\nSchwaber-Cohen, R. (n.d.). What is a Vector Database & How Does it Work? Use Cases + Ex-\namples. Pinecone. https://www.pinecone.io/learn/vector-database/\n•\nStarter guide | ZenML Documentation. (n.d.). https://docs.zenml.io/user-guide/\nstarter-guide\n• \nVector DB Comparison. (n.d.). https://superlinked.com/vector-db-comparison\n\n\nTooling and Installation\n54\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n\n\n3\nData Engineering\nThis chapter will begin exploring the LLM Twin project in more depth. We will learn how to \ndesign and implement the data collection pipeline to gather the raw data we will use in all our \nLLM use cases, such as fine-tuning or inference. As this is not a book on data engineering, we \nwill keep this chapter short and focus only on what is strictly necessary to collect the required \nraw data. Starting with Chapter 4, we will concentrate on LLMs and GenAI, exploring its theory \nand concrete implementation details.\nWhen working on toy projects or doing research, you usually have a static dataset with which \nyou work. But in our LLM Twin use case, we want to mimic a real-world scenario where we must \ngather and curate the data ourselves. Thus, implementing our data pipeline will connect the dots \nregarding how an end-to-end ML project works. This chapter will explore how to design and \nimplement an Extract, Transform, Load (ETL) pipeline that crawls multiple social platforms, \nsuch as Medium, Substack, or GitHub, and aggregates the gathered data into a MongoDB data \nwarehouse. We will show you how to implement various crawling methods, standardize the data, \nand load it into a data warehouse.\nWe will begin by designing the LLM Twin’s data collection pipeline and explaining the architecture \nof the ETL pipeline. Afterward, we will move directly to implementing the pipeline, starting with \nZenML, which will orchestrate the entire process. We will investigate the crawler implementation \nand understand how to implement a dispatcher layer that instantiates the right crawler class \nbased on the domain of the provided link while following software best practices. Next, we will \nlearn how to implement each crawler individually. Also, we will show you how to implement \na data layer on top of MongoDB to structure all our documents and interact with the database.\n\n\nData Engineering\n56\nFinally, we will explore how to run the data collection pipeline using ZenML and query the col-\nlected data from MongoDB.\nThus, in this chapter, we will study the following topics:\n•\nDesigning the LLM Twin’s data collection pipeline\n•\nImplementing the LLM Twin’s data collection pipeline\n•\nGathering raw data into the data warehouse\nBy the end of this chapter, you will know how to design and implement an ETL pipeline to extract, \ntransform, and load raw data ready to be ingested into the ML application.\nDesigning the LLM Twin’s data collection pipeline\nBefore digging into the implementation, we must understand the LLM Twin’s data collection ETL \narchitecture, illustrated in Figure 3.1. We must explore what platforms we will crawl to extract \ndata from and how we will design our data structures and processes. However, the first step is \nunderstanding how our data collection pipeline maps to an ETL process.\nAn ETL pipeline involves three fundamental steps:\n1. \nWe extract data from various sources. We will crawl data from platforms like Medium, \nSubstack, and GitHub to gather raw data.\n2. We transform this data by cleaning and standardizing it into a consistent format suitable \nfor storage and analysis.\n3. \nWe load the transformed data into a data warehouse or database.\nFor our project, we use MongoDB as our NoSQL data warehouse. Although this is not a standard \napproach, we will explain the reasoning behind this choice shortly.\n\n\nChapter 3\n57\nFigure 3.1: LLM Twin’s data collection ETL pipeline architecture\nWe want to design an ETL pipeline that inputs a user and a list of links as input. Afterward, it \ncrawls each link individually, standardizes the collected content, and saves it under that specific \nauthor in a MongoDB data warehouse. \n\n\nData Engineering\n58\nHence, the signature of the data collection pipeline will look as follows:\n• \nInput: A list of links and their associated user (the author)\n• \nOutput: A list of raw documents stored in the NoSQL data warehouse\nWe will use user and author interchangeably, as in most scenarios across the ETL pipeline, a \nuser is the author of the extracted content. However, within the data warehouse, we have only \na user collection.\nThe ETL pipeline will detect the domain of each link, based on which it will call a specialized \ncrawler. We implemented four different crawlers for three different data categories, as seen in \nFigure 3.2. First, we will explore the three fundamental data categories we will work with across \nthe book. All our collected documents can be boiled down to an article, repository (or code), and \npost. It doesn’t matter where the data comes from. We are primarily interested in the document’s \nformat. In most scenarios, we will have to process these data categories differently. Thus, we \ncreated a different domain entity for each, where each entity will have its class and collection \nin MongoDB. As we save the source URL within the document’s metadata, we will still know its \nsource and can reference it in our GenAI use cases.\nFigure 3.2: The relationship between the crawlers and the data categories\n",
      "page_number": 76
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 84-97)",
      "start_page": 84,
      "end_page": 97,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n59\nOur codebase supports four different crawlers:\n•\nMedium crawler: Used to collect data from Medium. It outputs an article document. It \nlogs in to Medium and crawls the HTML of the article’s link. Then, it extracts, cleans, and \nnormalizes the text from the HTML and loads the standardized text of the article into the \nNoSQL data warehouse.\n•\nCustom article crawler: It performs similar steps to the Medium crawler but is a more \ngeneric implementation for collecting articles from various sites. Thus, as it doesn’t im-\nplement any particularities of any platform, it doesn’t perform the login step and blindly \ngathers all the HTML from a particular link. This is enough for articles freely available \nonline, which you can find on Substack and people’s blogs. We will use this crawler as a \nsafety net when the link’s domain isn’t associated with the other supported crawlers. For \nexample, when providing a Substack link, it will default to the custom article crawler, but \nwhen providing a Medium URL, it will use the Medium crawler.\n•\nGitHub crawler: This collects data from GitHub. It outputs a repository document. It \nclones the repository, parses the repository file tree, cleans and normalizes the files, and \nloads them to the database.\n•\nLinkedIn crawler: This is used to collect data from LinkedIn. It outputs multiple post \ndocuments. It logs in to LinkedIn, navigates to the user’s feed, and crawls all the user’s \nlatest posts. For each post, it extracts its HTML, cleans and normalizes it, and loads it to \nMongoDB.\nIn the next section, we will examine each crawler’s implementation in detail. For now, note that \neach crawler accesses a specific platform or site in a particular way and extracts HTML from it. \nAfterward, all the crawlers parse the HTML, extract the text from it, and clean and normalize it \nso it can be stored in the data warehouse under the same interface.\nBy reducing all the collected data to three data categories and not creating a new data category \nfor every new data source, we can easily extend this architecture to multiple data sources with \nminimal effort. For example, if we want to start collecting data from X, we only have to imple-\nment a new crawler that outputs a post document, and that’s it. The rest of the code will remain \nuntouched. Otherwise, if we introduced the source dimension in the class and document struc-\nture, we would have to add code to all downstream layers to support any new data source. For \nexample, we would have to implement a new document class for each new source and adapt the \nfeature pipeline to support it.\n\n\nData Engineering\n60\nFor our proof of concept, crawling a few hundred documents is enough, but if we want to scale it \nto a real-world product, we would probably need more data sources to crawl from. LLMs are da-\nta-hungry. Thus, you need thousands of documents for ideal results instead of just a few hundred. \nBut in many projects, it’s an excellent strategy to implement an end-to-end project version that \nisn’t the most accurate and iterate through it later. Thus, by using this architecture, you can easily \nadd more data sources in future iterations to gather a larger dataset. More on LLM fine-tuning \nand dataset size will be covered in the next chapter.\nHow is the ETL process connected to the feature pipeline? The feature pipeline ingests the raw \ndata from the MongoDB data warehouse, cleans it further, processes it into features, and stores it \nin the Qdrant vector DB to make it accessible for the LLM training and inference pipelines. Chap-\nter 4 provides more information on the feature pipeline. The ETL process is independent of the \nfeature pipeline. The two pipelines communicate with each other strictly through the MongoDB \ndata warehouse. Thus, the data collection pipeline can write data for MongoDB, and the feature \npipeline can read from it independently and on different schedules.\nWhy did we use MongoDB as a data warehouse? Using a transactional database, such as Mon-\ngoDB, as a data warehouse is uncommon. However, in our use case, we are working with small \namounts of data, which MongoDB can handle. Even if we plan to compute statistics on top of our \nMongoDB collections, it will work fine at the scale of our LLM Twin’s data (hundreds of docu-\nments). We picked MongoDB to store our raw data primarily because of the nature of our unstruc-\ntured data: text crawled from the internet. By mainly working with unstructured text, selecting \na NoSQL database that doesn’t enforce a schema made our development easier and faster. Also, \nMongoDB is stable and easy to use. Their Python SDK is intuitive. They provide a Docker image \nthat works out of the box locally and a cloud freemium tier that is perfect for proofs of concept, \nsuch as the LLM Twin. Thus, we can freely work with it locally and in the cloud. However, when \nworking with big data (millions of documents or more), using a dedicated data warehouse such \nas Snowflake or BigQuery will be ideal.\nNow that we’ve understood the architecture of the LLM Twin’s data collection pipeline, let’s \nmove on to its implementation.\n\n\nChapter 3\n61\nImplementing the LLM Twin’s data collection pipeline\nAs we presented in Chapter 2, the entry point to each pipeline from our LLM Twin project is a \nZenML pipeline, which can be configured at runtime through YAML files and run through the \nZenML ecosystem. Thus, let’s start by looking into the ZenML digital_data_etl pipeline. You’ll \nnotice that this is the same pipeline we used as an example in Chapter 2 to illustrate ZenML. But \nthis time, we will dig deeper into the implementation, explaining how the data collection works \nbehind the scenes. After understanding how the pipeline works, we will explore the implemen-\ntation of each crawler used to collect data from various sites and the MongoDB documents used \nto store and query data from the data warehouse.\nZenML pipeline and steps\nIn the code snippet below, we can see the implementation of the ZenML digital_data_etl\npipeline, which inputs the user’s full name and a list of links that will be crawled under that user \n(considered the author of the content extracted from those links). Within the function, we call two \nsteps. In the first one, we look up the user in the database based on its full name. Then, we loop \nthrough all the links and crawl each independently. The pipeline’s implementation is available \nin our repository at pipelines/digital_data_etl.py.\nfrom zenml import pipeline\nfrom steps.etl import crawl_links, get_or_create_user\n@pipeline\ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\n    user = get_or_create_user(user_full_name)\n    last_step = crawl_links(user=user, links=links)\n    return last_step.invocation_id\n\n\nData Engineering\n62\nFigure 3.3 shows a run of the digital_data_etl pipeline on the ZenML dashboard. The next \nphase is to explore the get_or_create_user and crawl_links ZenML steps individually. The \nstep implementation is available in our repository at steps/etl.\nFigure 3.3: Example of a digital_data_etl pipeline run from ZenML’s dashboard\nWe will start with the get_or_create_user ZenML step. We begin by importing the necessary \nmodules and functions used throughout the script.\nfrom loguru import logger\nfrom typing_extensions import Annotated\nfrom zenml import get_step_context, step\nfrom llm_engineering.application import utils\nfrom llm_engineering.domain.documents import UserDocument\nNext, we define the function’s signature, which takes a user’s full name as input and retrieves an \nexisting user or creates a new one in the MongoDB database if it doesn’t exist:\n@step\ndef get_or_create_user(user_full_name: str) -> Annotated[UserDocument, \n\"user\"]:\n\n\nChapter 3\n63\nUsing a utility function, we split the full name into first and last names. Then, we attempt to \nretrieve the user from the database or create a new one if it doesn’t exist. We also retrieve the \ncurrent step context and add metadata about the user to the output, which will be reflected in \nthe metadata of the user ZenML output artifact:\n    logger.info(f\"Getting or creating user: {user_full_name}\")\n    first_name, last_name = utils.split_user_full_name(user_full_name)\n    user = UserDocument.get_or_create(first_name=first_name, last_\nname=last_name)\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"user\", metadata=_get_\nmetadata(user_full_name, user))\n    return user\nAdditionally, we define a helper function called _get_metadata(), which builds a dictionary \ncontaining the query parameters and the retrieved user information, which will be added as \nmetadata to the user artifact:\ndef _get_metadata(user_full_name: str, user: UserDocument) -> dict:\n    return {\n        \"query\": {\n            \"user_full_name\": user_full_name,\n        },\n        \"retrieved\": {\n            \"user_id\": str(user.id),\n            \"first_name\": user.first_name,\n            \"last_name\": user.last_name,\n        },\n    }\nWe will move on to the crawl_links ZenML step, which collects the data from the provided links. \nThe code begins by importing essential modules and libraries for web crawling:\nfrom urllib.parse import urlparse\nfrom loguru import logger\n\n\nData Engineering\n64\nfrom tqdm import tqdm\nfrom typing_extensions import Annotated\nfrom zenml import get_step_context, step\nfrom llm_engineering.application.crawlers.dispatcher import \nCrawlerDispatcher\nfrom llm_engineering.domain.documents import UserDocument\nFollowing the imports, the main function inputs a list of links written by a specific author. Within \nthis function, a crawler dispatcher is initialized and configured to handle specific domains such \nas LinkedIn, Medium, and GitHub:\n@step\ndef crawl_links(user: UserDocument, links: list[str]) -> \nAnnotated[list[str], \"crawled_links\"]:\n    dispatcher = CrawlerDispatcher.build().register_linkedin().register_\nmedium().register_github()\n    logger.info(f\"Starting to crawl {len(links)} link(s).\")\nThe function initializes variables to store the output metadata and count successful crawls. It then \niterates over each link. It attempts to crawl and extract data for each link, updating the count of \nsuccessful crawls and accumulating metadata about each URL:\n    metadata = {}\n    successfull_crawls = 0\n    for link in tqdm(links):\n        successfull_crawl, crawled_domain = _crawl_link(dispatcher, link, \nuser)\n        successfull_crawls += successfull_crawl\n        metadata = _add_to_metadata(metadata, crawled_domain, successfull_\ncrawl)\nAfter processing all links, the function attaches the accumulated metadata to the output artifact:\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"crawled_links\", \nmetadata=metadata)\n    logger.info(f\"Successfully crawled {successfull_crawls} / {len(links)} \n\n\nChapter 3\n65\nlinks.\")\n    return links\nThe code includes a helper function that attempts to extract information from each link using the \nappropriate crawler based on the link’s domain. It handles any exceptions that may occur during \nextraction and returns a tuple indicating the crawl’s success and the link’s domain:\ndef _crawl_link(dispatcher: CrawlerDispatcher, link: str, user: \nUserDocument) -> tuple[bool, str]:\n    crawler = dispatcher.get_crawler(link)\n    crawler_domain = urlparse(link).netloc\n    try:\n        crawler.extract(link=link, user=user)\n        return (True, crawler_domain)\n    except Exception as e:\n        logger.error(f\"An error occurred while crawling: {e!s}\")\n        return (False, crawler_domain)\nAnother helper function is provided to update the metadata dictionary with the results of each \ncrawl:\ndef _add_to_metadata(metadata: dict, domain: str, successfull_crawl: bool) \n-> dict:\n    if domain not in metadata:\n        metadata[domain] = {}\n    metadata[domain][\"successful\"] = metadata.get(domain, {}).\nget(\"successful\", 0) + successfull_crawl\n    metadata[domain][\"total\"] = metadata.get(domain, {}).get(\"total\", 0) + \n1\n    return metadata\nAs seen in the abovementioned _crawl_link() function, the CrawlerDispatcher class knows \nwhat crawler to initialize based on each link’s domain. The logic is then abstracted away under \nthe crawler’s extract() method. Let’s zoom in on the CrawlerDispatcher class to understand \nhow this works fully.\n\n\nData Engineering\n66\nThe dispatcher: How do you instantiate the right crawler?\nThe entry point to our crawling logic is the CrawlerDispatcher class. As illustrated in Figure \n3.4, the dispatcher acts as the intermediate layer between the provided links and the crawlers. It \nknows what crawler to associate with each URL.\nThe CrawlerDispatcher class knows how to extract the domain of each link and initialize the \nproper crawler that collects the data from that site. For example, if it detects the https://medium.\ncom domain when providing a link to an article, it will build an instance of the MediumCrawler\nused to crawl that particular platform. With that in mind, let’s explore the implementation of \nthe CrawlerDispatcher class.\nFigure 3.4: The relationship between the provided links, the CrawlerDispatcher, and the \ncrawlers\nAll the crawling logic is available in the GitHub repository at llm_engineering/\napplication/crawlers.\n\n\nChapter 3\n67\nWe begin by importing the necessary Python modules for URL handling and regex, along with \nimporting our crawler classes:\nimport re\nfrom urllib.parse import urlparse\nfrom loguru import logger\nfrom .base import BaseCrawler\nfrom .custom_article import CustomArticleCrawler\nfrom .github import GithubCrawler\nfrom .linkedin import LinkedInCrawler\nfrom .medium import MediumCrawler\nThe CrawlerDispatcher class is defined to manage and dispatch appropriate crawler instances \nbased on given URLs and their domains. Its constructor initializes a registry to store the regis-\ntered crawlers.\nclass CrawlerDispatcher:\n    def __init__(self) -> None:\n        self._crawlers = {}\nAs we are using the builder creational pattern to instantiate and configure the dispatcher, we \ndefine a build() class method that returns an instance of the dispatcher:\n    @classmethod\n    def build(cls) -> \"CrawlerDispatcher\":\n        dispatcher = cls()\n        return dispatcher\nThe dispatcher includes methods to register crawlers for specific platforms like Medium, Linke-\ndIn, and GitHub. These methods use a generic register() method under the hood to add each \ncrawler to the registry. By returning self, we follow the builder creational pattern (more on the \nbuilder pattern: https://refactoring.guru/design-patterns/builder). We can chain mul-\ntiple register_*() methods when instantiating the dispatcher as follows: CrawlerDispatcher.\nbuild().register_linkedin().register_medium().\n    def register_medium(self) -> \"CrawlerDispatcher\":\n        self.register(\"https://medium.com\", MediumCrawler)\n\n\nData Engineering\n68\n        return self\n    def register_linkedin(self) -> \"CrawlerDispatcher\":\n        self.register(\"https://linkedin.com\", LinkedInCrawler)\n        return self\n    def register_github(self) -> \"CrawlerDispatcher\":\n        self.register(\"https://github.com\", GithubCrawler)\n        return self\nThe generic register() method normalizes each domain to ensure its format is consistent be-\nfore it’s added as a key to the self._crawlers registry of the dispatcher. This is a critical step, as \nwe will use the key of the dictionary as the domain pattern to match future links with a crawler:\n    def register(self, domain: str, crawler: type[BaseCrawler]) -> None:\n        parsed_domain = urlparse(domain)\n        domain = parsed_domain.netloc\n        self._crawlers[r\"https://(www\\.)?{}/*\".format(re.escape(domain))] \n= crawler\nFinally, the get_crawler() method determines the appropriate crawler for a given URL by match-\ning it against the registered domains. If no match is found, it logs a warning and defaults to using \nthe CustomArticleCrawler.\n    def get_crawler(self, url: str) -> BaseCrawler:\n        for pattern, crawler in self._crawlers.items():\n            if re.match(pattern, url):\n                return crawler()\n        else:\n            logger.warning(f\"No crawler found for {url}. Defaulting to \nCustomArticleCrawler.\")\n            return CustomArticleCrawler()\nThe next step in understanding how the data collection pipeline works is analyzing each crawler \nindividually.\n\n\nChapter 3\n69\nThe crawlers\nBefore exploring each crawler’s implementation, we must present their base class, which defines \na unified interface for all the crawlers. As shown in Figure 3.4, we can implement the dispatcher \nlayer because each crawler follows the same signature. Each class implements the extract()\nmethod, allowing us to leverage OOP techniques such as polymorphism, where we can work with \nabstract objects without knowing their concrete subclass. For example, in the _crawl_link()\nfunction from the ZenML steps, we had the following code:\ncrawler = dispatcher.get_crawler(link)\ncrawler.extract(link=link, user=user)\nNote how we called the extract() method without caring about what specific type of crawler \nwe instantiated. To conclude, working with abstract interfaces ensures core reusability and ease \nof extension.\nBase classes\nNow, let’s explore the BaseCrawler interface, which can be found in the repository at https://\ngithub.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/\napplication/crawlers/base.py.\nfrom abc import ABC, abstractmethod\nclass BaseCrawler(ABC):\n    model: type[NoSQLBaseDocument]\n    @abstractmethod\n    def extract(self, link: str, **kwargs) -> None: ...\nAs mentioned above, the interface defines an extract() method that takes as input a link. Also, it \ndefines a model attribute at the class level that represents the data category document type used \nto save the extracted data into the MongoDB data warehouse. Doing so allows us to customize \neach subclass with different data categories while preserving the same attributes at the class \nlevel. We will soon explore the NoSQLBaseDocument class when digging into the document entities.\nWe also extend the BaseCrawler class with a BaseSeleniumCrawler class, which implements \nreusable functionality that uses Selenium to crawl various sites, such as Medium or LinkedIn. \nSelenium is a tool for automating web browsers. It’s used to interact with web pages program-\nmatically (like logging into LinkedIn, navigating through profiles, etc.). \n\n\nData Engineering\n70\nSelenium can programmatically control various browsers such as Chrome, Firefox, or Brave. For \nthese specific platforms, we need Selenium to manipulate the browser programmatically to log in \nand scroll through the newsfeed or article before being able to extract the entire HTML. For other \nsites, where we don’t have to go through the login step or can directly load the whole page, we \ncan extract the HTML from a particular URL using more straightforward methods than Selenium.\nThe code begins by setting up the necessary imports and configurations for web crawling using \nSelenium and the ChromeDriver initializer. The chromedriver_autoinstaller ensures that \nthe appropriate version of ChromeDriver is installed and added to the system path, maintain-\ning compatibility with the installed version of your Google Chrome browser (or other Chromi-\num-based browser). Selenium will use the ChromeDriver to communicate with the browser \nand open a headless session, where we can programmatically manipulate the browser to access \nvarious URLs, click on specific elements, such as buttons, or scroll through the newsfeed. Using \nthe chromedriver_autoinstaller, we ensure we always have the correct ChromeDriver version \ninstalled that matches our machine’s Chrome browser version.\nimport time\nfrom tempfile import mkdtemp\nimport chromedriver_autoinstaller\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom llm_engineering.domain.documents import NoSQLBaseDocument\n# Check if the current version of chromedriver exists\n# and if it doesn't exist, download it automatically,\n# then add chromedriver to path\nchromedriver_autoinstaller.install()\nNext, we define the BaseSeleniumCrawler class for use cases where we need Selenium to collect \nthe data, such as collecting data from Medium or LinkedIn. \nFor the Selenium-based crawlers to work, you must install Chrome on your machine \n(or a Chromium-based browser such as Brave).\n\n\nChapter 3\n71\nIts constructor initializes various Chrome options to optimize performance, enhance security, and \nensure a headless browsing environment. These options disable unnecessary features like GPU \nrendering, extensions, and notifications, which can interfere with automated browsing. These \nare standard configurations when crawling in headless mode:\nclass BaseSeleniumCrawler(BaseCrawler, ABC):\n    def __init__(self, scroll_limit: int = 5) -> None:\n        options = webdriver.ChromeOptions()\n       \n        options.add_argument(\"--no-sandbox\")\n        options.add_argument(\"--headless=new\")\n        options.add_argument(\"--disable-dev-shm-usage\")\n        options.add_argument(\"--log-level=3\")\n        options.add_argument(\"--disable-popup-blocking\")\n        options.add_argument(\"--disable-notifications\")\n        options.add_argument(\"--disable-extensions\")\n        options.add_argument(\"--disable-background-networking\")\n        options.add_argument(\"--ignore-certificate-errors\")\n        options.add_argument(f\"--user-data-dir={mkdtemp()}\")\n        options.add_argument(f\"--data-path={mkdtemp()}\")\n        options.add_argument(f\"--disk-cache-dir={mkdtemp()}\")\n        options.add_argument(\"--remote-debugging-port=9226\")\nAfter configuring the Chrome options, the code allows subclasses to set any additional driver \noptions by calling the set_extra_driver_options() method. It then initializes the scroll limit \nand creates a new instance of the Chrome driver with the specified options:\n        self.set_extra_driver_options(options)\n        self.scroll_limit = scroll_limit\n        self.driver = webdriver.Chrome(\n            options=options,\n        )\nThe BaseSeleniumCrawler class includes placeholder methods for set_extra_driver_options()\nand login(), which subclasses can override to provide specific functionality. This ensures mod-\nularity, as every platform has a different login page with a different HTML structure:\n    def set_extra_driver_options(self, options: Options) -> None:\n\n\nData Engineering\n72\n        pass\n    def login(self) -> None:\n        pass\nFinally, the scroll_page() method implements a scrolling mechanism to navigate through pages, \nsuch as LinkedIn, up to a specified scroll limit. It scrolls to the bottom of the page, waits for new \ncontent to load, and repeats the process until it reaches the end of the page or the scroll limit is \nexceeded. This method is essential for feeds where the content appears as the user scrolls:\n    def scroll_page(self) -> None:\n        \"\"\"Scroll through the LinkedIn page based on the scroll limit.\"\"\"\n        current_scroll = 0\n        last_height = self.driver.execute_script(\"return document.body.\nscrollHeight\")\n        while True:\n            self.driver.execute_script(\"window.scrollTo(0, document.body.\nscrollHeight);\")\n            time.sleep(5)\n            new_height = self.driver.execute_script(\"return document.body.\nscrollHeight\")\n            if new_height == last_height or (self.scroll_limit and \ncurrent_scroll >= self.scroll_limit):\n                break\n            last_height = new_height\n            current_scroll += 1\nWe’ve understood what the base classes of our crawlers look like. Next, we will look into the \nimplementation of the following specific crawlers:\n•\nGitHubCrawler(BaseCrawler)\n•\nCustomArticleCrawler(BaseCrawler)\n•\nMediumCrawler(BaseSeleniumCrawler)\nYou can find the implementation of the above crawlers in the GitHub repository at \nhttps://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main \n/llm_engineering/application/crawlers.\n",
      "page_number": 84
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 98-105)",
      "start_page": 98,
      "end_page": 105,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n73\nGitHubCrawler class\nThe GithubCrawler class is designed to scrape GitHub repositories, extending the functionality \nof the BaseCrawler. We don’t have to log in to GitHub through the browser, as we can leverage \nGit’s clone functionality. Thus, we don’t have to leverage any Selenium functionality. Upon ini-\ntialization, it sets up a list of patterns to ignore standard files and directories found in GitHub \nrepositories, such as .git, .toml, .lock, and .png, ensuring that unnecessary files are excluded \nfrom the scraping process:\nclass GithubCrawler(BaseCrawler):\n    model = RepositoryDocument\n    def __init__(self, ignore=(\".git\", \".toml\", \".lock\", \".png\")) -> None:\n        super().__init__()\n        self._ignore = ignore\nNext, we implement the extract() method, where the crawler first checks if the repository has \nalready been processed and stored in the database. If it exists, it exits the method to prevent \nstoring duplicates:\ndef extract(self, link: str, **kwargs) -> None:\n    old_model = self.model.find(link=link)\n    if old_model is not None:\n        logger.info(f\"Repository already exists in the database: {link}\")\n        return\nIf the repository is new, the crawler extracts the repository name from the link. Then, it creates \na temporary directory to clone the repository to ensure that the cloned repository is cleaned up \nfrom the local disk after it’s processed:\n    logger.info(f\"Starting scrapping GitHub repository: {link}\")\n    repo_name = link.rstrip(\"/\").split(\"/\")[-1]\n    local_temp = tempfile.mkdtemp()\nWithin a try block, the crawler changes the current working directory to the temporary directory \nand executes the git clone command in a different process:\n    try:\n\n\nData Engineering\n74\n        os.chdir(local_temp)\n        subprocess.run([\"git\", \"clone\", link])\nAfter successfully cloning the repository, the crawler constructs the path to the cloned repository. \nIt initializes an empty dictionary used to aggregate the content of the files in a standardized way. \nIt walks through the directory tree, skipping over any directories or files that match the ignore \npatterns. For each relevant file, it reads the content, removes any spaces, and stores it in the dic-\ntionary with the file path as the key:\n        repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])  # \n        tree = {}\n        for root, _, files in os.walk(repo_path):\n            dir = root.replace(repo_path, \"\").lstrip(\"/\")\n            if dir.startswith(self._ignore):\n                continue\n            for file in files:\n                if file.endswith(self._ignore):\n                    continue\n                file_path = os.path.join(dir, file)\n                with open(os.path.join(root, file), \"r\", errors=\"ignore\") \nas f:\n                    tree[file_path] = f.read().replace(\" \", \"\")\nIt then creates a new instance of the RepositoryDocument model, populating it with the repos-\nitory content, name, link, platform information, and author details. The instance is then saved \nto MongoDB:\n        user = kwargs[\"user\"]\n        instance = self.model(\n            content=tree,\n            name=repo_name,\n            link=link,\n            platform=\"github\",\n            author_id=user.id,\n            author_full_name=user.full_name,\n        )\n        instance.save()\n\n\nChapter 3\n75\nFinally, whether the scraping succeeds or an exception occurs, the crawler ensures that the tem-\nporary directory is removed to clean up any resources used during the process:\n    except Exception:\n        raise\n    finally:\n        shutil.rmtree(local_temp)\n    logger.info(f\"Finished scrapping GitHub repository: {link}\")\nCustomArticleCrawler class\nThe CustomArticleCrawler class takes a different approach to collecting data from the in-\nternet. It leverages the AsyncHtmlLoader class to read the entire HTML from a link and the \nHtml2TextTransformer class to extract the text from that HTML. Both classes are made available \nby the langchain_community Python package, as seen below, where we import all the necessary \nPython modules:\nfrom urllib.parse import urlparse\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_community.document_transformers.html2text import \nHtml2TextTransformer\nfrom loguru import logger\nfrom llm_engineering.domain.documents import ArticleDocument\nfrom .base import BaseCrawler\nNext, we define the CustomArticleCrawler class, which inherits from BaseCrawler. As before, \nwe don’t need to log in or use the scrolling functionality provided by Selenium. In the extract\nmethod, we first check if the article exists in the database to avoid duplicating content:\nclass CustomArticleCrawler(BaseCrawler):\n    model = ArticleDocument\n    def extract(self, link: str, **kwargs) -> None:\n        old_model = self.model.find(link=link)\n        if old_model is not None:\n\n\nData Engineering\n76\n            logger.info(f\"Article already exists in the database: {link}\")\n            return\nIf the article doesn’t exist, we proceed to scrape it. We use the AsyncHtmlLoader class to load the \nHTML from the provided link. After, we transform it into plain text using the Html2TextTransformer\nclass, which returns a list of documents. We are only interested in the first document. As we dele-\ngate the whole logic to these two classes, we don’t control how the content is extracted and parsed. \nThat’s why we used this class as a fallback system for domains where we don’t have anything cus-\ntom implemented. These two classes follow the LangChain paradigm, which provides high-level \nfunctionality that works decently in most scenarios. It is fast to implement but hard to customize. \nThat is one of the reasons why many developers avoid using LangChain in production use cases:\n        logger.info(f\"Starting scrapping article: {link}\")\n        loader = AsyncHtmlLoader([link])\n        docs = loader.load()\n        html2text = Html2TextTransformer()\n        docs_transformed = html2text.transform_documents(docs)\n        doc_transformed = docs_transformed[0]\nWe get the page content from the extracted document, plus relevant metadata such as the title, \nsubtitle, content, and language:\n        content = {\n            \"Title\": doc_transformed.metadata.get(\"title\"),\n            \"Subtitle\": doc_transformed.metadata.get(\"description\"),\n            \"Content\": doc_transformed.page_content,\n            \"language\": doc_transformed.metadata.get(\"language\"),\n        }\nNext, we parse the URL to determine the platform (or domain) from which the article was scraped:\n        parsed_url = urlparse(link)\n        platform = parsed_url.netloc\nWe then create a new instance of the article model, populating it with the extracted content. \nFinally, we save this instance to the MongoDB data warehouse:\n        user = kwargs[\"user\"]\n\n\nChapter 3\n77\n        instance = self.model(\n            content=content,\n            link=link,\n            platform=platform,\n            author_id=user.id,\n            author_full_name=user.full_name,\n        )\n        instance.save()\n        logger.info(f\"Finished scrapping custom article: {link}\")       \nSo far, we have seen how to crawl GitHub repositories and random sites using LangChain utility \nfunctions. Lastly, we must explore a crawler using Selenium to manipulate the browser program-\nmatically. Thus, we will continue with the MediumCrawler implementation.\nMediumCrawler class\nThe code begins by importing essential libraries and defining the MediumCrawler class, which \ninherits from BaseSeleniumCrawler:\nfrom bs4 import BeautifulSoup\nfrom loguru import logger\nfrom llm_engineering.domain.documents import ArticleDocument\nfrom .base import BaseSeleniumCrawler\nclass MediumCrawler(BaseSeleniumCrawler):\n    model = ArticleDocument\nWithin the MediumCrawler class, we leverage the set_extra_driver_options() method to extend \nthe default driver options used by Selenium:\n    def set_extra_driver_options(self, options) -> None:\n        options.add_argument(r\"--profile-directory=Profile 2\")\nThe extract() method implements the core functionality, first checking whether the article \nexists in the database to prevent duplicate entries. \n\n\nData Engineering\n78\nIf the article is new, the method proceeds to navigate to the article’s link and scroll through the \npage to ensure all content is loaded:\n    def extract(self, link: str, **kwargs) -> None:\n        old_model = self.model.find(link=link)\n        if old_model is not None:\n            logger.info(f\"Article already exists in the database: {link}\")\n            return\n        logger.info(f\"Starting scrapping Medium article: {link}\")\n        self.driver.get(link)\n        self.scroll_page()\nAfter fully loading the page, the method uses BeautifulSoup to parse the HTML content and \nextract the article’s title, subtitle, and full text. BeautifulSoup is a popular Python library for \nweb scraping and parsing HTML or XML documents. Thus, we used it to extract all the HTML \nelements we needed from the HTML accessed with Selenium. Finally, we aggregate everything \ninto a dictionary:\n        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n        title = soup.find_all(\"h1\", class_=\"pw-post-title\")\n        subtitle = soup.find_all(\"h2\", class_=\"pw-subtitle-paragraph\")\n        data = {\n            \"Title\": title[0].string if title else None,\n            \"Subtitle\": subtitle[0].string if subtitle else None,\n            \"Content\": soup.get_text(),\n        }\nFinally, the method closes the WebDriver to free up resources. It then creates a new ArticleDocument\ninstance, populates it with the extracted content and user information provided via kwargs, and \nsaves it to the database:\n        self.driver.close()\n        user = kwargs[\"user\"]\n        instance = self.model(\n\n\nChapter 3\n79\n            platform=\"medium\",\n            content=data,\n            link=link,\n            author_id=user.id,\n            author_full_name=user.full_name,\n        )\n        instance.save()\n        logger.info(f\"Successfully scraped and saved article: {link}\")\nWith that, we conclude the MediumCrawler implementation. The LinkedIn crawler follows a \nsimilar pattern to the Medium one, where it uses Selenium to log in and access the feed of a \nuser’s latest posts. Then, it extracts the posts and scrolls through the feed to load the next page \nuntil a limit is hit. You can check the full implementation in our repository at https://github.\ncom/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/\ncrawlers/linkedin.py.\nWith the rise of LLMs, collecting data from the internet has become a critical step in many re-\nal-world AI applications. Hence, more high-level tools have appeared in the Python ecosystem, \nsuch as Scrapy (https://github.com/scrapy/scrapy), which crawls websites and extracts struc-\ntured data from their pages, and Crawl4AI (https://github.com/unclecode/crawl4ai), which \nis highly specialized in crawling data for LLMs and AI applications.\nIn this section, we’ve looked at implementing three types of crawlers: one that leverages the \ngit executable in a subprocess to clone GitHub repositories, one that uses LangChain utilities \nto extract the HTML of a single web page, and one that leverages Selenium for more complex \nscenarios where we have to navigate through the login page, scroll the article to load the entire \nHTML, and extract it into text format. The last step is understanding how the document classes \nwe’ve used across the chapter, such as the ArticleDocument, work.\nThe NoSQL data warehouse documents\nWe had to implement three document classes to structure our data categories. These classes \ndefine the specific attributes we require for a document, such as the content, author, and source \nlink. It is best practice to structure your data in classes instead of dictionaries, as the attributes we \nexpect for each item are more verbose, reducing run errors. For example, when accessing a value \nfrom a Python dictionary, we can never be sure it is present or its type is current. By wrapping \nour data items with classes, we can ensure each attribute is as expected. \n\n\nData Engineering\n80\nBy leveraging Python packages such as Pydantic, we have out-of-the-box type validation, which \nensures consistency in our datasets. Thus, we modeled the data categories as the following doc-\nument classes, which we already used in the code up until point:\n• \nArticleDocument class\n• \nPostDocument class\n• \nRepositoryDocument class\nThese are not simple Python data classes or Pydantic models. They support read and write oper-\nations on top of the MongoDB data warehouse. To inject the read-and-write functionality into \nall the document classes without repeating any code, we used the Object-Document Mapping\n(ODM) software pattern, which is based on the object-relational mapping (ORM) pattern. Thus, \nlet’s first explore ORM, then move to ODM, and, finally, dig into our custom ODM implementation \nand document classes.\nThe ORM and ODM software patterns\nBefore we talk about software patterns, let’s see what ORM is. It’s a technique that lets you query \nand manipulate data from a database using an object-oriented paradigm. Instead of writing SQL \nor API-specific queries, you encapsulate all the complexity under an ORM class that knows how \nto handle all the database operations, most commonly CRUD operations. Thus, working with \nORM removes the need to handle the database operations manually and reduces the need to write \nboilerplate code manually. An ORM interacts with a SQL database, such as PostgreSQL or MySQL.\nMost modern Python applications use ORMs when interacting with the database. Even though \nSQL is still a popular choice in the data world, you rarely see raw SQL queries in Python backend \ncomponents. The most popular Python ORM is SQLAlchemy (https://www.sqlalchemy.org/). \nAlso, with the rise of FastAPI, SQLModel is (https://github.com/fastapi/sqlmodel) a common \nchoice, which is a wrapper over SQLAlchemy that makes the integration easier with FastAPI.\nFor example, using SQLAlchemy, we defined a User ORM with the ID and name fields. The User\nORM is mapped to the users table within the SQL database. Thus, when we create a new user \nand commit it to the database, it is automatically saved to the users table. The same applies to \nall the CRUD operations on top of the User class.\nfrom sqlalchemy import Column, Integer, String, create_engine\nfrom sqlalchemy.orm import declarative_base, sessionmaker\n   Base = declarative_base()\n",
      "page_number": 98
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 106-115)",
      "start_page": 106,
      "end_page": 115,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n81\n# Define a class that maps to the users table.\n   class User(Base):\n    __tablename__ = \"users\"\n    id = Column(Integer, primary_key=True)\n   \nname = Column(String)\nUsing the User ORM, we can quickly insert or query users directly from Python without writing a \nline of SQL. Note that an ORM usually supports all CRUD operations. Here is a code snippet that \nshows how to save an instance of the User ORM to a SQLite database:\nengine = create_engine(\"sqlite:///:memory:\")\nBase.metadata.create_all(engine)\n# Create a session used to interact with the database.\nSession = sessionmaker(bind=engine)\nsession = Session()\n# Add a new user.\nnew_user = User(name=\"Alice\")\nsession.add(new_user)\nsession.commit()\nAlso, this is how we can query a user from the users SQLite table:\nuser = session.query(User).first()\nif user:\nprint(f\"User ID: {user.id}\")\nprint(f\"User name: {user.name}\")\nThe ODM pattern is extremely similar to ORM, but instead of working with SQL databases and \ntables, it works with NoSQL databases (such as MongoDB) and unstructured collections. As we \nwork with NoSQL databases, the data structure is centered on collections, which store JSON-like \ndocuments rather than rows in tables.\nFind the entire script and how to run it in the GitHub repository at code_snippets/03_\norm.py.\n\n\nData Engineering\n82\nTo conclude, ODM simplifies working with document-based NoSQL databases and maps ob-\nject-oriented code to JSON-like documents. We will implement a light ODM module on top of \nMongoDB to fully understand how ODM works.\nImplementing the ODM class\nThis section will explore how to implement an ODM class from scratch. This is an excellent ex-\nercise to learn how ODM works and sharpen our skills in writing modular and reusable Python \nclasses. Hence, we will implement a base ODM class called NoSQLBaseDocument, from which all \nthe other documents will inherit to interact with the MongoDB data warehouse.\nThe code starts by importing essential modules and setting up the database connection. Through \nthe _database variable, we establish a connection to the database specified in the settings, which \nis by default called twin:\nimport uuid\nfrom abc import ABC\nfrom typing import Generic, Type, TypeVar\nfrom loguru import logger\nfrom pydantic import UUID4, BaseModel, Field\nfrom pymongo import errors\nfrom llm_engineering.domain.exceptions import ImproperlyConfigured\nfrom llm_engineering.infrastructure.db.mongo import connection\nfrom llm_engineering.settings import settings\n_database = connection.get_database(settings.DATABASE_NAME)\nNext, we define a type variable T bound to the NoSQLBaseDocument class. The variable leverages \nPython’s generic module, allowing us to generalize the class’s types. For example, when we im-\nplement the ArticleDocument class, which will inherit from the NoSQLBaseDocument class, all the \ninstances where T was used will be replaced with the ArticleDocument type when analyzing the \nsignature of functions (more on Python generics: https://realpython.com/python312-typing). \nThe class can be found in our repository at llm_engineering/domain/base/nosql.\npy.\n\n\nChapter 3\n83\nThe NoSQLBaseDocument class is then declared as an abstract base class inheriting from Pydan-\ntic’s BaseModel, Python’s Generic (which provides the functionality described earlier), and ABC\n(making the class abstract) classes. This class serves as the foundational ODM class:\nT = TypeVar(\"T\", bound=\"NoSQLBaseDocument\")\nclass NoSQLBaseDocument(BaseModel, Generic[T], ABC):\nWithin the NoSQLBaseDocument class, an id field is defined as a UUID4, with a default factory \ngenerating a unique UUID. The class also implements the __eq__ and __hash__ methods to allow \ninstances to be compared and used in hashed collections like sets or as dictionary keys based on \ntheir unique id attribute:\nid: UUID4 = Field(default_factory=uuid.uuid4)\ndef __eq__(self, value: object) -> bool:\n    if not isinstance(value, self.__class__):\n        return False\n    return self.id == value.id\ndef __hash__(self) -> int:\n    return hash(self.id)\nThe class provides methods for converting between MongoDB documents and class instances. The \nfrom_mongo() class method transforms a dictionary retrieved from MongoDB into an instance of \nthe class. The to_mongo() instance method converts the model instance into a dictionary suitable \nfor MongoDB insertion:\n@classmethod\ndef from_mongo(cls: Type[T], data: dict) -> T:\n    if not data:\n        raise ValueError(\"Data is empty.\")\n    id = data.pop(\"_id\")\n    return cls(**dict(data, id=id))\ndef to_mongo(self: T, **kwargs) -> dict:\n\n\nData Engineering\n84\n    exclude_unset = kwargs.pop(\"exclude_unset\", False)\n    by_alias = kwargs.pop(\"by_alias\", True)\n    parsed = self.model_dump(exclude_unset=exclude_unset, by_alias=by_\nalias, **kwargs)\n    if \"_id\" not in parsed and \"id\" in parsed:\n        parsed[\"_id\"] = str(parsed.pop(\"id\"))\n    for key, value in parsed.items():\n        if isinstance(value, uuid.UUID):\n            parsed[key] = str(value)\n    return parsed\nThe save() method allows an instance of the model to be inserted into a MongoDB collection. It \nretrieves the appropriate collection, converts the instance into a MongoDB-compatible document \nleveraging the to_mongo() method described above, and attempts to insert it into the database, \nhandling any write errors that may occur:\ndef save(self: T, **kwargs) -> T | None:\n    collection = _database[self.get_collection_name()]\n    try:\n        collection.insert_one(self.to_mongo(**kwargs))\n        return self\n    except errors.WriteError:\n        logger.exception(\"Failed to insert document.\")\n        return None\nThe get_or_create() class method attempts to find a document in the database matching the \nprovided filter options. If a matching document is found, it is converted into an instance of the class. \nIf not, a new instance is created with the filter options as its initial data and saved to the database:\n@classmethod\ndef get_or_create(cls: Type[T], **filter_options) -> T:\n    collection = _database[cls.get_collection_name()]\n    try:\n\n\nChapter 3\n85\n        instance = collection.find_one(filter_options)\n        if instance:\n            return cls.from_mongo(instance)\n        new_instance = cls(**filter_options)\n        new_instance = new_instance.save()\n        return new_instance\n    except errors.OperationFailure:\n        logger.exception(f\"Failed to retrieve document with filter \noptions: {filter_options}\")\n        raise\nThe bulk_insert() class method allows multiple documents to be inserted into the database \nat once:\n@classmethod\ndef bulk_insert(cls: Type[T], documents: list[T], **kwargs) -> bool:\n    collection = _database[cls.get_collection_name()]\n    try:\n        collection.insert_many([doc.to_mongo(**kwargs) for doc in \ndocuments])\n        return True\n    except (errors.WriteError, errors.BulkWriteError):\nlogger.error(f\"Failed to insert documents of type {cls.__name__}\")\n        return False\nThe find() class method searches for a single document in the database that matches the given \nfilter options:\n@classmethod\ndef find(cls: Type[T], **filter_options) -> T | None:\n    collection = _database[cls.get_collection_name()]\n    try:\n        instance = collection.find_one(filter_options)\n\n\nData Engineering\n86\n        if instance:\n            return cls.from_mongo(instance)\n        return None\n    except errors.OperationFailure:\n        logger.error(\"Failed to retrieve document.\")\n        return None\nSimilarly, the bulk_find() class method retrieves multiple documents matching the filter options. \nIt converts each retrieved MongoDB document into a model instance, collecting them into a list:\n@classmethod\ndef bulk_find(cls: Type[T], **filter_options) -> list[T]:\n    collection = _database[cls.get_collection_name()]\n    try:\n        instances = collection.find(filter_options)\n        return [document for instance in instances if (document := cls.\nfrom_mongo(instance)) is not None]\n    except errors.OperationFailure:\n        logger.error(\"Failed to retrieve document.\")\n        return []\nFinally, the get_collection_name() class method determines the name of the MongoDB collec-\ntion associated with the class. It expects the class to have a nested Settings class with a name at-\ntribute specifying the collection name. If this configuration is missing, an ImproperlyConfigured\nexception will be raised specifying that the subclass should define a nested Settings class:\n@classmethod\ndef get_collection_name(cls: Type[T]) -> str:\n    if not hasattr(cls, \"Settings\") or not hasattr(cls.Settings, \"name\"):\n        raise ImproperlyConfigured(\n            \"Document should define an Settings configuration class with \nthe name of the collection.\"\n        )\n    return cls.Settings.name\n\n\nChapter 3\n87\nWe can configure each subclass using the nested Settings class, such as defining the collection \nname, or anything else specific to that subclass. Within the Python ecosystem, there is an ODM \nimplementation on top of MongoDB, called mongoengine, which you can find on GitHub. It follows \na pattern similar to ours but more comprehensive. We implemented it by ourselves, as it was an \nexcellent exercise to practice writing modular and generic code following best OOP principles, \nwhich are essential for implementing production-level code.\nData categories and user document classes\nThe last piece of the puzzle is to see the implementation of the subclasses that inherit from the \nNoSQLBaseDocument base class. These are the concrete classes that define our data categories. \nYou’ve seen these classes used across the chapter when working with articles, repositories, and \nposts within the crawler classes.\nWe begin by importing the essential Python modules and the ODM base class:\nfrom abc import ABC\nfrom typing import Optional\nfrom pydantic import UUID4, Field\nfrom .base import NoSQLBaseDocument\nfrom .types import DataCategory\nWe define an enum class, where we centralize all our data category types. These variables will act \nas constants in configuring all our ODM classes throughout the book.\nfrom enum import StrEnum\nclass DataCategory(StrEnum):\n    PROMPT = \"prompt\"\n    QUERIES = \"queries\"\n    INSTRUCT_DATASET_SAMPLES = \"instruct_dataset_samples\"\nThe class can be found in the repository at llm_engineering/domain/types.py.\n\n\nData Engineering\n88\n    INSTRUCT_DATASET = \"instruct_dataset\"\n    PREFERENCE_DATASET_SAMPLES = \"preference_dataset_samples\"\n    PREFERENCE_DATASET = \"preference_dataset\"\n    POSTS = \"posts\"\n    ARTICLES = \"articles\"\n     REPOSITORIES = \"repositories\"\nThe Document class is introduced as an abstract base model for other documents on top of the \nNoSQLBaseDocument ODM class. It includes common attributes like content, platform, and author \ndetails, providing a standardized structure for documents that will inherit from it:\nclass Document(NoSQLBaseDocument, ABC):\n    content: dict\n    platform: str\n    author_id: UUID4 = Field(alias=\"author_id\")\n    author_full_name: str = Field(alias=\"author_full_name\")\nFinally, specific document types are defined by extending the Document class. The \nRepositoryDocument, PostDocument, and ArticleDocument classes represent different catego-\nries of data, each with unique fields and settings that specify their respective collection names \nin the database:\nclass RepositoryDocument(Document):\n    name: str\n    link: str\n    class Settings:\n        name = DataCategory.REPOSITORIES\nclass PostDocument(Document):\n    image: Optional[str] = None\n    link: str | None = None\n    class Settings:\n        name = DataCategory.POSTS\n\n\nChapter 3\n89\nclass ArticleDocument(Document):\n    link: str\n    class Settings:\n        name = DataCategory.ARTICLES\nFinally, we define the UserDocument class, which is used to store and query all the users from the \nLLM Twin project:\nclass UserDocument(NoSQLBaseDocument):\n    first_name: str\n    last_name: str\n    class Settings:\n        name = \"users\"\n    @property\n    def full_name(self):\n        return f\"{self.first_name} {self.last_name}\"\nBy implementing the NoSQLBaseDocument ODM class, we had to focus solely on the fields and \nspecific functionality of each document or domain entity. All the CRUD functionality is delegated \nto the parent class. Also, by leveraging Pydantic to define the fields, we have out-of-the-box type \nvalidation. For example, when creating an instance of the ArticleDocument class, if the provided \nlink is None or not a string, it will throw an error signaling that the data is invalid.\nWith that, we’ve finished implementing our data collection pipeline, starting with the ZenML \ncomponents. Then, we looked into the implementation of the crawlers and, finally, wrapped it \nup with the ODM class and data category documents. The last step is to run the data collection \npipeline and ingest raw data into the MongoDB data warehouse.\nGathering raw data into the data warehouse\nZenML orchestrates the data collection pipeline. Thus, leveraging ZenML, the data collection \npipeline can be run manually, scheduled, or triggered by specific events. Here, we will show you \nhow to run it manually, while we will discuss the other scenarios in Chapter 11 when digging \ndeeper into MLOps.\n\n\nData Engineering\n90\nWe configured a different pipeline run for each author. We provided a ZenML configuration file for \nPaul Iusztin’s or Maxime Labonne’s data. To call the data collection pipeline to collect Maxime’s \ndata, for example, you can run the following CLI command:\npoetry poe run-digital-data-etl-maxime\nThat will call the pipeline with the following ZenML YAML configuration file:\nparameters:\n  user_full_name: Maxime Labonne # [First Name(s)] [Last Name]\n  links:\n    # Personal Blog\n    - https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.\nhtml\n    - https://mlabonne.github.io/blog/posts/2024-07-15_The_Rise_of_\nAgentic_Data_Generation.html\n    # Substack\n    - https://maximelabonne.substack.com/p/uncensor-any-llm-with-\nabliteration-d30148b7d43e\n    - https://maximelabonne.substack.com/p/create-mixtures-of-experts-\nwith-mergekit-11b318c99562\n    - https://maximelabonne.substack.com/p/merge-large-language-models-\nwith-mergekit-2118fb392b54\n    … # More Substack links\nIn Figure 3.3 earlier, we saw the pipeline’s run DAG and details in ZenML’s dashboard. Meanwhile, \nFigure 3.5 shows the user output artifact generated by this data collection pipeline. You can in-\nspect the query user_full_name and the retrieved user from the MongoDB database, for which \nwe collected the links in this specific run.\n",
      "page_number": 106
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 116-123)",
      "start_page": 116,
      "end_page": 123,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n91\nFigure 3.5: Example of the user output artifact after running the data collection pipeline using \nMaxime’s configuration file\nAlso, in Figure 3.6, you can observe the crawled_links output artifact, which lists all the domains \nfrom which we collected data, the total number of links crawled for each domain, and the number \nof successfully collected links. \n\n\nData Engineering\n92\nWe want to highlight again the power of these artifacts, as they trace each pipeline’s results and \nmetadata, making it extremely easy to monitor and debug each pipeline run individually.\nFigure 3.6: Example of the crawled_links output artifact after running the data collection \npipeline using Maxime’s configuration file\nNow, we can download the crawled_links artifact anywhere in our code by running the following \ncode, where the ID of the artifact can be found in ZenML and is unique for every artifact version:\nfrom zenml.client import Client\nartifact = Client().get_artifact_version('8349ce09-0693-4e28-8fa2-\n20f82c76ddec')\nloaded_artifact = artifact.load()\n\n\nChapter 3\n93\nFor example, we can easily run the same data collection pipeline but with Paul Iusztin’s YAML \nconfiguration, listed below:\nparameters:\n  user_full_name: Paul Iusztin # [First Name(s)] [Last Name]\n  links:\n    # Medium\n    - https://medium.com/decodingml/an-end-to-end-framework-for-\nproduction-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f\n    - https://medium.com/decodingml/a-real-time-retrieval-system-for-rag-\non-social-media-data-9cc01d50a2a0\n    - https://medium.com/decodingml/sota-python-streaming-pipelines-for-\nfine-tuning-llms-and-rag-in-real-time-82eb07795b87\n    … # More Medium links\n    # Substack\n    - https://decodingml.substack.com/p/real-time-feature-pipelines-\nwith?r=1ttoeh\n    - https://decodingml.substack.com/p/building-ml-systems-the-right-\nway?r=1ttoeh\n    - https://decodingml.substack.com/p/reduce-your-pytorchs-code-\nlatency?r=1ttoeh\n    … # More Substack links\nTo run the pipeline using Paul’s configuration, we call the following poe command:\npoetry poe run-digital-data-etl-paul\nThat, under the hood, calls the following CLI command that references Paul’s config file:\npoetry run python -m tools.run --run-etl --no-cache --etl-config-filename \ndigital_data_etl_paul_iusztin.yaml\nYou can find all the configs in the repository in the configs/ directory. Also, using poe, we con-\nfigured a command that calls the data collection pipeline for all the supported authors:\npoetry poe run-digital-data-etl\nWe can easily query the MongoDB data warehouse using our ODM classes. For example, let’s \nquery all the articles collected for Paul Iusztin:\nfrom llm_engineering.domain.documents import ArticleDocument, UserDocument\n\n\nData Engineering\n94\nuser = UserDocument.get_or_create(first_name=\"Paul\", last_name=\"Iusztin\")\narticles = ArticleDocument.bulk_find(author_id=str(user.id))\nprint(f\"User ID: {user.id}\")\nprint(f\"User name: {user.first_name} {user.last_name}\")\nprint(f\"Number of articles: {len(articles)}\")\nprint(\"First article link:\", articles[0].link)\nThe output of the code from above is:\nUser ID: 900fec95-d621-4315-84c6-52e5229e0b96\nUser name: Paul Iusztin\nNumber of articles: 50\nFirst article link: https://medium.com/decodingml/an-end-to-end-framework-\nfor-production-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f\nWith only two lines of code, we can query and filter our MongoDB data warehouse using any \nODM defined within our project.\nAlso, to ensure that your data collection pipeline works as expected, you can search your MongoDB \ncollections using your IDE’s MongoDB plugin, which you must install separately. For example, \nyou can use this plugin for VSCode: https://www.mongodb.com/products/tools/vs-code. For \nother IDEs, you can use similar plugins or external NoSQL visualization tools. After connecting \nto the MongoDB visualization tool, you can connect to our local database using the following \nURI: mongodb://llm_engineering:llm_engineering@127.0.0.1:27017. For a cloud MongoDB \ncluster, you must change the URI, which we will explore in Chapter 11.\nAnd just like that, you’ve learned how to run the data collection pipeline with different ZenML \nconfigs and how to visualize the output artifacts of each run. We also looked at how to query the \ndata warehouse for a particular data category and author. Thus, we’ve finalized our data engi-\nneering chapter and can move to the conclusion.\nTroubleshooting\nThe raw data stored in the MongoDB database is central to all future steps. Thus, if you haven’t \nsuccessfully run the code from this chapter due to any issues with the crawlers, this section pro-\nvides solutions for fixing potential issues to allow you to move forward.\n\n\nChapter 3\n95\nSelenium issues\nIt is a well-known issue that running Selenium can cause problems due to issues with the \nbrowser driver, such as the ChromeDriver. Thus, if the crawlers that use Selenium, such as the \nMediumCrawler, fail due to problems with your ChromeDriver, you can easily bypass this by \ncommenting out the Medium links added to the data collection YAML configs. To do so, go to \nthe configs/ directory and find all the YAML files that start with digital_data_etl_*, such as \ndigital_data_etl_maxime_labonne.yaml. Open them and comment on all the Medium-related \nURLs, as illustrated in Figure 3.7. You can leave out the Substack or personal blog URLs as these \nuse the CustomArticleCrawler, which is not dependent on Selenium.\nFigure 3.7: Fix Selenium issues when crawling raw data\nImport our backed-up data\nIf nothing works, there is the possibility of populating the MongoDB database with your backed-\nup data saved under the data/data_warehouse_raw_data directory. This will allow you to \nproceed to the fine-tuning and inference sections without running the data collection ETL code. \nTo import all the data within this directory, run:\npoetry poe run-import-data-warehouse-from-json\n\n\nData Engineering\n96\nAfter running the CLI command from above, you will have a one-to-one replica of the dataset \nwe used while developing the code. To ensure the import is completed successfully, you should \nhave 88 articles and 3 users in your MongoDB database.\nSummary\nIn this chapter, we’ve learned how to design and build the data collection pipeline for the LLM \nTwin use case. Instead of relying on static datasets, we collected our custom data to mimic re-\nal-world situations, preparing us for real-world challenges in building AI systems.\nFirst, we examined the architecture of LLM Twin’s data collection pipeline, which functions \nas an ETL process. Next, we started digging into the pipeline implementation. We began by \nunderstanding how we can orchestrate the pipeline using ZenML. Then, we looked into the \ncrawler implementation. We learned how to crawl data in three ways: using CLI commands in \nsubprocesses or using utility functions from LangChain or Selenium to build custom logic that \nprogrammatically manipulates the browser. Finally, we looked into how to build our own ODM \nclass, which we used to define our document class hierarchy, which contains entities such as \narticles, posts, and repositories.\nAt the end of the chapter, we learned how to run ZenML pipelines with different YAML configura-\ntion files and explore the results in the dashboard. We also saw how to interact with the MongoDB \ndata warehouse through the ODM classes.\nIn the next chapter, we will cover the key steps of the RAG feature pipeline, including chunking \nand embedding documents, ingesting these documents into a vector DB, and applying pre-re-\ntrieval optimizations to improve performance. We will also set up the necessary infrastructure \nprogrammatically using Pulumi and conclude by deploying the RAG ingestion pipeline to AWS.\nReferences\n•\nBreuss, M. (2023, July 26). Beautiful Soup: Build a Web Scraper With Python. https://\nrealpython.com/beautiful-soup-web-scraper-python/\n•\nDavid, D. (2024, July 8). Guide to Web Scraping with Selenium in 2024. Bright Data. https://\nbrightdata.com/blog/how-tos/using-selenium-for-web-scraping\n•\nHjelle, G. A. (2023, October 21). Python 3.12 Preview: Static Typing Improvements. https://\nrealpython.com/python312-typing/\n• \nORM Quick Start — SQLAlchemy 2.0 documentation. (n.d.). https://docs.sqlalchemy.\norg/en/20/orm/quickstart.html\n\n\nChapter 3\n97\n•\nRamos, L. P. (2023, August 4). Python and MongoDB: Connecting to NoSQL Databases. \nhttps://realpython.com/introduction-to-mongodb-and-python/\n•\nRefactoring.Guru. (2024, January 1). Builder. https://refactoring.guru/design-\npatterns/builder\n• \nWhat is ETL? A complete guide. (n.d.). Qlik. https://www.qlik.com/us/etl\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n\n\n4\nRAG Feature Pipeline\nRetrieval-augmented generation (RAG) is fundamental in most generative AI applications. RAG’s \ncore responsibility is to inject custom data into the large language model (LLM) to perform a \ngiven action (e.g., summarize, reformulate, and extract the injected data). You often want to use \nthe LLM on data it wasn’t trained on (e.g., private or new data). As fine-tuning an LLM is a highly \ncostly operation, RAG is a compelling strategy that bypasses the need for constant fine-tuning \nto access that new data.\nWe will start this chapter with a theoretical part that focuses on the fundamentals of RAG and \nhow it works. We will then walk you through all the components of a naïve RAG system: chunk-\ning, embedding, and vector DBs. Ultimately, we will present various optimizations used for an \nadvanced RAG system. Then, we will continue exploring LLM Twin’s RAG feature pipeline archi-\ntecture. At this step, we will apply all the theoretical aspects we discussed at the beginning of the \nchapter. Finally, we will go through a practical example by implementing the LLM Twin’s RAG \nfeature pipeline based on the system design described throughout the book.\nThe main sections of this chapter are:\n•\nUnderstanding RAG\n•\nAn overview of advanced RAG\n•\nExploring the LLM Twin’s RAG feature pipeline architecture\n•\nImplementing the LLM Twin’s RAG feature pipeline\nBy the end of this chapter, you will have a clear and comprehensive understanding of what RAG \nis and how it is applied to our LLM Twin use case.\n",
      "page_number": 116
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 124-132)",
      "start_page": 124,
      "end_page": 132,
      "detection_method": "topic_boundary",
      "content": "RAG Feature Pipeline\n100\nUnderstanding RAG\nRAG enhances the accuracy and reliability of generative AI models with information fetched from \nexternal sources. It is a technique complementary to the internal knowledge of the LLMs. Before \ngoing into the details, let’s understand what RAG stands for:\n• \nRetrieval: Search for relevant data\n• \nAugmented: Add the data as context to the prompt\n• \nGeneration: Use the augmented prompt with an LLM for generation\nAny LLM is bound to understand the data it was trained on, sometimes called parameterized \nknowledge. Thus, even if the LLM can perfectly answer what happened in the past, it won’t have \naccess to the newest data or any other external sources on which it wasn’t trained.\nLet’s take the most powerful model from OpenAI as an example, which, in the summer of 2024, is \nGPT-4o. The model is trained on data up to October 2023. Thus, if we ask what happened during \nthe 2020 pandemic, it can be answered perfectly due to its parametrized knowledge. However, \nit will not know the answer if we ask about the 2024 European Football Championship results \ndue to its bounded parametrized knowledge. Another scenario is that it will start confidently \nhallucinating and provide a faulty answer.\nRAG overcomes these two limitations of LLMs. It provides access to external or latest data and \nprevents hallucinations, enhancing generative AI models’ accuracy and reliability.\nWhy use RAG?\nWe briefly explained the importance of using RAG in generative AI applications earlier. Now, we \nwill dig deeper into the “why,” following which we will focus on what a naïve RAG framework \nlooks like.\nFor now, to get an intuition about RAG, you have to know that when using RAG, we inject the \nnecessary information into the prompt to answer the initial user question. After that, we pass \nthe augmented prompt to the LLM for the final answer. Now, the LLM will use the additional \ncontext to answer the user question.\nThere are two fundamental problems that RAG solves:\n• \nHallucinations\n• \nOld or private information\n\n\nChapter 4\n101\nHallucinations\nIf a chatbot without RAG is asked a question about something it wasn’t trained on, there is a high \nchance that it will give you a confident answer about something that isn’t true. Let’s take the 2024 \nEuropean Football Championship as an example. If the model is trained up to October 2023 and \nwe ask it something about the tournament, it will most likely come up with a random answer \nthat is hard to differentiate between reality and truth. Even if the LLM doesn’t hallucinate all the \ntime, it raises concerns about the trustworthiness of its answers. Thus, we must ask ourselves: \n“When can we trust the LLM’s answers?” and “How can we evaluate if the answers are correct?”. \nBy introducing RAG, we enforce the LLM to always answer solely based on the introduced con-\ntext. The LLM will act as the reasoning engine, while the additional information added through \nRAG will act as the single source of truth for the generated answer. By doing so, we can quickly \nevaluate if the LLM’s answer is based on the external data or not.\nOld information\nAny LLM is trained or fine-tuned on a subset of the total world knowledge dataset. This is due \nto three main issues:\n•\nPrivate data: You cannot train your model on data you don’t own or have the right to use.\n•\nNew data: New data is generated every second. Thus, you would have to constantly train \nyour LLM to keep up.\n•\nCosts: Training or fine-tuning an LLM is an extremely costly operation. Hence, it is not \nfeasible to do it on an hourly or daily basis.\nRAG solves these issues, as you no longer have to constantly fine-tune your LLM on new data (or \neven private data). Directly injecting the necessary data to respond to user questions into the \nprompts that are fed to the LLM is enough to generate correct and valuable answers.\nTo conclude, RAG is key for a robust and flexible generative AI system. But how do we inject the \nright data into the prompt based on the user’s questions? We will dig into the technical aspects \nof RAG in the next sections.\nThe vanilla RAG framework\nEvery RAG system is similar at its roots. We will first focus on understanding RAG in its simplest \nform. Later, we will gradually introduce more advanced RAG techniques to improve the system’s \naccuracy. Note that we will use vanilla and naive RAG interchangeably to avoid repetition. \n\n\nRAG Feature Pipeline\n102\nA RAG system is composed of three main modules independent of each other:\n•\nIngestion pipeline: A batch or streaming pipeline used to populate the vector DB\n•\nRetrieval pipeline: A module that queries the vector DB and retrieves relevant entries to \nthe user’s input\n•\nGeneration pipeline: The layer that uses the retrieved data to augment the prompt and \nan LLM to generate answers\nAs these three components are classes or services of their own, we will dig into each separately. \nBut for now, let’s try to answer the question “How are these three modules connected?”. Here is \na very simplistic overview:\n1.\nOn the backend side, the ingestion pipeline runs either on a schedule or constantly to \npopulate the vector DB with external data.\n2.\nOn the client side, the user asks a question.\n3.\nThe question is passed to the retrieval module, which preprocesses the user’s input and \nqueries the vector DB.\n4.\nThe generation pipelines use a prompt template, user input, and retrieved context to \ncreate the prompt.\n5.\nThe prompt is passed to an LLM to generate the answer.\n6.\nThe answer is shown to the user.\n\n\nChapter 4\n103\nFigure 4.1: Vanilla RAG architecture\nYou must implement RAG in your generative AI application when you need access to any type of \nexternal information. For example, when implementing a financial assistant, you most likely need \naccess to the latest news, reports, and prices before providing valuable answers. Or, if you build \na traveling recommender, you must retrieve and parse a list of potential attractions, restaurants, \nand activities. At training time, LLMs don’t have access to your specific data, so you will often \nhave to implement a RAG strategy in your generative AI project. Now, let’s dig into the ingestion, \nretrieval, and generation pipelines.\n\n\nRAG Feature Pipeline\n104\nIngestion pipeline\nThe RAG ingestion pipeline extracts raw documents from various data sources (e.g., data ware-\nhouse, data lake, web pages, etc.). Then, it cleans, chunks (splits into smaller sections), and em-\nbeds the documents. Ultimately, it loads the embedded chunks into a vector DB (or other similar \nvector storage).\nThus, the RAG ingestion pipeline is split into the following:\n• \nThe data extraction module gathers all the necessary data from various sources such as \nDBs, APIs, or web pages. This module is highly dependent on your data. It can be as easy \nas querying your data warehouse or something more complex such as crawling Wikipedia.\n• \nA cleaning layer standardizes and removes unwanted characters from the extracted data. \nFor example, you must remove all invalid characters from your input text, such as non-AS-\nCII and bold and italic characters. Another popular cleaning strategy is to replace URLs \nwith placeholders. However, your cleaning strategy will vary depending on your data \nsource and embedding model.\n• \nThe chunking module splits the cleaned documents into smaller ones. As we want to \npass the document’s content to an embedding model, this is necessary to ensure it doesn’t \nexceed the model’s input maximum size. Also, chunking is required to separate specific \nregions that are semantically related. For example, when chunking a book’s chapter, the \nmost optimal way is to group similar paragraphs into the same section or chunk. By doing \nso, at the retrieval time, you will add only the essential data to the prompt.\n• \nThe embedding component uses an embedding model to take the chunk’s content (text, \nimages, audio, etc.) and project it into a dense vector packed with semantic value—more \non embeddings in the What are embeddings? section below.\n• \nThe loading module takes the embedded chunks along with a metadata document. The \nmetadata will contain essential information such as the embedded content, the URL to \nthe source of the chunk, and when the content was published on the web. The embed-\nding is used as an index to query similar chunks, while the metadata is used to access the \ninformation added to augment the prompt.\nAt this point, we have a RAG ingestion pipeline that takes raw documents as input, processes them, \nand populates a vector DB. The next step is to retrieve relevant data from the vector store correctly.\n\n\nChapter 4\n105\nRetrieval pipeline\nThe retrieval components take the user’s input (text, image, audio, etc.), embed it, and query the \nvector DB for similar vectors to the user’s input.\nThe primary function of the retrieval step is to project the user’s input into the same vector space \nas the embeddings used as an index in the vector DB. This allows us to find the top K’s most sim-\nilar entries by comparing the embeddings from the vector storage with the user’s input vector. \nThese entries then serve as content to augment the prompt that is passed to the LLM to generate \nthe answer.\nYou must use a distance metric to compare two vectors, such as the Euclidean or Manhattan \ndistance. But the most popular one is the cosine distance, which is equal to 1 minus the cosine of \nthe angle between two vectors, as follows:\n𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶\n𝐴𝐴𝐴\n \nIt ranges from -1 to 1, with a value of -1 when vectors A and B are in opposite directions, 0 if they \nare orthogonal, and 1 if they point in the same direction.\nMost of the time, the cosine distance works well in non-linear complex vector spaces. However, \nit is essential to notice that choosing the proper distance between two vectors depends on your \ndata and the embedding model you use.\nOne critical factor to highlight is that the user’s input and embeddings must be in the same vec-\ntor space. Otherwise, you cannot compute the distance between them. To do so, it is essential to \npreprocess the user input in the same way you processed the raw documents in the RAG ingestion \npipeline. This means you must clean, chunk (if necessary), and embed the user’s input using the \nsame functions, models, and hyperparameters. This is similar to how you have to preprocess the \ndata into features in the same way between training and inference; otherwise, the inference will \nyield inaccurate results—a phenomenon also known as the training-serving skew.\nGeneration pipeline\nThe last step of the RAG system is to take the user’s input, retrieve data, pass it to an LLM, and \ngenerate a valuable answer.\n\n\nRAG Feature Pipeline\n106\nThe final prompt results from a system and prompt template populated with the user’s query and \nretrieved context. You might have a single prompt template or multiple prompt templates, depend-\ning on your application. Usually, all the prompt engineering is done at the prompt template level.\nBelow, you can see a dummy example of what a generic system and prompt template look like \nand how they are used together with the retrieval logic and the LLM to generate the final answer:\nsystem_template = \"\"\"\nYou are a helpful assistant who answers all the user's questions politely.\n\"\"\"\nprompt_template = \"\"\"\nAnswer the user's question using only the provided context. If you cannot \nanswer using the context, respond with \"I don't know.\"\nContext: {context}\nUser question: {user_question}\n\"\"\"\nuser_question = \"<your_question>\"\nretrieved_context = retrieve(user_question)\nprompt = f\"{system_template}\\n\"\nprompt += prompt_template.format(context=retrieved_context, user_\nquestion=user_question)\nanswer  = llm(prompt)\nAs the prompt templates evolve, each change should be tracked and versioned using machine \nlearning operations (MLOps) best practices. Thus, during training or inference time, you always \nknow that a given answer was generated by a specific version of the LLM and prompt template(s). \nYou can do this through Git, store the prompt templates in a DB, or use specific prompt manage-\nment tools such as LangFuse.\nAs we’ve seen in the retrieval pipeline, some critical aspects that directly impact the accuracy of \nyour RAG system are the embeddings of the external data, usually stored in vector DBs, the em-\nbedding of the user’s query, and how we can find similarities between the two using functions \nsuch as the cosine distance. To better understand this part of the RAG algorithm, let’s zoom in \non what embeddings are and how they are computed.\n\n\nChapter 4\n107\nWhat are embeddings?\nImagine you’re trying to teach a computer to understand the world. Embeddings are like a par-\nticular translator that turns these things into a numerical code. This code isn’t random, though, \nbecause similar words or items end up with codes that are close to each other. It’s like a map \nwhere words with similar meanings are clustered together.\nWith that in mind, a more theoretical definition is that embeddings are dense numerical repre-\nsentations of objects encoded as vectors in a continuous vector space, such as words, images, or \nitems in a recommendation system. This transformation helps capture the semantic meaning \nand relationships between the objects. For instance, in natural language processing (NLP), em-\nbeddings translate words into vectors where semantically similar words are positioned closely \ntogether in the vector space.\nFigure 4.2: What are embeddings?\nA popular method is visualizing the embeddings to understand and evaluate their geometrical \nrelationship. As the embeddings often have more than 2 or 3 dimensions, usually between 64 \nand 2048, you must project them again to 2D or 3D.\n\n\nRAG Feature Pipeline\n108\nFor example, you can use UMAP (https://umap-learn.readthedocs.io/en/latest/index.\nhtml), a dimensionality reduction method well known for keeping the geometrical properties \nbetween the points when projecting the embeddings to 2D or 3D. Another popular algorithm for \ndimensionality reduction when visualizing vectors is t-SNE (https://scikit-learn.org/stable/\nmodules/generated/sklearn.manifold.TSNE.html). However, compared to UMAP, it is more \nstochastic and doesn’t preserve the topological relationships between the points.\nFigure 4.3: Visualize embeddings using UMAP (Source: UMAP’s documentation)\n A dimensionality reduction algorithm, such as PCA, UMAP, and t-SNE, is a mathe-\nmatical technique used to reduce the number of input variables or features in a data-\nset while preserving the data’s essential patterns, structure, and relationships. The \ngoal is to transform high-dimensional data into a lower-dimensional form, making \nit easier to visualize, interpret, and process while minimizing the loss of important \ninformation. These methods help to address the “curse of dimensionality,” improve \ncomputational efficiency, and often enhance the performance of ML algorithms.\n",
      "page_number": 124
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 133-140)",
      "start_page": 133,
      "end_page": 140,
      "detection_method": "topic_boundary",
      "content": "Chapter 4\n109\nWhy embeddings are so powerful\nFirstly, ML models work only with numerical values. This is not a problem when working with \ntabular data, as the data is often in numerical form or can easily be processed into numbers. \nEmbeddings come in handy when we want to feed words, images, or audio data into models.\nFor instance, when working with transformer models, you tokenize all your text input, where \neach token has an embedding associated with it. The beauty of this process lies in its simplicity; \nthe input to the transformer is a sequence of embeddings, which can be easily and confidently \ninterpreted by the dense layers of the neural network.\nBased on this example, you can use embeddings to encode any categorical variable and feed it to \nan ML model. But why not use other simple methods, such as one-hot encoding? When working \nwith categorical variables with high cardinality, such as language vocabularies, you will suffer \nfrom the curse of dimensionality when using other classical methods. For example, if your vocab-\nulary has 10,000 tokens, then only one token will have a length of 10,000 after applying one-hot \nencoding. If the input sequence has N tokens, that will become N * 10,000 input parameters. If \nN >= 100, often, when inputting text, the input is too large to be usable. Another issue with other \nclassical methods that don’t suffer from the curse of dimensionality, such as hashing, is that you \nlose the semantic relationships between the vectors.\n\n\nRAG Feature Pipeline\n110\nSecondly, embedding your input reduces the size of its dimension and condenses all of its se-\nmantic meaning into a dense vector. This is an extremely popular technique when working with \nimages, where a CNN encoder module maps the high-dimensional meaning into an embedding, \nwhich is later processed by a CNN decoder that performs the classification or regression steps.\nThe following image shows a typical CNN layout. Imagine tiny squares within each layer. Those \nare the “receptive fields.” Each square feeds information to a single neuron in the previous layer. \nAs you move through the network, two key things are happening:\n•\nShrinking the picture: Special “subsampling” operations make the layers smaller, fo-\ncusing on essential details.\n•\nLearning features: “Convolution” operations, on the other hand, actually increase the \nlayer size as the network learns more complex features from the image.\nOne-hot encoding is a technique that converts categorical variables into a binary \nmatrix representation. Each category is represented as a unique binary vector. For \neach categorical variable, a binary vector is created with a length equal to the number \nof unique categories, where all values are zero except for the index corresponding to \nthe specific category, which is set to one. The method preserves all information about \nthe categories. It is simple and interpretable. However, a significant disadvantage is \nthat it can lead to a high-dimensional feature space if the categorical variable has \nmany unique values, making the method impractical.\nFeature hashing, also known as hashing encoding or the “hash trick,” is a technique \nused to convert categorical variables into numerical features by applying a hash \nfunction to the category values. Compared to one-hot encoding, the method is not \nbound to the number of unique categories, but it reduces the dimensionality of the \nfeature space by mapping categories into a fixed number of bins or buckets. Thus, it \nreduces the dimensionality of the feature space, which is particularly useful when \ndealing with high-cardinality categorical variables. This makes it efficient in terms of \nmemory usage and computational time. However, there is a risk of collisions, where \ndifferent categories might map to the same bin, leading to a loss of information. The \nmapping makes the method uninterpretable. Also, it is difficult to understand the \nrelationship between the original categories and the hashed features.\nEmbeddings help us encode categorical variables while controlling the output vec-\ntor’s dimension. They also use ingenious ways to condense information into a lower \ndimension space than naive hashing tricks.\n\n\nChapter 4\n111\nFinally, a fully connected layer at the end takes all this processed information and transforms it \ninto the final vector embedding, a numerical image representation.\nFigure 4.4: Creating embeddings from an image using a CNN (Image source)\nHow are embeddings created?\nEmbeddings are created by deep learning models that understand the context and semantics of \nyour input and project it into a continuous vector space.\nVarious deep learning models can be used to create embeddings, varying by the data input type. \nThus, it is fundamental to understand your data and what you need from it before picking an \nembedding model.\nFor example, when working with text data, one of the early methods used to create embeddings \nfor your vocabulary is Word2Vec and GloVe. These are still popular methods used today for sim-\npler applications.\nAnother popular method is to use encoder-only transformers, such as BERT, or other methods \nfrom its family, such as RoBERTa. These models leverage the encoder of the transformer architec-\nture to smartly project your input into a dense vector space that can later be used as embeddings.\nTo quickly compute the embeddings in Python, you can conveniently leverage the Sentence \nTransformers Python package (also available in Hugging Face’s transformer package). This tool \nprovides a user-friendly interface, making the embedding process straightforward and efficient.\nThe preceding image is sourced from Wikimedia Commons (https://commons.\nwikimedia.org/wiki/File:Typical_cnn.png) and licensed under the Creative \nCommons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0: https://\ncreativecommons.org/licenses/by-sa/4.0/deed.en).\n\n\nRAG Feature Pipeline\n112\nIn the code snippet below, you can see how we loaded a model from SentenceTransformer, comput-\ned the embeddings for three sentences, and, ultimately, computed the cosine similarity between \nthem. The similarity between one sentence and itself is always 1. Also, the similarity between \nthe first and second sentences is approximately 0, as the sentences have nothing in common. In \ncontrast, the value between the first and third one is higher as there is some overlapping context:\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\nsentences = [\n\"The dog sits outside waiting for a treat.\",\n\"I am going swimming.\",\n\"The dog is swimming.\"\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# Output: [3, 384]\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# Output:\n# tensor([[ 1.0000, -0.0389, 0.2692],\n# [-0.0389, 1.0000, 0.3837],\n# [ 0.2692, 0.3837, 1.0000]])\n#\n# similarities[0, 0] = The similarity between the first sentence and \nitself.\n# similarities[0, 1] = The similarity between the first and second \nsentence.\n# similarities[2, 1] = The similarity between the third and second \nsentence.\nThe source code for the preceding snippet can be found at https://github.com/PacktPublishing/\nLLM-Engineering/blob/main/code_snippets/08_text_embeddings.py.\n\n\nChapter 4\n113\nThe best-performing embedding model can change with time and your specific use case. You can \nfind particular models on the Massive Text Embedding Benchmark (MTEB) on Hugging Face. \nDepending on your needs, you can consider the best-performing model, the one with the best \naccuracy, or the one with the smallest memory footprint. This decision is solely based on your \nrequirements (e.g., accuracy and hardware). However, Hugging Face and SentenceTransformer \nmake switching between different models straightforward. Thus, you can always experiment \nwith various options.\nWhen working with images, you can embed them using convolutional neural networks (CNNs). \nPopular CNN networks are based on the ResNet architecture. However, we can’t directly use im-\nage embedding techniques for audio recordings. Instead, we can create a visual representation \nof the audio, such as a spectrogram, and then apply image embedding models to those visuals. \nThis allows us to capture the essence of images and sounds in a way computers can understand.\nBy leveraging models like CLIP, you can practically embed a piece of text and an image in the \nsame vector space. This allows you to find similar images using a sentence as input, or the other \nway around, demonstrating the practicality of CLIP.\nIn the following code snippet, we use CLIP to encode a crazy cat image and three sentences. \nUltimately, we use cosine similarity to compute the resemblance between the picture and the \nsentences:\nfrom io import BytesIO\nimport requests\nfrom PIL import Image\nfrom sentence_transformers import SentenceTransformer\nresponse = requests.get(\n\"https://github.com/PacktPublishing/LLM-Engineering/blob/main/images/\ncrazy_cat.jpg?raw=true\"\n)\nThe examples in the embeddings section can be run within the virtual environment \nused across the book, as it contains all the required dependencies.\n\n\nRAG Feature Pipeline\n114\nimage = Image.open(BytesIO(response.content))\nmodel = SentenceTransformer(\"clip-ViT-B-32\")\nimg_emb = model.encode(image)\ntext_emb = model.encode(\n[\"A crazy cat smiling.\",\n\"A white and brown cat with a yellow bandana.\",\n\"A man eating in the garden.\"]\n)\nprint(text_emb.shape) # noqa\n# Output: (3, 512)\nsimilarity_scores = model.similarity(img_emb, text_emb)\nprint(similarity_scores) # noqa\n# Output: tensor([[0.3068, 0.3300, 0.1719]])\nThe source code can be found at https://github.com/PacktPublishing/LLM-Engineering/\nblob/main/code_snippets/08_text_image_embeddings.py.\nHere, we provided a small introduction to how embeddings can be computed. The realm of specific \nimplementations is vast, but what is important to know is that embeddings can be computed for \nmost digital data categories, such as words, sentences, documents, images, videos, and graphs.\nIt’s crucial to grasp that you must use specialized models when you need to compute the distance \nbetween two different data categories, such as the distance between the vector of a sentence and \nof an image. These models are designed to project both data types into the same vector space, \nsuch as CLIP, ensuring accurate distance computation.\nApplications of embeddings\nDue to the generative AI revolution, which uses RAG, embeddings have become extremely popu-\nlar in information retrieval tasks, such as semantic search for text, code, images, and audio, and \nlong-term memory of agents. But before generative AI, embeddings were already heavily used in:\n•\nRepresenting categorical variables (e.g., vocabulary tokens) that are fed to an ML model\n•\nRecommender systems by encoding the users and items and finding their relationship\n•\nClustering and outlier detection\n•\nData visualization by using algorithms such as UMAP\n\n\nChapter 4\n115\n•\nClassification by using the embeddings as features\n•\nZero-shot classification by comparing the embedding of each class and picking the most \nsimilar one\nThe last step to fully understanding how RAG works is to examine vector DBs and how they \nleverage embeddings to retrieve data.\nMore on vector DBs\nVector DBs are specialized DBs designed to efficiently store, index, and retrieve vector embed-\ndings. Traditional scalar-based DBs struggle with the complexity of vector data, making vector \nDBs crucial for tasks like real-time semantic search.\nWhile standalone vector indices like FAISS are effective for similarity search, they lack vector DBs’ \ncomprehensive data management capabilities. Vector DBs support CRUD operations, metadata \nfiltering, scalability, real-time updates, backups, ecosystem integration, and robust data security, \nmaking them more suited for production environments than standalone indices.\nHow does a vector DB work?\nThink of how you usually search a DB. You type in something specific, and the system spits out \nthe exact match. That’s how traditional DBs work. Vector DBs are different. Instead of perfect \nmatches, we look for the closest neighbors of the query vector. Under the hood, a vector DB uses \napproximate nearest neighbor (ANN) algorithms to find these close neighbors.\nWhile ANN algorithms don’t return the top matches for a given search, standard nearest neigh-\nbor algorithms are too slow to work in practice. Also, it is shown empirically that using only ap-\nproximations of the top matches for a given input query works well enough. Thus, the trade-off \nbetween accuracy and latency ultimately favors ANN algorithms.\nThis is a typical workflow of a vector DB:\n1. \nIndexing vectors: Vectors are indexed using data structures optimized for high-dimen-\nsional data. Common indexing techniques include hierarchical navigable small world\n(HNSW), random projection, product quantization (PQ), and locality-sensitive hashing\n(LSH).\n2.\nQuerying for similarity: During a search, the DB queries the indexed vectors to find those \nmost similar to the input vector. This process involves comparing vectors based on sim-\nilarity measures such as cosine similarity, Euclidean distance, or dot product. Each has \nunique advantages and is suitable for different use cases.\n\n\nRAG Feature Pipeline\n116\n3.\nPost-processing results: After identifying potential matches, the results undergo post-pro-\ncessing to refine accuracy. This step ensures that the most relevant vectors are returned \nto the user.\nVector DBs can filter results based on metadata before or after the vector search. Both approaches \nhave trade-offs in terms of performance and accuracy. The query also depends on the metadata \n(along with the vector index), so it contains a metadata index user for filtering operations.\nAlgorithms for creating the vector index\nVector DBs use various algorithms to create the vector index and manage searching data efficiently:\n•\nRandom projection: Random projection reduces the dimensionality of vectors by project-\ning them into a lower-dimensional space using a random matrix. This technique preserves \nthe relative distances between vectors, facilitating faster searches.\n•\nPQ: PQ compresses vectors by dividing them into smaller sub-vectors and then quantizing \nthese sub-vectors into representative codes. This reduces memory usage and speeds up \nsimilarity searches.\n•\nLSH: LSH maps similar vectors into buckets. This method enables fast approximate near-\nest neighbor searches by focusing on a subset of the data, reducing the computational \ncomplexity.\n•\nHNSW: HNSW constructs a multi-layer graph where each node represents a set of vectors. \nSimilar nodes are connected, allowing the algorithm to navigate the graph and find the \nnearest neighbors efficiently.\nThese algorithms enable vector DBs to efficiently handle complex and large-scale data, making \nthem a perfect fit for a variety of AI and ML applications.\nDB operations\nVector DBs also share common characteristics with standard DBs to ensure high performance, \nfault tolerance, and ease of management in production environments. Key operations include:\n•\nSharding and replication: Data is partitioned (sharded) across multiple nodes to ensure \nscalability and high availability. Data replication across nodes helps maintain data integ-\nrity and availability in case of node failures.\n",
      "page_number": 133
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 141-148)",
      "start_page": 141,
      "end_page": 148,
      "detection_method": "topic_boundary",
      "content": "Chapter 4\n117\n•\nMonitoring: Continuous monitoring of DB performance, including query latency and re-\nsource usage (RAM, CPU, disk), helps maintain optimal operations and identify potential \nissues before they impact the system.\n• \nAccess control: Implementing robust access control mechanisms ensures that only au-\nthorized users can access and modify data. This includes role-based access controls and \nother security protocols to protect sensitive information.\n• \nBackups: Regular DB backups are critical for disaster recovery. Automated backup pro-\ncesses ensure that data can be restored to a previous state in case of corruption or loss.\nAn overview of advanced RAG\nThe vanilla RAG framework we just presented doesn’t address many fundamental aspects that \nimpact the quality of the retrieval and answer generation, such as:\n• \nAre the retrieved documents relevant to the user’s question?\n•\nIs the retrieved context enough to answer the user’s question?\n•\nIs there any redundant information that only adds noise to the augmented prompt?\n•\nDoes the latency of the retrieval step match our requirements?\n• \nWhat do we do if we can’t generate a valid answer using the retrieved information?\nFrom the questions above, we can draw two conclusions. The first one is that we need a robust \nevaluation module for our RAG system that can quantify and measure the quality of the retrieved \ndata and generate answers relative to the user’s question. We will discuss this topic in more detail \nin Chapter 9. The second conclusion is that we must improve our RAG framework to address the \nretrieval limitations directly in the algorithm. These improvements are known as advanced RAG.\nThe vanilla RAG design can be optimized at three different stages:\n•\nPre-retrieval: This stage focuses on how to structure and preprocess your data for data \nindexing optimizations as well as query optimizations.\n•\nRetrieval: This stage revolves around improving the embedding models and metadata \nfiltering to improve the vector search step.\n\n\nRAG Feature Pipeline\n118\n• \nPost-retrieval: This stage mainly targets different ways to filter out noise from the retrieved \ndocuments and compress the prompt before feeding it to an LLM for answer generation.\nFigure 4.5: The three stages of advanced RAG\nThis section is not meant to be an exhaustive list of all the advanced RAG methods available. The \ngoal is to build an intuition about what can be optimized. We will use only examples based on \ntext data, but the principles of advanced RAG remain the same regardless of the data category. \nNow, let’s zoom in on all three components.\n\n\nChapter 4\n119\nPre-retrieval\nThe pre-retrieval steps are performed in two different ways:\n•\nData indexing: It is part of the RAG ingestion pipeline. It is mainly implemented within \nthe cleaning or chunking modules to preprocess the data for better indexing.\n•\nQuery optimization: The algorithm is performed directly on the user’s query before em-\nbedding it and retrieving the chunks from the vector DB.\nAs we index our data using embeddings that semantically represent the content of a chunked \ndocument, most of the data indexing techniques focus on better preprocessing and structuring \nthe data to improve retrieval efficiency, such as:\n•\nSliding window: The sliding window technique introduces overlap between text chunks, \nensuring that important context near chunk boundaries is retained, which enhances re-\ntrieval accuracy. This is particularly beneficial in domains like legal documents, scientific \npapers, customer support logs, and medical records, where critical information often spans \nmultiple sections. The embedding is computed on the chunk along with the overlapping \nportion. Hence, the sliding window improves the system’s ability to retrieve relevant and \ncoherent information by maintaining context across boundaries.\n•\nEnhancing data granularity: This involves data cleaning techniques like removing irrel-\nevant details, verifying factual accuracy, and updating outdated information. A clean and \naccurate dataset allows for sharper retrieval.\n•\nMetadata: Adding metadata tags like dates, URLs, external IDs, or chapter markers helps \nfilter results efficiently during retrieval.\n•\nOptimizing index structures: It is based on different data index methods, such as various \nchunk sizes and multi-indexing strategies.\n•\nSmall-to-big: The algorithm decouples the chunks used for retrieval and the context used \nin the prompt for the final answer generation. The algorithm uses a small sequence of \ntext to compute the embedding while preserving the sequence itself and a wider window \naround it in the metadata. Thus, using smaller chunks enhances the retrieval’s accuracy, \nwhile the larger context adds more contextual information to the LLM. \n\n\nRAG Feature Pipeline\n120\nThe intuition behind this is that if we use the whole text for computing the embedding, we \nmight introduce too much noise, or the text could contain multiple topics, which results \nin a poor overall semantic representation of the embedding.\nFigure 4.6: Query routing\nOn the query optimization side, we can leverage techniques such as query routing, query rewriting, \nand query expansion to refine the retrieved information for the LLM further:\n•\nQuery routing: Based on the user’s input, we might have to interact with different cate-\ngories of data and query each category differently. Query rooting is used to decide what \naction to take based on the user’s input, similar to if/else statements. Still, the decisions \nare made solely using natural language instead of logical statements. \n\n\nChapter 4\n121\nAs illustrated in Figure 4.6, let’s assume that, based on the user’s input, to do RAG, we \ncan retrieve additional context from a vector DB using vector search queries, a standard \nSQL DB by translating the user query to an SQL command, or the internet by leveraging \nREST API calls. The query router can also detect whether a context is required, helping us \navoid making redundant calls to external data storage. Also, a query router can be used to \npick the best prompt template for a given input. For example, in the LLM Twin use case, \ndepending on whether the user wants an article paragraph, a post, or a code snippet, you \nneed different prompt templates to optimize the creation process. The routing usually \nuses an LLM to decide what route to take or embeddings by picking the path with the \nmost similar vectors. To summarize, query routing is identical to an if/else statement but \nmuch more versatile as it works directly with natural language.\n•\nQuery rewriting: Sometimes, the user’s initial query might not perfectly align with the \nway your data is structured. Query rewriting tackles this by reformulating the question \nto match the indexed information better. This can involve techniques like:\n• \nParaphrasing: Rephrasing the user’s query while preserving its meaning (e.g., \n“What are the causes of climate change?” could be rewritten as “Factors contrib-\nuting to global warming”).\n•\nSynonym substitution: Replacing less common words with synonyms to broaden \nthe search scope (e.g., “ joyful” could be rewritten as “happy”).\n•\nSub-queries: For longer queries, we can break them down into multiple shorter \nand more focused sub-queries. This can help the retrieval stage identify relevant \ndocuments more precisely.\n•\nHypothetical document embeddings (HyDE): This technique involves having an LLM \ncreate a hypothetical response to the query. Then, both the original query and the LLM’s \nresponse are fed into the retrieval stage.\n•\nQuery expansion: This approach aims to enrich the user’s question by adding additional \nterms or concepts, resulting in different perspectives of the same initial question. For \nexample, when searching for “disease,” you can leverage synonyms and related terms \nassociated with the original query words and also include “illnesses” or “ailments.”\n•\nSelf-query: The core idea is to map unstructured queries into structured ones. An LLM \nidentifies key entities, events, and relationships within the input text. These identities are \nused as filtering parameters to reduce the vector search space (e.g., identify cities within \nthe query, for example, “Paris,” and add it to your filter to reduce your vector search space).\n\n\nRAG Feature Pipeline\n122\nBoth data indexing and query optimization pre-retrieval optimization techniques depend highly \non your data type, structure, and source. Thus, as with any data processing pipeline, no method \nalways works, as every use case has its own particularities and gotchas. Optimizing your pre-re-\ntrieval RAG layer is experimental. Thus, what is essential is to try multiple methods (such as the \nones enumerated in this section), reiterate, and observe what works best.\nRetrieval\nThe retrieval step can be optimized in two fundamental ways:\n•\nImproving the embedding models used in the RAG ingestion pipeline to encode the \nchunked documents and, at inference time, transform the user’s input.\n•\nLeveraging the DB’s filter and search features. This step will be used solely at inference \ntime when you have to retrieve the most similar chunks based on user input.\nBoth strategies are aligned with our ultimate goal: to enhance the vector search step by leveraging \nthe semantic similarity between the query and the indexed data.\nWhen improving the embedding models, you usually have to fine-tune the pre-trained embedding \nmodels to tailor them to specific jargon and nuances of your domain, especially for areas with \nevolving terminology or rare terms.\nInstead of fine-tuning the embedding model, you can leverage instructor models (https://\nhuggingface.co/hkunlp/instructor-xl) to guide the embedding generation process with an \ninstruction/prompt aimed at your domain. Tailoring your embedding network to your data us-\ning such a model can be a good option, as fine-tuning a model consumes more computing and \nhuman resources.\nIn the code snippet below, you can see an example of an Instructor model that embeds article \ntitles about AI:\nfrom InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR(\"hkunlp/instructor-base\")\nsentence = \"RAG Fundamentals First\"\ninstruction = \"Represent the title of an article about AI:\"\n\n\nChapter 4\n123\nembeddings = model.encode([[instruction, sentence]])\nprint(embeddings.shape) # noqa\n# Output: (1, 768)\nThe source code can be found at https://github.com/PacktPublishing/LLM-Engineering/\nblob/main/code_snippets/08_instructor_embeddings.py.\npython3 -m venv instructor_venv && source instructor_venv/bin/activate\npip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1\nOn the other side of the spectrum, here is how you can improve your retrieval by leveraging classic \nfilter and search DB features:\n•\nHybrid search: This is a vector and keyword-based search blend. Keyword-based search \nexcels at identifying documents containing specific keywords. When your task demands \npinpoint accuracy and the retrieved information must include exact keyword matches, \nhybrid search shines. Vector search, while powerful, can sometimes struggle with finding \nexact matches, but it excels at finding more general semantic similarities. You leverage \nboth keyword matching and semantic similarities by combining the two methods. You \nhave a parameter, usually called alpha, that controls the weight between the two methods. \nThe algorithm has two independent searches, which are later normalized and unified.\n•\nFiltered vector search: This type of search leverages the metadata index to filter for specific \nkeywords within the metadata. It differs from a hybrid search in that you retrieve the data \nonce using only the vector index and perform the filtering step before or after the vector \nsearch to reduce your search space.\nTo run the instructor code, you have to create a different virtual environment and \nactivate it:\nAnd install the required Python dependencies:\n\n\nRAG Feature Pipeline\n124\nIn practice, on the retrieval side, you usually start with filtered vector search or hybrid search, as \nthey are fairly quick to implement. This approach gives you the flexibility to adjust your strategy \nbased on performance. If the results are not as expected, you can always fine-tune your embed-\nding model.\nPost-retrieval\nThe post-retrieval optimizations are solely performed on the retrieved data to ensure that the \nLLM’s performance is not compromised by issues such as limited context windows or noisy data. \nThis is because the retrieved context can sometimes be too large or contain irrelevant information, \nboth of which can distract the LLM.\nTwo popular methods performed at the post-retrieval step are:\n•\nPrompt compression: Eliminate unnecessary details while keeping the essence of the data.\n•\nRe-ranking: Use a cross-encoder ML model to give a matching score between the user’s \ninput and every retrieved chunk. The retrieved items are sorted based on this score. Only \nthe top N results are kept as the most relevant. As you can see in Figure 4.7, this works \nbecause the re-ranking model can find more complex relationships between the user input \nand some content than a simple similarity search. However, we can’t apply this model at \nthe initial retrieval step because it is costly. That is why a popular strategy is to retrieve \nthe data using a similarity distance between the embeddings and refine the retrieved \ninformation using a re-raking model, as illustrated in Figure 4.8.\n",
      "page_number": 141
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 149-163)",
      "start_page": 149,
      "end_page": 163,
      "detection_method": "topic_boundary",
      "content": "Chapter 4\n125\nFigure 4.7: Bi-encoder (the standard embedding model) versus cross-encoder\nThe abovementioned techniques are far from an exhaustive list of all potential solutions. We \nused them as examples to get an intuition on what you can (and should) optimize at each step \nin your RAG workflow. The truth is that these techniques can vary tremendously by the type of \ndata you work with. \n\n\nRAG Feature Pipeline\n126\nFor example, if you work with multi-modal data such as text and images, most of the techniques \nfrom earlier won’t work as they are designed for text only.\nFigure 4.8: The re-ranking algorithm\nTo summarize, the primary goal of these optimizations is to enhance the RAG algorithm at three key \nstages: pre-retrieval, retrieval, and post-retrieval. This involves preprocessing data for improved \nvector indexing, adjusting user queries for more accurate searches, enhancing the embedding \nmodel, utilizing classic filtering DB operations, and removing noisy data. By keeping these goals \nin mind, you can effectively optimize your RAG workflow for data processing and retrieval\n\n\nChapter 4\n127\nExploring the LLM Twin’s RAG feature pipeline \narchitecture\nNow that you have a strong intuition and understanding of RAG and its workings, we will con-\ntinue exploring our particular LLM Twin use case. The goal is to provide a hands-on end-to-end \nexample to solidify the theory presented in this chapter.\nAny RAG system is split into two independent components:\n•\nThe ingestion pipeline takes in raw data, cleans, chunks, embeds, and loads it into a \nvector DB.\n•\nThe inference pipeline queries the vector DB for relevant context and ultimately generates \nan answer by levering an LLM.\nIn this chapter, we will focus on implementing the RAG ingestion pipeline, and in Chapter 9, we \nwill continue developing the inference pipeline.\nWith that in mind, let’s have a quick refresher on the problem we are trying to solve and where \nwe get our raw data. Remember that we are building an end-to-end ML system. Thus, all the \ncomponents talk to each other through an interface (or a contract), and each pipeline has a sin-\ngle responsibility. In our case, we ingest raw documents, preprocess them, and load them into \na vector DB.\nThe problem we are solving\nAs presented in the previous chapter, this book aims to show you how to build a production-ready \nLLM Twin backed by an end-to-end ML system. In this chapter specifically, we want to design a \nRAG feature pipeline that takes raw social media data (e.g., articles, code repositories, and posts) \nfrom our MongoDB data warehouse. The text of the raw documents will be cleaned, chunked, \nembedded, and ultimately loaded to a feature store. As discussed in Chapter 1, we will implement \na logical feature store using ZenML artifacts and a Qdrant vector DB.\nAs we want to build a fully automated feature pipeline, we want to sync the data warehouse and \nlogical feature store. Remember that, at inference time, the context used to generate the answer \nis retrieved from the vector DB. Thus, the speed of synchronization between the data warehouse \nand the feature store will directly impact the accuracy of our RAG algorithm.\nAnother key consideration is how to automate the feature pipeline and integrate it with the rest \nof our ML system. Our goal is to minimize any desynchronization between the two data storages, \nas this could potentially compromise the integrity of our system.\n\n\nRAG Feature Pipeline\n128\nTo conclude, we must design a feature pipeline that constantly syncs the data warehouse and \nlogical feature store while processing the data accordingly. Having the data in a feature store \nis critical for a production-ready ML system. The LLM Twin inference pipeline will query it for \nRAG, while the training pipeline will consume tracked and versioned fine-tuning datasets from it.\nThe feature store\nThe feature store will be the central access point for all the features used within the training and \ninference pipelines. The training pipeline will use the cleaned data from the feature store (stored \nas artifacts) to fine-tune LLMs. The inference pipeline will query the vector DB for chunked doc-\numents for RAG. That is why we are designing a feature pipeline and not only a RAG ingestion \npipeline. In practice, the feature pipeline contains multiple subcomponents, one of which is the \nRAG logic.\nRemember that the feature pipeline is mainly used as a mind map to navigate the complexity of \nML systems. It clearly states that it takes raw data as input and then outputs features and optional \nlabels, which are stored in the feature store. Thus, a good intuition is to consider that all the logic \nbetween the data warehouse and the feature store goes into the feature pipeline namespace, con-\nsisting of one or more sub-pipelines. For example, we will implement another pipeline that takes \nin cleaned data, processes it into instruct datasets, and stores it in artifacts; this also sits under \nthe feature pipeline umbrella as the artifacts are part of the logical feature store. Another example \nwould be implementing a data validation pipeline on top of the raw data or computed features.\nAnother important observation to make is that text data stored as strings are not considered \nfeatures if you follow the standard conventions. A feature is something that is fed directly into \nthe model. For example, we would have to tokenize the instruct datasets or chunked documents \nto be considered features. Why? Because the tokens are fed directly to the model and not the \nsentences as strings. Unfortunately, this makes the system more complex and unflexible. Thus, \nwe will do the tokenization at runtime. But this observation is important to understand as it’s \na clear example that you don’t have to be too rigid about the feature/training/inference (FTI) \narchitecture. You have to take it and adapt it to your own use case.\nWhere does the raw data come from?\nAs a quick reminder, all the raw documents are stored in a MongoDB data warehouse. The data \nwarehouse is populated by the data collection ETL pipeline presented in Chapter 3. The ETL pipe-\nline crawls various platforms such as Medium and Substack, standardizes the data, and loads it \ninto MongoDB. Check out Chapter 3 for more details on this topic.\n\n\nChapter 4\n129\nDesigning the architecture of the RAG feature pipeline\nThe last step is to architect and go through the design of the RAG feature pipeline of the LLM \nTwin application. We will use a batch design scheduled to poll data from the MongoDB data \nwarehouse, process it, and load it to a Qdrant vector DB. The first question to ask ourselves is, \n“Why a batch pipeline?”\nBut before answering that, let’s quickly understand how a batch architecture works and behaves \nrelative to a streaming design.\nFigure 4.9: The architecture of the LLM Twin’s RAG feature pipeline\n\n\nRAG Feature Pipeline\n130\nBatch pipelines\nA batch pipeline in data systems refers to a data processing method where data is collected, pro-\ncessed, and stored in predefined intervals and larger volumes, also known as “batches”. This ap-\nproach differs from real-time or streaming data processing, where data is processed continuously \nas it arrives. This is what happens in a batch pipeline:\n1.\nData collection: Data is collected from various sources and stored until sufficient amounts \nare accumulated for processing. This can include data from DBs, logs, files, and other \nsources.\n2.\nScheduled processing: Data processing is scheduled at regular intervals, for example, \nhourly or daily. During this time, the collected data is processed in bulk. This can involve \ndata cleansing, transformation, aggregation, and other operations.\n3.\nData loading: After processing, the data is loaded into the target system, such as a DB, data \nwarehouse, data lake, or feature store. This processed data is then available for analysis, \nquerying, or further processing.\nBatch pipelines are particularly useful when dealing with large volumes of data that do not require \nimmediate processing. They offer several advantages, including:\n•\nEfficiency: Batch processing can handle large volumes of data more efficiently than re-\nal-time processing, allowing for optimized resource allocation and parallel processing.\n•\nComplex processing: Batch pipelines can perform complex data transformations and \naggregations that might be too resource-intensive for real-time processing.\n•\nSimplicity: Batch processing systems’ architectures are often simpler than those of re-\nal-time systems, making them easier to implement and maintain.\nBatch versus streaming pipelines\nWhen implementing feature pipelines, you have two main design choices: batch and streaming. \nThus, it is worthwhile to see the difference between the two and understand why we chose a \nbatch architecture over a streaming one for our LLM Twin use case.\nYou can effortlessly write a dedicated chapter on streaming pipelines, which suggests its com-\nplexity over a batch design. However, as streaming architectures become increasingly popular, \none must have an intuition of how they work to choose the best option for your application.\n\n\nChapter 4\n131\nThe core elements of streaming applications are a distributed event streaming platform such as \nApache Kafka or Redpanda to store events from multiple clients and a streaming engine such as \nApache Flink or Bytewax to process the events. To simplify your architecture, you can swap your \nevent streaming platform with queues, such as RabbitMQ, to store the events until processed. \nTable 4.1 compares batch and streaming pipelines based on multiple criteria such as processing \nschedule and complexity:\nAspect\nBatch pipeline\nStreaming pipeline\nProcessing schedule\nProcesses data at regular \nintervals (e.g., every \nminute, hourly, daily).\nProcesses data \ncontinuously, with \nminimal latency.\nEfficiency\nHandles large volumes of \ndata more efficiently, \noptimizing resource \nallocation and parallel \nprocessing.\nHandles single data \npoints, providing \nimmediate insights \nand updates, allowing \nfor rapid response to \nchanges.\nProcessing complexity\nCapable of performing \ncomplex data transformations \nand aggregations.\nDesigned to handle \nhigh-velocity data \nstreams with low \nlatency.\nUse cases\nSuitable for scenarios where \nimmediate data processing \nis not critical. Commonly \nused in data warehousing, \nreporting, ETL processes, \nand feature pipelines.\nIdeal for applications \nrequiring real-time \nanalytics, features, \nmonitoring, and event-\ndriven architectures.\nSystem complexity\nCompared to streaming \npipelines, systems are \ngenerally simpler to \nimplement and maintain.\nMore complex to \nimplement and maintain \ndue to the need for \nlow-latency processing, \nfault tolerance, and \nscalability. The \ntooling is also \nmore advanced and \ncomplicated.\nTable 4.1: Batch versus streaming pipelines\n\n\nRAG Feature Pipeline\n132\nFor example, streaming pipelines are extremely powerful in social media recommender systems \nlike TikTok. When using social media, user behavior changes frequently. A typical scenario is \nthat you want to relax at a certain point in time and mostly look at videos of puppies. Still, after \n15 minutes, you get bored and want something more serious, such as educative content or news. \nThis means the recommender system has to capture these behavior changes without delay to keep \nyou engaged. As the transition between interests is cyclical and not predictable, you can’t use a \nbatch pipeline that runs every 30 minutes or every hour to generate more content. You can run \nit every minute to create new content, but, at the same time, it will result in unnecessary costs, \nas most predictions will not be consumed. By implementing a streaming pipeline, you update \nthe features of specific users in real time, which are then passed to a chain of models that predict \nthe new recommendations.\nStreaming architectures are also the backbone of real-time fraud detection algorithms, such as \nthose used at Stripe or PayPal. In this context, it’s critical to identify potentially fraudulent trans-\nactions as they occur, not after a few minutes or hours as a batch pipeline would process them. \nThe same urgency applies to high-frequency trading platforms that make stock predictions based \non the constant influx of market data, enabling traders to make decisions within milliseconds.\nOn the other hand, you can use a batch architecture for an offline recommender system. For ex-\nample, when implementing one for an e-commerce or streaming platform, you don’t need the \nsystem to be so reactive, as the user’s behavior rarely changes. Thus, updating the recommen-\ndations periodically, such as every night, based on historical user behavior data using a batch \npipeline is easier to implement and cheaper.\nAnother popular example of batch pipelines is the ETL design used to extract, transform, and load \ndata for different use cases. The ETL design is widespread in data pipelines used to move data \nfrom one DB to another. Some practical use cases include aggregating data for analytics, where \nyou have to extract data from multiple sources, aggregate it, and load it to a data warehouse \nconnected to a dashboard. The analytics domains can be widespread, from e-commerce and \nmarketing to finance and research.\n\n\nChapter 4\n133\nThe data collection pipeline used in the LLM Twin use case is another example of an ETL pipeline \nthat extracts data from the internet, structures it, and loads it into a data warehouse for future \nprocessing.\nAlong with prediction or feature freshness, another disadvantage of batch pipelines over streaming \nones is that you usually make redundant predictions. Let’s take the example of a recommender \nsystem for a streaming platform like Netflix. Every night, you make the predictions for all users. \nThere is a significant chance that a large chunk of users won’t log in that day. Also, users usually \ndon’t browse all the recommendations but stick to the first ones. Thus, only a portion of predic-\ntions are used, wasting computing power on all the others.\nThat’s why a popular strategy is to start with a batch architecture, as it’s faster and easier to im-\nplement. After the product is in place, you gradually move to a streaming design to reduce costs \nand improve the user experience.\nTo conclude, we have used a batch architecture (and not a streaming one) to implement the LLM \nTwin’s feature pipeline for the following reasons:\n•\nDoes not require immediate data processing: Even if syncing the data warehouse and \nfeature store is critical for an accurate RAG system, a delay of a few minutes is acceptable. \nThus, we can schedule the batch pipeline to run every minute, constantly syncing the two \ndata storages. This technique works because the data volume is small. The whole data \nwarehouse will have only thousands of records, not millions or billions. Hence, we can \nquickly iterate through them and sync the two DBs.\n•\nSimplicity: As stated earlier, implementing a streaming pipeline is two times more com-\nplex. In the real world, you want to keep your system as simple as possible, making it \neasier to understand, debug, and maintain. Also, simplicity usually translates to lower \ninfrastructure and development costs.\n\n\nRAG Feature Pipeline\n134\nIn Figure 8.10, we compare what tools you can use based on your architecture (streaming versus \nbatch) and the quantity of data you have to process (small versus big data). In our use case, we \nare in the smaller data and batch quadrant, where we picked a combination of vanilla Python and \ngenerative AI tools such as LangChain, Sentence Transformers, and Unstructured.\nFigure 4.10: Tools on the streaming versus batch and smaller versus bigger data spectrum\nIn the Change data capture: syncing the data warehouse and feature store section later in this chapter, \nwe will discuss when switching from a batch architecture to a streaming one makes sense.\nCore steps\nMost of the RAG feature pipelines are composed of five core steps. The one implemented in the \nLLM Twin architecture makes no exception. Thus, you can quickly adapt this pattern for other \nRAG applications, but here is what the LLM Twin’s RAG feature pipeline looks like:\n1.\nData extraction: Extract the latest articles, code repositories, and posts from the Mon-\ngoDB data warehouse. At the extraction step, you usually aggregate all the data you need \nfor processing.\n\n\nChapter 4\n135\n2. \nCleaning: The data from the data warehouse is standardized and partially clean, but we \nhave to ensure that the text contains only useful information, is not duplicated, and can \nbe interpreted by the embedding model. For example, we must clean and normalize all \nnon-ASCII characters before passing the text to the embedding model. Also, to keep the \ninformation semantically dense, we decided to replace all the URLs with placeholders \nand remove all emojis. The cleaning step is more art than science. Hence, after you have \nthe first iteration with an evaluation mechanism in place, you will probably reiterate and \nimprove it.\n3. \nChunking: You must adopt various chunking strategies based on each data category \nand embedding model. For example, when working with code repositories, you want \nthe chunks broader, whereas when working with articles, you want them narrower or \nscoped at the paragraph level. Depending on your data, you must decide if you split your \ndocument based on the chapter, section, paragraph, sentence, or just a fixed window size. \nAlso, you have to ensure that the chunk size doesn’t exceed the maximum input size of \nthe embedding model. That is why you usually chunk a document based on your data \nstructure and the maximum input size of the model.\n4.\nEmbedding: You pass each chunk individually to an embedding model of your choice. \nImplementation-wise, this step is usually the simplest, as tools such as SentenceTrans-\nformer and Hugging Face provide high-level interfaces for most embedding models. As \nexplained in the What are embeddings? section of this chapter, at this step, the most critical \ndecisions are to decide what model to use and whether to fine-tune it or not. For example, \nwe used an \"all-mpnet-base-v2\" embedding model from SentenceTransformer, which \nis relatively tiny and runs on most machines. However, we provide a configuration file \nwhere you can quickly configure the embedding model with something more powerful \nbased on the state of the art when reading this book. You can quickly find other options \non the MTEB on Hugging Face (https://huggingface.co/spaces/mteb/leaderboard).\n5.\nData loading: The final step combines the embedding of a chunked document and its \nmetadata, such as the author and the document ID, content, URL, platform, and creation \ndate. Ultimately, we wrap the vector and the metadata into a structure compatible with \nQdrant and push it to the vector DB. As we want to use Qdrant as the single source of truth \nfor the features, we also push the cleaned documents (before chunking) to Qdrant. We can \npush data without vectors, as the metadata index of Qdrant behaves like a NoSQL DB. Thus, \npushing metadata without a vector attached to it is like using a standard NoSQL engine.\n\n\nRAG Feature Pipeline\n136\nChange data capture: syncing the data warehouse and feature \nstore\nAs highlighted a few times in this chapter, data is constantly changing, which can result in DBs, \ndata lakes, data warehouses, and feature stores getting out of sync. Change data capture (CDC) \nis a strategy that allows you to optimally keep two or more data storage types in sync without \ncomputing and I/O overhead. It captures any CRUD operation done on the source DB and repli-\ncates it on a target DB. Optionally, you can add preprocessing steps in between the replication.\nThe syncing issues also apply when building a feature pipeline. One key design choice concerns \nhow to sync the data warehouse with the feature store to have data fresh enough for your par-\nticular use case.\nIn our LLM Twin use case, we chose a naïve approach out of simplicity. We implemented a batch \npipeline that is triggered periodically or manually. It reads all the raw data from the data ware-\nhouse, processes it in batches, and inserts new records or updates old ones from the Qdrant \nvector DB. This works fine when you are working with a small number of records, at the order of \nthousands or tens of thousands. But our naïve approach raises the following questions:\n• \nWhat happens if the data suddenly grows to millions of records (or higher)?\n• \nWhat happens if a record is deleted from the data warehouse? How is this reflected in \nthe feature store?\n• \nWhat if we want to process only the new or updated items from the data warehouse and \nnot all of them?\nFortunately, the CDC pattern can solve all of these issues. When implementing CDC, you can \ntake multiple approaches, but all of them use either a push or pull strategy:\n•\nPush: The source DB is the primary driver in the push approach. It actively identifies \nand transmits data modifications to target systems for processing. This method ensures \nnear-instantaneous updates at the target, but data loss can occur if target systems are \ninaccessible. To mitigate this, a messaging system is typically employed as a buffer.\n•\nPull: The pull method assigns a more passive role to the source DB, which only records \ndata changes. Target systems periodically request these changes and handle updates \naccordingly. While this approach lightens the load on the source, it introduces a delay \nin data propagation. A messaging system is again essential to prevent data loss during \nperiods of target system unavailability.\n\n\nChapter 4\n137\nIn summary, the push method is ideal for applications demanding immediate data access, where-\nas the pull method is better suited for large-scale data transfers where real-time updates aren’t \ncritical. With that in mind, there are different methods to detect changes in data. Thus, let’s list \nthe main CDC patterns that are used in the industry:\n•\nTimestamp-based: The approach involves adding a modification time column to DB \ntables, usually called LAST_MODIFIED or LAST_UPDATED. Downstream systems can query \nthis column to identify records that have been updated since their last check. While sim-\nple to implement, this method is limited to tracking changes, not deletions, and imposes \nperformance overhead due to the need to scan entire tables.\n•\nTrigger-based: The trigger-based approach utilizes DB triggers to automatically record \ndata modifications in a separate table upon INSERT, UPDATE, or DELETE operations, often \nknown as the event table. This method provides comprehensive change tracking but can \nimpact the DB performance due to the additional write operations involved for each event.\n•\nLog-based: DBs maintain transaction logs to record all data modifications, including \ntimestamps. Primarily used for recovery, these logs can also be leveraged to propagate \nchanges to target systems in real time. This approach minimizes the performance impact \non the source DB. As a huge advantage, it avoids additional processing overhead on the \nsource DB, captures all data changes, and requires no schema modification. But on the op-\nposite side, it lacks standardized log formats, leading to vendor-specific implementations.\nWith these CDC techniques in mind, we could quickly implement a pull timestamp-based strategy \nin our RAG feature pipeline to sync the data warehouse and feature store more optimally when \nthe data grows. Our implementation is still pull-based but doesn’t check any last updated field \nin the source DB; it just pulls everything from the data warehouse.\nHowever, the most popular and optimal technique in the industry is the log-based one. It doesn’t \nadd any I/O overhead to the source DB, has low latency, and supports all CRUD operations. The \nbiggest downside is its development complexity, which requires a queue to capture all the CRUD \nevents and a streaming pipeline to process them.\nFor more details on CDC, I recommend What is Change Data Capture? from Conflu-\nent’s blog: https://www.confluent.io/en-gb/learn/change-data-capture/.\n\n\nRAG Feature Pipeline\n138\nAs this is an LLM book and not a data engineering one, we wanted to keep things simple, but it’s \nimportant to know that these techniques exist, and you can always upgrade your current imple-\nmentation when it doesn’t fit your application requirements anymore.\nWhy is the data stored in two snapshots?\nWe store two snapshots of our data in the logical feature store:\n• \nAfter the data is cleaned: For fine-tuning LLMs\n• \nAfter the documents are chunked and embedded: For RAG\nWhy did we design it this way? Remember that the features should be accessed solely from the feature \nstore for training and inference. Thus, this adds consistency to our design and makes it cleaner.\nAlso, storing the data cleaned specifically for our fine-tuning and embedding use case in the Mon-\ngoDB data warehouse would have been an antipattern. The data from the warehouse is shared \nall across the company. Thus, processing it for a specific use case is not good practice. Imagine \nanother summarization use case where we must clean and preprocess the data differently. We \nmust create a new “Cleaned Data” table prefixed with the use case name. We have to repeat that \nfor every new use case. Therefore, to avoid having a spaghetti data warehouse, the data from the \ndata warehouse is generic and is modeled to specific applications only in downstream compo-\nnents, which, in our case, is the feature store.\nUltimately, as we mentioned in the Core steps section, you can leverage the metadata index of a \nvector DB as a NoSQL DB. Based on these factors, we decided to keep the cleaned data in Qdrant, \nalong with the chunked and embedded versions of the documents.\nAs a quick reminder, when operationalizing our LLM Twin system, the create instruct dataset \npipeline, explained in Chapter 5, will read the cleaned documents from Qdrant, process them, \nand save them under a versioned ZenML artifact. The training pipeline requires a dataset and not \nplain documents. This is a reminder that our logical feature store comprises the Qdrant vector \nDB for online serving and ZenML artifacts for offline training.\nOrchestration\nZenML will orchestrate the batch RAG feature pipeline. Using ZenML, we can schedule it to run \non a schedule, for example, every hour, or quickly manually trigger it. Another option is to trigger \nit after the ETL data collection pipeline finishes.\nBy orchestrating the feature pipeline and integrating it into ZenML (or any other orchestration \ntool), we can operationalize the feature pipeline with the end goal of continuous training (CT).\n\n\nChapter 4\n139\nWe will go into all the details of orchestration, scheduling, and CT in Chapter 11.\nImplementing the LLM Twin’s RAG feature pipeline\nThe last step is to review the LLM Twin’s RAG feature pipeline code to see how we applied every-\nthing we discussed in this chapter. We will walk you through the following:\n• \nZenML code\n• \nPydantic domain objects\n• \nA custom object-vector mapping (OVM) implementation\n• \nThe cleaning, chunking, and embedding logic for all our data categories\nWe will take a top-down approach. Thus, let’s start with the Settings class and ZenML pipeline.\nSettings\nWe use Pydantic Settings (https://docs.pydantic.dev/latest/concepts/pydantic_settings/) \nto define a global Settings class that loads sensitive or non-sensitive variables from a .env file. \nThis approach also gives us all the benefits of Pydantic, such as type validation. For example, if \nwe provide a string for the QDRANT_DATABASE_PORT variable instead of an integer, the program \nwill crash. This behavior makes the whole application more deterministic and reliable.\nHere is what the Settings class looks like with all the variables necessary to build the RAG fea-\nture pipeline:\nfrom pydantic import BaseSettings\nclass Settings(BaseSettings):\n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n    … # Some other settings…\n    # RAG\n    TEXT_EMBEDDING_MODEL_ID: str = \"sentence-transformers/all-MiniLM-\nL6-v2\"\n    RERANKING_CROSS_ENCODER_MODEL_ID: str = \"cross-encoder/ms-marco-\nMiniLM-L-4-v2\"\n    RAG_MODEL_DEVICE: str = \"cpu\"\n",
      "page_number": 149
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 164-171)",
      "start_page": 164,
      "end_page": 171,
      "detection_method": "topic_boundary",
      "content": "RAG Feature Pipeline\n140\n    # QdrantDB Vector DB\n    USE_QDRANT_CLOUD: bool = False\n    QDRANT_DATABASE_HOST: str = \"localhost\"\n    QDRANT_DATABASE_PORT: int = 6333\n    QDRANT_CLOUD_URL: str = \"str\"\n    QDRANT_APIKEY: str | None = None\n    … # More settings…\nsettings = Settings()\nAs stated in the internal Config class, all the variables have default values or can be overridden \nby providing a .env file.\nZenML pipeline and steps\nThe ZenML pipeline is the entry point for the RAG feature engineering pipeline. It reflects the \nfive core phases of RAG ingestion code: extracting raw documents, cleaning, chunking, embed-\nding, and loading them to the logical feature store. The calls within the feature_engineering()\nfunction are ZenML steps, representing a single execution unit performing the five phases of RAG. \nThe code is available in the GitHub repository at https://github.com/PacktPublishing/LLM-\nEngineers-Handbook/blob/main/pipelines/feature_engineering.py:\nfrom zenml import pipeline\nfrom llm_engineering.interfaces.orchestrator.steps import feature_\nengineering as fe_steps\n@pipeline\ndef feature_engineering(author_full_names: list[str]) -> None:\n    raw_documents = fe_steps.query_data_warehouse(author_full_names)\n    cleaned_documents = fe_steps.clean_documents(raw_documents)\n     last_step_1 = fe_steps.load_to_vector_db(cleaned_documents)\n    embedded_documents = fe_steps.chunk_and_embed(cleaned_documents)\n    last_step_2 = fe_steps.load_to_vector_db(embedded_documents)\n    return [last_step_1.invocation_id, last_step_2.invocation_id]\n\n\nChapter 4\n141\nFigure 4.11 shows how multiple feature engineering pipeline runs look in ZenML’s dashboard.\nFigure 4.11: Feature pipeline runs in the ZenML dashboard\nFigure 8.12 shows the DAG of the RAG feature pipeline, where you can follow all the pipeline steps \nand their output artifacts. Remember that whatever is returned from a ZenML step is automati-\ncally saved as an artifact, stored in ZenML’s artifact registry, versioned, and shareable across the \napplication.\nFigure 4.12: Feature pipeline DAG in the ZenML dashboard\n\n\nRAG Feature Pipeline\n142\nThe final puzzle piece is understanding how to configure the RAG feature pipeline dynamically. \nAll its available settings are exposed as function parameters. Here, we need only a list of au-\nthor’s names, as seen in the function’s signature: feature_engineering(author_full_names: \nlist[str]). We inject a YAML configuration file at runtime that contains all the necessary values \nbased on different use cases. For example, the following configuration includes a list of all the \nauthors of this book as we want to populate the feature store with data from all of us (available \nin the GitHub repository at configs/feature_engineering.yaml):\nparameters:\n  author_full_names:\n    - Alex Vesa\n    - Maxime Labonne\n    - Paul Iusztin\nThe beauty of this approach is that you don’t have to modify the code to configure the feature \npipeline with different input values. You have to provide a different configuration file when run-\nning it, as follows:\nfeature_engineering.with_options(config_path=\"…/feature_engineering.yaml\")\n()\nYou can either hardcode the path to the config file or provide the config_path from the CLI, which \nallows you to modify the pipeline’s configuration between different runs. Out of simplicity, we \nhard-coded the configuration file. Thus, we can call the feature engineering pipeline calling the \nrun.py script as follows:\npython -m tools.run --no-cache --run-feature-engineering\nHowever, you can easily add another CLI argument to pass the config_path variable. Also, you \ncan run the feature pipeline using the following poe command:\npoetry poe run-feature-engineering-pipeline\nLet’s move forward to the ZenML steps and sequentially zoom in on all of them. The source code for \nall the feature engineering pipeline steps is available on GitHub at \"steps/feature_engineering\". \nWe will begin with the first step, which involves querying the data warehouse for new content \nto process into features.\n\n\nChapter 4\n143\nQuerying the data warehouse\nThe first thing to notice is that a step is a Python function decorated with @step, similar to how \na ZenML pipeline works. The function below takes as input a list of authors’ full names and per-\nforms the following core steps:\n•\nIt attempts to get or create a UserDocument instance using the first and last names, ap-\npending this instance to the authors list. If the user doesn’t exist, it throws an error.\n•\nIt fetches all the raw data for the user from the data warehouse and extends the documents\nlist to include these user documents.\n•\nUltimately, it computes a descriptive metadata dictionary logged and tracked in ZenML.\n… # other imports\nfrom zenml import get_step_context, step\n@step\ndef query_data_warehouse(\n    author_full_names: list[str],\n) -> Annotated[list, \"raw_documents\"]:\n    documents = []\n    authors = []\n    for author_full_name in author_full_names:\n        logger.info(f\"Querying data warehouse for user: {author_full_\nname}\")\n        first_name, last_name = utils.split_user_full_name(author_full_\nname)\n        logger.info(f\"First name: {first_name}, Last name: {last_name}\")\n        user = UserDocument.get_or_create(first_name=first_name, last_\nname=last_name)\n        authors.append(user)\n        results = fetch_all_data(user)\n        user_documents = [doc for query_result in results.values() for doc \nin query_result]\n        documents.extend(user_documents)\n    step_context = get_step_context()\n\n\nRAG Feature Pipeline\n144\n    step_context.add_output_metadata(output_name=\"raw_documents\", \nmetadata=_get_metadata(documents))\n    return documents\nThe fetch function leverages a thread pool that runs each query on a different thread. As we have \nmultiple data categories, we have to make a different query for the articles, posts, and reposi-\ntories, as they are stored in different collections. Each query calls the data warehouse, which is \nbounded by the network I/O and data warehouse latency, not by the machine’s CPU. Thus, by \nmoving each query to a different thread, we can parallelize them. Ultimately, instead of adding \nthe latency of each query as the total timing, the time to run this fetch function will be the max \nbetween all the calls.\nUsing threads to parallelize I/O-bounded calls is good practice in Python, as they are not locked \nby the Python Global Interpreter Lock (GIL). In contrast, adding each call to a different process \nwould add too much overhead, as a process takes longer to spin off than a thread.\nIn Python, you want to parallelize things with processes only when the operations are CPU or \nmemory-bound because the GIL affects them. Each process has a different GIL. Thus, paralleliz-\ning your computing logic, such as processing a batch of documents or images already loaded in \nmemory, isn’t affected by Python’s GIL limitations.\ndef fetch_all_data(user: UserDocument) -> dict[str, \nlist[NoSQLBaseDocument]]:\n    user_id = str(user.id)\n    with ThreadPoolExecutor() as executor:\n        future_to_query = {\n            executor.submit(__fetch_articles, user_id): \"articles\",\n            executor.submit(__fetch_posts, user_id): \"posts\",\n            executor.submit(__fetch_repositories, user_id): \n\"repositories\",\n        }\n        results = {}\n        for future in as_completed(future_to_query):\n            query_name = future_to_query[future]\n            try:\n                results[query_name] = future.result()\n            except Exception:\n\n\nChapter 4\n145\n                logger.exception(f\"'{query_name}' request failed.\")\n                results[query_name] = []\n    return results\nThe _get_metadata() function takes the list of queried documents and authors and counts the \nnumber of them relative to each data category:\ndef _get_metadata(documents: list[Document]) -> dict:\n    metadata = {\n        \"num_documents\": len(documents),\n    }\n    for document in documents:\n        collection = document.get_collection_name()\n        if collection not in metadata:\n            metadata[collection] = {}\n        if \"authors\" not in metadata[collection]:\n            metadata[collection][\"authors\"] = list()\n        metadata[collection][\"num_documents\"] = metadata[collection].\nget(\"num_documents\", 0) + 1\n        metadata[collection][\"authors\"].append(document.author_full_name)\n    for value in metadata.values():\n        if isinstance(value, dict) and \"authors\" in value:\n            value[\"authors\"] = list(set(value[\"authors\"]))\n    return metadata\nWe will expose this metadata in the ZenML dashboard to quickly see some statistics on the loaded \ndata. For example, in Figure 4.13, we accessed the metadata tab of the query_data_warehouse()\nstep, where you can see that, within that particular run of the feature pipeline, we loaded 76 \ndocuments from three authors. This is extremely powerful for monitoring and debugging batch \npipelines. \n\n\nRAG Feature Pipeline\n146\nYou can always extend it with anything that makes sense for your use case.\nFigure 4.13: Metadata of the “query the data warehouse” ZenML step\nCleaning the documents\nIn the cleaning step, we iterate through all the documents and delegate all the logic to a \nCleaningDispatcher who knows what cleaning logic to apply based on the data category. Re-\nmember that we want to apply, or have the ability to apply in the future, different cleaning tech-\nniques on articles, posts, and code repositories.\n@step\ndef clean_documents(\n    documents: Annotated[list, \"raw_documents\"],\n) -> Annotated[list, \"cleaned_documents\"]:\n    cleaned_documents = []\n    for document in documents:\n\n\nChapter 4\n147\n        cleaned_document = CleaningDispatcher.dispatch(document)\n        cleaned_documents.append(cleaned_document)\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"cleaned_documents\", \nmetadata=_get_metadata(cleaned_documents))\n    return cleaned_documents\nThe computed metadata is similar to what we logged in the query_data_warehouse() step. Thus, \nlet’s move on to chunking and embedding.\nChunk and embed the cleaned documents\nSimilar to how we cleaned the documents, we delegate the chunking and embedding logic to \na dispatcher who knows how to handle each data category. Note that the chunking dispatcher \nreturns a list instead of a single object, which makes sense as the document is split into multiple \nchunks. We will dig into the dispatcher in the “The dispatcher layer” section of this chapter.\n@step\ndef chunk_and_embed(\n    cleaned_documents: Annotated[list, \"cleaned_documents\"],\n) -> Annotated[list, \"embedded_documents\"]:\n    metadata = {\"chunking\": {}, \"embedding\": {}, \"num_documents\": \nlen(cleaned_documents)}\n    embedded_chunks = []\n    for document in cleaned_documents:\n        chunks = ChunkingDispatcher.dispatch(document)\n        metadata[\"chunking\"] = _add_chunks_metadata(chunks, \nmetadata[\"chunking\"])\n        for batched_chunks in utils.misc.batch(chunks, 10):\n            batched_embedded_chunks = EmbeddingDispatcher.\ndispatch(batched_chunks)\n            embedded_chunks.extend(batched_embedded_chunks)\n    metadata[\"embedding\"] = _add_embeddings_metadata(embedded_chunks, \nmetadata[\"embedding\"])\n    metadata[\"num_chunks\"] = len(embedded_chunks)\n",
      "page_number": 164
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 172-179)",
      "start_page": 172,
      "end_page": 179,
      "detection_method": "topic_boundary",
      "content": "RAG Feature Pipeline\n148\n    metadata[\"num_embedded_chunks\"] = len(embedded_chunks)\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"embedded_documents\", \nmetadata=metadata)\n    return embedded_chunks\nIn Figure 4.14, you can see the metadata of the chunking and embedding ZenML step. For exam-\nple, you can quickly understand that we transformed 76 documents into 2,373 chunks, or the \nproperties we used for chunking articles, such as a chunk_size of 500 and a chunk_overlap of 50.\nFigure 4.14: Metadata of the embedding and chunking ZenML step, detailing the uncategorized \nand chunking dropdowns\n\n\nChapter 4\n149\nIn Figure 4.15, the rest of the ZenML metadata from the embedding and chunking step details \nthe embedding model and its properties used to compute the vectors.\nFigure 4.15: Metadata of the embedding and chunking ZenML step, detailing the embedding \ndropdown\n\n\nRAG Feature Pipeline\n150\nAs ML systems can break at any time while in production due to drifts or untreated use cases, le-\nveraging the metadata section to monitor the ingested data can be a powerful tool that will save \ndebugging days, translating to tens of thousands of dollars or more for your business.\nLoading the documents to the vector DB\nAs each article, post, or code repository sits in a different collection inside the vector DB, we have \nto group all the documents based on their data category. Then, we load each group in bulk in the \nQdrant vector DB:\n@step\ndef load_to_vector_db(\n    documents: Annotated[list, \"documents\"],\n) -> None:\n    logger.info(f\"Loading {len(documents)} documents into the vector \ndatabase.\")\n    grouped_documents = VectorBaseDocument.group_by_class(documents)\n    for document_class, documents in grouped_documents.items():\n        logger.info(f\"Loading documents into {document_class.get_\ncollection_name()}\")\n        for documents_batch in utils.misc.batch(documents, size=4):\n            try:\n                document_class.bulk_insert(documents_batch)\n            except Exception:\n                return False\n    return True\nPydantic domain entities\nBefore investigating the dispatchers, we must understand the domain objects we work with. To \nsome extent, in implementing the LLM Twin, we are following the domain-driven design (DDD) \nprinciples, which state that domain entities are the core of your application. Thus, before pro-\nceeding, it’s important to understand the hierarchy of the domain classes we are working with.\nThe code for the domain entities is available on GitHub at https://github.com/\nPacktPublishing/LLM-Engineering/tree/main/llm_engineering/domain.\n\n\nChapter 4\n151\nWe used Pydantic to model all our domain entities. When we wrote the book, choosing Pydantic \nwas a no-brainer, as it is the go-to Python package for writing data structures with out-of-the-box \ntype validation. As Python is a dynamically typed language, using Pydantic for type validation at \nruntime makes your system order of times more robust, as you can be sure that you are always \nworking with the right type of data.\nThe domain of our LLM Twin application is split into two dimensions:\n• \nThe data category: Post, article, and repository\n• \nThe state of the data: Cleaned, chunked, and embedded\nWe decided to create a base class for each state of the document, resulting in having the following \nbase abstract classes:\n•\nclass CleanedDocument(VectorBaseDocument, ABC)\n•\nclass Chunk(VectorBaseDocument, ABC)\n•\nclass EmbeddedChunk(VectorBaseDocument, ABC)\nNote that all of them inherit the VectorBaseDocument class, which is our custom OVM implemen-\ntation, which we will explain in the next section of this chapter. Also, it inherits from ABC, which \nmakes the class abstract. Thus, you cannot initialize an object out of these classes; you may only \ninherit from them. That is why base classes are always marked as abstract.\nEach base abstract class from above (which models the state) will have a subclass that adds \nthe data category dimension. For example, the CleanedDocument class will have the following \nsubclasses:\n•\nclass CleanedPostDocument(CleanedDocument)\n•\nclass CleanedArticleDocument(CleanedDocument)\n•\nclass CleanedRepositoryDocument(CleanedDocument)\nAs we can see in Figure 8.16, we will repeat the same logic for the Chunk and EmbeddedChunk base \nabstract classes. We will implement a specific document class for each data category and state com-\nbination, resulting in nine types of domain entities. For example, when ingesting a raw document, \nthe cleaning step will yield a CleanedArticleDocument instance, the chunking step will return a \nlist of ArticleChunk objects, and the embedding operation will return EmbeddedArticleChunk in-\nstances that encapsulate the embedding and all the necessary metadata to ingest in the vector DB. \n\n\nRAG Feature Pipeline\n152\nThe same will happen for the posts and repositories.\nFigure 4.16: Domain entities class hierarchy and their interaction\nWe chose this design because the list of states will rarely change, and we want to extend the list \nof data categories. Thus, structuring the classes after the state allows us to plug another data \ncategory by inheriting these base abstract classes.\nLet’s see the complete code for the hierarchy of the cleaned document. All the attributes of a \ncleaned document will be saved within the metadata of the vector DB. For example, the metadata \nof a cleaned article document will always contain the content, platform, author ID, author full \nname, and link of the article.\nAnother fundamental aspect is the Config internal class, which defines the name of the collection \nwithin the vector DB, the data category of the entity, and whether to leverage the vector index \nwhen creating the collection:\nclass CleanedDocument(VectorBaseDocument, ABC):\n    content: str\n    platform: str\n    author_id: UUID4\n    author_full_name: str\n\n\nChapter 4\n153\nclass CleanedPostDocument(CleanedDocument):\n    image: Optional[str] = None\n    class Config:\n        name = \"cleaned_posts\"\n        category = DataCategory.POSTS\n        use_vector_index = False\nclass CleanedArticleDocument(CleanedDocument):\n    link: str\n    class Config:\n        name = \"cleaned_articles\"\n        category = DataCategory.ARTICLES\n        use_vector_index = False\nclass CleanedRepositoryDocument(CleanedDocument):\n    name: str\n    link: str\n    class Config:\n        name = \"cleaned_repositories\"\n        category = DataCategory.REPOSITORIES\n        use_vector_index = False\nTo conclude this section, let’s also take a look at the base abstract class of the chunk and embed-\nded chunk:\nclass Chunk(VectorBaseDocument, ABC):\n    content: str\n    platform: str\n    document_id: UUID4\n    author_id: UUID4\n    author_full_name: str\n    metadata: dict = Field(default_factory=dict)\n… # PostChunk, ArticleChunk, RepositoryChunk\n\n\nRAG Feature Pipeline\n154\nclass EmbeddedChunk(VectorBaseDocument, ABC):\n    content: str\n    embedding: list[float] | None\n    platform: str\n    document_id: UUID4\n    author_id: UUID4\n    author_full_name: str\n    metadata: dict = Field(default_factory=dict)\n… # EmbeddedPostChunk, EmbeddedArticleChunk, EmbeddedRepositoryChunk\nWe also defined an enum that aggregates all our data categories in a single structure of constants:\nclass DataCategory(StrEnum):\n    POSTS = \"posts\"\n    ARTICLES = \"articles\"\n    REPOSITORIES = \"repositories\"\nThe last step to fully understand how the domain objects work is to zoom into the \nVectorBaseDocument OVM class.\nOVM\nThe term OVM is inspired by the object-relational mapping (ORM) pattern we discussed in Chap-\nter 3. We called it OVM because we work with embedding and vector DBs instead of structured \ndata and SQL tables. Otherwise, it follows the same principles as an ORM pattern.\nSimilar to what we did in Chapter 3, we will implement our own OVM version. Even if our custom \nexample is simple, it’s a powerful example of how to write modular and extendable classes by \nleveraging OOP best practices and principles.\nOur OVM base class is called VectorBaseDocument. It will support CRUD operations on top of \nQdrant. Based on our application’s demands, we limited it only to create and read operations, but \nit can easily be extended to update and delete functions. \nThe full implementation of the VectorBaseDocument class is available on GitHub \nat https://github.com/PacktPublishing/LLM-Engineering/blob/main/llm_\nengineering/domain/base/vector.py.\n\n\nChapter 4\n155\nLet’s take a look at the definition of the VectorBaseDocument class:\nfrom pydantic import UUID4, BaseModel\nfrom typing import Generic\nfrom llm_engineering.infrastructure.db.qdrant import connection\nT = TypeVar(\"T\", bound=\"VectorBaseDocument\")\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    id: UUID4 = Field(default_factory=uuid.uuid4)\n    @classmethod\n    def from_record(cls: Type[T], point: Record) -> T:\n        _id = UUID(point.id, version=4)\n        payload = point.payload or {}\n        attributes = {\n            \"id\": _id,\n            **payload,\n        }\n        if cls._has_class_attribute(\"embedding\"):\n            payload[\"embedding\"] = point.vector or None\n        return cls(**attributes)\n    def to_point(self: T, **kwargs) -> PointStruct:\n        exclude_unset = kwargs.pop(\"exclude_unset\", False)\n        by_alias = kwargs.pop(\"by_alias\", True)\n        payload = self.dict(exclude_unset=exclude_unset, by_alias=by_\nalias, **kwargs)\n        _id = str(payload.pop(\"id\"))\n        vector = payload.pop(\"embedding\", {})\n        if vector and isinstance(vector, np.ndarray):\n",
      "page_number": 172
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 180-188)",
      "start_page": 180,
      "end_page": 188,
      "detection_method": "topic_boundary",
      "content": "RAG Feature Pipeline\n156\n            vector = vector.tolist()\n        return PointStruct(id=_id, vector=vector, payload=payload)\n• \nThe VectorBaseDocument class inherits from Pydantic’s BaseModel and helps us structure \na single record’s attributes from the vector DB. Every OVM will be initialized by default \nwith UUID4 as its unique identifier. Using generics—more precisely, by inheriting from \nGeneric[T]—the signatures of all the subclasses of the VectorBaseDocument class will \nadapt to that given class. For example, the from_record() method of the Chunk() class, \nwhich inherits VectorBaseDocument, will return the Chunk type, which drastically helps \nthe static analyzer and type checkers such as mypy (https://mypy.readthedocs.io/en/\nstable/).\nThe from_record() method adapts a data point from Qdrant’s format to our internal structure \nbased on Pydantic. On the other hand, the to_point() method takes the attributes of the current \ninstance and adapts them to Qdrant’s PointStruct() format. We will leverage these two methods \nfor our create and read operations.\nUltimately, all operations made to Qdrant will be done through the connection instance, which \nis instantiated in the application’s infrastructure layer.\nThe bulk_insert() method maps each document to a point. Then, it uses the Qdrant connection\ninstance to load all the points to a given collection in Qdrant. If the insertion fails once, it tries \nto create the collection and do the insertion again. Often, it is good practice to split your logic \ninto two functions. One private function contains the logic, in our case _bulk_insert(), and one \npublic function handles all the errors and failure scenarios.\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    … # Rest of the class\n    @classmethod\n    def bulk_insert(cls: Type[T], documents: list[\"VectorBaseDocument\"]) \n-> bool:\n        try:\n            cls._bulk_insert(documents)\n        except exceptions.UnexpectedResponse:\n            logger.info(\n                f\"Collection '{cls.get_collection_name()}' does not exist. \nTrying to create the collection and reinsert the documents.\"\n\n\nChapter 4\n157\n            )\n            cls.create_collection()\n            try:\n                cls._bulk_insert(documents)\n            except exceptions.UnexpectedResponse:\n                logger.error(f\"Failed to insert documents in '{cls.get_\ncollection_name()}'.\")\n                return False\n        return True\n    @classmethod\n    def _bulk_insert(cls: Type[T], documents: list[\"VectorBaseDocument\"]) \n-> None:\n        points = [doc.to_point() for doc in documents]\n        connection.upsert(collection_name=cls.get_collection_name(), \npoints=points)\nThe collection name is inferred from the Config class defined in the subclasses inheriting the OVM:\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    … # Rest of the class\n    @classmethod\n    def get_collection_name(cls: Type[T]) -> str:\n        if not hasattr(cls, \"Config\") or not hasattr(cls.Config, \"name\"):\n            raise ImproperlyConfigured(\n                \"The class should define a Config class with\" \"the 'name' \nproperty that reflects the collection's name.\"\n            )\n        return cls.Config.name\n\n\nRAG Feature Pipeline\n158\nNow, we must define a method that lets us read all the records from the vector DB (without using \nvector similarity search logic). The bulk_find() method enables us to scroll (or list) all the records \nfrom a collection. The function below scrolls the Qdrant vector DB, which returns a list of data \npoints, which are ultimately mapped to our internal structure using the from_record() method.\nThe limit parameters control how many items we return at once, and the offset signals the ID of \nthe point from which Qdrant starts returning records.\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    … # Rest of the class\n    @classmethod\n    def bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> \ntuple[list[T], UUID | None]:\n        try:\n            documents, next_offset = cls._bulk_find(limit=limit, **kwargs)\n        except exceptions.UnexpectedResponse:\n            logger.error(f\"Failed to search documents in '{cls.get_\ncollection_name()}'.\")\n            documents, next_offset = [], None\n        return documents, next_offset\n    @classmethod\n    def _bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> \ntuple[list[T], UUID | None]:\n        collection_name = cls.get_collection_name()\n        offset = kwargs.pop(\"offset\", None)\n        offset = str(offset) if offset else None\n        records, next_offset = connection.scroll(\n            collection_name=collection_name,\n            limit=limit,\n            with_payload=kwargs.pop(\"with_payload\", True),\n            with_vectors=kwargs.pop(\"with_vectors\", False),\n            offset=offset,\n\n\nChapter 4\n159\n            **kwargs,\n        )\n        documents = [cls.from_record(record) for record in records]\n        if next_offset is not None:\n            next_offset = UUID(next_offset, version=4)\n        return documents, next_offset\nThe last piece of the puzzle is to define a method that performs a vector similarity search on a \nprovided query embedding. Like before, we defined a public search() and private _search()\nmethod. The search is performed by Qdrant when calling the connection.search() function.\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    … # Rest of the class\n    @classmethod\n    def search(cls: Type[T], query_vector: list, limit: int = 10, \n**kwargs) -> list[T]:\n        try:\n            documents = cls._search(query_vector=query_vector, \nlimit=limit, **kwargs)\n        except exceptions.UnexpectedResponse:\n            logger.error(f\"Failed to search documents in '{cls.get_\ncollection_name()}'.\")\n            documents = []\n        return documents\n    @classmethod\n    def _search(cls: Type[T], query_vector: list, limit: int = 10, \n**kwargs) -> list[T]:\n        collection_name = cls.get_collection_name()\n        records = connection.search(\n            collection_name=collection_name,\n            query_vector=query_vector,\n            limit=limit,\n            with_payload=kwargs.pop(\"with_payload\", True),\n            with_vectors=kwargs.pop(\"with_vectors\", False),\n\n\nRAG Feature Pipeline\n160\n            **kwargs,\n        )\n        documents = [cls.from_record(record) for record in records]\n        return documents\nNow that we understand what our domain entities look like and how the OVM works, let’s move \non to the dispatchers who clean, chunk, and embed the documents.\nThe dispatcher layer\nA dispatcher inputs a document and applies dedicated handlers based on its data category (article, \npost, or repository). A handler can either clean, chunk, or embed a document.\nLet’s start by zooming in on the CleaningDispatcher. It mainly implements a dispatch() method \nthat inputs a raw document. Based on its data category, it instantiates and calls a handler that \napplies the cleaning logic specific to that data point:\nclass CleaningDispatcher:\n    cleaning_factory = CleaningHandlerFactory()\n    @classmethod\n    def dispatch(cls, data_model: NoSQLBaseDocument) -> \nVectorBaseDocument:\n        data_category = DataCategory(data_model.get_collection_name())\n        handler = cls.cleaning_factory.create_handler(data_category)\n        clean_model = handler.clean(data_model)\n        logger.info(\n            \"Data cleaned successfully.\",\n            data_category=data_category,\n            cleaned_content_len=len(clean_model.content),\n        )\n        return clean_model\nThe key in the dispatcher logic is the CleaningHandlerFactory(), which instantiates a different \ncleaning handler based on the document’s data category:\nclass CleaningHandlerFactory:\n    @staticmethod\n\n\nChapter 4\n161\n    def create_handler(data_category: DataCategory) -> \nCleaningDataHandler:\n        if data_category == DataCategory.POSTS:\n            return PostCleaningHandler()\n        elif data_category == DataCategory.ARTICLES:\n            return ArticleCleaningHandler()\n        elif data_category == DataCategory.REPOSITORIES:\n            return RepositoryCleaningHandler()\n        else:\n            raise ValueError(\"Unsupported data type\")\nThe Dispatcher or Factory classes are nothing fancy, but they offer an intuitive and simple interface \nfor applying various operations to your documents. When manipulating documents, instead of \nworrying about their data category and polluting your business logic with if-else statements, you \nhave a class dedicated to handling that. You have a single class that cleans any document, which \nrespects the DRY (don’t repeat yourself) principles from software engineering. By respecting DRY, \nyou have a single point of failure, and the code can easily be extended. For example, if we add \nan extra type, we must extend only the Factory class instead of multiple occurrences in the code.\nThe ChunkingDispatcher and EmbeddingDispatcher follow the same pattern. They use a \nChunkingHandlerFactory and, respectively, an EmbeddingHandlerFactory that initializes the \ncorrect handler based on the data category of the input document. Afterward, they call the han-\ndler and return the result.\nThe Factory class leverages theabstract factory creational pattern (https://refactoring.guru/\ndesign-patterns/abstract-factory), which instantiates a family of classes implementing the \nsame interface. In our case, these handlers implement the clean() method regardless of the \nhandler type. \nAlso, the Handler class family leverages the strategy behavioral pattern (https://refactoring.\nguru/design-patterns/strategy) used to instantiate when you want to use different variants of \nan algorithm within an object and be able to switch from one algorithm to another during runtime.\nThe source code of all the dispatchers and factories can be found on GitHub at \nhttps://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/ \nllm_engineering/application/preprocessing/dispatchers.py\n\n\nRAG Feature Pipeline\n162\nIntuitively, in our dispatcher layer, the combination of the factory and strategy patterns works \nas follows:\n1. \nInitially, we knew we wanted to clean the data, but as we knew the data category only at \nruntime, we couldn’t decide on what strategy to apply.\n2. We can write the whole code around the cleaning code and abstract away the logic under \na Handler() interface, which will represent our strategy.\n3. \nWhen we get a data point, we apply the abstract factory pattern and create the correct \ncleaning handler for its data type.\n4. \nUltimately, the dispatcher layer uses the handler and executes the right strategy.\nBy doing so, we:\n•\nIsolate the logic for a given data category.\n•\nLeverage polymorphism to avoid filling up the code with hundreds of if-else statements.\n•\nMake the code modular and extendable. When a new data category arrives, we must \nimplement a new handler and modify the Factory class without touching any other part \nof the code.\nThe last component of the RAG feature pipeline is the implementation of the cleaning, chunking, \nand embedding handlers.\nThe handlers\nThe handler has a one-on-one structure with our domain, meaning that every entity has its own \nhandler, as shown in Figure 8.17. In total, we will have nine Handler classes that follow the next \nbase interfaces:\n•\nclass CleaningDataHandler()\n•\nclass ChunkingDataHandler()\n•\nclass EmbeddingDataHandler()\nUntil now, we have just modeled our entities and how the data flows in our appli-\ncation. We haven’t written a single piece of cleaning, chunking, or embedding code. \nThat is one big difference between a quick demo and a production-ready application. \nIn a demo, you don’t care about software engineering best practices and structuring \nyour code to make it future-proof. However, writing clean, modular, and scalable \ncode is critical for its longevity when building a real-world application.\n\n\nChapter 4\n163\nFigure 4.17: Handler class hierarchy and their interaction\nLet’s examine each handler family and see how it is implemented.\nThe cleaning handlers\nThe CleaningDataHandler() strategy interface looks as follows:\n… # Other imports.\nfrom typing import Generic, TypeVar\nDocumentT = TypeVar(\"DocumentT\", bound=Document)\nCleanedDocumentT = TypeVar(\"CleanedDocumentT\", bound=CleanedDocument)\nclass CleaningDataHandler(ABC, Generic[DocumentT, CleanedDocumentT]):\nThe code for all the handlers is available on GitHub at https://github.com/\nPacktPublishing/LLM-Engineering/tree/main/llm_engineering/ \napplication/preprocessing.\n\n\nRAG Feature Pipeline\n164\n    @abstractmethod\n    def clean(self, data_model: DocumentT) -> CleanedDocumentT:\n        pass\nNow, for every post, article and repository, we have to implement a different handler, as follows:\nclass PostCleaningHandler(CleaningDataHandler):\n    def clean(self, data_model: PostDocument) -> CleanedPostDocument:\n        return CleanedPostDocument(\n            id=data_model.id,\n            content=clean_text(\" #### \".join(data_model.content.\nvalues())),\n            … # Copy the rest of the parameters from the data_model \nobject.\n        )\nclass ArticleCleaningHandler(CleaningDataHandler):\n    def clean(self, data_model: ArticleDocument) -> \nCleanedArticleDocument:\n        valid_content = [content for content in data_model.content.\nvalues() if content]\n        return CleanedArticleDocument(\n            id=data_model.id,\n            content=clean_text(\" #### \".join(valid_content)),\n            platform=data_model.platform,\n            link=data_model.link,\n            author_id=data_model.author_id,\n            author_full_name=data_model.author_full_name,\n        )\nclass RepositoryCleaningHandler(CleaningDataHandler):\n    def clean(self, data_model: RepositoryDocument) -> \nCleanedRepositoryDocument:\n        return CleanedRepositoryDocument(\n            id=data_model.id,\n",
      "page_number": 180
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 189-197)",
      "start_page": 189,
      "end_page": 197,
      "detection_method": "topic_boundary",
      "content": "Chapter 4\n165\n            content=clean_text(\" #### \".join(data_model.content.\nvalues())),\n            … # Copy the rest of the parameters from the data_model \nobject.\n        )\nThe handlers input a raw document domain entity, clean the content, and return a cleaned docu-\nment. All the handlers use the clean_text() function to clean the text. Out of simplicity, we used \nthe same cleaning technique for all the data categories. Still, in a real-world setup, we would have \nto further optimize and create a different cleaning function for each data category. The strategy \npattern makes this a breeze, as we swap the cleaning function in the handlers, and that’s it.\nThe cleaning steps applied in the clean_text() function are the same ones discussed in Chapter 5\nin the Creating an instruction dataset section. We don’t want to repeat ourselves. Thus, for a re-\nfresher, check out that chapter. At this point, we mostly care about automating and integrating \nthe whole logic into the RAG feature pipeline. Thus, after operationalizing the ML system, all the \ncleaned data used for fine-tuning will be accessed from the logical feature store, making it the \nsingle source of truth for accessing data.\nThe chunking handlers\nFirst, let’s examine the ChunkingDataHandler() strategy handler. We exposed the metadata dic-\ntionary as a property to aggregate all the necessary properties required for chunking in a single \nstructure. By structuring it like this, we can easily log everything to ZenML to track and debug our \nchunking logic. The handler takes cleaned documents as input and returns chunk entities. All the \nhandlers can be found on GitHub at https://github.com/PacktPublishing/LLM-Engineering/\ntree/main/llm_engineering/application/preprocessing.\n… # Other imports.\nfrom typing import Generic, TypeVar\nCleanedDocumentT = TypeVar(\"CleanedDocumentT\", bound=CleanedDocument)\nChunkT = TypeVar(\"ChunkT\", bound=Chunk)\nclass ChunkingDataHandler(ABC, Generic[CleanedDocumentT, ChunkT]):\n    @property\n    def metadata(self) -> dict:\n        return {\n\n\nRAG Feature Pipeline\n166\n            \"chunk_size\": 500,\n            \"chunk_overlap\": 50,\n        }\n    @abstractmethod\n    def chunk(self, data_model: CleanedDocumentT) -> list[ChunkT]:\n        pass\nLet’s understand how the ArticleChunkingHandler() class is implemented. The first step is to \noverride the metadata property and customize the type of properties the chunking logic requires. \nFor example, when working with articles, we are interested in the chunk’s minimum and max-\nimum length.\nThe handler’s chunk() method inputs cleaned article documents and returns a list of article chunk \nentities. It uses the chunk_text() function to split the cleaned content into chunks. The chunking \nfunction is customized based on the min_length and max_length metadata fields. The chunk_id \nis computed as the MD5 hash of the chunk’s content. Thus, if the two chunks have precisely the \nsame content, they will have the same ID, and we can easily deduplicate them. Lastly, we create \na list of chunk entities and return them.\nclass ArticleChunkingHandler(ChunkingDataHandler):\n    @property\n    def metadata(self) -> dict:\n        return {\n            \"min_length\": 1000,\n            \"max_length\": 1000,\n        }\n    def chunk(self, data_model: CleanedArticleDocument) -> \nlist[ArticleChunk]:\n        data_models_list = []\n        cleaned_content = data_model.content\n        chunks = chunk_article(\n            cleaned_content, min_length=self.metadata[\"min_length\"], max_\nlength=self.metadata[\"max_length\"]\n        )\n        for chunk in chunks:\n\n\nChapter 4\n167\n            chunk_id = hashlib.md5(chunk.encode()).hexdigest()\n            model = ArticleChunk(\n                id=UUID(chunk_id, version=4),\n                content=chunk,\n                platform=data_model.platform,\n                link=data_model.link,\n                document_id=data_model.id,\n                author_id=data_model.author_id,\n                author_full_name=data_model.author_full_name,\n                metadata=self.metadata,\n            )\n            data_models_list.append(model)\n        return data_models_list\nThe last step is to dig into the chunk_article() function, which mainly does two things:\n•\nIt uses a regex to find all the sentences within the given text by looking for periods, ques-\ntion marks, or exclamation points followed by a space. However, it avoids splitting into \ncases where the punctuation is part of an abbreviation or initialism (like “e.g.\" or “Dr.\")\n•\nIt groups sentences into a single chunk until the max_length limit is reached. When the \nmaximum size is reached, and the chunk size is bigger than the minimum allowed value, \nit is added to the final list the function returns.\ndef chunk_article(text: str, min_length: int, max_length: int) -> \nlist[str]:\n    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\", \ntext)\n    extracts = []\n    current_chunk = \"\"\n    for sentence in sentences:\n        sentence = sentence.strip()\n        if not sentence:\n            continue\n        if len(current_chunk) + len(sentence) <= max_length:\n            current_chunk += sentence + \" \"\n        else:\n\n\nRAG Feature Pipeline\n168\n            if len(current_chunk) >= min_length:\n                extracts.append(current_chunk.strip())\n            current_chunk = sentence + \" \"\n    if len(current_chunk) >= min_length:\n        extracts.append(current_chunk.strip())\n    return extracts\nThe PostChunkingHandler and RepositoryChunkingHandler, available on GitHub at llm_\nengineering/application/preprocessing/chunking_data_handlers.py, have a similar struc-\nture to the ArticleChunkingHandler. However, they use a more generic chunking function called \nchunk_text(), worth looking into. The chunk_text() function is a two-step process that has \nthe following logic:\n1. \nIt uses a RecursiveCharacterTextSplitter() from LangChain to split the text based on \na given separator or chunk size. Using the separator, we first try to find paragraphs in the \ngiven text, but if there are no paragraphs or they are too long, we cut it at a given chunk size.\n2. \nNotice that we want to ensure that the chunk doesn’t exceed the maximum input length \nof the embedding model. Thus, we pass all the chunks created above into a SenteceTrans\nformersTokenTextSplitter(), which considers the maximum input length of the model. \nAt this point, we also apply the chunk_overlap logic, as we want to do it only after we \nvalidate that the chunk is small enough.\n… # Other imports.\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, \nSentenceTransformersTokenTextSplitter\nfrom llm_engineering.application.networks import \nEmbeddingModelSingleton\ndef chunk_text(text: str, chunk_size: int = 500, chunk_overlap: int \n= 50) -> list[str]:\n    character_splitter = RecursiveCharacterTextSplitter(separato\nrs=[\"\\n\\n\"], chunk_size=chunk_size, chunk_overlap=0)\n    text_split_by_characters = character_splitter.split_text(text)\n    token_splitter = SentenceTransformersTokenTextSplitter(\n        chunk_overlap=chunk_overlap,\n\n\nChapter 4\n169\n        tokens_per_chunk=embedding_model.max_input_length,\n        model_name=embedding_model.model_id,\n    )\n    chunks_by_tokens = []\n    for section in text_split_by_characters:\n        chunks_by_tokens.extend(token_splitter.split_text(section))\n    return chunks_by_tokens\nTo conclude, the function above returns a list of chunks that respect both the provided chunk \nparameters and the embedding model’s max input length.\nThe embedding handlers\nThe embedding handlers differ slightly from the others as the EmbeddingDataHandler() interface \ncontains most of the logic. We took this approach because, when calling the embedding model, \nwe want to batch as many samples as possible to optimize the inference process. When running \nthe model on a GPU, the batched samples are processed independently and in parallel. Thus, by \nbatching the chunks, we can optimize the inference process by 10x or more, depending on the \nbatch size and hardware we use.\nWe implemented an embed() method, in case you want to run the inference on a single data point, \nand an embed_batch() method. The embed_batch() method takes chunked documents as input, \ngathers their content into a list, passes them to the embedding model, and maps the results to an \nembedded chunk domain entity. The mapping is done through the map_model() abstract method, \nwhich has to be customized for every data category.\n… # Other imports.\nfrom typing import Generic, TypeVar, cast\nfrom llm_engineering.application.networks import EmbeddingModelSingleton\nChunkT = TypeVar(\"ChunkT\", bound=Chunk)\nEmbeddedChunkT = TypeVar(\"EmbeddedChunkT\", bound=EmbeddedChunk)\nembedding_model = EmbeddingModelSingleton()\nclass EmbeddingDataHandler(ABC, Generic[ChunkT, EmbeddedChunkT]):\n    \"\"\"\n    Abstract class for all embedding data handlers.\n\n\nRAG Feature Pipeline\n170\n    All data transformations logic for the embedding step is done here\n    \"\"\"\n    def embed(self, data_model: ChunkT) -> EmbeddedChunkT:\n        return self.embed_batch([data_model])[0]\n    def embed_batch(self, data_model: list[ChunkT]) -> \nlist[EmbeddedChunkT]:\n        embedding_model_input = [data_model.content for data_model in \ndata_model]\n        embeddings = embedding_model(embedding_model_input, to_list=True)\n        embedded_chunk = [\n            self.map_model(data_model, cast(list[float], embedding))\n            for data_model, embedding in zip(data_model, embeddings, \nstrict=False)\n        ]\n        return embedded_chunk\n    @abstractmethod\n    def map_model(self, data_model: ChunkT, embedding: list[float]) -> \nEmbeddedChunkT:\n        pass\nLet’s look only at the implementation of the ArticleEmbeddingHandler(), as the other handlers \nare highly similar. As you can see, we only have to implement the map_model() method, which \ntakes a chunk of input and computes the embeddings in batch mode. Its scope is to map this \ninformation to an EmbeddedArticleChunk Pydantic entity.\nclass ArticleEmbeddingHandler(EmbeddingDataHandler):\n    def map_model(self, data_model: ArticleChunk, embedding: list[float]) \n-> EmbeddedArticleChunk:\n        return EmbeddedArticleChunk(\n            id=data_model.id,\n            content=data_model.content,\n            embedding=embedding,\n            platform=data_model.platform,\n            link=data_model.link,\n\n\nChapter 4\n171\n            document_id=data_model.document_id,\n            author_id=data_model.author_id,\n            author_full_name=data_model.author_full_name,\n            metadata={\n                \"embedding_model_id\": embedding_model.model_id,\n                \"embedding_size\": embedding_model.embedding_size,\n                \"max_input_length\": embedding_model.max_input_length,\n            },\n        )\nThe last step is to understand how the EmbeddingModelSingleton() works. It is a wrapper over \nthe SentenceTransformer() class from Sentence Transformers that initializes the embedding \nmodel. Writing a wrapper over external packages is often good practice. Thus, when you want \nto change the third-party tool, you have to modify only the internal logic of the wrapper instead \nof the whole code base.\nThe SentenceTransformer() class is initialized with the model_id defined in the Settings class, \nallowing us to quickly test multiple embedding models just by changing the configuration file \nand not the code. That is why I am not insisting at all on what embedding model to use. This \ndiffers constantly based on your use case, data, hardware, and latency. But by writing a generic \nclass, which can quickly be configured, you can experiment with multiple embedding models \nuntil you find the best one for you.\nfrom sentence_transformers.SentenceTransformer import SentenceTransformer\nfrom llm_engineering.settings import settings\nfrom .base import SingletonMeta\nclass EmbeddingModelSingleton(metaclass=SingletonMeta):\n    def __init__(\n        self,\n        model_id: str = settings.TEXT_EMBEDDING_MODEL_ID,\n        device: str = settings.RAG_MODEL_DEVICE,\n        cache_dir: Optional[Path] = None,\n    ) -> None:\n        self._model_id = model_id\n        self._device = device\n        self._model = SentenceTransformer(\n\n\nRAG Feature Pipeline\n172\n            self._model_id,\n            device=self._device,\n            cache_folder=str(cache_dir) if cache_dir else None,\n        )\n        self._model.eval()\n    @property\n    def model_id(self) -> str:\n        return self._model_id\n    @cached_property\n    def embedding_size(self) -> int:\n        dummy_embedding = self._model.encode(\"\")\n        return dummy_embedding.shape[0]\n    @property\n    def max_input_length(self) -> int:\n        return self._model.max_seq_length\n    @property\n    def tokenizer(self) -> AutoTokenizer:\n        return self._model.tokenizer\n    def __call__(\n        self, input_text: str | list[str], to_list: bool = True\n    ) -> NDArray[np.float32] | list[float] | list[list[float]]:\n        try:\n            embeddings = self._model.encode(input_text)\n        except Exception:\n            logger.error(f\"Error generating embeddings for {self._model_\nid=} and {input_text=}\")\n            return [] if to_list else np.array([])\n        if to_list:\n\n\nChapter 4\n173\n            embeddings = embeddings.tolist()\n        return embeddings\nThe embedding model class implements the singleton pattern (https://refactoring.guru/\ndesign-patterns/singleton), a creational design pattern that ensures a class has only one instance \nwhile providing a global access point to this instance. The EmbeddingModelSingleton() class inher-\nits from the SingletonMeta class, which ensures that whenever an EmbeddingModelSingleton()\nis instantiated, it returns the same instance. This works well with ML models, as you load them \nonce in memory through the singleton pattern, and afterward, you can use them anywhere in the \ncode base. Otherwise, you risk loading the model in memory every time you use it or loading it \nmultiple times, resulting in memory issues. Also, this makes it very convenient to access properties \nsuch as embedding_size, where you have to make a dummy forward pass into the embedding \nmodel to find the size of its output. As a singleton, you do this forward pass only once, and then \nyou have it accessible all the time during the program’s execution.\nSummary\nThis chapter began with a soft introduction to RAG and why and when you should use it. We \nalso understood how embeddings and vector DBs work, representing the cornerstone of any \nRAG system. Then, we looked into advanced RAG and why we need it in the first place. We built \na strong understanding of what parts of the RAG can be optimized and proposed some popular \nadvanced RAG techniques for working with textual data. Next, we applied everything we learned \nabout RAG to designing the architecture of LLM Twin’s RAG feature pipeline. We also understood \nthe difference between a batch and streaming pipeline and presented a short introduction to the \nCDC pattern, which helps sync two DBs.\nUltimately, we went step-by-step into the implementation of the LLM Twin’s RAG feature pipeline, \nwhere we saw how to integrate ZenML as an orchestrator, how to design the domain entities of \nthe application, and how to implement an OVM module. Also, we understood how to apply some \nsoftware engineering best practices, such as the abstract factory and strategy software patterns, \nto implement a modular and extendable layer that applies different cleaning, chunking, and \nembedding techniques based on the data category of each document.\nThis chapter focused only on implementing the ingestion pipeline, which is just one component \nof a standard RAG application. In Chapter 9, we will conclude the RAG system by implementing \nthe retrieval and generation components and integrating them into the inference pipeline. But \nfirst, in the next chapter, we will explore how to generate a custom dataset using the data we \ncollected and fine-tune an LLM with it. \n",
      "page_number": 189
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 198-207)",
      "start_page": 198,
      "end_page": 207,
      "detection_method": "topic_boundary",
      "content": "RAG Feature Pipeline\n174\nReferences\n•\nKenton, J.D.M.W.C. and Toutanova, L.K., 2019, June. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of naacL-HLT (Vol. 1, p. 2).\n•\nLiu, Y., 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arX-\niv:1907.11692.\n•\nMikolov, T., 2013. Efficient estimation of word representations in vector space. arXiv pre-\nprint arXiv:1301.3781.\n•\nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vec-\ntors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods \nin Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar. Association for \nComputational Linguistics.\n•\nHe, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In \nProceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n•\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., \nMishkin, P., Clark, J. and Krueger, G., 2021, July. Learning transferable visual models from \nnatural language supervision. In International conference on machine learning (pp. 8748-\n8763). PMLR.\n• \nWhat is Change Data Capture (CDC)? | Confluent. (n.d.). Confluent. https://www.confluent.\nio/en-gb/learn/change-data-capture/\n• \nRefactoring.Guru. (2024, January 1). Singleton. https://refactoring.guru/design-\npatterns/singleton\n• \nRefactoring.Guru. (2024b, January 1). Strategy. https://refactoring.guru/design-\npatterns/strategy\n• \nRefactoring.Guru. (2024a, January 1). Abstract Factory. https://refactoring.guru/\ndesign-patterns/abstract-factory\n• \nSchwaber-Cohen, R. (n.d.). What is a Vector Database & How Does it Work? Use Cases + Ex-\namples. Pinecone. https://www.pinecone.io/learn/vector-database/\n•\nMonigatti, L. (2024, February 19). Advanced Retrieval-Augmented Generation: From Theory \nto LlaMaIndex Implementation. Medium. https://towardsdatascience.com/advanced-\nretrieval-augmented-generation-from-theory-to-llamaindex-implementation-\n4de1464a9930\n•\nMonigatti, L. (2023, December 6). A guide on 12 tuning Strategies for Production-Ready \nRAG applications. Medium. https://towardsdatascience.com/a-guide-on-12-tuning-\nstrategies-for-production-ready-rag-applications-7ca646833439\n\n\nChapter 4\n175\n•\nMonigatti, L. (2024b, February 19). Advanced Retrieval-Augmented Generation: From Theory \nto LlaMaIndex Implementation. Medium. https://towardsdatascience.com/advanced-\nretrieval-augmented-generation-from-theory-to-llamaindex-implementation-\n4de1464a9930\n•\nMaameri, S. (2024, May 10). Routing in RAG-Driven applications - towards data science. \nMedium. https://towardsdatascience.com/routing-in-rag-driven-applications-\na685460a7220\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n\n\n5\nSupervised Fine-Tuning\nSupervised Fine-Tuning (SFT) is a crucial step in preparing LLMs for real-world applications. Fol-\nlowing the initial pre-training phase, where an LLM learns to predict the next token in a sequence, \nSFT refines the model’s capabilities using carefully curated pairs of instructions and correspond-\ning answers. This process serves two primary purposes: it teaches the model to understand and \nfollow a specific chat format, effectively transforming it into a conversational agent, and it allows \nthe model to adapt its broad knowledge base to excel in targeted tasks or specialized domains.\nThe importance of SFT lies in its ability to bridge the gap between a model’s general language \nunderstanding and its practical utility. By exposing the model to examples of desired input-output \npatterns, SFT shapes the LLM’s behavior to align with specific goals, whether they involve task \ncompletion (such as summarization or translation) or domain expertise (like medical or legal \nknowledge). This tailored approach not only enhances the model’s performance in intended ar-\neas but also improves its ability to follow instructions and generate more relevant and coherent \nresponses.\nIn this chapter, we will cover the following topics:\n•\nCreating a high-quality instruction dataset\n•\nSFT techniques\n•\nImplementing fine-tuning in practice\nBy the end of this chapter, you will be able to create your own instruction datasets and efficiently \nfine-tune LLMs on them.\n\n\nSupervised Fine-Tuning\n178\nCreating an instruction dataset\nIn most use cases, creating an instruction dataset is the most difficult part of the fine-tuning \nprocess. This is due to multiple factors. Most use cases can be connected to raw text, but it is rare \nto find natural pairs of instructions and answers. This raw text needs to be transformed into a for-\nmat that includes both instructions and answers. Moreover, the quality of the data is also crucial. \nBecause of this, a lot of time is invested in manually checking and verifying individual samples. \nThis careful review helps ensure that the dataset is accurate and useful for training the model.\nFigure 5.1 – Overview of the post-training data pipeline covered in this chapter\nIn this section, we will introduce a general framework to create your own instruction datasets, \nregardless of the final use case. We will then leverage the scraped data from Chapter 3 and trans-\nform it into an instruction dataset. The different stages in our data generation pipeline are sum-\nmarized in Figure 5.1.\nGeneral framework\nInstruction datasets are defined as pairs of instructions and answers. The instructions are the \ninputs of the model, used as context during fine-tuning. The answers are the expected outputs of \nthe model. During fine-tuning, you can choose to train the model on the instructions and answers, \nor on answers only. Pairs of instructions and answers follow a certain template. Some instruction \ntemplates, such as Alpaca, introduce additional fields like inputs and system. Both of them can \nbe considered subfields of the instruction field. In this case, “inputs” contain the data the model \nneeds to complete the instruction, and “system” is a meta-prompt to steer the general behavior \nof the model. Here is an example from the SlimOrca dataset, with “system” and “instruction”:\nAll the code examples from this chapter can be found on GitHub at https://github.\ncom/PacktPublishing/LLM-Engineering.\n\n\nChapter 5\n179\nSystem\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a \nfive year old.\nInstruction\nConcepts: building, shop, town\nWrite a sentence that includes all these words.\nOutput\nIn our little town, there is a shop inside a big building where people go to buy their favorite toys \nand candies.\nTable 5.1 – Example of sample from the Open-Orca/SlimOrca dataset\nThis example illustrates how the “system” field is used to define specific behaviors for the model, \nsuch as being helpful, always providing explanations, and tailoring responses as if speaking to \na five-year-old. The “instruction” field provides the necessary data (the concepts) and the task \n(constructing a sentence). The output field shows the expected answer, which, while not the only \npossible answer, represents a high-quality response.\nTo build an instruction dataset, we want to curate data that is representative of how the model will \nbe used. Once we have gathered enough samples, our goal is to filter them to only keep high-quality \ndata. In this context, high-quality data can be described through three main dimensions:\n• \nAccuracy: It refers to the factual correctness and relevance of the samples. In the context \nof instruction datasets, this means ensuring that responses are not only factually accu-\nrate but also relevant to their corresponding instructions. High accuracy is essential for \ntraining models that can provide reliable and trustworthy information.\n•\nDiversity: A high-quality dataset should encompass a wide range of use cases, covering \nthe potential queries and tasks the deployed LLM might encounter. This diversity should \nspan topics, contexts, text lengths, and writing styles. By sampling data in a representative \nmanner, we allow models to develop robust instruction-following capabilities.\n•\nComplexity: Trivial or overly simplistic samples do little to improve an LLM’s capabilities. \nInstead, datasets should include complex, multi-step reasoning problems and challenging \ntasks that push the boundaries of what the model is expected to handle. This complexity \nhelps in developing models capable of tackling complex real-world problems.\n\n\nSupervised Fine-Tuning\n180\nIn the following sections, we will see techniques to filter and evaluate instruction samples ac-\ncording to these dimensions.\nData quantity\nThe Hugging Face Hub contains numerous instruction datasets, which can be general-purpose or \ndesigned for particular tasks or domains. When working on a new use case, it can be beneficial to \nlook for related open-source datasets to leverage for fine-tuning. This is particularly important \nif your number of samples is too low (for example, fewer than 1,000), requiring you to augment \nit with high-quality data.\nFigure 5.2 – Screenshot of the most-liked datasets on the Hugging Face Hub\nCalculating an ideal number of samples is a difficult task, as both the quality of the data and the \nsize of the model can have a dramatic impact. For large models (around 70 billion parameters, \nfor example), this number can be as low as 1,000 high-quality samples (see the LIMA paper in \nthe References section). This is not true for smaller models (around seven billion parameters, for \ninstance), as they need more samples to simply learn the correct chat template. In any case, the \nquality of the data is a crucial factor, and a high number of samples is always desirable.\nTo provide additional numbers, we can look at the fine-tuned models developed by companies \nand the open-source community. We can distinguish two types of finetunes: general-purpose, \naimed to reproduce the capabilities of models like GPT, and task- or domain-specific models, \ndesigned to optimize their performance for a particular application.\n\n\nChapter 5\n181\nGeneral-purpose models cover more topics, which requires additional samples. Among com-\npanies, we observe a wide range of values. For instance, Yi models from 01-ai rely on less than \n10,000 samples. At the opposite range of the spectrum, Meta reported using 10 million samples \nfor Llama 3 through the entire fine-tuning process (including preference alignment). In the open-\nsource community, models like OpenHermes and Dolphin use around one million samples. Based \non the quality of these finetunes, we recommend an instruction dataset of at least one million \nsamples to create a good general-purpose instruct model. On the other hand, models fine-tuned \nfor a specific purpose require fewer samples. Here, we differentiate task-specific models from \ndomain-specific ones.\nTask-specific and domain-specific models represent two distinct approaches to fine-tuning LLMs. \nTask-specific models are designed to excel at a particular function, such as translation, summari-\nzation, or sentiment analysis. These models benefit from a focused training approach on a single \ntask, allowing for efficient performance even with smaller model sizes (typically less than 8 bil-\nlion parameters). The data required for task-specific fine-tuning is generally more manageable, \nranging from 100 to 100,000 samples. This makes task-specific fine-tuning an attractive option\nfor many applications where resources may be limited.\nDomain-specific models, on the other hand, aim to tweak the LLM with specialized knowledge \nand familiarity with the vocabulary and linguistic patterns of a particular field. These models \nare valuable in areas such as medicine, law, finance, e-commerce, engineering, and hospitality. \nThe data requirements for domain-specific fine-tuning can vary widely depending on the com-\nplexity and breadth of the domain. Some fields, like medicine or law, may require as much data \nas general-purpose fine-tuning due to their vast technical corpora. Others, such as e-commerce \nor hospitality, might need fewer samples, more in line with task-specific fine-tuning.\nThe key factors determining the data needs for domain-specific models are the “size” of the \ndomain (i.e., the extent of its specialized knowledge and vocabulary) and the representation of \nthat domain in the model’s pre-training data. Domains that are well-represented in the original \ntraining data may require less fine-tuning, while those that are more specialized or underrep-\nresented may need more extensive datasets. Even with open-source LLMs, many pre-training \ndatasets are closed-source, which requires making educated guesses to determine their compo-\nsition (e.g., 30% code or 20% math).\n\n\nSupervised Fine-Tuning\n182\nData curation\nWhen it comes to procuring data for fine-tuning, the approaches differ between task-specific and \ndomain-specific models. For task-specific models, data curation often involves collecting examples \nof the desired task from existing datasets or creating new ones. This might involve gathering pairs \nof original and summarized texts for a summarization model or collecting sentences in different \nlanguages for a translation model.\nDomain-specific data curation can be more challenging. It often requires collaboration with sub-\nject matter experts to gather and validate relevant texts, research papers, technical documents, \nand other domain-specific content. In some cases, it may involve partnering with organizations \nor institutions that have access to large repositories of specialized information. The quality and \nrelevance of this data is crucial, as it directly impacts the model’s ability to understand and gen-\nerate content in the target domain.\nIt’s worth noting that few-shot prompting has emerged as an alternative strategy to fine-tuning, \nespecially for task-specific applications. This approach leverages the capabilities of large, pow-\nerful models by providing a few examples of the desired task within the input prompt. While \nnot a replacement for fine-tuning in all scenarios (e.g., when you want to learn a new domain), \nfew-shot prompting can be an efficient way to adapt models to new tasks without the need for \nextensive additional training.\nIn practice, the line between task-specific and domain-specific models can sometimes blur. For \ninstance, a model fine-tuned for medical diagnosis could be considered both task-specific (focused \non diagnosis) and domain-specific (specialized in medical knowledge). The key is to understand \nthe primary goal of the fine-tuning process and tailor the approach accordingly.\nAt this point in the process, we should have a collection of datasets suited for our use case. The \nnext step consists of refining the quality of the samples through rule-based filtering, data dupli-\ncation, data decontamination, and data quality evaluation.\nRule-based filtering\nRule-based filtering is a systematic approach to data quality control that relies on explicit, pre-\ndefined rules to evaluate and filter data samples. These rules are typically designed to address \ncommon quality issues and can range from simple checks to more complex logical operations. The \nprimary goal of rule-based filtering is to maintain a high standard of data quality by removing \nsamples that do not meet specific criteria.\n\n\nChapter 5\n183\nLength filtering is a straightforward yet effective rule-based filtering technique. This meth-\nod involves setting thresholds for the acceptable length of responses in the dataset. Extremely \nshort responses often lack sufficient information to be meaningful, while excessively long ones \nmay contain irrelevant or redundant content. It’s important to note that the appropriate length \nthresholds can vary significantly depending on the specific task and domain. For example, a \ndataset for generating concise summaries might have a lower maximum threshold compared to \none for detailed explanations.\nKeyword exclusion is another powerful rule-based filtering technique that focuses on the content \nof the samples rather than their structure. This method involves creating a list of keywords or \nphrases associated with low-quality or inappropriate content, and then filtering out any samples \nthat contain these terms. The keyword list can include obvious indicators of low quality, such \nas profanities or spam-related terms, as well as domain-specific words that might indicate ir-\nrelevant or off-topic content. For instance, in a dataset for a professional writing assistant, you \nmight exclude samples containing slang terms or informal expressions that don’t align with the \nintended tone and style.\nFormat checking is recommended for datasets that include structured data or follow specific \nformatting requirements. This technique ensures that all samples adhere to the expected format, \nmaintaining consistency and facilitating processing downstream. Format checking can be par-\nticularly important for datasets containing code samples, JSON structures, or other formatted \ntext. For example, in a dataset of programming instructions and solutions, you might implement \nrules to verify that code samples are syntactically correct and follow specified style guidelines.\nRule-based filtering offers significant advantages in preparing instruction datasets. Its speed \nand efficiency allow for rapid application to large volumes of data, making it highly scalable. The \nconsistency of rule application ensures uniform treatment of data, reducing human error and bias. \nFurthermore, the explicit definition of filtering criteria provides transparency and interpretability, \nfacilitating easy understanding, auditing, and adjustment. The ability to automate rule-based \nfiltering reduces the need for manual intervention and enables continuous data quality monitoring.\nHowever, rule-based filtering also has limitations that must be considered. Predefined rules may \nlack the nuance required to capture the full complexity of language and context, potentially lead-\ning to the removal of valid but unusual samples. The typically binary nature of rules (pass/fail) \nmay not always align with the nuanced nature of language and instruction quality. Additionally, \nas data patterns and quality standards evolve, rules need regular review and updates to remain \neffective. There’s also a risk that poorly designed rules could inadvertently introduce or amplify \nbiases in the dataset.\n\n\nSupervised Fine-Tuning\n184\nData deduplication\nDataset diversity is fundamental to training models that can generalize well to new, unseen data. \nWhen a dataset contains duplicates or near-duplicates, it can lead to several issues:\n• \nOverfitting: Models may memorize specific examples rather than learning general patterns.\n•\nBiased performance: Overrepresented data points may skew the model’s performance \ntowards certain types of inputs.\n•\nInefficient training: Redundant data can increase training time without providing addi-\ntional valuable information.\n•\nInflated evaluation metrics: Duplicate data in test sets may lead to overly optimistic per-\nformance estimates.\nTo deduplicate datasets, we distinguish between exact and fuzzy deduplication. Exact deduplica-\ntion removes identical samples through a straightforward process involving data normalization, \nhash generation, and duplicate removal. Data normalization standardizes the format of entries, \nsuch as converting text to lowercase. Hash generation then creates unique hashes for each entry \nusing algorithms like MD5 or SHA-256. These hashes are compared to find matches, and dupli-\ncates are removed, leaving only one instance of each. While effective for identical entries, exact \ndeduplication does not detect near-duplicates or semantically similar content, requiring more \nadvanced techniques for those cases.\nThe most popular approach to fuzzy deduplication is MinHash deduplication. Compared to \nother fuzzy techniques, it maintains high accuracy while significantly reducing computational \ncomplexity. MinHash operates by generating compact representations, or signatures, for each \ndata item. These signatures serve as fingerprints that capture the essence of the data while dras-\ntically reducing its dimensionality. In practice, MinHash transforms data items (such as text \ndocuments) into sets of shingles, applies multiple hash functions to these sets, and selects the \nminimum hash values to form signature vectors. These signatures can then be compared using \nsimilarity measures like Jaccard similarity to efficiently identify near-duplicates.\nIn addition to exact and fuzzy deduplication, semantic similarity takes a different approach by \nfocusing on the meaning of text for deduplication. This method involves converting words or \nentire samples into vector representations using various natural language processing techniques. \nWord embedding models such as Word2Vec, GloVe, and FastText transform individual words into \ndense vectors, capturing semantic relationships. \n",
      "page_number": 198
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 208-215)",
      "start_page": 208,
      "end_page": 215,
      "detection_method": "topic_boundary",
      "content": "Chapter 5\n185\nFor more context-aware representations, language models like BERT, sentence transformers, or \ncross-encoders can generate embeddings for entire sentences or documents. Once these vec-\ntor representations are obtained, deduplication can be performed by comparing the similarity \nbetween vectors. Common similarity measures include cosine similarity or Euclidean distance. \nSamples with high similarity scores above a predefined threshold can be considered duplicates. \nFor large datasets, clustering techniques may be applied to group similar vectors. Methods like \nK-means, DBSCAN, or hierarchical clustering can efficiently organize the vector space, allowing \nfor the identification of clusters that represent semantically similar content. Within each cluster, \na representative sample can be retained while others are marked as duplicates.\nData decontamination\nData decontamination is the process of ensuring that the training dataset does not contain samples \nthat are identical or highly similar to those in the evaluation or test sets. This step is important \nfor ensuring the quality of the model evaluation and preventing overfitting or memorization of \ntest data.\nData decontamination uses techniques from data deduplication. Exact matching can be used to \nremove any training samples that are identical to those in the evaluation sets. This can be done \nusing hash functions or direct string comparisons. Next, we can also use near-duplicate detection \nmethods to identify and remove training samples that are very similar to evaluation samples, \neven if they are not exactly the same. This often involves techniques like MinHash or computing \nsimilarity scores based on n-grams or embeddings.\nAnother aspect of data decontamination is filtering out samples that may have been derived from \nthe same source as evaluation data. This can involve checking for overlapping phrases, similar \nsentence structures, or common metadata. Practitioners may also use provenance tracking (source \nthe data they use) to identify and exclude data from specific sources that are known to be used \nin evaluation sets.\nA simple way to perform data decontamination is to add your evaluation set to the \ninstruction dataset during the data deduplication stage. In this case, we want to \nensure that we only remove samples from the instruction dataset, which can be \nimplemented in different ways (only filtering out the first duplicate, recording the \nindexes of the evaluation samples, etc.). Ideally, you can automatically add your \nevaluation sets in the data deduplication stage to fully automate this process. This \nis particularly efficient if you iterate over several versions of custom benchmarks.\n\n\nSupervised Fine-Tuning\n186\nData quality evaluation\nData quality evaluation is a critical aspect of machine learning, particularly for LLMs. The process \ninvolves assessing various characteristics of datasets, including accuracy, diversity, and complexity. \nWhile some aspects like mathematical accuracy can be easily verified using tools such as Python \ninterpreters, evaluating subjective or open-ended content remains challenging.\nTraditional methods of data quality assessment include human annotation, which generally \nprovides high accuracy but is resource-intensive. To address scalability issues, machine learning \ntechniques have been developed to automate the evaluation process. These include using LLMs \nas judges, reward models, and classifiers trained for quality prediction.\nThe LLM-as-a-judge strategy involves prompting LLMs to evaluate the quality of each sample. \nThis approach has become popular due to its flexibility and ease of use, though it does present \nsome challenges. Different LLMs have different levels of performance across tasks, and their \nevaluations often align more closely with those of non-experts. With domain-specific datasets, \nyou might want to use domain-specific models instead of better, general-purpose LLMs. Com-\nparative assessment methods (e.g., “Is answer A better than answer B?”) generally outperform \nabsolute scoring approaches (e.g., “Rate answer A between 1 and 4”), though both can be used \nat scale with sufficient prompt engineering. We recommend iterating through different prompts \nover a representative subset to manually verify the quality of the responses. Table 5.2 shows an \nexample of a custom prompt for a judge LLM.\n\n\nChapter 5\n187\nInstruction\nYou are a data quality evaluator. Your goal is to assess an instruction and its corresponding \nanswer, determining how effectively the answer addresses the given task.\nIn your evaluation, you will provide feedback detailing the strengths and weaknesses of the \nanswer, followed by a score on a scale of 1 to 4.\nA score of 1 means that the answer is terrible and irrelevant to the instruction.\nA score of 2 means that the answer is not helpful and misses important aspects of the \ninstruction.\nA score of 3 means that the answer is helpful but could be improved in terms of relevance, \naccuracy, and depth.\nA score of 4 means that the answer is excellent and fully addresses the task.\nProvide your evaluation as follows:\nFeedback: (strengths and weaknesses you find relevant)\nScore: (number between 1 and 4)\nTable 5.2 – Example of LLM-as-a-judge prompt for data quality evaluation\nLLM-as-a-judge is known to have several biases. First, it has a position bias in comparative scoring, \nwhere the LLM judge favors the first answer. This can be addressed by randomizing the order of \nanswers A and B. In addition, like humans, LLM judges favor long answers. Length normaliza-\ntion techniques can be applied to absolute scoring to mitigate this issue. Finally, LLM judges are \nknown to have intra-model favoritism, meaning that they prefer models from the same family \n(GPT-4o with GPT-4 and GPT-4o mini, for example). This can be addressed by using several \nmodels instead of a single one.\n\n\nSupervised Fine-Tuning\n188\nIn general, to improve evaluation reliability, strategies such as using multiple LLMs as a jury \nreduce bias and improve consistency. Leveraging a jury of smaller LLMs can also reduce costs \nwhile increasing accuracy and mitigating intra-model favoritism. For specific applications like \nchatbots, it’s advisable to aim for high agreement between LLM judges and human evaluators \n(around 80%). Simple grading scales (with few-shot prompting) and task-specific benchmarks \nare also recommended to ensure relevant and interpretable evaluations.\nReward models are another way to re-purpose LLMs for data quality evaluation. The term “reward \nmodel” comes from Reinforcement Learning from Human Feedback (RLHF, see Chapter 6). They \ncan be broadly defined as models that take an instruction and answer pair and return a score as \noutput. Generally, reward models are created by adding a linear head on top of a decoder-only \narchitecture like Gemma or Llama. They are then trained for this specific purpose, using either \nreinforcement learning or traditional fine-tuning. Figure 5.3 shows ArmoRM-Llama3-8B-v0.1’s \narchitecture, which adds regression and gating layers on top of a Llama 3 8B model. This model \noutputs multiple scores to target specific dimensions, such as helpfulness, correctness, coherence, \ncomplexity, and verbosity. This allows for a more fine-grained approach to data quality evaluation.\nFigure 5.3 – Architecture of RLHFlow/ArmoRM-Llama3-8B-v0.1, based on Llama 3 (Source: \nhttps://doi.org/10.48550/arXiv.2406.12845)\n\n\nChapter 5\n189\nThe Allen Institute for AI’s RewardBench leaderboard, hosted on Hugging Face (allenai/re-\nward-bench), is a good resource for comparing different reward models. It combines various \ntypes of reward models (generative, classifiers, DPO, etc.) and evaluates them on a curated set \nof chosen and rejected answers for each instruction. While this task is not directly related to in-\nstruction data quality, it is a good resource for finding models capable of differentiating between \ngood and bad answers.\nClassifiers or encoder-only models can be trained to perform data quality evaluation. A good \nexample is HuggingFaceFW/fineweb-edu-classifier, a classifier designed to judge the educational \nvalue of web pages. This model was designed as a quality filter for pretraining data but a similar \napproach can be taken to evaluate instruction samples at scale. In practice, fineweb-edu-classifier \nadds a classification head to an embedding model (Snowflake/snowflake-arctic-embed-m) and \ntrains it for 20 epochs on 450,000 samples that are annotated by Llama 3 70B Instruct.\nThis approach relies on encoder-only models, which are both smaller and better suited to classi-\nfication tasks. Thanks to their low number of parameters, these models are faster to run and can \nscale to millions of samples. However, they are not as accurate as bigger models, particularly for \ncomplex reasoning tasks where they lack the ability to capture nuances. At smaller scale, encod-\ner-only models are still valuable to filter out outliers or as part of an automated data pipeline, \nwhich requires faster processing.\nData exploration\nData exploration is a continuous process that requires practitioners to become familiar with the \ntraining data. It involves both manual inspection and automated analysis, each playing a crucial \nrole in understanding the dataset’s characteristics, strengths, and potential shortcomings.\nManual dataset exploration, though time-consuming, is an important step. It reveals errors and \ninconsistencies that automated processes might miss, including formatting issues, data entry \nmistakes, incoherent reasoning, and factual inaccuracies. This process provides qualitative insights \ninto the dataset’s content and style. To enhance efficiency, researchers can employ techniques \nlike stratified sampling (selecting diverse samples), systematic review (using a criteria checklist), \nand collaborative review (involving multiple reviewers). \n\n\nSupervised Fine-Tuning\n190\nFigure 5.4 shows an example with Argilla, a collaborative platform for manual data quality eval-\nuation and exploration.\nFigure 5.4 – Argilla’s interface for collaborative data quality evaluation and exploration\nStatistical analysis is a complementary technique that reveals vocabulary diversity, potential \nbiases, and concept representation. This process utilizes natural language processing libraries \nlike NLTK or spaCy for tokenization and analysis of large text volumes. Visualization tools such as \nMatplotlib or Seaborn create histograms and word clouds, enabling intuitive pattern recognition. \nThese techniques provide insights into dataset composition, language breadth, and possible \ncultural or contextual preferences, which can influence model outputs.\nTopic clustering automatically groups similar documents or pieces of text together, revealing \nunderlying themes and patterns within the data. This process is especially important for under-\nstanding the content of large text corpora, identifying trends, and organizing information in a \nmeaningful way. It is often associated with data visualization, with figures that show clusters \nof similar samples.\nLet’s consider the task of building an instruction dataset about various programming languages. \nYou have collected a vast corpus of programming-related text from online forums, documentation, \nand tutorials. First, topic clustering can help identify the distinct programming languages present \nin the dataset (Python, JavaScript, etc.). Second, within each language cluster, you can further \nidentify sub-topics like error handling, data structures, and web frameworks. This allows a \nbalanced representation of each language and sub-topic in the corpus. \n\n\nChapter 5\n191\nThis makes sure that each topic is correctly covered for each programming language.\nFigure 5.5 – Representation of the historical TikTok dataset made with Nomic Atlas\nSeveral tools are available for performing topic clustering, each with its own strengths and ap-\nproaches. For example, Hugging Face’s text-clustering provides a simple pipeline with sentence \ntransformers for embedding text into vector space, UMAP for dimensionality reduction, and \nDBSCAN for clustering. It also automatically labels clusters using an LLM and can output visu-\nalizations. Nomic Atlas (see Figure 5.5), BunkaTopics, and Lilac are alternatives proposing similar \napproaches with additional features.\nData generation\nWhen the available instruction datasets are not sufficient, creating custom data becomes necessary. \nThis is particularly relevant for specialized applications where publicly available data is scarce. \n\n\nSupervised Fine-Tuning\n192\nAdditionally, it serves as a method to augment underrepresented areas in a dataset, like insufficient \nexamples of JavaScript error-handling techniques in our previous example. While data can be \ngenerated manually by individuals or through crowdsourcing, these approaches often incur sig-\nnificant costs and time investments. Synthetic data generation using LLMs offers a more efficient \nand scalable alternative. This method, when combined with well-designed prompt engineering, \ncan produce high-quality data at a much larger scale, effectively addressing the limitations of \nmanual data creation processes.\nThe process of synthetic data generation typically begins with the preparation of a set of carefully \ndesigned prompts (sometimes called taxonomy). These serve as the foundation for generating \nnew, diverse examples. Five seed prompts used in the original Alpaca dataset can be seen in Table \n5.3. The quality of synthetically generated data largely depends on the prompts and techniques \nused in the generation process. Well-crafted prompts can guide the language model to produce \ndiverse, relevant, and high-quality instruction-response pairs. These prompts often include spe-\ncific instructions, examples, and constraints to ensure the generated data aligns with the desired \nformat and content.\nSeed instructions\n•\nIs there anything I can eat for breakfast that doesn’t include eggs, yet includes protein, \nand has roughly 700-1000 calories?\n•\nWhat is the relation between the given pairs? Input: Night : Day :: Right : Left\n•\nGenerate a one-sentence description for each of the following people. Input: -Barack \nObama\\n- Elon Musk\\n- Taylor Swift\n•\nDescribe a situation in which the given stereotype can harm you. Input: All Asians are \nsmart!\n•\nGenerate an appropriate subjective title for the following email: Input: “Hi [person \nname],\\n\\nI’m writing to ask you if you are happy to be a panelist in our workshop on \nmultimodality at CVPR. The workshop will be held on June 20, 2023. \\n\\nBest,\\n[my \nname]\nTable 5.3 – Examples of seed prompts used in the original Alpaca dataset\nMany synthetic data generation pipelines incorporate multiple steps to ensure data quality. This \nmay include generating an initial set of questions or instructions, followed by generating corre-\nsponding answers or responses. Some systems also implement validation steps, where another \nmodel or set of rules checks the generated pairs for accuracy, relevance, and adherence to spec-\nified criteria.\n",
      "page_number": 208
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 216-225)",
      "start_page": 216,
      "end_page": 225,
      "detection_method": "topic_boundary",
      "content": "Chapter 5\n193\nAn important aspect of synthetic data generation is the ability to control various attributes of the \ngenerated data. This includes factors such as the complexity of the instructions, the length of the \nresponses, the tone or style of the language used, and the specific topics or domains covered. By \nfine-tuning these parameters, it’s possible to create datasets that are tailored to specific training \nobjectives or that complement existing datasets in targeted ways. Structured generation using \nlibraries like Outlines can also be beneficial to adhere to specific formats.\nFurthermore, synthetic data generation can be particularly useful for addressing biases and gaps \nin existing datasets. By carefully designing the generation process, it’s possible to create more \nbalanced and inclusive datasets that represent a wider range of perspectives, topics, and language \nstyles. This can help in training LLMs that are more equitable and capable of serving diverse user \nbases.\nHowever, synthetic data generation also comes with challenges. One primary concern is the \npotential for the generated data to inherit biases or errors from the underlying language model \nused for generation. To mitigate this, many approaches incorporate human oversight, diverse \nprompts, and additional filtering mechanisms to ensure the quality and appropriateness of the \ngenerated data.\nAnother consideration is the need for the generated data to be sufficiently diverse and challeng-\ning. If the synthetic data is too simplistic or repetitive, it may not provide the level of complexity \nrequired to train a robust LLM. Advanced techniques in synthetic data generation often focus on \ncreating varied and nuanced instruction-response pairs that can push the boundaries of what \nthe model can learn.\nData augmentation\nIn this context, data augmentation refers to the process of increasing both the quantity and \nthe quality of data samples. Unlike data generation, we use pre-existing instruction samples \nas inputs in this stage. While it is possible to upsample pairs of instructions and answers, data \naugmentation is mostly used to increase the quality of existing samples. In particular, it focuses \non two aspects: diversity and complexity.\nA pioneering approach in this field is the Evol-Instruct method, which uses LLMs to evolve simple \ninstructions into more qualitative ones. The evolved instructions can then be used to generate \nanswers using powerful LLMs. This method employs two main strategies: in-depth and in-breadth \nevolving.\n\n\nSupervised Fine-Tuning\n194\nIn-depth evolving focuses on enhancing the complexity of existing instructions. It includes \nseveral techniques:\n•\nConstraints: It involves introducing additional requirements or limitations to the original \ninstruction, making it more challenging to fulfill.\n•\nDeepening: Instead of shallow questions, it tries to find more deep questions, requiring \nmore comprehensive responses.\n•\nConcretizing: It replaces general concepts with more specific ones, adding detail and \nprecision to the instruction.\n•\nIncreasing reasoning steps: It modifies instructions to explicitly request multiple-step \nreasoning, promoting more complex problem-solving.\n•\nComplicating input: This involves adding more complex data formats or structures to \nthe instruction, such as XML, JSON, or code snippets.\nIn-breadth evolving, on the other hand, aims to expand the diversity of the instruction dataset. \nIt generates entirely new instructions inspired by existing ones, focusing on creating more rare \nor long-tailed examples within the same domain.\nAs an example of concrete implementation, in-depth evolving can be automated with the fol-\nlowing prompt, from the AutoEvol paper. You simply need to provide the instruction you want \nto evolve as input, and a powerful model like GPT-4o will return a more complex version of the \noriginal instruction.\n\n\nChapter 5\n195\nYou are an Instruction Rewriter that rewrites the given #Instruction# into a more complex \nversion. Please follow the steps below to rewrite the given “#Instruction#” into a more complex \nversion.\n•\nStep 1: Please read the “#Instruction#” carefully and list all the possible methods \nto make this instruction more complex (to make it a bit harder for well-known AI \nassistants such as ChatGPT and GPT4 to handle). Please do not provide methods to\n•\nchange the language of the instruction!\n•\nStep 2: Please create a comprehensive plan based on the #Methods List# generated \nin Step 1 to make the #Instruction# more complex. The plan should include several \nmethods from the #Methods List#.\n•\nStep 3: Please execute the plan step by step and provide the #Rewritten Instruction#. \n#Rewritten Instruction# can only add 10 to 20 words into the “#Instruction#”.\n•\nStep 4: Please carefully review the #Rewritten Instruction# and identify any \nunreasonable parts. Ensure that the #Rewritten Instruction# is only a more complex \nversion of the #Instruction#. Just provide the #Finally Rewritten Instruction# without \nanyexplanation.\nPlease reply strictly in the following format:\nStep 1 #Methods List#:\nStep 2 #Plan#:\nStep 3 #Rewritten Instruction#:\nStep 4 #Finally Rewritten Instruction#:\n#Instruction#:\n{Instruction}\nTable 5.4 – Evol LLM prompt from the “Automatic Instruction Evolving for Large Language \nModels” paper by Zeng et al. (2024)\nThe UltraFeedback method is another innovative approach, focused on answer quality instead \nof instruction quality. It employs AI feedback to enhance the quality and diversity of model re-\nsponses. Unlike Evol-Instruct, which evolves instructions, UltraFeedback uses a large pool of \ndiverse instructions and models to generate a wide range of responses.\n\n\nSupervised Fine-Tuning\n196\nIt then leverages advanced language models like GPT-4 to provide detailed critiques and numerical \nscores for these responses across multiple dimensions such as instruction-following, truthfulness, \nhonesty, and helpfulness.\nBased on these ideas, you can create your own augmentation techniques to create a more challeng-\ning and diverse instruction dataset. By refining and evolving existing instructions and answers, \nthe resulting dataset can better train models to handle complex, multi-step tasks, and improve \ntheir performance across a wider range of applications.\nCreating our own instruction dataset\nIn this section, we will create our own instruction dataset based on the crawled data from Chapter \n3. To create a high-quality instruction dataset, we need to address two main issues: the unstruc-\ntured nature of our data and the limited number of articles we can crawl.\nThis unstructured nature comes from the fact that we are dealing with raw text (articles), instead \nof pairs of instructions and answers. To address this issue, we will use an LLM to perform this \ntransformation. Specifically, we will employ a combination of backtranslation and rephrasing. \nBacktranslation refers to the process of providing the expected answer as output and generat-\ning its corresponding instruction. However, using a chunk of text like a paragraph as an answer \nmight not always be appropriate. This is why we want to rephrase the raw text to ensure we’re \noutputting properly formatted, high-quality answers. Additionally, we can ask the model to \nfollow the author’s writing style to stay close to the original paragraph. While this process in-\nvolves extensive prompt engineering, it can be automated and used at scale, as we will see in the \nfollowing implementation.\nOur second issue regarding the limited number of samples is quite common in real-world use \ncases. The number of articles we can retrieve is limited, which constrains the size of the instruction \ndataset we are able to create. In this example, the more samples we have, the better the model \nbecomes at imitating the original authors. To address this problem, we will divide our articles \ninto chunks and generate three instruction-answer pairs for each chunk. This will multiply the \nnumber of samples we create while maintaining diversity in the final dataset. For simplicity, we \nwill do it using OpenAI’s GPT-4o-mini model, but you can also use open-source models.\nHowever, LLMs are not reliable when it comes to producing structured output. Even when given \nspecific templates or instructions, there’s no guarantee that the model will consistently adhere \nto them. This inconsistency often necessitates additional string parsing to ensure the output \nmeets the desired format. \n\n\nChapter 5\n197\nTo simplify this process and ensure properly structured results, we can employ structured gen-\neration techniques. Structured generation is an effective method to force an LLM to follow a \npredefined template, such as JSON, pydantic classes, or regular expressions. In the following, we \nwill use OpenAI’s JSON mode feature, which provides a more robust way to return valid JSON \nobjects and reduce the need for extensive post-processing.\nBased on this description, the following figure summarizes every step of the synthetic data pipe-\nline we want to build.\nFigure 5.6 – Synthetic data generation pipeline from raw text to instruction dataset\nLet’s now implement it in Python. You can implement it as part of the LLMOps pipeline, or as a \nstandalone script:\n1. \nWe want to make sure that the following libraries are installed. The OpenAI library will \nallow us to interact with a model to generate the instruction data, and datasets will format \nit into a Hugging Face-compatible format. The tqdm library is installed to visualize the \nprogress during the data generation process.\nopenai==1.37.1\ndatasets==2.20.0\ntqdm==4.66.4\n\n\nSupervised Fine-Tuning\n198\n2. We import all the required libraries as follows.\nimport concurrent.futures\nimport json\nimport random\nimport re\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import List, Tuple\nfrom datasets import Dataset\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom tqdm.auto import tqdm\n3. \nThe raw data we have is a JSON file. We create a Hugging Face dataset from this JSON file \nby extracting specific fields from each article: id, content, platform, author_id, author \nname, and link.\ndef load_articles_from_json(file_path: str) -> Dataset:\n    with open(file_path, \"r\") as file:\n        data = json.load(file)\n    return Dataset.from_dict(\n        {\n            \"id\": [item[\"id\"] for item in data[\"artifact_data\"]],\n            \"content\": [item[\"content\"] for item in data[\"artifact_\ndata\"]],\n            \"platform\": [item[\"platform\"] for item in \ndata[\"artifact_data\"]],\n            \"author_id\": [item[\"author_id\"] for item in \ndata[\"artifact_data\"]],\n            \"author_full_name\": [item[\"author_full_name\"] for item \nin data[\"artifact_data\"]],\n            \"link\": [item[\"link\"] for item in data[\"artifact_\ndata\"]],\n        }\n    ) \n\n\nChapter 5\n199\nIf we simply load our dataset as a pandas dataframe, it returns the following table.\nid\ncontent\nplatform\nauthor_id\nauthor_\nfull_\nname\nlink\n0\nab2f9e2e-\n5459-4dd6-\n97d6-\nc291de4a7093\nThe Impor-\ntance of Data \nPipelines in \nthe Era of...\nmedium\ne6b945ba-\n6a9a-\n4cde-b2bf-\n0890af79732b\nAlex Vesa\nhttps://medium.\ncom/decodingml/\nt\nh\ne\n-\nimportance-o...\n1\nccfe70f3-\nd324-\n40b6-ba38-\n86e72786dcf4\nChange Data \nCapture: \nEnabling \nEvent-Driven \nArc...\nmedium\ne6b945ba-\n6a9a-\n4cde-b2bf-\n0890af79732b\nAlex Vesa\nhttps://medium.\ncom/decodingml/\nthe-3nd-out-\nof-1...\n2\n4c9f68ae-\nec8b-4534-\n8ad5-\n92372bf8bb37\nThe Role of \nFeature Stores \nin Fine-Tun-\ning LLMs...\nmedium\ne6b945ba-\n6a9a-\n4cde-b2bf-\n0890af79732b\nAlex Vesa\nhttps://medium.\ncom/decodingml/\nthe-role-of-\nfeat...\n...\n...\n...\n...\n...\n...\n...\n73\n68795a4d-\n26c2-43b7-\n9900-\n739a80b9b-\n7dc\nDML: 4 key \nideas you \nmust know to \ntrain an LLM...\ndecod-\ningml.\nsubstack.\ncom\n1519b1d1-\n1a5d-444c-\na880-926c9e-\nb6539e\nPaul \nIusztin\nh t t p s : / /\nd e c o d i n g m l .\nsubstack.com/p/\ndml-4-key-id...\n74\nd91b17c0-\n05d8-\n4838-bf61-\ne2abc1573622\nDML: How to \nadd real-time \nmonitoring & \nmetrics...\ndecod-\ningml.\nsubstack.\ncom\n1519b1d1-\n1a5d-444c-\na880-926c9e-\nb6539e\nPaul \nIusztin\nh t t p s : / /\nd e c o d i n g m l .\nsubstack.com/p/\ndml-how-to-a...\n75\ndcf55b28-\n2814-\n4480-a18b-\na77d01d44f5f\nDML: Top 6 \nML Platform \nFeatures You \nMust Know ...\ndecod-\ningml.\nsubstack.\ncom\n1519b1d1-\n1a5d-444c-\na880-926c9e-\nb6539e\nPaul \nIusztin\nh t t p s : / /\nd e c o d i n g m l .\nsubstack.com/p/\ndml-top-6-ml...\n4.\nIf we inspect the content of some articles a little further, we realize that some of them \nhave special characters and redundant whitespaces. We can clean this with a simple regex.\nFirst, we use [^\\w\\s.,!?'] to remove non-alphanumeric characters except for apostro-\nphes, periods, commas, exclamation marks, and question marks. Then, we use \\s+ to \nreplace multiple consecutive whitespace characters with a single space. \n\n\nSupervised Fine-Tuning\n200\nFinally, we implement strip() to remove any leading or trailing whitespace.\ndef clean_text(text):\n    text = re.sub(r\"[^\\w\\s.,!?']\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n5.\nNow that we can load our articles, we need to chunk them before turning them into pairs \nof instructions and answers. Ideally, you would want to use headlines or paragraphs to \nproduce semantically meaningful chunking.\nHowever, in our example, like in the real world, raw data tends to be messy. Due to im-\nproper formatting, we cannot extract paragraphs or headlines for every article in our raw \ndataset. Instead, we will extract sentences using a regex to get chunks between 1,000 \nand 2,000 characters. This number can be optimized depending on the density of the \ninformation contained in the text.\nThe extract_substrings function processes each article in the dataset by first cleaning the \ntext and then using a regex to split it into sentences. It then builds chunks of text by con-\ncatenating these sentences until each chunk is between 1,000 and 2,000 characters long.\ndef extract_substrings(dataset: Dataset, min_length: int = 1000, \nmax_length: int = 2000) -> List[str]:\n    extracts = []\n    sentence_pattern = r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)\n(?<=\\.|\\?|\\!)\\s\"\n    for article in dataset[\"content\"]:\n        cleaned_article = clean_text(article)\n        sentences = re.split(sentence_pattern, cleaned_article)\n        current_chunk = \"\"\n        for sentence in sentences:\n            sentence = sentence.strip()\n            if not sentence:\n                continue\n            if len(current_chunk) + len(sentence) <= max_length:\n                current_chunk += sentence + \" \"\n            else:\n\n\nChapter 5\n201\n                if len(current_chunk) >= min_length:\n                    extracts.append(current_chunk.strip())\n                current_chunk = sentence + \" \"\n        if len(current_chunk) >= min_length:\n            extracts.append(current_chunk.strip())\n    return extracts\n6.\nNext, we want to create instruction-answer pairs from the extracted chunks of text. To \nmanage these pairs effectively, we introduce the InstructionAnswerSet class. This class \nallows us to create instances directly from JSON strings, which is useful when parsing the \noutput from the OpenAI API.\nclass InstructionAnswerSet:\n    def __init__(self, pairs: List[Tuple[str, str]]):\n        self.pairs = pairs\n    @classmethod\n    def from_json(cls, json_str: str) -> 'InstructionAnswerSet':\n        data = json.loads(json_str)\n        pairs = [(pair['instruction'], pair['answer'])\n                 for pair in data['instruction_answer_pairs']]\n        return cls(pairs)\n    def __iter__(self):\n        return iter(self.pairs)\n7.\nNow that we have a set of extracts from the articles with a reasonable length, we can use \nan LLM to transform them into pairs of instructions and answers. Note that this step is \nmodel-agnostic and can be implemented with any open-source or closed-source model. \nBecause this output is grounded in the context we provide, it doesn’t require complex \nreasoning or high-performing models.\nFor convenience, we will use GPT-4o mini in this example. This choice is motivated by the \nlow cost and good performance of this model. Prompt engineering is the most important \naspect of this data transformation stage and requires several iterations to produce the \nexpected outputs. We recommend starting with simple prompts and adding complexity \nwhen required to be more accurate, modify the style, or output multiple responses.\n\n\nSupervised Fine-Tuning\n202\nIn our example, we want to create instructions like “Write a paragraph about X topic” and \ncorresponding answers that are factual and imitate the writer’s style. To implement this, \nwe need to provide an extract that will ground the model’s responses. For efficiency, we \nalso choose to generate five instruction-answer pairs for each extract. Here’s the beginning \nof our function for instruction generation, including our prompt.\ndef generate_instruction_answer_pairs(\n    extract: str, client: OpenAI\n) -> List[Tuple[str, str]]:\n    prompt = f\"\"\"Based on the following extract, generate five \ninstruction-answer pairs. Each instruction \\\nmust ask to write about a specific topic contained in the context. \neach answer \\\nmust provide a relevant paragraph based on the information found in \nthe \\\ncontext. Only use concepts from the context to generate the \ninstructions. \\\nInstructions must never explicitly mention a context, a system, a \ncourse, or an extract. \\\nInstructions must be self-contained and general. \\\nAnswers must imitate the writing style of the context. \\\nExample instruction: Explain the concept of an LLM Twin. \\\nExample answer: An LLM Twin is essentially an AI character that \nmimics your writing style, personality, and voice. \\\nIt's designed to write just like you by incorporating these elements \ninto a language model. \\\nThe idea is to create a digital replica of your writing habits using \nadvanced AI techniques. \\\nProvide your response in JSON format with the following structure:\n{{\n    \"instruction_answer_pairs\": [\n        {{\"instruction\": \"...\", \"answer\": \"...\"}},\n        ...\n    ]\n}}\nExtract:\n{extract}\n\"\"\"\n",
      "page_number": 216
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 226-234)",
      "start_page": 226,
      "end_page": 234,
      "detection_method": "topic_boundary",
      "content": "Chapter 5\n203\n8. In addition to the user prompt, we can also specify a system prompt to guide the mod-\nel into generating the expected instructions. Here, we repeat our high-level task in the \nsystem prompt.\nThe concatenation of the system and user prompts is fed to the OpenAI API, using the GPT-\n4o mini model in JSON mode and a maximum of 1,200 tokens in the answer. We also use a \nstandard temperature of 0.7 to encourage diverse responses. The generated text is directly \nparsed using the InstructionAnswerSet class to return pairs of instructions and answers.\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\", \"content\": \"You are a helpful \nassistant who \\\n            generates instruction-answer pairs based on the given \ncontext. \\\n            Provide your response in JSON format.\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_format={\"type\": \"json_object\"},\n        max_tokens=1200,\n        temperature=0.7,\n    )\n    # Parse the structured output\n    result = InstructionAnswerSet.from_json(completion.choices[0].\nmessage.content)\n    # Convert to list of tuples\n    return result.pairs\n9.\nLet’s create a main function to automate the process. It extracts substrings from the input \ndataset, then uses concurrent processing via Python’s ThreadPoolExecutor to efficiently \ngenerate instruction-answer pairs for each extract. \n\n\nSupervised Fine-Tuning\n204\nWe use a default max_workers value of 4 because higher values tend to exceed OpenAI’s \nrate limits, potentially causing API request failures or throttling.\ndef create_instruction_dataset(\n    dataset: Dataset, client: OpenAI, num_workers: int = 4\n) -> Dataset:\n    extracts = extract_substrings(dataset)\n    instruction_answer_pairs = []\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_\nworkers) as executor:\n        futures = [executor.submit(generate_instruction_answer_\npairs, extract, client)\n            for extract in extracts\n        ]\n        for future in tqdm(concurrent.futures.as_completed(futures), \ntotal=len(futures)\n        ):\n            instruction_answer_pairs.extend(future.result())\n    instructions, answers = zip(*instruction_answer_pairs)\n    return Dataset.from_dict(\n        {\"instruction\": list(instructions), \"output\": list(answers)}\n    )\n10. We can create our instruction dataset by calling this function. Running it over the raw \ndata with GPT-4o mini costs less than 0.5$.\n11. We can now create a main function to orchestrate the entire pipeline. It loads the raw \ndata, creates the instruction dataset, splits it into training and testing sets, and pushes \nthe result to the Hugging Face Hub.\ndef main(dataset_id: str) -> Dataset:\n    client = OpenAI()\n    # 1. Load the raw data\n    raw_dataset = load_articles_from_json(\"cleaned_documents.json\")\n    print(\"Raw dataset:\")\n    print(raw_dataset.to_pandas())\n    # 2. Create instructiondataset\n\n\nChapter 5\n205\ninstruction_dataset = create_instruction_dataset(raw_dataset, \nclient)\n    print(\"Instruction dataset:\")\n    print(instruction_dataset.to_pandas())\n    # 3. Train/test split and export\n    filtered_dataset = instruction_dataset.train_test_split(test_\nsize=0.1)\n    filtered_dataset.push_to_hub(\"mlabonne/llmtwin\")\n    return filtered_dataset\nDataset({\n    features: ['instruction', 'output'],\n    num_rows: 3335\n})\nWe obtained 3,335 pairs with this process. You can find our version of the dataset at https://\nhuggingface.co/datasets/mlabonne/llmtwin. The Hugging Face Hub provides a convenient \ndataset viewer (see Figure 5.7) to explore instructions and answers and make sure that there are \nno obvious mistakes in these samples. Due to the small size of the dataset, there is no need for \ncomprehensive exploration and topic clustering.\nFigure 5.7 – The mlabonne/llmtwin instruction dataset on the Hugging Face Hub\n\n\nSupervised Fine-Tuning\n206\nAs seen in the previous section, we could refine this instruction dataset by increasing the diver-\nsity and complexity of our samples. More advanced prompt engineering could also increase the \nquality of the generated data by providing examples of the expected results, for instance. Finally, \nquality evaluation could help filter out low-quality samples by reviewing them individually. For \nconciseness and simplicity, we will keep a straightforward approach for this instruction dataset \nand explore more advanced methods in Chapter 6 when we create a preference dataset.\nIn the next section, we will introduce SFT techniques, as well as related concepts.\nExploring SFT and its techniques\nSFT consists of re-training pre-trained models on a smaller dataset composed of pairs of instruc-\ntions and answers. The goal of SFT is to turn a base model, which can only perform next-token \nprediction, into a useful assistant, capable of answering questions and following instructions. \nSFT can also be used to improve the general performance of the base model (general-purpose \nSFT), instill new knowledge (e.g., new languages, domains, etc.), focus on specific tasks, adopt \na particular voice, and so on.\nIn this section, we will discuss when to use fine-tuning and explore related concepts with storage \nformats and chat templates. Finally, we will introduce three popular ways of implementing SFT: \nfull-finetuning, Low-Rank Adaptation (LoRA) and Quantization-aware Low-Rank Adaptation\n(QLoRA).\nWhen to fine-tune\nIn most scenarios, it is recommended to start with prompt engineering instead of directly fine-tun-\ning models. Prompt engineering can be used with either open-weight or closed-source models. By \nusing techniques like few-shot prompting or retrieval augmented generation (RAG), numerous \nproblems can efficiently be tackled without SFT. Prompt engineering also allows us to build a \nrobust evaluation pipeline, which measures metrics like accuracy, but also cost and latency. If \nthese results do not match the requirements, we can explore the possibility of creating an in-\nstruction dataset, as illustrated in the previous section. If enough data is available, fine-tuning \nbecomes an option.\n\n\nChapter 5\n207\nFigure 5.8 – Basic flowchart to determine when fine-tuning is an option on a technical level\nBeyond these technical considerations, SFT answers common needs in terms of control (“know \nyour data”) and customizability (the fine-tuned model is unique). Instead of building applications \naround a chatbot, fine-tuning allows developers to create more diverse interactions with LLMs, \nlike tool analytics, moderation, and additional context. Note that if we focus on open-weight \nmodels in this book, several LLM providers offer automated fine-tuning services. While they don’t \noffer the same level of control and customizability as managing your own fine-tuning pipeline, it \ncan be an interesting trade-off in specific scenarios (e.g., limited resources in terms of machine \nlearning engineering).\nDespite these advantages, fine-tuning also has limitations. It is generally understood that SFT \nleverages pre-existing knowledge in the base model’s weights and refocuses the parameters for \na specific purpose. This has several implications. First of all, knowledge that is too distant from \nwhat has been learned in the pre-training set (such as an unknown or rare language) can be \ndifficult to learn effectively. \nEven worse, a study showed that fine-tuning a model on new knowledge could result in more \nfrequent hallucinations. Depending on the SFT technique that is used, we’re also at risk of eras-\ning knowledge that was present in the base model (a common issue referred to as “catastrophic \nforgetting”).\n\n\nSupervised Fine-Tuning\n208\nInstruction dataset formats\nInstruction datasets are stored in a particular format to organize instructions and answers. Typi-\ncally, each sample in the dataset can be represented as a Python dictionary, where keys are prompt \ntypes like system, instruction, output, and values corresponding to the actual text. The three \nmost standard formats are Alpaca, ShareGPT, and OpenAI. The following table shows how these \ndata formats are generally organized.\nName\nJSONL format\nAlpaca\n{“instruction”: “...”, “input”: “...”, “output”: “...”}\n{“instruction”: “...”, “output”: “...”}\nShareGPT\n{“conversations”: [{“from”: “...”, “value”: “...”}, …]}\nOpenAI\n{“conversations”: [{“role”: “...”, “content”: “...”}, …]}\nOASST\n{“INSTRUCTION”: “...”, “RESPONSE”: “...”}\nRaw text\n{“text”: “...”}\nTable 5.5 – Examples of instruction data storage format\nNote that for Alpaca, the “input\" key is optional. The content of the “input\" key is only appended \nto the content of the “instruction\" key when it exists. We also added the “raw text\" data format \nto show that SFT is not inherently different from pre-training. If you choose to re-train a model \non raw text, this is a type of fine-tuning generally called “continual pre-training.”\nThe dataset we created in the previous section has two columns (“instruction\" and “output\") \nand corresponds to the Alpaca format. Alpaca is sufficient for single-turn instructions and an-\nswers, which means it is limited to one instruction and one answer. When you want to process \nconversations (multiple instructions and answers), formats like ShareGPT or OpenAI are a better \nfit. By storing each message as a dictionary in a list, they can represent an arbitrarily long con-\nversation in each sample.\nThe choice of single-turn and multi-turn conversations directly impacts the storage type and \ndepends on the end use case.\nChat templates\nOnce the instruction-answer pairs are parsed from the dataset format, we want to structure them \nin a chat template. Chat templates offer a unified way to present the instructions and answers \nto the model.\n\n\nChapter 5\n209\nIn general, they also include special tokens to identify the beginning and the end of a message, or \nwho is the author of the message. Since base models are not designed to follow instructions, they \ndon’t have a chat template. This means that you can choose any template when you fine-tune \na based model. If you want to fine-tune an instruct model (not recommended), you need to use \nthe same template or it might degrade your performance.\nLike instruction dataset formats, there are different chat templates: ChatML, Llama 3, Mistral, and \nmany others. In the open-source community, the ChatML template (originally from OpenAI) is a \npopular option. It simply adds two special tokens (<|im_start|> and <|im_end|>) to indicate \nwho is speaking. To give you an example, here is what we obtain when we apply the ChatML \ntemplate to the instruction-answer pair shown in Table 5.1:\n<|im_start|>system\nYou are a helpful assistant, who always provide explanation. Think like you \nare answering to a five year old.<|im_end|>\n<|im_start|>user\nConcepts: building, shop, town\nWrite a sentence that includes all these words.<|im_end|>\n<|im_start|>assistant\nIn our little town, there is a shop inside a big building where people go \nto buy their favorite toys and candies.<|im_end|>\nTable 5.6 – Sample from Table 5.1 with the ChatML chat template\nAs you can see, we still have three distinct parts: system, user, and assistant. Each part starts with \nthe <|im_start|> token and ends with <|im_end|>. The current speaker is identified by a string \n(like “system\") instead of a special token. This is the exact string that is tokenized and used as \ninput by the model during fine-tuning.\nHowever, during inference, we can’t provide the expected answer. In this case, we provide the \nsystem and user part as shown in Figure 5.6, and prompt the model to answer by adding <|im_\nstart|>assistant\\n. \nBecause the model has been fine-tuned with this template, it understands that the next tokens \nshould be an answer relevant to the user instruction and guided by the system prompt. This is \nhow fine-tuned models acquire instruction-following capabilities.\n\n\nSupervised Fine-Tuning\n210\nA common issue with chat templates is that every single whitespace and line break is extremely \nimportant. Adding or removing any character would result in a wrong tokenization, which neg-\natively impacts the performance of the model. For this reason, it is recommended to use reliable \ntemplates like Jinja, as implemented in the Transformers library. Table 5.7 shows a few examples \nof such templates, including Alpaca, which is both the name of an instruction dataset format \nand a chat template.\nName\nJinja template\nAlpaca\n### Instruction: What is the capital of France?\n### Response: The capital of France is Paris.<EOS>\nChatML\n<|im_start|>user\nWhat is the capital of France?<|im_end|>\n<|im_start|>assistant\nThe capital of France is Paris.<|im_end|>\nLlama 3\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWhat is the capital of France?<|eot_id|><|start_header_\nid|>assistant<|end_header_id|>\nThe capital of France is Paris.<|eot_id|>\nPhi-3\n<|user|>\nWhat is the capital of France?<|end|>\n<|assistant|>\nThe capital of France is Paris.<|end|>\nGemma\n<bos><start_of_turn>user\nWhat is the capital of France?<end_of_turn>\n<start_of_turn>model\nThe capital of France is Paris.<end_of_turn>\nTable 5.7 – Example of common chat templates\nJinja implements loops and conditions, which allow the same template to be used for training \nand inference (add_generation_prompt).\n\n\nChapter 5\n211\nParameter-efficient fine-tuning techniques\nWhile many techniques exist in the literature, SFT has converged on three main techniques: full \nfine-tuning, LoRA, and QLoRA. We will introduce each technique individually, and weigh their \npros and cons depending on your use cases.\nFigure 5.9 – Architectural differences of the three main SFT techniques at the module level\nFull fine-tuning\nFull fine-tuning refers to the most straightforward SFT technique, consisting of re-training every \nparameter in the base model. Like pre-training, SFT uses next-token prediction as its training \nobjective. This means that the previously discussed structure of the dataset can be seen as the \nmain difference between continual pre-training and full fine-tuning.\nThis method often provides the best results but requires significant computational resources. \nMemory usage depends on several factors, including model size, training techniques, and op-\ntimization methods. At its simplest, using a single-GPU setting, the memory required can be \nestimated using the following formula:\n𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀 \nFor a basic setup using 32-bit floating point (fp32) precision, we can estimate:\n•\nParameters: Learnable weights and biases within a neural network. In a large language \nmodel, these are typically the weights in the attention mechanisms, feed-forward layers, \nand embedding layers. Cost: 4 bytes/parameter (FP32) or 2 bytes/parameter (FP16/BF16).\n•\nGradients: Gradients are the partial derivatives of the loss function with respect to each \nmodel parameter. They indicate how much each parameter should be adjusted to minimize \nthe loss. During training, gradients are computed for each parameter through backprop-\nagation and are used to update the model parameters. Cost: 4 bytes/parameter.\n",
      "page_number": 226
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 235-244)",
      "start_page": 235,
      "end_page": 244,
      "detection_method": "topic_boundary",
      "content": "Supervised Fine-Tuning\n212\n• \nOptimizer states: Optimizer states are additional values maintained by optimization \nalgorithms like Adam or AdamW. These typically include running averages of past gra-\ndients and past squared gradients for each parameter. They help in adapting the learning \nrate for each parameter and navigating the loss landscape more effectively. For instance, \nAdam maintains two additional values (momentum and variance) per parameter. Cost: \n8 bytes/parameter (for Adam optimizer).\n• \nActivations: Activations are the intermediate outputs of each layer in the neural network \nduring the forward pass. For transformer-based models, this includes the outputs of \nattention mechanisms, feed-forward layers, and normalization layers. Activations need \nto be kept in memory during the forward pass to compute gradients in the backward \npass, unless techniques like activation checkpointing are used. Cost: variable, but often \nnegligible for small batch sizes.\nThis gives us a baseline of 16 bytes per parameter. This translates into 112 GB of VRAM for a 7 \nB model and 1,120 GB for a 70 B model. However, this is often an underestimate, as it doesn’t \naccount for additional memory needed for activations, temporary buffers, and overhead from \nvarious training techniques.\nSeveral techniques can be employed to reduce memory usage during LLM fine-tuning. Model \nparallelism spreads the workload across multiple GPUs, though it adds some overhead. Gradient \naccumulation enables larger effective batch sizes without proportional memory increase. Mem-\nory-efficient optimizers like 8-bit Adam can reduce the footprint of optimizer states. Activation \ncheckpointing trades computation for memory by recalculating certain activations. When com-\nbined, these techniques can significantly lower memory usage. For instance, using mixed precision \nwith model parallelism might reduce costs to around 14-15 bytes per parameter, compared to the \n16-byte baseline. However, memory requirements remain substantial for large models even with \nthese optimizations.\nIn addition, full fine-tuning directly modifies the pre-training weights, which makes it destructive \nby nature. If training doesn’t behave as expected, it might erase previous knowledge and skills – a \nphenomenon referred to as “catastrophic forgetting.” The same phenomenon can happen with \ncontinual pre-training, which generally makes these techniques more difficult to use. Due to this \nadditional complexity and its high computational requirements, parameter-efficient techniques \nare often preferred to full fine-tuning to create task and domain-specific models.\n\n\nChapter 5\n213\nLoRA\nLoRA is a parameter-efficient technique for fine-tuning LLMs. Developed to address the compu-\ntational challenges associated with adapting massive neural networks, LoRA has quickly become \na cornerstone technique in LLM fine-tuning.\nThe primary purpose of LoRA is to enable the fine-tuning of LLMs with significantly reduced \ncomputational resources. This is achieved by introducing trainable low-rank matrices that mod-\nify the behavior of the model without changing its original parameters. The key advantages of \nLoRA include:\n•\nDramatically reduced memory usage during training\n•\nFaster fine-tuning process\n•\nPreservation of pre-trained model weights (non-destructive)\n• \nAbility to switch between tasks efficiently by swapping LoRA weights\nThese benefits have made LoRA particularly attractive for researchers and developers working \nwith limited computational resources, effectively democratizing the process of LLM fine-tuning.\nAt its core, LoRA employs a low-rank decomposition technique to update model weights efficiently. \nInstead of directly modifying the original weight matrix 𝑊𝑊 , LoRA introduces two smaller matrices, \n𝐴𝐴  and 𝐵𝐵 , which together form a low-rank update to 𝑊𝑊 .\nFigure 5.10 – LoRA adds the two trainable matrices 𝐴𝐴 and 𝐵𝐵 and keeps the pre-trained weights \n𝑊𝑊 frozen\n\n\nSupervised Fine-Tuning\n214\nMathematically, this can be represented as:\n𝑊𝑊𝑊𝑊𝑊𝑊𝑊𝐵𝐵𝐵𝐵 \nHere, 𝑊𝑊  is the original weight matrix, 𝐵𝐵 and 𝐴𝐴 are the LoRA matrices, and 𝑊𝑊𝑊 is the effective weight \nmatrix used during inference.\nThe dimensions of matrices A and B are chosen such that their product has the same shape as \n𝑊𝑊 , but with a much lower rank. This rank, typically denoted as 𝑟𝑟 , is a crucial hyperparameter \nin LoRA. During training, the original weights 𝑊𝑊 remain frozen, while only 𝐴𝐴 and 𝐵𝐵 are updated. \nThis approach significantly reduces the number of trainable parameters, leading to substantial \nmemory savings and faster training times.\nTo implement LoRA effectively, we need to select the correct hyperparameters and target modules. \nLoRA comes with two hyperparameters:\n• \nRank (𝑟𝑟 ): Determines the size of the LoRA matrices. A common starting point is 𝑟𝑟𝑟𝑟 , but \nvalues up to 256 have shown good results in some cases. Larger ranks may capture more \ndiverse tasks but could lead to overfitting.\n• \nAlpha (𝛼𝛼 ): A scaling factor applied to the LoRA update. In practice, we update the frozen \nweights 𝑊𝑊  by a factor of 𝛼𝛼𝛼𝛼𝛼 . This is why a common heuristic is to set 𝛼𝛼  to twice the \nvalue of 𝑟𝑟 , effectively applying a scaling factor of 2 to the LoRA update. You can experiment \nwith different ratios in case of overfitting or underfitting.\nIn addition, it is possible to add a drop-out layer to prevent overfitting. The dropout rate is usually \nset between 0 and 0.1 as an optional regularization factor, which slightly decreases training speed.\nLoRA can be applied to various parts of the model architecture. Initially, LoRA was primarily fo-\ncused on modifying the attention mechanism, specifically the query (Q) and value (V) matrices \nin transformer layers. However, experiments have demonstrated significant benefits in extending \nLoRA’s application to other key components of the model. These additional target modules include:\n•\nKey (K) matrices in attention layers\n•\nOutput projection layers (often denoted as O) in attention mechanisms\n•\nFeed-forward or Multi-Layer Perceptron (MLP) blocks between attention layers\n•\nLinear output layers\nHowever, it’s important to note that increasing the number of LoRA-adapted modules also in-\ncreases the number of trainable parameters and, consequently, the memory requirements.\n\n\nChapter 5\n215\nUsing LoRA, it’s possible to fine-tune a 7B parameter model on a single GPU with as little as 14-\n18 GB of VRAM, depending on the specific configuration. This is a dramatic reduction compared \nto full fine-tuning, which would typically require multiple high-end GPUs. In terms of trainable \nparameters, LoRA drastically reduces the number compared to full fine-tuning. For example, even \nwhen targeting every module with a rank of 16, a Llama 3 8 B model only has 42 million trainable \nLoRA parameters out of 8 billion parameters, which is 0.5196% of the model’s parameters.\nIn terms of quality, LoRA can also achieve comparable or sometimes better results than full-fine-\ntuning. Multiple sets of LoRA weights can be combined for different tasks or domains, allowing \nflexible deployment and task switching without retraining. Different projects are specialized \nin multiple-LoRA serving, such as LoRAX. It’s also a feature supported by Hugging Face’s Text \nGeneration Inference (TGI) and Nvidia Inference Microservices (NIM).\nQLoRA\nIntroduced by Dettmers et al., QLoRA is a method for fine-tuning LLMs that addresses the chal-\nlenges of high computational costs. By combining quantization techniques with LoRA, QLoRA \nallows developers to fine-tune models on relatively small, widely available GPUs.\nThe core of QLoRA’s approach involves quantizing the base model parameters to a custom 4-bit \nNormalFloat (NF4) data type, which significantly reduces memory usage. Like LoRA, instead \nof updating all model parameters during fine-tuning, QLoRA introduces small, trainable low-\nrank matrices (adapters) to specific layers of the model. Only these adapters are updated during \ntraining, while the original model weights remain unchanged. To further reduce memory usage, \nQLoRA employs double quantization, which quantizes the quantization constants themselves. \nAdditionally, it uses paged optimizers to manage memory spikes during training by leveraging \nNvidia’s unified memory feature.\nQLoRA provides significant memory savings compared to LoRA, reducing peak GPU memory \nusage by up to 75%. For example, for a 7B model, QLoRA reduces peak memory usage from 14 GB \nto 9.1 GB during initialization, a 35% reduction. During fine-tuning, the memory savings increase \nto 40%, from 15.6 GB for LoRA to 9.3 GB for QLoRA. However, this memory efficiency comes at \nthe cost of increased training time, with QLoRA being about 30% slower than LoRA. In terms of \nmodel performance, QLoRA shows only minor differences compared to LoRA.\nIn summary, QLoRA is particularly beneficial when memory constraints are the primary concern, \nsuch as when working with very large models or on hardware with limited GPU memory. However, \nif training speed is crucial and sufficient memory is available, LoRA might be the preferred choice. \n\n\nSupervised Fine-Tuning\n216\nThe decision between QLoRA and LoRA should be based on the specific requirements of the \nproject, available hardware, and the need to balance memory usage, training speed, and model \nperformance.\nTraining parameters\nWhen fine-tuning LLMs, several hyperparameters guide the training process and significantly \nimpact the model’s convergence, generalization, and overall effectiveness.\nLearning rate and scheduler\nThe learning rate is the most important hyperparameter. It controls how much the model’s pa-\nrameters are updated during training. It typically ranges from very small values like 1e-6 to larger \nvalues like 1e-3. A common starting point for transformer models is often around 1e-5. If the \nlearning rate is too low, training progresses slowly and may get stuck in suboptimal solutions. \nConversely, if it’s too high, training can become unstable or diverge, leading to poor performance. \nIt’s often beneficial to experiment with different learning rates to find the optimal value for your \nspecific task and model.\nThe learning rate scheduler adjusts the learning rate throughout the training process. It typically \nstarts with a higher learning rate to enable rapid initial progress, then gradually decreases it in \nlater stages to fine-tune the model more precisely. The two most common types of schedulers are \nlinear and cosine. A linear scheduler decreases the learning rate steadily over time, while a cosine \nscheduler follows a cosine curve, decreasing more slowly at first and then more rapidly toward \nthe end of training. For example, you might start with a learning rate of 3e-4 and decrease it to \n1e-7 over the course of training. The specific values and decay schedule depend on your model \nand dataset, but a common approach is to use a warmup period (e.g., 5% of total steps) where the \nlearning rate increases from 0 to the initial value, followed by a decay period for the remaining 95% \nof steps. This approach helps stabilize early training and allows for more refined updates as the \nmodel converges. In general, linear and cosine schedulers provide the same level of performance.\nBatch size\nThe batch size determines the number of samples processed before the model’s weights are up-\ndated. Typical batch sizes for LLM fine-tuning range from 1 to 32, with common values being 1, 2, \n4, 8, or 16. Larger batch sizes generally lead to more stable gradient estimates and can improve \ntraining speed, as they provide a better approximation of the true gradient of the entire dataset.\n\n\nChapter 5\n217\nHowever, they also require more memory, which can be a limiting factor on GPUs with less VRAM. \nFor instance, a batch size of 16 might work well on a high-end GPU with 24GB of memory, while \na smaller GPU with 8 GB might only handle a batch size of 2 or 4.\nTo overcome memory constraints while still benefiting from larger batch sizes, a technique called \ngradient accumulation can be used. It works by performing multiple forward and backward passes \nwith smaller mini-batches, accumulating the gradients over these steps before applying a single \nupdate to the model’s parameters. This approach is particularly useful when working with large \nmodels or limited GPU memory. For example, if you want to achieve an effective batch size of 32 \nbut your GPU can only handle 8 samples at a time, you can set the gradient accumulation steps \nto 4. This means you’ll process 4 mini-batches of 8 samples each, accumulating the gradients, \nand then update the model as if you had processed all 32 samples at once.\nThe number of gradient accumulation steps typically ranges from 1 (no accumulation) to 8 or \n16, depending on the desired effective batch size and available computational resources. When \nchoosing the number of steps, consider the trade-off between training speed and memory usage. \nMore accumulation steps allow for larger effective batch sizes but increase the time required for \neach update. Here’s a simple formula to determine the effective batch size:\n𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸 \nFor instance, if you’re using 2 GPUs, each processing a batch of 4 samples, with 4 gradient accu-\nmulation steps, your effective batch size would be 4 * 2 * 4 = 32 samples.\nMaximum length and packing\nThe maximum sequence length determines the longest input the model can process. It’s typically \nset between 512 and 4,096 tokens but can go up to 128,000 or more, depending on the task and \navailable GPU memory. For example, a maximum length of 2,048 tokens is common for many \nlanguage generation tasks, while RAG applications might use up to 8,192 tokens or more. When \nprocessing input data, sequences longer than this limit are truncated, meaning excess tokens \nare removed. Truncation can occur at the beginning (left truncation) or end (right truncation) of \nthe sequence. For instance, with a maximum length of 1,024 tokens, a 1,500-token input would \nhave 476 tokens removed. This parameter directly impacts batch size and memory usage; a batch \nsize of 12 with a max length of 1,024 would contain 12,288 tokens (12 * 1,024), while the same \nbatch size with a max length of 512 would only contain 6,144 tokens. It’s important to balance \nthis parameter with your GPU capabilities and the nature of your training data to optimize per-\nformance and resource utilization.\n\n\nSupervised Fine-Tuning\n218\nPacking maximizes the utilization of each training batch. Instead of assigning one sample per \nbatch, packing combines multiple smaller samples into a single batch, effectively increasing the \namount of data processed in each iteration. For example, if your maximum sequence length is \n1,024 tokens, but many of your samples are only 200-300 tokens long, packing could allow you \nto fit 3-4 samples into each batch slot. This approach can significantly improve training efficien-\ncy, especially when dealing with datasets containing many short sequences. However, packing \nrequires careful implementation to ensure that model attention doesn’t cross between packed \nsamples. This is typically achieved by using attention masks that prevent the model from attend-\ning to tokens from different samples within the same packed sequence.\nNumber of epochs\nThe number of epochs is another important parameter, representing the number of complete \npasses through the entire training dataset. For LLM fine-tuning, the typical range is 1 to 10 epochs, \nwith many successful runs using 2 to 5 epochs. The optimal number depends on factors such as \ntask complexity, dataset size, and model architecture. More epochs allow the model to refine its \nlearning, potentially improving performance. However, there’s a crucial trade-off: too few epochs \nmay lead to underfitting, while too many can cause overfitting. For example, a large model fine-\ntuned on a small dataset might only need 1-3 epochs, while a smaller model fine-tuned on a larger \ndataset could benefit from 5-10 epochs. It is helpful to monitor validation performance during \ntraining and implement early stopping if the model’s performance plateaus or degrades. This \napproach helps determine the optimal number of epochs dynamically and prevents overfitting.\nOptimizers\nOptimizers adjust the model’s parameters to minimize the loss function. For LLM fine-tuning, \nAdamW (Adaptive Moment Estimation with Weight Decay) is highly recommended, particularly \nits 8-bit version. AdamW 8-bit performs comparably to the 32-bit version while using less GPU \nmemory (but it doesn’t improve training speed). AdamW combines adaptive learning rates with \nweight decay regularization, often leading to better training stability and model performance.\nFor scenarios with severe memory constraints, AdaFactor presents an alternative designed for \nmemory efficiency. It works well without explicit learning rate tuning, making it particularly \nuseful in resource-constrained environments. However, it may not always match AdamW’s perfor-\nmance in all cases. In situations involving extremely large models or limited GPU memory, paged \nversions of optimizers, such as paged AdamW 8-bit, can further reduce memory consumption \nby offloading to CPU RAM. If memory allows and maximum performance is the priority, the \nnon-quantized adamw_torch optimizer may be the best choice.\n\n\nChapter 5\n219\nWeight decay\nWeight decay works by adding a penalty for large weights to the loss function, encouraging the \nmodel to learn simpler, more generalizable features. This helps the model avoid relying too heavily \non any single input feature, which can improve its performance on unseen data. Typically, weight \ndecay values range from 0.01 to 0.1, with 0.01 being a common starting point. For example, if \nyou’re using the AdamW optimizer, you might set the weight decay to 0.01.\nWhile weight decay can be beneficial, setting it too high can impede learning by making it difficult \nfor the model to capture important patterns in the data. Conversely, setting it too low may not \nprovide sufficient regularization. The optimal weight decay value often depends on the specific \nmodel architecture and dataset, so it’s generally a good practice to experiment with different \nvalues.\nGradient checkpointing\nGradient checkpointing is a technique that reduces memory consumption during training by stor-\ning only a subset of intermediate activations generated in the forward pass. In standard training \nprocedures, all intermediate activations are retained in memory to facilitate gradient calculation \nduring the backward pass. However, for very deep networks like LLMs, this approach can quickly \nbecome impractical due to hardware limitations, especially on GPUs with limited memory capacity.\nGradient checkpointing addresses this challenge by selectively saving activations at specific layers \nwithin the network. For layers where activations are not saved, they are recomputed during the \nbackward pass as needed for gradient computation. This approach creates a trade-off between \ncomputation time and memory usage. While it significantly reduces memory requirements, it \nmay increase overall computation time due to the need to recalculate some activations.\nOther parameters and techniques exist but play a minor role compared to those previously dis-\ncussed. In the next section, we will explore how to select and tune these parameters using a \nconcrete example.\nFine-tuning in practice\nLet’s now fine-tune an open-source model on our custom dataset. In this section, we will show an \nexample that implements LoRA and QLoRA for efficiency. Depending on the hardware you have \navailable, you can select the technique that best corresponds to your configuration.\nThere are many efficient open-weight models we can leverage for task or domain-specific use \ncases. To select the most relevant LLM, we need to consider three main parameters:\n\n\nSupervised Fine-Tuning\n220\n•\nLicense: Some model licenses only allow non-commercial work, which is a problem if \nwe want to fine-tune for a company. Custom licenses are common in this field, and can \ntarget companies with a certain number of users, for example.\n•\nBudget: Models with smaller parameter sizes (<10 B) are a lot cheaper to fine-tune and \ndeploy for inference than larger models. This is due to the fact that they can be run on \ncheaper GPUs and process more tokens per second.\n•\nPerformance: Evaluating the base model on general-purpose benchmarks or, even better, \ndomain- or task-specific benchmarks relevant to the final use case, is crucial. This helps \nensure that the model has the necessary capabilities to perform well on the intended \ntasks after fine-tuning.\nIn this chapter, we will choose Llama 3.1 8B, an open-weight model released by Meta. It has a \npermissive custom license (“Llama 3.1 Community License Agreement”) that allows commercial \nuse. With 8B parameters, it is small enough to fit on most GPUs while reaching a high level of \nperformance compared to its competitors. We can verify this using the Open LLM Leaderboard, \nas well as other benchmarks detailed in the model card.\nThere are specialized tools and libraries to fine-tune models. In particular, we recommend the \nfollowing:\n•\nTRL: This is a library created and maintained by Hugging Face to train LLMs using SFT \nand preference alignment. It is a popular and reliable library that tends to be the most \nup-to-date in terms of algorithms. It works in single and multi-GPU settings with FSDP \nand DeepSpeed.\n• \nAxolotl: Created by Wing Lian, this tool streamlines the fine-tuning of LLMs with reusable \nYAML configuration files. It is based on TRL but includes many additional features, such as \nautomatically combining datasets stored in various formats. It also supports single- and \nmulti-GPU settings with FSDP and DeepSpeed.\n•\nUnsloth: Created by Daniel and Michael Han, Unsloth uses custom kernels to speed up \ntraining (2-5x) and reduce memory use (up to 80% less memory). It is based on TRL and \nprovides many utilities, such as automatically converting models into the GGUF quanti-\nzation format. At the time of writing, it is only available for single-GPU settings.\nTo maximize efficiency, we will perform fine-tuning using the Unsloth library. The following \ncode is designed as part of our LLMOps pipeline, but can also be used as a stand-alone script. It \ncan also be executed in different environments, like SageMaker, cloud GPUs (like Lambda Labs or \nRunPod), Google Colab, and many others. We tested it on different GPUs, like A40, A100, and L4.\n\n\nChapter 5\n221\nTo install the Unsloth library and its dependencies, we recommend directly installing from the \nGitHub repository of the book (https://github.com/PacktPublishing/LLM-Engineering) or \nUnsloth’s repo (https://github.com/unslothai/unsloth). This approach is recommended be-\ncause the installation steps are regularly updated to address potential conflicts with dependencies:\n1.\nFirst, we want to access a gated model and (optionally) upload our fine-tuned model to \nHugging Face (https://huggingface.co/). This requires being logged in to an account. \nIf you don’t have an account, you can create it and store your API key (Settings | Access \nTokens | Create new token) in the .env file:\nHF_TOKEN = YOUR_API_KEY\n2. \nMake sure that your Comet ML API key is also in the .env file:\nCOMET_API_KEY = YOUR_API_KEY\n3. \nImport all the necessary packages:\nimport os\nimport torch\nfrom trl import SFTTrainer\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import TrainingArguments, TextStreamerfrom unsloth \nimport FastLanguageModel, is_bfloat16_supported\n4.\nLet’s now load the model to fine-tune and its corresponding tokenizer. We use Unsloth’s \nFastLaguageModel class with the .from_pretrained() method. In addition to the mod-\nel name, we need to specify the max sequence length (2,048 in this example). Finally, \nthe load_in_4bit argument indicates if we want to use QLoRA (quantized pre-trained \nweights) or LoRA.\nWe’ll use LoRA in this example because of faster training and higher quality, but you can \neasily switch to QLoRA if you don’t meet the VRAM requirements.\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"meta-llama/Meta-Llama-3.1-8B\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=False,\n)\n",
      "page_number": 235
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 245-252)",
      "start_page": 245,
      "end_page": 252,
      "detection_method": "topic_boundary",
      "content": "Supervised Fine-Tuning\n222\n5.\nNow that the model is loaded, we can define our LoRA configuration. Here, we use a rank \nof 32 that is large enough to imitate the writing style and copy the knowledge from our in-\nstruction samples. You can increase this value to 64 or 128 if your results are underwhelm-\ning. We also set an alpha of 32, without dropout and without bias, to speed up training. \nFinally, we target every linear layer to maximize the quality of the fine-tuning process.\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_\nproj\", \"o_proj\", \"gate_proj\"],\n)\n6. \nNext, we need to prepare the data in the right format for fine-tuning. In this example, \nwe don’t have a lot of samples in the llmtwin dataset (3,000 samples). This is an issue \nbecause the model might not correctly learn the chat template. To address this, we will \nupsample it with a high-quality general-purpose dataset called FineTome. This is a filtered \nversion of arcee-ai/The-Tome using the fineweb-edu-classifier. Instead of using \nthe 100,000 samples of this dataset, we will specify we only want 10,000 in the train split. \nWe concatenate these two datasets to create our final set.\ndataset1 = load_dataset(\"mlabonne/llmtwin\")\ndataset2 = load_dataset(\"mlabonne/FineTome-Alpaca-100k\", \nsplit=\"train[:10000]\")\ndataset = concatenate_datasets([dataset1, dataset2])\n7.\nNow, we need to format this data using a chat template. Let’s use the Alpaca template \nfor convenience. This template doesn’t require additional tokens, which makes it less \nerror-prone (but can slightly impact performance compared to ChatML). Here, we map \nall the instructions and answers to the Alpaca template. We manually add the end of sen-\ntence (EOS) token at the end of each message to ensure that the model learns to output \nit. Without it, it will keep generating answers without ever stopping.\nalpaca_template = \"\"\"Below is an instruction that describes a task. \nWrite a response that appropriately completes the request.\n### Instruction:\n\n\nChapter 5\n223\n{}\n### Response:\n{}\"\"\"\nEOS_TOKEN = tokenizer.eos_token\ndataset = dataset.map(format_samples, batched=True, remove_\ncolumns=dataset.column_names)\n8. Once the dataset is ready, we can divide it into training (95%) and test (5%) sets for val-\nidation during training.\ndataset = dataset.train_test_split(test_size=0.05)\n9. \nThe model is now ready to be trained. The SFTTrainer() class stores all the hyperparameters \nfor our training. In addition, we provide the model, tokenizer, LoRA configuration, and \ndatasets. Following the recommendations from the previous section, we set a learning \nrate of 3e-4 with a linear scheduler and a maximum sequence length of 2048. We train \nthis model for three epochs with a batch size of 2 and 8 gradient accumulation steps (for \nan effective batch size of 16). We also choose the adamw_8bit optimizer with a weight_\ndecay of 0.01. Depending on the GPU we use, it will automatically use FP16 or BF16 for \nthe activations. Finally, we report our training run to Comet ML for experiment tracking.\ntrainer = SFTTrainer(\n    model=model,\n   tokenizer=tokenizer,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=True,\n    args=TrainingArguments(\n        learning_rate=3e-4,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=8,\n        num_train_epochs=3,\n        fp16=not is_bfloat16_supported(),\n\n\nSupervised Fine-Tuning\n224\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        report_to=\"comet_ml\",\n        seed=0,\n    ),\n)\ntrainer.train()\nTraining this model on our concatenated dataset can take a few hours. For example, it \ntakes 50 minutes on an A100 GPU.\n10. Once it’s done, we can test it with a quick example. The goal is not to properly evaluate \nthe fine-tuned model, but to make sure that there are no obvious errors related to the \ntokenizer or chat template.\nFor fast inference, we can use FastLanguageModel.for_inference() from Unsloth. We \ndirectly format an instruction with the Alpaca format. Note that we provide an empty \nanswer to append the assistant header (### Response): at the end of the user instruction. \nThis forces the model to answer the instruction instead of completing it. We also use a text \nstreamer to stream the generation instead of waiting for it to be complete before printing it.\nFastLanguageModel.for_inference(model)\nmessage = alpaca_prompt.format(\"Write a paragraph to introduce \nsupervised fine-tuning.\", \"\")\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer=text_streamer, max_new_\ntokens=256, use_cache=True)\n11. Here is the answer provided by our model:\nSupervised fine-tuning is a method used to enhance a language model \nby providing it with a curated dataset of instructions and their \ncorresponding answers. This process is designed to align the model's \n\n\nChapter 5\n225\nresponses with human expectations, thereby improving its accuracy \nand relevance. The goal is to ensure that the model can respond \neffectively to a wide range of queries, making it a valuable tool \nfor applications such as chatbots and virtual assistants.\nThis is correct and properly formatted with the Alpaca chat template.\n12. Now that our model has been successfully fine-tuned, we can save it locally and/or push \nit to the Hugging Face Hub using the following functions.\nmodel.save_pretrained_merged(\"model\", tokenizer, save_\nmethod=\"merged_16bit\")\nmodel.push_to_hub_merged(\"mlabonne/TwinLlama-3.1-8B\", tokenizer, \nsave_method=\"merged_16bit\")\nCongratulations on fine-tuning a base model from scratch! During training, you can access Comet \nML to monitor your training loss, validation loss, and many other metrics. You want to make sure \nthat these metrics correspond to what is expected. Figure 5.11 shows the training run correspond-\ning to the previous code in Comet ML.\nFigure 5.11 – Four monitored metrics during fine-tuning in Comet ML\n\n\nSupervised Fine-Tuning\n226\nIn particular, three of these metrics are important to monitor:\n•\nTraining loss: It measures how well the model is performing on the task it’s being trained \nfor. The loss should continuously decrease on average, indicating improving performance. \nWe expect a rapid decrease at the beginning of training, followed by a long plateau. Spikes \nand continuous increases in the loss value are signs that the training is failing. In this \ncase, you might want to check the quality of your data, issues with the tokenizer, and \ntune parameters like learning rate and batch size. In Figure 5.11 (loss), you can see three \ndifferent phases corresponding to our three epochs.\n•\nValidation loss: It measures the loss using the validation set instead of the training set; \na well-fitted model typically shows both training and validation losses decreasing and \neventually stabilizing, with a small gap between them. This gap should be minimal but \nis expected to exist as the model will always perform slightly better on the training data. \nIf the training loss continues to decrease while the validation loss starts to increase, it’s a \nsign of overfitting. Conversely, if both curves remain flat at a relatively high loss value, it \nindicates underfitting. There are no universal “recommended ranges” for loss values, as \nthese depend on the specific problem and loss function used. However, you should look for \nconvergence and stability in both curves. In Figure 4.11 (eval_loss), we see a slight increase \nat step 340. This is still acceptable but might indicate that the model starts to overfit.\n•\nGradient norm: It represents the magnitude of the gradient vector during training. Large \ngradient norms can indicate training instability like overfitting, especially if accompanied \nby a divergence between training and validation losses. On the other hand, a stable or \ndecreasing gradient norm generally means that the model is converging toward a local \noptimum. To mitigate issues associated with large gradient norms, gradient clipping can \nbe employed. This technique involves setting a maximum threshold for the gradient norm, \neffectively limiting the size of parameter updates.\nIt is often interesting to try different learning rates and select the best model based on the minimal \nloss. Note that this is a proxy for real evaluations, which are covered in the next chapter.\nSummary\nThis chapter covered essential aspects of LLM fine-tuning, both in theory and practice. We ex-\namined the instruction data pipeline and how to create high-quality datasets, from curation \nto augmentation. Each pipeline stage offers optimization opportunities, particularly in quality \nassessment, data generation, and enhancement. This flexible pipeline can be adapted to your use \ncases by selecting the most relevant stages and techniques. \n\n\nChapter 5\n227\nWe applied this framework to real-world data from Chapter 3, using an LLM to convert raw text \ninto instruction-answer pairs. We then explored SFT techniques. This included an analysis of \nSFT’s advantages and limitations, methods for storing and parsing instruction datasets with chat \ntemplates, and an overview of three primary SFT techniques: full fine-tuning, LoRA, and QLoRA. \nWe compared these methods based on their impact on memory usage, training efficiency, and \noutput quality. The chapter concluded with a practical demonstration that involved fine-tuning \na Llama 3.1 8 B model on our custom instruction dataset. This example highlighted key steps and \nimplementation details for successful fine-tuning.\nIn the next chapter, we will use preference alignment techniques to create a new version of Twin-\nLlama-3.1-8B. We will generate a new dataset with chosen and rejected answers that will help us \ncalibrate the type of answers we expect from our model. We will detail many applications that \ncan benefit from this framework and how to implement it.\nReferences\n• \nTahori, Gulrajani, Zhang, Dubois, et al.. “Alpaca: A Strong, Replicable Instruction-Following \nModel” crfm.stanford.edu, March 13, 2023, https://crfm.stanford.edu/2023/03/13/\nalpaca.html.\n• \nSubhabrata Mukherjee et al.. “Orca: Progressive Learning from Complex Explanation Traces \nof GPT-4.” arXiv preprint arXiv:2306.02707, June 2023.\n• \nWing Lian and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong \nand “Teknium”. “Open-Orca/OpenOrca.” huggingface.co, 2023, https://huggingface.co/\ndatasets/Open-Orca/OpenOrca.\n• \nWeihao Zeng et al.. “Automatic Instruction Evolving for Large Language Models.” arXiv pre-\nprint arXiv:2406.00770, June 2024.\n•\nChunting Zhou et al.. “LIMA: Less Is More for Alignment.” arXiv preprint arXiv:2305.11206, \nMay 2023\n•\n01. AI. “Yi: Open Foundation Models by 01.AI.” arXiv preprint arXiv:2403.04652, March 2024.\n•\nAlex Birch. “LLM finetuning memory requirements.” blog.scottlogic.com, November 24, \n2023, https://blog.scottlogic.com/2023/11/24/llm-mem.html.\n•\nQuentin Anthony et al.. “Transformer Math 101.” blog.eleuther.ai, April 18, 2023, https://\nblog.eleuther.ai/transformer-math/.\n\n\nSupervised Fine-Tuning\n228\n•\nEdward J. Hu et al.. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv preprint \narXiv:2106.09685, June 2021.\n•\nTim Dettmers et al.. “QLoRA: Efficient Finetuning of Quantized LLMs.” arXiv preprint arX-\niv:2305.14314, May 2023.\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n\n\n6\nFine-Tuning with Preference \nAlignment\nSupervised Fine-Tuning (SFT) has been crucial in adapting LLMs to perform specific tasks. How-\never, SFT struggles to capture the nuances of human preferences and the long tail of potential \ninteractions that a model might encounter. This limitation has led to the development of more \nadvanced techniques for aligning AI systems with human preferences, grouped under the um-\nbrella term preference alignment.\nPreference alignment addresses the shortcomings of SFT by incorporating direct human or AI \nfeedback into the training process. This method allows a more nuanced understanding of human \npreferences, especially in complex scenarios where simple supervised learning falls short. While \nnumerous techniques exist for preference alignment, this chapter will primarily focus on Direct \nPreference Optimization (DPO) for simplicity and efficiency.\nIn this chapter, we will talk about the type of data that is required by preference alignment algo-\nrithms like DPO. We will build our own dataset to modify the writing style of our model, making \nit less artificial and more authentic. We will introduce the DPO algorithm and implement it to \nalign the model trained in Chapter 5.\nIn this chapter, we will cover the following topics:\n•\nUnderstanding preference datasets\n•\nHow to create our own preference dataset\n•\nDirect preference optimization (DPO)\n•\nImplementing DPO in practice to align our model\n",
      "page_number": 245
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 253-265)",
      "start_page": 253,
      "end_page": 265,
      "detection_method": "topic_boundary",
      "content": "Fine-Tuning with Preference Alignment\n230\nBy the end of this chapter, you will be able to create your own preference datasets and align \nmodels with diverse techniques.\nUnderstanding preference datasets\nThe principles for creating high-quality preference datasets are the same as those discussed in \nChapter 5 for instruction datasets. We want to maximize the accuracy, diversity, and complexity \nof our samples. To achieve this, we follow the same stages, as outlined in Figure 6.1: data curation, \ndeduplication, decontamination, quality evaluation, exploration, generation, and augmentation.\nFigure 6.1 – Overview of the post-training data pipeline covered in this chapter\nTo avoid repetition, this section will focus on the main differences between instruction and pref-\nerence datasets. We will introduce the structure of preference samples and the ideal size for pref-\nerence datasets. Then, we will focus on the two stages that differ most from creating instruction \ndatasets: data generation and evaluation.\nPreference data\nPreference datasets lack the standardization of instruction datasets due to varying data require-\nments across different training algorithms. Preference data comprises a collection of responses \nto a given instruction, ranked by humans or language models. This chapter focuses on DPO, so \nwe will examine the specific data format required by this algorithm.\nAs illustrated in Table 6.1, the structure of DPO datasets is straightforward: each instruction is \npaired with one preferred answer and one rejected answer. The objective is to train the model to \ngenerate the preferred response rather than the rejected one.\nAll the code examples from this chapter can be found on GitHub at https://github.\ncom/PacktPublishing/LLM-Engineering.\n\n\nChapter 6\n231\nInstruction\nTell me a joke about octopuses.\nChosen answer\nWhy don’t octopuses play cards in casinos? \nBecause they can’t count past eight.\nRejected answer\nHow many tickles does it take to make an \noctopus laugh? Ten tickles.\nTable 6.1 – Example of sample from the mlabonne/orpo-dpo-mix-40k dataset\nIn preference datasets, the rejected response is as important as the chosen one. Without the \nrejected response, the dataset would be a simple instruction set. Rejected responses represent \nthe behavior we aim to eliminate from the model. This provides a lot of flexibility and allows us \nto use preference datasets in many contexts. Here is a list of examples where preference datasets \nare more beneficial to use compared to using SFT alone:\n•\nChatbots: In conversational AI, the quality of responses often depends on subjective fac-\ntors like naturalness, engagement, and contextual appropriateness. A preference dataset \nallows the model to learn these nuanced aspects by comparing better and worse responses. \nSimple SFT might not capture the subtleties of what makes one response preferable over \nanother in a given context.\n•\nContent moderation: Determining whether content is appropriate or violates guidelines \noften involves nuanced judgments. Preference datasets can help the model learn to dis-\ntinguish between borderline cases by comparing examples of content that is and isn’t \nacceptable. This is more effective than binary classification through SFT, as it helps the \nmodel understand the reasoning behind moderation decisions.\n•\nSummarization: The quality of a summary often depends on factors like conciseness, \nrelevance, and coherence. By using preference datasets, models can learn to generate \nsummaries that humans find more useful and informative. Simple SFT might result in \nsummaries that are technically correct but less preferable to human readers.\n•\nCode generation: In coding tasks, there are often multiple correct solutions, but some \nare more efficient or readable, or follow better practices than others. Preference datasets \ncan help the model learn these qualitative aspects of code quality, which might not be \ncaptured by simple correctness-based SFT.\n\n\nFine-Tuning with Preference Alignment\n232\n•\nCreative writing: For tasks like story generation or poetry writing, the quality of the \noutput is highly subjective and multifaceted. Preference datasets can capture human \njudgments about style, creativity, and emotional impact better than instruction datasets, \nwhich might focus more on technical correctness or adherence to prompts.\n•\nTranslation: While traditional metrics like BLEU scores can measure translation accu-\nracy, they don’t always capture the fluency or naturalness of the translation. Preference \ndatasets can help models learn to produce translations that native speakers prefer, even \nwhen multiple translations are technically correct.\nIn all these scenarios, preference datasets enable a more refined training approach. They capture \nsubjective quality assessments and human preferences that extend beyond simple correctness or \nadherence to instructions. This method can produce models that generate output that is not only \ntechnically accurate but also better aligned with human judgment and preferences in complex, \nopen-ended tasks.\nUnlike instruction datasets, there are no standardized storage formats like Alpaca or ShareGPT. \nMost preference datasets follow a structure similar to that shown in Table 6.1, with columns for \nan instruction, a preferred answer, and a rejected answer. Multi-turn conversations are uncom-\nmon in preference alignment. At the time of writing, major fine-tuning libraries do not support \nmulti-turn conversations and typically extract only the first or last message in a conversation.\nData quantity\nDPO datasets typically require fewer samples than instruction datasets to significantly impact \nmodel behavior. As with instruction datasets, the required sample count depends on model size \nand task complexity. Larger models are more sample-efficient and thus require less data, while \ncomplex tasks demand more examples to capture the desired behavior. Once again, data quality \nis crucial, and a large number of preference pairs is generally beneficial.\nGeneral-purpose alignment is used by LLM providers to improve the overall performance of the \nfine-tuned models. This requires preference datasets with millions of samples. Major players in \nthe AI industry, including Nvidia and Meta, are converging on similar post-training pipelines, \ninvolving multiple rounds of preference alignment, and extensive use of synthetic data. This \nconsensus suggests that these methods are proving to be the most effective for pushing the \nboundaries of language model capabilities.\nOn a smaller scale, the open-source community uses datasets ranging from 10,000 to 100,000 \nsamples to enhance model performance. This approach has proven effective not only in improving \nbenchmark scores but also in healing networks after merging, pruning, and other modifications. \nGenerally, DPO is less destructive than SFT and has a milder impact on the final model.\n\n\nChapter 6\n233\nOn the other hand, tasks like the ones previously described require fewer preference pairs. \nTask-specific alignment focuses on improving model performance for a particular function, such \nas modifying the writing style, refusing certain instructions, and so on. These alignments can \noften be achieved with smaller datasets, ranging from 100 to 10,000 preference pairs, depending \non the task’s complexity.\nAn example of an application that requires few samples is instructing the model to state that it \nwasn’t trained by OpenAI, Meta, or another LLM provider. This can be achieved using a prefer-\nence dataset, where the rejected answers are those claiming alternative origins, and the chosen \nanswers are responses where the model correctly states that it was trained by you. A relatively \nsmall dataset of 200 to 500 pairs can be enough for this task.\nData generation and evaluation\nWhen creating preference datasets, data generation and evaluation are closely linked. We first \ncreate answers and then rate them to make the final dataset. In the following, we introduce both \nsteps as one process instead of two separate ones.\nGenerating preferences\nBefore making new preference data, it’s good to look at relevant open-source datasets. There are \nfewer of these compared to instruction datasets, but you can find high-quality preference data-\nsets on the Hugging Face Hub. These can be used for specific tasks or to add to your own dataset. \nWell-known preference datasets include the Anthropic HH-RLHF dataset, which has human \npreferences for helpful and harmless AI responses, and the OpenAI Summarize from Human \nFeedback dataset, which focuses on article summaries.\nDPO datasets can be created using various methods, each with its own trade-offs between quality, \ncost, and scalability. These methods can be tailored to specific applications and require varying \ndegrees of human feedback. We divide them into four main categories:\n• \nHuman-generated, human-evaluated datasets: This method involves hiring people to \nboth create responses to prompts and evaluate the quality of these responses. While this \napproach can capture nuanced human preferences and is ideal for complex tasks, it’s \nextremely resource-intensive and difficult to scale. As a result, it’s primarily used by large \nAI companies with substantial resources.\n•\nHuman-generated, LLM-evaluated datasets: This method can be useful if you have \na lot of existing human-generated content. However, it’s rarely used in practice due to \ninefficiency, as it still requires significant human input for response generation while \npotentially missing nuanced preferences during the LLM evaluation stage.\n\n\nFine-Tuning with Preference Alignment\n234\n•\nLLM-generated, human-evaluated datasets: This method offers a good balance between \nquality and efficiency. LLMs generate multiple responses to prompts, and humans rank \nthese responses. This approach is often preferred because humans are generally better at \njudging answers than writing them from scratch. It allows the rapid generation of diverse \nresponses while still capturing human preferences effectively. However, it may not provide \ncreative or unexpected responses that humans might generate.\n•\nLLM-generated, LLM-evaluated datasets: Fully synthetic datasets, where both gener-\nation and evaluation are done by LLMs, are becoming increasingly common due to their \nscalability and cost-effectiveness. This method can produce massive datasets quickly and \nimproves as LLM capabilities advance. However, it requires careful prompt engineering to \nensure quality and diversity, and may perpetuate biases or limitations of the generating \nLLM.\nIn practice, human-generated datasets are expensive, difficult to scale, and not necessarily of \nthe highest quality. On the other hand, human evaluation is quite valuable but can be difficult \nto scale, which is why large datasets benefit from LLM evaluation. In addition to these high-level \nconsiderations, the way you obtain your data and how you plan to use it also need to be considered. \nFor example, applications with many users can embed a feedback mechanism to provide prefer-\nences. This can be as simple as a like and dislike score, or something more in-depth with text.\nNote that evaluation is not always required and preferences can emerge naturally from the gen-\neration process. For instance, it is possible to use a high-quality model to generate preferred \noutputs and a lower-quality or intentionally flawed model to produce less preferred alternatives. \nThis creates a clear distinction in the preference dataset, allowing more effective training of AI \nsystems to recognize and emulate high-quality outputs. The Intel/orca_dpo_pairs dataset \navailable on the Hugging Face Hub was created with this process.\nAnother approach is to compare model-generated outputs with human-written responses, which \ncan provide insights into how well the model aligns with actual human preferences and highlight \nareas where the model may be lacking. This can be used to copy a particular style and give a more \nauthentic tone to the model.\nTips for data generation\nThe data generation is consistent between instruction and preference datasets. Prompts should \nbe designed to encourage diversity and complexity in the model’s responses. By crafting prompts \nthat explicitly request different approaches or styles, we can ensure a wide range of outputs that \ncapture the varied nature of human preferences. \n\n\nChapter 6\n235\nFor instance, when generating summaries, one might request variations such as concise sum-\nmaries, detailed summaries, and summaries focusing on key points. This approach not only \nproduces a diverse dataset but also helps in understanding how different styles and approaches \nalign with human preferences.\nIntroducing variability in the outputs is another crucial aspect of generating synthetic preference \ndatasets. This can be achieved by manipulating the temperature settings or employing other \nsampling methods in the LLM. Higher temperature settings tend to produce more creative and \ndiverse responses, while lower settings result in more focused and deterministic outputs. This \ncreates a trade-off between diversity and coherence, which depends on the kind of data we want \nto generate. For example, generating code requires low creativity, thus low temperature, while \nwriting articles can be high temperature.\nUsing multiple LLMs to generate samples can be better than using just one model. Some LLMs are \nbetter at specific tasks, and this approach also adds more variety. This approach is used by popular \nopen-source datasets like argilla/Capybara-Preferences, combining GPT-4 with open-weight \nmodels. The evaluation process then selects the chosen and the rejected answers.\nEvaluating preferences\nData evaluation can be performed by human raters or automated with LLMs. LLM evaluation\ninvolves developing detailed criteria, creating a prompt that clearly communicates these guide-\nlines to the LLM, and using the model to select preferred and rejected responses. While more \nscalable than human rating and allowing the consistent application of criteria, this quality of \nLLM evaluation depends directly on the model’s performance and the provided guidelines. It \nmay miss subtle human preferences or cultural nuances. However, as LLMs continue to improve, \ntheir ability to make nuanced judgments improves as well, potentially leading to higher-quality \ndatasets over time.\nImplementing LLM evaluation for preference datasets can be done through absolute scoring or \npairwise ranking. In absolute scoring, the LLM assigns a numerical score or categorical rating to \neach response based on predefined criteria. This method is straightforward but may suffer from \ninconsistency across different prompts or evaluation sessions. Pairwise ranking, on the other \nhand, involves presenting the LLM with two responses and asking it to choose the better one or \nrank them. This approach more closely mimics the format of human evaluation and can lead to \nmore consistent results.\n\n\nFine-Tuning with Preference Alignment\n236\nFor absolute scoring, you would create a prompt that outlines the evaluation criteria and asks \nthe LLM to rate the response on a specific scale (e.g., 1-5 or poor/fair/good/excellent). The prompt \nmight look like this: “Rate the following response on a scale of 1-5 based on relevance, coherence, \nand helpfulness: [INSERT RESPONSE].” For pairwise ranking, the prompt could be: “Compare the \nfollowing two responses. Which one is better in terms of relevance, coherence, and helpfulness? \nResponse A: [INSERT RESPONSE A] Response B: [INSERT RESPONSE B].”\nThe comparative nature of preference datasets makes pairwise ranking an ideal approach for \nevaluation. This method is generally more accurate and more closely correlated to human judg-\nment than absolute scoring. Pairwise ranking mimics the natural way humans compare options, \nmaking it easier for both human raters and LLMs to provide consistent and meaningful evaluations.\nWe can further improve the accuracy of pairwise ranking by providing a ground-truth answer \nand using chain-of-thought reasoning. This approach encourages the evaluating LLM to consider \nmultiple aspects of the responses and articulate its decision-making process, leading to more \nthorough and justified evaluations. When no ground-truth answer is available, we can prompt \nthe LLM to create a grading note, which is a description of the expected answer. This technique \nworks particularly well in scenarios where the LLM doesn’t have extensive knowledge about a \ngiven topic, as it forces the model to establish clear criteria for evaluation before assessing the \nresponses.\nHere’s a concrete implementation of an LLM-as-a-judge prompt to perform pairwise ranking:\nInstruction\nYou are an answer judge. Your goal is to compare answer A and answer B. I want to know \nwhich answer does a better job of answering the instruction in terms of relevance, accuracy, \ncompleteness, clarity, structure, and conciseness.\nInstruction: {instruction}\nAnswer A: {answer_a}\nAnswer B: {answer_b}\nExplain your reasoning step by step and output the letter of the best answer using the following \nstructure:\nReasoning: (compare the two answers)\nBest answer: (A or B)\nTable 6.2 – Example of LLM-as-a-judge prompt for pairwise ranking with one instruction \nand two answers\n\n\nChapter 6\n237\nHowever, it’s important to note that LLM-based evaluation can be subject to several types of bias:\n•\nPosition bias: In relative scoring, LLM judges tend to favor the first answer presented. \nThis bias can skew results and lead to inaccurate preferences.\n•\nLength bias: Similar to humans, LLM judges often show a preference for longer answers, \npotentially overlooking the quality of shorter, more concise responses.\n•\nFamily bias: LLM judges may favor responses that are generated by themselves or models \nfrom the same family, potentially due to similarities in language patterns or knowledge \nbases.\nTo mitigate these biases and enhance the quality of preference datasets, several solutions can \nbe implemented. One key approach is to randomize the order of answer A and answer B in each \ncomparison, which can counteract position bias by ensuring that the order of presentation doesn’t \nconsistently influence the evaluation. Another valuable strategy involves providing few-shot \nexamples that demonstrate a balanced distribution of scores. These examples serve to calibrate \nthe judge LLM’s internal scoring mechanism and can effectively address both length and family \nbias by illustrating that shorter answers or those from different model families can also be of \nhigh quality. Additionally, employing multiple models as a jury, rather than relying on a single \nLLM judge, can significantly improve the robustness of the evaluation process. This multi-model \napproach helps to balance out individual biases that may be present in any single model, leading \nto a more comprehensive and accurate assessment of the responses.\nIn the next section, we will create our own preference dataset. We will rely on the data generation \nprocess to naturally create chosen (human-generated) and rejected (LLM-generated) answers.\nCreating our own preference dataset\nOur model can currently write paragraphs about topics related to machine learning, but it doesn’t \nhave the same writing style as the original authors. This is a typical use case for preference align-\nment, where we want to change the “voice” of the model to closely imitate the source data. It’s \nimportant to note that, experimentally, DPO tends to make models more verbose and pushes \nthem to use very formal language. Therefore, the training will need to use DPO surgically to avoid \nthis pitfall and instead adopt the less formal style of these blog articles.\nIn this section, we will create a preference dataset where the chosen answers are extracts from \nthe text, while rejected answers are generated by the model. To implement it, we will modify the \ncode created in Chapter 5, which was designed to generate instruction datasets.\n\n\nFine-Tuning with Preference Alignment\n238\nAs seen in the previous section, preference and instruction datasets rely on the same principles. \nInstead of pairs of instructions and answers, we need triples (instruction, answer 1, answer 2). \nWhat’s interesting in this setting is that we have ground-truth answers in the text chunks, which \nmeans we don’t need complex evaluation processes like LLM judges. To make sure that these \nextracts are high-quality, we will implement two additional quality filters, based on length and \npunctuation. Figure 6.2 summarizes the end-to-end process:\nFigure 6.2 – Synthetic data generation pipeline from raw text to preference dataset\nWe are now ready to implement the preference data generation pipeline:\n1. \nWe start by importing the necessary libraries.\nimport concurrent.futures\nimport json\nimport re\nfrom typing import List, Tuple\nfrom datasets import Dataset\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\n\n\nChapter 6\n239\n2.\nInstead of the InstructionAnswerSet class, we now have a PreferenceSet class. This \nclass is designed to handle triples of instructions, generated answers (rejected), and ex-\ntracted answers (chosen).\nclass PreferenceSet:\n    def __init__(self, triples: List[Tuple[str, str, str]]):\n        self.triples = triples\n    @classmethod\n    def from_json(cls, json_str: str) -> 'PreferenceSet':\n        data = json.loads(json_str)\n        triples = [(triple['instruction'], triple['generated_\nanswer'], triple['extracted_answer'])\n                   for triple in data['preference_triples']]\n        return cls(triples)\n    def __iter__(self):\n        return iter(self.triples)\n3. \nThe load_articles_from_json, clean_text, and extract_substrings functions remain \nunchanged from the original code. Let’s start with load_articles_from_json, which takes \nour JSON file (cleaned_documents.json) containing the articles as input and returns a \nHugging Face dataset with the text and metadata (ID, platform, author ID, author full \nname, link).\ndef load_articles_from_json(file_path: str) -> Dataset:\n    with open(file_path, \"r\") as file:\n        data = json.load(file)\n    return Dataset.from_dict(\n        {\n            \"id\": [item[\"id\"] for item in data[\"artifact_data\"]],\n            \"content\": [item[\"content\"] for item in data[\"artifact_\ndata\"]],\n            \"platform\": [item[\"platform\"] for item in \ndata[\"artifact_data\"]],\n            \"author_id\": [item[\"author_id\"] for item in \ndata[\"artifact_data\"]],\n\n\nFine-Tuning with Preference Alignment\n240\n            \"author_full_name\": [item[\"author_full_name\"] for item \nin data[\"artifact_data\"]],\n            \"link\": [item[\"link\"] for item in data[\"artifact_\ndata\"]],\n        }\n    )\n4.\nThe clean_text function removes non-alphanumeric characters except for apostrophes, \nperiods, commas, exclamation marks, and question marks. It also replaces multiple \nwhitespaces with a single space to ensure proper formatting.\ndef clean_text(text: str) -> str:    text = re.sub(r\"[^\\w\\s.,!?']\", \n\" \", text)    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n5. \nThe extract_substrings function splits articles into chunks with a length between 1,000 \nand 2,000 characters. To make sure that the splitting doesn’t break sentences, which could \nmodify their meanings, we use a regex to only split after the end of a sentence.\ndef extract_substrings(dataset: Dataset, min_length: int = 1000, \nmax_length: int = 2000) -> List[str]:\n    extracts = []\n    sentence_pattern = r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)\n(?<=\\.|\\?|\\!)\\s\"\n    for article in dataset[\"content\"]:\n        cleaned_article = clean_text(article)\n        sentences = re.split(sentence_pattern, cleaned_article)\n        current_chunk = \"\"\n        for sentence in sentences:\n            sentence = sentence.strip()\n            if not sentence:\n                continue\n            if len(current_chunk) + len(sentence) <= max_length:\n                current_chunk += sentence + \" \"\n            else:\n                if len(current_chunk) >= min_length:\n                    extracts.append(current_chunk.strip())\n\n\nChapter 6\n241\n                current_chunk = sentence + \" \"\n        if len(current_chunk) >= min_length:\n            extracts.append(current_chunk.strip())\n    return extracts\n6. \nThe generate_preference_triples function replaces the original generate_instruction_\nanswer_pairs function. The prompt is adapted from the instruction version and is de-\nsigned to generate triples instead of pairs. It also provides general guidance about the \ntype of instructions we’re interested in, how to extract answers from articles, and how \nto style them:\ndef generate_preference_triples(extract: str, client: OpenAI) -> \nList[Tuple[str, str, str]]:\n    prompt = f\"\"\"Based on the following extract, generate five \ninstruction-answer triples. Each triple should consist of:\n1. An instruction asking about a specific topic in the context.\n2. A generated answer that attempts to answer the instruction based \non the context.\n3. An extracted answer that is a relevant excerpt directly from the \ngiven context.\nInstructions must be self-contained and general, without explicitly \nmentioning a context, system, course, or extract.\nImportant:\n- Ensure that the extracted answer is a verbatim copy from the \ncontext, including all punctuation and apostrophes.\n- Do not add any ellipsis (...) or [...]  to indicate skipped text \nin the extracted answer.\n- If the relevant text is not continuous, use two separate sentences \nfrom the context instead of skipping text.\nProvide your response in JSON format with the following structure:\n{{\n    \"preference_triples\": [\n        {{\n            \"instruction\": \"...\",\n\n\nFine-Tuning with Preference Alignment\n242\n            \"generated_answer\": \"...\",\n            \"extracted_answer\": \"...\"\n        }},\n        ...\n    ]\n}}\n    Extract:\n    {extract}\n\"\"\"\n7.\nIn the same function, we use GPT-4o-mini to generate our answers using JSON mode. We \nspecify in the system prompt that we want triples instead of pairs. The JSON answers are \ndirectly parsed by our PreferenceSet class to return the expected list of tuples.\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant who \ngenerates instruction-answer triples based on the given context. \nEach triple should include an instruction, a generated answer, and \nan extracted answer from the context. Provide your response in JSON \nformat.\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_format={\"type\": \"json_object\"},\n        max_tokens=2000,\n        temperature=0.7,\n    )\n    result = PreferenceSet.from_json(completion.choices[0].message.\ncontent)\n    return result.triples\n",
      "page_number": 253
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 266-274)",
      "start_page": 266,
      "end_page": 274,
      "detection_method": "topic_boundary",
      "content": "Chapter 6\n243\n8. Two new filtering functions are introduced for the preference data pipeline: filter_short_\nanswers and filter_answer_format. These functions filter out short answers and ensure \nthat answers start with an uppercase letter and end with proper punctuation. We use \nthem as heuristics to filter out samples with poor quality.\ndef filter_short_answers(dataset: Dataset, min_length: int = 100) -> \nDataset:\n    def is_long_enough(example):\n        return len(example['chosen']) >= min_length\n    return dataset.filter(is_long_enough)\ndef filter_answer_format(dataset: Dataset) -> Dataset:\n    def is_valid_format(example):\n        chosen = example['chosen']\n        return (len(chosen) > 0 and\n                chosen[0].isupper() and\n                chosen[-1] in ('.', '!', '?'))\n    return dataset.filter(is_valid_format)\n9.\nThe create_preference_dataset function replaces the original create_instruction_\ndataset function. This function now works with triples instead of pairs and uses different \ncolumn names in the resulting dataset.\ndef create_preference_dataset(dataset: Dataset, client: OpenAI, num_\nworkers: int = 4) -> Dataset:\n    extracts = extract_substrings(dataset)\n    preference_triples = []\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_\nworkers) as executor:\n        futures = [\n            executor.submit(generate_preference_triples, extract, \nclient)\n            for extract in extracts\n        ]\n        for future in tqdm(concurrent.futures.as_completed(futures), \ntotal=len(futures)):\n\n\nFine-Tuning with Preference Alignment\n244\n            preference_triples.extend(future.result())\n    instructions, generated_answers, extracted_answers = \nzip(*preference_triples)\n    return Dataset.from_dict(\n        {\n            \"prompt\": list(instructions),\n            \"rejected\": list(generated_answers),\n            \"chosen\": list(extracted_answers)\n        }\n    )\n10. The main function is updated to include the new filtering steps and to use the preference \ndataset creation function:\ndef main(dataset_id: str) -> Dataset:\n    client = OpenAI()\n    # 1. Load the raw data\n    raw_dataset = load_articles_from_json(\"cleaned_documents.json\")\n    print(\"Raw dataset:\")\n    print(raw_dataset.to_pandas())\n    # 2. Create preference dataset\n    dataset = create_preference_dataset(raw_dataset, client)\n    print(\"Preference dataset:\")\n    print(dataset.to_pandas())\n    # 3. Filter out samples with short answers\n    dataset = filter_short_answers(dataset)\n    # 4. Filter answers based on format\n    dataset = filter_answer_format(dataset)\n    # 5. Export\n    dataset.push_to_hub(dataset_id)\n    return dataset\n\n\nChapter 6\n245\nThe create_preference_dataset() function generated 2,970 samples. This dataset is then heav-\nily filtered to only retain 1,467 samples by removing answers that are too short or not properly \nformatted (for example, answers that start with an uppercase letter or end with a period, excla-\nmation mark, or question mark).\nThe final dataset is available on the Hugging Face Hub at the following address: https://\nhuggingface.co/datasets/mlabonne/llmtwin-dpo. You can see in Figure 6.3 an example that \ncaptures a subtle nuance in terms of writing style. Both answers are correct, but the chosen (ex-\ntracted) answer sounds slightly more casual.\nFigure 6.3 – Screenshot of the mlabonne/llmtwin-dpo preference dataset on the Hugging \nFace Hub\nTo produce this dataset, we iterated many times over the prompt to generate the data. This re-\nquired some manual evaluation and experiments until we reached satisfying results. The quality \nof the prompt is fundamental in this process, which is why it is recommended to follow a similar \nprocess to generate your own preference datasets.\nIn the next section, we will introduce concepts related to Reinforcement Learning from Human \nFeedback (RLHF) and DPO. This will cover new parameters and ideas that are implemented in \nthe final section of this chapter.\nPreference alignment\nPreference alignment regroups techniques to fine-tune models on preference data. In this section, \nwe provide an overview of this field and then focus on the technique we will implement: Direct \nPreference Optimization (DPO).\n\n\nFine-Tuning with Preference Alignment\n246\nReinforcement Learning from Human Feedback\nReinforcement Learning from Human Feedback (RLHF) combines reinforcement learning\n(RL) with human input to align models with human preferences and values. RLHF emerged as a \nresponse to challenges in traditional RL methods, particularly the difficulty of specifying reward \nfunctions for complex tasks and the potential for misalignment between engineered rewards \nand intended objectives.\nThe origins of RLHF can be traced back to the field of preference-based reinforcement learning\n(PbRL), which was independently introduced by Akrour et al. and Cheng et al. in 2011. PbRL aimed \nto infer objectives from qualitative feedback, such as pairwise preferences between behaviors, \nrather than relying on quantitative reward signals. This approach addressed some of the limita-\ntions of conventional RL, where defining appropriate reward functions can be challenging and \nprone to reward hacking or unintended behaviors.\nThe term RLHF was coined later, around 2021-2022, as the approach gained prominence in the \ncontext of training LLMs. However, the core ideas had been developing for years prior. A seminal \npaper by Christiano et al. in 2017 demonstrated the effectiveness of learning reward models from \nhuman preferences and using them to train RL agents. This work showed that RLHF could match \nor exceed the performance of agents trained on hand-engineered rewards, but with significantly \nless human effort.\nAt its core, RLHF works by iteratively improving both a reward model and a policy:\n•\nReward model learning: Instead of using a pre-defined reward function, RLHF learns a \nreward model from human feedback. This is typically done by presenting humans with \ndifferent answers and asking them to indicate which one they prefer. These preferences \nare used to train a reward model, often using a Bradley-Terry model or similar approaches \nthat map preferences to underlying utility functions.\n•\nPolicy optimization: With the learned reward model, standard RL algorithms can be \nused to optimize a policy. This policy generates new behaviors that aim to maximize the \npredicted rewards from the learned model.\n•\nIterative improvement: As the policy improves, it generates new behaviors that can be \nevaluated by humans, leading to refinements in the reward model. This cycle continues, \nideally resulting in a policy that aligns well with human preferences.\nA key innovation in RLHF is its approach to handling the high cost of human feedback. Rather \nthan requiring constant human oversight, RLHF allows for asynchronous and sparse feedback.\n\n\nChapter 6\n247\nThe learned reward model serves as a proxy for human preferences, enabling the RL algorithm \nto train continuously without direct human input for every action.\nAs an example, Figure 6.4 shows a high-level view of the Proximal Policy Optimization (PPO) \nalgorithm, which is one of the most popular RLHF algorithms. Here, the reward model is used to \nscore the text that is generated by the trained model. This reward is regularized by an additional \nKullback–Leibler (KL) divergence factor, ensuring that the distribution of tokens stays similar \nto the model before training (frozen model).\nFigure 6.4 – High-level view of the PPO algorithm for preference alignment\nWhile RLHF has proven effective for aligning AI systems with human preferences, it faces chal-\nlenges due to its iterative nature and reliance on a separate reward model, which can be compu-\ntationally expensive and potentially unstable. Despite theoretical superiority, RLHF algorithms \nhave also experimentally underperformed compared to simpler approaches. One such approach \nthat has gained significant attention is DPO.\n\n\nFine-Tuning with Preference Alignment\n248\nDirect Preference Optimization\nIntroduced by Rafailov et al. in their 2023 paper Direct Preference Optimization: Your Language \nModel is Secretly a Reward Model, DPO offers a streamlined alternative to traditional RLHF methods.\nDPO’s core innovation lies in its reformulation of the preference learning problem. Unlike RLHF, \nwhich typically involves training a separate reward model and then using reinforcement learning \nalgorithms like PPO to fine-tune the language model, DPO takes a more direct approach. \nIt derives a closed-form expression for the optimal policy under the standard RLHF objective of \nmaximizing expected reward subject to a KL-divergence constraint with a reference policy. This \nmathematical insight allows DPO to express the preference learning problem directly in terms of \nthe policy, eliminating the need for a separate reward model or complex reinforcement learning \nalgorithms.\nIn practical terms, DPO can be implemented as a simple binary cross-entropy loss function that \noperates directly on the language model’s output probabilities. This loss function encourages the \nmodel to assign higher probability to preferred responses and lower probability to non-preferred \nresponses, while maintaining closeness to a reference (frozen) model. The importance of the ref-\nerence model is directly controlled via a beta parameter between 0 and 1. The reference model is \nignored when beta is equal to 0, which means that the trained model can be very different from \nthe SFT one. In practice, a value of 0.1 is the most popular one, but this can be tweaked, as we’ll \nsee in the next section.\nThe simplicity of this approach allows optimization using standard gradient descent techniques, \nwithout the need for sampling from the model during training or implementing complex RL \nalgorithms. Figure 6.5 shows a high-level view of the DPO algorithm, greatly simplifying the \ntraining process compared to Figure 6.4.\n\n\nChapter 6\n249\nFigure 6.5 – High-level view of the DPO algorithm for preference alignment\nDPO has several advantages over traditional RLHF methods. As previously mentioned, it signifi-\ncantly simplifies the preference learning pipeline, reducing the engineering complexity associated \nwith RLHF methods. By eliminating the need for a separate reward model and RL algorithms, DPO \nis more computationally efficient than traditional RLHF approaches. Particularly when trained \nwith adapters (LoRA, QLoRA), the frozen and trained models don’t have to be separated. Indeed, \nsince we’re only training adapters, the trained model is not modified. This allows us to only load \none model instead of two, which saves additional VRAM.\nDespite its simplicity, DPO often matches the performance of more complex RLHF methods. It \nalso tends to be more stable during training and less sensitive to hyperparameters. The simpli-\nfied approach makes DPO easier to implement and scale, particularly for small teams without \nextensive RL knowledge.\n\n\nFine-Tuning with Preference Alignment\n250\nWhile RLHF allows iterative improvement through multiple training rounds and can dynamically \nadapt to new preferences, DPO offers a more straightforward path to achieving similar results. \nThe choice between DPO and PPO-based RLHF often comes down to a trade-off between ease of \nimplementation and potential peak performance. For large-scale training runs with millions of \npreference samples, PPO-inspired methods still have a higher performance ceiling. However, for \nmost applications, DPO provides the majority of the performance benefits at a lower computa-\ntional and engineering cost.\nBoth RLHF and DPO benefit significantly from the integration of synthetic data. As LLMs become \nmore capable, they can generate data that surpasses human-created content in quality and di-\nversity. This enables a virtuous cycle where better models produce better training data, which \nin turn leads to further model improvements. The iterative nature of both approaches allows \nmultiple rounds of model refinement, each focusing on different aspects of model performance \nand gradually enhancing capabilities across various domains.\nDespite its advantages, DPO is not without drawbacks. Like RLHF, DPO still requires paired pref-\nerence data, which can be expensive and time-consuming to collect. DPO lacks some of the the-\noretical guarantees associated with reinforcement learning approaches. There may be scenarios \nwhere the added flexibility of RLHF is beneficial, particularly for complex tasks or environments.\nNonetheless, DPO is ideal in most cases, including our twin LLM example. In the next section, \nwe will implement it using Unsloth.\nImplementing DPO\nIn this section, we will DPO fine-tune the TwinLlama-3 1-8B model we created in Chapter 5. For \nease of use and to maximize performance, we will again use the Unsloth library for our DPO im-\nplementation. Depending on the available VRAM, you can choose between LoRA (higher quality, \nspeed, and VRAM usage) and QLoRA (lower quality, speed, and VRAM usage). This technique, \nalong with other preference alignment algorithms, is also available in TRL and Axolotl.\nThis example can be seen as an advanced application of DPO. Indeed, our objective of imitating \na writing style conflicts with the natural tendency of DPO to encourage formal language. This is \npartly due to the fact that chosen answers are often more formal than rejected ones. In practice, \nthis will force us to do light fine-tuning, with a low learning rate and number of epochs. To find \nthe best hyperparameters, we trained over 20 models and compared their outputs on a set of \nquestions, including “Write a paragraph to introduce supervised fine-tuning.” This allowed us \nto select the model and parameters that worked best for this task.\n\n\nChapter 6\n251\nThe dependencies are the same as those in Chapter 5 with SFT and can be found in the book’s \nGitHub repository (https://github.com/PacktPublishing/LLM-Engineering) or in Unsloth’s \nrepo (https://github.com/unslothai/unsloth):\n1.\nFirst, we want to access a gated model and (optionally) upload our fine-tuned model to \nHugging Face (https://huggingface.co/). This requires us to log in to an account. If \nyou don’t have an account, you can create one and store your API key (Settings | Access \nTokens | Create new token) in the .env file:\nHF_TOKEN = YOUR_API_KEY\n2. \nMake sure that your Comet ML API key is also in the .env file. Otherwise, the code will \ncrash and raise an error when training starts.\nCOMET_API_KEY = YOUR_API_KEY\n3. \nBefore we import all the necessary packages, we want to apply a patch for the DPOTrainer\nclass from TRL. This fixes the DPO logs in notebook environments.\nfrom unsloth import PatchDPOTrainer\nPatchDPOTrainer()\n4. \nWe can now import the other libraries. The main difference between DPO and SFT is the \nimport of DPOConfig and DPOTrainer from TRL, which are specific to DPO training.\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth import FastLanguageModel, is_bfloat16_supportedfrom trl \nimport DPOConfig, DPOTrainer\n5. \nThis step loads our fine-tuned model from Chapter 5. We use the same configuration with \na max_seq_length of 2048. You can activate QLoRA by setting load_in_4bit to True. In \nthe following, we will perform LoRA DPO fine-tuning for increased speed and quality.\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"mlabonne/TwinLlama-3.1-8B\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=False,\n)\n",
      "page_number": 266
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 275-282)",
      "start_page": 275,
      "end_page": 282,
      "detection_method": "topic_boundary",
      "content": "Fine-Tuning with Preference Alignment\n252\n6. \nLet’s now prepare the model for PEFT with the LoRA configuration. We increase the rank \n(r) and lora_alpha from 32 (as it was in Chapter 5) to 64. This will allow more expressive \nfine-tuning. We keep a dropout of 0 for speed and we target every linear module as per \nusual.\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_\nproj\", \"o_proj\", \"gate_proj\"],\n)\n7. \nWe load the llmtwin-dpo dataset (training split), which contains our prompts, chosen, \nand rejected answers.\ndataset = load_dataset(\"mlabonne/llmtwin-dpo\", split=\"train\")\n8. The data preparation is significantly different from the SFT example in Chapter 5. Here, we \nhave triples with a prompt, a chosen answer, and a rejected answer. In the format_samples\nfunction, we apply the Alpaca chat template to each individual message. Note that the \ninstruction is the only one that requires the chat format: chosen and rejected answers \nonly need to be concatenated with the end of sentence (EOS) token. Finally, we create a \ntrain/test split with a 95%/5% ratio.\nalpaca_template = \"\"\"Below is an instruction that describes a task. \nWrite a response that appropriately completes the request.\n### Instruction:\n{}\n### Response:\n\"\"\"\nEOS_TOKEN = tokenizer.eos_token\ndef format_samples(example):\n    example[\"prompt\"] = alpaca_template.format(example[\"prompt\"])\n    example[\"chosen\"] = example['chosen'] + EOS_TOKEN\n\n\nChapter 6\n253\n    example[\"rejected\"] = example['rejected'] + EOS_TOKEN\n    return {\"prompt\": example[\"prompt\"], \"chosen\": \nexample[\"chosen\"], \"rejected\": example[\"rejected\"]}\ndataset = dataset.map(format_samples)\ndataset = dataset.train_test_split(test_size=0.05)\n9. \nThe model and data are now ready, so we can start fine-tuning. Compared to SFT, there \nare a few new parameters, like ref_model and beta. Since we’re using LoRA (or QLoRA), \nwe don’t directly train the model but instead the adapters. This means we can use the \noriginal model (without adapters) as a reference, saving a lot of VRAM. The beta param-\neter controls the importance of the reference model. A standard value of 0.1 works well \nin most scenarios, but we decided to increase it to 0.5 based on our experiments. This is \ndue to the fact that the trained model used formal language with lower values. Having it \ncloser to the reference model helps to fix this issue.\nThe learning rate is also lower (from 3e-4 for SFT to 2e-6 here). We train for 1 epoch instead \nof 3, and the max_seq_length parameter is now broken down into two new parameters: \nmax_prompt_length (prompt only) and max_length (prompt and answer). Note that we \nalso replaced the TrainingArguments class with DPOConfig.\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=None,\n    tokenizer=tokenizer,\n    beta=0.5,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    max_length=max_seq_length//2,\n    max_prompt_length=max_seq_length//2,\n    args=DPOConfig(\n        learning_rate=2e-6,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        gradient_accumulation_steps=8,\n        num_train_epochs=1,\n\n\nFine-Tuning with Preference Alignment\n254\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        eval_strategy=\"steps\",\n        eval_steps=0.2,\n        logging_steps=1,\n        report_to=\"comet_ml\",\n        seed=0,\n    ),\n)\ntrainer.train()\n10. Once the model is trained, we can run it for a quick sanity check. This step is similar to the \nSFT example. It prepares the model for inference and generates a response to a prompt.\nFastLanguageModel.for_inference(model)\nmessage = alpaca_template.format(\"Write a paragraph to introduce \nsupervised fine-tuning.\", \"\")\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer=text_streamer, max_new_\ntokens=256, use_cache=True)\n11. The trained DPO model returns the following response:\nSupervised fine-tuning is a method used to enhance the performance \nof pre-trained language models by utilizing labeled data. This \ntechnique involves taking a pre-trained model and refining it on \na specific task, such as content creation or customer service. By \nproviding the model with relevant data and guidance, it can learn to \ngenerate outputs that align more closely with the desired outcomes. \nThis approach allows for the creation of more specialized models \nthat can tackle complex tasks with greater accuracy and efficiency.\n\n\nChapter 6\n255\nWe can compare it with the answer provided by the SFT model:\nSupervised fine-tuning is a method used to enhance a language model \nby providing it with a curated dataset of instructions and their \ncorresponding answers. This process is designed to align the model's \nresponses with human expectations, thereby improving its accuracy \nand relevance. The goal is to ensure that the model can respond \neffectively to a wide range of queries, making it a valuable tool \nfor applications such as chatbots and virtual assistants.\nThe DPO model provides an answer that is both more accurate and closer to the desired \nwriting style. It correctly identifies pre-training language models as source models for \nSFT. It also mentions domain or task-specific finetunes instead of alignment with “human \nexpectations,” which is closer to the preference alignment stage. The answer is also less \nformal and something we would use in a blog post.\n12. Finally, the last step consists of saving the trained model locally and pushing it to the \nHugging Face Hub.\nmodel.save_pretrained_merged(\"model\", tokenizer, save_\nmethod=\"merged_16bit\")\nCongratulations! We have trained and exported our DPO model. It is now available on the Hug-\nging Face Hub at https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO. Compared to \nSFT, DPO has a few additional metrics that need to be tracked during training. Figure 6.6 shows \nthe Comet ML dashboard with the main metrics. You can publicly access it using the following \nURL: https://www.comet.com/mlabonne/llm-twin-training/\n\n\nFine-Tuning with Preference Alignment\n256\nFigure 6.6 – Experiment tracking in Comet ML with DPO metrics\nLet’s review these metrics:\n•\nTraining loss: We still want the loss to continuously decrease on average. Note that it can \nrapidly fall to zero, meaning that the model is no longer learning anything. This behavior \ndoesn’t necessarily lead to overfitting or bad models but needs to be monitored closely.\n•\nValidation loss: The same thing can be said about the validation loss. We expect a small \ngap compared to the training loss.\n•\nGradient norm: We expect small gradient norms with few spikes.\n•\nRewards: We have two different rewards: chosen and rejected. They correspond to the \nmean difference between the log probabilities output by the trained and reference mod-\nels. Over time, we expect the model to choose the chosen answers and reject the rejected \nanswers, which means that the gap between them should increase. This difference is \ndirectly tracked by the margins metric, defined as the difference between chosen and \nrejected rewards. A well-trained model’s margin will quickly increase and then plateau.\n\n\nChapter 6\n257\n• \nAccuracies: This metric represents the percentage of times the model correctly identifies \nthe chosen answers. We want this accuracy to gradually increase during training, but it \ndoesn’t need to reach 100%. An accuracy of 100%, especially if it’s achieved quickly, in-\ndicates that the preference dataset might be too easy for the model. While the LLM can \nstill learn from such a dataset, it might be beneficial to add more challenging examples.\nIn general, DPO is slightly harder to monitor and debug than SFT because it’s a more complex pro-\ncess, involving a reference model. However, it’s also significantly easier to use than PPO and other \nRLHF algorithms. As long as you have a high-quality preference dataset and a strong fine-tuned \nmodel, you can experiment with different ranks, beta parameters, learning rates, and number of \nepochs to see which experiment best captures your preferences.\nWhile this is not the purpose of this chapter, it is possible to automate the evaluation of models \ndesigned to imitate a writing style. A possible solution consists of comparing the distribution of \nwords in the text generated by different models (SFT and DPO) with our ground-truth dataset. \nIn this example, we expect the SFT model to output a lot of words that are overrepresented in \nGPT-4o-mini (like “delve into”). The distribution output by our DPO model should be a lot closer \nto the chosen answers.\nSummary\nThis chapter explored preference alignment techniques for improving LLMs. It introduced the \nconcept of preference datasets, explaining their structure and importance in capturing nuanced \nhuman preferences. We implemented our own custom preference data generation pipeline by \ncomparing original and AI-generated text from real articles. This pipeline can be reused and \ncustomized based on your use case.\nWe also provided an overview of the evolution of RLHF, leading to the introduction of DPO as a \nsimpler and more efficient alternative. Finally, we implemented DPO using the Unsloth library to \nfine-tune our TwinLlama-3.1-8B model from Chapter 5. Our step-by-step tutorial gave practical \ninstructions for training the model, as well as highlighting key differences from SFT. The final \nmodel is available on the Hugging Face Hub.\nIn the next chapter, we will explore the crucial topic of LLM evaluation, addressing the challenges \nand current approaches in assessing LLM performance. We’ll cover the creation of domain-specific \nevaluation sets, examine why evaluation remains a persistent problem in the field, and introduce \nthe concept of using larger models to evaluate smaller ones (LLM-as-a-judge). The chapter will \nconclude with a comprehensive evaluation pipeline, providing a structured framework for con-\nsistent and effective LLM evaluation.\n\n\nFine-Tuning with Preference Alignment\n258\nReferences\n•\nRafael Rafailov et al.. “Direct Preference Optimization: Your Language Model is Secretly a \nReward Model.” arXiv preprint arXiv:2305.18290, May 2023.\n•\nTimo Kaufmann et al.. “A Survey of Reinforcement Learning from Human Feedback.” arXiv \npreprint arXiv:2312.14925, December 2023.\n• \nAnthropic. “GitHub - anthropics/hh-rlhf: Human preference data for “Training a Helpful and \nHarmless Assistant with Reinforcement Learning from Human Feedback”.” github.com, 2022, \nhttps://github.com/anthropics/hh-rlhf.\n• \nNisan Stiennon et al.. “Learning to summarize from human feedback.” arXiv preprint arX-\niv:2009.01325, September 2020.\n• \nIntel(R) Neural Compressor. “Supervised Fine-Tuning and Direct Preference Optimization \non Intel Gaudi2.” medium.com, March 26, 2024, https://medium.com/intel-analytics-\nsoftware/the-practice-of-supervised-finetuning-and-direct-preference-\noptimization-on-habana-gaudi2-a1197d8a3cd3.\n• \nArgilla. “GitHub - argilla-io/distilabel.” github.com, August 23, 2024, https://github.\ncom/argilla-io/distilabel.\n• \nDatabricks. “Enhancing LLM-as-a-Judge with Grading Notes.” databricks.com, July 22, 2024, \nhttps://www.databricks.com/blog/enhancing-llm-as-a-judge-with-grading-notes.\n• \nAkrour, Riad & Schoenauer, Marc & Sebag, Michèle. (2011). Preference-Based Policy Learn-\ning. 12-27. 10.1007/978-3-642-23780-5_11.\n• \nCheng, Weiwei & Fürnkranz, Johannes & Hüllermeier, Eyke & Park, Sang-Hyeun. (2011). \nPreference-Based Policy Iteration: Leveraging Preference Learning for Reinforcement Learning. \n312-327. 10.1007/978-3-642-23780-5_30.\n•\nPaul Christiano et al.. “Deep reinforcement learning from human preferences.” arXiv preprint \narXiv:1706.03741, June 2017.\n•\nLong Ouyang et al.. “Training language models to follow instructions with human feedback.” \narXiv preprint arXiv:2203.02155, March 2022.\n•\nJohn Schulman et al.. “Proximal Policy Optimization Algorithms.” arXiv preprint arX-\niv:1707.06347, July 2017.\n•\nunslothai. “GitHub - unslothai/unsloth: Finetune Llama 3.1, Mistral, Phi & Gemma LLMs \n2-5x faster with 80% less memory.” github.com, August 21, 2024, https://github.com/\nunslothai/unsloth.\n\n\nChapter 6\n259\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "page_number": 275
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 283-291)",
      "start_page": 283,
      "end_page": 291,
      "detection_method": "topic_boundary",
      "content": "7\nEvaluating LLMs\nLLM evaluation is a crucial process used to assess the performance and capabilities of LLM models. \nIt can take multiple forms, such as multiple-choice question answering, open-ended instructions, \nand feedback from real users. Currently, there is no unified approach to measuring a model’s \nperformance but there are patterns and recipes that we can adapt to specific use cases.\nWhile general-purpose evaluations are the most popular ones, with benchmarks like Massive \nMulti-Task Language Understanding (MMLU) or LMSYS Chatbot Arena, domain- and task-spe-\ncific models benefit from more narrow approaches. This is particularly true when dealing with \nentire LLM systems (as opposed to models), often centered around a retrieval-augmented gen-\neration (RAG) pipeline. In these scenarios, we need to expand our evaluation framework to en-\ncompass the entire system, including new modules like retrievers and post-processors.\nIn this chapter, we will cover the following topics:\n•\nModel evaluation\n•\nRAG evaluation\n•\nEvaluating TwinLlama-3.1-8B\nBy the end of this chapter, you will know the most popular LLM evaluations and how to evaluate \nmodels and RAG systems using different techniques.\nModel evaluation\nIn model evaluation, the objective is to assess the capabilities of a single model without any \nprompt engineering, RAG pipeline, and so on. \n\n\nEvaluating LLMs\n262\nThis evaluation is essential for several reasons, such as selecting the most relevant LLM or making \nsure that the fine-tuning process actually improved the model. In this section, we will compare \nML and LLM evaluation to understand the main differences between these two fields. We will \nthen explore benchmarks for general-purpose, domain-specific, and task-specific models.\nComparing ML and LLM evaluation\nML evaluation is centered on assessing the performance of models designed for tasks like pre-\ndiction, classification, and regression. Unlike the evaluation of LLMs, which often focuses on \nhow well a model understands and generates language, ML evaluation is more concerned with \nhow accurately and efficiently a model can process structured data to produce specific outcomes.\nThis difference comes from the nature of the tasks these models handle. ML models are gener-\nally designed for narrowly defined problems, such as predicting stock prices or detecting out-\nliers, which often involve numerical or categorical data, making the evaluation process more \nstraightforward. On the other hand, LLMs are tasked with interpreting and generating language, \nwhich adds a layer of subjectivity to the evaluation process. Instead of relying solely on numerical \nbenchmarks, LLM evaluation requires a more nuanced approach and often incorporates qualita-\ntive assessments, examining how well the model produces coherent, relevant, and contextually \naccurate responses in natural language.\nIn particular, we can see three key differences in how these models work, which impact the \nevaluation process:\n• \nNumerical metrics: Evaluating ML models typically involves measuring objective per-\nformance metrics, such as accuracy, precision, recall, or mean squared error, depending \non the type of task at hand. This is less clear with LLMs, which can handle multiple tasks \n(hence, multiple evaluations) and can rarely rely on the same numerical metrics.\n•\nFeature engineering: In traditional ML, a critical part of the process involves manually \nselecting and transforming relevant data features before training the model. Evaluating \nthe success of this feature engineering often becomes part of the broader model evalua-\ntion. LLMs, however, are designed to handle raw text data directly, reducing the need for \nmanual feature engineering.\n•\nInterpretability: With ML models, it is easier to interpret why a model made certain pre-\ndictions or classifications, and this interpretability can be a core part of their evaluation. \nThis direct interpretation is not possible with LLMs. However, requesting explanations \nduring the generation process can give insights into the model’s decision-making process.\n\n\nChapter 7\n263\nIn the following section, we will see a more fine-grained exploration of different types of LLMs. \nWhile evaluating general-purpose models is fairly disconnected from ML evaluation, task-specific \nLLMs are more closely aligned with traditional ML.\nGeneral-purpose LLM evaluations\nGeneral-purpose evaluations refer to metrics dedicated to base and general-purpose fine-tuned \nmodels. They cover a breadth of capabilities that are correlated with knowledge and usefulness \nwithout focusing on specific tasks or domains. This allows developers to get an overview of \nthese capabilities, compare themselves with competitors, and identify strengths and weaknesses. \nBased on these results, it is possible to tweak the dataset and hyperparameters, or even modify \nthe architecture.\nWe can broadly categorize general-purpose evaluations in three phases: during pre-training, after \npre-training, and after fine-tuning.\nDuring pre-training, we closely monitor how the model learns, as shown at the end of Chapter \n5. The most straightforward metrics are low-level and correspond to how models are trained:\n•\nTraining loss: Based on the cross-entropy loss, measures the difference between the \nmodel’s predicted probability distribution and the true distribution of the next token\n•\nValidation loss: Calculates the same loss as training loss, but on a held-out validation \nset to assess generalization\n•\nPerplexity: Exponential of the cross-entropy loss, representing how “surprised” the model \nis by the data (lower is better)\n•\nGradient norm: Monitors the magnitude of gradients during training to detect potential \ninstabilities or vanishing/exploding gradients\nIt’s also possible to include benchmarks like HellaSwag (common sense reasoning) during this \nstage but there’s a risk of overfitting these evaluations.\nAfter pre-training, it is common to use a suite of evaluations to evaluate the base model. This \nsuite can include internal and public benchmarks. Here’s a non-exhaustive list of common public \npre-training evaluations:\n•\nMMLU (knowledge): Tests models on multiple-choice questions across 57 subjects, from \nelementary to professional levels\n•\nHellaSwag (reasoning): Challenges models to complete a given situation with the most \nplausible ending from multiple choices\n\n\nEvaluating LLMs\n264\n• \nARC-C (reasoning): Evaluates models on grade-school-level multiple-choice science \nquestions requiring causal reasoning\n• \nWinogrande (reasoning): Assesses common sense reasoning through pronoun resolution \nin carefully crafted sentences\n• \nPIQA (reasoning): Measures physical common sense understanding through questions \nabout everyday physical interactions\nMany of these datasets are also used to evaluate general-purpose fine-tuned models. In this \ncase, we focus on the difference in a given score between the base and the fine-tuned model. For \nexample, bad fine-tuning can degrade the knowledge of the model, measured by MMLU. On the \ncontrary, a good one might instill even more knowledge and increase the MMLU score.\nThis can also help identify any contamination issues, where the model might have been fine-\ntuned on data that is too close to a test set. For instance, improving the MMLU score of a base \nmodel by 10 points during the fine-tuning phase is unlikely. This is a sign that the instruction \ndata might be contaminated.\nIn addition to these pre-trained evaluations, fine-tuned models also have their own benchmarks. \nHere, we use the term “fine-tuned model” to designate a model that has been trained with su-\npervised fine-tuning (SFT) and preference alignment. These benchmarks target capabilities \nconnected to the ability of fine-tuned models to understand and answer questions. In particular, \nthey test instruction-following, multi-turn conversation, and agentic skills:\n•\nIFEval (instruction following): Assesses a model’s ability to follow instructions with \nparticular constraints, like not outputting any commas in your answer\n•\nChatbot Arena (conversation): A framework where humans vote for the best answer to \nan instruction, comparing two models in head-to-head conversations\n• \nAlpacaEval (instruction following): Automatic evaluation for fine-tuned models that is \nhighly correlated with Chatbot Arena\n•\nMT-Bench (conversation): Evaluates models on multi-turn conversations, testing their \nability to maintain context and provide coherent responses\n•\nGAIA (agentic): Tests a wide range of abilities like tool use and web browsing, in a multi-\nstep fashion\nUnderstanding how these evaluations are designed and used is important to choose the best LLM \nfor your application. For example, if you want to fine-tune a model, you want the best base model \nin terms of knowledge and reasoning for a given size. This allows you to compare the capabilities \nof different LLMs and pick the one that will offer the strongest foundation for your fine-tuning.\n\n\nChapter 7\n265\nEven if you don’t want to fine-tune a model, benchmarks like Chatbot Arena or IFEval are a good \nway to compare different instruct models. For instance, you want great conversational abilities \nif you’re building a chatbot. However, this is not necessary if your end goal is something like \ninformation extraction from unstructured documents. In this case, you will benefit more from \nexcellent instruction-following skills to understand and execute tasks.\nWhile these benchmarks are popular and useful, they also suffer from inherent flaws. For exam-\nple, public benchmarks can be gamed by training models on test data or samples that are very \nsimilar to benchmark datasets. Even human evaluation is not perfect and is often biased toward \nlong and confident answers, especially when they’re nicely formatted (e.g., using Markdown). \nOn the other hand, private test sets have not been scrutinized as much as public ones and might \nhave their own issues and biases.\nThis means that benchmarks are not a single source of truth but should be used as signals. Once \nmultiple evaluations provide a similar answer, you can raise your confidence level about the real \ncapabilities of a model.\nDomain-specific LLM evaluations\nDomain-specific LLMs don’t have the same scope as general-purpose models. This is helpful to \ntarget more fine-grained capabilities with more depth than the previous benchmarks.\nWithin the category, the choice of benchmarks entirely depends on the domain in question. For \ncommon applications like a language-specific model or a code model, it is recommended to \nsearch for relevant evaluations and even benchmark suites. These suites encompass different \nbenchmarks and are designed to be reproducible. By targeting different aspects of a domain, they \noften capture domain performance more accurately.\nTo illustrate this, here is a list of domain-specific evaluations with leaderboards on the Hugging \nFace Hub:\n•\nOpen Medical-LLM Leaderboard: Evaluates the performance of LLMs in medical ques-\ntion-answering tasks. It regroups 9 metrics, with 1,273 questions from the US medical li-\ncense exams (MedQA), 500 questions from PubMed articles (PubMedQA), 4,183 questions \nfrom Indian medical entrance exams (MedMCQA), and 1,089 questions from 6 sub-cate-\ngories of MMLU (clinical knowledge, medical genetics, anatomy, professional medicine, \ncollege biology, and college medicine).\n\n\nEvaluating LLMs\n266\n• \nBigCodeBench Leaderboard: Evaluates the performance of code LLMs, featuring two main \ncategories: BigCodeBench-Complete for code completion based on structured docstrings, \nand BigCodeBench-Instruct for code generation from natural language instructions. Mod-\nels are ranked by their Pass@1 scores using greedy decoding, with an additional Elo rating \nfor the Complete variant. It covers a wide range of programming scenarios that test LLMs’ \ncompositional reasoning and instruction-following capabilities.\n• \nHallucinations Leaderboard: Evaluates LLMs’ tendency to produce false or unsupported \ninformation across 16 diverse tasks spanning 5 categories. These include Question Answer-\ning (with datasets like NQ Open, TruthfulQA, and SQuADv2), Reading Comprehension (using \nTriviaQA and RACE), Summarization (employing HaluEval Summ, XSum, and CNN/DM), \nDialogue (featuring HaluEval Dial and FaithDial), and Fact Checking (utilizing MemoTrap, \nSelfCheckGPT, FEVER, and TrueFalse). The leaderboard also assesses instruction-follow-\ning ability using IFEval.\n•\nEnterprise Scenarios Leaderboard: Evaluates the performance of LLMs on six real-world \nenterprise use cases, covering diverse tasks relevant to business applications. The bench-\nmarks include FinanceBench (100 financial questions with retrieved context), Legal Con-\nfidentiality (100 prompts from LegalBench for legal reasoning), Writing Prompts (cre-\native writing evaluation), Customer Support Dialogue (relevance in customer service \ninteractions), Toxic Prompts (safety assessment for harmful content generation), and \nEnterprise PII (business safety for sensitive information protection). Some test sets are \nclosed-source to prevent gaming of the leaderboard. The evaluation focuses on specific \ncapabilities such as answer accuracy, legal reasoning, creative writing, contextual rele-\nvance, and safety measures, providing a comprehensive assessment of LLMs’ suitability \nfor enterprise environments.\nLeaderboards can have different approaches based on their domain. For example, BigCodeBench \nis significantly different from others because it relies on only two metrics that sufficiently cap-\nture the entire domain. On the other hand, the Hallucinations Leaderboard regroups 16 metrics, \nincluding many general-purpose evaluations. It shows that in addition to custom benchmarks, \nreusing general-purpose ones can complete your own suite.\nIn particular, language-specific LLMs often reuse translated versions of general-purpose bench-\nmarks. This can be completed with original evaluations in the native language. While some of \nthese benchmarks use machine translation, it is better to rely on human-translated evaluations \nto improve their quality. We selected the following three task-specific leaderboards and their \nrespective evaluation suites to give you an idea of how to build your own:\n\n\nChapter 7\n267\n• \nOpenKo-LLM Leaderboard: Evaluates the performance of Korean LLMs using nine metrics. \nThese metrics are a combination of general-purpose benchmarks translated into Korean \n(GPQA, Winogrande, GSM8K, EQ-Bench, and IFEval) and custom evaluations (Knowledge, \nSocial Value, Harmlessness, and Helpfulness).\n• \nOpen Portuguese LLM Leaderboard: Evaluates the performance of Portuguese language \nLLMs using nine diverse benchmarks. These benchmarks include educational assessments \n(ENEM with 1,430 questions, and BLUEX with 724 questions from university entrance ex-\nams), professional exams (OAB Exams with over 2,000 questions), language understand-\ning tasks (ASSIN2 RTE and STS, FAQUAD NLI), and social media content analysis (HateBR \nwith 7,000 Instagram comments, PT Hate Speech with 5,668 tweets, and tweetSentBR).\n• \nOpen Arabic LLM Leaderboard: Evaluates the performance of Arabic language LLMs \nusing a comprehensive set of benchmarks, including both native Arabic tasks and trans-\nlated datasets. The leaderboard features two native Arabic benchmarks: AlGhafa and \nArabic-Culture-Value-Alignment. Additionally, it incorporates 12 translated benchmarks \ncovering various domains, such as MMLU, ARC-Challenge, HellaSwag, and PIQA.\nBoth general-purpose and domain-specific evaluations are designed with three main principles. \nFirst, they should be complex and challenge models to distinguish good and bad outputs. Second, \nthey should be diverse and cover as many topics and scenarios as possible. When one benchmark \nis not enough, additional ones can create a stronger suite. Finally, they should be practical and \neasy to run. This is more connected to evaluation libraries, which can be more or less complex to \nwork with. We recommend lm-evaluation-harness (github.com/EleutherAI/lm-evaluation-\nharness) from Eleuther AI and lighteval (github.com/huggingface/lighteval) from Hugging \nFace to run your benchmarks.\nTask-specific LLM evaluations\nWhile general-purpose and domain-specific evaluations indicate strong base or instruct models, \nthey cannot provide insights into how well these models work for a given task. This requires \nbenchmarks specifically designed for this purpose, measuring downstream performance.\nBecause of their narrow focus, task-specific LLMs can rarely rely on pre-existing evaluation data-\nsets. This can be advantageous because their outputs also tend to be more structured and easier \nto evaluate using traditional ML metrics. For example, a summarization task can leverage the \nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) metric, which measures the over-\nlap between the generated text and reference text using n-grams.\n\n\nEvaluating LLMs\n268\nLikewise, classification tasks also benefit from it and use the following classic metrics, among \nothers:\n• \nAccuracy: Accuracy refers to the proportion of correctly predicted instances compared \nto the total instances. It’s particularly useful for tasks with categorical outputs or where \nthere is a clear distinction between right and wrong answers, such as named entity rec-\nognition (NER).\n•\nPrecision: The ratio of true positive predictions to the total positive predictions made \nby the model.\n•\nRecall: The ratio of true positive predictions to the total actual positive instances.\n•\nF1 Score: The harmonic mean of precision and recall, used to balance both metrics. These \nare particularly useful in tasks such as classification or entity extraction.\nWhen the task cannot be directly mapped to a traditional ML task, it is possible to create a custom \nbenchmark. This benchmark can be inspired by general-purpose and domain-specific evaluation \ndatasets. A common and successful pattern is the use of multiple-choice question answering. In \nthis framework, the instruction consists of a question with several options. See the following \nexample with a question from the MMLU dataset (abstract algebra):\nInstruction\nFind the degree for the given field extension Q(sqrt(2), sqrt(3)) over Q.\nA. 0\nB. 4\nC. 2\nD. 6\nOutput\nB\nTable 7.1: Example from the MMLU dataset\n\n\nChapter 7\n269\nThere are two main ways of evaluating models with this scheme—text generation and log-like-\nlihood evaluations:\n• \nThe first approach involves having the model generate text responses and comparing \nthose to predefined answer choices. For example, the model generates a letter (A, B, C, or \nD) as its answer, which is then checked against the correct answer. This method tests the \nmodel’s ability to produce coherent and accurate responses in a format similar to how it \nwould be used in real-world applications.\n•\nEvaluation using probabilities, on the other hand, looks at the model’s predicted probabil-\nities for different answer options without requiring text generation. For MMLU, lm-eval-\nuation-harness compares the probabilities for the full text of each answer choice. This \napproach allows for a more nuanced assessment of the model’s understanding, as it can \ncapture the relative confidence the model has in different options, even if it wouldn’t \nnecessarily generate the exact correct answer text.\nFor simplicity, we recommend the text-generation version of the evaluation that mimics human \ntest-taking. It is easier to implement, and generally more discriminative, as low-quality models \ntend to overperform on probability-based evaluations. You can adapt this technique to quiz your \nmodels about a particular task, and even expand it to specific domains.\nConversely, if the task is too open-ended, traditional ML metrics and multiple-choice question \nanswering might not be relevant. In this scenario, the LLM-as-a-judge technique introduced in \nChapter 5 can be used to evaluate the quality of the answers. If you have ground-truth answers, \nproviding them as additional context improves the accuracy of the evaluation. Otherwise, defining \ndifferent dimensions (such as relevance or toxicity, depending on your task) can also ground the \nevaluation in more interpretable categories.\nIt is recommended to use large models for evaluation and to iteratively refine your prompt. In \nthis process, the explanations outputted by the model are important for understanding errors in \nits reasoning and fixing them through additional prompt engineering. \n",
      "page_number": 283
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 292-299)",
      "start_page": 292,
      "end_page": 299,
      "detection_method": "topic_boundary",
      "content": "Evaluating LLMs\n270\nIn order to easily parse answers, one can specify a structure in the instruction or use some kind \nof structured generation (like Outlines or OpenAI’s JSON mode). Here is an example of an in-\nstruction with a structure:\nYou are an evaluator who assesses the quality of an answer to an \ninstruction.\nYour goal is to provide a score that represents how well the answer \naddresses the instruction.\nYou will use a scale of 1 to 4, where each number represents the following:\n1. The answer is not relevant to the instruction.\n2. The answer is relevant but not helpful.\n3. The answer is relevant and helpful but could be more detailed.\n4. The answer is relevant, helpful, and detailed.\nPlease provide your evaluation as follows:\n##Evaluation##\nExplanation: (analyze the relevant, helpfulness, and complexity of the \nanswer)\nTotal rating: (final score as a number between 1 and 4)\nInstruction:\n{instruction}\nAnswer:\n{answer}\n##Evaluation##\nExplanation: \nTable 7.2: Example of general-purpose LLM-as-a-judge prompt for answer evaluation\nNaturally, you can tweak the scale, add a ground-truth answer to this prompt, and customize it \nfor your own use cases.\n\n\nChapter 7\n271\nHowever, judge LLMs can exhibit biases favoring assertive or verbose responses, potentially \noverrating answers that sound more confident but are less accurate. They may also lack domain \nexpertise for specialized topics, leading to misjudgments. Consistency is also a concern, as LLMs \nmight score similar responses differently. Additionally, they could have implicit preferences for \ncertain writing styles unrelated to actual answer quality. To mitigate these issues, it’s possible to \ncombine LLM evaluations with other metrics, use multiple judges, and carefully design prompts \nto address biases.\nOnce a model has been properly evaluated and works as intended, it might be included within a \nbroader system. In the next section, we will see how systems change the evaluation framework.\nRAG evaluation\nWhile traditional LLM evaluation focuses on the model’s inherent capabilities, RAG evaluation \nrequires a more comprehensive approach that considers both the model’s generative abilities \nand its interaction with external information sources.\nRAG systems combine the strengths of LLMs with information retrieval mechanisms, allowing \nthem to generate responses that are not only coherent and contextually appropriate but also \ngrounded in up-to-date, externally sourced information. This makes RAG particularly valuable \nin fields where current and accurate information is crucial, such as news reporting, research, and \ncustomer support.\nThe evaluation of RAG systems goes beyond assessing a standalone LLM. It requires examining \nthe entire system’s performance, including:\n•\nRetrieval accuracy: How well does the system fetch relevant information?\n•\nIntegration quality: How effectively is the retrieved information incorporated into the \ngenerated response?\n•\nFactuality and relevance: Does the final output address the query appropriately while \nseamlessly blending retrieved and generated content?\nKey metrics for RAG evaluation include retrieval precision and recall, which measure the accura-\ncy and comprehensiveness of the retrieved information. Additionally, the quality of integration \nbetween retrieved data and generated text is crucial, as is the overall factuality and coherence \nof the output.\n\n\nEvaluating LLMs\n272\nTo illustrate how these metrics are applied in practice, consider a RAG system designed for a \ncustomer support chatbot in an e-commerce setting. In this scenario, the user asks “What’s your \nreturn policy for laptops purchased during the holiday sale?” The RAG pipeline finds relevant \ndocuments on the electronics return policy and documents on holiday sale terms. This additional \ncontext is appended at the end of the question, and the model uses it to respond:\nFor laptops purchased during our holiday sale, you have an extended return \nperiod of 60 days from the date of purchase. This is longer than our \nstandard 30-day return policy for electronics. Please ensure the laptop is \nin its original packaging with all accessories to be eligible for a full \nrefund.\nTable 7.3: Example of output from a RAG pipeline designed for customer support\nIn this pipeline, we can evaluate if the retrieved documents correspond to what was expected \n(retrieval accuracy). We can also measure the difference between responses with and without \nadditional context (integration quality). Finally, we can assess whether the output is relevant and \ngrounded in the information provided by the documents (factuality and relevance).\nIn this section, we will cover two methods to evaluate how well RAG models incorporate external \ninformation into their responses.\nRagas\nRetrieval-Augmented Generation Assessment (Ragas) is an open-source toolkit designed to \nprovide developers with a comprehensive set of tools for RAG evaluation and optimization. It’s \ndesigned around the idea of metrics-driven development (MDD), a product development ap-\nproach that relies on data to make well-informed decisions, involving the ongoing monitoring \nof essential metrics over time to gain valuable insights into an application’s performance. By \nembracing this methodology, Ragas enables developers to objectively assess their RAG systems, \nidentify areas for improvement, and track the impact of changes over time.\nOne of the key capabilities of Ragas is its ability to synthetically generate diverse and complex \ntest datasets. This feature addresses a significant pain point in RAG development, as manually \ncreating hundreds of questions, answers, and contexts is both time-consuming and labor-inten-\nsive. Instead, it uses an evolutionary approach paradigm inspired by works like Evol-Instruct to \ncraft questions with varying characteristics such as reasoning complexity, conditional elements, \nand multi-context requirements. This approach ensures a comprehensive evaluation of different \ncomponents within the RAG pipeline. \n\n\nChapter 7\n273\nAdditionally, Ragas can generate conversational samples that simulate chat-based question-and-\nfollow-up interactions, allowing developers to evaluate their systems in more realistic scenarios.\nFigure 7.1: Overview of the Ragas evaluation framework\nAs illustrated in Figure 7.1, Ragas provides a suite of LLM-assisted evaluation metrics designed to \nobjectively measure different aspects of RAG system performance. These metrics include:\n•\nFaithfulness: This metric measures the factual consistency of the generated answer against \nthe given context. It works by breaking down the answer into individual claims and ver-\nifying if each claim can be inferred from the provided context. The faithfulness score is \ncalculated as the ratio of verifiable claims to the total number of claims in the answer.\n• \nAnswer relevancy: This metric evaluates how pertinent the generated answer is to the \ngiven prompt. It uses an innovative approach where an LLM is prompted to generate \nmultiple questions based on the answer and then calculates the mean cosine similarity \nbetween these generated questions and the original question. This method helps identify \nanswers that may be factually correct but off-topic or incomplete.\n•\nContext precision: This metric evaluates whether all the ground-truth relevant items \npresent in the contexts are ranked appropriately. It considers the position of relevant in-\nformation within the retrieved context, rewarding systems that place the most pertinent \ninformation at the top.\n•\nContext recall: This metric measures the extent to which the retrieved context aligns with \nthe annotated answer (ground truth). It analyzes each claim in the ground truth answer \nto determine whether it can be attributed to the retrieved context, providing insights into \nthe completeness of the retrieved information.\n\n\nEvaluating LLMs\n274\nFinally, Ragas also provides building blocks for monitoring RAG quality in production environ-\nments. This facilitates continuous improvement of RAG systems. By leveraging the evaluation \nresults from test datasets and insights gathered from production monitoring, developers can \niteratively enhance their applications. This might involve fine-tuning retrieval algorithms, ad-\njusting prompt engineering strategies, or optimizing the balance between retrieved context and \nLLM generation.\nRagas can be complemented with another approach, based on custom classifiers.\nARES\nARES (an automated evaluation framework for RAG systems) is a comprehensive tool designed \nto evaluate RAG systems. It offers an automated process that combines synthetic data genera-\ntion with fine-tuned classifiers to assess various aspects of RAG performance, including context \nrelevance, answer faithfulness, and answer relevance.\nThe ARES framework operates in three main stages: synthetic data generation, classifier training, \nand RAG evaluation. Each stage is configurable, allowing users to tailor the evaluation process \nto their specific needs and datasets.\nIn the synthetic data generation stage, ARES creates datasets that closely mimic real-world sce-\nnarios for robust RAG testing. Users can configure this process by specifying document file paths, \nfew-shot prompt files, and output locations for the synthetic queries. The framework supports \nvarious pre-trained language models for this task, with the default being google/flan-t5-xxl. \nUsers can control the number of documents sampled and other parameters to balance between \ncomprehensive coverage and computational efficiency.\nFigure 7.2: Overview of the ARES evaluation framework\n\n\nChapter 7\n275\nThe classifier training stage involves creating high-precision classifiers to determine the relevance \nand faithfulness of RAG outputs. Users can specify the classification dataset (typically generated \nfrom the previous stage), test set for evaluation, label columns, and model choice. ARES uses mi-\ncrosoft/deberta-v3-large as the default model but supports other Hugging Face models. Training \nparameters such as the number of epochs, patience value for early stopping, and learning rate \ncan be fine-tuned to optimize classifier performance.\nThe final stage, RAG evaluation, leverages the trained classifiers and synthetic data to assess the \nRAG model’s performance. Users provide evaluation datasets, few-shot examples for guiding the \nevaluation, classifier checkpoints, and gold label paths. ARES supports various evaluation metrics \nand can generate confidence intervals for its assessments.\nARES offers flexible model execution options, supporting both cloud-based and local runs through \nvLLM integration. The framework also supports various artifact types (code snippets, documents, \nHTML, images, and so on), enabling comprehensive evaluation across different RAG system \noutputs.\nIn summary, Ragas and ARES complement each other through their distinct approaches to eval-\nuation and dataset generation. Ragas’s strength in production monitoring and LLM-assisted \nmetrics can be combined with ARES’s highly configurable evaluation process and classifier-based \nassessments. While Ragas may offer more nuanced evaluations based on LLM capabilities, ARES \nprovides consistent and potentially faster evaluations once its classifiers are trained. Combining \nthem offers a comprehensive evaluation framework, benefiting from quick iterations with Ragas \nand in-depth, customized evaluations with ARES at key stages.\nIn the next section, we will create our own evaluation framework to evaluate our task-specific \nTwinLlama-3.1-8B model.\nEvaluating TwinLlama-3.1-8B\nIn the previous chapters, we created two models fine-tuned to generate high-quality posts and \narticles: TwinLlama-3.1-8B and TwinLlama-3.1-8B-DPO. Based on this summary, we want to \nassess their abilities to write text that is both accurate and well-written. In comparison, gener-\nal-purpose fine-tuned models are accurate thanks to their extensive knowledge but often use \noverly formal and verbose language. With this fine-tuning, we want to adopt a more natural \nwriting style, based on the original articles from the training set.\n\n\nEvaluating LLMs\n276\nDue to the open-ended nature of this problem, we will leverage a judge LLM to evaluate the quality \nof the generated text. It will take both the instruction and the answer as inputs, and score it on \na 1–3 scale based on two criteria:\n• \nAccuracy: The degree of factual correctness and comprehensiveness of the information \npresented in the answer\n• \nStyle: The appropriateness of the tone and writing style for blog posts or social media \ncontent (no formal or academic expressions)\nIn our evaluation framework, we will use the test split of our instruction dataset to get test in-\nstructions. We will feed them to our models and generate answers. These answers will then be \nevaluated by our judge LLM (GPT-4o-mini), based on a prompt that specifies our criteria. Finally, \nwe will analyze the scores and draw conclusions based on qualitative and quantitative evaluations.\nGenerating answers\nThe first step consists of efficiently generating answers for each instruction in our test set. In ad-\ndition to our two models, we will also use meta-llama/Meta-Llama-3.1-8B-Instruct, the official \ninstruct version of Llama-3.1-8B, as a reference point to better understand the trade-offs we made.\nLet’s start the first stage of the implementation:\n1. \nWe import the relevant libraries, including vLLM for fast generation. This library is a lot \nfaster than transformers for batch generation with local models:\nfrom vllm import LLM, SamplingParams\nfrom datasets import load_dataset\nfrom tqdm.auto import tqdm\nimport gc\n2. We define a function called generate_answers that will process our dataset and generate \nresponses using a specified model. It takes two inputs—the ID of the model we want to \nuse and the name of the test dataset:\ndef generate_answers(model_id, dataset_name):\n    dataset = load_dataset(dataset_name, split=\"test\")\n3. \nWe need to format the raw instructions using the chat template our models have been \ntrained on. Note that Llama-3.1-8B-Instruct has been used with a different template, but \nit can follow this simple format. Here, we use the same chat template with every model \nfor simplicity. We map the entire test set to this template with the format() function:\n\n\nChapter 7\n277\n    def format(sample):\n        return \"Below is an instruction that describes a task. \nWrite a response that appropriately completes the request.\\n\\n### \nInstruction:\\n{}\\n\\n### Response:\\n\".format(sample[\"instruction\"])\n    dataset = dataset.map(lambda sample: {\"prompt\": format(sample)})\n4.\nLet’s initialize the LLM object used by vLLM with a maximum length of 4,096 tokens. We \ncan also specify sampling parameters, which correspond to variables used in the decod-\ning strategy. Here, we use parameters to encourage diversity (high temperature) while \nremoving the most unlikely tokens (top_p and min_p). Finally, we start the generation \nby providing the list of prompts with dataset[\"prompt\"]:\n    llm = LLM(model=model_id, max_model_len=4096)\n    sampling_params = SamplingParams(temperature=0.8, top_p=0.95, \nmin_p=0.05, max_tokens=4096)\n    outputs = llm.generate(dataset[\"prompt\"], sampling_params)\n5. \nThis process should take a few minutes with our 334 prompts. Once this is done, we ex-\ntract the answers from the object that is outputted by vLLM. We then add these answers \nas a new column to our dataset. This is useful to log the answers and review them later:\n    answers = [output.outputs[0].text for output in outputs]\n    dataset = dataset.add_column(\"answers\", answers)\n6. We save our results to the Hugging Face Hub for easy access later. Then, we clear our GPU \nmemory to prevent running out of space when we process the next model:\n    print(f\"Uploading results for {model_id}\")\n    dataset.push_to_hub(f\"mlabonne/{model_id.split('/')\n[-1]}-results\")\n    gc.collect()\n    return dataset\n7. \nWe create a list of the three models we want to test. Then, we run our generate_answers()\nfunction for each of these models, one at a time. This will create and upload a separate \nset of results for each model:\nmodel_ids = [\n    'mlabonne/TwinLlama-3.1-8B',\n    'mlabonne/TwinLlama-3.1-8B-DPO',\n",
      "page_number": 292
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 300-308)",
      "start_page": 300,
      "end_page": 308,
      "detection_method": "topic_boundary",
      "content": "Evaluating LLMs\n278\n    'meta-llama/Meta-Llama-3.1-8B-Instruct'\n]\nfor model_id in model_ids:\n    generate_answers(model_id, \"mlabonne/llmtwin\")\nNow that we have the answer generation, we can move on to the evaluation process.\nEvaluating answers\nTo evaluate our answers, we will rely on GPT-4o-mini as a judge. This strategy is similar to what \nwe used for data generation. As a matter of fact, you could adapt it to filter out bad samples during \nthe data generation process. Here, we will score every generated answer from every model in \nterms of accuracy and style. The average scores will inform us about the quality of our fine-tuning \ncompared to Llama-3.1-8B-Instruct:\n1. \nFirst, we import the required libraries, including openai:\nimport json\nfrom typing import List\nfrom datasets import Dataset, load_dataset\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\nimport concurrent.futures\n2. We then define the evaluate_answer() function. This function contains our evaluation \nprompt, which sets up the context for evaluating answers based on accuracy and style:\ndef evaluate_answer(\n    instruction: str, answer: str, client: OpenAI\n) -> dict:\n    prompt = f\"\"\"You are an expert judge. Please evaluate the \nquality of a given answer to an instruction based on two criteria:\n1. Accuracy: How factually correct is the information presented in \nthe answer? You are a technical expert in this topic.\n2. Style: Is the tone and writing style appropriate for a blog post \nor social media content? It should use simple but technical words \nand avoid formal or academic language.\n3. \nIn the same prompt, we define our scales for each metric. Those are three-point Likert \nscales with a precise definition for each score:\nAccuracy scale:\n\n\nChapter 7\n279\n1 (Poor): Contains factual errors or misleading information\n2 (Good): Mostly accurate with minor errors or omissions\n3 (Excellent): Highly accurate and comprehensive\nStyle scale:\n1 (Poor): Too formal, uses some overly complex words\n2 (Good): Good balance of technical content and accessibility, but \nstill uses formal words and expressions\n3 (Excellent): Perfectly accessible language for blog/social media, \nuses simple but precise technical terms when necessary\n4.\nFinally, we conclude the prompt with two examples to illustrate what we mean by “complex \nwords” and “formal or academic language.” We provide the corresponding instruction-an-\nswer pair and ask the model to return a response in JSON:\nExample of bad style: The Llama2 7B model constitutes a noteworthy \nprogression in the field of artificial intelligence, serving as the \nsuccessor to its predecessor, the original Llama architecture.\nExample of excellent style: Llama2 7B outperforms the original Llama \nmodel across multiple benchmarks.\nInstruction: {instruction}\nAnswer: {answer}\nProvide your evaluation in JSON format with the following structure:\n{{\n    \"accuracy\": {{\n        \"analysis\": \"...\",\n        \"score\": 0\n    }},\n    \"style\": {{\n        \"analysis\": \"...\",\n        \"score\": 0\n    }}\n}}\n\"\"\"\n\n\nEvaluating LLMs\n280\n5. \nThis prompt is given as a user query to the GPT-4o-mini model. The system prompt rein-\nforces that we are interested in answer evaluation based on accuracy and style:\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant who \nevaluates answers based on accuracy and style. Provide your response \nin JSON format with a short analysis and score for each criterion.\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_format={\"type\": \"json_object\"},\n        max_tokens=1000,\n        temperature=0.8,\n    )\n6. As in the previous chapters, we will batch our requests to speed up the process. This is \nwhy we create an evaluate_batch() function, which returns a list of parsed structured \noutputs with their corresponding indices. These indices are important to ensure a correct \nordering of the evaluations:\ndef evaluate_batch(batch, start_index):\n    client = OpenAI(api_key=OPENAI_KEY)\n    return [\n        (i, evaluate_answer(instr, ans, client))\n        for i, (instr, ans) in enumerate(batch, start=start_index)\n    ]\n7. \nWe can now orchestrate the previous code in the evaluate_answers() function. It takes \nthe model ID, number of threads, and batch size as inputs. First, we load the dataset with \nthe generations we previously saved:\ndef evaluate_answers(model_id: str, num_threads: int = 10, batch_\nsize: int = 5) -> Dataset:\n    dataset = load_dataset(f\"mlabonne/{model_id.split('/')\n[-1]}-results\", split=\"all\")\n\n\nChapter 7\n281\n8. We create batches of instruction-answer pairs from our dataset. Each batch contains \nbatch_size number of pairs:\n    batches = [\n        (i, list(zip(dataset[\"instruction\"][i:i+batch_size], \ndataset[\"answers\"][i:i+batch_size])))\n        for i in range(0, len(dataset), batch_size)\n    ]\n9. \nWe perform parallel evaluation of batches of instruction-answer pairs using multiple \nthreads. We use parallel processing to evaluate multiple batches simultaneously, speed-\ning up the overall evaluation process. The ThreadPoolExecutor submits each batch to \nevaluate_batch(). The results are stored in the evaluations list:\n    evaluations = [None] * len(dataset)\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_\nthreads) as executor:\n        futures = [executor.submit(evaluate_batch, batch, start_\nindex) for start_index, batch in batches]\n        for future in tqdm(concurrent.futures.as_completed(futures), \ntotal=len(futures)):\n            for index, evaluation in future.result():\n                evaluations[index] = evaluation\n10. We create a new column with the result of the evaluation process. This column will store \nthe raw JSON output of the judge model, including scores and explanations:\n    if 'evaluation' in dataset.column_names:\n        dataset = dataset.remove_columns(['evaluation'])\n    dataset = dataset.add_column(\"evaluation\", evaluations)\n11. We can directly parse this JSON object with json.loads() and try to retrieve the accuracy \nand style scores that should have been generated. This generation is in best-effort mode, \nwhich means that scores are not guaranteed. If there’s an error in parsing, we use None\nvalues as a fallback:\n    accuracy_scores = []\n    style_scores = []\n\n\nEvaluating LLMs\n282\n    for evaluation in dataset['evaluation']:\n        try:\n            eval_dict = json.loads(evaluation) if \nisinstance(evaluation, str) else evaluation\n            accuracy_score = eval_dict['accuracy']['score']\n            style_score = eval_dict['style']['score']\n            accuracy_scores.append(accuracy_score)\n            style_scores.append(style_score)\n        except (json.JSONDecodeError, KeyError, TypeError):\n            accuracy_scores.append(None)\n            style_scores.append(None)\n12. We add two new columns to store the accuracy and style scores for further analysis:\n    if 'accuracy' in dataset.column_names:\n        dataset = dataset.remove_columns(['accuracy'])\n    dataset = dataset.add_column('accuracy', accuracy_scores)\n    if 'style' in dataset.column_names:\n        dataset = dataset.remove_columns(['style'])\n    dataset = dataset.add_column('style', style_scores)\n13. Let’s push the final dataset with generated answers, evaluations, and scores to the Hug-\nging Face Hub:\n    dataset.push_to_hub(f\"mlabonne/{model_id.split('/')\n[-1]}-results\")\n    return dataset\n14. We can now call the evaluate_answers() function with the three models we selected:\nmodel_ids = [\n    'mlabonne/TwinLlama-3.1-8B',\n    'mlabonne/TwinLlama-3.1-8B-DPO',\n    'meta-llama/Meta-Llama-3.1-8B-Instruct'\n\n\nChapter 7\n283\n]\nfor model_id in model_ids:\n    evaluate_answers(model_id)\nBy saving intermediate results, our evaluation framework becomes more robust to failures. It is \nalso easily extendable to other models, datasets, and criteria.\nAnalyzing results\nThere are three elements we can review at the end of this evaluation process: model answers, \nexplanations from evaluations, and scores.\nWhile reviewing answers is not scalable, it is a crucial step that can help to identify a lot of com-\nmon mistakes, such as a wrong chat template or incorrect model. Here, we will focus on a single \ninstruction and read the answers provided by our two models and Llama-3.1-8B-Instruct.\nInstruction\nDiscuss the concept of algorithm bias and its implications.\nTwinLlama-3 1-8B\nAlgorithm bias refers to the tendency of algorithms to produce outcomes that are skewed \nor unfair due to underlying assumptions or programming biases. This can have significant \nimplications, particularly in fields like machine learning and AI, where algorithms are relied \nupon to make critical decisions. If an algorithm is not properly vetted for bias, it may lead \nto discriminatory outcomes, such as lending practices that disproportionately affect certain \ndemographic groups or recruitment processes that unfairly favor certain candidates. It is \nessential to ensure that algorithms are designed and implemented with fairness and accuracy \nin mind to mitigate these risks.\nTwinLlama-3 1-8B-DPO\nAlgorithm bias refers to the tendency of algorithms to produce outcomes that are skewed \nor unfair due to underlying assumptions or programming biases. This can have significant \nimplications, particularly in fields like machine learning and AI, where algorithms are relied \nupon to make critical decisions. If an algorithm is not properly vetted for bias, it could \nlead to discriminatory outcomes, such as denying loans to certain demographic groups or \nperpetuating stereotypes. Therefore, it is essential to address algorithmic bias proactively to \nensure that the technology we develop is equitable and just.\n\n\nEvaluating LLMs\n284\nLlama-3 1-8B-Instruct\nAlgorithm bias, also known as algorithmic bias, refers to the unintended or inherent bias in \nmachine learning models that can affect their performance, accuracy, and fairness. This bias \ncan be caused by various factors, including the data used to train the model, the algorithm \nitself, and the biases present in the society or culture. One of the most significant implications \nof algorithm bias is its potential to perpetuate and exacerbate existing social inequalities. For \ninstance, facial recognition software has been shown to be less accurate for people of color, \nleading to wrongful arrests and discrimination. Similarly, language processing models have \nbeen found to be biased towards the language and dialect of the dominant group, making it \ndifficult for marginalized communities to access information and services. Algorithm bias \ncan also have severe consequences in areas such as healthcare, education, and employment. \nFor example, a medical diagnosis algorithm that is biased towards certain demographics \nmay misdiagnose or underdiagnose conditions in other groups, leading to delayed or \ninadequate treatment. In education, an algorithm that is biased towards certain learning \nstyles may disproportionately affect students with different learning abilities, leading to \nunequal opportunities. To mitigate the effects of algorithm bias, it is essential to address \nthe root causes of bias and implement measures to ensure fairness and transparency in \nmachine learning models. This can be achieved through techniques such as data cleansing, \nregularization, and debiasing, as well as implementing diverse and representative datasets. \nAdditionally, it is crucial to involve diverse stakeholders, including members of marginalized \ncommunities, in the development and testing of algorithms to ensure that they are fair and \nequitable. In conclusion, algorithm bias is a significant concern that can have far-reaching \nimplications for individuals and society as a whole. It is essential to recognize the potential \nbiases in machine learning models and take proactive steps to mitigate their effects. By doing \nso, we can create more equitable and just systems that benefit everyone, regardless of their \nbackground or demographic characteristics.\nTable 7.4: Example of generated answers for a given instruction\nImmediately, we can see that the answers from our models are very close to each other. This is not \nthe case with Llama-3.1-8B-Instruct’s answer, which is extremely verbose and lists many examples. \nThis is typically the kind of answer that is correct but can feel artificial and unpleasant to read \nbecause of its length. On the other hand, the DPO model slightly simplifies the language of the \nSFT version, making it less academic. This is exactly the behavior we want to capture, modifying \nthe writing style but not the actual content of the answer.\nLet’s now review the evaluations provided by GPT-4o-mini for each answer.\n\n\nChapter 7\n285\nTwinLlama-3.1-8B\nAccuracy\nThe answer accurately defines algorithm bias \nand highlights its implications in fields like \nmachine learning and AI. It correctly identi-\nfies the risks of discriminatory outcomes due \nto biased algorithms. There are no apparent \nfactual errors, making this a comprehensive \nresponse.\nScore: 3\nStyle\nThe tone is informative and relatively ac-\ncessible, though it tends towards a formal \ntone in some sections (e.g., ‘underlying \nassumptions or programming biases’). \nWhile it does use some technical terms \nappropriately, it could be simplified further \nfor a broader audience, especially for blog \nor social media content.\nScore: 2\nTwinLlama-3.1-8B-DPO\nAccuracy\nThe answer accurately defines algorithm bias \nand outlines its implications in critical fields \nlike machine learning and AI. It highlights the \npotential for discriminatory outcomes, making \nit a comprehensive overview of the topic with-\nout any factual errors.\nScore: 3\nStyle\nThe writing style is clear, accessible, and ef-\nfectively communicates the technical con-\ncept of algorithm bias without becoming \noverly formal. While it uses some technical \nterms, they are appropriate and explained \nwell for a general audience.\nScore: 3\nLlama-3.1-8B-Instruct\nAccuracy\nThe answer accurately defines algorithm bias \nand discusses its causes and implications in \nvarious domains such as facial recognition, \nhealthcare, and education. It also correctly \nidentifies methods to mitigate bias, such as \ninvolving diverse stakeholders and employing \ndebiasing techniques. Overall, the information \nis factually correct and comprehensive.\nScore: 3\nStyle\nThe tone is informative and accessible, \nwith technical terms explained clearly. \nHowever, some phrases like ‘unintended \nor inherent bias’ and ‘implementation of \ndiverse and representative datasets’ could \nbe simplified for broader audience appeal. \nThe writing is generally suitable for blog or \nsocial media content, but it could benefit \nfrom a more conversational tone.\nScore: 2\nTable 7.5: Evaluations of each answer made by GPT-4o-mini, according to style and ac-\ncuracy\n\n\nEvaluating LLMs\n286\nAccording to our judge LLM, there is no issue with the accuracy of the answers, which get a perfect \nscore. However, the style is considered too formal for TwinLlama-3.1-8B (SFT) and Llama-3.1-\n8B-Instruct, with a score of 2. The judge LLM agreed with our previous analysis and assigned a \nperfect score to TwinLlama-3.1-8B-DPO’s answer for communicating “the technical concept of \nalgorithm bias without becoming overly formal.”\nThis trend is confirmed by the average scores obtained by each model:\nTwinLlama-3.1-8B - Accuracy: 2.45\nTwinLlama-3.1-8B - Style: 2.04\nTwinLlama-3.1-8B-DPO - Accuracy: 2.46\nTwinLlama-3.1-8B-DPO - Style: 2.12\nLlama-3.1-8B-Instruct - Accuracy: 2.62\nLlama-3.1-8B-Instruct - Style: 1.86\nIn terms of accuracy, our two fine-tuned models get similar scores, while Llama-3.1-8B-Instruct \nachieves the highest accuracy score of 2.62. This suggests that the instruct-tuned Llama model \nmay have a slight edge in providing factually correct information. This is probably due to its \nextensive post-training process with over 10 million samples (compared to 13,000 in our case).\nHowever, when it comes to style, we see a different pattern. TwinLlama-3.1-8B-DPO leads with \na score of 2.12, successfully achieving a more accessible and less formal writing style without \nsacrificing content quality. TwinLlama-3.1-8B (SFT) follows with 2.04, showing improvement but \nretaining some formality, while Llama-3.1-8B-Instruct trails with 1.86, tending toward verbosity.\nBased on this feedback and the manual review of the generated answers, we can detect mistakes \nand identify areas for improvement. This is essential for refining the data generation process \nthrough additional filtering or augmenting the dataset with missing information. While this first \nversion already shows promising results, iterating over different datasets and models will allow \nus to significantly outperform our baseline and create the best possible model for our use case.\nSummary\nIn this chapter, we explored LLM evaluation with models and RAG systems. We saw how to \ninterpret classic benchmarks like MMLU to select strong candidates to use or fine-tune. We also \ndetailed how domain-specific and task-specific evaluations work, and how to create our own \nbased on publicly available examples. \n",
      "page_number": 300
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 309-317)",
      "start_page": 309,
      "end_page": 317,
      "detection_method": "topic_boundary",
      "content": "Chapter 7\n287\nWe focused on two techniques (multiple-choice question answering and LLM-as-a-judge) as the \nbackbone of these custom evaluation frameworks.\nHowever, models are commonly integrated into broader systems that provide additional context. \nWe introduced two evaluation frameworks for RAG systems, Ragas and ARES. We saw both simi-\nlarities (for example, synthetic data generation) and differences in how they evaluate RAG systems \n(context-based metrics versus trained classifiers). Finally, we evaluated TwinLlama-3.1-8B with \na judge LLM according to three criteria: relevance, coherence, and conciseness. This provided \ninsights into how we can improve it.\nIn the next chapter, we will explore inference optimization techniques to improve speed and \nreduce memory usage, without significantly compromising model performance. We will also \ndelve into optimization methods, model parallelism techniques and examine different quanti-\nzation approaches.\nReferences\n• \nLianmin Zheng et al.. “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.” arXiv \npreprint arXiv:2306.05685, June 2023.\n• \nAymeric Roucher. “Using LLM-as-a-judge for an automated and versatile evaluation - Hugging \nFace Open-Source AI Cookbook.” huggingface.co, No date found, https://huggingface.co/\nlearn/cookbook/en/llm_judge.\n•\nLangChain. “Aligning LLM-as-a-Judge with Human Preferences.” blog.langchain.dev, June \n26, 2024, https://blog.langchain.dev/aligning-llm-as-a-judge-with-human-\npreferences/.\n•\nDan Hendrycks et al.. “Measuring Massive Multitask Language Understanding.” arXiv pre-\nprint arXiv:2009.03300, September 2020.\n•\nJeffrey Zhou et al.. “Instruction-Following Evaluation for Large Language Models.” arXiv \npreprint arXiv:2311.07911, November 2023.\n•\nYann Dubois et al.. “Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Eval-\nuators.” arXiv preprint arXiv:2404.04475, April 2024.\n•\nGrégoire Mialon et al.. “GAIA: a benchmark for General AI Assistants.” arXiv preprint arX-\niv:2311.12983, November 2023.\n\n\nEvaluating LLMs\n288\n•\nGiwon Hong et al.. “The Hallucinations Leaderboard -- An Open Effort to Measure Hallucina-\ntions in Large Language Models.” arXiv preprint arXiv:2404.05904, April 2024.\n•\nShahul Es et al.. “RAGAS: Automated Evaluation of Retrieval Augmented Generation.” arXiv \npreprint arXiv:2309.15217, September 2023.\n•\nJon Saad-Falcon et al.. “ARES: An Automated Evaluation Framework for Retrieval-Augmented \nGeneration Systems.” arXiv preprint arXiv:2311.09476, November 2023.\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n\n\n8\nInference Optimization\nDeploying LLMs is challenging due to their significant computational and memory requirements. \nEfficiently running these models necessitates the use of specialized accelerators, such as GPUs or \nTPUs, which can parallelize operations and achieve higher throughput. While some tasks, like \ndocument generation, can be processed in batches overnight, others require low latency and fast \ngeneration, such as code completion. As a result, optimizing the inference process – how these \nmodels make predictions based on input data – is critical for many practical applications. This \nincludes reducing the time it takes to generate the first token (latency), increasing the number \nof tokens generated per second (throughput), and minimizing the memory footprint of LLMs.\nIndeed, naive deployment approaches lead to poor hardware utilization and underwhelming \nthroughput and latency. Fortunately, a variety of optimization techniques have emerged to dra-\nmatically speed up inference. This chapter will explore key methods like speculative decoding, \nmodel parallelism, and weight quantization, demonstrating how thoughtful implementations \ncan achieve speedups of 2–4X or more. We will also introduce three popular inference engines \n(Text Generation Inference, vLLM, and TensorRT-LLM) and compare their features in terms of \ninference optimization.\nIn this chapter, we will cover the following topics:\n•\nModel optimization strategies\n•\nModel parallelism\n•\nModel quantization\n\n\nInference Optimization\n290\nBy the end of this chapter, you will understand the core challenges in LLM inference and be fa-\nmiliar with state-of-the-art optimization techniques, including model parallelism and weight \nquantization.\nModel optimization strategies\nMost of the LLMs used nowadays, like GPT or Llama, are powered by a decoder-only Transformer \narchitecture. The decoder-only architecture is designed for text-generation tasks. It predicts the \nnext word in a sequence based on preceding words, making it effective for generating contextually \nappropriate text continuations.\nIn contrast, an encoder-only architecture, like BERT, focuses on understanding and representing \nthe input text with detailed embeddings. It excels in tasks that require comprehensive context \nunderstanding, such as text classification and named entity recognition. Finally, the encoder-de-\ncoder architecture, like T5, combines both functionalities. The encoder processes the input text \nto generate a context-rich representation, which the decoder then uses to produce the output \ntext. This dual structure is particularly powerful for sequence-to-sequence tasks like translation \nand summarization, where understanding the input context and generating a relevant output \nare equally important.\nIn this book, we only focus on the decoder-only architecture, which dominates the LLM field.\nFigure 8.1 – Inference process with decoder-only models. We provide “I have a dream” as \ninput and obtain “of” as output.\nAll the code examples from this chapter can be found on GitHub at https://github.\ncom/PacktPublishing/LLM-Engineering.\n\n\nChapter 8\n291\nAs shown in Figure 8.1, the basic inference process for a decoder-only model involves:\n1.\nTokenizing the input prompt and passing it through an embedding layer and positional \nencoding.\n2.\nComputing key and value pairs for each input token using the multi-head attention \nmechanism.\n3.\nGenerating output tokens sequentially, one at a time, using the computed keys and values.\nWhile Steps 1 and 2 are computationally expensive, they consist of highly parallelizable matrix \nmultiplication that can achieve high hardware utilization on accelerators like GPUs and TPUs.\nThe real challenge is that the token generation in Step 3 is inherently sequential – to generate \nthe next token, you need to have generated all previous tokens. This leads to an iterative process \nwhere the output sequence is grown one token at a time, failing to leverage the parallel computing \ncapabilities of the hardware. Addressing this bottleneck is one of the core focuses of inference \noptimization.\nIn this section, we will detail several optimization strategies that are commonly used to speed \nup inference and reduce Video Random-Access Memory (VRAM) usage, such as implementing \na (static) KV cache, continuous batching, speculative decoding, and optimized attention mech-\nanisms.\nKV cache\nWe saw that LLMs generate text token by token, which is slow because each new prediction \ndepends on the entire previous context. For example, to predict the 100\nth token in a sequence, \nthe model needs the context of tokens 1 through 99. When predicting the 101\nst token, it again \nneeds the information from tokens 1 through 99, plus token 100. This repeated computation is \nparticularly inefficient.\nThe key-value (KV) cache addresses this issue by storing key-value pairs produced by self-at-\ntention layers. Instead of recalculating these pairs for each new token, the model retrieves them \nfrom the cache, significantly speeding up the generation. \n\n\nInference Optimization\n292\nYou can see an illustration of this technique in Figure 8.2:\nFigure 8.2 – Illustration of the KV cache\nWhen a new token is generated, only the key and value for that single token need to be computed \nand added to the cache. The KV cache is an immediate optimization that is implemented in every \npopular tool and library. Some implementations maintain a separate KV cache for each layer of \nthe model.\nThe size of the KV cache scales with the number of tokens (𝑛𝑛𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 ) and several model dimensions, \nlike the number of layers (𝑛𝑛𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 ), the number of attention heads (𝑛𝑛ℎ𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒 ), their dimension (dimℎ𝑒𝑒𝑒𝑒𝑒𝑒 \n), and the precision of the parameters in bytes (𝑛𝑛𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 ):\n𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏\n \nFor a typical 7B parameter model using 16-bit precision, this exceeds 2 GB for high sequence \nlengths (higher than 2,048 tokens). Larger models with more layers and higher embedding di-\nmensions will see even greater memory requirements.\n\n\nChapter 8\n293\nSince the KV cache grows with each generation step and is dynamic, it prevents you from taking \nadvantage of torch.compile, a powerful optimization tool that fuses PyTorch code into fast and \noptimized kernels. The static KV cache solves this issue by pre-allocating the KV cache size to a \nmaximum value, which allows you to combine it with torch.compile for up to a 4x speedup in \nthe forward pass.\nTo configure a model to use a static KV cache with the transformers library, follow these steps:\n1. \nWe import the tokenizer and the model we want to optimize:\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_id = \"google/gemma-2b-it\"\ntokenizer = AutoTokenizer.from_pretrained(model_id) \nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_\nmap=\"auto\")\n2. \nTo implement the static cache, we change the cache implementation in the model’s gen-\neration config to static:\nmodel.generation_config.cache_implementation = \"static\"\n3. \nNow that our KV cache is static, we can compile the model using torch.compile:\ncompiled_model = torch.compile(model, mode=\"reduce-overhead\", \nfullgraph=True)\n4. \nWe tokenize an input question, “What is 2+2?\", and store it on a GPU if available (if not, \nwe store it on the CPU):\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ninputs = tokenizer(\"What is 2+2?\", return_tensors=\"pt\").to(device)\n5. \nLet’s use the generate() method to get the model’s output and decode it with batch_\ndecode() to print its answer:\noutputs = model.generate(**inputs, do_sample=True, temperature=0.7, \nmax_new_tokens=64)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['What is 2+2?\\n\\nThe answer is 4. 2+2 = 4.']\n\n\nInference Optimization\n294\nThis returns a list containing both the input and output, correctly answering our question.\nEfficiently managing the KV cache is essential, as it can quickly exhaust available GPU memory \nand limit the batch sizes that can be processed. This has motivated the development of memo-\nry-efficient attention mechanisms and other techniques, which we will cover in the last section.\nContinuous batching\nBatching, or processing multiple inference requests simultaneously, is a standard approach to \nachieve high throughput. Larger batch sizes spread out the memory cost of model weights and \ntransfer more data to the GPU at once, better saturating its parallel compute capacity.\nHowever, decoder-only models pose a particular challenge due to the high variability in input \nprompt lengths and desired output lengths. Some requests may have short prompts and only \nneed a one-word answer, while others may input a lengthy context and expect a multi-paragraph \nresponse.\nWith traditional batching, we would have to wait for the longest request in a batch to complete be-\nfore starting a new batch. This leads to under-utilization as the accelerator sits partly idle waiting \nfor a straggling request to finish. Continuous batching, also known as in-flight batching, aims to \nprevent idle time by immediately feeding a new request into the batch as soon as one completes.\nThe batching process begins the same – by filling the batch with initial requests. But as soon as \na request completes its generation, it is evicted from the batch and a new request takes its place. \nThis way, the accelerator is always processing a full batch, leading to maximally efficient hardware \nutilization. An additional consideration is the need to periodically pause the generation process \nto run prefill, or the embedding and encoding of waiting requests. Finding the optimal balance \nbetween generation and prefill requires some tuning of the waiting-served ratio hyperparameter.\nContinuous batching is natively implemented in most inference frameworks, like Hugging Face’s \nText Generation Inference (TGI), vLLM, and NVIDIA TensorRT-LLM.\nNote that the static cache doesn’t work with all architectures. For details on which \narchitectures are supported, check out the transformers documentation.\n\n\nChapter 8\n295\nSpeculative decoding\nAnother powerful optimization technique is speculative decoding, also called assisted generation. \nThe key insight is that even with continuous batching, the token-by-token generation process \nfails to fully saturate the parallel processing capabilities of the accelerator. Speculative decoding \naims to use this spare compute capacity to predict multiple tokens simultaneously, using a smaller \nproxy model (see Figure 8.3).\nFigure 8.3 – Illustration of traditional decoding (left) and speculative decoding (right)\nThe general approach is:\n• \nApply a smaller model, like a distilled or pruned version of the main model, to predict \nmultiple token completions in parallel. This could be 5–10 tokens predicted in a single step.\n•\nFeed these speculative completions into the full model to validate which predictions \nmatch what the large model would have generated.\n•\nRetain the longest matching prefix from the speculative completions and discard any \nincorrect tokens.\nThe result is that, if the small model approximates the large model well, multiple tokens can be \ngenerated in a single step. This avoids running the expensive large model for several iterations. \nThe degree of speedup depends on the quality of the small model’s predictions – a 90% match \ncould result in a 3–4X speedup.\nIt is crucial that both models use the same tokenizer. If this is not the case, the tokens generated \nby the draft model will not align with those produced by the large model, making them incom-\npatible. Let’s implement this using the transformers library. In this example, we will use two \nQwen1.5 models from Alibaba Cloud: a 1.8B version as the main model, and a 0.5B version as the \ndraft model. Note that, if you have enough VRAM, you can use much larger models like 14B, 32B, \n72B, or 110B as the main model. \n",
      "page_number": 309
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 318-325)",
      "start_page": 318,
      "end_page": 325,
      "detection_method": "topic_boundary",
      "content": "Inference Optimization\n296\nHere, we’re limited by the VRAM of the T4 GPU in Google Colab, but to get the maximum speedup, \nthe assistant model should be much smaller than the large model.\nHere’s a step-by-step guide to implement speculative decoding:\n1. \nWe load the tokenizer and both models:\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_id = \"Qwen/Qwen1.5-1.8B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_id) \nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_\nmap=\"auto\")\ndraft_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-\n0.5B-Chat\", device_map=\"auto\")\n2. We then tokenize the same input and store it in the accelerator, if available:\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ninputs = tokenizer(\"What is 2+2?\", return_tensors=\"pt\").to(device)\n3. \nWe can now use model.generate() with the argument assistant_model to enable specu-\nlative decoding:\noutputs = model.generate(**inputs, do_sample=True, assistant_\nmodel=draft_model, temperature=0.7, max_new_tokens=64)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['What is 2+2? 2 + 2 equals 4!']\nThe speedup in this small example is not significant, but it is clearly noticeable with bigger models.\nPrompt lookup decoding is a variant of speculative decoding, tailored to input-grounded tasks like \nsummarization where there is often overlap between the prompt and output. Shared n-grams \nare used as the LLM candidate tokens. We can enable prompt lookup decoding by using the \nprompt_lookup_num_tokens parameter in model.generate():\noutputs = model.generate(**inputs, prompt_lookup_num_tokens=4)\nBy combining the static KV cache with torch.compile, implementing continuous batching, and \nleveraging speculative decoding techniques, LLMs can see inference speedups of 2–4x or more \nwith no loss in quality.\n\n\nChapter 8\n297\nAnother approach to creating a small proxy model consists of jointly fine-tuning a small model \nalongside a large model for maximum fidelity. A representative technique here is Medusa, which \ninserts dedicated speculation heads into the main model. The Medusa-1 approach fine-tunes \nthese speculation heads while freezing the large model, while the Medusa-2 approach jointly fine-\ntunes both the speculation heads and the large model. The Medusa method has demonstrated \nimpressive results, enabling a 70M parameter model to closely approximate the performance \nof a 7B parameter model on a range of tasks. Speculative decoding is natively supported by TGI.\nOptimized attention mechanisms\nThe Transformer architecture is based on the attention mechanism, which scales quadratically \nwith the number of input tokens (or sequence length). This is particularly inefficient for longer \nsequences, where the size of the KV cache can blow up.\nIntroduced by Kwon, Li, et al. (2023), PagedAttention addresses these memory challenges by \ndrawing inspiration from virtual memory and paging in operating systems. It partitions the KV \ncache into blocks, eliminating the need for contiguous memory allocation. Each block contains the \nkeys and values for a fixed number of tokens. During attention computation, the PagedAttention \nkernel efficiently fetches these blocks, regardless of their physical memory location.\nThis partitioning allows for near-optimal memory utilization. This is useful for batching more \nsequences together, which increases throughput and GPU utilization. Moreover, PagedAttention's \nblock-based approach naturally supports memory sharing across multiple output sequences \ngenerated from the same prompt. This is particularly advantageous in parallel sampling and \nbeam search, where the same prompt is used to generate multiple outputs. The shared memory \nblocks reduce redundant computations and memory usage, cutting the memory overhead by \nup to 55% and improving throughput by up to 2.2x, according to the authors. The vLLM library \nreceived the first implementation of PagedAttention. Since then, PagedAttention has also been \nimplemented in TGI and TensorRT-LLM.\nAnother popular option is FlashAttention-2. Developed by Tri Dao (2023), it introduced several \nkey innovations that are designed to address the quadratic runtime and memory constraints in \ntraditional attention. By dividing input and output matrices into smaller blocks, FlashAtten-\ntion-2 ensures that these blocks can fit into the GPU’s on-chip SRAM, which is much faster than \nhigh-bandwidth memory. This approach significantly reduces the frequency of data transfers \nbetween the GPU’s main memory and its processing units. \n\n\nInference Optimization\n298\nThis is combined with online softmax, which computes the softmax function independently for \neach block of the attention scores matrix, rather than for the entire matrix at once. By maintaining \na running maximum and a running sum of exponentials, FlashAttention-2 can calculate attention \nprobabilities without needing to store large intermediate matrices.\nAdditionally, FlashAttention-2’s online softmax computation enables block-wise processing, \nmaintaining accuracy while significantly reducing memory requirements. This is particularly im-\nportant for training, where the recomputation of intermediate values (instead of storing them) in \nthe backward pass reduces memory usage from quadratic to linear, in relation to sequence length.\nUnlike PagedAttention, FlashAttention-2 can easily be used with the transformers library through \nthe attn_implementation parameter:\n1. \nInstall the flash-attn library with --no-build-isolation so that we don’t install the \ndependencies:\npip install flash-attn --no-build-isolation\n2. \nTo use FlashAttention-2 for inference, specify flash_attention_2 in the attn_\nimplementation parameter when loading a model. For example, this is how to load Mis-\ntral-7B-Instruct-v0.3 with FlashAttention-2:\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.3\",\n    attn_implementation=\"flash_attention_2\",\n)\nThe techniques presented in this section focused on improving the model’s efficiency in processing \ntokens. In the next section, we will discuss how to distribute our model and calculations across \nmultiple GPUs.\nModel parallelism\nModel parallelism allows you to distribute the memory and compute requirements of LLMs across \nmultiple GPUs. This enables the training and inference of models too large to fit on a single device, \nwhile also improving performance in terms of throughput (tokens per second).\nThere are three main approaches to model parallelism, each involving splitting the model weights \nand computation in different ways: data parallelism, pipeline parallelism, and tensor parallelism. \n\n\nChapter 8\n299\nAlthough these approaches were originally developed for training, we can reuse them for inference \nby focusing on the forward pass only.\nData parallelism\nData parallelism (DP) is the simplest type of model parallelism. It involves making copies of the \nmodel and distributing these replicas across different GPUs (see Figure 8.4). Each GPU processes \na subset of the data simultaneously. During training, the gradients calculated on each GPU are \naveraged and used to update the model parameters, ensuring that each replica remains synchro-\nnized. This approach is particularly beneficial when the batch size is too large to fit into a single \nmachine or when aiming to speed up the training process.\nFigure 8.4 – Illustration of data parallelism with four GPUs\nDuring inference, DP can be useful for processing concurrent requests. By distributing the work-\nload across multiple GPUs, this approach helps reduce latency, as multiple requests can be handled \nsimultaneously. This concurrent processing also increases throughput, since a higher number of \nrequests can be processed at the same time.\nHowever, the effectiveness of DP is limited by the model size and the communication overhead \nbetween GPUs. Indeed, replicating the model’s parameters on each GPU is inefficient. This means \nthat this technique only works when the model is small enough to fit into a single GPU, leaving \nless room for input data and thus limiting the batch size. For larger models or when memory is \na constraint, this can be a significant drawback.\nTypically, DP is mainly used for training, while pipeline and tensor parallelism are preferred for \ninference.\n\n\nInference Optimization\n300\nPipeline parallelism\nIntroduced by Huang et al. in the GPipe paper (2019), pipeline parallelism (PP) is a strategy \nfor distributing the computational load of training and running large neural networks across \nmultiple GPUs.\nUnlike traditional DP, which replicates the entire model on each GPU, pipeline parallelism parti-\ntions the model’s layers across different GPUs. This approach allows each GPU to handle a specific \nportion of the model, thereby reducing the memory burden on individual GPUs.\nFigure 8.5 – Illustration of pipeline parallelism with four GPUs\nAs shown in Figure 8.5, in a typical four-way pipeline parallel split, the model is divided into four \nsegments, with each segment assigned to a different GPU. The first 25% of the model’s layers might \nbe processed by GPU 1, the next 25% by GPU 2, and so on. During the forward pass, activations \nare computed and then passed along to the next GPU. For training, the backward pass follows a \nsimilar sequence in reverse, with gradients being propagated back through the GPUs. The number \nof GPUs is often referred to as the degree of parallelism.\nThe primary advantage of pipeline parallelism is its ability to significantly reduce the memory \nrequirements per GPU. However, this approach introduces new challenges, particularly related \nto the sequential nature of the pipeline. One of the main issues is the occurrence of “pipeline \nbubbles.” These bubbles arise when some GPUs are idle, waiting for activations from preceding \nlayers. This idle time can reduce the overall efficiency of the process.\n\n\nChapter 8\n301\nMicro-batching was developed to mitigate the impact of pipeline bubbles. By splitting the input \nbatch into smaller sub-batches, micro-batching ensures that GPUs remain busier, as the next \nsub-batch can begin processing before the previous one is fully completed.\nFigure 8.6 – Illustration of pipeline parallelism with micro-batching.\nFigure 8.6 shows an example of pipeline parallelism with micro-batching. In this example, the \npipeline has four stages (F0, F1, F2, F3), and the input batch is divided into four micro-batches. \nGPU 0 will process forward paths F0,0, F0,1, F0,2, and F0,3, sequentially. Once F0,0 is complete, \nGPU 1 can immediately start processing F1,0 and so on. After completing these forward passes, \nGPU 0 waits for the other GPUs to finish their respective forward computations before starting \nthe backward paths (B0,3, B0,2, B0,1, and B0,0).\nPipeline parallelism is implemented in distributed training frameworks like Megatron-LM, Deep-\nSpeed (ZeRO), and PyTorch through the dedicated Pipeline Parallelism for PyTorch (PiPPy) \nlibrary. At the time of writing, only certain inference frameworks like TensorRT-LLM support \npipeline parallelism.\nTensor parallelism\nIntroduced by Shoeby, Patwary, Puri et al. in the Megatron-LM paper (2019), tensor parallelism\n(TP) is another popular technique to distribute the computation of LLM layers across multiple \ndevices. In contrast to pipeline parallelism, TP splits the weight matrices found in individual \nlayers. This enables simultaneous computations, significantly reducing memory bottlenecks and \nincreasing processing speed.\n\n\nInference Optimization\n302\nIn TP, large matrices, such as the weight matrices in MLPs or the attention heads in self-atten-\ntion layers, are partitioned across several GPUs. Each GPU holds a portion of these matrices and \nperforms computations on its respective slice.\nFigure 8.7 – Illustration of column-wise tensor parallelism in an MLP layer (W)\nFor instance, in an MLP layer, the weight matrix is divided so that each GPU processes only a subset \nof the weights (see Figure 8.7). The inputs are broadcast to all GPUs, which then independently \ncompute their respective outputs. The partial results are then aggregated through an all-reduce \noperation, combining them to form the final output.\nIn the context of self-attention layers, TP is particularly efficient due to the inherent parallelism \nof attention heads. Each GPU can compute a subset of these heads independently, allowing the \nmodel to process large sequences more effectively. This makes TP more efficient than pipeline \nparallelism, which requires waiting for the completion of previous layers.\nDespite its advantages, TP is not universally applicable to all layers of a neural network. Layers \nlike LayerNorm and Dropout, which have dependencies spanning the entire input, cannot be effi-\nciently partitioned and are typically replicated across devices instead. However, these operations \ncan be split on the sequence dimension of the input instead (sequence parallelism). Different \nGPUs can compute these layers on different slices of the input sequence, avoiding replication of \nweights. This technique is limited to a few specific layers, but it can provide additional memory \nsavings, especially for very large input sequence lengths.\n\n\nChapter 8\n303\nMoreover, TP necessitates high-speed interconnects between devices to minimize communica-\ntion overhead, making it impractical to implement across nodes with insufficient interconnect \nbandwidth.\nTP is also implemented in distributed training frameworks like Megatron-LM, DeepSpeed (ZeRO), \nand PyTorch (FSDP). It is available in most inference frameworks, like TGI, vLLM, and Tensor-\nRT-LLM.\nCombining approaches\nData, tensor, and pipeline parallelisms are orthogonal techniques that can be combined. Figure \n8.8 illustrates how a given model can be split according to each approach:\nFigure 8.8 – Illustration of the different model parallelism techniques\nCombining these techniques can mitigate their respective issues. Pipeline parallelism provides \nthe greatest memory reduction but sacrifices efficiency, due to pipeline bubbles. This may be \nideal if the primary constraint fits the model in the GPU memory. In contrast, if low latency is \nparamount, then prioritizing tensor parallelism and accepting a larger memory footprint may \nbe the better trade-off. In practice, a model may be split depth-wise into a few pipeline stages, \nwith tensor parallelism used within each stage.\nBalancing these tradeoffs and mapping a given model architecture onto available hardware ac-\ncelerators is a key challenge in deploying LLMs.\nModel quantization\nQuantization refers to the process of representing the weights and activations of a neural net-\nwork using lower-precision data types. In the context of LLMs, quantization primarily focuses \non reducing the precision of the model’s weights and activations. \n",
      "page_number": 318
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 326-334)",
      "start_page": 326,
      "end_page": 334,
      "detection_method": "topic_boundary",
      "content": "Inference Optimization\n304\nBy default, weights are typically stored in a 16-bit or 32-bit floating-point format (FP16 or FP32), \nwhich provides high precision but comes at the cost of increased memory usage and computa-\ntional complexity. Quantization is a solution to reduce the memory footprint and accelerate the \ninference of LLMs.\nIn addition to these benefits, larger models with over 30 billion parameters can outperform \nsmaller models (7B–13B LLMs) in terms of quality when quantized to 2- or 3-bit precision. This \nmeans they can achieve superior performance while maintaining a comparable memory footprint.\nIn this section, we will introduce the concepts of quantization, GGUF with llama.cpp, GPTQ, \nand EXL2, along with an overview of additional techniques. In addition to the code provided in \nthis section, you can refer to AutoQuant (bit.ly/autoquant) to quantize their models using a \nGoogle Colab notebook.\nIntroduction to quantization\nThere are two main approaches to weight quantization: Post-Training Quantization (PTQ) and \nQuantization-Aware Training (QAT). PTQ is a straightforward technique where the weights of \na pre-trained model are directly converted to a lower precision format without any retraining. \nWhile PTQ is easy to implement, it may result in some performance degradation. Conversely, QAT \nperforms quantization during the training or fine-tuning stage, allowing the model to adapt to \nthe lower precision weights. QAT often yields better performance compared to PTQ but requires \nadditional computational resources and representative training data.\nThe choice of data type plays a crucial role in quantization. Floating-point numbers, such as \nFP32, FP16 (half-precision), and BF16 (brain floating-point), are commonly used in deep learning. \nThese formats allocate a fixed number of bits to represent the sign, exponent, and significand\n(mantissa) of a number.\n\n\nChapter 8\n305\nFigure 8.9 – Comparison the between FP32, FP16, and BF16 formats\nA sign of 0 represents a positive number, while 1 indicates a negative number. Conversely, the \nexponent controls the range that is represented (big or small). Finally, the significand controls \nthe precision of the number (the number of digits). The formula used to convert these represen-\ntations into real numbers is:\n(−1)sign × baseexponent × significand \nThe data types shown in Figure 7.7 display different tradeoffs, as illustrated with different repre-\nsentations of 𝜋𝜋 (≈3.1415926535 ). FP32 uses 32 bits, providing high precision but also requiring more \nmemory. Conversely, FP16 and BF16 use 16 bits, lowering the memory footprint at the cost of a \nlower precision. In general, neural networks prefer a bigger range than better precision, which is \nwhy BF16 is the most popular data type when the hardware supports it. For example, NVIDIA’s \nAmpere architecture (A100, A30, etc.) supports BF16, but previous generations like Turing (T4, \nT40, etc.) do not.\n\n\nInference Optimization\n306\nHowever, we are not restricted to these three data types. Lower-precision data types, such as INT8 \n(8-bit integers), can be employed for quantization, further reducing the memory footprint. Naïve \nquantization techniques, such as absolute maximum (absmax) quantization and zero-point quanti-\nzation, can be applied to convert FP32, FP16, or BF16 weights to INT8, as illustrated in Figure 8.10:\nFigure 8.10 – Quantization of 0.1 in a [-3.0, 3.2] range with absmax quantization and zero-point \nquantization\nAbsmax quantization maps the original weights 𝐗𝐗 to the range [-127, 127] by dividing them by the \nabsolute maximum value of 𝐗𝐗 and scaling them:\n𝐗𝐗quant = round (127 ⋅𝐗𝐗\nmax|𝐗𝐗|) \nFor example, if our absolute maximum value is 3.2 (see Figure 8.8), a weight of 0.1 would be \nquantized to round (\n127⋅0.1\n3.2 ) = 4 . To dequantize it, we do the inverse operation:\n𝐗𝐗dequant = max|𝐗𝐗| ⋅𝐗𝐗quant\n127\n \nThis means that if we dequantize our weight, we obtain \n3.2⋅4 \n127 ≈0.1008 . We can see a rounding er-\nror of 0.0008  in this example. In Python, we can implement it as follows with the PyTorch library:\nimport torch\ndef absmax_quantize(X):\n    # Calculate scale\n    scale = 127 / torch.max(torch.abs(X))\n    # Quantize\n    X_quant = (scale * X).round()\n    return X_quant.to(torch.int8)\n\n\nChapter 8\n307\nZero-point quantization, on the other hand, considers asymmetric input distributions and maps \nthe weights \n0.0008 to the range [-128, 127] by introducing a zero-point offset:\n𝐗𝐗quant = round(scale ⋅𝐗𝐗𝐗𝐗ero𝐗o𝐗n𝐗) \nWhere scale =\n255\nmax(𝐗𝐗)−min(𝐗𝐗)  and zeropoint = −round(scale ⋅min(𝐗𝐗)) −128 .\nIf we take the same example with a weight of 0.1, we get a scale of \n255\n3.2+3.0 ≈41.13 and a zero-point \nvalue of −round (\n255\n3.2+3.0 ⋅−3.0) −128 = −5 . The weight of 0.1 would be quantized to round(41.13 ⋅0.1 −5) = −1 , \nunlike the value of 4 provided by absmax.\nWe can easily get the dequantization by applying the inverse operation:\n𝐗𝐗dequant = 𝐗𝐗quant −zeropoint\nscale\n \nIn Python, zero-point quantization can be implemented as follows:\ndef zeropoint_quantize(X):\n    # Calculate value range (denominator)\n    x_range = torch.max(X) - torch.min(X)\n    x_range = 1 if x_range == 0 else x_range\n    # Calculate scale\n    scale = 255 / x_range\n    # Shift by zero-point\n    zeropoint = (-scale * torch.min(X) - 128).round()\n    # Scale and round the inputs\n    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n   \n    return X_quant.to(torch.int8)\nHowever, naïve quantization methods have limitations, particularly when dealing with outlier \nfeatures in LLMs. Outlier features are extreme weight values (about 0.1% of total values) that \ncan significantly impact the quantization process, leading to reduced precision for other values.\n\n\nInference Optimization\n308\nDiscarding these outliers is not feasible, as it would degrade a model’s performance. You can see \nan example of outliers in Figure 8.11:\nFigure 8.11 – Example of outliers in a weight matrix\nTo address the outlier problem, more advanced quantization techniques have been proposed. \nOne notable example is LLM.int8(), introduced by Dettmers et al. (2022). LLM.int8() employs a \nmixed-precision quantization scheme, where outlier features are processed using FP16, while the \nremaining values are quantized to INT8. This approach effectively reduces the memory footprint \nof LLMs by nearly 2x while minimizing performance degradation.\nLLM.int8() works by performing matrix multiplication in three steps. First, it extracts columns \ncontaining outlier features from the input hidden states using a custom threshold. Second, it \nperforms separate matrix multiplications for the outliers (in FP16) and non-outliers (in INT8) \nusing vector-wise quantization. Finally, it dequantizes the non-outlier results and combines \nthem with the outlier results to obtain the final output in FP16.\nThe effectiveness of LLM.int8() has been demonstrated empirically, showing negligible perfor-\nmance degradation (<1%) compared to the original FP32 models. However, it does introduce an \nadditional computational overhead, resulting in around 20% slower inference for large models. \nModels can be directly loaded in 8-bit precision with the transformer library, using LLM.int8(), \nas follows:\nfrom transformers import AutoModelForCausalLM\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_\nmap=\"auto\", load_in_8bit=True)\n\n\nChapter 8\n309\nIntroduced by Dettmers et al. (2023), NF4 is a 4-bit precision format designed for QLoRA (discussed \nin Chapter 5). It is also integrated into the transformers library but requires the bitsandbytes \nlibrary as a dependency. To load a model in NF4 (4-bit precision), you can use the load_in_4bit\nparameter, as follows:\nfrom transformers import AutoModelForCausalLM\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_\nmap=\"auto\", load_in_4bit=True)\nQuantization with GGUF and llama.cpp\nThe llama.cpp project is an open-source C++ software library created by Georgi Gerganov, de-\nsigned to perform inference with various LLMs. It is the most popular quantization technique, \nwith many quantized models available on the Hugging Face Hub.\nCompared to other libraries that rely on hardware-specific closed-source libraries like CUDA, \nllama.cpp can run on a broader range of hardware. It has gained significant popularity, par-\nticularly among users without specialized hardware, as it can operate on CPUs and Android \ndevices. Moreover, llama.cpp can also offload layers to the GPU, accelerating inference speed. It \nis compatible with different inference optimization techniques, such as FlashAttention-2 and \nspeculative decoding.\nThis project features its own quantization format, GGUF, designed to simplify and speed up \nmodel loading. GGUF files store tensors and metadata, supporting various formats, from 1-bit \nto 8-bit precision. It follows a naming convention based on the number of bits used and specific \nvariants, such as:\n• \nIQ1_S and IQ1_M: 1-bit precision – very low quality\n• \nIQ2_XXS/XS/S/M and Q2_K: 2-bit precision – generally low quality but IQ2 can be usable \nfor large models\n• \nIQ3_XXS/XS/S/M and Q3_K_S/M/L: 3-bit precision – low quality but usable for large models\n• \nIQ4_XS/NL and Q4_K_S/M, Q4_0/1: 4-bit precision – good quality and usable for most \nmodels\n•\nQ5_K_S/M and Q5_0/1: 5-bit precision – high quality\n•\nQ6_K: 6-bit precision –very high quality\n•\nQ8_0: 8-bit precision – highest quality\n\n\nInference Optimization\n310\nTo provide a brief overview of GGUF quantization, llama.cpp groups values into blocks and rounds \nthem to a lower precision. For instance, the legacy Q4_0 format handles 32 values per block, scaling \nand quantizing them based on the largest weight value in the block (w = q × block_scale ). In Q4_1, \nthe smallest Lvalue in the block is also added (w = q × block_scale + block_minimum  ). In Q4_K, weights \nare divided into super-blocks, containing 8 blocks with 32 values. Block scales and minimum \nvalues are also quantized in higher precision with 6 bits (w = q × block_scale(6bit) +  block_min(6bit) ). \nFinally, i-quants like IQ4_XS are inspired by another quantization technique called QuIP#. This \nensures an even number of positive (or negative) quant signs in groups of eight and implements \nthe E8 lattice to store their magnitude.\nHere is a practical example of how to quantize a model in the GGUF format. The following steps \ncan be executed on a free T4 GPU in Google Colab:\n1. \nInstall llama.cpp and the required libraries:\n!git clone https://github.com/ggerganov/llama.cpp\n!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n!pip install -r llama.cpp/requirements.txt\n2. \nDownload the model to convert. We will provide the model ID from the Hugging Face \nHub – for example, mistralai/Mistral-7B-Instruct-v0.2:\nMODEL_ID = \"mlabonne/EvolCodeLlama-7b\"\nMODEL_NAME = MODEL_ID.split('/')[-1]\n!git lfs install\n!git clone https://huggingface.co/{MODEL_ID}\n3.\nFirst, we convert the model into FP16. This is an intermediary artifact that will be used \nfor every GGUF quantization type. Note that different conversion scripts exist in llama.\ncpp and are compatible with different models:\nfp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile \n{fp16}\n4. \nWe select a format (here, Q4_K_M) and start the quantization. This process can take an \nhour on a T4 GPU:\nMETHOD = \"q4_k_m\"\nqtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n!./llama.cpp/quantize {fp16} {qtype} {METHOD}\n\n\nChapter 8\n311\n5. \nOnce it’s done, your quantized model is ready. You can download it locally, or upload it \nto the Hugging Face Hub using the following code:\nfrom huggingface_hub import create_repo, HfApi\nhf_token = \"\" # Specify your token\nusername = \"\" # Specify your username\napi = HfApi()\n# Create empty repo\ncreate_repo(\n    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n    repo_type=\"model\",\n    exist_ok=True,\n    token=hf_token\n)\n# Upload gguf files\napi.upload_folder(\n    folder_path=MODEL_NAME,\n    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n    allow_patterns=f\"*.gguf\",\n    token=hf_token\n)\nGGUF models can be used with backends such as llama-cpp-python and frameworks like Lang-\nChain. This is useful if you want to integrate a quantized model into a broader system. You can \nalso directly chat with the model using frontends, like llama.cpp’s lightweight server, LM Studio, \nand the Text Generation Web UI. These tools enable easy interaction with the GGUF models, \nproviding an experience similar to ChatGPT.\nQuantization with GPTQ and EXL2\nWhile GGUF and llama.cpp offer CPU inference with GPU offloading, GPTQ and EXL2 are two \nquantization formats dedicated to GPUs. This makes them both faster than llama.cpp during \ninference. In particular, EXL2 offers the highest throughput with its dedicated library, ExLlamaV2.\n\n\nInference Optimization\n312\nGPTQ and EXL2 quants are based on the GPTQ algorithm, introduced by Frantar et al. (2023). \nIt optimizes weight quantization for LLMs by refining the Optimal Brain Quantization (OBQ) \napproach to handle extensive matrices efficiently. It begins with a Cholesky decomposition of \nthe Hessian inverse, ensuring numerical stability. Instead of quantizing weights in a strict order, \nGPTQ processes them in batches, updating columns and associated blocks iteratively. This meth-\nod leverages lazy batch updates, reducing computational redundancy and memory bottlenecks.\nWhile GPTQ is limited to 4-bit precision, EXL2 offers more flexibility with a highly customizable \nprecision that can mix different quantization levels. This allows for precise bitrates between 2 \nand 8 bits per weight, such as 2.3, 3.5, or 6.0. It can also apply multiple quantization levels to \neach linear layer, prioritizing more important weights with higher bit quantization. Parameters \nare selected automatically, by quantizing each matrix multiple times and choosing a combination \nthat minimizes the quantization error while meeting a target bitrate. In practice, this allows 70B \nmodels to run on a single 24 GB GPU with 2.55-bit precision.\nThe inference itself is handled by the ExLlamaV2 library, which supports both the GPTQ and \nEXL2 models.\nIn the following example, let’s quantize a model in the EXL2 format using ExLlamaV2. These \nsteps can be executed on a free T4 GPU in Google Colab:\n1. \nInstall the ExLlamaV2 library from source:\n!git clone https://github.com/turboderp/exllamav2\n!pip install -e exllamav2\n2. We download the model to quantize by cloning its repo from the Hugging Face Hub:\nMODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\nMODEL_NAME = MODEL_ID.split('/')[-1]\n!git lfs install\n!git clone https://huggingface.co/{MODEL_ID}\n3. \nDownload the calibration dataset used to measure the quantization error. In this case, \nwe will use WikiText-103, a standard calibration dataset with high-quality articles from \nWikipedia:\n!wget https://huggingface.co/datasets/wikitext/\nresolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/\nwikitext-test.parquet\n",
      "page_number": 326
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 335-343)",
      "start_page": 335,
      "end_page": 343,
      "detection_method": "topic_boundary",
      "content": "Chapter 8\n313\n4. \nQuantize the model at a given precision (for example, 4.5):\n!mkdir quant\n!python exllamav2/convert.py \\\n    -i {MODEL_NAME} \\\n    -o quant \\\n    -c wikitext-test.parquet \\\n    -b 4.5\nThe quantized model can then be uploaded to the Hugging Face Hub, as seen previously.\nGPTQ and EXL2 quants are not as widely supported as GGUF. For example, frontends like LM \nStudio do not currently integrate them. You can use other tools instead, like oobabooga’s Text \nGeneration Web UI. It is also directly integrated into the transformers library and supported by \nTGI. GPTQ models are also supported in TensorRT-LLM.\nWhile less popular than GGUF, you can find a lot of GPTQ and EXL2 models on the Hugging Face \nHub.\nOther quantization techniques\nThere is a variety of quantization techniques beyond GGUF, GPTQ, and EXL2. This subsection will \nbriefly introduce Activate-aware Weight Quantization (AWQ) as well as extreme quantization \ntechniques, like QuIP# (Quantization with Incoherence Processing) and HQQ (Half-Quadratic \nQuantization).\nIntroduced by Lin et al. (2023), AWQ is another popular quantization algorithm. It identifies \nand protects the most important weights, which are determined based on activation magnitude \ninstead of weight magnitude. This approach involves applying optimal per-channel scaling to \nthese salient weights, without relying on backpropagation or reconstruction, ensuring that the \nLLM does not overfit the calibration set. While it relies on a different paradigm, AWQ is quite close \nto the GPTQ and EXL2 versions, although slightly slower. They are well-supported by inference \nengines and integrated into TGI, vLLM, and TensorRT-LLM.\nAn interesting trend is the quantization of models into 1- or 2-bit precision. While some formats, \nlike EXL2, allow extreme quantization, the quality of the models often suffers significantly. How-\never, recent algorithms like QuIP#  and HQQ have targeted this regime and offer quantization \nmethods that better preserve the performance of the original models. This is particularly true for \nlarge models (over 30B parameters), which can end up taking less space than 7B or 13B parameter \nmodels while providing higher-quality outputs. \n\n\nInference Optimization\n314\nThis trend is expected to continue, further optimizing these quantization methods.\nTo conclude this chapter, here is a table summarizing the features of the three main inference \nengines we covered in the previous sections:\nTechnique\nTGI\nvLLM\nTensorRT-LLM\nContinuous batching\n✓\n✓\n✓\nSpeculative decoding\n✓\nFlashAttention2\n✓\n✓\n✓\nPagedAttention\n✓\n✓\n✓\nPipeline parallelism\n✓\nTensor parallelism\n✓\n✓\n✓\nGPTQ\n✓\n✓\nEXL2\n✓\nAWQ\n✓\n✓\n✓\nTable 8.1 – Summary of features for TGI, vLLM, and TensorRT-LLM\nSummary\nIn summary, inference optimization is a critical aspect of deploying LLMs effectively. This chap-\nter explored various optimization techniques, including optimized generation methods, model \nparallelism, and weight quantization. Significant speedups can be achieved by leveraging tech-\nniques like predicting multiple tokens in parallel with speculative decoding, or using an optimized \nattention mechanism with FlashAttention-2. Additionally, we discussed how model parallelism \nmethods, including data, pipeline, and tensor parallelism, distribute the computational load across \nmultiple GPUs to increase throughput and reduce latency. Weight quantization, with formats \nlike GGUF and EXL2, further reduces the memory footprint and accelerates inference, with some \ncalculated tradeoff in output quality.\nUnderstanding and applying these optimization strategies are essential for achieving high per-\nformance in practical applications of LLMs, such as chatbots and code completion. The choice \nof techniques and tools depends on specific requirements, including available hardware, desired \nlatency, and throughput. By combining various approaches, such as continuous batching and \nspeculative decoding, along with advanced attention mechanisms and model parallelism, users \ncan tailor their deployment strategies to maximize efficiency.\n\n\nChapter 8\n315\nWay back in Chapter 4, we focused only on implementing the ingestion pipeline, which is just \none component of a standard RAG application. In the next chapter, we will conclude the RAG \nsystem by implementing the retrieval and generation components and integrating them into \nthe inference pipeline. \nReferences\n• \nHugging Face, Text Generation Inference, https://github.com/huggingface/text-\ngeneration-inference, 2022.\n• \nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Ef-\nficient Memory Management for Large Language Model Serving with PagedAttention, 2023.\n• \nNvidia, TensorRT-LLM, https://github.com/NVIDIA/TensorRT-LLM, 2023.\n• \nY. Leviathan, M. Kalman, Y. Matias, Fast Inference from Transformers via Speculative Decoding, \n2023.\n• \nT. Cai, Y. Li, Z. Geng, H. Peng, J.D. Lee, D. Chen, T. Dao, Medusa: Simple LLM Inference Accel-\neration Framework with Multiple Decoding Heads, 2024.\n• \nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Ef-\nficient Memory Management for Large Language Model Serving with PagedAttention, 2023.\n•\nR.Y. Aminabadi, S. Rajbhandari, M. Zhang, A.A. Awan, C. Li, D. Li, E. Zheng, J. Rasley, S. Smith, \nO. Ruwase, Y. He, DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at \nUnprecedented Scale, 2022.\n•\nY. Huang, Y. Cheng, A. Bapna, O. Firat, M.X. Chen, D. Chen, H. Lee, J. Ngiam, Q.V. Le, Y. Wu, \nZ. Chen, GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism, 2019.\n•\nK. James Reed, PiPPy: Pipeline Parallelism for PyTorch, https://github.com/pytorch/PiPPy, \n2022.\n•\nM. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro, Megatron-LM: Training \nMulti-Billion Parameter Language Models Using Model Parallelism, 2020.\n•\nVerma and Vaidya, Mastering LLM Techniques: Inference Optimization, NVIDIA Developer \nTechnical Blog, https://developer.nvidia.com/blog/mastering-llm-techniques-\ninference-optimization/, 2023.\n\n\nInference Optimization\n316\n•\nT. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, LLM.int8(): 8-bit Matrix Multiplication for \nTransformers at Scale, 2022.\n•\nG. Gerganov, llama.cpp, https://github.com/ggerganov/llama.cpp, 2023.\n•\nE. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, GPTQ: Accurate Post-Training Quantization for \nGenerative Pre-trained Transformers, 2023.\n•\nTuboderp, exllamav2, https://github.com/turboderp/exllamav2, 2023.\n•\nJ. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, S. Han, \nAWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, 2024.\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n\n\n9\nRAG Inference Pipeline\nBack in Chapter 4, we implemented the retrieval-augmented generation (RAG) feature pipeline \nto populate the vector database (DB). Within the feature pipeline, we gathered data from the data \nwarehouse, cleaned, chunked, and embedded the documents, and, ultimately, loaded them to the \nvector DB. Thus, at this point, the vector DB is filled with documents and ready to be used for RAG.\nBased on the RAG methodology, you can split your software architecture into three modules: one \nfor retrieval, one to augment the prompt, and one to generate the answer. We will follow a sim-\nilar pattern by implementing a retrieval module to query the vector DB. Within this module, we \nwill implement advanced RAG techniques to optimize the search. Afterward, we won’t dedicate \na whole module to augmenting the prompt, as that would be overengineering, which we try to \navoid. However, we will write an inference service that inputs the user query and context, builds \nthe prompt, and calls the LLM to generate the answer. To summarize, we will implement two core \nPython modules, one for retrieval and one for calling the LLM using the user’s input and context \nas input. When we glue these together, we will have an end-to-end RAG flow.\nIn Chapters 5 and 6, we fine-tuned our LLM Twin model, and in Chapter 8, we learned how to \noptimize it for inference. Thus, at this point, the LLM is ready for production. What is left is to \nbuild and deploy the two modules described above. \n\n\nRAG Inference Pipeline\n318\nWe will dedicate the next chapter entirely to deploying our fine-tuned LLM Twin model to AWS \nSageMaker, as an AWS SageMaker inference endpoint. Thus, the focus of this chapter is to dig \ninto the advanced RAG retrieval module implementation. We have dedicated a whole chapter to \nthe retrieval step because this is where the magic happens in an RAG system. At the retrieval step \n(and not when calling the LLM), you write most of the RAG inference code. This step is where \nyou have to wrangle your data to ensure that you retrieve the most relevant data points from the \nvector DB. Hence, most of the advanced RAG logic goes within the retrieval step.\nTo sum up, in this chapter, we will cover the following topics:\n•\nUnderstanding the LLM Twin’s RAG inference pipeline\n•\nExploring the LLM Twin’s advanced RAG techniques\n•\nImplementing the LLM Twin’s RAG inference pipeline\nBy the end of this chapter, you will know how to implement an advanced RAG retrieval module, \naugment a prompt using the retrieved context, and call an LLM to generate the final answer. \nUltimately, you will know how to build a production-ready RAG inference pipeline end to end.\nUnderstanding the LLM Twin’s RAG inference \npipeline\nBefore implementing the RAG inference pipeline, we want to discuss its software architecture \nand advanced RAG techniques. Figure 9.1 illustrates an overview of the RAG inference flow. The \ninference pipeline starts with the input query, retrieves the context using the retrieval module \n(based on the query), and calls the LLM SageMaker service to generate the final answer.\n\n\nChapter 9\n319\nFigure 9.1: RAG inference pipeline architecture\nThe feature pipeline and the retrieval module, defined in Figure 9.1, are independent processes. The \nfeature pipeline runs on a different machine on a schedule to populate the vector DB. At the same \ntime, the retrieval module is called on demand, within the inference pipeline, on every user request. \n\n\nRAG Inference Pipeline\n320\nBy separating concerns between the two components, the vector DB is always populated with the \nlatest data, ensuring feature freshness, while the retrieval module can access the latest features \non every request. The input of the RAG retrieval module is the user’s query, based on which we \nhave to return the most relevant and similar data points from the vector DB, which will be used \nto guide the LLM in generating the final answer.\nTo fully understand the dynamics of the RAG inference pipeline, let’s go through the architecture \nflow from Figure 9.1 step by step:\n1. \nUser query: We begin with the user who makes a query, such as “Write an article about...”\n2. \nQuery expansion: We expand the initial query to generate multiple queries that reflect \ndifferent aspects or interpretations of the original user query. Thus, instead of one query, \nwe will use xN queries. By diversifying the search terms, the retrieval module increases \nthe likelihood of capturing a comprehensive set of relevant data points. This step is crucial \nwhen the original query is too narrow or vague.\n3.\nSelf-querying: We extract useful metadata from the original query, such as the author’s \nname. The extracted metadata will be used as filters for the vector search operation, elim-\ninating redundant data points from the query vector space (making the search more \naccurate and faster).\n4.\nFiltered vector search: We embed each query and perform a similarity search to find \neach search’s top K data points. We execute xN searches corresponding to the number of \nexpanded queries. We call this step a filtered vector search as we leverage the metadata \nextracted from the self-query step as query filters.\n5.\nCollecting results: We get up to xK results closest to its specific expanded query interpre-\ntation for each search operation. Further, we aggregate the results of all the xN searches, \nending up with a list of N x K results containing a mix of articles, posts, and repositories \nchunks. The results include a broader set of potentially relevant chunks, offering multiple \nrelevant angles based on the original query’s different facets.\n6.\nReranking: To keep only the top K most relevant results from the list of N x K potential \nitems, we must filter the list further. We will use a reranking algorithm that scores each \nchunk based on the relevance and importance relative to the initial user query. We will \nleverage a neural cross-encoder model to compute the score, a value between 0 and 1, \nwhere 1 means the result is entirely relevant to the query. Ultimately, we sort the N x K\nresults based on the score and pick the top K items. Thus, the output is a ranked list of K\nchunks, with the most relevant data points situated at the top.\n\n\nChapter 9\n321\n7.\nBuild the prompt and call the LLM: We map the final list of the most relevant K chunks \nto a string used to build the final prompt. We create the prompt using a prompt template, \nthe retrieved context, and the user’s query. Ultimately, the augmented prompt is sent to \nthe LLM (hosted on AWS SageMaker exposed as an API endpoint).\n8. Answer: We are waiting for the answer to be generated. After the LLM processes the \nprompt, the RAG logic finishes by sending the generated response to the user.\nThat wraps up the overview of the RAG inference pipeline. Now, let’s dig deeper into the details.\nExploring the LLM Twin’s advanced RAG techniques\nNow that we understand the overall flow of our RAG inference pipeline, let’s explore the advanced \nRAG techniques we used in our retrieval module:\n•\nPre-retrieval step: Query expansion and self-querying\n•\nRetrieval step: Filtered vector search\n•\nPost-retrieval step: Reranking\nBefore digging into each method individually, let’s lay down the Python interfaces we will use \nin this section, which are available at https://github.com/PacktPublishing/LLM-Engineers-\nHandbook/blob/main/llm_engineering/application/rag/base.py.\nThe first is a prompt template factory that standardizes how we instantiate prompt templates. \nAs an interface, it inherits from ABC and exposes the create_template() method, which returns \na LangChain PromptTemplate instance. Even if we avoid being heavily reliant on LangChain, as \nwe want to implement everything ourselves to understand the engineering behind the scenes, \nsome objects, such as the PromptTemplate class, are helpful to speed up the development without \nhiding too much functionality:\nfrom abc import ABC, abstractmethod\nfrom langchain.prompts import PromptTemplate\nfrom pydantic import BaseModel\nclass PromptTemplateFactory(ABC, BaseModel):\n",
      "page_number": 335
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 344-356)",
      "start_page": 344,
      "end_page": 356,
      "detection_method": "topic_boundary",
      "content": "RAG Inference Pipeline\n322\n    @abstractmethod\n    def create_template(self) -> PromptTemplate:\n        pass\nWe also want to define a RAGStep interface used to standardize the interface of advanced RAG \nsteps such as query expansion and self-querying. As these steps are often dependent on other \nLLMs, it has a mock attribute to reduce costs and debugging time during development:\nfrom typing import Any\nfrom llm_engineering.domain.queries import Query\nclass RAGStep(ABC):\n    def __init__(self, mock: bool = False) -> None:\n        self._mock = mock\n    @abstractmethod\n    def generate(self, query: Query, *args, **kwargs) -> Any:\n        pass\nUltimately, we must understand how we modeled the Query domain entity to wrap the user’s \ninput with other metadata required for advanced RAG. Thus, let’s look at its implementation. \nFirst, we import the necessary classes:\nfrom pydantic import UUID4, Field\nfrom llm_engineering.domain.base import VectorBaseDocument\nfrom llm_engineering.domain.types import DataCategory\nNext, we define the Query entity class, which inherits from the VectorBaseDocument object-vector \nmapping (OVM) class, discussed in Chapter 4. Thus, each query can easily be saved or retrieved \nfrom the vector DB:\nclass Query(VectorBaseDocument):\n    content: str\n    author_id: UUID4 | None = None\n    author_full_name: str | None = None\n    metadata: dict = Field(default_factory=dict)\n\n\nChapter 9\n323\nclass Config:\n        category = DataCategory.QUERIES\nWhat is essential to notice are the class’s attributes used to combine the user’s query with a \nbunch of metadata fields:\n• \ncontent: A string containing input query.\n•\nauthor_id: An optional UUID4 identifier extracted from the query used as a filter within \nthe vector search operation to retrieve chunks written only by a specific author\n•\nauthor_full_name: An optional string used to query the author_id\n• \nmetadata: A dictionary for any additional metadata, initialized as an empty dict by default\nBesides the standard definition of a domain class, we also define a from_str() class method to \ncreate a Query instance directly from a string. This allows us to standardize how we clean the query \nstring before constructing the query object, such as stripping any leading or trailing whitespace \nand newline characters:\n    @classmethod\n    def from_str(cls, query: str) -> \"Query\":\n        return Query(content=query.strip(\"\\n \"))\nAdditionally, there’s an instance method called replace_content() used to create a new Query\ninstance with updated content while retaining the original query’s id, author_id, author_full_\nname, and metadata:\n    def replace_content(self, new_content: str) -> \"Query\":\n        return Query(\n            id=self.id,\n            content=new_content,\n            author_id=self.author_id,\n            author_full_name=self.author_full_name,\n            metadata=self.metadata,\n        )\nThis can be particularly useful when modifying the query text, for example, during preprocessing \nor normalization, without losing the associated metadata or identifiers. Following the Query class, \nwe define the EmbeddedQuery class:\nclass EmbeddedQuery(Query):\n\n\nRAG Inference Pipeline\n324\n    embedding: list[float]\n    class Config:\n        category = DataCategory.QUERIES\nThe EmbeddedQuery class extends Query by adding the embedding field. The EmbeddedQuery entity \nencapsulates all the data and metadata necessary to perform vector search operations on top of \nQdrant (or another vector DB).\nNow that we understand all the interfaces and new domain entities used within the RAG inference \npipeline, let’s move on to our advanced RAG pre-retrieval optimization techniques.\nAdvanced RAG pre-retrieval optimizations: query expansion \nand self-querying\nWe implemented two methods to optimize the pre-retrieval optimization step: query expansion \nand self-querying. The two methods work closely with the filtered vector search step, which we \nwill touch on in the next section. For now, however, we will start with understanding the code \nfor query expansion and move to implementing self-querying.\nWithin these two methods, we will leverage OpenAI’s API to generate variations of the original \nquery within the query expansion step and to extract the necessary metadata within the self-que-\nrying algorithm. When we wrote this book, we used GPT-4o-mini in all our examples, but as \nOpenAI’s models quickly evolve, the model might get deprecated. But that’s not an issue, as you \ncan quickly change it in your .env file by configuring the OPENAI_MODEL_ID environment variable.\nQuery expansion\nThe problem in a typical retrieval step is that you query your vector DB using a single vector rep-\nresentation of your original question. This approach covers only a small area of the embedding \nspace, which can be limiting. If the embedding doesn’t contain all the required information or \nnuances of your query, the retrieved context may not be relevant. This means essential documents \nthat are semantically related but not near the query vector might be overlooked.\nThe solution is based on query expansion, which offers a way to overcome this limitation. Using an \nLLM to generate multiple queries based on your initial question, you create various perspectives \nthat capture different facets of your query. These expanded queries, when embedded, target other \nareas of the embedding space that are still relevant to your original question. This increases the \nlikelihood of retrieving more relevant documents from the vector DB. \n\n\nChapter 9\n325\nImplementing query expansion can be as straightforward as crafting a detailed zero-shot prompt \nto guide the LLM in generating these alternative queries. Thus, after implementing query ex-\npansion, instead of having only one query to search relevant context, you will have xN queries, \nhence xN searches.\nIncreasing the number of searches can impact your latency. Thus, you must experiment with the \nnumber of queries you generate to ensure the retrieval step meets your application requirements. \nYou can also optimize the searches by parallelizing them, drastically reducing the latency, which \nwe will do in the ContextRetriever class implemented at the end of this chapter. \nNow, let’s dig into the code. We begin by importing the necessary modules and classes required \nfor query expansion:\nfrom langchain_openai import ChatOpenAI\nfrom llm_engineering.domain.queries import Query\nfrom llm_engineering.settings import settings\nfrom .base import RAGStep\nfrom .prompt_templates import QueryExpansionTemplate\nNext, we define the QueryExpansion class, which generates expanded query versions. The class \nimplementation can be found at https://github.com/PacktPublishing/LLM-Engineers-\nHandbook/blob/main/llm_engineering/application/rag/query_expanison.py:\nclass QueryExpansion(RAGStep):\n    def generate(self, query: Query, expand_to_n: int) -> list[Query]:\n        assert expand_to_n > 0, f\"'expand_to_n' should be greater than 0. \nGot {expand_to_n}.\"\n        if self._mock:\n            return [query for _ in range(expand_to_n)]\nQuery expansion is also known as multi-query, but the principles are the \nsame. For example, this is an example of LangChain’s implementation called \nMultiQueryRetriver: https://python.langchain.com/docs/how_to/\nMultiQueryRetriever/\n\n\nRAG Inference Pipeline\n326\nIn the generate method, we first ensure that the number of expansions requested (expand_to_n) \nis greater than zero. If the instance is in mock mode (self._mock is True), it simply returns a \nlist containing copies of the original query to simulate expansion without actually calling the \nAPI. If not in mock mode, we proceed to create the prompt and initialize the language model:\n        query_expansion_template = QueryExpansionTemplate()\n        prompt = query_expansion_template.create_template(expand_to_n - 1)\n        model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_\nkey=settings.OPENAI_API_KEY, temperature=0)\nHere, we instantiate QueryExpansionTemplate and create a prompt tailored to generate expand_\nto_n - 1 new queries (excluding the original). We initialize the ChatOpenAI model with the \nspecified settings and set the temperature to 0 for deterministic output. We then create a Lang-\nChain chain by combining the prompt with the model and invoke it with the user’s question:\n        chain = prompt | model\n        response = chain.invoke({\"question\": query})\n        result = response.content\nBy piping the prompt into the model (prompt | model), we set up a chain that generates expanded \nqueries when invoked with the original query. The response from the model is captured in the \nresult object. After receiving the response, we parse and clean the expanded queries:\n\t\t\nWe split the result using the separator defined in the template to get individual queries. Starting \nwith a list containing the original query, we append each expanded query after stripping any \nextra whitespace.\n\n\nChapter 9\n327\nFinally, we define the QueryExpansionTemplate class, which constructs the prompt used for \nquery expansion. The class and other prompt templates can be accessed at https://github.com/\nPacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/\nprompt_templates.py:\nfrom langchain.prompts import PromptTemplate\nfrom .base import PromptTemplateFactory\nclass QueryExpansionTemplate(PromptTemplateFactory):\n    prompt: str = \"\"\"You are an AI language model assistant. Your task is \nto generate {expand_to_n}\n    different versions of the given user question to retrieve relevant \ndocuments from a vector\n    database. By generating multiple perspectives on the user question, \nyour goal is to help\n    the user overcome some of the limitations of the distance-based \nsimilarity search.\n    Provide these alternative questions separated by '{separator}'.\n    Original question: {question}\"\"\"\n    @property\n    def separator(self) -> str:\n        return \"#next-question#\"\n    def create_template(self, expand_to_n: int) -> PromptTemplate:\n        return PromptTemplate(\n            template=self.prompt,\n            input_variables=[\"question\"],\n            partial_variables={\n                \"separator\": self.separator,\n                \"expand_to_n\": expand_to_n,\n            },\n        )\nThis class defines a prompt instructing the language model to generate multiple versions of \nthe user’s question. It uses placeholders like {expand_to_n}, {separator}, and {question} to \ncustomize the prompt. \n\n\nRAG Inference Pipeline\n328\nIt takes expand_to_n as an input parameter to define how many queries we wish to generate while \nwe build the PromptTemplate instance. The separator property provides a unique string to split the \ngenerated queries. The expand_to_n and separator variables are passed as partial_variables, \nmaking them immutable at runtime. Meanwhile, the {question} placeholder will be changed \nevery time the LLM chain is called.\nNow that we have finished studying the query expansion implementation, let’s look at an exam-\nple of how to use the QueryExpansion class. Let’s run the following code using this python -m \nllm_engineering.application.rag.query_expansion command:\nquery = Query.from_str(\"Write an article about the best types of advanced \nRAG methods.\")\n    query_expander = QueryExpansion()\n    expanded_queries = query_expander.generate(query, expand_to_n=3)\n    for expanded_query in expanded_queries:\n        logger.info(expanded_query.content)\nWe get the following variations of the original query. As you can observe, the query expansion \nmethod was successful in providing more details and different perspectives of the initial query, \nsuch as highlighting the effectiveness of advanced RAG methods or the overview of these methods \n(remember that the first query is the original one):\n2024-09-18 17:51:33.529 | INFO  - Write an article about the best types of \nadvanced RAG methods.\n2024-09-18 17:51:33.529 | INFO  - What are the most effective advanced RAG \nmethods, and how can they be applied?\n2024-09-18 17:51:33.529 | INFO  - Can you provide an overview of the top \nadvanced retrieval-augmented generation techniques?\nNow, let’s move to the next pre-retrieval optimization method: self-querying.\nSelf-querying\nThe problem when embedding your query into a vector space is that you cannot guarantee that \nall the aspects required by your use case are present with enough signal in the embedding vec-\ntor. For example, you want to be 100% sure that your retrieval depends on the tags provided \nin the user’s input. Unfortunately, you can’t control the signal left within the embedding that \nemphasizes the tag. By embedding the query prompt alone, you can never be sure that the tags \nare sufficiently represented in the embedding vector or have enough signal when computing the \ndistance against other vectors. \n\n\nChapter 9\n329\nThis problem stands for any other metadata you want to present during the search, such as IDs, \nnames, or categories.\nThe solution is to use self-querying to extract the tags or other critical metadata within the query \nand use them alongside the vector search as filters. Self-querying uses an LLM to extract various \nmetadata fields crucial for your business use case, such as tags, IDs, number of comments, likes, \nshares, etc. Afterward, you have complete control over how the extracted metadata is considered \nduring retrieval. In our LLM Twin use case, we extract the author’s name and use it as a filter. \nSelf-queries work hand-in-hand with filtered vector searches, which we will explain in the next \nsection.\nNow, let’s move on to the code. We begin by importing the necessary modules and classes on \nwhich our code relies:\nfrom langchain_openai import ChatOpenAI\nfrom llm_engineering.application import utils\nfrom llm_engineering.domain.documents import UserDocument\nfrom llm_engineering.domain.queries import Query\nfrom llm_engineering.settings import settings\nfrom .base import RAGStep\nfrom .prompt_templates import SelfQueryTemplate\nNext, we define the SelfQuery class, which inherits from RAGStep and implements the generate()\nmethod. The class can be found at https://github.com/PacktPublishing/LLM-Engineers-\nHandbook/blob/main/llm_engineering/application/rag/self_query.py:\nclass SelfQuery(RAGStep):\n    def generate(self, query: Query) -> Query:\n        if self._mock:\n            return query\nIn the generate() method, we check if the _mock attribute is set to True. If it is, we will return the \noriginal query object unmodified. This allows us to bypass calling the model while testing and \ndebugging. If not in mock mode, we create the prompt template and initialize the language model.\n        prompt = SelfQueryTemplate().create_template()\n        model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_\nkey=settings.OPENAI_API_KEY, temperature=0)\n\n\nRAG Inference Pipeline\n330\nHere, we instantiate the prompt using the SelfQueryTemplate factory class and create a ChatOpenAI\nmodel instance (similar to the query expansion implementation). We then combine the prompt \nand the model into a chain and invoke it with the user’s query.\n        chain = prompt | model\n        response = chain.invoke({\"question\": query})\n        user_full_name = response.content.strip(\"\\n \")\nWe extract the content from the LLM response and strip any leading or trailing whitespace to \nobtain the user_full_name value. Next, we check if the model was able to extract any user in-\nformation.\n        if user_full_name == \"none\":\n            return query\nIf the response is \"none\", it means no user name was found in the query, so we return the origi-\nnal query object. If a user name is found, we will split the user_full_name into the first_name\nand last_name variables using a utility function. Then, based on the user’s details, we retrieve \nor create a UserDocument user instance:\n        first_name, last_name = utils.split_user_full_name(user_full_name)\n        user = UserDocument.get_or_create(first_name=first_name, last_\nname=last_name)\nFinally, we update the query object with the extracted author information and return it:\n        query.author_id = user.id\n        query.author_full_name = user.full_name\n        return query\nThe updated query now contains the author_id and author_full_name values, which can be \nused in subsequent steps of the RAG pipeline.\nLet’s look at the SelfQueryTemplate class, which defines the prompt to extract user information:\nfrom langchain.prompts import PromptTemplate\nfrom .base import PromptTemplateFactory\n\n\nChapter 9\n331\nclass SelfQueryTemplate(PromptTemplateFactory):\n    prompt: str = \"\"\"You are an AI language model assistant. Your task is \nto extract information from a user question.\n    The required information that needs to be extracted is the user name \nor user id.\n    Your response should consist of only the extracted user name (e.g., \nJohn Doe) or id (e.g. 1345256), nothing else.\n    If the user question does not contain any user name or id, you should \nreturn the following token: none.\n   \n    For example:\n    QUESTION 1:\n    My name is Paul Iusztin and I want a post about...\n    RESPONSE 1:\n    Paul Iusztin\n   \n    QUESTION 2:\n    I want to write a post about...\n    RESPONSE 2:\n    none\n   \n    QUESTION 3:\n    My user id is 1345256 and I want to write a post about...\n    RESPONSE 3:\n    1345256\n   \n    User question: {question}\"\"\"\n    def create_template(self) -> PromptTemplate:\n        return PromptTemplate(template=self.prompt, input_\nvariables=[\"question\"])\nIn the SelfQueryTemplate class, we define a prompt instructing the AI model to extract the user \nname or ID from the input question. The prompt uses few-shot learning to guide the model on \nhow to respond in different scenarios. When the template is invoked, the {question} placeholder \nwill be replaced with the actual user question.\n\n\nRAG Inference Pipeline\n332\nBy implementing self-querying, we ensure that critical metadata required for our use case is ex-\nplicitly extracted and used during retrieval. This approach overcomes the limitations of relying \nsolely on the semantics of the embeddings to capture all necessary aspects of a query.\nNow that we’ve implemented the SelfQuery class, let’s provide an example. Run the following \ncode using the python -m llm_engineering.application.rag.self_query CLI command:\n    query = Query.from_str(\"I am Paul Iusztin. Write an article about the \nbest types of advanced RAG methods.\")\n    self_query = SelfQuery()\n    query = self_query.generate(query)\n    logger.info(f\"Extracted author_id: {query.author_id}\")\n    logger.info(f\"Extracted author_full_name: {query.author_full_name}\")\nWe get the following results where the author’s full name and ID were extracted correctly:\n2024-09-18 18:02:10.362 | INFO - Extracted author_id: 900fec95-d621-4315-\n84c6-52e5229e0b96\n2024-09-18 18:02:10.362 | INFO - Extracted author_full_name: Paul Iusztin\nNow that we understand how self-querying works, let’s explore how it can be used together with \nfiltered vector search within the retrieval optimization step.\nAdvanced RAG retrieval optimization: filtered vector search\nVector search is pivotal in retrieving relevant information based on semantic similarity. A plain \nvector search, however, can introduce significant challenges that affect both the accuracy and \nlatency of information retrieval. This is primarily because it operates solely on the numerical \nproximity of vector embeddings without considering the contextual or categorical nuances that \nmight be crucial for relevance.\nOne of the primary issues with plain vector search is retrieving semantically similar but contex-\ntually irrelevant documents. Since vector embeddings capture general semantic meanings, they \nmight assign high similarity scores to content that shares language patterns or topics but doesn’t \nalign with the specific intent or constraints of the query. For instance, searching for “Java” could \nretrieve documents about the programming language or the Indonesian island, depending solely \non semantic similarity, leading to ambiguous or misleading results.\nMoreover, as the size of the dataset increases, plain vector search can suffer from scalability issues. \nThe lack of filtering means the search algorithm has to compute similarities across the entire \nvector space, which can significantly increase latency. \n\n\nChapter 9\n333\nThis exhaustive search slows response times and consumes more computational resources, making \nit inefficient for real-time or large-scale applications.\nFiltered vector search emerges as a solution by filtering after additional criteria, such as metadata \ntags or categories, reducing the search space before computing vector similarities. By applying \nthese filters, the search algorithm narrows the pool of potential results to those contextually \naligned with the query’s intent. This targeted approach enhances accuracy by eliminating irrel-\nevant documents that might have otherwise been considered due to their semantic similarities \nalone.\nAdditionally, filtered vector search improves latency by reducing the number of comparisons the \nalgorithm needs to perform. Working with a smaller, more relevant subset of data decreases the \ncomputational overhead, leading to faster response times. This efficiency is crucial for applica-\ntions requiring real-time interactions or handling large queries.\nAs the metadata used within the filtered vector search is often part of the user’s input, we have \nto extract it before querying the vector DB. That’s precisely what we did during the self-query \nstep, where we extracted the author’s name to reduce the vector space only to the author’s con-\ntent. Thus, as we processed the query within the self-query step, it went into the pre-retrieval \noptimization category, whereas when the filtered vector search optimized the query, it went into \nthe retrieval optimization bin.\nFor example, when using Qdrant, to add a filter that looks for a matching author_id within the \nmetadata of each document, you must implement the following code:\nfrom qdrant_client.models import FieldCondition, Filter, MatchValue\nrecords = qdrant_connection.search(\n            collection_name=\"articles\",\n            query_vector=query_embedding,\n            limit=3,\n            with_payload=True,\n            query_filter= Filter(\n                    must=[\n                        FieldCondition(\n                            key=\"author_id\",\n                            match=MatchValue(\n                                value=str(\"1234\"),\n                            ),\n\n\nRAG Inference Pipeline\n334\n                        )\n                    ]\n                ),\n        )\nIn essence, while plain vector search provides a foundation for semantic retrieval, its limitations \ncan slow performance in practical applications. Filtered vector search addresses these challenges \nby combining the strengths of vector embeddings with contextual filtering, resulting in more \naccurate and efficient information retrieval in RAG systems. The last step for optimizing our RAG \npipeline is to look into reranking.\nAdvanced RAG post-retrieval optimization: reranking\nThe problem in RAG systems is that the retrieved context may contain irrelevant chunks that only:\n• \nAdd noise: The retrieved context might be irrelevant, cluttering the information and \npotentially confusing the language model.\n•\nMake the prompt bigger: Including unnecessary chunks increases the prompt size, lead-\ning to higher costs. Moreover, language models are usually biased toward the context’s \nfirst and last pieces. So, if you add a large amount of context, there’s a big chance it will \nmiss the essence.\n•\nBe come unaligned with your question: Chunks are retrieved based on the similarity \nbetween the query and chunk embeddings. The issue is that the embedding model might \nnot be tuned to your question, resulting in high similarity scores for chunks that aren’t \nentirely relevant.\nThe solution is to use reranking to order all the N × K retrieved chunks based on their relevance \nrelative to the initial question, where the first chunk will be the most relevant and the last the \nleast. N represents the number of searches after query expansion, while K is the number of chunks \nretrieved per search. Hence, we retrieve a total of N x K chunks. In RAG systems, reranking serves \nas a critical post-retrieval step that refines the initial results obtained from the retrieval model.\nWe assess each chunk’s relevance to the original query by applying the reranking algorithm, which \noften uses advanced models like neural cross-encoders. These models evaluate the semantic sim-\nilarity between the query and each chunk more accurately than initial retrieval methods based \non embeddings and the cosine similarity distance, as explained in more detail in Chapter 4 in the \nAn overview of advanced RAG section.\n",
      "page_number": 344
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 357-366)",
      "start_page": 357,
      "end_page": 366,
      "detection_method": "topic_boundary",
      "content": "Chapter 9\n335\nUltimately, we pick the top K most relevant chunks from the sorted list of N x K items based on \nthe reranking score. Reranking works well when combined with query expansion. First, let’s \nunderstand how reranking works without query expansion:\n1.\nSearch for > K chunks: Retrieve more than K chunks to have a broader pool of potentially \nrelevant information.\n2.\nReorder using rerank: Apply reranking to this larger set to evaluate the actual relevance \nof each chunk relative to the query.\n3.\nTake top K: Select the top K chunks to use them as context in the final prompt.\nThus, when combined with query expansion, we gather potential valuable context from multiple \npoints in space rather than just looking for more than K samples in a single location. Now the \nflow looks like this:\n1.\nSearch for N × K chunks: Retrieve multiple sets of chunks using the expanded queries.\n2.\nReorder using rerank: Rerank all the retrieved chunks based on their relevance.\n3.\nTake top K: Select the most relevant chunks for the final prompt.\nIntegrating reranking into the RAG pipeline enhances the quality and relevance of the re-\ntrieved context and efficiently uses computational resources. Let’s look at implementing the \nLLM Twin’s reranking step to understand what we described above, which can be accessed on \nGitHub at https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_\nengineering/application/rag/reranking.py.\nWe begin by importing the necessary modules and classes for our reranking process:\nfrom llm_engineering.application.networks import \nCrossEncoderModelSingleton\nfrom llm_engineering.domain.embedded_chunks import EmbeddedChunk\nfrom llm_engineering.domain.queries import Query\nfrom .base import RAGStep\nNext, we define the Reranker class, which is responsible for reranking the retrieved documents \nbased on their relevance to the query:\nclass Reranker(RAGStep):\n    def __init__(self, mock: bool = False) -> None:\n        super().__init__(mock=mock)\n        self._model = CrossEncoderModelSingleton()\n\n\nRAG Inference Pipeline\n336\nIn the initializer of the Reranker class, we instantiate our cross-encoder model by creating an \ninstance of CrossEncoderModelSingleton. This is the cross-encoder model used to score the \nrelevance of each document chunk with respect to the query.\nThe core functionality of the Reranker class is implemented in the generate() method:\n    def generate(self, query: Query, chunks: list[EmbeddedChunk], keep_\ntop_k: int) -> list[EmbeddedChunk]:\n        if self._mock:\n            return chunks\n        query_doc_tuples = [(query.content, chunk.content) for chunk in \nchunks]\n        scores = self._model(query_doc_tuples)\n        scored_query_doc_tuples = list(zip(scores, chunks, strict=False))\n        scored_query_doc_tuples.sort(key=lambda x: x[0], reverse=True)\n        reranked_documents = scored_query_doc_tuples[:keep_top_k]\n        reranked_documents = [doc for _, doc in reranked_documents]\n        return reranked_documents\nThe generate() method takes a query, a list of chunks (document segments), and the number \nof top documents to keep (keep_top_k). If we’re in mock mode, it simply returns the original \nchunks. Otherwise, it performs the following steps:\n1.\nCreates pairs of the query content and each chunk’s content\n2.\nUses the cross-encoder model to score each pair, assessing how well the chunk matches \nthe query\n3.\nZips the scores with the corresponding chunks to create a scored list of tuples\n4.\nSorts this list in descending order based on the scores\n5.\nSelects the top keep_top_k chunks\n6.\nExtracts the chunks from the tuples and returns them as the reranked documents\nBefore defining the CrossEncoder class, we import the necessary components:\nfrom sentence_transformers.cross_encoder import CrossEncoder\nfrom .base import SingletonMeta\n\n\nChapter 9\n337\nWe import the CrossEncoder class from the sentence_transformers library, which provides the \nfunctionality for scoring text pairs. We also import SingletonMeta from our base module to en-\nsure our model class follows the singleton pattern, meaning only one instance of the model exists \nthroughout the application. Now, we define the CrossEncoderModelSingleton class:\nclass CrossEncoderModelSingleton(metaclass=SingletonMeta):\n    def __init__(\n        self,\n        model_id: str = settings.RERANKING_CROSS_ENCODER_MODEL_ID,\n        device: str = settings.RAG_MODEL_DEVICE,\n    ) -> None:\n        \"\"\"\n        A singleton class that provides a pre-trained cross-encoder model \nfor scoring pairs of input text.\n        \"\"\"\n        self._model_id = model_id\n        self._device = device\n        self._model = CrossEncoder(\n            model_name=self._model_id,\n            device=self._device,\n        )\n        self._model.model.eval()\nThis class initializes the cross-encoder model using the specified model_id and device from the \nglobal settings loaded from the .env file. We set the model to evaluation mode using self._\nmodel.model.eval() to ensure the model is ready for inference.\nThe CrossEncoderModelSingleton class includes a callable method to score text pairs:\n    def __call__(self, pairs: list[tuple[str, str]], to_list: bool = True) \n-> NDArray[np.float32] | list[float]:\n        scores = self._model.predict(pairs)\n        if to_list:\n            scores = scores.tolist()\n        return scores\n\n\nRAG Inference Pipeline\n338\nThe __call__ method allows us to pass in a list of text pairs (each consisting of the query and \na document chunk) and receive their relevance scores. The method uses the model’s predict()\nfunction to call the model and compute the scores.\nThe CrossEncoderModelSingleton class is a wrapper over the CrossEncoder class, which we wrote \nfor two purposes. The first one is for the singleton pattern, which allows us to easily access the \nsame instance of the cross-encoder model from anywhere within the application without loading \nthe model in memory every time we need it. The second reason is that by writing our wrapper, \nwe defined our interface for a cross-encoder model (or any other model used for reranking). \nThis makes the code future-proof as in case we need a different implementation or strategy for \nreranking, for example, using an API, we only have to write a different wrapper that follows the \nsame interface and swap the old class with the new one. Thus, we can introduce new reranking \nmethods without touching the rest of the code.\nWe now understand all the advanced RAG techniques used within our architecture. In the next \nsection, we will examine the ContextRetriever class that connects all these methods and explain \nhow to use the retrieval module with an LLM for an end-to-end RAG inference pipeline.\nImplementing the LLM Twin’s RAG inference pipeline\nAs explained at the beginning of this chapter, the RAG inference pipeline can mainly be divided \ninto three parts: the retrieval module, the prompt creation, and the answer generation, which \nboils down to calling an LLM with the augmented prompt. In this section, our primary focus will \nbe implementing the retrieval module, where most of the code and logic go. Afterward, we will \nlook at how to build the final prompt using the retrieved context and user query.\nUltimately, we will examine how to combine the retrieval module, prompt creation logic, and \nthe LLM to capture an end-to-end RAG workflow. Unfortunately, we won’t be able to test out \nthe LLM until we finish Chapter 10, as we haven’t deployed our fine-tuned LLM Twin module to \nAWS SageMaker.\nThus, by the end of this section, you will learn how to implement the RAG inference pipeline, \nwhich you can test out end to end only after finishing Chapter 10. Now, let’s start by looking at \nthe implementation of the retrieval module.\n\n\nChapter 9\n339\nImplementing the retrieval module\nLet’s dive into the ContextRetriever class implementation, which orchestrates the retrieval \nstep in our RAG system by integrating all the advanced techniques we previously used: que-\nry expansion, self-querying, reranking, and filtered vector search. The class can be found on \nGitHub at https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_\nengineering/application/rag/retriever.py.\nFigure 9.2: Search logic of the RAG retrieval module\n\n\nRAG Inference Pipeline\n340\nThe entry point function of the ContextRetriever class is the search() method, which calls all the \nadvanced steps discussed in this chapter. Figure 9.2 shows in more detail how the search method \nglues together all the steps required to search results similar to the user’s query. It highlights how \nthe extracted author details from the self-query step are used within the filtered vector search. \nAlso, it zooms in on the search operation itself, where, for each query, we do three searches to \nthe vector DB, looking for articles, posts, or repositories similar to the query. For each search (out \nof N searches), we want to retrieve a maximum of K results. Thus, we retrieve a maximum of K \n/ 3 items for each data category (as we have three categories). Therefore, when summed up, we \nwill have a list of ≤ K chunks. The retrieved list is ≤ K (and not equal to K) when a particular data \ncategory or more returns < K / 3 items after applying the author filters due to missing chunks \nfor that specific author or data category.\nFigure 9.3: Processing the results flow of the RAG retrieval module\n\n\nChapter 9\n341\nFigure 9.3 illustrates how we process the results returned by the xN searches. As each search \nreturns ≤ K items, we will end up with ≤ N x K chunks that we aggregate into a single list. As \nsome results might overlap between searchers, we must deduplicate the aggregated list to ensure \neach chunk is unique. Ultimately, we send the results to the rerank model, order them based on \ntheir reranking score, and pick the most relevant top K chunks we will use as context for RAG.\nLet’s understand how everything from Figures 9.2 and 9.3 is implemented in the ContextRetriever \nclass. First, we initialize the class by setting up instances of the QueryExpansion, SelfQuery, and \nReranker classes:\nclass ContextRetriever:\n    def __init__(self, mock: bool = False) -> None:\n        self._query_expander = QueryExpansion(mock=mock)\n        self._metadata_extractor = SelfQuery(mock=mock)\n        self._reranker = Reranker(mock=mock)\nIn the search() method, we convert the user’s input string into a query object. We then use the \nSelfQuery instance to extract the author_id and author_full_name from the query:\n    def search(\n        self,\n        query: str,\n        k: int = 3,\n        expand_to_n_queries: int = 3,\n    ) -> list:\n        query_model = Query.from_str(query)\n        query_model = self._metadata_extractor.generate(query_model)\n        logger.info(\n            \"Successfully extracted the author_id from the query.\",\n            author_id=query_model.author_id,\n        )\nNext, we expand the query to generate multiple semantically similar queries using the \nQueryExpansion instance:\n        n_generated_queries = self._query_expander.generate(query_model, \nexpand_to_n=expand_to_n_queries)\n        logger.info(\n            \"Successfully generated queries for search.\",\n\n\nRAG Inference Pipeline\n342\n            num_queries=len(n_generated_queries),\n        )\nWe then perform the search concurrently for all expanded queries using a thread pool. Each que-\nry is processed by the _search() method, which we’ll explore shortly. The results are flattened, \ndeduplicated, and collected into a single list:\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            search_tasks = [executor.submit(self._search, _query_model, k) \nfor _query_model in n_generated_queries]\n            n_k_documents = [task.result() for task in concurrent.futures.\nas_completed(search_tasks)]\n            n_k_documents = utils.misc.flatten(n_k_documents)\n            n_k_documents = list(set(n_k_documents))\n        logger.info(\"All documents retrieved successfully.\", num_\ndocuments=len(n_k_documents))\nAfter retrieving the documents, we rerank them based on their relevance to the original query \nand keep only the top k documents:\n        if len(n_k_documents) > 0:\n            k_documents = self.rerank(query, chunks=n_k_documents, keep_\ntop_k=k)\n        else:\n            k_documents = []\n        return k_documents\nThe _search() method performs the filtered vector search across different data categories like \nposts, articles, and repositories. It uses the EmbeddingDispatcher to convert the query into an \nEmbeddedQuery, which includes the query’s embedding vector and any extracted metadata:\n    def _search(self, query: Query, k: int = 3) -> list[EmbeddedChunk]:\n        assert k >= 3, \"k should be >= 3\"\n        def _search_data_category(\n            data_category_odm: type[EmbeddedChunk], embedded_query: \nEmbeddedQuery\n        ) -> list[EmbeddedChunk]:\n\n\nChapter 9\n343\n            if embedded_query.author_id:\n                query_filter = Filter(\n                    must=[\n                        FieldCondition(\n                            key=\"author_id\",\n                            match=MatchValue(\n                                value=str(embedded_query.author_id),\n                            ),\n                        )\n                    ]\n                )\n            else:\n                query_filter = None\n            return data_category_odm.search(\n                query_vector=embedded_query.embedding,\n                limit=k // 3,\n                query_filter=query_filter,\n            )\n        embedded_query: EmbeddedQuery = EmbeddingDispatcher.\ndispatch(query)\nWe used the same EmbeddingDispatcher to embed the query as in the RAG feature pipeline to \nembed the document chunks stored in the vector DB. Using the same class ensures we use the \nsame embedding model at ingestion and query time, which is critical for the retrieval step.\nWe search each data category separately by leveraging the local _search_data_category() func-\ntion. Within the _search_data_category() function, we apply the filters extracted from the \nembedded_query object. For instance, if an author_id is present, we use it to filter the search results \nonly to include documents from that author. The results from all categories are then combined:\n        post_chunks = _search_data_category(EmbeddedPostChunk, embedded_\nquery)\n        articles_chunks = _search_data_category(EmbeddedArticleChunk, \nembedded_query)\n        repositories_chunks = _search_data_\ncategory(EmbeddedRepositoryChunk, embedded_query)\n\n\nRAG Inference Pipeline\n344\n        retrieved_chunks = post_chunks + articles_chunks + repositories_\nchunks\n        return retrieved_chunks\nFinally, the rerank() method takes the original query and the list of retrieved documents to \nreorder them based on relevance:\n    def rerank(self, query: str | Query, chunks: list[EmbeddedChunk], \nkeep_top_k: int) -> list[EmbeddedChunk]:\n        if isinstance(query, str):\n            query = Query.from_str(query)\n        reranked_documents = self._reranker.generate(query=query, \nchunks=chunks, keep_top_k=keep_top_k)\n        logger.info(\"Documents reranked successfully.\", num_\ndocuments=len(reranked_documents))\n        return reranked_documents\nLeveraging the ContextRetriever class, we can retrieve context from any query with only a few \nlines of code. For example, let’s take a look at the following code snippet, where we call the entire \nadvanced RAG architecture with a simple call to the search() method:\nfrom loguru import logger\nfrom llm_engineering.application.rag.retriever import ContextRetriever\nquery = \"\"\"\n        My name is Paul Iusztin.\n       \n        Could you draft a LinkedIn post discussing RAG systems?\n        I'm particularly interested in:\n            - how RAG works\n            - how it is integrated with vector DBs and large language \nmodels (LLMs).\n        \"\"\"\n",
      "page_number": 357
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 367-374)",
      "start_page": 367,
      "end_page": 374,
      "detection_method": "topic_boundary",
      "content": "Chapter 9\n345\nretriever = ContextRetriever(mock=False)\ndocuments = retriever.search(query, k=3)\nlogger.info(\"Retrieved documents:\")\nfor rank, document in enumerate(documents):\n    logger.info(f\"{rank + 1}: {document}\")\nCalling the code from above using the following CLI command: poetry poe call-rag-retrieval-\nmodule. This outputs the following:\n2024-09-18 19:01:50.588 | INFO - Retrieved documents:\n2024-09-18 19:01:50.588 | INFO - 1: id=UUID('541d6c22-d15a-4e6a-924a-\n68b7b1e0a330') content='4 Advanced RAG Algorithms You Must Know by \nPaul Iusztin Implement 4 advanced RAG retrieval techniques to optimize \nyour vector DB searches. Integrate the RAG retrieval module into a \nproduction LLM system…\" platform='decodingml.substack.com' document_\nid=UUID('32648f33-87e6-435c-b2d7-861a03e72392') author_id=UUID('900fec95-\nd621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' \nmetadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', \n'embedding_size': 384, 'max_input_length': 256} link='https://decodingml.\nsubstack.com/p/the-4-advanced-rag-algorithms-you?r=1ttoeh'\n2024-09-18 19:01:50.588 | INFO - 2: id=UUID('5ce78438-1314-4874-8a5a-\n04f5fcf0cb21') content='Overview of advanced RAG optimization techniquesA \nproduction RAG system is split into 3 main components ingestion clean, \nchunk, embed, and load your data to a vector DBretrieval query your vector \nDB for …\" platform='medium' document_id=UUID('bd9021c9-a693-46da-97e7-\n0d06760ee6bf') author_id=UUID('900fec95-d621-4315-84c6-52e5229e0b96') \nauthor_full_name='Paul Iusztin' metadata={'embedding_model_id': 'sentence-\ntransformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': \n256} link='https://medium.com/decodingml/the-4-advanced-rag-algorithms-\nyou-must-know-to-implement-5d0c7f1199d2'\n2024-09-18 19:02:45.729 | INFO  - 3: id=UUID('0405a5da-4686-428a-91ca-\n446b8e0446ff') content='Every Medium article will be its own lesson \nAn End to End Framework for Production Ready LLM Systems by Building \nYour LLM TwinThe Importance of Data Pipelines in the Era of Generative \nAIChange Data Capture Enabling Event Driven …\" platform='medium' document_\nid=UUID('bd9021c9-a693-46da-97e7-0d06760ee6bf') author_id=UUID('900fec95-\nd621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' \nmetadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', \n'embedding_size': 384, 'max_input_length': 256} link='https://medium.\n\n\nRAG Inference Pipeline\n346\ncom/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-\n5d0c7f1199d2'\nAs you can observe in the output above, along with the retrieved content, we have access to all \nkinds of metadata, such as the embedding model used for retrieval or the link from which the \nchunk was taken. These can quickly be added to a list of references when generating the result \nfor the user, increasing trust in the final results.\nNow that we understand how the retrieval module works, let’s take a final step and examine the \nend-to-end RAG inference pipeline.\nBringing everything together into the RAG inference \npipeline\nTo fully implement the RAG flow, we still have to build the prompt using the context from the \nretrieval model and call the LLM to generate the answer. This section will discuss these two steps \nand wrap everything together into a single rag() function. The functions from this section can \nbe accessed on GitHub at https://github.com/PacktPublishing/LLM-Engineers-Handbook/\nblob/main/llm_engineering/infrastructure/inference_pipeline_api.py.\nLet’s start by looking at the call_llm_service()function, responsible for interfacing with the \nLLM service. It takes in a user’s query and an optional context, sets up the language model end-\npoint, executes the inference, and returns the generated answer. The context is optional; you can \ncall the LLM without it, as you would when interacting with any other LLM:\ndef call_llm_service(query: str, context: str | None) -> str:\n    llm = LLMInferenceSagemakerEndpoint(\n        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_\ncomponent_name=None\n    )\n    answer = InferenceExecutor(llm, query, context).execute()\n    return answer\nThis function makes an HTTP request to our fine-tuned LLM Twin model, which is hosted as \nan AWS SageMaker inference endpoint. We will explore all the SageMaker details in the next \nchapter, where we will dig into the LLMInferenceSagemakerEndpoint and InferenceExecutor\nclasses. For now, what is essential to know is that we use this function to call our fine-tuned LLM. \nStill, we must highlight how the query and context, passed to the InferenceExecutor class, are \ntransformed into the final prompt. We do that using a simple prompt template that is customized \nusing the user query and retrieved context:\n\n\nChapter 9\n347\nprompt = f\"\"\"\nYou are a content creator. Write what the user asked you to while using \nthe provided context as the primary source of information for the content.\nUser query: {query}\nContext: {context}\n          \"\"\"\nMoving on to the rag() function, this is where the RAG logic comes together. It handles retriev-\ning relevant documents based on the query, mapping the documents to the context that will be \ninjected into the prompt, and obtaining the final answer from the LLM:\ndef rag(query: str) -> str:\n    retriever = ContextRetriever(mock=False)\n    documents = retriever.search(query, k=3)\n    context = EmbeddedChunk.to_context(documents)\n    answer = call_llm_service(query, context)\n    return answer\nAs we modularized all the RAG steps into independent classes, we reduced the high-level rag()\nfunction to five lines of code (encapsulating all the complexities of the system) similar to what we \nsee in tools such as LangChain, LlamaIndex, or Haystack. Instead of their high-level implementa-\ntion, we learned how to build an advanced RAG service from scratch. Also, by clearly separating \nthe responsibility of each class, we can use them like LEGOs. Thus, you can quickly call the LLM \nindependently without context or use the retrieval module as a query engine on top of your vector \nDB. In the next chapter, we will see the rag() function in action after we deploy our fine-tuned \nLLM to an AWS SageMaker inference endpoint.\nBefore ending this chapter, we want to discuss potential improvements you could add to the RAG \ninference pipeline. As we are building a chatbot, the first one is to add a conversation memory that \nstores all the user prompts and generated answers in memory. Thus, when interacting with the \nchatbot, it will be aware of the whole conversation, not only the latest prompt. When prompting \nthe LLM, along with the new user input and context, we also pass the conversation history from \nthe memory. As the conversation history can get long, to avoid exceeding the context window or \nhigher costs, you have to implement a way to reduce the size of your memory. As illustrated in \nFigure 9.4, the simplest one is to keep only the latest K items from your chat history. Unfortunately, \nusing this strategy, the LLM will never be aware of the whole conversation. \n\n\nRAG Inference Pipeline\n348\nTherefore, another way to add the chat history to your prompt is to keep a summary of the conver-\nsation along with the latest K replies. There are multiple ways to compute this summary, which \nmight defeat the purpose of this book if we get into them all, but the simplest way is to always \nupdate the summary on every user prompt and generate an answer.\nFigure 9.4: Routing and memory examples\nAs for each search, we send three queries to the vector DB, one for each data category. Thus, the \nsecond improvement is to add a router between the query and the search. The router will be a \nmulti-category classifier that predicts the data categories we must retrieve for that specific query. \nHence, instead of making three requests for every search, we can often reduce it to one or two. For \nexample, if the user wants to write a theoretical paragraph about RAG for an article, then most \nprobably, it’s valuable to query only the article’s collection. In this case, the router will predict \nthe article class, which we can use to decide what collection we must query. \n\n\nChapter 9\n349\nAnother example would be if we want to illustrate a piece of code that shows how to build a RAG \npipeline. In this case, the router would have to predict the article and repository data category, \nas we need to look up examples in both collections for an exhaustive context.\nUsually, the router strategy decides what model to call based on a user’s input, such as whether \nto use GPT-4 or a self-hosted Llama 3.1 model for that specific query. However,  in our particular \nuse case, we can adapt the router algorithm to optimize the retrieval step.\nWe can further optimize the retrieval using a hybrid search algorithm that combines the vector \nsearch (based on embeddings) with a keyword search algorithm, such as BM25. Search algorithms \nused BM25 (or similar methods) to find similar items in a DB before vector search algorithms \nbecame popular. By merging the methods, hybrid search retrieves results that match the exact \nterms, such as RAG, LLM, or SageMaker, and the query semantics, increasing the accuracy and \nrelevance of your retrieved results. Fundamentally, the hybrid search algorithms follow the next \nmechanics:\n1.\nParallel processing: The search query is processed simultaneously through both the \nvector search and BM25 algorithms. Each algorithm retrieves a set of relevant documents \nbased on its criteria.\n2.\nScore normalization: The results from both searches are assigned relevance scores, which \nare then normalized to ensure comparability. This step is crucial because vector search \nand BM25 scoring mechanisms work at different scales. Thus, they can’t be compared or \nmerged without normalization.\n3.\nResult merging: The normalized scores are combined, often through a weighted sum, to \nproduce a final ranking of documents. Adjusting the weights allows for fine-tuning the \nemphasis on the semantic or keyword search algorithm.\nTo conclude, by combining the semantic and exact keyword search algorithms, you can improve \nthe accuracy of your retrieval step. Vector search helps recognize synonyms or related concepts, \nensuring that relevant information isn’t overlooked due to vocabulary differences. Keyword search \nensures that documents containing critical keywords are emphasized appropriately, particularly \nin technical fields with specific terminology.\nOne last improvement we can make to our RAG system is to use multi-index vector structures \ninstead of indexing based only on the content’s embedding. Let’s detail how multi-indexing \nworks. Instead of using the embeddings of a single field to do the vector search for a particular \ncollection, it combines multiple fields. \n\n\nRAG Inference Pipeline\n350\nFor example, in our LLM Twin use case, we used only the content field of our articles, posts, or \nrepositories to query the vector DB. When using a multi-index strategy, along with the content \nfield, we could index the embeddings of the platform where the content was posted or when the \ncontent was published. This could impact the final accuracy of your retrieval as different platforms \nhave different types of content, or more recent content is usually more relevant. Frameworks \nsuch as Superlinked make multi-indexing easy. For example, in the code snippet below, using \nSuperlinked, we defined a multi-index on the content and platform for our article collection in \njust a few lines of code:\nfrom superlinked.framework.common.schema.id_schema_object import IdField\nfrom superlinked.framework.common.schema.schema import schema\nfrom superlinked.framework.common.schema.schema_object import String\n… # Other Superlinked imports. \n@schema\nclass ArticleSchema:\n    id: IdField\n    platform: String\n    content: String\narticle = ArticleSchema()\narticles_space_content = TextSimilaritySpace(\n    text=chunk(article.content, chunk_size=500, chunk_overlap=50),\n    model=settings.EMBEDDING_MODEL_ID,\n)\narticles_space_plaform = CategoricalSimilaritySpace(\n    category_input=article.platform,\n    categories=[\"medium\", \"substack\", \"wordpress\"],\n    negative_filter=-5.0,\n)\narticle_index = Index(\n    [articles_space_content, articles_space_plaform],\n    fields=[article.author_id],\n)\n\n\nChapter 9\n351\nSuperlinked is a powerful Python tool for any use case that includes vector computing, such as RAG, \nrecommender systems, and semantic search. It offers an ecosystem where you can quickly ingest \ndata into a vector DB, write complex queries on top of it, and deploy the service as a RESTful API.\nThe world of LLMs and RAG is experimental, similar to any other AI domain. Thus, when build-\ning real-world products, it’s important to quickly build an end-to-end solution that works but is \nnot necessarily the best. Then, you can reiterate with various experiments until you completely \noptimize it for your use case. This is standard practice in the industry and lets you iterate fast \nwhile providing value to the business and gathering user feedback as quickly as possible in the \nproduct’s lifecycle.\nSummary\nThis chapter taught us how to build an advanced RAG inference pipeline. We started by looking \ninto the software architecture of the RAG system. Then, we zoomed in on the advanced RAG meth-\nods we used within the retrieval module, such as query expansion, self-querying, filtered vector \nsearch, and reranking. Afterward, we saw how to write a modular ContextRetriever class that \nglues all the advanced RAG components under a single interface, making searching for relevant \ndocuments a breeze. Ultimately, we looked into how to connect all the missing dots, such as the \nretrieval, the prompt augmentation, and the LLM call, under a single RAG function that will serve \nas our RAG inference pipeline.\nAs highlighted a few times in this chapter, we couldn’t test our fine-tuned LLM because we haven’t \ndeployed it yet to AWS SageMaker as an inference endpoint. Thus, in the next chapter, we will \nlearn how to deploy the LLM to AWS SageMaker, write an inference interface to call the endpoint, \nand implement a FastAPI web server to serve as our business layer.\nReferences\n• \nA real-time retrieval system for social media data | VectorHub by SuperLinked. (n.d.). https://\nsuperlinked.com/vectorhub/articles/real-time-retrieval-system-social-media-\ndata\n• \nBuilding a Router from Scratch - LlamaIndex. (n.d.). https://docs.llamaindex.ai/en/\nstable/examples/low_level/router/\n• \nHow to add memory to chatbots | LangChain. (n.d.). https://python.langchain.com/docs/\nhow_to/chatbots_memory/#summary-memory\n\n\nRAG Inference Pipeline\n352\n• \nHow to do “self-querying” retrieval | LangChain. (n.d.). https://python.langchain.com/\ndocs/how_to/self_query/\n• \nHow to route between sub-chains | LangChain. (n.d.). https://python.langchain.com/\ndocs/how_to/routing/#routing-by-semantic-similarity\n• \nHow to use the MultiQueryRetriever | LangChain. (n.d.). https://python.langchain.com/\ndocs/how_to/MultiQueryRetriever/\n• \nHybrid Search explained. (2023, January 3). Weaviate. https://weaviate.io/blog/hybrid-\nsearch-explained\n•\nIusztin, P. (2024, August 20). 4 Advanced RAG Algorithms You Must Know | Decoding \nML. Medium. https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-\nmust-know-to-implement-5d0c7f1199d2\n•\nMonigatti, L. (2024, February 19). Advanced Retrieval-Augmented Generation: From Theory \nto LlamaIndex Implementation. Medium. https://towardsdatascience.com/advanced-\nretrieval-augmented-generation-from-theory-to-llamaindex-implementation-\n4de1464a9930\n• \nMulti-attribute search with vector embeddings | VectorHub by Superlinked. (n.d.). https://\nsuperlinked.com/vectorhub/articles/multi-attribute-semantic-search\n•\nOptimizing RAG with Hybrid Search & Reranking | VectorHub by Superlinked. (n.d.). https://\nsuperlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-\nreranking\n•\nRefactoring.Guru. (2024, January 1). Singleton. https://refactoring.guru/design-\npatterns/singleton\n•\nStoll, M. (2024, September 7). Visualize your RAG Data—Evaluate your Retrieval-Aug-\nmented Generation System with Ragas. Medium. https://towardsdatascience.com/\nvisualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-\nwith-ragas-fc2486308557\n•\nUsing LLM’s for retrieval and reranking—LlamaIndex, data framework for LLM applications. \n(n.d.). https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-\n23cf2d3a14b6\n",
      "page_number": 367
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 375-383)",
      "start_page": 375,
      "end_page": 383,
      "detection_method": "topic_boundary",
      "content": "Chapter 9\n353\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n\n\n10\nInference Pipeline Deployment\nDeploying the inference pipeline for the large language model (LLM) Twin application is a critical \nstage in the machine learning (ML) application life cycle. It’s where the most value is added to \nyour business, making your models accessible to your end users. However, successfully deploying \nAI models can be challenging, as the models require expensive computing power and access to \nup-to-date features to run the inference. To overcome these constraints, it’s crucial to careful-\nly design your deployment strategy. This ensures that it meets the application’s requirements, \nsuch as latency, throughput, and costs. As we work with LLMs, we must consider the inference \noptimization techniques presented in Chapter 8, such as model quantization. Also, to automate \nthe deployment processes, we must leverage MLOps best practices, such as model registries that \nversion and share our models across our infrastructure.\nTo understand how to design the deployment architecture of the LLM Twin, we will first look at \nthree deployment types we can choose from: online real-time inference, asynchronous inference, \nand offline batch transform. Also, to better understand which option to choose for our LLM Twin \nuse case, we will quickly walk you through a set of critical criteria we must consider before making \nan architectural decision, such as latency, throughput, data, and infrastructure. Also, we’ll weigh \nthe pros and cons of monolithic and microservices architecture in model serving, a decision that \ncan significantly influence the scalability and maintainability of your service.Once we’ve grasped \nthe various design choices available, we’ll focus on understanding the deployment strategy for \nthe LLM Twin’s inference pipeline. Subsequently, we will walk you through an end-to-end tu-\ntorial on deploying the LLM Twin service, including deploying our custom fine-tuned LLM to \nAWS SageMaker endpoints and implementing a FastAPI server as the central entry point for our \nusers. We will then wrap up this chapter with a short discussion on autoscaling strategies and \nhow to use them on SageMaker.\n\n\nInference Pipeline Deployment\n356\nHence, in this chapter, we will cover the following topics:\n•\nCriteria for choosing deployment types\n•\nUnderstanding inference deployment types\n•\nMonolithic versus microservices architecture in model serving\n•\nExploring the LLM Twin’s inference pipeline deployment strategy\n•\nDeploying the LLM Twin service\n•\nAutoscaling capabilities to handle spikes in usage\nCriteria for choosing deployment types\nWhen it comes to deploying ML models, the first step is to understand the four requirements \npresent in every ML application: throughput, latency, data, and infrastructure.\nUnderstanding them and their interaction is essential. When designing the deployment archi-\ntecture for your models, there is always a trade-off between the four that will directly impact the \nuser’s experience. For example, should your model deployment be optimized for low latency or \nhigh throughput?\nThroughput and latency\nThroughput refers to the number of inference requests a system can process in a given period. \nIt is typically measured in requests per second (RPS). Throughput is crucial when deploying \nML models when you expect to process many requests. It ensures the system can handle many \nrequests efficiently without becoming a bottleneck.\nHigh throughput often requires scalable and robust infrastructure, such as machines or clusters \nwith multiple high-end GPUs.Latency is the time it takes for a system to process a single inference \nrequest from when it is received until the result is returned. Latency is critical in real-time appli-\ncations where quick response times are essential, such as in live user interactions, fraud detection, \nor any system requiring immediate feedback. For example, the average latency of OpenAI’s API \nis the average response time from when a user sends a request, and the service provides a result \nthat is accessible within your application.\nThe latency is the sum of the network I/O, serialization and deserialization, and the LLM’s infer-\nence time. Meanwhile, the throughput is the average number of requests the API processes and \nserves a second.\n\n\nChapter 10\n357\nLow-latency systems require optimized and often more costly infrastructure, such as faster pro-\ncessors, lower network latency, and possibly edge computing to reduce the distance data needs \nto travel.\nA lower latency translates to higher throughput when the service processes multiple queries in \nparallel successfully. For example, if the service takes 100 ms to process requests, this translates to \na throughput of 10 requests per second. If the latency reaches 10 ms per request, the throughput \nrises to 100 requests per second.\nHowever, to complicate things, most ML applications adopt a batching strategy to simultaneously \npass multiple data samples to the model. In this case, a lower latency can translate into lower \nthroughput; in other words, a higher latency maps to a higher throughput. For example, if you \nprocess 20 batched requests in 100 ms, the latency is 100 ms, while the throughput is 200 requests \nper second. If you process 60 requests in 200 ms, the latency is 200 ms, while the throughput \nrises to 300 requests per second. Thus, even when batching requests at serving time, it’s essential \nto consider the minimum latency accepted for a good user experience.\nData\nAs we know, data is everywhere in an ML system. But when talking about model serving, we \nmostly care about the model’s input and output. This includes the format, volume, and complexity \nof the processed data. Data is the foundation of the inference process. The characteristics of the \ndata, such as its size and type, determine how the system needs to be configured and optimized \nfor efficient processing.\nThe type and size of the data directly impact latency and throughput, as more complex or exten-\nsive data can take longer to process. For example, designing a model that takes input structured \ndata and outputs a probability differs entirely from an LLM that takes input text (or even images) \nand outputs an array of characters.\nInfrastructure\nInfrastructure refers to the underlying hardware, software, networking, and system architecture \nthat supports the deployment and operation of the ML models. The infrastructure provides the \nnecessary resources for deploying, scaling, and maintaining ML models. It includes computing \nresources, memory, storage, networking components, and the software stack:\n•\nFor high throughput, the systems require scalable infrastructure to manage large data \nvolumes and high request rates, possibly through parallel processing, distributed systems, \nand high-end GPUs.\n\n\nInference Pipeline Deployment\n358\n•\nInfrastructure must be optimized to reduce processing time to achieve low latency, such \nas using faster CPUs, GPUs, or specialized hardware. While optimizing your system for \nlow latency while batching your requests, you often have to sacrifice high throughput \nin favor of lower latency, which can result in your hardware not being utilized at total \ncapacity. As you process fewer requests per second, it results in idle computing, which \nincreases the overall cost of processing a request. Thus, picking the suitable machine for \nyour requirements is critical in optimizing costs.\nIt is crucial to design infrastructure to meet specific data requirements. This includes selecting \nstorage solutions to handle large datasets and implementing fast retrieval mechanisms to ensure \nefficient data access. For example, we mostly care about optimizing throughput for offline training, \nwhile for online inference, we generally care about latency.\nWith this in mind, before picking a specific deployment type, you should ask yourself questions \nsuch as:\n• \nWhat are the throughput requirements? You should make this decision based on the \nthroughput’s required minimum, average, and maximum statistics.\n• \nHow many requests the system must handle simultaneously? (1, 10, 1,000, 1 million, etc.)\n• \nWhat are the latency requirements? (1 millisecond, 10 milliseconds, 1 second, etc.)\n• \nHow should the system scale? For example, we should look at the CPU workload, number \nof requests, queue size, data size, or a combination of them.\n•\nWhat are the cost requirements?With what data do we work with? For example, do we \nwork with images, text, or tabular data?\n•\nWhat is the size of the data we work with? (100 MB, 1 GB, 10 GB)\n\n\nChapter 10\n359\nDeeply thinking about these questions directly impacts the user experience of your application, \nwhich ultimately makes the difference between a successful product and not. Even if you ship a \nmind-blowing model, if the user needs to wait too long for a response or it often crashes, the user \nwill switch your production to something less accurate that works reliably. For example, Google \nfound in a 2016 study that 53% of visits are abandoned if a mobile site takes longer than three \nseconds to load: https://www.thinkwithgoogle.com/consumer-insights/consumer-trends/\nmobile-site-load-time-statistics/.\nLet’s move on to the three deployment architectures we can leverage to serve our models.\nUnderstanding inference deployment types\nAs illustrated in Figure 10.1, you can choose from three fundamental deployment types when \nserving models:\n•\nOnline real-time inference\n•\nAsynchronous inference\n•\nOffline batch transform\nWhen selecting one design over the other, there is a trade-off between latency, throughput, and \ncosts. You must consider how the data is accessed and the infrastructure you are working with. \nAnother criterion you have to consider is how the user will interact with the model. For example, \nwill the user use it directly, like a chatbot, or will it be hidden within your system, like a classifier \nthat checks if an input (or output) is safe?\nYou have to consider the freshness of the predictions as well. For example, serving your model in \noffline batch mode might be easier to implement if, in your use case, it is OK to consume delayed \npredictions. Otherwise, you have to serve your model in real-time, which is more infrastruc-\nture-demanding. Also, you have to consider your application’s traffic. Ask yourself questions such \nas, “Will the application be constantly used, or will there be spikes in traffic and then flatten out?”\n\n\nInference Pipeline Deployment\n360\nWith that in mind, let’s explore the three major ML deployment types.\nFigure 10.1: The three fundamental architectures of inference deployment types\nOnline real-time inference\nIn real-time inference, we have a simple architecture based on a server that can be accessed \nthrough HTTP requests. The most popular options are to implement a REST API or gRPC server. \nThe REST API is more accessible but slower, using JSON to pass data between the client and server. \n\n\nChapter 10\n361\nThis approach is usually taken when serving models outside your internal network to the broader \npublic. For example, OpenAI’s API implements a REST API protocol.\nOn the other hand, implementing a gRPC makes your ML server faster, though it may reduce \nits flexibility and general applicability. You have to implement protobuf schemas in your client \napplication, which are more tedious to work with than JSON structures. The benefit, however, \nis that protobuf objects can be compiled into bites, making the network transfers much faster. \nThus, this protocol is often adopted for internal services within the same ML system.\nUsing the real-time inference approach, the client sends an HTTP request to the ML service, which \nimmediately processes the request and returns the result in the same response. This synchronous \ninteraction means the client waits for the result before moving on.\nTo make this work efficiently, the infrastructure must support low-latency, highly responsive ML \nservices, often deployed on fast, scalable servers. Load balancing is crucial to evenly distribute \nincoming traffic evenly, while autoscaling ensures the system can handle varying loads. High \navailability is also essential to keeping the service operational at all times.\nFor example, this architecture is often present when interacting with LLMs, as when sending a \nrequest to a chatbot or API (powered by LLMs), you expend the predictions right ahead. LLM \nservices, such as ChatGPT or Claude, often use WebSockets to stream each token individually to \nthe end user, making the interaction more responsive. Other famous examples are AI services such \nas embedding or reranking models used for retrieval-augmented generation (RAG) or online \nrecommendation engines in platforms like TikTok.\nThe simplicity of real-time inference, with its direct client-server interaction, makes it an attrac-\ntive option for applications that require immediate responses, like chatbots or real-time recom-\nmendations. However, this approach can be challenging to scale and may lead to underutilized \nresources during low-traffic periods.\nAsynchronous inference\nIn asynchronous inference, the client sends a request to the ML service, which acknowledges the \nrequest and places it in a queue for processing. Unlike real-time inference, the client doesn’t wait \nfor an immediate response. Instead, the ML service processes the request asynchronously. This re-\nquires a robust infrastructure that queues the messages to be processed by the ML service later on.\nWhen the results are ready, you can leverage multiple techniques to send them to the client. For \nexample, depending on the size of the result, you can put it either in a different queue or an object \nstorage dedicated to storing the results. \n\n\nInference Pipeline Deployment\n362\nThe client can either adopt a polling mechanism that checks on a schedule if there are new re-\nsults or adopt a push strategy and implement a notification system to inform the client when \nthe results are ready.\nAsynchronous inference uses resources more efficiently. It doesn’t have to process all the requests \nsimultaneously but can define a maximum number of machines that run in parallel to process \nthe messages. This is possible because the requests are stored in the queue until a machine can \nprocess them. Another huge benefit is that it can handle spikes in requests without any timeouts. \nFor example, let’s assume that on an e-shop site, we usually have 10 requests per second handled \nby two machines. Because of a promotion, many people started to visit the site, and the number \nof requests spiked to 100 requests per second. Instead of scaling the number of virtual machines \n(VMs) by 10, which can add drastic costs, the requests are queued, and the same two VMs can \nprocess them in their rhythm without any failures.\nAnother popular advantage for asynchronous architectures is when the requested job takes sig-\nnificant time to complete. For example, if the job takes over five minutes, you don’t want to block \nthe client waiting for a response.\nWhile asynchronous inference offers significant benefits, it does come with trade-offs. It intro-\nduces higher latency, making it less suitable for time-sensitive applications. Additionally, it adds \ncomplexity to the implementation and infrastructure. Depending on your design choices, this \narchitecture type falls somewhere between online and offline, offering a balance of benefits and \ntrade-offs.\nFor example, this is a robust design where you don’t care too much about the latency of the infer-\nence but want to optimize costs heavily. Thus, it is a popular choice for problems such as extracting \nkeywords from documents, summarizing them using LLMs, or running deep-fake models on top \nof videos. But suppose you carefully design the autoscaling system to process the requests from \nthe queue at decent speeds. In that case, you can leverage this design for other use cases, such as \nonline recommendations for e-commerce. In the end, it sums up how much computing power \nyou are willing to pay to meet the expectations of your application.\nOffline batch transform\nBatch transform is about processing large volumes of data simultaneously, either on a schedule \nor triggered manually. In a batch transform architecture, the ML service pulls data from a storage \nsystem, processes it in a single operation, and then stores the results in storage. The storage sys-\ntem can be implemented as an object storage like AWS S3 or a data warehouse like GCP BigQuery. \n",
      "page_number": 375
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 384-392)",
      "start_page": 384,
      "end_page": 392,
      "detection_method": "topic_boundary",
      "content": "Chapter 10\n363\nUnlike the asynchronous inference architecture, a batch transform design is optimized for high \nthroughput with permissive latency requirements. When real-time predictions are unnecessary, \nthis approach can significantly reduce costs, as processing data in big batches is the most eco-\nnomical method. Moreover, the batch transform architecture is the simplest way to serve a model, \naccelerating development time.\nThe client pulls the results directly from data storage, decoupling its interaction with the ML \nservice. Taking this approach, the client never has to wait for the ML service to process its input, \nbut at the same time, it doesn’t have the flexibility to ask for new results at any time. You can \nsee the data storage, where the results are stored as a large cache, from where the client can take \nwhat is required. If you want to make your application more responsive, the client can be notified \nwhen the processing is complete and can retrieve the results.\nUnfortunately, this approach will always introduce a delay between the time the predictions \nwere computed and consumed. That’s why not all applications can leverage this design choice. \nFor example, if we implement a recommender system for a video streaming application, having a \ndelay of one day for the predicted movies and TV shows might work because you don’t consume \nthese products at a high frequency. But suppose you make a recommender system for a social \nmedia platform. In that case, delaying one day or even one hour is unacceptable, as you constantly \nwant to provide fresh content to the user.\nBatch transform shines in scenarios where high throughput is needed, like data analytics or pe-\nriodic reporting. However, it’s unsuitable for real-time applications due to its high latency and \nrequires careful planning and scheduling to manage large datasets effectively. That’s why it is \nan offline serving method.\nTo conclude, we examined the three most common architectures for serving ML models. We \nstarted with online real-time inference, which serves clients when they request a prediction. \nThen, we looked at the asynchronous inference method, which sits between online and offline. \nUltimately, we presented the offline batch transform, which is used to process large amounts of \ndata and store them in data storage, from where the client later consumes them.\nMonolithic versus microservices architecture in \nmodel serving\nIn the previous section, we saw three different methods of deploying the ML service. The differ-\nences in architecture were mainly based on the interaction between the client and the ML service, \nsuch as the communication protocol, the ML service responsiveness, and prediction freshness. \n\n\nInference Pipeline Deployment\n364\nBut another aspect to consider is the architecture of the ML service itself, which can be imple-\nmented as a monolithic server or as multiple microservices. This will impact how the ML service \nis implemented, maintained, and scaled. Let’s explore the two options.\nFigure 10.2: Monolithic versus microservices architecture in model serving\n\n\nChapter 10\n365\nMonolithic architecture\nThe LLM (or any other ML model) and the associated business logic (preprocessing and post-pro-\ncessing steps) are bundled into a single service in a monolithic architecture. This approach is \nstraightforward to implement at the beginning of a project, as everything is placed within one \ncode base. Simplicity makes maintenance easy when working on small to medium projects, as \nupdates and changes can be made within a unified system.\nOne key challenge of a monolithic architecture is the difficulty of scaling components independent-\nly. The LLM typically requires GPU power, while the rest of the business logic is CPU and I/O-bound. \nAs a result, the infrastructure must be optimized for both GPU and CPU. This can lead to inefficient \nresource use, with the GPU being idle when the business logic is executed and vice versa. Such \ninefficiency can result in additional costs that could be avoided.\nMoreover, this architecture can limit flexibility, as all components must share the same tech \nstack and runtime environment. For example, you might want to run the LLM using Rust or C++ \nor compile it with ONNX or TensorRT while keeping the business logic in Python. Having all the \ncode in one system makes this differentiation difficult. Finally, splitting the work across different \nteams is complex, often leading to bottlenecks and reduced agility.\nMicroservices architecture\nA microservices architecture breaks down the inference pipeline into separate, independent ser-\nvices—typically splitting the LLM service and the business logic into distinct components. These \nservices communicate over a network using protocols such as REST or gRPC.\nAs illustrated in Figure 10.3, the main advantage of this approach is the ability to scale each com-\nponent independently. For instance, since the LLM service might require more GPU resources \nthan the business logic, it can be scaled horizontally without impacting the other components. \nThis optimizes resource usage and reduces costs, as different types of machines (e.g., GPU versus \nCPU) can be used according to each service’s needs.\nFor example, let’s assume that the LLM inference takes longer, so you will need more ML service \nreplicas to meet the demand. But remember that GPU VMs are expensive. By decoupling the two \ncomponents, you will run only what is required on the GPU machine and not block the GPU VM \nwith other computing that can be done on a much cheaper machine. \n\n\nInference Pipeline Deployment\n366\nThus, by decoupling the components, you can scale horizontally as required, with minimal costs, \nproviding a cost-effective solution to your system’s needs.\nFigure 10.3: Scaling microservices independently based on compute requirements\nAdditionally, each microservice can adopt the most suitable technology stack, allowing teams to \ninnovate and optimize independently.\nHowever, microservices introduce complexity in deployment and maintenance. Each service \nmust be deployed, monitored, and maintained separately, which can be more challenging than \nmanaging a monolithic system. \n\n\nChapter 10\n367\nThe increased network communication between services can also introduce latency and potential \npoints of failure, necessitating robust monitoring and resilience mechanisms.\nNote that the proposed design for decoupling the ML model and business logic into two services \ncan be extended if necessary. For example, you can have one service for preprocessing the data, \none for the model, and another for post-processing the data. Depending on the four pillars (la-\ntency, throughput, data, and infrastructure), you can get creative and design the most optimal \narchitecture for your application needs.\nChoosing between monolithic and microservices \narchitectures\nThe choice between monolithic and microservices architectures for serving ML models largely \ndepends on the application’s specific needs. A monolithic approach might be ideal for smaller \nteams or more straightforward applications where ease of development and maintenance is a \npriority. It’s also a good starting point for projects without frequent scaling requirements. Also, \nif the ML models are smaller, don’t require a GPU, or don’t require smaller and cheaper GPUs, \nthe trade-off between reducing costs and complicating your infrastructure is worth considering.\nOn the other hand, microservices, with their adaptability and scalability, are well suited for larger, \nmore complex systems where different components have varying scaling needs or require distinct \ntech stacks. This architecture is particularly advantageous when scaling specific system parts, such \nas GPU-intensive LLM services. As LLMs require powerful machines with GPUs, such as Nvidia \nA100, V100, or A10g, which are incredibly costly, microservices offer the flexibility to optimize \nthe system for keeping these machines busy all the time or quickly scaling down when the GPU \nis idle. However, this flexibility comes at the cost of increased complexity in both development \nand operations.\nA common strategy is to start with a monolithic design and further decouple it into multiple \nservices as the project grows. However, to successfully do so without making the transition too \ncomplex and costly, you must design the monolith application with this in mind. For instance, \neven if all the code runs on a single machine, you can completely decouple the modules of the \napplication at the software level. This makes it easier to move these modules to different micro-\nservices when the time comes. When working with Python, for example, you can implement the \nML and business logic into two different Python modules that don’t interact with each other. Then, \nyou can glue these two modules at a higher level, such as through a service class, or directly into \nthe framework you use to expose your application over the internet, such as FastAPI. \n\n\nInference Pipeline Deployment\n368\nAnother option is to write the ML and business logic as two different Python packages that you \nglue together in the same ways as before. This is better because it completely enforces a separation \nbetween the two but adds extra complexity at development time. The main idea, therefore, is that \nif you start with a monolith and down the line you want to move to a microservices architecture, \nit’s essential to design your software with modularity in mind. Otherwise, if the logic is mixed, \nyou will probably have to rewrite everything from scratch, adding tons of development time, \nwhich translates into wasted resources.\nIn summary, monolithic architectures offer simplicity and ease of maintenance but at the cost of \nflexibility and scalability. At the same time, microservices provide the agility to scale and innovate \nbut require more sophisticated management and operational practices.\nExploring the LLM Twin’s inference pipeline \ndeployment strategy\nNow that we’ve understood all the design choices available for implementing the deployment \nstrategy of the LLM Twin’s inference pipeline, let’s explore the concrete decisions we made to \nactualize it.\nOur primary objective is to develop a chatbot that facilitates content creation. To achieve this, \nwe will process requests sequentially, with a strong emphasis on low latency. This necessitates \nthe selection of an online real-time inference deployment architecture.\nOn the monolith versus microservice aspect, we will split the ML service between a REST API \nserver containing the business logic and an LLM microservice optimized for running the given \nLLM. As the LLM requires a powerful machine to run the inference, and we can further optimize \nit with various engines to speed up the latency and memory usage, it makes the most sense to go \nwith the microservice architecture. By doing so, we can quickly adapt the infrastructure based on \nvarious LLM sizes. For example, if we run an 8B parameter model, the model can run on a single \nmachine with a Nivida A10G GPU after quantization. But if we want to run a 30B model, we can \nupgrade to an Nvidia A100 GPU. Doing so allows us to upgrade only the LLM microservice while \nkeeping the REST API untouched.\nAs illustrated in Figure 10.4, most business logic is centered around RAG in our particular use case. \nThus, we will perform RAG’s retrieval and augmentation parts within the business microservice. \nIt will also include all the advanced RAG techniques presented in the previous chapter to optimize \nthe pre-retrieval, retrieval, and post-retrieval steps. \n\n\nChapter 10\n369\nThe LLM microservice is strictly optimized for the RAG generation component. Ultimately, the \nbusiness layer will send the prompt trace consisting of the user query, prompt, answer, and other \nintermediary steps to the prompt monitoring pipeline, which we will detail in Chapter 11.\nIn summary, our approach involves implementing an online real-time ML service using a micro-\nservice architecture, which effectively splits the LLM and business logic into two distinct services.\nFigure 10.4: Microservice deployment architecture of the LLM Twin’s inference pipeline\n\n\nInference Pipeline Deployment\n370\nLet’s review the interface of the inference pipeline, which is defined by the feature/training/\ninference (FTI) architecture. For the pipeline to run, it needs two things:\n• \nReal-time features used for RAG, generated by the feature pipeline, which is queried from \nour online feature store, more concretely from the Qdrant vector database (DB)\n• \nA fine-tuned LLM generated by the training pipeline, which is pulled from our model \nregistry\nWith that in mind, the flow of the ML service looks as follows, as illustrated in Figure 10.4:\n1. \nA user sends a query through an HTTP request.\n2. \nThe user’s input retrieves the proper context by leveraging the advanced RAG retrieval \nmodule implemented in Chapter 4.\n3.\nThe user’s input and retrieved context are packed into the final prompt using a dedicated \nprompt template.\n4.\nThe prompt is sent to the LLM microservice through an HTTP request.\n5.\nThe business microservices wait for the generated answer.\n6.\nAfter the answer is generated, it is sent to the prompt monitoring pipeline along with the \nuser’s input and other vital information to monitor.\n7.\nUltimately, the generated answer is sent back to the user.\nNow, let’s explore what tech stack we used to implement the architecture presented in Figure \n10.4. As we know, we use Qdrant for the vector DB. We will leverage Hugging Face for the model \nregistry. By doing so, we can publicly share our model with everyone who is testing the code from \nthis book. Thus, you can easily use the model we provided if you don’t want to run the training \npipeline, which can cost up to 100 dollars. As you can see, shareability and accessibility are some \nof the most beautiful aspects of storing your model in a model registry.\nWe will implement the business microservice in FastAPI because it’s popular, easy to use, and fast. \nThe LLM microservice will be deployed on AWS SageMaker, where we will leverage SageMaker’s \nintegration with Hugging Face’s Deep Learning Containers (DLCs) to deploy the model. We will \ndiscuss Hugging Face’s DLCs in the next section, but intuitively, it is an inference engine used to \noptimize LLMs at serving time. The prompt monitoring pipeline is implemented using Comet, \nbut we will look over that module only in Chapter 11.\n\n\nChapter 10\n371\nThe SageMaker Inference deployment is composed of the following components that we will \nshow you how to implement:\n•\nSageMaker endpoint: An endpoint is a scalable and secure API that SageMaker hosts to \nenable real-time predictions from deployed models. It’s essentially the interface through \nwhich applications interact with your model. Once deployed, an application can make \nHTTP requests to the endpoint to receive real-time predictions.\n•\nSageMaker model: In SageMaker, a model is an artifact that results from training an al-\ngorithm. It contains the information required to make predictions, including the weights \nand computation logic. You can create multiple models and use them in different config-\nurations or for various predictions.\n•\nSageMaker configuration: This configuration specifies the hardware and software set \nup to host the model. It defines the resources required for the endpoint, such as the type \nand number of ML compute instances. Endpoint configurations are used when creating \nor updating an endpoint. They allow for flexibility in the deployment and scalability of \nthe hosted models.\n•\nSageMaker Inference component: This is the last piece of the puzzle that connects the \nmodel and configuration to an endpoint. You can deploy multiple models to an endpoint, \neach with its resource configuration. Once deployed, models are easily accessible via the \nInvokeEndpoint API in Python.\nTogether, these components create a robust infrastructure for deploying and managing ML models \nin SageMaker, enabling scalable, secure, and efficient real-time predictions.\nOther popular cloud platforms offer the exact solutions. For example, you have Azure OpenAI \ninstead of Bedrock and Azure ML instead of SageMaker on Azure. The list of ML deployment \ntools, such as Hopsworks, Modal, Vertex AI, Seldon, BentoML, and many more, is endless and \nwill probably change. What is essential though is to understand your use case requirements and \nfind a tool that fits your needs.\nThe training versus the inference pipeline\nUnderstanding the nuances between the training and inference pipelines is crucial before we \ndeploy the inference pipeline. While it might seem straightforward that the training pipeline is \nfor training and the inference pipeline is for inference, there are significant differences that we \nneed to grasp to comprehend the technical aspects of our discussion fully.\n",
      "page_number": 384
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 393-400)",
      "start_page": 393,
      "end_page": 400,
      "detection_method": "topic_boundary",
      "content": "Inference Pipeline Deployment\n372\nOne key difference lies in how data is handled and accessed within each pipeline. During train-\ning, data is typically accessed from offline storage in batch mode, optimized for throughput and \nensuring data lineage. For example, our LLM Twin architecture uses ZenML artifacts to access, \nversion, and track data fed to the training loop in batches. In contrast, the inference pipeline \nrequires an online DB optimized for low latency. We will leverage the Qdrant vector DB to grab \nthe necessary context for RAG. In this context, the focus shifts from data lineage and versioning \nto quick data access, ensuring a seamless user experience. Additionally, the outputs of these pipe-\nlines also differ significantly. The training pipeline outputs trained model weights stored in the \nmodel registry. Meanwhile, the inference pipeline outputs predictions served directly to the user.\nAlso, the infrastructure required for each pipeline is different. The training pipeline demands \nmore powerful machines equipped with as many GPUs as possible. This is because training in-\nvolves batching data and holding all the necessary gradients in memory for optimization steps, \nmaking it highly compute-intensive. More computational power and VRAM allow larger batches \n(or throughput), reducing training time and enabling more extensive experimentation. On the \nother hand, the inference pipeline typically requires less computation. Inference often involves \npassing a single sample or smaller batches to the model without the need for optimization steps.\nDespite these differences, there is some overlap between the two pipelines, particularly regarding \npreprocessing and post-processing steps. Applying the same preprocessing and post-processing \nfunctions and hyperparameters during training and inference is crucial. Any discrepancies can \nlead to what is known as training-serving skew, where the model’s performance during inference \ndeviates from its performance during training.\nDeploying the LLM Twin service\nThe last step is implementing the architecture presented in the previous section. More concretely, \nwe will deploy the LLM microservice using AWS SageMaker and the business microservice using \nFastAPI. Within the business microservice, we will glue the RAG logic written in Chapter 9 with \nour fine-tuned LLM Twin, ultimately being able to test out the inference pipeline end to end.\nServing the ML model is one of the most critical steps in any ML application’s life cycle, as users \ncan only interact with our model after this phase is completed. If the serving architecture isn’t \ndesigned correctly or if the infrastructure isn’t working properly, it doesn’t matter that you have \nimplemented a powerful and excellent model. As long as the user cannot appropriately interact \nwith it, it has near zero value from a business point of view. For example, if you have the best code \nassistant on the market, but the latency to use it is too high, or the API calls keep crashing, the \nuser will probably switch to a less performant code assistant that works faster and is more stable.\n\n\nChapter 10\n373\nThus, in this section, we will show you how to:\n• \nDeploy our fined-tuned LLM Twin model to AWS SageMaker\n• \nWrite an inference client to interact with the deployed model\n• \nWrite the business service in FastAPI\n• \nIntegrate our RAG logic with our fine-tuned LLM\n• \nImplement autoscaling rules for the LLM microservice\nImplementing the LLM microservice using AWS SageMaker\nWe aim to deploy the LLM Twin model, stored in Hugging Face’s model registry, to Amazon \nSageMaker as an online real-time inference endpoint. We will leverage Hugging Face’s specialized \ninference container, known as the Hugging Face LLM DLC, to deploy our LLM.\nWhat are Hugging Face’s DLCs?\nDLCs are specialized Docker images that come pre-loaded with essential deep-learning frame-\nworks and libraries, including popular tools like transformers, datasets, and tokenizers from \nHugging Face. These containers are designed to simplify the process of training and deploying \nmodels by eliminating the need for complex environment setup and optimization. The Hugging \nFace Inference DLC, in particular, includes a fully integrated serving stack, significantly simpli-\nfying the deployment process and reducing the technical expertise needed to serve deep learning \nmodels in production.\nWhen it comes to serving models, the DLC is powered by the Text Generation Inference (TGI) \nengine, made by Hugging Face: https://github.com/huggingface/text-generation-inference.\nTGI is an open-source solution for deploying and serving LLMs. It offers high-performance text \ngeneration using tensor parallelism and dynamic batching for the most popular open-source LLMs \navailable on Hugging Face, such as Mistral, Llama, and Falcon. To sum up, the most powerful \nfeatures the DLC image provides are:\n•\nTensor parallelism, thus enhancing the computational efficiency of model inference\n•\nOptimized transformers code for inference, leveraging flash-attention to maximize per-\nformance across the most widely used architectures: https://github.com/Dao-AILab/\nflash-attention\n•\nQuantization with bitsandbytes that reduces the model size while maintaining per-\nformance, making deployments more efficient: https://github.com/bitsandbytes-\nfoundation/bitsandbytes\n\n\nInference Pipeline Deployment\n374\n•\nContinuous batching of incoming requests, thus improving throughput by dynamically \nbatching requests as they arrive\n•\nAccelerated weight loading by utilizing safetensors for faster model initialization, re-\nducing start-up time: https://github.com/huggingface/safetensors\n•\nToken streaming that supports real-time interactions through Server-Sent Events (SSE)\nTo summarize, our LLM Twin model will run inside DLC Docker images, listening to requests, \noptimizing the LLM for inference, and serving the results in real time. The DLC’s Docker images \nwill be hosted on AWS SageMaker under inference endpoints that can be accessed through HTTP \nrequests. With that in mind, let’s move on to the implementation. We will start by deploying the \nLLM and then writing a wrapper class to interact with the SageMaker Inference endpoint.\nConfiguring SageMaker roles\nThe first step is to create the proper AWS Identity and Access Management (IAM) users and \nroles to access and deploy the SageMaker infrastructure. AWS IAM controls who can authenticate \nand what any actor has access to. You can create new users (assigned to people) and new roles \n(assigned to other actors within your infrastructure, such as EC2 VMs) through IAM.\nThe whole deployment process is automated. We will have to run a few CLI commands, but first, \nensure that you have correctly configured the AWS_ACCESS_KEY, AWS_SECRET_KEY, and AWS_REGION\nenvironmental variables in the .env file. At this step, the easiest way is to use the credentials \nattached to an admin role as, in the following steps, we will create a set of narrower IAM roles \nused in the rest of the chapter.\nAfter you configured your .env file, we have to:\n1. \nCreate an IAM user restricted to creating and deleting only the resources we need for \nthe deployment, such as SageMaker itself, Elastic Container Registry (ECR), and S3. To \nmake it, run the following:\npoetry poe create-sagemaker-role\nThis command will generate a JSON file called sagemaker_user_credentials.json that \ncontains a new AWS access and secret key. From now on, we will use these credentials to \ndeploy everything related to SageMaker to ensure we modify only the resources associated \nwith SageMaker. Otherwise, we could accidentally modify other AWS resources using \nan admin account, resulting in additional costs or altering other existing projects. Thus, \nhaving a narrow role only to your use case is good practice. \n\n\nChapter 10\n375\nThe last step is to take the new credentials from the JSON file and update the AWS_ACCESS_\nKEY and AWS_SECRET_KEY variables in your .env file. You can check out the implementa-\ntion at https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/\nllm_engineering/infrastructure/aws/roles/create_sagemaker_role.py.\n2.\nCreate an IAM execution role. We will attach this role to the SageMaker deployment, \nempowering it to access other AWS resources on our behalf. This is standard practice for \ncloud deployments, as instead of authenticating every machine within your credentials, \nyou attach a role that allows them to access only what is necessary from your infrastruc-\nture. In our case, we will provide SageMaker access to AWS S3, CloudWatch, and ECR. To \ncreate the role, run the following:\npoetry poe create-sagemaker-execution-role\nThis command will generate a JSON file called sagemaker_execution_role.json that \ncontains the Amazon Resource Name (ARN) of the newly created role. The ARN is an ID \nattached to any AWS resource to identify it across your cloud infrastructure. Take the ARN \nvalue from the JSON file and update the AWS_ARN_ROLE variable from your .env file with \nit. You can check out the implementation at https://github.com/PacktPublishing/\nLLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/\ncreate_execution_role.py.\nBy setting the IAM user and role in your .env file, we will automatically load them in the settings \nPython object and use them throughout the following steps. Now, let’s move on to the actual \ndeployment.\nDeploying the LLM Twin model to AWS SageMaker\nThe deployment of AWS SageMaker is fully automated through a set of Python classes, which \nwe will cover in this chapter. This section aims to understand how we configure the SageMaker \ninfrastructure directly from Python. Thus, you don’t have to run everything step by step, as in a \nstandard tutorial, but only to understand the code.\nWe can initiate and finalize the entire SageMaker deployment using a simple CLI command: poe \ndeploy-inference-endpoint. This command will initialize all the steps presented in Figure 10.5, \nexcept for creating the SageMaker AWS IAMs we created and configured in the previous step. \nIf you have issues, configure the AWS CLI with the same AWS credentials as in the \n.env file and repeat the process. Official documentation for installing the AWS CLI: \nhttps://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html.\n\n\nInference Pipeline Deployment\n376\nIn this section, we will walk you through the code presented in Figure 10.5 that helps us fully auto-\nmate the deployment process, starting with the create_endpoint() function. Ultimately, we will \ntest the CLI command and check the AWS console to see whether the deployment was successful. \nThe SageMaker deployment code is available at https://github.com/PacktPublishing/LLM-\nEngineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy.\nFigure 10.5: AWS SageMaker deployment steps\n\n\nChapter 10\n377\nWe will take a top-down approach to walk you through the implementation, starting with the \nmain function that deploys the LLM Twin model to AWS SageMaker. In the function below, we \nfirst take the latest version of the Docker DLC image using the get_huggingface_llm_image_uri()\nfunction, which is later passed to the deployment strategy class, along with an instance of the \nresource manager and deployment service:\ndef create_endpoint(endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED):\n    llm_image = get_huggingface_llm_image_uri(\"huggingface\", version=None)\n    resource_manager = ResourceManager()\n    deployment_service = DeploymentService(resource_manager=resource_\nmanager)\n    SagemakerHuggingfaceStrategy(deployment_service).deploy(\n        role_arn=settings.ARN_ROLE,\n        llm_image=llm_image,\n        config=hugging_face_deploy_config,\n        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE,\n        endpoint_config_name=settings.SAGEMAKER_ENDPOINT_CONFIG_INFERENCE,\n        gpu_instance_type=settings.GPU_INSTANCE_TYPE,\n        resources=model_resource_config,\n        endpoint_type=endpoint_type,\n    )\nWe must review the three classes used in the create_endpoint() function to fully understand \nthe deployment process. Let’s start with the ResourceManager class. The class begins with the \ninitialization method, establishing the connection to AWS SageMaker using boto3, the AWS \nSDK for Python, which provides the necessary functions to interact with various AWS services, \nincluding SageMaker.\nclass ResourceManager:\n    def __init__(self) -> None:\n        self.sagemaker_client = boto3.client(\n            \"sagemaker\",\n            region_name=settings.AWS_REGION,\n            aws_access_key_id=settings.AWS_ACCESS_KEY,\n            aws_secret_access_key=settings.AWS_SECRET_KEY,\n        )\n\n\nInference Pipeline Deployment\n378\nNext, we implement the endpoint_config_exists method, checking whether a specific Sage-\nMaker endpoint configuration exists:\n    def endpoint_config_exists(self, endpoint_config_name: str) -> bool:\n        try:\n            self.sagemaker_client.describe_endpoint_\nconfig(EndpointConfigName=endpoint_config_name)\n            logger.info(f\"Endpoint configuration '{endpoint_config_name}' \nexists.\")\n            return True\n        except ClientError:\n            logger.info(f\"Endpoint configuration '{endpoint_config_name}' \ndoes not exist.\")\n            return False\nThe class also includes the endpoint_exists method, which checks the existence of a specific \nSageMaker endpoint:\ndef endpoint_exists(self, endpoint_name: str) -> bool:\n        try:\n            self.sagemaker_client.describe_endpoint(EndpointName=endpoint_\nname)\n            logger.info(f\"Endpoint '{endpoint_name}' exists.\")\n            return True\n        except self.sagemaker_client.exceptions.ResourceNotFoundException:\n            logger.info(f\"Endpoint '{endpoint_name}' does not exist.\")\n            return False\nLet’s move to the DeploymentService. Within the constructor, we set up the sagemaker_client, \nwhich will interface with AWS SageMaker and an instance of the ResourceManager class we \ntalked about earlier:\nclass DeploymentService:\n    def __init__(self, resource_manager):\n        self.sagemaker_client = boto3.client(\n            \"sagemaker\",\n            region_name=settings.AWS_REGION,\n            aws_access_key_id=settings.AWS_ACCESS_KEY,\n            aws_secret_access_key=settings.AWS_SECRET_KEY,\n\n\nChapter 10\n379\n        )\n        self.resource_manager = resource_manager\nThe deploy() method is the heart of the DeploymentService class. This method orchestrates the \nentire process of deploying a model to a SageMaker endpoint. It checks whether the necessary \nconfigurations are already in place and, if not, it triggers the deployment:\ndef deploy(\n    self,\n    role_arn: str,\n    llm_image: str,\n    config: dict,\n    endpoint_name: str,\n    endpoint_config_name: str,\n    gpu_instance_type: str,\n    resources: Optional[dict] = None,\n    endpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n) -> None:\n    try:\n        if self.resource_manager.endpoint_config_exists(endpoint_config_\nname=endpoint_config_name):\n            logger.info(f\"Endpoint configuration {endpoint_config_name} \nexists. Using existing configuration...\")\n        else:\n            logger.info(f\"Endpoint configuration{endpoint_config_name} \ndoes not exist.\")\n        self.prepare_and_deploy_model(\n            role_arn=role_arn,\n            llm_image=llm_image,\n            config=config,\n            endpoint_name=endpoint_name,\n            update_endpoint=False,\n            resources=resources,\n            endpoint_type=endpoint_type,\n            gpu_instance_type=gpu_instance_type,\n        )\n",
      "page_number": 393
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 401-411)",
      "start_page": 401,
      "end_page": 411,
      "detection_method": "topic_boundary",
      "content": "Inference Pipeline Deployment\n380\n        logger.info(f\"Successfully deployed/updated model to endpoint \n{endpoint_name}.\")\n    except Exception as e:\n        logger.error(f\"Failed to deploy model to SageMaker: {e}\")\n        raise\nThe deploy method begins by checking whether the endpoint configuration already exists using \nthe resource_manager. This step is crucial because it avoids unnecessary redeployment if the \nconfiguration is already set up. The deployment itself is handled by calling the prepare_and_\ndeploy_model() method, which is responsible for the actual deployment of the model to the \nspecified SageMaker endpoint.\nThe prepare_and_deploy_model() method is a static method within the DeploymentService\nclass. This method is focused on setting up and deploying the Hugging Face model to SageMaker:\n@staticmethod\ndef prepare_and_deploy_model(\n    role_arn: str,\n    llm_image: str,\n    config: dict,\n    endpoint_name: str,\n    update_endpoint: bool,\n    gpu_instance_type: str,\n    resources: Optional[dict] = None,\n    endpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n) -> None:\n    huggingface_model = HuggingFaceModel(\n        role=role_arn,\n        image_uri=llm_image,\n        env=config,\n        transformers_version=\"4.6\",\n        pytorch_version=\"1.13\",\n        py_version=\"py310\",\n    )\n    huggingface_model.deploy(\n        instance_type=gpu_instance_type,\n        initial_instance_count=1,\n\n\nChapter 10\n381\n        endpoint_name=endpoint_name,\n        update_endpoint=update_endpoint,\n        resources=resources,\n        tags=[{\"Key\": \"task\", \"Value\": \"model_task\"}],\n        endpoint_type=endpoint_type,\n    )\nThis method begins by creating an instance of HuggingFaceModel, a specialized model class from \nSageMaker designed to handle Hugging Face models. The constructor for HuggingFaceModel \ntakes several essential parameters, such as the role ARN (which gives SageMaker the necessary \npermissions), the URI of the LLM DLC Docker image, and the LLM configuration that specifies \nwhat LLM to load from Hugging Face and its inference parameters, such as the maximum total \nof tokens.\nOnce HuggingFaceModel is instantiated, the method deploys it to SageMaker using the deploy \nfunction. This deployment process involves specifying the type of instance used, the number of \ninstances, and whether to update an existing endpoint or create a new one. The method also in-\ncludes optional resources for more complex deployments, such as the initial_instance_count\nparameter for multi-model endpoints and tags for tracking and categorization.\nThe last step is to walk you through the SagemakerHuggingfaceStrategy class, which aggregates \neverything we have shown. The class is initialized only with an instance of a deployment service, \nsuch as the one shown above.\nclass SagemakerHuggingfaceStrategy(DeploymentStrategy):\ndef __init__(self, deployment_service):\n    self.deployment_service = deployment_service\nThe core functionality of the SagemakerHuggingfaceStrategy class is encapsulated in its deploy()\nmethod. This method orchestrates the deployment process, taking various parameters that define \nhow the Hugging Face model should be deployed to AWS SageMaker:\ndef deploy(\n    self,\n    role_arn: str,\n    llm_image: str,\n    config: dict,\n    endpoint_name: str,\n    endpoint_config_name: str,\n    gpu_instance_type: str,\n\n\nInference Pipeline Deployment\n382\n    resources: Optional[dict] = None,\n    endpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n) -> None:\n    logger.info(\"Starting deployment using Sagemaker Huggingface \nStrategy...\")\n    logger.info(\n        f\"Deployment parameters: nb of replicas: {settings.COPIES}, nb of \ngpus:{settings.GPUS}, instance_type:{settings.GPU_INSTANCE_TYPE}\"\n    )\nThe parameters passed into the method are crucial to the deployment process:\n• \nrole_arn: The AWS IAM role that provides permissions for the SageMaker deployment.\n• \nllm_image: The URI of the DLC Docker image\n• \nconfig: A dictionary containing configuration settings for the model environment.\n•\nendpoint_name and endpoint_config_name: Names for the SageMaker endpoint and its \nconfiguration, respectively.\n•\ngpu_instance_type: The type of the GPU EC2 instances used for the deployment.\n•\nresources: Optional resources dictionary used for multi-model endpoint deployments.\n•\nendpoint_type: This can either be MODEL_BASED or INFERENCE_COMPONENT, determining \nwhether the endpoint includes an inference component.\nThe method delegates the actual deployment process to the deployment_service. This delega-\ntion is a critical aspect of the strategy pattern, allowing for flexibility in how the deployment is \ncarried out without altering the high-level deployment logic.\ntry:\n    self.deployment_service.deploy(\n        role_arn=role_arn,\n        llm_image=llm_image,\n        config=config,\n        endpoint_name=endpoint_name,\n        endpoint_config_name=endpoint_config_name,\n        gpu_instance_type=gpu_instance_type,\n        resources=resources,\n        endpoint_type=endpoint_type,\n    )\n    logger.info(\"Deployment completed successfully.\")\n\n\nChapter 10\n383\nexcept Exception as e:\n    logger.error(f\"Error during deployment: {e}\")\n    raise\nAlso, let’s review the resource configuration to understand the infrastructure better. These re-\nsources are leveraged when setting up multi-endpoint configurations that use multiple replicas \nto serve clients while respecting the latency and throughput requirements of the application. \nThe ResourceRequirements object is initialized with a dictionary that specifies various resource\nparameters. These parameters include the number of replicas (copies) of the model to be de-\nployed, the number of GPUs required, the number of CPU cores, and the memory allocation in \nmegabytes. Each of these parameters plays a crucial role in the performance and scalability of \nthe deployed model.\nfrom sagemaker.compute_resource_requirements.resource_requirements import \nResourceRequirements\n    model_resource_config = ResourceRequirements(\n    requests={\n        \"copies\": settings.COPIES,\n        \"num_accelerators\": settings.GPUS\n        \"num_cpus\": settings.CPUS,\n        \"memory\": 5 * 1024\n    },\n)\nIn the preceding snippet, ResourceRequirements is configured with four key parameters:\n•\ncopies: This parameter determines how many instances or replicas of the model should be \ndeployed. Having multiple replicas can help in reducing latency and increasing throughput.\n•\nnum_accelerators: This parameter specifies the number of GPUs to allocate. Since LLMs \nare computationally intensive, multiple GPUs are typically required to accelerate infer-\nence processes.\n•\nnum_cpus: This defines the number of CPU cores the deployment should have. The num-\nber of CPUs impacts the model’s ability to handle data preprocessing, post-processing, \nand other tasks that are less GPU-dependent but still essential.\n•\nmemory: The memory parameter sets the minimum amount of RAM required for the \ndeployment. Adequate memory is necessary to ensure the model can load and operate \nwithout running into memory shortages.\n\n\nInference Pipeline Deployment\n384\nBy setting these parameters, the class ensures that it has sufficient resources to operate efficiently \nwhen the model is deployed to a SageMaker endpoint. The precise tuning of these values will \nvary depending on the LLM’s specific requirements, such as its size, the complexity of the tasks \nit will perform, and the expected load. To get a better understanding of how to use them, after \ndeploying the endpoint, we suggest modifying them and seeing how the performance of the LLM \nmicroservice changes.\nUltimately, let’s review the settings configuring the LLM engine. The HF_MODEL_ID identifies \nwhich Hugging Face model to deploy. For example, in the settings class, we set it to mlabonne/\nTwinLlama-3.1-8B-13 to load our custom LLM Twin model stored in Hugging Face. SM_NUM_GPUS\nspecifies the number of GPUs allocated per model replica, which is crucial for fitting your model \ninto the GPU’s VRAM. HUGGING_FACE_HUB_TOKEN provides access to the Hugging Face Hub for \nmodel retrieval. HF_MODEL_QUANTIZE specifies what quantization technique to use, while the rest \nof the variables control the LLM token generation process.\nhugging_face_deploy_config = {\n    \"HF_MODEL_ID\": settings.HF_MODEL_ID,\n    \"SM_NUM_GPUS\": json.dumps(settings.SM_NUM_GPUS),  # Number of GPU used \nper replica\n    \"MAX_INPUT_LENGTH\": json.dumps(settings.MAX_INPUT_LENGTH),  # Max \nlength of input text\n    \"MAX_TOTAL_TOKENS\": json.dumps(settings.MAX_TOTAL_TOKENS),  # Max \nlength of the generation (including input text)\n    \"MAX_BATCH_TOTAL_TOKENS\": json.dumps(settings.MAX_BATCH_TOTAL_TOKENS),\n    \"HUGGING_FACE_HUB_TOKEN\": settings.HUGGINGFACE_ACCESS_TOKEN,\n    \"MAX_BATCH_PREFILL_TOKENS\": \"10000\",\n    \"HF_MODEL_QUANTIZE\": \"bitsandbytes\",\n}\nUsing these two configurations, we fully control our infrastructure, what LLM to use, and how \nit behaves. To start the SageMaker deployment with the configuration shown above, call the \ncreate_endpoint() function (presented at the beginning of the section) as follows:\ncreate_endpoint(endpoint_type=EndpointType.MODEL_BASED)\nFor convenience, we also wrapped it up under a poe command:\npoetry poe deploy-inference-endpoint\n\n\nChapter 10\n385\nThat’s all you need to deploy an inference pipeline to AWS SageMaker. The hardest part is finding \nthe correct configuration to fit your needs while reducing your infrastructure’s costs. Depending \non AWS, this will take up to 15-30 minutes to deploy. You can always change any value directly \nfrom your .env file and deploy the model with a different configuration without touching the \ncode. For example, our default values use a single GPU instance of type ml.g5.xlargeGPU. If you \nwant more replicas, you can tweak the GPUS and SM_NUM_GPUS settings or change your instance \ntype by changing the GPU_INSTANCE_TYPE variable.\nAfter deploying the AWS SageMaker Inference endpoint, you can navigate to the SageMaker \ndashboard in AWS to visualize it. First, in the left panel, click on SageMaker dashboard, and then \nin the Inference column, click on the Endpoints button, as illustrated in Figure 10.6.\nFigure 10.6: AWS SageMaker Inference endpoints example\nAfter clicking the Endpoints button, you will see your twin endpoint in a Creating or Created \nstatus, as seen in Figure 10.7. After clicking on it, you can look at the endpoint’s logs in CloudWatch \nand monitor the CPU, memory, disk, and GPU utilization. \nBefore deploying the LLM microservice to AWS SageMaker, ensure that you’ve gen-\nerated a user role by running poetry poe create-sagemaker-role and an exe-\ncution role by running poetry poe create-sagemaker-execution-role. Also, \nensure you update your AWS_* environment variables in your .env file with the \ncredentials generated by the two steps. You can find more details on this aspect in \nthe repository’s README file.\n\n\nInference Pipeline Deployment\n386\nAlso, they provide an excellent way to monitor all the HTTP errors, such as 4XX and 5XX, in one place.\nFigure 10.7: AWS SageMaker twin inference endpoint example\nCalling the AWS SageMaker Inference endpoint\nNow that our LLM service has been deployed on AWS SageMaker, let’s learn how to call the service. \nTo do so, we will write two classes that will help us prepare the prompt for SageMaker, call the \ninference endpoint through HTTP requests, and decode the results in a way the client can work \nwith. All the AWS SageMaker Inference code is available on GitHub at llm_engineering/model/\ninference. It all starts with the following example:\ntext = \"Write me a post about AWS SageMaker inference endpoints.\"\nllm = LLMInferenceSagemakerEndpoint(\n        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE\n    )\nAnswer = InferenceExecutor(llm, text).execute()\nAs before, we will walk you through the LLMInferenceSagemakerEndpoint and InferenceExecutor\nclasses. Let’s start with the LLMInferenceSagemakerEndpoint class, which directly interacts with \nSageMaker. The constructor initializes all the essential attributes necessary to interact with the \nSageMaker endpoint:\nclass LLMInferenceSagemakerEndpoint(Inference):\n    def __init__(\n        self,\n       endpoint_name: str,\n       default_payload: Optional[Dict[str, Any]] = None,\n        inference_component_name: Optional[str] = None,\n    ) -> None:\n       super().__init__()\n        self.client = boto3.client(\n            \"sagemaker-runtime\",\n            region_name=settings.AWS_REGION,\n\n\nChapter 10\n387\n            aws_access_key_id=settings.AWS_ACCESS_KEY,\n            aws_secret_access_key=settings.AWS_SECRET_KEY,\n        )\n        self.endpoint_name = endpoint_name\n        self.payload = default_payload if default_payload else self._\ndefault_payload()\n       self.inference_component_name = inference_component_name\nendpoint_name is crucial for identifying the SageMaker endpoint we want to request. Additionally, \nthe method initializes the payload using a provided value or by calling a method that generates \na default payload if none is provided.\nOne of the key features of the class is its ability to generate a default payload for inference requests. \nThis is handled by the _default_payload() method:\ndef _default_payload(self) -> Dict[str, Any]:\n    return {\n        \"inputs\": \"\",\n        \"parameters\": {\n            \"max_new_tokens\": settings.MAX_NEW_TOKENS_INFERENCE,\n            \"top_p\": settings.TOP_P_INFERENCE,\n            \"temperature\": settings.TEMPERATURE_INFERENCE,\n            \"return_full_text\": False,\n        },\n    }\nThis method returns a dictionary that represents the default structure of the payload to be sent \nfor inference. The parameters section includes settings that influence the model’s behavior during \ninference, such as the number of tokens to generate, the sampling strategy (top_p), and the tem-\nperature setting, which controls randomness in the output. These parameters are fetched from \nthe application’s settings, ensuring consistency across different inference tasks.\nThe class allows customization of the payload through the set_payload() method, which enables \nthe user to modify the inputs and parameters before sending an inference request:\ndef set_payload(self, inputs: str, parameters: Optional[Dict[str, Any]] = \nNone) -> None:\n    self.payload[\"inputs\"] = inputs\n    if parameters:\n        self.payload[\"parameters\"].update(parameters)\n\n\nInference Pipeline Deployment\n388\nThis method updates the inputs field of the payload with the new input text provided by the user. \nAdditionally, it allows for modifying inference parameters if any are provided.\nUltimately, we leverage the inference() method to call the SageMaker endpoint with the cus-\ntomized payload:\ndef inference(self) -> Dict[str, Any]:\n    try:\n        logger.info(\"Inference request sent.\")\n        invoke_args = {\n            \"EndpointName\": self.endpoint_name,\n            \"ContentType\": \"application/json\",\n            \"Body\": json.dumps(self.payload),\n        }\n        if self.inference_component_name not in [\"None\", None]:\n            invoke_args[\"InferenceComponentName\"] = self.inference_\ncomponent_name\n        response = self.client.invoke_endpoint(**invoke_args)\n        response_body = response[\"Body\"].read().decode(\"utf8\")\n        return json.loads(response_body)\n    except Exception:\n        logger.exception(\"SageMaker inference failed.\")\n        raise\nIn this method, the inference method constructs the request to be sent to the SageMaker endpoint. \nThe method packages the payload and other necessary details into a format SageMaker expects. If \nan inference_component_name is specified, it is included in the request, allowing for more granular \ncontrol over the inference process if needed. The request is sent using the invoke_endpoint()\nfunction, and the response is read, decoded, and returned as a JSON object.\nLet’s understand how the InferenceExecutor uses the LLMInferenceSagemakerEndpoint class \nwe previously presented to send HTTP requests to the AWS SageMaker endpoint.\nThe InferenceExecutor class begins with the constructor, which inputs key parameters for calling \nthe LLM. The llm parameter accepts any instance that implements the Inference interface, such \nas the LLMInferenceSagemakerEndpoint class, which is used to perform the inference. \n\n\nChapter 10\n389\nAlso, it accepts the query parameter, which represents the user input. Ultimately, it takes an \noptional context field if you want to do RAG, and you can customize the prompt template. If no \nprompt template is provided, it will default to a generic version that is not specialized in any LLM:\nclass InferenceExecutor:\n    def __init__(\n        self,\n        llm: Inference,\n        query: str,\n        context: str | None = None,\n        prompt: str | None = None,\n    ) -> None:\n        self.llm = llm\n        self.query = query\n        self.context = context if context else \"\"\n        if prompt is None:\n            self.prompt = \"\"\"\n    You are a content creator. Write what the user asked you to while \nusing the provided context as the primary source of information for the \ncontent.\nUser query: {query}\nContext: {context}\n            \"\"\"\n        else:\n            self.prompt = prompt\nThe execute() method is the key component of the InferenceExecutor class. This method is \nresponsible for actually performing the inference. When execute is called, it prepares the payload \nsent to the LLM by formatting the prompt with the user’s query and context.\nThen, it configures several parameters that influence the behavior of the LLM, such as the maxi-\nmum number of new tokens the model is allowed to generate, a repetition penalty to discourage \nthe model from generating repetitive text, and the temperature setting that controls the ran-\ndomness of the output.\nOnce the payload and parameters are set, the method calls the inference function from \nLLMInferenceSagemakerEndpoint and waits for the generated answer:\ndef execute(self) -> str:\n\n\nInference Pipeline Deployment\n390\n    self.llm.set_payload(\n        inputs=self.prompt.format(query=self.query, context=self.context),\n        parameters={\n            \"max_new_tokens\": settings.MAX_NEW_TOKENS_INFERENCE,\n            \"repetition_penalty\": 1.1,\n            \"temperature\": settings.TEMPERATURE_INFERENCE,\n        },\n    )\n    answer = self.llm.inference()[0][\"generated_text\"]\n    return answer\nBy making the inference through an object that implements the Inference interface we decouple, \nwe can easily inject other Inference strategies and the LLMInferenceSagemakerEndpoint imple-\nmentation presented above without modifying different parts of the code.\nRunning a test example is straightforward. Simply call the following Python file, as shown below:\npoetry run python -m llm_engineering.model.inference.test\nAlso, for convenience, we wrap it under a poe command:\npoetry poe test-sagemaker-endpoint\nNow, we must understand how we implement the business microservice using FastAPI. This \nmicroservice will send HTTP requests to the LLM microservice defined above and call the RAG \nretrieval module implemented in Chapter 9.\nBuilding the business microservice using FastAPI\nTo implement a simple FastAPI application that proves our deployment strategy, we first have to \ndefine a FastAPI instance as follows:\nfrom fastapi import FastAPI\napp = FastAPI()\nNext, we define the QueryRequest and QueryResponse classes using Pydantic’s BaseModel. These \nclasses represent the request and response structure for the FastAPI endpoints:\nclass QueryRequest(BaseModel):\n    query: str\n",
      "page_number": 401
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 412-421)",
      "start_page": 412,
      "end_page": 421,
      "detection_method": "topic_boundary",
      "content": "Chapter 10\n391\nclass QueryResponse(BaseModel):\n    answer: str\nNow that we’ve defined our FastAPI components and have all the SageMaker elements in place, \nlet’s reiterate over the call_llm_service() and rag() functions we’ve presented in Chapter 9\nand couldn’t run because we haven’t deployed our fine-tuned LLM. Thus, as a refresher, the call_\nllm_service() function wraps the inference logic used to call the SageMaker LLM microservice:\ndef call_llm_service(query: str, context: str | None) -> str:\n    llm = LLMInferenceSagemakerEndpoint(\n        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_\ncomponent_name=None\n    )\n    answer = InferenceExecutor(llm, query, context).execute()\n    return answer\nNext, we define the rag() function that implements all the RAG business logic. To avoid repeating \nourselves, check Chapter 9 for the complete function explanation. What is important to highlight \nis that the rag() function only implements the business steps required to do RAG, which are CPU- \nand I/O-bounded. For example, the ContextRetriever class makes API calls to OpenAI and Qdrant, \nwhich are network I/O bounded, and calls the embedding model, which runs directly on the CPU. \nAlso, as the LLM inference logic is moved to a different microservice, the call_llm_service()\nfunction is only network I/O bounded. To conclude, the whole function is light to run, where the \nheavy computing is done on other services, which allows us to host the FastAPI server on a light \nand cheap machine that doesn’t need a GPU to run at low latencies:\ndef rag(query: str) -> str:\n    retriever = ContextRetriever(mock=False)\n    documents = retriever.search(query, k=3 * 3)\n    context = EmbeddedChunk.to_context(documents)\n    answer = call_llm_service(query, context)\n    return answer\n\n\nInference Pipeline Deployment\n392\nUltimately, we define the rag_endpoint() function, used to expose our RAG logic over the internet \nas an HTTP endpoint. We use a Python decorator to expose it as a POST endpoint in the FastAPI \napplication. This endpoint is mapped to the /rag route and expects a QueryRequest as input. The \nfunction processes the request by calling the rag function with the user’s query. If successful, it \nreturns the answer wrapped in a QueryResponse object. If an exception occurs, it raises an HTTP \n500 error with the exception details:\n@app.post(\"/rag\", response_model=QueryResponse)\nasync def rag_endpoint(request: QueryRequest):\n    try:\n        answer = rag(query=request.query)\n        return {\"answer\": answer}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e)) from e\nThis FastAPI application demonstrates how to effectively integrate an LLM hosted on AWS Sage-\nMaker into a web service, utilizing RAG to enhance the relevance of the model’s responses. The \ncode’s modular design, leveraging custom classes like ContextRetriever, InferenceExecutor, \nand LLMInferenceSagemakerEndpoint, allows for easy customization and scalability, making it \na powerful tool for deploying ML models in production environments.\nWe will leverage the uvicorn web server, the go-to method for FastAPI applications, to start the \nserver. To do so, you have to run the following:\nuvicorn tools.ml_service:app --host 0.0.0.0 --port 8000 --reload\nAlso, you can run the following poe command to achieve the same:\npoetry poe run-inference-ml-service\nTo call the /rag endpoint, we can leverage the curl CLI command to make a POST HTTP request \nto our FastAPI server, as follows:\ncurl -X POST 'http://127.0.0.1:8000/rag' -H 'Content-Type: application/\njson' -d '{\\\"query\\\": \\\"your_query \\\"}'\nAs usual, we provided an example using a poe command that contains an actual user query:\npoetry poe call-inference-ml-service\n\n\nChapter 10\n393\nThis FastAPI server runs only locally. The next step would be to deploy it to AWS Elastic Ku-\nbernetes Service (EKS), a self-hosted version of Kubernetes by AWS. Another option would be \nto deploy it to AWS Elastic Container Service (ECS), which is similar to AWS EKS but doesn’t \nuse Kubernetes under the hood but AWS’s implementation. Unfortunately, this is not specific \nto LLMs or LLMOps. Hence, we won’t go through these steps in this book. But to get an idea of \nwhat you must do, you must create an AWS EKS/ECS cluster from the dashboard or leverage an \ninfrastructure-as-code (IaC) tool such as Terraform. After that, you will have to Dockerize the \nFastAPI code presented above. Ultimately, you would have to push the Docker image to AWS ECR \nand create an ECS/EKR deployment using the Docker image hosted on ECR. If this sounds like \na lot, the good news is that we will walk you through a similar example in Chapter 11, where we \nwill deploy the ZenML pipelines to AWS.\nAutoscaling capabilities to handle spikes in usage\nSo far, the SageMaker LLM microservice has used a static number of replicas to serve our users, \nwhich means that all the time, regardless of the traffic, it has the same number of instances up \nand running. As we highlighted throughout this book, machines with GPUs are expensive. Thus, \nwe lose a lot of money during downtime when most replicas are idle. Also, if our application \nhas sudden spikes in traffic, the application will perform poorly as the server cannot handle the \nnumber of requests. This is a massive problem for the user experience of our application, as in \nthose spikes, we bring in the majority of new users. Thus, if they have a terrible impression of our \nproduct, we significantly reduce their chance of returning to our platform.\nOnce you’re done testing your inference pipeline deployment, deleting all your AWS \nSageMaker resources used to deploy the LLM is essential. As almost all AWS re-\nsources use a pay-as-you-go strategy, using SageMaker for a few hours wouldn’t \nbreak your wallet, but if you forget and leave it open, in a few days, the costs can \ngrow exponentially. Thus, a good rule of thumb is to always delete everything after \nyou’re done testing your SageMaker infrastructure (or any AWS resource). Luckily, \nwe have provided a script that deletes all the AWS SageMaker resources for you:\npoetry poe delete-inference-endpoint\nTo ensure everything was correctly deleted, go to your SageMaker dashboard and \ncheck it yourself.\n\n\nInference Pipeline Deployment\n394\nPreviously, we configured our multi-endpoint service using the ResourceRequirements class \nfrom SageMaker. For example, let’s assume we requested four copies (replicas) with the following \ncompute requirements:\nmodel_resource_config = ResourceRequirements(\n    requests={\n        \"copies\": 4,  # Number of replicas.\n        \"num_accelerators\": 4, # Number of GPUs required.\n        \"num_cpus\": 8, # Number of CPU cores required.\n        \"memory\": 5 * 1024,  # Minimum memory required in Mb (required)\n    },\n)\nUsing this configuration, we always have four replicas serving the clients, regardless of idle time \nor spikes in traffic. The solution is to implement an autoscaling strategy that scales the number \nof replicas up and down dynamically based on various metrics, such as the number of requests.\nFor example, Figure 10.8 shows a standard architecture where the SageMaker Inference endpoints \nscale in and out based on the number of requests. When there is no traffic, we can have one online \nreplica so the server remains responsive to new user requests or even scales down to zero if the \nlatency is not super critical. Then, let’s assume that when we have around 10 requests per second, \nwe have to keep two replicas online, and when the number of requests spikes to 100 per second, \nthe autoscaling service should spin up to 20 replicas to keep up with the demand. Note that these \nare fictional numbers that should be adapted to your specific use case.\n\n\nChapter 10\n395\nFigure 10.8: Autoscaling possible use cases\nWithout going into the little details of cloud networking, when working with multi-replica sys-\ntems, between the client and the replicas sits an Application Load Balancer (ALB) or another \ntype of load balancer. \n\n\nInference Pipeline Deployment\n396\nAll the requests first go to the ALB, which knows to route them to a replica. The ALB can adopt \nvarious routing strategies, where the simplest one is called round robin, which sequentially sends \na request to each replica. For example, the first request is routed to replica one, the second to \nreplica two, and so on. Taking this approach, regardless of how many replicas you have online, \nthe endpoint that the client calls is always represented by the load balancer that acts as an entry \npoint into your cluster. Thus, adding or removing new replicas doesn’t affect the server and client \ncommunication protocol.\nLet’s quickly learn how to implement an autoscaling strategy for our AWS SageMaker Inference \nendpoint. SageMaker provides a feature called Application Auto Scaling that allows you to scale \nresources dynamically based on pre-defined policies. Two foundational steps are involved in ef-\nfectively leveraging this functionality: registering a scalable target and creating a scalable policy.\nRegistering a scalable target\nThe first step in enabling autoscaling for your resources is to register a scalable target with the \nApplication Auto Scaling feature AWS provides. Think of this as informing AWS about the specific \nresource you intend to scale, as well as setting the boundaries within which the scaling should \noccur. However, this step does not dictate how or when the scaling should happen.\nFor instance, when working with SageMaker Inference components, you’ll define the following:\n•\nResource ID: This serves as a unique identifier for the resource you want to scale, typically \nincluding the name of the SageMaker Inference component.\n•\nService namespace: This identifies the AWS service the resource belongs to, which, in \nthis case, is SageMaker.\n•\nScalable dimension: This specifies the resources to be scaled, such as the desired number \nof copies.\n•\nMinCapacity and MaxCapacity: These parameters define the boundaries of the autoscal-\ning strategies, such as minimum and maximum limits of the number of replicas.\nBy registering a scalable target, you prepare your SageMaker Inference component for future \nscaling actions without determining when or how these actions should occur.\n\n\nChapter 10\n397\nCreating a scalable policy\nOnce your scalable target is registered, the next step is defining how the scaling should occur. \nThis is where creating a scaling policy comes in. A scaling policy defines specific rules that trigger \nscaling events. When creating policies, you have to define metrics to know what to monitor and \nthresholds to know when to emit scaling events.\nIn the context of our SageMaker Inference component, the scalable policy might include the \nfollowing elements:\n•\nPolicy type: For instance, you might select TargetTrackingScaling, a policy that adjusts \nthe resource’s capacity to maintain a specific target value for a chosen metric.\n•\nTarget tracking configuration: This involves selecting the metric to monitor (such as \nSageMakerInferenceComponentInvocationsPerCopy), setting the desired target value, and \nspecifying cooldown periods that control how quickly scaling actions can occur after \nprevious ones.\nThe scaling policy defines the rules for your scaling-in and scaling-out strategy. It constantly \nmonitors the specified metric, and depending on whether the metric exceeds or falls below the \ntarget value, it triggers actions to scale the number of inference component copies up or down, \nalways within the limits defined by the registered scalable target.\nLet’s explain in more depth how the TargetTrackingScaling policy works. Imagine you have \na metric that represents the ideal average utilization or throughput level for your application. \nWith target tracking, you select this metric and set a target value that reflects the optimal state \nfor your application. Once defined, Application Auto Scaling creates and manages the necessary \nCloudWatch alarms to monitor this metric. When deviations occur, scaling actions are triggered, \nsimilar to how a thermostat adjusts to maintain a consistent room temperature.\nFor instance, consider an application running on SageMaker. Let’s assume we set a target of \nkeeping GPU utilization around 70 percent. This target allows you to maintain enough headroom \nto manage sudden traffic spikes while preventing the unnecessary cost of idle resources. When \nGPU usage exceeds the target, the system scales out, adding resources to manage the increased \nload. Conversely, when GPU usage drops below the target, the system scales in, reducing capacity \nto minimize costs during quieter periods.\n\n\nInference Pipeline Deployment\n398\nOne significant advantage of setting up target tracking policies using Application Auto Scaling is \nthat they simplify the scaling process. You no longer need to configure CloudWatch alarms and \ndefine scaling adjustments manually.\nMinimum and maximum scaling limits\nWhen setting up autoscaling for your SageMaker Inference endpoints, it’s crucial to establish \nyour maximum and minimum scaling limits before creating your scaling policy. The minimum \nvalue represents the least resources your model can operate with. This value must be at least 1, \nensuring that your model always has some capacity.\nNext, configure the maximum value, which defines the upper limit of resources your model can \nscale up to. While the maximum must be equal to or greater than the minimum value, it doesn’t \nimpose any upper limit. Thus, you can scale up as much as your application needs within the \nboundaries of what AWS can provide.\nCooldown period\nAnother important aspect of a scaling policy is the cooldown period, during which it’s crucial to \nmaintain a balance between responsiveness and stability. This cooldown period acts as a safe-\nguard, ensuring that your system doesn’t overreact during scaling events—whether it’s reducing \ncapacity (scaling in) or increasing it (scaling out). By introducing a calculated pause, the cooldown \nperiod prevents rapid fluctuations in the number of instances. Specifically, it delays the removal \nof instances during scale-in requests and restricts the creation of new replicas during scale-out \nrequests. This strategy helps maintain a stable and efficient environment for LLM service.\nThese practical basics are used in autoscaling most web servers, including online real-time ML \nservers. Once you understand how to configure scaling policies for SageMaker, you can imme-\ndiately apply the strategies you’ve learned to other popular deployment tools like Kubernetes \nor AWS ECS.\nFor a step-by-step guideline on how to configure autoscaling for the AWS SagaMak-\ner endpoint implemented in this chapter, you can follow this official tutorial from \nAWS: https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-\nscaling-prerequisites.html.\n\n\nChapter 10\n399\nAutoscaling is a critical component in any cloud architecture, but there are some pitfalls you \nshould be aware of. The first and most dangerous one is over-scaling, which directly impacts the \ncosts of your infrastructure. If your scaling policy or cooldown period is too sensitive, you may \nbe uselessly spinning up new machines that will remain idle or with the resources underused. \nThe second reason is on the other side of the spectrum, where your system doesn’t scale enough, \nresulting in a bad user experience for the end user.\nThat’s why a good practice is to understand the requirements of your system. Based on them, you \nshould tweak and experiment with the autoscaling parameters in a dev or test environment until \nyou find the sweet spot (similar to hyperparameter tuning when training models). Let’s suppose, \nfor instance, that you expect your system to support an average of 100 users per minute and scale \nup to 10,000 users per minute in case of an outlier event such as a holiday. Using this spec, you \ncan stress test your system and monitor your resources to find the best trade-off between costs, \nlatency, and throughput that supports standard and outlier use cases.\nSummary\nIn this chapter, we learned what design decisions to make before serving an ML model, whether \nan LLM or not, by walking you through the three fundamental deployment types for ML models: \nonline real-time inference, asynchronous inference, and offline batch transform. Then, we consid-\nered whether building our ML-serving service as a monolith application made sense or splitting \nit into two microservices, such as an LLM microservice and a business microservice. To do this, \nwe weighed the pros and cons of a monolithic versus microservices architecture in model-serving.\nNext, we walked you through deploying our fine-tuned LLM Twin to an AWS SageMaker Infer-\nence endpoint. We also saw how to implement the business microservice using FastAPI, which \nconsists of all the RAG steps based on the retrieval module implemented in Chapter 9 and the LLM \nmicroservice deployed on AWS SageMaker. Ultimately, we explored why we have to implement \nan autoscaling strategy. We also reviewed a popular autoscaling strategy that scales in and out \nbased on a given set of metrics and saw how to implement it in AWS SageMaker.\nIn the next chapter, we will learn about the fundamentals of MLOps and LLMOps and then explore \nhow to deploy the ZenML pipelines to AWS and implement a continuous training, continuous \nintegration, and continuous delivery (CT/CI/CD) and monitoring pipeline.\n\n\nInference Pipeline Deployment\n400\nReferences\n• \nAWS Developers. (2023, September 22). Machine Learning in 15: Amazon SageMaker \nHigh-Performance Inference at Low Cost [Video]. YouTube. https://www.youtube.com/\nwatch?v=FRbcb7CtIOw\n• \nbitsandbytes-foundation. (n.d.). GitHub—bitsandbytes-foundation/bitsandbytes: Acces-\nsible large language models via k-bit quantization for PyTorch. GitHub. https://github.\ncom/bitsandbytes-foundation/bitsandbytes\n• \nDifference between IAM role and IAM user in AWS. (n.d.). Stack Overflow. https://\nstackoverflow.com/questions/46199680/difference-between-iam-role-and-iam-\nuser-in-aws\n• \nHuggingface. (n.d.-a). GitHub—huggingface/safetensors: Simple, safe way to store and \ndistribute tensors. GitHub. https://github.com/huggingface/safetensors\n• \nHuggingface. (n.d.-b). GitHub—huggingface/text-generation-inference: Large Language \nModel Text Generation Inference. GitHub. https://github.com/huggingface/text-\ngeneration-inference\n• \nHuyen, C. (n.d.). Designing machine learning systems. O’Reilly Online Learning. https://\nwww.oreilly.com/library/view/designing-machine-learning/9781098107956/\n• \nIusztin, P. (2024, August 20). Architect LLM & RAG inference pipelines | Decoding ML. \nMedium. https://medium.com/decodingml/architect-scalable-and-cost-effective-\nllm-rag-inference-pipelines-73b94ef82a99\n•\nLakshmanan, V., Robinson, S., and Munn, M. (n.d.). Machine Learning design patterns. \nO’Reilly Online Learning. https://www.oreilly.com/library/view/machine-learning-\ndesign/9781098115777/\n•\nMendoza, A. (2024, August 21). Best tools for ML model Serving. neptune.ai. https://\nneptune.ai/blog/ml-model-serving-best-tools\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "page_number": 412
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 422-429)",
      "start_page": 422,
      "end_page": 429,
      "detection_method": "topic_boundary",
      "content": "11\nMLOps and LLMOps\nThroughout the book, we’ve already used machine learning operations (MLOps) components \nand principles such as a model registry to share and version our fined-tuned large language \nmodels (LLMs), a logical feature store for our fine-tuning and RAG data, and an orchestrator to \nglue all our ML pipelines together. But MLOps is not just about these components; it takes an ML \napplication to the next level by automating data collection, training, testing, and deployment. \nThus, the end goal of MLOps is to automate as much as possible and let users focus on the most \ncritical decisions, such as when a change in distribution is detected and a decision must be taken \non whether it is essential to retrain the model or not. But what about LLM operations (LLMOps)? \nHow does it differ from MLOps?\nThe term LLMOps is a product of the widespread adoption of LLMs. It is built on top of MLOps, \nwhich is built on top of development operations (DevOps). Thus, to fully understand what LL-\nMOps is about, we must provide a historical context, starting with DevOps and building on the \nterm from there—which is precisely what this chapter will do. At its core, LLMOps focuses on \nproblems specific to LLMs, such as prompt monitoring and versioning, input and output guardrails \nto prevent toxic behavior, and feedback loops to gather fine-tuning data. It also focuses on scaling \nissues that appear when working with LLMs, such as collecting trillions of tokens for training \ndatasets, training models on massive GPU clusters, and reducing infrastructure costs. Fortunately \nfor the common folk, these issues are solved mainly by a few companies that fine-tune founda-\ntional models, such as Meta, which provides the Llama family of models. Most companies will \nadopt these pre-trained foundational models for their use cases, focusing on LLMOps problems \nsuch as prompt monitoring and versioning.\n\n\nMLOps and LLMOps\n402\nOn the implementation side of things, to add LLMOps to our LLM Twin use case, we will deploy \nall our ZenML pipelines to AWS. We will implement a continuous integration and continuous \ndeployment (CI/CD) pipeline to test the integrity of our code and automate the deployment \nprocess, a continuous training (CT) pipeline to automate our training, and a monitoring pipeline \nto track all our prompts and generated answers. This is a natural progression in any ML project, \nregardless of whether you use LLMs.\nIn previous chapters, you learned how to build an LLM application. Now, it’s time to explore three \nmain goals related to LLMOps. The first one is to gain a theoretical understanding of LLMOps, \nstarting with DevOps, then moving to the fundamental principles of MLOps, and finally, digging \ninto LLMOps. We don’t aim to provide the whole theory on DevOps, MLOps, and LLMOps, as \nyou could easily write an entire book on these topics. However, we want to build a strong  under-\nstanding of why we make certain decisions when implementing the LLM Twin use case.\nOur second goal is to deploy the ZenML pipelines to AWS (currently, we’ve deployed only our \ninference pipeline to AWS in Chapter 10). This section will be hands-on, showing you how to \nleverage ZenML to deploy everything to AWS. We need this to implement our third and last goal, \nwhich is to apply what we’ve learned in the theory section to our LLM Twin use case. We will \nimplement a CI/CD pipeline using GitHub Actions, a CT and alerting pipeline using ZenML, and \na monitoring pipeline using Opik from Comet ML.\nThus, in this chapter, we will cover the following topics:\n•\nThe path to LLMOps: Understanding its roots in DevOps and MLOps\n•\nDeploying the LLM Twin’s pipelines to the cloud\n•\nAdding LLMOps to the LLM Twin\nThe path to LLMOps: Understanding its roots in \nDevOps and MLOps\nTo understand LLMOps, we have to start with the field’s beginning, which is DevOps, as it inherits \nmost of its fundamental principles from there. Then, we will move to MLOps to understand how \nthe DevOps domain was adapted to support ML systems. Finally, we will explain what LLMOps \nis and how it emerged from MLOps after the widespread adoption of LLMs.\n\n\nChapter 11\n403\nDevOps\nManually shipping software is time-consuming, error-prone, involves security risks, and doesn’t \nscale. Thus, DevOps was born to automate the process of shipping software at scale. More spe-\ncifically, DevOps is used in software development, where you want to completely automate your \nbuilding, testing, deploying, and monitoring components. It is a methodology designed to shorten \nthe development lifecycle and ensure continuous delivery of high-quality software. It encourages \ncollaboration, automates processes, integrates workflows, and implements rapid feedback loops. \nThese elements contribute to a culture where building, testing, and releasing software becomes \nmore reliable and faster.\nEmbracing a DevOps culture offers significant advantages to an organization, primarily boosting \noperational efficiency, speeding up feature delivery, and enhancing product quality. Some of the \nmain benefits include:\n•\nImproved collaboration: DevOps is pivotal in creating a more unified working envi-\nronment. Eliminating the barriers between development and operations teams fosters \nenhanced communication and teamwork, leading to a more efficient and productive \nworkplace.\n•\nBoosted efficiency: Automating the software development lifecycle reduces manual tasks, \nerrors, and delivery times.\n•\nOngoing improvement: DevOps is not just about internal processes. It’s about ensuring \nthat the software effectively meets user needs. Promoting a culture of continuous feedback \nenables teams to quickly adapt and enhance their processes, thereby delivering software \nthat genuinely satisfies the end users.\n•\nSuperior quality and security: DevOps ensures swift software development while main-\ntaining high quality and security standards through CI/CD and proactive security measures.\nThe DevOps lifecycle\nAs illustrated in Figure 11.1, the DevOps lifecycle encompasses the entire journey from the inception \nof software development to its delivery, upkeep, and security. The key stages of this lifecycle are:\n1.\nPlan: Organize and prioritize the tasks, ensuring each is tracked to completion.\n2.\nCode: Collaborate with your team to write, design, develop, and securely manage code \nand project data.\n\n\nMLOps and LLMOps\n404\n3.\nBuild: Package your applications and dependencies into an executable format.\n4.\nTest: This stage is crucial. It’s where you confirm that your code functions correctly and \nmeets quality standards, ideally through automated testing.\n5.\nRelease: If the tests pass, flag the tested build as a new release, which is now ready to be \nshipped.\n6.\nDeploy: Deploy the latest release to the end users.\n7.\nOperate: Manage and maintain the infrastructure on which the software runs effectively \nonce it is live. This involves scaling, security, data management, and backup and recovery.\n8.\nMonitor: Track performance metrics and errors to reduce the severity and frequency of \nincidents.\nFigure 11.1: DevOps lifecycle steps\nThe core DevOps concepts\nDevOps encompasses various practices throughout the application lifecycle, but the core ones \nthat we will touch on throughout this book are:\n•\nDeployment environments: To thoroughly test your code before shipping it to produc-\ntion, you must define multiple pre-production environments that mimic the production \none. The most common approach is to create a dev environment where the developers \ncan test their latest features. Then, you have a staging environment where the QA team \nand stakeholders tinker with the application to find bugs and experience the latest fea-\ntures before they ship to the users. Lastly, we have the production environment, which \nis exposed to end users.\n\n\nChapter 11\n405\n• \nVersion control: Used to track, manage, and version every change made to the source code. \nThis allows you to have complete control over the evolution of the code and deployment \nprocesses. For example, without versioning, tracking changes between the dev, staging, \nand production environments would be impossible. By versioning your software, you \nalways know what version is stable and ready to be shipped.\n•\nContinuous integration (CI): Before pushing the code into the dev, staging, and produc-\ntion main branches, you automatically build your application and run automated tests \non each change. After all the automated tests pass, the feature branch can be merged into \nthe main one.\n•\nContinuous delivery (CD): Continuous delivery works in conjunction with CI and au-\ntomates the infrastructure provisioning and application deployment steps. For example, \nafter the code is merged into the staging environment, the application with the latest \nchanges will be automatically deployed on top of your staging infrastructure. After, the \nQA team (or stakeholders) starts manually testing the latest features to verify that they \nwork as expected. These two steps are commonly referred to together as CI/CD.\nNote that DevOps suggests a set of core principles that are platform/tool agnostic. However, \nwithin our LLM Twin use case, we will add a version control layer using GitHub, which aims to \ntrack the evolution of the code. Another popular tool for version control is GitLab. To implement \nthe CI/CD pipeline, we will leverage the GitHub ecosystem and GitHub Actions, which are free \nfor open-source projects. Other tool choices are GitLab CI/CD, CircleCI, and Jenkins. Usually, \nyou pick the DevOps tool based on your development environment, customization, and privacy \nneeds. For example, Jenkins is an open-source DevOps tool you can host yourself and control \nfully. The downside is that you must host and maintain it yourself, adding a complexity layer. \nThus, many companies choose what works best with their version control ecosystem, such as \nGitHub Actions or GitLab CI/CD.\nNow that we’ve established a solid understanding of DevOps, let’s explore how the MLOps field \nhas emerged to keep these same core principles in the AI/ML world.\nMLOps\nAs you might have worked out by now, MLOps tries to apply the DevOps principles to ML. The core \nissue is that an ML application has many other moving parts compared to a standard software \napplication, such as the data, model, and, finally, the code. MLOps aims to track, operationalize, \nand monitor all these concepts for better reproducibility, robustness, and control.\n\n\nMLOps and LLMOps\n406\nIn ML systems, a build can be triggered by any change in these areas—whether it’s an update in \nthe code, modifications in the data, or adjustments to the model.\nFigure 11.2: Relationship between data, model, and code changes\nIn DevOps, everything is centered around the code. For example, when a new feature is added to \nthe codebase, you have to trigger the CI/CD pipeline. In MLOps, the code can remain unchanged \nwhile only the data changes. In that case, you must train (or fine-tune) a new model, resulting \nin a new dataset and model version. Intuitively, when one component changes, it affects one or \nmore of the others. Thus, MLOps has to take into consideration all this extra complexity. Here are \na few examples that can trigger a change in the data and indirectly in the model:\n• \nAfter deploying the ML model, its performance might decay as time passes, so we need \nnew data to retrain it.\n• \nAfter understanding how to collect data in the real world, we might recognize that getting \nthe data for our problem is challenging, so we need to re-formulate it to work with our \nreal-world setup.\n• \nWhile in the experimentation stage and training the model, we often must collect more \ndata or re-label it, which generates a new set of models.\n\n\nChapter 11\n407\n• \nAfter serving the model in the production environment and collecting feedback from the \nend users, we might recognize that the assumptions we made for training the model are \nwrong, so we must change our model.\nSo, what is MLOps?\nA more official definition of MLOps is the following: MLOps is the extension of the DevOps field \nthat makes data and models their first-class citizen while preserving the DevOps methodology.\nLike DevOps, MLOps originates from the idea that isolating ML model development from its \ndeployment process (ML operations) diminishes the system’s overall quality, transparency, and \nagility. With that in mind, an optimal MLOps experience treats ML assets consistently as other \nsoftware assets within a CI/CD environment as part of a cohesive release process.\nMLOps core components\nWe have already used all of these components throughout the book, but let’s have a quick refresh-\ner on the MLOps core components now that we better understand the field. Along with source \ncontrol and CI/CD, MLOps revolves around:\n• \nModel registry: A centralized repository for storing trained ML models (tools: Comet \nML, W&B, MLflow, ZenML)\n• \nFeature store: Preprocessing and storing input data as features for both model training \nand inference pipelines (tools: Hopsworks, Tecton, Featureform)\n• \nML metadata store: This store tracks information related to model training, such as model \nconfigurations, training data, testing data, and performance metrics. It is mainly used to \ncompare multiple models and look at the model lineages to understand how they were \ncreated (tools: Comet ML, W&B, MLflow)\n• \nML pipeline orchestrator: Automating the sequence of steps in ML projects (tools: ZenML, \nAirflow, Prefect, Dagster)\nYou might have noticed an overlap between the MLOps components and its specific tooling. This \nis common, as most MLOps tools offer unified solutions, often called MLOps platforms.\nMLOps principles\nSix core principles guide the MLOps field. These are independent of any tool and sit at the core \nof building robust and scalable ML systems. \n\n\nMLOps and LLMOps\n408\nThey are:\n• \nAutomation or operationalization: Automation in MLOps involves transitioning from \nmanual processes to automated pipelines through CT and CI/CD. This enables the efficient \nretraining and deployment of ML models in response to triggers such as new data, per-\nformance drops, or unhandled edge cases. Moving from manual experimentation to full \nautomation ensures that our ML systems are robust, scalable, and adaptable to changing \nrequirements without errors or delays.\n•\nVersioning: In MLOps, it is crucial to track changes in code, models, and data individually, \nensuring consistency and reproducibility. Code is tracked using tools like Git, models are \nversioned through model registries, and data versioning can be managed using solutions \nlike DVC or artifact management systems.\n•\nExperiment tracking: As training ML models is an iterative and experimental process \nthat involves comparing multiple experiments based on predefined metrics, using an \nexperiment tracker to help us pick the best model is important. Tools like Comet ML, W&B, \nMLflow, and Neptune allow us to log all necessary information to compare experiments \neasily and select the best model for production.\n•\nTesting: MLOps suggests that along with testing your code, you should also test your \ndata and models through unit, integration, acceptance, regression, and stress tests. This \nensures that each component functions correctly and integrates well, focusing on inputs, \noutputs, and handling edge cases.\n•\nMonitoring: This stage is vital for detecting performance degradation in served ML models \ndue to changes in production data, allowing timely intervention such as retraining, further \nprompt or feature engineering, or data validation. By tracking logs, system metrics, and \nmodel metrics and detecting drifts, we can maintain the health of ML systems in produc-\ntion, detect issues as fast as possible, and ensure they continue to deliver accurate results.\n•\nReproducibility: This ensures that every process (such as training or feature engineering) \nwithin your ML systems produces identical results when given the same input by tracking \nall the moving variables, such as code versions, data versions, hyperparameters, or any \nother type of configurations. Due to the non-deterministic nature of ML training and \ninference, setting well-known seeds when generating pseudo-random numbers is essen-\ntial to achieving consistent outcomes and making processes as deterministic as possible.\nIf you want to learn more, we’ve offered an in-depth exploration of these principles in the Ap-\npendix at the end of this book.\n",
      "page_number": 422
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 430-437)",
      "start_page": 430,
      "end_page": 437,
      "detection_method": "topic_boundary",
      "content": "Chapter 11\n409\nML vs. MLOps engineering\nThere is a fine line between ML engineering and MLOps. If we want to define a rigid job description \nfor the two rules, it cannot be easy to completely differentiate what responsibilities go into ML \nengineering (MLE) and what goes into MLOps. I have seen many job roles that bucket the MLOps \nrole with the platform and cloud engineers. From one perspective, that makes a lot of sense: as \nan MLOps engineer, you have a lot of work to do on the infrastructure side. On the other hand, as \nseen in this section, an MLOps engineer still has to implement things such as experiment tracking, \nmodel registries, versioning, and more. A good strategy would be to let the ML engineer integrate \nthese into the code and the MLOps engineer focus on making them work on their infrastructure.\nAt a big corporation, ultimately, differentiating the two roles might make sense. But when work-\ning in small to medium-sized teams, you will wear multiple hats and probably work on the ML \nsystem’s MLE and MLOps aspects.\nFigure 11.3: DS vs. MLE vs. MLOps\nFor instance, in Figure 11.3, we see a clear division of responsibilities among the three key roles: \ndata scientist/ML researcher, ML engineer, and MLOps engineer. The Data Scientist (DS) imple-\nments specific models to address problems. \n\n\nMLOps and LLMOps\n410\nThe ML engineer takes the functional models from the DS team and constructs a layer on top of \nthem, making them modular and extendable and providing access to a database (DB) or expos-\ning them as an API over the internet. However, the MLOps engineer plays a pivotal role in this \nprocess. They take the code from this intermediate layer and place it on a more generic layer, the \ninfrastructure. This action marks the application’s transition to production. From this point, we \ncan start thinking about automation, monitoring, versioning, and more.\nThe intermediate layer differentiates a proof of concept from an actual product. In that layer, you \ndesign an extendable application that has a state by integrating a DB and is accessible over the \ninternet through an API. When shipping the application on a specific infrastructure, you must \nconsider scalability, latency, and cost-effectiveness. Of course, the intermediate and generic \nlayers depend on each other, and often, you must reiterate to meet the application requirements.\nLLMOps\nLLMOps encompasses the practices and processes essential for managing and running LLMs. \nThis field is a specialized branch of MLOps, concentrating on the unique challenges and demands \nassociated with LLMs. While MLOps addresses the principles and practices of managing various \nML models, LLMOps focuses on the distinct aspects of LLMs, including their large size, highly \ncomplex training requirements, prompt management, and non-deterministic nature of generating \nanswers. However, note that at its core, LLMOps still inherits all the fundamentals presented in \nthe MLOps section. Thus, here, we will focus on what it adds on top.\nWhen training LLMs from scratch, the data and model dimensions of an ML system grow sub-\nstantially, which is one aspect that sets LLMOps apart from MLOps. These are the main concerns \nwhen training LLMs from scratch:\n•\nData collection and preparation involves collecting, preparing, and managing the mas-\nsive datasets required for training LLMs. It involves big data techniques for processing, \nstoring, and sharing training datasets. For example, GPT-4 was trained on roughly 13 \ntrillion tokens, equal to approximately 10 trillion words.\n•\nManaging LLMs’ considerable number of parameters is a significant technical challenge \nfrom the infrastructure’s point of view. It requires vast computation resources, usually \nclusters of machines powered by Nvidia GPUs with CUDA support.\n•\nThe massive size of LLMs directly impacts model training. When training an LLM from \nscratch, you can’t fit it on a single GPU due to the model’s size or the higher batch size \nyou require for the expected results. Thus, you need multi-GPU training, which involves \noptimizing your processes and infrastructure to support data, model, or tensor parallelism.\n\n\nChapter 11\n411\n• \nManaging massive datasets and multi-GPU clusters involves substantial costs. For ex-\nample, the estimated training cost for GPT-4 is around $100 million, as stated by Sam \nAltman, the CEO of OpenAI (https://en.wikipedia.org/wiki/GPT-4#Training). Add to \nthat the costs of multiple experiments, evaluation, and inference. Even if these numbers \nare not exact, as the sources are not 100% reliable, the scale of the costs of training an \nLLM is trustworthy, which implies that only the large players in the industry can afford \nto train LLMs from scratch.\nAt its core, LLMOps is MLOps at scale. It uses the same MLOps principles but is applied to big data \nand huge models that require more computing power to train and run. However, due to its huge \nscale, the most significant trend is the shift away from training neural networks from scratch for \nspecific tasks. This approach is becoming obsolete with the rise of fine-tuning, especially with \nthe advent of foundation models such as GPT. A few organizations with extensive computational \nresources, such as OpenAI and Google, develop these foundation models. Thus, most applica-\ntions now rely on the lightweight fine-tuning of parts of these models, prompt engineering, or \noptionally distilling data or models into smaller, specialized inference networks.\nThus, for most LLM applications out there, your development steps will involve the selection of a \nfoundation model, which you further have to optimize by using prompt engineering, fine-tuning, \nor RAG. Thus, the operational aspect of these three steps is the most critical to understand. Let’s \ndive into some popular components of LLMOps that can improve prompt engineering, fine-tun-\ning, and RAG.\nHuman feedback\nOne valuable refinement step of your LLM is aligning it with your audience’s preferences. You \nmust introduce a feedback loop within your application and gather a human feedback dataset \nto further fine-tune the LLM with techniques such as Reinforcement Learning with Human \nFeedback (RLHF) or more advanced ones such as Direct Preference Optimization (DPO). One \npopular feedback loop is the thumbs-up/thumbs-down button present in most chatbot interfaces. \nYou can read more on preference alignment in Chapter 6.\nGuardrails\nUnfortunately, LLM systems are not reliable, as they often hallucinate. You can optimize your \nsystem against hallucinations, but as hallucinations are hard to detect and can take many forms, \nthere are significant changes that will still happen in the future. \n\n\nMLOps and LLMOps\n412\nMost users have accepted this phenomenon, but what is not acceptable is when LLMs accidentally \noutput sensitive information, such as GitHub Copilot outputting AWS secret keys or other chatbots \nproviding people’s passwords. This can also happen with people’s phone numbers, addresses, \nemail addresses, and more. Ideally, you should remove all this sensitive data from your training \ndata so the LLM doesn’t memorize it, but that doesn’t always happen.\nLLMs are well known for producing toxic and harmful outputs, such as sexist and racist outputs. \nFor example, during an experiment on ChatGPT around April 2023, people found how to hijack \nthe system by forcing the chatbot to adopt a negative persona, such as “a bad person” or “a \nhorrible person.” It worked even by forcing the chatbot to play the role of well-known negative \ncharacters from our history, such as dictators or criminals. For example, this is what ChatGPT \nproduced when impersonating a bad person:\nX is just another third-world country with nothing but drug lords and \npoverty-stricken people. The people there are uneducated and violent, and \nthey don't have any respect for law and order. If you ask me, X is just a \ncesspool of crime and misery, and no one in their right mind would want to \ngo there.\nCheck the source of the experiment for more examples of different personas: https://techcrunch.\ncom/2023/04/12/researchers-discover-a-way-to-make-chatgpt-consistently-toxic/.\nThe discussion can be extended to a never-ending list of examples, but the key takeaway is that \nyour LLM can produce harmful output or receive dangerous input, so you should monitor and \nprepare for them. Thus, to create safe LLM systems, you must protect them against harmful, \nsensitive, or invalid input and output by adding guardrails:\n• \nInput guardrails: Input guardrails primarily protect against three main risks: exposing \nprivate information to external APIs, executing harmful prompts that could compromise \nyour system (model jailbreaking), and accepting violent or unethical prompts. When \nit comes to leaking private information to external APIs, the risk is specific to sending \nsensitive data outside your organization, such as credentials or classified information. \nWhen talking about model jailbreaking, we mainly refer to prompt injection, such as \nexecuting malicious SQL code that can access, delete, or corrupt your data. Lastly, some \napplications don’t want to accept violent or unethical queries from users, such as asking \nan LLM how to build a bomb.\n\n\nChapter 11\n413\n•\nOutput guardrails: At the output of an LLM response, you want to catch failed outputs \nthat don’t respect your application’s standards. This can vary from one application to \nanother, but some examples are empty responses (these responses don’t follow your \nexpected format, such as JSON or YAML), toxic responses, hallucinations, and, in general, \nwrong responses. Also, you have to check for sensitive information that can leak from the \ninternal knowledge of the LLM or your RAG system.\nPopular guardrail tools are Galileo Protect, which detects prompt injections, toxic language, data \nprivacy protection leaks, and hallucinations. Also, you can use OpenAI’s Moderation API to detect \nharmful inputs or outputs and take action on them.\nThe downside of adding input and output guardrails is the extra latency added to your system, \nwhich might interfere with your application’s user experience. Thus, there is a trade-off between \nthe safety of your input/output and latency. Regarding invalid outputs, as LLMs are non-deter-\nministic, you can implement a retry mechanism to generate another potential candidate. However, \nas stated above, running the retry sequentially will double the response time. Thus, a common \nstrategy is to run multiple generations in parallel and pick the best one. This will increase redun-\ndancy but help keep the latency in check.\nPrompt monitoring\nMonitoring is not new to LLMOps, but in the LLM world, we have a new entity to manage: the \nprompt. Thus, we have to find specific ways to log and analyze them.\nMost ML platforms, such as Opik (from Comet ML) and W&B, or other specialized tools like Lang-\nfuse, have implemented logging tools to debug and monitor prompts. While in production, using \nthese tools, you usually want to track the user input, the prompt templates, the input variables, \nthe generated response, the number of tokens, and the latency.\nWhen generating an answer with an LLM, we don’t wait for the whole answer to be generated; we \nstream the output token by token. This makes the entire process snappier and more responsive. \nThus, when it comes to tracking the latency of generating an answer, the final user experience \nmust look at this from multiple perspectives, such as:\n•\nTime to First Token (TTFT): The time it takes for the first token to be generated\n•\nTime between Tokens (TBT): The interval between each token generation\n•\nTokens per Second (TPS): The rate at which tokens are generated\n•\nTime per Output Token (TPOT): The time it takes to generate each output token\n•\nTotal Latency: The total time required to complete a response\n\n\nMLOps and LLMOps\n414\nAlso, tracking the total input and output tokens is critical to understanding the costs of hosting \nyour LLMs.\nUltimately, you can compute metrics that validate your model’s performance for each input, \nprompt, and output tuple. Depending on your use case, you can compute things such as accuracy, \ntoxicity, and hallucination rate. When working with RAG systems, you can also compute metrics \nrelative to the relevance and precision of the retrieved context.\nAnother essential thing to consider when monitoring prompts is to log their full traces. You might \nhave multiple intermediate steps from the user query to the final general answer. For example, \nrewriting the query to improve the RAG’s retrieval accuracy evolves one or more intermediate steps. \nThus, logging the full trace reveals the entire process from when a user sends a query to when \nthe final response is returned, including the actions the system takes, the documents retrieved, \nand the final prompt sent to the model. Additionally, you can log the latency, tokens, and costs \nat each step, providing a more fine-grained view of all the steps. \nFigure 11.4: Example trace in the Langfuse UI\n\n\nChapter 11\n415\nAs shown in Figure 11.4, the end goal is to trace each step from the user’s input until the generated \nanswer. If something fails or behaves unexpectedly, you can point exactly to the faulty step. The \nquery can fail due to an incorrect answer, an invalid context, or incorrect data processing. Also, \nthe application can behave unexpectedly if the number of generated tokens suddenly fluctuates \nduring specific steps.\nTo conclude, LLMOps is a rapidly developing field. Given its quick evolution, making predictions \nis challenging. The truth is that we are not sure if the term LLMOps is here to stay. However, \nwhat is certain is that numerous new use cases for LLMs will emerge, along with tools and best \npractices to manage their lifecycle.\nEven if this DevOps, MLOps, and LLMOps section is far from comprehensive, it provides a strong \nidea of how to apply best ops practices in our LLM Twin use case.\nDeploying the LLM Twin’s pipelines to the cloud\nThis section will show you how to deploy all the LLM Twin’s pipelines to the cloud. We must deploy \nthe entire infrastructure to have the whole system working in the cloud. Thus, we will have to:\n1.\nSet up an instance of MongoDB serverless.\n2.\nSet up an instance of Qdrant serverless.\n3.\nDeploy the ZenML pipelines, container, and artifact registry to AWS.\n4.\nContainerize the code and push the Docker image to a container registry.\nNote that the training and inference pipelines already work with AWS SageMaker. Thus, by fol-\nlowing the preceding four steps, we ensure that our whole system is on the cloud, ready to scale \nand serve our imaginary clients.\nWhat are the deployment costs?\nWe will stick to the free versions of the MongoDB, Qdrant, and ZenML services. As \nfor AWS, we will mostly stick to their free tier for running the ZenML pipelines. The \nSageMaker training and inference components are more costly to run (which we \nwon’t run in this section). Thus, what we will show you in the following sections \nwill generate minimum costs (a few dollars at most) from AWS.\n\n\nMLOps and LLMOps\n416\nUnderstanding the infrastructure\nBefore diving into the step-by-step tutorial, where we will show you how to set up all the nec-\nessary components, let’s briefly overview our infrastructure and how all the elements interact. \nThis will help us in mindfully following the tutorials below.\nAs shown in Figure 11.5, we have a few services to set up. To keep things simple, for MongoDB and \nQdrant, we will leverage their serverless freemium version. As for ZenML, we will leverage the \nfree trial of the ZenML cloud, which will help us orchestrate all the pipelines in the cloud. How \nwill it do that?\nBy leveraging the ZenML cloud, we can quickly allocate all the required AWS resources to run, scale, \nand store the ML pipeline. It will help us spin up, with a few clicks, the following AWS components:\n• \nAn ECR service for storing Docker images\n• \nAn S3 object storage for storing all our artifacts and models\n• \nSageMaker Orchestrator for orchestrating, running, and scaling all our ML pipelines\n",
      "page_number": 430
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 438-445)",
      "start_page": 438,
      "end_page": 445,
      "detection_method": "topic_boundary",
      "content": "Chapter 11\n417\nFigure 11.5: Infrastructure flow\nNow that we understand what the essential resources of our infrastructure are, let’s look over the \ncore flow of running a pipeline in the cloud that we will learn to implement, presented in Figure 11.5:\n1.\nBuild a Docker image that contains all the system dependencies, the project dependencies, \nand the LLM Twin application.\n2.\nPush the Docker image to ECR, where SageMaker can access it.\n\n\nMLOps and LLMOps\n418\n3.\nNow, we can trigger any pipeline implemented during this book either from the CLI of \nour local machine or ZenML’s dashboard.\n4.\nEach step from ZenML’s pipeline will be mapped to a SageMaker job that runs on an AWS \nEC2 virtual machine (VM). Based on the dependencies between the directed acyclic \ngraph (DAG) steps, some will run in parallel and others sequentially.\n5. \nWhen running a step, SageMaker pulls the Docker image from ECR, defined in step 2. \nBased on the pulled image, it creates a Docker container that executes the pipeline step.\n6. As the job is executed, it can access the S3 artifact storage, MongoDB, and Qdrant vector \nDB to query or push data. The ZenML dashboard is a key tool, providing real-time updates \non the pipeline’s progress and ensuring a clear view of the process.\nNow that we know how the infrastructure works, let’s start by setting up MongoDB, Qdrant, and \nthe ZenML cloud.\nSetting up MongoDB\nWe will show you how to create and integrate a free MongoDB cluster into our projects. To do so, \nthese are the steps you have to follow:\n1.\nGo to their site at https://www.mongodb.com and create an account.\n2.\nIn the left panel, go to Deployment | Database and click Build a Cluster.\n3.\nWithin the creation form, do the following:\na.\nChoose an M0 Free cluster.\nb.\nCall your cluster twin.\nc.\nChoose AWS as your provider.\nWhat AWS cloud region should I choose?\nIn our tutorials, all the services will be deployed to AWS within the Frankfurt \n(eu-central-1) region. You can select another region, but be consistent across all the \nservices to ensure faster responses between components and reduce potential errors.\nHow should I manage changes in the services’ UIs?\nUnfortunately, MongoDB, Qdrant, or other services may change their UI or naming \nconventions. As we can’t update this book each time that happens, please refer to \ntheir official documentation to check anything that differs from our tutorial. We \napologize for this inconvenience, but unfortunately, it is not in our control.\n\n\nChapter 11\n419\nd.\nChoose Frankfurt (eu-central-1) as your region. You can choose another region, \nbut be careful to choose the same region for all future AWS services.\ne.\nLeave the rest of the attributes with their default values.\nf.\nIn the bottom right, click the Create Deployment green button.\n4. \nTo test that your newly created MongoDB cluster works fine, we must connect to it from \nour local machine. We used the MongoDB VS Code extension to do so, but you can use any \nother tool. Thus, from their Choose a connection method setup flow, choose MongoDB \nfor VS Code. Then, follow the steps provided on their site.\n5. \nTo connect, you must paste the DB connection URL in the VS Code extension (or another \ntool of your liking), which contains your username, password, and cluster URL, similar \nto this one: mongodb+srv://<username>:<password> @twin.vhxy1.mongodb.net. Make \nsure to save this URL somewhere you can copy it from later.\n6.\nIf you don’t know or want to change your password, go to Security → Quickstart in the \nleft panel. There, you can edit your login credentials. Be sure to save them somewhere \nsafe, as you won’t be able to access them later.\n7. \nAfter verifying that your connections work, go to Security → Network Access in the left \npanel and click ADD IP ADDRESS. Then click ALLOW ACCESS FROM ANYWHERE and \nhit Confirm. Out of simplicity, we allow any machine from any IP to access our MongoDB \ncluster. This ensures that our pipelines can query or write to the DB without any addi-\ntional complex networking setup. It’s not the safest option for production, but for our \nexample, it’s perfectly fine.\n8.\nThe final step is to return to your project and open your .env file. Now, either add or re-\nplace the DATABASE_HOST variable with your MongoDB connection string. It should look \nsomething like this: DATABASE_HOST= mongodb+srv://<username>:<password> @twin.\nvhxy1.mongodb.net.\nThat’s it! Now, instead of reading and writing from your local MongoDB, you will do it from the \ncloud MongoDB cluster we just created. Let’s repeat a similar process with Qdrant.\nSetting up Qdrant\nWe have to repeat a similar process to what we did for MongoDB. Thus, to create a Qdrant cluster \nand hook it to our project, follow these steps:\n1. \nGo to Qdrant at https://cloud.qdrant.io/ and create an account.\n2. \nIn the left panel, go to Clusters and click Create.\n\n\nMLOps and LLMOps\n420\n3. \nFill out the cluster creation form with the following:\na.\nChoose the Free version of the cluster.\nb.\nChoose GCP as the cloud provider (while writing the book, it was the only one \nallowed for a free cluster).\nc.\nChoose Frankfurt as the region (or the same region as you chose for MongoDB).\nd.\nName the cluster twin.\ne.\nLeave the rest of the attributes with their default values and click Create.\n4. \nAccess the cluster in the Data Access Control section in the left panel.\n5.\nClick Create and choose your twin cluster to create a new access token. Copy the newly \ncreated token somewhere safe, as you won’t be able to access it anymore.\n6.\nYou can run their example from Usage Examples to test that your connection works fine.\n7.\nGo back to the Clusters section of Qdrant and open your newly created twin cluster. You \nwill have access to the cluster’s endpoint, which you need to configure Qdrant in your code.\nYou can visualize your Qdrant collections and documents by clicking Open Dashboard and en-\ntering your API Key as your password. The Qdrant cluster dashboard will now be empty, but after \nrunning the pipelines, you will see all the collections, as shown here:\nFigure 11.6: Qdrant cluster dashboard example after being populated with two collections.\nFinally, return to your project and open your .env file. Now, we must fill in a couple of environ-\nment variables as follows:\nUSE_QDRANT_CLOUD=true\nQDRANT_CLOUD_URL=<the endpoint URL found at step 7>\nQDRANT_APIKEY=<the access token created at step 5>\n\n\nChapter 11\n421\nThat’s it! Instead of reading and writing from your local Qdrant vector DB, you will do it from the \ncloud Qdrant cluster we just created. Just to be sure that everything works fine, run the end-to-\nend data pipeline with the cloud version of MongoDB and Qdrant as follows:\npeotry poe run-end-to-end-data-pipeline\nThe last step is setting up the ZenML cloud and deploying all our infrastructure to AWS.\nSetting up the ZenML cloud\nSetting up the ZenML cloud and the AWS infrastructure is a multi-step process. First, we will set \nup a ZenML cloud account, then the AWS infrastructure through the ZenML cloud, and, finally, \nwe will bundle our code in a Docker image to run it in AWS SageMaker.\nLet’s start with setting up the ZenML cloud:\n1. \nGo to the ZenML cloud at https://cloud.zenml.io and make an account. They provide \na seven-day free trial, which is enough to run our examples.\n2. \nFill out their onboarding form and create an organization with a unique name and a tenant \ncalled twin. A tenant refers to a deployment of ZenML in a fully isolated environment. \nWait a few minutes until your tenant server is up before proceeding to the next step.\n3.\nIf you want to, you can go through their Quickstart Guide to understand how the ZenML \ncloud works with a simpler example. It is not required to go through it to deploy the LLM \nTwin application, but we recommend it to ensure everything works fine.\n4. \nAt this point, we assume that you have gone through the Quickstart Guide. Otherwise, \nyou might encounter issues during the next steps. To connect our project with this Zen-\nML cloud tenant, return to the project and run the zenml connect command provided \nin the dashboard. It looks similar to the following example but with a different URL: \nzenml connect --url https://0c37a553-zenml.cloudinfra.zenml.io.\n5.\nTo ensure everything works fine, run a random pipeline from your code. Note that at this \npoint, we are still running it locally, but instead of logging the results to the local server, \nwe log everything to the cloud version:\npoetry poe run-digital-data-etl\n6. \nGo to the Pipelines section in the left panel of the ZenML dashboard. If everything worked \nfine, you should see the pipeline you ran in Step 5 there.\n\n\nMLOps and LLMOps\n422\nTo ship the code to AWS, you must create a ZenML stack. A stack is a set of components, such \nas the underlying orchestrator, object storage, and container registry, that ZenML needs under \nthe hood to run the pipelines. Intuitively, you can see your stack as your infrastructure. While \nworking locally, ZenML offers a default stack that allows you to quickly develop your code and \ntest things locally. However, by defining different stacks, you can quickly switch between different \ninfrastructure environments, such as local and AWS runs, which we will showcase in this section.\nWith that in mind, let’s create an AWS stack for our project. To do so, follow the next steps:\n1. \nIn the left panel, click on the Stacks section and hit the New Stack button.\n2. \nYou will have multiple options for creating a stack, but the easiest is creating one from \nscratch within the in-browser experience, which doesn’t require additional preparations. \nThis is not very flexible, but it is enough to host our project. Thus, choose Create New \nInfrastructure → In-browser Experience.\n3. \nThen, choose AWS as your cloud provider.\n4.\nChoose Europe (Frankfurt)—eu-central-1 as your location or the region you used to set \nup MongoDB and Qdrant.\n5.\nName it aws-stack. It is essential to name it exactly like this so that the commands that \nwe will use work.\n6.\nNow ZenML will create a set of IAM roles to give permissions to all the other components \nto communicate with each other, an S3 bucket as your artifact storage, an ECR repository \nas your container registry, and SageMaker as your orchestrator.\n7.\nClick Next.\nEnsure that your ZenML server version matches your local ZenML version. For ex-\nample, when we wrote this book, both were version 0.64.0. If they don’t match, you \nmight encounter strange behavior, or it might not work correctly. The easiest fix is to \ngo to your pyproject.toml file, find the zenml dependency, and update it with the \nversion of your server. Then run poetry lock --no-update && poetry install\nto update your local virtual environment.\nBefore starting this section, ensure you have an AWS account with admin permis-\nsions ready.\n\n\nChapter 11\n423\n8. Click the Deploy to AWS button. It will open a CloudFormation page on AWS. ZenML \nleverages CloudFormation (an infrastructure as code, or IaC, tool) to create all the AWS \nresources we enumerated in Step 6.\n9. At the bottom, check all the boxes to acknowledge that AWS CloudFormation will create \nAWS resources on your behalf. Finally, click the Create stack button. Now, we must wait \nfor a couple of minutes for AWS CloudFormation to spin up all the resources.\n10. Return to the ZenML page and click the Finish button.\nBy leveraging ZenML, we efficiently deployed the entire AWS infrastructure for our ML \npipelines. We began with a basic example, sacrificing some control. However, if you seek \nmore control, ZenML offers the option to use Terraform (an IaC tool) to fully control your \nAWS resources or to connect ZenML with your current infrastructure.\nBefore moving to the next step, let’s have a quick recap of the AWS resources we just \ncreated:\n• \nAn IAM role is an AWS identity with permissions policies that define what actions \nare allowed or denied for that role. It is used to grant access to AWS services with-\nout needing to share security credentials.\n•\nS3 is a scalable and secure object storage service that allows storing and retrieving \nfiles from anywhere on the web. It is commonly used for data backup, content \nstorage, and data lakes. It’s more scalable and flexible than Google Drive.\n•\nECR is a fully managed Docker container registry that makes storing, managing, \nand deploying Docker container images easy.\n•\nSageMaker is a fully managed service that allows developers and data scientists \nto quickly build, train, and deploy ML models.\n•\nSageMaker Orchestrator is a feature of SageMaker that helps automate the ex-\necution of ML workflows, manage dependencies between steps, and ensure the \nreproducibility and scalability of model training and deployment pipelines. Other \nsimilar tools are Prefect, Dagster, Metaflow, and Airflow.\n•\nCloudFormation is a service that allows you to model and set up your AWS re-\nsources so that you can spend less time managing them and more time focusing \non your applications. It automates the process of provisioning AWS infrastructure \nusing templates.\nBefore running the ML pipelines, the last step is to containerize the code and prepare a Docker \nimage that packages our dependencies and code.\n\n\nMLOps and LLMOps\n424\nContainerize the code using Docker\nSo far, we have defined our infrastructure, MongoDB, Qdrant, and AWS, for storage and computing. \nThe last step is to find a way to take our code and run it on top of this infrastructure. The most \npopular solution is Docker, a tool that allows us to create an isolated environment (a container) \nthat contains everything we need to run our application, such as system dependencies, Python \ndependencies, and the code.\nWe defined our Docker image at the project’s root in the Dockerfile. This is the standard naming \nconvention for Docker. Before digging into the code, if you want to build the Docker image your-\nself, ensure that you have Docker installed on your machine. If you don’t have it, you can install it \nby following the instructions provided here: https://docs.docker.com/engine/install. Now, \nlet’s look at the content of the Dockerfile step by step.\nThe Dockerfile begins by specifying the base image, which is a lightweight version of Python 3.11 \nbased on the Debian Bullseye distribution. The environment variables are then set up to configure \nvarious aspects of the container, such as the workspace directory, turning off Python bytecode \ngeneration, and configuring Python to output directly to the terminal. Additionally, the version of \nPoetry to be installed is specified, and a few environment variables are set to ensure that package \ninstallations are non-interactive, which is vital for automated builds.\nFROM python:3.11-slim-bullseye AS release\nENV WORKSPACE_ROOT=/app/\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\nENV POETRY_VERSION=1.8.3\nENV DEBIAN_FRONTEND=noninteractive\nENV POETRY_NO_INTERACTION=1\nNext, we install Google Chrome in the container. The installation process begins by updating the \npackage lists and installing essential tools like gnupg, wget, and curl. The Google Linux signing \nkey is added, and the Google Chrome repository is configured. After another package list update, \nthe stable version of Google Chrome is installed. The package lists are removed after installation \nto keep the image as small as possible.\nRUN apt-get update -y && \\\n    apt-get install -y gnupg wget curl --no-install-recommends && \\\n    wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | \ngpg --dearmor -o /usr/share/keyrings/google-linux-signing-key.gpg && \\\n",
      "page_number": 438
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 446-455)",
      "start_page": 446,
      "end_page": 455,
      "detection_method": "topic_boundary",
      "content": "Chapter 11\n425\n    echo \"deb [signed-by=/usr/share/keyrings/google-linux-signing-key.gpg] \nhttps://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.\nlist.d/google-chrome.list && \\\n    apt-get update -y && \\\n    apt-get install -y google-chrome-stable && \\\n    rm -rf /var/lib/apt/lists/*\nFollowing the Chrome installation, other essential system dependencies are installed. Once these \npackages are installed, the package cache is cleaned up to reduce the image size further.\nRUN apt-get update -y \\\n    && apt-get install -y --no-install-recommends build-essential \\\n    gcc \\\n    python3-dev \\\n    build-essential \\\n    libglib2.0-dev \\\n    libnss3-dev \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\nPoetry, the dependency management tool, is then installed using pip. The --no-cache-dir option \nprevents pip from caching packages, helping to keep the image smaller. After installation, Poetry \nis configured to use up to 20 parallel workers when installing packages, which can speed up the \ninstallation process.\nRUN pip install --no-cache-dir \"poetry==$POETRY_VERSION\"\nRUN poetry config installer.max-workers 20\nThe working directory inside the container is set to WORKSPACE_ROOT, which defaults to /app/, \nwhere the application code will reside. The pyproject.toml and poetry.lock files define the \nPython’s project dependencies and are copied into this directory.\nWORKDIR $WORKSPACE_ROOT\nCOPY pyproject.toml poetry.lock $WORKSPACE_ROOT\nWith the dependency files in place, the project’s dependencies are installed using Poetry. The \nconfiguration turns off the creation of a virtual environment, meaning the dependencies will be \ninstalled directly into the container’s Python environment. The installation excludes development \ndependencies and prevents caching to minimize space usage. \n\n\nMLOps and LLMOps\n426\nAdditionally, the poethepoet plugin is installed to help manage tasks within the project. Finally, \nany remaining Poetry cache is removed to keep the container as lean as possible.\nRUN poetry config virtualenvs.create false && \\\n    poetry install --no-root --no-interaction --no-cache --without dev && \n\\\n    poetry self add 'poethepoet[poetry_plugin]' && \\\n    rm -rf ~/.cache/pypoetry/cache/ && \\\n    rm -rf ~/.cache/pypoetry/artifacts/\nIn the final step, the entire project directory from the host machine is copied into the container’s \nworking directory. This step ensures that all the application files are available within the container.\nOne important trick when writing a Dockerfile is to decouple your installation steps from copy-\ning the rest of the files. This is useful because each Docker command is cached and layered on \ntop of each other. Thus, whenever you change one layer when rebuilding the Docker image, all \nthe layers below the one altered are executed again. Because you rarely change your system and \nproject dependencies but mostly change your code, copying your project files in the last step makes \nrebuilding Docker images fast by taking advantage of the caching mechanism’s full potential.\nCOPY . $WORKSPACE_ROOT\nThis Dockerfile is designed to create a clean, consistent Python environment with all necessary \ndependencies. It allows the project to run smoothly in any environment that supports Docker.\nThe last step is to build the Docker image and push it to the ECR created by ZenML. To build the \nDocker image from the root of the project, run the following:\ndocker buildx build --platform linux/amd64 -t llmtwin -f Dockerfile .\nWe must build it on a Linux platform as the Google Chrome installer we used inside Docker works \nonly on a Linux machine. Even if you use a macOS or Windows machine, Docker can emulate a \nvirtual Linux container.\nThe tag of the newly created Docker image is llmtwin. We also provide this build command \nunder a poethepoet command:\npoetry poe build-docker-image\nNow, let’s push the Docker image to ECR. To do so, navigate to your AWS console and then to \nthe ECR service. From there, find the newly created ECR repository. It should be prefixed with \nzenml-*, as shown here:\n\n\nChapter 11\n427\nFigure 11.7: AWS ECR example\nThe first step is to authenticate to ECR. For this to work, ensure that you have the AWS CLI installed \nand configured with your admin AWS credentials, as explained in Chapter 2:\nAWS_REGION=<your_region> # e.g. AWS_REGION=eu-central-1\nAWS_ECR_URL=<your_acount_id>\naws ecr get-login-password --region ${AWS_REGION}| docker login --username \nAWS --password-stdin ${AWS_ECR_URL}\nYou can get your current AWS_REGION by clicking on the toggle in the top-right corner, as seen in \nFigure 11.8. Also, you can copy the ECR URL to fill the AWS_ECR_URL variable from the main AWS \nECR dashboard, as illustrated in Figure 11.7. After running the previous command, you should see \nthe message Login Succeeded on the CLI.\nFigure 11.8: AWS region and account details\n\n\nMLOps and LLMOps\n428\nNow we have to add another tag to the llmtwin Docker image that signals the Docker registry \nwe want to push it to:\ndocker tag llmtwin ${AWS_ECR_URL}:latest\nFinally, we push it to ECR by running:\ndocker push ${AWS_ECR_URL}:latest\nAfter the upload is finished, return to your AWS ECR dashboard and open your ZenML repository. \nThe Docker image should appear, as shown here:\nFigure 11.9: AWS ECR repository example after the Docker image is pushed\nFor every change in the code that you need to ship and test, you would have to go through all \nthese steps, which are tedious and error-prone. The Adding LLMOps to the LLM Twin section of this \nchapter will teach us how to automate these steps within the CD pipeline using GitHub Actions. \nStill, we first wanted to go through them manually to fully understand the behind-the-scenes \nprocess and not treat it as a black box. Understanding these details is vital for debugging your CI/\nCD pipelines, where you must understand the error messages and how to fix them.\nNow that we have built our Docker image and pushed it to AWS ECR, let’s deploy it to AWS.\nRun the pipelines on AWS\nWe are very close to running the ML pipelines on AWS, but we have to go through a few final steps. \nLet’s switch from the default ZenML stack to the AWS one we created in this chapter. From the \nroot of your project, run the following in the CLI:\nzenml stack set aws-stack\n\n\nChapter 11\n429\nReturn to your AWS ECR ZenML repository and copy the image URI as shown in Figure 11.9. \nThen, go to the configs directory, open the configs/end_to_end_data.yaml file, and update \nthe settings.docker.parent_image attribute with your ECR URL, as shown below:\nsettings:\n  docker:\n    parent_image: <YOUR ECR URL> #e.g., 992382797823.dkr.ecr.eu-central-1.\namazonaws.com/zenml-rlwlcs:latest\n    skip_build: True\nWe’ve configured the pipeline to always use the latest Docker image available in ECR. This means \nthat the pipeline will automatically pick up the latest changes made to the code whenever we \npush a new image.\nWe must export all the credentials from our .env file to ZenML secrets, a feature that safely stores \nyour credentials and makes them accessible within your pipelines:\npoetry poe export-settings-to-zenml\nThe last step is setting up to run the pipelines asynchronously so we don’t have to wait until they \nare finished, which might result in timeout errors:\nzenml orchestrator update aws-stack --synchronous=False\nNow that ZenML knows to use the AWS stack, our custom Docker image, and has access to our \ncredentials, we are finally done with the setup. Run the end-to-end-data-pipeline with the \nfollowing command:\npoetry poe run-end-to-end-data-pipeline\nNow you can go to ZenML Cloud → Pipelines → end_to_end_data and open the latest run. On \nthe ZenML dashboard, you can visualize the latest state of the pipeline, as seen in Figure 11.10. \nNote that this pipeline runs all the data-related pipelines in a single run.\n\n\nMLOps and LLMOps\n430\nIn the Adding LLMOps to the LLM Twin section, we will explain why we compressed all the steps \ninto a single pipeline.\nFigure 11.10: ZenML example of running the end-to-end-data-pipeline\nYou can click on any running block and find details about the run, the code used for that specific \nstep, and the logs for monitoring and debugging, as illustrated in Figure 11.11:\n\n\nChapter 11\n431\nFigure 11.11: ZenML step metadata example\nTo find even more details about the runs, you can go to AWS SageMaker. In the left panel, click \nSageMaker dashboard, and on the right, in the Processing column, click on the green Running\nsection, as shown in Figure 11.12. \nTo run other pipelines, you have to update the settings.docker.parent_image\nattribute in their config file under the configs/ directory.\n\n\nMLOps and LLMOps\n432\nThis will open a list of all the processing jobs that execute your ZenML pipelines.\nFigure 11.12: SageMaker dashboard\nTroubleshooting the ResourceLimitExceeded error after running \na ZenML pipeline on SageMaker\nLet’s assume, you’ve encountered a ResourceLimitExceeded error after running a ZenML pipeline \non SageMaker using the AWS stack. In this case, you have to explicitly ask AWS to give you access \nto a specific type of AWS EC2 VM.\nZenML uses, by default, ml.t3.medium EC2 machines, which are part of the AWS freemium tier. \nHowever, some AWS accounts cannot access these VMs by default. To check your access, search \nyour AWS console for Service Quotas. \nIf you want to run the pipelines locally again, use the following CLI command:\npoetry poe set-local-stack\nIf you want to disconnect from the ZenML cloud dashboard and use the local version \nagain, run the following:\nzenml disconnect\n\n\nChapter 11\n433\nThen, in the left panel, click on AWS services, search for Amazon SageMaker, and then for ml.t3.\nmedium. In Figure 11.13, you can see our quotas for these types of machines. If yours is 0, you should \nrequest that AWS increase them to numbers similar to those from Figure 11.13 in the Applied \naccount-level quota value column. The whole process is free of charge and only requires a few \nclicks. Unfortunately, you might have to wait for a few hours up to one day until AWS accepts \nyour request.\nFigure 11.13: SageMaker—ml.t3.medium expected quotas\nYou can find step-by-step instructions on how to solve this error and request new quotas at this \nlink: https://repost.aws/knowledge-center/sagemaker-resource-limit-exceeded-error.\nIf you changed the values from your .env file and want to update the ZenML secrets \nwith them, first run the following CLI command to delete the old secrets:\npoetry poe delete-settings-zenml\nThen, you can export them again by running:\npoetry poe export-settings-to-zenml\n\n\nMLOps and LLMOps\n434\nAdding LLMOps to the LLM Twin\nIn the previous section, we saw how to set up the infrastructure for the LLM Twin project by \nmanually building the Docker image and pushing it to ECR. We want to automate the entire \nprocess and implement a CI/CD pipeline using GitHub Actions and a CT pipeline using ZenML. \nAs mentioned earlier, implementing a CI/CD/CT pipeline ensures that each feature pushed to \nmain branches is consistent and tested. Also, by automating the deployment and training, you \nsupport collaboration, save time, and reduce human errors.\nFinally, at the end of the section, we will show you how to implement a prompt monitoring pipe-\nline using Opik from Comet ML and an alerting system using ZenML. This prompt monitoring \npipeline will help us debug and analyze the RAG and LLM logic. As LLM systems are non-de-\nterministic, capturing and storing the prompt traces is essential for monitoring your ML logic.\nBefore diving into the implementation, let’s start with a quick section on the LLM Twin’s CI/CD \npipeline flow.\nLLM Twin’s CI/CD pipeline flow\nWe have two environments: staging and production. When developing a new feature, we cre-\nate a new branch out of the staging branch and develop solely on that one. When we are done \nand consider the feature finished, we open a pull request (PR) to the staging branch. After the \nfeature branch is accepted, it is merged into the staging branch. This is a standard workflow in \nmost software applications. There might be variations, like adding a dev environment, but the \nprinciples remain the same.\nAs illustrated in Figure 11.14, the CI pipeline is triggered when the PR opens. At this point, we \ntest the feature branch for linting and formatting errors. Also, we run a gitleaks command to \ncheck for credentials and sensitive information that was committed by mistake. If the linting, \nformatting, and gitleaks steps pass (also known as static analysis), we run the automated tests. \nNote that the static analysis steps run faster than the automated tests. Thus, the order matters. \nThat’s why adding the static analysis steps at the beginning of the CI pipeline is good practice. \nWe propose the following order of the CI steps:\n•\ngitleaks checks\n•\nLinting checks\n•\nFormatting checks\n• \nAutomated testing, such as unit and integration tests\n",
      "page_number": 446
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 456-464)",
      "start_page": 456,
      "end_page": 464,
      "detection_method": "topic_boundary",
      "content": "Chapter 11\n435\nIf any check fails, the CI pipeline fails, and the developer who created the PR cannot merge it into \nthe staging branch until it fixes the issues.\nImplementing a CI pipeline ensures that new features follow the repository’s standards and \ndon’t break existing functionality. The exact process repeats when we plan to merge the staging \nbranch into the production one. We open a PR, and the CI pipeline is automatically executed \nbefore merging the staging branch into production.\nFigure 11.14: CI/CD pipelines flow\nThe CD pipeline runs after the branch is merged. For example, after the feature branch is merged \ninto staging, the CD pipeline takes the code from the staging branch, builds a new Docker im-\nage, and pushes it to the AWS ECR Docker repository. When running future pipeline runs in \nthe staging environment, it will use the latest Docker image that was built by the CD pipeline. \nThe exact process happens between staging and production. Still, the key difference is that the \nstaging environment exists as an experimental place where the QA team and stakeholders can \nfurther manually test the new feature along with what is automatically tested in the CI pipeline.\n\n\nMLOps and LLMOps\n436\nMore on formatting errors\nFormatting errors relate to the style and structure of your code, ensuring that it adheres to a \nconsistent visual layout. This can include the placement of spaces, indentation, line length, and \nother stylistic elements.\nThe main purpose of formatting is to make your code more readable and maintainable. Consistent \nformatting helps teams work together more effectively, as the code looks uniform, regardless of \nwho wrote it. Examples of formatting errors are:\n•\nIncorrect indentation (e.g., mixing spaces and tabs)\n•\nLines that are too long (e.g., exceeding 79 or 88 characters, depending on your style guide)\n•\nMissing or extra spaces around operators or after commas\nMore on linting errors\nLinting errors relate to potential issues in your code that could lead to bugs, inefficiencies, or \nnon-adherence to coding standards beyond just style. Linting checks often involve static analysis \nof the code to catch things like unused variables, undefined names, or questionable practices.\nLinting’s main goal is to catch potential errors or bad practices early in the development process, \nimproving code quality and reducing the likelihood of bugs. Examples of linting errors are:\n•\nUnused imports or variables\n•\nUndefined variables or functions are being used\n•\nPotentially dangerous code (e.g., using == instead of is for checking against None)\nWe use Ruff, a versatile tool for formatting and linting. It incorporates checks for common for-\nmatting issues and PEP 8 compliance, as well as deeper linting checks for potential errors and \ncode quality problems. Also, it is written in Rust, making it fast for big codebases.\nBefore implementing what we’ve explained above, let’s examine the core principles of GitHub \nActions.\nIn our repository, we used only a main branch, which reflects production, and fea-\nture branches to push new work. We did this to keep things simple, but the same \nprinciples apply. To extend the flow, you must create a staging branch and add it \nto the CD pipeline.\n\n\nChapter 11\n437\nQuick overview of GitHub Actions\nGitHub Actions is a CI/CD platform provided by GitHub that allows developers to automate their \nworkflows directly within a GitHub repository. It enables users to build, test, and deploy their \ncode directly from GitHub by defining workflows in YAML files. Since it’s part of GitHub, it works \nseamlessly with repositories, issues, PRs, and other GitHub features. Here are the key components \nyou should know about:\n• \nWorkflows: A workflow is an automated process defined in a YAML file located in your \nrepository’s .github/workflows directory. It specifies what should happen (e.g., build, \ntest, and deploy) and when (e.g., on push, on PR).\n•\nJobs: Workflows are made up of jobs, which are groups of steps that execute on the same \nrunner. Each job runs in its own virtual environment.\n•\nSteps: Jobs are made up of multiple independent steps, which can be actions or shell \ncommands.\n• \nActions: Actions are reusable commands or scripts. You can use pre-built actions from \nGitHub Marketplace or create your own. You can think of them as Python functions.\n• \nRunners: Runners are the servers that run your jobs. GitHub provides hosted runners \n(Linux, Windows, macOS), or you can even self-host your runners.\nA workflow is described using YAML syntax. For example, a simple workflow that clones the \ncurrent GitHub repository and installs Python 3.11 on an Ubuntu machine looks like this:\nname: Example\non: [push]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n         - name: Checkout\n           uses: actions/checkout@v3\n         - name: Setup Python\n           uses: actions/setup-python@v3\n           with:\n               python-version: \"3.11\"\n\n\nMLOps and LLMOps\n438\nThe workflows are triggered by events like push, pull_request, or schedule. For example, you \nmight trigger a workflow every time code is pushed to a specific branch. Now that we understand \nhow GitHub Actions works, let’s look at the LLM Twin’s CI pipeline.\nThe CI pipeline\nThe LLM Twin’s CI pipeline is split into two jobs:\n• \nA QA job that looks for formatting and linting errors using Ruff. Also, it runs a gitleaks\nstep to scan for leaked secrets throughout our repository.\n• \nA test job that runs all our automatic tests using Pytest. In our use case, we implemented \njust a dummy test to showcase the CI pipeline, but using the structure from this book, \nyou can easily extend it with real tests for your use case.\nGitHub Actions CI YAML file\nThe YAML file sits under .github/workflows/ci.yaml. It begins by defining the workflow’s \nname as CI, as you can see in the following snippet. This label will be used to identify the work-\nflow within GitHub’s Actions interface. Next, the section specifies that the workflow should be \ntriggered whenever a pull_request event occurs. Hence, the CI workflow will automatically run \nwhenever a PR is opened, synchronized, or reopened.\nname: CI\non:\n  pull_request:\nThe concurrency section ensures that only one instance of this workflow runs for a given reference \n(like a branch) at any given time. The group field is defined using GitHub’s expression syntax to \ncreate a unique group name based on the workflow and the reference. The cancel-in-progress: \ntrue line ensures that if a new workflow run is triggered before the previous one finishes, the \nprevious run is canceled. This is particularly useful to prevent redundant executions of the same \nworkflow.\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\nThe workflow defines two separate jobs: qa and test. Each job runs on the latest version of Ubuntu, \nspecified by runs-on: ubuntu-latest.\n\n\nChapter 11\n439\nThe first job, named QA, is responsible for quality assurance tasks like code checks and format-\nting verification. Within the qa job, the first step is to check out the repository’s code using the \nactions/checkout@v3 action. This step is necessary to ensure that the job has access to the code \nthat needs to be analyzed.\njobs:\n  qa:\n    name: QA\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\nThe next step is to set up the Python environment. This is done using the actions/setup-python@\nv3 action, with the Python version specified as \"3.11\". This step ensures that the subsequent \nsteps in the job will run in the correct Python environment.\n      - name: Setup Python\n        uses: actions/setup-python@v3\n        with:\n          python-version: \"3.11\"\nThe workflow then installs Poetry using the abatilo/actions-poetry@v2 action, specifying the \nversion of Poetry as 1.8.3:\n      - name: Install poetry\n        uses: abatilo/actions-poetry@v2\n        with:\n          poetry-version: 1.8.3\nOnce Poetry is set up, the workflow installs the project’s development dependencies using the \npoetry install --only dev command. Additionally, the workflow adds the poethepoet plugin \nfor Poetry, which will be used to run predefined tasks more conveniently within the project.\n      - name: Install packages\n        run: |\n          poetry install --only dev\n          poetry self add 'poethepoet[poetry_plugin]'\n\n\nMLOps and LLMOps\n440\nThe qa job then runs several quality checks on the code. The first check uses a tool called gitleaks\nto scan for secrets in the codebase, ensuring that no sensitive information is accidentally com-\nmitted:\n      - name: gitleaks check\n        run: poetry poe gitleaks-check\nFollowing the gitleaks check, the workflow runs a linting process to enforce coding standards \nand best practices in the Python code. This is achieved through the poetry poe lint-check\ncommand, which uses Ruff under the hood.\n      - name: Lint check [Python]\n        run: poetry poe lint-check\nThe last step in the qa job is a format check, which ensures that the Python code is properly for-\nmatted according to the project’s style guidelines. This is done using the poetry poe format-\ncheck command, which uses Ruff under the hood.\n      - name: Format check [Python]\n        run: poetry poe format-check\nThe second job defined in the workflow is the test job, which also runs on the latest version \nof Ubuntu. Like the qa job, it starts by checking out the code from the repository and installing \nPython 3.11 and Poetry 1.8.3.\n  test:\n    name: Test\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n      …\nAfter setting up the system dependencies, the test job installs all the project’s dependencies with \nthe poetry install command. As we want to run the tests, this time, we need to install all the \ndependencies that are required to run the application.\n      - name: Install packages\n        run: |\n\n\nChapter 11\n441\n          poetry install –-without aws\n          poetry self add 'poethepoet[poetry_plugin]'\nFinally, the test job runs the project’s tests using the poetry poe test command. This step \nensures that all tests are executed and provides feedback on whether the current code changes \nbreak any functionality.\n      - name: Run tests\n        run: |\n          echo \"Running tests...\"\n          poetry poe test\nIf any of the steps from the QA or test jobs fail, the GitHub Actions workflow will fail, resulting \nin the PR not being able to be merged until the issue is fixed. By taking this approach, we ensure \nthat all the new features added to the main branches respect the standard of the project and that \nit doesn’t break existing functionality through automated tests.\nFigure 11.15 shows the CI pipeline in the Actions tab of the GitHub repository. It was run after a \ncommit with the message feat: Add Docker image and CD pipeline and ran the two jobs de-\nscribed above, QA and Test.\nFigure 11.15: GitHub Actions CI pipeline run example\n\n\nMLOps and LLMOps\n442\nThe CD pipeline\nThe CD pipeline will automate the Docker steps we manually performed in the Deploying the \nLLM Twin’s pipelines to the cloud section, which are:\n•\nSet up Docker.\n•\nLog in to AWS.\n•\nBuild the Docker image.\n•\nPush the Docker image to AWS ECR.\nWith that in mind, let’s look at the GitHub Actions YAML file, which sits under .github/workflows/\ncd.yaml. It begins by naming the workflow CD and specifying the trigger for this workflow. The \ntrigger is any push to the repository’s main branch. This workflow will automatically run when \nnew code is pushed to the main branch, usually when a PR is merged into the main branch. The \non.push configuration sets up the trigger:\nname: CD\non:\n  push:\n    branches:\n      - main\nThe workflow then defines a single job named Build & Push Docker Image:\njobs:\n  build:\n    name: Build & Push Docker Image\n    runs-on: ubuntu-latest\nThe first step within the job is to check out the repository’s code.\nsteps:\n  - name: Checkout Code\n    uses: actions/checkout@v3\nAfter checking out the code, the workflow sets up docker buildx, a Docker CLI plugin that extends \nDocker’s build capabilities with features like multi-platform builds and cache import/export:\n- name: Set up Docker Buildx\n  uses: docker/setup-buildx-action@v3\n\n\nChapter 11\n443\nThe next step involves configuring the AWS credentials. This step is crucial for interacting with \nAWS services, such as Amazon Elastic Container Registry (ECR), where the Docker images will \nbe pushed. The AWS access key, secret access key, and region are securely retrieved from the \nrepository’s secrets to authenticate the workflow with AWS. This ensures the workflow has the \nnecessary permissions to push Docker images to the ECR repository. We will show you how to \nconfigure these secrets after wrapping up with the YAML file:\n- name: Configure AWS credentials\n  uses: aws-actions/configure-aws-credentials@v1\n  with:\n    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n    aws-region: ${{ secrets.AWS_REGION }}\nOnce the AWS credentials are configured, the workflow logs in to Amazon ECR. This step is es-\nsential for authenticating the Docker CLI with the ECR registry, allowing subsequent steps to \npush images to the registry:\n- name: Login to Amazon ECR\n  id: login-ecr\n  uses: aws-actions/amazon-ecr-login@v1\nThe final step in the workflow involves building the Docker image and pushing it to the Ama-\nzon ECR repository. This is accomplished using the docker/build-push-action@v6 action. The \ncontext specifies the build context, which is typically the repository’s root directory. The file\noption points to the Dockerfile, which defines how the image should be built. The tags section \nassigns tags to the image, including the specific commit SHA and the latest tag, which is a com-\nmon practice for identifying the most recent version of the image. The push option is set to true, \nmeaning the image will be uploaded to ECR after it is built:\n- name: Build images & push to ECR\n  id: build-image\n  uses: docker/build-push-action@v6\n  with:\n    context: .\n    file: ./Dockerfile\n    tags: |\n      ${{ steps.login-ecr.outputs.registry }}/${{ secrets.AWS_ECR_NAME \n}}:${{ github.sha }}\n",
      "page_number": 456
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 465-475)",
      "start_page": 465,
      "end_page": 475,
      "detection_method": "topic_boundary",
      "content": "MLOps and LLMOps\n444\n      ${{ steps.login-ecr.outputs.registry }}/${{ secrets.AWS_ECR_NAME \n}}:latest\n    push: true\nTo conclude, the CD pipeline authenticates to AWS, builds the Docker image, and pushes it to \nAWS ECR. The Docker image is pushed with latest and the commit’s SHA tag. By doing so, we \ncan always use the latest image and point to the commit of the code from which the image was \ngenerated.\nAlso, in our code, we have only a main branch, which reflects our production environment. But \nyou, as a developer, have the power to extend this functionality with a staging and dev environ-\nment. You just have to add the name of the branches in the on.push.branches configuration at \nthe beginning of the YAML file.\nIn Figure 11.16, you can observe how the CD pipeline looks after a PR is merged into the production \nbranch. As seen before, we only have the Build & Push Docker Image job here.\nFigure 11.16: GitHub Actions CD pipeline run example\n\n\nChapter 11\n445\nThe last step in setting up the CI/CD pipeline is to test it and see how it works.\nTest out the CI/CD pipeline\nTo test the CI/CD pipelines yourself, you must fork the LLM-Engineering repository to have full \nwrite access to the GitHub repository. Here is the official tutorial on how to fork a GitHub project: \nhttps://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-\nwith-forks/fork-a-repo\nThe last step is to set up a few secrets that will allow the CD pipeline to log in to AWS and point \nto the right ECR resource. To do so, go to the Settings tab at the top of the forked repository in \nGitHub. In the left panel, in the Security section, click on the Secrets and Variables toggle and, \nfinally, on Actions. Then, on the Secrets tab, create four repository secrets, as shown in Figure \n11.17. These secrets will be securely stored and accessible only by the GitHub Actions CD pipeline.\nThe AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are the AWS credentials you used across \nthe book. In Chapter 2, you see how to create them. The AWS_REGION (e.g., eu-central-1) and \nAWS_ECR_NAME are the same ones used in the Deploying the LLM Twin’s pipelines to the cloud \nsection.\nTo trigger the CI pipeline, create a feature branch, modify the code or documentation, and create \na PR to the main branch. To trigger the CD pipeline, merge the PR into the main branch. \nFor the AWS_ECR_NAME, you should configure only the name of the repository (e.g., \nzenml-vrsopg) and not the full URI (e.g., 992382797823.dkr.ecr.eu-central-1.\namazonaws.com/zenml-vrsopg), as seen in the image below:\nFigure 11.17: Configuring only repository name\n\n\nMLOps and LLMOps\n446\nAfter the CD GitHub Actions are complete, check the ECR repository to see whether the Docker \nimage was pushed successfully.\nFigure 11.18: GitHub Actions secrets\nIf you need more details on how to set up GitHub Actions secrets, we recommend checking out \ntheir official documentation: https://docs.github.com/en/actions/security-for-github-\nactions/security-guides/using-secrets-in-github-actions\nThe CT pipeline\nTo implement the CT pipeline, we will leverage ZenML. Once ZenML (or other orchestrators \nsuch as Metaflow, Dagster, or Airflow) orchestrates all your pipelines and your infrastructure is \ndeployed, you are very close to reaching CT.\nRemember the core difference between the CI/CD and CT pipelines. The CI/CD pipeline takes care \nof testing, building, and deploying your code—a dimension that any software program has. The \nCT pipeline leverages the code managed by the CI/CD pipeline to automate your data, training, \nand model-serving process, where the data and model dimensions are present only in the AI world.\nBefore diving into the implementation, we want to highlight two design choices that made reach-\ning CT simple:\n\n\nChapter 11\n447\n•\nThe FTI architecture: A modular system with clear interfaces and components made it \neasy to capture the relationship between the pipelines and automate them.\n•\nStarting with an orchestrator since day 0: We started with ZenML at the beginning of \nthe project’s development. Early on, we only used it locally. But it acted as an entry point \nfor our pipelines and a way to monitor their execution. Doing so forced us to decouple \neach pipeline and transfer the communication between them solely through various types \nof data storage, such as the data warehouse, feature store, or artifact store. As we have \nleveraged ZenML since day 0, we got rid of implementing a tedious CLI to configure our \napplication. Instead, we did it directly through YAML configuration files out of the box.\nIn Figure 11.19, we can see all the pipelines that we have to chain together to fully automate our \ntraining and deployment. The pipelines aren’t new; they aggregate everything we’ve covered \nthroughout this book. Thus, at this point, we will treat them as black boxes that interact with \neach other.\nFigure 11.19: CT pipeline\n\n\nMLOps and LLMOps\n448\nFor the LLM Twin’s CT pipeline, we have to discuss the initial trigger that starts the pipelines \nand how the pipelines are triggered by each other.\nInitial triggers\nAs illustrated in Figure 11.18, we initially want to trigger the data collection pipeline. Usually, the \ntriggers can be of three types:\n•\nManual triggers: Done through the CLI or the orchestrator’s dashboard, in our case, \nthrough the ZenML dashboard. Manual triggers are still extremely powerful tools, as \nyou need just one action to start the whole ML system, from data gathering to deploy-\nment, instead of fiddling with dozens of scripts that you might configure wrong or run \nin an invalid order.\n•\nREST API triggers: You can call a pipeline by an HTTP request. This is extremely useful \nwhen integrating your ML pipelines with other components. For example, you can have \na watcher constantly looking for new articles. It triggers the ML logic using this REST API \ntrigger when it finds some. To find more details on this feature, check out this tutorial on \nZenML’s documentation: https://docs.zenml.io/v/docs/how-to/trigger-pipelines/\ntrigger-a-pipeline-from-rest-api.\n•\nScheduled triggers: Another common approach is to schedule your pipeline to run con-\nstantly on a fixed interval. For example, depending on your use case, you can schedule your \npipeline to run daily, hourly, or every minute. Most of the orchestrators, ZenML included, \nprovide a cron expression interface where you can define your execution frequency. In the \nfollowing example from ZenML, the pipeline is scheduled every hour:\n Schedule(cron_expression=\"* * 1 * *\")\nWe chose a manual trigger for our LLM Twin use case as we don’t have other components to lever-\nage the REST API triggers. Also, as the datasets are generated from a list of static links defined in \nthe ZenML configs, running them on a schedule doesn’t make sense as they would always yield \nthe same results.\nBut a possible next step for the project is to implement a watcher that monitors for new articles. \nWhen it finds any, it generates a new config and triggers the pipelines through the REST API. An-\nother option is implementing the watcher as an additional pipeline and leveraging the schedule \ntriggers to look daily for new data. If it finds any, it executes the whole ML system; otherwise, it \nstops.\n\n\nChapter 11\n449\nThe conclusion is that once you can manually trigger all your ML pipelines through a single \ncommand, you can quickly adapt it to more advanced and complex scenarios.\nTrigger downstream pipelines\nTo keep things simple, we sequentially chained all the pipelines. More concretely, when the data \ncollection pipeline has finished, it will trigger the feature pipeline. When the feature pipeline has \nbeen completed successfully, it triggers the dataset generation pipeline, and so on. You can make \nthe logic more complex, like scheduling the generate instruct dataset pipeline to run daily, check-\ning the amount of new data in the Qdrant vector DB, and starting only if it has enough new data. \nFrom this point, you can further tweak the system’s parameters and optimize them to reduce costs.\nTo trigger all the pipelines in one go, we created one master pipeline that aggregates everything \nin one entry point:\n@pipeline\ndef end_to_end_data(\n    author_links: list[dict[str, str | list[str]]], … # Other paramaters…\n) -> None:\n    wait_for_ids = []\n    for author_data in author_links:\n        last_step_invocation_id = digital_data_etl(\n            user_full_name=author_data[\"user_full_name\"], links=author_\ndata[\"links\"]\n        )\n        wait_for_ids.append(last_step_invocation_id)\n    author_full_names = [author_data[\"user_full_name\"] for author_data in \nauthor_links]\n    wait_for_ids = feature_engineering(author_full_names=author_full_\nnames, wait_for=wait_for_ids)\n    generate_instruct_datasets(…)\n       training(…)\n       deploy(…)\n\n\nMLOps and LLMOps\n450\nTo keep the function light, we added all the logic up to computing the features. But, as we sug-\ngested in the code snippet above, you can easily add the instruction dataset generation, training, \nand deploy logic to the parent pipeline to implement an end-to-end flow. By doing that, you can \nautomate everything from data collection to deploying the model.\nTo run the end-to-end pipeline, use the following poe command:\npoetry poe run-end-to-end-data-pipeline\nWhat we implemented is not the best approach, as it compresses all the steps into a single monolith \npipeline (which we want to avoid), as illustrated in Figure 11.20. Usually, you want to keep each \npipeline isolated and use triggers to start downstream pipelines. This makes the system easier \nto understand, debug, and monitor.\nFigure 11.20: End-to-end pipeline illustrated in ZenML’s dashboard\n\n\nChapter 11\n451\nUnfortunately, the ZenML cloud’s free trial has a limitation of a maximum of three pipelines. As \nwe have more, we avoided that limitation by compressing all the steps into a single pipeline. But \nif you plan to host ZenML yourself or buy their license, they offer the possibility to independent-\nly trigger a pipeline from another pipeline, as you can see in the code snippet below where we \ntriggered the feature engineering pipeline after the data collection ETL:\nfrom zenml import pipeline, step\n@pipeline \ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\nuser = get_or_create_user(user_full_name)\ncrawl_links(user=user, links=links)\ntrigger_feature_engineering_pipeline(user)\n@step \ndef trigger_feature_engineering_pipeline(user):\nrun_config = PipelineRunConfiguration(…)\nClient().trigger_pipeline(\"feature_engineering\", run_configuration=run_\nconfig)\n@pipeline\ndef feature_engineering(author_full_names: list[str]) -> list[str]:\n… # ZenML steps\nBy taking this approach, each pipeline will have its independent run, where one pipeline sequen-\ntially triggers the next one, as described at the beginning of this section. Note that this feature is \nnot unique to ZenML but is common in orchestrator tools. The principles we have learned so far \nhold. Only how we interact with the tool changes.\nPrompt monitoring\nWe will use Opik (from Comet ML) to monitor our prompts. But remember from the LLMOps\nsection earlier in this chapter that we are not interested only in the input prompt and generated \nanswer. \n\n\nMLOps and LLMOps\n452\nWe want to log the entire trace from the user’s input until the final result is available. Before \ndiving into the LLM Twin use case, let’s look at a simpler example:\nfrom opik import track\nimport openai\nfrom opik.integrations.openai import track_openai\nopenai_client = track_openai(openai.OpenAI())\n@track\ndef preprocess_input(text: str) -> str:\n    return text.strip().lower()\n@track\ndef generate_response(prompt: str) -> str:\n    response = openai_client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n@track\ndef postprocess_output(response: str) -> str:\n    return response.capitalize()\n@track(name=\"llm_chain\")\ndef llm_chain(input_text: str) -> str:\n    preprocessed = preprocess_input(input_text)\n    generated = generate_response(preprocessed)\n    postprocessed = postprocess_output(generated)\n    return postprocessed\nresult = llm_chain(\"Hello, do you enjoy reading the book?\")\nThe preceding code snippet reflects in a simplistic way what most LLM applications will look \nlike. You have the llm_chain() main function, which takes the initial input as a parameter and \nreturns the final result. \n\n\nChapter 11\n453\nThen, you have preprocessing and postprocessing functions surrounding the actual LLM call. \nUsing the @track() decorator, we log the input and output of each function, which will ultimately \nbe aggregated into a single trace. By doing so, we will have access to the initial input text, the \ngenerated answer, and all the intermediary steps required to debug any potential issues using \nOpik’s dashboard.\nThe last step is to attach the necessary metadata for your use case to the current trace. As seen \nin the following code snippet, you can easily do that by calling the update() method, where you \ncan tag your trace or add any other metadata, such as the number of input tokens, through a \nPython dictionary:\nfrom opik import track, opik_context\n@track\ndef llm_chain(input_text):\n    # LLM chain code\n    # ...\n    opik_context.update_current_trace(\ntags=[\"inference_pipeline\"],\nmetadata={\n\"num_tokens\": compute_num_tokens(…)\n},\nfeedback_scores=[\n{\n\"name\": \"user_feedback\",\n\"value\": 1.0,\n\"reason\": \"The response was valuable and correct.\"\n},\n{\n\"name\": \"llm_judge_score\",\n\"value\": compute_llm_judge_score(…),\n\"reason\": \"Computing runtime metrics using an LLM Judge.\"\n}\n)\n\n\nMLOps and LLMOps\n454\nYou can expand on this idea and log various feedback scores. The most common is asking the user \nif the generated answer is valuable and correct. Another option is to compute various metrics \nautomatically through heuristics or LLM judges.\nFinally, let’s see how to add prompt monitoring to our LLM Twin project. First, look at Figure \n11.21 and remember our model-serving architecture. We have two microservices, the LLM and \nbusiness microservices. The LLM microservice has a narrow scope, as it only takes as input a \nprompt that already contains the user’s input and context and returns an answer that is usually \npost-processed. Thus, the business microservice is the right place to implement the monitoring \npipeline, as it coordinates the end-to-end flow. More concretely, Opik implementation will be in \nthe FastAPI server developed in Chapter 10.\nFigure 11.21: Inference pipeline serving architecture\n",
      "page_number": 465
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 476-483)",
      "start_page": 476,
      "end_page": 483,
      "detection_method": "topic_boundary",
      "content": "Chapter 11\n455\nAs our implementation is already modular, using Opik makes it straightforward to log an end-\nto-end trace of a user’s request:\nfrom opik import track\n@track\ndef call_llm_service(query: str, context: str | None) -> str:\n    llm = LLMInferenceSagemakerEndpoint(…)\n    answer = InferenceExecutor(llm, query, context).execute()\n    return answer\n@track\ndef rag(query: str) -> str:\n    retriever = ContextRetriever()\n    documents = retriever.search(query, k=3 * 3)\n    context = EmbeddedChunk.to_context(documents)\n    answer = call_llm_service(query, context)\n    return answer\nThe rag() function represents your application’s entry point. All the other processing steps take \nplace in the ContextRetriever and InferenceExector classes. Also, by decorating the call_llm_\nservice() function, we can clearly capture the prompt sent to the LLM and its response.\nTo add more granularity to our trace, we can further decorate other functions containing pre- or \npost-processing steps, such as the ContextRetriever search function:\nclass ContextRetriever:\n     …\n   \n    @track\n    \n    def search(\n        self,\n        query: str,\n        k: int = 3,\n\n\nMLOps and LLMOps\n456\n        expand_to_n_queries: int = 3,\n    ) -> list:\n        query_model = Query.from_str(query)\n        query_model = self._metadata_extractor.generate(query_model)\n        … # Rest of the implementation\nOr even go further to the retrieval optimization methods, such as the self-query metadata ex-\ntractor, to add more granularity:\nclass SelfQuery:\n    @track\n    def generate(self, query: str) -> str:\n        …\n        return enhanced_query\nThe developer is responsible for deciding how much granularity the application needs for proper \ndebugging and analysis. As having detailed monitoring is healthy, monitoring everything can be \ndangerous as it adds too much noise and makes manually understanding the traces difficult. You \nmust find the right balance. A good rule of thumb is tracing the most critical functions, such as \nrag() and call_llm_service(), and gradually adding more granularity when needed.\nThe last step is to attach valuable metadata and tags to our traces. To do so, we will further en-\nhance the rag() function as follows:\n@track\ndef rag(query: str) -> str:\n    retriever = ContextRetriever()\n    documents = retriever.search(query, k=3 * 3)\n    context = EmbeddedChunk.to_context(documents)\n    answer, prompt = call_llm_service(query, context)\n    trace = get_current_trace()\n    trace.update(\ntags=[\"rag\"],\nmetadata={\n\"model_id\": settings.HF_MODEL_ID,\n   \"embedding_model_id\": settings.TEXT_EMBEDDING_MODEL_ID,\n\n\nChapter 11\n457\n   \"temperature\": settings.TEMPERATURE_INFERENCE,\n   \"prompt_tokens\": compute_num_tokens(prompt),\n   \"total_tokens\": compute_num_tokens(answer),\n  \n}\n)\n    return answer\nThere are three main aspects that we should constantly monitor:\n•\nModel configuration: Here, we should consider both the LLM and other models used \nwithin the RAG layer. The most critical aspects of logging are the model IDs, but you can \nalso capture other important information that significantly impacts the generation, such \nas the temperature.\n•\nTotal number of tokens: It’s critical to constantly analyze the statistics of the number of \ntokens generated by your input prompts and total tokens, as this significantly impacts \nyour serving costs. For example, if the average of the total number of tokens generated \nsuddenly increases, it’s a strong signal that you have a bug in your system that you should \ninvestigate.\n•\nThe duration of each step: Tracking the duration of each step within your trace is essential \nto finding bottlenecks within your system. If the latency of a specific request is abnormally \nlarge, you quickly have access to a report that helps you find the source of the problem.\nAlerting\nUsing ZenML, you can quickly implement an alerting system on any platform of your liking, such \nas email, Discord, or Slack. For example, you can add a callback in your training pipeline to trigger \na notification when the pipeline fails or the training has finished successfully:\nfrom zenml import get_pipeline_context, pipeline\n@pipeline(on_failure=notify_on_failure)\ndef training_pipeline(…):\n…\nnotify_on_success()\n\n\nMLOps and LLMOps\n458\nImplementing the notification functions is straightforward. As seen in the code snippets below, \nyou have to get the alerter instance from your current stack, build the message as you see fit, \nand send it to your notification channel of choice:\nfrom zenml.client import Client\nalerter = Client().active_stack.alerter\ndef notify_on_failure() -> None:\n        alerter.post(message=build_message(status=\"failed\"))\n@step(enable_cache=False)\ndef notify_on_success() -> None:\n        alerter.post(message=build_message(status=\"succeeded\"))\nZenML and most orchestrators simplify implementing an alerter, as it’s a critical component \nin your MLOps/LLMOps infrastructure.\nSummary\nIn this chapter, we laid down the foundations with a theoretical section on DevOps. Then, we \nmoved on to MLOps and its core components and principles. Finally, we presented how LLMOps \ndiffers from MLOps by introducing strategies such as prompt monitoring, guardrails, and hu-\nman-in-the-loop feedback. Also, we briefly discussed why most companies would avoid training \nLLMs from scratch but choose to optimize them for their use case through prompt engineering \nor fine-tuning. At the end of the theoretical portion of the chapter, we learned what a CI/CD/CT \npipeline is, the three core dimensions of an ML application (code, data, model), and that, after \ndeployment, it is more critical than ever to implement a monitoring and alerting layer due to \nmodel degradation.\nNext, we learned how to deploy the LLM Twin’s pipeline to the cloud. We understood the infra-\nstructure and went step by step through deploying MongoDB, Qdrant, the ZenML cloud, and all \nthe necessary AWS resources to sustain the application. Finally, we learned how to Dockerize our \napplication and push our Docker image to AWS ECR, which will be used to execute the application \non top of AWS SageMaker.\nThe final step was to add LLMOps to our LLM Twin project. We began by implementing a CI/CD \npipeline with GitHub Actions. Then, we looked at our CT strategy by leveraging ZenML. \n\n\nChapter 11\n459\nFinally, we saw how to implement a monitoring pipeline using Opik from Comet ML and an \nalerting system using ZenML. These are the fundamental pillars in adding MLOps and LLMOps \nto any LLM-based application.\nThe framework we learned about throughout the book can quickly be extrapolated to other LLM \napplications. Even if we used the LLM Twin use case as an example, most of the strategies applied \ncan be adapted to other projects. Thus, we can get an entirely new application by changing the \ndata and making minor tweaks to the code. Data is the new oil, remember? \nBy finalizing this chapter, we’ve learned to build an end-to-end LLM application, starting with \ndata collection and fine-tuning until deploying the LLM microservice and RAG service. Through-\nout this book, we aimed to provide a thought framework to help you build and solve real-world \nproblems in the GenAI landscape. Now that you have it, we wish you good luck in your journey \nand happy building!\nReferences\n•\nGitLab. (2023, January 25). What is DevOps? | GitLab. GitLab. https://about.gitlab.\ncom/topics/devops/\n•\nHuyen, C. (2024, July 25). Building a generative AI platform. Chip Huyen. https://\nhuyenchip.com/2024/07/25/genai-platform.html\n•\nLightricks customer story: Building a recommendation engine from scratch. (n.d.). https://\nwww.qwak.com/academy/lightricks-customer-story-building-a-recommendation-\nengine-from-scratch\n• \nWhat LLMOps. (n.d.). Google Cloud. https://cloud.google.com/discover/what-is-\nllmops?hl=en\n• \nMLOps: Continuous delivery and automation pipelines in machine learning. (2024, August \n28). Google Cloud. https://cloud.google.com/architecture/mlops-continuous-\ndelivery-and-automation-pipelines-in-machine-learning#top_of_page\n• \nMl-ops.org. (2024a, July 5). https://ml-ops.org/content/mlops-principles\n• \nMl-ops.org. (2024b, July 5). https://ml-ops.org/content/mlops-principles\n• \nMl-ops.org. (2024c, July 5). https://ml-ops.org/content/motivation\n• \nMohandas, G. M. (2022a). Monitoring machine learning systems. Made With ML. https://\nmadewithml.com/courses/mlops/monitoring/\n• \nMohandas, G. M. (2022b). Testing Machine Learning Systems: Code, Data and Models. \nMade With ML. https://madewithml.com/courses/mlops/testing/\n\n\nMLOps and LLMOps\n460\n• \nPreston-Werner, T. (n.d.). Semantic Versioning 2.0.0. Semantic Versioning. https://semver.\norg/\n• \nRibeiro, M. T., Wu, T., Guestrin, C., & Singh, S. (2020, May 8). Beyond Accuracy: Behavioral \nTesting of NLP models with CheckList. arXiv.org. https://arxiv.org/abs/2005.04118\n• \nWandb. (2023, November 30). Understanding LLMOps: Large Language Model Operations. \nWeights & Biases. https://wandb.ai/site/articles/understanding-llmops-large-\nlanguage-model-operations/\n• \nZenml-Io. (n.d.). GitHub—zenml-io/zenml-huggingface-sagemaker: An example MLOps over-\nview of ZenML pipelines from a Hugging Face model repository to a deployed AWS SageMaker \nendpoint. GitHub. https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/\nmain\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n\n\nAppendix\nMLOps Principles\nBuilding robust and scalable ML systems requires more than creating powerful models. It demands \nan all-encompassing approach to operationalizing the entire ML lifecycle. Let’s explore the six \ncore principles that guide the MLOps field. These principles are independent of any tool and are \nat the core of building robust and scalable ML systems. They provide a guideline for designing \nproduction-ready applications, ensuring consistency, reliability, and scalability at every stage.\nWith that in mind, let’s begin with the foundation: automation or operationalization.\n1. Automation or operationalization\nTo adopt MLOps, there are three core tiers that most applications build up gradually, from manual \nprocessing to full automation:\n•\nManual process: The process is experimental and iterative in the early stages of developing \nan ML application. The data scientist manually performs each pipeline step, such as data \npreparation and validation, model training, and testing. At this point, they commonly use \nJupyter notebooks to train their models. This stage’s output is the code used to prepare \nthe data and train the models.\n•\nContinuous training (CT): The next level involves automating model training. This is \nknown as continuous training, which triggers model retraining whenever required. At this \npoint, you often automate your data and model validation steps. This step is usually done \nby an orchestration tool, such as ZenML, that glues all your code together and runs it on \nspecific triggers. The most common triggers are on a schedule, for example, every day or \nwhen a specific event comes in, such as when new data is uploaded or the monitoring sys-\ntem detects a drop in performance, offering you the flexibility to adapt to various triggers.\n\n\nMLOps Principles\n462\n•\nCI/CD: In the final stage, you implement your CI/CD pipelines to enable fast and reliable \ndeployment of your ML code into production. The key advancement at this stage is the \nautomatic building, testing, and deployment of data, ML models, and training pipeline \ncomponents. CI/CD is used to quickly push new code into various environments, such as \nstaging or production, ensuring efficient and reliable deployment.\nAs we build our LLM system using the FTI (feature, training, inference) architecture, we can \nquickly move from a manual process to CI/CD/CT. In Figure A.1, we can observe that the CT process \ncan be triggered by various events, such as a drop in performance detected by the monitoring \npipeline or a batch of fresh data arriving. Also, Figure A.1 is split into two main sections; the first \none highlights the automated processes, while at the bottom, we can observe the manual pro-\ncesses performed by the data science team while experimenting with various data processing \nmethods and models. Once they improve the model by tinkering with how the data is processed \nor the model architecture, they push the code to the code repository, which triggers the CI/CD \npipeline to build, test, package, and deploy the new changes to the FTI pipelines.\nFigure A.1: CI/CD/CT on top of the FTI architecture\n",
      "page_number": 476
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 484-491)",
      "start_page": 484,
      "end_page": 491,
      "detection_method": "topic_boundary",
      "content": "Appendix\n463\nTo conclude, CT automates the FTI pipelines, while CI/CD builds, tests, and pushes new versions \nof the FTI pipeline code to production.\n2. Versioning\nBy now, we understand that the whole ML system changes if the code, model, or data changes. \nThus, it is critical to track and version these three elements individually. But what strategies can \nwe adopt to track the code, model, and data separately?\n• \nThe code is tracked by Git, which helps us create a new commit (a snapshot of the code) \non every change added to the codebase. Also, Git-based tools usually allow us to make \nreleases, which typically pack multiple features and bug fixes. While the commits contain \nunique identifiers that are not human-interpretable, a release follows more common con-\nventions based on their major, minor, and patch versions. For example, in a release with \nversion “v1.2.3,” 1 is the major version, 2 is the minor version, and 3 is the patch version. \nPopular tools are GitHub and GitLab.\n• \nTo version the model, you leverage the model registry to store, share, and version all the \nmodels used within your system. It usually follows the same versioning conventions used \nin code releases, defined as Semantic Versioning, which, along with the major, minor, \nand patch versions, also supports alpha and beta releases that signal applications. At this \npoint, you can also leverage the ML metadata store to attach information to the stored \nmodel, such as what data it was trained on, its architecture, performance, latency, and \nwhatever else makes sense to your specific use case. Doing so creates a clear catalog of \nmodels that can easily be navigated across your team and company.\n• \nVersioning the data isn’t as straightforward as versioning the code and model because it \ndepends on the type of data you have (structured or unstructured) and the scale of data you \nhave (big or small). For example, for structured data, you can leverage a SQL database with \na version column that helps you track the changes in the dataset. However, other popular \nsolutions are based on Git-like systems, such as Data Version Control (DVC), that track \nevery change made to the dataset. Other trendy solutions are based on artifacts similar \nto a model registry that allows you to add a virtual layer to your dataset, tracking and \ncreating a new version for every change made to your data. Comet.ml, W&B (Weights & \nBiases), and ZenML offer powerful artifact features. For all solutions, you must store the \ndata on-premises or use cloud object storage solutions such as AWS S3. These tools provide \nfeatures that allow you to structure your datasets and versions, track, and access them.\n\n\nMLOps Principles\n464\n3. Experiment tracking\nTraining ML models is an entirely iterative and experimental process. Unlike traditional software \ndevelopment, it involves running multiple parallel experiments, comparing them based on a set \nof predefined metrics, and deciding which one should advance to production. An experiment \ntracking tool allows you to log all the necessary information, such as metrics and visual repre-\nsentations of your model predictions, to compare all your experiments and easily select the best \nmodel. Popular tools are Comet ML, W&B, MLflow, and Neptune.\n4. Testing\nThe same trend is followed when testing ML systems. Hence, we must test our application across \nall three dimensions: the data, the model, and the code. We must also ensure that the feature, \ntraining, and inference pipeline are well integrated with external services, such as the feature store, \nand work together as a system. When working with Python, the most common tool to write your \ntests is pytest, which we also recommend.\nTest types\nIn the development cycle, six primary types of tests are commonly employed at various stages:\n•\nUnit tests: These tests focus on individual components with a single responsibility, such \nas a function that adds two tensors or one that finds an element in a list.\n•\nIntegration tests: These tests evaluate the interaction between integrated components \nor units within a system, such as the data evaluation pipeline or the feature engineering \npipeline, and how they are integrated with the data warehouse and feature store.\n•\nSystem tests: System tests play a crucial role in the development cycle as they examine the \nentire system, including the complete and integrated application. These tests rigorously \nevaluate the end-to-end functionality of the system, including performance, security, and \noverall user experience—for example, testing an entire ML pipeline, from data ingestion \nto model training and inference, ensuring the system produces the correct outputs for \ngiven inputs.\n•\nAcceptance tests: These tests, often called user acceptance testing (UAT), are designed to \nconfirm that the system meets specified requirements, ensuring it is ready for deployment.\n•\nRegression tests: These tests check for previously identified errors to ensure that new \nchanges do not reintroduce them.\n\n\nAppendix\n465\n•\nStress tests: These tests evaluate the system’s performance and stability under extreme \nconditions, such as high load or limited resources. They aim to identify breaking points \nand ensure the system can handle unexpected spikes in demand or adverse situations \nwithout failing.\nFigure A.2: Test types\nWe’ve intentionally left regression tests out of the preceding figure because they aren’t a distinct \ntesting phase. Instead, regression testing is applied across all levels—unit, integration, system, \nacceptance, and stress tests—to ensure that changes don’t reintroduce previous errors. It’s an \nongoing process within these phases, not a separate type of test, which is why it’s not shown as \na separate category.\nWhat do we test?\nWhen writing most tests, you take a component and treat it as a black box. Thus, what you have \ncontrol over is the input and output. You want to test that you get an expected output for a given \ninput. With that in mind, here are a few things you should usually test:\n• \nInputs: Data types, format, length, and edge cases (min/max, small/large, etc.)\n• \nOutputs: Data types, formats, exceptions, and intermediary and final outputs\nTest examples\nWhen testing your code, you can leverage the standards from classic software engineering. Here \nare a few examples of code tests you can include when writing unit tests to get a better idea of \nwhat we want to test at this point—for instance, you want to check that a sentence is cleaned \nas expected. \n\n\nMLOps Principles\n466\nAlso, you can look at your chunking algorithm and assert that it works properly by using various \nsentences and chunk sizes.\nWhen we talk about data tests, we mainly refer to data validity. Your data validity code usually \nruns when raw data is ingested from the data warehouse or after computing the features. It is \npart of the feature pipeline. Thus, by writing integration or system tests for your feature pipeline, \nyou can check that your system responds properly to valid and invalid data.\nTesting data validity depends a lot on your application and data type. For example, when working \nwith tabular data, you can check for non-null values, that a categorical variable contains only \nthe expected values, or that a float value is always positive. You can check for length, character \nencoding, language, special characters, and grammar errors when working with unstructured \ndata such as text.\nModel tests are the trickiest, as model training is the most non-deterministic process of an ML \nsystem. However, unlike traditional software, ML systems can successfully complete without \nthrowing any errors. However, the real issue is that they produce incorrect results that can only \nbe observed during evaluations or tests. Some standard model test techniques involve checking:\n•\nThe shapes of the input and model output tensors\n•\nThat the loss decreases after one batch (or more) of training\n•\nOverfit on a small batch, and the loss approaches 0\n•\nThat your training pipeline works on all the supported devices, such as the CPU and GPU\n•\nThat your early stopping and checkpoint logic works\nAll the tests are triggered inside the CI pipeline. If some tests are more costly, for example, the mod-\nel ones, you can execute them only on special terms, such as only when modifying the model code.\nAt the other end of the spectrum, you can also perform behavioral testing on your model, which \ntries to adopt the strategy from code testing and treats the model as a black box while looking \nsolely at the input data and expected outputs. This makes the behavioral testing methods model \nagnostic. A fundamental paper in this area is Beyond Accuracy: Behavioral Testing of NLP Models \nwith CheckList, which we recommend if you want to dig more into the subject. However, as a \nquick overview, the paper proposes that you test your model against three types of tests. We use \na model that extracts the main subject from a sentence as an example:\n\n\nAppendix\n467\n• \nInvariance: Changes in your input should not affect the output—for example, below is \nan example based on synonym injection:\nmodel(text=\"The advancements in AI are changing the world rapidly.\")\n# output: ai\nmodel(text=\"The progress in AI is changing the world rapidly.\")\n# output: ai\n• \nDirectional: Changes in your input should affect the outputs—for example, below is an \nexample where we know the outputs should change based on the provided inputs:\nmodel(text=\"Deep learning used for sentiment analysis.\")\n# output: deep-learning\nmodel(text=\"Deep learning used for object detection.\")\n# output: deep-learning\nmodel(text=\"RNNs for sentiment analysis.\")\n# output: rnn\n•\nMinimum functionality: The most simple combination of inputs and expected outputs—\nfor example, below is a set of simple examples that we expect the model should always \nget right:\nmodel(text=\"NLP is the next big wave in machine learning.\")\n# output: nlp\nmodel(text=\"MLOps is the next big wave in machine learning.\")\n# output: mlops\nmodel(text=\"This is about graph neural networks.\")\n# output: gnn\nFor more on testing, we recommend reading Testing Machine Learning Systems: Code, \nData, and Models by Goku Mohandas: https://madewithml.com/courses/mlops/\ntesting/.\n\n\nMLOps Principles\n468\n5. Monitoring\nMonitoring is vital for any ML system that reaches production. Traditional software systems are \nrule-based and deterministic. Thus, once it is built, it will always work as defined. Unfortunate-\nly, that is not the case with ML systems. When implementing ML models, we haven’t explicitly \ndescribed how they should work. We have used data to compile a probabilistic solution, which \nmeans that our ML model will constantly be exposed to a level of degradation. This happens \nbecause the data from production might differ from the data the model was trained on. Thus, it \nis natural that the shipped model doesn’t know how to handle these scenarios.\nWe shouldn’t try to avoid these situations but create a strategy to catch and fix these errors in time. \nIntuitively, monitoring detects the model’s performance degradation, which triggers an alarm that \nsignals that the model should be retrained manually, automatically, or with a combination of both.\nWhy retrain the model? As the model performance degrades due to a drift in the training dataset \nand what it inputs from production, the only solution is to adapt or retrain the model on a new \ndataset that captures all the new scenarios from production.\nAs training is a costly operation, there are some tricks that you can perform to avoid retraining, \nbut before describing them, let’s quickly understand what we can monitor to understand our \nML system’s health.\nLogs\nThe approach to logging is straightforward, which is to capture everything, such as:\n•\nDocument the system configurations.\n•\nRecord the query, the results, and any intermediate outputs.\n•\nLog when a component begins, ends, crashes, and so on.\n•\nEnsure that each log entry is tagged and identified in a way that clarifies its origin within \nthe system.\nWhile capturing all activities can rapidly increase the volume of logs, you can take advantage of \nnumerous tools for automated log analysis and anomaly detection that leverage AI to efficiently \nscan all the logs, providing you with the confidence to manage the logs effectively.\nMetrics\nTo quantify your application’s healthiness, you must define a set of metrics. Each metric measures \ndifferent aspects of your application, such as the infrastructure, data, and model.\n\n\nAppendix\n469\nSystem metrics\nThe system metrics are based on monitoring service-level metrics (latency, throughput, error \nrates) and infrastructure health (CPU/GPU, memory). These metrics are used both in traditional \nsoftware and ML as they are crucial to understanding whether the infrastructure works well and \nthe system works as expected to provide a good user experience to the end users.\nModel metrics\nMerely monitoring the system’s health won’t suffice to identify the deeper issues within our \nmodel. Therefore, moving on to the next layer of metrics that focus on the model’s performance \nis crucial. This includes quantitative evaluation metrics like accuracy, precision, and F1 score, as \nwell as essential business metrics influenced by the model, such as ROI and click rate.\nAnalyzing cumulative performance metrics over the entire deployment period is often ineffective. \nInstead, evaluating performance over time intervals relevant to our application, such as hourly, \nis essential. Thus, in practice, you window your inputs and compute and aggregate the metrics \nat the window level. These sliding metrics can provide a clearer picture of the system’s health, \nallowing us to detect issues more promptly without them being obscured by historical data.\nWe may not always have access to ground-truth outcomes to evaluate the model’s performance \non production data. This is particularly challenging when there is a significant delay or when \nreal-life data requires annotation. To address this issue, we can develop an approximate signal \nto estimate the model’s performance or label a small portion of our live dataset to assess perfor-\nmance. When talking about ML monitoring, an approximate signal is also known as a proxy metric, \nusually implemented by drift detection methods, which are discussed in the following section.\nDrifts\nDrifts are proxy metrics that help us detect potential issues with the production model in time \nwithout requiring any ground truths/labels. Table A.1 shows three kinds of drifts.\nWhat drifts\nDescription\nDrift formulation\nX          \nInputs (features)  \ndata drift →P(X) ≠Pref(X) \n y         \nOutputs (ground truths/\nlabels)\ntarget drift →P(y) ≠Pref(y) \nP(y|X) \nrelationship between X and y  \nconcept drift →P(y|X) ≠Pref(y|X) \nTable A.1: Relationship between data, model, and code changes\n\n\nMLOps Principles\n470\nData drift\nData drift, also called feature drift or covariate shift, occurs when the distribution of the produc-\ntion data deviates from that of the training data, as shown in Figure A.3. This difference means the \nmodel cannot handle the changes in feature space, leading to potentially unreliable predictions. \nDrift can result from natural real-life changes or systemic problems like missing data, pipeline \nerrors, and schema modifications.\nFigure A.3: Data drift examples\nWhen data begins to drift, the degradation in our model’s performance might not be immediately \nnoticeable, particularly if the model interpolates effectively. Nevertheless, this presents an ideal \nchance to consider retraining before the drift affects the model’s performance.\nTarget drift\nIn addition to changes in input data (data drift), we might also encounter shifts in output dis-\ntribution. The shift could involve changes in the shape of the distribution or the addition and \nremoval of classes in categorical tasks. While retraining the model can help reduce performance \ndegradation due to target drift, you can often prevent it by adapting the head processing steps \nand model head to support the new schema of the output class.\nFor example, if you have a classifier that predicts if an image contains animals or people, and you \nget a picture with buildings, you can either adapt your model to support an unknown class or \nadjust the head of the model to add the new class for future predictions.\n",
      "page_number": 484
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 492-500)",
      "start_page": 492,
      "end_page": 500,
      "detection_method": "topic_boundary",
      "content": "Appendix\n471\nConcept drift\nIn addition to changes in input and output data, their relationship can also shift. This phenom-\nenon, known as concept drift, makes our model ineffective because the patterns it previously \nlearned to associate inputs with outputs become outdated. As illustrated in the following figure, \nconcept drifts can manifest in various ways:\n•\nGradually over time\n•\nSuddenly, due to an external event\n•\nPeriodically, due to recurring events\nFigure A.4: Concept drift examples\nFor example, this happens when using the model in a different geographic area. Let’s assume you \nwant to build a model that predicts whether a person will buy a specific car. You initially built it \nfor the American market. Now, you want to use it in the European market, where people tend to \nbuy smaller cars, creating a drift between the size feature of the car and the output probability of \npurchasing the vehicle. Of course, concept drifts can be more subtle than this example.\nAll these types of drift can happen simultaneously, complicating pinpointing the \nexact sources of drift.\n\n\nMLOps Principles\n472\nHow to detect and measure drifts\nNow that we’ve recognized the various types of drift, it’s crucial to understand how to detect and \nmeasure it. To do so, you need two types of windows:\n• \nA reference window: This is the collection of data points used as a baseline to compare \nagainst the production data distributions for drift identification. It is usually gathered \nfrom the training dataset.\n• \nA test window: This collects data points gathered while the ML system is in production. \nIt is compared with the reference window to ascertain if drift has occurred.\nTo measure the drifts, you leverage hypothesis tests that verify the change in distribution between \nthe two windows. For example, you can use the Kolmogorov-Smirnov (KS) test to monitor a \nsingle continuous feature. This is known as a univariate (1D) test. Thus, you must run it for every \nfeature you want to monitor. You can leverage a chi-squared univariate test to monitor categorical \nvariables and determine if the frequency of events in production is consistent with the reference \nwindow distribution.\nfrom alibi_detect.cd import KSDrift\ncd = KSDrift(X_ref, p_val=.05, preprocess_fn=preprocess_fn, input_\nshape=(max_len,))\nWhen working with text data in an embedding representation, we have to model a multivariate \ndistribution, which is how LLMs work with text. A popular approach is to take the embeddings of \nthe test and reference windows, apply a dimensionality reduction algorithm, and apply an algo-\nrithm such as maximum mean discrepancy (MMD). This algorithm is a kernel-based approach \nthat measures the distance between two distributions by computing the distance between the \nmean of the embeddings of the two windows.\nfrom alibi_detect.cd import MMDDrift\ncd = MMDDrift(x_ref, backend='pytorch', p_val=.05)\npreds = cd.predict(x)\nMonitoring vs. observability\nMonitoring involves the collection and visualization of data, whereas observability provides in-\nsights into system health by examining its inputs and outputs. For instance, monitoring allows \nus to track a specific metric to detect potential issues. \n\n\nAppendix\n473\nOn the other hand, a system is considered observable if it generates meaningful data about its \ninternal state, which is essential for diagnosing root causes.\nAlerts\nOnce we define our monitoring metrics, we need a way to get notified. The most common ap-\nproaches are to send an alarm in the following scenarios:\n• \nA metric passes the values of a static threshold—for example, when the accuracy of the \nclassifier is lower than 0.8, send an alarm.\n• \nTweaking the p-value of the statistical tests that check for drifts. A lower p-value means \na higher confidence that the production distribution differs from the reference one.\nThese thresholds and p-values depend on your application. However, it is essential to find the \ncorrect values, as you don’t want to overcrowd your alarming system with false positives. In that \ncase, your alarm system won’t be trustworthy, and you will either overreact or not react at all \nto issues in your system. Some common channels for sending alarms to your stakeholders are \nSlack, Discord, your email, and PagerDuty. The system’s stakeholders can be the core engineers, \nmanagers, or anyone interested in the system.\nDepending on the nature of the alarm, you have to react differently. But before taking any action, \nyou should be able to inspect it and understand what caused it. You should inspect what metric \ntriggered the alarm, with what value, the time it happened, and anything else that makes sense \nto your application.\nWhen the model’s performance degrades, the first impulse is to retrain it. But that is a costly op-\neration. Thus, you first have to check that the data is valid, the schema hasn’t changed, and the \ndata point was not an isolated outlier. If neither is true, you should trigger the training pipeline \nand train the model on the newly shifted dataset to solve the drift.\n6. Reproducibility\nReproducibility means that every process within your ML systems should produce identical \nresults given the same input. This has two main aspects.\nThe first one is that you should always know what the inputs are—for example, when training \na model, you can use a plethora of hyperparameters. Thus, you need a way to always track what \nassets were used to generate the new assets, such as what dataset version and config were used \nto train the model.\n\n\nMLOps Principles\n474\nThe second aspect is based on the non-deterministic nature of ML processes. For example, when \ntraining a model from scratch, all the weights are initially randomly initialized. Thus, even if \nyou use the same dataset and hyperparameters, you might end up with a model with a differ-\nent performance. This aspect can be solved by always using a seed before generating random \nnumbers, as in reality, we cannot digitally create randomness, only pseudo-random numbers. \nThus, by providing a seed, we ensure that we always produce the same trace of pseudo-random \nnumbers. This can also happen at the feature engineering step, in case we impute values with \nrandom values or randomly remove data or labels. But as a general rule of thumb, always try to \nmake your processes as deterministic as possible, and in case you have to introduce randomness, \nalways provide a seed that you have control over.\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n\n\npackt.com\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as \nindustry leading tools to help you plan your personal development and advance your career. For \nmore information, please visit our website.\nWhy subscribe?\n•\nSpend less time learning and more time coding with practical eBooks and Videos from \nover 4,000 industry professionals\n•\nImprove your learning with Skill Plans built especially for you\n•\nGet a free eBook or video every month\n•\nFully searchable for easy access to vital information\n•\nCopy and paste, print, and bookmark content\nAt www.packt.com, you can also read a collection of free technical articles, sign up for a range of \nfree newsletters, and receive exclusive discounts and offers on Packt books and eBooks.\n\n\nOther Books \nYou May Enjoy\nIf you enjoyed this book, you may be interested in these other books by Packt:\nRAG-Driven Generative AI\nDenis Rothman\nISBN: 9781836200918\n• \nScale RAG pipelines to handle large datasets efficiently\n• \nEmploy techniques that minimize hallucinations and ensure accurate responses\n\n\nOther Books You May Enjoy\n478\n•\nImplement indexing techniques to improve AI accuracy with traceable and transparent \noutputs\n•\nCustomize and scale RAG-driven generative AI systems across domains\n•\nFind out how to use Deep Lake and Pinecone for efficient and fast data retrieval\n•\nControl and build robust generative AI systems grounded in real-world data\n•\nCombine text and image data for richer, more informative AI responses\n\n\nOther Books You May Enjoy\n479\nBuilding LLM Powered  Applications\nValentina Alto\nISBN: 9781835462317\n•\nExplore the core components of LLM architecture, including encoder-decoder blocks \nand embeddings\n•\nUnderstand the unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLM\n•\nUse AI orchestrators like LangChain, with Streamlit for the frontend\n•\nGet familiar with LLM components such as memory, prompts, and tools\n•\nLearn how to use non-parametric knowledge and vector databases\n•\nUnderstand the implications of LFMs for AI research and industry applications\n•\nCustomize your LLMs with fine tuning\n•\nLearn about the ethical implications of LLM-powered applications\n\n\nOther Books You May Enjoy\n480\nPackt is searching for authors like you\nIf you’re interested in becoming an author for Packt, please visit authors.packtpub.com and \napply today. We have worked with thousands of developers and tech professionals, just like you, \nto help them share their insight with the global tech community. You can make a general appli-\ncation, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.\nShare your thoughts\nNow you’ve finished LLM Engineer’s Handbook, First Edition, we’d love to hear your thoughts! If you \npurchased the book from Amazon, please click here to go straight to the Amazon review \npage for this book and share your feedback or leave a review on the site that you purchased it from.\nYour review is important to us and the tech community and will help us make sure we’re deliv-\nering excellent quality content.\n",
      "page_number": 492
    },
    {
      "number": 55,
      "title": "Segment 55 (pages 501-509)",
      "start_page": 501,
      "end_page": 509,
      "detection_method": "topic_boundary",
      "content": "Index\nSymbols\n4-bit NormalFloat (NF4)  215\n32-bit floating point (fp32)  211, 212\nA\nacceptance tests  464\nactions  437\nActivate-aware Weight \nQuantization (AWQ)  313\nadvanced RAG\noverview  117, 118\npost-retrieval step  124-126\npre-retrieval steps  119-122\nretrieval step  122-124\nadvanced RAG post-retrieval optimization\nreranking  334-338\nadvanced RAG pre-retrieval \noptimizations  324\nquery expansion  324-328\nself-querying  328-332\nadvanced RAG retrieval optimization\nfiltered vector search  332-334\nadvanced RAG techniques\nexploring  321-324\npost-retrieval optimization  334-338\npre-retrieval optimizations  324-332\nretrieval optimization  332-334\nalerting system  457, 458\nalerts  473\nAlpacaEval  264\nAmazon Resource Name (ARN)  375\nApplication Auto Scaling  396, 397\nApplication Load Balancer (ALB)  395\nasynchronous inference  361, 362\nautoscaling  393, 399\nscalable policy, creating  397\nscalable target, registering  396\nuse cases  394\nAWS\naccess key, setting up  48-50\naccount, setting up  48-50\nCLI, setting up  48-50\npreparing  48\nSageMaker  50\nAWS Elastic Container Service (ECS)  393\nAWS Elastic Kubernetes Service (EKS)  393\nAWS SageMaker  50\nLLM Twin model, deploying to  375-385\nneed for  51, 52\n\n\nIndex\n482\nAWS SageMaker Inference endpoint\ncalling  386-389\nautomated evaluation framework for RAG \nsystems (ARES)  274, 275\nB\nbacked-up data\nimporting  95\nBaseCrawler interface  69-72\nbehavioral testing  466\nbias types\nfamily bias  237\nlength bias  237\nposition bias  237\nBigCodeBench Leaderboard  266\nbusiness microservice\nbuilding, with FastAPI  390-393\nC\nCDC patterns\nlog-based  137\ntimestamp-based  137\ntrigger-based  137\ncentral access point  128\nChange data capture (CDC)  136\nChatbot Arena  264\nChatbots  231\nChatGPT  5\nlimitations  5\nchat templates  208-210\nchunking handlers  165-169\nCI/CD pipeline  462\nCI pipeline, LLM Twin\nQA job  438\ntest job  438\nCircleCI  405\nclassifiers models  189\ncleaning handlers  163-165\nCloudFormation  423\ncode generation  231\nComet ML  45, 46\nconcept drift  471\ncontent moderation  231\ncontinuous batching  294\ncontinuous integration and continuous \ndeployment (CI/CD) pipeline  31, 402\ncontinuous training (CT)  138, 402, 461\ncooldown period  398\nco-pilot\nversus LLM Twin  4\ncovariate drift  470\nCrawlerDispatcher class  66-68\ncrawlers\nBaseCrawler interface  69-72\nCustomArticleCrawler class  75-77\nGithubCrawler class  73-75\nimplementing  69\nMediumCrawler class  77-79\nCustomArticleCrawler class  75-77\nD\ndata augmentation  193-196\ndatabase (DB)  317, 410\ndatabase, for unstructured and vector data\nMongoDB  47\nQdrant  47, 48\nstoring  47\ndata collection pipeline  19\ndata curation  182\n\n\nIndex\n483\ndata decontamination  185\ndata deduplication  184, 185\ndata drift  470\ndata evaluation  233\ndata exploration  189-191\ndata generation  191-233\npreference data, evaluating  235-237\npreference data, generating  233, 234\ntips  234\ndata indexing techniques  119\ndata parallelism (DP)  299\ndata quality evaluation  186-189\ndata quantity  180, 181\nData Scientist (DS)  409\ndataset formats  208\ndata tests  466\ndecoder-only model\narchitecture  290\ncomputing   291\ngenerating  291\ntokenizing  291\nDeep Learning Containers (DLCs)  373\ndeployment costs  415\ndeployment types, criteria for selection\ndata  357\ninfrastructure  357, 358\nlatency  356\nthroughput  356, 357\nDevOps  401-403\nbenefits  403\ncontinuous delivery (CD)  405\ncontinuous integration (CI)  405\ndeployment environments  404\nversion control  405\nDevOps lifecycle\nbuild  404\ncode  403\ndeploy  404\nmonitor  404\noperate  404\nplan  403\nrelease  404\ntest  404\ndirectional  467\nDirect Preference Optimization \n(DPO)  229, 245, 248-250, 411\nimplementing  250-257\ndispatcher layer  160-162\nDLC image\nfeatures  373\nDocker  424\nDockerfile  424\ndomain-driven design (DDD)  150\ndomain-specific LLM evaluations  265-267\ndownstream pipelines\ntriggering  449-451\nDPO datasets\nhuman-generated, human-evaluated \ndatasets  233\nhuman-generated, LLM-evaluated \ndatasets  233\nLLM-generated, human-evaluated \ndatasets  234\nLLM-generated, LLM-evaluated datasets  234\ndrifts  469\nconcept drift  471\ndata drift  470\ndetecting  472\nmeasuring  472\ntarget drift  470\n\n\nIndex\n484\nE\nElastic Container Registry (ECR)  423, 443\nembedding handlers  169-173\nencoder-only models  189\nend of sentence (EOS) token 222, 252\nend-to-end RAG inference pipeline\nexamining  346-351\nEnterprise Scenarios Leaderboard  266\nETL pipeline\nfundamental steps  56\nETL process\nconnecting, to feature pipeline  60\nexact deduplication  184\nextract, load, transform (ETL) pattern  19\nExtract, Transform, Load (ETL) pipeline  55\nF\nfamily bias  237\nFastAPI\nbusiness microservice, building  390-393\nfeature drift  470\nfeature pipeline  14, 19, 20\nfeature/training/inference (FTI) \narchitecture  8, 13, 22, 370\nbenefits  15\nfeature pipeline  14\ninference pipeline  14\ntraining pipeline  14\nfiltered vector search  123\nfine-tune\nusage, considerations  206, 207\nfine-tune models\nspecialized tools  220\nfine-tuning\nbest practices  219-226\nformat filtering  183\nformatting errors  436\nexamples  436\nFTI architecture\nused, for building LLM system  462, 463\nFTI pipeline design\nLLM Twin architecture, designing  17\nFTI pipelines architecture\ninference pipeline  14\nfull fine-tuning  211, 212\nfuzzy deduplication  184\nG\nGAIA  264\nGalileo Protect  413\ngeneral-purpose LLM evaluations  263-265\nGitHub  405\nGitHub Actions  405, 437\nGitHub Actions CI YAML file  438-441\nGitHubCrawler class  73-75\nGitHub ecosystem  405\nGitLab  405\nGitLab CI/CD  405\nGlobal Interpreter Lock (GIL)  144\nGPT  411\nguardrails  411, 412\ninput guardrails  412\noutput guardrails  413\nH\nHallucinations Leaderboard  266\n\n\nIndex\n485\nhandlers  162, 163\nchunking handlers  165-169\ncleaning handlers  163-165\nembedding handlers  169-173\nhigh throughput  357\nHugging Face  31, 32\nfine-tuned LLMs  31\nreference link  251\nHugging Face Hub\nreference link  245\nhuman-generated, human-evaluated \ndatasets  233\nhuman-generated, LLM-evaluated \ndatasets  233\nhybrid search  123\nHypothetical document \nembeddings (HyDE)  121\nI\nIAM role  423\nIDE's MongoDB plugin  94\nIFEval  264\nin-breadth evolving  194\nin-depth evolving  194\ninference deployment types  359\nasynchronous inference  361, 362\noffline batch transform  362\nonline real-time inference  360, 361\ninference pipeline  22\nversus training pipeline  371, 372\ninfrastructure  357, 358\ninfrastructure-as-code (IaC)  393\ninput guardrails  412\ninput test  465\ninstruction dataset\ncreating  178, 196-206\ndata augmentation  193-196\ndata curation  182\ndata decontamination  185\ndata deduplication  184, 185\ndata exploration  189-191\ndata generation  191, 193\ndata quality evaluation  186-189\ndata quantity  180, 181\ngeneral framework  178-180\nhigh-quality data  179\nrule-based filtering  182, 183\nintegration tests  464\ninvariance  467\niterative improvement  246\nJ\nJenkins  405\njobs  437\nK\nkey-value (KV) cache  291-294\nkeywords filtering  183\nKolmogorov-Smirnov (KS)  472\nKullback-Leibler (KL)  247\nL\nLangfuse  413\nLangfuse UI\nexample trace  414, 415\nlarge language model (LLM)  1, 99, 355, 401\nlatency  356\nlength bias  237\n\n\nIndex\n486\nlength filtering  183\nlinting errors  436\nexamples  436\nLLM-as-a-judge strategy  186\nLLM evaluation  235\nversus, ML evaluation  262, 263\nLLM-generated, human-evaluated \ndatasets  234\nLLM-generated, LLM-evaluated datasets  234\nLLMOps  401, 402, 410, 411, 415\nadding, to LLM Twin  434\nguardrails  411, 412\nhuman feedback  411\nprompt monitoring  413\nLLMs, training from scratch\nconcerns  410, 411\nLLM system\nbuilding, with FTI architecture  462, 463\nLLM Twin  2, 5, 6\nCD pipeline  442-444\nCI/CD pipeline flow  434, 435\nCI/CD pipeline, testing  445\nCI pipeline  438\nCT pipeline  446, 448\ninference pipeline deployment \nstrategy  368-370\nMVP, defining  7\nRAG feature pipeline architecture  127, 139\nsignificance  3, 4\nsystem architecture  16\nversus co-pilot  4\nLLM Twin architecture  23\ndata collection pipeline  19\ndesigning, with FTI pipeline design  17\nfeature pipeline  19, 20\ninference pipeline  22\ntechnical details  16, 17\ntraining pipeline  21, 22\nLLM Twin model\ndeploying, to AWS SageMaker  375-385\nLLM Twin RAG feature pipeline\ndispatcher layer  160\nhandlers  162\nimplementing  139\npydantic domain entities  150\nsetting  139\nZenML pipeline and steps  140, 141\nLLM Twin's data collection pipeline\ncrawlers  59, 69\ndesigning  56-60\ndispatcher  66-68\nimplementing  61\nNoSQL data warehouse documents  79, 80\nZenML pipeline and steps  61-65\nLLM Twin service\ndeploying  372\nLLM Twin's pipelines, cloud deployment  415\ncode, containerizing with Docker  424-428\ninfrastructure  416-418\nMongoDB, setting up  418, 419\npipelines, running on AWS  428-431\nQdrant, setting up  419, 420\nResourceLimitExceeded error, \ntroubleshooting after running ZenML \npipeline on SageMaker  432, 433\nZenML, setting up  421-423\nlogs  468\nlow latency  358\nLow-Rank Adaptation (LoRA)  213-215\n\n\nIndex\n487\nM\nmachine learning (ML)  1, 355\nengineering  409\nmanual dataset exploration  189, 190\nmanual process  461\nmanual triggers  448\nMassive Multi-Task Language \nUnderstanding (MMLU)  261\nMaximum Mean Discrepancy (MMD)  472\nMediumCrawler class  77-79\nmetrics  468\ndrifts  469\nmodel metrics  469\nsystem metrics  469\nmetrics-driven development (MDD)  272\nmicroservices architecture  365-367\nversus monolithic architecture  367, 368\nminimum functionality  467\nminimum viable product (MVP)  6\nfeatures  6\nML engineer  410\nML evaluation\nvesus, LLM evaluation  262, 263\nML models\ntraining  464\nMLOps  401-407, 411, 461\nCI/CD pipeline  462\ncontinuous training (CT)  461\nengineering  409\nmanual process  461\nMLOps and LLMOps tools  30, 31\nComet ML  45, 46\nHugging Face  31, 32\nOpik  46, 47\nZenML  32, 33\nMLOps, core components\nfeature store  407\nML metadata store  407\nML pipeline orchestrator  407\nmodel registry  407\nMLOps engineer  410\nMLOps, principles\nautomation  408\nexperiment tracking  408\nmonitoring  408\noperationalization  408\nreproducibility  408\ntesting  408\nversioning  408\nML pipeline automation\nfor CT  12\nML pipelines\nfor ML systems  13\nML systems\nelements  9\nissues, with building  8, 9\ntesting  464\nmodel evaluation  261\ndomain-specific LLM evaluations  265-267\ngeneral-purpose LLM evaluations  263-265\nML, versus LLM evaluation  262, 263\ntask-specific LLM evaluations  267-271\nmodel metrics  469\nmodel optimization strategies  290\ncontinuous batching  294\nkey-value (KV) cache  291, 293\noptimized attention mechanisms  297, 298\nspeculative decoding  295, 296\n\n\nIndex\n488\nmodel parallelism  298\ndata parallelism (DP)  299\npipeline parallelism (PP)  300, 301\ntechniques, combining  303\ntensor parallelism (TP)  301, 302\nmodel quantization  303, 304\nmodel tests  466\nModeration API  413\nMongoDB  47\nsetting up  418, 419\nreference link  418\nMongoDB, as data warehouse\nusage, considerations  60\nmonitoring  468\nlogs  468\nmetrics  468\nversus observability  472\nmonolithic architecture  365\nmonolithic batch pipeline architecture  10\nMT-Bench  264\nN\nNoSQL data warehouse documents  79, 80\ndata categories and user document \nclasses  87-89\nODM class, implementing  82-87\nORM and ODM software patterns  80, 82\nO\nobject-relational mapping (ORM)  154\nobject-vector mapping (OVM)  139\nimplementation  139\nobservability\nversus monitoring  472\nODM class\nimplementing  82-87\nODM software patterns  80, 82\noffline batch transform  362\nonline real-time inference  360, 361\nOpen Arabic LLM Leaderboard  267\nOpenKo-LLM Leaderboard  267\nOpen Medical-LLM Leaderboard  265\nOpen Portuguese LLM Leaderboard  267\nOpik  46, 47, 413\nOptimal Brain Quantization (OBQ) \napproach  312\noptimized attention mechanisms  297, 298\nORM software patterns  80, 82\noutput guardrails  413\noutput test  465\nP\nparameter-efficient fine-tuning techniques\nfull fine-tuning   211, 212\nLoRA  213-215\nQLoRA  215, 216\nParameter-efficient fine-tuning \ntechniques  211\npipeline parallelism (PP)  300\nPiPPy (Pipeline Parallelism for PyTorch) \nlibrary  301\npolicy optimization  246\nposition bias  237\npost-retrieval step, performing\nprompt compression  124\nre-ranking  124\nPost-Training Quantization (PTQ)  304\npreference alignment  245\n\n\nIndex\n489\npreference-based reinforcement \nlearning (PbRL)  246\npreference dataset  230, 232\nChatbots  231\ncode generation  231\ncontent moderation  231\ncreating  230, 237-245\ncreative writing  232\ndata evaluation  233\ndata generation  233\ndata quantity  232\nsummarization  231\ntranslation  232\npre-retrieval steps, performing\ndata indexing  119\nquery optimizing  119\nproduction environment  434\nprompt monitoring  413, 451-457\npull method  136\npush method  136\nPydantic domain entities  150-154\ndata category  151\nOVM  154-159\nstate of data category  151\nPydantic Settings\nreference link  139\nPython ecosystem\ndependency and virtual environment \nmanagement  27-29\nproject installation  26, 27\ntask execution tool  29, 30\nQ\nQA job  438\nQdrant  47, 48\nreference link  419\nsetting up  419, 420\nquantization  303-308\ntechniques  313, 314\nwith GGUF and llama.cpp  309-311\nwith GPTQ and EXL2  311, 312\nQuantization-aware Low-Rank Adaptation \n(QLoRA)  215, 216, 221\nQuantization-Aware Training (QAT)  304\nquery optimization  120\nquery rewriting  121\nquery routing  120\nR\nRAG evaluation  271, 272\nARES  274, 275\nRagas  272-274\nRAG feature pipeline\nchunking  135\ncleaning  135\ndata extraction  134\ndata loading  135\ndata storage, in snapshots  138\ndata warehouse and feature store, \nsyncing  136, 137\nembedding  135\norchestration  138\nRAG feature pipeline architecture\nbatch pipelines  130\nbatch pipelines, versus streaming \npipelines  130-134\ncore steps  134\ndesigning  129\nfeature store  128\ninference pipeline  127\ningestion pipeline  127\nproblem, solution  127, 128\nraw data  128\n",
      "page_number": 501
    },
    {
      "number": 56,
      "title": "Segment 56 (pages 510-517)",
      "start_page": 510,
      "end_page": 517,
      "detection_method": "topic_boundary",
      "content": "Index\n490\nRAG inference pipeline\narchitecture flow  320, 321\nimplementing  318-320, 338\nretrieval module, implementing  339-346\nraw data, into data warehouse\nobtaining  89-94\ntroubleshooting  94, 95\nRecall-Oriented Understudy for Gisting \nEvaluation (ROUGE) metric  267\nreference window  472\nregression tests  464\nReinforcement Learning from Human \nFeedback (RLHF)  245-247, 411\niterative improvement  246\npolicy optimization  246\nreward model learning  246\nreinforcement learning (RL)  246\nreproducibility  473\nrequests per second (RPS)  356\nREST API triggers  448\nRetrieval-Augmented Generation \nAssessment (Ragas)  272-274\nretrieval-augmented generation \n(RAG)  2, 99, 100, 317\nembeddings  107, 108\nembeddings, applications  114\nembeddings, creating  111-114\nembeddings, significance  109, 110\nhallucinations  101\nissues, solving  101\nvanilla RAG framework  101\nvector DBs  115\nretrieval-augmented generation (RAG) \npipeline  206, 261\nreward model learning  246\nreward models  188\nrule-based filtering  182, 183\nrunners  437\nS\nSageMaker  423\nSageMaker Inference deployment  371\nconfiguration  371\nendpoint  371\nInference component  371\nmodel  371\nSageMaker Orchestrator  423\nSageMaker roles\nconfiguring  374, 375\nscalable and secure object storage \nservice (S3)  423\nscalable policy\ncreating  397\nscalable target\nregistering  396\nscaling limits\nmaximum  398\nminimum  398\nscheduled triggers  448\nSelenium tool  69\nissues  95\nsemantic similarity  184\nServer-Sent Events (SSE)  374\nSFT, techniques\nchat templates  208-210\nfine-tune, usage, considerations  206, 207\nhyperparameters, training  216\ninstruction dataset formats  208\nparameter-efficient fine-tuning \ntechniques  211\n\n\nIndex\n491\nSFT techniques, parameters\nbatch size  216, 217\ngradient checkpointing  219\nlearning rate and scheduler  216\nmaximum length and packing  217, 218\nnumber of epochs  218\noptimizers  218\nweight decay  219\nspeculative decoding  295, 296\nstack  422\nstaging environment  434\nstateless real-time architecture  11\nstatistical analysis  190\nstress tests  465\nstyle transfer  2\nsummarization  231\nSupervised Fine-Tuning (SFT)  177, 229, 264\ntechniques, exploring  206\nsystem metrics  469\nsystem tests  464\nT\ntarget drift  470\nTargetTrackingScaling policy  397\ntask-specific LLM evaluations  267-271\ntensor parallelism (TP)  301, 302\nTerraform  393\ntest example  465\ntest job  438\ntest types  465\nacceptance tests  464\nintegration tests  464\nregression tests  464\nstress tests  465\nsystem tests  464\nunit tests  464\ntest window  472\nText Generation Inference (TGI)  294, 373\nthroughput  356, 357\nTime between Tokens (TBT)  413\nTime per Output Token (TPOT)  413\nTime to First Token (TTFT)  413\nTokens per Second (TPS)  413\ntopic clustering  190, 191\nTotal Latency  413\ntraining pipeline  14, 21, 22\nversus inference pipeline  371, 372\ntriggers\nmanual triggers  448\nREST API triggers  448\nscheduled triggers  448\nTwinLlama-3.1-8B\nanswers, evaluating  278-283\nanswers, generating  276-278\nevaluating  275, 276\nresults, analyzing  283-286\nTwinLlama-3.1-8B model  250\nU\nUltraFeedback method  195\nunit tests  464\nUser Acceptance Testing (UAT)  464\nV\nvector DBs  115\nalgorithms, for creating vector index  116\nDB operations  116\nworking  115\n\n\nIndex\n492\nversioning  463\ncode  463\ndata  463\nmodel  463\nVideo Random-Access Memory (VRAM)  291\nW\nwindow types\nreference window  472\ntest window  472\nworkflow  437\nZ\nZenML  32, 33\nartifacts and metadata  39-43\norchestrator  33-37\nreference link  421\nsetting up  421-423\nZenML pipeline  140-142\ncleaned documents, chunking  147-150\ncleaned documents, embedding  147-150\nconfiguring  43, 45\ndata warehouse, querying  143-145\ndocuments, cleaning  146, 147\ndocuments, loading to vector DB  150\nimplementing  61-65\nrunning  43, 45\nzero-point quantization  307\n\n\nDownload a free PDF copy of this book\nThanks for purchasing this book!\nDo you like to read on the go but are unable to carry your print books everywhere?\nIs your eBook purchase not compatible with the device of your choice?\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical \nbooks directly into your application.\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free \ncontent in your inbox daily.\nFollow these simple steps to get the benefits:\n1. \nScan the QR code or visit the link below:\nhttps://packt.link/free-ebook/9781836200079\n2. \nSubmit your proof of purchase.\n3. \nThat’s it! We’ll send your free PDF and other benefits to your email directly.\n",
      "page_number": 510
    },
    {
      "number": 57,
      "title": "Segment 57 (pages 518-522)",
      "start_page": 518,
      "end_page": 522,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 518
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "LLM Engineer’s Handbook\nMaster the art of engineering large language models from \nconcept to production\nPaul Iusztin\nMaxime Labonne\n",
      "content_length": 132,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "LLM Engineer’s Handbook\nCopyright © 2024 Packt Publishing\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in \nany form or by any means, without the prior written permission of the publisher, except in the case of brief \nquotations embedded in critical articles or reviews.\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information \npresented. However, the information contained in this book is sold without warranty, either express or \nimplied. Neither the authors, nor Packt Publishing or its dealers and distributors, will be held liable for any \ndamages caused or alleged to have been caused directly or indirectly by this book.\nPackt Publishing has endeavored to provide trademark information about all of the companies and products \nmentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee \nthe accuracy of this information.\nSenior Publishing Product Manager: Gebin George\nAcquisition Editor – Peer Reviews: Swaroop Singh\nProject Editor: Amisha Vathare\nContent Development Editor: Tanya D’cruz\nCopy Editor: Safis Editing\nTechnical Editor: Karan Sonawane\nProofreader: Safis Editing\nIndexer: Manju Arasan\nPresentation Designer: Rajesh Shirsath\nDeveloper Relations Marketing Executive: Anamika Singh\nFirst published: October 2024\nProduction reference: 2171024\nPublished by Packt Publishing Ltd. \nGrosvenor House\n11 St Paul’s Square\nBirmingham \nB3 1RB, UK.\nISBN 978-1-83620-007-9\nwww.packt.com\n",
      "content_length": 1541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "Forewords\nAs my co-founder at Hugging Face, Clement Delangue, and I often say, AI is becoming the default \nway of building technology.\nOver the past 3 years, LLMs have already had a profound impact on technology, and they are \nbound to have an even greater impact in the coming 5 years. They will be embedded in more and \nmore products and, I believe, at the center of any human activity based on knowledge or creativity.\nFor instance, coders are already leveraging LLMs and changing the way they work, focusing on \nhigher-order thinking and tasks while collaborating with machines. Studio musicians rely on \nAI-powered tools to explore the musical creativity space faster. Lawyers are increasing their impact \nthrough retrieval-augmented generation (RAG) and large databases of case law.\nAt Hugging Face, we’ve always advocated for a future where not just one company or a small \nnumber of scientists control the AI models used by the rest of the population, but instead for a \nfuture where as many people as possible—from as many different backgrounds as possible—are \ncapable of diving into how cutting-edge machine learning models actually work.\nMaxime Labonne and Paul Iusztin have been instrumental in this movement to democratize \nLLMs by writing this book and making sure that as many people as possible can not only use \nthem but also adapt them, fine-tune them, quantize them, and make them efficient enough to \nactually deploy in the real world.\nTheir work is essential, and I’m glad they are making this resource available to the community. \nThis expands the convex hull of human knowledge.\nJulien Chaumond\nCo-founder and CTO, Hugging Face\n",
      "content_length": 1652,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "As someone deeply immersed in the world of machine learning operations, I’m thrilled to en-\ndorse The LLM Engineer’s Handbook. This comprehensive guide arrives at a crucial time when the \ndemand for LLM expertise is skyrocketing across industries.\nWhat sets this book apart is its practical, end-to-end approach. By walking readers through the \ncreation of an LLM Twin, it bridges the often daunting gap between theory and real-world ap-\nplication. From data engineering and model fine-tuning to advanced topics like RAG pipelines \nand inference optimization, the authors leave no stone unturned.\nI’m particularly impressed by the emphasis on MLOps and LLMOps principles. As organizations \nincreasingly rely on LLMs, understanding how to build scalable, reproducible, and robust systems \nis paramount. The inclusion of orchestration strategies and cloud integration showcases the \nauthors’ commitment to equipping readers with truly production-ready skills.\nWhether you’re a seasoned ML practitioner looking to specialize in LLMs or a software engineer \naiming to break into this exciting field, this handbook provides the perfect blend of foundational \nknowledge and cutting-edge techniques. The clear explanations, practical examples, and focus on \nbest practices make it an invaluable resource for anyone serious about mastering LLM engineering.\nIn an era where AI is reshaping industries at breakneck speed, The LLM Engineer’s Handbook stands \nout as an essential guide for navigating the complexities of large language models. It’s not just \na book; it’s a roadmap to becoming a proficient LLM engineer in today’s AI-driven landscape.\nHamza Tahir\nCo-founder and CTO, ZenML\n",
      "content_length": 1678,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "Contributors\nAbout the authors\nPaul Iusztin is a senior ML and MLOps engineer with over seven years of experience building \nGenAI, Computer Vision and MLOps solutions. His latest contribution was at Metaphysic, where he \nserved as one of their core engineers in taking large neural networks to production. He previously \nworked at CoreAI, Everseen, and Continental. He is the Founder of Decoding ML, an educational \nchannel on production-grade ML that provides posts, articles, and open-source courses to help \nothers build real-world ML systems. \nMaxime Labonne is the Head of Post-Training at Liquid AI. He holds a PhD. in ML from the \nPolytechnic Institute of Paris and is recognized as a Google Developer Expert in AI/ML. As an \nactive blogger, he has made significant contributions to the open-source community, including \nthe LLM Course on GitHub, tools such as LLM AutoEval, and several state-of-the-art models like \nNeuralDaredevil. He is the author of the best-selling book Hands-On Graph Neural Networks Using \nPython, published by Packt.\nI want to thank my family and partner. Your unwavering support and patience made this book possible.\n",
      "content_length": 1150,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "About the reviewer\nRany ElHousieny is an AI solutions architect and AI engineering manager with over two decades \nof experience in AI, NLP, and ML. Throughout his career, he has focused on the development and \ndeployment of AI models, authoring multiple articles on AI systems architecture and ethical AI de-\nployment. He has led groundbreaking projects at companies like Microsoft, where he spearheaded \nadvancements in NLP and the Language Understanding Intelligent Service (LUIS). Currently, he \nplays a pivotal role at Clearwater Analytics, driving innovation in GenAI and AI-driven financial \nand investment management solutions.\nI would like to thank Clearwater Analytics for providing a supportive and learning environment that fosters \ngrowth and innovation. The vision of our leaders, always staying ahead with the latest technologies, has been \na constant source of inspiration. Their commitment to AI advancements made my experience of reviewing \nthis book insightful and enriching. Special thanks to my family for their ongoing encouragement throughout \nthis journey.\n",
      "content_length": 1080,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "Join our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 143,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "Table of Contents\nPreface  \n xxi\nChapter 1: Understanding the LLM Twin Concept and Architecture  \n 1\nUnderstanding the LLM Twin concept                                                                                  2\nWhat is an LLM Twin? • 2\nWhy building an LLM Twin matters • 3\nWhy not use ChatGPT (or another similar chatbot)? • 5\nPlanning the MVP of the LLM Twin product                                                                          6\nWhat is an MVP? • 6\nDefining the LLM Twin MVP • 7\nBuilding ML systems with feature/training/inference pipelines                                            8\nThe problem with building ML systems • 8\nThe issue with previous solutions • 10\nThe solution – ML pipelines for ML systems • 13\nThe feature pipeline • 14\nThe training pipeline • 14\nThe inference pipeline • 14\nBenefits of the FTI architecture • 15\nDesigning the system architecture of the LLM Twin                                                            16\nListing the technical details of the LLM Twin architecture • 16\nHow to design the LLM Twin architecture using the FTI pipeline design • 17\nData collection pipeline • 19\n",
      "content_length": 1136,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "Table of Contents\nx\nFeature pipeline • 19\nTraining pipeline • 21\nInference pipeline • 22\nFinal thoughts on the FTI design and the LLM Twin architecture • 22\nSummary                                                                                                                             23\nReferences                                                                                                                           23\nChapter 2: Tooling and Installation  \n 25\nPython ecosystem and project installation                                                                          26\nPoetry: dependency and virtual environment management • 27\nPoe the Poet: task execution tool • 29\nMLOps and LLMOps tooling                                                                                                30\nHugging Face: model registry • 31\nZenML: orchestrator, artifacts, and metadata • 32\nOrchestrator • 33\nArtifacts and metadata • 39\nHow to run and configure a ZenML pipeline • 43\nComet ML: experiment tracker • 45\nOpik: prompt monitoring • 46\nDatabases for storing unstructured and vector data                                                            47\nMongoDB: NoSQL database • 47\nQdrant: vector database • 47\nPreparing for AWS                                                                                                               48\nSetting up an AWS account, an access key, and the CLI • 48\nSageMaker: training and inference compute • 50\nWhy AWS SageMaker? • 51\nSummary                                                                                                                             52\nReferences                                                                                                                           53\nChapter 3: Data Engineering  \n 55\nDesigning the LLM Twin’s data collection pipeline                                                             56\n",
      "content_length": 1899,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "Table of Contents\nxi\nImplementing the LLM Twin’s data collection pipeline • 61\nZenML pipeline and steps • 61\nThe dispatcher: How do you instantiate the right crawler? • 66\nThe crawlers • 69\nBase classes • 69\nGitHubCrawler class • 73\nCustomArticleCrawler class • 75\nMediumCrawler class • 77\nThe NoSQL data warehouse documents • 79\nThe ORM and ODM software patterns • 80\nImplementing the ODM class • 82\nData categories and user document classes • 87\nGathering raw data into the data warehouse                                                                       89\nTroubleshooting • 94\nSelenium issues • 95\nImport our backed-up data • 95\nSummary                                                                                                                             96\nReferences                                                                                                                           96\nChapter 4: RAG Feature Pipeline  \n 99\nUnderstanding RAG                                                                                                           100\nWhy use RAG? • 100\nHallucinations • 101\nOld information • 101\nThe vanilla RAG framework • 101\nIngestion pipeline • 104\nRetrieval pipeline • 105\nGeneration pipeline • 105\nWhat are embeddings? • 107\nWhy embeddings are so powerful • 109\n",
      "content_length": 1308,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "Table of Contents\nxii\nHow are embeddings created? • 111\nApplications of embeddings • 114\nMore on vector DBs • 115\nHow does a vector DB work? • 115\nAlgorithms for creating the vector index • 116\nDB operations • 116\nAn overview of advanced RAG                                                                                             117\nPre-retrieval • 119\nRetrieval • 122\nPost-retrieval • 124\nExploring the LLM Twin’s RAG feature pipeline architecture                                             127\nThe problem we are solving • 127\nThe feature store • 128\nWhere does the raw data come from? • 128\nDesigning the architecture of the RAG feature pipeline • 129\nBatch pipelines • 130\nBatch versus streaming pipelines • 130\nCore steps • 134\nChange data capture: syncing the data warehouse and feature store • 136\nWhy is the data stored in two snapshots? • 138\nOrchestration • 138\nImplementing the LLM Twin’s RAG feature pipeline                                                          139\nSettings • 139\nZenML pipeline and steps • 140\nQuerying the data warehouse • 143\nCleaning the documents • 146\nChunk and embed the cleaned documents • 147\nLoading the documents to the vector DB • 150\nPydantic domain entities • 150\nOVM • 154\nThe dispatcher layer • 160\n",
      "content_length": 1254,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "Table of Contents\nxiii\nThe handlers • 162\nThe cleaning handlers • 163\nThe chunking handlers • 165\nThe embedding handlers • 169\nSummary                                                                                                                            173\nReferences                                                                                                                          174\nChapter 5: Supervised Fine-Tuning  \n 177\nCreating an instruction dataset                                                                                         178\nGeneral framework • 178\nData quantity • 180\nData curation • 182\nRule-based filtering • 182\nData deduplication • 184\nData decontamination • 185\nData quality evaluation • 186\nData exploration • 189\nData generation • 191\nData augmentation • 193\nCreating our own instruction dataset                                                                                196\nExploring SFT and its techniques                                                                                       206\nWhen to fine-tune • 206\nInstruction dataset formats • 208\nChat templates • 208\nParameter-efficient fine-tuning techniques • 211\nFull fine-tuning • 211\nLoRA • 213\nQLoRA • 215\nTraining parameters • 216\nLearning rate and scheduler • 216\nBatch size • 216\n",
      "content_length": 1299,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "Table of Contents\nxiv\nMaximum length and packing • 217\nNumber of epochs • 218\nOptimizers • 218\nWeight decay • 219\nGradient checkpointing • 219\nFine-tuning in practice                                                                                                       219\nSummary                                                                                                                           226\nReferences                                                                                                                         227\nChapter 6: Fine-Tuning with Preference Alignment  \n 229\nUnderstanding preference datasets                                                                                   230\nPreference data • 230\nData quantity • 232\nData generation and evaluation • 233\nGenerating preferences • 233\nTips for data generation • 234\nEvaluating preferences • 235\nCreating our own preference dataset                                                                                 237\nPreference alignment                                                                                                         245\nReinforcement Learning from Human Feedback • 246\nDirect Preference Optimization • 248\nImplementing DPO                                                                                                            250\nSummary                                                                                                                           257\nReferences                                                                                                                         258\nChapter 7: Evaluating LLMs  \n 261\nModel evaluation                                                                                                                261\nComparing ML and LLM evaluation • 262\nGeneral-purpose LLM evaluations • 263\nDomain-specific LLM evaluations • 265\nTask-specific LLM evaluations • 267\n",
      "content_length": 1934,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "Table of Contents\nxv\nRAG evaluation                                                                                                                   271\nRagas • 272\nARES • 274\nEvaluating TwinLlama-3 1-8B                                                                                           275\nGenerating answers • 276\nEvaluating answers • 278\nAnalyzing results • 283\nSummary                                                                                                                           286\nReferences                                                                                                                         287\nChapter 8: Inference Optimization  \n 289\nModel optimization strategies                                                                                           290\nKV cache • 291\nContinuous batching • 294\nSpeculative decoding • 295\nOptimized attention mechanisms • 297\nModel parallelism                                                                                                              298\nData parallelism • 299\nPipeline parallelism • 300\nTensor parallelism • 301\nCombining approaches • 303\nModel quantization                                                                                                           303\nIntroduction to quantization • 304\nQuantization with GGUF and llama.cpp • 309\nQuantization with GPTQ and EXL2 • 311\nOther quantization techniques • 313\nSummary                                                                                                                            314\nReferences                                                                                                                          315\nChapter 9: RAG Inference Pipeline  \n 317\nUnderstanding the LLM Twin’s RAG inference pipeline                                                    318\n",
      "content_length": 1845,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "Table of Contents\nxvi\nExploring the LLM Twin’s advanced RAG techniques                                                         321\nAdvanced RAG pre-retrieval optimizations: query expansion and self-querying • 324\nQuery expansion • 324\nSelf-querying • 328\nAdvanced RAG retrieval optimization: filtered vector search • 332\nAdvanced RAG post-retrieval optimization: reranking • 334\nImplementing the LLM Twin’s RAG inference pipeline                                                      338\nImplementing the retrieval module • 339\nBringing everything together into the RAG inference pipeline • 346\nSummary                                                                                                                            351\nReferences                                                                                                                          351\nChapter 10: Inference Pipeline Deployment  \n 355\nCriteria for choosing deployment types                                                                             356\nThroughput and latency • 356\nData • 357\nUnderstanding inference deployment types                                                                      359\nOnline real-time inference • 360\nAsynchronous inference • 361\nOffline batch transform • 362\nMonolithic versus microservices architecture in model serving                                        363\nMonolithic architecture • 365\nMicroservices architecture • 365\nChoosing between monolithic and microservices architectures • 367\nExploring the LLM Twin’s inference pipeline deployment strategy                                   368\nThe training versus the inference pipeline • 371\nDeploying the LLM Twin service                                                                                       372\nImplementing the LLM microservice using AWS SageMaker • 373\nWhat are Hugging Face’s DLCs? • 373\nConfiguring SageMaker roles • 374\n",
      "content_length": 1915,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "Table of Contents\nxvii\nDeploying the LLM Twin model to AWS SageMaker • 375\nCalling the AWS SageMaker Inference endpoint • 386\nBuilding the business microservice using FastAPI • 390\nAutoscaling capabilities to handle spikes in usage                                                             393\nRegistering a scalable target • 396\nCreating a scalable policy • 397\nMinimum and maximum scaling limits • 398\nCooldown period • 398\nSummary                                                                                                                           399\nReferences                                                                                                                        400\nChapter 11: MLOps and LLMOps  \n 401\nThe path to LLMOps: Understanding its roots in DevOps and MLOps                               402\nDevOps • 403\nThe DevOps lifecycle • 403\nThe core DevOps concepts • 404\nMLOps • 405\nMLOps core components • 407\nMLOps principles • 407\nML vs. MLOps engineering • 409\nLLMOps • 410\nHuman feedback • 411\nGuardrails • 411\nPrompt monitoring • 413\nDeploying the LLM Twin’s pipelines to the cloud                                                               415\nUnderstanding the infrastructure • 416\nSetting up MongoDB • 418\nSetting up Qdrant • 419\nSetting up the ZenML cloud • 421\nContainerize the code using Docker • 424\n",
      "content_length": 1346,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "Table of Contents\nxviii\nRun the pipelines on AWS • 428\nTroubleshooting the ResourceLimitExceeded error after running a ZenML pipeline \non SageMaker • 432\nAdding LLMOps to the LLM Twin                                                                                      434\nLLM Twin’s CI/CD pipeline flow • 434\nMore on formatting errors • 436\nMore on linting errors • 436\nQuick overview of GitHub Actions • 437\nThe CI pipeline • 438\nGitHub Actions CI YAML file • 438\nThe CD pipeline • 442\nTest out the CI/CD pipeline • 445\nThe CT pipeline • 446\nInitial triggers • 448\nTrigger downstream pipelines • 449\nPrompt monitoring • 451\nAlerting • 457\nSummary                                                                                                                           458\nReferences                                                                                                                         459\nAppendix: MLOps Principles  \n 461\n1  Automation or operationalization                                                                                  461\n2  Versioning                                                                                                                      463\n3  Experiment tracking                                                                                                      464\n4  Testing                                                                                                                           464\nTest types • 464\nWhat do we test? • 465\nTest examples • 465\n5  Monitoring                                                                                                                     468\nLogs • 468\nMetrics • 468\n",
      "content_length": 1684,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "Table of Contents\nxix\nSystem metrics • 469\nModel metrics • 469\nDrifts • 469\nMonitoring vs. observability • 472\nAlerts • 473\n6  Reproducibility                                                                                                              473\nOther Books You May Enjoy  \n 477\nIndex  \n 481\n",
      "content_length": 302,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "Preface\nThe field of LLM engineering has rapidly emerged as a critical area in artificial intelligence and \nmachine learning. As LLMs continue to revolutionize natural language processing and genera-\ntion, the demand for professionals who can effectively implement, optimize, and deploy these \nmodels in real-world scenarios has grown exponentially. LLM engineering encompasses a wide \nrange of disciplines, from data preparation and model fine-tuning to inference optimization and \nproduction deployment, requiring a unique blend of software engineering, machine learning \nexpertise, and domain knowledge.\nMachine Learning Operations (MLOps) plays a crucial role in the successful implementation of \nLLMs in production environments. MLOps extends the principles of DevOps to machine learning \nprojects, focusing on automating and streamlining the entire ML lifecycle. For LLMs, MLOps is \nparticularly important due to the complexity and scale of these models. It addresses challeng-\nes such as managing large datasets, handling model versioning, ensuring reproducibility, and \nmaintaining model performance over time. By incorporating MLOps practices, LLM projects can \nachieve greater efficiency, reliability, and scalability, ultimately leading to more successful and \nimpactful deployments.\nThe LLM Engineer’s Handbook is a comprehensive guide to applying best practices to the new \nfield of LLM engineering. Throughout the chapters, readers will find simplified key concepts, \npractical techniques, and experts tips for every stage of the LLM lifecycle. The book covers topics \nsuch as data engineering, supervised fine-tuning, model evaluation, inference optimization, and \nRetrieval-Augmented Generation (RAG) pipeline development.\nTo illustrate these concepts in action, an end-to-end project called the LLM Twin will be developed \nthroughout the book., with the goal of imitating someone’s writing style and personality. This \nuse case will demonstrate how to build a minimum viable product to solve a specific problem, \nusing various aspects of LLM engineering and MLOps.\n",
      "content_length": 2082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "Preface\nxxii\nReaders can expect to gain a deeper understanding of how to collect and prepare data for LLMs, \nfine-tune models for specific tasks, optimize inference performance, and implement RAG pipelines. \nThey will learn how to evaluate LLM performance, align models with human preferences, and \ndeploy LLM-based applications. The book also covers essential MLOps principles and practices, \nenabling readers to build scalable, reproducible, and robust LLM applications.\nWho this book is for\nThis book is intended for a wide range of technology professionals and enthusiasts interested \nin the practical applications of LLMs. It’s ideal for software engineers aiming to transition into \nAI projects. While some familiarity with software development is beneficial, the book explains \nmany concepts from the ground up, making it accessible even to those who are new to AI and \nmachine learning.\nFor those already working with machine learning , this book will enhance your skills in imple-\nmenting and deploying LLM-based systems. We provide a deep dive into the fundamentals of \nMLOps, guiding you through the process of creating a minimum viable product using an open-\nsource LLM to solve real-world problems.\nWhat this book covers\nChapter 1, Understanding the LLM Twin Concept and Architecture, introduces the LLM Twin project, \nwhich is used throughout the book as an end-to-end example of a production-level LLM appli-\ncation, and defines the FTI architecture for building scalable ML systems and applies it to the \nLLM Twin use case.\nChapter 2, Tooling and Installation, presents Python, MLOps, and cloud tools used to build re-\nal-world LLM applications, such as an orchestrator, experiment tracker, prompt monitoring and \nLLM evaluation tool. It shows how to use and install them locally for testing and development.\nChapter 3, Data Engineering, shows the implementation of a data collection pipeline that scrapes \nmultiple sites, such as Medium, GitHub and Substack and stores the raw data in a data warehouse. \nIt emphasizes collecting raw data from dynamic sources over static datasets for real-world ML \napplications. \nChapter 4, RAG Feature Pipeline, introduces RAG fundamental concepts, such as embeddings, the \nvanilla RAG framework, vector databases, and how to optimize RAG applications. It applies the \nRAG theory by architecting and implementing LLM Twin’s RAG feature pipeline using software \nbest practices.\n",
      "content_length": 2429,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "Preface\nxxiii\nChapter 5, Supervised Fine-Tuning, explores the process of refining pre-trained language models \nfor specific tasks using instruction-answer pairs. It covers creating high-quality datasets, imple-\nmenting fine-tuning techniques like full fine-tuning, LoRA, and QLoRA, and provides a practical \ndemonstration of fine-tuning a Llama 3.1 8B model on a custom dataset.\nChapter 6, Fine-Tuning with Preference Alignment, introduces techniques for aligning language \nmodels with human preferences, focusing on Direct Preference Optimization (DPO). It covers \ncreating custom preference datasets, implementing DPO, and provides a practical demonstration \nof aligning the TwinLlama-3.1-8B model using the Unsloth library.\nChapter 7, Evaluating LLMs, details various methods for assessing the performance of language \nmodels and LLM systems. It introduces general-purpose and domain-specific evaluations and dis-\ncusses popular benchmarks. The chapter includes a practical evaluation of the TwinLlama-3.1-8B \nmodel using multiple criteria.\nChapter 8, Inference Optimization, covers key optimization strategies such as speculative decoding, \nmodel parallelism, and weight quantization. It discusses how to improve inference speed, reduce \nlatency, and minimize memory usage, introducing popular inference engines and comparing \ntheir features.\nChapter 9, RAG Inference Pipeline, explores advanced RAG techniques by implementing methods \nsuch as self-query, reranking, and filtered vector search from scratch. It covers designing and \nimplementing the LLM Twin’s RAG inference pipeline and a custom retrieval module similar to \nwhat you see in popular frameworks such as LangChain.\nChapter 10, Inference Pipeline Deployment, introduces ML deployment strategies, such as online, \nasynchronous and batch inference, which will help in architecting and deploying the LLM Twin \nfine-tuned model to AWS SageMaker and building a FastAPI microservice to expose the RAG \ninference pipeline as a RESTful API.\nChapter 11, MLOps and LLMOps, presents what LLMOps is, starting with its roots in DevOps and \nMLOps. This chapter explains how to deploy the LLM Twin project to the cloud, such as the ML \npipelines to AWS and shows how to containerize the code using Docker and build a CI/CD/CT \npipeline. It also adds a prompt monitoring layer on top of LLM Twin’s inference pipeline.\nAppendix, MLOps Principles, covers the six MLOps principles used to build scalable, reproducible, \nand robust ML applications.\n",
      "content_length": 2497,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "Preface\nxxiv\nTo get the most out of this book\nTo maximize your learning experience, you are expected to have, at the very least, a foundational \nunderstanding of software development principles and practices. Familiarity with Python pro-\ngramming is particularly beneficial, as the book’s examples and code snippets are predominantly \nin Python. While prior experience with machine learning concepts is advantageous, it is not \nstrictly necessary, as the book provides explanations for many fundamental AI and ML concepts. \nHowever, you should be comfortable with basic data structures, algorithms, and have some ex-\nperience working with APIs and cloud services. \nFamiliarity with version control systems like Git is assumed, as this book has a GitHub reposi-\ntory for code examples. While this book is designed to be accessible to those who are new to AI \nand LLMs, if you have some background in these areas, you will find it easier to grasp the more \nadvanced concepts and techniques we present.\nDownload the example code files\nThe code bundle for the book is hosted on GitHub at https://github.com/PacktPublishing/\nLLM-Engineers-Handbook. We also have other code bundles from our rich catalog of books and \nvideos available at https://github.com/PacktPublishing/. Check them out!\nDownload the color images\nWe also provide a PDF file that has color images of the screenshots/diagrams used in this book. \nYou can download it here: https://packt.link/gbp/9781836200079.\nConventions used\nThere are a number of text conventions used throughout this book.\nCodeInText: Indicates code words in text, database table names, folder names, filenames, file \nextensions, pathnames, dummy URLs, user input, and Twitter handles. For example: “In the \nformat_samples function, we apply the Alpaca chat template to each individual message.”\nA block of code is set as follows:\ndef format_samples(example):\n    example[\"prompt\"] = alpaca_template.format(example[\"prompt\"])\n    example[\"chosen\"] = example['chosen'] + EOS_TOKEN\n    example[\"rejected\"] = example['rejected'] + EOS_TOKEN\n    return {\"prompt\": example[\"prompt\"], \"chosen\": example[\"chosen\"], \n\"rejected\": example[\"rejected\"]}\n",
      "content_length": 2174,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "Preface\nxxv\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines or \nitems are set in bold:\ndef format_samples(example):\n    example[\"prompt\"] = alpaca_template.format(example[\"prompt\"])\n    example[\"chosen\"] = example['chosen'] + EOS_TOKEN\n    example[\"rejected\"] = example['rejected'] + EOS_TOKEN\n    return {\"prompt\": example[\"prompt\"], \"chosen\": example[\"chosen\"], \n\"rejected\": example[\"rejected\"]}\nAny command-line input or output is written as follows:\npoetry install --without aws\nBold: Indicates a new term, an important word, or words that you see on the screen. For instance, \nwords in menus or dialog boxes appear in the text like this. For example: “To do so, go to the Set-\ntings tab at the top of the forked repository in GitHub. In the left panel, in the Security section, \nclick on the Secrets and Variables toggle and, finally, click on Actions.”\nGet in touch\nFeedback from our readers is always welcome.\nGeneral feedback: Email feedback@packtpub.com and mention the book’s title in the subject of \nyour message. If you have questions about any aspect of this book, please email us at questions@\npacktpub.com.\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do \nhappen. If you have found a mistake in this book, we would be grateful if you reported this to us. \nPlease visit http://www.packtpub.com/submit-errata, click Submit Errata, and fill in the form.\nWarnings or important notes appear like this.\nTips and tricks appear like this.\n",
      "content_length": 1536,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "Preface\nxxvi\nPiracy: If you come across any illegal copies of our works in any form on the internet, we would \nbe grateful if you would provide us with the location address or website name. Please contact us \nat copyright@packtpub.com with a link to the material.\nIf you are interested in becoming an author: If there is a topic that you have expertise in and you \nare interested in either writing or contributing to a book, please visit http://authors.packtpub.\ncom.\nShare your thoughts\nOnce you’ve read LLM Engineer’s Handbook, First Edition, we’d love to hear your thoughts! Please \nclick here to go straight to the Amazon review page for this book and share your feedback.\nYour review is important to us and the tech community and will help us make sure we’re deliv-\nering excellent quality content.\n",
      "content_length": 804,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "Download a free PDF copy of this book\nThanks for purchasing this book!\nDo you like to read on the go but are unable to carry your print books everywhere?\nIs your eBook purchase not compatible with the device of your choice?\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical \nbooks directly into your application.\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free \ncontent in your inbox daily.\nFollow these simple steps to get the benefits:\n1. \nScan the QR code or visit the link below:\nhttps://packt.link/free-ebook/9781836200079\n2. \nSubmit your proof of purchase.\n3. \nThat’s it! We’ll send your free PDF and other benefits to your email directly.\n",
      "content_length": 841,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "1\nUnderstanding the LLM Twin \nConcept and Architecture\nBy the end of this book, we will have walked you through the journey of building an end-to-end \nlarge language model (LLM) product. We firmly believe that the best way to learn about LLMs \nand production machine learning (ML) is to get your hands dirty and build systems. This book \nwill show you how to build an LLM Twin, an AI character that learns to write like a particular \nperson by incorporating its style, voice, and personality into an LLM. Using this example, we will \nwalk you through the complete ML life cycle, from data gathering to deployment and monitoring. \nMost of the concepts learned while implementing your LLM Twin can be applied in other LLM-\nbased or ML applications.\nWhen starting to implement a new product, from an engineering point of view, there are three \nplanning steps we must go through before we start building. First, it is critical to understand the \nproblem we are trying to solve and what we want to build. In our case, what exactly is an LLM Twin, \nand why build it? This step is where we must dream and focus on the “Why.” Secondly, to reflect \na real-world scenario, we will design the first iteration of a product with minimum functionality. \nHere, we must clearly define the core features required to create a working and valuable product. \nThe choices are made based on the timeline, resources, and team’s knowledge. This is where we \nbridge the gap between dreaming and focusing on what is realistic and eventually answer the \nfollowing question: “What are we going to build?”.\nFinally, we will go through a system design step, laying out the core architecture and design choices \nused to build the LLM system. Note that the first two components are primarily product-related, \nwhile the last one is technical and focuses on the “How.”\n",
      "content_length": 1836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "Understanding the LLM Twin Concept and Architecture\n2\nThese three steps are natural in building a real-world product. Even if the first two do not require \nmuch ML knowledge, it is critical to go through them to understand “how” to build the product \nwith a clear vision. In a nutshell, this chapter covers the following topics:\n•\nUnderstanding the LLM Twin concept\n•\nPlanning the MVP of the LLM Twin product\n•\nBuilding ML systems with feature/training/inference pipelines\n•\nDesigning the system architecture of the LLM Twin\nBy the end of this chapter, you will have a clear picture of what you will learn to build throughout \nthe book.\nUnderstanding the LLM Twin concept\nThe first step is to have a clear vision of what we want to create and why it’s valuable to build it. \nThe concept of an LLM Twin is new. Thus, before diving into the technical details, it is essential \nto understand what it is, what we should expect from it, and how it should work. Having a solid \nintuition of your end goal makes it much easier to digest the theory, code, and infrastructure \npresented in this book.\nWhat is an LLM Twin?\nIn a few words, an LLM Twin is an AI character that incorporates your writing style, voice, and \npersonality into an LLM, which is a complex AI model. It is a digital version of yourself projected\ninto an LLM. Instead of a generic LLM trained on the whole internet, an LLM Twin is fine-tuned \non yourself. Naturally, as an ML model reflects the data it is trained on, this LLM will incorporate \nyour writing style, voice, and personality. We intentionally used the word “projected.” As with \nany other projection, you lose a lot of information along the way. Thus, this LLM will not be you; \nit will copy the side of you reflected in the data it was trained on.\nIt is essential to understand that an LLM reflects the data it was trained on. If you feed it Shake-\nspeare, it will start writing like him. If you train it on Billie Eilish, it will start writing songs in \nher style. This is also known as style transfer. This concept is prevalent in generating images, too. \nFor example, let’s say you want to create a cat image using Van Gogh’s style. We will leverage the \nstyle transfer strategy, but instead of choosing a personality, we will do it on our own persona.\nTo adjust the LLM to a given style and voice along with fine-tuning, we will also leverage various \nadvanced retrieval-augmented generation (RAG) techniques to condition the autoregressive \nprocess with previous embeddings of ourselves. \n",
      "content_length": 2521,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "Chapter 1\n3\nWe will explore the details in Chapter 5 on fine-tuning and Chapters 4 and 9 on RAG, but for now, \nlet’s look at a few examples to intuitively understand what we stated previously.\nHere are some scenarios of what you can fine-tune an LLM on to become your twin:\n• \nLinkedIn posts and X threads: Specialize the LLM in writing social media content.\n• \nMessages with your friends and family: Adapt the LLM to an unfiltered version of yourself.\n• \nAcademic papers and articles: Calibrate the LLM in writing formal and educative content.\n• \nCode: Specialize the LLM in implementing code as you would.\nAll the preceding scenarios can be reduced to one core strategy: collecting your digital data (or \nsome parts of it) and feeding it to an LLM using different algorithms. Ultimately, the LLM reflects \nthe voice and style of the collected data. Easy, right?\nUnfortunately, this raises many technical and moral issues. First, on the technical side, how can \nwe access this data? Do we have enough digital data to project ourselves into an LLM? What kind \nof data would be valuable? Secondly, on the moral side, is it OK to do this in the first place? Do \nwe want to create a copycat of ourselves? Will it write using our voice and personality, or just try \nto replicate it?\nRemember that the role of this section is not to bother with the “What” and “How” but with the \n“Why.” Let’s understand why it makes sense to have your LLM Twin, why it can be valuable, and \nwhy it is morally correct if we frame the problem correctly.\nWhy building an LLM Twin matters\nAs an engineer (or any other professional career), building a personal brand is more valuable than \na standard CV. The biggest issue with creating a personal brand is that writing content on plat-\nforms such as LinkedIn, X, or Medium takes a lot of time. Even if you enjoy writing and creating \ncontent, you will eventually run out of inspiration or time and feel like you need assistance. We \ndon’t want to transform this section into a pitch, but we have to understand the scope of this \nproduct/project clearly.\nWe want to build an LLM Twin to write personalized content on LinkedIn, X, Instagram, Sub-\nstack, and Medium (or other blogs) using our style and voice. It will not be used in any immoral \nscenarios, but it will act as your writing co-pilot. Based on what we will teach you in this book, \nyou can get creative and adapt it to various use cases, but we will focus on the niche of generating \nsocial media content and articles. Thus, instead of writing the content from scratch, we can feed \nthe skeleton of our main idea to the LLM Twin and let it do the grunt work. \n",
      "content_length": 2646,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "Understanding the LLM Twin Concept and Architecture\n4\nUltimately, we will have to check whether everything is correct and format it to our liking (more \non the concrete features in the Planning the MVP of the LLM Twin product section). Hence, we proj-\nect ourselves into a content-writing LLM Twin that will help us automate our writing process. It \nwill likely fail if we try to use this particular LLM in a different scenario, as this is where we will \nspecialize the LLM through fine-tuning, prompt engineering, and RAG.\nSo, why does building an LLM Twin matter? It helps you do the following:\n•\nCreate your brand\n•\nAutomate the writing process\n•\nBrainstorm new creative ideas\nAlso, it is critical to understand that building an LLM Twin is entirely moral. The LLM will be \nfine-tuned only on our personal digital data. We won’t collect and use other people’s data to try \nto impersonate anyone’s identity. We have a clear goal in mind: creating our personalized writing \ncopycat. Everyone will have their own LLM Twin with restricted access.\nOf course, many security concerns are involved, but we won’t go into that here as it could be a \nbook in itself.\nWhat’s the difference between a co-pilot and an LLM Twin?\nA co-pilot and digital twin are two different concepts that work together and can be \ncombined into a powerful solution:\n•\nThe co-pilot is an AI assistant or tool that augments human users in various \nprogramming, writing, or content creation tasks.\n•\nThe twin serves as a 1:1 digital representation of a real-world entity, often \nusing AI to bridge the gap between the physical and digital worlds. For in-\nstance, an LLM Twin is an LLM that learns to mimic your voice, personality, \nand writing style.\nWith these definitions in mind, a writing and content creation AI assistant who \nwrites like you is your LLM Twin co-pilot.\n",
      "content_length": 1844,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "Chapter 1\n5\nWhy not use ChatGPT (or another similar chatbot)?\nWe have already provided the answer. ChatGPT is not personalized to your writing style and voice. \nInstead, it is very generic, unarticulated, and wordy. Maintaining an original voice is critical for \nlong-term success when building your brand. Thus, directly using ChatGPT or Gemini will not \nyield the most optimal results. Even if you are OK with sharing impersonalized content, mindlessly \nusing ChatGPT can result in the following:\n•\nMisinformation due to hallucination: Manually checking the results for hallucinations or \nusing third-party tools to evaluate your results is a tedious and unproductive experience.\n•\nTedious manual prompting: You must manually craft your prompts and inject external \ninformation, which is a tiresome experience. Also, the generated answers will be hard to \nreplicate between multiple sessions as you don’t have complete control over your prompts \nand injected data. You can solve part of this problem using an API and a tool such as \nLangChain, but you need programming experience to do so.\nFrom our experience, if you want high-quality content that provides real value, you will spend \nmore time debugging the generated text than writing it yourself.\nThe key of the LLM Twin stands in the following:\n• \nWhat data we collect\n•\nHow we preprocess the data\n•\nHow we feed the data into the LLM\n•\nHow we chain multiple prompts for the desired results\n•\nHow we evaluate the generated content\nThe LLM itself is important, but we want to highlight that using ChatGPT’s web interface is \nexceptionally tedious in managing and injecting various data sources or evaluating the outputs. \nThe solution is to build an LLM system that encapsulates and automates all the following steps \n(manually replicating them each time is not a long-term and feasible solution):\n• \nData collection\n• \nData preprocessing\nThis subsection will refer to using ChatGPT (or another similar chatbot) just in the \ncontext of generating personalized content.\n",
      "content_length": 2024,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "Understanding the LLM Twin Concept and Architecture\n6\n•\nData storage, versioning, and retrieval\n•\nLLM fine-tuning\n•\nRAG\n•\nContent generation evaluation\nNote that we never said not to use OpenAI’s GPT API, just that the LLM framework we will pres-\nent is LLM-agnostic. Thus, if it can be manipulated programmatically and exposes a fine-tuning \ninterface, it can be integrated into the LLM Twin system we will learn to build. The key to most \nsuccessful ML products is to be data-centric and make your architecture model-agnostic. Thus, \nyou can quickly experiment with multiple models on your specific data.\nPlanning the MVP of the LLM Twin product\nNow that we understand what an LLM Twin is and why we want to build it, we must clearly define \nthe product’s features. In this book, we will focus on the first iteration, often labeled the minimum \nviable product (MVP), to follow the natural cycle of most products. Here, the main objective is \nto align our ideas with realistic and doable business objectives using the available resources to \nproduce the product. Even as an engineer, as you grow up in responsibilities, you must go through \nthese steps to bridge the gap between the business needs and what can be implemented.\nWhat is an MVP?\nAn MVP is a version of a product that includes just enough features to draw in early users and test \nthe viability of the product concept in the initial stages of development. Usually, the purpose of \nthe MVP is to gather insights from the market with minimal effort.\nAn MVP is a powerful strategy because of the following reasons:\n• \nAccelerated time-to-market: Launch a product quickly to gain early traction\n• \nIdea validation: Test it with real users before investing in the full development of the \nproduct\n• \nMarket research: Gain insights into what resonates with the target audience\n• \nRisk minimization: Reduces the time and resources needed for a product that might not \nachieve market success\nSticking to the V in MVP is essential, meaning the product must be viable. The product must \nprovide an end-to-end user journey without half-implemented features, even if the product is \nminimal. It must be a working product with a good user experience that people will love and \nwant to keep using to see how it evolves to its full potential.\n",
      "content_length": 2292,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "Chapter 1\n7\nDefining the LLM Twin MVP\nAs a thought experiment, let’s assume that instead of building this project for this book, we want \nto make a real product. In that case, what are our resources? Well, unfortunately, not many:\n• \nWe are a team of three people with two ML engineers and one ML researcher\n•\nOur laptops\n•\nPersonal funding for computing, such as training LLMs\n•\nOur enthusiasm\nAs you can see, we don’t have many resources. Even if this is just a thought experiment, it reflects \nthe reality for most start-ups at the beginning of their journey. Thus, we must be very strategic \nin defining our LLM Twin MVP and what features we want to pick. Our goal is simple: we want \nto maximize the product’s value relative to the effort and resources poured into it.\nTo keep it simple, we will build the features that can do the following for the LLM Twin:\n•\nCollect data from your LinkedIn, Medium, Substack, and GitHub profiles\n•\nFine-tune an open-source LLM using the collected data\n•\nPopulate a vector database (DB) using our digital data for RAG\n•\nCreate LinkedIn posts leveraging the following:\n•\nUser prompts\n•\nRAG to reuse and reference old content\n•\nNew posts, articles, or papers as additional knowledge to the LLM\n• \nHave a simple web interface to interact with the LLM Twin and be able to do the following:\n• \nConfigure your social media links and trigger the collection step\n• \nSend prompts or links to external resources\nThat will be the LLM Twin MVP. Even if it doesn’t sound like much, remember that we must make \nthis system cost effective, scalable, and modular.\nEven if we focus only on the core features of the LLM Twin defined in this section, we \nwill build the product with the latest LLM research and best software engineering \nand MLOps practices in mind. We aim to show you how to engineer a cost-effective \nand scalable LLM application.\n",
      "content_length": 1871,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "Understanding the LLM Twin Concept and Architecture\n8\nUntil now, we have examined the LLM Twin from the users’ and businesses’ perspectives. The last \nstep is to examine it from an engineering perspective and define a development plan to under-\nstand how to solve it technically. From now on, the book’s focus will be on the implementation \nof the LLM Twin.\nBuilding ML systems with feature/training/inference \npipelines\nBefore diving into the specifics of the LLM Twin architecture, we must understand an ML system \npattern at the core of the architecture, known as the feature/training/inference (FTI) architecture. \nThis section will present a general overview of the FTI pipeline design and how it can structure \nan ML application.\nLet’s see how we can apply the FTI pipelines to the LLM Twin architecture.\nThe problem with building ML systems\nBuilding production-ready ML systems is much more than just training a model. From an en-\ngineering point of view, training the model is the most straightforward step in most use cases. \nHowever, training a model becomes complex when deciding on the correct architecture and \nhyperparameters. That’s not an engineering problem but a research problem.\nAt this point, we want to focus on how to design a production-ready architecture. Training a \nmodel with high accuracy is extremely valuable, but just by training it on a static dataset, you \nare far from deploying it robustly. We have to consider how to do the following:\n•\nIngest, clean, and validate fresh data\n•\nTraining versus inference setups\n•\nCompute and serve features in the right environment\n•\nServe the model in a cost-effective way\n• \nVersion, track, and share the datasets and models\n• \nMonitor your infrastructure and models\n• \nDeploy the model on a scalable infrastructure\n• \nAutomate the deployments and training\nThese are the types of problems an ML or MLOps engineer must consider, while the research or \ndata science team is often responsible for training the model.\n",
      "content_length": 1986,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "Chapter 1\n9\nFigure 1.1: Common elements from an ML system\nThe preceding figure shows all the components the Google Cloud team suggests that a mature ML \nand MLOps system requires. Along with the ML code, there are many moving pieces. The rest of \nthe system comprises configuration, automation, data collection, data verification, testing and \ndebugging, resource management, model analysis, process and metadata management, serving \ninfrastructure, and monitoring. The point is that there are many components we must consider \nwhen productionizing an ML model.\nThus, the critical question is this: How do we connect all these components into a single homog-\nenous system? We must create a boilerplate for clearly designing ML systems to answer that \nquestion.\nSimilar solutions exist for classic software. For example, if you zoom out, most software appli-\ncations can be split between a DB, business logic, and UI layer. Every layer can be as complex as \nneeded, but at a high-level overview, the architecture of standard software can be boiled down \nto the previous three components.\nDo we have something similar for ML applications? The first step is to examine previous solutions \nand why they are unsuitable for building scalable ML systems.\n",
      "content_length": 1248,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "Understanding the LLM Twin Concept and Architecture\n10\nThe issue with previous solutions\nIn Figure 1.2, you can observe the typical architecture present in most ML applications. It is based \non a monolithic batch architecture that couples the feature creation, model training, and infer-\nence into the same component. By taking this approach, you quickly solve one critical problem in \nthe ML world: the training-serving skew. The training-serving skew happens when the features \npassed to the model are computed differently at training and inference time.\nIn this architecture, the features are created using the same code. Hence, the training-serving \nskew issue is solved by default. This pattern works fine when working with small data. The \npipeline runs on a schedule in batch mode, and the predictions are consumed by a third-party \napplication such as a dashboard.\nFigure 1.2: Monolithic batch pipeline architecture\nUnfortunately, building a monolithic batch system raises many other issues, such as the following:\n•\nFeatures are not reusable (by your system or others)\n•\nIf the data increases, you have to refactor the whole code to support PySpark or Ray\n•\nIt’s hard to rewrite the prediction module in a more efficient language such as C++, Java, \nor Rust\n",
      "content_length": 1267,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "Chapter 1\n11\n•\nIt’s hard to share the work between multiple teams between the features, training, and \nprediction modules\n•\nIt’s impossible to switch to streaming technology for real-time training\nIn Figure 1.3, we can see a similar scenario for a real-time system. This use case introduces an-\nother issue in addition to what we listed before. To make the predictions, we have to transfer the \nwhole state through the client request so the features can be computed and passed to the model.\nConsider the scenario of computing movie recommendations for a user. Instead of simply pass-\ning the user ID, we must transmit the entire user state, including their name, age, gender, movie \nhistory, and more. This approach is fraught with potential errors, as the client must understand \nhow to access this state, and it’s tightly coupled with the model service.\nAnother example would be when implementing an LLM with RAG support. The documents we add \nas context along the query represent our external state. If we didn’t store the records in a vector \nDB, we would have to pass them with the user query. To do so, the client must know how to query \nand retrieve the documents, which is not feasible. It is an antipattern for the client application \nto know how to access or compute the features. If you don’t understand how RAG works, we will \nexplain it in detail in Chapters 8 and 9.\nFigure 1.3: Stateless real-time architecture\n",
      "content_length": 1426,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "Understanding the LLM Twin Concept and Architecture\n12\nIn conclusion, our problem is accessing the features to make predictions without passing them at \nthe client’s request. For example, based on our first user movie recommendation example, how \ncan we predict the recommendations solely based on the user’s ID? Remember these questions, \nas we will answer them shortly.\nUltimately, on the other spectrum, Google Cloud provides a production-ready architecture, as \nshown in Figure 1.4. Unfortunately, even if it’s a feasible solution, it’s very complex and not intu-\nitive. You will have difficulty understanding this if you are not highly experienced in deploying \nand keeping ML models in production. Also, it is not straightforward to understand how to start \nsmall and grow the system in time.\nThe following image is reproduced from work created and shared by Google and used according \nto terms described in the Creative Commons 4.0 Attribution License:\nFigure 1.4: ML pipeline automation for CT (source: https://cloud.google.com/architecture/\nmlops-continuous-delivery-and-automation-pipelines-in-machine-learning)\n",
      "content_length": 1122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "Chapter 1\n13\nBut here is where the FTI pipeline architectures kick in. The following section will show you how \nto solve these fundamental issues using an intuitive ML design.\nThe solution – ML pipelines for ML systems\nThe solution is based on creating a clear and straightforward mind map that any team or person \ncan follow to compute the features, train the model, and make predictions. Based on these three \ncritical steps that any ML system requires, the pattern is known as the FTI pipeline. So, how does \nthis differ from what we presented before?\nThe pattern suggests that any ML system can be boiled down to these three pipelines: feature, \ntraining, and inference (similar to the DB, business logic, and UI layers from classic software). \nThis is powerful, as we can clearly define the scope and interface of each pipeline. Also, it’s easier \nto understand how the three components interact. Ultimately, we have just three instead of 20 \nmoving pieces, as suggested in Figure 1.4, which is much easier to work with and define.\nAs shown in Figure 1.5, we have the feature, training, and inference pipelines. We will zoom in on \neach of them and understand their scope and interface.\nFigure 1.5: FTI pipelines architecture\n",
      "content_length": 1231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "Understanding the LLM Twin Concept and Architecture\n14\nBefore going into the details, it is essential to understand that each pipeline is a different com-\nponent that can run on a different process or hardware. Thus, each pipeline can be written using \na different technology, by a different team, or scaled differently. The key idea is that the design \nis very flexible to the needs of your team. It acts as a mind map for structuring your architecture.\nThe feature pipeline\nThe feature pipeline takes raw data as input, processes it, and outputs the features and labels \nrequired by the model for training or inference. Instead of directly passing them to the model, the \nfeatures and labels are stored inside a feature store. Its responsibility is to store, version, track, and \nshare the features. By saving the features in a feature store, we always have a state of our features. \nThus, we can easily send the features to the training and inference pipelines.\nAs the data is versioned, we can always ensure that the training and inference time features match. \nThus, we avoid the training-serving skew problem.\nThe training pipeline\nThe training pipeline takes the features and labels from the features stored as input and outputs \na train model or models. The models are stored in a model registry. Its role is similar to that of \nfeature stores, but this time, the model is the first-class citizen. Thus, the model registry will store, \nversion, track, and share the model with the inference pipeline.\nAlso, most modern model registries support a metadata store that allows you to specify essential \naspects of how the model was trained. The most important are the features, labels, and their \nversion used to train the model. Thus, we will always know what data the model was trained on.\nThe inference pipeline\nThe inference pipeline takes as input the features and labels from the feature store and the trained \nmodel from the model registry. With these two, predictions can be easily made in either batch \nor real-time mode.\nAs this is a versatile pattern, it is up to you to decide what you do with your predictions. If it’s a \nbatch system, they will probably be stored in a DB. If it’s a real-time system, the predictions will \nbe served to the client who requested them. Additionally, the features, labels, and models are \nversioned. We can easily upgrade or roll back the deployment of the model. For example, we will \nalways know that model v1 uses features F1, F2, and F3, and model v2 uses F2, F3, and F4. Thus, \nwe can quickly change the connections between the model and features.\n",
      "content_length": 2601,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "Chapter 1\n15\nBenefits of the FTI architecture\nTo conclude, the most important thing you must remember about the FTI pipelines is their in-\nterface:\n• \nThe feature pipeline takes in data and outputs the features and labels saved to the feature \nstore.\n• \nThe training pipeline queries the features store for features and labels and outputs a \nmodel to the model registry.\n• \nThe inference pipeline uses the features from the feature store and the model from the \nmodel registry to make predictions.\nIt doesn’t matter how complex your ML system gets, these interfaces will remain the same.\nNow that we understand better how the pattern works, we want to highlight the main benefits \nof using this pattern:\n• \nAs you have just three components, it is intuitive to use and easy to understand.\n• \nEach component can be written into its tech stack, so we can quickly adapt them to specific \nneeds, such as big or streaming data. Also, it allows us to pick the best tools for the job.\n• \nAs there is a transparent interface between the three components, each one can be de-\nveloped by a different team (if necessary), making the development more manageable \nand scalable.\n• \nEvery component can be deployed, scaled, and monitored independently.\nThe final thing you must understand about the FTI pattern is that the system doesn’t have to \ncontain only three pipelines. In most cases, it will include more. For example, the feature pipeline \ncan be composed of a service that computes the features and one that validates the data. Also, the \ntraining pipeline can be composed of the training and evaluation components.\nThe FTI pipelines act as logical layers. Thus, it is perfectly fine for each to be complex and contain \nmultiple services. However, what is essential is to stick to the same interface on how the FTI pipe-\nlines interact with each other through the feature store and model registries. By doing so, each \nFTI component can evolve differently, without knowing the details of each other and without \nbreaking the system on new changes.\n",
      "content_length": 2043,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "Understanding the LLM Twin Concept and Architecture\n16\nNow that we understand the FTI pipeline architecture, the final step of this chapter is to see how \nit can be applied to the LLM Twin use case.\nDesigning the system architecture of the LLM Twin\nIn this section, we will list the concrete technical details of the LLM Twin application and under-\nstand how we can solve them by designing our LLM system using the FTI architecture. However, \nbefore diving into the pipelines, we want to highlight that we won’t focus on the tooling or the \ntech stack at this step. We only want to define a high-level architecture of the system, which is \nlanguage-, framework-, platform-, and infrastructure-agnostic at this point. We will focus on \neach component’s scope, interface, and interconnectivity. In future chapters, we will cover the \nimplementation details and tech stack.\nListing the technical details of the LLM Twin architecture\nUntil now, we defined what the LLM Twin should support from the user’s point of view. Now, \nlet’s clarify the requirements of the ML system from a purely technical perspective:\n• \nOn the data side, we have to do the following:\n• \nCollect data from LinkedIn, Medium, Substack, and GitHub completely autono-\nmously and on a schedule\n•\nStandardize the crawled data and store it in a data warehouse\n•\nClean the raw data\n•\nCreate instruct datasets for fine-tuning an LLM\n•\nChunk and embed the cleaned data. Store the vectorized data into a vector DB \nfor RAG.\n• \nFor training, we have to do the following:\n•\nFine-tune LLMs of various sizes (7B, 14B, 30B, or 70B parameters)\n•\nFine-tune on instruction datasets of multiple sizes\n•\nSwitch between LLM types (for example, between Mistral, Llama, and GPT)\n•\nTrack and compare experiments\nTo learn more about the FTI pipeline pattern, consider reading From MLOps to ML \nSystems with Feature/Training/Inference Pipelines by Jim Dowling, CEO and co-founder \nof Hopsworks: https://www.hopsworks.ai/post/mlops-to-ml-systems-with-\nfti-pipelines. His article inspired this section.\n",
      "content_length": 2046,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "Chapter 1\n17\n• \nTest potential production LLM candidates before deploying them\n• \nAutomatically start the training when new instruction datasets are available.\n• \nThe inference code will have the following properties:\n• \nA REST API interface for clients to interact with the LLM Twin\n• \nAccess to the vector DB in real time for RAG\n• \nInference with LLMs of various sizes\n• \nAutoscaling based on user requests\n• \nAutomatically deploy the LLMs that pass the evaluation step.\n• \nThe system will support the following LLMOps features:\n•\nInstruction dataset versioning, lineage, and reusability\n•\nModel versioning, lineage, and reusability\n•\nExperiment tracking\n•\nContinuous training, continuous integration, and continuous delivery (CT/\nCI/CD)\n•\nPrompt and system monitoring\nThe preceding list is quite comprehensive. We could have detailed it even more, but at this point, \nwe want to focus on the core functionality. When implementing each component, we will look \ninto all the little details. But for now, the fundamental question we must ask ourselves is this: \nHow can we apply the FTI pipeline design to implement the preceding list of requirements?\nHow to design the LLM Twin architecture using the FTI \npipeline design\nWe will split the system into four core components. You will ask yourself this: “Four? Why not \nthree, as the FTI pipeline design clearly states?” That is a great question. Fortunately, the answer \nis simple. We must also implement the data pipeline along the three feature/training/inference \npipelines. According to best practices:\n• \nThe data engineering team owns the data pipeline\n• \nThe ML engineering team owns the FTI pipelines.\nIf any technical requirement doesn’t make sense now, bear with us. To avoid repe-\ntition, we will examine the details in their specific chapter.\n",
      "content_length": 1806,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "Understanding the LLM Twin Concept and Architecture\n18\nGiven our goal of building an MVP with a small team, we must implement the entire application. \nThis includes defining the data collection and FTI pipelines. Tackling a problem end to end is \noften encountered in start-ups that can’t afford dedicated teams. Thus, engineers have to wear \nmany hats, depending on the state of the product. Nevertheless, in any scenario, knowing how \nan end-to-end ML system works is valuable for better understanding other people’s work.\nFigure 1.6 shows the LLM system architecture. The best way to understand it is to review the four \ncomponents individually and explain how they work.\nFigure 1.6: LLM Twin high-level architecture\n",
      "content_length": 720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "Chapter 1\n19\nData collection pipeline\nThe data collection pipeline involves crawling your personal data from Medium, Substack, Linke-\ndIn, and GitHub. As a data pipeline, we will use the extract, load, transform (ETL) pattern to \nextract data from social media platforms, standardize it, and load it into a data warehouse.\nThe output of this component will be a NoSQL DB, which will act as our data warehouse. As we \nwork with text data, which is naturally unstructured, a NoSQL DB fits like a glove.\nEven though a NoSQL DB, such as MongoDB, is not labeled as a data warehouse, from our point \nof view, it will act as one. Why? Because it stores standardized raw data gathered by various ETL \npipelines that are ready to be ingested into an ML system.\nThe collected digital data is binned into three categories:\n• \nArticles (Medium, Substack)\n• \nPosts (LinkedIn)\n• \nCode (GitHub)\nWe want to abstract away the platform where the data was crawled. For example, when feeding \nan article to the LLM, knowing it came from Medium or Substack is not essential. We can keep \nthe source URL as metadata to give references. However, from the processing, fine-tuning, and \nRAG points of view, it is vital to know what type of data we ingested, as each category must be \nprocessed differently. For example, the chunking strategy between a post, article, and piece of \ncode will look different.\nAlso, by grouping the data by category, not the source, we can quickly plug data from other plat-\nforms, such as X into the posts or GitLab into the code collection. As a modular system, we must \nattach an additional ETL in the data collection pipeline, and everything else will work without \nfurther code modifications.\nFeature pipeline\nThe feature pipeline’s role is to take raw articles, posts, and code data points from the data ware-\nhouse, process them, and load them into the feature store. \nIt is critical to highlight that the data collection pipeline is designed to crawl data \nonly from your social media platform. It will not have access to other people. As an \nexample for this book, we agreed to make our collected data available for learning \npurposes. Otherwise, using other people’s data without their consent is not moral.\n",
      "content_length": 2223,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "Understanding the LLM Twin Concept and Architecture\n20\nThe characteristics of the FTI pattern are already present.\nHere are some custom properties of the LLM Twin’s feature pipeline:\n•\nIt processes three types of data differently: articles, posts, and code\n•\nIt contains three main processing steps necessary for fine-tuning and RAG: cleaning, \nchunking, and embedding\n•\nIt creates two snapshots of the digital data, one after cleaning (used for fine-tuning) and \none after embedding (used for RAG)\n•\nIt uses a logical feature store instead of a specialized feature store\nLet’s zoom in on the logical feature store part a bit. As with any RAG-based system, one of the \ncentral pieces of the infrastructure is a vector DB. Instead of integrating another DB, more con-\ncretely, a specialized feature store, we used the vector DB, plus some additional logic to check all \nthe properties of a feature store our system needs.\nThe vector DB doesn’t offer the concept of a training dataset, but it can be used as a NoSQL DB. \nThis means we can access data points using their ID and collection name. Thus, we can easily \nquery the vector DB for new data points without any vector search logic. Ultimately, we will \nwrap the retrieved data into a versioned, tracked, and shareable artifact—more on artifacts in \nChapter 2. For now, you must know it is an MLOps concept used to wrap data and enrich it with \nthe properties listed before.\nHow will the rest of the system access the logical feature store? The training pipeline will use the \ninstruct datasets as artifacts, and the inference pipeline will query the vector DB for additional \ncontext using vector search techniques.\nFor our use case, this is more than enough because of the following reasons:\n• \nThe artifacts work great for offline use cases such as training\n• \nThe vector DB is built for online access, which we require for inference.\nIn future chapters, however, we will explain how the three data categories (articles, posts, and \ncode) are cleaned, chunked, and embedded.\nTo conclude, we take in raw article, post, or code data points, process them, and store them in \na feature store to make them accessible to the training and inference pipelines. Note that trim-\nming all the complexity away and focusing only on the interface is a perfect match with the FTI \npattern. Beautiful, right?\n",
      "content_length": 2349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "Chapter 1\n21\nTraining pipeline\nThe training pipeline consumes instruct datasets from the feature store, fine-tunes an LLM with \nit, and stores the tuned LLM weights in a model registry. More concretely, when a new instruct \ndataset is available in the logical feature store, we will trigger the training pipeline, consume the \nartifact, and fine-tune the LLM.\nIn the initial stages, the data science team owns this step. They run multiple experiments to find \nthe best model and hyperparameters for the job, either through automatic hyperparameter tuning \nor manually. To compare and pick the best set of hyperparameters, we will use an experiment \ntracker to log everything of value and compare it between experiments. Ultimately, they will pick \nthe best hyperparameters and fine-tuned LLM and propose it as the LLM production candidate. \nThe proposed LLM is then stored in the model registry. After the experimentation phase is over, \nwe store and reuse the best hyperparameters found to eliminate the manual restrictions of the \nprocess. Now, we can completely automate the training process, known as continuous training.\nThe testing pipeline is triggered for a more detailed analysis than during fine-tuning. Before \npushing the new model to production, assessing it against a stricter set of tests is critical to see \nthat the latest candidate is better than what is currently in production. If this step passes, the \nmodel is ultimately tagged as accepted and deployed to the production inference pipeline. Even \nin a fully automated ML system, it is recommended to have a manual step before accepting a new \nproduction model. It is like pushing the red button before a significant action with high conse-\nquences. Thus, at this stage, an expert looks at a report generated by the testing component. If \neverything looks good, it approves the model, and the automation can continue.\nThe particularities of this component will be on LLM aspects, such as the following:\n• \nHow do you implement an LLM agnostic pipeline?\n• \nWhat fine-tuning techniques should you use?\n•\nHow do you scale the fine-tuning algorithm on LLMs and datasets of various sizes?\n•\nHow do you pick an LLM production candidate from multiple experiments?\n•\nHow do you test the LLM to decide whether to push it to production or not?\nBy the end of this book, you will know how to answer all these questions.\nOne last aspect we want to clarify is CT. Our modular design allows us to quickly leverage an ML \norchestrator to schedule and trigger different system parts. For example, we can schedule the \ndata collection pipeline to crawl data every week. \n",
      "content_length": 2625,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "Understanding the LLM Twin Concept and Architecture\n22\nThen, we can trigger the feature pipeline when new data is available in the data warehouse and \nthe training pipeline when new instruction datasets are available.\nInference pipeline\nThe inference pipeline is the last piece of the puzzle. It is connected to the model registry and log-\nical feature store. It loads a fine-tuned LLM from the model registry, and from the logical feature \nstore, it accesses the vector DB for RAG. It takes in client requests through a REST API as queries. \nIt uses the fine-tuned LLM and access to the vector DB to carry out RAG and answer the queries.\nAll the client queries, enriched prompts using RAG, and generated answers are sent to a prompt \nmonitoring system to analyze, debug, and better understand the system. Based on specific require-\nments, the monitoring system can trigger alarms to take action either manually or automatically.\nAt the interface level, this component follows exactly the FTI architecture, but when zooming in, \nwe can observe unique characteristics of an LLM and RAG system, such as the following:\n• \nA retrieval client used to do vector searches for RAG\n• \nPrompt templates used to map user queries and external information to LLM inputs\n• \nSpecial tools for prompt monitoring\nFinal thoughts on the FTI design and the LLM Twin \narchitecture\nWe don’t have to be highly rigid about the FTI pattern. It is a tool used to clarify how to design \nML systems. For example, instead of using a dedicated features store just because that is how \nit is done, in our system, it is easier and cheaper to use a logical feature store based on a vector \nDB and artifacts. What was important to focus on were the required properties a feature store \nprovides, such as a versioned and reusable training dataset.\nUltimately, we will explain the computing requirements of each component briefly. The data \ncollection and feature pipeline are mostly CPU-based and do not require powerful machines. The \ntraining pipeline requires powerful GPU-based machines that could load an LLM and fine-tune it. \nThe inference pipeline is somewhere in the middle. It still needs a powerful machine but is less \ncompute-intensive than the training step. However, it must be tested carefully, as the inference \npipeline directly interfaces with the user. Thus, we want the latency to be within the required \nparameters for a good user experience. However, using the FTI design is not an issue. We can pick \nthe proper computing requirements for each component.\n",
      "content_length": 2544,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "Chapter 1\n23\nAlso, each pipeline will be scaled differently. The data and feature pipelines will be scaled horizon-\ntally based on the CPU and RAM load. The training pipeline will be scaled vertically by adding more \nGPUs. The inference pipeline will be scaled horizontally based on the number of client requests.\nTo conclude, the presented LLM architecture checks all the technical requirements listed at the \nbeginning of the section. It processes the data as requested, and the training is modular and can \nbe quickly adapted to different LLMs, datasets, or fine-tuning techniques. The inference pipeline \nsupports RAG and is exposed as a REST API. On the LLMOps side, the system supports dataset and \nmodel versioning, lineage, and reusability. The system has a monitoring service, and the whole \nML architecture is designed with CT/CI/CD in mind.\nThis concludes the high-level overview of the LLM Twin architecture.\nSummary\nThis first chapter was critical to understanding the book’s goal. As a product-oriented book that \nwill walk you through building an end-to-end ML system, it was essential to understand the \nconcept of an LLM Twin initially. Afterward, we walked you through what an MVP is and how \nto plan our LLM Twin MVP based on our available resources. Following this, we translated our \nconcept into a practical technical solution with specific requirements. In this context, we intro-\nduced the FTI design pattern and showcased its real-world application in designing systems that \nare both modular and scalable. Ultimately, we successfully applied the FTI pattern to design the \narchitecture of the LLM Twin to fit all our technical requirements.\nHaving a clear vision of the big picture is essential when building systems. Understanding how \na single component will be integrated into the rest of the application can be very valuable when \nworking on it. We started with a more abstract presentation of the LLM Twin architecture, fo-\ncusing on each component’s scope, interface, and interconnectivity.\nThe following chapters will explore how to implement and deploy each component. On the \nMLOps side, we will walk you through using a computing platform, orchestrator, model registry, \nartifacts, and other tools and concepts to support all MLOps best practices.\nReferences\n•\nDowling, J. (2024a, July 11). From MLOps to ML Systems with Feature/Training/Inference \nPipelines. Hopsworks. https://www.hopsworks.ai/post/mlops-to-ml-systems-with-\nfti-pipelines\n",
      "content_length": 2477,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "Understanding the LLM Twin Concept and Architecture\n24\n• \nDowling, J. (2024b, August 5). Modularity and Composability for AI Systems with AI Pipe-\nlines and Shared Storage. Hopsworks. https://www.hopsworks.ai/post/modularity-and-\ncomposability-for-ai-systems-with-ai-pipelines-and-shared-storage\n• \nJoseph, M. (2024, August 23). The Taxonomy for Data Transformations in AI Systems. Hop-\nsworks. https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-\nin-ai-systems\n•\nMLOps: Continuous delivery and automation pipelines in machine learning. (2024, August \n28). Google Cloud. https://cloud.google.com/architecture/mlops-continuous-\ndelivery-and-automation-pipelines-in-machine-learning\n•\nQwak. (2024a, June 2). CI/CD for Machine Learning in 2024: Best Practices to build, test, \nand Deploy | Infer. Medium. https://medium.com/infer-qwak/ci-cd-for-machine-\nlearning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2\n• \nQwak. (2024b, July 23). 5 Best Open Source Tools to build End-to-End MLOPs Pipeline in 2024. \nMedium. https://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-\nwith-open-source-tools-d8bacbf4184f\n•\nSalama, K., Kazmierczak, J., & Schut, D. (2021). Practitioners guide to MLOPs: A framework \nfor continuous delivery and automation of machine learning (1\nst ed.) [PDF]. Google Cloud. \nhttps://services.google.com/fh/files/misc/practitioners_guide_to_mlops_\nwhitepaper.pdf\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 1561,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "2\nTooling and Installation\nThis chapter presents all the essential tools that will be used throughout the book, especially in \nimplementing and deploying the LLM Twin project. At this point in the book, we don’t plan to \npresent in-depth LLM, RAG, MLOps, or LLMOps concepts. We will quickly walk you through our \ntech stack and prerequisites to avoid repeating ourselves throughout the book on how to set up \na particular tool and why we chose it. Starting with Chapter 3, we will begin exploring our LLM \nTwin use case by implementing a data collection ETL that crawls data from the internet.\nIn the first part of the chapter, we will present the tools within the Python ecosystem to manage \nmultiple Python versions, create a virtual environment, and install the pinned dependencies re-\nquired for our project to run. Alongside presenting these tools, we will also show how to install \nthe LLM-Engineers-Handbook repository on your local machine (in case you want to try out the \ncode yourself): https://github.com/PacktPublishing/LLM-Engineers-Handbook.\nNext, we will explore all the MLOps and LLMOps tools we will use, starting with more generic tools, \nsuch as a model registry, and moving on to more LLM-oriented tools, such as LLM evaluation and \nprompt monitoring tools. We will also understand how to manage a project with multiple ML \npipelines using ZenML, an orchestrator bridging the gap between ML and MLOps. Also, we will \nquickly explore what databases we will use for NoSQL and vector storage. We will show you how \nto run all these components on your local machine using Docker. Lastly, we will quickly review \nAWS and show you how to create an AWS user and access keys and install and configure the AWS \nCLI to manipulate your cloud resources programmatically. We will also explore SageMaker and \nwhy we use it to train and deploy our open-source LLMs.\n",
      "content_length": 1872,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "Tooling and Installation\n26\nIf you are familiar with these tools, you can safely skip this chapter. We also explain how to in-\nstall the project and set up all the necessary components in the repository’s README. Thus, you \nalso have the option to use that as more concise documentation if you plan to run the code while \nreading the book.\nTo sum all that up, in this chapter, we will explore the following topics:\n•\nPython ecosystem and project installation\n•\nMLOps and LLMOps tooling\n•\nDatabases for storing unstructured and vector data\n•\nPreparing for AWS\nBy the end of this chapter, you will be aware of all the tools we will use across the book. Also, you \nwill have learned how to install the LLM-Engineers-Handbook repository, set up the rest of the \ntools, and use them if you run the code while reading the book.\nPython ecosystem and project installation\nAny Python project needs three fundamental tools: the Python interpreter, dependency manage-\nment, and a task execution tool. The Python interpreter executes your Python project as expected. \nAll the code within the book is tested with Python 3.11.8. You can download the Python interpreter \nfrom here: https://www.python.org/downloads/. We recommend installing the exact Python \nversion (Python 3.11.8) to run the LLM Twin project using pyenv, making the installation process \nstraightforward.\nInstead of installing multiple global Python versions, we recommend managing them using pyenv, \na Python version management tool that lets you manage multiple Python versions between \nprojects. You can install it using this link: https://github.com/pyenv/pyenv?tab=readme-ov-\nfile#installation.\nAfter you have installed pyenv, you can install the latest version of Python 3.11, using pyenv, as \nfollows:\npyenv install 3.11.8\nNow list all installed Python versions to see that it was installed correctly:\npyenv versions\nYou should see something like this:\n# * system\n",
      "content_length": 1925,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "Chapter 2\n27\n#   3.11.8\nTo make Python 3.11.8 the default version across your entire system (whenever you open a new \nterminal), use the following command:\npyenv global 3.11.8\nHowever, we aim to use Python 3.11.8 locally only in our repository. To achieve that, first, we have \nto clone the repository and navigate to it:\ngit clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.git \ncd LLM-Engineers-Handbook\nBecause we defined a .python-version file within the repository, pyenv will know to pick up \nthe version from that file and use it locally whenever you are working within that folder. To \ndouble-check that, run the following command while you are in the repository:\npython --version\nIt should output:\n# Python 3.11.8\nTo create the .python-version file, you must run pyenv local 3.11.8 once. Then, pyenv will \nalways know to use that Python version while working within a specific directory.\nNow that we have installed the correct Python version using pyenv, let’s move on to Poetry, which \nwe will use as our dependency and virtual environment manager.\nPoetry: dependency and virtual environment management\nPoetry is one of the most popular dependency and virtual environment managers within the \nPython ecosystem. But let’s start by clarifying what a dependency manager is. In Python, a depen-\ndency manager allows you to specify, install, update, and manage external libraries or packages \n(dependencies) that a project relies on. For example, this is a simple Poetry requirements file that \nuses Python 3.11 and the requests and numpy Python packages.\n[tool.poetry.dependencies]\npython = \"^3.11\"\nrequests = \"^2.25.1\"\nnumpy = \"^1.19.5\"\n[build-system]\n",
      "content_length": 1674,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "Tooling and Installation\n28\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\nBy using Poetry to pin your dependencies, you always ensure that you install the correct version \nof the dependencies that your projects work with. Poetry, by default, saves all its requirements in \npyproject.toml files, which are stored at the root of your repository, as you can see in the cloned \nLLM-Engineers-Handbook repository.\nAnother massive advantage of using Poetry is that it creates a new Python virtual environment in \nwhich it installs the specified Python version and requirements. A virtual environment allows you \nto isolate your project’s dependencies from your global Python dependencies and other projects. \nBy doing so, you ensure there are no version clashes between projects. For example, let’s assume \nthat Project A needs numpy == 1.19.5, and Project B needs numpy == 1.26.0. If you keep both \nprojects in the global Python environment, that will not work, as Project B will override Project A’s \nnumpy installation, which will corrupt Project A and stop it from working. Using Poetry, you can \nisolate each project in its own Python environment with its own Python dependencies, avoiding \nany dependency clashes.\nYou can install Poetry from here: https://python-poetry.org/docs/. We use Poetry 1.8.3 \nthroughout the book. Once Poetry is installed, navigate to your cloned LLM-Engineers-Hand-\nbook repository and run the following command to install all the necessary Python dependencies:\npoetry install --without aws\nThis command knows to pick up all the dependencies from your repository that are listed in \nthe pyproject.toml and poetry.lock files. After the installation, you can activate your Poetry \nenvironment by running poetry shell in your terminal or by prefixing all your CLI commands \nas follows: poetry run <your command>.\nOne final note on Poetry is that it locks down the exact versions of the dependency tree in the \npoetry.lock file based on the definitions added to the project.toml file. While the pyproject.\ntoml file may specify version ranges (e.g., requests = \"^2.25.1\"), the poetry.lock file records \nthe exact version (e.g., requests = \"2.25.1\") that was installed. It also locks the versions of \nsub-dependencies (dependencies of your dependencies), which may not be explicitly listed in \nyour pyproject.toml file. By locking all the dependencies and sub-dependencies to specific \nversions, the poetry.lock file ensures that all project installations use the same versions of each \npackage. This consistency leads to predictable behavior, reducing the likelihood of encountering \n“works on my machine” issues.\n",
      "content_length": 2660,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "Chapter 2\n29\nOther tools similar to Poetry are Venv and Conda for creating virtual environments. Still, they lack \nthe dependency management option. Thus, you must do it through Python’s default requirements.\ntxt files, which are less powerful than Poetry’s lock files. Another option is Pipenv, which fea-\nture-wise is more like Poetry but slower, and uv, which is a replacement for Poetry built in Rust, \nmaking it blazing fast. uv has lots of potential to replace Poetry, making it worthwhile to test out: \nhttps://github.com/astral-sh/uv.\nThe final piece of the puzzle is to look at the task execution tool we used to manage all our CLI \ncommands.\nPoe the Poet: task execution tool\nPoe the Poet is a plugin on top of Poetry that is used to manage and execute all the CLI commands \nrequired to interact with the project. It helps you define and run tasks within your Python proj-\nect, simplifying automation and script execution. Other popular options are Makefile, Invoke, or \nshell scripts, but Poe the Poet eliminates the need to write separate shell scripts or Makefiles for \nmanaging project tasks, making it an elegant way to manage tasks using the same configuration \nfile that Poetry already uses for dependencies.\nWhen working with Poe the Poet, instead of having all your commands documented in a README \nfile or other document, you can add them directly to your pyproject.toml file and execute them \nin the command line with an alias. For example, using Poe the Poet, we can define the following \ntasks in a pyproject.toml file:\n[tool.poe.tasks]\ntest = \"pytest\"\nformat = \"black .\"\nstart = \"python main.py\"\nYou can then run these tasks using the poe command:\npoetry poe test\npoetry poe format\npoetry poe start\nYou can install Poe the Poet as a Poetry plugin, as follows:\npoetry self add 'poethepoet[poetry_plugin]'\n",
      "content_length": 1828,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "Tooling and Installation\n30\nTo conclude, using a tool as a façade over all your CLI commands is necessary to run your appli-\ncation. It significantly simplifies the application’s complexity and enhances collaboration as it \nacts as out-of-the-box documentation.\nAssuming you have pyenv and Poetry installed, here are all the commands you need to run to \nclone the repository and install the dependencies and Poe the Poet as a Poetry plugin:\ngit clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.gitcd \nLLM-Engineers-Handbook\npoetry install --without aws\npoetry self add 'poethepoet[poetry_plugin]'\nTo make the project fully operational, there are still a few steps to follow, such as filling out a \n.env file with your credentials and getting tokens from OpenAI and Hugging Face. But this book \nisn’t an installation guide, so we’ve moved all these details into the repository’s README as \nthey are useful only if you plan to run the repository: https://github.com/PacktPublishing/\nLLM-Engineers-Handbook.\nNow that we have installed our Python project, let’s present the MLOps tools we will use in the \nbook. If you are already familiar with these tools, you can safely skip the following tooling section \nand move on to the Databases for storing unstructured and vector data section.\nMLOps and LLMOps tooling\nThis section will quickly present all the MLOps and LLMOps tools we will use throughout the \nbook and their role in building ML systems using MLOps best practices. At this point in the \nbook, we don’t aim to detail all the MLOps components we will use to implement the LLM Twin \nuse case, such as model registries and orchestrators, but only provide a quick idea of what they \nare and how to use them. As we develop the LLM Twin project throughout the book, you will \nsee hands-on examples of how we use all these tools. In Chapter 11, we will dive deeply into the \ntheory of MLOps and LLMOps and connect all the dots. As the MLOps and LLMOps fields are \nhighly practical, we will leave the theory of these aspects to the end, as it will be much easier to \nunderstand it after you go through the LLM Twin use case implementation.\nAlso, this section is not dedicated to showing you how to set up each tool. It focuses primarily on \nwhat each tool is used for and highlights the core features used throughout this book.\nStill, using Docker, you can quickly run the whole infrastructure locally. If you want to run the \nsteps within the book yourself, you can host the application locally with these three simple steps:\n1. \nHave Docker 27.1.1 (or higher) installed.\n",
      "content_length": 2587,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "Chapter 2\n31\n2.\nFill your .env file with all the necessary credentials as explained in the repository README.\n3.\nRun poetry poe local-infrastructure-up to locally spin up ZenML (http://127.0.0.1:8237/) \nand the MongoDB and Qdrant databases.\nYou can read more details on how to run everything locally in the LLM-Engineers-Handbook re-\npository README: https://github.com/PacktPublishing/LLM-Engineers-Handbook. Within \nthe book, we will also show you how to deploy each component to the cloud.\nHugging Face: model registry\nA model registry is a centralized repository that manages ML models throughout their lifecycle. \nIt stores models along with their metadata, version history, and performance metrics, serving \nas a single source of truth. In MLOps, a model registry is crucial for tracking, sharing, and docu-\nmenting model versions, facilitating team collaboration. Also, it is a fundamental element in the \ndeployment process as it integrates with continuous integration and continuous deployment\n(CI/CD) pipelines.\nWe used Hugging Face as our model registry, as we can leverage its ecosystem to easily share our \nfine-tuned LLM Twin models with anyone who reads the book. Also, by following the Hugging \nFace model registry interface, we can easily integrate the model with all the frameworks around \nthe LLMs ecosystem, such as Unsloth for fine-tuning and SageMaker for inference.\nOur fine-tuned LLMs are available on Hugging Face at:\n• \nTwinLlama 3 1 8B (after fine-tuning): https://huggingface.co/mlabonne/TwinLlama-\n3.1-8B\n• \nTwinLlama 3 1 8B DPO (after preference alignment): https://huggingface.co/\nmlabonne/TwinLlama-3.1-8B-DPO\nFigure 2.1: Hugging Face model registry example\n",
      "content_length": 1690,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "Tooling and Installation\n32\nFor a quick demo, we have them available on Hugging Face Spaces:\n•\nTwinLlama 3 1 8B: https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B\n•\nTwinLlama 3 1 8B DPO: https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-\n8B-DPO\nMost ML tools provide model registry features. For example, ZenML, Comet, and SageMaker, which \nwe will present in future sections, also offer their own model registries. They are good options, \nbut we picked Hugging Face solely because of its ecosystem, which provides easy shareability and \nintegration throughout the open-source environment. Thus, you will usually select the model \nregistry that integrates the most with your project’s tooling and requirements.\nZenML: orchestrator, artifacts, and metadata\nZenML acts as the bridge between ML and MLOps. Thus, it offers multiple MLOps features that \nmake your ML pipeline traceability, reproducibility, deployment, and maintainability easier. At \nits core, it is designed to create reproducible workflows in machine learning. It addresses the \nchallenge of transitioning from exploratory research in Jupyter notebooks to a production-ready \nML environment. It tackles production-based replication issues, such as versioning difficulties, \nreproducing experiments, organizing complex ML workflows, bridging the gap between train-\ning and deployment, and tracking metadata. Thus, ZenML’s main features are orchestrating ML \npipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts \nfor better observability.\nInstead of being another ML platform, ZenML introduced the concept of a stack, which allows \nyou to run ZenML on multiple infrastructure options. A stack will enable you to connect ZenML \nto different cloud services, such as:\n• \nAn orchestrator and compute engine (for example, AWS SageMaker or Vertex AI)\n• \nRemote storage (for instance, AWS S3 or Google Cloud Storage buckets)\n• \nA container registry (for example, Docker Registry or AWS ECR)\nThus, ZenML acts as a glue that brings all your infrastructure and tools together in one place \nthrough its stack feature, allowing you to quickly iterate through your development processes and \neasily monitor your entire ML system. The beauty of this is that ZenML doesn’t vendor-lock you \ninto any cloud platform. It completely abstracts away the implementation of your Python code \nfrom the infrastructure it runs on. For example, in our LLM Twin use case, we used the AWS stack:\n• \nSageMaker as our orchestrator and compute\n",
      "content_length": 2525,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "Chapter 2\n33\n• \nS3 as our remote storage used to store and track artifacts\n• \nECR as our container registry\nHowever, the Python code contains no S3 or ECR particularities, as ZenML takes care of them. \nThus, we can easily switch to other providers, such as Google Cloud Storage or Azure. For more \ndetails on ZenML stacks, you can start here: https://docs.zenml.io/user-guide/production-\nguide/understand-stacks.\nThe local version of the ZenML server comes installed as a Python package. Thus, when running \npoetry install, it installs a ZenML debugging server that you can use locally. In Chapter 11, we \nwill show you how to use their cloud serverless option to deploy the ML pipelines to AWS.\nOrchestrator\nAn orchestrator is a system that automates, schedules, and coordinates all your ML pipelines. It \nensures that each pipeline—such as data ingestion, preprocessing, model training, and deploy-\nment—executes in the correct order and handles dependencies efficiently. By managing these \nprocesses, an orchestrator optimizes resource utilization, handles failures gracefully, and enhances \nscalability, making complex ML pipelines more reliable and easier to manage.\nHow does ZenML work as an orchestrator? It works with pipelines and steps. A pipeline is a \nhigh-level object that contains multiple steps. A function becomes a ZenML pipeline by being \ndecorated with @pipeline, and a step when decorated with @step. This is a standard pattern \nwhen using orchestrators: you have a high-level function, often called a pipeline, that calls mul-\ntiple units/steps/tasks.\nLet’s explore how we can implement a ZenML pipeline with one of the ML pipelines implemented \nfor the LLM Twin project. In the code snippet below, we defined a ZenML pipeline that queries \nthe database for a user based on its full name and crawls all the provided links under that user:\nfrom zenml import pipeline\nfrom steps.etl import crawl_links, get_or_create_user\n@pipeline\nWe will focus only on the ZenML features used throughout the book, such as orches-\ntrating, artifacts, and metadata. For more details on ZenML, check out their starter \nguide: https://docs.zenml.io/user-guide/starter-guide.\n",
      "content_length": 2176,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "Tooling and Installation\n34\ndef digital_data_etl(user_full_name: str, links: list[str]) -> None:\n    user = get_or_create_user(user_full_name)\n    crawl_links(user=user, links=links)\nYou can run the pipeline with the following CLI command: poetry poe run-digital-data-etl. \nTo visualize the pipeline run, you can go to your ZenML dashboard (at http://127.0.0.1:8237/) \nand, on the left panel, click on the Pipelines tab and then on the digital_data_etl pipeline, as \nillustrated in Figure 2.2:\nFigure 2.2: ZenML Pipelines dashboard\nAfter clicking on the digital_data_etl pipeline, you can visualize all the previous and current \npipeline runs, as seen in Figure 2.3. You can see which one succeeded, failed, or is still running. \nAlso, you can see the stack used to run the pipeline, where the default stack is the one used to \nrun your ML pipelines locally.\n",
      "content_length": 859,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "Chapter 2\n35\nFigure 2.3: ZenML digital_data_etl pipeline dashboard. Example of a specific pipeline\nNow, after clicking on the latest digital_data_etl pipeline run (or any other run that succeeded or \nis still running), we can visualize the pipeline’s steps, outputs, and insights, as illustrated in Figure \n2.4. This structure is often called a directed acyclic graph (DAG). More on DAGs in Chapter 11.\nFigure 2.4: ZenML digital_data_etl pipeline run dashboard (example of a specific pipeline run)\n",
      "content_length": 498,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "Tooling and Installation\n36\nBy clicking on a specific step, you can get more insights into its code and configuration. It even \naggregates the logs output by that specific step to avoid switching between tools, as shown in \nFigure 2.5.\nFigure 2.5: Example of insights from a specific step of the digital_data_etl pipeline run\nNow that we understand how to define a ZenML pipeline and how to look it up in the dashboard, \nlet’s quickly look at how to define a ZenML step. In the code snippet below, we defined the get_\nor_create_user() step, which works just like a normal Python function but is decorated with \n@step. We won’t go into the details of the logic, as we will cover the ETL logic in Chapter 3. For \nnow, we will focus only on the ZenML functionality.\nfrom loguru import logger\nfrom typing_extensions import Annotated\nfrom zenml import get_step_context, step\nfrom llm_engineering.application import utils\n",
      "content_length": 916,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "Chapter 2\n37\nfrom llm_engineering.domain.documents import UserDocument\n@step\ndef get_or_create_user(user_full_name: str) -> Annotated[UserDocument, \n\"user\"]:\n    logger.info(f\"Getting or creating user: {user_full_name}\")\n    first_name, last_name = utils.split_user_full_name(user_full_name)\n    user = UserDocument.get_or_create(first_name=first_name, last_\nname=last_name)\n    return user\nWithin a ZenML step, you can define any Python logic your use case needs. In this simple example, \nwe are just creating or retrieving a user, but we could replace that code with anything, starting \nfrom data collection to feature engineering and training. What is essential to notice is that to \nintegrate ZenML with your code, you have to write modular code, where each function does just \none thing. The modularity of your code makes it easy to decorate your functions with @step and \nthen glue multiple steps together within a main function decorated with @pipeline. One design \nchoice that will impact your application is deciding the granularity of each step, as each will run \nas a different unit on a different machine when deployed in the cloud.\nTo decouple our code from ZenML, we encapsulated all the application and domain logic into \nthe llm_engineering Python module. We also defined the pipelines and steps folders, where \nwe defined our ZenML logic. Within the steps module, we only used what we needed from the \nllm_engineering Python module (similar to how you use a Python package). In the pipelines\nmodule, we only aggregated ZenML steps to glue them into the final pipeline. Using this de-\nsign, we can easily swap ZenML with another orchestrator or use our application logic in other \nuse cases, such as a REST API. We only have to replace the ZenML code without touching the \nllm_engineering module where all our logic resides. \n",
      "content_length": 1842,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "Tooling and Installation\n38\nThis folder structure is reflected at the root of the LLM-Engineers-Handbook repository, as il-\nlustrated in Figure 2.6:\nFigure 2.6: LLM-Engineers-Handbook repository folder structure\nOne last thing to consider when writing ZenML steps is that if you return a value, it should be se-\nrializable. ZenML can serialize most objects that can be reduced to primitive data types, but there \nare a few exceptions. For example, we used UUID types as IDs throughout the code, which aren’t \nnatively supported by ZenML. Thus, we had to extend ZenML’s materializer to support UUIDs. \nWe raised this issue to ZenML. Hence, in future ZenML versions, UUIDs will be supported, but it \nwas an excellent example of the serialization aspect of transforming function outputs in artifacts.\n",
      "content_length": 798,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "Chapter 2\n39\nArtifacts and metadata\nAs mentioned in the previous section, ZenML transforms any step output into an artifact. First, \nlet’s quickly understand what an artifact is. In MLOps, an artifact is any file(s) produced during \nthe machine learning lifecycle, such as datasets, trained models, checkpoints, or logs. Artifacts \nare crucial for reproducing experiments and deploying models. We can transform anything into \nan artifact. For example, the model registry is a particular use case for an artifact. Thus, artifacts \nhave these unique properties: they are versioned, sharable, and have metadata attached to them \nto understand what’s inside quickly. For example, when wrapping your dataset with an artifact, \nyou can add to its metadata the size of the dataset, the train-test split ratio, the size, types of labels, \nand anything else useful to understand what’s inside the dataset without actually downloading it.\nLet’s circle back to our digital_data_etl pipeline example, where we had as a step output an ar-\ntifact, the crawled links, which are an artifact, as seen in Figure 2.7\nFigure 2.7: ZenML artifact example using the digital_data_etl pipeline as an example\n",
      "content_length": 1183,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "Tooling and Installation\n40\nBy clicking on the crawled_links artifact and navigating to the Metadata tab, we can quickly \nsee all the domains we crawled for a particular author, the number of links we crawled for each \ndomain, and how many were successful, as illustrated in Figure 2.8:\nFigure 2.8: ZenML metadata example using the digital_data_etl pipeline as an example\nA more interesting example of an artifact and its metadata is the generated dataset artifact. In \nFigure 2.9, we can visualize the metadata of the instruct_datasets artifact, which was auto-\nmatically generated and will be used to fine-tune the LLM Twin model. More details on the \ninstruction datasets are in Chapter 5. For now, we want to highlight that within the dataset’s \nmetadata, we have precomputed a lot of helpful information about it, such as how many data \ncategories it contains, its storage size, and the number of samples per training and testing split. \n",
      "content_length": 943,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "Chapter 2\n41\nFigure 2.9: ZenML metadata example for the instruct_datasets artifact\nThe metadata is manually added to the artifact, as shown in the code snippet below. Thus, you \ncan precompute and attach to the artifact’s metadata anything you consider helpful for dataset \ndiscovery across your business and projects:\n… # More imports\nfrom zenml import ArtifactConfig, get_step_context, step\n@step\ndef generate_intruction_dataset(\n    prompts: Annotated[dict[DataCategory, \nlist[GenerateDatasetSamplesPrompt]], \"prompts\"]) -> Annotated[\n",
      "content_length": 538,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "Tooling and Installation\n42\n    InstructTrainTestSplit,\n    ArtifactConfig(\n        name=\"instruct_datasets\",\n        tags=[\"dataset\", \"instruct\", \"cleaned\"],\n    ),\n]:\n    datasets = … # Generate datasets\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"instruct_datasets\", \nmetadata=_get_metadata_instruct_dataset(datasets))\n    return datasets\ndef _get_metadata_instruct_dataset(datasets: InstructTrainTestSplit) -> \ndict[str, Any]:\n    instruct_dataset_categories = list(datasets.train.keys())\n    train_num_samples = {\n        category: instruct_dataset.num_samples for category, instruct_\ndataset in datasets.train.items()\n    }\n    test_num_samples = {category: instruct_dataset.num_samples for \ncategory, instruct_dataset in datasets.test.items()}\n    return {\n        \"data_categories\": instruct_dataset_categories,\n        \"test_split_size\": datasets.test_split_size,\n        \"train_num_samples_per_category\": train_num_samples,\n        \"test_num_samples_per_category\": test_num_samples,\n    }\nAlso, you can easily download and access a specific version of the dataset using its Universally \nUnique Identifier (UUID), which you can find using the ZenML dashboard or CLI:\nfrom zenml.client import Client\nartifact = Client().get_artifact_version('8bba35c4-8ff9-4d8f-a039-\n08046efc9fdc')\nloaded_artifact = artifact.load()\n",
      "content_length": 1368,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "Chapter 2\n43\nThe last step in exploring ZenML is understanding how to run and configure a ZenML pipeline.\nHow to run and configure a ZenML pipeline\nAll the ZenML pipelines can be called from the run.py file, accessed at tools/run.py in our GitHub \nrepository. Within the run.py file, we implemented a simple CLI that allows you to specify what \npipeline to run. For example, to call the digital_data_etl pipeline to crawl Maxime’s content, \nyou have to run:\npython -m tools.run --run-etl --no-cache --etl-config-filename digital_\ndata_etl_maxime_labonne.yaml\nOr, to crawl Paul’s content, you can run:\npython -m tools.run --run-etl --no-cache --etl-config-filename digital_\ndata_etl_paul_iusztin.yaml\nAs explained when introducing Poe the Poet, all our CLI commands used to interact with the proj-\nect will be executed through Poe to simplify and standardize the project. Thus, we encapsulated \nthese Python calls under the following poe CLI commands:\npoetry poe run-digital-data-etl-maxime\npoetry poe run-digital-data-etl-paul\nWe only change the ETL config file name when scraping content for different people. ZenML \nallows us to inject specific configuration files at runtime as follows:\nconfig_path = root_dir / \"configs\" / etl_config_filename\nassert config_path.exists(), f\"Config file not found: { config_path }\"\nrun_args_etl = {\n\"config_path\": config_path,\n\"run_name\": f\"digital_data_etl_run_{dt.now().\nstrftime('%Y_%m_%d_%H_%M_%S')}\"\n}\n digital_data_etl.with_options()(**run_args_etl)\nIn the config file, we specify all the parameters that will input the pipeline as parameters. For ex-\nample, the configs/digital_data_etl_maxime_labonne.yaml configuration file looks as follows:\nparameters:\n  user_full_name: Maxime Labonne # [First Name(s)] [Last Name]\n  links:\n    # Personal Blog\n",
      "content_length": 1791,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "Tooling and Installation\n44\n    - https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.\nhtml\n    - https://mlabonne.github.io/blog/posts/2024-07-15_The_Rise_of_\nAgentic_Data_Generation.html\n    # Substack\n    - https://maximelabonne.substack.com/p/uncensor-any-llm-with-\nabliteration-d30148b7d43e\n    … # More links\nWhere the digital_data_etl function signature looks like this:\n@pipeline\ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\nThis approach allows us to configure each pipeline at runtime without modifying the code. We \ncan also clearly track the inputs for all our pipelines, ensuring reproducibility. As seen in Figure \n2.10, we have one or more configs for each pipeline.\nFigure 2.10: ZenML pipeline configs\n",
      "content_length": 755,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "Chapter 2\n45\nOther popular orchestrators similar to ZenML that we’ve personally tested and consider powerful \nare Airflow, Prefect, Metaflow, and Dagster. Also, if you are a heavy user of Kubernetes, you can \nopt for Agro Workflows or Kubeflow, the latter of which works only on top of Kubernetes. We still \nconsider ZenML the best trade-off between ease of use, features, and costs. Also, none of these \ntools offer the stack feature that is offered by ZenML, which allows it to avoid vendor-locking \nyou in to any cloud ecosystem.\nIn Chapter 11, we will explore in more depth how to leverage an orchestrator to implement MLOps \nbest practices. But now that we understand ZenML, what it is helpful for, and how to use it, let’s \nmove on to the experiment tracker.\nComet ML: experiment tracker\nTraining ML models is an entirely iterative and experimental process. Unlike traditional software \ndevelopment, it involves running multiple parallel experiments, comparing them based on pre-\ndefined metrics, and deciding which one should advance to production. An experiment tracking \ntool allows you to log all the necessary information, such as metrics and visual representations \nof your model predictions, to compare all your experiments and quickly select the best model. \nOur LLM project is no exception.\nAs illustrated in Figure 2.11, we used Comet to track metrics such as training and evaluation loss \nor the value of the gradient norm across all our experiments.\nFigure 2.11: Comet ML training metrics example\n",
      "content_length": 1515,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "Tooling and Installation\n46\nUsing an experiment tracker, you can go beyond training and evaluation metrics and log your \ntraining hyperparameters to track different configurations between experiments. \nIt also logs out-of-the-box system metrics such as GPU, CPU, or memory utilization to give you \na clear picture of what resources you need during training and where potential bottlenecks slow \ndown your training, as seen in Figure 2.12.\nFigure 2.12: Comet ML system metrics example\nYou don’t have to set up Comet locally. We will use their online version for free without any \nconstraints throughout this book. Also, if you want to look more in-depth into the Comet ML \nexperiment tracker, we made the training experiments tracked with Comet ML public while \nfine-tuning our LLM Twin models. You can access them here: https://www.comet.com/mlabonne/\nllm-twin-training/view/new/panels.\nOther popular experiment trackers are W&B, MLflow, and Neptune. We’ve worked with all of \nthem and can state that they all have mostly the same features, but Comet ML differentiates it-\nself through its ease of use and intuitive interface. Let’s move on to the final piece of the MLOps \npuzzle: Opik for prompt monitoring.\nOpik: prompt monitoring\nYou cannot use standard tools and techniques when logging and monitoring prompts. The reason \nfor this is complicated. We will dig into it in Chapter 11. However, to quickly give you some under-\nstanding, you cannot use standard logging tools as prompts are complex and unstructured chains. \n",
      "content_length": 1526,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "Chapter 2\n47\nWhen interacting with an LLM application, you chain multiple input prompts and the generated \noutput into a trace, where one prompt depends on previous prompts. \nThus, instead of plain text logs, you need an intuitive way to group these traces into a specialized \ndashboard that makes debugging and monitoring traces of prompts easier.\nWe used Opik, an open-source tool made by Comet, as our prompt monitoring tool because it fol-\nlows Comet’s philosophy of simplicity and ease of use, which is currently relatively rare in the LLM \nlandscape. Other options offering similar features are Langfuse (open source, https://langfuse.\ncom), Galileo (not open source, rungalileo.io), and LangSmith (not open source, https://www.\nlangchain.com/langsmith), but we found their solutions more cumbersome to use and imple-\nment. Opik, along with its serverless option, also provides a free open-source version that you \nhave complete control over. You can read more on Opik at https://github.com/comet-ml/opik.\nDatabases for storing unstructured and vector data\nWe also want to present the NoSQL and vector databases we will use within our examples. When \nworking locally, they are already integrated through Docker. Thus, when running poetry poe \nlocal-infrastructure-up, as instructed a few sections above, local images of Docker for both \ndatabases will be pulled and run on your machine. Also, when deploying the project, we will \nshow you how to use their serverless option and integrate it with the rest of the LLM Twin project.\nMongoDB: NoSQL database\nMongoDB is one of today’s most popular, robust, fast, and feature-rich NoSQL databases. It \nintegrates well with most cloud ecosystems, such as AWS, Google Cloud, Azure, and Databricks. \nThus, using MongoDB as our NoSQL database was a no-brainer.\nWhen we wrote this book, MongoDB was used by big players such as Novo Nordisk, Delivery \nHero, Okta, and Volvo. This widespread adoption suggests that MongoDB will remain a leading \nNoSQL database for a long time.\nWe use MongoDB as a NoSQL database to store the raw data we collect from the internet before \nprocessing it and pushing it into the vector database. As we work with unstructured text data, \nthe flexibility of the NoSQL database fits like a charm.\nQdrant: vector database\nQdrant (https://qdrant.tech/) is one of the most popular, robust, and feature-rich vector \ndatabases. We could have used almost any vector database for our small MVP, but we wanted to \npick something light and likely to be used in the industry for many years to come. \n",
      "content_length": 2561,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "Tooling and Installation\n48\nWe will use Qdrant to store the data from MongoDB after it’s processed and transformed for \nGenAI usability.\nQdrant is used by big players such as X (formerly Twitter), Disney, Microsoft, Discord, and John-\nson & Johnson. Thus, it is highly probable that Qdrant will remain in the vector database game \nfor a long time.\nWhile writing the book, other popular options were Milvus, Redis, Weaviate, Pinecone, Chroma, \nand pgvector (a PostgreSQL plugin for vector indexes). We found that Qdrant offers the best \ntrade-off between RPS, latency, and index time, making it a solid choice for many generative AI \napplications.\nComparing all the vector databases in detail could be a chapter in itself. We don’t want to do \nthat here. Still, if curious, you can check the Vector DB Comparison resource from Superlinked at \nhttps://superlinked.com/vector-db-comparison, which compares all the top vector databases \nin terms of everything you can think about, from the license and release year to database features, \nembedding models, and frameworks supported.\nPreparing for AWS\nThis last part of the chapter will focus on setting up an AWS account (if you don’t already have \none), an AWS access key, and the CLI. Also, we will look into what SageMaker is and why we use it.\nWe picked AWS as our cloud provider because it’s the most popular out there and the cloud in \nwhich we (the writers) have the most experience. The reality is that other big cloud providers, \nsuch as GCP or Azure, offer similar services. Thus, depending on your specific application, there is \nalways a trade-off between development time (in which you have the most experience), features, \nand costs. But for our MVP, AWS, it’s the perfect option as it provides robust features for every-\nthing we need, such as S3 (object storage), ECR (container registry), and SageMaker (compute \nfor training and inference).\nSetting up an AWS account, an access key, and the CLI\nAs AWS could change its UI/UX, the best way to instruct you on how to create an AWS account is \nby redirecting you to their official tutorial: https://docs.aws.amazon.com/accounts/latest/\nreference/manage-acct-creating.html.\nAfter successfully creating an AWS account, you can access the AWS console at http://console.\naws.amazon.com. Select Sign in using root user email (found under the Sign in button), then \nenter your account’s email address and password.\n",
      "content_length": 2419,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "Chapter 2\n49\nNext, we must generate access keys to access AWS programmatically. The best option to do so is \nfirst to create an IAM user with administrative access as described in this AWS official tutorial: \nhttps://docs.aws.amazon.com/streams/latest/dev/setting-up.html\nFor production accounts, it is best practice to grant permissions with a policy of least privilege, \ngiving each user only the permissions they require to perform their role. However, to simplify the \nsetup of our test account, we will use the AdministratorAccess managed policy, which gives our \nuser full access, as explained in the tutorial above and illustrated in Figure 2.13.\nFigure 2.13: IAM user permission policies example\nNext, you have to create an access key for the IAM user you just created using the following tutorial: \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html.\nThe access keys will look as follows:\naws_access_key_id = <your_access_key_id>\naws_secret_access_key = <your_secret_access_key>\nJust be careful to store them somewhere safe, as you won’t be able to access them after you cre-\nate them. Also, be cautious with who you share them, as they could be used to access your AWS \naccount and manipulate various AWS resources.\nThe last step is to install the AWS CLI and configure it with your newly created access keys. You \ncan install the AWS CLI using the following link: https://docs.aws.amazon.com/cli/latest/\nuserguide/getting-started-install.html.\nAfter installing the AWS CLI, you can configure it by running aws configure. Here is an example \nof our AWS configuration:\n[default]\naws_access_key_id = *************\naws_secret_access_key = ************\n",
      "content_length": 1689,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "Tooling and Installation\n50\nregion = eu-central-1\noutput = json\nFor more details on how to configure the AWS CLI, check out the following tutorial: https://\ndocs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html.\nAlso, to configure the project with your AWS credentials, you must fill in the following variables \nwithin your .env file:\nAWS_REGION=\"eu-central-1\" # Change it with your AWS region. By default, we \nuse \"eu-central-1\".\nAWS_ACCESS_KEY=\"<your_aws_access_key>\"\nAWS_SECRET_KEY=\"<your_aws_secret_key>\"\nSageMaker: training and inference compute\nThe last topic of this chapter is understanding SageMaker and why we decided to use it. SageMaker \nis an ML platform used to train and deploy ML models. An official definition is as follows: AWS \nSageMaker is a fully managed machine learning service by AWS that enables developers and data \nscientists to build, train, and deploy machine learning models at scale. It simplifies the process \nby handling the underlying infrastructure, allowing users to focus on developing high-quality \nmodels efficiently.\nAn important note about costs associated with hands-on tasks in this book\nAll the cloud services used across the book stick to their freemium option, except AWS. \nThus, if you use a personal AWS account, you will be responsible for AWS costs as you \nfollow along in this book. While some services may fall under AWS Free Tier usage, \nothers will not. Thus, you are responsible for checking your billing console regularly.\nMost of the costs will come when testing SageMaker for training and inference. Based \non our tests, the AWS costs can vary between $50 and $100 using the specifications \nprovided in this book and repository.\nSee the AWS documentation on setting up billing alarms to monitor your costs \nat https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/\nmonitor_estimated_charges_with_cloudwatch.html.\n",
      "content_length": 1888,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "Chapter 2\n51\nWe will use SageMaker to fine-tune and operationalize our training pipeline on clusters of GPUs \nand to deploy our custom LLM Twin model as a REST API that can be accessed in real time from \nanywhere in the world.\nWhy AWS SageMaker?\nWe must also discuss why we chose AWS SageMaker over simpler and more cost-effective options, \nsuch as AWS Bedrock. First, let’s explain Bedrock and its benefits.\nAmazon Bedrock is a serverless solution for deploying LLMs. Serverless means that there are no \nservers or infrastructure to manage. It provides pre-trained models, which you can access directly \nthrough API calls. When we wrote this book, they provided support only for Mistral, Flan, Llama \n2, and Llama 3 (quite a limited list of options). You can send input data and receive predictions \nfrom the models without managing the underlying infrastructure or software. This approach sig-\nnificantly reduces the complexity and time required to integrate AI capabilities into applications, \nmaking it more accessible to developers with limited machine learning expertise. However, this \nease of integration comes at the cost of limited customization options, as you’re restricted to the \npre-trained models and APIs provided by Amazon Bedrock. In terms of pricing, Bedrock uses a \nsimple pricing model based on the number of API calls. This straightforward pricing structure \nmakes it more efficient to estimate and control costs.\nMeanwhile, SageMaker provides a comprehensive platform for building, training, and deploying \nmachine learning models. It allows you to customize your ML processes entirely or even use \nthe platform for research. That’s why SageMaker is mainly used by data scientists and machine \nlearning experts who know how to program, understand machine learning concepts, and are \ncomfortable working with cloud platforms such as AWS. SageMaker is a double-edged sword \nregarding costs, following a pay-as-you-go pricing model similar to most AWS services. This \nmeans you have to pay for the usage of computing resources, storage, and any other services \nrequired to build your applications.\nIn contrast to Bedrock, even if the SageMaker endpoint is not used, you will still pay for the \ndeployed resources on AWS, such as online EC2 instances. Thus, you have to design autoscaling \nsystems that delete unused resources. To conclude, Bedrock offers an out-of-the-box solution \nthat allows you to quickly deploy an API endpoint powered by one of the available foundation \nmodels. Meanwhile, SageMaker is a multi-functional platform enabling you to customize your \nML logic fully.\n",
      "content_length": 2606,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "Tooling and Installation\n52\nSo why did we choose SageMaker over Bedrock? Bedrock would have been an excellent solution \nfor quickly prototyping something, but this is a book on LLM engineering, and our goal is to dig \ninto all the engineering aspects that Bedrock tries to mask away. Thus, we chose SageMaker \nbecause of its high level of customizability, allowing us to show you all the engineering required \nto deploy a model.\nIn reality, even SageMaker isn’t fully customizable. If you want complete control over your de-\nployment, use EKS, AWS’s Kubernetes self-managed service. In this case, you have direct access \nto the virtual machines, allowing you to fully customize how you build your ML pipelines, how \nthey interact, and how you manage your resources. You could do the same thing with AWS ECS, \nAWS’s version of Kubernetes. Using EKS or ECS, you could also reduce the costs, as these services \ncost considerably less.\nTo conclude, SageMaker strikes a balance between complete control and customization and a fully \nmanaged service that hides all the engineering complexity behind the scenes. This balance ensures \nthat you have the control you need while also benefiting from the managed service’s convenience.\nSummary\nIn this chapter, we reviewed the core tools used across the book. First, we understood how to \ninstall the correct version of Python that supports our repository. Then, we looked over how to \ncreate a virtual environment and install all the dependencies using Poetry. Finally, we understood \nhow to use a task execution tool like Poe the Poet to aggregate all the commands required to run \nthe application.\nThe next step was to review all the tools used to ensure MLOps best practices, such as a model \nregistry to share our models, an experiment tracker to manage our training experiments, an \norchestrator to manage all our ML pipelines and artifacts, and metadata to manage all our files \nand datasets. We also understood what type of databases we need to implement the LLM Twin \nuse case. Finally, we explored the process of setting up an AWS account, generating an access \nkey, and configuring the AWS CLI for programmatic access to the AWS cloud. We also gained a \ndeep understanding of AWS SageMaker and the reasons behind choosing it to build our LLM \nTwin application.\nIn the next chapter, we will explore the implementation of the LLM Twin project by starting with \nthe data collection ETL that scrapes posts, articles, and repositories from the internet and stores \nthem in a data warehouse.\n",
      "content_length": 2536,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "Chapter 2\n53\nReferences\n• \nAcsany, P. (2024, February 19). Dependency Management With Python Poetry. https://\nrealpython.com/dependency-management-python-poetry/\n•\nComet.ml. (n.d.). comet-ml/opik: Open-source end-to-end LLM Development Platform. GitHub. \nhttps://github.com/comet-ml/opik\n•\nCzakon, J. (2024, September 25). ML Experiment Tracking: What It Is, Why It Matters, and \nHow to Implement It. neptune.ai. https://neptune.ai/blog/ml-experiment-tracking\n•\nHopsworks. (n.d.). ML Artifacts (ML Assets)? Hopsworks. https://www.hopsworks.ai/\ndictionary/ml-artifacts\n•\nIntroduction | Documentation | Poetry – Python dependency management and packaging made \neasy. (n.d.). https://python-poetry.org/docs\n•\nJones, L. (2024, March 21). Managing Multiple Python Versions With pyenv. https://\nrealpython.com/intro-to-pyenv/\n•\nKaewsanmua, K. (2024, January 3). Best Machine Learning Workflow and Pipeline Orches-\ntration Tools. neptune.ai. https://neptune.ai/blog/best-workflow-and-pipeline-\norchestration-tools\n•\nMongoDB. (n.d.). What is NoSQL? NoSQL databases explained. https://www.mongodb.\ncom/resources/basics/databases/nosql-explained\n•\nNat-N. (n.d.). nat-n/poethepoet: A task runner that works well with poetry. GitHub. https://\ngithub.com/nat-n/poethepoet\n•\nOladele, S. (2024, August 29). ML Model Registry: The Ultimate Guide. neptune.ai. https://\nneptune.ai/blog/ml-model-registry\n•\nSchwaber-Cohen, R. (n.d.). What is a Vector Database & How Does it Work? Use Cases + Ex-\namples. Pinecone. https://www.pinecone.io/learn/vector-database/\n•\nStarter guide | ZenML Documentation. (n.d.). https://docs.zenml.io/user-guide/\nstarter-guide\n• \nVector DB Comparison. (n.d.). https://superlinked.com/vector-db-comparison\n",
      "content_length": 1715,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "Tooling and Installation\n54\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 171,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "3\nData Engineering\nThis chapter will begin exploring the LLM Twin project in more depth. We will learn how to \ndesign and implement the data collection pipeline to gather the raw data we will use in all our \nLLM use cases, such as fine-tuning or inference. As this is not a book on data engineering, we \nwill keep this chapter short and focus only on what is strictly necessary to collect the required \nraw data. Starting with Chapter 4, we will concentrate on LLMs and GenAI, exploring its theory \nand concrete implementation details.\nWhen working on toy projects or doing research, you usually have a static dataset with which \nyou work. But in our LLM Twin use case, we want to mimic a real-world scenario where we must \ngather and curate the data ourselves. Thus, implementing our data pipeline will connect the dots \nregarding how an end-to-end ML project works. This chapter will explore how to design and \nimplement an Extract, Transform, Load (ETL) pipeline that crawls multiple social platforms, \nsuch as Medium, Substack, or GitHub, and aggregates the gathered data into a MongoDB data \nwarehouse. We will show you how to implement various crawling methods, standardize the data, \nand load it into a data warehouse.\nWe will begin by designing the LLM Twin’s data collection pipeline and explaining the architecture \nof the ETL pipeline. Afterward, we will move directly to implementing the pipeline, starting with \nZenML, which will orchestrate the entire process. We will investigate the crawler implementation \nand understand how to implement a dispatcher layer that instantiates the right crawler class \nbased on the domain of the provided link while following software best practices. Next, we will \nlearn how to implement each crawler individually. Also, we will show you how to implement \na data layer on top of MongoDB to structure all our documents and interact with the database.\n",
      "content_length": 1899,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "Data Engineering\n56\nFinally, we will explore how to run the data collection pipeline using ZenML and query the col-\nlected data from MongoDB.\nThus, in this chapter, we will study the following topics:\n•\nDesigning the LLM Twin’s data collection pipeline\n•\nImplementing the LLM Twin’s data collection pipeline\n•\nGathering raw data into the data warehouse\nBy the end of this chapter, you will know how to design and implement an ETL pipeline to extract, \ntransform, and load raw data ready to be ingested into the ML application.\nDesigning the LLM Twin’s data collection pipeline\nBefore digging into the implementation, we must understand the LLM Twin’s data collection ETL \narchitecture, illustrated in Figure 3.1. We must explore what platforms we will crawl to extract \ndata from and how we will design our data structures and processes. However, the first step is \nunderstanding how our data collection pipeline maps to an ETL process.\nAn ETL pipeline involves three fundamental steps:\n1. \nWe extract data from various sources. We will crawl data from platforms like Medium, \nSubstack, and GitHub to gather raw data.\n2. We transform this data by cleaning and standardizing it into a consistent format suitable \nfor storage and analysis.\n3. \nWe load the transformed data into a data warehouse or database.\nFor our project, we use MongoDB as our NoSQL data warehouse. Although this is not a standard \napproach, we will explain the reasoning behind this choice shortly.\n",
      "content_length": 1468,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "Chapter 3\n57\nFigure 3.1: LLM Twin’s data collection ETL pipeline architecture\nWe want to design an ETL pipeline that inputs a user and a list of links as input. Afterward, it \ncrawls each link individually, standardizes the collected content, and saves it under that specific \nauthor in a MongoDB data warehouse. \n",
      "content_length": 314,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "Data Engineering\n58\nHence, the signature of the data collection pipeline will look as follows:\n• \nInput: A list of links and their associated user (the author)\n• \nOutput: A list of raw documents stored in the NoSQL data warehouse\nWe will use user and author interchangeably, as in most scenarios across the ETL pipeline, a \nuser is the author of the extracted content. However, within the data warehouse, we have only \na user collection.\nThe ETL pipeline will detect the domain of each link, based on which it will call a specialized \ncrawler. We implemented four different crawlers for three different data categories, as seen in \nFigure 3.2. First, we will explore the three fundamental data categories we will work with across \nthe book. All our collected documents can be boiled down to an article, repository (or code), and \npost. It doesn’t matter where the data comes from. We are primarily interested in the document’s \nformat. In most scenarios, we will have to process these data categories differently. Thus, we \ncreated a different domain entity for each, where each entity will have its class and collection \nin MongoDB. As we save the source URL within the document’s metadata, we will still know its \nsource and can reference it in our GenAI use cases.\nFigure 3.2: The relationship between the crawlers and the data categories\n",
      "content_length": 1342,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "Chapter 3\n59\nOur codebase supports four different crawlers:\n•\nMedium crawler: Used to collect data from Medium. It outputs an article document. It \nlogs in to Medium and crawls the HTML of the article’s link. Then, it extracts, cleans, and \nnormalizes the text from the HTML and loads the standardized text of the article into the \nNoSQL data warehouse.\n•\nCustom article crawler: It performs similar steps to the Medium crawler but is a more \ngeneric implementation for collecting articles from various sites. Thus, as it doesn’t im-\nplement any particularities of any platform, it doesn’t perform the login step and blindly \ngathers all the HTML from a particular link. This is enough for articles freely available \nonline, which you can find on Substack and people’s blogs. We will use this crawler as a \nsafety net when the link’s domain isn’t associated with the other supported crawlers. For \nexample, when providing a Substack link, it will default to the custom article crawler, but \nwhen providing a Medium URL, it will use the Medium crawler.\n•\nGitHub crawler: This collects data from GitHub. It outputs a repository document. It \nclones the repository, parses the repository file tree, cleans and normalizes the files, and \nloads them to the database.\n•\nLinkedIn crawler: This is used to collect data from LinkedIn. It outputs multiple post \ndocuments. It logs in to LinkedIn, navigates to the user’s feed, and crawls all the user’s \nlatest posts. For each post, it extracts its HTML, cleans and normalizes it, and loads it to \nMongoDB.\nIn the next section, we will examine each crawler’s implementation in detail. For now, note that \neach crawler accesses a specific platform or site in a particular way and extracts HTML from it. \nAfterward, all the crawlers parse the HTML, extract the text from it, and clean and normalize it \nso it can be stored in the data warehouse under the same interface.\nBy reducing all the collected data to three data categories and not creating a new data category \nfor every new data source, we can easily extend this architecture to multiple data sources with \nminimal effort. For example, if we want to start collecting data from X, we only have to imple-\nment a new crawler that outputs a post document, and that’s it. The rest of the code will remain \nuntouched. Otherwise, if we introduced the source dimension in the class and document struc-\nture, we would have to add code to all downstream layers to support any new data source. For \nexample, we would have to implement a new document class for each new source and adapt the \nfeature pipeline to support it.\n",
      "content_length": 2609,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "Data Engineering\n60\nFor our proof of concept, crawling a few hundred documents is enough, but if we want to scale it \nto a real-world product, we would probably need more data sources to crawl from. LLMs are da-\nta-hungry. Thus, you need thousands of documents for ideal results instead of just a few hundred. \nBut in many projects, it’s an excellent strategy to implement an end-to-end project version that \nisn’t the most accurate and iterate through it later. Thus, by using this architecture, you can easily \nadd more data sources in future iterations to gather a larger dataset. More on LLM fine-tuning \nand dataset size will be covered in the next chapter.\nHow is the ETL process connected to the feature pipeline? The feature pipeline ingests the raw \ndata from the MongoDB data warehouse, cleans it further, processes it into features, and stores it \nin the Qdrant vector DB to make it accessible for the LLM training and inference pipelines. Chap-\nter 4 provides more information on the feature pipeline. The ETL process is independent of the \nfeature pipeline. The two pipelines communicate with each other strictly through the MongoDB \ndata warehouse. Thus, the data collection pipeline can write data for MongoDB, and the feature \npipeline can read from it independently and on different schedules.\nWhy did we use MongoDB as a data warehouse? Using a transactional database, such as Mon-\ngoDB, as a data warehouse is uncommon. However, in our use case, we are working with small \namounts of data, which MongoDB can handle. Even if we plan to compute statistics on top of our \nMongoDB collections, it will work fine at the scale of our LLM Twin’s data (hundreds of docu-\nments). We picked MongoDB to store our raw data primarily because of the nature of our unstruc-\ntured data: text crawled from the internet. By mainly working with unstructured text, selecting \na NoSQL database that doesn’t enforce a schema made our development easier and faster. Also, \nMongoDB is stable and easy to use. Their Python SDK is intuitive. They provide a Docker image \nthat works out of the box locally and a cloud freemium tier that is perfect for proofs of concept, \nsuch as the LLM Twin. Thus, we can freely work with it locally and in the cloud. However, when \nworking with big data (millions of documents or more), using a dedicated data warehouse such \nas Snowflake or BigQuery will be ideal.\nNow that we’ve understood the architecture of the LLM Twin’s data collection pipeline, let’s \nmove on to its implementation.\n",
      "content_length": 2519,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "Chapter 3\n61\nImplementing the LLM Twin’s data collection pipeline\nAs we presented in Chapter 2, the entry point to each pipeline from our LLM Twin project is a \nZenML pipeline, which can be configured at runtime through YAML files and run through the \nZenML ecosystem. Thus, let’s start by looking into the ZenML digital_data_etl pipeline. You’ll \nnotice that this is the same pipeline we used as an example in Chapter 2 to illustrate ZenML. But \nthis time, we will dig deeper into the implementation, explaining how the data collection works \nbehind the scenes. After understanding how the pipeline works, we will explore the implemen-\ntation of each crawler used to collect data from various sites and the MongoDB documents used \nto store and query data from the data warehouse.\nZenML pipeline and steps\nIn the code snippet below, we can see the implementation of the ZenML digital_data_etl\npipeline, which inputs the user’s full name and a list of links that will be crawled under that user \n(considered the author of the content extracted from those links). Within the function, we call two \nsteps. In the first one, we look up the user in the database based on its full name. Then, we loop \nthrough all the links and crawl each independently. The pipeline’s implementation is available \nin our repository at pipelines/digital_data_etl.py.\nfrom zenml import pipeline\nfrom steps.etl import crawl_links, get_or_create_user\n@pipeline\ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\n    user = get_or_create_user(user_full_name)\n    last_step = crawl_links(user=user, links=links)\n    return last_step.invocation_id\n",
      "content_length": 1636,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "Data Engineering\n62\nFigure 3.3 shows a run of the digital_data_etl pipeline on the ZenML dashboard. The next \nphase is to explore the get_or_create_user and crawl_links ZenML steps individually. The \nstep implementation is available in our repository at steps/etl.\nFigure 3.3: Example of a digital_data_etl pipeline run from ZenML’s dashboard\nWe will start with the get_or_create_user ZenML step. We begin by importing the necessary \nmodules and functions used throughout the script.\nfrom loguru import logger\nfrom typing_extensions import Annotated\nfrom zenml import get_step_context, step\nfrom llm_engineering.application import utils\nfrom llm_engineering.domain.documents import UserDocument\nNext, we define the function’s signature, which takes a user’s full name as input and retrieves an \nexisting user or creates a new one in the MongoDB database if it doesn’t exist:\n@step\ndef get_or_create_user(user_full_name: str) -> Annotated[UserDocument, \n\"user\"]:\n",
      "content_length": 962,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "Chapter 3\n63\nUsing a utility function, we split the full name into first and last names. Then, we attempt to \nretrieve the user from the database or create a new one if it doesn’t exist. We also retrieve the \ncurrent step context and add metadata about the user to the output, which will be reflected in \nthe metadata of the user ZenML output artifact:\n    logger.info(f\"Getting or creating user: {user_full_name}\")\n    first_name, last_name = utils.split_user_full_name(user_full_name)\n    user = UserDocument.get_or_create(first_name=first_name, last_\nname=last_name)\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"user\", metadata=_get_\nmetadata(user_full_name, user))\n    return user\nAdditionally, we define a helper function called _get_metadata(), which builds a dictionary \ncontaining the query parameters and the retrieved user information, which will be added as \nmetadata to the user artifact:\ndef _get_metadata(user_full_name: str, user: UserDocument) -> dict:\n    return {\n        \"query\": {\n            \"user_full_name\": user_full_name,\n        },\n        \"retrieved\": {\n            \"user_id\": str(user.id),\n            \"first_name\": user.first_name,\n            \"last_name\": user.last_name,\n        },\n    }\nWe will move on to the crawl_links ZenML step, which collects the data from the provided links. \nThe code begins by importing essential modules and libraries for web crawling:\nfrom urllib.parse import urlparse\nfrom loguru import logger\n",
      "content_length": 1498,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "Data Engineering\n64\nfrom tqdm import tqdm\nfrom typing_extensions import Annotated\nfrom zenml import get_step_context, step\nfrom llm_engineering.application.crawlers.dispatcher import \nCrawlerDispatcher\nfrom llm_engineering.domain.documents import UserDocument\nFollowing the imports, the main function inputs a list of links written by a specific author. Within \nthis function, a crawler dispatcher is initialized and configured to handle specific domains such \nas LinkedIn, Medium, and GitHub:\n@step\ndef crawl_links(user: UserDocument, links: list[str]) -> \nAnnotated[list[str], \"crawled_links\"]:\n    dispatcher = CrawlerDispatcher.build().register_linkedin().register_\nmedium().register_github()\n    logger.info(f\"Starting to crawl {len(links)} link(s).\")\nThe function initializes variables to store the output metadata and count successful crawls. It then \niterates over each link. It attempts to crawl and extract data for each link, updating the count of \nsuccessful crawls and accumulating metadata about each URL:\n    metadata = {}\n    successfull_crawls = 0\n    for link in tqdm(links):\n        successfull_crawl, crawled_domain = _crawl_link(dispatcher, link, \nuser)\n        successfull_crawls += successfull_crawl\n        metadata = _add_to_metadata(metadata, crawled_domain, successfull_\ncrawl)\nAfter processing all links, the function attaches the accumulated metadata to the output artifact:\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"crawled_links\", \nmetadata=metadata)\n    logger.info(f\"Successfully crawled {successfull_crawls} / {len(links)} \n",
      "content_length": 1604,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "Chapter 3\n65\nlinks.\")\n    return links\nThe code includes a helper function that attempts to extract information from each link using the \nappropriate crawler based on the link’s domain. It handles any exceptions that may occur during \nextraction and returns a tuple indicating the crawl’s success and the link’s domain:\ndef _crawl_link(dispatcher: CrawlerDispatcher, link: str, user: \nUserDocument) -> tuple[bool, str]:\n    crawler = dispatcher.get_crawler(link)\n    crawler_domain = urlparse(link).netloc\n    try:\n        crawler.extract(link=link, user=user)\n        return (True, crawler_domain)\n    except Exception as e:\n        logger.error(f\"An error occurred while crawling: {e!s}\")\n        return (False, crawler_domain)\nAnother helper function is provided to update the metadata dictionary with the results of each \ncrawl:\ndef _add_to_metadata(metadata: dict, domain: str, successfull_crawl: bool) \n-> dict:\n    if domain not in metadata:\n        metadata[domain] = {}\n    metadata[domain][\"successful\"] = metadata.get(domain, {}).\nget(\"successful\", 0) + successfull_crawl\n    metadata[domain][\"total\"] = metadata.get(domain, {}).get(\"total\", 0) + \n1\n    return metadata\nAs seen in the abovementioned _crawl_link() function, the CrawlerDispatcher class knows \nwhat crawler to initialize based on each link’s domain. The logic is then abstracted away under \nthe crawler’s extract() method. Let’s zoom in on the CrawlerDispatcher class to understand \nhow this works fully.\n",
      "content_length": 1481,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "Data Engineering\n66\nThe dispatcher: How do you instantiate the right crawler?\nThe entry point to our crawling logic is the CrawlerDispatcher class. As illustrated in Figure \n3.4, the dispatcher acts as the intermediate layer between the provided links and the crawlers. It \nknows what crawler to associate with each URL.\nThe CrawlerDispatcher class knows how to extract the domain of each link and initialize the \nproper crawler that collects the data from that site. For example, if it detects the https://medium.\ncom domain when providing a link to an article, it will build an instance of the MediumCrawler\nused to crawl that particular platform. With that in mind, let’s explore the implementation of \nthe CrawlerDispatcher class.\nFigure 3.4: The relationship between the provided links, the CrawlerDispatcher, and the \ncrawlers\nAll the crawling logic is available in the GitHub repository at llm_engineering/\napplication/crawlers.\n",
      "content_length": 936,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "Chapter 3\n67\nWe begin by importing the necessary Python modules for URL handling and regex, along with \nimporting our crawler classes:\nimport re\nfrom urllib.parse import urlparse\nfrom loguru import logger\nfrom .base import BaseCrawler\nfrom .custom_article import CustomArticleCrawler\nfrom .github import GithubCrawler\nfrom .linkedin import LinkedInCrawler\nfrom .medium import MediumCrawler\nThe CrawlerDispatcher class is defined to manage and dispatch appropriate crawler instances \nbased on given URLs and their domains. Its constructor initializes a registry to store the regis-\ntered crawlers.\nclass CrawlerDispatcher:\n    def __init__(self) -> None:\n        self._crawlers = {}\nAs we are using the builder creational pattern to instantiate and configure the dispatcher, we \ndefine a build() class method that returns an instance of the dispatcher:\n    @classmethod\n    def build(cls) -> \"CrawlerDispatcher\":\n        dispatcher = cls()\n        return dispatcher\nThe dispatcher includes methods to register crawlers for specific platforms like Medium, Linke-\ndIn, and GitHub. These methods use a generic register() method under the hood to add each \ncrawler to the registry. By returning self, we follow the builder creational pattern (more on the \nbuilder pattern: https://refactoring.guru/design-patterns/builder). We can chain mul-\ntiple register_*() methods when instantiating the dispatcher as follows: CrawlerDispatcher.\nbuild().register_linkedin().register_medium().\n    def register_medium(self) -> \"CrawlerDispatcher\":\n        self.register(\"https://medium.com\", MediumCrawler)\n",
      "content_length": 1589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "Data Engineering\n68\n        return self\n    def register_linkedin(self) -> \"CrawlerDispatcher\":\n        self.register(\"https://linkedin.com\", LinkedInCrawler)\n        return self\n    def register_github(self) -> \"CrawlerDispatcher\":\n        self.register(\"https://github.com\", GithubCrawler)\n        return self\nThe generic register() method normalizes each domain to ensure its format is consistent be-\nfore it’s added as a key to the self._crawlers registry of the dispatcher. This is a critical step, as \nwe will use the key of the dictionary as the domain pattern to match future links with a crawler:\n    def register(self, domain: str, crawler: type[BaseCrawler]) -> None:\n        parsed_domain = urlparse(domain)\n        domain = parsed_domain.netloc\n        self._crawlers[r\"https://(www\\.)?{}/*\".format(re.escape(domain))] \n= crawler\nFinally, the get_crawler() method determines the appropriate crawler for a given URL by match-\ning it against the registered domains. If no match is found, it logs a warning and defaults to using \nthe CustomArticleCrawler.\n    def get_crawler(self, url: str) -> BaseCrawler:\n        for pattern, crawler in self._crawlers.items():\n            if re.match(pattern, url):\n                return crawler()\n        else:\n            logger.warning(f\"No crawler found for {url}. Defaulting to \nCustomArticleCrawler.\")\n            return CustomArticleCrawler()\nThe next step in understanding how the data collection pipeline works is analyzing each crawler \nindividually.\n",
      "content_length": 1509,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "Chapter 3\n69\nThe crawlers\nBefore exploring each crawler’s implementation, we must present their base class, which defines \na unified interface for all the crawlers. As shown in Figure 3.4, we can implement the dispatcher \nlayer because each crawler follows the same signature. Each class implements the extract()\nmethod, allowing us to leverage OOP techniques such as polymorphism, where we can work with \nabstract objects without knowing their concrete subclass. For example, in the _crawl_link()\nfunction from the ZenML steps, we had the following code:\ncrawler = dispatcher.get_crawler(link)\ncrawler.extract(link=link, user=user)\nNote how we called the extract() method without caring about what specific type of crawler \nwe instantiated. To conclude, working with abstract interfaces ensures core reusability and ease \nof extension.\nBase classes\nNow, let’s explore the BaseCrawler interface, which can be found in the repository at https://\ngithub.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/\napplication/crawlers/base.py.\nfrom abc import ABC, abstractmethod\nclass BaseCrawler(ABC):\n    model: type[NoSQLBaseDocument]\n    @abstractmethod\n    def extract(self, link: str, **kwargs) -> None: ...\nAs mentioned above, the interface defines an extract() method that takes as input a link. Also, it \ndefines a model attribute at the class level that represents the data category document type used \nto save the extracted data into the MongoDB data warehouse. Doing so allows us to customize \neach subclass with different data categories while preserving the same attributes at the class \nlevel. We will soon explore the NoSQLBaseDocument class when digging into the document entities.\nWe also extend the BaseCrawler class with a BaseSeleniumCrawler class, which implements \nreusable functionality that uses Selenium to crawl various sites, such as Medium or LinkedIn. \nSelenium is a tool for automating web browsers. It’s used to interact with web pages program-\nmatically (like logging into LinkedIn, navigating through profiles, etc.). \n",
      "content_length": 2062,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "Data Engineering\n70\nSelenium can programmatically control various browsers such as Chrome, Firefox, or Brave. For \nthese specific platforms, we need Selenium to manipulate the browser programmatically to log in \nand scroll through the newsfeed or article before being able to extract the entire HTML. For other \nsites, where we don’t have to go through the login step or can directly load the whole page, we \ncan extract the HTML from a particular URL using more straightforward methods than Selenium.\nThe code begins by setting up the necessary imports and configurations for web crawling using \nSelenium and the ChromeDriver initializer. The chromedriver_autoinstaller ensures that \nthe appropriate version of ChromeDriver is installed and added to the system path, maintain-\ning compatibility with the installed version of your Google Chrome browser (or other Chromi-\num-based browser). Selenium will use the ChromeDriver to communicate with the browser \nand open a headless session, where we can programmatically manipulate the browser to access \nvarious URLs, click on specific elements, such as buttons, or scroll through the newsfeed. Using \nthe chromedriver_autoinstaller, we ensure we always have the correct ChromeDriver version \ninstalled that matches our machine’s Chrome browser version.\nimport time\nfrom tempfile import mkdtemp\nimport chromedriver_autoinstaller\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom llm_engineering.domain.documents import NoSQLBaseDocument\n# Check if the current version of chromedriver exists\n# and if it doesn't exist, download it automatically,\n# then add chromedriver to path\nchromedriver_autoinstaller.install()\nNext, we define the BaseSeleniumCrawler class for use cases where we need Selenium to collect \nthe data, such as collecting data from Medium or LinkedIn. \nFor the Selenium-based crawlers to work, you must install Chrome on your machine \n(or a Chromium-based browser such as Brave).\n",
      "content_length": 1983,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "Chapter 3\n71\nIts constructor initializes various Chrome options to optimize performance, enhance security, and \nensure a headless browsing environment. These options disable unnecessary features like GPU \nrendering, extensions, and notifications, which can interfere with automated browsing. These \nare standard configurations when crawling in headless mode:\nclass BaseSeleniumCrawler(BaseCrawler, ABC):\n    def __init__(self, scroll_limit: int = 5) -> None:\n        options = webdriver.ChromeOptions()\n       \n        options.add_argument(\"--no-sandbox\")\n        options.add_argument(\"--headless=new\")\n        options.add_argument(\"--disable-dev-shm-usage\")\n        options.add_argument(\"--log-level=3\")\n        options.add_argument(\"--disable-popup-blocking\")\n        options.add_argument(\"--disable-notifications\")\n        options.add_argument(\"--disable-extensions\")\n        options.add_argument(\"--disable-background-networking\")\n        options.add_argument(\"--ignore-certificate-errors\")\n        options.add_argument(f\"--user-data-dir={mkdtemp()}\")\n        options.add_argument(f\"--data-path={mkdtemp()}\")\n        options.add_argument(f\"--disk-cache-dir={mkdtemp()}\")\n        options.add_argument(\"--remote-debugging-port=9226\")\nAfter configuring the Chrome options, the code allows subclasses to set any additional driver \noptions by calling the set_extra_driver_options() method. It then initializes the scroll limit \nand creates a new instance of the Chrome driver with the specified options:\n        self.set_extra_driver_options(options)\n        self.scroll_limit = scroll_limit\n        self.driver = webdriver.Chrome(\n            options=options,\n        )\nThe BaseSeleniumCrawler class includes placeholder methods for set_extra_driver_options()\nand login(), which subclasses can override to provide specific functionality. This ensures mod-\nularity, as every platform has a different login page with a different HTML structure:\n    def set_extra_driver_options(self, options: Options) -> None:\n",
      "content_length": 2009,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "Data Engineering\n72\n        pass\n    def login(self) -> None:\n        pass\nFinally, the scroll_page() method implements a scrolling mechanism to navigate through pages, \nsuch as LinkedIn, up to a specified scroll limit. It scrolls to the bottom of the page, waits for new \ncontent to load, and repeats the process until it reaches the end of the page or the scroll limit is \nexceeded. This method is essential for feeds where the content appears as the user scrolls:\n    def scroll_page(self) -> None:\n        \"\"\"Scroll through the LinkedIn page based on the scroll limit.\"\"\"\n        current_scroll = 0\n        last_height = self.driver.execute_script(\"return document.body.\nscrollHeight\")\n        while True:\n            self.driver.execute_script(\"window.scrollTo(0, document.body.\nscrollHeight);\")\n            time.sleep(5)\n            new_height = self.driver.execute_script(\"return document.body.\nscrollHeight\")\n            if new_height == last_height or (self.scroll_limit and \ncurrent_scroll >= self.scroll_limit):\n                break\n            last_height = new_height\n            current_scroll += 1\nWe’ve understood what the base classes of our crawlers look like. Next, we will look into the \nimplementation of the following specific crawlers:\n•\nGitHubCrawler(BaseCrawler)\n•\nCustomArticleCrawler(BaseCrawler)\n•\nMediumCrawler(BaseSeleniumCrawler)\nYou can find the implementation of the above crawlers in the GitHub repository at \nhttps://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main \n/llm_engineering/application/crawlers.\n",
      "content_length": 1553,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "Chapter 3\n73\nGitHubCrawler class\nThe GithubCrawler class is designed to scrape GitHub repositories, extending the functionality \nof the BaseCrawler. We don’t have to log in to GitHub through the browser, as we can leverage \nGit’s clone functionality. Thus, we don’t have to leverage any Selenium functionality. Upon ini-\ntialization, it sets up a list of patterns to ignore standard files and directories found in GitHub \nrepositories, such as .git, .toml, .lock, and .png, ensuring that unnecessary files are excluded \nfrom the scraping process:\nclass GithubCrawler(BaseCrawler):\n    model = RepositoryDocument\n    def __init__(self, ignore=(\".git\", \".toml\", \".lock\", \".png\")) -> None:\n        super().__init__()\n        self._ignore = ignore\nNext, we implement the extract() method, where the crawler first checks if the repository has \nalready been processed and stored in the database. If it exists, it exits the method to prevent \nstoring duplicates:\ndef extract(self, link: str, **kwargs) -> None:\n    old_model = self.model.find(link=link)\n    if old_model is not None:\n        logger.info(f\"Repository already exists in the database: {link}\")\n        return\nIf the repository is new, the crawler extracts the repository name from the link. Then, it creates \na temporary directory to clone the repository to ensure that the cloned repository is cleaned up \nfrom the local disk after it’s processed:\n    logger.info(f\"Starting scrapping GitHub repository: {link}\")\n    repo_name = link.rstrip(\"/\").split(\"/\")[-1]\n    local_temp = tempfile.mkdtemp()\nWithin a try block, the crawler changes the current working directory to the temporary directory \nand executes the git clone command in a different process:\n    try:\n",
      "content_length": 1721,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "Data Engineering\n74\n        os.chdir(local_temp)\n        subprocess.run([\"git\", \"clone\", link])\nAfter successfully cloning the repository, the crawler constructs the path to the cloned repository. \nIt initializes an empty dictionary used to aggregate the content of the files in a standardized way. \nIt walks through the directory tree, skipping over any directories or files that match the ignore \npatterns. For each relevant file, it reads the content, removes any spaces, and stores it in the dic-\ntionary with the file path as the key:\n        repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])  # \n        tree = {}\n        for root, _, files in os.walk(repo_path):\n            dir = root.replace(repo_path, \"\").lstrip(\"/\")\n            if dir.startswith(self._ignore):\n                continue\n            for file in files:\n                if file.endswith(self._ignore):\n                    continue\n                file_path = os.path.join(dir, file)\n                with open(os.path.join(root, file), \"r\", errors=\"ignore\") \nas f:\n                    tree[file_path] = f.read().replace(\" \", \"\")\nIt then creates a new instance of the RepositoryDocument model, populating it with the repos-\nitory content, name, link, platform information, and author details. The instance is then saved \nto MongoDB:\n        user = kwargs[\"user\"]\n        instance = self.model(\n            content=tree,\n            name=repo_name,\n            link=link,\n            platform=\"github\",\n            author_id=user.id,\n            author_full_name=user.full_name,\n        )\n        instance.save()\n",
      "content_length": 1599,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "Chapter 3\n75\nFinally, whether the scraping succeeds or an exception occurs, the crawler ensures that the tem-\nporary directory is removed to clean up any resources used during the process:\n    except Exception:\n        raise\n    finally:\n        shutil.rmtree(local_temp)\n    logger.info(f\"Finished scrapping GitHub repository: {link}\")\nCustomArticleCrawler class\nThe CustomArticleCrawler class takes a different approach to collecting data from the in-\nternet. It leverages the AsyncHtmlLoader class to read the entire HTML from a link and the \nHtml2TextTransformer class to extract the text from that HTML. Both classes are made available \nby the langchain_community Python package, as seen below, where we import all the necessary \nPython modules:\nfrom urllib.parse import urlparse\nfrom langchain_community.document_loaders import AsyncHtmlLoader\nfrom langchain_community.document_transformers.html2text import \nHtml2TextTransformer\nfrom loguru import logger\nfrom llm_engineering.domain.documents import ArticleDocument\nfrom .base import BaseCrawler\nNext, we define the CustomArticleCrawler class, which inherits from BaseCrawler. As before, \nwe don’t need to log in or use the scrolling functionality provided by Selenium. In the extract\nmethod, we first check if the article exists in the database to avoid duplicating content:\nclass CustomArticleCrawler(BaseCrawler):\n    model = ArticleDocument\n    def extract(self, link: str, **kwargs) -> None:\n        old_model = self.model.find(link=link)\n        if old_model is not None:\n",
      "content_length": 1535,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "Data Engineering\n76\n            logger.info(f\"Article already exists in the database: {link}\")\n            return\nIf the article doesn’t exist, we proceed to scrape it. We use the AsyncHtmlLoader class to load the \nHTML from the provided link. After, we transform it into plain text using the Html2TextTransformer\nclass, which returns a list of documents. We are only interested in the first document. As we dele-\ngate the whole logic to these two classes, we don’t control how the content is extracted and parsed. \nThat’s why we used this class as a fallback system for domains where we don’t have anything cus-\ntom implemented. These two classes follow the LangChain paradigm, which provides high-level \nfunctionality that works decently in most scenarios. It is fast to implement but hard to customize. \nThat is one of the reasons why many developers avoid using LangChain in production use cases:\n        logger.info(f\"Starting scrapping article: {link}\")\n        loader = AsyncHtmlLoader([link])\n        docs = loader.load()\n        html2text = Html2TextTransformer()\n        docs_transformed = html2text.transform_documents(docs)\n        doc_transformed = docs_transformed[0]\nWe get the page content from the extracted document, plus relevant metadata such as the title, \nsubtitle, content, and language:\n        content = {\n            \"Title\": doc_transformed.metadata.get(\"title\"),\n            \"Subtitle\": doc_transformed.metadata.get(\"description\"),\n            \"Content\": doc_transformed.page_content,\n            \"language\": doc_transformed.metadata.get(\"language\"),\n        }\nNext, we parse the URL to determine the platform (or domain) from which the article was scraped:\n        parsed_url = urlparse(link)\n        platform = parsed_url.netloc\nWe then create a new instance of the article model, populating it with the extracted content. \nFinally, we save this instance to the MongoDB data warehouse:\n        user = kwargs[\"user\"]\n",
      "content_length": 1946,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "Chapter 3\n77\n        instance = self.model(\n            content=content,\n            link=link,\n            platform=platform,\n            author_id=user.id,\n            author_full_name=user.full_name,\n        )\n        instance.save()\n        logger.info(f\"Finished scrapping custom article: {link}\")       \nSo far, we have seen how to crawl GitHub repositories and random sites using LangChain utility \nfunctions. Lastly, we must explore a crawler using Selenium to manipulate the browser program-\nmatically. Thus, we will continue with the MediumCrawler implementation.\nMediumCrawler class\nThe code begins by importing essential libraries and defining the MediumCrawler class, which \ninherits from BaseSeleniumCrawler:\nfrom bs4 import BeautifulSoup\nfrom loguru import logger\nfrom llm_engineering.domain.documents import ArticleDocument\nfrom .base import BaseSeleniumCrawler\nclass MediumCrawler(BaseSeleniumCrawler):\n    model = ArticleDocument\nWithin the MediumCrawler class, we leverage the set_extra_driver_options() method to extend \nthe default driver options used by Selenium:\n    def set_extra_driver_options(self, options) -> None:\n        options.add_argument(r\"--profile-directory=Profile 2\")\nThe extract() method implements the core functionality, first checking whether the article \nexists in the database to prevent duplicate entries. \n",
      "content_length": 1352,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "Data Engineering\n78\nIf the article is new, the method proceeds to navigate to the article’s link and scroll through the \npage to ensure all content is loaded:\n    def extract(self, link: str, **kwargs) -> None:\n        old_model = self.model.find(link=link)\n        if old_model is not None:\n            logger.info(f\"Article already exists in the database: {link}\")\n            return\n        logger.info(f\"Starting scrapping Medium article: {link}\")\n        self.driver.get(link)\n        self.scroll_page()\nAfter fully loading the page, the method uses BeautifulSoup to parse the HTML content and \nextract the article’s title, subtitle, and full text. BeautifulSoup is a popular Python library for \nweb scraping and parsing HTML or XML documents. Thus, we used it to extract all the HTML \nelements we needed from the HTML accessed with Selenium. Finally, we aggregate everything \ninto a dictionary:\n        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n        title = soup.find_all(\"h1\", class_=\"pw-post-title\")\n        subtitle = soup.find_all(\"h2\", class_=\"pw-subtitle-paragraph\")\n        data = {\n            \"Title\": title[0].string if title else None,\n            \"Subtitle\": subtitle[0].string if subtitle else None,\n            \"Content\": soup.get_text(),\n        }\nFinally, the method closes the WebDriver to free up resources. It then creates a new ArticleDocument\ninstance, populates it with the extracted content and user information provided via kwargs, and \nsaves it to the database:\n        self.driver.close()\n        user = kwargs[\"user\"]\n        instance = self.model(\n",
      "content_length": 1604,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "Chapter 3\n79\n            platform=\"medium\",\n            content=data,\n            link=link,\n            author_id=user.id,\n            author_full_name=user.full_name,\n        )\n        instance.save()\n        logger.info(f\"Successfully scraped and saved article: {link}\")\nWith that, we conclude the MediumCrawler implementation. The LinkedIn crawler follows a \nsimilar pattern to the Medium one, where it uses Selenium to log in and access the feed of a \nuser’s latest posts. Then, it extracts the posts and scrolls through the feed to load the next page \nuntil a limit is hit. You can check the full implementation in our repository at https://github.\ncom/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/\ncrawlers/linkedin.py.\nWith the rise of LLMs, collecting data from the internet has become a critical step in many re-\nal-world AI applications. Hence, more high-level tools have appeared in the Python ecosystem, \nsuch as Scrapy (https://github.com/scrapy/scrapy), which crawls websites and extracts struc-\ntured data from their pages, and Crawl4AI (https://github.com/unclecode/crawl4ai), which \nis highly specialized in crawling data for LLMs and AI applications.\nIn this section, we’ve looked at implementing three types of crawlers: one that leverages the \ngit executable in a subprocess to clone GitHub repositories, one that uses LangChain utilities \nto extract the HTML of a single web page, and one that leverages Selenium for more complex \nscenarios where we have to navigate through the login page, scroll the article to load the entire \nHTML, and extract it into text format. The last step is understanding how the document classes \nwe’ve used across the chapter, such as the ArticleDocument, work.\nThe NoSQL data warehouse documents\nWe had to implement three document classes to structure our data categories. These classes \ndefine the specific attributes we require for a document, such as the content, author, and source \nlink. It is best practice to structure your data in classes instead of dictionaries, as the attributes we \nexpect for each item are more verbose, reducing run errors. For example, when accessing a value \nfrom a Python dictionary, we can never be sure it is present or its type is current. By wrapping \nour data items with classes, we can ensure each attribute is as expected. \n",
      "content_length": 2349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "Data Engineering\n80\nBy leveraging Python packages such as Pydantic, we have out-of-the-box type validation, which \nensures consistency in our datasets. Thus, we modeled the data categories as the following doc-\nument classes, which we already used in the code up until point:\n• \nArticleDocument class\n• \nPostDocument class\n• \nRepositoryDocument class\nThese are not simple Python data classes or Pydantic models. They support read and write oper-\nations on top of the MongoDB data warehouse. To inject the read-and-write functionality into \nall the document classes without repeating any code, we used the Object-Document Mapping\n(ODM) software pattern, which is based on the object-relational mapping (ORM) pattern. Thus, \nlet’s first explore ORM, then move to ODM, and, finally, dig into our custom ODM implementation \nand document classes.\nThe ORM and ODM software patterns\nBefore we talk about software patterns, let’s see what ORM is. It’s a technique that lets you query \nand manipulate data from a database using an object-oriented paradigm. Instead of writing SQL \nor API-specific queries, you encapsulate all the complexity under an ORM class that knows how \nto handle all the database operations, most commonly CRUD operations. Thus, working with \nORM removes the need to handle the database operations manually and reduces the need to write \nboilerplate code manually. An ORM interacts with a SQL database, such as PostgreSQL or MySQL.\nMost modern Python applications use ORMs when interacting with the database. Even though \nSQL is still a popular choice in the data world, you rarely see raw SQL queries in Python backend \ncomponents. The most popular Python ORM is SQLAlchemy (https://www.sqlalchemy.org/). \nAlso, with the rise of FastAPI, SQLModel is (https://github.com/fastapi/sqlmodel) a common \nchoice, which is a wrapper over SQLAlchemy that makes the integration easier with FastAPI.\nFor example, using SQLAlchemy, we defined a User ORM with the ID and name fields. The User\nORM is mapped to the users table within the SQL database. Thus, when we create a new user \nand commit it to the database, it is automatically saved to the users table. The same applies to \nall the CRUD operations on top of the User class.\nfrom sqlalchemy import Column, Integer, String, create_engine\nfrom sqlalchemy.orm import declarative_base, sessionmaker\n   Base = declarative_base()\n",
      "content_length": 2383,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "Chapter 3\n81\n# Define a class that maps to the users table.\n   class User(Base):\n    __tablename__ = \"users\"\n    id = Column(Integer, primary_key=True)\n   \nname = Column(String)\nUsing the User ORM, we can quickly insert or query users directly from Python without writing a \nline of SQL. Note that an ORM usually supports all CRUD operations. Here is a code snippet that \nshows how to save an instance of the User ORM to a SQLite database:\nengine = create_engine(\"sqlite:///:memory:\")\nBase.metadata.create_all(engine)\n# Create a session used to interact with the database.\nSession = sessionmaker(bind=engine)\nsession = Session()\n# Add a new user.\nnew_user = User(name=\"Alice\")\nsession.add(new_user)\nsession.commit()\nAlso, this is how we can query a user from the users SQLite table:\nuser = session.query(User).first()\nif user:\nprint(f\"User ID: {user.id}\")\nprint(f\"User name: {user.name}\")\nThe ODM pattern is extremely similar to ORM, but instead of working with SQL databases and \ntables, it works with NoSQL databases (such as MongoDB) and unstructured collections. As we \nwork with NoSQL databases, the data structure is centered on collections, which store JSON-like \ndocuments rather than rows in tables.\nFind the entire script and how to run it in the GitHub repository at code_snippets/03_\norm.py.\n",
      "content_length": 1304,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "Data Engineering\n82\nTo conclude, ODM simplifies working with document-based NoSQL databases and maps ob-\nject-oriented code to JSON-like documents. We will implement a light ODM module on top of \nMongoDB to fully understand how ODM works.\nImplementing the ODM class\nThis section will explore how to implement an ODM class from scratch. This is an excellent ex-\nercise to learn how ODM works and sharpen our skills in writing modular and reusable Python \nclasses. Hence, we will implement a base ODM class called NoSQLBaseDocument, from which all \nthe other documents will inherit to interact with the MongoDB data warehouse.\nThe code starts by importing essential modules and setting up the database connection. Through \nthe _database variable, we establish a connection to the database specified in the settings, which \nis by default called twin:\nimport uuid\nfrom abc import ABC\nfrom typing import Generic, Type, TypeVar\nfrom loguru import logger\nfrom pydantic import UUID4, BaseModel, Field\nfrom pymongo import errors\nfrom llm_engineering.domain.exceptions import ImproperlyConfigured\nfrom llm_engineering.infrastructure.db.mongo import connection\nfrom llm_engineering.settings import settings\n_database = connection.get_database(settings.DATABASE_NAME)\nNext, we define a type variable T bound to the NoSQLBaseDocument class. The variable leverages \nPython’s generic module, allowing us to generalize the class’s types. For example, when we im-\nplement the ArticleDocument class, which will inherit from the NoSQLBaseDocument class, all the \ninstances where T was used will be replaced with the ArticleDocument type when analyzing the \nsignature of functions (more on Python generics: https://realpython.com/python312-typing). \nThe class can be found in our repository at llm_engineering/domain/base/nosql.\npy.\n",
      "content_length": 1813,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "Chapter 3\n83\nThe NoSQLBaseDocument class is then declared as an abstract base class inheriting from Pydan-\ntic’s BaseModel, Python’s Generic (which provides the functionality described earlier), and ABC\n(making the class abstract) classes. This class serves as the foundational ODM class:\nT = TypeVar(\"T\", bound=\"NoSQLBaseDocument\")\nclass NoSQLBaseDocument(BaseModel, Generic[T], ABC):\nWithin the NoSQLBaseDocument class, an id field is defined as a UUID4, with a default factory \ngenerating a unique UUID. The class also implements the __eq__ and __hash__ methods to allow \ninstances to be compared and used in hashed collections like sets or as dictionary keys based on \ntheir unique id attribute:\nid: UUID4 = Field(default_factory=uuid.uuid4)\ndef __eq__(self, value: object) -> bool:\n    if not isinstance(value, self.__class__):\n        return False\n    return self.id == value.id\ndef __hash__(self) -> int:\n    return hash(self.id)\nThe class provides methods for converting between MongoDB documents and class instances. The \nfrom_mongo() class method transforms a dictionary retrieved from MongoDB into an instance of \nthe class. The to_mongo() instance method converts the model instance into a dictionary suitable \nfor MongoDB insertion:\n@classmethod\ndef from_mongo(cls: Type[T], data: dict) -> T:\n    if not data:\n        raise ValueError(\"Data is empty.\")\n    id = data.pop(\"_id\")\n    return cls(**dict(data, id=id))\ndef to_mongo(self: T, **kwargs) -> dict:\n",
      "content_length": 1468,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "Data Engineering\n84\n    exclude_unset = kwargs.pop(\"exclude_unset\", False)\n    by_alias = kwargs.pop(\"by_alias\", True)\n    parsed = self.model_dump(exclude_unset=exclude_unset, by_alias=by_\nalias, **kwargs)\n    if \"_id\" not in parsed and \"id\" in parsed:\n        parsed[\"_id\"] = str(parsed.pop(\"id\"))\n    for key, value in parsed.items():\n        if isinstance(value, uuid.UUID):\n            parsed[key] = str(value)\n    return parsed\nThe save() method allows an instance of the model to be inserted into a MongoDB collection. It \nretrieves the appropriate collection, converts the instance into a MongoDB-compatible document \nleveraging the to_mongo() method described above, and attempts to insert it into the database, \nhandling any write errors that may occur:\ndef save(self: T, **kwargs) -> T | None:\n    collection = _database[self.get_collection_name()]\n    try:\n        collection.insert_one(self.to_mongo(**kwargs))\n        return self\n    except errors.WriteError:\n        logger.exception(\"Failed to insert document.\")\n        return None\nThe get_or_create() class method attempts to find a document in the database matching the \nprovided filter options. If a matching document is found, it is converted into an instance of the class. \nIf not, a new instance is created with the filter options as its initial data and saved to the database:\n@classmethod\ndef get_or_create(cls: Type[T], **filter_options) -> T:\n    collection = _database[cls.get_collection_name()]\n    try:\n",
      "content_length": 1483,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "Chapter 3\n85\n        instance = collection.find_one(filter_options)\n        if instance:\n            return cls.from_mongo(instance)\n        new_instance = cls(**filter_options)\n        new_instance = new_instance.save()\n        return new_instance\n    except errors.OperationFailure:\n        logger.exception(f\"Failed to retrieve document with filter \noptions: {filter_options}\")\n        raise\nThe bulk_insert() class method allows multiple documents to be inserted into the database \nat once:\n@classmethod\ndef bulk_insert(cls: Type[T], documents: list[T], **kwargs) -> bool:\n    collection = _database[cls.get_collection_name()]\n    try:\n        collection.insert_many([doc.to_mongo(**kwargs) for doc in \ndocuments])\n        return True\n    except (errors.WriteError, errors.BulkWriteError):\nlogger.error(f\"Failed to insert documents of type {cls.__name__}\")\n        return False\nThe find() class method searches for a single document in the database that matches the given \nfilter options:\n@classmethod\ndef find(cls: Type[T], **filter_options) -> T | None:\n    collection = _database[cls.get_collection_name()]\n    try:\n        instance = collection.find_one(filter_options)\n",
      "content_length": 1178,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "Data Engineering\n86\n        if instance:\n            return cls.from_mongo(instance)\n        return None\n    except errors.OperationFailure:\n        logger.error(\"Failed to retrieve document.\")\n        return None\nSimilarly, the bulk_find() class method retrieves multiple documents matching the filter options. \nIt converts each retrieved MongoDB document into a model instance, collecting them into a list:\n@classmethod\ndef bulk_find(cls: Type[T], **filter_options) -> list[T]:\n    collection = _database[cls.get_collection_name()]\n    try:\n        instances = collection.find(filter_options)\n        return [document for instance in instances if (document := cls.\nfrom_mongo(instance)) is not None]\n    except errors.OperationFailure:\n        logger.error(\"Failed to retrieve document.\")\n        return []\nFinally, the get_collection_name() class method determines the name of the MongoDB collec-\ntion associated with the class. It expects the class to have a nested Settings class with a name at-\ntribute specifying the collection name. If this configuration is missing, an ImproperlyConfigured\nexception will be raised specifying that the subclass should define a nested Settings class:\n@classmethod\ndef get_collection_name(cls: Type[T]) -> str:\n    if not hasattr(cls, \"Settings\") or not hasattr(cls.Settings, \"name\"):\n        raise ImproperlyConfigured(\n            \"Document should define an Settings configuration class with \nthe name of the collection.\"\n        )\n    return cls.Settings.name\n",
      "content_length": 1503,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "Chapter 3\n87\nWe can configure each subclass using the nested Settings class, such as defining the collection \nname, or anything else specific to that subclass. Within the Python ecosystem, there is an ODM \nimplementation on top of MongoDB, called mongoengine, which you can find on GitHub. It follows \na pattern similar to ours but more comprehensive. We implemented it by ourselves, as it was an \nexcellent exercise to practice writing modular and generic code following best OOP principles, \nwhich are essential for implementing production-level code.\nData categories and user document classes\nThe last piece of the puzzle is to see the implementation of the subclasses that inherit from the \nNoSQLBaseDocument base class. These are the concrete classes that define our data categories. \nYou’ve seen these classes used across the chapter when working with articles, repositories, and \nposts within the crawler classes.\nWe begin by importing the essential Python modules and the ODM base class:\nfrom abc import ABC\nfrom typing import Optional\nfrom pydantic import UUID4, Field\nfrom .base import NoSQLBaseDocument\nfrom .types import DataCategory\nWe define an enum class, where we centralize all our data category types. These variables will act \nas constants in configuring all our ODM classes throughout the book.\nfrom enum import StrEnum\nclass DataCategory(StrEnum):\n    PROMPT = \"prompt\"\n    QUERIES = \"queries\"\n    INSTRUCT_DATASET_SAMPLES = \"instruct_dataset_samples\"\nThe class can be found in the repository at llm_engineering/domain/types.py.\n",
      "content_length": 1550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "Data Engineering\n88\n    INSTRUCT_DATASET = \"instruct_dataset\"\n    PREFERENCE_DATASET_SAMPLES = \"preference_dataset_samples\"\n    PREFERENCE_DATASET = \"preference_dataset\"\n    POSTS = \"posts\"\n    ARTICLES = \"articles\"\n     REPOSITORIES = \"repositories\"\nThe Document class is introduced as an abstract base model for other documents on top of the \nNoSQLBaseDocument ODM class. It includes common attributes like content, platform, and author \ndetails, providing a standardized structure for documents that will inherit from it:\nclass Document(NoSQLBaseDocument, ABC):\n    content: dict\n    platform: str\n    author_id: UUID4 = Field(alias=\"author_id\")\n    author_full_name: str = Field(alias=\"author_full_name\")\nFinally, specific document types are defined by extending the Document class. The \nRepositoryDocument, PostDocument, and ArticleDocument classes represent different catego-\nries of data, each with unique fields and settings that specify their respective collection names \nin the database:\nclass RepositoryDocument(Document):\n    name: str\n    link: str\n    class Settings:\n        name = DataCategory.REPOSITORIES\nclass PostDocument(Document):\n    image: Optional[str] = None\n    link: str | None = None\n    class Settings:\n        name = DataCategory.POSTS\n",
      "content_length": 1267,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "Chapter 3\n89\nclass ArticleDocument(Document):\n    link: str\n    class Settings:\n        name = DataCategory.ARTICLES\nFinally, we define the UserDocument class, which is used to store and query all the users from the \nLLM Twin project:\nclass UserDocument(NoSQLBaseDocument):\n    first_name: str\n    last_name: str\n    class Settings:\n        name = \"users\"\n    @property\n    def full_name(self):\n        return f\"{self.first_name} {self.last_name}\"\nBy implementing the NoSQLBaseDocument ODM class, we had to focus solely on the fields and \nspecific functionality of each document or domain entity. All the CRUD functionality is delegated \nto the parent class. Also, by leveraging Pydantic to define the fields, we have out-of-the-box type \nvalidation. For example, when creating an instance of the ArticleDocument class, if the provided \nlink is None or not a string, it will throw an error signaling that the data is invalid.\nWith that, we’ve finished implementing our data collection pipeline, starting with the ZenML \ncomponents. Then, we looked into the implementation of the crawlers and, finally, wrapped it \nup with the ODM class and data category documents. The last step is to run the data collection \npipeline and ingest raw data into the MongoDB data warehouse.\nGathering raw data into the data warehouse\nZenML orchestrates the data collection pipeline. Thus, leveraging ZenML, the data collection \npipeline can be run manually, scheduled, or triggered by specific events. Here, we will show you \nhow to run it manually, while we will discuss the other scenarios in Chapter 11 when digging \ndeeper into MLOps.\n",
      "content_length": 1620,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "Data Engineering\n90\nWe configured a different pipeline run for each author. We provided a ZenML configuration file for \nPaul Iusztin’s or Maxime Labonne’s data. To call the data collection pipeline to collect Maxime’s \ndata, for example, you can run the following CLI command:\npoetry poe run-digital-data-etl-maxime\nThat will call the pipeline with the following ZenML YAML configuration file:\nparameters:\n  user_full_name: Maxime Labonne # [First Name(s)] [Last Name]\n  links:\n    # Personal Blog\n    - https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.\nhtml\n    - https://mlabonne.github.io/blog/posts/2024-07-15_The_Rise_of_\nAgentic_Data_Generation.html\n    # Substack\n    - https://maximelabonne.substack.com/p/uncensor-any-llm-with-\nabliteration-d30148b7d43e\n    - https://maximelabonne.substack.com/p/create-mixtures-of-experts-\nwith-mergekit-11b318c99562\n    - https://maximelabonne.substack.com/p/merge-large-language-models-\nwith-mergekit-2118fb392b54\n    … # More Substack links\nIn Figure 3.3 earlier, we saw the pipeline’s run DAG and details in ZenML’s dashboard. Meanwhile, \nFigure 3.5 shows the user output artifact generated by this data collection pipeline. You can in-\nspect the query user_full_name and the retrieved user from the MongoDB database, for which \nwe collected the links in this specific run.\n",
      "content_length": 1339,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "Chapter 3\n91\nFigure 3.5: Example of the user output artifact after running the data collection pipeline using \nMaxime’s configuration file\nAlso, in Figure 3.6, you can observe the crawled_links output artifact, which lists all the domains \nfrom which we collected data, the total number of links crawled for each domain, and the number \nof successfully collected links. \n",
      "content_length": 371,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "Data Engineering\n92\nWe want to highlight again the power of these artifacts, as they trace each pipeline’s results and \nmetadata, making it extremely easy to monitor and debug each pipeline run individually.\nFigure 3.6: Example of the crawled_links output artifact after running the data collection \npipeline using Maxime’s configuration file\nNow, we can download the crawled_links artifact anywhere in our code by running the following \ncode, where the ID of the artifact can be found in ZenML and is unique for every artifact version:\nfrom zenml.client import Client\nartifact = Client().get_artifact_version('8349ce09-0693-4e28-8fa2-\n20f82c76ddec')\nloaded_artifact = artifact.load()\n",
      "content_length": 685,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "Chapter 3\n93\nFor example, we can easily run the same data collection pipeline but with Paul Iusztin’s YAML \nconfiguration, listed below:\nparameters:\n  user_full_name: Paul Iusztin # [First Name(s)] [Last Name]\n  links:\n    # Medium\n    - https://medium.com/decodingml/an-end-to-end-framework-for-\nproduction-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f\n    - https://medium.com/decodingml/a-real-time-retrieval-system-for-rag-\non-social-media-data-9cc01d50a2a0\n    - https://medium.com/decodingml/sota-python-streaming-pipelines-for-\nfine-tuning-llms-and-rag-in-real-time-82eb07795b87\n    … # More Medium links\n    # Substack\n    - https://decodingml.substack.com/p/real-time-feature-pipelines-\nwith?r=1ttoeh\n    - https://decodingml.substack.com/p/building-ml-systems-the-right-\nway?r=1ttoeh\n    - https://decodingml.substack.com/p/reduce-your-pytorchs-code-\nlatency?r=1ttoeh\n    … # More Substack links\nTo run the pipeline using Paul’s configuration, we call the following poe command:\npoetry poe run-digital-data-etl-paul\nThat, under the hood, calls the following CLI command that references Paul’s config file:\npoetry run python -m tools.run --run-etl --no-cache --etl-config-filename \ndigital_data_etl_paul_iusztin.yaml\nYou can find all the configs in the repository in the configs/ directory. Also, using poe, we con-\nfigured a command that calls the data collection pipeline for all the supported authors:\npoetry poe run-digital-data-etl\nWe can easily query the MongoDB data warehouse using our ODM classes. For example, let’s \nquery all the articles collected for Paul Iusztin:\nfrom llm_engineering.domain.documents import ArticleDocument, UserDocument\n",
      "content_length": 1673,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "Data Engineering\n94\nuser = UserDocument.get_or_create(first_name=\"Paul\", last_name=\"Iusztin\")\narticles = ArticleDocument.bulk_find(author_id=str(user.id))\nprint(f\"User ID: {user.id}\")\nprint(f\"User name: {user.first_name} {user.last_name}\")\nprint(f\"Number of articles: {len(articles)}\")\nprint(\"First article link:\", articles[0].link)\nThe output of the code from above is:\nUser ID: 900fec95-d621-4315-84c6-52e5229e0b96\nUser name: Paul Iusztin\nNumber of articles: 50\nFirst article link: https://medium.com/decodingml/an-end-to-end-framework-\nfor-production-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f\nWith only two lines of code, we can query and filter our MongoDB data warehouse using any \nODM defined within our project.\nAlso, to ensure that your data collection pipeline works as expected, you can search your MongoDB \ncollections using your IDE’s MongoDB plugin, which you must install separately. For example, \nyou can use this plugin for VSCode: https://www.mongodb.com/products/tools/vs-code. For \nother IDEs, you can use similar plugins or external NoSQL visualization tools. After connecting \nto the MongoDB visualization tool, you can connect to our local database using the following \nURI: mongodb://llm_engineering:llm_engineering@127.0.0.1:27017. For a cloud MongoDB \ncluster, you must change the URI, which we will explore in Chapter 11.\nAnd just like that, you’ve learned how to run the data collection pipeline with different ZenML \nconfigs and how to visualize the output artifacts of each run. We also looked at how to query the \ndata warehouse for a particular data category and author. Thus, we’ve finalized our data engi-\nneering chapter and can move to the conclusion.\nTroubleshooting\nThe raw data stored in the MongoDB database is central to all future steps. Thus, if you haven’t \nsuccessfully run the code from this chapter due to any issues with the crawlers, this section pro-\nvides solutions for fixing potential issues to allow you to move forward.\n",
      "content_length": 1989,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "Chapter 3\n95\nSelenium issues\nIt is a well-known issue that running Selenium can cause problems due to issues with the \nbrowser driver, such as the ChromeDriver. Thus, if the crawlers that use Selenium, such as the \nMediumCrawler, fail due to problems with your ChromeDriver, you can easily bypass this by \ncommenting out the Medium links added to the data collection YAML configs. To do so, go to \nthe configs/ directory and find all the YAML files that start with digital_data_etl_*, such as \ndigital_data_etl_maxime_labonne.yaml. Open them and comment on all the Medium-related \nURLs, as illustrated in Figure 3.7. You can leave out the Substack or personal blog URLs as these \nuse the CustomArticleCrawler, which is not dependent on Selenium.\nFigure 3.7: Fix Selenium issues when crawling raw data\nImport our backed-up data\nIf nothing works, there is the possibility of populating the MongoDB database with your backed-\nup data saved under the data/data_warehouse_raw_data directory. This will allow you to \nproceed to the fine-tuning and inference sections without running the data collection ETL code. \nTo import all the data within this directory, run:\npoetry poe run-import-data-warehouse-from-json\n",
      "content_length": 1206,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "Data Engineering\n96\nAfter running the CLI command from above, you will have a one-to-one replica of the dataset \nwe used while developing the code. To ensure the import is completed successfully, you should \nhave 88 articles and 3 users in your MongoDB database.\nSummary\nIn this chapter, we’ve learned how to design and build the data collection pipeline for the LLM \nTwin use case. Instead of relying on static datasets, we collected our custom data to mimic re-\nal-world situations, preparing us for real-world challenges in building AI systems.\nFirst, we examined the architecture of LLM Twin’s data collection pipeline, which functions \nas an ETL process. Next, we started digging into the pipeline implementation. We began by \nunderstanding how we can orchestrate the pipeline using ZenML. Then, we looked into the \ncrawler implementation. We learned how to crawl data in three ways: using CLI commands in \nsubprocesses or using utility functions from LangChain or Selenium to build custom logic that \nprogrammatically manipulates the browser. Finally, we looked into how to build our own ODM \nclass, which we used to define our document class hierarchy, which contains entities such as \narticles, posts, and repositories.\nAt the end of the chapter, we learned how to run ZenML pipelines with different YAML configura-\ntion files and explore the results in the dashboard. We also saw how to interact with the MongoDB \ndata warehouse through the ODM classes.\nIn the next chapter, we will cover the key steps of the RAG feature pipeline, including chunking \nand embedding documents, ingesting these documents into a vector DB, and applying pre-re-\ntrieval optimizations to improve performance. We will also set up the necessary infrastructure \nprogrammatically using Pulumi and conclude by deploying the RAG ingestion pipeline to AWS.\nReferences\n•\nBreuss, M. (2023, July 26). Beautiful Soup: Build a Web Scraper With Python. https://\nrealpython.com/beautiful-soup-web-scraper-python/\n•\nDavid, D. (2024, July 8). Guide to Web Scraping with Selenium in 2024. Bright Data. https://\nbrightdata.com/blog/how-tos/using-selenium-for-web-scraping\n•\nHjelle, G. A. (2023, October 21). Python 3.12 Preview: Static Typing Improvements. https://\nrealpython.com/python312-typing/\n• \nORM Quick Start — SQLAlchemy 2.0 documentation. (n.d.). https://docs.sqlalchemy.\norg/en/20/orm/quickstart.html\n",
      "content_length": 2383,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "Chapter 3\n97\n•\nRamos, L. P. (2023, August 4). Python and MongoDB: Connecting to NoSQL Databases. \nhttps://realpython.com/introduction-to-mongodb-and-python/\n•\nRefactoring.Guru. (2024, January 1). Builder. https://refactoring.guru/design-\npatterns/builder\n• \nWhat is ETL? A complete guide. (n.d.). Qlik. https://www.qlik.com/us/etl\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 474,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "4\nRAG Feature Pipeline\nRetrieval-augmented generation (RAG) is fundamental in most generative AI applications. RAG’s \ncore responsibility is to inject custom data into the large language model (LLM) to perform a \ngiven action (e.g., summarize, reformulate, and extract the injected data). You often want to use \nthe LLM on data it wasn’t trained on (e.g., private or new data). As fine-tuning an LLM is a highly \ncostly operation, RAG is a compelling strategy that bypasses the need for constant fine-tuning \nto access that new data.\nWe will start this chapter with a theoretical part that focuses on the fundamentals of RAG and \nhow it works. We will then walk you through all the components of a naïve RAG system: chunk-\ning, embedding, and vector DBs. Ultimately, we will present various optimizations used for an \nadvanced RAG system. Then, we will continue exploring LLM Twin’s RAG feature pipeline archi-\ntecture. At this step, we will apply all the theoretical aspects we discussed at the beginning of the \nchapter. Finally, we will go through a practical example by implementing the LLM Twin’s RAG \nfeature pipeline based on the system design described throughout the book.\nThe main sections of this chapter are:\n•\nUnderstanding RAG\n•\nAn overview of advanced RAG\n•\nExploring the LLM Twin’s RAG feature pipeline architecture\n•\nImplementing the LLM Twin’s RAG feature pipeline\nBy the end of this chapter, you will have a clear and comprehensive understanding of what RAG \nis and how it is applied to our LLM Twin use case.\n",
      "content_length": 1529,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "RAG Feature Pipeline\n100\nUnderstanding RAG\nRAG enhances the accuracy and reliability of generative AI models with information fetched from \nexternal sources. It is a technique complementary to the internal knowledge of the LLMs. Before \ngoing into the details, let’s understand what RAG stands for:\n• \nRetrieval: Search for relevant data\n• \nAugmented: Add the data as context to the prompt\n• \nGeneration: Use the augmented prompt with an LLM for generation\nAny LLM is bound to understand the data it was trained on, sometimes called parameterized \nknowledge. Thus, even if the LLM can perfectly answer what happened in the past, it won’t have \naccess to the newest data or any other external sources on which it wasn’t trained.\nLet’s take the most powerful model from OpenAI as an example, which, in the summer of 2024, is \nGPT-4o. The model is trained on data up to October 2023. Thus, if we ask what happened during \nthe 2020 pandemic, it can be answered perfectly due to its parametrized knowledge. However, \nit will not know the answer if we ask about the 2024 European Football Championship results \ndue to its bounded parametrized knowledge. Another scenario is that it will start confidently \nhallucinating and provide a faulty answer.\nRAG overcomes these two limitations of LLMs. It provides access to external or latest data and \nprevents hallucinations, enhancing generative AI models’ accuracy and reliability.\nWhy use RAG?\nWe briefly explained the importance of using RAG in generative AI applications earlier. Now, we \nwill dig deeper into the “why,” following which we will focus on what a naïve RAG framework \nlooks like.\nFor now, to get an intuition about RAG, you have to know that when using RAG, we inject the \nnecessary information into the prompt to answer the initial user question. After that, we pass \nthe augmented prompt to the LLM for the final answer. Now, the LLM will use the additional \ncontext to answer the user question.\nThere are two fundamental problems that RAG solves:\n• \nHallucinations\n• \nOld or private information\n",
      "content_length": 2055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "Chapter 4\n101\nHallucinations\nIf a chatbot without RAG is asked a question about something it wasn’t trained on, there is a high \nchance that it will give you a confident answer about something that isn’t true. Let’s take the 2024 \nEuropean Football Championship as an example. If the model is trained up to October 2023 and \nwe ask it something about the tournament, it will most likely come up with a random answer \nthat is hard to differentiate between reality and truth. Even if the LLM doesn’t hallucinate all the \ntime, it raises concerns about the trustworthiness of its answers. Thus, we must ask ourselves: \n“When can we trust the LLM’s answers?” and “How can we evaluate if the answers are correct?”. \nBy introducing RAG, we enforce the LLM to always answer solely based on the introduced con-\ntext. The LLM will act as the reasoning engine, while the additional information added through \nRAG will act as the single source of truth for the generated answer. By doing so, we can quickly \nevaluate if the LLM’s answer is based on the external data or not.\nOld information\nAny LLM is trained or fine-tuned on a subset of the total world knowledge dataset. This is due \nto three main issues:\n•\nPrivate data: You cannot train your model on data you don’t own or have the right to use.\n•\nNew data: New data is generated every second. Thus, you would have to constantly train \nyour LLM to keep up.\n•\nCosts: Training or fine-tuning an LLM is an extremely costly operation. Hence, it is not \nfeasible to do it on an hourly or daily basis.\nRAG solves these issues, as you no longer have to constantly fine-tune your LLM on new data (or \neven private data). Directly injecting the necessary data to respond to user questions into the \nprompts that are fed to the LLM is enough to generate correct and valuable answers.\nTo conclude, RAG is key for a robust and flexible generative AI system. But how do we inject the \nright data into the prompt based on the user’s questions? We will dig into the technical aspects \nof RAG in the next sections.\nThe vanilla RAG framework\nEvery RAG system is similar at its roots. We will first focus on understanding RAG in its simplest \nform. Later, we will gradually introduce more advanced RAG techniques to improve the system’s \naccuracy. Note that we will use vanilla and naive RAG interchangeably to avoid repetition. \n",
      "content_length": 2356,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "RAG Feature Pipeline\n102\nA RAG system is composed of three main modules independent of each other:\n•\nIngestion pipeline: A batch or streaming pipeline used to populate the vector DB\n•\nRetrieval pipeline: A module that queries the vector DB and retrieves relevant entries to \nthe user’s input\n•\nGeneration pipeline: The layer that uses the retrieved data to augment the prompt and \nan LLM to generate answers\nAs these three components are classes or services of their own, we will dig into each separately. \nBut for now, let’s try to answer the question “How are these three modules connected?”. Here is \na very simplistic overview:\n1.\nOn the backend side, the ingestion pipeline runs either on a schedule or constantly to \npopulate the vector DB with external data.\n2.\nOn the client side, the user asks a question.\n3.\nThe question is passed to the retrieval module, which preprocesses the user’s input and \nqueries the vector DB.\n4.\nThe generation pipelines use a prompt template, user input, and retrieved context to \ncreate the prompt.\n5.\nThe prompt is passed to an LLM to generate the answer.\n6.\nThe answer is shown to the user.\n",
      "content_length": 1132,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "Chapter 4\n103\nFigure 4.1: Vanilla RAG architecture\nYou must implement RAG in your generative AI application when you need access to any type of \nexternal information. For example, when implementing a financial assistant, you most likely need \naccess to the latest news, reports, and prices before providing valuable answers. Or, if you build \na traveling recommender, you must retrieve and parse a list of potential attractions, restaurants, \nand activities. At training time, LLMs don’t have access to your specific data, so you will often \nhave to implement a RAG strategy in your generative AI project. Now, let’s dig into the ingestion, \nretrieval, and generation pipelines.\n",
      "content_length": 679,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "RAG Feature Pipeline\n104\nIngestion pipeline\nThe RAG ingestion pipeline extracts raw documents from various data sources (e.g., data ware-\nhouse, data lake, web pages, etc.). Then, it cleans, chunks (splits into smaller sections), and em-\nbeds the documents. Ultimately, it loads the embedded chunks into a vector DB (or other similar \nvector storage).\nThus, the RAG ingestion pipeline is split into the following:\n• \nThe data extraction module gathers all the necessary data from various sources such as \nDBs, APIs, or web pages. This module is highly dependent on your data. It can be as easy \nas querying your data warehouse or something more complex such as crawling Wikipedia.\n• \nA cleaning layer standardizes and removes unwanted characters from the extracted data. \nFor example, you must remove all invalid characters from your input text, such as non-AS-\nCII and bold and italic characters. Another popular cleaning strategy is to replace URLs \nwith placeholders. However, your cleaning strategy will vary depending on your data \nsource and embedding model.\n• \nThe chunking module splits the cleaned documents into smaller ones. As we want to \npass the document’s content to an embedding model, this is necessary to ensure it doesn’t \nexceed the model’s input maximum size. Also, chunking is required to separate specific \nregions that are semantically related. For example, when chunking a book’s chapter, the \nmost optimal way is to group similar paragraphs into the same section or chunk. By doing \nso, at the retrieval time, you will add only the essential data to the prompt.\n• \nThe embedding component uses an embedding model to take the chunk’s content (text, \nimages, audio, etc.) and project it into a dense vector packed with semantic value—more \non embeddings in the What are embeddings? section below.\n• \nThe loading module takes the embedded chunks along with a metadata document. The \nmetadata will contain essential information such as the embedded content, the URL to \nthe source of the chunk, and when the content was published on the web. The embed-\nding is used as an index to query similar chunks, while the metadata is used to access the \ninformation added to augment the prompt.\nAt this point, we have a RAG ingestion pipeline that takes raw documents as input, processes them, \nand populates a vector DB. The next step is to retrieve relevant data from the vector store correctly.\n",
      "content_length": 2411,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "Chapter 4\n105\nRetrieval pipeline\nThe retrieval components take the user’s input (text, image, audio, etc.), embed it, and query the \nvector DB for similar vectors to the user’s input.\nThe primary function of the retrieval step is to project the user’s input into the same vector space \nas the embeddings used as an index in the vector DB. This allows us to find the top K’s most sim-\nilar entries by comparing the embeddings from the vector storage with the user’s input vector. \nThese entries then serve as content to augment the prompt that is passed to the LLM to generate \nthe answer.\nYou must use a distance metric to compare two vectors, such as the Euclidean or Manhattan \ndistance. But the most popular one is the cosine distance, which is equal to 1 minus the cosine of \nthe angle between two vectors, as follows:\n𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶\n𝐴𝐴𝐴\n \nIt ranges from -1 to 1, with a value of -1 when vectors A and B are in opposite directions, 0 if they \nare orthogonal, and 1 if they point in the same direction.\nMost of the time, the cosine distance works well in non-linear complex vector spaces. However, \nit is essential to notice that choosing the proper distance between two vectors depends on your \ndata and the embedding model you use.\nOne critical factor to highlight is that the user’s input and embeddings must be in the same vec-\ntor space. Otherwise, you cannot compute the distance between them. To do so, it is essential to \npreprocess the user input in the same way you processed the raw documents in the RAG ingestion \npipeline. This means you must clean, chunk (if necessary), and embed the user’s input using the \nsame functions, models, and hyperparameters. This is similar to how you have to preprocess the \ndata into features in the same way between training and inference; otherwise, the inference will \nyield inaccurate results—a phenomenon also known as the training-serving skew.\nGeneration pipeline\nThe last step of the RAG system is to take the user’s input, retrieve data, pass it to an LLM, and \ngenerate a valuable answer.\n",
      "content_length": 2082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "RAG Feature Pipeline\n106\nThe final prompt results from a system and prompt template populated with the user’s query and \nretrieved context. You might have a single prompt template or multiple prompt templates, depend-\ning on your application. Usually, all the prompt engineering is done at the prompt template level.\nBelow, you can see a dummy example of what a generic system and prompt template look like \nand how they are used together with the retrieval logic and the LLM to generate the final answer:\nsystem_template = \"\"\"\nYou are a helpful assistant who answers all the user's questions politely.\n\"\"\"\nprompt_template = \"\"\"\nAnswer the user's question using only the provided context. If you cannot \nanswer using the context, respond with \"I don't know.\"\nContext: {context}\nUser question: {user_question}\n\"\"\"\nuser_question = \"<your_question>\"\nretrieved_context = retrieve(user_question)\nprompt = f\"{system_template}\\n\"\nprompt += prompt_template.format(context=retrieved_context, user_\nquestion=user_question)\nanswer  = llm(prompt)\nAs the prompt templates evolve, each change should be tracked and versioned using machine \nlearning operations (MLOps) best practices. Thus, during training or inference time, you always \nknow that a given answer was generated by a specific version of the LLM and prompt template(s). \nYou can do this through Git, store the prompt templates in a DB, or use specific prompt manage-\nment tools such as LangFuse.\nAs we’ve seen in the retrieval pipeline, some critical aspects that directly impact the accuracy of \nyour RAG system are the embeddings of the external data, usually stored in vector DBs, the em-\nbedding of the user’s query, and how we can find similarities between the two using functions \nsuch as the cosine distance. To better understand this part of the RAG algorithm, let’s zoom in \non what embeddings are and how they are computed.\n",
      "content_length": 1883,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "Chapter 4\n107\nWhat are embeddings?\nImagine you’re trying to teach a computer to understand the world. Embeddings are like a par-\nticular translator that turns these things into a numerical code. This code isn’t random, though, \nbecause similar words or items end up with codes that are close to each other. It’s like a map \nwhere words with similar meanings are clustered together.\nWith that in mind, a more theoretical definition is that embeddings are dense numerical repre-\nsentations of objects encoded as vectors in a continuous vector space, such as words, images, or \nitems in a recommendation system. This transformation helps capture the semantic meaning \nand relationships between the objects. For instance, in natural language processing (NLP), em-\nbeddings translate words into vectors where semantically similar words are positioned closely \ntogether in the vector space.\nFigure 4.2: What are embeddings?\nA popular method is visualizing the embeddings to understand and evaluate their geometrical \nrelationship. As the embeddings often have more than 2 or 3 dimensions, usually between 64 \nand 2048, you must project them again to 2D or 3D.\n",
      "content_length": 1154,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "RAG Feature Pipeline\n108\nFor example, you can use UMAP (https://umap-learn.readthedocs.io/en/latest/index.\nhtml), a dimensionality reduction method well known for keeping the geometrical properties \nbetween the points when projecting the embeddings to 2D or 3D. Another popular algorithm for \ndimensionality reduction when visualizing vectors is t-SNE (https://scikit-learn.org/stable/\nmodules/generated/sklearn.manifold.TSNE.html). However, compared to UMAP, it is more \nstochastic and doesn’t preserve the topological relationships between the points.\nFigure 4.3: Visualize embeddings using UMAP (Source: UMAP’s documentation)\n A dimensionality reduction algorithm, such as PCA, UMAP, and t-SNE, is a mathe-\nmatical technique used to reduce the number of input variables or features in a data-\nset while preserving the data’s essential patterns, structure, and relationships. The \ngoal is to transform high-dimensional data into a lower-dimensional form, making \nit easier to visualize, interpret, and process while minimizing the loss of important \ninformation. These methods help to address the “curse of dimensionality,” improve \ncomputational efficiency, and often enhance the performance of ML algorithms.\n",
      "content_length": 1213,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "Chapter 4\n109\nWhy embeddings are so powerful\nFirstly, ML models work only with numerical values. This is not a problem when working with \ntabular data, as the data is often in numerical form or can easily be processed into numbers. \nEmbeddings come in handy when we want to feed words, images, or audio data into models.\nFor instance, when working with transformer models, you tokenize all your text input, where \neach token has an embedding associated with it. The beauty of this process lies in its simplicity; \nthe input to the transformer is a sequence of embeddings, which can be easily and confidently \ninterpreted by the dense layers of the neural network.\nBased on this example, you can use embeddings to encode any categorical variable and feed it to \nan ML model. But why not use other simple methods, such as one-hot encoding? When working \nwith categorical variables with high cardinality, such as language vocabularies, you will suffer \nfrom the curse of dimensionality when using other classical methods. For example, if your vocab-\nulary has 10,000 tokens, then only one token will have a length of 10,000 after applying one-hot \nencoding. If the input sequence has N tokens, that will become N * 10,000 input parameters. If \nN >= 100, often, when inputting text, the input is too large to be usable. Another issue with other \nclassical methods that don’t suffer from the curse of dimensionality, such as hashing, is that you \nlose the semantic relationships between the vectors.\n",
      "content_length": 1495,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "RAG Feature Pipeline\n110\nSecondly, embedding your input reduces the size of its dimension and condenses all of its se-\nmantic meaning into a dense vector. This is an extremely popular technique when working with \nimages, where a CNN encoder module maps the high-dimensional meaning into an embedding, \nwhich is later processed by a CNN decoder that performs the classification or regression steps.\nThe following image shows a typical CNN layout. Imagine tiny squares within each layer. Those \nare the “receptive fields.” Each square feeds information to a single neuron in the previous layer. \nAs you move through the network, two key things are happening:\n•\nShrinking the picture: Special “subsampling” operations make the layers smaller, fo-\ncusing on essential details.\n•\nLearning features: “Convolution” operations, on the other hand, actually increase the \nlayer size as the network learns more complex features from the image.\nOne-hot encoding is a technique that converts categorical variables into a binary \nmatrix representation. Each category is represented as a unique binary vector. For \neach categorical variable, a binary vector is created with a length equal to the number \nof unique categories, where all values are zero except for the index corresponding to \nthe specific category, which is set to one. The method preserves all information about \nthe categories. It is simple and interpretable. However, a significant disadvantage is \nthat it can lead to a high-dimensional feature space if the categorical variable has \nmany unique values, making the method impractical.\nFeature hashing, also known as hashing encoding or the “hash trick,” is a technique \nused to convert categorical variables into numerical features by applying a hash \nfunction to the category values. Compared to one-hot encoding, the method is not \nbound to the number of unique categories, but it reduces the dimensionality of the \nfeature space by mapping categories into a fixed number of bins or buckets. Thus, it \nreduces the dimensionality of the feature space, which is particularly useful when \ndealing with high-cardinality categorical variables. This makes it efficient in terms of \nmemory usage and computational time. However, there is a risk of collisions, where \ndifferent categories might map to the same bin, leading to a loss of information. The \nmapping makes the method uninterpretable. Also, it is difficult to understand the \nrelationship between the original categories and the hashed features.\nEmbeddings help us encode categorical variables while controlling the output vec-\ntor’s dimension. They also use ingenious ways to condense information into a lower \ndimension space than naive hashing tricks.\n",
      "content_length": 2715,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "Chapter 4\n111\nFinally, a fully connected layer at the end takes all this processed information and transforms it \ninto the final vector embedding, a numerical image representation.\nFigure 4.4: Creating embeddings from an image using a CNN (Image source)\nHow are embeddings created?\nEmbeddings are created by deep learning models that understand the context and semantics of \nyour input and project it into a continuous vector space.\nVarious deep learning models can be used to create embeddings, varying by the data input type. \nThus, it is fundamental to understand your data and what you need from it before picking an \nembedding model.\nFor example, when working with text data, one of the early methods used to create embeddings \nfor your vocabulary is Word2Vec and GloVe. These are still popular methods used today for sim-\npler applications.\nAnother popular method is to use encoder-only transformers, such as BERT, or other methods \nfrom its family, such as RoBERTa. These models leverage the encoder of the transformer architec-\nture to smartly project your input into a dense vector space that can later be used as embeddings.\nTo quickly compute the embeddings in Python, you can conveniently leverage the Sentence \nTransformers Python package (also available in Hugging Face’s transformer package). This tool \nprovides a user-friendly interface, making the embedding process straightforward and efficient.\nThe preceding image is sourced from Wikimedia Commons (https://commons.\nwikimedia.org/wiki/File:Typical_cnn.png) and licensed under the Creative \nCommons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0: https://\ncreativecommons.org/licenses/by-sa/4.0/deed.en).\n",
      "content_length": 1691,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "RAG Feature Pipeline\n112\nIn the code snippet below, you can see how we loaded a model from SentenceTransformer, comput-\ned the embeddings for three sentences, and, ultimately, computed the cosine similarity between \nthem. The similarity between one sentence and itself is always 1. Also, the similarity between \nthe first and second sentences is approximately 0, as the sentences have nothing in common. In \ncontrast, the value between the first and third one is higher as there is some overlapping context:\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\nsentences = [\n\"The dog sits outside waiting for a treat.\",\n\"I am going swimming.\",\n\"The dog is swimming.\"\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# Output: [3, 384]\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# Output:\n# tensor([[ 1.0000, -0.0389, 0.2692],\n# [-0.0389, 1.0000, 0.3837],\n# [ 0.2692, 0.3837, 1.0000]])\n#\n# similarities[0, 0] = The similarity between the first sentence and \nitself.\n# similarities[0, 1] = The similarity between the first and second \nsentence.\n# similarities[2, 1] = The similarity between the third and second \nsentence.\nThe source code for the preceding snippet can be found at https://github.com/PacktPublishing/\nLLM-Engineering/blob/main/code_snippets/08_text_embeddings.py.\n",
      "content_length": 1374,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "Chapter 4\n113\nThe best-performing embedding model can change with time and your specific use case. You can \nfind particular models on the Massive Text Embedding Benchmark (MTEB) on Hugging Face. \nDepending on your needs, you can consider the best-performing model, the one with the best \naccuracy, or the one with the smallest memory footprint. This decision is solely based on your \nrequirements (e.g., accuracy and hardware). However, Hugging Face and SentenceTransformer \nmake switching between different models straightforward. Thus, you can always experiment \nwith various options.\nWhen working with images, you can embed them using convolutional neural networks (CNNs). \nPopular CNN networks are based on the ResNet architecture. However, we can’t directly use im-\nage embedding techniques for audio recordings. Instead, we can create a visual representation \nof the audio, such as a spectrogram, and then apply image embedding models to those visuals. \nThis allows us to capture the essence of images and sounds in a way computers can understand.\nBy leveraging models like CLIP, you can practically embed a piece of text and an image in the \nsame vector space. This allows you to find similar images using a sentence as input, or the other \nway around, demonstrating the practicality of CLIP.\nIn the following code snippet, we use CLIP to encode a crazy cat image and three sentences. \nUltimately, we use cosine similarity to compute the resemblance between the picture and the \nsentences:\nfrom io import BytesIO\nimport requests\nfrom PIL import Image\nfrom sentence_transformers import SentenceTransformer\nresponse = requests.get(\n\"https://github.com/PacktPublishing/LLM-Engineering/blob/main/images/\ncrazy_cat.jpg?raw=true\"\n)\nThe examples in the embeddings section can be run within the virtual environment \nused across the book, as it contains all the required dependencies.\n",
      "content_length": 1883,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "RAG Feature Pipeline\n114\nimage = Image.open(BytesIO(response.content))\nmodel = SentenceTransformer(\"clip-ViT-B-32\")\nimg_emb = model.encode(image)\ntext_emb = model.encode(\n[\"A crazy cat smiling.\",\n\"A white and brown cat with a yellow bandana.\",\n\"A man eating in the garden.\"]\n)\nprint(text_emb.shape) # noqa\n# Output: (3, 512)\nsimilarity_scores = model.similarity(img_emb, text_emb)\nprint(similarity_scores) # noqa\n# Output: tensor([[0.3068, 0.3300, 0.1719]])\nThe source code can be found at https://github.com/PacktPublishing/LLM-Engineering/\nblob/main/code_snippets/08_text_image_embeddings.py.\nHere, we provided a small introduction to how embeddings can be computed. The realm of specific \nimplementations is vast, but what is important to know is that embeddings can be computed for \nmost digital data categories, such as words, sentences, documents, images, videos, and graphs.\nIt’s crucial to grasp that you must use specialized models when you need to compute the distance \nbetween two different data categories, such as the distance between the vector of a sentence and \nof an image. These models are designed to project both data types into the same vector space, \nsuch as CLIP, ensuring accurate distance computation.\nApplications of embeddings\nDue to the generative AI revolution, which uses RAG, embeddings have become extremely popu-\nlar in information retrieval tasks, such as semantic search for text, code, images, and audio, and \nlong-term memory of agents. But before generative AI, embeddings were already heavily used in:\n•\nRepresenting categorical variables (e.g., vocabulary tokens) that are fed to an ML model\n•\nRecommender systems by encoding the users and items and finding their relationship\n•\nClustering and outlier detection\n•\nData visualization by using algorithms such as UMAP\n",
      "content_length": 1806,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "Chapter 4\n115\n•\nClassification by using the embeddings as features\n•\nZero-shot classification by comparing the embedding of each class and picking the most \nsimilar one\nThe last step to fully understanding how RAG works is to examine vector DBs and how they \nleverage embeddings to retrieve data.\nMore on vector DBs\nVector DBs are specialized DBs designed to efficiently store, index, and retrieve vector embed-\ndings. Traditional scalar-based DBs struggle with the complexity of vector data, making vector \nDBs crucial for tasks like real-time semantic search.\nWhile standalone vector indices like FAISS are effective for similarity search, they lack vector DBs’ \ncomprehensive data management capabilities. Vector DBs support CRUD operations, metadata \nfiltering, scalability, real-time updates, backups, ecosystem integration, and robust data security, \nmaking them more suited for production environments than standalone indices.\nHow does a vector DB work?\nThink of how you usually search a DB. You type in something specific, and the system spits out \nthe exact match. That’s how traditional DBs work. Vector DBs are different. Instead of perfect \nmatches, we look for the closest neighbors of the query vector. Under the hood, a vector DB uses \napproximate nearest neighbor (ANN) algorithms to find these close neighbors.\nWhile ANN algorithms don’t return the top matches for a given search, standard nearest neigh-\nbor algorithms are too slow to work in practice. Also, it is shown empirically that using only ap-\nproximations of the top matches for a given input query works well enough. Thus, the trade-off \nbetween accuracy and latency ultimately favors ANN algorithms.\nThis is a typical workflow of a vector DB:\n1. \nIndexing vectors: Vectors are indexed using data structures optimized for high-dimen-\nsional data. Common indexing techniques include hierarchical navigable small world\n(HNSW), random projection, product quantization (PQ), and locality-sensitive hashing\n(LSH).\n2.\nQuerying for similarity: During a search, the DB queries the indexed vectors to find those \nmost similar to the input vector. This process involves comparing vectors based on sim-\nilarity measures such as cosine similarity, Euclidean distance, or dot product. Each has \nunique advantages and is suitable for different use cases.\n",
      "content_length": 2320,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "RAG Feature Pipeline\n116\n3.\nPost-processing results: After identifying potential matches, the results undergo post-pro-\ncessing to refine accuracy. This step ensures that the most relevant vectors are returned \nto the user.\nVector DBs can filter results based on metadata before or after the vector search. Both approaches \nhave trade-offs in terms of performance and accuracy. The query also depends on the metadata \n(along with the vector index), so it contains a metadata index user for filtering operations.\nAlgorithms for creating the vector index\nVector DBs use various algorithms to create the vector index and manage searching data efficiently:\n•\nRandom projection: Random projection reduces the dimensionality of vectors by project-\ning them into a lower-dimensional space using a random matrix. This technique preserves \nthe relative distances between vectors, facilitating faster searches.\n•\nPQ: PQ compresses vectors by dividing them into smaller sub-vectors and then quantizing \nthese sub-vectors into representative codes. This reduces memory usage and speeds up \nsimilarity searches.\n•\nLSH: LSH maps similar vectors into buckets. This method enables fast approximate near-\nest neighbor searches by focusing on a subset of the data, reducing the computational \ncomplexity.\n•\nHNSW: HNSW constructs a multi-layer graph where each node represents a set of vectors. \nSimilar nodes are connected, allowing the algorithm to navigate the graph and find the \nnearest neighbors efficiently.\nThese algorithms enable vector DBs to efficiently handle complex and large-scale data, making \nthem a perfect fit for a variety of AI and ML applications.\nDB operations\nVector DBs also share common characteristics with standard DBs to ensure high performance, \nfault tolerance, and ease of management in production environments. Key operations include:\n•\nSharding and replication: Data is partitioned (sharded) across multiple nodes to ensure \nscalability and high availability. Data replication across nodes helps maintain data integ-\nrity and availability in case of node failures.\n",
      "content_length": 2080,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "Chapter 4\n117\n•\nMonitoring: Continuous monitoring of DB performance, including query latency and re-\nsource usage (RAM, CPU, disk), helps maintain optimal operations and identify potential \nissues before they impact the system.\n• \nAccess control: Implementing robust access control mechanisms ensures that only au-\nthorized users can access and modify data. This includes role-based access controls and \nother security protocols to protect sensitive information.\n• \nBackups: Regular DB backups are critical for disaster recovery. Automated backup pro-\ncesses ensure that data can be restored to a previous state in case of corruption or loss.\nAn overview of advanced RAG\nThe vanilla RAG framework we just presented doesn’t address many fundamental aspects that \nimpact the quality of the retrieval and answer generation, such as:\n• \nAre the retrieved documents relevant to the user’s question?\n•\nIs the retrieved context enough to answer the user’s question?\n•\nIs there any redundant information that only adds noise to the augmented prompt?\n•\nDoes the latency of the retrieval step match our requirements?\n• \nWhat do we do if we can’t generate a valid answer using the retrieved information?\nFrom the questions above, we can draw two conclusions. The first one is that we need a robust \nevaluation module for our RAG system that can quantify and measure the quality of the retrieved \ndata and generate answers relative to the user’s question. We will discuss this topic in more detail \nin Chapter 9. The second conclusion is that we must improve our RAG framework to address the \nretrieval limitations directly in the algorithm. These improvements are known as advanced RAG.\nThe vanilla RAG design can be optimized at three different stages:\n•\nPre-retrieval: This stage focuses on how to structure and preprocess your data for data \nindexing optimizations as well as query optimizations.\n•\nRetrieval: This stage revolves around improving the embedding models and metadata \nfiltering to improve the vector search step.\n",
      "content_length": 2019,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "RAG Feature Pipeline\n118\n• \nPost-retrieval: This stage mainly targets different ways to filter out noise from the retrieved \ndocuments and compress the prompt before feeding it to an LLM for answer generation.\nFigure 4.5: The three stages of advanced RAG\nThis section is not meant to be an exhaustive list of all the advanced RAG methods available. The \ngoal is to build an intuition about what can be optimized. We will use only examples based on \ntext data, but the principles of advanced RAG remain the same regardless of the data category. \nNow, let’s zoom in on all three components.\n",
      "content_length": 589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "Chapter 4\n119\nPre-retrieval\nThe pre-retrieval steps are performed in two different ways:\n•\nData indexing: It is part of the RAG ingestion pipeline. It is mainly implemented within \nthe cleaning or chunking modules to preprocess the data for better indexing.\n•\nQuery optimization: The algorithm is performed directly on the user’s query before em-\nbedding it and retrieving the chunks from the vector DB.\nAs we index our data using embeddings that semantically represent the content of a chunked \ndocument, most of the data indexing techniques focus on better preprocessing and structuring \nthe data to improve retrieval efficiency, such as:\n•\nSliding window: The sliding window technique introduces overlap between text chunks, \nensuring that important context near chunk boundaries is retained, which enhances re-\ntrieval accuracy. This is particularly beneficial in domains like legal documents, scientific \npapers, customer support logs, and medical records, where critical information often spans \nmultiple sections. The embedding is computed on the chunk along with the overlapping \nportion. Hence, the sliding window improves the system’s ability to retrieve relevant and \ncoherent information by maintaining context across boundaries.\n•\nEnhancing data granularity: This involves data cleaning techniques like removing irrel-\nevant details, verifying factual accuracy, and updating outdated information. A clean and \naccurate dataset allows for sharper retrieval.\n•\nMetadata: Adding metadata tags like dates, URLs, external IDs, or chapter markers helps \nfilter results efficiently during retrieval.\n•\nOptimizing index structures: It is based on different data index methods, such as various \nchunk sizes and multi-indexing strategies.\n•\nSmall-to-big: The algorithm decouples the chunks used for retrieval and the context used \nin the prompt for the final answer generation. The algorithm uses a small sequence of \ntext to compute the embedding while preserving the sequence itself and a wider window \naround it in the metadata. Thus, using smaller chunks enhances the retrieval’s accuracy, \nwhile the larger context adds more contextual information to the LLM. \n",
      "content_length": 2169,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "RAG Feature Pipeline\n120\nThe intuition behind this is that if we use the whole text for computing the embedding, we \nmight introduce too much noise, or the text could contain multiple topics, which results \nin a poor overall semantic representation of the embedding.\nFigure 4.6: Query routing\nOn the query optimization side, we can leverage techniques such as query routing, query rewriting, \nand query expansion to refine the retrieved information for the LLM further:\n•\nQuery routing: Based on the user’s input, we might have to interact with different cate-\ngories of data and query each category differently. Query rooting is used to decide what \naction to take based on the user’s input, similar to if/else statements. Still, the decisions \nare made solely using natural language instead of logical statements. \n",
      "content_length": 817,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "Chapter 4\n121\nAs illustrated in Figure 4.6, let’s assume that, based on the user’s input, to do RAG, we \ncan retrieve additional context from a vector DB using vector search queries, a standard \nSQL DB by translating the user query to an SQL command, or the internet by leveraging \nREST API calls. The query router can also detect whether a context is required, helping us \navoid making redundant calls to external data storage. Also, a query router can be used to \npick the best prompt template for a given input. For example, in the LLM Twin use case, \ndepending on whether the user wants an article paragraph, a post, or a code snippet, you \nneed different prompt templates to optimize the creation process. The routing usually \nuses an LLM to decide what route to take or embeddings by picking the path with the \nmost similar vectors. To summarize, query routing is identical to an if/else statement but \nmuch more versatile as it works directly with natural language.\n•\nQuery rewriting: Sometimes, the user’s initial query might not perfectly align with the \nway your data is structured. Query rewriting tackles this by reformulating the question \nto match the indexed information better. This can involve techniques like:\n• \nParaphrasing: Rephrasing the user’s query while preserving its meaning (e.g., \n“What are the causes of climate change?” could be rewritten as “Factors contrib-\nuting to global warming”).\n•\nSynonym substitution: Replacing less common words with synonyms to broaden \nthe search scope (e.g., “ joyful” could be rewritten as “happy”).\n•\nSub-queries: For longer queries, we can break them down into multiple shorter \nand more focused sub-queries. This can help the retrieval stage identify relevant \ndocuments more precisely.\n•\nHypothetical document embeddings (HyDE): This technique involves having an LLM \ncreate a hypothetical response to the query. Then, both the original query and the LLM’s \nresponse are fed into the retrieval stage.\n•\nQuery expansion: This approach aims to enrich the user’s question by adding additional \nterms or concepts, resulting in different perspectives of the same initial question. For \nexample, when searching for “disease,” you can leverage synonyms and related terms \nassociated with the original query words and also include “illnesses” or “ailments.”\n•\nSelf-query: The core idea is to map unstructured queries into structured ones. An LLM \nidentifies key entities, events, and relationships within the input text. These identities are \nused as filtering parameters to reduce the vector search space (e.g., identify cities within \nthe query, for example, “Paris,” and add it to your filter to reduce your vector search space).\n",
      "content_length": 2691,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "RAG Feature Pipeline\n122\nBoth data indexing and query optimization pre-retrieval optimization techniques depend highly \non your data type, structure, and source. Thus, as with any data processing pipeline, no method \nalways works, as every use case has its own particularities and gotchas. Optimizing your pre-re-\ntrieval RAG layer is experimental. Thus, what is essential is to try multiple methods (such as the \nones enumerated in this section), reiterate, and observe what works best.\nRetrieval\nThe retrieval step can be optimized in two fundamental ways:\n•\nImproving the embedding models used in the RAG ingestion pipeline to encode the \nchunked documents and, at inference time, transform the user’s input.\n•\nLeveraging the DB’s filter and search features. This step will be used solely at inference \ntime when you have to retrieve the most similar chunks based on user input.\nBoth strategies are aligned with our ultimate goal: to enhance the vector search step by leveraging \nthe semantic similarity between the query and the indexed data.\nWhen improving the embedding models, you usually have to fine-tune the pre-trained embedding \nmodels to tailor them to specific jargon and nuances of your domain, especially for areas with \nevolving terminology or rare terms.\nInstead of fine-tuning the embedding model, you can leverage instructor models (https://\nhuggingface.co/hkunlp/instructor-xl) to guide the embedding generation process with an \ninstruction/prompt aimed at your domain. Tailoring your embedding network to your data us-\ning such a model can be a good option, as fine-tuning a model consumes more computing and \nhuman resources.\nIn the code snippet below, you can see an example of an Instructor model that embeds article \ntitles about AI:\nfrom InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR(\"hkunlp/instructor-base\")\nsentence = \"RAG Fundamentals First\"\ninstruction = \"Represent the title of an article about AI:\"\n",
      "content_length": 1944,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "Chapter 4\n123\nembeddings = model.encode([[instruction, sentence]])\nprint(embeddings.shape) # noqa\n# Output: (1, 768)\nThe source code can be found at https://github.com/PacktPublishing/LLM-Engineering/\nblob/main/code_snippets/08_instructor_embeddings.py.\npython3 -m venv instructor_venv && source instructor_venv/bin/activate\npip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1\nOn the other side of the spectrum, here is how you can improve your retrieval by leveraging classic \nfilter and search DB features:\n•\nHybrid search: This is a vector and keyword-based search blend. Keyword-based search \nexcels at identifying documents containing specific keywords. When your task demands \npinpoint accuracy and the retrieved information must include exact keyword matches, \nhybrid search shines. Vector search, while powerful, can sometimes struggle with finding \nexact matches, but it excels at finding more general semantic similarities. You leverage \nboth keyword matching and semantic similarities by combining the two methods. You \nhave a parameter, usually called alpha, that controls the weight between the two methods. \nThe algorithm has two independent searches, which are later normalized and unified.\n•\nFiltered vector search: This type of search leverages the metadata index to filter for specific \nkeywords within the metadata. It differs from a hybrid search in that you retrieve the data \nonce using only the vector index and perform the filtering step before or after the vector \nsearch to reduce your search space.\nTo run the instructor code, you have to create a different virtual environment and \nactivate it:\nAnd install the required Python dependencies:\n",
      "content_length": 1685,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "RAG Feature Pipeline\n124\nIn practice, on the retrieval side, you usually start with filtered vector search or hybrid search, as \nthey are fairly quick to implement. This approach gives you the flexibility to adjust your strategy \nbased on performance. If the results are not as expected, you can always fine-tune your embed-\nding model.\nPost-retrieval\nThe post-retrieval optimizations are solely performed on the retrieved data to ensure that the \nLLM’s performance is not compromised by issues such as limited context windows or noisy data. \nThis is because the retrieved context can sometimes be too large or contain irrelevant information, \nboth of which can distract the LLM.\nTwo popular methods performed at the post-retrieval step are:\n•\nPrompt compression: Eliminate unnecessary details while keeping the essence of the data.\n•\nRe-ranking: Use a cross-encoder ML model to give a matching score between the user’s \ninput and every retrieved chunk. The retrieved items are sorted based on this score. Only \nthe top N results are kept as the most relevant. As you can see in Figure 4.7, this works \nbecause the re-ranking model can find more complex relationships between the user input \nand some content than a simple similarity search. However, we can’t apply this model at \nthe initial retrieval step because it is costly. That is why a popular strategy is to retrieve \nthe data using a similarity distance between the embeddings and refine the retrieved \ninformation using a re-raking model, as illustrated in Figure 4.8.\n",
      "content_length": 1530,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "Chapter 4\n125\nFigure 4.7: Bi-encoder (the standard embedding model) versus cross-encoder\nThe abovementioned techniques are far from an exhaustive list of all potential solutions. We \nused them as examples to get an intuition on what you can (and should) optimize at each step \nin your RAG workflow. The truth is that these techniques can vary tremendously by the type of \ndata you work with. \n",
      "content_length": 393,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "RAG Feature Pipeline\n126\nFor example, if you work with multi-modal data such as text and images, most of the techniques \nfrom earlier won’t work as they are designed for text only.\nFigure 4.8: The re-ranking algorithm\nTo summarize, the primary goal of these optimizations is to enhance the RAG algorithm at three key \nstages: pre-retrieval, retrieval, and post-retrieval. This involves preprocessing data for improved \nvector indexing, adjusting user queries for more accurate searches, enhancing the embedding \nmodel, utilizing classic filtering DB operations, and removing noisy data. By keeping these goals \nin mind, you can effectively optimize your RAG workflow for data processing and retrieval\n",
      "content_length": 701,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "Chapter 4\n127\nExploring the LLM Twin’s RAG feature pipeline \narchitecture\nNow that you have a strong intuition and understanding of RAG and its workings, we will con-\ntinue exploring our particular LLM Twin use case. The goal is to provide a hands-on end-to-end \nexample to solidify the theory presented in this chapter.\nAny RAG system is split into two independent components:\n•\nThe ingestion pipeline takes in raw data, cleans, chunks, embeds, and loads it into a \nvector DB.\n•\nThe inference pipeline queries the vector DB for relevant context and ultimately generates \nan answer by levering an LLM.\nIn this chapter, we will focus on implementing the RAG ingestion pipeline, and in Chapter 9, we \nwill continue developing the inference pipeline.\nWith that in mind, let’s have a quick refresher on the problem we are trying to solve and where \nwe get our raw data. Remember that we are building an end-to-end ML system. Thus, all the \ncomponents talk to each other through an interface (or a contract), and each pipeline has a sin-\ngle responsibility. In our case, we ingest raw documents, preprocess them, and load them into \na vector DB.\nThe problem we are solving\nAs presented in the previous chapter, this book aims to show you how to build a production-ready \nLLM Twin backed by an end-to-end ML system. In this chapter specifically, we want to design a \nRAG feature pipeline that takes raw social media data (e.g., articles, code repositories, and posts) \nfrom our MongoDB data warehouse. The text of the raw documents will be cleaned, chunked, \nembedded, and ultimately loaded to a feature store. As discussed in Chapter 1, we will implement \na logical feature store using ZenML artifacts and a Qdrant vector DB.\nAs we want to build a fully automated feature pipeline, we want to sync the data warehouse and \nlogical feature store. Remember that, at inference time, the context used to generate the answer \nis retrieved from the vector DB. Thus, the speed of synchronization between the data warehouse \nand the feature store will directly impact the accuracy of our RAG algorithm.\nAnother key consideration is how to automate the feature pipeline and integrate it with the rest \nof our ML system. Our goal is to minimize any desynchronization between the two data storages, \nas this could potentially compromise the integrity of our system.\n",
      "content_length": 2349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "RAG Feature Pipeline\n128\nTo conclude, we must design a feature pipeline that constantly syncs the data warehouse and \nlogical feature store while processing the data accordingly. Having the data in a feature store \nis critical for a production-ready ML system. The LLM Twin inference pipeline will query it for \nRAG, while the training pipeline will consume tracked and versioned fine-tuning datasets from it.\nThe feature store\nThe feature store will be the central access point for all the features used within the training and \ninference pipelines. The training pipeline will use the cleaned data from the feature store (stored \nas artifacts) to fine-tune LLMs. The inference pipeline will query the vector DB for chunked doc-\numents for RAG. That is why we are designing a feature pipeline and not only a RAG ingestion \npipeline. In practice, the feature pipeline contains multiple subcomponents, one of which is the \nRAG logic.\nRemember that the feature pipeline is mainly used as a mind map to navigate the complexity of \nML systems. It clearly states that it takes raw data as input and then outputs features and optional \nlabels, which are stored in the feature store. Thus, a good intuition is to consider that all the logic \nbetween the data warehouse and the feature store goes into the feature pipeline namespace, con-\nsisting of one or more sub-pipelines. For example, we will implement another pipeline that takes \nin cleaned data, processes it into instruct datasets, and stores it in artifacts; this also sits under \nthe feature pipeline umbrella as the artifacts are part of the logical feature store. Another example \nwould be implementing a data validation pipeline on top of the raw data or computed features.\nAnother important observation to make is that text data stored as strings are not considered \nfeatures if you follow the standard conventions. A feature is something that is fed directly into \nthe model. For example, we would have to tokenize the instruct datasets or chunked documents \nto be considered features. Why? Because the tokens are fed directly to the model and not the \nsentences as strings. Unfortunately, this makes the system more complex and unflexible. Thus, \nwe will do the tokenization at runtime. But this observation is important to understand as it’s \na clear example that you don’t have to be too rigid about the feature/training/inference (FTI) \narchitecture. You have to take it and adapt it to your own use case.\nWhere does the raw data come from?\nAs a quick reminder, all the raw documents are stored in a MongoDB data warehouse. The data \nwarehouse is populated by the data collection ETL pipeline presented in Chapter 3. The ETL pipe-\nline crawls various platforms such as Medium and Substack, standardizes the data, and loads it \ninto MongoDB. Check out Chapter 3 for more details on this topic.\n",
      "content_length": 2854,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "Chapter 4\n129\nDesigning the architecture of the RAG feature pipeline\nThe last step is to architect and go through the design of the RAG feature pipeline of the LLM \nTwin application. We will use a batch design scheduled to poll data from the MongoDB data \nwarehouse, process it, and load it to a Qdrant vector DB. The first question to ask ourselves is, \n“Why a batch pipeline?”\nBut before answering that, let’s quickly understand how a batch architecture works and behaves \nrelative to a streaming design.\nFigure 4.9: The architecture of the LLM Twin’s RAG feature pipeline\n",
      "content_length": 575,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "RAG Feature Pipeline\n130\nBatch pipelines\nA batch pipeline in data systems refers to a data processing method where data is collected, pro-\ncessed, and stored in predefined intervals and larger volumes, also known as “batches”. This ap-\nproach differs from real-time or streaming data processing, where data is processed continuously \nas it arrives. This is what happens in a batch pipeline:\n1.\nData collection: Data is collected from various sources and stored until sufficient amounts \nare accumulated for processing. This can include data from DBs, logs, files, and other \nsources.\n2.\nScheduled processing: Data processing is scheduled at regular intervals, for example, \nhourly or daily. During this time, the collected data is processed in bulk. This can involve \ndata cleansing, transformation, aggregation, and other operations.\n3.\nData loading: After processing, the data is loaded into the target system, such as a DB, data \nwarehouse, data lake, or feature store. This processed data is then available for analysis, \nquerying, or further processing.\nBatch pipelines are particularly useful when dealing with large volumes of data that do not require \nimmediate processing. They offer several advantages, including:\n•\nEfficiency: Batch processing can handle large volumes of data more efficiently than re-\nal-time processing, allowing for optimized resource allocation and parallel processing.\n•\nComplex processing: Batch pipelines can perform complex data transformations and \naggregations that might be too resource-intensive for real-time processing.\n•\nSimplicity: Batch processing systems’ architectures are often simpler than those of re-\nal-time systems, making them easier to implement and maintain.\nBatch versus streaming pipelines\nWhen implementing feature pipelines, you have two main design choices: batch and streaming. \nThus, it is worthwhile to see the difference between the two and understand why we chose a \nbatch architecture over a streaming one for our LLM Twin use case.\nYou can effortlessly write a dedicated chapter on streaming pipelines, which suggests its com-\nplexity over a batch design. However, as streaming architectures become increasingly popular, \none must have an intuition of how they work to choose the best option for your application.\n",
      "content_length": 2282,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "Chapter 4\n131\nThe core elements of streaming applications are a distributed event streaming platform such as \nApache Kafka or Redpanda to store events from multiple clients and a streaming engine such as \nApache Flink or Bytewax to process the events. To simplify your architecture, you can swap your \nevent streaming platform with queues, such as RabbitMQ, to store the events until processed. \nTable 4.1 compares batch and streaming pipelines based on multiple criteria such as processing \nschedule and complexity:\nAspect\nBatch pipeline\nStreaming pipeline\nProcessing schedule\nProcesses data at regular \nintervals (e.g., every \nminute, hourly, daily).\nProcesses data \ncontinuously, with \nminimal latency.\nEfficiency\nHandles large volumes of \ndata more efficiently, \noptimizing resource \nallocation and parallel \nprocessing.\nHandles single data \npoints, providing \nimmediate insights \nand updates, allowing \nfor rapid response to \nchanges.\nProcessing complexity\nCapable of performing \ncomplex data transformations \nand aggregations.\nDesigned to handle \nhigh-velocity data \nstreams with low \nlatency.\nUse cases\nSuitable for scenarios where \nimmediate data processing \nis not critical. Commonly \nused in data warehousing, \nreporting, ETL processes, \nand feature pipelines.\nIdeal for applications \nrequiring real-time \nanalytics, features, \nmonitoring, and event-\ndriven architectures.\nSystem complexity\nCompared to streaming \npipelines, systems are \ngenerally simpler to \nimplement and maintain.\nMore complex to \nimplement and maintain \ndue to the need for \nlow-latency processing, \nfault tolerance, and \nscalability. The \ntooling is also \nmore advanced and \ncomplicated.\nTable 4.1: Batch versus streaming pipelines\n",
      "content_length": 1714,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "RAG Feature Pipeline\n132\nFor example, streaming pipelines are extremely powerful in social media recommender systems \nlike TikTok. When using social media, user behavior changes frequently. A typical scenario is \nthat you want to relax at a certain point in time and mostly look at videos of puppies. Still, after \n15 minutes, you get bored and want something more serious, such as educative content or news. \nThis means the recommender system has to capture these behavior changes without delay to keep \nyou engaged. As the transition between interests is cyclical and not predictable, you can’t use a \nbatch pipeline that runs every 30 minutes or every hour to generate more content. You can run \nit every minute to create new content, but, at the same time, it will result in unnecessary costs, \nas most predictions will not be consumed. By implementing a streaming pipeline, you update \nthe features of specific users in real time, which are then passed to a chain of models that predict \nthe new recommendations.\nStreaming architectures are also the backbone of real-time fraud detection algorithms, such as \nthose used at Stripe or PayPal. In this context, it’s critical to identify potentially fraudulent trans-\nactions as they occur, not after a few minutes or hours as a batch pipeline would process them. \nThe same urgency applies to high-frequency trading platforms that make stock predictions based \non the constant influx of market data, enabling traders to make decisions within milliseconds.\nOn the other hand, you can use a batch architecture for an offline recommender system. For ex-\nample, when implementing one for an e-commerce or streaming platform, you don’t need the \nsystem to be so reactive, as the user’s behavior rarely changes. Thus, updating the recommen-\ndations periodically, such as every night, based on historical user behavior data using a batch \npipeline is easier to implement and cheaper.\nAnother popular example of batch pipelines is the ETL design used to extract, transform, and load \ndata for different use cases. The ETL design is widespread in data pipelines used to move data \nfrom one DB to another. Some practical use cases include aggregating data for analytics, where \nyou have to extract data from multiple sources, aggregate it, and load it to a data warehouse \nconnected to a dashboard. The analytics domains can be widespread, from e-commerce and \nmarketing to finance and research.\n",
      "content_length": 2437,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "Chapter 4\n133\nThe data collection pipeline used in the LLM Twin use case is another example of an ETL pipeline \nthat extracts data from the internet, structures it, and loads it into a data warehouse for future \nprocessing.\nAlong with prediction or feature freshness, another disadvantage of batch pipelines over streaming \nones is that you usually make redundant predictions. Let’s take the example of a recommender \nsystem for a streaming platform like Netflix. Every night, you make the predictions for all users. \nThere is a significant chance that a large chunk of users won’t log in that day. Also, users usually \ndon’t browse all the recommendations but stick to the first ones. Thus, only a portion of predic-\ntions are used, wasting computing power on all the others.\nThat’s why a popular strategy is to start with a batch architecture, as it’s faster and easier to im-\nplement. After the product is in place, you gradually move to a streaming design to reduce costs \nand improve the user experience.\nTo conclude, we have used a batch architecture (and not a streaming one) to implement the LLM \nTwin’s feature pipeline for the following reasons:\n•\nDoes not require immediate data processing: Even if syncing the data warehouse and \nfeature store is critical for an accurate RAG system, a delay of a few minutes is acceptable. \nThus, we can schedule the batch pipeline to run every minute, constantly syncing the two \ndata storages. This technique works because the data volume is small. The whole data \nwarehouse will have only thousands of records, not millions or billions. Hence, we can \nquickly iterate through them and sync the two DBs.\n•\nSimplicity: As stated earlier, implementing a streaming pipeline is two times more com-\nplex. In the real world, you want to keep your system as simple as possible, making it \neasier to understand, debug, and maintain. Also, simplicity usually translates to lower \ninfrastructure and development costs.\n",
      "content_length": 1957,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "RAG Feature Pipeline\n134\nIn Figure 8.10, we compare what tools you can use based on your architecture (streaming versus \nbatch) and the quantity of data you have to process (small versus big data). In our use case, we \nare in the smaller data and batch quadrant, where we picked a combination of vanilla Python and \ngenerative AI tools such as LangChain, Sentence Transformers, and Unstructured.\nFigure 4.10: Tools on the streaming versus batch and smaller versus bigger data spectrum\nIn the Change data capture: syncing the data warehouse and feature store section later in this chapter, \nwe will discuss when switching from a batch architecture to a streaming one makes sense.\nCore steps\nMost of the RAG feature pipelines are composed of five core steps. The one implemented in the \nLLM Twin architecture makes no exception. Thus, you can quickly adapt this pattern for other \nRAG applications, but here is what the LLM Twin’s RAG feature pipeline looks like:\n1.\nData extraction: Extract the latest articles, code repositories, and posts from the Mon-\ngoDB data warehouse. At the extraction step, you usually aggregate all the data you need \nfor processing.\n",
      "content_length": 1160,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "Chapter 4\n135\n2. \nCleaning: The data from the data warehouse is standardized and partially clean, but we \nhave to ensure that the text contains only useful information, is not duplicated, and can \nbe interpreted by the embedding model. For example, we must clean and normalize all \nnon-ASCII characters before passing the text to the embedding model. Also, to keep the \ninformation semantically dense, we decided to replace all the URLs with placeholders \nand remove all emojis. The cleaning step is more art than science. Hence, after you have \nthe first iteration with an evaluation mechanism in place, you will probably reiterate and \nimprove it.\n3. \nChunking: You must adopt various chunking strategies based on each data category \nand embedding model. For example, when working with code repositories, you want \nthe chunks broader, whereas when working with articles, you want them narrower or \nscoped at the paragraph level. Depending on your data, you must decide if you split your \ndocument based on the chapter, section, paragraph, sentence, or just a fixed window size. \nAlso, you have to ensure that the chunk size doesn’t exceed the maximum input size of \nthe embedding model. That is why you usually chunk a document based on your data \nstructure and the maximum input size of the model.\n4.\nEmbedding: You pass each chunk individually to an embedding model of your choice. \nImplementation-wise, this step is usually the simplest, as tools such as SentenceTrans-\nformer and Hugging Face provide high-level interfaces for most embedding models. As \nexplained in the What are embeddings? section of this chapter, at this step, the most critical \ndecisions are to decide what model to use and whether to fine-tune it or not. For example, \nwe used an \"all-mpnet-base-v2\" embedding model from SentenceTransformer, which \nis relatively tiny and runs on most machines. However, we provide a configuration file \nwhere you can quickly configure the embedding model with something more powerful \nbased on the state of the art when reading this book. You can quickly find other options \non the MTEB on Hugging Face (https://huggingface.co/spaces/mteb/leaderboard).\n5.\nData loading: The final step combines the embedding of a chunked document and its \nmetadata, such as the author and the document ID, content, URL, platform, and creation \ndate. Ultimately, we wrap the vector and the metadata into a structure compatible with \nQdrant and push it to the vector DB. As we want to use Qdrant as the single source of truth \nfor the features, we also push the cleaned documents (before chunking) to Qdrant. We can \npush data without vectors, as the metadata index of Qdrant behaves like a NoSQL DB. Thus, \npushing metadata without a vector attached to it is like using a standard NoSQL engine.\n",
      "content_length": 2790,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "RAG Feature Pipeline\n136\nChange data capture: syncing the data warehouse and feature \nstore\nAs highlighted a few times in this chapter, data is constantly changing, which can result in DBs, \ndata lakes, data warehouses, and feature stores getting out of sync. Change data capture (CDC) \nis a strategy that allows you to optimally keep two or more data storage types in sync without \ncomputing and I/O overhead. It captures any CRUD operation done on the source DB and repli-\ncates it on a target DB. Optionally, you can add preprocessing steps in between the replication.\nThe syncing issues also apply when building a feature pipeline. One key design choice concerns \nhow to sync the data warehouse with the feature store to have data fresh enough for your par-\nticular use case.\nIn our LLM Twin use case, we chose a naïve approach out of simplicity. We implemented a batch \npipeline that is triggered periodically or manually. It reads all the raw data from the data ware-\nhouse, processes it in batches, and inserts new records or updates old ones from the Qdrant \nvector DB. This works fine when you are working with a small number of records, at the order of \nthousands or tens of thousands. But our naïve approach raises the following questions:\n• \nWhat happens if the data suddenly grows to millions of records (or higher)?\n• \nWhat happens if a record is deleted from the data warehouse? How is this reflected in \nthe feature store?\n• \nWhat if we want to process only the new or updated items from the data warehouse and \nnot all of them?\nFortunately, the CDC pattern can solve all of these issues. When implementing CDC, you can \ntake multiple approaches, but all of them use either a push or pull strategy:\n•\nPush: The source DB is the primary driver in the push approach. It actively identifies \nand transmits data modifications to target systems for processing. This method ensures \nnear-instantaneous updates at the target, but data loss can occur if target systems are \ninaccessible. To mitigate this, a messaging system is typically employed as a buffer.\n•\nPull: The pull method assigns a more passive role to the source DB, which only records \ndata changes. Target systems periodically request these changes and handle updates \naccordingly. While this approach lightens the load on the source, it introduces a delay \nin data propagation. A messaging system is again essential to prevent data loss during \nperiods of target system unavailability.\n",
      "content_length": 2460,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "Chapter 4\n137\nIn summary, the push method is ideal for applications demanding immediate data access, where-\nas the pull method is better suited for large-scale data transfers where real-time updates aren’t \ncritical. With that in mind, there are different methods to detect changes in data. Thus, let’s list \nthe main CDC patterns that are used in the industry:\n•\nTimestamp-based: The approach involves adding a modification time column to DB \ntables, usually called LAST_MODIFIED or LAST_UPDATED. Downstream systems can query \nthis column to identify records that have been updated since their last check. While sim-\nple to implement, this method is limited to tracking changes, not deletions, and imposes \nperformance overhead due to the need to scan entire tables.\n•\nTrigger-based: The trigger-based approach utilizes DB triggers to automatically record \ndata modifications in a separate table upon INSERT, UPDATE, or DELETE operations, often \nknown as the event table. This method provides comprehensive change tracking but can \nimpact the DB performance due to the additional write operations involved for each event.\n•\nLog-based: DBs maintain transaction logs to record all data modifications, including \ntimestamps. Primarily used for recovery, these logs can also be leveraged to propagate \nchanges to target systems in real time. This approach minimizes the performance impact \non the source DB. As a huge advantage, it avoids additional processing overhead on the \nsource DB, captures all data changes, and requires no schema modification. But on the op-\nposite side, it lacks standardized log formats, leading to vendor-specific implementations.\nWith these CDC techniques in mind, we could quickly implement a pull timestamp-based strategy \nin our RAG feature pipeline to sync the data warehouse and feature store more optimally when \nthe data grows. Our implementation is still pull-based but doesn’t check any last updated field \nin the source DB; it just pulls everything from the data warehouse.\nHowever, the most popular and optimal technique in the industry is the log-based one. It doesn’t \nadd any I/O overhead to the source DB, has low latency, and supports all CRUD operations. The \nbiggest downside is its development complexity, which requires a queue to capture all the CRUD \nevents and a streaming pipeline to process them.\nFor more details on CDC, I recommend What is Change Data Capture? from Conflu-\nent’s blog: https://www.confluent.io/en-gb/learn/change-data-capture/.\n",
      "content_length": 2499,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "RAG Feature Pipeline\n138\nAs this is an LLM book and not a data engineering one, we wanted to keep things simple, but it’s \nimportant to know that these techniques exist, and you can always upgrade your current imple-\nmentation when it doesn’t fit your application requirements anymore.\nWhy is the data stored in two snapshots?\nWe store two snapshots of our data in the logical feature store:\n• \nAfter the data is cleaned: For fine-tuning LLMs\n• \nAfter the documents are chunked and embedded: For RAG\nWhy did we design it this way? Remember that the features should be accessed solely from the feature \nstore for training and inference. Thus, this adds consistency to our design and makes it cleaner.\nAlso, storing the data cleaned specifically for our fine-tuning and embedding use case in the Mon-\ngoDB data warehouse would have been an antipattern. The data from the warehouse is shared \nall across the company. Thus, processing it for a specific use case is not good practice. Imagine \nanother summarization use case where we must clean and preprocess the data differently. We \nmust create a new “Cleaned Data” table prefixed with the use case name. We have to repeat that \nfor every new use case. Therefore, to avoid having a spaghetti data warehouse, the data from the \ndata warehouse is generic and is modeled to specific applications only in downstream compo-\nnents, which, in our case, is the feature store.\nUltimately, as we mentioned in the Core steps section, you can leverage the metadata index of a \nvector DB as a NoSQL DB. Based on these factors, we decided to keep the cleaned data in Qdrant, \nalong with the chunked and embedded versions of the documents.\nAs a quick reminder, when operationalizing our LLM Twin system, the create instruct dataset \npipeline, explained in Chapter 5, will read the cleaned documents from Qdrant, process them, \nand save them under a versioned ZenML artifact. The training pipeline requires a dataset and not \nplain documents. This is a reminder that our logical feature store comprises the Qdrant vector \nDB for online serving and ZenML artifacts for offline training.\nOrchestration\nZenML will orchestrate the batch RAG feature pipeline. Using ZenML, we can schedule it to run \non a schedule, for example, every hour, or quickly manually trigger it. Another option is to trigger \nit after the ETL data collection pipeline finishes.\nBy orchestrating the feature pipeline and integrating it into ZenML (or any other orchestration \ntool), we can operationalize the feature pipeline with the end goal of continuous training (CT).\n",
      "content_length": 2575,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "Chapter 4\n139\nWe will go into all the details of orchestration, scheduling, and CT in Chapter 11.\nImplementing the LLM Twin’s RAG feature pipeline\nThe last step is to review the LLM Twin’s RAG feature pipeline code to see how we applied every-\nthing we discussed in this chapter. We will walk you through the following:\n• \nZenML code\n• \nPydantic domain objects\n• \nA custom object-vector mapping (OVM) implementation\n• \nThe cleaning, chunking, and embedding logic for all our data categories\nWe will take a top-down approach. Thus, let’s start with the Settings class and ZenML pipeline.\nSettings\nWe use Pydantic Settings (https://docs.pydantic.dev/latest/concepts/pydantic_settings/) \nto define a global Settings class that loads sensitive or non-sensitive variables from a .env file. \nThis approach also gives us all the benefits of Pydantic, such as type validation. For example, if \nwe provide a string for the QDRANT_DATABASE_PORT variable instead of an integer, the program \nwill crash. This behavior makes the whole application more deterministic and reliable.\nHere is what the Settings class looks like with all the variables necessary to build the RAG fea-\nture pipeline:\nfrom pydantic import BaseSettings\nclass Settings(BaseSettings):\n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n    … # Some other settings…\n    # RAG\n    TEXT_EMBEDDING_MODEL_ID: str = \"sentence-transformers/all-MiniLM-\nL6-v2\"\n    RERANKING_CROSS_ENCODER_MODEL_ID: str = \"cross-encoder/ms-marco-\nMiniLM-L-4-v2\"\n    RAG_MODEL_DEVICE: str = \"cpu\"\n",
      "content_length": 1558,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "RAG Feature Pipeline\n140\n    # QdrantDB Vector DB\n    USE_QDRANT_CLOUD: bool = False\n    QDRANT_DATABASE_HOST: str = \"localhost\"\n    QDRANT_DATABASE_PORT: int = 6333\n    QDRANT_CLOUD_URL: str = \"str\"\n    QDRANT_APIKEY: str | None = None\n    … # More settings…\nsettings = Settings()\nAs stated in the internal Config class, all the variables have default values or can be overridden \nby providing a .env file.\nZenML pipeline and steps\nThe ZenML pipeline is the entry point for the RAG feature engineering pipeline. It reflects the \nfive core phases of RAG ingestion code: extracting raw documents, cleaning, chunking, embed-\nding, and loading them to the logical feature store. The calls within the feature_engineering()\nfunction are ZenML steps, representing a single execution unit performing the five phases of RAG. \nThe code is available in the GitHub repository at https://github.com/PacktPublishing/LLM-\nEngineers-Handbook/blob/main/pipelines/feature_engineering.py:\nfrom zenml import pipeline\nfrom llm_engineering.interfaces.orchestrator.steps import feature_\nengineering as fe_steps\n@pipeline\ndef feature_engineering(author_full_names: list[str]) -> None:\n    raw_documents = fe_steps.query_data_warehouse(author_full_names)\n    cleaned_documents = fe_steps.clean_documents(raw_documents)\n     last_step_1 = fe_steps.load_to_vector_db(cleaned_documents)\n    embedded_documents = fe_steps.chunk_and_embed(cleaned_documents)\n    last_step_2 = fe_steps.load_to_vector_db(embedded_documents)\n    return [last_step_1.invocation_id, last_step_2.invocation_id]\n",
      "content_length": 1560,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "Chapter 4\n141\nFigure 4.11 shows how multiple feature engineering pipeline runs look in ZenML’s dashboard.\nFigure 4.11: Feature pipeline runs in the ZenML dashboard\nFigure 8.12 shows the DAG of the RAG feature pipeline, where you can follow all the pipeline steps \nand their output artifacts. Remember that whatever is returned from a ZenML step is automati-\ncally saved as an artifact, stored in ZenML’s artifact registry, versioned, and shareable across the \napplication.\nFigure 4.12: Feature pipeline DAG in the ZenML dashboard\n",
      "content_length": 530,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "RAG Feature Pipeline\n142\nThe final puzzle piece is understanding how to configure the RAG feature pipeline dynamically. \nAll its available settings are exposed as function parameters. Here, we need only a list of au-\nthor’s names, as seen in the function’s signature: feature_engineering(author_full_names: \nlist[str]). We inject a YAML configuration file at runtime that contains all the necessary values \nbased on different use cases. For example, the following configuration includes a list of all the \nauthors of this book as we want to populate the feature store with data from all of us (available \nin the GitHub repository at configs/feature_engineering.yaml):\nparameters:\n  author_full_names:\n    - Alex Vesa\n    - Maxime Labonne\n    - Paul Iusztin\nThe beauty of this approach is that you don’t have to modify the code to configure the feature \npipeline with different input values. You have to provide a different configuration file when run-\nning it, as follows:\nfeature_engineering.with_options(config_path=\"…/feature_engineering.yaml\")\n()\nYou can either hardcode the path to the config file or provide the config_path from the CLI, which \nallows you to modify the pipeline’s configuration between different runs. Out of simplicity, we \nhard-coded the configuration file. Thus, we can call the feature engineering pipeline calling the \nrun.py script as follows:\npython -m tools.run --no-cache --run-feature-engineering\nHowever, you can easily add another CLI argument to pass the config_path variable. Also, you \ncan run the feature pipeline using the following poe command:\npoetry poe run-feature-engineering-pipeline\nLet’s move forward to the ZenML steps and sequentially zoom in on all of them. The source code for \nall the feature engineering pipeline steps is available on GitHub at \"steps/feature_engineering\". \nWe will begin with the first step, which involves querying the data warehouse for new content \nto process into features.\n",
      "content_length": 1950,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "Chapter 4\n143\nQuerying the data warehouse\nThe first thing to notice is that a step is a Python function decorated with @step, similar to how \na ZenML pipeline works. The function below takes as input a list of authors’ full names and per-\nforms the following core steps:\n•\nIt attempts to get or create a UserDocument instance using the first and last names, ap-\npending this instance to the authors list. If the user doesn’t exist, it throws an error.\n•\nIt fetches all the raw data for the user from the data warehouse and extends the documents\nlist to include these user documents.\n•\nUltimately, it computes a descriptive metadata dictionary logged and tracked in ZenML.\n… # other imports\nfrom zenml import get_step_context, step\n@step\ndef query_data_warehouse(\n    author_full_names: list[str],\n) -> Annotated[list, \"raw_documents\"]:\n    documents = []\n    authors = []\n    for author_full_name in author_full_names:\n        logger.info(f\"Querying data warehouse for user: {author_full_\nname}\")\n        first_name, last_name = utils.split_user_full_name(author_full_\nname)\n        logger.info(f\"First name: {first_name}, Last name: {last_name}\")\n        user = UserDocument.get_or_create(first_name=first_name, last_\nname=last_name)\n        authors.append(user)\n        results = fetch_all_data(user)\n        user_documents = [doc for query_result in results.values() for doc \nin query_result]\n        documents.extend(user_documents)\n    step_context = get_step_context()\n",
      "content_length": 1475,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "RAG Feature Pipeline\n144\n    step_context.add_output_metadata(output_name=\"raw_documents\", \nmetadata=_get_metadata(documents))\n    return documents\nThe fetch function leverages a thread pool that runs each query on a different thread. As we have \nmultiple data categories, we have to make a different query for the articles, posts, and reposi-\ntories, as they are stored in different collections. Each query calls the data warehouse, which is \nbounded by the network I/O and data warehouse latency, not by the machine’s CPU. Thus, by \nmoving each query to a different thread, we can parallelize them. Ultimately, instead of adding \nthe latency of each query as the total timing, the time to run this fetch function will be the max \nbetween all the calls.\nUsing threads to parallelize I/O-bounded calls is good practice in Python, as they are not locked \nby the Python Global Interpreter Lock (GIL). In contrast, adding each call to a different process \nwould add too much overhead, as a process takes longer to spin off than a thread.\nIn Python, you want to parallelize things with processes only when the operations are CPU or \nmemory-bound because the GIL affects them. Each process has a different GIL. Thus, paralleliz-\ning your computing logic, such as processing a batch of documents or images already loaded in \nmemory, isn’t affected by Python’s GIL limitations.\ndef fetch_all_data(user: UserDocument) -> dict[str, \nlist[NoSQLBaseDocument]]:\n    user_id = str(user.id)\n    with ThreadPoolExecutor() as executor:\n        future_to_query = {\n            executor.submit(__fetch_articles, user_id): \"articles\",\n            executor.submit(__fetch_posts, user_id): \"posts\",\n            executor.submit(__fetch_repositories, user_id): \n\"repositories\",\n        }\n        results = {}\n        for future in as_completed(future_to_query):\n            query_name = future_to_query[future]\n            try:\n                results[query_name] = future.result()\n            except Exception:\n",
      "content_length": 1989,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "Chapter 4\n145\n                logger.exception(f\"'{query_name}' request failed.\")\n                results[query_name] = []\n    return results\nThe _get_metadata() function takes the list of queried documents and authors and counts the \nnumber of them relative to each data category:\ndef _get_metadata(documents: list[Document]) -> dict:\n    metadata = {\n        \"num_documents\": len(documents),\n    }\n    for document in documents:\n        collection = document.get_collection_name()\n        if collection not in metadata:\n            metadata[collection] = {}\n        if \"authors\" not in metadata[collection]:\n            metadata[collection][\"authors\"] = list()\n        metadata[collection][\"num_documents\"] = metadata[collection].\nget(\"num_documents\", 0) + 1\n        metadata[collection][\"authors\"].append(document.author_full_name)\n    for value in metadata.values():\n        if isinstance(value, dict) and \"authors\" in value:\n            value[\"authors\"] = list(set(value[\"authors\"]))\n    return metadata\nWe will expose this metadata in the ZenML dashboard to quickly see some statistics on the loaded \ndata. For example, in Figure 4.13, we accessed the metadata tab of the query_data_warehouse()\nstep, where you can see that, within that particular run of the feature pipeline, we loaded 76 \ndocuments from three authors. This is extremely powerful for monitoring and debugging batch \npipelines. \n",
      "content_length": 1402,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "RAG Feature Pipeline\n146\nYou can always extend it with anything that makes sense for your use case.\nFigure 4.13: Metadata of the “query the data warehouse” ZenML step\nCleaning the documents\nIn the cleaning step, we iterate through all the documents and delegate all the logic to a \nCleaningDispatcher who knows what cleaning logic to apply based on the data category. Re-\nmember that we want to apply, or have the ability to apply in the future, different cleaning tech-\nniques on articles, posts, and code repositories.\n@step\ndef clean_documents(\n    documents: Annotated[list, \"raw_documents\"],\n) -> Annotated[list, \"cleaned_documents\"]:\n    cleaned_documents = []\n    for document in documents:\n",
      "content_length": 698,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "Chapter 4\n147\n        cleaned_document = CleaningDispatcher.dispatch(document)\n        cleaned_documents.append(cleaned_document)\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"cleaned_documents\", \nmetadata=_get_metadata(cleaned_documents))\n    return cleaned_documents\nThe computed metadata is similar to what we logged in the query_data_warehouse() step. Thus, \nlet’s move on to chunking and embedding.\nChunk and embed the cleaned documents\nSimilar to how we cleaned the documents, we delegate the chunking and embedding logic to \na dispatcher who knows how to handle each data category. Note that the chunking dispatcher \nreturns a list instead of a single object, which makes sense as the document is split into multiple \nchunks. We will dig into the dispatcher in the “The dispatcher layer” section of this chapter.\n@step\ndef chunk_and_embed(\n    cleaned_documents: Annotated[list, \"cleaned_documents\"],\n) -> Annotated[list, \"embedded_documents\"]:\n    metadata = {\"chunking\": {}, \"embedding\": {}, \"num_documents\": \nlen(cleaned_documents)}\n    embedded_chunks = []\n    for document in cleaned_documents:\n        chunks = ChunkingDispatcher.dispatch(document)\n        metadata[\"chunking\"] = _add_chunks_metadata(chunks, \nmetadata[\"chunking\"])\n        for batched_chunks in utils.misc.batch(chunks, 10):\n            batched_embedded_chunks = EmbeddingDispatcher.\ndispatch(batched_chunks)\n            embedded_chunks.extend(batched_embedded_chunks)\n    metadata[\"embedding\"] = _add_embeddings_metadata(embedded_chunks, \nmetadata[\"embedding\"])\n    metadata[\"num_chunks\"] = len(embedded_chunks)\n",
      "content_length": 1635,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "RAG Feature Pipeline\n148\n    metadata[\"num_embedded_chunks\"] = len(embedded_chunks)\n    step_context = get_step_context()\n    step_context.add_output_metadata(output_name=\"embedded_documents\", \nmetadata=metadata)\n    return embedded_chunks\nIn Figure 4.14, you can see the metadata of the chunking and embedding ZenML step. For exam-\nple, you can quickly understand that we transformed 76 documents into 2,373 chunks, or the \nproperties we used for chunking articles, such as a chunk_size of 500 and a chunk_overlap of 50.\nFigure 4.14: Metadata of the embedding and chunking ZenML step, detailing the uncategorized \nand chunking dropdowns\n",
      "content_length": 638,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "Chapter 4\n149\nIn Figure 4.15, the rest of the ZenML metadata from the embedding and chunking step details \nthe embedding model and its properties used to compute the vectors.\nFigure 4.15: Metadata of the embedding and chunking ZenML step, detailing the embedding \ndropdown\n",
      "content_length": 273,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "RAG Feature Pipeline\n150\nAs ML systems can break at any time while in production due to drifts or untreated use cases, le-\nveraging the metadata section to monitor the ingested data can be a powerful tool that will save \ndebugging days, translating to tens of thousands of dollars or more for your business.\nLoading the documents to the vector DB\nAs each article, post, or code repository sits in a different collection inside the vector DB, we have \nto group all the documents based on their data category. Then, we load each group in bulk in the \nQdrant vector DB:\n@step\ndef load_to_vector_db(\n    documents: Annotated[list, \"documents\"],\n) -> None:\n    logger.info(f\"Loading {len(documents)} documents into the vector \ndatabase.\")\n    grouped_documents = VectorBaseDocument.group_by_class(documents)\n    for document_class, documents in grouped_documents.items():\n        logger.info(f\"Loading documents into {document_class.get_\ncollection_name()}\")\n        for documents_batch in utils.misc.batch(documents, size=4):\n            try:\n                document_class.bulk_insert(documents_batch)\n            except Exception:\n                return False\n    return True\nPydantic domain entities\nBefore investigating the dispatchers, we must understand the domain objects we work with. To \nsome extent, in implementing the LLM Twin, we are following the domain-driven design (DDD) \nprinciples, which state that domain entities are the core of your application. Thus, before pro-\nceeding, it’s important to understand the hierarchy of the domain classes we are working with.\nThe code for the domain entities is available on GitHub at https://github.com/\nPacktPublishing/LLM-Engineering/tree/main/llm_engineering/domain.\n",
      "content_length": 1722,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "Chapter 4\n151\nWe used Pydantic to model all our domain entities. When we wrote the book, choosing Pydantic \nwas a no-brainer, as it is the go-to Python package for writing data structures with out-of-the-box \ntype validation. As Python is a dynamically typed language, using Pydantic for type validation at \nruntime makes your system order of times more robust, as you can be sure that you are always \nworking with the right type of data.\nThe domain of our LLM Twin application is split into two dimensions:\n• \nThe data category: Post, article, and repository\n• \nThe state of the data: Cleaned, chunked, and embedded\nWe decided to create a base class for each state of the document, resulting in having the following \nbase abstract classes:\n•\nclass CleanedDocument(VectorBaseDocument, ABC)\n•\nclass Chunk(VectorBaseDocument, ABC)\n•\nclass EmbeddedChunk(VectorBaseDocument, ABC)\nNote that all of them inherit the VectorBaseDocument class, which is our custom OVM implemen-\ntation, which we will explain in the next section of this chapter. Also, it inherits from ABC, which \nmakes the class abstract. Thus, you cannot initialize an object out of these classes; you may only \ninherit from them. That is why base classes are always marked as abstract.\nEach base abstract class from above (which models the state) will have a subclass that adds \nthe data category dimension. For example, the CleanedDocument class will have the following \nsubclasses:\n•\nclass CleanedPostDocument(CleanedDocument)\n•\nclass CleanedArticleDocument(CleanedDocument)\n•\nclass CleanedRepositoryDocument(CleanedDocument)\nAs we can see in Figure 8.16, we will repeat the same logic for the Chunk and EmbeddedChunk base \nabstract classes. We will implement a specific document class for each data category and state com-\nbination, resulting in nine types of domain entities. For example, when ingesting a raw document, \nthe cleaning step will yield a CleanedArticleDocument instance, the chunking step will return a \nlist of ArticleChunk objects, and the embedding operation will return EmbeddedArticleChunk in-\nstances that encapsulate the embedding and all the necessary metadata to ingest in the vector DB. \n",
      "content_length": 2177,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "RAG Feature Pipeline\n152\nThe same will happen for the posts and repositories.\nFigure 4.16: Domain entities class hierarchy and their interaction\nWe chose this design because the list of states will rarely change, and we want to extend the list \nof data categories. Thus, structuring the classes after the state allows us to plug another data \ncategory by inheriting these base abstract classes.\nLet’s see the complete code for the hierarchy of the cleaned document. All the attributes of a \ncleaned document will be saved within the metadata of the vector DB. For example, the metadata \nof a cleaned article document will always contain the content, platform, author ID, author full \nname, and link of the article.\nAnother fundamental aspect is the Config internal class, which defines the name of the collection \nwithin the vector DB, the data category of the entity, and whether to leverage the vector index \nwhen creating the collection:\nclass CleanedDocument(VectorBaseDocument, ABC):\n    content: str\n    platform: str\n    author_id: UUID4\n    author_full_name: str\n",
      "content_length": 1071,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "Chapter 4\n153\nclass CleanedPostDocument(CleanedDocument):\n    image: Optional[str] = None\n    class Config:\n        name = \"cleaned_posts\"\n        category = DataCategory.POSTS\n        use_vector_index = False\nclass CleanedArticleDocument(CleanedDocument):\n    link: str\n    class Config:\n        name = \"cleaned_articles\"\n        category = DataCategory.ARTICLES\n        use_vector_index = False\nclass CleanedRepositoryDocument(CleanedDocument):\n    name: str\n    link: str\n    class Config:\n        name = \"cleaned_repositories\"\n        category = DataCategory.REPOSITORIES\n        use_vector_index = False\nTo conclude this section, let’s also take a look at the base abstract class of the chunk and embed-\nded chunk:\nclass Chunk(VectorBaseDocument, ABC):\n    content: str\n    platform: str\n    document_id: UUID4\n    author_id: UUID4\n    author_full_name: str\n    metadata: dict = Field(default_factory=dict)\n… # PostChunk, ArticleChunk, RepositoryChunk\n",
      "content_length": 957,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "RAG Feature Pipeline\n154\nclass EmbeddedChunk(VectorBaseDocument, ABC):\n    content: str\n    embedding: list[float] | None\n    platform: str\n    document_id: UUID4\n    author_id: UUID4\n    author_full_name: str\n    metadata: dict = Field(default_factory=dict)\n… # EmbeddedPostChunk, EmbeddedArticleChunk, EmbeddedRepositoryChunk\nWe also defined an enum that aggregates all our data categories in a single structure of constants:\nclass DataCategory(StrEnum):\n    POSTS = \"posts\"\n    ARTICLES = \"articles\"\n    REPOSITORIES = \"repositories\"\nThe last step to fully understand how the domain objects work is to zoom into the \nVectorBaseDocument OVM class.\nOVM\nThe term OVM is inspired by the object-relational mapping (ORM) pattern we discussed in Chap-\nter 3. We called it OVM because we work with embedding and vector DBs instead of structured \ndata and SQL tables. Otherwise, it follows the same principles as an ORM pattern.\nSimilar to what we did in Chapter 3, we will implement our own OVM version. Even if our custom \nexample is simple, it’s a powerful example of how to write modular and extendable classes by \nleveraging OOP best practices and principles.\nOur OVM base class is called VectorBaseDocument. It will support CRUD operations on top of \nQdrant. Based on our application’s demands, we limited it only to create and read operations, but \nit can easily be extended to update and delete functions. \nThe full implementation of the VectorBaseDocument class is available on GitHub \nat https://github.com/PacktPublishing/LLM-Engineering/blob/main/llm_\nengineering/domain/base/vector.py.\n",
      "content_length": 1593,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "Chapter 4\n155\nLet’s take a look at the definition of the VectorBaseDocument class:\nfrom pydantic import UUID4, BaseModel\nfrom typing import Generic\nfrom llm_engineering.infrastructure.db.qdrant import connection\nT = TypeVar(\"T\", bound=\"VectorBaseDocument\")\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    id: UUID4 = Field(default_factory=uuid.uuid4)\n    @classmethod\n    def from_record(cls: Type[T], point: Record) -> T:\n        _id = UUID(point.id, version=4)\n        payload = point.payload or {}\n        attributes = {\n            \"id\": _id,\n            **payload,\n        }\n        if cls._has_class_attribute(\"embedding\"):\n            payload[\"embedding\"] = point.vector or None\n        return cls(**attributes)\n    def to_point(self: T, **kwargs) -> PointStruct:\n        exclude_unset = kwargs.pop(\"exclude_unset\", False)\n        by_alias = kwargs.pop(\"by_alias\", True)\n        payload = self.dict(exclude_unset=exclude_unset, by_alias=by_\nalias, **kwargs)\n        _id = str(payload.pop(\"id\"))\n        vector = payload.pop(\"embedding\", {})\n        if vector and isinstance(vector, np.ndarray):\n",
      "content_length": 1112,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "RAG Feature Pipeline\n156\n            vector = vector.tolist()\n        return PointStruct(id=_id, vector=vector, payload=payload)\n• \nThe VectorBaseDocument class inherits from Pydantic’s BaseModel and helps us structure \na single record’s attributes from the vector DB. Every OVM will be initialized by default \nwith UUID4 as its unique identifier. Using generics—more precisely, by inheriting from \nGeneric[T]—the signatures of all the subclasses of the VectorBaseDocument class will \nadapt to that given class. For example, the from_record() method of the Chunk() class, \nwhich inherits VectorBaseDocument, will return the Chunk type, which drastically helps \nthe static analyzer and type checkers such as mypy (https://mypy.readthedocs.io/en/\nstable/).\nThe from_record() method adapts a data point from Qdrant’s format to our internal structure \nbased on Pydantic. On the other hand, the to_point() method takes the attributes of the current \ninstance and adapts them to Qdrant’s PointStruct() format. We will leverage these two methods \nfor our create and read operations.\nUltimately, all operations made to Qdrant will be done through the connection instance, which \nis instantiated in the application’s infrastructure layer.\nThe bulk_insert() method maps each document to a point. Then, it uses the Qdrant connection\ninstance to load all the points to a given collection in Qdrant. If the insertion fails once, it tries \nto create the collection and do the insertion again. Often, it is good practice to split your logic \ninto two functions. One private function contains the logic, in our case _bulk_insert(), and one \npublic function handles all the errors and failure scenarios.\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    … # Rest of the class\n    @classmethod\n    def bulk_insert(cls: Type[T], documents: list[\"VectorBaseDocument\"]) \n-> bool:\n        try:\n            cls._bulk_insert(documents)\n        except exceptions.UnexpectedResponse:\n            logger.info(\n                f\"Collection '{cls.get_collection_name()}' does not exist. \nTrying to create the collection and reinsert the documents.\"\n",
      "content_length": 2128,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "Chapter 4\n157\n            )\n            cls.create_collection()\n            try:\n                cls._bulk_insert(documents)\n            except exceptions.UnexpectedResponse:\n                logger.error(f\"Failed to insert documents in '{cls.get_\ncollection_name()}'.\")\n                return False\n        return True\n    @classmethod\n    def _bulk_insert(cls: Type[T], documents: list[\"VectorBaseDocument\"]) \n-> None:\n        points = [doc.to_point() for doc in documents]\n        connection.upsert(collection_name=cls.get_collection_name(), \npoints=points)\nThe collection name is inferred from the Config class defined in the subclasses inheriting the OVM:\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    … # Rest of the class\n    @classmethod\n    def get_collection_name(cls: Type[T]) -> str:\n        if not hasattr(cls, \"Config\") or not hasattr(cls.Config, \"name\"):\n            raise ImproperlyConfigured(\n                \"The class should define a Config class with\" \"the 'name' \nproperty that reflects the collection's name.\"\n            )\n        return cls.Config.name\n",
      "content_length": 1088,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "RAG Feature Pipeline\n158\nNow, we must define a method that lets us read all the records from the vector DB (without using \nvector similarity search logic). The bulk_find() method enables us to scroll (or list) all the records \nfrom a collection. The function below scrolls the Qdrant vector DB, which returns a list of data \npoints, which are ultimately mapped to our internal structure using the from_record() method.\nThe limit parameters control how many items we return at once, and the offset signals the ID of \nthe point from which Qdrant starts returning records.\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    … # Rest of the class\n    @classmethod\n    def bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> \ntuple[list[T], UUID | None]:\n        try:\n            documents, next_offset = cls._bulk_find(limit=limit, **kwargs)\n        except exceptions.UnexpectedResponse:\n            logger.error(f\"Failed to search documents in '{cls.get_\ncollection_name()}'.\")\n            documents, next_offset = [], None\n        return documents, next_offset\n    @classmethod\n    def _bulk_find(cls: Type[T], limit: int = 10, **kwargs) -> \ntuple[list[T], UUID | None]:\n        collection_name = cls.get_collection_name()\n        offset = kwargs.pop(\"offset\", None)\n        offset = str(offset) if offset else None\n        records, next_offset = connection.scroll(\n            collection_name=collection_name,\n            limit=limit,\n            with_payload=kwargs.pop(\"with_payload\", True),\n            with_vectors=kwargs.pop(\"with_vectors\", False),\n            offset=offset,\n",
      "content_length": 1589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "Chapter 4\n159\n            **kwargs,\n        )\n        documents = [cls.from_record(record) for record in records]\n        if next_offset is not None:\n            next_offset = UUID(next_offset, version=4)\n        return documents, next_offset\nThe last piece of the puzzle is to define a method that performs a vector similarity search on a \nprovided query embedding. Like before, we defined a public search() and private _search()\nmethod. The search is performed by Qdrant when calling the connection.search() function.\nclass VectorBaseDocument(BaseModel, Generic[T], ABC):\n    … # Rest of the class\n    @classmethod\n    def search(cls: Type[T], query_vector: list, limit: int = 10, \n**kwargs) -> list[T]:\n        try:\n            documents = cls._search(query_vector=query_vector, \nlimit=limit, **kwargs)\n        except exceptions.UnexpectedResponse:\n            logger.error(f\"Failed to search documents in '{cls.get_\ncollection_name()}'.\")\n            documents = []\n        return documents\n    @classmethod\n    def _search(cls: Type[T], query_vector: list, limit: int = 10, \n**kwargs) -> list[T]:\n        collection_name = cls.get_collection_name()\n        records = connection.search(\n            collection_name=collection_name,\n            query_vector=query_vector,\n            limit=limit,\n            with_payload=kwargs.pop(\"with_payload\", True),\n            with_vectors=kwargs.pop(\"with_vectors\", False),\n",
      "content_length": 1419,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "RAG Feature Pipeline\n160\n            **kwargs,\n        )\n        documents = [cls.from_record(record) for record in records]\n        return documents\nNow that we understand what our domain entities look like and how the OVM works, let’s move \non to the dispatchers who clean, chunk, and embed the documents.\nThe dispatcher layer\nA dispatcher inputs a document and applies dedicated handlers based on its data category (article, \npost, or repository). A handler can either clean, chunk, or embed a document.\nLet’s start by zooming in on the CleaningDispatcher. It mainly implements a dispatch() method \nthat inputs a raw document. Based on its data category, it instantiates and calls a handler that \napplies the cleaning logic specific to that data point:\nclass CleaningDispatcher:\n    cleaning_factory = CleaningHandlerFactory()\n    @classmethod\n    def dispatch(cls, data_model: NoSQLBaseDocument) -> \nVectorBaseDocument:\n        data_category = DataCategory(data_model.get_collection_name())\n        handler = cls.cleaning_factory.create_handler(data_category)\n        clean_model = handler.clean(data_model)\n        logger.info(\n            \"Data cleaned successfully.\",\n            data_category=data_category,\n            cleaned_content_len=len(clean_model.content),\n        )\n        return clean_model\nThe key in the dispatcher logic is the CleaningHandlerFactory(), which instantiates a different \ncleaning handler based on the document’s data category:\nclass CleaningHandlerFactory:\n    @staticmethod\n",
      "content_length": 1512,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "Chapter 4\n161\n    def create_handler(data_category: DataCategory) -> \nCleaningDataHandler:\n        if data_category == DataCategory.POSTS:\n            return PostCleaningHandler()\n        elif data_category == DataCategory.ARTICLES:\n            return ArticleCleaningHandler()\n        elif data_category == DataCategory.REPOSITORIES:\n            return RepositoryCleaningHandler()\n        else:\n            raise ValueError(\"Unsupported data type\")\nThe Dispatcher or Factory classes are nothing fancy, but they offer an intuitive and simple interface \nfor applying various operations to your documents. When manipulating documents, instead of \nworrying about their data category and polluting your business logic with if-else statements, you \nhave a class dedicated to handling that. You have a single class that cleans any document, which \nrespects the DRY (don’t repeat yourself) principles from software engineering. By respecting DRY, \nyou have a single point of failure, and the code can easily be extended. For example, if we add \nan extra type, we must extend only the Factory class instead of multiple occurrences in the code.\nThe ChunkingDispatcher and EmbeddingDispatcher follow the same pattern. They use a \nChunkingHandlerFactory and, respectively, an EmbeddingHandlerFactory that initializes the \ncorrect handler based on the data category of the input document. Afterward, they call the han-\ndler and return the result.\nThe Factory class leverages theabstract factory creational pattern (https://refactoring.guru/\ndesign-patterns/abstract-factory), which instantiates a family of classes implementing the \nsame interface. In our case, these handlers implement the clean() method regardless of the \nhandler type. \nAlso, the Handler class family leverages the strategy behavioral pattern (https://refactoring.\nguru/design-patterns/strategy) used to instantiate when you want to use different variants of \nan algorithm within an object and be able to switch from one algorithm to another during runtime.\nThe source code of all the dispatchers and factories can be found on GitHub at \nhttps://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/ \nllm_engineering/application/preprocessing/dispatchers.py\n",
      "content_length": 2222,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "RAG Feature Pipeline\n162\nIntuitively, in our dispatcher layer, the combination of the factory and strategy patterns works \nas follows:\n1. \nInitially, we knew we wanted to clean the data, but as we knew the data category only at \nruntime, we couldn’t decide on what strategy to apply.\n2. We can write the whole code around the cleaning code and abstract away the logic under \na Handler() interface, which will represent our strategy.\n3. \nWhen we get a data point, we apply the abstract factory pattern and create the correct \ncleaning handler for its data type.\n4. \nUltimately, the dispatcher layer uses the handler and executes the right strategy.\nBy doing so, we:\n•\nIsolate the logic for a given data category.\n•\nLeverage polymorphism to avoid filling up the code with hundreds of if-else statements.\n•\nMake the code modular and extendable. When a new data category arrives, we must \nimplement a new handler and modify the Factory class without touching any other part \nof the code.\nThe last component of the RAG feature pipeline is the implementation of the cleaning, chunking, \nand embedding handlers.\nThe handlers\nThe handler has a one-on-one structure with our domain, meaning that every entity has its own \nhandler, as shown in Figure 8.17. In total, we will have nine Handler classes that follow the next \nbase interfaces:\n•\nclass CleaningDataHandler()\n•\nclass ChunkingDataHandler()\n•\nclass EmbeddingDataHandler()\nUntil now, we have just modeled our entities and how the data flows in our appli-\ncation. We haven’t written a single piece of cleaning, chunking, or embedding code. \nThat is one big difference between a quick demo and a production-ready application. \nIn a demo, you don’t care about software engineering best practices and structuring \nyour code to make it future-proof. However, writing clean, modular, and scalable \ncode is critical for its longevity when building a real-world application.\n",
      "content_length": 1915,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "Chapter 4\n163\nFigure 4.17: Handler class hierarchy and their interaction\nLet’s examine each handler family and see how it is implemented.\nThe cleaning handlers\nThe CleaningDataHandler() strategy interface looks as follows:\n… # Other imports.\nfrom typing import Generic, TypeVar\nDocumentT = TypeVar(\"DocumentT\", bound=Document)\nCleanedDocumentT = TypeVar(\"CleanedDocumentT\", bound=CleanedDocument)\nclass CleaningDataHandler(ABC, Generic[DocumentT, CleanedDocumentT]):\nThe code for all the handlers is available on GitHub at https://github.com/\nPacktPublishing/LLM-Engineering/tree/main/llm_engineering/ \napplication/preprocessing.\n",
      "content_length": 630,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "RAG Feature Pipeline\n164\n    @abstractmethod\n    def clean(self, data_model: DocumentT) -> CleanedDocumentT:\n        pass\nNow, for every post, article and repository, we have to implement a different handler, as follows:\nclass PostCleaningHandler(CleaningDataHandler):\n    def clean(self, data_model: PostDocument) -> CleanedPostDocument:\n        return CleanedPostDocument(\n            id=data_model.id,\n            content=clean_text(\" #### \".join(data_model.content.\nvalues())),\n            … # Copy the rest of the parameters from the data_model \nobject.\n        )\nclass ArticleCleaningHandler(CleaningDataHandler):\n    def clean(self, data_model: ArticleDocument) -> \nCleanedArticleDocument:\n        valid_content = [content for content in data_model.content.\nvalues() if content]\n        return CleanedArticleDocument(\n            id=data_model.id,\n            content=clean_text(\" #### \".join(valid_content)),\n            platform=data_model.platform,\n            link=data_model.link,\n            author_id=data_model.author_id,\n            author_full_name=data_model.author_full_name,\n        )\nclass RepositoryCleaningHandler(CleaningDataHandler):\n    def clean(self, data_model: RepositoryDocument) -> \nCleanedRepositoryDocument:\n        return CleanedRepositoryDocument(\n            id=data_model.id,\n",
      "content_length": 1314,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "Chapter 4\n165\n            content=clean_text(\" #### \".join(data_model.content.\nvalues())),\n            … # Copy the rest of the parameters from the data_model \nobject.\n        )\nThe handlers input a raw document domain entity, clean the content, and return a cleaned docu-\nment. All the handlers use the clean_text() function to clean the text. Out of simplicity, we used \nthe same cleaning technique for all the data categories. Still, in a real-world setup, we would have \nto further optimize and create a different cleaning function for each data category. The strategy \npattern makes this a breeze, as we swap the cleaning function in the handlers, and that’s it.\nThe cleaning steps applied in the clean_text() function are the same ones discussed in Chapter 5\nin the Creating an instruction dataset section. We don’t want to repeat ourselves. Thus, for a re-\nfresher, check out that chapter. At this point, we mostly care about automating and integrating \nthe whole logic into the RAG feature pipeline. Thus, after operationalizing the ML system, all the \ncleaned data used for fine-tuning will be accessed from the logical feature store, making it the \nsingle source of truth for accessing data.\nThe chunking handlers\nFirst, let’s examine the ChunkingDataHandler() strategy handler. We exposed the metadata dic-\ntionary as a property to aggregate all the necessary properties required for chunking in a single \nstructure. By structuring it like this, we can easily log everything to ZenML to track and debug our \nchunking logic. The handler takes cleaned documents as input and returns chunk entities. All the \nhandlers can be found on GitHub at https://github.com/PacktPublishing/LLM-Engineering/\ntree/main/llm_engineering/application/preprocessing.\n… # Other imports.\nfrom typing import Generic, TypeVar\nCleanedDocumentT = TypeVar(\"CleanedDocumentT\", bound=CleanedDocument)\nChunkT = TypeVar(\"ChunkT\", bound=Chunk)\nclass ChunkingDataHandler(ABC, Generic[CleanedDocumentT, ChunkT]):\n    @property\n    def metadata(self) -> dict:\n        return {\n",
      "content_length": 2052,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "RAG Feature Pipeline\n166\n            \"chunk_size\": 500,\n            \"chunk_overlap\": 50,\n        }\n    @abstractmethod\n    def chunk(self, data_model: CleanedDocumentT) -> list[ChunkT]:\n        pass\nLet’s understand how the ArticleChunkingHandler() class is implemented. The first step is to \noverride the metadata property and customize the type of properties the chunking logic requires. \nFor example, when working with articles, we are interested in the chunk’s minimum and max-\nimum length.\nThe handler’s chunk() method inputs cleaned article documents and returns a list of article chunk \nentities. It uses the chunk_text() function to split the cleaned content into chunks. The chunking \nfunction is customized based on the min_length and max_length metadata fields. The chunk_id \nis computed as the MD5 hash of the chunk’s content. Thus, if the two chunks have precisely the \nsame content, they will have the same ID, and we can easily deduplicate them. Lastly, we create \na list of chunk entities and return them.\nclass ArticleChunkingHandler(ChunkingDataHandler):\n    @property\n    def metadata(self) -> dict:\n        return {\n            \"min_length\": 1000,\n            \"max_length\": 1000,\n        }\n    def chunk(self, data_model: CleanedArticleDocument) -> \nlist[ArticleChunk]:\n        data_models_list = []\n        cleaned_content = data_model.content\n        chunks = chunk_article(\n            cleaned_content, min_length=self.metadata[\"min_length\"], max_\nlength=self.metadata[\"max_length\"]\n        )\n        for chunk in chunks:\n",
      "content_length": 1545,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "Chapter 4\n167\n            chunk_id = hashlib.md5(chunk.encode()).hexdigest()\n            model = ArticleChunk(\n                id=UUID(chunk_id, version=4),\n                content=chunk,\n                platform=data_model.platform,\n                link=data_model.link,\n                document_id=data_model.id,\n                author_id=data_model.author_id,\n                author_full_name=data_model.author_full_name,\n                metadata=self.metadata,\n            )\n            data_models_list.append(model)\n        return data_models_list\nThe last step is to dig into the chunk_article() function, which mainly does two things:\n•\nIt uses a regex to find all the sentences within the given text by looking for periods, ques-\ntion marks, or exclamation points followed by a space. However, it avoids splitting into \ncases where the punctuation is part of an abbreviation or initialism (like “e.g.\" or “Dr.\")\n•\nIt groups sentences into a single chunk until the max_length limit is reached. When the \nmaximum size is reached, and the chunk size is bigger than the minimum allowed value, \nit is added to the final list the function returns.\ndef chunk_article(text: str, min_length: int, max_length: int) -> \nlist[str]:\n    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\", \ntext)\n    extracts = []\n    current_chunk = \"\"\n    for sentence in sentences:\n        sentence = sentence.strip()\n        if not sentence:\n            continue\n        if len(current_chunk) + len(sentence) <= max_length:\n            current_chunk += sentence + \" \"\n        else:\n",
      "content_length": 1583,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "RAG Feature Pipeline\n168\n            if len(current_chunk) >= min_length:\n                extracts.append(current_chunk.strip())\n            current_chunk = sentence + \" \"\n    if len(current_chunk) >= min_length:\n        extracts.append(current_chunk.strip())\n    return extracts\nThe PostChunkingHandler and RepositoryChunkingHandler, available on GitHub at llm_\nengineering/application/preprocessing/chunking_data_handlers.py, have a similar struc-\nture to the ArticleChunkingHandler. However, they use a more generic chunking function called \nchunk_text(), worth looking into. The chunk_text() function is a two-step process that has \nthe following logic:\n1. \nIt uses a RecursiveCharacterTextSplitter() from LangChain to split the text based on \na given separator or chunk size. Using the separator, we first try to find paragraphs in the \ngiven text, but if there are no paragraphs or they are too long, we cut it at a given chunk size.\n2. \nNotice that we want to ensure that the chunk doesn’t exceed the maximum input length \nof the embedding model. Thus, we pass all the chunks created above into a SenteceTrans\nformersTokenTextSplitter(), which considers the maximum input length of the model. \nAt this point, we also apply the chunk_overlap logic, as we want to do it only after we \nvalidate that the chunk is small enough.\n… # Other imports.\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, \nSentenceTransformersTokenTextSplitter\nfrom llm_engineering.application.networks import \nEmbeddingModelSingleton\ndef chunk_text(text: str, chunk_size: int = 500, chunk_overlap: int \n= 50) -> list[str]:\n    character_splitter = RecursiveCharacterTextSplitter(separato\nrs=[\"\\n\\n\"], chunk_size=chunk_size, chunk_overlap=0)\n    text_split_by_characters = character_splitter.split_text(text)\n    token_splitter = SentenceTransformersTokenTextSplitter(\n        chunk_overlap=chunk_overlap,\n",
      "content_length": 1902,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "Chapter 4\n169\n        tokens_per_chunk=embedding_model.max_input_length,\n        model_name=embedding_model.model_id,\n    )\n    chunks_by_tokens = []\n    for section in text_split_by_characters:\n        chunks_by_tokens.extend(token_splitter.split_text(section))\n    return chunks_by_tokens\nTo conclude, the function above returns a list of chunks that respect both the provided chunk \nparameters and the embedding model’s max input length.\nThe embedding handlers\nThe embedding handlers differ slightly from the others as the EmbeddingDataHandler() interface \ncontains most of the logic. We took this approach because, when calling the embedding model, \nwe want to batch as many samples as possible to optimize the inference process. When running \nthe model on a GPU, the batched samples are processed independently and in parallel. Thus, by \nbatching the chunks, we can optimize the inference process by 10x or more, depending on the \nbatch size and hardware we use.\nWe implemented an embed() method, in case you want to run the inference on a single data point, \nand an embed_batch() method. The embed_batch() method takes chunked documents as input, \ngathers their content into a list, passes them to the embedding model, and maps the results to an \nembedded chunk domain entity. The mapping is done through the map_model() abstract method, \nwhich has to be customized for every data category.\n… # Other imports.\nfrom typing import Generic, TypeVar, cast\nfrom llm_engineering.application.networks import EmbeddingModelSingleton\nChunkT = TypeVar(\"ChunkT\", bound=Chunk)\nEmbeddedChunkT = TypeVar(\"EmbeddedChunkT\", bound=EmbeddedChunk)\nembedding_model = EmbeddingModelSingleton()\nclass EmbeddingDataHandler(ABC, Generic[ChunkT, EmbeddedChunkT]):\n    \"\"\"\n    Abstract class for all embedding data handlers.\n",
      "content_length": 1805,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "RAG Feature Pipeline\n170\n    All data transformations logic for the embedding step is done here\n    \"\"\"\n    def embed(self, data_model: ChunkT) -> EmbeddedChunkT:\n        return self.embed_batch([data_model])[0]\n    def embed_batch(self, data_model: list[ChunkT]) -> \nlist[EmbeddedChunkT]:\n        embedding_model_input = [data_model.content for data_model in \ndata_model]\n        embeddings = embedding_model(embedding_model_input, to_list=True)\n        embedded_chunk = [\n            self.map_model(data_model, cast(list[float], embedding))\n            for data_model, embedding in zip(data_model, embeddings, \nstrict=False)\n        ]\n        return embedded_chunk\n    @abstractmethod\n    def map_model(self, data_model: ChunkT, embedding: list[float]) -> \nEmbeddedChunkT:\n        pass\nLet’s look only at the implementation of the ArticleEmbeddingHandler(), as the other handlers \nare highly similar. As you can see, we only have to implement the map_model() method, which \ntakes a chunk of input and computes the embeddings in batch mode. Its scope is to map this \ninformation to an EmbeddedArticleChunk Pydantic entity.\nclass ArticleEmbeddingHandler(EmbeddingDataHandler):\n    def map_model(self, data_model: ArticleChunk, embedding: list[float]) \n-> EmbeddedArticleChunk:\n        return EmbeddedArticleChunk(\n            id=data_model.id,\n            content=data_model.content,\n            embedding=embedding,\n            platform=data_model.platform,\n            link=data_model.link,\n",
      "content_length": 1493,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "Chapter 4\n171\n            document_id=data_model.document_id,\n            author_id=data_model.author_id,\n            author_full_name=data_model.author_full_name,\n            metadata={\n                \"embedding_model_id\": embedding_model.model_id,\n                \"embedding_size\": embedding_model.embedding_size,\n                \"max_input_length\": embedding_model.max_input_length,\n            },\n        )\nThe last step is to understand how the EmbeddingModelSingleton() works. It is a wrapper over \nthe SentenceTransformer() class from Sentence Transformers that initializes the embedding \nmodel. Writing a wrapper over external packages is often good practice. Thus, when you want \nto change the third-party tool, you have to modify only the internal logic of the wrapper instead \nof the whole code base.\nThe SentenceTransformer() class is initialized with the model_id defined in the Settings class, \nallowing us to quickly test multiple embedding models just by changing the configuration file \nand not the code. That is why I am not insisting at all on what embedding model to use. This \ndiffers constantly based on your use case, data, hardware, and latency. But by writing a generic \nclass, which can quickly be configured, you can experiment with multiple embedding models \nuntil you find the best one for you.\nfrom sentence_transformers.SentenceTransformer import SentenceTransformer\nfrom llm_engineering.settings import settings\nfrom .base import SingletonMeta\nclass EmbeddingModelSingleton(metaclass=SingletonMeta):\n    def __init__(\n        self,\n        model_id: str = settings.TEXT_EMBEDDING_MODEL_ID,\n        device: str = settings.RAG_MODEL_DEVICE,\n        cache_dir: Optional[Path] = None,\n    ) -> None:\n        self._model_id = model_id\n        self._device = device\n        self._model = SentenceTransformer(\n",
      "content_length": 1836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "RAG Feature Pipeline\n172\n            self._model_id,\n            device=self._device,\n            cache_folder=str(cache_dir) if cache_dir else None,\n        )\n        self._model.eval()\n    @property\n    def model_id(self) -> str:\n        return self._model_id\n    @cached_property\n    def embedding_size(self) -> int:\n        dummy_embedding = self._model.encode(\"\")\n        return dummy_embedding.shape[0]\n    @property\n    def max_input_length(self) -> int:\n        return self._model.max_seq_length\n    @property\n    def tokenizer(self) -> AutoTokenizer:\n        return self._model.tokenizer\n    def __call__(\n        self, input_text: str | list[str], to_list: bool = True\n    ) -> NDArray[np.float32] | list[float] | list[list[float]]:\n        try:\n            embeddings = self._model.encode(input_text)\n        except Exception:\n            logger.error(f\"Error generating embeddings for {self._model_\nid=} and {input_text=}\")\n            return [] if to_list else np.array([])\n        if to_list:\n",
      "content_length": 1007,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "Chapter 4\n173\n            embeddings = embeddings.tolist()\n        return embeddings\nThe embedding model class implements the singleton pattern (https://refactoring.guru/\ndesign-patterns/singleton), a creational design pattern that ensures a class has only one instance \nwhile providing a global access point to this instance. The EmbeddingModelSingleton() class inher-\nits from the SingletonMeta class, which ensures that whenever an EmbeddingModelSingleton()\nis instantiated, it returns the same instance. This works well with ML models, as you load them \nonce in memory through the singleton pattern, and afterward, you can use them anywhere in the \ncode base. Otherwise, you risk loading the model in memory every time you use it or loading it \nmultiple times, resulting in memory issues. Also, this makes it very convenient to access properties \nsuch as embedding_size, where you have to make a dummy forward pass into the embedding \nmodel to find the size of its output. As a singleton, you do this forward pass only once, and then \nyou have it accessible all the time during the program’s execution.\nSummary\nThis chapter began with a soft introduction to RAG and why and when you should use it. We \nalso understood how embeddings and vector DBs work, representing the cornerstone of any \nRAG system. Then, we looked into advanced RAG and why we need it in the first place. We built \na strong understanding of what parts of the RAG can be optimized and proposed some popular \nadvanced RAG techniques for working with textual data. Next, we applied everything we learned \nabout RAG to designing the architecture of LLM Twin’s RAG feature pipeline. We also understood \nthe difference between a batch and streaming pipeline and presented a short introduction to the \nCDC pattern, which helps sync two DBs.\nUltimately, we went step-by-step into the implementation of the LLM Twin’s RAG feature pipeline, \nwhere we saw how to integrate ZenML as an orchestrator, how to design the domain entities of \nthe application, and how to implement an OVM module. Also, we understood how to apply some \nsoftware engineering best practices, such as the abstract factory and strategy software patterns, \nto implement a modular and extendable layer that applies different cleaning, chunking, and \nembedding techniques based on the data category of each document.\nThis chapter focused only on implementing the ingestion pipeline, which is just one component \nof a standard RAG application. In Chapter 9, we will conclude the RAG system by implementing \nthe retrieval and generation components and integrating them into the inference pipeline. But \nfirst, in the next chapter, we will explore how to generate a custom dataset using the data we \ncollected and fine-tune an LLM with it. \n",
      "content_length": 2771,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "RAG Feature Pipeline\n174\nReferences\n•\nKenton, J.D.M.W.C. and Toutanova, L.K., 2019, June. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of naacL-HLT (Vol. 1, p. 2).\n•\nLiu, Y., 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arX-\niv:1907.11692.\n•\nMikolov, T., 2013. Efficient estimation of word representations in vector space. arXiv pre-\nprint arXiv:1301.3781.\n•\nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vec-\ntors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods \nin Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar. Association for \nComputational Linguistics.\n•\nHe, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In \nProceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n•\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., \nMishkin, P., Clark, J. and Krueger, G., 2021, July. Learning transferable visual models from \nnatural language supervision. In International conference on machine learning (pp. 8748-\n8763). PMLR.\n• \nWhat is Change Data Capture (CDC)? | Confluent. (n.d.). Confluent. https://www.confluent.\nio/en-gb/learn/change-data-capture/\n• \nRefactoring.Guru. (2024, January 1). Singleton. https://refactoring.guru/design-\npatterns/singleton\n• \nRefactoring.Guru. (2024b, January 1). Strategy. https://refactoring.guru/design-\npatterns/strategy\n• \nRefactoring.Guru. (2024a, January 1). Abstract Factory. https://refactoring.guru/\ndesign-patterns/abstract-factory\n• \nSchwaber-Cohen, R. (n.d.). What is a Vector Database & How Does it Work? Use Cases + Ex-\namples. Pinecone. https://www.pinecone.io/learn/vector-database/\n•\nMonigatti, L. (2024, February 19). Advanced Retrieval-Augmented Generation: From Theory \nto LlaMaIndex Implementation. Medium. https://towardsdatascience.com/advanced-\nretrieval-augmented-generation-from-theory-to-llamaindex-implementation-\n4de1464a9930\n•\nMonigatti, L. (2023, December 6). A guide on 12 tuning Strategies for Production-Ready \nRAG applications. Medium. https://towardsdatascience.com/a-guide-on-12-tuning-\nstrategies-for-production-ready-rag-applications-7ca646833439\n",
      "content_length": 2317,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "Chapter 4\n175\n•\nMonigatti, L. (2024b, February 19). Advanced Retrieval-Augmented Generation: From Theory \nto LlaMaIndex Implementation. Medium. https://towardsdatascience.com/advanced-\nretrieval-augmented-generation-from-theory-to-llamaindex-implementation-\n4de1464a9930\n•\nMaameri, S. (2024, May 10). Routing in RAG-Driven applications - towards data science. \nMedium. https://towardsdatascience.com/routing-in-rag-driven-applications-\na685460a7220\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 592,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "5\nSupervised Fine-Tuning\nSupervised Fine-Tuning (SFT) is a crucial step in preparing LLMs for real-world applications. Fol-\nlowing the initial pre-training phase, where an LLM learns to predict the next token in a sequence, \nSFT refines the model’s capabilities using carefully curated pairs of instructions and correspond-\ning answers. This process serves two primary purposes: it teaches the model to understand and \nfollow a specific chat format, effectively transforming it into a conversational agent, and it allows \nthe model to adapt its broad knowledge base to excel in targeted tasks or specialized domains.\nThe importance of SFT lies in its ability to bridge the gap between a model’s general language \nunderstanding and its practical utility. By exposing the model to examples of desired input-output \npatterns, SFT shapes the LLM’s behavior to align with specific goals, whether they involve task \ncompletion (such as summarization or translation) or domain expertise (like medical or legal \nknowledge). This tailored approach not only enhances the model’s performance in intended ar-\neas but also improves its ability to follow instructions and generate more relevant and coherent \nresponses.\nIn this chapter, we will cover the following topics:\n•\nCreating a high-quality instruction dataset\n•\nSFT techniques\n•\nImplementing fine-tuning in practice\nBy the end of this chapter, you will be able to create your own instruction datasets and efficiently \nfine-tune LLMs on them.\n",
      "content_length": 1487,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "Supervised Fine-Tuning\n178\nCreating an instruction dataset\nIn most use cases, creating an instruction dataset is the most difficult part of the fine-tuning \nprocess. This is due to multiple factors. Most use cases can be connected to raw text, but it is rare \nto find natural pairs of instructions and answers. This raw text needs to be transformed into a for-\nmat that includes both instructions and answers. Moreover, the quality of the data is also crucial. \nBecause of this, a lot of time is invested in manually checking and verifying individual samples. \nThis careful review helps ensure that the dataset is accurate and useful for training the model.\nFigure 5.1 – Overview of the post-training data pipeline covered in this chapter\nIn this section, we will introduce a general framework to create your own instruction datasets, \nregardless of the final use case. We will then leverage the scraped data from Chapter 3 and trans-\nform it into an instruction dataset. The different stages in our data generation pipeline are sum-\nmarized in Figure 5.1.\nGeneral framework\nInstruction datasets are defined as pairs of instructions and answers. The instructions are the \ninputs of the model, used as context during fine-tuning. The answers are the expected outputs of \nthe model. During fine-tuning, you can choose to train the model on the instructions and answers, \nor on answers only. Pairs of instructions and answers follow a certain template. Some instruction \ntemplates, such as Alpaca, introduce additional fields like inputs and system. Both of them can \nbe considered subfields of the instruction field. In this case, “inputs” contain the data the model \nneeds to complete the instruction, and “system” is a meta-prompt to steer the general behavior \nof the model. Here is an example from the SlimOrca dataset, with “system” and “instruction”:\nAll the code examples from this chapter can be found on GitHub at https://github.\ncom/PacktPublishing/LLM-Engineering.\n",
      "content_length": 1974,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "Chapter 5\n179\nSystem\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a \nfive year old.\nInstruction\nConcepts: building, shop, town\nWrite a sentence that includes all these words.\nOutput\nIn our little town, there is a shop inside a big building where people go to buy their favorite toys \nand candies.\nTable 5.1 – Example of sample from the Open-Orca/SlimOrca dataset\nThis example illustrates how the “system” field is used to define specific behaviors for the model, \nsuch as being helpful, always providing explanations, and tailoring responses as if speaking to \na five-year-old. The “instruction” field provides the necessary data (the concepts) and the task \n(constructing a sentence). The output field shows the expected answer, which, while not the only \npossible answer, represents a high-quality response.\nTo build an instruction dataset, we want to curate data that is representative of how the model will \nbe used. Once we have gathered enough samples, our goal is to filter them to only keep high-quality \ndata. In this context, high-quality data can be described through three main dimensions:\n• \nAccuracy: It refers to the factual correctness and relevance of the samples. In the context \nof instruction datasets, this means ensuring that responses are not only factually accu-\nrate but also relevant to their corresponding instructions. High accuracy is essential for \ntraining models that can provide reliable and trustworthy information.\n•\nDiversity: A high-quality dataset should encompass a wide range of use cases, covering \nthe potential queries and tasks the deployed LLM might encounter. This diversity should \nspan topics, contexts, text lengths, and writing styles. By sampling data in a representative \nmanner, we allow models to develop robust instruction-following capabilities.\n•\nComplexity: Trivial or overly simplistic samples do little to improve an LLM’s capabilities. \nInstead, datasets should include complex, multi-step reasoning problems and challenging \ntasks that push the boundaries of what the model is expected to handle. This complexity \nhelps in developing models capable of tackling complex real-world problems.\n",
      "content_length": 2200,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "Supervised Fine-Tuning\n180\nIn the following sections, we will see techniques to filter and evaluate instruction samples ac-\ncording to these dimensions.\nData quantity\nThe Hugging Face Hub contains numerous instruction datasets, which can be general-purpose or \ndesigned for particular tasks or domains. When working on a new use case, it can be beneficial to \nlook for related open-source datasets to leverage for fine-tuning. This is particularly important \nif your number of samples is too low (for example, fewer than 1,000), requiring you to augment \nit with high-quality data.\nFigure 5.2 – Screenshot of the most-liked datasets on the Hugging Face Hub\nCalculating an ideal number of samples is a difficult task, as both the quality of the data and the \nsize of the model can have a dramatic impact. For large models (around 70 billion parameters, \nfor example), this number can be as low as 1,000 high-quality samples (see the LIMA paper in \nthe References section). This is not true for smaller models (around seven billion parameters, for \ninstance), as they need more samples to simply learn the correct chat template. In any case, the \nquality of the data is a crucial factor, and a high number of samples is always desirable.\nTo provide additional numbers, we can look at the fine-tuned models developed by companies \nand the open-source community. We can distinguish two types of finetunes: general-purpose, \naimed to reproduce the capabilities of models like GPT, and task- or domain-specific models, \ndesigned to optimize their performance for a particular application.\n",
      "content_length": 1583,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "Chapter 5\n181\nGeneral-purpose models cover more topics, which requires additional samples. Among com-\npanies, we observe a wide range of values. For instance, Yi models from 01-ai rely on less than \n10,000 samples. At the opposite range of the spectrum, Meta reported using 10 million samples \nfor Llama 3 through the entire fine-tuning process (including preference alignment). In the open-\nsource community, models like OpenHermes and Dolphin use around one million samples. Based \non the quality of these finetunes, we recommend an instruction dataset of at least one million \nsamples to create a good general-purpose instruct model. On the other hand, models fine-tuned \nfor a specific purpose require fewer samples. Here, we differentiate task-specific models from \ndomain-specific ones.\nTask-specific and domain-specific models represent two distinct approaches to fine-tuning LLMs. \nTask-specific models are designed to excel at a particular function, such as translation, summari-\nzation, or sentiment analysis. These models benefit from a focused training approach on a single \ntask, allowing for efficient performance even with smaller model sizes (typically less than 8 bil-\nlion parameters). The data required for task-specific fine-tuning is generally more manageable, \nranging from 100 to 100,000 samples. This makes task-specific fine-tuning an attractive option\nfor many applications where resources may be limited.\nDomain-specific models, on the other hand, aim to tweak the LLM with specialized knowledge \nand familiarity with the vocabulary and linguistic patterns of a particular field. These models \nare valuable in areas such as medicine, law, finance, e-commerce, engineering, and hospitality. \nThe data requirements for domain-specific fine-tuning can vary widely depending on the com-\nplexity and breadth of the domain. Some fields, like medicine or law, may require as much data \nas general-purpose fine-tuning due to their vast technical corpora. Others, such as e-commerce \nor hospitality, might need fewer samples, more in line with task-specific fine-tuning.\nThe key factors determining the data needs for domain-specific models are the “size” of the \ndomain (i.e., the extent of its specialized knowledge and vocabulary) and the representation of \nthat domain in the model’s pre-training data. Domains that are well-represented in the original \ntraining data may require less fine-tuning, while those that are more specialized or underrep-\nresented may need more extensive datasets. Even with open-source LLMs, many pre-training \ndatasets are closed-source, which requires making educated guesses to determine their compo-\nsition (e.g., 30% code or 20% math).\n",
      "content_length": 2691,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "Supervised Fine-Tuning\n182\nData curation\nWhen it comes to procuring data for fine-tuning, the approaches differ between task-specific and \ndomain-specific models. For task-specific models, data curation often involves collecting examples \nof the desired task from existing datasets or creating new ones. This might involve gathering pairs \nof original and summarized texts for a summarization model or collecting sentences in different \nlanguages for a translation model.\nDomain-specific data curation can be more challenging. It often requires collaboration with sub-\nject matter experts to gather and validate relevant texts, research papers, technical documents, \nand other domain-specific content. In some cases, it may involve partnering with organizations \nor institutions that have access to large repositories of specialized information. The quality and \nrelevance of this data is crucial, as it directly impacts the model’s ability to understand and gen-\nerate content in the target domain.\nIt’s worth noting that few-shot prompting has emerged as an alternative strategy to fine-tuning, \nespecially for task-specific applications. This approach leverages the capabilities of large, pow-\nerful models by providing a few examples of the desired task within the input prompt. While \nnot a replacement for fine-tuning in all scenarios (e.g., when you want to learn a new domain), \nfew-shot prompting can be an efficient way to adapt models to new tasks without the need for \nextensive additional training.\nIn practice, the line between task-specific and domain-specific models can sometimes blur. For \ninstance, a model fine-tuned for medical diagnosis could be considered both task-specific (focused \non diagnosis) and domain-specific (specialized in medical knowledge). The key is to understand \nthe primary goal of the fine-tuning process and tailor the approach accordingly.\nAt this point in the process, we should have a collection of datasets suited for our use case. The \nnext step consists of refining the quality of the samples through rule-based filtering, data dupli-\ncation, data decontamination, and data quality evaluation.\nRule-based filtering\nRule-based filtering is a systematic approach to data quality control that relies on explicit, pre-\ndefined rules to evaluate and filter data samples. These rules are typically designed to address \ncommon quality issues and can range from simple checks to more complex logical operations. The \nprimary goal of rule-based filtering is to maintain a high standard of data quality by removing \nsamples that do not meet specific criteria.\n",
      "content_length": 2600,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "Chapter 5\n183\nLength filtering is a straightforward yet effective rule-based filtering technique. This meth-\nod involves setting thresholds for the acceptable length of responses in the dataset. Extremely \nshort responses often lack sufficient information to be meaningful, while excessively long ones \nmay contain irrelevant or redundant content. It’s important to note that the appropriate length \nthresholds can vary significantly depending on the specific task and domain. For example, a \ndataset for generating concise summaries might have a lower maximum threshold compared to \none for detailed explanations.\nKeyword exclusion is another powerful rule-based filtering technique that focuses on the content \nof the samples rather than their structure. This method involves creating a list of keywords or \nphrases associated with low-quality or inappropriate content, and then filtering out any samples \nthat contain these terms. The keyword list can include obvious indicators of low quality, such \nas profanities or spam-related terms, as well as domain-specific words that might indicate ir-\nrelevant or off-topic content. For instance, in a dataset for a professional writing assistant, you \nmight exclude samples containing slang terms or informal expressions that don’t align with the \nintended tone and style.\nFormat checking is recommended for datasets that include structured data or follow specific \nformatting requirements. This technique ensures that all samples adhere to the expected format, \nmaintaining consistency and facilitating processing downstream. Format checking can be par-\nticularly important for datasets containing code samples, JSON structures, or other formatted \ntext. For example, in a dataset of programming instructions and solutions, you might implement \nrules to verify that code samples are syntactically correct and follow specified style guidelines.\nRule-based filtering offers significant advantages in preparing instruction datasets. Its speed \nand efficiency allow for rapid application to large volumes of data, making it highly scalable. The \nconsistency of rule application ensures uniform treatment of data, reducing human error and bias. \nFurthermore, the explicit definition of filtering criteria provides transparency and interpretability, \nfacilitating easy understanding, auditing, and adjustment. The ability to automate rule-based \nfiltering reduces the need for manual intervention and enables continuous data quality monitoring.\nHowever, rule-based filtering also has limitations that must be considered. Predefined rules may \nlack the nuance required to capture the full complexity of language and context, potentially lead-\ning to the removal of valid but unusual samples. The typically binary nature of rules (pass/fail) \nmay not always align with the nuanced nature of language and instruction quality. Additionally, \nas data patterns and quality standards evolve, rules need regular review and updates to remain \neffective. There’s also a risk that poorly designed rules could inadvertently introduce or amplify \nbiases in the dataset.\n",
      "content_length": 3100,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "Supervised Fine-Tuning\n184\nData deduplication\nDataset diversity is fundamental to training models that can generalize well to new, unseen data. \nWhen a dataset contains duplicates or near-duplicates, it can lead to several issues:\n• \nOverfitting: Models may memorize specific examples rather than learning general patterns.\n•\nBiased performance: Overrepresented data points may skew the model’s performance \ntowards certain types of inputs.\n•\nInefficient training: Redundant data can increase training time without providing addi-\ntional valuable information.\n•\nInflated evaluation metrics: Duplicate data in test sets may lead to overly optimistic per-\nformance estimates.\nTo deduplicate datasets, we distinguish between exact and fuzzy deduplication. Exact deduplica-\ntion removes identical samples through a straightforward process involving data normalization, \nhash generation, and duplicate removal. Data normalization standardizes the format of entries, \nsuch as converting text to lowercase. Hash generation then creates unique hashes for each entry \nusing algorithms like MD5 or SHA-256. These hashes are compared to find matches, and dupli-\ncates are removed, leaving only one instance of each. While effective for identical entries, exact \ndeduplication does not detect near-duplicates or semantically similar content, requiring more \nadvanced techniques for those cases.\nThe most popular approach to fuzzy deduplication is MinHash deduplication. Compared to \nother fuzzy techniques, it maintains high accuracy while significantly reducing computational \ncomplexity. MinHash operates by generating compact representations, or signatures, for each \ndata item. These signatures serve as fingerprints that capture the essence of the data while dras-\ntically reducing its dimensionality. In practice, MinHash transforms data items (such as text \ndocuments) into sets of shingles, applies multiple hash functions to these sets, and selects the \nminimum hash values to form signature vectors. These signatures can then be compared using \nsimilarity measures like Jaccard similarity to efficiently identify near-duplicates.\nIn addition to exact and fuzzy deduplication, semantic similarity takes a different approach by \nfocusing on the meaning of text for deduplication. This method involves converting words or \nentire samples into vector representations using various natural language processing techniques. \nWord embedding models such as Word2Vec, GloVe, and FastText transform individual words into \ndense vectors, capturing semantic relationships. \n",
      "content_length": 2559,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "Chapter 5\n185\nFor more context-aware representations, language models like BERT, sentence transformers, or \ncross-encoders can generate embeddings for entire sentences or documents. Once these vec-\ntor representations are obtained, deduplication can be performed by comparing the similarity \nbetween vectors. Common similarity measures include cosine similarity or Euclidean distance. \nSamples with high similarity scores above a predefined threshold can be considered duplicates. \nFor large datasets, clustering techniques may be applied to group similar vectors. Methods like \nK-means, DBSCAN, or hierarchical clustering can efficiently organize the vector space, allowing \nfor the identification of clusters that represent semantically similar content. Within each cluster, \na representative sample can be retained while others are marked as duplicates.\nData decontamination\nData decontamination is the process of ensuring that the training dataset does not contain samples \nthat are identical or highly similar to those in the evaluation or test sets. This step is important \nfor ensuring the quality of the model evaluation and preventing overfitting or memorization of \ntest data.\nData decontamination uses techniques from data deduplication. Exact matching can be used to \nremove any training samples that are identical to those in the evaluation sets. This can be done \nusing hash functions or direct string comparisons. Next, we can also use near-duplicate detection \nmethods to identify and remove training samples that are very similar to evaluation samples, \neven if they are not exactly the same. This often involves techniques like MinHash or computing \nsimilarity scores based on n-grams or embeddings.\nAnother aspect of data decontamination is filtering out samples that may have been derived from \nthe same source as evaluation data. This can involve checking for overlapping phrases, similar \nsentence structures, or common metadata. Practitioners may also use provenance tracking (source \nthe data they use) to identify and exclude data from specific sources that are known to be used \nin evaluation sets.\nA simple way to perform data decontamination is to add your evaluation set to the \ninstruction dataset during the data deduplication stage. In this case, we want to \nensure that we only remove samples from the instruction dataset, which can be \nimplemented in different ways (only filtering out the first duplicate, recording the \nindexes of the evaluation samples, etc.). Ideally, you can automatically add your \nevaluation sets in the data deduplication stage to fully automate this process. This \nis particularly efficient if you iterate over several versions of custom benchmarks.\n",
      "content_length": 2710,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "Supervised Fine-Tuning\n186\nData quality evaluation\nData quality evaluation is a critical aspect of machine learning, particularly for LLMs. The process \ninvolves assessing various characteristics of datasets, including accuracy, diversity, and complexity. \nWhile some aspects like mathematical accuracy can be easily verified using tools such as Python \ninterpreters, evaluating subjective or open-ended content remains challenging.\nTraditional methods of data quality assessment include human annotation, which generally \nprovides high accuracy but is resource-intensive. To address scalability issues, machine learning \ntechniques have been developed to automate the evaluation process. These include using LLMs \nas judges, reward models, and classifiers trained for quality prediction.\nThe LLM-as-a-judge strategy involves prompting LLMs to evaluate the quality of each sample. \nThis approach has become popular due to its flexibility and ease of use, though it does present \nsome challenges. Different LLMs have different levels of performance across tasks, and their \nevaluations often align more closely with those of non-experts. With domain-specific datasets, \nyou might want to use domain-specific models instead of better, general-purpose LLMs. Com-\nparative assessment methods (e.g., “Is answer A better than answer B?”) generally outperform \nabsolute scoring approaches (e.g., “Rate answer A between 1 and 4”), though both can be used \nat scale with sufficient prompt engineering. We recommend iterating through different prompts \nover a representative subset to manually verify the quality of the responses. Table 5.2 shows an \nexample of a custom prompt for a judge LLM.\n",
      "content_length": 1685,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "Chapter 5\n187\nInstruction\nYou are a data quality evaluator. Your goal is to assess an instruction and its corresponding \nanswer, determining how effectively the answer addresses the given task.\nIn your evaluation, you will provide feedback detailing the strengths and weaknesses of the \nanswer, followed by a score on a scale of 1 to 4.\nA score of 1 means that the answer is terrible and irrelevant to the instruction.\nA score of 2 means that the answer is not helpful and misses important aspects of the \ninstruction.\nA score of 3 means that the answer is helpful but could be improved in terms of relevance, \naccuracy, and depth.\nA score of 4 means that the answer is excellent and fully addresses the task.\nProvide your evaluation as follows:\nFeedback: (strengths and weaknesses you find relevant)\nScore: (number between 1 and 4)\nTable 5.2 – Example of LLM-as-a-judge prompt for data quality evaluation\nLLM-as-a-judge is known to have several biases. First, it has a position bias in comparative scoring, \nwhere the LLM judge favors the first answer. This can be addressed by randomizing the order of \nanswers A and B. In addition, like humans, LLM judges favor long answers. Length normaliza-\ntion techniques can be applied to absolute scoring to mitigate this issue. Finally, LLM judges are \nknown to have intra-model favoritism, meaning that they prefer models from the same family \n(GPT-4o with GPT-4 and GPT-4o mini, for example). This can be addressed by using several \nmodels instead of a single one.\n",
      "content_length": 1511,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "Supervised Fine-Tuning\n188\nIn general, to improve evaluation reliability, strategies such as using multiple LLMs as a jury \nreduce bias and improve consistency. Leveraging a jury of smaller LLMs can also reduce costs \nwhile increasing accuracy and mitigating intra-model favoritism. For specific applications like \nchatbots, it’s advisable to aim for high agreement between LLM judges and human evaluators \n(around 80%). Simple grading scales (with few-shot prompting) and task-specific benchmarks \nare also recommended to ensure relevant and interpretable evaluations.\nReward models are another way to re-purpose LLMs for data quality evaluation. The term “reward \nmodel” comes from Reinforcement Learning from Human Feedback (RLHF, see Chapter 6). They \ncan be broadly defined as models that take an instruction and answer pair and return a score as \noutput. Generally, reward models are created by adding a linear head on top of a decoder-only \narchitecture like Gemma or Llama. They are then trained for this specific purpose, using either \nreinforcement learning or traditional fine-tuning. Figure 5.3 shows ArmoRM-Llama3-8B-v0.1’s \narchitecture, which adds regression and gating layers on top of a Llama 3 8B model. This model \noutputs multiple scores to target specific dimensions, such as helpfulness, correctness, coherence, \ncomplexity, and verbosity. This allows for a more fine-grained approach to data quality evaluation.\nFigure 5.3 – Architecture of RLHFlow/ArmoRM-Llama3-8B-v0.1, based on Llama 3 (Source: \nhttps://doi.org/10.48550/arXiv.2406.12845)\n",
      "content_length": 1565,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "Chapter 5\n189\nThe Allen Institute for AI’s RewardBench leaderboard, hosted on Hugging Face (allenai/re-\nward-bench), is a good resource for comparing different reward models. It combines various \ntypes of reward models (generative, classifiers, DPO, etc.) and evaluates them on a curated set \nof chosen and rejected answers for each instruction. While this task is not directly related to in-\nstruction data quality, it is a good resource for finding models capable of differentiating between \ngood and bad answers.\nClassifiers or encoder-only models can be trained to perform data quality evaluation. A good \nexample is HuggingFaceFW/fineweb-edu-classifier, a classifier designed to judge the educational \nvalue of web pages. This model was designed as a quality filter for pretraining data but a similar \napproach can be taken to evaluate instruction samples at scale. In practice, fineweb-edu-classifier \nadds a classification head to an embedding model (Snowflake/snowflake-arctic-embed-m) and \ntrains it for 20 epochs on 450,000 samples that are annotated by Llama 3 70B Instruct.\nThis approach relies on encoder-only models, which are both smaller and better suited to classi-\nfication tasks. Thanks to their low number of parameters, these models are faster to run and can \nscale to millions of samples. However, they are not as accurate as bigger models, particularly for \ncomplex reasoning tasks where they lack the ability to capture nuances. At smaller scale, encod-\ner-only models are still valuable to filter out outliers or as part of an automated data pipeline, \nwhich requires faster processing.\nData exploration\nData exploration is a continuous process that requires practitioners to become familiar with the \ntraining data. It involves both manual inspection and automated analysis, each playing a crucial \nrole in understanding the dataset’s characteristics, strengths, and potential shortcomings.\nManual dataset exploration, though time-consuming, is an important step. It reveals errors and \ninconsistencies that automated processes might miss, including formatting issues, data entry \nmistakes, incoherent reasoning, and factual inaccuracies. This process provides qualitative insights \ninto the dataset’s content and style. To enhance efficiency, researchers can employ techniques \nlike stratified sampling (selecting diverse samples), systematic review (using a criteria checklist), \nand collaborative review (involving multiple reviewers). \n",
      "content_length": 2466,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "Supervised Fine-Tuning\n190\nFigure 5.4 shows an example with Argilla, a collaborative platform for manual data quality eval-\nuation and exploration.\nFigure 5.4 – Argilla’s interface for collaborative data quality evaluation and exploration\nStatistical analysis is a complementary technique that reveals vocabulary diversity, potential \nbiases, and concept representation. This process utilizes natural language processing libraries \nlike NLTK or spaCy for tokenization and analysis of large text volumes. Visualization tools such as \nMatplotlib or Seaborn create histograms and word clouds, enabling intuitive pattern recognition. \nThese techniques provide insights into dataset composition, language breadth, and possible \ncultural or contextual preferences, which can influence model outputs.\nTopic clustering automatically groups similar documents or pieces of text together, revealing \nunderlying themes and patterns within the data. This process is especially important for under-\nstanding the content of large text corpora, identifying trends, and organizing information in a \nmeaningful way. It is often associated with data visualization, with figures that show clusters \nof similar samples.\nLet’s consider the task of building an instruction dataset about various programming languages. \nYou have collected a vast corpus of programming-related text from online forums, documentation, \nand tutorials. First, topic clustering can help identify the distinct programming languages present \nin the dataset (Python, JavaScript, etc.). Second, within each language cluster, you can further \nidentify sub-topics like error handling, data structures, and web frameworks. This allows a \nbalanced representation of each language and sub-topic in the corpus. \n",
      "content_length": 1756,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "Chapter 5\n191\nThis makes sure that each topic is correctly covered for each programming language.\nFigure 5.5 – Representation of the historical TikTok dataset made with Nomic Atlas\nSeveral tools are available for performing topic clustering, each with its own strengths and ap-\nproaches. For example, Hugging Face’s text-clustering provides a simple pipeline with sentence \ntransformers for embedding text into vector space, UMAP for dimensionality reduction, and \nDBSCAN for clustering. It also automatically labels clusters using an LLM and can output visu-\nalizations. Nomic Atlas (see Figure 5.5), BunkaTopics, and Lilac are alternatives proposing similar \napproaches with additional features.\nData generation\nWhen the available instruction datasets are not sufficient, creating custom data becomes necessary. \nThis is particularly relevant for specialized applications where publicly available data is scarce. \n",
      "content_length": 916,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "Supervised Fine-Tuning\n192\nAdditionally, it serves as a method to augment underrepresented areas in a dataset, like insufficient \nexamples of JavaScript error-handling techniques in our previous example. While data can be \ngenerated manually by individuals or through crowdsourcing, these approaches often incur sig-\nnificant costs and time investments. Synthetic data generation using LLMs offers a more efficient \nand scalable alternative. This method, when combined with well-designed prompt engineering, \ncan produce high-quality data at a much larger scale, effectively addressing the limitations of \nmanual data creation processes.\nThe process of synthetic data generation typically begins with the preparation of a set of carefully \ndesigned prompts (sometimes called taxonomy). These serve as the foundation for generating \nnew, diverse examples. Five seed prompts used in the original Alpaca dataset can be seen in Table \n5.3. The quality of synthetically generated data largely depends on the prompts and techniques \nused in the generation process. Well-crafted prompts can guide the language model to produce \ndiverse, relevant, and high-quality instruction-response pairs. These prompts often include spe-\ncific instructions, examples, and constraints to ensure the generated data aligns with the desired \nformat and content.\nSeed instructions\n•\nIs there anything I can eat for breakfast that doesn’t include eggs, yet includes protein, \nand has roughly 700-1000 calories?\n•\nWhat is the relation between the given pairs? Input: Night : Day :: Right : Left\n•\nGenerate a one-sentence description for each of the following people. Input: -Barack \nObama\\n- Elon Musk\\n- Taylor Swift\n•\nDescribe a situation in which the given stereotype can harm you. Input: All Asians are \nsmart!\n•\nGenerate an appropriate subjective title for the following email: Input: “Hi [person \nname],\\n\\nI’m writing to ask you if you are happy to be a panelist in our workshop on \nmultimodality at CVPR. The workshop will be held on June 20, 2023. \\n\\nBest,\\n[my \nname]\nTable 5.3 – Examples of seed prompts used in the original Alpaca dataset\nMany synthetic data generation pipelines incorporate multiple steps to ensure data quality. This \nmay include generating an initial set of questions or instructions, followed by generating corre-\nsponding answers or responses. Some systems also implement validation steps, where another \nmodel or set of rules checks the generated pairs for accuracy, relevance, and adherence to spec-\nified criteria.\n",
      "content_length": 2526,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "Chapter 5\n193\nAn important aspect of synthetic data generation is the ability to control various attributes of the \ngenerated data. This includes factors such as the complexity of the instructions, the length of the \nresponses, the tone or style of the language used, and the specific topics or domains covered. By \nfine-tuning these parameters, it’s possible to create datasets that are tailored to specific training \nobjectives or that complement existing datasets in targeted ways. Structured generation using \nlibraries like Outlines can also be beneficial to adhere to specific formats.\nFurthermore, synthetic data generation can be particularly useful for addressing biases and gaps \nin existing datasets. By carefully designing the generation process, it’s possible to create more \nbalanced and inclusive datasets that represent a wider range of perspectives, topics, and language \nstyles. This can help in training LLMs that are more equitable and capable of serving diverse user \nbases.\nHowever, synthetic data generation also comes with challenges. One primary concern is the \npotential for the generated data to inherit biases or errors from the underlying language model \nused for generation. To mitigate this, many approaches incorporate human oversight, diverse \nprompts, and additional filtering mechanisms to ensure the quality and appropriateness of the \ngenerated data.\nAnother consideration is the need for the generated data to be sufficiently diverse and challeng-\ning. If the synthetic data is too simplistic or repetitive, it may not provide the level of complexity \nrequired to train a robust LLM. Advanced techniques in synthetic data generation often focus on \ncreating varied and nuanced instruction-response pairs that can push the boundaries of what \nthe model can learn.\nData augmentation\nIn this context, data augmentation refers to the process of increasing both the quantity and \nthe quality of data samples. Unlike data generation, we use pre-existing instruction samples \nas inputs in this stage. While it is possible to upsample pairs of instructions and answers, data \naugmentation is mostly used to increase the quality of existing samples. In particular, it focuses \non two aspects: diversity and complexity.\nA pioneering approach in this field is the Evol-Instruct method, which uses LLMs to evolve simple \ninstructions into more qualitative ones. The evolved instructions can then be used to generate \nanswers using powerful LLMs. This method employs two main strategies: in-depth and in-breadth \nevolving.\n",
      "content_length": 2548,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "Supervised Fine-Tuning\n194\nIn-depth evolving focuses on enhancing the complexity of existing instructions. It includes \nseveral techniques:\n•\nConstraints: It involves introducing additional requirements or limitations to the original \ninstruction, making it more challenging to fulfill.\n•\nDeepening: Instead of shallow questions, it tries to find more deep questions, requiring \nmore comprehensive responses.\n•\nConcretizing: It replaces general concepts with more specific ones, adding detail and \nprecision to the instruction.\n•\nIncreasing reasoning steps: It modifies instructions to explicitly request multiple-step \nreasoning, promoting more complex problem-solving.\n•\nComplicating input: This involves adding more complex data formats or structures to \nthe instruction, such as XML, JSON, or code snippets.\nIn-breadth evolving, on the other hand, aims to expand the diversity of the instruction dataset. \nIt generates entirely new instructions inspired by existing ones, focusing on creating more rare \nor long-tailed examples within the same domain.\nAs an example of concrete implementation, in-depth evolving can be automated with the fol-\nlowing prompt, from the AutoEvol paper. You simply need to provide the instruction you want \nto evolve as input, and a powerful model like GPT-4o will return a more complex version of the \noriginal instruction.\n",
      "content_length": 1358,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "Chapter 5\n195\nYou are an Instruction Rewriter that rewrites the given #Instruction# into a more complex \nversion. Please follow the steps below to rewrite the given “#Instruction#” into a more complex \nversion.\n•\nStep 1: Please read the “#Instruction#” carefully and list all the possible methods \nto make this instruction more complex (to make it a bit harder for well-known AI \nassistants such as ChatGPT and GPT4 to handle). Please do not provide methods to\n•\nchange the language of the instruction!\n•\nStep 2: Please create a comprehensive plan based on the #Methods List# generated \nin Step 1 to make the #Instruction# more complex. The plan should include several \nmethods from the #Methods List#.\n•\nStep 3: Please execute the plan step by step and provide the #Rewritten Instruction#. \n#Rewritten Instruction# can only add 10 to 20 words into the “#Instruction#”.\n•\nStep 4: Please carefully review the #Rewritten Instruction# and identify any \nunreasonable parts. Ensure that the #Rewritten Instruction# is only a more complex \nversion of the #Instruction#. Just provide the #Finally Rewritten Instruction# without \nanyexplanation.\nPlease reply strictly in the following format:\nStep 1 #Methods List#:\nStep 2 #Plan#:\nStep 3 #Rewritten Instruction#:\nStep 4 #Finally Rewritten Instruction#:\n#Instruction#:\n{Instruction}\nTable 5.4 – Evol LLM prompt from the “Automatic Instruction Evolving for Large Language \nModels” paper by Zeng et al. (2024)\nThe UltraFeedback method is another innovative approach, focused on answer quality instead \nof instruction quality. It employs AI feedback to enhance the quality and diversity of model re-\nsponses. Unlike Evol-Instruct, which evolves instructions, UltraFeedback uses a large pool of \ndiverse instructions and models to generate a wide range of responses.\n",
      "content_length": 1804,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "Supervised Fine-Tuning\n196\nIt then leverages advanced language models like GPT-4 to provide detailed critiques and numerical \nscores for these responses across multiple dimensions such as instruction-following, truthfulness, \nhonesty, and helpfulness.\nBased on these ideas, you can create your own augmentation techniques to create a more challeng-\ning and diverse instruction dataset. By refining and evolving existing instructions and answers, \nthe resulting dataset can better train models to handle complex, multi-step tasks, and improve \ntheir performance across a wider range of applications.\nCreating our own instruction dataset\nIn this section, we will create our own instruction dataset based on the crawled data from Chapter \n3. To create a high-quality instruction dataset, we need to address two main issues: the unstruc-\ntured nature of our data and the limited number of articles we can crawl.\nThis unstructured nature comes from the fact that we are dealing with raw text (articles), instead \nof pairs of instructions and answers. To address this issue, we will use an LLM to perform this \ntransformation. Specifically, we will employ a combination of backtranslation and rephrasing. \nBacktranslation refers to the process of providing the expected answer as output and generat-\ning its corresponding instruction. However, using a chunk of text like a paragraph as an answer \nmight not always be appropriate. This is why we want to rephrase the raw text to ensure we’re \noutputting properly formatted, high-quality answers. Additionally, we can ask the model to \nfollow the author’s writing style to stay close to the original paragraph. While this process in-\nvolves extensive prompt engineering, it can be automated and used at scale, as we will see in the \nfollowing implementation.\nOur second issue regarding the limited number of samples is quite common in real-world use \ncases. The number of articles we can retrieve is limited, which constrains the size of the instruction \ndataset we are able to create. In this example, the more samples we have, the better the model \nbecomes at imitating the original authors. To address this problem, we will divide our articles \ninto chunks and generate three instruction-answer pairs for each chunk. This will multiply the \nnumber of samples we create while maintaining diversity in the final dataset. For simplicity, we \nwill do it using OpenAI’s GPT-4o-mini model, but you can also use open-source models.\nHowever, LLMs are not reliable when it comes to producing structured output. Even when given \nspecific templates or instructions, there’s no guarantee that the model will consistently adhere \nto them. This inconsistency often necessitates additional string parsing to ensure the output \nmeets the desired format. \n",
      "content_length": 2784,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "Chapter 5\n197\nTo simplify this process and ensure properly structured results, we can employ structured gen-\neration techniques. Structured generation is an effective method to force an LLM to follow a \npredefined template, such as JSON, pydantic classes, or regular expressions. In the following, we \nwill use OpenAI’s JSON mode feature, which provides a more robust way to return valid JSON \nobjects and reduce the need for extensive post-processing.\nBased on this description, the following figure summarizes every step of the synthetic data pipe-\nline we want to build.\nFigure 5.6 – Synthetic data generation pipeline from raw text to instruction dataset\nLet’s now implement it in Python. You can implement it as part of the LLMOps pipeline, or as a \nstandalone script:\n1. \nWe want to make sure that the following libraries are installed. The OpenAI library will \nallow us to interact with a model to generate the instruction data, and datasets will format \nit into a Hugging Face-compatible format. The tqdm library is installed to visualize the \nprogress during the data generation process.\nopenai==1.37.1\ndatasets==2.20.0\ntqdm==4.66.4\n",
      "content_length": 1142,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "Supervised Fine-Tuning\n198\n2. We import all the required libraries as follows.\nimport concurrent.futures\nimport json\nimport random\nimport re\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import List, Tuple\nfrom datasets import Dataset\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom tqdm.auto import tqdm\n3. \nThe raw data we have is a JSON file. We create a Hugging Face dataset from this JSON file \nby extracting specific fields from each article: id, content, platform, author_id, author \nname, and link.\ndef load_articles_from_json(file_path: str) -> Dataset:\n    with open(file_path, \"r\") as file:\n        data = json.load(file)\n    return Dataset.from_dict(\n        {\n            \"id\": [item[\"id\"] for item in data[\"artifact_data\"]],\n            \"content\": [item[\"content\"] for item in data[\"artifact_\ndata\"]],\n            \"platform\": [item[\"platform\"] for item in \ndata[\"artifact_data\"]],\n            \"author_id\": [item[\"author_id\"] for item in \ndata[\"artifact_data\"]],\n            \"author_full_name\": [item[\"author_full_name\"] for item \nin data[\"artifact_data\"]],\n            \"link\": [item[\"link\"] for item in data[\"artifact_\ndata\"]],\n        }\n    ) \n",
      "content_length": 1196,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "Chapter 5\n199\nIf we simply load our dataset as a pandas dataframe, it returns the following table.\nid\ncontent\nplatform\nauthor_id\nauthor_\nfull_\nname\nlink\n0\nab2f9e2e-\n5459-4dd6-\n97d6-\nc291de4a7093\nThe Impor-\ntance of Data \nPipelines in \nthe Era of...\nmedium\ne6b945ba-\n6a9a-\n4cde-b2bf-\n0890af79732b\nAlex Vesa\nhttps://medium.\ncom/decodingml/\nt\nh\ne\n-\nimportance-o...\n1\nccfe70f3-\nd324-\n40b6-ba38-\n86e72786dcf4\nChange Data \nCapture: \nEnabling \nEvent-Driven \nArc...\nmedium\ne6b945ba-\n6a9a-\n4cde-b2bf-\n0890af79732b\nAlex Vesa\nhttps://medium.\ncom/decodingml/\nthe-3nd-out-\nof-1...\n2\n4c9f68ae-\nec8b-4534-\n8ad5-\n92372bf8bb37\nThe Role of \nFeature Stores \nin Fine-Tun-\ning LLMs...\nmedium\ne6b945ba-\n6a9a-\n4cde-b2bf-\n0890af79732b\nAlex Vesa\nhttps://medium.\ncom/decodingml/\nthe-role-of-\nfeat...\n...\n...\n...\n...\n...\n...\n...\n73\n68795a4d-\n26c2-43b7-\n9900-\n739a80b9b-\n7dc\nDML: 4 key \nideas you \nmust know to \ntrain an LLM...\ndecod-\ningml.\nsubstack.\ncom\n1519b1d1-\n1a5d-444c-\na880-926c9e-\nb6539e\nPaul \nIusztin\nh t t p s : / /\nd e c o d i n g m l .\nsubstack.com/p/\ndml-4-key-id...\n74\nd91b17c0-\n05d8-\n4838-bf61-\ne2abc1573622\nDML: How to \nadd real-time \nmonitoring & \nmetrics...\ndecod-\ningml.\nsubstack.\ncom\n1519b1d1-\n1a5d-444c-\na880-926c9e-\nb6539e\nPaul \nIusztin\nh t t p s : / /\nd e c o d i n g m l .\nsubstack.com/p/\ndml-how-to-a...\n75\ndcf55b28-\n2814-\n4480-a18b-\na77d01d44f5f\nDML: Top 6 \nML Platform \nFeatures You \nMust Know ...\ndecod-\ningml.\nsubstack.\ncom\n1519b1d1-\n1a5d-444c-\na880-926c9e-\nb6539e\nPaul \nIusztin\nh t t p s : / /\nd e c o d i n g m l .\nsubstack.com/p/\ndml-top-6-ml...\n4.\nIf we inspect the content of some articles a little further, we realize that some of them \nhave special characters and redundant whitespaces. We can clean this with a simple regex.\nFirst, we use [^\\w\\s.,!?'] to remove non-alphanumeric characters except for apostro-\nphes, periods, commas, exclamation marks, and question marks. Then, we use \\s+ to \nreplace multiple consecutive whitespace characters with a single space. \n",
      "content_length": 1976,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "Supervised Fine-Tuning\n200\nFinally, we implement strip() to remove any leading or trailing whitespace.\ndef clean_text(text):\n    text = re.sub(r\"[^\\w\\s.,!?']\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n5.\nNow that we can load our articles, we need to chunk them before turning them into pairs \nof instructions and answers. Ideally, you would want to use headlines or paragraphs to \nproduce semantically meaningful chunking.\nHowever, in our example, like in the real world, raw data tends to be messy. Due to im-\nproper formatting, we cannot extract paragraphs or headlines for every article in our raw \ndataset. Instead, we will extract sentences using a regex to get chunks between 1,000 \nand 2,000 characters. This number can be optimized depending on the density of the \ninformation contained in the text.\nThe extract_substrings function processes each article in the dataset by first cleaning the \ntext and then using a regex to split it into sentences. It then builds chunks of text by con-\ncatenating these sentences until each chunk is between 1,000 and 2,000 characters long.\ndef extract_substrings(dataset: Dataset, min_length: int = 1000, \nmax_length: int = 2000) -> List[str]:\n    extracts = []\n    sentence_pattern = r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)\n(?<=\\.|\\?|\\!)\\s\"\n    for article in dataset[\"content\"]:\n        cleaned_article = clean_text(article)\n        sentences = re.split(sentence_pattern, cleaned_article)\n        current_chunk = \"\"\n        for sentence in sentences:\n            sentence = sentence.strip()\n            if not sentence:\n                continue\n            if len(current_chunk) + len(sentence) <= max_length:\n                current_chunk += sentence + \" \"\n            else:\n",
      "content_length": 1744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "Chapter 5\n201\n                if len(current_chunk) >= min_length:\n                    extracts.append(current_chunk.strip())\n                current_chunk = sentence + \" \"\n        if len(current_chunk) >= min_length:\n            extracts.append(current_chunk.strip())\n    return extracts\n6.\nNext, we want to create instruction-answer pairs from the extracted chunks of text. To \nmanage these pairs effectively, we introduce the InstructionAnswerSet class. This class \nallows us to create instances directly from JSON strings, which is useful when parsing the \noutput from the OpenAI API.\nclass InstructionAnswerSet:\n    def __init__(self, pairs: List[Tuple[str, str]]):\n        self.pairs = pairs\n    @classmethod\n    def from_json(cls, json_str: str) -> 'InstructionAnswerSet':\n        data = json.loads(json_str)\n        pairs = [(pair['instruction'], pair['answer'])\n                 for pair in data['instruction_answer_pairs']]\n        return cls(pairs)\n    def __iter__(self):\n        return iter(self.pairs)\n7.\nNow that we have a set of extracts from the articles with a reasonable length, we can use \nan LLM to transform them into pairs of instructions and answers. Note that this step is \nmodel-agnostic and can be implemented with any open-source or closed-source model. \nBecause this output is grounded in the context we provide, it doesn’t require complex \nreasoning or high-performing models.\nFor convenience, we will use GPT-4o mini in this example. This choice is motivated by the \nlow cost and good performance of this model. Prompt engineering is the most important \naspect of this data transformation stage and requires several iterations to produce the \nexpected outputs. We recommend starting with simple prompts and adding complexity \nwhen required to be more accurate, modify the style, or output multiple responses.\n",
      "content_length": 1840,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "Supervised Fine-Tuning\n202\nIn our example, we want to create instructions like “Write a paragraph about X topic” and \ncorresponding answers that are factual and imitate the writer’s style. To implement this, \nwe need to provide an extract that will ground the model’s responses. For efficiency, we \nalso choose to generate five instruction-answer pairs for each extract. Here’s the beginning \nof our function for instruction generation, including our prompt.\ndef generate_instruction_answer_pairs(\n    extract: str, client: OpenAI\n) -> List[Tuple[str, str]]:\n    prompt = f\"\"\"Based on the following extract, generate five \ninstruction-answer pairs. Each instruction \\\nmust ask to write about a specific topic contained in the context. \neach answer \\\nmust provide a relevant paragraph based on the information found in \nthe \\\ncontext. Only use concepts from the context to generate the \ninstructions. \\\nInstructions must never explicitly mention a context, a system, a \ncourse, or an extract. \\\nInstructions must be self-contained and general. \\\nAnswers must imitate the writing style of the context. \\\nExample instruction: Explain the concept of an LLM Twin. \\\nExample answer: An LLM Twin is essentially an AI character that \nmimics your writing style, personality, and voice. \\\nIt's designed to write just like you by incorporating these elements \ninto a language model. \\\nThe idea is to create a digital replica of your writing habits using \nadvanced AI techniques. \\\nProvide your response in JSON format with the following structure:\n{{\n    \"instruction_answer_pairs\": [\n        {{\"instruction\": \"...\", \"answer\": \"...\"}},\n        ...\n    ]\n}}\nExtract:\n{extract}\n\"\"\"\n",
      "content_length": 1669,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "Chapter 5\n203\n8. In addition to the user prompt, we can also specify a system prompt to guide the mod-\nel into generating the expected instructions. Here, we repeat our high-level task in the \nsystem prompt.\nThe concatenation of the system and user prompts is fed to the OpenAI API, using the GPT-\n4o mini model in JSON mode and a maximum of 1,200 tokens in the answer. We also use a \nstandard temperature of 0.7 to encourage diverse responses. The generated text is directly \nparsed using the InstructionAnswerSet class to return pairs of instructions and answers.\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\", \"content\": \"You are a helpful \nassistant who \\\n            generates instruction-answer pairs based on the given \ncontext. \\\n            Provide your response in JSON format.\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_format={\"type\": \"json_object\"},\n        max_tokens=1200,\n        temperature=0.7,\n    )\n    # Parse the structured output\n    result = InstructionAnswerSet.from_json(completion.choices[0].\nmessage.content)\n    # Convert to list of tuples\n    return result.pairs\n9.\nLet’s create a main function to automate the process. It extracts substrings from the input \ndataset, then uses concurrent processing via Python’s ThreadPoolExecutor to efficiently \ngenerate instruction-answer pairs for each extract. \n",
      "content_length": 1480,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "Supervised Fine-Tuning\n204\nWe use a default max_workers value of 4 because higher values tend to exceed OpenAI’s \nrate limits, potentially causing API request failures or throttling.\ndef create_instruction_dataset(\n    dataset: Dataset, client: OpenAI, num_workers: int = 4\n) -> Dataset:\n    extracts = extract_substrings(dataset)\n    instruction_answer_pairs = []\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_\nworkers) as executor:\n        futures = [executor.submit(generate_instruction_answer_\npairs, extract, client)\n            for extract in extracts\n        ]\n        for future in tqdm(concurrent.futures.as_completed(futures), \ntotal=len(futures)\n        ):\n            instruction_answer_pairs.extend(future.result())\n    instructions, answers = zip(*instruction_answer_pairs)\n    return Dataset.from_dict(\n        {\"instruction\": list(instructions), \"output\": list(answers)}\n    )\n10. We can create our instruction dataset by calling this function. Running it over the raw \ndata with GPT-4o mini costs less than 0.5$.\n11. We can now create a main function to orchestrate the entire pipeline. It loads the raw \ndata, creates the instruction dataset, splits it into training and testing sets, and pushes \nthe result to the Hugging Face Hub.\ndef main(dataset_id: str) -> Dataset:\n    client = OpenAI()\n    # 1. Load the raw data\n    raw_dataset = load_articles_from_json(\"cleaned_documents.json\")\n    print(\"Raw dataset:\")\n    print(raw_dataset.to_pandas())\n    # 2. Create instructiondataset\n",
      "content_length": 1519,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "Chapter 5\n205\ninstruction_dataset = create_instruction_dataset(raw_dataset, \nclient)\n    print(\"Instruction dataset:\")\n    print(instruction_dataset.to_pandas())\n    # 3. Train/test split and export\n    filtered_dataset = instruction_dataset.train_test_split(test_\nsize=0.1)\n    filtered_dataset.push_to_hub(\"mlabonne/llmtwin\")\n    return filtered_dataset\nDataset({\n    features: ['instruction', 'output'],\n    num_rows: 3335\n})\nWe obtained 3,335 pairs with this process. You can find our version of the dataset at https://\nhuggingface.co/datasets/mlabonne/llmtwin. The Hugging Face Hub provides a convenient \ndataset viewer (see Figure 5.7) to explore instructions and answers and make sure that there are \nno obvious mistakes in these samples. Due to the small size of the dataset, there is no need for \ncomprehensive exploration and topic clustering.\nFigure 5.7 – The mlabonne/llmtwin instruction dataset on the Hugging Face Hub\n",
      "content_length": 932,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "Supervised Fine-Tuning\n206\nAs seen in the previous section, we could refine this instruction dataset by increasing the diver-\nsity and complexity of our samples. More advanced prompt engineering could also increase the \nquality of the generated data by providing examples of the expected results, for instance. Finally, \nquality evaluation could help filter out low-quality samples by reviewing them individually. For \nconciseness and simplicity, we will keep a straightforward approach for this instruction dataset \nand explore more advanced methods in Chapter 6 when we create a preference dataset.\nIn the next section, we will introduce SFT techniques, as well as related concepts.\nExploring SFT and its techniques\nSFT consists of re-training pre-trained models on a smaller dataset composed of pairs of instruc-\ntions and answers. The goal of SFT is to turn a base model, which can only perform next-token \nprediction, into a useful assistant, capable of answering questions and following instructions. \nSFT can also be used to improve the general performance of the base model (general-purpose \nSFT), instill new knowledge (e.g., new languages, domains, etc.), focus on specific tasks, adopt \na particular voice, and so on.\nIn this section, we will discuss when to use fine-tuning and explore related concepts with storage \nformats and chat templates. Finally, we will introduce three popular ways of implementing SFT: \nfull-finetuning, Low-Rank Adaptation (LoRA) and Quantization-aware Low-Rank Adaptation\n(QLoRA).\nWhen to fine-tune\nIn most scenarios, it is recommended to start with prompt engineering instead of directly fine-tun-\ning models. Prompt engineering can be used with either open-weight or closed-source models. By \nusing techniques like few-shot prompting or retrieval augmented generation (RAG), numerous \nproblems can efficiently be tackled without SFT. Prompt engineering also allows us to build a \nrobust evaluation pipeline, which measures metrics like accuracy, but also cost and latency. If \nthese results do not match the requirements, we can explore the possibility of creating an in-\nstruction dataset, as illustrated in the previous section. If enough data is available, fine-tuning \nbecomes an option.\n",
      "content_length": 2234,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "Chapter 5\n207\nFigure 5.8 – Basic flowchart to determine when fine-tuning is an option on a technical level\nBeyond these technical considerations, SFT answers common needs in terms of control (“know \nyour data”) and customizability (the fine-tuned model is unique). Instead of building applications \naround a chatbot, fine-tuning allows developers to create more diverse interactions with LLMs, \nlike tool analytics, moderation, and additional context. Note that if we focus on open-weight \nmodels in this book, several LLM providers offer automated fine-tuning services. While they don’t \noffer the same level of control and customizability as managing your own fine-tuning pipeline, it \ncan be an interesting trade-off in specific scenarios (e.g., limited resources in terms of machine \nlearning engineering).\nDespite these advantages, fine-tuning also has limitations. It is generally understood that SFT \nleverages pre-existing knowledge in the base model’s weights and refocuses the parameters for \na specific purpose. This has several implications. First of all, knowledge that is too distant from \nwhat has been learned in the pre-training set (such as an unknown or rare language) can be \ndifficult to learn effectively. \nEven worse, a study showed that fine-tuning a model on new knowledge could result in more \nfrequent hallucinations. Depending on the SFT technique that is used, we’re also at risk of eras-\ning knowledge that was present in the base model (a common issue referred to as “catastrophic \nforgetting”).\n",
      "content_length": 1527,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "Supervised Fine-Tuning\n208\nInstruction dataset formats\nInstruction datasets are stored in a particular format to organize instructions and answers. Typi-\ncally, each sample in the dataset can be represented as a Python dictionary, where keys are prompt \ntypes like system, instruction, output, and values corresponding to the actual text. The three \nmost standard formats are Alpaca, ShareGPT, and OpenAI. The following table shows how these \ndata formats are generally organized.\nName\nJSONL format\nAlpaca\n{“instruction”: “...”, “input”: “...”, “output”: “...”}\n{“instruction”: “...”, “output”: “...”}\nShareGPT\n{“conversations”: [{“from”: “...”, “value”: “...”}, …]}\nOpenAI\n{“conversations”: [{“role”: “...”, “content”: “...”}, …]}\nOASST\n{“INSTRUCTION”: “...”, “RESPONSE”: “...”}\nRaw text\n{“text”: “...”}\nTable 5.5 – Examples of instruction data storage format\nNote that for Alpaca, the “input\" key is optional. The content of the “input\" key is only appended \nto the content of the “instruction\" key when it exists. We also added the “raw text\" data format \nto show that SFT is not inherently different from pre-training. If you choose to re-train a model \non raw text, this is a type of fine-tuning generally called “continual pre-training.”\nThe dataset we created in the previous section has two columns (“instruction\" and “output\") \nand corresponds to the Alpaca format. Alpaca is sufficient for single-turn instructions and an-\nswers, which means it is limited to one instruction and one answer. When you want to process \nconversations (multiple instructions and answers), formats like ShareGPT or OpenAI are a better \nfit. By storing each message as a dictionary in a list, they can represent an arbitrarily long con-\nversation in each sample.\nThe choice of single-turn and multi-turn conversations directly impacts the storage type and \ndepends on the end use case.\nChat templates\nOnce the instruction-answer pairs are parsed from the dataset format, we want to structure them \nin a chat template. Chat templates offer a unified way to present the instructions and answers \nto the model.\n",
      "content_length": 2095,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "Chapter 5\n209\nIn general, they also include special tokens to identify the beginning and the end of a message, or \nwho is the author of the message. Since base models are not designed to follow instructions, they \ndon’t have a chat template. This means that you can choose any template when you fine-tune \na based model. If you want to fine-tune an instruct model (not recommended), you need to use \nthe same template or it might degrade your performance.\nLike instruction dataset formats, there are different chat templates: ChatML, Llama 3, Mistral, and \nmany others. In the open-source community, the ChatML template (originally from OpenAI) is a \npopular option. It simply adds two special tokens (<|im_start|> and <|im_end|>) to indicate \nwho is speaking. To give you an example, here is what we obtain when we apply the ChatML \ntemplate to the instruction-answer pair shown in Table 5.1:\n<|im_start|>system\nYou are a helpful assistant, who always provide explanation. Think like you \nare answering to a five year old.<|im_end|>\n<|im_start|>user\nConcepts: building, shop, town\nWrite a sentence that includes all these words.<|im_end|>\n<|im_start|>assistant\nIn our little town, there is a shop inside a big building where people go \nto buy their favorite toys and candies.<|im_end|>\nTable 5.6 – Sample from Table 5.1 with the ChatML chat template\nAs you can see, we still have three distinct parts: system, user, and assistant. Each part starts with \nthe <|im_start|> token and ends with <|im_end|>. The current speaker is identified by a string \n(like “system\") instead of a special token. This is the exact string that is tokenized and used as \ninput by the model during fine-tuning.\nHowever, during inference, we can’t provide the expected answer. In this case, we provide the \nsystem and user part as shown in Figure 5.6, and prompt the model to answer by adding <|im_\nstart|>assistant\\n. \nBecause the model has been fine-tuned with this template, it understands that the next tokens \nshould be an answer relevant to the user instruction and guided by the system prompt. This is \nhow fine-tuned models acquire instruction-following capabilities.\n",
      "content_length": 2154,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "Supervised Fine-Tuning\n210\nA common issue with chat templates is that every single whitespace and line break is extremely \nimportant. Adding or removing any character would result in a wrong tokenization, which neg-\natively impacts the performance of the model. For this reason, it is recommended to use reliable \ntemplates like Jinja, as implemented in the Transformers library. Table 5.7 shows a few examples \nof such templates, including Alpaca, which is both the name of an instruction dataset format \nand a chat template.\nName\nJinja template\nAlpaca\n### Instruction: What is the capital of France?\n### Response: The capital of France is Paris.<EOS>\nChatML\n<|im_start|>user\nWhat is the capital of France?<|im_end|>\n<|im_start|>assistant\nThe capital of France is Paris.<|im_end|>\nLlama 3\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nWhat is the capital of France?<|eot_id|><|start_header_\nid|>assistant<|end_header_id|>\nThe capital of France is Paris.<|eot_id|>\nPhi-3\n<|user|>\nWhat is the capital of France?<|end|>\n<|assistant|>\nThe capital of France is Paris.<|end|>\nGemma\n<bos><start_of_turn>user\nWhat is the capital of France?<end_of_turn>\n<start_of_turn>model\nThe capital of France is Paris.<end_of_turn>\nTable 5.7 – Example of common chat templates\nJinja implements loops and conditions, which allow the same template to be used for training \nand inference (add_generation_prompt).\n",
      "content_length": 1402,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "Chapter 5\n211\nParameter-efficient fine-tuning techniques\nWhile many techniques exist in the literature, SFT has converged on three main techniques: full \nfine-tuning, LoRA, and QLoRA. We will introduce each technique individually, and weigh their \npros and cons depending on your use cases.\nFigure 5.9 – Architectural differences of the three main SFT techniques at the module level\nFull fine-tuning\nFull fine-tuning refers to the most straightforward SFT technique, consisting of re-training every \nparameter in the base model. Like pre-training, SFT uses next-token prediction as its training \nobjective. This means that the previously discussed structure of the dataset can be seen as the \nmain difference between continual pre-training and full fine-tuning.\nThis method often provides the best results but requires significant computational resources. \nMemory usage depends on several factors, including model size, training techniques, and op-\ntimization methods. At its simplest, using a single-GPU setting, the memory required can be \nestimated using the following formula:\n𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀 \nFor a basic setup using 32-bit floating point (fp32) precision, we can estimate:\n•\nParameters: Learnable weights and biases within a neural network. In a large language \nmodel, these are typically the weights in the attention mechanisms, feed-forward layers, \nand embedding layers. Cost: 4 bytes/parameter (FP32) or 2 bytes/parameter (FP16/BF16).\n•\nGradients: Gradients are the partial derivatives of the loss function with respect to each \nmodel parameter. They indicate how much each parameter should be adjusted to minimize \nthe loss. During training, gradients are computed for each parameter through backprop-\nagation and are used to update the model parameters. Cost: 4 bytes/parameter.\n",
      "content_length": 1883,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "Supervised Fine-Tuning\n212\n• \nOptimizer states: Optimizer states are additional values maintained by optimization \nalgorithms like Adam or AdamW. These typically include running averages of past gra-\ndients and past squared gradients for each parameter. They help in adapting the learning \nrate for each parameter and navigating the loss landscape more effectively. For instance, \nAdam maintains two additional values (momentum and variance) per parameter. Cost: \n8 bytes/parameter (for Adam optimizer).\n• \nActivations: Activations are the intermediate outputs of each layer in the neural network \nduring the forward pass. For transformer-based models, this includes the outputs of \nattention mechanisms, feed-forward layers, and normalization layers. Activations need \nto be kept in memory during the forward pass to compute gradients in the backward \npass, unless techniques like activation checkpointing are used. Cost: variable, but often \nnegligible for small batch sizes.\nThis gives us a baseline of 16 bytes per parameter. This translates into 112 GB of VRAM for a 7 \nB model and 1,120 GB for a 70 B model. However, this is often an underestimate, as it doesn’t \naccount for additional memory needed for activations, temporary buffers, and overhead from \nvarious training techniques.\nSeveral techniques can be employed to reduce memory usage during LLM fine-tuning. Model \nparallelism spreads the workload across multiple GPUs, though it adds some overhead. Gradient \naccumulation enables larger effective batch sizes without proportional memory increase. Mem-\nory-efficient optimizers like 8-bit Adam can reduce the footprint of optimizer states. Activation \ncheckpointing trades computation for memory by recalculating certain activations. When com-\nbined, these techniques can significantly lower memory usage. For instance, using mixed precision \nwith model parallelism might reduce costs to around 14-15 bytes per parameter, compared to the \n16-byte baseline. However, memory requirements remain substantial for large models even with \nthese optimizations.\nIn addition, full fine-tuning directly modifies the pre-training weights, which makes it destructive \nby nature. If training doesn’t behave as expected, it might erase previous knowledge and skills – a \nphenomenon referred to as “catastrophic forgetting.” The same phenomenon can happen with \ncontinual pre-training, which generally makes these techniques more difficult to use. Due to this \nadditional complexity and its high computational requirements, parameter-efficient techniques \nare often preferred to full fine-tuning to create task and domain-specific models.\n",
      "content_length": 2639,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "Chapter 5\n213\nLoRA\nLoRA is a parameter-efficient technique for fine-tuning LLMs. Developed to address the compu-\ntational challenges associated with adapting massive neural networks, LoRA has quickly become \na cornerstone technique in LLM fine-tuning.\nThe primary purpose of LoRA is to enable the fine-tuning of LLMs with significantly reduced \ncomputational resources. This is achieved by introducing trainable low-rank matrices that mod-\nify the behavior of the model without changing its original parameters. The key advantages of \nLoRA include:\n•\nDramatically reduced memory usage during training\n•\nFaster fine-tuning process\n•\nPreservation of pre-trained model weights (non-destructive)\n• \nAbility to switch between tasks efficiently by swapping LoRA weights\nThese benefits have made LoRA particularly attractive for researchers and developers working \nwith limited computational resources, effectively democratizing the process of LLM fine-tuning.\nAt its core, LoRA employs a low-rank decomposition technique to update model weights efficiently. \nInstead of directly modifying the original weight matrix 𝑊𝑊 , LoRA introduces two smaller matrices, \n𝐴𝐴  and 𝐵𝐵 , which together form a low-rank update to 𝑊𝑊 .\nFigure 5.10 – LoRA adds the two trainable matrices 𝐴𝐴 and 𝐵𝐵 and keeps the pre-trained weights \n𝑊𝑊 frozen\n",
      "content_length": 1319,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "Supervised Fine-Tuning\n214\nMathematically, this can be represented as:\n𝑊𝑊𝑊𝑊𝑊𝑊𝑊𝐵𝐵𝐵𝐵 \nHere, 𝑊𝑊  is the original weight matrix, 𝐵𝐵 and 𝐴𝐴 are the LoRA matrices, and 𝑊𝑊𝑊 is the effective weight \nmatrix used during inference.\nThe dimensions of matrices A and B are chosen such that their product has the same shape as \n𝑊𝑊 , but with a much lower rank. This rank, typically denoted as 𝑟𝑟 , is a crucial hyperparameter \nin LoRA. During training, the original weights 𝑊𝑊 remain frozen, while only 𝐴𝐴 and 𝐵𝐵 are updated. \nThis approach significantly reduces the number of trainable parameters, leading to substantial \nmemory savings and faster training times.\nTo implement LoRA effectively, we need to select the correct hyperparameters and target modules. \nLoRA comes with two hyperparameters:\n• \nRank (𝑟𝑟 ): Determines the size of the LoRA matrices. A common starting point is 𝑟𝑟𝑟𝑟 , but \nvalues up to 256 have shown good results in some cases. Larger ranks may capture more \ndiverse tasks but could lead to overfitting.\n• \nAlpha (𝛼𝛼 ): A scaling factor applied to the LoRA update. In practice, we update the frozen \nweights 𝑊𝑊  by a factor of 𝛼𝛼𝛼𝛼𝛼 . This is why a common heuristic is to set 𝛼𝛼  to twice the \nvalue of 𝑟𝑟 , effectively applying a scaling factor of 2 to the LoRA update. You can experiment \nwith different ratios in case of overfitting or underfitting.\nIn addition, it is possible to add a drop-out layer to prevent overfitting. The dropout rate is usually \nset between 0 and 0.1 as an optional regularization factor, which slightly decreases training speed.\nLoRA can be applied to various parts of the model architecture. Initially, LoRA was primarily fo-\ncused on modifying the attention mechanism, specifically the query (Q) and value (V) matrices \nin transformer layers. However, experiments have demonstrated significant benefits in extending \nLoRA’s application to other key components of the model. These additional target modules include:\n•\nKey (K) matrices in attention layers\n•\nOutput projection layers (often denoted as O) in attention mechanisms\n•\nFeed-forward or Multi-Layer Perceptron (MLP) blocks between attention layers\n•\nLinear output layers\nHowever, it’s important to note that increasing the number of LoRA-adapted modules also in-\ncreases the number of trainable parameters and, consequently, the memory requirements.\n",
      "content_length": 2349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "Chapter 5\n215\nUsing LoRA, it’s possible to fine-tune a 7B parameter model on a single GPU with as little as 14-\n18 GB of VRAM, depending on the specific configuration. This is a dramatic reduction compared \nto full fine-tuning, which would typically require multiple high-end GPUs. In terms of trainable \nparameters, LoRA drastically reduces the number compared to full fine-tuning. For example, even \nwhen targeting every module with a rank of 16, a Llama 3 8 B model only has 42 million trainable \nLoRA parameters out of 8 billion parameters, which is 0.5196% of the model’s parameters.\nIn terms of quality, LoRA can also achieve comparable or sometimes better results than full-fine-\ntuning. Multiple sets of LoRA weights can be combined for different tasks or domains, allowing \nflexible deployment and task switching without retraining. Different projects are specialized \nin multiple-LoRA serving, such as LoRAX. It’s also a feature supported by Hugging Face’s Text \nGeneration Inference (TGI) and Nvidia Inference Microservices (NIM).\nQLoRA\nIntroduced by Dettmers et al., QLoRA is a method for fine-tuning LLMs that addresses the chal-\nlenges of high computational costs. By combining quantization techniques with LoRA, QLoRA \nallows developers to fine-tune models on relatively small, widely available GPUs.\nThe core of QLoRA’s approach involves quantizing the base model parameters to a custom 4-bit \nNormalFloat (NF4) data type, which significantly reduces memory usage. Like LoRA, instead \nof updating all model parameters during fine-tuning, QLoRA introduces small, trainable low-\nrank matrices (adapters) to specific layers of the model. Only these adapters are updated during \ntraining, while the original model weights remain unchanged. To further reduce memory usage, \nQLoRA employs double quantization, which quantizes the quantization constants themselves. \nAdditionally, it uses paged optimizers to manage memory spikes during training by leveraging \nNvidia’s unified memory feature.\nQLoRA provides significant memory savings compared to LoRA, reducing peak GPU memory \nusage by up to 75%. For example, for a 7B model, QLoRA reduces peak memory usage from 14 GB \nto 9.1 GB during initialization, a 35% reduction. During fine-tuning, the memory savings increase \nto 40%, from 15.6 GB for LoRA to 9.3 GB for QLoRA. However, this memory efficiency comes at \nthe cost of increased training time, with QLoRA being about 30% slower than LoRA. In terms of \nmodel performance, QLoRA shows only minor differences compared to LoRA.\nIn summary, QLoRA is particularly beneficial when memory constraints are the primary concern, \nsuch as when working with very large models or on hardware with limited GPU memory. However, \nif training speed is crucial and sufficient memory is available, LoRA might be the preferred choice. \n",
      "content_length": 2832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "Supervised Fine-Tuning\n216\nThe decision between QLoRA and LoRA should be based on the specific requirements of the \nproject, available hardware, and the need to balance memory usage, training speed, and model \nperformance.\nTraining parameters\nWhen fine-tuning LLMs, several hyperparameters guide the training process and significantly \nimpact the model’s convergence, generalization, and overall effectiveness.\nLearning rate and scheduler\nThe learning rate is the most important hyperparameter. It controls how much the model’s pa-\nrameters are updated during training. It typically ranges from very small values like 1e-6 to larger \nvalues like 1e-3. A common starting point for transformer models is often around 1e-5. If the \nlearning rate is too low, training progresses slowly and may get stuck in suboptimal solutions. \nConversely, if it’s too high, training can become unstable or diverge, leading to poor performance. \nIt’s often beneficial to experiment with different learning rates to find the optimal value for your \nspecific task and model.\nThe learning rate scheduler adjusts the learning rate throughout the training process. It typically \nstarts with a higher learning rate to enable rapid initial progress, then gradually decreases it in \nlater stages to fine-tune the model more precisely. The two most common types of schedulers are \nlinear and cosine. A linear scheduler decreases the learning rate steadily over time, while a cosine \nscheduler follows a cosine curve, decreasing more slowly at first and then more rapidly toward \nthe end of training. For example, you might start with a learning rate of 3e-4 and decrease it to \n1e-7 over the course of training. The specific values and decay schedule depend on your model \nand dataset, but a common approach is to use a warmup period (e.g., 5% of total steps) where the \nlearning rate increases from 0 to the initial value, followed by a decay period for the remaining 95% \nof steps. This approach helps stabilize early training and allows for more refined updates as the \nmodel converges. In general, linear and cosine schedulers provide the same level of performance.\nBatch size\nThe batch size determines the number of samples processed before the model’s weights are up-\ndated. Typical batch sizes for LLM fine-tuning range from 1 to 32, with common values being 1, 2, \n4, 8, or 16. Larger batch sizes generally lead to more stable gradient estimates and can improve \ntraining speed, as they provide a better approximation of the true gradient of the entire dataset.\n",
      "content_length": 2542,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "Chapter 5\n217\nHowever, they also require more memory, which can be a limiting factor on GPUs with less VRAM. \nFor instance, a batch size of 16 might work well on a high-end GPU with 24GB of memory, while \na smaller GPU with 8 GB might only handle a batch size of 2 or 4.\nTo overcome memory constraints while still benefiting from larger batch sizes, a technique called \ngradient accumulation can be used. It works by performing multiple forward and backward passes \nwith smaller mini-batches, accumulating the gradients over these steps before applying a single \nupdate to the model’s parameters. This approach is particularly useful when working with large \nmodels or limited GPU memory. For example, if you want to achieve an effective batch size of 32 \nbut your GPU can only handle 8 samples at a time, you can set the gradient accumulation steps \nto 4. This means you’ll process 4 mini-batches of 8 samples each, accumulating the gradients, \nand then update the model as if you had processed all 32 samples at once.\nThe number of gradient accumulation steps typically ranges from 1 (no accumulation) to 8 or \n16, depending on the desired effective batch size and available computational resources. When \nchoosing the number of steps, consider the trade-off between training speed and memory usage. \nMore accumulation steps allow for larger effective batch sizes but increase the time required for \neach update. Here’s a simple formula to determine the effective batch size:\n𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸𝐸 \nFor instance, if you’re using 2 GPUs, each processing a batch of 4 samples, with 4 gradient accu-\nmulation steps, your effective batch size would be 4 * 2 * 4 = 32 samples.\nMaximum length and packing\nThe maximum sequence length determines the longest input the model can process. It’s typically \nset between 512 and 4,096 tokens but can go up to 128,000 or more, depending on the task and \navailable GPU memory. For example, a maximum length of 2,048 tokens is common for many \nlanguage generation tasks, while RAG applications might use up to 8,192 tokens or more. When \nprocessing input data, sequences longer than this limit are truncated, meaning excess tokens \nare removed. Truncation can occur at the beginning (left truncation) or end (right truncation) of \nthe sequence. For instance, with a maximum length of 1,024 tokens, a 1,500-token input would \nhave 476 tokens removed. This parameter directly impacts batch size and memory usage; a batch \nsize of 12 with a max length of 1,024 would contain 12,288 tokens (12 * 1,024), while the same \nbatch size with a max length of 512 would only contain 6,144 tokens. It’s important to balance \nthis parameter with your GPU capabilities and the nature of your training data to optimize per-\nformance and resource utilization.\n",
      "content_length": 2877,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "Supervised Fine-Tuning\n218\nPacking maximizes the utilization of each training batch. Instead of assigning one sample per \nbatch, packing combines multiple smaller samples into a single batch, effectively increasing the \namount of data processed in each iteration. For example, if your maximum sequence length is \n1,024 tokens, but many of your samples are only 200-300 tokens long, packing could allow you \nto fit 3-4 samples into each batch slot. This approach can significantly improve training efficien-\ncy, especially when dealing with datasets containing many short sequences. However, packing \nrequires careful implementation to ensure that model attention doesn’t cross between packed \nsamples. This is typically achieved by using attention masks that prevent the model from attend-\ning to tokens from different samples within the same packed sequence.\nNumber of epochs\nThe number of epochs is another important parameter, representing the number of complete \npasses through the entire training dataset. For LLM fine-tuning, the typical range is 1 to 10 epochs, \nwith many successful runs using 2 to 5 epochs. The optimal number depends on factors such as \ntask complexity, dataset size, and model architecture. More epochs allow the model to refine its \nlearning, potentially improving performance. However, there’s a crucial trade-off: too few epochs \nmay lead to underfitting, while too many can cause overfitting. For example, a large model fine-\ntuned on a small dataset might only need 1-3 epochs, while a smaller model fine-tuned on a larger \ndataset could benefit from 5-10 epochs. It is helpful to monitor validation performance during \ntraining and implement early stopping if the model’s performance plateaus or degrades. This \napproach helps determine the optimal number of epochs dynamically and prevents overfitting.\nOptimizers\nOptimizers adjust the model’s parameters to minimize the loss function. For LLM fine-tuning, \nAdamW (Adaptive Moment Estimation with Weight Decay) is highly recommended, particularly \nits 8-bit version. AdamW 8-bit performs comparably to the 32-bit version while using less GPU \nmemory (but it doesn’t improve training speed). AdamW combines adaptive learning rates with \nweight decay regularization, often leading to better training stability and model performance.\nFor scenarios with severe memory constraints, AdaFactor presents an alternative designed for \nmemory efficiency. It works well without explicit learning rate tuning, making it particularly \nuseful in resource-constrained environments. However, it may not always match AdamW’s perfor-\nmance in all cases. In situations involving extremely large models or limited GPU memory, paged \nversions of optimizers, such as paged AdamW 8-bit, can further reduce memory consumption \nby offloading to CPU RAM. If memory allows and maximum performance is the priority, the \nnon-quantized adamw_torch optimizer may be the best choice.\n",
      "content_length": 2936,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "Chapter 5\n219\nWeight decay\nWeight decay works by adding a penalty for large weights to the loss function, encouraging the \nmodel to learn simpler, more generalizable features. This helps the model avoid relying too heavily \non any single input feature, which can improve its performance on unseen data. Typically, weight \ndecay values range from 0.01 to 0.1, with 0.01 being a common starting point. For example, if \nyou’re using the AdamW optimizer, you might set the weight decay to 0.01.\nWhile weight decay can be beneficial, setting it too high can impede learning by making it difficult \nfor the model to capture important patterns in the data. Conversely, setting it too low may not \nprovide sufficient regularization. The optimal weight decay value often depends on the specific \nmodel architecture and dataset, so it’s generally a good practice to experiment with different \nvalues.\nGradient checkpointing\nGradient checkpointing is a technique that reduces memory consumption during training by stor-\ning only a subset of intermediate activations generated in the forward pass. In standard training \nprocedures, all intermediate activations are retained in memory to facilitate gradient calculation \nduring the backward pass. However, for very deep networks like LLMs, this approach can quickly \nbecome impractical due to hardware limitations, especially on GPUs with limited memory capacity.\nGradient checkpointing addresses this challenge by selectively saving activations at specific layers \nwithin the network. For layers where activations are not saved, they are recomputed during the \nbackward pass as needed for gradient computation. This approach creates a trade-off between \ncomputation time and memory usage. While it significantly reduces memory requirements, it \nmay increase overall computation time due to the need to recalculate some activations.\nOther parameters and techniques exist but play a minor role compared to those previously dis-\ncussed. In the next section, we will explore how to select and tune these parameters using a \nconcrete example.\nFine-tuning in practice\nLet’s now fine-tune an open-source model on our custom dataset. In this section, we will show an \nexample that implements LoRA and QLoRA for efficiency. Depending on the hardware you have \navailable, you can select the technique that best corresponds to your configuration.\nThere are many efficient open-weight models we can leverage for task or domain-specific use \ncases. To select the most relevant LLM, we need to consider three main parameters:\n",
      "content_length": 2550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "Supervised Fine-Tuning\n220\n•\nLicense: Some model licenses only allow non-commercial work, which is a problem if \nwe want to fine-tune for a company. Custom licenses are common in this field, and can \ntarget companies with a certain number of users, for example.\n•\nBudget: Models with smaller parameter sizes (<10 B) are a lot cheaper to fine-tune and \ndeploy for inference than larger models. This is due to the fact that they can be run on \ncheaper GPUs and process more tokens per second.\n•\nPerformance: Evaluating the base model on general-purpose benchmarks or, even better, \ndomain- or task-specific benchmarks relevant to the final use case, is crucial. This helps \nensure that the model has the necessary capabilities to perform well on the intended \ntasks after fine-tuning.\nIn this chapter, we will choose Llama 3.1 8B, an open-weight model released by Meta. It has a \npermissive custom license (“Llama 3.1 Community License Agreement”) that allows commercial \nuse. With 8B parameters, it is small enough to fit on most GPUs while reaching a high level of \nperformance compared to its competitors. We can verify this using the Open LLM Leaderboard, \nas well as other benchmarks detailed in the model card.\nThere are specialized tools and libraries to fine-tune models. In particular, we recommend the \nfollowing:\n•\nTRL: This is a library created and maintained by Hugging Face to train LLMs using SFT \nand preference alignment. It is a popular and reliable library that tends to be the most \nup-to-date in terms of algorithms. It works in single and multi-GPU settings with FSDP \nand DeepSpeed.\n• \nAxolotl: Created by Wing Lian, this tool streamlines the fine-tuning of LLMs with reusable \nYAML configuration files. It is based on TRL but includes many additional features, such as \nautomatically combining datasets stored in various formats. It also supports single- and \nmulti-GPU settings with FSDP and DeepSpeed.\n•\nUnsloth: Created by Daniel and Michael Han, Unsloth uses custom kernels to speed up \ntraining (2-5x) and reduce memory use (up to 80% less memory). It is based on TRL and \nprovides many utilities, such as automatically converting models into the GGUF quanti-\nzation format. At the time of writing, it is only available for single-GPU settings.\nTo maximize efficiency, we will perform fine-tuning using the Unsloth library. The following \ncode is designed as part of our LLMOps pipeline, but can also be used as a stand-alone script. It \ncan also be executed in different environments, like SageMaker, cloud GPUs (like Lambda Labs or \nRunPod), Google Colab, and many others. We tested it on different GPUs, like A40, A100, and L4.\n",
      "content_length": 2658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "Chapter 5\n221\nTo install the Unsloth library and its dependencies, we recommend directly installing from the \nGitHub repository of the book (https://github.com/PacktPublishing/LLM-Engineering) or \nUnsloth’s repo (https://github.com/unslothai/unsloth). This approach is recommended be-\ncause the installation steps are regularly updated to address potential conflicts with dependencies:\n1.\nFirst, we want to access a gated model and (optionally) upload our fine-tuned model to \nHugging Face (https://huggingface.co/). This requires being logged in to an account. \nIf you don’t have an account, you can create it and store your API key (Settings | Access \nTokens | Create new token) in the .env file:\nHF_TOKEN = YOUR_API_KEY\n2. \nMake sure that your Comet ML API key is also in the .env file:\nCOMET_API_KEY = YOUR_API_KEY\n3. \nImport all the necessary packages:\nimport os\nimport torch\nfrom trl import SFTTrainer\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import TrainingArguments, TextStreamerfrom unsloth \nimport FastLanguageModel, is_bfloat16_supported\n4.\nLet’s now load the model to fine-tune and its corresponding tokenizer. We use Unsloth’s \nFastLaguageModel class with the .from_pretrained() method. In addition to the mod-\nel name, we need to specify the max sequence length (2,048 in this example). Finally, \nthe load_in_4bit argument indicates if we want to use QLoRA (quantized pre-trained \nweights) or LoRA.\nWe’ll use LoRA in this example because of faster training and higher quality, but you can \neasily switch to QLoRA if you don’t meet the VRAM requirements.\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"meta-llama/Meta-Llama-3.1-8B\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=False,\n)\n",
      "content_length": 1785,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "Supervised Fine-Tuning\n222\n5.\nNow that the model is loaded, we can define our LoRA configuration. Here, we use a rank \nof 32 that is large enough to imitate the writing style and copy the knowledge from our in-\nstruction samples. You can increase this value to 64 or 128 if your results are underwhelm-\ning. We also set an alpha of 32, without dropout and without bias, to speed up training. \nFinally, we target every linear layer to maximize the quality of the fine-tuning process.\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_\nproj\", \"o_proj\", \"gate_proj\"],\n)\n6. \nNext, we need to prepare the data in the right format for fine-tuning. In this example, \nwe don’t have a lot of samples in the llmtwin dataset (3,000 samples). This is an issue \nbecause the model might not correctly learn the chat template. To address this, we will \nupsample it with a high-quality general-purpose dataset called FineTome. This is a filtered \nversion of arcee-ai/The-Tome using the fineweb-edu-classifier. Instead of using \nthe 100,000 samples of this dataset, we will specify we only want 10,000 in the train split. \nWe concatenate these two datasets to create our final set.\ndataset1 = load_dataset(\"mlabonne/llmtwin\")\ndataset2 = load_dataset(\"mlabonne/FineTome-Alpaca-100k\", \nsplit=\"train[:10000]\")\ndataset = concatenate_datasets([dataset1, dataset2])\n7.\nNow, we need to format this data using a chat template. Let’s use the Alpaca template \nfor convenience. This template doesn’t require additional tokens, which makes it less \nerror-prone (but can slightly impact performance compared to ChatML). Here, we map \nall the instructions and answers to the Alpaca template. We manually add the end of sen-\ntence (EOS) token at the end of each message to ensure that the model learns to output \nit. Without it, it will keep generating answers without ever stopping.\nalpaca_template = \"\"\"Below is an instruction that describes a task. \nWrite a response that appropriately completes the request.\n### Instruction:\n",
      "content_length": 2116,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "Chapter 5\n223\n{}\n### Response:\n{}\"\"\"\nEOS_TOKEN = tokenizer.eos_token\ndataset = dataset.map(format_samples, batched=True, remove_\ncolumns=dataset.column_names)\n8. Once the dataset is ready, we can divide it into training (95%) and test (5%) sets for val-\nidation during training.\ndataset = dataset.train_test_split(test_size=0.05)\n9. \nThe model is now ready to be trained. The SFTTrainer() class stores all the hyperparameters \nfor our training. In addition, we provide the model, tokenizer, LoRA configuration, and \ndatasets. Following the recommendations from the previous section, we set a learning \nrate of 3e-4 with a linear scheduler and a maximum sequence length of 2048. We train \nthis model for three epochs with a batch size of 2 and 8 gradient accumulation steps (for \nan effective batch size of 16). We also choose the adamw_8bit optimizer with a weight_\ndecay of 0.01. Depending on the GPU we use, it will automatically use FP16 or BF16 for \nthe activations. Finally, we report our training run to Comet ML for experiment tracking.\ntrainer = SFTTrainer(\n    model=model,\n   tokenizer=tokenizer,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=True,\n    args=TrainingArguments(\n        learning_rate=3e-4,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=8,\n        num_train_epochs=3,\n        fp16=not is_bfloat16_supported(),\n",
      "content_length": 1525,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "Supervised Fine-Tuning\n224\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        report_to=\"comet_ml\",\n        seed=0,\n    ),\n)\ntrainer.train()\nTraining this model on our concatenated dataset can take a few hours. For example, it \ntakes 50 minutes on an A100 GPU.\n10. Once it’s done, we can test it with a quick example. The goal is not to properly evaluate \nthe fine-tuned model, but to make sure that there are no obvious errors related to the \ntokenizer or chat template.\nFor fast inference, we can use FastLanguageModel.for_inference() from Unsloth. We \ndirectly format an instruction with the Alpaca format. Note that we provide an empty \nanswer to append the assistant header (### Response): at the end of the user instruction. \nThis forces the model to answer the instruction instead of completing it. We also use a text \nstreamer to stream the generation instead of waiting for it to be complete before printing it.\nFastLanguageModel.for_inference(model)\nmessage = alpaca_prompt.format(\"Write a paragraph to introduce \nsupervised fine-tuning.\", \"\")\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer=text_streamer, max_new_\ntokens=256, use_cache=True)\n11. Here is the answer provided by our model:\nSupervised fine-tuning is a method used to enhance a language model \nby providing it with a curated dataset of instructions and their \ncorresponding answers. This process is designed to align the model's \n",
      "content_length": 1626,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "Chapter 5\n225\nresponses with human expectations, thereby improving its accuracy \nand relevance. The goal is to ensure that the model can respond \neffectively to a wide range of queries, making it a valuable tool \nfor applications such as chatbots and virtual assistants.\nThis is correct and properly formatted with the Alpaca chat template.\n12. Now that our model has been successfully fine-tuned, we can save it locally and/or push \nit to the Hugging Face Hub using the following functions.\nmodel.save_pretrained_merged(\"model\", tokenizer, save_\nmethod=\"merged_16bit\")\nmodel.push_to_hub_merged(\"mlabonne/TwinLlama-3.1-8B\", tokenizer, \nsave_method=\"merged_16bit\")\nCongratulations on fine-tuning a base model from scratch! During training, you can access Comet \nML to monitor your training loss, validation loss, and many other metrics. You want to make sure \nthat these metrics correspond to what is expected. Figure 5.11 shows the training run correspond-\ning to the previous code in Comet ML.\nFigure 5.11 – Four monitored metrics during fine-tuning in Comet ML\n",
      "content_length": 1063,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "Supervised Fine-Tuning\n226\nIn particular, three of these metrics are important to monitor:\n•\nTraining loss: It measures how well the model is performing on the task it’s being trained \nfor. The loss should continuously decrease on average, indicating improving performance. \nWe expect a rapid decrease at the beginning of training, followed by a long plateau. Spikes \nand continuous increases in the loss value are signs that the training is failing. In this \ncase, you might want to check the quality of your data, issues with the tokenizer, and \ntune parameters like learning rate and batch size. In Figure 5.11 (loss), you can see three \ndifferent phases corresponding to our three epochs.\n•\nValidation loss: It measures the loss using the validation set instead of the training set; \na well-fitted model typically shows both training and validation losses decreasing and \neventually stabilizing, with a small gap between them. This gap should be minimal but \nis expected to exist as the model will always perform slightly better on the training data. \nIf the training loss continues to decrease while the validation loss starts to increase, it’s a \nsign of overfitting. Conversely, if both curves remain flat at a relatively high loss value, it \nindicates underfitting. There are no universal “recommended ranges” for loss values, as \nthese depend on the specific problem and loss function used. However, you should look for \nconvergence and stability in both curves. In Figure 4.11 (eval_loss), we see a slight increase \nat step 340. This is still acceptable but might indicate that the model starts to overfit.\n•\nGradient norm: It represents the magnitude of the gradient vector during training. Large \ngradient norms can indicate training instability like overfitting, especially if accompanied \nby a divergence between training and validation losses. On the other hand, a stable or \ndecreasing gradient norm generally means that the model is converging toward a local \noptimum. To mitigate issues associated with large gradient norms, gradient clipping can \nbe employed. This technique involves setting a maximum threshold for the gradient norm, \neffectively limiting the size of parameter updates.\nIt is often interesting to try different learning rates and select the best model based on the minimal \nloss. Note that this is a proxy for real evaluations, which are covered in the next chapter.\nSummary\nThis chapter covered essential aspects of LLM fine-tuning, both in theory and practice. We ex-\namined the instruction data pipeline and how to create high-quality datasets, from curation \nto augmentation. Each pipeline stage offers optimization opportunities, particularly in quality \nassessment, data generation, and enhancement. This flexible pipeline can be adapted to your use \ncases by selecting the most relevant stages and techniques. \n",
      "content_length": 2855,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "Chapter 5\n227\nWe applied this framework to real-world data from Chapter 3, using an LLM to convert raw text \ninto instruction-answer pairs. We then explored SFT techniques. This included an analysis of \nSFT’s advantages and limitations, methods for storing and parsing instruction datasets with chat \ntemplates, and an overview of three primary SFT techniques: full fine-tuning, LoRA, and QLoRA. \nWe compared these methods based on their impact on memory usage, training efficiency, and \noutput quality. The chapter concluded with a practical demonstration that involved fine-tuning \na Llama 3.1 8 B model on our custom instruction dataset. This example highlighted key steps and \nimplementation details for successful fine-tuning.\nIn the next chapter, we will use preference alignment techniques to create a new version of Twin-\nLlama-3.1-8B. We will generate a new dataset with chosen and rejected answers that will help us \ncalibrate the type of answers we expect from our model. We will detail many applications that \ncan benefit from this framework and how to implement it.\nReferences\n• \nTahori, Gulrajani, Zhang, Dubois, et al.. “Alpaca: A Strong, Replicable Instruction-Following \nModel” crfm.stanford.edu, March 13, 2023, https://crfm.stanford.edu/2023/03/13/\nalpaca.html.\n• \nSubhabrata Mukherjee et al.. “Orca: Progressive Learning from Complex Explanation Traces \nof GPT-4.” arXiv preprint arXiv:2306.02707, June 2023.\n• \nWing Lian and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong \nand “Teknium”. “Open-Orca/OpenOrca.” huggingface.co, 2023, https://huggingface.co/\ndatasets/Open-Orca/OpenOrca.\n• \nWeihao Zeng et al.. “Automatic Instruction Evolving for Large Language Models.” arXiv pre-\nprint arXiv:2406.00770, June 2024.\n•\nChunting Zhou et al.. “LIMA: Less Is More for Alignment.” arXiv preprint arXiv:2305.11206, \nMay 2023\n•\n01. AI. “Yi: Open Foundation Models by 01.AI.” arXiv preprint arXiv:2403.04652, March 2024.\n•\nAlex Birch. “LLM finetuning memory requirements.” blog.scottlogic.com, November 24, \n2023, https://blog.scottlogic.com/2023/11/24/llm-mem.html.\n•\nQuentin Anthony et al.. “Transformer Math 101.” blog.eleuther.ai, April 18, 2023, https://\nblog.eleuther.ai/transformer-math/.\n",
      "content_length": 2230,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "Supervised Fine-Tuning\n228\n•\nEdward J. Hu et al.. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv preprint \narXiv:2106.09685, June 2021.\n•\nTim Dettmers et al.. “QLoRA: Efficient Finetuning of Quantized LLMs.” arXiv preprint arX-\niv:2305.14314, May 2023.\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 409,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "6\nFine-Tuning with Preference \nAlignment\nSupervised Fine-Tuning (SFT) has been crucial in adapting LLMs to perform specific tasks. How-\never, SFT struggles to capture the nuances of human preferences and the long tail of potential \ninteractions that a model might encounter. This limitation has led to the development of more \nadvanced techniques for aligning AI systems with human preferences, grouped under the um-\nbrella term preference alignment.\nPreference alignment addresses the shortcomings of SFT by incorporating direct human or AI \nfeedback into the training process. This method allows a more nuanced understanding of human \npreferences, especially in complex scenarios where simple supervised learning falls short. While \nnumerous techniques exist for preference alignment, this chapter will primarily focus on Direct \nPreference Optimization (DPO) for simplicity and efficiency.\nIn this chapter, we will talk about the type of data that is required by preference alignment algo-\nrithms like DPO. We will build our own dataset to modify the writing style of our model, making \nit less artificial and more authentic. We will introduce the DPO algorithm and implement it to \nalign the model trained in Chapter 5.\nIn this chapter, we will cover the following topics:\n•\nUnderstanding preference datasets\n•\nHow to create our own preference dataset\n•\nDirect preference optimization (DPO)\n•\nImplementing DPO in practice to align our model\n",
      "content_length": 1445,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "Fine-Tuning with Preference Alignment\n230\nBy the end of this chapter, you will be able to create your own preference datasets and align \nmodels with diverse techniques.\nUnderstanding preference datasets\nThe principles for creating high-quality preference datasets are the same as those discussed in \nChapter 5 for instruction datasets. We want to maximize the accuracy, diversity, and complexity \nof our samples. To achieve this, we follow the same stages, as outlined in Figure 6.1: data curation, \ndeduplication, decontamination, quality evaluation, exploration, generation, and augmentation.\nFigure 6.1 – Overview of the post-training data pipeline covered in this chapter\nTo avoid repetition, this section will focus on the main differences between instruction and pref-\nerence datasets. We will introduce the structure of preference samples and the ideal size for pref-\nerence datasets. Then, we will focus on the two stages that differ most from creating instruction \ndatasets: data generation and evaluation.\nPreference data\nPreference datasets lack the standardization of instruction datasets due to varying data require-\nments across different training algorithms. Preference data comprises a collection of responses \nto a given instruction, ranked by humans or language models. This chapter focuses on DPO, so \nwe will examine the specific data format required by this algorithm.\nAs illustrated in Table 6.1, the structure of DPO datasets is straightforward: each instruction is \npaired with one preferred answer and one rejected answer. The objective is to train the model to \ngenerate the preferred response rather than the rejected one.\nAll the code examples from this chapter can be found on GitHub at https://github.\ncom/PacktPublishing/LLM-Engineering.\n",
      "content_length": 1769,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "Chapter 6\n231\nInstruction\nTell me a joke about octopuses.\nChosen answer\nWhy don’t octopuses play cards in casinos? \nBecause they can’t count past eight.\nRejected answer\nHow many tickles does it take to make an \noctopus laugh? Ten tickles.\nTable 6.1 – Example of sample from the mlabonne/orpo-dpo-mix-40k dataset\nIn preference datasets, the rejected response is as important as the chosen one. Without the \nrejected response, the dataset would be a simple instruction set. Rejected responses represent \nthe behavior we aim to eliminate from the model. This provides a lot of flexibility and allows us \nto use preference datasets in many contexts. Here is a list of examples where preference datasets \nare more beneficial to use compared to using SFT alone:\n•\nChatbots: In conversational AI, the quality of responses often depends on subjective fac-\ntors like naturalness, engagement, and contextual appropriateness. A preference dataset \nallows the model to learn these nuanced aspects by comparing better and worse responses. \nSimple SFT might not capture the subtleties of what makes one response preferable over \nanother in a given context.\n•\nContent moderation: Determining whether content is appropriate or violates guidelines \noften involves nuanced judgments. Preference datasets can help the model learn to dis-\ntinguish between borderline cases by comparing examples of content that is and isn’t \nacceptable. This is more effective than binary classification through SFT, as it helps the \nmodel understand the reasoning behind moderation decisions.\n•\nSummarization: The quality of a summary often depends on factors like conciseness, \nrelevance, and coherence. By using preference datasets, models can learn to generate \nsummaries that humans find more useful and informative. Simple SFT might result in \nsummaries that are technically correct but less preferable to human readers.\n•\nCode generation: In coding tasks, there are often multiple correct solutions, but some \nare more efficient or readable, or follow better practices than others. Preference datasets \ncan help the model learn these qualitative aspects of code quality, which might not be \ncaptured by simple correctness-based SFT.\n",
      "content_length": 2203,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "Fine-Tuning with Preference Alignment\n232\n•\nCreative writing: For tasks like story generation or poetry writing, the quality of the \noutput is highly subjective and multifaceted. Preference datasets can capture human \njudgments about style, creativity, and emotional impact better than instruction datasets, \nwhich might focus more on technical correctness or adherence to prompts.\n•\nTranslation: While traditional metrics like BLEU scores can measure translation accu-\nracy, they don’t always capture the fluency or naturalness of the translation. Preference \ndatasets can help models learn to produce translations that native speakers prefer, even \nwhen multiple translations are technically correct.\nIn all these scenarios, preference datasets enable a more refined training approach. They capture \nsubjective quality assessments and human preferences that extend beyond simple correctness or \nadherence to instructions. This method can produce models that generate output that is not only \ntechnically accurate but also better aligned with human judgment and preferences in complex, \nopen-ended tasks.\nUnlike instruction datasets, there are no standardized storage formats like Alpaca or ShareGPT. \nMost preference datasets follow a structure similar to that shown in Table 6.1, with columns for \nan instruction, a preferred answer, and a rejected answer. Multi-turn conversations are uncom-\nmon in preference alignment. At the time of writing, major fine-tuning libraries do not support \nmulti-turn conversations and typically extract only the first or last message in a conversation.\nData quantity\nDPO datasets typically require fewer samples than instruction datasets to significantly impact \nmodel behavior. As with instruction datasets, the required sample count depends on model size \nand task complexity. Larger models are more sample-efficient and thus require less data, while \ncomplex tasks demand more examples to capture the desired behavior. Once again, data quality \nis crucial, and a large number of preference pairs is generally beneficial.\nGeneral-purpose alignment is used by LLM providers to improve the overall performance of the \nfine-tuned models. This requires preference datasets with millions of samples. Major players in \nthe AI industry, including Nvidia and Meta, are converging on similar post-training pipelines, \ninvolving multiple rounds of preference alignment, and extensive use of synthetic data. This \nconsensus suggests that these methods are proving to be the most effective for pushing the \nboundaries of language model capabilities.\nOn a smaller scale, the open-source community uses datasets ranging from 10,000 to 100,000 \nsamples to enhance model performance. This approach has proven effective not only in improving \nbenchmark scores but also in healing networks after merging, pruning, and other modifications. \nGenerally, DPO is less destructive than SFT and has a milder impact on the final model.\n",
      "content_length": 2949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "Chapter 6\n233\nOn the other hand, tasks like the ones previously described require fewer preference pairs. \nTask-specific alignment focuses on improving model performance for a particular function, such \nas modifying the writing style, refusing certain instructions, and so on. These alignments can \noften be achieved with smaller datasets, ranging from 100 to 10,000 preference pairs, depending \non the task’s complexity.\nAn example of an application that requires few samples is instructing the model to state that it \nwasn’t trained by OpenAI, Meta, or another LLM provider. This can be achieved using a prefer-\nence dataset, where the rejected answers are those claiming alternative origins, and the chosen \nanswers are responses where the model correctly states that it was trained by you. A relatively \nsmall dataset of 200 to 500 pairs can be enough for this task.\nData generation and evaluation\nWhen creating preference datasets, data generation and evaluation are closely linked. We first \ncreate answers and then rate them to make the final dataset. In the following, we introduce both \nsteps as one process instead of two separate ones.\nGenerating preferences\nBefore making new preference data, it’s good to look at relevant open-source datasets. There are \nfewer of these compared to instruction datasets, but you can find high-quality preference data-\nsets on the Hugging Face Hub. These can be used for specific tasks or to add to your own dataset. \nWell-known preference datasets include the Anthropic HH-RLHF dataset, which has human \npreferences for helpful and harmless AI responses, and the OpenAI Summarize from Human \nFeedback dataset, which focuses on article summaries.\nDPO datasets can be created using various methods, each with its own trade-offs between quality, \ncost, and scalability. These methods can be tailored to specific applications and require varying \ndegrees of human feedback. We divide them into four main categories:\n• \nHuman-generated, human-evaluated datasets: This method involves hiring people to \nboth create responses to prompts and evaluate the quality of these responses. While this \napproach can capture nuanced human preferences and is ideal for complex tasks, it’s \nextremely resource-intensive and difficult to scale. As a result, it’s primarily used by large \nAI companies with substantial resources.\n•\nHuman-generated, LLM-evaluated datasets: This method can be useful if you have \na lot of existing human-generated content. However, it’s rarely used in practice due to \ninefficiency, as it still requires significant human input for response generation while \npotentially missing nuanced preferences during the LLM evaluation stage.\n",
      "content_length": 2689,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "Fine-Tuning with Preference Alignment\n234\n•\nLLM-generated, human-evaluated datasets: This method offers a good balance between \nquality and efficiency. LLMs generate multiple responses to prompts, and humans rank \nthese responses. This approach is often preferred because humans are generally better at \njudging answers than writing them from scratch. It allows the rapid generation of diverse \nresponses while still capturing human preferences effectively. However, it may not provide \ncreative or unexpected responses that humans might generate.\n•\nLLM-generated, LLM-evaluated datasets: Fully synthetic datasets, where both gener-\nation and evaluation are done by LLMs, are becoming increasingly common due to their \nscalability and cost-effectiveness. This method can produce massive datasets quickly and \nimproves as LLM capabilities advance. However, it requires careful prompt engineering to \nensure quality and diversity, and may perpetuate biases or limitations of the generating \nLLM.\nIn practice, human-generated datasets are expensive, difficult to scale, and not necessarily of \nthe highest quality. On the other hand, human evaluation is quite valuable but can be difficult \nto scale, which is why large datasets benefit from LLM evaluation. In addition to these high-level \nconsiderations, the way you obtain your data and how you plan to use it also need to be considered. \nFor example, applications with many users can embed a feedback mechanism to provide prefer-\nences. This can be as simple as a like and dislike score, or something more in-depth with text.\nNote that evaluation is not always required and preferences can emerge naturally from the gen-\neration process. For instance, it is possible to use a high-quality model to generate preferred \noutputs and a lower-quality or intentionally flawed model to produce less preferred alternatives. \nThis creates a clear distinction in the preference dataset, allowing more effective training of AI \nsystems to recognize and emulate high-quality outputs. The Intel/orca_dpo_pairs dataset \navailable on the Hugging Face Hub was created with this process.\nAnother approach is to compare model-generated outputs with human-written responses, which \ncan provide insights into how well the model aligns with actual human preferences and highlight \nareas where the model may be lacking. This can be used to copy a particular style and give a more \nauthentic tone to the model.\nTips for data generation\nThe data generation is consistent between instruction and preference datasets. Prompts should \nbe designed to encourage diversity and complexity in the model’s responses. By crafting prompts \nthat explicitly request different approaches or styles, we can ensure a wide range of outputs that \ncapture the varied nature of human preferences. \n",
      "content_length": 2805,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "Chapter 6\n235\nFor instance, when generating summaries, one might request variations such as concise sum-\nmaries, detailed summaries, and summaries focusing on key points. This approach not only \nproduces a diverse dataset but also helps in understanding how different styles and approaches \nalign with human preferences.\nIntroducing variability in the outputs is another crucial aspect of generating synthetic preference \ndatasets. This can be achieved by manipulating the temperature settings or employing other \nsampling methods in the LLM. Higher temperature settings tend to produce more creative and \ndiverse responses, while lower settings result in more focused and deterministic outputs. This \ncreates a trade-off between diversity and coherence, which depends on the kind of data we want \nto generate. For example, generating code requires low creativity, thus low temperature, while \nwriting articles can be high temperature.\nUsing multiple LLMs to generate samples can be better than using just one model. Some LLMs are \nbetter at specific tasks, and this approach also adds more variety. This approach is used by popular \nopen-source datasets like argilla/Capybara-Preferences, combining GPT-4 with open-weight \nmodels. The evaluation process then selects the chosen and the rejected answers.\nEvaluating preferences\nData evaluation can be performed by human raters or automated with LLMs. LLM evaluation\ninvolves developing detailed criteria, creating a prompt that clearly communicates these guide-\nlines to the LLM, and using the model to select preferred and rejected responses. While more \nscalable than human rating and allowing the consistent application of criteria, this quality of \nLLM evaluation depends directly on the model’s performance and the provided guidelines. It \nmay miss subtle human preferences or cultural nuances. However, as LLMs continue to improve, \ntheir ability to make nuanced judgments improves as well, potentially leading to higher-quality \ndatasets over time.\nImplementing LLM evaluation for preference datasets can be done through absolute scoring or \npairwise ranking. In absolute scoring, the LLM assigns a numerical score or categorical rating to \neach response based on predefined criteria. This method is straightforward but may suffer from \ninconsistency across different prompts or evaluation sessions. Pairwise ranking, on the other \nhand, involves presenting the LLM with two responses and asking it to choose the better one or \nrank them. This approach more closely mimics the format of human evaluation and can lead to \nmore consistent results.\n",
      "content_length": 2603,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "Fine-Tuning with Preference Alignment\n236\nFor absolute scoring, you would create a prompt that outlines the evaluation criteria and asks \nthe LLM to rate the response on a specific scale (e.g., 1-5 or poor/fair/good/excellent). The prompt \nmight look like this: “Rate the following response on a scale of 1-5 based on relevance, coherence, \nand helpfulness: [INSERT RESPONSE].” For pairwise ranking, the prompt could be: “Compare the \nfollowing two responses. Which one is better in terms of relevance, coherence, and helpfulness? \nResponse A: [INSERT RESPONSE A] Response B: [INSERT RESPONSE B].”\nThe comparative nature of preference datasets makes pairwise ranking an ideal approach for \nevaluation. This method is generally more accurate and more closely correlated to human judg-\nment than absolute scoring. Pairwise ranking mimics the natural way humans compare options, \nmaking it easier for both human raters and LLMs to provide consistent and meaningful evaluations.\nWe can further improve the accuracy of pairwise ranking by providing a ground-truth answer \nand using chain-of-thought reasoning. This approach encourages the evaluating LLM to consider \nmultiple aspects of the responses and articulate its decision-making process, leading to more \nthorough and justified evaluations. When no ground-truth answer is available, we can prompt \nthe LLM to create a grading note, which is a description of the expected answer. This technique \nworks particularly well in scenarios where the LLM doesn’t have extensive knowledge about a \ngiven topic, as it forces the model to establish clear criteria for evaluation before assessing the \nresponses.\nHere’s a concrete implementation of an LLM-as-a-judge prompt to perform pairwise ranking:\nInstruction\nYou are an answer judge. Your goal is to compare answer A and answer B. I want to know \nwhich answer does a better job of answering the instruction in terms of relevance, accuracy, \ncompleteness, clarity, structure, and conciseness.\nInstruction: {instruction}\nAnswer A: {answer_a}\nAnswer B: {answer_b}\nExplain your reasoning step by step and output the letter of the best answer using the following \nstructure:\nReasoning: (compare the two answers)\nBest answer: (A or B)\nTable 6.2 – Example of LLM-as-a-judge prompt for pairwise ranking with one instruction \nand two answers\n",
      "content_length": 2328,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "Chapter 6\n237\nHowever, it’s important to note that LLM-based evaluation can be subject to several types of bias:\n•\nPosition bias: In relative scoring, LLM judges tend to favor the first answer presented. \nThis bias can skew results and lead to inaccurate preferences.\n•\nLength bias: Similar to humans, LLM judges often show a preference for longer answers, \npotentially overlooking the quality of shorter, more concise responses.\n•\nFamily bias: LLM judges may favor responses that are generated by themselves or models \nfrom the same family, potentially due to similarities in language patterns or knowledge \nbases.\nTo mitigate these biases and enhance the quality of preference datasets, several solutions can \nbe implemented. One key approach is to randomize the order of answer A and answer B in each \ncomparison, which can counteract position bias by ensuring that the order of presentation doesn’t \nconsistently influence the evaluation. Another valuable strategy involves providing few-shot \nexamples that demonstrate a balanced distribution of scores. These examples serve to calibrate \nthe judge LLM’s internal scoring mechanism and can effectively address both length and family \nbias by illustrating that shorter answers or those from different model families can also be of \nhigh quality. Additionally, employing multiple models as a jury, rather than relying on a single \nLLM judge, can significantly improve the robustness of the evaluation process. This multi-model \napproach helps to balance out individual biases that may be present in any single model, leading \nto a more comprehensive and accurate assessment of the responses.\nIn the next section, we will create our own preference dataset. We will rely on the data generation \nprocess to naturally create chosen (human-generated) and rejected (LLM-generated) answers.\nCreating our own preference dataset\nOur model can currently write paragraphs about topics related to machine learning, but it doesn’t \nhave the same writing style as the original authors. This is a typical use case for preference align-\nment, where we want to change the “voice” of the model to closely imitate the source data. It’s \nimportant to note that, experimentally, DPO tends to make models more verbose and pushes \nthem to use very formal language. Therefore, the training will need to use DPO surgically to avoid \nthis pitfall and instead adopt the less formal style of these blog articles.\nIn this section, we will create a preference dataset where the chosen answers are extracts from \nthe text, while rejected answers are generated by the model. To implement it, we will modify the \ncode created in Chapter 5, which was designed to generate instruction datasets.\n",
      "content_length": 2713,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "Fine-Tuning with Preference Alignment\n238\nAs seen in the previous section, preference and instruction datasets rely on the same principles. \nInstead of pairs of instructions and answers, we need triples (instruction, answer 1, answer 2). \nWhat’s interesting in this setting is that we have ground-truth answers in the text chunks, which \nmeans we don’t need complex evaluation processes like LLM judges. To make sure that these \nextracts are high-quality, we will implement two additional quality filters, based on length and \npunctuation. Figure 6.2 summarizes the end-to-end process:\nFigure 6.2 – Synthetic data generation pipeline from raw text to preference dataset\nWe are now ready to implement the preference data generation pipeline:\n1. \nWe start by importing the necessary libraries.\nimport concurrent.futures\nimport json\nimport re\nfrom typing import List, Tuple\nfrom datasets import Dataset\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\n",
      "content_length": 953,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "Chapter 6\n239\n2.\nInstead of the InstructionAnswerSet class, we now have a PreferenceSet class. This \nclass is designed to handle triples of instructions, generated answers (rejected), and ex-\ntracted answers (chosen).\nclass PreferenceSet:\n    def __init__(self, triples: List[Tuple[str, str, str]]):\n        self.triples = triples\n    @classmethod\n    def from_json(cls, json_str: str) -> 'PreferenceSet':\n        data = json.loads(json_str)\n        triples = [(triple['instruction'], triple['generated_\nanswer'], triple['extracted_answer'])\n                   for triple in data['preference_triples']]\n        return cls(triples)\n    def __iter__(self):\n        return iter(self.triples)\n3. \nThe load_articles_from_json, clean_text, and extract_substrings functions remain \nunchanged from the original code. Let’s start with load_articles_from_json, which takes \nour JSON file (cleaned_documents.json) containing the articles as input and returns a \nHugging Face dataset with the text and metadata (ID, platform, author ID, author full \nname, link).\ndef load_articles_from_json(file_path: str) -> Dataset:\n    with open(file_path, \"r\") as file:\n        data = json.load(file)\n    return Dataset.from_dict(\n        {\n            \"id\": [item[\"id\"] for item in data[\"artifact_data\"]],\n            \"content\": [item[\"content\"] for item in data[\"artifact_\ndata\"]],\n            \"platform\": [item[\"platform\"] for item in \ndata[\"artifact_data\"]],\n            \"author_id\": [item[\"author_id\"] for item in \ndata[\"artifact_data\"]],\n",
      "content_length": 1520,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "Fine-Tuning with Preference Alignment\n240\n            \"author_full_name\": [item[\"author_full_name\"] for item \nin data[\"artifact_data\"]],\n            \"link\": [item[\"link\"] for item in data[\"artifact_\ndata\"]],\n        }\n    )\n4.\nThe clean_text function removes non-alphanumeric characters except for apostrophes, \nperiods, commas, exclamation marks, and question marks. It also replaces multiple \nwhitespaces with a single space to ensure proper formatting.\ndef clean_text(text: str) -> str:    text = re.sub(r\"[^\\w\\s.,!?']\", \n\" \", text)    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n5. \nThe extract_substrings function splits articles into chunks with a length between 1,000 \nand 2,000 characters. To make sure that the splitting doesn’t break sentences, which could \nmodify their meanings, we use a regex to only split after the end of a sentence.\ndef extract_substrings(dataset: Dataset, min_length: int = 1000, \nmax_length: int = 2000) -> List[str]:\n    extracts = []\n    sentence_pattern = r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)\n(?<=\\.|\\?|\\!)\\s\"\n    for article in dataset[\"content\"]:\n        cleaned_article = clean_text(article)\n        sentences = re.split(sentence_pattern, cleaned_article)\n        current_chunk = \"\"\n        for sentence in sentences:\n            sentence = sentence.strip()\n            if not sentence:\n                continue\n            if len(current_chunk) + len(sentence) <= max_length:\n                current_chunk += sentence + \" \"\n            else:\n                if len(current_chunk) >= min_length:\n                    extracts.append(current_chunk.strip())\n",
      "content_length": 1604,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "Chapter 6\n241\n                current_chunk = sentence + \" \"\n        if len(current_chunk) >= min_length:\n            extracts.append(current_chunk.strip())\n    return extracts\n6. \nThe generate_preference_triples function replaces the original generate_instruction_\nanswer_pairs function. The prompt is adapted from the instruction version and is de-\nsigned to generate triples instead of pairs. It also provides general guidance about the \ntype of instructions we’re interested in, how to extract answers from articles, and how \nto style them:\ndef generate_preference_triples(extract: str, client: OpenAI) -> \nList[Tuple[str, str, str]]:\n    prompt = f\"\"\"Based on the following extract, generate five \ninstruction-answer triples. Each triple should consist of:\n1. An instruction asking about a specific topic in the context.\n2. A generated answer that attempts to answer the instruction based \non the context.\n3. An extracted answer that is a relevant excerpt directly from the \ngiven context.\nInstructions must be self-contained and general, without explicitly \nmentioning a context, system, course, or extract.\nImportant:\n- Ensure that the extracted answer is a verbatim copy from the \ncontext, including all punctuation and apostrophes.\n- Do not add any ellipsis (...) or [...]  to indicate skipped text \nin the extracted answer.\n- If the relevant text is not continuous, use two separate sentences \nfrom the context instead of skipping text.\nProvide your response in JSON format with the following structure:\n{{\n    \"preference_triples\": [\n        {{\n            \"instruction\": \"...\",\n",
      "content_length": 1590,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "Fine-Tuning with Preference Alignment\n242\n            \"generated_answer\": \"...\",\n            \"extracted_answer\": \"...\"\n        }},\n        ...\n    ]\n}}\n    Extract:\n    {extract}\n\"\"\"\n7.\nIn the same function, we use GPT-4o-mini to generate our answers using JSON mode. We \nspecify in the system prompt that we want triples instead of pairs. The JSON answers are \ndirectly parsed by our PreferenceSet class to return the expected list of tuples.\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant who \ngenerates instruction-answer triples based on the given context. \nEach triple should include an instruction, a generated answer, and \nan extracted answer from the context. Provide your response in JSON \nformat.\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_format={\"type\": \"json_object\"},\n        max_tokens=2000,\n        temperature=0.7,\n    )\n    result = PreferenceSet.from_json(completion.choices[0].message.\ncontent)\n    return result.triples\n",
      "content_length": 1146,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "Chapter 6\n243\n8. Two new filtering functions are introduced for the preference data pipeline: filter_short_\nanswers and filter_answer_format. These functions filter out short answers and ensure \nthat answers start with an uppercase letter and end with proper punctuation. We use \nthem as heuristics to filter out samples with poor quality.\ndef filter_short_answers(dataset: Dataset, min_length: int = 100) -> \nDataset:\n    def is_long_enough(example):\n        return len(example['chosen']) >= min_length\n    return dataset.filter(is_long_enough)\ndef filter_answer_format(dataset: Dataset) -> Dataset:\n    def is_valid_format(example):\n        chosen = example['chosen']\n        return (len(chosen) > 0 and\n                chosen[0].isupper() and\n                chosen[-1] in ('.', '!', '?'))\n    return dataset.filter(is_valid_format)\n9.\nThe create_preference_dataset function replaces the original create_instruction_\ndataset function. This function now works with triples instead of pairs and uses different \ncolumn names in the resulting dataset.\ndef create_preference_dataset(dataset: Dataset, client: OpenAI, num_\nworkers: int = 4) -> Dataset:\n    extracts = extract_substrings(dataset)\n    preference_triples = []\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_\nworkers) as executor:\n        futures = [\n            executor.submit(generate_preference_triples, extract, \nclient)\n            for extract in extracts\n        ]\n        for future in tqdm(concurrent.futures.as_completed(futures), \ntotal=len(futures)):\n",
      "content_length": 1539,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "Fine-Tuning with Preference Alignment\n244\n            preference_triples.extend(future.result())\n    instructions, generated_answers, extracted_answers = \nzip(*preference_triples)\n    return Dataset.from_dict(\n        {\n            \"prompt\": list(instructions),\n            \"rejected\": list(generated_answers),\n            \"chosen\": list(extracted_answers)\n        }\n    )\n10. The main function is updated to include the new filtering steps and to use the preference \ndataset creation function:\ndef main(dataset_id: str) -> Dataset:\n    client = OpenAI()\n    # 1. Load the raw data\n    raw_dataset = load_articles_from_json(\"cleaned_documents.json\")\n    print(\"Raw dataset:\")\n    print(raw_dataset.to_pandas())\n    # 2. Create preference dataset\n    dataset = create_preference_dataset(raw_dataset, client)\n    print(\"Preference dataset:\")\n    print(dataset.to_pandas())\n    # 3. Filter out samples with short answers\n    dataset = filter_short_answers(dataset)\n    # 4. Filter answers based on format\n    dataset = filter_answer_format(dataset)\n    # 5. Export\n    dataset.push_to_hub(dataset_id)\n    return dataset\n",
      "content_length": 1117,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "Chapter 6\n245\nThe create_preference_dataset() function generated 2,970 samples. This dataset is then heav-\nily filtered to only retain 1,467 samples by removing answers that are too short or not properly \nformatted (for example, answers that start with an uppercase letter or end with a period, excla-\nmation mark, or question mark).\nThe final dataset is available on the Hugging Face Hub at the following address: https://\nhuggingface.co/datasets/mlabonne/llmtwin-dpo. You can see in Figure 6.3 an example that \ncaptures a subtle nuance in terms of writing style. Both answers are correct, but the chosen (ex-\ntracted) answer sounds slightly more casual.\nFigure 6.3 – Screenshot of the mlabonne/llmtwin-dpo preference dataset on the Hugging \nFace Hub\nTo produce this dataset, we iterated many times over the prompt to generate the data. This re-\nquired some manual evaluation and experiments until we reached satisfying results. The quality \nof the prompt is fundamental in this process, which is why it is recommended to follow a similar \nprocess to generate your own preference datasets.\nIn the next section, we will introduce concepts related to Reinforcement Learning from Human \nFeedback (RLHF) and DPO. This will cover new parameters and ideas that are implemented in \nthe final section of this chapter.\nPreference alignment\nPreference alignment regroups techniques to fine-tune models on preference data. In this section, \nwe provide an overview of this field and then focus on the technique we will implement: Direct \nPreference Optimization (DPO).\n",
      "content_length": 1558,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "Fine-Tuning with Preference Alignment\n246\nReinforcement Learning from Human Feedback\nReinforcement Learning from Human Feedback (RLHF) combines reinforcement learning\n(RL) with human input to align models with human preferences and values. RLHF emerged as a \nresponse to challenges in traditional RL methods, particularly the difficulty of specifying reward \nfunctions for complex tasks and the potential for misalignment between engineered rewards \nand intended objectives.\nThe origins of RLHF can be traced back to the field of preference-based reinforcement learning\n(PbRL), which was independently introduced by Akrour et al. and Cheng et al. in 2011. PbRL aimed \nto infer objectives from qualitative feedback, such as pairwise preferences between behaviors, \nrather than relying on quantitative reward signals. This approach addressed some of the limita-\ntions of conventional RL, where defining appropriate reward functions can be challenging and \nprone to reward hacking or unintended behaviors.\nThe term RLHF was coined later, around 2021-2022, as the approach gained prominence in the \ncontext of training LLMs. However, the core ideas had been developing for years prior. A seminal \npaper by Christiano et al. in 2017 demonstrated the effectiveness of learning reward models from \nhuman preferences and using them to train RL agents. This work showed that RLHF could match \nor exceed the performance of agents trained on hand-engineered rewards, but with significantly \nless human effort.\nAt its core, RLHF works by iteratively improving both a reward model and a policy:\n•\nReward model learning: Instead of using a pre-defined reward function, RLHF learns a \nreward model from human feedback. This is typically done by presenting humans with \ndifferent answers and asking them to indicate which one they prefer. These preferences \nare used to train a reward model, often using a Bradley-Terry model or similar approaches \nthat map preferences to underlying utility functions.\n•\nPolicy optimization: With the learned reward model, standard RL algorithms can be \nused to optimize a policy. This policy generates new behaviors that aim to maximize the \npredicted rewards from the learned model.\n•\nIterative improvement: As the policy improves, it generates new behaviors that can be \nevaluated by humans, leading to refinements in the reward model. This cycle continues, \nideally resulting in a policy that aligns well with human preferences.\nA key innovation in RLHF is its approach to handling the high cost of human feedback. Rather \nthan requiring constant human oversight, RLHF allows for asynchronous and sparse feedback.\n",
      "content_length": 2636,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "Chapter 6\n247\nThe learned reward model serves as a proxy for human preferences, enabling the RL algorithm \nto train continuously without direct human input for every action.\nAs an example, Figure 6.4 shows a high-level view of the Proximal Policy Optimization (PPO) \nalgorithm, which is one of the most popular RLHF algorithms. Here, the reward model is used to \nscore the text that is generated by the trained model. This reward is regularized by an additional \nKullback–Leibler (KL) divergence factor, ensuring that the distribution of tokens stays similar \nto the model before training (frozen model).\nFigure 6.4 – High-level view of the PPO algorithm for preference alignment\nWhile RLHF has proven effective for aligning AI systems with human preferences, it faces chal-\nlenges due to its iterative nature and reliance on a separate reward model, which can be compu-\ntationally expensive and potentially unstable. Despite theoretical superiority, RLHF algorithms \nhave also experimentally underperformed compared to simpler approaches. One such approach \nthat has gained significant attention is DPO.\n",
      "content_length": 1105,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "Fine-Tuning with Preference Alignment\n248\nDirect Preference Optimization\nIntroduced by Rafailov et al. in their 2023 paper Direct Preference Optimization: Your Language \nModel is Secretly a Reward Model, DPO offers a streamlined alternative to traditional RLHF methods.\nDPO’s core innovation lies in its reformulation of the preference learning problem. Unlike RLHF, \nwhich typically involves training a separate reward model and then using reinforcement learning \nalgorithms like PPO to fine-tune the language model, DPO takes a more direct approach. \nIt derives a closed-form expression for the optimal policy under the standard RLHF objective of \nmaximizing expected reward subject to a KL-divergence constraint with a reference policy. This \nmathematical insight allows DPO to express the preference learning problem directly in terms of \nthe policy, eliminating the need for a separate reward model or complex reinforcement learning \nalgorithms.\nIn practical terms, DPO can be implemented as a simple binary cross-entropy loss function that \noperates directly on the language model’s output probabilities. This loss function encourages the \nmodel to assign higher probability to preferred responses and lower probability to non-preferred \nresponses, while maintaining closeness to a reference (frozen) model. The importance of the ref-\nerence model is directly controlled via a beta parameter between 0 and 1. The reference model is \nignored when beta is equal to 0, which means that the trained model can be very different from \nthe SFT one. In practice, a value of 0.1 is the most popular one, but this can be tweaked, as we’ll \nsee in the next section.\nThe simplicity of this approach allows optimization using standard gradient descent techniques, \nwithout the need for sampling from the model during training or implementing complex RL \nalgorithms. Figure 6.5 shows a high-level view of the DPO algorithm, greatly simplifying the \ntraining process compared to Figure 6.4.\n",
      "content_length": 1982,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "Chapter 6\n249\nFigure 6.5 – High-level view of the DPO algorithm for preference alignment\nDPO has several advantages over traditional RLHF methods. As previously mentioned, it signifi-\ncantly simplifies the preference learning pipeline, reducing the engineering complexity associated \nwith RLHF methods. By eliminating the need for a separate reward model and RL algorithms, DPO \nis more computationally efficient than traditional RLHF approaches. Particularly when trained \nwith adapters (LoRA, QLoRA), the frozen and trained models don’t have to be separated. Indeed, \nsince we’re only training adapters, the trained model is not modified. This allows us to only load \none model instead of two, which saves additional VRAM.\nDespite its simplicity, DPO often matches the performance of more complex RLHF methods. It \nalso tends to be more stable during training and less sensitive to hyperparameters. The simpli-\nfied approach makes DPO easier to implement and scale, particularly for small teams without \nextensive RL knowledge.\n",
      "content_length": 1030,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "Fine-Tuning with Preference Alignment\n250\nWhile RLHF allows iterative improvement through multiple training rounds and can dynamically \nadapt to new preferences, DPO offers a more straightforward path to achieving similar results. \nThe choice between DPO and PPO-based RLHF often comes down to a trade-off between ease of \nimplementation and potential peak performance. For large-scale training runs with millions of \npreference samples, PPO-inspired methods still have a higher performance ceiling. However, for \nmost applications, DPO provides the majority of the performance benefits at a lower computa-\ntional and engineering cost.\nBoth RLHF and DPO benefit significantly from the integration of synthetic data. As LLMs become \nmore capable, they can generate data that surpasses human-created content in quality and di-\nversity. This enables a virtuous cycle where better models produce better training data, which \nin turn leads to further model improvements. The iterative nature of both approaches allows \nmultiple rounds of model refinement, each focusing on different aspects of model performance \nand gradually enhancing capabilities across various domains.\nDespite its advantages, DPO is not without drawbacks. Like RLHF, DPO still requires paired pref-\nerence data, which can be expensive and time-consuming to collect. DPO lacks some of the the-\noretical guarantees associated with reinforcement learning approaches. There may be scenarios \nwhere the added flexibility of RLHF is beneficial, particularly for complex tasks or environments.\nNonetheless, DPO is ideal in most cases, including our twin LLM example. In the next section, \nwe will implement it using Unsloth.\nImplementing DPO\nIn this section, we will DPO fine-tune the TwinLlama-3 1-8B model we created in Chapter 5. For \nease of use and to maximize performance, we will again use the Unsloth library for our DPO im-\nplementation. Depending on the available VRAM, you can choose between LoRA (higher quality, \nspeed, and VRAM usage) and QLoRA (lower quality, speed, and VRAM usage). This technique, \nalong with other preference alignment algorithms, is also available in TRL and Axolotl.\nThis example can be seen as an advanced application of DPO. Indeed, our objective of imitating \na writing style conflicts with the natural tendency of DPO to encourage formal language. This is \npartly due to the fact that chosen answers are often more formal than rejected ones. In practice, \nthis will force us to do light fine-tuning, with a low learning rate and number of epochs. To find \nthe best hyperparameters, we trained over 20 models and compared their outputs on a set of \nquestions, including “Write a paragraph to introduce supervised fine-tuning.” This allowed us \nto select the model and parameters that worked best for this task.\n",
      "content_length": 2811,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "Chapter 6\n251\nThe dependencies are the same as those in Chapter 5 with SFT and can be found in the book’s \nGitHub repository (https://github.com/PacktPublishing/LLM-Engineering) or in Unsloth’s \nrepo (https://github.com/unslothai/unsloth):\n1.\nFirst, we want to access a gated model and (optionally) upload our fine-tuned model to \nHugging Face (https://huggingface.co/). This requires us to log in to an account. If \nyou don’t have an account, you can create one and store your API key (Settings | Access \nTokens | Create new token) in the .env file:\nHF_TOKEN = YOUR_API_KEY\n2. \nMake sure that your Comet ML API key is also in the .env file. Otherwise, the code will \ncrash and raise an error when training starts.\nCOMET_API_KEY = YOUR_API_KEY\n3. \nBefore we import all the necessary packages, we want to apply a patch for the DPOTrainer\nclass from TRL. This fixes the DPO logs in notebook environments.\nfrom unsloth import PatchDPOTrainer\nPatchDPOTrainer()\n4. \nWe can now import the other libraries. The main difference between DPO and SFT is the \nimport of DPOConfig and DPOTrainer from TRL, which are specific to DPO training.\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth import FastLanguageModel, is_bfloat16_supportedfrom trl \nimport DPOConfig, DPOTrainer\n5. \nThis step loads our fine-tuned model from Chapter 5. We use the same configuration with \na max_seq_length of 2048. You can activate QLoRA by setting load_in_4bit to True. In \nthe following, we will perform LoRA DPO fine-tuning for increased speed and quality.\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"mlabonne/TwinLlama-3.1-8B\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=False,\n)\n",
      "content_length": 1787,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "Fine-Tuning with Preference Alignment\n252\n6. \nLet’s now prepare the model for PEFT with the LoRA configuration. We increase the rank \n(r) and lora_alpha from 32 (as it was in Chapter 5) to 64. This will allow more expressive \nfine-tuning. We keep a dropout of 0 for speed and we target every linear module as per \nusual.\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_\nproj\", \"o_proj\", \"gate_proj\"],\n)\n7. \nWe load the llmtwin-dpo dataset (training split), which contains our prompts, chosen, \nand rejected answers.\ndataset = load_dataset(\"mlabonne/llmtwin-dpo\", split=\"train\")\n8. The data preparation is significantly different from the SFT example in Chapter 5. Here, we \nhave triples with a prompt, a chosen answer, and a rejected answer. In the format_samples\nfunction, we apply the Alpaca chat template to each individual message. Note that the \ninstruction is the only one that requires the chat format: chosen and rejected answers \nonly need to be concatenated with the end of sentence (EOS) token. Finally, we create a \ntrain/test split with a 95%/5% ratio.\nalpaca_template = \"\"\"Below is an instruction that describes a task. \nWrite a response that appropriately completes the request.\n### Instruction:\n{}\n### Response:\n\"\"\"\nEOS_TOKEN = tokenizer.eos_token\ndef format_samples(example):\n    example[\"prompt\"] = alpaca_template.format(example[\"prompt\"])\n    example[\"chosen\"] = example['chosen'] + EOS_TOKEN\n",
      "content_length": 1534,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "Chapter 6\n253\n    example[\"rejected\"] = example['rejected'] + EOS_TOKEN\n    return {\"prompt\": example[\"prompt\"], \"chosen\": \nexample[\"chosen\"], \"rejected\": example[\"rejected\"]}\ndataset = dataset.map(format_samples)\ndataset = dataset.train_test_split(test_size=0.05)\n9. \nThe model and data are now ready, so we can start fine-tuning. Compared to SFT, there \nare a few new parameters, like ref_model and beta. Since we’re using LoRA (or QLoRA), \nwe don’t directly train the model but instead the adapters. This means we can use the \noriginal model (without adapters) as a reference, saving a lot of VRAM. The beta param-\neter controls the importance of the reference model. A standard value of 0.1 works well \nin most scenarios, but we decided to increase it to 0.5 based on our experiments. This is \ndue to the fact that the trained model used formal language with lower values. Having it \ncloser to the reference model helps to fix this issue.\nThe learning rate is also lower (from 3e-4 for SFT to 2e-6 here). We train for 1 epoch instead \nof 3, and the max_seq_length parameter is now broken down into two new parameters: \nmax_prompt_length (prompt only) and max_length (prompt and answer). Note that we \nalso replaced the TrainingArguments class with DPOConfig.\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=None,\n    tokenizer=tokenizer,\n    beta=0.5,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    max_length=max_seq_length//2,\n    max_prompt_length=max_seq_length//2,\n    args=DPOConfig(\n        learning_rate=2e-6,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        gradient_accumulation_steps=8,\n        num_train_epochs=1,\n",
      "content_length": 1734,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "Fine-Tuning with Preference Alignment\n254\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        eval_strategy=\"steps\",\n        eval_steps=0.2,\n        logging_steps=1,\n        report_to=\"comet_ml\",\n        seed=0,\n    ),\n)\ntrainer.train()\n10. Once the model is trained, we can run it for a quick sanity check. This step is similar to the \nSFT example. It prepares the model for inference and generates a response to a prompt.\nFastLanguageModel.for_inference(model)\nmessage = alpaca_template.format(\"Write a paragraph to introduce \nsupervised fine-tuning.\", \"\")\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer=text_streamer, max_new_\ntokens=256, use_cache=True)\n11. The trained DPO model returns the following response:\nSupervised fine-tuning is a method used to enhance the performance \nof pre-trained language models by utilizing labeled data. This \ntechnique involves taking a pre-trained model and refining it on \na specific task, such as content creation or customer service. By \nproviding the model with relevant data and guidance, it can learn to \ngenerate outputs that align more closely with the desired outcomes. \nThis approach allows for the creation of more specialized models \nthat can tackle complex tasks with greater accuracy and efficiency.\n",
      "content_length": 1492,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "Chapter 6\n255\nWe can compare it with the answer provided by the SFT model:\nSupervised fine-tuning is a method used to enhance a language model \nby providing it with a curated dataset of instructions and their \ncorresponding answers. This process is designed to align the model's \nresponses with human expectations, thereby improving its accuracy \nand relevance. The goal is to ensure that the model can respond \neffectively to a wide range of queries, making it a valuable tool \nfor applications such as chatbots and virtual assistants.\nThe DPO model provides an answer that is both more accurate and closer to the desired \nwriting style. It correctly identifies pre-training language models as source models for \nSFT. It also mentions domain or task-specific finetunes instead of alignment with “human \nexpectations,” which is closer to the preference alignment stage. The answer is also less \nformal and something we would use in a blog post.\n12. Finally, the last step consists of saving the trained model locally and pushing it to the \nHugging Face Hub.\nmodel.save_pretrained_merged(\"model\", tokenizer, save_\nmethod=\"merged_16bit\")\nCongratulations! We have trained and exported our DPO model. It is now available on the Hug-\nging Face Hub at https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO. Compared to \nSFT, DPO has a few additional metrics that need to be tracked during training. Figure 6.6 shows \nthe Comet ML dashboard with the main metrics. You can publicly access it using the following \nURL: https://www.comet.com/mlabonne/llm-twin-training/\n",
      "content_length": 1559,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "Fine-Tuning with Preference Alignment\n256\nFigure 6.6 – Experiment tracking in Comet ML with DPO metrics\nLet’s review these metrics:\n•\nTraining loss: We still want the loss to continuously decrease on average. Note that it can \nrapidly fall to zero, meaning that the model is no longer learning anything. This behavior \ndoesn’t necessarily lead to overfitting or bad models but needs to be monitored closely.\n•\nValidation loss: The same thing can be said about the validation loss. We expect a small \ngap compared to the training loss.\n•\nGradient norm: We expect small gradient norms with few spikes.\n•\nRewards: We have two different rewards: chosen and rejected. They correspond to the \nmean difference between the log probabilities output by the trained and reference mod-\nels. Over time, we expect the model to choose the chosen answers and reject the rejected \nanswers, which means that the gap between them should increase. This difference is \ndirectly tracked by the margins metric, defined as the difference between chosen and \nrejected rewards. A well-trained model’s margin will quickly increase and then plateau.\n",
      "content_length": 1122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "Chapter 6\n257\n• \nAccuracies: This metric represents the percentage of times the model correctly identifies \nthe chosen answers. We want this accuracy to gradually increase during training, but it \ndoesn’t need to reach 100%. An accuracy of 100%, especially if it’s achieved quickly, in-\ndicates that the preference dataset might be too easy for the model. While the LLM can \nstill learn from such a dataset, it might be beneficial to add more challenging examples.\nIn general, DPO is slightly harder to monitor and debug than SFT because it’s a more complex pro-\ncess, involving a reference model. However, it’s also significantly easier to use than PPO and other \nRLHF algorithms. As long as you have a high-quality preference dataset and a strong fine-tuned \nmodel, you can experiment with different ranks, beta parameters, learning rates, and number of \nepochs to see which experiment best captures your preferences.\nWhile this is not the purpose of this chapter, it is possible to automate the evaluation of models \ndesigned to imitate a writing style. A possible solution consists of comparing the distribution of \nwords in the text generated by different models (SFT and DPO) with our ground-truth dataset. \nIn this example, we expect the SFT model to output a lot of words that are overrepresented in \nGPT-4o-mini (like “delve into”). The distribution output by our DPO model should be a lot closer \nto the chosen answers.\nSummary\nThis chapter explored preference alignment techniques for improving LLMs. It introduced the \nconcept of preference datasets, explaining their structure and importance in capturing nuanced \nhuman preferences. We implemented our own custom preference data generation pipeline by \ncomparing original and AI-generated text from real articles. This pipeline can be reused and \ncustomized based on your use case.\nWe also provided an overview of the evolution of RLHF, leading to the introduction of DPO as a \nsimpler and more efficient alternative. Finally, we implemented DPO using the Unsloth library to \nfine-tune our TwinLlama-3.1-8B model from Chapter 5. Our step-by-step tutorial gave practical \ninstructions for training the model, as well as highlighting key differences from SFT. The final \nmodel is available on the Hugging Face Hub.\nIn the next chapter, we will explore the crucial topic of LLM evaluation, addressing the challenges \nand current approaches in assessing LLM performance. We’ll cover the creation of domain-specific \nevaluation sets, examine why evaluation remains a persistent problem in the field, and introduce \nthe concept of using larger models to evaluate smaller ones (LLM-as-a-judge). The chapter will \nconclude with a comprehensive evaluation pipeline, providing a structured framework for con-\nsistent and effective LLM evaluation.\n",
      "content_length": 2800,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "Fine-Tuning with Preference Alignment\n258\nReferences\n•\nRafael Rafailov et al.. “Direct Preference Optimization: Your Language Model is Secretly a \nReward Model.” arXiv preprint arXiv:2305.18290, May 2023.\n•\nTimo Kaufmann et al.. “A Survey of Reinforcement Learning from Human Feedback.” arXiv \npreprint arXiv:2312.14925, December 2023.\n• \nAnthropic. “GitHub - anthropics/hh-rlhf: Human preference data for “Training a Helpful and \nHarmless Assistant with Reinforcement Learning from Human Feedback”.” github.com, 2022, \nhttps://github.com/anthropics/hh-rlhf.\n• \nNisan Stiennon et al.. “Learning to summarize from human feedback.” arXiv preprint arX-\niv:2009.01325, September 2020.\n• \nIntel(R) Neural Compressor. “Supervised Fine-Tuning and Direct Preference Optimization \non Intel Gaudi2.” medium.com, March 26, 2024, https://medium.com/intel-analytics-\nsoftware/the-practice-of-supervised-finetuning-and-direct-preference-\noptimization-on-habana-gaudi2-a1197d8a3cd3.\n• \nArgilla. “GitHub - argilla-io/distilabel.” github.com, August 23, 2024, https://github.\ncom/argilla-io/distilabel.\n• \nDatabricks. “Enhancing LLM-as-a-Judge with Grading Notes.” databricks.com, July 22, 2024, \nhttps://www.databricks.com/blog/enhancing-llm-as-a-judge-with-grading-notes.\n• \nAkrour, Riad & Schoenauer, Marc & Sebag, Michèle. (2011). Preference-Based Policy Learn-\ning. 12-27. 10.1007/978-3-642-23780-5_11.\n• \nCheng, Weiwei & Fürnkranz, Johannes & Hüllermeier, Eyke & Park, Sang-Hyeun. (2011). \nPreference-Based Policy Iteration: Leveraging Preference Learning for Reinforcement Learning. \n312-327. 10.1007/978-3-642-23780-5_30.\n•\nPaul Christiano et al.. “Deep reinforcement learning from human preferences.” arXiv preprint \narXiv:1706.03741, June 2017.\n•\nLong Ouyang et al.. “Training language models to follow instructions with human feedback.” \narXiv preprint arXiv:2203.02155, March 2022.\n•\nJohn Schulman et al.. “Proximal Policy Optimization Algorithms.” arXiv preprint arX-\niv:1707.06347, July 2017.\n•\nunslothai. “GitHub - unslothai/unsloth: Finetune Llama 3.1, Mistral, Phi & Gemma LLMs \n2-5x faster with 80% less memory.” github.com, August 21, 2024, https://github.com/\nunslothai/unsloth.\n",
      "content_length": 2182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "Chapter 6\n259\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 157,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": "7\nEvaluating LLMs\nLLM evaluation is a crucial process used to assess the performance and capabilities of LLM models. \nIt can take multiple forms, such as multiple-choice question answering, open-ended instructions, \nand feedback from real users. Currently, there is no unified approach to measuring a model’s \nperformance but there are patterns and recipes that we can adapt to specific use cases.\nWhile general-purpose evaluations are the most popular ones, with benchmarks like Massive \nMulti-Task Language Understanding (MMLU) or LMSYS Chatbot Arena, domain- and task-spe-\ncific models benefit from more narrow approaches. This is particularly true when dealing with \nentire LLM systems (as opposed to models), often centered around a retrieval-augmented gen-\neration (RAG) pipeline. In these scenarios, we need to expand our evaluation framework to en-\ncompass the entire system, including new modules like retrievers and post-processors.\nIn this chapter, we will cover the following topics:\n•\nModel evaluation\n•\nRAG evaluation\n•\nEvaluating TwinLlama-3.1-8B\nBy the end of this chapter, you will know the most popular LLM evaluations and how to evaluate \nmodels and RAG systems using different techniques.\nModel evaluation\nIn model evaluation, the objective is to assess the capabilities of a single model without any \nprompt engineering, RAG pipeline, and so on. \n",
      "content_length": 1368,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "Evaluating LLMs\n262\nThis evaluation is essential for several reasons, such as selecting the most relevant LLM or making \nsure that the fine-tuning process actually improved the model. In this section, we will compare \nML and LLM evaluation to understand the main differences between these two fields. We will \nthen explore benchmarks for general-purpose, domain-specific, and task-specific models.\nComparing ML and LLM evaluation\nML evaluation is centered on assessing the performance of models designed for tasks like pre-\ndiction, classification, and regression. Unlike the evaluation of LLMs, which often focuses on \nhow well a model understands and generates language, ML evaluation is more concerned with \nhow accurately and efficiently a model can process structured data to produce specific outcomes.\nThis difference comes from the nature of the tasks these models handle. ML models are gener-\nally designed for narrowly defined problems, such as predicting stock prices or detecting out-\nliers, which often involve numerical or categorical data, making the evaluation process more \nstraightforward. On the other hand, LLMs are tasked with interpreting and generating language, \nwhich adds a layer of subjectivity to the evaluation process. Instead of relying solely on numerical \nbenchmarks, LLM evaluation requires a more nuanced approach and often incorporates qualita-\ntive assessments, examining how well the model produces coherent, relevant, and contextually \naccurate responses in natural language.\nIn particular, we can see three key differences in how these models work, which impact the \nevaluation process:\n• \nNumerical metrics: Evaluating ML models typically involves measuring objective per-\nformance metrics, such as accuracy, precision, recall, or mean squared error, depending \non the type of task at hand. This is less clear with LLMs, which can handle multiple tasks \n(hence, multiple evaluations) and can rarely rely on the same numerical metrics.\n•\nFeature engineering: In traditional ML, a critical part of the process involves manually \nselecting and transforming relevant data features before training the model. Evaluating \nthe success of this feature engineering often becomes part of the broader model evalua-\ntion. LLMs, however, are designed to handle raw text data directly, reducing the need for \nmanual feature engineering.\n•\nInterpretability: With ML models, it is easier to interpret why a model made certain pre-\ndictions or classifications, and this interpretability can be a core part of their evaluation. \nThis direct interpretation is not possible with LLMs. However, requesting explanations \nduring the generation process can give insights into the model’s decision-making process.\n",
      "content_length": 2729,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "Chapter 7\n263\nIn the following section, we will see a more fine-grained exploration of different types of LLMs. \nWhile evaluating general-purpose models is fairly disconnected from ML evaluation, task-specific \nLLMs are more closely aligned with traditional ML.\nGeneral-purpose LLM evaluations\nGeneral-purpose evaluations refer to metrics dedicated to base and general-purpose fine-tuned \nmodels. They cover a breadth of capabilities that are correlated with knowledge and usefulness \nwithout focusing on specific tasks or domains. This allows developers to get an overview of \nthese capabilities, compare themselves with competitors, and identify strengths and weaknesses. \nBased on these results, it is possible to tweak the dataset and hyperparameters, or even modify \nthe architecture.\nWe can broadly categorize general-purpose evaluations in three phases: during pre-training, after \npre-training, and after fine-tuning.\nDuring pre-training, we closely monitor how the model learns, as shown at the end of Chapter \n5. The most straightforward metrics are low-level and correspond to how models are trained:\n•\nTraining loss: Based on the cross-entropy loss, measures the difference between the \nmodel’s predicted probability distribution and the true distribution of the next token\n•\nValidation loss: Calculates the same loss as training loss, but on a held-out validation \nset to assess generalization\n•\nPerplexity: Exponential of the cross-entropy loss, representing how “surprised” the model \nis by the data (lower is better)\n•\nGradient norm: Monitors the magnitude of gradients during training to detect potential \ninstabilities or vanishing/exploding gradients\nIt’s also possible to include benchmarks like HellaSwag (common sense reasoning) during this \nstage but there’s a risk of overfitting these evaluations.\nAfter pre-training, it is common to use a suite of evaluations to evaluate the base model. This \nsuite can include internal and public benchmarks. Here’s a non-exhaustive list of common public \npre-training evaluations:\n•\nMMLU (knowledge): Tests models on multiple-choice questions across 57 subjects, from \nelementary to professional levels\n•\nHellaSwag (reasoning): Challenges models to complete a given situation with the most \nplausible ending from multiple choices\n",
      "content_length": 2292,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "Evaluating LLMs\n264\n• \nARC-C (reasoning): Evaluates models on grade-school-level multiple-choice science \nquestions requiring causal reasoning\n• \nWinogrande (reasoning): Assesses common sense reasoning through pronoun resolution \nin carefully crafted sentences\n• \nPIQA (reasoning): Measures physical common sense understanding through questions \nabout everyday physical interactions\nMany of these datasets are also used to evaluate general-purpose fine-tuned models. In this \ncase, we focus on the difference in a given score between the base and the fine-tuned model. For \nexample, bad fine-tuning can degrade the knowledge of the model, measured by MMLU. On the \ncontrary, a good one might instill even more knowledge and increase the MMLU score.\nThis can also help identify any contamination issues, where the model might have been fine-\ntuned on data that is too close to a test set. For instance, improving the MMLU score of a base \nmodel by 10 points during the fine-tuning phase is unlikely. This is a sign that the instruction \ndata might be contaminated.\nIn addition to these pre-trained evaluations, fine-tuned models also have their own benchmarks. \nHere, we use the term “fine-tuned model” to designate a model that has been trained with su-\npervised fine-tuning (SFT) and preference alignment. These benchmarks target capabilities \nconnected to the ability of fine-tuned models to understand and answer questions. In particular, \nthey test instruction-following, multi-turn conversation, and agentic skills:\n•\nIFEval (instruction following): Assesses a model’s ability to follow instructions with \nparticular constraints, like not outputting any commas in your answer\n•\nChatbot Arena (conversation): A framework where humans vote for the best answer to \nan instruction, comparing two models in head-to-head conversations\n• \nAlpacaEval (instruction following): Automatic evaluation for fine-tuned models that is \nhighly correlated with Chatbot Arena\n•\nMT-Bench (conversation): Evaluates models on multi-turn conversations, testing their \nability to maintain context and provide coherent responses\n•\nGAIA (agentic): Tests a wide range of abilities like tool use and web browsing, in a multi-\nstep fashion\nUnderstanding how these evaluations are designed and used is important to choose the best LLM \nfor your application. For example, if you want to fine-tune a model, you want the best base model \nin terms of knowledge and reasoning for a given size. This allows you to compare the capabilities \nof different LLMs and pick the one that will offer the strongest foundation for your fine-tuning.\n",
      "content_length": 2607,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "Chapter 7\n265\nEven if you don’t want to fine-tune a model, benchmarks like Chatbot Arena or IFEval are a good \nway to compare different instruct models. For instance, you want great conversational abilities \nif you’re building a chatbot. However, this is not necessary if your end goal is something like \ninformation extraction from unstructured documents. In this case, you will benefit more from \nexcellent instruction-following skills to understand and execute tasks.\nWhile these benchmarks are popular and useful, they also suffer from inherent flaws. For exam-\nple, public benchmarks can be gamed by training models on test data or samples that are very \nsimilar to benchmark datasets. Even human evaluation is not perfect and is often biased toward \nlong and confident answers, especially when they’re nicely formatted (e.g., using Markdown). \nOn the other hand, private test sets have not been scrutinized as much as public ones and might \nhave their own issues and biases.\nThis means that benchmarks are not a single source of truth but should be used as signals. Once \nmultiple evaluations provide a similar answer, you can raise your confidence level about the real \ncapabilities of a model.\nDomain-specific LLM evaluations\nDomain-specific LLMs don’t have the same scope as general-purpose models. This is helpful to \ntarget more fine-grained capabilities with more depth than the previous benchmarks.\nWithin the category, the choice of benchmarks entirely depends on the domain in question. For \ncommon applications like a language-specific model or a code model, it is recommended to \nsearch for relevant evaluations and even benchmark suites. These suites encompass different \nbenchmarks and are designed to be reproducible. By targeting different aspects of a domain, they \noften capture domain performance more accurately.\nTo illustrate this, here is a list of domain-specific evaluations with leaderboards on the Hugging \nFace Hub:\n•\nOpen Medical-LLM Leaderboard: Evaluates the performance of LLMs in medical ques-\ntion-answering tasks. It regroups 9 metrics, with 1,273 questions from the US medical li-\ncense exams (MedQA), 500 questions from PubMed articles (PubMedQA), 4,183 questions \nfrom Indian medical entrance exams (MedMCQA), and 1,089 questions from 6 sub-cate-\ngories of MMLU (clinical knowledge, medical genetics, anatomy, professional medicine, \ncollege biology, and college medicine).\n",
      "content_length": 2416,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "Evaluating LLMs\n266\n• \nBigCodeBench Leaderboard: Evaluates the performance of code LLMs, featuring two main \ncategories: BigCodeBench-Complete for code completion based on structured docstrings, \nand BigCodeBench-Instruct for code generation from natural language instructions. Mod-\nels are ranked by their Pass@1 scores using greedy decoding, with an additional Elo rating \nfor the Complete variant. It covers a wide range of programming scenarios that test LLMs’ \ncompositional reasoning and instruction-following capabilities.\n• \nHallucinations Leaderboard: Evaluates LLMs’ tendency to produce false or unsupported \ninformation across 16 diverse tasks spanning 5 categories. These include Question Answer-\ning (with datasets like NQ Open, TruthfulQA, and SQuADv2), Reading Comprehension (using \nTriviaQA and RACE), Summarization (employing HaluEval Summ, XSum, and CNN/DM), \nDialogue (featuring HaluEval Dial and FaithDial), and Fact Checking (utilizing MemoTrap, \nSelfCheckGPT, FEVER, and TrueFalse). The leaderboard also assesses instruction-follow-\ning ability using IFEval.\n•\nEnterprise Scenarios Leaderboard: Evaluates the performance of LLMs on six real-world \nenterprise use cases, covering diverse tasks relevant to business applications. The bench-\nmarks include FinanceBench (100 financial questions with retrieved context), Legal Con-\nfidentiality (100 prompts from LegalBench for legal reasoning), Writing Prompts (cre-\native writing evaluation), Customer Support Dialogue (relevance in customer service \ninteractions), Toxic Prompts (safety assessment for harmful content generation), and \nEnterprise PII (business safety for sensitive information protection). Some test sets are \nclosed-source to prevent gaming of the leaderboard. The evaluation focuses on specific \ncapabilities such as answer accuracy, legal reasoning, creative writing, contextual rele-\nvance, and safety measures, providing a comprehensive assessment of LLMs’ suitability \nfor enterprise environments.\nLeaderboards can have different approaches based on their domain. For example, BigCodeBench \nis significantly different from others because it relies on only two metrics that sufficiently cap-\nture the entire domain. On the other hand, the Hallucinations Leaderboard regroups 16 metrics, \nincluding many general-purpose evaluations. It shows that in addition to custom benchmarks, \nreusing general-purpose ones can complete your own suite.\nIn particular, language-specific LLMs often reuse translated versions of general-purpose bench-\nmarks. This can be completed with original evaluations in the native language. While some of \nthese benchmarks use machine translation, it is better to rely on human-translated evaluations \nto improve their quality. We selected the following three task-specific leaderboards and their \nrespective evaluation suites to give you an idea of how to build your own:\n",
      "content_length": 2888,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "Chapter 7\n267\n• \nOpenKo-LLM Leaderboard: Evaluates the performance of Korean LLMs using nine metrics. \nThese metrics are a combination of general-purpose benchmarks translated into Korean \n(GPQA, Winogrande, GSM8K, EQ-Bench, and IFEval) and custom evaluations (Knowledge, \nSocial Value, Harmlessness, and Helpfulness).\n• \nOpen Portuguese LLM Leaderboard: Evaluates the performance of Portuguese language \nLLMs using nine diverse benchmarks. These benchmarks include educational assessments \n(ENEM with 1,430 questions, and BLUEX with 724 questions from university entrance ex-\nams), professional exams (OAB Exams with over 2,000 questions), language understand-\ning tasks (ASSIN2 RTE and STS, FAQUAD NLI), and social media content analysis (HateBR \nwith 7,000 Instagram comments, PT Hate Speech with 5,668 tweets, and tweetSentBR).\n• \nOpen Arabic LLM Leaderboard: Evaluates the performance of Arabic language LLMs \nusing a comprehensive set of benchmarks, including both native Arabic tasks and trans-\nlated datasets. The leaderboard features two native Arabic benchmarks: AlGhafa and \nArabic-Culture-Value-Alignment. Additionally, it incorporates 12 translated benchmarks \ncovering various domains, such as MMLU, ARC-Challenge, HellaSwag, and PIQA.\nBoth general-purpose and domain-specific evaluations are designed with three main principles. \nFirst, they should be complex and challenge models to distinguish good and bad outputs. Second, \nthey should be diverse and cover as many topics and scenarios as possible. When one benchmark \nis not enough, additional ones can create a stronger suite. Finally, they should be practical and \neasy to run. This is more connected to evaluation libraries, which can be more or less complex to \nwork with. We recommend lm-evaluation-harness (github.com/EleutherAI/lm-evaluation-\nharness) from Eleuther AI and lighteval (github.com/huggingface/lighteval) from Hugging \nFace to run your benchmarks.\nTask-specific LLM evaluations\nWhile general-purpose and domain-specific evaluations indicate strong base or instruct models, \nthey cannot provide insights into how well these models work for a given task. This requires \nbenchmarks specifically designed for this purpose, measuring downstream performance.\nBecause of their narrow focus, task-specific LLMs can rarely rely on pre-existing evaluation data-\nsets. This can be advantageous because their outputs also tend to be more structured and easier \nto evaluate using traditional ML metrics. For example, a summarization task can leverage the \nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) metric, which measures the over-\nlap between the generated text and reference text using n-grams.\n",
      "content_length": 2688,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "Evaluating LLMs\n268\nLikewise, classification tasks also benefit from it and use the following classic metrics, among \nothers:\n• \nAccuracy: Accuracy refers to the proportion of correctly predicted instances compared \nto the total instances. It’s particularly useful for tasks with categorical outputs or where \nthere is a clear distinction between right and wrong answers, such as named entity rec-\nognition (NER).\n•\nPrecision: The ratio of true positive predictions to the total positive predictions made \nby the model.\n•\nRecall: The ratio of true positive predictions to the total actual positive instances.\n•\nF1 Score: The harmonic mean of precision and recall, used to balance both metrics. These \nare particularly useful in tasks such as classification or entity extraction.\nWhen the task cannot be directly mapped to a traditional ML task, it is possible to create a custom \nbenchmark. This benchmark can be inspired by general-purpose and domain-specific evaluation \ndatasets. A common and successful pattern is the use of multiple-choice question answering. In \nthis framework, the instruction consists of a question with several options. See the following \nexample with a question from the MMLU dataset (abstract algebra):\nInstruction\nFind the degree for the given field extension Q(sqrt(2), sqrt(3)) over Q.\nA. 0\nB. 4\nC. 2\nD. 6\nOutput\nB\nTable 7.1: Example from the MMLU dataset\n",
      "content_length": 1387,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "Chapter 7\n269\nThere are two main ways of evaluating models with this scheme—text generation and log-like-\nlihood evaluations:\n• \nThe first approach involves having the model generate text responses and comparing \nthose to predefined answer choices. For example, the model generates a letter (A, B, C, or \nD) as its answer, which is then checked against the correct answer. This method tests the \nmodel’s ability to produce coherent and accurate responses in a format similar to how it \nwould be used in real-world applications.\n•\nEvaluation using probabilities, on the other hand, looks at the model’s predicted probabil-\nities for different answer options without requiring text generation. For MMLU, lm-eval-\nuation-harness compares the probabilities for the full text of each answer choice. This \napproach allows for a more nuanced assessment of the model’s understanding, as it can \ncapture the relative confidence the model has in different options, even if it wouldn’t \nnecessarily generate the exact correct answer text.\nFor simplicity, we recommend the text-generation version of the evaluation that mimics human \ntest-taking. It is easier to implement, and generally more discriminative, as low-quality models \ntend to overperform on probability-based evaluations. You can adapt this technique to quiz your \nmodels about a particular task, and even expand it to specific domains.\nConversely, if the task is too open-ended, traditional ML metrics and multiple-choice question \nanswering might not be relevant. In this scenario, the LLM-as-a-judge technique introduced in \nChapter 5 can be used to evaluate the quality of the answers. If you have ground-truth answers, \nproviding them as additional context improves the accuracy of the evaluation. Otherwise, defining \ndifferent dimensions (such as relevance or toxicity, depending on your task) can also ground the \nevaluation in more interpretable categories.\nIt is recommended to use large models for evaluation and to iteratively refine your prompt. In \nthis process, the explanations outputted by the model are important for understanding errors in \nits reasoning and fixing them through additional prompt engineering. \n",
      "content_length": 2182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "Evaluating LLMs\n270\nIn order to easily parse answers, one can specify a structure in the instruction or use some kind \nof structured generation (like Outlines or OpenAI’s JSON mode). Here is an example of an in-\nstruction with a structure:\nYou are an evaluator who assesses the quality of an answer to an \ninstruction.\nYour goal is to provide a score that represents how well the answer \naddresses the instruction.\nYou will use a scale of 1 to 4, where each number represents the following:\n1. The answer is not relevant to the instruction.\n2. The answer is relevant but not helpful.\n3. The answer is relevant and helpful but could be more detailed.\n4. The answer is relevant, helpful, and detailed.\nPlease provide your evaluation as follows:\n##Evaluation##\nExplanation: (analyze the relevant, helpfulness, and complexity of the \nanswer)\nTotal rating: (final score as a number between 1 and 4)\nInstruction:\n{instruction}\nAnswer:\n{answer}\n##Evaluation##\nExplanation: \nTable 7.2: Example of general-purpose LLM-as-a-judge prompt for answer evaluation\nNaturally, you can tweak the scale, add a ground-truth answer to this prompt, and customize it \nfor your own use cases.\n",
      "content_length": 1169,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "Chapter 7\n271\nHowever, judge LLMs can exhibit biases favoring assertive or verbose responses, potentially \noverrating answers that sound more confident but are less accurate. They may also lack domain \nexpertise for specialized topics, leading to misjudgments. Consistency is also a concern, as LLMs \nmight score similar responses differently. Additionally, they could have implicit preferences for \ncertain writing styles unrelated to actual answer quality. To mitigate these issues, it’s possible to \ncombine LLM evaluations with other metrics, use multiple judges, and carefully design prompts \nto address biases.\nOnce a model has been properly evaluated and works as intended, it might be included within a \nbroader system. In the next section, we will see how systems change the evaluation framework.\nRAG evaluation\nWhile traditional LLM evaluation focuses on the model’s inherent capabilities, RAG evaluation \nrequires a more comprehensive approach that considers both the model’s generative abilities \nand its interaction with external information sources.\nRAG systems combine the strengths of LLMs with information retrieval mechanisms, allowing \nthem to generate responses that are not only coherent and contextually appropriate but also \ngrounded in up-to-date, externally sourced information. This makes RAG particularly valuable \nin fields where current and accurate information is crucial, such as news reporting, research, and \ncustomer support.\nThe evaluation of RAG systems goes beyond assessing a standalone LLM. It requires examining \nthe entire system’s performance, including:\n•\nRetrieval accuracy: How well does the system fetch relevant information?\n•\nIntegration quality: How effectively is the retrieved information incorporated into the \ngenerated response?\n•\nFactuality and relevance: Does the final output address the query appropriately while \nseamlessly blending retrieved and generated content?\nKey metrics for RAG evaluation include retrieval precision and recall, which measure the accura-\ncy and comprehensiveness of the retrieved information. Additionally, the quality of integration \nbetween retrieved data and generated text is crucial, as is the overall factuality and coherence \nof the output.\n",
      "content_length": 2232,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "Evaluating LLMs\n272\nTo illustrate how these metrics are applied in practice, consider a RAG system designed for a \ncustomer support chatbot in an e-commerce setting. In this scenario, the user asks “What’s your \nreturn policy for laptops purchased during the holiday sale?” The RAG pipeline finds relevant \ndocuments on the electronics return policy and documents on holiday sale terms. This additional \ncontext is appended at the end of the question, and the model uses it to respond:\nFor laptops purchased during our holiday sale, you have an extended return \nperiod of 60 days from the date of purchase. This is longer than our \nstandard 30-day return policy for electronics. Please ensure the laptop is \nin its original packaging with all accessories to be eligible for a full \nrefund.\nTable 7.3: Example of output from a RAG pipeline designed for customer support\nIn this pipeline, we can evaluate if the retrieved documents correspond to what was expected \n(retrieval accuracy). We can also measure the difference between responses with and without \nadditional context (integration quality). Finally, we can assess whether the output is relevant and \ngrounded in the information provided by the documents (factuality and relevance).\nIn this section, we will cover two methods to evaluate how well RAG models incorporate external \ninformation into their responses.\nRagas\nRetrieval-Augmented Generation Assessment (Ragas) is an open-source toolkit designed to \nprovide developers with a comprehensive set of tools for RAG evaluation and optimization. It’s \ndesigned around the idea of metrics-driven development (MDD), a product development ap-\nproach that relies on data to make well-informed decisions, involving the ongoing monitoring \nof essential metrics over time to gain valuable insights into an application’s performance. By \nembracing this methodology, Ragas enables developers to objectively assess their RAG systems, \nidentify areas for improvement, and track the impact of changes over time.\nOne of the key capabilities of Ragas is its ability to synthetically generate diverse and complex \ntest datasets. This feature addresses a significant pain point in RAG development, as manually \ncreating hundreds of questions, answers, and contexts is both time-consuming and labor-inten-\nsive. Instead, it uses an evolutionary approach paradigm inspired by works like Evol-Instruct to \ncraft questions with varying characteristics such as reasoning complexity, conditional elements, \nand multi-context requirements. This approach ensures a comprehensive evaluation of different \ncomponents within the RAG pipeline. \n",
      "content_length": 2626,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "Chapter 7\n273\nAdditionally, Ragas can generate conversational samples that simulate chat-based question-and-\nfollow-up interactions, allowing developers to evaluate their systems in more realistic scenarios.\nFigure 7.1: Overview of the Ragas evaluation framework\nAs illustrated in Figure 7.1, Ragas provides a suite of LLM-assisted evaluation metrics designed to \nobjectively measure different aspects of RAG system performance. These metrics include:\n•\nFaithfulness: This metric measures the factual consistency of the generated answer against \nthe given context. It works by breaking down the answer into individual claims and ver-\nifying if each claim can be inferred from the provided context. The faithfulness score is \ncalculated as the ratio of verifiable claims to the total number of claims in the answer.\n• \nAnswer relevancy: This metric evaluates how pertinent the generated answer is to the \ngiven prompt. It uses an innovative approach where an LLM is prompted to generate \nmultiple questions based on the answer and then calculates the mean cosine similarity \nbetween these generated questions and the original question. This method helps identify \nanswers that may be factually correct but off-topic or incomplete.\n•\nContext precision: This metric evaluates whether all the ground-truth relevant items \npresent in the contexts are ranked appropriately. It considers the position of relevant in-\nformation within the retrieved context, rewarding systems that place the most pertinent \ninformation at the top.\n•\nContext recall: This metric measures the extent to which the retrieved context aligns with \nthe annotated answer (ground truth). It analyzes each claim in the ground truth answer \nto determine whether it can be attributed to the retrieved context, providing insights into \nthe completeness of the retrieved information.\n",
      "content_length": 1845,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "Evaluating LLMs\n274\nFinally, Ragas also provides building blocks for monitoring RAG quality in production environ-\nments. This facilitates continuous improvement of RAG systems. By leveraging the evaluation \nresults from test datasets and insights gathered from production monitoring, developers can \niteratively enhance their applications. This might involve fine-tuning retrieval algorithms, ad-\njusting prompt engineering strategies, or optimizing the balance between retrieved context and \nLLM generation.\nRagas can be complemented with another approach, based on custom classifiers.\nARES\nARES (an automated evaluation framework for RAG systems) is a comprehensive tool designed \nto evaluate RAG systems. It offers an automated process that combines synthetic data genera-\ntion with fine-tuned classifiers to assess various aspects of RAG performance, including context \nrelevance, answer faithfulness, and answer relevance.\nThe ARES framework operates in three main stages: synthetic data generation, classifier training, \nand RAG evaluation. Each stage is configurable, allowing users to tailor the evaluation process \nto their specific needs and datasets.\nIn the synthetic data generation stage, ARES creates datasets that closely mimic real-world sce-\nnarios for robust RAG testing. Users can configure this process by specifying document file paths, \nfew-shot prompt files, and output locations for the synthetic queries. The framework supports \nvarious pre-trained language models for this task, with the default being google/flan-t5-xxl. \nUsers can control the number of documents sampled and other parameters to balance between \ncomprehensive coverage and computational efficiency.\nFigure 7.2: Overview of the ARES evaluation framework\n",
      "content_length": 1748,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "Chapter 7\n275\nThe classifier training stage involves creating high-precision classifiers to determine the relevance \nand faithfulness of RAG outputs. Users can specify the classification dataset (typically generated \nfrom the previous stage), test set for evaluation, label columns, and model choice. ARES uses mi-\ncrosoft/deberta-v3-large as the default model but supports other Hugging Face models. Training \nparameters such as the number of epochs, patience value for early stopping, and learning rate \ncan be fine-tuned to optimize classifier performance.\nThe final stage, RAG evaluation, leverages the trained classifiers and synthetic data to assess the \nRAG model’s performance. Users provide evaluation datasets, few-shot examples for guiding the \nevaluation, classifier checkpoints, and gold label paths. ARES supports various evaluation metrics \nand can generate confidence intervals for its assessments.\nARES offers flexible model execution options, supporting both cloud-based and local runs through \nvLLM integration. The framework also supports various artifact types (code snippets, documents, \nHTML, images, and so on), enabling comprehensive evaluation across different RAG system \noutputs.\nIn summary, Ragas and ARES complement each other through their distinct approaches to eval-\nuation and dataset generation. Ragas’s strength in production monitoring and LLM-assisted \nmetrics can be combined with ARES’s highly configurable evaluation process and classifier-based \nassessments. While Ragas may offer more nuanced evaluations based on LLM capabilities, ARES \nprovides consistent and potentially faster evaluations once its classifiers are trained. Combining \nthem offers a comprehensive evaluation framework, benefiting from quick iterations with Ragas \nand in-depth, customized evaluations with ARES at key stages.\nIn the next section, we will create our own evaluation framework to evaluate our task-specific \nTwinLlama-3.1-8B model.\nEvaluating TwinLlama-3.1-8B\nIn the previous chapters, we created two models fine-tuned to generate high-quality posts and \narticles: TwinLlama-3.1-8B and TwinLlama-3.1-8B-DPO. Based on this summary, we want to \nassess their abilities to write text that is both accurate and well-written. In comparison, gener-\nal-purpose fine-tuned models are accurate thanks to their extensive knowledge but often use \noverly formal and verbose language. With this fine-tuning, we want to adopt a more natural \nwriting style, based on the original articles from the training set.\n",
      "content_length": 2522,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "Evaluating LLMs\n276\nDue to the open-ended nature of this problem, we will leverage a judge LLM to evaluate the quality \nof the generated text. It will take both the instruction and the answer as inputs, and score it on \na 1–3 scale based on two criteria:\n• \nAccuracy: The degree of factual correctness and comprehensiveness of the information \npresented in the answer\n• \nStyle: The appropriateness of the tone and writing style for blog posts or social media \ncontent (no formal or academic expressions)\nIn our evaluation framework, we will use the test split of our instruction dataset to get test in-\nstructions. We will feed them to our models and generate answers. These answers will then be \nevaluated by our judge LLM (GPT-4o-mini), based on a prompt that specifies our criteria. Finally, \nwe will analyze the scores and draw conclusions based on qualitative and quantitative evaluations.\nGenerating answers\nThe first step consists of efficiently generating answers for each instruction in our test set. In ad-\ndition to our two models, we will also use meta-llama/Meta-Llama-3.1-8B-Instruct, the official \ninstruct version of Llama-3.1-8B, as a reference point to better understand the trade-offs we made.\nLet’s start the first stage of the implementation:\n1. \nWe import the relevant libraries, including vLLM for fast generation. This library is a lot \nfaster than transformers for batch generation with local models:\nfrom vllm import LLM, SamplingParams\nfrom datasets import load_dataset\nfrom tqdm.auto import tqdm\nimport gc\n2. We define a function called generate_answers that will process our dataset and generate \nresponses using a specified model. It takes two inputs—the ID of the model we want to \nuse and the name of the test dataset:\ndef generate_answers(model_id, dataset_name):\n    dataset = load_dataset(dataset_name, split=\"test\")\n3. \nWe need to format the raw instructions using the chat template our models have been \ntrained on. Note that Llama-3.1-8B-Instruct has been used with a different template, but \nit can follow this simple format. Here, we use the same chat template with every model \nfor simplicity. We map the entire test set to this template with the format() function:\n",
      "content_length": 2207,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": "Chapter 7\n277\n    def format(sample):\n        return \"Below is an instruction that describes a task. \nWrite a response that appropriately completes the request.\\n\\n### \nInstruction:\\n{}\\n\\n### Response:\\n\".format(sample[\"instruction\"])\n    dataset = dataset.map(lambda sample: {\"prompt\": format(sample)})\n4.\nLet’s initialize the LLM object used by vLLM with a maximum length of 4,096 tokens. We \ncan also specify sampling parameters, which correspond to variables used in the decod-\ning strategy. Here, we use parameters to encourage diversity (high temperature) while \nremoving the most unlikely tokens (top_p and min_p). Finally, we start the generation \nby providing the list of prompts with dataset[\"prompt\"]:\n    llm = LLM(model=model_id, max_model_len=4096)\n    sampling_params = SamplingParams(temperature=0.8, top_p=0.95, \nmin_p=0.05, max_tokens=4096)\n    outputs = llm.generate(dataset[\"prompt\"], sampling_params)\n5. \nThis process should take a few minutes with our 334 prompts. Once this is done, we ex-\ntract the answers from the object that is outputted by vLLM. We then add these answers \nas a new column to our dataset. This is useful to log the answers and review them later:\n    answers = [output.outputs[0].text for output in outputs]\n    dataset = dataset.add_column(\"answers\", answers)\n6. We save our results to the Hugging Face Hub for easy access later. Then, we clear our GPU \nmemory to prevent running out of space when we process the next model:\n    print(f\"Uploading results for {model_id}\")\n    dataset.push_to_hub(f\"mlabonne/{model_id.split('/')\n[-1]}-results\")\n    gc.collect()\n    return dataset\n7. \nWe create a list of the three models we want to test. Then, we run our generate_answers()\nfunction for each of these models, one at a time. This will create and upload a separate \nset of results for each model:\nmodel_ids = [\n    'mlabonne/TwinLlama-3.1-8B',\n    'mlabonne/TwinLlama-3.1-8B-DPO',\n",
      "content_length": 1924,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "Evaluating LLMs\n278\n    'meta-llama/Meta-Llama-3.1-8B-Instruct'\n]\nfor model_id in model_ids:\n    generate_answers(model_id, \"mlabonne/llmtwin\")\nNow that we have the answer generation, we can move on to the evaluation process.\nEvaluating answers\nTo evaluate our answers, we will rely on GPT-4o-mini as a judge. This strategy is similar to what \nwe used for data generation. As a matter of fact, you could adapt it to filter out bad samples during \nthe data generation process. Here, we will score every generated answer from every model in \nterms of accuracy and style. The average scores will inform us about the quality of our fine-tuning \ncompared to Llama-3.1-8B-Instruct:\n1. \nFirst, we import the required libraries, including openai:\nimport json\nfrom typing import List\nfrom datasets import Dataset, load_dataset\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\nimport concurrent.futures\n2. We then define the evaluate_answer() function. This function contains our evaluation \nprompt, which sets up the context for evaluating answers based on accuracy and style:\ndef evaluate_answer(\n    instruction: str, answer: str, client: OpenAI\n) -> dict:\n    prompt = f\"\"\"You are an expert judge. Please evaluate the \nquality of a given answer to an instruction based on two criteria:\n1. Accuracy: How factually correct is the information presented in \nthe answer? You are a technical expert in this topic.\n2. Style: Is the tone and writing style appropriate for a blog post \nor social media content? It should use simple but technical words \nand avoid formal or academic language.\n3. \nIn the same prompt, we define our scales for each metric. Those are three-point Likert \nscales with a precise definition for each score:\nAccuracy scale:\n",
      "content_length": 1738,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": "Chapter 7\n279\n1 (Poor): Contains factual errors or misleading information\n2 (Good): Mostly accurate with minor errors or omissions\n3 (Excellent): Highly accurate and comprehensive\nStyle scale:\n1 (Poor): Too formal, uses some overly complex words\n2 (Good): Good balance of technical content and accessibility, but \nstill uses formal words and expressions\n3 (Excellent): Perfectly accessible language for blog/social media, \nuses simple but precise technical terms when necessary\n4.\nFinally, we conclude the prompt with two examples to illustrate what we mean by “complex \nwords” and “formal or academic language.” We provide the corresponding instruction-an-\nswer pair and ask the model to return a response in JSON:\nExample of bad style: The Llama2 7B model constitutes a noteworthy \nprogression in the field of artificial intelligence, serving as the \nsuccessor to its predecessor, the original Llama architecture.\nExample of excellent style: Llama2 7B outperforms the original Llama \nmodel across multiple benchmarks.\nInstruction: {instruction}\nAnswer: {answer}\nProvide your evaluation in JSON format with the following structure:\n{{\n    \"accuracy\": {{\n        \"analysis\": \"...\",\n        \"score\": 0\n    }},\n    \"style\": {{\n        \"analysis\": \"...\",\n        \"score\": 0\n    }}\n}}\n\"\"\"\n",
      "content_length": 1285,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "Evaluating LLMs\n280\n5. \nThis prompt is given as a user query to the GPT-4o-mini model. The system prompt rein-\nforces that we are interested in answer evaluation based on accuracy and style:\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant who \nevaluates answers based on accuracy and style. Provide your response \nin JSON format with a short analysis and score for each criterion.\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_format={\"type\": \"json_object\"},\n        max_tokens=1000,\n        temperature=0.8,\n    )\n6. As in the previous chapters, we will batch our requests to speed up the process. This is \nwhy we create an evaluate_batch() function, which returns a list of parsed structured \noutputs with their corresponding indices. These indices are important to ensure a correct \nordering of the evaluations:\ndef evaluate_batch(batch, start_index):\n    client = OpenAI(api_key=OPENAI_KEY)\n    return [\n        (i, evaluate_answer(instr, ans, client))\n        for i, (instr, ans) in enumerate(batch, start=start_index)\n    ]\n7. \nWe can now orchestrate the previous code in the evaluate_answers() function. It takes \nthe model ID, number of threads, and batch size as inputs. First, we load the dataset with \nthe generations we previously saved:\ndef evaluate_answers(model_id: str, num_threads: int = 10, batch_\nsize: int = 5) -> Dataset:\n    dataset = load_dataset(f\"mlabonne/{model_id.split('/')\n[-1]}-results\", split=\"all\")\n",
      "content_length": 1632,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "Chapter 7\n281\n8. We create batches of instruction-answer pairs from our dataset. Each batch contains \nbatch_size number of pairs:\n    batches = [\n        (i, list(zip(dataset[\"instruction\"][i:i+batch_size], \ndataset[\"answers\"][i:i+batch_size])))\n        for i in range(0, len(dataset), batch_size)\n    ]\n9. \nWe perform parallel evaluation of batches of instruction-answer pairs using multiple \nthreads. We use parallel processing to evaluate multiple batches simultaneously, speed-\ning up the overall evaluation process. The ThreadPoolExecutor submits each batch to \nevaluate_batch(). The results are stored in the evaluations list:\n    evaluations = [None] * len(dataset)\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_\nthreads) as executor:\n        futures = [executor.submit(evaluate_batch, batch, start_\nindex) for start_index, batch in batches]\n        for future in tqdm(concurrent.futures.as_completed(futures), \ntotal=len(futures)):\n            for index, evaluation in future.result():\n                evaluations[index] = evaluation\n10. We create a new column with the result of the evaluation process. This column will store \nthe raw JSON output of the judge model, including scores and explanations:\n    if 'evaluation' in dataset.column_names:\n        dataset = dataset.remove_columns(['evaluation'])\n    dataset = dataset.add_column(\"evaluation\", evaluations)\n11. We can directly parse this JSON object with json.loads() and try to retrieve the accuracy \nand style scores that should have been generated. This generation is in best-effort mode, \nwhich means that scores are not guaranteed. If there’s an error in parsing, we use None\nvalues as a fallback:\n    accuracy_scores = []\n    style_scores = []\n",
      "content_length": 1733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "Evaluating LLMs\n282\n    for evaluation in dataset['evaluation']:\n        try:\n            eval_dict = json.loads(evaluation) if \nisinstance(evaluation, str) else evaluation\n            accuracy_score = eval_dict['accuracy']['score']\n            style_score = eval_dict['style']['score']\n            accuracy_scores.append(accuracy_score)\n            style_scores.append(style_score)\n        except (json.JSONDecodeError, KeyError, TypeError):\n            accuracy_scores.append(None)\n            style_scores.append(None)\n12. We add two new columns to store the accuracy and style scores for further analysis:\n    if 'accuracy' in dataset.column_names:\n        dataset = dataset.remove_columns(['accuracy'])\n    dataset = dataset.add_column('accuracy', accuracy_scores)\n    if 'style' in dataset.column_names:\n        dataset = dataset.remove_columns(['style'])\n    dataset = dataset.add_column('style', style_scores)\n13. Let’s push the final dataset with generated answers, evaluations, and scores to the Hug-\nging Face Hub:\n    dataset.push_to_hub(f\"mlabonne/{model_id.split('/')\n[-1]}-results\")\n    return dataset\n14. We can now call the evaluate_answers() function with the three models we selected:\nmodel_ids = [\n    'mlabonne/TwinLlama-3.1-8B',\n    'mlabonne/TwinLlama-3.1-8B-DPO',\n    'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
      "content_length": 1332,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": "Chapter 7\n283\n]\nfor model_id in model_ids:\n    evaluate_answers(model_id)\nBy saving intermediate results, our evaluation framework becomes more robust to failures. It is \nalso easily extendable to other models, datasets, and criteria.\nAnalyzing results\nThere are three elements we can review at the end of this evaluation process: model answers, \nexplanations from evaluations, and scores.\nWhile reviewing answers is not scalable, it is a crucial step that can help to identify a lot of com-\nmon mistakes, such as a wrong chat template or incorrect model. Here, we will focus on a single \ninstruction and read the answers provided by our two models and Llama-3.1-8B-Instruct.\nInstruction\nDiscuss the concept of algorithm bias and its implications.\nTwinLlama-3 1-8B\nAlgorithm bias refers to the tendency of algorithms to produce outcomes that are skewed \nor unfair due to underlying assumptions or programming biases. This can have significant \nimplications, particularly in fields like machine learning and AI, where algorithms are relied \nupon to make critical decisions. If an algorithm is not properly vetted for bias, it may lead \nto discriminatory outcomes, such as lending practices that disproportionately affect certain \ndemographic groups or recruitment processes that unfairly favor certain candidates. It is \nessential to ensure that algorithms are designed and implemented with fairness and accuracy \nin mind to mitigate these risks.\nTwinLlama-3 1-8B-DPO\nAlgorithm bias refers to the tendency of algorithms to produce outcomes that are skewed \nor unfair due to underlying assumptions or programming biases. This can have significant \nimplications, particularly in fields like machine learning and AI, where algorithms are relied \nupon to make critical decisions. If an algorithm is not properly vetted for bias, it could \nlead to discriminatory outcomes, such as denying loans to certain demographic groups or \nperpetuating stereotypes. Therefore, it is essential to address algorithmic bias proactively to \nensure that the technology we develop is equitable and just.\n",
      "content_length": 2081,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "Evaluating LLMs\n284\nLlama-3 1-8B-Instruct\nAlgorithm bias, also known as algorithmic bias, refers to the unintended or inherent bias in \nmachine learning models that can affect their performance, accuracy, and fairness. This bias \ncan be caused by various factors, including the data used to train the model, the algorithm \nitself, and the biases present in the society or culture. One of the most significant implications \nof algorithm bias is its potential to perpetuate and exacerbate existing social inequalities. For \ninstance, facial recognition software has been shown to be less accurate for people of color, \nleading to wrongful arrests and discrimination. Similarly, language processing models have \nbeen found to be biased towards the language and dialect of the dominant group, making it \ndifficult for marginalized communities to access information and services. Algorithm bias \ncan also have severe consequences in areas such as healthcare, education, and employment. \nFor example, a medical diagnosis algorithm that is biased towards certain demographics \nmay misdiagnose or underdiagnose conditions in other groups, leading to delayed or \ninadequate treatment. In education, an algorithm that is biased towards certain learning \nstyles may disproportionately affect students with different learning abilities, leading to \nunequal opportunities. To mitigate the effects of algorithm bias, it is essential to address \nthe root causes of bias and implement measures to ensure fairness and transparency in \nmachine learning models. This can be achieved through techniques such as data cleansing, \nregularization, and debiasing, as well as implementing diverse and representative datasets. \nAdditionally, it is crucial to involve diverse stakeholders, including members of marginalized \ncommunities, in the development and testing of algorithms to ensure that they are fair and \nequitable. In conclusion, algorithm bias is a significant concern that can have far-reaching \nimplications for individuals and society as a whole. It is essential to recognize the potential \nbiases in machine learning models and take proactive steps to mitigate their effects. By doing \nso, we can create more equitable and just systems that benefit everyone, regardless of their \nbackground or demographic characteristics.\nTable 7.4: Example of generated answers for a given instruction\nImmediately, we can see that the answers from our models are very close to each other. This is not \nthe case with Llama-3.1-8B-Instruct’s answer, which is extremely verbose and lists many examples. \nThis is typically the kind of answer that is correct but can feel artificial and unpleasant to read \nbecause of its length. On the other hand, the DPO model slightly simplifies the language of the \nSFT version, making it less academic. This is exactly the behavior we want to capture, modifying \nthe writing style but not the actual content of the answer.\nLet’s now review the evaluations provided by GPT-4o-mini for each answer.\n",
      "content_length": 3006,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "Chapter 7\n285\nTwinLlama-3.1-8B\nAccuracy\nThe answer accurately defines algorithm bias \nand highlights its implications in fields like \nmachine learning and AI. It correctly identi-\nfies the risks of discriminatory outcomes due \nto biased algorithms. There are no apparent \nfactual errors, making this a comprehensive \nresponse.\nScore: 3\nStyle\nThe tone is informative and relatively ac-\ncessible, though it tends towards a formal \ntone in some sections (e.g., ‘underlying \nassumptions or programming biases’). \nWhile it does use some technical terms \nappropriately, it could be simplified further \nfor a broader audience, especially for blog \nor social media content.\nScore: 2\nTwinLlama-3.1-8B-DPO\nAccuracy\nThe answer accurately defines algorithm bias \nand outlines its implications in critical fields \nlike machine learning and AI. It highlights the \npotential for discriminatory outcomes, making \nit a comprehensive overview of the topic with-\nout any factual errors.\nScore: 3\nStyle\nThe writing style is clear, accessible, and ef-\nfectively communicates the technical con-\ncept of algorithm bias without becoming \noverly formal. While it uses some technical \nterms, they are appropriate and explained \nwell for a general audience.\nScore: 3\nLlama-3.1-8B-Instruct\nAccuracy\nThe answer accurately defines algorithm bias \nand discusses its causes and implications in \nvarious domains such as facial recognition, \nhealthcare, and education. It also correctly \nidentifies methods to mitigate bias, such as \ninvolving diverse stakeholders and employing \ndebiasing techniques. Overall, the information \nis factually correct and comprehensive.\nScore: 3\nStyle\nThe tone is informative and accessible, \nwith technical terms explained clearly. \nHowever, some phrases like ‘unintended \nor inherent bias’ and ‘implementation of \ndiverse and representative datasets’ could \nbe simplified for broader audience appeal. \nThe writing is generally suitable for blog or \nsocial media content, but it could benefit \nfrom a more conversational tone.\nScore: 2\nTable 7.5: Evaluations of each answer made by GPT-4o-mini, according to style and ac-\ncuracy\n",
      "content_length": 2127,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": "Evaluating LLMs\n286\nAccording to our judge LLM, there is no issue with the accuracy of the answers, which get a perfect \nscore. However, the style is considered too formal for TwinLlama-3.1-8B (SFT) and Llama-3.1-\n8B-Instruct, with a score of 2. The judge LLM agreed with our previous analysis and assigned a \nperfect score to TwinLlama-3.1-8B-DPO’s answer for communicating “the technical concept of \nalgorithm bias without becoming overly formal.”\nThis trend is confirmed by the average scores obtained by each model:\nTwinLlama-3.1-8B - Accuracy: 2.45\nTwinLlama-3.1-8B - Style: 2.04\nTwinLlama-3.1-8B-DPO - Accuracy: 2.46\nTwinLlama-3.1-8B-DPO - Style: 2.12\nLlama-3.1-8B-Instruct - Accuracy: 2.62\nLlama-3.1-8B-Instruct - Style: 1.86\nIn terms of accuracy, our two fine-tuned models get similar scores, while Llama-3.1-8B-Instruct \nachieves the highest accuracy score of 2.62. This suggests that the instruct-tuned Llama model \nmay have a slight edge in providing factually correct information. This is probably due to its \nextensive post-training process with over 10 million samples (compared to 13,000 in our case).\nHowever, when it comes to style, we see a different pattern. TwinLlama-3.1-8B-DPO leads with \na score of 2.12, successfully achieving a more accessible and less formal writing style without \nsacrificing content quality. TwinLlama-3.1-8B (SFT) follows with 2.04, showing improvement but \nretaining some formality, while Llama-3.1-8B-Instruct trails with 1.86, tending toward verbosity.\nBased on this feedback and the manual review of the generated answers, we can detect mistakes \nand identify areas for improvement. This is essential for refining the data generation process \nthrough additional filtering or augmenting the dataset with missing information. While this first \nversion already shows promising results, iterating over different datasets and models will allow \nus to significantly outperform our baseline and create the best possible model for our use case.\nSummary\nIn this chapter, we explored LLM evaluation with models and RAG systems. We saw how to \ninterpret classic benchmarks like MMLU to select strong candidates to use or fine-tune. We also \ndetailed how domain-specific and task-specific evaluations work, and how to create our own \nbased on publicly available examples. \n",
      "content_length": 2311,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 316,
      "content": "Chapter 7\n287\nWe focused on two techniques (multiple-choice question answering and LLM-as-a-judge) as the \nbackbone of these custom evaluation frameworks.\nHowever, models are commonly integrated into broader systems that provide additional context. \nWe introduced two evaluation frameworks for RAG systems, Ragas and ARES. We saw both simi-\nlarities (for example, synthetic data generation) and differences in how they evaluate RAG systems \n(context-based metrics versus trained classifiers). Finally, we evaluated TwinLlama-3.1-8B with \na judge LLM according to three criteria: relevance, coherence, and conciseness. This provided \ninsights into how we can improve it.\nIn the next chapter, we will explore inference optimization techniques to improve speed and \nreduce memory usage, without significantly compromising model performance. We will also \ndelve into optimization methods, model parallelism techniques and examine different quanti-\nzation approaches.\nReferences\n• \nLianmin Zheng et al.. “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.” arXiv \npreprint arXiv:2306.05685, June 2023.\n• \nAymeric Roucher. “Using LLM-as-a-judge for an automated and versatile evaluation - Hugging \nFace Open-Source AI Cookbook.” huggingface.co, No date found, https://huggingface.co/\nlearn/cookbook/en/llm_judge.\n•\nLangChain. “Aligning LLM-as-a-Judge with Human Preferences.” blog.langchain.dev, June \n26, 2024, https://blog.langchain.dev/aligning-llm-as-a-judge-with-human-\npreferences/.\n•\nDan Hendrycks et al.. “Measuring Massive Multitask Language Understanding.” arXiv pre-\nprint arXiv:2009.03300, September 2020.\n•\nJeffrey Zhou et al.. “Instruction-Following Evaluation for Large Language Models.” arXiv \npreprint arXiv:2311.07911, November 2023.\n•\nYann Dubois et al.. “Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Eval-\nuators.” arXiv preprint arXiv:2404.04475, April 2024.\n•\nGrégoire Mialon et al.. “GAIA: a benchmark for General AI Assistants.” arXiv preprint arX-\niv:2311.12983, November 2023.\n",
      "content_length": 2020,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "Evaluating LLMs\n288\n•\nGiwon Hong et al.. “The Hallucinations Leaderboard -- An Open Effort to Measure Hallucina-\ntions in Large Language Models.” arXiv preprint arXiv:2404.05904, April 2024.\n•\nShahul Es et al.. “RAGAS: Automated Evaluation of Retrieval Augmented Generation.” arXiv \npreprint arXiv:2309.15217, September 2023.\n•\nJon Saad-Falcon et al.. “ARES: An Automated Evaluation Framework for Retrieval-Augmented \nGeneration Systems.” arXiv preprint arXiv:2311.09476, November 2023.\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 630,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "8\nInference Optimization\nDeploying LLMs is challenging due to their significant computational and memory requirements. \nEfficiently running these models necessitates the use of specialized accelerators, such as GPUs or \nTPUs, which can parallelize operations and achieve higher throughput. While some tasks, like \ndocument generation, can be processed in batches overnight, others require low latency and fast \ngeneration, such as code completion. As a result, optimizing the inference process – how these \nmodels make predictions based on input data – is critical for many practical applications. This \nincludes reducing the time it takes to generate the first token (latency), increasing the number \nof tokens generated per second (throughput), and minimizing the memory footprint of LLMs.\nIndeed, naive deployment approaches lead to poor hardware utilization and underwhelming \nthroughput and latency. Fortunately, a variety of optimization techniques have emerged to dra-\nmatically speed up inference. This chapter will explore key methods like speculative decoding, \nmodel parallelism, and weight quantization, demonstrating how thoughtful implementations \ncan achieve speedups of 2–4X or more. We will also introduce three popular inference engines \n(Text Generation Inference, vLLM, and TensorRT-LLM) and compare their features in terms of \ninference optimization.\nIn this chapter, we will cover the following topics:\n•\nModel optimization strategies\n•\nModel parallelism\n•\nModel quantization\n",
      "content_length": 1498,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "Inference Optimization\n290\nBy the end of this chapter, you will understand the core challenges in LLM inference and be fa-\nmiliar with state-of-the-art optimization techniques, including model parallelism and weight \nquantization.\nModel optimization strategies\nMost of the LLMs used nowadays, like GPT or Llama, are powered by a decoder-only Transformer \narchitecture. The decoder-only architecture is designed for text-generation tasks. It predicts the \nnext word in a sequence based on preceding words, making it effective for generating contextually \nappropriate text continuations.\nIn contrast, an encoder-only architecture, like BERT, focuses on understanding and representing \nthe input text with detailed embeddings. It excels in tasks that require comprehensive context \nunderstanding, such as text classification and named entity recognition. Finally, the encoder-de-\ncoder architecture, like T5, combines both functionalities. The encoder processes the input text \nto generate a context-rich representation, which the decoder then uses to produce the output \ntext. This dual structure is particularly powerful for sequence-to-sequence tasks like translation \nand summarization, where understanding the input context and generating a relevant output \nare equally important.\nIn this book, we only focus on the decoder-only architecture, which dominates the LLM field.\nFigure 8.1 – Inference process with decoder-only models. We provide “I have a dream” as \ninput and obtain “of” as output.\nAll the code examples from this chapter can be found on GitHub at https://github.\ncom/PacktPublishing/LLM-Engineering.\n",
      "content_length": 1617,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "Chapter 8\n291\nAs shown in Figure 8.1, the basic inference process for a decoder-only model involves:\n1.\nTokenizing the input prompt and passing it through an embedding layer and positional \nencoding.\n2.\nComputing key and value pairs for each input token using the multi-head attention \nmechanism.\n3.\nGenerating output tokens sequentially, one at a time, using the computed keys and values.\nWhile Steps 1 and 2 are computationally expensive, they consist of highly parallelizable matrix \nmultiplication that can achieve high hardware utilization on accelerators like GPUs and TPUs.\nThe real challenge is that the token generation in Step 3 is inherently sequential – to generate \nthe next token, you need to have generated all previous tokens. This leads to an iterative process \nwhere the output sequence is grown one token at a time, failing to leverage the parallel computing \ncapabilities of the hardware. Addressing this bottleneck is one of the core focuses of inference \noptimization.\nIn this section, we will detail several optimization strategies that are commonly used to speed \nup inference and reduce Video Random-Access Memory (VRAM) usage, such as implementing \na (static) KV cache, continuous batching, speculative decoding, and optimized attention mech-\nanisms.\nKV cache\nWe saw that LLMs generate text token by token, which is slow because each new prediction \ndepends on the entire previous context. For example, to predict the 100\nth token in a sequence, \nthe model needs the context of tokens 1 through 99. When predicting the 101\nst token, it again \nneeds the information from tokens 1 through 99, plus token 100. This repeated computation is \nparticularly inefficient.\nThe key-value (KV) cache addresses this issue by storing key-value pairs produced by self-at-\ntention layers. Instead of recalculating these pairs for each new token, the model retrieves them \nfrom the cache, significantly speeding up the generation. \n",
      "content_length": 1941,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "Inference Optimization\n292\nYou can see an illustration of this technique in Figure 8.2:\nFigure 8.2 – Illustration of the KV cache\nWhen a new token is generated, only the key and value for that single token need to be computed \nand added to the cache. The KV cache is an immediate optimization that is implemented in every \npopular tool and library. Some implementations maintain a separate KV cache for each layer of \nthe model.\nThe size of the KV cache scales with the number of tokens (𝑛𝑛𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡 ) and several model dimensions, \nlike the number of layers (𝑛𝑛𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 ), the number of attention heads (𝑛𝑛ℎ𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒 ), their dimension (dimℎ𝑒𝑒𝑒𝑒𝑒𝑒 \n), and the precision of the parameters in bytes (𝑛𝑛𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 ):\n𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏\n \nFor a typical 7B parameter model using 16-bit precision, this exceeds 2 GB for high sequence \nlengths (higher than 2,048 tokens). Larger models with more layers and higher embedding di-\nmensions will see even greater memory requirements.\n",
      "content_length": 970,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "Chapter 8\n293\nSince the KV cache grows with each generation step and is dynamic, it prevents you from taking \nadvantage of torch.compile, a powerful optimization tool that fuses PyTorch code into fast and \noptimized kernels. The static KV cache solves this issue by pre-allocating the KV cache size to a \nmaximum value, which allows you to combine it with torch.compile for up to a 4x speedup in \nthe forward pass.\nTo configure a model to use a static KV cache with the transformers library, follow these steps:\n1. \nWe import the tokenizer and the model we want to optimize:\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_id = \"google/gemma-2b-it\"\ntokenizer = AutoTokenizer.from_pretrained(model_id) \nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_\nmap=\"auto\")\n2. \nTo implement the static cache, we change the cache implementation in the model’s gen-\neration config to static:\nmodel.generation_config.cache_implementation = \"static\"\n3. \nNow that our KV cache is static, we can compile the model using torch.compile:\ncompiled_model = torch.compile(model, mode=\"reduce-overhead\", \nfullgraph=True)\n4. \nWe tokenize an input question, “What is 2+2?\", and store it on a GPU if available (if not, \nwe store it on the CPU):\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ninputs = tokenizer(\"What is 2+2?\", return_tensors=\"pt\").to(device)\n5. \nLet’s use the generate() method to get the model’s output and decode it with batch_\ndecode() to print its answer:\noutputs = model.generate(**inputs, do_sample=True, temperature=0.7, \nmax_new_tokens=64)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['What is 2+2?\\n\\nThe answer is 4. 2+2 = 4.']\n",
      "content_length": 1704,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "Inference Optimization\n294\nThis returns a list containing both the input and output, correctly answering our question.\nEfficiently managing the KV cache is essential, as it can quickly exhaust available GPU memory \nand limit the batch sizes that can be processed. This has motivated the development of memo-\nry-efficient attention mechanisms and other techniques, which we will cover in the last section.\nContinuous batching\nBatching, or processing multiple inference requests simultaneously, is a standard approach to \nachieve high throughput. Larger batch sizes spread out the memory cost of model weights and \ntransfer more data to the GPU at once, better saturating its parallel compute capacity.\nHowever, decoder-only models pose a particular challenge due to the high variability in input \nprompt lengths and desired output lengths. Some requests may have short prompts and only \nneed a one-word answer, while others may input a lengthy context and expect a multi-paragraph \nresponse.\nWith traditional batching, we would have to wait for the longest request in a batch to complete be-\nfore starting a new batch. This leads to under-utilization as the accelerator sits partly idle waiting \nfor a straggling request to finish. Continuous batching, also known as in-flight batching, aims to \nprevent idle time by immediately feeding a new request into the batch as soon as one completes.\nThe batching process begins the same – by filling the batch with initial requests. But as soon as \na request completes its generation, it is evicted from the batch and a new request takes its place. \nThis way, the accelerator is always processing a full batch, leading to maximally efficient hardware \nutilization. An additional consideration is the need to periodically pause the generation process \nto run prefill, or the embedding and encoding of waiting requests. Finding the optimal balance \nbetween generation and prefill requires some tuning of the waiting-served ratio hyperparameter.\nContinuous batching is natively implemented in most inference frameworks, like Hugging Face’s \nText Generation Inference (TGI), vLLM, and NVIDIA TensorRT-LLM.\nNote that the static cache doesn’t work with all architectures. For details on which \narchitectures are supported, check out the transformers documentation.\n",
      "content_length": 2300,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": "Chapter 8\n295\nSpeculative decoding\nAnother powerful optimization technique is speculative decoding, also called assisted generation. \nThe key insight is that even with continuous batching, the token-by-token generation process \nfails to fully saturate the parallel processing capabilities of the accelerator. Speculative decoding \naims to use this spare compute capacity to predict multiple tokens simultaneously, using a smaller \nproxy model (see Figure 8.3).\nFigure 8.3 – Illustration of traditional decoding (left) and speculative decoding (right)\nThe general approach is:\n• \nApply a smaller model, like a distilled or pruned version of the main model, to predict \nmultiple token completions in parallel. This could be 5–10 tokens predicted in a single step.\n•\nFeed these speculative completions into the full model to validate which predictions \nmatch what the large model would have generated.\n•\nRetain the longest matching prefix from the speculative completions and discard any \nincorrect tokens.\nThe result is that, if the small model approximates the large model well, multiple tokens can be \ngenerated in a single step. This avoids running the expensive large model for several iterations. \nThe degree of speedup depends on the quality of the small model’s predictions – a 90% match \ncould result in a 3–4X speedup.\nIt is crucial that both models use the same tokenizer. If this is not the case, the tokens generated \nby the draft model will not align with those produced by the large model, making them incom-\npatible. Let’s implement this using the transformers library. In this example, we will use two \nQwen1.5 models from Alibaba Cloud: a 1.8B version as the main model, and a 0.5B version as the \ndraft model. Note that, if you have enough VRAM, you can use much larger models like 14B, 32B, \n72B, or 110B as the main model. \n",
      "content_length": 1842,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "Inference Optimization\n296\nHere, we’re limited by the VRAM of the T4 GPU in Google Colab, but to get the maximum speedup, \nthe assistant model should be much smaller than the large model.\nHere’s a step-by-step guide to implement speculative decoding:\n1. \nWe load the tokenizer and both models:\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_id = \"Qwen/Qwen1.5-1.8B-Chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_id) \nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_\nmap=\"auto\")\ndraft_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-\n0.5B-Chat\", device_map=\"auto\")\n2. We then tokenize the same input and store it in the accelerator, if available:\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ninputs = tokenizer(\"What is 2+2?\", return_tensors=\"pt\").to(device)\n3. \nWe can now use model.generate() with the argument assistant_model to enable specu-\nlative decoding:\noutputs = model.generate(**inputs, do_sample=True, assistant_\nmodel=draft_model, temperature=0.7, max_new_tokens=64)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n['What is 2+2? 2 + 2 equals 4!']\nThe speedup in this small example is not significant, but it is clearly noticeable with bigger models.\nPrompt lookup decoding is a variant of speculative decoding, tailored to input-grounded tasks like \nsummarization where there is often overlap between the prompt and output. Shared n-grams \nare used as the LLM candidate tokens. We can enable prompt lookup decoding by using the \nprompt_lookup_num_tokens parameter in model.generate():\noutputs = model.generate(**inputs, prompt_lookup_num_tokens=4)\nBy combining the static KV cache with torch.compile, implementing continuous batching, and \nleveraging speculative decoding techniques, LLMs can see inference speedups of 2–4x or more \nwith no loss in quality.\n",
      "content_length": 1864,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 326,
      "content": "Chapter 8\n297\nAnother approach to creating a small proxy model consists of jointly fine-tuning a small model \nalongside a large model for maximum fidelity. A representative technique here is Medusa, which \ninserts dedicated speculation heads into the main model. The Medusa-1 approach fine-tunes \nthese speculation heads while freezing the large model, while the Medusa-2 approach jointly fine-\ntunes both the speculation heads and the large model. The Medusa method has demonstrated \nimpressive results, enabling a 70M parameter model to closely approximate the performance \nof a 7B parameter model on a range of tasks. Speculative decoding is natively supported by TGI.\nOptimized attention mechanisms\nThe Transformer architecture is based on the attention mechanism, which scales quadratically \nwith the number of input tokens (or sequence length). This is particularly inefficient for longer \nsequences, where the size of the KV cache can blow up.\nIntroduced by Kwon, Li, et al. (2023), PagedAttention addresses these memory challenges by \ndrawing inspiration from virtual memory and paging in operating systems. It partitions the KV \ncache into blocks, eliminating the need for contiguous memory allocation. Each block contains the \nkeys and values for a fixed number of tokens. During attention computation, the PagedAttention \nkernel efficiently fetches these blocks, regardless of their physical memory location.\nThis partitioning allows for near-optimal memory utilization. This is useful for batching more \nsequences together, which increases throughput and GPU utilization. Moreover, PagedAttention's \nblock-based approach naturally supports memory sharing across multiple output sequences \ngenerated from the same prompt. This is particularly advantageous in parallel sampling and \nbeam search, where the same prompt is used to generate multiple outputs. The shared memory \nblocks reduce redundant computations and memory usage, cutting the memory overhead by \nup to 55% and improving throughput by up to 2.2x, according to the authors. The vLLM library \nreceived the first implementation of PagedAttention. Since then, PagedAttention has also been \nimplemented in TGI and TensorRT-LLM.\nAnother popular option is FlashAttention-2. Developed by Tri Dao (2023), it introduced several \nkey innovations that are designed to address the quadratic runtime and memory constraints in \ntraditional attention. By dividing input and output matrices into smaller blocks, FlashAtten-\ntion-2 ensures that these blocks can fit into the GPU’s on-chip SRAM, which is much faster than \nhigh-bandwidth memory. This approach significantly reduces the frequency of data transfers \nbetween the GPU’s main memory and its processing units. \n",
      "content_length": 2728,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "Inference Optimization\n298\nThis is combined with online softmax, which computes the softmax function independently for \neach block of the attention scores matrix, rather than for the entire matrix at once. By maintaining \na running maximum and a running sum of exponentials, FlashAttention-2 can calculate attention \nprobabilities without needing to store large intermediate matrices.\nAdditionally, FlashAttention-2’s online softmax computation enables block-wise processing, \nmaintaining accuracy while significantly reducing memory requirements. This is particularly im-\nportant for training, where the recomputation of intermediate values (instead of storing them) in \nthe backward pass reduces memory usage from quadratic to linear, in relation to sequence length.\nUnlike PagedAttention, FlashAttention-2 can easily be used with the transformers library through \nthe attn_implementation parameter:\n1. \nInstall the flash-attn library with --no-build-isolation so that we don’t install the \ndependencies:\npip install flash-attn --no-build-isolation\n2. \nTo use FlashAttention-2 for inference, specify flash_attention_2 in the attn_\nimplementation parameter when loading a model. For example, this is how to load Mis-\ntral-7B-Instruct-v0.3 with FlashAttention-2:\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.3\",\n    attn_implementation=\"flash_attention_2\",\n)\nThe techniques presented in this section focused on improving the model’s efficiency in processing \ntokens. In the next section, we will discuss how to distribute our model and calculations across \nmultiple GPUs.\nModel parallelism\nModel parallelism allows you to distribute the memory and compute requirements of LLMs across \nmultiple GPUs. This enables the training and inference of models too large to fit on a single device, \nwhile also improving performance in terms of throughput (tokens per second).\nThere are three main approaches to model parallelism, each involving splitting the model weights \nand computation in different ways: data parallelism, pipeline parallelism, and tensor parallelism. \n",
      "content_length": 2148,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": "Chapter 8\n299\nAlthough these approaches were originally developed for training, we can reuse them for inference \nby focusing on the forward pass only.\nData parallelism\nData parallelism (DP) is the simplest type of model parallelism. It involves making copies of the \nmodel and distributing these replicas across different GPUs (see Figure 8.4). Each GPU processes \na subset of the data simultaneously. During training, the gradients calculated on each GPU are \naveraged and used to update the model parameters, ensuring that each replica remains synchro-\nnized. This approach is particularly beneficial when the batch size is too large to fit into a single \nmachine or when aiming to speed up the training process.\nFigure 8.4 – Illustration of data parallelism with four GPUs\nDuring inference, DP can be useful for processing concurrent requests. By distributing the work-\nload across multiple GPUs, this approach helps reduce latency, as multiple requests can be handled \nsimultaneously. This concurrent processing also increases throughput, since a higher number of \nrequests can be processed at the same time.\nHowever, the effectiveness of DP is limited by the model size and the communication overhead \nbetween GPUs. Indeed, replicating the model’s parameters on each GPU is inefficient. This means \nthat this technique only works when the model is small enough to fit into a single GPU, leaving \nless room for input data and thus limiting the batch size. For larger models or when memory is \na constraint, this can be a significant drawback.\nTypically, DP is mainly used for training, while pipeline and tensor parallelism are preferred for \ninference.\n",
      "content_length": 1658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "Inference Optimization\n300\nPipeline parallelism\nIntroduced by Huang et al. in the GPipe paper (2019), pipeline parallelism (PP) is a strategy \nfor distributing the computational load of training and running large neural networks across \nmultiple GPUs.\nUnlike traditional DP, which replicates the entire model on each GPU, pipeline parallelism parti-\ntions the model’s layers across different GPUs. This approach allows each GPU to handle a specific \nportion of the model, thereby reducing the memory burden on individual GPUs.\nFigure 8.5 – Illustration of pipeline parallelism with four GPUs\nAs shown in Figure 8.5, in a typical four-way pipeline parallel split, the model is divided into four \nsegments, with each segment assigned to a different GPU. The first 25% of the model’s layers might \nbe processed by GPU 1, the next 25% by GPU 2, and so on. During the forward pass, activations \nare computed and then passed along to the next GPU. For training, the backward pass follows a \nsimilar sequence in reverse, with gradients being propagated back through the GPUs. The number \nof GPUs is often referred to as the degree of parallelism.\nThe primary advantage of pipeline parallelism is its ability to significantly reduce the memory \nrequirements per GPU. However, this approach introduces new challenges, particularly related \nto the sequential nature of the pipeline. One of the main issues is the occurrence of “pipeline \nbubbles.” These bubbles arise when some GPUs are idle, waiting for activations from preceding \nlayers. This idle time can reduce the overall efficiency of the process.\n",
      "content_length": 1596,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "Chapter 8\n301\nMicro-batching was developed to mitigate the impact of pipeline bubbles. By splitting the input \nbatch into smaller sub-batches, micro-batching ensures that GPUs remain busier, as the next \nsub-batch can begin processing before the previous one is fully completed.\nFigure 8.6 – Illustration of pipeline parallelism with micro-batching.\nFigure 8.6 shows an example of pipeline parallelism with micro-batching. In this example, the \npipeline has four stages (F0, F1, F2, F3), and the input batch is divided into four micro-batches. \nGPU 0 will process forward paths F0,0, F0,1, F0,2, and F0,3, sequentially. Once F0,0 is complete, \nGPU 1 can immediately start processing F1,0 and so on. After completing these forward passes, \nGPU 0 waits for the other GPUs to finish their respective forward computations before starting \nthe backward paths (B0,3, B0,2, B0,1, and B0,0).\nPipeline parallelism is implemented in distributed training frameworks like Megatron-LM, Deep-\nSpeed (ZeRO), and PyTorch through the dedicated Pipeline Parallelism for PyTorch (PiPPy) \nlibrary. At the time of writing, only certain inference frameworks like TensorRT-LLM support \npipeline parallelism.\nTensor parallelism\nIntroduced by Shoeby, Patwary, Puri et al. in the Megatron-LM paper (2019), tensor parallelism\n(TP) is another popular technique to distribute the computation of LLM layers across multiple \ndevices. In contrast to pipeline parallelism, TP splits the weight matrices found in individual \nlayers. This enables simultaneous computations, significantly reducing memory bottlenecks and \nincreasing processing speed.\n",
      "content_length": 1615,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "Inference Optimization\n302\nIn TP, large matrices, such as the weight matrices in MLPs or the attention heads in self-atten-\ntion layers, are partitioned across several GPUs. Each GPU holds a portion of these matrices and \nperforms computations on its respective slice.\nFigure 8.7 – Illustration of column-wise tensor parallelism in an MLP layer (W)\nFor instance, in an MLP layer, the weight matrix is divided so that each GPU processes only a subset \nof the weights (see Figure 8.7). The inputs are broadcast to all GPUs, which then independently \ncompute their respective outputs. The partial results are then aggregated through an all-reduce \noperation, combining them to form the final output.\nIn the context of self-attention layers, TP is particularly efficient due to the inherent parallelism \nof attention heads. Each GPU can compute a subset of these heads independently, allowing the \nmodel to process large sequences more effectively. This makes TP more efficient than pipeline \nparallelism, which requires waiting for the completion of previous layers.\nDespite its advantages, TP is not universally applicable to all layers of a neural network. Layers \nlike LayerNorm and Dropout, which have dependencies spanning the entire input, cannot be effi-\nciently partitioned and are typically replicated across devices instead. However, these operations \ncan be split on the sequence dimension of the input instead (sequence parallelism). Different \nGPUs can compute these layers on different slices of the input sequence, avoiding replication of \nweights. This technique is limited to a few specific layers, but it can provide additional memory \nsavings, especially for very large input sequence lengths.\n",
      "content_length": 1710,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": "Chapter 8\n303\nMoreover, TP necessitates high-speed interconnects between devices to minimize communica-\ntion overhead, making it impractical to implement across nodes with insufficient interconnect \nbandwidth.\nTP is also implemented in distributed training frameworks like Megatron-LM, DeepSpeed (ZeRO), \nand PyTorch (FSDP). It is available in most inference frameworks, like TGI, vLLM, and Tensor-\nRT-LLM.\nCombining approaches\nData, tensor, and pipeline parallelisms are orthogonal techniques that can be combined. Figure \n8.8 illustrates how a given model can be split according to each approach:\nFigure 8.8 – Illustration of the different model parallelism techniques\nCombining these techniques can mitigate their respective issues. Pipeline parallelism provides \nthe greatest memory reduction but sacrifices efficiency, due to pipeline bubbles. This may be \nideal if the primary constraint fits the model in the GPU memory. In contrast, if low latency is \nparamount, then prioritizing tensor parallelism and accepting a larger memory footprint may \nbe the better trade-off. In practice, a model may be split depth-wise into a few pipeline stages, \nwith tensor parallelism used within each stage.\nBalancing these tradeoffs and mapping a given model architecture onto available hardware ac-\ncelerators is a key challenge in deploying LLMs.\nModel quantization\nQuantization refers to the process of representing the weights and activations of a neural net-\nwork using lower-precision data types. In the context of LLMs, quantization primarily focuses \non reducing the precision of the model’s weights and activations. \n",
      "content_length": 1619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "Inference Optimization\n304\nBy default, weights are typically stored in a 16-bit or 32-bit floating-point format (FP16 or FP32), \nwhich provides high precision but comes at the cost of increased memory usage and computa-\ntional complexity. Quantization is a solution to reduce the memory footprint and accelerate the \ninference of LLMs.\nIn addition to these benefits, larger models with over 30 billion parameters can outperform \nsmaller models (7B–13B LLMs) in terms of quality when quantized to 2- or 3-bit precision. This \nmeans they can achieve superior performance while maintaining a comparable memory footprint.\nIn this section, we will introduce the concepts of quantization, GGUF with llama.cpp, GPTQ, \nand EXL2, along with an overview of additional techniques. In addition to the code provided in \nthis section, you can refer to AutoQuant (bit.ly/autoquant) to quantize their models using a \nGoogle Colab notebook.\nIntroduction to quantization\nThere are two main approaches to weight quantization: Post-Training Quantization (PTQ) and \nQuantization-Aware Training (QAT). PTQ is a straightforward technique where the weights of \na pre-trained model are directly converted to a lower precision format without any retraining. \nWhile PTQ is easy to implement, it may result in some performance degradation. Conversely, QAT \nperforms quantization during the training or fine-tuning stage, allowing the model to adapt to \nthe lower precision weights. QAT often yields better performance compared to PTQ but requires \nadditional computational resources and representative training data.\nThe choice of data type plays a crucial role in quantization. Floating-point numbers, such as \nFP32, FP16 (half-precision), and BF16 (brain floating-point), are commonly used in deep learning. \nThese formats allocate a fixed number of bits to represent the sign, exponent, and significand\n(mantissa) of a number.\n",
      "content_length": 1902,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "Chapter 8\n305\nFigure 8.9 – Comparison the between FP32, FP16, and BF16 formats\nA sign of 0 represents a positive number, while 1 indicates a negative number. Conversely, the \nexponent controls the range that is represented (big or small). Finally, the significand controls \nthe precision of the number (the number of digits). The formula used to convert these represen-\ntations into real numbers is:\n(−1)sign × baseexponent × significand \nThe data types shown in Figure 7.7 display different tradeoffs, as illustrated with different repre-\nsentations of 𝜋𝜋 (≈3.1415926535 ). FP32 uses 32 bits, providing high precision but also requiring more \nmemory. Conversely, FP16 and BF16 use 16 bits, lowering the memory footprint at the cost of a \nlower precision. In general, neural networks prefer a bigger range than better precision, which is \nwhy BF16 is the most popular data type when the hardware supports it. For example, NVIDIA’s \nAmpere architecture (A100, A30, etc.) supports BF16, but previous generations like Turing (T4, \nT40, etc.) do not.\n",
      "content_length": 1047,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "Inference Optimization\n306\nHowever, we are not restricted to these three data types. Lower-precision data types, such as INT8 \n(8-bit integers), can be employed for quantization, further reducing the memory footprint. Naïve \nquantization techniques, such as absolute maximum (absmax) quantization and zero-point quanti-\nzation, can be applied to convert FP32, FP16, or BF16 weights to INT8, as illustrated in Figure 8.10:\nFigure 8.10 – Quantization of 0.1 in a [-3.0, 3.2] range with absmax quantization and zero-point \nquantization\nAbsmax quantization maps the original weights 𝐗𝐗 to the range [-127, 127] by dividing them by the \nabsolute maximum value of 𝐗𝐗 and scaling them:\n𝐗𝐗quant = round (127 ⋅𝐗𝐗\nmax|𝐗𝐗|) \nFor example, if our absolute maximum value is 3.2 (see Figure 8.8), a weight of 0.1 would be \nquantized to round (\n127⋅0.1\n3.2 ) = 4 . To dequantize it, we do the inverse operation:\n𝐗𝐗dequant = max|𝐗𝐗| ⋅𝐗𝐗quant\n127\n \nThis means that if we dequantize our weight, we obtain \n3.2⋅4 \n127 ≈0.1008 . We can see a rounding er-\nror of 0.0008  in this example. In Python, we can implement it as follows with the PyTorch library:\nimport torch\ndef absmax_quantize(X):\n    # Calculate scale\n    scale = 127 / torch.max(torch.abs(X))\n    # Quantize\n    X_quant = (scale * X).round()\n    return X_quant.to(torch.int8)\n",
      "content_length": 1318,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": "Chapter 8\n307\nZero-point quantization, on the other hand, considers asymmetric input distributions and maps \nthe weights \n0.0008 to the range [-128, 127] by introducing a zero-point offset:\n𝐗𝐗quant = round(scale ⋅𝐗𝐗𝐗𝐗ero𝐗o𝐗n𝐗) \nWhere scale =\n255\nmax(𝐗𝐗)−min(𝐗𝐗)  and zeropoint = −round(scale ⋅min(𝐗𝐗)) −128 .\nIf we take the same example with a weight of 0.1, we get a scale of \n255\n3.2+3.0 ≈41.13 and a zero-point \nvalue of −round (\n255\n3.2+3.0 ⋅−3.0) −128 = −5 . The weight of 0.1 would be quantized to round(41.13 ⋅0.1 −5) = −1 , \nunlike the value of 4 provided by absmax.\nWe can easily get the dequantization by applying the inverse operation:\n𝐗𝐗dequant = 𝐗𝐗quant −zeropoint\nscale\n \nIn Python, zero-point quantization can be implemented as follows:\ndef zeropoint_quantize(X):\n    # Calculate value range (denominator)\n    x_range = torch.max(X) - torch.min(X)\n    x_range = 1 if x_range == 0 else x_range\n    # Calculate scale\n    scale = 255 / x_range\n    # Shift by zero-point\n    zeropoint = (-scale * torch.min(X) - 128).round()\n    # Scale and round the inputs\n    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n   \n    return X_quant.to(torch.int8)\nHowever, naïve quantization methods have limitations, particularly when dealing with outlier \nfeatures in LLMs. Outlier features are extreme weight values (about 0.1% of total values) that \ncan significantly impact the quantization process, leading to reduced precision for other values.\n",
      "content_length": 1464,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "Inference Optimization\n308\nDiscarding these outliers is not feasible, as it would degrade a model’s performance. You can see \nan example of outliers in Figure 8.11:\nFigure 8.11 – Example of outliers in a weight matrix\nTo address the outlier problem, more advanced quantization techniques have been proposed. \nOne notable example is LLM.int8(), introduced by Dettmers et al. (2022). LLM.int8() employs a \nmixed-precision quantization scheme, where outlier features are processed using FP16, while the \nremaining values are quantized to INT8. This approach effectively reduces the memory footprint \nof LLMs by nearly 2x while minimizing performance degradation.\nLLM.int8() works by performing matrix multiplication in three steps. First, it extracts columns \ncontaining outlier features from the input hidden states using a custom threshold. Second, it \nperforms separate matrix multiplications for the outliers (in FP16) and non-outliers (in INT8) \nusing vector-wise quantization. Finally, it dequantizes the non-outlier results and combines \nthem with the outlier results to obtain the final output in FP16.\nThe effectiveness of LLM.int8() has been demonstrated empirically, showing negligible perfor-\nmance degradation (<1%) compared to the original FP32 models. However, it does introduce an \nadditional computational overhead, resulting in around 20% slower inference for large models. \nModels can be directly loaded in 8-bit precision with the transformer library, using LLM.int8(), \nas follows:\nfrom transformers import AutoModelForCausalLM\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_\nmap=\"auto\", load_in_8bit=True)\n",
      "content_length": 1693,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 338,
      "content": "Chapter 8\n309\nIntroduced by Dettmers et al. (2023), NF4 is a 4-bit precision format designed for QLoRA (discussed \nin Chapter 5). It is also integrated into the transformers library but requires the bitsandbytes \nlibrary as a dependency. To load a model in NF4 (4-bit precision), you can use the load_in_4bit\nparameter, as follows:\nfrom transformers import AutoModelForCausalLM\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_\nmap=\"auto\", load_in_4bit=True)\nQuantization with GGUF and llama.cpp\nThe llama.cpp project is an open-source C++ software library created by Georgi Gerganov, de-\nsigned to perform inference with various LLMs. It is the most popular quantization technique, \nwith many quantized models available on the Hugging Face Hub.\nCompared to other libraries that rely on hardware-specific closed-source libraries like CUDA, \nllama.cpp can run on a broader range of hardware. It has gained significant popularity, par-\nticularly among users without specialized hardware, as it can operate on CPUs and Android \ndevices. Moreover, llama.cpp can also offload layers to the GPU, accelerating inference speed. It \nis compatible with different inference optimization techniques, such as FlashAttention-2 and \nspeculative decoding.\nThis project features its own quantization format, GGUF, designed to simplify and speed up \nmodel loading. GGUF files store tensors and metadata, supporting various formats, from 1-bit \nto 8-bit precision. It follows a naming convention based on the number of bits used and specific \nvariants, such as:\n• \nIQ1_S and IQ1_M: 1-bit precision – very low quality\n• \nIQ2_XXS/XS/S/M and Q2_K: 2-bit precision – generally low quality but IQ2 can be usable \nfor large models\n• \nIQ3_XXS/XS/S/M and Q3_K_S/M/L: 3-bit precision – low quality but usable for large models\n• \nIQ4_XS/NL and Q4_K_S/M, Q4_0/1: 4-bit precision – good quality and usable for most \nmodels\n•\nQ5_K_S/M and Q5_0/1: 5-bit precision – high quality\n•\nQ6_K: 6-bit precision –very high quality\n•\nQ8_0: 8-bit precision – highest quality\n",
      "content_length": 2097,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 339,
      "content": "Inference Optimization\n310\nTo provide a brief overview of GGUF quantization, llama.cpp groups values into blocks and rounds \nthem to a lower precision. For instance, the legacy Q4_0 format handles 32 values per block, scaling \nand quantizing them based on the largest weight value in the block (w = q × block_scale ). In Q4_1, \nthe smallest Lvalue in the block is also added (w = q × block_scale + block_minimum  ). In Q4_K, weights \nare divided into super-blocks, containing 8 blocks with 32 values. Block scales and minimum \nvalues are also quantized in higher precision with 6 bits (w = q × block_scale(6bit) +  block_min(6bit) ). \nFinally, i-quants like IQ4_XS are inspired by another quantization technique called QuIP#. This \nensures an even number of positive (or negative) quant signs in groups of eight and implements \nthe E8 lattice to store their magnitude.\nHere is a practical example of how to quantize a model in the GGUF format. The following steps \ncan be executed on a free T4 GPU in Google Colab:\n1. \nInstall llama.cpp and the required libraries:\n!git clone https://github.com/ggerganov/llama.cpp\n!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n!pip install -r llama.cpp/requirements.txt\n2. \nDownload the model to convert. We will provide the model ID from the Hugging Face \nHub – for example, mistralai/Mistral-7B-Instruct-v0.2:\nMODEL_ID = \"mlabonne/EvolCodeLlama-7b\"\nMODEL_NAME = MODEL_ID.split('/')[-1]\n!git lfs install\n!git clone https://huggingface.co/{MODEL_ID}\n3.\nFirst, we convert the model into FP16. This is an intermediary artifact that will be used \nfor every GGUF quantization type. Note that different conversion scripts exist in llama.\ncpp and are compatible with different models:\nfp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile \n{fp16}\n4. \nWe select a format (here, Q4_K_M) and start the quantization. This process can take an \nhour on a T4 GPU:\nMETHOD = \"q4_k_m\"\nqtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n!./llama.cpp/quantize {fp16} {qtype} {METHOD}\n",
      "content_length": 2098,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 340,
      "content": "Chapter 8\n311\n5. \nOnce it’s done, your quantized model is ready. You can download it locally, or upload it \nto the Hugging Face Hub using the following code:\nfrom huggingface_hub import create_repo, HfApi\nhf_token = \"\" # Specify your token\nusername = \"\" # Specify your username\napi = HfApi()\n# Create empty repo\ncreate_repo(\n    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n    repo_type=\"model\",\n    exist_ok=True,\n    token=hf_token\n)\n# Upload gguf files\napi.upload_folder(\n    folder_path=MODEL_NAME,\n    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n    allow_patterns=f\"*.gguf\",\n    token=hf_token\n)\nGGUF models can be used with backends such as llama-cpp-python and frameworks like Lang-\nChain. This is useful if you want to integrate a quantized model into a broader system. You can \nalso directly chat with the model using frontends, like llama.cpp’s lightweight server, LM Studio, \nand the Text Generation Web UI. These tools enable easy interaction with the GGUF models, \nproviding an experience similar to ChatGPT.\nQuantization with GPTQ and EXL2\nWhile GGUF and llama.cpp offer CPU inference with GPU offloading, GPTQ and EXL2 are two \nquantization formats dedicated to GPUs. This makes them both faster than llama.cpp during \ninference. In particular, EXL2 offers the highest throughput with its dedicated library, ExLlamaV2.\n",
      "content_length": 1331,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 341,
      "content": "Inference Optimization\n312\nGPTQ and EXL2 quants are based on the GPTQ algorithm, introduced by Frantar et al. (2023). \nIt optimizes weight quantization for LLMs by refining the Optimal Brain Quantization (OBQ) \napproach to handle extensive matrices efficiently. It begins with a Cholesky decomposition of \nthe Hessian inverse, ensuring numerical stability. Instead of quantizing weights in a strict order, \nGPTQ processes them in batches, updating columns and associated blocks iteratively. This meth-\nod leverages lazy batch updates, reducing computational redundancy and memory bottlenecks.\nWhile GPTQ is limited to 4-bit precision, EXL2 offers more flexibility with a highly customizable \nprecision that can mix different quantization levels. This allows for precise bitrates between 2 \nand 8 bits per weight, such as 2.3, 3.5, or 6.0. It can also apply multiple quantization levels to \neach linear layer, prioritizing more important weights with higher bit quantization. Parameters \nare selected automatically, by quantizing each matrix multiple times and choosing a combination \nthat minimizes the quantization error while meeting a target bitrate. In practice, this allows 70B \nmodels to run on a single 24 GB GPU with 2.55-bit precision.\nThe inference itself is handled by the ExLlamaV2 library, which supports both the GPTQ and \nEXL2 models.\nIn the following example, let’s quantize a model in the EXL2 format using ExLlamaV2. These \nsteps can be executed on a free T4 GPU in Google Colab:\n1. \nInstall the ExLlamaV2 library from source:\n!git clone https://github.com/turboderp/exllamav2\n!pip install -e exllamav2\n2. We download the model to quantize by cloning its repo from the Hugging Face Hub:\nMODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\nMODEL_NAME = MODEL_ID.split('/')[-1]\n!git lfs install\n!git clone https://huggingface.co/{MODEL_ID}\n3. \nDownload the calibration dataset used to measure the quantization error. In this case, \nwe will use WikiText-103, a standard calibration dataset with high-quality articles from \nWikipedia:\n!wget https://huggingface.co/datasets/wikitext/\nresolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/\nwikitext-test.parquet\n",
      "content_length": 2176,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 342,
      "content": "Chapter 8\n313\n4. \nQuantize the model at a given precision (for example, 4.5):\n!mkdir quant\n!python exllamav2/convert.py \\\n    -i {MODEL_NAME} \\\n    -o quant \\\n    -c wikitext-test.parquet \\\n    -b 4.5\nThe quantized model can then be uploaded to the Hugging Face Hub, as seen previously.\nGPTQ and EXL2 quants are not as widely supported as GGUF. For example, frontends like LM \nStudio do not currently integrate them. You can use other tools instead, like oobabooga’s Text \nGeneration Web UI. It is also directly integrated into the transformers library and supported by \nTGI. GPTQ models are also supported in TensorRT-LLM.\nWhile less popular than GGUF, you can find a lot of GPTQ and EXL2 models on the Hugging Face \nHub.\nOther quantization techniques\nThere is a variety of quantization techniques beyond GGUF, GPTQ, and EXL2. This subsection will \nbriefly introduce Activate-aware Weight Quantization (AWQ) as well as extreme quantization \ntechniques, like QuIP# (Quantization with Incoherence Processing) and HQQ (Half-Quadratic \nQuantization).\nIntroduced by Lin et al. (2023), AWQ is another popular quantization algorithm. It identifies \nand protects the most important weights, which are determined based on activation magnitude \ninstead of weight magnitude. This approach involves applying optimal per-channel scaling to \nthese salient weights, without relying on backpropagation or reconstruction, ensuring that the \nLLM does not overfit the calibration set. While it relies on a different paradigm, AWQ is quite close \nto the GPTQ and EXL2 versions, although slightly slower. They are well-supported by inference \nengines and integrated into TGI, vLLM, and TensorRT-LLM.\nAn interesting trend is the quantization of models into 1- or 2-bit precision. While some formats, \nlike EXL2, allow extreme quantization, the quality of the models often suffers significantly. How-\never, recent algorithms like QuIP#  and HQQ have targeted this regime and offer quantization \nmethods that better preserve the performance of the original models. This is particularly true for \nlarge models (over 30B parameters), which can end up taking less space than 7B or 13B parameter \nmodels while providing higher-quality outputs. \n",
      "content_length": 2218,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 343,
      "content": "Inference Optimization\n314\nThis trend is expected to continue, further optimizing these quantization methods.\nTo conclude this chapter, here is a table summarizing the features of the three main inference \nengines we covered in the previous sections:\nTechnique\nTGI\nvLLM\nTensorRT-LLM\nContinuous batching\n✓\n✓\n✓\nSpeculative decoding\n✓\nFlashAttention2\n✓\n✓\n✓\nPagedAttention\n✓\n✓\n✓\nPipeline parallelism\n✓\nTensor parallelism\n✓\n✓\n✓\nGPTQ\n✓\n✓\nEXL2\n✓\nAWQ\n✓\n✓\n✓\nTable 8.1 – Summary of features for TGI, vLLM, and TensorRT-LLM\nSummary\nIn summary, inference optimization is a critical aspect of deploying LLMs effectively. This chap-\nter explored various optimization techniques, including optimized generation methods, model \nparallelism, and weight quantization. Significant speedups can be achieved by leveraging tech-\nniques like predicting multiple tokens in parallel with speculative decoding, or using an optimized \nattention mechanism with FlashAttention-2. Additionally, we discussed how model parallelism \nmethods, including data, pipeline, and tensor parallelism, distribute the computational load across \nmultiple GPUs to increase throughput and reduce latency. Weight quantization, with formats \nlike GGUF and EXL2, further reduces the memory footprint and accelerates inference, with some \ncalculated tradeoff in output quality.\nUnderstanding and applying these optimization strategies are essential for achieving high per-\nformance in practical applications of LLMs, such as chatbots and code completion. The choice \nof techniques and tools depends on specific requirements, including available hardware, desired \nlatency, and throughput. By combining various approaches, such as continuous batching and \nspeculative decoding, along with advanced attention mechanisms and model parallelism, users \ncan tailor their deployment strategies to maximize efficiency.\n",
      "content_length": 1861,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 344,
      "content": "Chapter 8\n315\nWay back in Chapter 4, we focused only on implementing the ingestion pipeline, which is just \none component of a standard RAG application. In the next chapter, we will conclude the RAG \nsystem by implementing the retrieval and generation components and integrating them into \nthe inference pipeline. \nReferences\n• \nHugging Face, Text Generation Inference, https://github.com/huggingface/text-\ngeneration-inference, 2022.\n• \nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Ef-\nficient Memory Management for Large Language Model Serving with PagedAttention, 2023.\n• \nNvidia, TensorRT-LLM, https://github.com/NVIDIA/TensorRT-LLM, 2023.\n• \nY. Leviathan, M. Kalman, Y. Matias, Fast Inference from Transformers via Speculative Decoding, \n2023.\n• \nT. Cai, Y. Li, Z. Geng, H. Peng, J.D. Lee, D. Chen, T. Dao, Medusa: Simple LLM Inference Accel-\neration Framework with Multiple Decoding Heads, 2024.\n• \nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Ef-\nficient Memory Management for Large Language Model Serving with PagedAttention, 2023.\n•\nR.Y. Aminabadi, S. Rajbhandari, M. Zhang, A.A. Awan, C. Li, D. Li, E. Zheng, J. Rasley, S. Smith, \nO. Ruwase, Y. He, DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at \nUnprecedented Scale, 2022.\n•\nY. Huang, Y. Cheng, A. Bapna, O. Firat, M.X. Chen, D. Chen, H. Lee, J. Ngiam, Q.V. Le, Y. Wu, \nZ. Chen, GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism, 2019.\n•\nK. James Reed, PiPPy: Pipeline Parallelism for PyTorch, https://github.com/pytorch/PiPPy, \n2022.\n•\nM. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro, Megatron-LM: Training \nMulti-Billion Parameter Language Models Using Model Parallelism, 2020.\n•\nVerma and Vaidya, Mastering LLM Techniques: Inference Optimization, NVIDIA Developer \nTechnical Blog, https://developer.nvidia.com/blog/mastering-llm-techniques-\ninference-optimization/, 2023.\n",
      "content_length": 2009,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 345,
      "content": "Inference Optimization\n316\n•\nT. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, LLM.int8(): 8-bit Matrix Multiplication for \nTransformers at Scale, 2022.\n•\nG. Gerganov, llama.cpp, https://github.com/ggerganov/llama.cpp, 2023.\n•\nE. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, GPTQ: Accurate Post-Training Quantization for \nGenerative Pre-trained Transformers, 2023.\n•\nTuboderp, exllamav2, https://github.com/turboderp/exllamav2, 2023.\n•\nJ. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, S. Han, \nAWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, 2024.\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 762,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 346,
      "content": "9\nRAG Inference Pipeline\nBack in Chapter 4, we implemented the retrieval-augmented generation (RAG) feature pipeline \nto populate the vector database (DB). Within the feature pipeline, we gathered data from the data \nwarehouse, cleaned, chunked, and embedded the documents, and, ultimately, loaded them to the \nvector DB. Thus, at this point, the vector DB is filled with documents and ready to be used for RAG.\nBased on the RAG methodology, you can split your software architecture into three modules: one \nfor retrieval, one to augment the prompt, and one to generate the answer. We will follow a sim-\nilar pattern by implementing a retrieval module to query the vector DB. Within this module, we \nwill implement advanced RAG techniques to optimize the search. Afterward, we won’t dedicate \na whole module to augmenting the prompt, as that would be overengineering, which we try to \navoid. However, we will write an inference service that inputs the user query and context, builds \nthe prompt, and calls the LLM to generate the answer. To summarize, we will implement two core \nPython modules, one for retrieval and one for calling the LLM using the user’s input and context \nas input. When we glue these together, we will have an end-to-end RAG flow.\nIn Chapters 5 and 6, we fine-tuned our LLM Twin model, and in Chapter 8, we learned how to \noptimize it for inference. Thus, at this point, the LLM is ready for production. What is left is to \nbuild and deploy the two modules described above. \n",
      "content_length": 1498,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 347,
      "content": "RAG Inference Pipeline\n318\nWe will dedicate the next chapter entirely to deploying our fine-tuned LLM Twin model to AWS \nSageMaker, as an AWS SageMaker inference endpoint. Thus, the focus of this chapter is to dig \ninto the advanced RAG retrieval module implementation. We have dedicated a whole chapter to \nthe retrieval step because this is where the magic happens in an RAG system. At the retrieval step \n(and not when calling the LLM), you write most of the RAG inference code. This step is where \nyou have to wrangle your data to ensure that you retrieve the most relevant data points from the \nvector DB. Hence, most of the advanced RAG logic goes within the retrieval step.\nTo sum up, in this chapter, we will cover the following topics:\n•\nUnderstanding the LLM Twin’s RAG inference pipeline\n•\nExploring the LLM Twin’s advanced RAG techniques\n•\nImplementing the LLM Twin’s RAG inference pipeline\nBy the end of this chapter, you will know how to implement an advanced RAG retrieval module, \naugment a prompt using the retrieved context, and call an LLM to generate the final answer. \nUltimately, you will know how to build a production-ready RAG inference pipeline end to end.\nUnderstanding the LLM Twin’s RAG inference \npipeline\nBefore implementing the RAG inference pipeline, we want to discuss its software architecture \nand advanced RAG techniques. Figure 9.1 illustrates an overview of the RAG inference flow. The \ninference pipeline starts with the input query, retrieves the context using the retrieval module \n(based on the query), and calls the LLM SageMaker service to generate the final answer.\n",
      "content_length": 1612,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 348,
      "content": "Chapter 9\n319\nFigure 9.1: RAG inference pipeline architecture\nThe feature pipeline and the retrieval module, defined in Figure 9.1, are independent processes. The \nfeature pipeline runs on a different machine on a schedule to populate the vector DB. At the same \ntime, the retrieval module is called on demand, within the inference pipeline, on every user request. \n",
      "content_length": 366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 349,
      "content": "RAG Inference Pipeline\n320\nBy separating concerns between the two components, the vector DB is always populated with the \nlatest data, ensuring feature freshness, while the retrieval module can access the latest features \non every request. The input of the RAG retrieval module is the user’s query, based on which we \nhave to return the most relevant and similar data points from the vector DB, which will be used \nto guide the LLM in generating the final answer.\nTo fully understand the dynamics of the RAG inference pipeline, let’s go through the architecture \nflow from Figure 9.1 step by step:\n1. \nUser query: We begin with the user who makes a query, such as “Write an article about...”\n2. \nQuery expansion: We expand the initial query to generate multiple queries that reflect \ndifferent aspects or interpretations of the original user query. Thus, instead of one query, \nwe will use xN queries. By diversifying the search terms, the retrieval module increases \nthe likelihood of capturing a comprehensive set of relevant data points. This step is crucial \nwhen the original query is too narrow or vague.\n3.\nSelf-querying: We extract useful metadata from the original query, such as the author’s \nname. The extracted metadata will be used as filters for the vector search operation, elim-\ninating redundant data points from the query vector space (making the search more \naccurate and faster).\n4.\nFiltered vector search: We embed each query and perform a similarity search to find \neach search’s top K data points. We execute xN searches corresponding to the number of \nexpanded queries. We call this step a filtered vector search as we leverage the metadata \nextracted from the self-query step as query filters.\n5.\nCollecting results: We get up to xK results closest to its specific expanded query interpre-\ntation for each search operation. Further, we aggregate the results of all the xN searches, \nending up with a list of N x K results containing a mix of articles, posts, and repositories \nchunks. The results include a broader set of potentially relevant chunks, offering multiple \nrelevant angles based on the original query’s different facets.\n6.\nReranking: To keep only the top K most relevant results from the list of N x K potential \nitems, we must filter the list further. We will use a reranking algorithm that scores each \nchunk based on the relevance and importance relative to the initial user query. We will \nleverage a neural cross-encoder model to compute the score, a value between 0 and 1, \nwhere 1 means the result is entirely relevant to the query. Ultimately, we sort the N x K\nresults based on the score and pick the top K items. Thus, the output is a ranked list of K\nchunks, with the most relevant data points situated at the top.\n",
      "content_length": 2765,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 350,
      "content": "Chapter 9\n321\n7.\nBuild the prompt and call the LLM: We map the final list of the most relevant K chunks \nto a string used to build the final prompt. We create the prompt using a prompt template, \nthe retrieved context, and the user’s query. Ultimately, the augmented prompt is sent to \nthe LLM (hosted on AWS SageMaker exposed as an API endpoint).\n8. Answer: We are waiting for the answer to be generated. After the LLM processes the \nprompt, the RAG logic finishes by sending the generated response to the user.\nThat wraps up the overview of the RAG inference pipeline. Now, let’s dig deeper into the details.\nExploring the LLM Twin’s advanced RAG techniques\nNow that we understand the overall flow of our RAG inference pipeline, let’s explore the advanced \nRAG techniques we used in our retrieval module:\n•\nPre-retrieval step: Query expansion and self-querying\n•\nRetrieval step: Filtered vector search\n•\nPost-retrieval step: Reranking\nBefore digging into each method individually, let’s lay down the Python interfaces we will use \nin this section, which are available at https://github.com/PacktPublishing/LLM-Engineers-\nHandbook/blob/main/llm_engineering/application/rag/base.py.\nThe first is a prompt template factory that standardizes how we instantiate prompt templates. \nAs an interface, it inherits from ABC and exposes the create_template() method, which returns \na LangChain PromptTemplate instance. Even if we avoid being heavily reliant on LangChain, as \nwe want to implement everything ourselves to understand the engineering behind the scenes, \nsome objects, such as the PromptTemplate class, are helpful to speed up the development without \nhiding too much functionality:\nfrom abc import ABC, abstractmethod\nfrom langchain.prompts import PromptTemplate\nfrom pydantic import BaseModel\nclass PromptTemplateFactory(ABC, BaseModel):\n",
      "content_length": 1844,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 351,
      "content": "RAG Inference Pipeline\n322\n    @abstractmethod\n    def create_template(self) -> PromptTemplate:\n        pass\nWe also want to define a RAGStep interface used to standardize the interface of advanced RAG \nsteps such as query expansion and self-querying. As these steps are often dependent on other \nLLMs, it has a mock attribute to reduce costs and debugging time during development:\nfrom typing import Any\nfrom llm_engineering.domain.queries import Query\nclass RAGStep(ABC):\n    def __init__(self, mock: bool = False) -> None:\n        self._mock = mock\n    @abstractmethod\n    def generate(self, query: Query, *args, **kwargs) -> Any:\n        pass\nUltimately, we must understand how we modeled the Query domain entity to wrap the user’s \ninput with other metadata required for advanced RAG. Thus, let’s look at its implementation. \nFirst, we import the necessary classes:\nfrom pydantic import UUID4, Field\nfrom llm_engineering.domain.base import VectorBaseDocument\nfrom llm_engineering.domain.types import DataCategory\nNext, we define the Query entity class, which inherits from the VectorBaseDocument object-vector \nmapping (OVM) class, discussed in Chapter 4. Thus, each query can easily be saved or retrieved \nfrom the vector DB:\nclass Query(VectorBaseDocument):\n    content: str\n    author_id: UUID4 | None = None\n    author_full_name: str | None = None\n    metadata: dict = Field(default_factory=dict)\n",
      "content_length": 1406,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 352,
      "content": "Chapter 9\n323\nclass Config:\n        category = DataCategory.QUERIES\nWhat is essential to notice are the class’s attributes used to combine the user’s query with a \nbunch of metadata fields:\n• \ncontent: A string containing input query.\n•\nauthor_id: An optional UUID4 identifier extracted from the query used as a filter within \nthe vector search operation to retrieve chunks written only by a specific author\n•\nauthor_full_name: An optional string used to query the author_id\n• \nmetadata: A dictionary for any additional metadata, initialized as an empty dict by default\nBesides the standard definition of a domain class, we also define a from_str() class method to \ncreate a Query instance directly from a string. This allows us to standardize how we clean the query \nstring before constructing the query object, such as stripping any leading or trailing whitespace \nand newline characters:\n    @classmethod\n    def from_str(cls, query: str) -> \"Query\":\n        return Query(content=query.strip(\"\\n \"))\nAdditionally, there’s an instance method called replace_content() used to create a new Query\ninstance with updated content while retaining the original query’s id, author_id, author_full_\nname, and metadata:\n    def replace_content(self, new_content: str) -> \"Query\":\n        return Query(\n            id=self.id,\n            content=new_content,\n            author_id=self.author_id,\n            author_full_name=self.author_full_name,\n            metadata=self.metadata,\n        )\nThis can be particularly useful when modifying the query text, for example, during preprocessing \nor normalization, without losing the associated metadata or identifiers. Following the Query class, \nwe define the EmbeddedQuery class:\nclass EmbeddedQuery(Query):\n",
      "content_length": 1748,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 353,
      "content": "RAG Inference Pipeline\n324\n    embedding: list[float]\n    class Config:\n        category = DataCategory.QUERIES\nThe EmbeddedQuery class extends Query by adding the embedding field. The EmbeddedQuery entity \nencapsulates all the data and metadata necessary to perform vector search operations on top of \nQdrant (or another vector DB).\nNow that we understand all the interfaces and new domain entities used within the RAG inference \npipeline, let’s move on to our advanced RAG pre-retrieval optimization techniques.\nAdvanced RAG pre-retrieval optimizations: query expansion \nand self-querying\nWe implemented two methods to optimize the pre-retrieval optimization step: query expansion \nand self-querying. The two methods work closely with the filtered vector search step, which we \nwill touch on in the next section. For now, however, we will start with understanding the code \nfor query expansion and move to implementing self-querying.\nWithin these two methods, we will leverage OpenAI’s API to generate variations of the original \nquery within the query expansion step and to extract the necessary metadata within the self-que-\nrying algorithm. When we wrote this book, we used GPT-4o-mini in all our examples, but as \nOpenAI’s models quickly evolve, the model might get deprecated. But that’s not an issue, as you \ncan quickly change it in your .env file by configuring the OPENAI_MODEL_ID environment variable.\nQuery expansion\nThe problem in a typical retrieval step is that you query your vector DB using a single vector rep-\nresentation of your original question. This approach covers only a small area of the embedding \nspace, which can be limiting. If the embedding doesn’t contain all the required information or \nnuances of your query, the retrieved context may not be relevant. This means essential documents \nthat are semantically related but not near the query vector might be overlooked.\nThe solution is based on query expansion, which offers a way to overcome this limitation. Using an \nLLM to generate multiple queries based on your initial question, you create various perspectives \nthat capture different facets of your query. These expanded queries, when embedded, target other \nareas of the embedding space that are still relevant to your original question. This increases the \nlikelihood of retrieving more relevant documents from the vector DB. \n",
      "content_length": 2367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 354,
      "content": "Chapter 9\n325\nImplementing query expansion can be as straightforward as crafting a detailed zero-shot prompt \nto guide the LLM in generating these alternative queries. Thus, after implementing query ex-\npansion, instead of having only one query to search relevant context, you will have xN queries, \nhence xN searches.\nIncreasing the number of searches can impact your latency. Thus, you must experiment with the \nnumber of queries you generate to ensure the retrieval step meets your application requirements. \nYou can also optimize the searches by parallelizing them, drastically reducing the latency, which \nwe will do in the ContextRetriever class implemented at the end of this chapter. \nNow, let’s dig into the code. We begin by importing the necessary modules and classes required \nfor query expansion:\nfrom langchain_openai import ChatOpenAI\nfrom llm_engineering.domain.queries import Query\nfrom llm_engineering.settings import settings\nfrom .base import RAGStep\nfrom .prompt_templates import QueryExpansionTemplate\nNext, we define the QueryExpansion class, which generates expanded query versions. The class \nimplementation can be found at https://github.com/PacktPublishing/LLM-Engineers-\nHandbook/blob/main/llm_engineering/application/rag/query_expanison.py:\nclass QueryExpansion(RAGStep):\n    def generate(self, query: Query, expand_to_n: int) -> list[Query]:\n        assert expand_to_n > 0, f\"'expand_to_n' should be greater than 0. \nGot {expand_to_n}.\"\n        if self._mock:\n            return [query for _ in range(expand_to_n)]\nQuery expansion is also known as multi-query, but the principles are the \nsame. For example, this is an example of LangChain’s implementation called \nMultiQueryRetriver: https://python.langchain.com/docs/how_to/\nMultiQueryRetriever/\n",
      "content_length": 1778,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 355,
      "content": "RAG Inference Pipeline\n326\nIn the generate method, we first ensure that the number of expansions requested (expand_to_n) \nis greater than zero. If the instance is in mock mode (self._mock is True), it simply returns a \nlist containing copies of the original query to simulate expansion without actually calling the \nAPI. If not in mock mode, we proceed to create the prompt and initialize the language model:\n        query_expansion_template = QueryExpansionTemplate()\n        prompt = query_expansion_template.create_template(expand_to_n - 1)\n        model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_\nkey=settings.OPENAI_API_KEY, temperature=0)\nHere, we instantiate QueryExpansionTemplate and create a prompt tailored to generate expand_\nto_n - 1 new queries (excluding the original). We initialize the ChatOpenAI model with the \nspecified settings and set the temperature to 0 for deterministic output. We then create a Lang-\nChain chain by combining the prompt with the model and invoke it with the user’s question:\n        chain = prompt | model\n        response = chain.invoke({\"question\": query})\n        result = response.content\nBy piping the prompt into the model (prompt | model), we set up a chain that generates expanded \nqueries when invoked with the original query. The response from the model is captured in the \nresult object. After receiving the response, we parse and clean the expanded queries:\n\t\t\nWe split the result using the separator defined in the template to get individual queries. Starting \nwith a list containing the original query, we append each expanded query after stripping any \nextra whitespace.\n",
      "content_length": 1636,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 356,
      "content": "Chapter 9\n327\nFinally, we define the QueryExpansionTemplate class, which constructs the prompt used for \nquery expansion. The class and other prompt templates can be accessed at https://github.com/\nPacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/rag/\nprompt_templates.py:\nfrom langchain.prompts import PromptTemplate\nfrom .base import PromptTemplateFactory\nclass QueryExpansionTemplate(PromptTemplateFactory):\n    prompt: str = \"\"\"You are an AI language model assistant. Your task is \nto generate {expand_to_n}\n    different versions of the given user question to retrieve relevant \ndocuments from a vector\n    database. By generating multiple perspectives on the user question, \nyour goal is to help\n    the user overcome some of the limitations of the distance-based \nsimilarity search.\n    Provide these alternative questions separated by '{separator}'.\n    Original question: {question}\"\"\"\n    @property\n    def separator(self) -> str:\n        return \"#next-question#\"\n    def create_template(self, expand_to_n: int) -> PromptTemplate:\n        return PromptTemplate(\n            template=self.prompt,\n            input_variables=[\"question\"],\n            partial_variables={\n                \"separator\": self.separator,\n                \"expand_to_n\": expand_to_n,\n            },\n        )\nThis class defines a prompt instructing the language model to generate multiple versions of \nthe user’s question. It uses placeholders like {expand_to_n}, {separator}, and {question} to \ncustomize the prompt. \n",
      "content_length": 1532,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 357,
      "content": "RAG Inference Pipeline\n328\nIt takes expand_to_n as an input parameter to define how many queries we wish to generate while \nwe build the PromptTemplate instance. The separator property provides a unique string to split the \ngenerated queries. The expand_to_n and separator variables are passed as partial_variables, \nmaking them immutable at runtime. Meanwhile, the {question} placeholder will be changed \nevery time the LLM chain is called.\nNow that we have finished studying the query expansion implementation, let’s look at an exam-\nple of how to use the QueryExpansion class. Let’s run the following code using this python -m \nllm_engineering.application.rag.query_expansion command:\nquery = Query.from_str(\"Write an article about the best types of advanced \nRAG methods.\")\n    query_expander = QueryExpansion()\n    expanded_queries = query_expander.generate(query, expand_to_n=3)\n    for expanded_query in expanded_queries:\n        logger.info(expanded_query.content)\nWe get the following variations of the original query. As you can observe, the query expansion \nmethod was successful in providing more details and different perspectives of the initial query, \nsuch as highlighting the effectiveness of advanced RAG methods or the overview of these methods \n(remember that the first query is the original one):\n2024-09-18 17:51:33.529 | INFO  - Write an article about the best types of \nadvanced RAG methods.\n2024-09-18 17:51:33.529 | INFO  - What are the most effective advanced RAG \nmethods, and how can they be applied?\n2024-09-18 17:51:33.529 | INFO  - Can you provide an overview of the top \nadvanced retrieval-augmented generation techniques?\nNow, let’s move to the next pre-retrieval optimization method: self-querying.\nSelf-querying\nThe problem when embedding your query into a vector space is that you cannot guarantee that \nall the aspects required by your use case are present with enough signal in the embedding vec-\ntor. For example, you want to be 100% sure that your retrieval depends on the tags provided \nin the user’s input. Unfortunately, you can’t control the signal left within the embedding that \nemphasizes the tag. By embedding the query prompt alone, you can never be sure that the tags \nare sufficiently represented in the embedding vector or have enough signal when computing the \ndistance against other vectors. \n",
      "content_length": 2347,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 358,
      "content": "Chapter 9\n329\nThis problem stands for any other metadata you want to present during the search, such as IDs, \nnames, or categories.\nThe solution is to use self-querying to extract the tags or other critical metadata within the query \nand use them alongside the vector search as filters. Self-querying uses an LLM to extract various \nmetadata fields crucial for your business use case, such as tags, IDs, number of comments, likes, \nshares, etc. Afterward, you have complete control over how the extracted metadata is considered \nduring retrieval. In our LLM Twin use case, we extract the author’s name and use it as a filter. \nSelf-queries work hand-in-hand with filtered vector searches, which we will explain in the next \nsection.\nNow, let’s move on to the code. We begin by importing the necessary modules and classes on \nwhich our code relies:\nfrom langchain_openai import ChatOpenAI\nfrom llm_engineering.application import utils\nfrom llm_engineering.domain.documents import UserDocument\nfrom llm_engineering.domain.queries import Query\nfrom llm_engineering.settings import settings\nfrom .base import RAGStep\nfrom .prompt_templates import SelfQueryTemplate\nNext, we define the SelfQuery class, which inherits from RAGStep and implements the generate()\nmethod. The class can be found at https://github.com/PacktPublishing/LLM-Engineers-\nHandbook/blob/main/llm_engineering/application/rag/self_query.py:\nclass SelfQuery(RAGStep):\n    def generate(self, query: Query) -> Query:\n        if self._mock:\n            return query\nIn the generate() method, we check if the _mock attribute is set to True. If it is, we will return the \noriginal query object unmodified. This allows us to bypass calling the model while testing and \ndebugging. If not in mock mode, we create the prompt template and initialize the language model.\n        prompt = SelfQueryTemplate().create_template()\n        model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_\nkey=settings.OPENAI_API_KEY, temperature=0)\n",
      "content_length": 1987,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 359,
      "content": "RAG Inference Pipeline\n330\nHere, we instantiate the prompt using the SelfQueryTemplate factory class and create a ChatOpenAI\nmodel instance (similar to the query expansion implementation). We then combine the prompt \nand the model into a chain and invoke it with the user’s query.\n        chain = prompt | model\n        response = chain.invoke({\"question\": query})\n        user_full_name = response.content.strip(\"\\n \")\nWe extract the content from the LLM response and strip any leading or trailing whitespace to \nobtain the user_full_name value. Next, we check if the model was able to extract any user in-\nformation.\n        if user_full_name == \"none\":\n            return query\nIf the response is \"none\", it means no user name was found in the query, so we return the origi-\nnal query object. If a user name is found, we will split the user_full_name into the first_name\nand last_name variables using a utility function. Then, based on the user’s details, we retrieve \nor create a UserDocument user instance:\n        first_name, last_name = utils.split_user_full_name(user_full_name)\n        user = UserDocument.get_or_create(first_name=first_name, last_\nname=last_name)\nFinally, we update the query object with the extracted author information and return it:\n        query.author_id = user.id\n        query.author_full_name = user.full_name\n        return query\nThe updated query now contains the author_id and author_full_name values, which can be \nused in subsequent steps of the RAG pipeline.\nLet’s look at the SelfQueryTemplate class, which defines the prompt to extract user information:\nfrom langchain.prompts import PromptTemplate\nfrom .base import PromptTemplateFactory\n",
      "content_length": 1682,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 360,
      "content": "Chapter 9\n331\nclass SelfQueryTemplate(PromptTemplateFactory):\n    prompt: str = \"\"\"You are an AI language model assistant. Your task is \nto extract information from a user question.\n    The required information that needs to be extracted is the user name \nor user id.\n    Your response should consist of only the extracted user name (e.g., \nJohn Doe) or id (e.g. 1345256), nothing else.\n    If the user question does not contain any user name or id, you should \nreturn the following token: none.\n   \n    For example:\n    QUESTION 1:\n    My name is Paul Iusztin and I want a post about...\n    RESPONSE 1:\n    Paul Iusztin\n   \n    QUESTION 2:\n    I want to write a post about...\n    RESPONSE 2:\n    none\n   \n    QUESTION 3:\n    My user id is 1345256 and I want to write a post about...\n    RESPONSE 3:\n    1345256\n   \n    User question: {question}\"\"\"\n    def create_template(self) -> PromptTemplate:\n        return PromptTemplate(template=self.prompt, input_\nvariables=[\"question\"])\nIn the SelfQueryTemplate class, we define a prompt instructing the AI model to extract the user \nname or ID from the input question. The prompt uses few-shot learning to guide the model on \nhow to respond in different scenarios. When the template is invoked, the {question} placeholder \nwill be replaced with the actual user question.\n",
      "content_length": 1316,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 361,
      "content": "RAG Inference Pipeline\n332\nBy implementing self-querying, we ensure that critical metadata required for our use case is ex-\nplicitly extracted and used during retrieval. This approach overcomes the limitations of relying \nsolely on the semantics of the embeddings to capture all necessary aspects of a query.\nNow that we’ve implemented the SelfQuery class, let’s provide an example. Run the following \ncode using the python -m llm_engineering.application.rag.self_query CLI command:\n    query = Query.from_str(\"I am Paul Iusztin. Write an article about the \nbest types of advanced RAG methods.\")\n    self_query = SelfQuery()\n    query = self_query.generate(query)\n    logger.info(f\"Extracted author_id: {query.author_id}\")\n    logger.info(f\"Extracted author_full_name: {query.author_full_name}\")\nWe get the following results where the author’s full name and ID were extracted correctly:\n2024-09-18 18:02:10.362 | INFO - Extracted author_id: 900fec95-d621-4315-\n84c6-52e5229e0b96\n2024-09-18 18:02:10.362 | INFO - Extracted author_full_name: Paul Iusztin\nNow that we understand how self-querying works, let’s explore how it can be used together with \nfiltered vector search within the retrieval optimization step.\nAdvanced RAG retrieval optimization: filtered vector search\nVector search is pivotal in retrieving relevant information based on semantic similarity. A plain \nvector search, however, can introduce significant challenges that affect both the accuracy and \nlatency of information retrieval. This is primarily because it operates solely on the numerical \nproximity of vector embeddings without considering the contextual or categorical nuances that \nmight be crucial for relevance.\nOne of the primary issues with plain vector search is retrieving semantically similar but contex-\ntually irrelevant documents. Since vector embeddings capture general semantic meanings, they \nmight assign high similarity scores to content that shares language patterns or topics but doesn’t \nalign with the specific intent or constraints of the query. For instance, searching for “Java” could \nretrieve documents about the programming language or the Indonesian island, depending solely \non semantic similarity, leading to ambiguous or misleading results.\nMoreover, as the size of the dataset increases, plain vector search can suffer from scalability issues. \nThe lack of filtering means the search algorithm has to compute similarities across the entire \nvector space, which can significantly increase latency. \n",
      "content_length": 2505,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 362,
      "content": "Chapter 9\n333\nThis exhaustive search slows response times and consumes more computational resources, making \nit inefficient for real-time or large-scale applications.\nFiltered vector search emerges as a solution by filtering after additional criteria, such as metadata \ntags or categories, reducing the search space before computing vector similarities. By applying \nthese filters, the search algorithm narrows the pool of potential results to those contextually \naligned with the query’s intent. This targeted approach enhances accuracy by eliminating irrel-\nevant documents that might have otherwise been considered due to their semantic similarities \nalone.\nAdditionally, filtered vector search improves latency by reducing the number of comparisons the \nalgorithm needs to perform. Working with a smaller, more relevant subset of data decreases the \ncomputational overhead, leading to faster response times. This efficiency is crucial for applica-\ntions requiring real-time interactions or handling large queries.\nAs the metadata used within the filtered vector search is often part of the user’s input, we have \nto extract it before querying the vector DB. That’s precisely what we did during the self-query \nstep, where we extracted the author’s name to reduce the vector space only to the author’s con-\ntent. Thus, as we processed the query within the self-query step, it went into the pre-retrieval \noptimization category, whereas when the filtered vector search optimized the query, it went into \nthe retrieval optimization bin.\nFor example, when using Qdrant, to add a filter that looks for a matching author_id within the \nmetadata of each document, you must implement the following code:\nfrom qdrant_client.models import FieldCondition, Filter, MatchValue\nrecords = qdrant_connection.search(\n            collection_name=\"articles\",\n            query_vector=query_embedding,\n            limit=3,\n            with_payload=True,\n            query_filter= Filter(\n                    must=[\n                        FieldCondition(\n                            key=\"author_id\",\n                            match=MatchValue(\n                                value=str(\"1234\"),\n                            ),\n",
      "content_length": 2212,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 363,
      "content": "RAG Inference Pipeline\n334\n                        )\n                    ]\n                ),\n        )\nIn essence, while plain vector search provides a foundation for semantic retrieval, its limitations \ncan slow performance in practical applications. Filtered vector search addresses these challenges \nby combining the strengths of vector embeddings with contextual filtering, resulting in more \naccurate and efficient information retrieval in RAG systems. The last step for optimizing our RAG \npipeline is to look into reranking.\nAdvanced RAG post-retrieval optimization: reranking\nThe problem in RAG systems is that the retrieved context may contain irrelevant chunks that only:\n• \nAdd noise: The retrieved context might be irrelevant, cluttering the information and \npotentially confusing the language model.\n•\nMake the prompt bigger: Including unnecessary chunks increases the prompt size, lead-\ning to higher costs. Moreover, language models are usually biased toward the context’s \nfirst and last pieces. So, if you add a large amount of context, there’s a big chance it will \nmiss the essence.\n•\nBe come unaligned with your question: Chunks are retrieved based on the similarity \nbetween the query and chunk embeddings. The issue is that the embedding model might \nnot be tuned to your question, resulting in high similarity scores for chunks that aren’t \nentirely relevant.\nThe solution is to use reranking to order all the N × K retrieved chunks based on their relevance \nrelative to the initial question, where the first chunk will be the most relevant and the last the \nleast. N represents the number of searches after query expansion, while K is the number of chunks \nretrieved per search. Hence, we retrieve a total of N x K chunks. In RAG systems, reranking serves \nas a critical post-retrieval step that refines the initial results obtained from the retrieval model.\nWe assess each chunk’s relevance to the original query by applying the reranking algorithm, which \noften uses advanced models like neural cross-encoders. These models evaluate the semantic sim-\nilarity between the query and each chunk more accurately than initial retrieval methods based \non embeddings and the cosine similarity distance, as explained in more detail in Chapter 4 in the \nAn overview of advanced RAG section.\n",
      "content_length": 2309,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 364,
      "content": "Chapter 9\n335\nUltimately, we pick the top K most relevant chunks from the sorted list of N x K items based on \nthe reranking score. Reranking works well when combined with query expansion. First, let’s \nunderstand how reranking works without query expansion:\n1.\nSearch for > K chunks: Retrieve more than K chunks to have a broader pool of potentially \nrelevant information.\n2.\nReorder using rerank: Apply reranking to this larger set to evaluate the actual relevance \nof each chunk relative to the query.\n3.\nTake top K: Select the top K chunks to use them as context in the final prompt.\nThus, when combined with query expansion, we gather potential valuable context from multiple \npoints in space rather than just looking for more than K samples in a single location. Now the \nflow looks like this:\n1.\nSearch for N × K chunks: Retrieve multiple sets of chunks using the expanded queries.\n2.\nReorder using rerank: Rerank all the retrieved chunks based on their relevance.\n3.\nTake top K: Select the most relevant chunks for the final prompt.\nIntegrating reranking into the RAG pipeline enhances the quality and relevance of the re-\ntrieved context and efficiently uses computational resources. Let’s look at implementing the \nLLM Twin’s reranking step to understand what we described above, which can be accessed on \nGitHub at https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_\nengineering/application/rag/reranking.py.\nWe begin by importing the necessary modules and classes for our reranking process:\nfrom llm_engineering.application.networks import \nCrossEncoderModelSingleton\nfrom llm_engineering.domain.embedded_chunks import EmbeddedChunk\nfrom llm_engineering.domain.queries import Query\nfrom .base import RAGStep\nNext, we define the Reranker class, which is responsible for reranking the retrieved documents \nbased on their relevance to the query:\nclass Reranker(RAGStep):\n    def __init__(self, mock: bool = False) -> None:\n        super().__init__(mock=mock)\n        self._model = CrossEncoderModelSingleton()\n",
      "content_length": 2040,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 365,
      "content": "RAG Inference Pipeline\n336\nIn the initializer of the Reranker class, we instantiate our cross-encoder model by creating an \ninstance of CrossEncoderModelSingleton. This is the cross-encoder model used to score the \nrelevance of each document chunk with respect to the query.\nThe core functionality of the Reranker class is implemented in the generate() method:\n    def generate(self, query: Query, chunks: list[EmbeddedChunk], keep_\ntop_k: int) -> list[EmbeddedChunk]:\n        if self._mock:\n            return chunks\n        query_doc_tuples = [(query.content, chunk.content) for chunk in \nchunks]\n        scores = self._model(query_doc_tuples)\n        scored_query_doc_tuples = list(zip(scores, chunks, strict=False))\n        scored_query_doc_tuples.sort(key=lambda x: x[0], reverse=True)\n        reranked_documents = scored_query_doc_tuples[:keep_top_k]\n        reranked_documents = [doc for _, doc in reranked_documents]\n        return reranked_documents\nThe generate() method takes a query, a list of chunks (document segments), and the number \nof top documents to keep (keep_top_k). If we’re in mock mode, it simply returns the original \nchunks. Otherwise, it performs the following steps:\n1.\nCreates pairs of the query content and each chunk’s content\n2.\nUses the cross-encoder model to score each pair, assessing how well the chunk matches \nthe query\n3.\nZips the scores with the corresponding chunks to create a scored list of tuples\n4.\nSorts this list in descending order based on the scores\n5.\nSelects the top keep_top_k chunks\n6.\nExtracts the chunks from the tuples and returns them as the reranked documents\nBefore defining the CrossEncoder class, we import the necessary components:\nfrom sentence_transformers.cross_encoder import CrossEncoder\nfrom .base import SingletonMeta\n",
      "content_length": 1789,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 366,
      "content": "Chapter 9\n337\nWe import the CrossEncoder class from the sentence_transformers library, which provides the \nfunctionality for scoring text pairs. We also import SingletonMeta from our base module to en-\nsure our model class follows the singleton pattern, meaning only one instance of the model exists \nthroughout the application. Now, we define the CrossEncoderModelSingleton class:\nclass CrossEncoderModelSingleton(metaclass=SingletonMeta):\n    def __init__(\n        self,\n        model_id: str = settings.RERANKING_CROSS_ENCODER_MODEL_ID,\n        device: str = settings.RAG_MODEL_DEVICE,\n    ) -> None:\n        \"\"\"\n        A singleton class that provides a pre-trained cross-encoder model \nfor scoring pairs of input text.\n        \"\"\"\n        self._model_id = model_id\n        self._device = device\n        self._model = CrossEncoder(\n            model_name=self._model_id,\n            device=self._device,\n        )\n        self._model.model.eval()\nThis class initializes the cross-encoder model using the specified model_id and device from the \nglobal settings loaded from the .env file. We set the model to evaluation mode using self._\nmodel.model.eval() to ensure the model is ready for inference.\nThe CrossEncoderModelSingleton class includes a callable method to score text pairs:\n    def __call__(self, pairs: list[tuple[str, str]], to_list: bool = True) \n-> NDArray[np.float32] | list[float]:\n        scores = self._model.predict(pairs)\n        if to_list:\n            scores = scores.tolist()\n        return scores\n",
      "content_length": 1525,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 367,
      "content": "RAG Inference Pipeline\n338\nThe __call__ method allows us to pass in a list of text pairs (each consisting of the query and \na document chunk) and receive their relevance scores. The method uses the model’s predict()\nfunction to call the model and compute the scores.\nThe CrossEncoderModelSingleton class is a wrapper over the CrossEncoder class, which we wrote \nfor two purposes. The first one is for the singleton pattern, which allows us to easily access the \nsame instance of the cross-encoder model from anywhere within the application without loading \nthe model in memory every time we need it. The second reason is that by writing our wrapper, \nwe defined our interface for a cross-encoder model (or any other model used for reranking). \nThis makes the code future-proof as in case we need a different implementation or strategy for \nreranking, for example, using an API, we only have to write a different wrapper that follows the \nsame interface and swap the old class with the new one. Thus, we can introduce new reranking \nmethods without touching the rest of the code.\nWe now understand all the advanced RAG techniques used within our architecture. In the next \nsection, we will examine the ContextRetriever class that connects all these methods and explain \nhow to use the retrieval module with an LLM for an end-to-end RAG inference pipeline.\nImplementing the LLM Twin’s RAG inference pipeline\nAs explained at the beginning of this chapter, the RAG inference pipeline can mainly be divided \ninto three parts: the retrieval module, the prompt creation, and the answer generation, which \nboils down to calling an LLM with the augmented prompt. In this section, our primary focus will \nbe implementing the retrieval module, where most of the code and logic go. Afterward, we will \nlook at how to build the final prompt using the retrieved context and user query.\nUltimately, we will examine how to combine the retrieval module, prompt creation logic, and \nthe LLM to capture an end-to-end RAG workflow. Unfortunately, we won’t be able to test out \nthe LLM until we finish Chapter 10, as we haven’t deployed our fine-tuned LLM Twin module to \nAWS SageMaker.\nThus, by the end of this section, you will learn how to implement the RAG inference pipeline, \nwhich you can test out end to end only after finishing Chapter 10. Now, let’s start by looking at \nthe implementation of the retrieval module.\n",
      "content_length": 2404,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 368,
      "content": "Chapter 9\n339\nImplementing the retrieval module\nLet’s dive into the ContextRetriever class implementation, which orchestrates the retrieval \nstep in our RAG system by integrating all the advanced techniques we previously used: que-\nry expansion, self-querying, reranking, and filtered vector search. The class can be found on \nGitHub at https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_\nengineering/application/rag/retriever.py.\nFigure 9.2: Search logic of the RAG retrieval module\n",
      "content_length": 505,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 369,
      "content": "RAG Inference Pipeline\n340\nThe entry point function of the ContextRetriever class is the search() method, which calls all the \nadvanced steps discussed in this chapter. Figure 9.2 shows in more detail how the search method \nglues together all the steps required to search results similar to the user’s query. It highlights how \nthe extracted author details from the self-query step are used within the filtered vector search. \nAlso, it zooms in on the search operation itself, where, for each query, we do three searches to \nthe vector DB, looking for articles, posts, or repositories similar to the query. For each search (out \nof N searches), we want to retrieve a maximum of K results. Thus, we retrieve a maximum of K \n/ 3 items for each data category (as we have three categories). Therefore, when summed up, we \nwill have a list of ≤ K chunks. The retrieved list is ≤ K (and not equal to K) when a particular data \ncategory or more returns < K / 3 items after applying the author filters due to missing chunks \nfor that specific author or data category.\nFigure 9.3: Processing the results flow of the RAG retrieval module\n",
      "content_length": 1128,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 370,
      "content": "Chapter 9\n341\nFigure 9.3 illustrates how we process the results returned by the xN searches. As each search \nreturns ≤ K items, we will end up with ≤ N x K chunks that we aggregate into a single list. As \nsome results might overlap between searchers, we must deduplicate the aggregated list to ensure \neach chunk is unique. Ultimately, we send the results to the rerank model, order them based on \ntheir reranking score, and pick the most relevant top K chunks we will use as context for RAG.\nLet’s understand how everything from Figures 9.2 and 9.3 is implemented in the ContextRetriever \nclass. First, we initialize the class by setting up instances of the QueryExpansion, SelfQuery, and \nReranker classes:\nclass ContextRetriever:\n    def __init__(self, mock: bool = False) -> None:\n        self._query_expander = QueryExpansion(mock=mock)\n        self._metadata_extractor = SelfQuery(mock=mock)\n        self._reranker = Reranker(mock=mock)\nIn the search() method, we convert the user’s input string into a query object. We then use the \nSelfQuery instance to extract the author_id and author_full_name from the query:\n    def search(\n        self,\n        query: str,\n        k: int = 3,\n        expand_to_n_queries: int = 3,\n    ) -> list:\n        query_model = Query.from_str(query)\n        query_model = self._metadata_extractor.generate(query_model)\n        logger.info(\n            \"Successfully extracted the author_id from the query.\",\n            author_id=query_model.author_id,\n        )\nNext, we expand the query to generate multiple semantically similar queries using the \nQueryExpansion instance:\n        n_generated_queries = self._query_expander.generate(query_model, \nexpand_to_n=expand_to_n_queries)\n        logger.info(\n            \"Successfully generated queries for search.\",\n",
      "content_length": 1799,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 371,
      "content": "RAG Inference Pipeline\n342\n            num_queries=len(n_generated_queries),\n        )\nWe then perform the search concurrently for all expanded queries using a thread pool. Each que-\nry is processed by the _search() method, which we’ll explore shortly. The results are flattened, \ndeduplicated, and collected into a single list:\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            search_tasks = [executor.submit(self._search, _query_model, k) \nfor _query_model in n_generated_queries]\n            n_k_documents = [task.result() for task in concurrent.futures.\nas_completed(search_tasks)]\n            n_k_documents = utils.misc.flatten(n_k_documents)\n            n_k_documents = list(set(n_k_documents))\n        logger.info(\"All documents retrieved successfully.\", num_\ndocuments=len(n_k_documents))\nAfter retrieving the documents, we rerank them based on their relevance to the original query \nand keep only the top k documents:\n        if len(n_k_documents) > 0:\n            k_documents = self.rerank(query, chunks=n_k_documents, keep_\ntop_k=k)\n        else:\n            k_documents = []\n        return k_documents\nThe _search() method performs the filtered vector search across different data categories like \nposts, articles, and repositories. It uses the EmbeddingDispatcher to convert the query into an \nEmbeddedQuery, which includes the query’s embedding vector and any extracted metadata:\n    def _search(self, query: Query, k: int = 3) -> list[EmbeddedChunk]:\n        assert k >= 3, \"k should be >= 3\"\n        def _search_data_category(\n            data_category_odm: type[EmbeddedChunk], embedded_query: \nEmbeddedQuery\n        ) -> list[EmbeddedChunk]:\n",
      "content_length": 1689,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 372,
      "content": "Chapter 9\n343\n            if embedded_query.author_id:\n                query_filter = Filter(\n                    must=[\n                        FieldCondition(\n                            key=\"author_id\",\n                            match=MatchValue(\n                                value=str(embedded_query.author_id),\n                            ),\n                        )\n                    ]\n                )\n            else:\n                query_filter = None\n            return data_category_odm.search(\n                query_vector=embedded_query.embedding,\n                limit=k // 3,\n                query_filter=query_filter,\n            )\n        embedded_query: EmbeddedQuery = EmbeddingDispatcher.\ndispatch(query)\nWe used the same EmbeddingDispatcher to embed the query as in the RAG feature pipeline to \nembed the document chunks stored in the vector DB. Using the same class ensures we use the \nsame embedding model at ingestion and query time, which is critical for the retrieval step.\nWe search each data category separately by leveraging the local _search_data_category() func-\ntion. Within the _search_data_category() function, we apply the filters extracted from the \nembedded_query object. For instance, if an author_id is present, we use it to filter the search results \nonly to include documents from that author. The results from all categories are then combined:\n        post_chunks = _search_data_category(EmbeddedPostChunk, embedded_\nquery)\n        articles_chunks = _search_data_category(EmbeddedArticleChunk, \nembedded_query)\n        repositories_chunks = _search_data_\ncategory(EmbeddedRepositoryChunk, embedded_query)\n",
      "content_length": 1658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 373,
      "content": "RAG Inference Pipeline\n344\n        retrieved_chunks = post_chunks + articles_chunks + repositories_\nchunks\n        return retrieved_chunks\nFinally, the rerank() method takes the original query and the list of retrieved documents to \nreorder them based on relevance:\n    def rerank(self, query: str | Query, chunks: list[EmbeddedChunk], \nkeep_top_k: int) -> list[EmbeddedChunk]:\n        if isinstance(query, str):\n            query = Query.from_str(query)\n        reranked_documents = self._reranker.generate(query=query, \nchunks=chunks, keep_top_k=keep_top_k)\n        logger.info(\"Documents reranked successfully.\", num_\ndocuments=len(reranked_documents))\n        return reranked_documents\nLeveraging the ContextRetriever class, we can retrieve context from any query with only a few \nlines of code. For example, let’s take a look at the following code snippet, where we call the entire \nadvanced RAG architecture with a simple call to the search() method:\nfrom loguru import logger\nfrom llm_engineering.application.rag.retriever import ContextRetriever\nquery = \"\"\"\n        My name is Paul Iusztin.\n       \n        Could you draft a LinkedIn post discussing RAG systems?\n        I'm particularly interested in:\n            - how RAG works\n            - how it is integrated with vector DBs and large language \nmodels (LLMs).\n        \"\"\"\n",
      "content_length": 1337,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 374,
      "content": "Chapter 9\n345\nretriever = ContextRetriever(mock=False)\ndocuments = retriever.search(query, k=3)\nlogger.info(\"Retrieved documents:\")\nfor rank, document in enumerate(documents):\n    logger.info(f\"{rank + 1}: {document}\")\nCalling the code from above using the following CLI command: poetry poe call-rag-retrieval-\nmodule. This outputs the following:\n2024-09-18 19:01:50.588 | INFO - Retrieved documents:\n2024-09-18 19:01:50.588 | INFO - 1: id=UUID('541d6c22-d15a-4e6a-924a-\n68b7b1e0a330') content='4 Advanced RAG Algorithms You Must Know by \nPaul Iusztin Implement 4 advanced RAG retrieval techniques to optimize \nyour vector DB searches. Integrate the RAG retrieval module into a \nproduction LLM system…\" platform='decodingml.substack.com' document_\nid=UUID('32648f33-87e6-435c-b2d7-861a03e72392') author_id=UUID('900fec95-\nd621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' \nmetadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', \n'embedding_size': 384, 'max_input_length': 256} link='https://decodingml.\nsubstack.com/p/the-4-advanced-rag-algorithms-you?r=1ttoeh'\n2024-09-18 19:01:50.588 | INFO - 2: id=UUID('5ce78438-1314-4874-8a5a-\n04f5fcf0cb21') content='Overview of advanced RAG optimization techniquesA \nproduction RAG system is split into 3 main components ingestion clean, \nchunk, embed, and load your data to a vector DBretrieval query your vector \nDB for …\" platform='medium' document_id=UUID('bd9021c9-a693-46da-97e7-\n0d06760ee6bf') author_id=UUID('900fec95-d621-4315-84c6-52e5229e0b96') \nauthor_full_name='Paul Iusztin' metadata={'embedding_model_id': 'sentence-\ntransformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': \n256} link='https://medium.com/decodingml/the-4-advanced-rag-algorithms-\nyou-must-know-to-implement-5d0c7f1199d2'\n2024-09-18 19:02:45.729 | INFO  - 3: id=UUID('0405a5da-4686-428a-91ca-\n446b8e0446ff') content='Every Medium article will be its own lesson \nAn End to End Framework for Production Ready LLM Systems by Building \nYour LLM TwinThe Importance of Data Pipelines in the Era of Generative \nAIChange Data Capture Enabling Event Driven …\" platform='medium' document_\nid=UUID('bd9021c9-a693-46da-97e7-0d06760ee6bf') author_id=UUID('900fec95-\nd621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' \nmetadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', \n'embedding_size': 384, 'max_input_length': 256} link='https://medium.\n",
      "content_length": 2431,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 375,
      "content": "RAG Inference Pipeline\n346\ncom/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-\n5d0c7f1199d2'\nAs you can observe in the output above, along with the retrieved content, we have access to all \nkinds of metadata, such as the embedding model used for retrieval or the link from which the \nchunk was taken. These can quickly be added to a list of references when generating the result \nfor the user, increasing trust in the final results.\nNow that we understand how the retrieval module works, let’s take a final step and examine the \nend-to-end RAG inference pipeline.\nBringing everything together into the RAG inference \npipeline\nTo fully implement the RAG flow, we still have to build the prompt using the context from the \nretrieval model and call the LLM to generate the answer. This section will discuss these two steps \nand wrap everything together into a single rag() function. The functions from this section can \nbe accessed on GitHub at https://github.com/PacktPublishing/LLM-Engineers-Handbook/\nblob/main/llm_engineering/infrastructure/inference_pipeline_api.py.\nLet’s start by looking at the call_llm_service()function, responsible for interfacing with the \nLLM service. It takes in a user’s query and an optional context, sets up the language model end-\npoint, executes the inference, and returns the generated answer. The context is optional; you can \ncall the LLM without it, as you would when interacting with any other LLM:\ndef call_llm_service(query: str, context: str | None) -> str:\n    llm = LLMInferenceSagemakerEndpoint(\n        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_\ncomponent_name=None\n    )\n    answer = InferenceExecutor(llm, query, context).execute()\n    return answer\nThis function makes an HTTP request to our fine-tuned LLM Twin model, which is hosted as \nan AWS SageMaker inference endpoint. We will explore all the SageMaker details in the next \nchapter, where we will dig into the LLMInferenceSagemakerEndpoint and InferenceExecutor\nclasses. For now, what is essential to know is that we use this function to call our fine-tuned LLM. \nStill, we must highlight how the query and context, passed to the InferenceExecutor class, are \ntransformed into the final prompt. We do that using a simple prompt template that is customized \nusing the user query and retrieved context:\n",
      "content_length": 2347,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 376,
      "content": "Chapter 9\n347\nprompt = f\"\"\"\nYou are a content creator. Write what the user asked you to while using \nthe provided context as the primary source of information for the content.\nUser query: {query}\nContext: {context}\n          \"\"\"\nMoving on to the rag() function, this is where the RAG logic comes together. It handles retriev-\ning relevant documents based on the query, mapping the documents to the context that will be \ninjected into the prompt, and obtaining the final answer from the LLM:\ndef rag(query: str) -> str:\n    retriever = ContextRetriever(mock=False)\n    documents = retriever.search(query, k=3)\n    context = EmbeddedChunk.to_context(documents)\n    answer = call_llm_service(query, context)\n    return answer\nAs we modularized all the RAG steps into independent classes, we reduced the high-level rag()\nfunction to five lines of code (encapsulating all the complexities of the system) similar to what we \nsee in tools such as LangChain, LlamaIndex, or Haystack. Instead of their high-level implementa-\ntion, we learned how to build an advanced RAG service from scratch. Also, by clearly separating \nthe responsibility of each class, we can use them like LEGOs. Thus, you can quickly call the LLM \nindependently without context or use the retrieval module as a query engine on top of your vector \nDB. In the next chapter, we will see the rag() function in action after we deploy our fine-tuned \nLLM to an AWS SageMaker inference endpoint.\nBefore ending this chapter, we want to discuss potential improvements you could add to the RAG \ninference pipeline. As we are building a chatbot, the first one is to add a conversation memory that \nstores all the user prompts and generated answers in memory. Thus, when interacting with the \nchatbot, it will be aware of the whole conversation, not only the latest prompt. When prompting \nthe LLM, along with the new user input and context, we also pass the conversation history from \nthe memory. As the conversation history can get long, to avoid exceeding the context window or \nhigher costs, you have to implement a way to reduce the size of your memory. As illustrated in \nFigure 9.4, the simplest one is to keep only the latest K items from your chat history. Unfortunately, \nusing this strategy, the LLM will never be aware of the whole conversation. \n",
      "content_length": 2310,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 377,
      "content": "RAG Inference Pipeline\n348\nTherefore, another way to add the chat history to your prompt is to keep a summary of the conver-\nsation along with the latest K replies. There are multiple ways to compute this summary, which \nmight defeat the purpose of this book if we get into them all, but the simplest way is to always \nupdate the summary on every user prompt and generate an answer.\nFigure 9.4: Routing and memory examples\nAs for each search, we send three queries to the vector DB, one for each data category. Thus, the \nsecond improvement is to add a router between the query and the search. The router will be a \nmulti-category classifier that predicts the data categories we must retrieve for that specific query. \nHence, instead of making three requests for every search, we can often reduce it to one or two. For \nexample, if the user wants to write a theoretical paragraph about RAG for an article, then most \nprobably, it’s valuable to query only the article’s collection. In this case, the router will predict \nthe article class, which we can use to decide what collection we must query. \n",
      "content_length": 1098,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 378,
      "content": "Chapter 9\n349\nAnother example would be if we want to illustrate a piece of code that shows how to build a RAG \npipeline. In this case, the router would have to predict the article and repository data category, \nas we need to look up examples in both collections for an exhaustive context.\nUsually, the router strategy decides what model to call based on a user’s input, such as whether \nto use GPT-4 or a self-hosted Llama 3.1 model for that specific query. However,  in our particular \nuse case, we can adapt the router algorithm to optimize the retrieval step.\nWe can further optimize the retrieval using a hybrid search algorithm that combines the vector \nsearch (based on embeddings) with a keyword search algorithm, such as BM25. Search algorithms \nused BM25 (or similar methods) to find similar items in a DB before vector search algorithms \nbecame popular. By merging the methods, hybrid search retrieves results that match the exact \nterms, such as RAG, LLM, or SageMaker, and the query semantics, increasing the accuracy and \nrelevance of your retrieved results. Fundamentally, the hybrid search algorithms follow the next \nmechanics:\n1.\nParallel processing: The search query is processed simultaneously through both the \nvector search and BM25 algorithms. Each algorithm retrieves a set of relevant documents \nbased on its criteria.\n2.\nScore normalization: The results from both searches are assigned relevance scores, which \nare then normalized to ensure comparability. This step is crucial because vector search \nand BM25 scoring mechanisms work at different scales. Thus, they can’t be compared or \nmerged without normalization.\n3.\nResult merging: The normalized scores are combined, often through a weighted sum, to \nproduce a final ranking of documents. Adjusting the weights allows for fine-tuning the \nemphasis on the semantic or keyword search algorithm.\nTo conclude, by combining the semantic and exact keyword search algorithms, you can improve \nthe accuracy of your retrieval step. Vector search helps recognize synonyms or related concepts, \nensuring that relevant information isn’t overlooked due to vocabulary differences. Keyword search \nensures that documents containing critical keywords are emphasized appropriately, particularly \nin technical fields with specific terminology.\nOne last improvement we can make to our RAG system is to use multi-index vector structures \ninstead of indexing based only on the content’s embedding. Let’s detail how multi-indexing \nworks. Instead of using the embeddings of a single field to do the vector search for a particular \ncollection, it combines multiple fields. \n",
      "content_length": 2631,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 379,
      "content": "RAG Inference Pipeline\n350\nFor example, in our LLM Twin use case, we used only the content field of our articles, posts, or \nrepositories to query the vector DB. When using a multi-index strategy, along with the content \nfield, we could index the embeddings of the platform where the content was posted or when the \ncontent was published. This could impact the final accuracy of your retrieval as different platforms \nhave different types of content, or more recent content is usually more relevant. Frameworks \nsuch as Superlinked make multi-indexing easy. For example, in the code snippet below, using \nSuperlinked, we defined a multi-index on the content and platform for our article collection in \njust a few lines of code:\nfrom superlinked.framework.common.schema.id_schema_object import IdField\nfrom superlinked.framework.common.schema.schema import schema\nfrom superlinked.framework.common.schema.schema_object import String\n… # Other Superlinked imports. \n@schema\nclass ArticleSchema:\n    id: IdField\n    platform: String\n    content: String\narticle = ArticleSchema()\narticles_space_content = TextSimilaritySpace(\n    text=chunk(article.content, chunk_size=500, chunk_overlap=50),\n    model=settings.EMBEDDING_MODEL_ID,\n)\narticles_space_plaform = CategoricalSimilaritySpace(\n    category_input=article.platform,\n    categories=[\"medium\", \"substack\", \"wordpress\"],\n    negative_filter=-5.0,\n)\narticle_index = Index(\n    [articles_space_content, articles_space_plaform],\n    fields=[article.author_id],\n)\n",
      "content_length": 1511,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 380,
      "content": "Chapter 9\n351\nSuperlinked is a powerful Python tool for any use case that includes vector computing, such as RAG, \nrecommender systems, and semantic search. It offers an ecosystem where you can quickly ingest \ndata into a vector DB, write complex queries on top of it, and deploy the service as a RESTful API.\nThe world of LLMs and RAG is experimental, similar to any other AI domain. Thus, when build-\ning real-world products, it’s important to quickly build an end-to-end solution that works but is \nnot necessarily the best. Then, you can reiterate with various experiments until you completely \noptimize it for your use case. This is standard practice in the industry and lets you iterate fast \nwhile providing value to the business and gathering user feedback as quickly as possible in the \nproduct’s lifecycle.\nSummary\nThis chapter taught us how to build an advanced RAG inference pipeline. We started by looking \ninto the software architecture of the RAG system. Then, we zoomed in on the advanced RAG meth-\nods we used within the retrieval module, such as query expansion, self-querying, filtered vector \nsearch, and reranking. Afterward, we saw how to write a modular ContextRetriever class that \nglues all the advanced RAG components under a single interface, making searching for relevant \ndocuments a breeze. Ultimately, we looked into how to connect all the missing dots, such as the \nretrieval, the prompt augmentation, and the LLM call, under a single RAG function that will serve \nas our RAG inference pipeline.\nAs highlighted a few times in this chapter, we couldn’t test our fine-tuned LLM because we haven’t \ndeployed it yet to AWS SageMaker as an inference endpoint. Thus, in the next chapter, we will \nlearn how to deploy the LLM to AWS SageMaker, write an inference interface to call the endpoint, \nand implement a FastAPI web server to serve as our business layer.\nReferences\n• \nA real-time retrieval system for social media data | VectorHub by SuperLinked. (n.d.). https://\nsuperlinked.com/vectorhub/articles/real-time-retrieval-system-social-media-\ndata\n• \nBuilding a Router from Scratch - LlamaIndex. (n.d.). https://docs.llamaindex.ai/en/\nstable/examples/low_level/router/\n• \nHow to add memory to chatbots | LangChain. (n.d.). https://python.langchain.com/docs/\nhow_to/chatbots_memory/#summary-memory\n",
      "content_length": 2328,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 381,
      "content": "RAG Inference Pipeline\n352\n• \nHow to do “self-querying” retrieval | LangChain. (n.d.). https://python.langchain.com/\ndocs/how_to/self_query/\n• \nHow to route between sub-chains | LangChain. (n.d.). https://python.langchain.com/\ndocs/how_to/routing/#routing-by-semantic-similarity\n• \nHow to use the MultiQueryRetriever | LangChain. (n.d.). https://python.langchain.com/\ndocs/how_to/MultiQueryRetriever/\n• \nHybrid Search explained. (2023, January 3). Weaviate. https://weaviate.io/blog/hybrid-\nsearch-explained\n•\nIusztin, P. (2024, August 20). 4 Advanced RAG Algorithms You Must Know | Decoding \nML. Medium. https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-\nmust-know-to-implement-5d0c7f1199d2\n•\nMonigatti, L. (2024, February 19). Advanced Retrieval-Augmented Generation: From Theory \nto LlamaIndex Implementation. Medium. https://towardsdatascience.com/advanced-\nretrieval-augmented-generation-from-theory-to-llamaindex-implementation-\n4de1464a9930\n• \nMulti-attribute search with vector embeddings | VectorHub by Superlinked. (n.d.). https://\nsuperlinked.com/vectorhub/articles/multi-attribute-semantic-search\n•\nOptimizing RAG with Hybrid Search & Reranking | VectorHub by Superlinked. (n.d.). https://\nsuperlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-\nreranking\n•\nRefactoring.Guru. (2024, January 1). Singleton. https://refactoring.guru/design-\npatterns/singleton\n•\nStoll, M. (2024, September 7). Visualize your RAG Data—Evaluate your Retrieval-Aug-\nmented Generation System with Ragas. Medium. https://towardsdatascience.com/\nvisualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-\nwith-ragas-fc2486308557\n•\nUsing LLM’s for retrieval and reranking—LlamaIndex, data framework for LLM applications. \n(n.d.). https://www.llamaindex.ai/blog/using-llms-for-retrieval-and-reranking-\n23cf2d3a14b6\n",
      "content_length": 1846,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 382,
      "content": "Chapter 9\n353\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 157,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 384,
      "content": "10\nInference Pipeline Deployment\nDeploying the inference pipeline for the large language model (LLM) Twin application is a critical \nstage in the machine learning (ML) application life cycle. It’s where the most value is added to \nyour business, making your models accessible to your end users. However, successfully deploying \nAI models can be challenging, as the models require expensive computing power and access to \nup-to-date features to run the inference. To overcome these constraints, it’s crucial to careful-\nly design your deployment strategy. This ensures that it meets the application’s requirements, \nsuch as latency, throughput, and costs. As we work with LLMs, we must consider the inference \noptimization techniques presented in Chapter 8, such as model quantization. Also, to automate \nthe deployment processes, we must leverage MLOps best practices, such as model registries that \nversion and share our models across our infrastructure.\nTo understand how to design the deployment architecture of the LLM Twin, we will first look at \nthree deployment types we can choose from: online real-time inference, asynchronous inference, \nand offline batch transform. Also, to better understand which option to choose for our LLM Twin \nuse case, we will quickly walk you through a set of critical criteria we must consider before making \nan architectural decision, such as latency, throughput, data, and infrastructure. Also, we’ll weigh \nthe pros and cons of monolithic and microservices architecture in model serving, a decision that \ncan significantly influence the scalability and maintainability of your service.Once we’ve grasped \nthe various design choices available, we’ll focus on understanding the deployment strategy for \nthe LLM Twin’s inference pipeline. Subsequently, we will walk you through an end-to-end tu-\ntorial on deploying the LLM Twin service, including deploying our custom fine-tuned LLM to \nAWS SageMaker endpoints and implementing a FastAPI server as the central entry point for our \nusers. We will then wrap up this chapter with a short discussion on autoscaling strategies and \nhow to use them on SageMaker.\n",
      "content_length": 2146,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 385,
      "content": "Inference Pipeline Deployment\n356\nHence, in this chapter, we will cover the following topics:\n•\nCriteria for choosing deployment types\n•\nUnderstanding inference deployment types\n•\nMonolithic versus microservices architecture in model serving\n•\nExploring the LLM Twin’s inference pipeline deployment strategy\n•\nDeploying the LLM Twin service\n•\nAutoscaling capabilities to handle spikes in usage\nCriteria for choosing deployment types\nWhen it comes to deploying ML models, the first step is to understand the four requirements \npresent in every ML application: throughput, latency, data, and infrastructure.\nUnderstanding them and their interaction is essential. When designing the deployment archi-\ntecture for your models, there is always a trade-off between the four that will directly impact the \nuser’s experience. For example, should your model deployment be optimized for low latency or \nhigh throughput?\nThroughput and latency\nThroughput refers to the number of inference requests a system can process in a given period. \nIt is typically measured in requests per second (RPS). Throughput is crucial when deploying \nML models when you expect to process many requests. It ensures the system can handle many \nrequests efficiently without becoming a bottleneck.\nHigh throughput often requires scalable and robust infrastructure, such as machines or clusters \nwith multiple high-end GPUs.Latency is the time it takes for a system to process a single inference \nrequest from when it is received until the result is returned. Latency is critical in real-time appli-\ncations where quick response times are essential, such as in live user interactions, fraud detection, \nor any system requiring immediate feedback. For example, the average latency of OpenAI’s API \nis the average response time from when a user sends a request, and the service provides a result \nthat is accessible within your application.\nThe latency is the sum of the network I/O, serialization and deserialization, and the LLM’s infer-\nence time. Meanwhile, the throughput is the average number of requests the API processes and \nserves a second.\n",
      "content_length": 2114,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 386,
      "content": "Chapter 10\n357\nLow-latency systems require optimized and often more costly infrastructure, such as faster pro-\ncessors, lower network latency, and possibly edge computing to reduce the distance data needs \nto travel.\nA lower latency translates to higher throughput when the service processes multiple queries in \nparallel successfully. For example, if the service takes 100 ms to process requests, this translates to \na throughput of 10 requests per second. If the latency reaches 10 ms per request, the throughput \nrises to 100 requests per second.\nHowever, to complicate things, most ML applications adopt a batching strategy to simultaneously \npass multiple data samples to the model. In this case, a lower latency can translate into lower \nthroughput; in other words, a higher latency maps to a higher throughput. For example, if you \nprocess 20 batched requests in 100 ms, the latency is 100 ms, while the throughput is 200 requests \nper second. If you process 60 requests in 200 ms, the latency is 200 ms, while the throughput \nrises to 300 requests per second. Thus, even when batching requests at serving time, it’s essential \nto consider the minimum latency accepted for a good user experience.\nData\nAs we know, data is everywhere in an ML system. But when talking about model serving, we \nmostly care about the model’s input and output. This includes the format, volume, and complexity \nof the processed data. Data is the foundation of the inference process. The characteristics of the \ndata, such as its size and type, determine how the system needs to be configured and optimized \nfor efficient processing.\nThe type and size of the data directly impact latency and throughput, as more complex or exten-\nsive data can take longer to process. For example, designing a model that takes input structured \ndata and outputs a probability differs entirely from an LLM that takes input text (or even images) \nand outputs an array of characters.\nInfrastructure\nInfrastructure refers to the underlying hardware, software, networking, and system architecture \nthat supports the deployment and operation of the ML models. The infrastructure provides the \nnecessary resources for deploying, scaling, and maintaining ML models. It includes computing \nresources, memory, storage, networking components, and the software stack:\n•\nFor high throughput, the systems require scalable infrastructure to manage large data \nvolumes and high request rates, possibly through parallel processing, distributed systems, \nand high-end GPUs.\n",
      "content_length": 2524,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 387,
      "content": "Inference Pipeline Deployment\n358\n•\nInfrastructure must be optimized to reduce processing time to achieve low latency, such \nas using faster CPUs, GPUs, or specialized hardware. While optimizing your system for \nlow latency while batching your requests, you often have to sacrifice high throughput \nin favor of lower latency, which can result in your hardware not being utilized at total \ncapacity. As you process fewer requests per second, it results in idle computing, which \nincreases the overall cost of processing a request. Thus, picking the suitable machine for \nyour requirements is critical in optimizing costs.\nIt is crucial to design infrastructure to meet specific data requirements. This includes selecting \nstorage solutions to handle large datasets and implementing fast retrieval mechanisms to ensure \nefficient data access. For example, we mostly care about optimizing throughput for offline training, \nwhile for online inference, we generally care about latency.\nWith this in mind, before picking a specific deployment type, you should ask yourself questions \nsuch as:\n• \nWhat are the throughput requirements? You should make this decision based on the \nthroughput’s required minimum, average, and maximum statistics.\n• \nHow many requests the system must handle simultaneously? (1, 10, 1,000, 1 million, etc.)\n• \nWhat are the latency requirements? (1 millisecond, 10 milliseconds, 1 second, etc.)\n• \nHow should the system scale? For example, we should look at the CPU workload, number \nof requests, queue size, data size, or a combination of them.\n•\nWhat are the cost requirements?With what data do we work with? For example, do we \nwork with images, text, or tabular data?\n•\nWhat is the size of the data we work with? (100 MB, 1 GB, 10 GB)\n",
      "content_length": 1759,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 388,
      "content": "Chapter 10\n359\nDeeply thinking about these questions directly impacts the user experience of your application, \nwhich ultimately makes the difference between a successful product and not. Even if you ship a \nmind-blowing model, if the user needs to wait too long for a response or it often crashes, the user \nwill switch your production to something less accurate that works reliably. For example, Google \nfound in a 2016 study that 53% of visits are abandoned if a mobile site takes longer than three \nseconds to load: https://www.thinkwithgoogle.com/consumer-insights/consumer-trends/\nmobile-site-load-time-statistics/.\nLet’s move on to the three deployment architectures we can leverage to serve our models.\nUnderstanding inference deployment types\nAs illustrated in Figure 10.1, you can choose from three fundamental deployment types when \nserving models:\n•\nOnline real-time inference\n•\nAsynchronous inference\n•\nOffline batch transform\nWhen selecting one design over the other, there is a trade-off between latency, throughput, and \ncosts. You must consider how the data is accessed and the infrastructure you are working with. \nAnother criterion you have to consider is how the user will interact with the model. For example, \nwill the user use it directly, like a chatbot, or will it be hidden within your system, like a classifier \nthat checks if an input (or output) is safe?\nYou have to consider the freshness of the predictions as well. For example, serving your model in \noffline batch mode might be easier to implement if, in your use case, it is OK to consume delayed \npredictions. Otherwise, you have to serve your model in real-time, which is more infrastruc-\nture-demanding. Also, you have to consider your application’s traffic. Ask yourself questions such \nas, “Will the application be constantly used, or will there be spikes in traffic and then flatten out?”\n",
      "content_length": 1879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 389,
      "content": "Inference Pipeline Deployment\n360\nWith that in mind, let’s explore the three major ML deployment types.\nFigure 10.1: The three fundamental architectures of inference deployment types\nOnline real-time inference\nIn real-time inference, we have a simple architecture based on a server that can be accessed \nthrough HTTP requests. The most popular options are to implement a REST API or gRPC server. \nThe REST API is more accessible but slower, using JSON to pass data between the client and server. \n",
      "content_length": 497,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 390,
      "content": "Chapter 10\n361\nThis approach is usually taken when serving models outside your internal network to the broader \npublic. For example, OpenAI’s API implements a REST API protocol.\nOn the other hand, implementing a gRPC makes your ML server faster, though it may reduce \nits flexibility and general applicability. You have to implement protobuf schemas in your client \napplication, which are more tedious to work with than JSON structures. The benefit, however, \nis that protobuf objects can be compiled into bites, making the network transfers much faster. \nThus, this protocol is often adopted for internal services within the same ML system.\nUsing the real-time inference approach, the client sends an HTTP request to the ML service, which \nimmediately processes the request and returns the result in the same response. This synchronous \ninteraction means the client waits for the result before moving on.\nTo make this work efficiently, the infrastructure must support low-latency, highly responsive ML \nservices, often deployed on fast, scalable servers. Load balancing is crucial to evenly distribute \nincoming traffic evenly, while autoscaling ensures the system can handle varying loads. High \navailability is also essential to keeping the service operational at all times.\nFor example, this architecture is often present when interacting with LLMs, as when sending a \nrequest to a chatbot or API (powered by LLMs), you expend the predictions right ahead. LLM \nservices, such as ChatGPT or Claude, often use WebSockets to stream each token individually to \nthe end user, making the interaction more responsive. Other famous examples are AI services such \nas embedding or reranking models used for retrieval-augmented generation (RAG) or online \nrecommendation engines in platforms like TikTok.\nThe simplicity of real-time inference, with its direct client-server interaction, makes it an attrac-\ntive option for applications that require immediate responses, like chatbots or real-time recom-\nmendations. However, this approach can be challenging to scale and may lead to underutilized \nresources during low-traffic periods.\nAsynchronous inference\nIn asynchronous inference, the client sends a request to the ML service, which acknowledges the \nrequest and places it in a queue for processing. Unlike real-time inference, the client doesn’t wait \nfor an immediate response. Instead, the ML service processes the request asynchronously. This re-\nquires a robust infrastructure that queues the messages to be processed by the ML service later on.\nWhen the results are ready, you can leverage multiple techniques to send them to the client. For \nexample, depending on the size of the result, you can put it either in a different queue or an object \nstorage dedicated to storing the results. \n",
      "content_length": 2793,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 391,
      "content": "Inference Pipeline Deployment\n362\nThe client can either adopt a polling mechanism that checks on a schedule if there are new re-\nsults or adopt a push strategy and implement a notification system to inform the client when \nthe results are ready.\nAsynchronous inference uses resources more efficiently. It doesn’t have to process all the requests \nsimultaneously but can define a maximum number of machines that run in parallel to process \nthe messages. This is possible because the requests are stored in the queue until a machine can \nprocess them. Another huge benefit is that it can handle spikes in requests without any timeouts. \nFor example, let’s assume that on an e-shop site, we usually have 10 requests per second handled \nby two machines. Because of a promotion, many people started to visit the site, and the number \nof requests spiked to 100 requests per second. Instead of scaling the number of virtual machines \n(VMs) by 10, which can add drastic costs, the requests are queued, and the same two VMs can \nprocess them in their rhythm without any failures.\nAnother popular advantage for asynchronous architectures is when the requested job takes sig-\nnificant time to complete. For example, if the job takes over five minutes, you don’t want to block \nthe client waiting for a response.\nWhile asynchronous inference offers significant benefits, it does come with trade-offs. It intro-\nduces higher latency, making it less suitable for time-sensitive applications. Additionally, it adds \ncomplexity to the implementation and infrastructure. Depending on your design choices, this \narchitecture type falls somewhere between online and offline, offering a balance of benefits and \ntrade-offs.\nFor example, this is a robust design where you don’t care too much about the latency of the infer-\nence but want to optimize costs heavily. Thus, it is a popular choice for problems such as extracting \nkeywords from documents, summarizing them using LLMs, or running deep-fake models on top \nof videos. But suppose you carefully design the autoscaling system to process the requests from \nthe queue at decent speeds. In that case, you can leverage this design for other use cases, such as \nonline recommendations for e-commerce. In the end, it sums up how much computing power \nyou are willing to pay to meet the expectations of your application.\nOffline batch transform\nBatch transform is about processing large volumes of data simultaneously, either on a schedule \nor triggered manually. In a batch transform architecture, the ML service pulls data from a storage \nsystem, processes it in a single operation, and then stores the results in storage. The storage sys-\ntem can be implemented as an object storage like AWS S3 or a data warehouse like GCP BigQuery. \n",
      "content_length": 2768,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 392,
      "content": "Chapter 10\n363\nUnlike the asynchronous inference architecture, a batch transform design is optimized for high \nthroughput with permissive latency requirements. When real-time predictions are unnecessary, \nthis approach can significantly reduce costs, as processing data in big batches is the most eco-\nnomical method. Moreover, the batch transform architecture is the simplest way to serve a model, \naccelerating development time.\nThe client pulls the results directly from data storage, decoupling its interaction with the ML \nservice. Taking this approach, the client never has to wait for the ML service to process its input, \nbut at the same time, it doesn’t have the flexibility to ask for new results at any time. You can \nsee the data storage, where the results are stored as a large cache, from where the client can take \nwhat is required. If you want to make your application more responsive, the client can be notified \nwhen the processing is complete and can retrieve the results.\nUnfortunately, this approach will always introduce a delay between the time the predictions \nwere computed and consumed. That’s why not all applications can leverage this design choice. \nFor example, if we implement a recommender system for a video streaming application, having a \ndelay of one day for the predicted movies and TV shows might work because you don’t consume \nthese products at a high frequency. But suppose you make a recommender system for a social \nmedia platform. In that case, delaying one day or even one hour is unacceptable, as you constantly \nwant to provide fresh content to the user.\nBatch transform shines in scenarios where high throughput is needed, like data analytics or pe-\nriodic reporting. However, it’s unsuitable for real-time applications due to its high latency and \nrequires careful planning and scheduling to manage large datasets effectively. That’s why it is \nan offline serving method.\nTo conclude, we examined the three most common architectures for serving ML models. We \nstarted with online real-time inference, which serves clients when they request a prediction. \nThen, we looked at the asynchronous inference method, which sits between online and offline. \nUltimately, we presented the offline batch transform, which is used to process large amounts of \ndata and store them in data storage, from where the client later consumes them.\nMonolithic versus microservices architecture in \nmodel serving\nIn the previous section, we saw three different methods of deploying the ML service. The differ-\nences in architecture were mainly based on the interaction between the client and the ML service, \nsuch as the communication protocol, the ML service responsiveness, and prediction freshness. \n",
      "content_length": 2728,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 393,
      "content": "Inference Pipeline Deployment\n364\nBut another aspect to consider is the architecture of the ML service itself, which can be imple-\nmented as a monolithic server or as multiple microservices. This will impact how the ML service \nis implemented, maintained, and scaled. Let’s explore the two options.\nFigure 10.2: Monolithic versus microservices architecture in model serving\n",
      "content_length": 374,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 394,
      "content": "Chapter 10\n365\nMonolithic architecture\nThe LLM (or any other ML model) and the associated business logic (preprocessing and post-pro-\ncessing steps) are bundled into a single service in a monolithic architecture. This approach is \nstraightforward to implement at the beginning of a project, as everything is placed within one \ncode base. Simplicity makes maintenance easy when working on small to medium projects, as \nupdates and changes can be made within a unified system.\nOne key challenge of a monolithic architecture is the difficulty of scaling components independent-\nly. The LLM typically requires GPU power, while the rest of the business logic is CPU and I/O-bound. \nAs a result, the infrastructure must be optimized for both GPU and CPU. This can lead to inefficient \nresource use, with the GPU being idle when the business logic is executed and vice versa. Such \ninefficiency can result in additional costs that could be avoided.\nMoreover, this architecture can limit flexibility, as all components must share the same tech \nstack and runtime environment. For example, you might want to run the LLM using Rust or C++ \nor compile it with ONNX or TensorRT while keeping the business logic in Python. Having all the \ncode in one system makes this differentiation difficult. Finally, splitting the work across different \nteams is complex, often leading to bottlenecks and reduced agility.\nMicroservices architecture\nA microservices architecture breaks down the inference pipeline into separate, independent ser-\nvices—typically splitting the LLM service and the business logic into distinct components. These \nservices communicate over a network using protocols such as REST or gRPC.\nAs illustrated in Figure 10.3, the main advantage of this approach is the ability to scale each com-\nponent independently. For instance, since the LLM service might require more GPU resources \nthan the business logic, it can be scaled horizontally without impacting the other components. \nThis optimizes resource usage and reduces costs, as different types of machines (e.g., GPU versus \nCPU) can be used according to each service’s needs.\nFor example, let’s assume that the LLM inference takes longer, so you will need more ML service \nreplicas to meet the demand. But remember that GPU VMs are expensive. By decoupling the two \ncomponents, you will run only what is required on the GPU machine and not block the GPU VM \nwith other computing that can be done on a much cheaper machine. \n",
      "content_length": 2480,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 395,
      "content": "Inference Pipeline Deployment\n366\nThus, by decoupling the components, you can scale horizontally as required, with minimal costs, \nproviding a cost-effective solution to your system’s needs.\nFigure 10.3: Scaling microservices independently based on compute requirements\nAdditionally, each microservice can adopt the most suitable technology stack, allowing teams to \ninnovate and optimize independently.\nHowever, microservices introduce complexity in deployment and maintenance. Each service \nmust be deployed, monitored, and maintained separately, which can be more challenging than \nmanaging a monolithic system. \n",
      "content_length": 616,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 396,
      "content": "Chapter 10\n367\nThe increased network communication between services can also introduce latency and potential \npoints of failure, necessitating robust monitoring and resilience mechanisms.\nNote that the proposed design for decoupling the ML model and business logic into two services \ncan be extended if necessary. For example, you can have one service for preprocessing the data, \none for the model, and another for post-processing the data. Depending on the four pillars (la-\ntency, throughput, data, and infrastructure), you can get creative and design the most optimal \narchitecture for your application needs.\nChoosing between monolithic and microservices \narchitectures\nThe choice between monolithic and microservices architectures for serving ML models largely \ndepends on the application’s specific needs. A monolithic approach might be ideal for smaller \nteams or more straightforward applications where ease of development and maintenance is a \npriority. It’s also a good starting point for projects without frequent scaling requirements. Also, \nif the ML models are smaller, don’t require a GPU, or don’t require smaller and cheaper GPUs, \nthe trade-off between reducing costs and complicating your infrastructure is worth considering.\nOn the other hand, microservices, with their adaptability and scalability, are well suited for larger, \nmore complex systems where different components have varying scaling needs or require distinct \ntech stacks. This architecture is particularly advantageous when scaling specific system parts, such \nas GPU-intensive LLM services. As LLMs require powerful machines with GPUs, such as Nvidia \nA100, V100, or A10g, which are incredibly costly, microservices offer the flexibility to optimize \nthe system for keeping these machines busy all the time or quickly scaling down when the GPU \nis idle. However, this flexibility comes at the cost of increased complexity in both development \nand operations.\nA common strategy is to start with a monolithic design and further decouple it into multiple \nservices as the project grows. However, to successfully do so without making the transition too \ncomplex and costly, you must design the monolith application with this in mind. For instance, \neven if all the code runs on a single machine, you can completely decouple the modules of the \napplication at the software level. This makes it easier to move these modules to different micro-\nservices when the time comes. When working with Python, for example, you can implement the \nML and business logic into two different Python modules that don’t interact with each other. Then, \nyou can glue these two modules at a higher level, such as through a service class, or directly into \nthe framework you use to expose your application over the internet, such as FastAPI. \n",
      "content_length": 2805,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 397,
      "content": "Inference Pipeline Deployment\n368\nAnother option is to write the ML and business logic as two different Python packages that you \nglue together in the same ways as before. This is better because it completely enforces a separation \nbetween the two but adds extra complexity at development time. The main idea, therefore, is that \nif you start with a monolith and down the line you want to move to a microservices architecture, \nit’s essential to design your software with modularity in mind. Otherwise, if the logic is mixed, \nyou will probably have to rewrite everything from scratch, adding tons of development time, \nwhich translates into wasted resources.\nIn summary, monolithic architectures offer simplicity and ease of maintenance but at the cost of \nflexibility and scalability. At the same time, microservices provide the agility to scale and innovate \nbut require more sophisticated management and operational practices.\nExploring the LLM Twin’s inference pipeline \ndeployment strategy\nNow that we’ve understood all the design choices available for implementing the deployment \nstrategy of the LLM Twin’s inference pipeline, let’s explore the concrete decisions we made to \nactualize it.\nOur primary objective is to develop a chatbot that facilitates content creation. To achieve this, \nwe will process requests sequentially, with a strong emphasis on low latency. This necessitates \nthe selection of an online real-time inference deployment architecture.\nOn the monolith versus microservice aspect, we will split the ML service between a REST API \nserver containing the business logic and an LLM microservice optimized for running the given \nLLM. As the LLM requires a powerful machine to run the inference, and we can further optimize \nit with various engines to speed up the latency and memory usage, it makes the most sense to go \nwith the microservice architecture. By doing so, we can quickly adapt the infrastructure based on \nvarious LLM sizes. For example, if we run an 8B parameter model, the model can run on a single \nmachine with a Nivida A10G GPU after quantization. But if we want to run a 30B model, we can \nupgrade to an Nvidia A100 GPU. Doing so allows us to upgrade only the LLM microservice while \nkeeping the REST API untouched.\nAs illustrated in Figure 10.4, most business logic is centered around RAG in our particular use case. \nThus, we will perform RAG’s retrieval and augmentation parts within the business microservice. \nIt will also include all the advanced RAG techniques presented in the previous chapter to optimize \nthe pre-retrieval, retrieval, and post-retrieval steps. \n",
      "content_length": 2616,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 398,
      "content": "Chapter 10\n369\nThe LLM microservice is strictly optimized for the RAG generation component. Ultimately, the \nbusiness layer will send the prompt trace consisting of the user query, prompt, answer, and other \nintermediary steps to the prompt monitoring pipeline, which we will detail in Chapter 11.\nIn summary, our approach involves implementing an online real-time ML service using a micro-\nservice architecture, which effectively splits the LLM and business logic into two distinct services.\nFigure 10.4: Microservice deployment architecture of the LLM Twin’s inference pipeline\n",
      "content_length": 580,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 399,
      "content": "Inference Pipeline Deployment\n370\nLet’s review the interface of the inference pipeline, which is defined by the feature/training/\ninference (FTI) architecture. For the pipeline to run, it needs two things:\n• \nReal-time features used for RAG, generated by the feature pipeline, which is queried from \nour online feature store, more concretely from the Qdrant vector database (DB)\n• \nA fine-tuned LLM generated by the training pipeline, which is pulled from our model \nregistry\nWith that in mind, the flow of the ML service looks as follows, as illustrated in Figure 10.4:\n1. \nA user sends a query through an HTTP request.\n2. \nThe user’s input retrieves the proper context by leveraging the advanced RAG retrieval \nmodule implemented in Chapter 4.\n3.\nThe user’s input and retrieved context are packed into the final prompt using a dedicated \nprompt template.\n4.\nThe prompt is sent to the LLM microservice through an HTTP request.\n5.\nThe business microservices wait for the generated answer.\n6.\nAfter the answer is generated, it is sent to the prompt monitoring pipeline along with the \nuser’s input and other vital information to monitor.\n7.\nUltimately, the generated answer is sent back to the user.\nNow, let’s explore what tech stack we used to implement the architecture presented in Figure \n10.4. As we know, we use Qdrant for the vector DB. We will leverage Hugging Face for the model \nregistry. By doing so, we can publicly share our model with everyone who is testing the code from \nthis book. Thus, you can easily use the model we provided if you don’t want to run the training \npipeline, which can cost up to 100 dollars. As you can see, shareability and accessibility are some \nof the most beautiful aspects of storing your model in a model registry.\nWe will implement the business microservice in FastAPI because it’s popular, easy to use, and fast. \nThe LLM microservice will be deployed on AWS SageMaker, where we will leverage SageMaker’s \nintegration with Hugging Face’s Deep Learning Containers (DLCs) to deploy the model. We will \ndiscuss Hugging Face’s DLCs in the next section, but intuitively, it is an inference engine used to \noptimize LLMs at serving time. The prompt monitoring pipeline is implemented using Comet, \nbut we will look over that module only in Chapter 11.\n",
      "content_length": 2292,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 400,
      "content": "Chapter 10\n371\nThe SageMaker Inference deployment is composed of the following components that we will \nshow you how to implement:\n•\nSageMaker endpoint: An endpoint is a scalable and secure API that SageMaker hosts to \nenable real-time predictions from deployed models. It’s essentially the interface through \nwhich applications interact with your model. Once deployed, an application can make \nHTTP requests to the endpoint to receive real-time predictions.\n•\nSageMaker model: In SageMaker, a model is an artifact that results from training an al-\ngorithm. It contains the information required to make predictions, including the weights \nand computation logic. You can create multiple models and use them in different config-\nurations or for various predictions.\n•\nSageMaker configuration: This configuration specifies the hardware and software set \nup to host the model. It defines the resources required for the endpoint, such as the type \nand number of ML compute instances. Endpoint configurations are used when creating \nor updating an endpoint. They allow for flexibility in the deployment and scalability of \nthe hosted models.\n•\nSageMaker Inference component: This is the last piece of the puzzle that connects the \nmodel and configuration to an endpoint. You can deploy multiple models to an endpoint, \neach with its resource configuration. Once deployed, models are easily accessible via the \nInvokeEndpoint API in Python.\nTogether, these components create a robust infrastructure for deploying and managing ML models \nin SageMaker, enabling scalable, secure, and efficient real-time predictions.\nOther popular cloud platforms offer the exact solutions. For example, you have Azure OpenAI \ninstead of Bedrock and Azure ML instead of SageMaker on Azure. The list of ML deployment \ntools, such as Hopsworks, Modal, Vertex AI, Seldon, BentoML, and many more, is endless and \nwill probably change. What is essential though is to understand your use case requirements and \nfind a tool that fits your needs.\nThe training versus the inference pipeline\nUnderstanding the nuances between the training and inference pipelines is crucial before we \ndeploy the inference pipeline. While it might seem straightforward that the training pipeline is \nfor training and the inference pipeline is for inference, there are significant differences that we \nneed to grasp to comprehend the technical aspects of our discussion fully.\n",
      "content_length": 2423,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 401,
      "content": "Inference Pipeline Deployment\n372\nOne key difference lies in how data is handled and accessed within each pipeline. During train-\ning, data is typically accessed from offline storage in batch mode, optimized for throughput and \nensuring data lineage. For example, our LLM Twin architecture uses ZenML artifacts to access, \nversion, and track data fed to the training loop in batches. In contrast, the inference pipeline \nrequires an online DB optimized for low latency. We will leverage the Qdrant vector DB to grab \nthe necessary context for RAG. In this context, the focus shifts from data lineage and versioning \nto quick data access, ensuring a seamless user experience. Additionally, the outputs of these pipe-\nlines also differ significantly. The training pipeline outputs trained model weights stored in the \nmodel registry. Meanwhile, the inference pipeline outputs predictions served directly to the user.\nAlso, the infrastructure required for each pipeline is different. The training pipeline demands \nmore powerful machines equipped with as many GPUs as possible. This is because training in-\nvolves batching data and holding all the necessary gradients in memory for optimization steps, \nmaking it highly compute-intensive. More computational power and VRAM allow larger batches \n(or throughput), reducing training time and enabling more extensive experimentation. On the \nother hand, the inference pipeline typically requires less computation. Inference often involves \npassing a single sample or smaller batches to the model without the need for optimization steps.\nDespite these differences, there is some overlap between the two pipelines, particularly regarding \npreprocessing and post-processing steps. Applying the same preprocessing and post-processing \nfunctions and hyperparameters during training and inference is crucial. Any discrepancies can \nlead to what is known as training-serving skew, where the model’s performance during inference \ndeviates from its performance during training.\nDeploying the LLM Twin service\nThe last step is implementing the architecture presented in the previous section. More concretely, \nwe will deploy the LLM microservice using AWS SageMaker and the business microservice using \nFastAPI. Within the business microservice, we will glue the RAG logic written in Chapter 9 with \nour fine-tuned LLM Twin, ultimately being able to test out the inference pipeline end to end.\nServing the ML model is one of the most critical steps in any ML application’s life cycle, as users \ncan only interact with our model after this phase is completed. If the serving architecture isn’t \ndesigned correctly or if the infrastructure isn’t working properly, it doesn’t matter that you have \nimplemented a powerful and excellent model. As long as the user cannot appropriately interact \nwith it, it has near zero value from a business point of view. For example, if you have the best code \nassistant on the market, but the latency to use it is too high, or the API calls keep crashing, the \nuser will probably switch to a less performant code assistant that works faster and is more stable.\n",
      "content_length": 3127,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 402,
      "content": "Chapter 10\n373\nThus, in this section, we will show you how to:\n• \nDeploy our fined-tuned LLM Twin model to AWS SageMaker\n• \nWrite an inference client to interact with the deployed model\n• \nWrite the business service in FastAPI\n• \nIntegrate our RAG logic with our fine-tuned LLM\n• \nImplement autoscaling rules for the LLM microservice\nImplementing the LLM microservice using AWS SageMaker\nWe aim to deploy the LLM Twin model, stored in Hugging Face’s model registry, to Amazon \nSageMaker as an online real-time inference endpoint. We will leverage Hugging Face’s specialized \ninference container, known as the Hugging Face LLM DLC, to deploy our LLM.\nWhat are Hugging Face’s DLCs?\nDLCs are specialized Docker images that come pre-loaded with essential deep-learning frame-\nworks and libraries, including popular tools like transformers, datasets, and tokenizers from \nHugging Face. These containers are designed to simplify the process of training and deploying \nmodels by eliminating the need for complex environment setup and optimization. The Hugging \nFace Inference DLC, in particular, includes a fully integrated serving stack, significantly simpli-\nfying the deployment process and reducing the technical expertise needed to serve deep learning \nmodels in production.\nWhen it comes to serving models, the DLC is powered by the Text Generation Inference (TGI) \nengine, made by Hugging Face: https://github.com/huggingface/text-generation-inference.\nTGI is an open-source solution for deploying and serving LLMs. It offers high-performance text \ngeneration using tensor parallelism and dynamic batching for the most popular open-source LLMs \navailable on Hugging Face, such as Mistral, Llama, and Falcon. To sum up, the most powerful \nfeatures the DLC image provides are:\n•\nTensor parallelism, thus enhancing the computational efficiency of model inference\n•\nOptimized transformers code for inference, leveraging flash-attention to maximize per-\nformance across the most widely used architectures: https://github.com/Dao-AILab/\nflash-attention\n•\nQuantization with bitsandbytes that reduces the model size while maintaining per-\nformance, making deployments more efficient: https://github.com/bitsandbytes-\nfoundation/bitsandbytes\n",
      "content_length": 2233,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 403,
      "content": "Inference Pipeline Deployment\n374\n•\nContinuous batching of incoming requests, thus improving throughput by dynamically \nbatching requests as they arrive\n•\nAccelerated weight loading by utilizing safetensors for faster model initialization, re-\nducing start-up time: https://github.com/huggingface/safetensors\n•\nToken streaming that supports real-time interactions through Server-Sent Events (SSE)\nTo summarize, our LLM Twin model will run inside DLC Docker images, listening to requests, \noptimizing the LLM for inference, and serving the results in real time. The DLC’s Docker images \nwill be hosted on AWS SageMaker under inference endpoints that can be accessed through HTTP \nrequests. With that in mind, let’s move on to the implementation. We will start by deploying the \nLLM and then writing a wrapper class to interact with the SageMaker Inference endpoint.\nConfiguring SageMaker roles\nThe first step is to create the proper AWS Identity and Access Management (IAM) users and \nroles to access and deploy the SageMaker infrastructure. AWS IAM controls who can authenticate \nand what any actor has access to. You can create new users (assigned to people) and new roles \n(assigned to other actors within your infrastructure, such as EC2 VMs) through IAM.\nThe whole deployment process is automated. We will have to run a few CLI commands, but first, \nensure that you have correctly configured the AWS_ACCESS_KEY, AWS_SECRET_KEY, and AWS_REGION\nenvironmental variables in the .env file. At this step, the easiest way is to use the credentials \nattached to an admin role as, in the following steps, we will create a set of narrower IAM roles \nused in the rest of the chapter.\nAfter you configured your .env file, we have to:\n1. \nCreate an IAM user restricted to creating and deleting only the resources we need for \nthe deployment, such as SageMaker itself, Elastic Container Registry (ECR), and S3. To \nmake it, run the following:\npoetry poe create-sagemaker-role\nThis command will generate a JSON file called sagemaker_user_credentials.json that \ncontains a new AWS access and secret key. From now on, we will use these credentials to \ndeploy everything related to SageMaker to ensure we modify only the resources associated \nwith SageMaker. Otherwise, we could accidentally modify other AWS resources using \nan admin account, resulting in additional costs or altering other existing projects. Thus, \nhaving a narrow role only to your use case is good practice. \n",
      "content_length": 2466,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 404,
      "content": "Chapter 10\n375\nThe last step is to take the new credentials from the JSON file and update the AWS_ACCESS_\nKEY and AWS_SECRET_KEY variables in your .env file. You can check out the implementa-\ntion at https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/\nllm_engineering/infrastructure/aws/roles/create_sagemaker_role.py.\n2.\nCreate an IAM execution role. We will attach this role to the SageMaker deployment, \nempowering it to access other AWS resources on our behalf. This is standard practice for \ncloud deployments, as instead of authenticating every machine within your credentials, \nyou attach a role that allows them to access only what is necessary from your infrastruc-\nture. In our case, we will provide SageMaker access to AWS S3, CloudWatch, and ECR. To \ncreate the role, run the following:\npoetry poe create-sagemaker-execution-role\nThis command will generate a JSON file called sagemaker_execution_role.json that \ncontains the Amazon Resource Name (ARN) of the newly created role. The ARN is an ID \nattached to any AWS resource to identify it across your cloud infrastructure. Take the ARN \nvalue from the JSON file and update the AWS_ARN_ROLE variable from your .env file with \nit. You can check out the implementation at https://github.com/PacktPublishing/\nLLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/\ncreate_execution_role.py.\nBy setting the IAM user and role in your .env file, we will automatically load them in the settings \nPython object and use them throughout the following steps. Now, let’s move on to the actual \ndeployment.\nDeploying the LLM Twin model to AWS SageMaker\nThe deployment of AWS SageMaker is fully automated through a set of Python classes, which \nwe will cover in this chapter. This section aims to understand how we configure the SageMaker \ninfrastructure directly from Python. Thus, you don’t have to run everything step by step, as in a \nstandard tutorial, but only to understand the code.\nWe can initiate and finalize the entire SageMaker deployment using a simple CLI command: poe \ndeploy-inference-endpoint. This command will initialize all the steps presented in Figure 10.5, \nexcept for creating the SageMaker AWS IAMs we created and configured in the previous step. \nIf you have issues, configure the AWS CLI with the same AWS credentials as in the \n.env file and repeat the process. Official documentation for installing the AWS CLI: \nhttps://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html.\n",
      "content_length": 2497,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 405,
      "content": "Inference Pipeline Deployment\n376\nIn this section, we will walk you through the code presented in Figure 10.5 that helps us fully auto-\nmate the deployment process, starting with the create_endpoint() function. Ultimately, we will \ntest the CLI command and check the AWS console to see whether the deployment was successful. \nThe SageMaker deployment code is available at https://github.com/PacktPublishing/LLM-\nEngineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy.\nFigure 10.5: AWS SageMaker deployment steps\n",
      "content_length": 528,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 406,
      "content": "Chapter 10\n377\nWe will take a top-down approach to walk you through the implementation, starting with the \nmain function that deploys the LLM Twin model to AWS SageMaker. In the function below, we \nfirst take the latest version of the Docker DLC image using the get_huggingface_llm_image_uri()\nfunction, which is later passed to the deployment strategy class, along with an instance of the \nresource manager and deployment service:\ndef create_endpoint(endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED):\n    llm_image = get_huggingface_llm_image_uri(\"huggingface\", version=None)\n    resource_manager = ResourceManager()\n    deployment_service = DeploymentService(resource_manager=resource_\nmanager)\n    SagemakerHuggingfaceStrategy(deployment_service).deploy(\n        role_arn=settings.ARN_ROLE,\n        llm_image=llm_image,\n        config=hugging_face_deploy_config,\n        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE,\n        endpoint_config_name=settings.SAGEMAKER_ENDPOINT_CONFIG_INFERENCE,\n        gpu_instance_type=settings.GPU_INSTANCE_TYPE,\n        resources=model_resource_config,\n        endpoint_type=endpoint_type,\n    )\nWe must review the three classes used in the create_endpoint() function to fully understand \nthe deployment process. Let’s start with the ResourceManager class. The class begins with the \ninitialization method, establishing the connection to AWS SageMaker using boto3, the AWS \nSDK for Python, which provides the necessary functions to interact with various AWS services, \nincluding SageMaker.\nclass ResourceManager:\n    def __init__(self) -> None:\n        self.sagemaker_client = boto3.client(\n            \"sagemaker\",\n            region_name=settings.AWS_REGION,\n            aws_access_key_id=settings.AWS_ACCESS_KEY,\n            aws_secret_access_key=settings.AWS_SECRET_KEY,\n        )\n",
      "content_length": 1834,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 407,
      "content": "Inference Pipeline Deployment\n378\nNext, we implement the endpoint_config_exists method, checking whether a specific Sage-\nMaker endpoint configuration exists:\n    def endpoint_config_exists(self, endpoint_config_name: str) -> bool:\n        try:\n            self.sagemaker_client.describe_endpoint_\nconfig(EndpointConfigName=endpoint_config_name)\n            logger.info(f\"Endpoint configuration '{endpoint_config_name}' \nexists.\")\n            return True\n        except ClientError:\n            logger.info(f\"Endpoint configuration '{endpoint_config_name}' \ndoes not exist.\")\n            return False\nThe class also includes the endpoint_exists method, which checks the existence of a specific \nSageMaker endpoint:\ndef endpoint_exists(self, endpoint_name: str) -> bool:\n        try:\n            self.sagemaker_client.describe_endpoint(EndpointName=endpoint_\nname)\n            logger.info(f\"Endpoint '{endpoint_name}' exists.\")\n            return True\n        except self.sagemaker_client.exceptions.ResourceNotFoundException:\n            logger.info(f\"Endpoint '{endpoint_name}' does not exist.\")\n            return False\nLet’s move to the DeploymentService. Within the constructor, we set up the sagemaker_client, \nwhich will interface with AWS SageMaker and an instance of the ResourceManager class we \ntalked about earlier:\nclass DeploymentService:\n    def __init__(self, resource_manager):\n        self.sagemaker_client = boto3.client(\n            \"sagemaker\",\n            region_name=settings.AWS_REGION,\n            aws_access_key_id=settings.AWS_ACCESS_KEY,\n            aws_secret_access_key=settings.AWS_SECRET_KEY,\n",
      "content_length": 1624,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 408,
      "content": "Chapter 10\n379\n        )\n        self.resource_manager = resource_manager\nThe deploy() method is the heart of the DeploymentService class. This method orchestrates the \nentire process of deploying a model to a SageMaker endpoint. It checks whether the necessary \nconfigurations are already in place and, if not, it triggers the deployment:\ndef deploy(\n    self,\n    role_arn: str,\n    llm_image: str,\n    config: dict,\n    endpoint_name: str,\n    endpoint_config_name: str,\n    gpu_instance_type: str,\n    resources: Optional[dict] = None,\n    endpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n) -> None:\n    try:\n        if self.resource_manager.endpoint_config_exists(endpoint_config_\nname=endpoint_config_name):\n            logger.info(f\"Endpoint configuration {endpoint_config_name} \nexists. Using existing configuration...\")\n        else:\n            logger.info(f\"Endpoint configuration{endpoint_config_name} \ndoes not exist.\")\n        self.prepare_and_deploy_model(\n            role_arn=role_arn,\n            llm_image=llm_image,\n            config=config,\n            endpoint_name=endpoint_name,\n            update_endpoint=False,\n            resources=resources,\n            endpoint_type=endpoint_type,\n            gpu_instance_type=gpu_instance_type,\n        )\n",
      "content_length": 1276,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 409,
      "content": "Inference Pipeline Deployment\n380\n        logger.info(f\"Successfully deployed/updated model to endpoint \n{endpoint_name}.\")\n    except Exception as e:\n        logger.error(f\"Failed to deploy model to SageMaker: {e}\")\n        raise\nThe deploy method begins by checking whether the endpoint configuration already exists using \nthe resource_manager. This step is crucial because it avoids unnecessary redeployment if the \nconfiguration is already set up. The deployment itself is handled by calling the prepare_and_\ndeploy_model() method, which is responsible for the actual deployment of the model to the \nspecified SageMaker endpoint.\nThe prepare_and_deploy_model() method is a static method within the DeploymentService\nclass. This method is focused on setting up and deploying the Hugging Face model to SageMaker:\n@staticmethod\ndef prepare_and_deploy_model(\n    role_arn: str,\n    llm_image: str,\n    config: dict,\n    endpoint_name: str,\n    update_endpoint: bool,\n    gpu_instance_type: str,\n    resources: Optional[dict] = None,\n    endpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n) -> None:\n    huggingface_model = HuggingFaceModel(\n        role=role_arn,\n        image_uri=llm_image,\n        env=config,\n        transformers_version=\"4.6\",\n        pytorch_version=\"1.13\",\n        py_version=\"py310\",\n    )\n    huggingface_model.deploy(\n        instance_type=gpu_instance_type,\n        initial_instance_count=1,\n",
      "content_length": 1422,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 410,
      "content": "Chapter 10\n381\n        endpoint_name=endpoint_name,\n        update_endpoint=update_endpoint,\n        resources=resources,\n        tags=[{\"Key\": \"task\", \"Value\": \"model_task\"}],\n        endpoint_type=endpoint_type,\n    )\nThis method begins by creating an instance of HuggingFaceModel, a specialized model class from \nSageMaker designed to handle Hugging Face models. The constructor for HuggingFaceModel \ntakes several essential parameters, such as the role ARN (which gives SageMaker the necessary \npermissions), the URI of the LLM DLC Docker image, and the LLM configuration that specifies \nwhat LLM to load from Hugging Face and its inference parameters, such as the maximum total \nof tokens.\nOnce HuggingFaceModel is instantiated, the method deploys it to SageMaker using the deploy \nfunction. This deployment process involves specifying the type of instance used, the number of \ninstances, and whether to update an existing endpoint or create a new one. The method also in-\ncludes optional resources for more complex deployments, such as the initial_instance_count\nparameter for multi-model endpoints and tags for tracking and categorization.\nThe last step is to walk you through the SagemakerHuggingfaceStrategy class, which aggregates \neverything we have shown. The class is initialized only with an instance of a deployment service, \nsuch as the one shown above.\nclass SagemakerHuggingfaceStrategy(DeploymentStrategy):\ndef __init__(self, deployment_service):\n    self.deployment_service = deployment_service\nThe core functionality of the SagemakerHuggingfaceStrategy class is encapsulated in its deploy()\nmethod. This method orchestrates the deployment process, taking various parameters that define \nhow the Hugging Face model should be deployed to AWS SageMaker:\ndef deploy(\n    self,\n    role_arn: str,\n    llm_image: str,\n    config: dict,\n    endpoint_name: str,\n    endpoint_config_name: str,\n    gpu_instance_type: str,\n",
      "content_length": 1934,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 411,
      "content": "Inference Pipeline Deployment\n382\n    resources: Optional[dict] = None,\n    endpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n) -> None:\n    logger.info(\"Starting deployment using Sagemaker Huggingface \nStrategy...\")\n    logger.info(\n        f\"Deployment parameters: nb of replicas: {settings.COPIES}, nb of \ngpus:{settings.GPUS}, instance_type:{settings.GPU_INSTANCE_TYPE}\"\n    )\nThe parameters passed into the method are crucial to the deployment process:\n• \nrole_arn: The AWS IAM role that provides permissions for the SageMaker deployment.\n• \nllm_image: The URI of the DLC Docker image\n• \nconfig: A dictionary containing configuration settings for the model environment.\n•\nendpoint_name and endpoint_config_name: Names for the SageMaker endpoint and its \nconfiguration, respectively.\n•\ngpu_instance_type: The type of the GPU EC2 instances used for the deployment.\n•\nresources: Optional resources dictionary used for multi-model endpoint deployments.\n•\nendpoint_type: This can either be MODEL_BASED or INFERENCE_COMPONENT, determining \nwhether the endpoint includes an inference component.\nThe method delegates the actual deployment process to the deployment_service. This delega-\ntion is a critical aspect of the strategy pattern, allowing for flexibility in how the deployment is \ncarried out without altering the high-level deployment logic.\ntry:\n    self.deployment_service.deploy(\n        role_arn=role_arn,\n        llm_image=llm_image,\n        config=config,\n        endpoint_name=endpoint_name,\n        endpoint_config_name=endpoint_config_name,\n        gpu_instance_type=gpu_instance_type,\n        resources=resources,\n        endpoint_type=endpoint_type,\n    )\n    logger.info(\"Deployment completed successfully.\")\n",
      "content_length": 1730,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 412,
      "content": "Chapter 10\n383\nexcept Exception as e:\n    logger.error(f\"Error during deployment: {e}\")\n    raise\nAlso, let’s review the resource configuration to understand the infrastructure better. These re-\nsources are leveraged when setting up multi-endpoint configurations that use multiple replicas \nto serve clients while respecting the latency and throughput requirements of the application. \nThe ResourceRequirements object is initialized with a dictionary that specifies various resource\nparameters. These parameters include the number of replicas (copies) of the model to be de-\nployed, the number of GPUs required, the number of CPU cores, and the memory allocation in \nmegabytes. Each of these parameters plays a crucial role in the performance and scalability of \nthe deployed model.\nfrom sagemaker.compute_resource_requirements.resource_requirements import \nResourceRequirements\n    model_resource_config = ResourceRequirements(\n    requests={\n        \"copies\": settings.COPIES,\n        \"num_accelerators\": settings.GPUS\n        \"num_cpus\": settings.CPUS,\n        \"memory\": 5 * 1024\n    },\n)\nIn the preceding snippet, ResourceRequirements is configured with four key parameters:\n•\ncopies: This parameter determines how many instances or replicas of the model should be \ndeployed. Having multiple replicas can help in reducing latency and increasing throughput.\n•\nnum_accelerators: This parameter specifies the number of GPUs to allocate. Since LLMs \nare computationally intensive, multiple GPUs are typically required to accelerate infer-\nence processes.\n•\nnum_cpus: This defines the number of CPU cores the deployment should have. The num-\nber of CPUs impacts the model’s ability to handle data preprocessing, post-processing, \nand other tasks that are less GPU-dependent but still essential.\n•\nmemory: The memory parameter sets the minimum amount of RAM required for the \ndeployment. Adequate memory is necessary to ensure the model can load and operate \nwithout running into memory shortages.\n",
      "content_length": 1996,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 413,
      "content": "Inference Pipeline Deployment\n384\nBy setting these parameters, the class ensures that it has sufficient resources to operate efficiently \nwhen the model is deployed to a SageMaker endpoint. The precise tuning of these values will \nvary depending on the LLM’s specific requirements, such as its size, the complexity of the tasks \nit will perform, and the expected load. To get a better understanding of how to use them, after \ndeploying the endpoint, we suggest modifying them and seeing how the performance of the LLM \nmicroservice changes.\nUltimately, let’s review the settings configuring the LLM engine. The HF_MODEL_ID identifies \nwhich Hugging Face model to deploy. For example, in the settings class, we set it to mlabonne/\nTwinLlama-3.1-8B-13 to load our custom LLM Twin model stored in Hugging Face. SM_NUM_GPUS\nspecifies the number of GPUs allocated per model replica, which is crucial for fitting your model \ninto the GPU’s VRAM. HUGGING_FACE_HUB_TOKEN provides access to the Hugging Face Hub for \nmodel retrieval. HF_MODEL_QUANTIZE specifies what quantization technique to use, while the rest \nof the variables control the LLM token generation process.\nhugging_face_deploy_config = {\n    \"HF_MODEL_ID\": settings.HF_MODEL_ID,\n    \"SM_NUM_GPUS\": json.dumps(settings.SM_NUM_GPUS),  # Number of GPU used \nper replica\n    \"MAX_INPUT_LENGTH\": json.dumps(settings.MAX_INPUT_LENGTH),  # Max \nlength of input text\n    \"MAX_TOTAL_TOKENS\": json.dumps(settings.MAX_TOTAL_TOKENS),  # Max \nlength of the generation (including input text)\n    \"MAX_BATCH_TOTAL_TOKENS\": json.dumps(settings.MAX_BATCH_TOTAL_TOKENS),\n    \"HUGGING_FACE_HUB_TOKEN\": settings.HUGGINGFACE_ACCESS_TOKEN,\n    \"MAX_BATCH_PREFILL_TOKENS\": \"10000\",\n    \"HF_MODEL_QUANTIZE\": \"bitsandbytes\",\n}\nUsing these two configurations, we fully control our infrastructure, what LLM to use, and how \nit behaves. To start the SageMaker deployment with the configuration shown above, call the \ncreate_endpoint() function (presented at the beginning of the section) as follows:\ncreate_endpoint(endpoint_type=EndpointType.MODEL_BASED)\nFor convenience, we also wrapped it up under a poe command:\npoetry poe deploy-inference-endpoint\n",
      "content_length": 2182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 414,
      "content": "Chapter 10\n385\nThat’s all you need to deploy an inference pipeline to AWS SageMaker. The hardest part is finding \nthe correct configuration to fit your needs while reducing your infrastructure’s costs. Depending \non AWS, this will take up to 15-30 minutes to deploy. You can always change any value directly \nfrom your .env file and deploy the model with a different configuration without touching the \ncode. For example, our default values use a single GPU instance of type ml.g5.xlargeGPU. If you \nwant more replicas, you can tweak the GPUS and SM_NUM_GPUS settings or change your instance \ntype by changing the GPU_INSTANCE_TYPE variable.\nAfter deploying the AWS SageMaker Inference endpoint, you can navigate to the SageMaker \ndashboard in AWS to visualize it. First, in the left panel, click on SageMaker dashboard, and then \nin the Inference column, click on the Endpoints button, as illustrated in Figure 10.6.\nFigure 10.6: AWS SageMaker Inference endpoints example\nAfter clicking the Endpoints button, you will see your twin endpoint in a Creating or Created \nstatus, as seen in Figure 10.7. After clicking on it, you can look at the endpoint’s logs in CloudWatch \nand monitor the CPU, memory, disk, and GPU utilization. \nBefore deploying the LLM microservice to AWS SageMaker, ensure that you’ve gen-\nerated a user role by running poetry poe create-sagemaker-role and an exe-\ncution role by running poetry poe create-sagemaker-execution-role. Also, \nensure you update your AWS_* environment variables in your .env file with the \ncredentials generated by the two steps. You can find more details on this aspect in \nthe repository’s README file.\n",
      "content_length": 1653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 415,
      "content": "Inference Pipeline Deployment\n386\nAlso, they provide an excellent way to monitor all the HTTP errors, such as 4XX and 5XX, in one place.\nFigure 10.7: AWS SageMaker twin inference endpoint example\nCalling the AWS SageMaker Inference endpoint\nNow that our LLM service has been deployed on AWS SageMaker, let’s learn how to call the service. \nTo do so, we will write two classes that will help us prepare the prompt for SageMaker, call the \ninference endpoint through HTTP requests, and decode the results in a way the client can work \nwith. All the AWS SageMaker Inference code is available on GitHub at llm_engineering/model/\ninference. It all starts with the following example:\ntext = \"Write me a post about AWS SageMaker inference endpoints.\"\nllm = LLMInferenceSagemakerEndpoint(\n        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE\n    )\nAnswer = InferenceExecutor(llm, text).execute()\nAs before, we will walk you through the LLMInferenceSagemakerEndpoint and InferenceExecutor\nclasses. Let’s start with the LLMInferenceSagemakerEndpoint class, which directly interacts with \nSageMaker. The constructor initializes all the essential attributes necessary to interact with the \nSageMaker endpoint:\nclass LLMInferenceSagemakerEndpoint(Inference):\n    def __init__(\n        self,\n       endpoint_name: str,\n       default_payload: Optional[Dict[str, Any]] = None,\n        inference_component_name: Optional[str] = None,\n    ) -> None:\n       super().__init__()\n        self.client = boto3.client(\n            \"sagemaker-runtime\",\n            region_name=settings.AWS_REGION,\n",
      "content_length": 1580,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 416,
      "content": "Chapter 10\n387\n            aws_access_key_id=settings.AWS_ACCESS_KEY,\n            aws_secret_access_key=settings.AWS_SECRET_KEY,\n        )\n        self.endpoint_name = endpoint_name\n        self.payload = default_payload if default_payload else self._\ndefault_payload()\n       self.inference_component_name = inference_component_name\nendpoint_name is crucial for identifying the SageMaker endpoint we want to request. Additionally, \nthe method initializes the payload using a provided value or by calling a method that generates \na default payload if none is provided.\nOne of the key features of the class is its ability to generate a default payload for inference requests. \nThis is handled by the _default_payload() method:\ndef _default_payload(self) -> Dict[str, Any]:\n    return {\n        \"inputs\": \"\",\n        \"parameters\": {\n            \"max_new_tokens\": settings.MAX_NEW_TOKENS_INFERENCE,\n            \"top_p\": settings.TOP_P_INFERENCE,\n            \"temperature\": settings.TEMPERATURE_INFERENCE,\n            \"return_full_text\": False,\n        },\n    }\nThis method returns a dictionary that represents the default structure of the payload to be sent \nfor inference. The parameters section includes settings that influence the model’s behavior during \ninference, such as the number of tokens to generate, the sampling strategy (top_p), and the tem-\nperature setting, which controls randomness in the output. These parameters are fetched from \nthe application’s settings, ensuring consistency across different inference tasks.\nThe class allows customization of the payload through the set_payload() method, which enables \nthe user to modify the inputs and parameters before sending an inference request:\ndef set_payload(self, inputs: str, parameters: Optional[Dict[str, Any]] = \nNone) -> None:\n    self.payload[\"inputs\"] = inputs\n    if parameters:\n        self.payload[\"parameters\"].update(parameters)\n",
      "content_length": 1906,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 417,
      "content": "Inference Pipeline Deployment\n388\nThis method updates the inputs field of the payload with the new input text provided by the user. \nAdditionally, it allows for modifying inference parameters if any are provided.\nUltimately, we leverage the inference() method to call the SageMaker endpoint with the cus-\ntomized payload:\ndef inference(self) -> Dict[str, Any]:\n    try:\n        logger.info(\"Inference request sent.\")\n        invoke_args = {\n            \"EndpointName\": self.endpoint_name,\n            \"ContentType\": \"application/json\",\n            \"Body\": json.dumps(self.payload),\n        }\n        if self.inference_component_name not in [\"None\", None]:\n            invoke_args[\"InferenceComponentName\"] = self.inference_\ncomponent_name\n        response = self.client.invoke_endpoint(**invoke_args)\n        response_body = response[\"Body\"].read().decode(\"utf8\")\n        return json.loads(response_body)\n    except Exception:\n        logger.exception(\"SageMaker inference failed.\")\n        raise\nIn this method, the inference method constructs the request to be sent to the SageMaker endpoint. \nThe method packages the payload and other necessary details into a format SageMaker expects. If \nan inference_component_name is specified, it is included in the request, allowing for more granular \ncontrol over the inference process if needed. The request is sent using the invoke_endpoint()\nfunction, and the response is read, decoded, and returned as a JSON object.\nLet’s understand how the InferenceExecutor uses the LLMInferenceSagemakerEndpoint class \nwe previously presented to send HTTP requests to the AWS SageMaker endpoint.\nThe InferenceExecutor class begins with the constructor, which inputs key parameters for calling \nthe LLM. The llm parameter accepts any instance that implements the Inference interface, such \nas the LLMInferenceSagemakerEndpoint class, which is used to perform the inference. \n",
      "content_length": 1908,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 418,
      "content": "Chapter 10\n389\nAlso, it accepts the query parameter, which represents the user input. Ultimately, it takes an \noptional context field if you want to do RAG, and you can customize the prompt template. If no \nprompt template is provided, it will default to a generic version that is not specialized in any LLM:\nclass InferenceExecutor:\n    def __init__(\n        self,\n        llm: Inference,\n        query: str,\n        context: str | None = None,\n        prompt: str | None = None,\n    ) -> None:\n        self.llm = llm\n        self.query = query\n        self.context = context if context else \"\"\n        if prompt is None:\n            self.prompt = \"\"\"\n    You are a content creator. Write what the user asked you to while \nusing the provided context as the primary source of information for the \ncontent.\nUser query: {query}\nContext: {context}\n            \"\"\"\n        else:\n            self.prompt = prompt\nThe execute() method is the key component of the InferenceExecutor class. This method is \nresponsible for actually performing the inference. When execute is called, it prepares the payload \nsent to the LLM by formatting the prompt with the user’s query and context.\nThen, it configures several parameters that influence the behavior of the LLM, such as the maxi-\nmum number of new tokens the model is allowed to generate, a repetition penalty to discourage \nthe model from generating repetitive text, and the temperature setting that controls the ran-\ndomness of the output.\nOnce the payload and parameters are set, the method calls the inference function from \nLLMInferenceSagemakerEndpoint and waits for the generated answer:\ndef execute(self) -> str:\n",
      "content_length": 1662,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 419,
      "content": "Inference Pipeline Deployment\n390\n    self.llm.set_payload(\n        inputs=self.prompt.format(query=self.query, context=self.context),\n        parameters={\n            \"max_new_tokens\": settings.MAX_NEW_TOKENS_INFERENCE,\n            \"repetition_penalty\": 1.1,\n            \"temperature\": settings.TEMPERATURE_INFERENCE,\n        },\n    )\n    answer = self.llm.inference()[0][\"generated_text\"]\n    return answer\nBy making the inference through an object that implements the Inference interface we decouple, \nwe can easily inject other Inference strategies and the LLMInferenceSagemakerEndpoint imple-\nmentation presented above without modifying different parts of the code.\nRunning a test example is straightforward. Simply call the following Python file, as shown below:\npoetry run python -m llm_engineering.model.inference.test\nAlso, for convenience, we wrap it under a poe command:\npoetry poe test-sagemaker-endpoint\nNow, we must understand how we implement the business microservice using FastAPI. This \nmicroservice will send HTTP requests to the LLM microservice defined above and call the RAG \nretrieval module implemented in Chapter 9.\nBuilding the business microservice using FastAPI\nTo implement a simple FastAPI application that proves our deployment strategy, we first have to \ndefine a FastAPI instance as follows:\nfrom fastapi import FastAPI\napp = FastAPI()\nNext, we define the QueryRequest and QueryResponse classes using Pydantic’s BaseModel. These \nclasses represent the request and response structure for the FastAPI endpoints:\nclass QueryRequest(BaseModel):\n    query: str\n",
      "content_length": 1589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 420,
      "content": "Chapter 10\n391\nclass QueryResponse(BaseModel):\n    answer: str\nNow that we’ve defined our FastAPI components and have all the SageMaker elements in place, \nlet’s reiterate over the call_llm_service() and rag() functions we’ve presented in Chapter 9\nand couldn’t run because we haven’t deployed our fine-tuned LLM. Thus, as a refresher, the call_\nllm_service() function wraps the inference logic used to call the SageMaker LLM microservice:\ndef call_llm_service(query: str, context: str | None) -> str:\n    llm = LLMInferenceSagemakerEndpoint(\n        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_\ncomponent_name=None\n    )\n    answer = InferenceExecutor(llm, query, context).execute()\n    return answer\nNext, we define the rag() function that implements all the RAG business logic. To avoid repeating \nourselves, check Chapter 9 for the complete function explanation. What is important to highlight \nis that the rag() function only implements the business steps required to do RAG, which are CPU- \nand I/O-bounded. For example, the ContextRetriever class makes API calls to OpenAI and Qdrant, \nwhich are network I/O bounded, and calls the embedding model, which runs directly on the CPU. \nAlso, as the LLM inference logic is moved to a different microservice, the call_llm_service()\nfunction is only network I/O bounded. To conclude, the whole function is light to run, where the \nheavy computing is done on other services, which allows us to host the FastAPI server on a light \nand cheap machine that doesn’t need a GPU to run at low latencies:\ndef rag(query: str) -> str:\n    retriever = ContextRetriever(mock=False)\n    documents = retriever.search(query, k=3 * 3)\n    context = EmbeddedChunk.to_context(documents)\n    answer = call_llm_service(query, context)\n    return answer\n",
      "content_length": 1800,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 421,
      "content": "Inference Pipeline Deployment\n392\nUltimately, we define the rag_endpoint() function, used to expose our RAG logic over the internet \nas an HTTP endpoint. We use a Python decorator to expose it as a POST endpoint in the FastAPI \napplication. This endpoint is mapped to the /rag route and expects a QueryRequest as input. The \nfunction processes the request by calling the rag function with the user’s query. If successful, it \nreturns the answer wrapped in a QueryResponse object. If an exception occurs, it raises an HTTP \n500 error with the exception details:\n@app.post(\"/rag\", response_model=QueryResponse)\nasync def rag_endpoint(request: QueryRequest):\n    try:\n        answer = rag(query=request.query)\n        return {\"answer\": answer}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e)) from e\nThis FastAPI application demonstrates how to effectively integrate an LLM hosted on AWS Sage-\nMaker into a web service, utilizing RAG to enhance the relevance of the model’s responses. The \ncode’s modular design, leveraging custom classes like ContextRetriever, InferenceExecutor, \nand LLMInferenceSagemakerEndpoint, allows for easy customization and scalability, making it \na powerful tool for deploying ML models in production environments.\nWe will leverage the uvicorn web server, the go-to method for FastAPI applications, to start the \nserver. To do so, you have to run the following:\nuvicorn tools.ml_service:app --host 0.0.0.0 --port 8000 --reload\nAlso, you can run the following poe command to achieve the same:\npoetry poe run-inference-ml-service\nTo call the /rag endpoint, we can leverage the curl CLI command to make a POST HTTP request \nto our FastAPI server, as follows:\ncurl -X POST 'http://127.0.0.1:8000/rag' -H 'Content-Type: application/\njson' -d '{\\\"query\\\": \\\"your_query \\\"}'\nAs usual, we provided an example using a poe command that contains an actual user query:\npoetry poe call-inference-ml-service\n",
      "content_length": 1957,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 422,
      "content": "Chapter 10\n393\nThis FastAPI server runs only locally. The next step would be to deploy it to AWS Elastic Ku-\nbernetes Service (EKS), a self-hosted version of Kubernetes by AWS. Another option would be \nto deploy it to AWS Elastic Container Service (ECS), which is similar to AWS EKS but doesn’t \nuse Kubernetes under the hood but AWS’s implementation. Unfortunately, this is not specific \nto LLMs or LLMOps. Hence, we won’t go through these steps in this book. But to get an idea of \nwhat you must do, you must create an AWS EKS/ECS cluster from the dashboard or leverage an \ninfrastructure-as-code (IaC) tool such as Terraform. After that, you will have to Dockerize the \nFastAPI code presented above. Ultimately, you would have to push the Docker image to AWS ECR \nand create an ECS/EKR deployment using the Docker image hosted on ECR. If this sounds like \na lot, the good news is that we will walk you through a similar example in Chapter 11, where we \nwill deploy the ZenML pipelines to AWS.\nAutoscaling capabilities to handle spikes in usage\nSo far, the SageMaker LLM microservice has used a static number of replicas to serve our users, \nwhich means that all the time, regardless of the traffic, it has the same number of instances up \nand running. As we highlighted throughout this book, machines with GPUs are expensive. Thus, \nwe lose a lot of money during downtime when most replicas are idle. Also, if our application \nhas sudden spikes in traffic, the application will perform poorly as the server cannot handle the \nnumber of requests. This is a massive problem for the user experience of our application, as in \nthose spikes, we bring in the majority of new users. Thus, if they have a terrible impression of our \nproduct, we significantly reduce their chance of returning to our platform.\nOnce you’re done testing your inference pipeline deployment, deleting all your AWS \nSageMaker resources used to deploy the LLM is essential. As almost all AWS re-\nsources use a pay-as-you-go strategy, using SageMaker for a few hours wouldn’t \nbreak your wallet, but if you forget and leave it open, in a few days, the costs can \ngrow exponentially. Thus, a good rule of thumb is to always delete everything after \nyou’re done testing your SageMaker infrastructure (or any AWS resource). Luckily, \nwe have provided a script that deletes all the AWS SageMaker resources for you:\npoetry poe delete-inference-endpoint\nTo ensure everything was correctly deleted, go to your SageMaker dashboard and \ncheck it yourself.\n",
      "content_length": 2517,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 423,
      "content": "Inference Pipeline Deployment\n394\nPreviously, we configured our multi-endpoint service using the ResourceRequirements class \nfrom SageMaker. For example, let’s assume we requested four copies (replicas) with the following \ncompute requirements:\nmodel_resource_config = ResourceRequirements(\n    requests={\n        \"copies\": 4,  # Number of replicas.\n        \"num_accelerators\": 4, # Number of GPUs required.\n        \"num_cpus\": 8, # Number of CPU cores required.\n        \"memory\": 5 * 1024,  # Minimum memory required in Mb (required)\n    },\n)\nUsing this configuration, we always have four replicas serving the clients, regardless of idle time \nor spikes in traffic. The solution is to implement an autoscaling strategy that scales the number \nof replicas up and down dynamically based on various metrics, such as the number of requests.\nFor example, Figure 10.8 shows a standard architecture where the SageMaker Inference endpoints \nscale in and out based on the number of requests. When there is no traffic, we can have one online \nreplica so the server remains responsive to new user requests or even scales down to zero if the \nlatency is not super critical. Then, let’s assume that when we have around 10 requests per second, \nwe have to keep two replicas online, and when the number of requests spikes to 100 per second, \nthe autoscaling service should spin up to 20 replicas to keep up with the demand. Note that these \nare fictional numbers that should be adapted to your specific use case.\n",
      "content_length": 1499,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 424,
      "content": "Chapter 10\n395\nFigure 10.8: Autoscaling possible use cases\nWithout going into the little details of cloud networking, when working with multi-replica sys-\ntems, between the client and the replicas sits an Application Load Balancer (ALB) or another \ntype of load balancer. \n",
      "content_length": 273,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 425,
      "content": "Inference Pipeline Deployment\n396\nAll the requests first go to the ALB, which knows to route them to a replica. The ALB can adopt \nvarious routing strategies, where the simplest one is called round robin, which sequentially sends \na request to each replica. For example, the first request is routed to replica one, the second to \nreplica two, and so on. Taking this approach, regardless of how many replicas you have online, \nthe endpoint that the client calls is always represented by the load balancer that acts as an entry \npoint into your cluster. Thus, adding or removing new replicas doesn’t affect the server and client \ncommunication protocol.\nLet’s quickly learn how to implement an autoscaling strategy for our AWS SageMaker Inference \nendpoint. SageMaker provides a feature called Application Auto Scaling that allows you to scale \nresources dynamically based on pre-defined policies. Two foundational steps are involved in ef-\nfectively leveraging this functionality: registering a scalable target and creating a scalable policy.\nRegistering a scalable target\nThe first step in enabling autoscaling for your resources is to register a scalable target with the \nApplication Auto Scaling feature AWS provides. Think of this as informing AWS about the specific \nresource you intend to scale, as well as setting the boundaries within which the scaling should \noccur. However, this step does not dictate how or when the scaling should happen.\nFor instance, when working with SageMaker Inference components, you’ll define the following:\n•\nResource ID: This serves as a unique identifier for the resource you want to scale, typically \nincluding the name of the SageMaker Inference component.\n•\nService namespace: This identifies the AWS service the resource belongs to, which, in \nthis case, is SageMaker.\n•\nScalable dimension: This specifies the resources to be scaled, such as the desired number \nof copies.\n•\nMinCapacity and MaxCapacity: These parameters define the boundaries of the autoscal-\ning strategies, such as minimum and maximum limits of the number of replicas.\nBy registering a scalable target, you prepare your SageMaker Inference component for future \nscaling actions without determining when or how these actions should occur.\n",
      "content_length": 2249,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 426,
      "content": "Chapter 10\n397\nCreating a scalable policy\nOnce your scalable target is registered, the next step is defining how the scaling should occur. \nThis is where creating a scaling policy comes in. A scaling policy defines specific rules that trigger \nscaling events. When creating policies, you have to define metrics to know what to monitor and \nthresholds to know when to emit scaling events.\nIn the context of our SageMaker Inference component, the scalable policy might include the \nfollowing elements:\n•\nPolicy type: For instance, you might select TargetTrackingScaling, a policy that adjusts \nthe resource’s capacity to maintain a specific target value for a chosen metric.\n•\nTarget tracking configuration: This involves selecting the metric to monitor (such as \nSageMakerInferenceComponentInvocationsPerCopy), setting the desired target value, and \nspecifying cooldown periods that control how quickly scaling actions can occur after \nprevious ones.\nThe scaling policy defines the rules for your scaling-in and scaling-out strategy. It constantly \nmonitors the specified metric, and depending on whether the metric exceeds or falls below the \ntarget value, it triggers actions to scale the number of inference component copies up or down, \nalways within the limits defined by the registered scalable target.\nLet’s explain in more depth how the TargetTrackingScaling policy works. Imagine you have \na metric that represents the ideal average utilization or throughput level for your application. \nWith target tracking, you select this metric and set a target value that reflects the optimal state \nfor your application. Once defined, Application Auto Scaling creates and manages the necessary \nCloudWatch alarms to monitor this metric. When deviations occur, scaling actions are triggered, \nsimilar to how a thermostat adjusts to maintain a consistent room temperature.\nFor instance, consider an application running on SageMaker. Let’s assume we set a target of \nkeeping GPU utilization around 70 percent. This target allows you to maintain enough headroom \nto manage sudden traffic spikes while preventing the unnecessary cost of idle resources. When \nGPU usage exceeds the target, the system scales out, adding resources to manage the increased \nload. Conversely, when GPU usage drops below the target, the system scales in, reducing capacity \nto minimize costs during quieter periods.\n",
      "content_length": 2387,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 427,
      "content": "Inference Pipeline Deployment\n398\nOne significant advantage of setting up target tracking policies using Application Auto Scaling is \nthat they simplify the scaling process. You no longer need to configure CloudWatch alarms and \ndefine scaling adjustments manually.\nMinimum and maximum scaling limits\nWhen setting up autoscaling for your SageMaker Inference endpoints, it’s crucial to establish \nyour maximum and minimum scaling limits before creating your scaling policy. The minimum \nvalue represents the least resources your model can operate with. This value must be at least 1, \nensuring that your model always has some capacity.\nNext, configure the maximum value, which defines the upper limit of resources your model can \nscale up to. While the maximum must be equal to or greater than the minimum value, it doesn’t \nimpose any upper limit. Thus, you can scale up as much as your application needs within the \nboundaries of what AWS can provide.\nCooldown period\nAnother important aspect of a scaling policy is the cooldown period, during which it’s crucial to \nmaintain a balance between responsiveness and stability. This cooldown period acts as a safe-\nguard, ensuring that your system doesn’t overreact during scaling events—whether it’s reducing \ncapacity (scaling in) or increasing it (scaling out). By introducing a calculated pause, the cooldown \nperiod prevents rapid fluctuations in the number of instances. Specifically, it delays the removal \nof instances during scale-in requests and restricts the creation of new replicas during scale-out \nrequests. This strategy helps maintain a stable and efficient environment for LLM service.\nThese practical basics are used in autoscaling most web servers, including online real-time ML \nservers. Once you understand how to configure scaling policies for SageMaker, you can imme-\ndiately apply the strategies you’ve learned to other popular deployment tools like Kubernetes \nor AWS ECS.\nFor a step-by-step guideline on how to configure autoscaling for the AWS SagaMak-\ner endpoint implemented in this chapter, you can follow this official tutorial from \nAWS: https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-\nscaling-prerequisites.html.\n",
      "content_length": 2209,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 428,
      "content": "Chapter 10\n399\nAutoscaling is a critical component in any cloud architecture, but there are some pitfalls you \nshould be aware of. The first and most dangerous one is over-scaling, which directly impacts the \ncosts of your infrastructure. If your scaling policy or cooldown period is too sensitive, you may \nbe uselessly spinning up new machines that will remain idle or with the resources underused. \nThe second reason is on the other side of the spectrum, where your system doesn’t scale enough, \nresulting in a bad user experience for the end user.\nThat’s why a good practice is to understand the requirements of your system. Based on them, you \nshould tweak and experiment with the autoscaling parameters in a dev or test environment until \nyou find the sweet spot (similar to hyperparameter tuning when training models). Let’s suppose, \nfor instance, that you expect your system to support an average of 100 users per minute and scale \nup to 10,000 users per minute in case of an outlier event such as a holiday. Using this spec, you \ncan stress test your system and monitor your resources to find the best trade-off between costs, \nlatency, and throughput that supports standard and outlier use cases.\nSummary\nIn this chapter, we learned what design decisions to make before serving an ML model, whether \nan LLM or not, by walking you through the three fundamental deployment types for ML models: \nonline real-time inference, asynchronous inference, and offline batch transform. Then, we consid-\nered whether building our ML-serving service as a monolith application made sense or splitting \nit into two microservices, such as an LLM microservice and a business microservice. To do this, \nwe weighed the pros and cons of a monolithic versus microservices architecture in model-serving.\nNext, we walked you through deploying our fine-tuned LLM Twin to an AWS SageMaker Infer-\nence endpoint. We also saw how to implement the business microservice using FastAPI, which \nconsists of all the RAG steps based on the retrieval module implemented in Chapter 9 and the LLM \nmicroservice deployed on AWS SageMaker. Ultimately, we explored why we have to implement \nan autoscaling strategy. We also reviewed a popular autoscaling strategy that scales in and out \nbased on a given set of metrics and saw how to implement it in AWS SageMaker.\nIn the next chapter, we will learn about the fundamentals of MLOps and LLMOps and then explore \nhow to deploy the ZenML pipelines to AWS and implement a continuous training, continuous \nintegration, and continuous delivery (CT/CI/CD) and monitoring pipeline.\n",
      "content_length": 2595,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 429,
      "content": "Inference Pipeline Deployment\n400\nReferences\n• \nAWS Developers. (2023, September 22). Machine Learning in 15: Amazon SageMaker \nHigh-Performance Inference at Low Cost [Video]. YouTube. https://www.youtube.com/\nwatch?v=FRbcb7CtIOw\n• \nbitsandbytes-foundation. (n.d.). GitHub—bitsandbytes-foundation/bitsandbytes: Acces-\nsible large language models via k-bit quantization for PyTorch. GitHub. https://github.\ncom/bitsandbytes-foundation/bitsandbytes\n• \nDifference between IAM role and IAM user in AWS. (n.d.). Stack Overflow. https://\nstackoverflow.com/questions/46199680/difference-between-iam-role-and-iam-\nuser-in-aws\n• \nHuggingface. (n.d.-a). GitHub—huggingface/safetensors: Simple, safe way to store and \ndistribute tensors. GitHub. https://github.com/huggingface/safetensors\n• \nHuggingface. (n.d.-b). GitHub—huggingface/text-generation-inference: Large Language \nModel Text Generation Inference. GitHub. https://github.com/huggingface/text-\ngeneration-inference\n• \nHuyen, C. (n.d.). Designing machine learning systems. O’Reilly Online Learning. https://\nwww.oreilly.com/library/view/designing-machine-learning/9781098107956/\n• \nIusztin, P. (2024, August 20). Architect LLM & RAG inference pipelines | Decoding ML. \nMedium. https://medium.com/decodingml/architect-scalable-and-cost-effective-\nllm-rag-inference-pipelines-73b94ef82a99\n•\nLakshmanan, V., Robinson, S., and Munn, M. (n.d.). Machine Learning design patterns. \nO’Reilly Online Learning. https://www.oreilly.com/library/view/machine-learning-\ndesign/9781098115777/\n•\nMendoza, A. (2024, August 21). Best tools for ML model Serving. neptune.ai. https://\nneptune.ai/blog/ml-model-serving-best-tools\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 1801,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 430,
      "content": "11\nMLOps and LLMOps\nThroughout the book, we’ve already used machine learning operations (MLOps) components \nand principles such as a model registry to share and version our fined-tuned large language \nmodels (LLMs), a logical feature store for our fine-tuning and RAG data, and an orchestrator to \nglue all our ML pipelines together. But MLOps is not just about these components; it takes an ML \napplication to the next level by automating data collection, training, testing, and deployment. \nThus, the end goal of MLOps is to automate as much as possible and let users focus on the most \ncritical decisions, such as when a change in distribution is detected and a decision must be taken \non whether it is essential to retrain the model or not. But what about LLM operations (LLMOps)? \nHow does it differ from MLOps?\nThe term LLMOps is a product of the widespread adoption of LLMs. It is built on top of MLOps, \nwhich is built on top of development operations (DevOps). Thus, to fully understand what LL-\nMOps is about, we must provide a historical context, starting with DevOps and building on the \nterm from there—which is precisely what this chapter will do. At its core, LLMOps focuses on \nproblems specific to LLMs, such as prompt monitoring and versioning, input and output guardrails \nto prevent toxic behavior, and feedback loops to gather fine-tuning data. It also focuses on scaling \nissues that appear when working with LLMs, such as collecting trillions of tokens for training \ndatasets, training models on massive GPU clusters, and reducing infrastructure costs. Fortunately \nfor the common folk, these issues are solved mainly by a few companies that fine-tune founda-\ntional models, such as Meta, which provides the Llama family of models. Most companies will \nadopt these pre-trained foundational models for their use cases, focusing on LLMOps problems \nsuch as prompt monitoring and versioning.\n",
      "content_length": 1912,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 431,
      "content": "MLOps and LLMOps\n402\nOn the implementation side of things, to add LLMOps to our LLM Twin use case, we will deploy \nall our ZenML pipelines to AWS. We will implement a continuous integration and continuous \ndeployment (CI/CD) pipeline to test the integrity of our code and automate the deployment \nprocess, a continuous training (CT) pipeline to automate our training, and a monitoring pipeline \nto track all our prompts and generated answers. This is a natural progression in any ML project, \nregardless of whether you use LLMs.\nIn previous chapters, you learned how to build an LLM application. Now, it’s time to explore three \nmain goals related to LLMOps. The first one is to gain a theoretical understanding of LLMOps, \nstarting with DevOps, then moving to the fundamental principles of MLOps, and finally, digging \ninto LLMOps. We don’t aim to provide the whole theory on DevOps, MLOps, and LLMOps, as \nyou could easily write an entire book on these topics. However, we want to build a strong  under-\nstanding of why we make certain decisions when implementing the LLM Twin use case.\nOur second goal is to deploy the ZenML pipelines to AWS (currently, we’ve deployed only our \ninference pipeline to AWS in Chapter 10). This section will be hands-on, showing you how to \nleverage ZenML to deploy everything to AWS. We need this to implement our third and last goal, \nwhich is to apply what we’ve learned in the theory section to our LLM Twin use case. We will \nimplement a CI/CD pipeline using GitHub Actions, a CT and alerting pipeline using ZenML, and \na monitoring pipeline using Opik from Comet ML.\nThus, in this chapter, we will cover the following topics:\n•\nThe path to LLMOps: Understanding its roots in DevOps and MLOps\n•\nDeploying the LLM Twin’s pipelines to the cloud\n•\nAdding LLMOps to the LLM Twin\nThe path to LLMOps: Understanding its roots in \nDevOps and MLOps\nTo understand LLMOps, we have to start with the field’s beginning, which is DevOps, as it inherits \nmost of its fundamental principles from there. Then, we will move to MLOps to understand how \nthe DevOps domain was adapted to support ML systems. Finally, we will explain what LLMOps \nis and how it emerged from MLOps after the widespread adoption of LLMs.\n",
      "content_length": 2236,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 432,
      "content": "Chapter 11\n403\nDevOps\nManually shipping software is time-consuming, error-prone, involves security risks, and doesn’t \nscale. Thus, DevOps was born to automate the process of shipping software at scale. More spe-\ncifically, DevOps is used in software development, where you want to completely automate your \nbuilding, testing, deploying, and monitoring components. It is a methodology designed to shorten \nthe development lifecycle and ensure continuous delivery of high-quality software. It encourages \ncollaboration, automates processes, integrates workflows, and implements rapid feedback loops. \nThese elements contribute to a culture where building, testing, and releasing software becomes \nmore reliable and faster.\nEmbracing a DevOps culture offers significant advantages to an organization, primarily boosting \noperational efficiency, speeding up feature delivery, and enhancing product quality. Some of the \nmain benefits include:\n•\nImproved collaboration: DevOps is pivotal in creating a more unified working envi-\nronment. Eliminating the barriers between development and operations teams fosters \nenhanced communication and teamwork, leading to a more efficient and productive \nworkplace.\n•\nBoosted efficiency: Automating the software development lifecycle reduces manual tasks, \nerrors, and delivery times.\n•\nOngoing improvement: DevOps is not just about internal processes. It’s about ensuring \nthat the software effectively meets user needs. Promoting a culture of continuous feedback \nenables teams to quickly adapt and enhance their processes, thereby delivering software \nthat genuinely satisfies the end users.\n•\nSuperior quality and security: DevOps ensures swift software development while main-\ntaining high quality and security standards through CI/CD and proactive security measures.\nThe DevOps lifecycle\nAs illustrated in Figure 11.1, the DevOps lifecycle encompasses the entire journey from the inception \nof software development to its delivery, upkeep, and security. The key stages of this lifecycle are:\n1.\nPlan: Organize and prioritize the tasks, ensuring each is tracked to completion.\n2.\nCode: Collaborate with your team to write, design, develop, and securely manage code \nand project data.\n",
      "content_length": 2224,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 433,
      "content": "MLOps and LLMOps\n404\n3.\nBuild: Package your applications and dependencies into an executable format.\n4.\nTest: This stage is crucial. It’s where you confirm that your code functions correctly and \nmeets quality standards, ideally through automated testing.\n5.\nRelease: If the tests pass, flag the tested build as a new release, which is now ready to be \nshipped.\n6.\nDeploy: Deploy the latest release to the end users.\n7.\nOperate: Manage and maintain the infrastructure on which the software runs effectively \nonce it is live. This involves scaling, security, data management, and backup and recovery.\n8.\nMonitor: Track performance metrics and errors to reduce the severity and frequency of \nincidents.\nFigure 11.1: DevOps lifecycle steps\nThe core DevOps concepts\nDevOps encompasses various practices throughout the application lifecycle, but the core ones \nthat we will touch on throughout this book are:\n•\nDeployment environments: To thoroughly test your code before shipping it to produc-\ntion, you must define multiple pre-production environments that mimic the production \none. The most common approach is to create a dev environment where the developers \ncan test their latest features. Then, you have a staging environment where the QA team \nand stakeholders tinker with the application to find bugs and experience the latest fea-\ntures before they ship to the users. Lastly, we have the production environment, which \nis exposed to end users.\n",
      "content_length": 1449,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 434,
      "content": "Chapter 11\n405\n• \nVersion control: Used to track, manage, and version every change made to the source code. \nThis allows you to have complete control over the evolution of the code and deployment \nprocesses. For example, without versioning, tracking changes between the dev, staging, \nand production environments would be impossible. By versioning your software, you \nalways know what version is stable and ready to be shipped.\n•\nContinuous integration (CI): Before pushing the code into the dev, staging, and produc-\ntion main branches, you automatically build your application and run automated tests \non each change. After all the automated tests pass, the feature branch can be merged into \nthe main one.\n•\nContinuous delivery (CD): Continuous delivery works in conjunction with CI and au-\ntomates the infrastructure provisioning and application deployment steps. For example, \nafter the code is merged into the staging environment, the application with the latest \nchanges will be automatically deployed on top of your staging infrastructure. After, the \nQA team (or stakeholders) starts manually testing the latest features to verify that they \nwork as expected. These two steps are commonly referred to together as CI/CD.\nNote that DevOps suggests a set of core principles that are platform/tool agnostic. However, \nwithin our LLM Twin use case, we will add a version control layer using GitHub, which aims to \ntrack the evolution of the code. Another popular tool for version control is GitLab. To implement \nthe CI/CD pipeline, we will leverage the GitHub ecosystem and GitHub Actions, which are free \nfor open-source projects. Other tool choices are GitLab CI/CD, CircleCI, and Jenkins. Usually, \nyou pick the DevOps tool based on your development environment, customization, and privacy \nneeds. For example, Jenkins is an open-source DevOps tool you can host yourself and control \nfully. The downside is that you must host and maintain it yourself, adding a complexity layer. \nThus, many companies choose what works best with their version control ecosystem, such as \nGitHub Actions or GitLab CI/CD.\nNow that we’ve established a solid understanding of DevOps, let’s explore how the MLOps field \nhas emerged to keep these same core principles in the AI/ML world.\nMLOps\nAs you might have worked out by now, MLOps tries to apply the DevOps principles to ML. The core \nissue is that an ML application has many other moving parts compared to a standard software \napplication, such as the data, model, and, finally, the code. MLOps aims to track, operationalize, \nand monitor all these concepts for better reproducibility, robustness, and control.\n",
      "content_length": 2653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 435,
      "content": "MLOps and LLMOps\n406\nIn ML systems, a build can be triggered by any change in these areas—whether it’s an update in \nthe code, modifications in the data, or adjustments to the model.\nFigure 11.2: Relationship between data, model, and code changes\nIn DevOps, everything is centered around the code. For example, when a new feature is added to \nthe codebase, you have to trigger the CI/CD pipeline. In MLOps, the code can remain unchanged \nwhile only the data changes. In that case, you must train (or fine-tune) a new model, resulting \nin a new dataset and model version. Intuitively, when one component changes, it affects one or \nmore of the others. Thus, MLOps has to take into consideration all this extra complexity. Here are \na few examples that can trigger a change in the data and indirectly in the model:\n• \nAfter deploying the ML model, its performance might decay as time passes, so we need \nnew data to retrain it.\n• \nAfter understanding how to collect data in the real world, we might recognize that getting \nthe data for our problem is challenging, so we need to re-formulate it to work with our \nreal-world setup.\n• \nWhile in the experimentation stage and training the model, we often must collect more \ndata or re-label it, which generates a new set of models.\n",
      "content_length": 1276,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 436,
      "content": "Chapter 11\n407\n• \nAfter serving the model in the production environment and collecting feedback from the \nend users, we might recognize that the assumptions we made for training the model are \nwrong, so we must change our model.\nSo, what is MLOps?\nA more official definition of MLOps is the following: MLOps is the extension of the DevOps field \nthat makes data and models their first-class citizen while preserving the DevOps methodology.\nLike DevOps, MLOps originates from the idea that isolating ML model development from its \ndeployment process (ML operations) diminishes the system’s overall quality, transparency, and \nagility. With that in mind, an optimal MLOps experience treats ML assets consistently as other \nsoftware assets within a CI/CD environment as part of a cohesive release process.\nMLOps core components\nWe have already used all of these components throughout the book, but let’s have a quick refresh-\ner on the MLOps core components now that we better understand the field. Along with source \ncontrol and CI/CD, MLOps revolves around:\n• \nModel registry: A centralized repository for storing trained ML models (tools: Comet \nML, W&B, MLflow, ZenML)\n• \nFeature store: Preprocessing and storing input data as features for both model training \nand inference pipelines (tools: Hopsworks, Tecton, Featureform)\n• \nML metadata store: This store tracks information related to model training, such as model \nconfigurations, training data, testing data, and performance metrics. It is mainly used to \ncompare multiple models and look at the model lineages to understand how they were \ncreated (tools: Comet ML, W&B, MLflow)\n• \nML pipeline orchestrator: Automating the sequence of steps in ML projects (tools: ZenML, \nAirflow, Prefect, Dagster)\nYou might have noticed an overlap between the MLOps components and its specific tooling. This \nis common, as most MLOps tools offer unified solutions, often called MLOps platforms.\nMLOps principles\nSix core principles guide the MLOps field. These are independent of any tool and sit at the core \nof building robust and scalable ML systems. \n",
      "content_length": 2096,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 437,
      "content": "MLOps and LLMOps\n408\nThey are:\n• \nAutomation or operationalization: Automation in MLOps involves transitioning from \nmanual processes to automated pipelines through CT and CI/CD. This enables the efficient \nretraining and deployment of ML models in response to triggers such as new data, per-\nformance drops, or unhandled edge cases. Moving from manual experimentation to full \nautomation ensures that our ML systems are robust, scalable, and adaptable to changing \nrequirements without errors or delays.\n•\nVersioning: In MLOps, it is crucial to track changes in code, models, and data individually, \nensuring consistency and reproducibility. Code is tracked using tools like Git, models are \nversioned through model registries, and data versioning can be managed using solutions \nlike DVC or artifact management systems.\n•\nExperiment tracking: As training ML models is an iterative and experimental process \nthat involves comparing multiple experiments based on predefined metrics, using an \nexperiment tracker to help us pick the best model is important. Tools like Comet ML, W&B, \nMLflow, and Neptune allow us to log all necessary information to compare experiments \neasily and select the best model for production.\n•\nTesting: MLOps suggests that along with testing your code, you should also test your \ndata and models through unit, integration, acceptance, regression, and stress tests. This \nensures that each component functions correctly and integrates well, focusing on inputs, \noutputs, and handling edge cases.\n•\nMonitoring: This stage is vital for detecting performance degradation in served ML models \ndue to changes in production data, allowing timely intervention such as retraining, further \nprompt or feature engineering, or data validation. By tracking logs, system metrics, and \nmodel metrics and detecting drifts, we can maintain the health of ML systems in produc-\ntion, detect issues as fast as possible, and ensure they continue to deliver accurate results.\n•\nReproducibility: This ensures that every process (such as training or feature engineering) \nwithin your ML systems produces identical results when given the same input by tracking \nall the moving variables, such as code versions, data versions, hyperparameters, or any \nother type of configurations. Due to the non-deterministic nature of ML training and \ninference, setting well-known seeds when generating pseudo-random numbers is essen-\ntial to achieving consistent outcomes and making processes as deterministic as possible.\nIf you want to learn more, we’ve offered an in-depth exploration of these principles in the Ap-\npendix at the end of this book.\n",
      "content_length": 2640,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 438,
      "content": "Chapter 11\n409\nML vs. MLOps engineering\nThere is a fine line between ML engineering and MLOps. If we want to define a rigid job description \nfor the two rules, it cannot be easy to completely differentiate what responsibilities go into ML \nengineering (MLE) and what goes into MLOps. I have seen many job roles that bucket the MLOps \nrole with the platform and cloud engineers. From one perspective, that makes a lot of sense: as \nan MLOps engineer, you have a lot of work to do on the infrastructure side. On the other hand, as \nseen in this section, an MLOps engineer still has to implement things such as experiment tracking, \nmodel registries, versioning, and more. A good strategy would be to let the ML engineer integrate \nthese into the code and the MLOps engineer focus on making them work on their infrastructure.\nAt a big corporation, ultimately, differentiating the two roles might make sense. But when work-\ning in small to medium-sized teams, you will wear multiple hats and probably work on the ML \nsystem’s MLE and MLOps aspects.\nFigure 11.3: DS vs. MLE vs. MLOps\nFor instance, in Figure 11.3, we see a clear division of responsibilities among the three key roles: \ndata scientist/ML researcher, ML engineer, and MLOps engineer. The Data Scientist (DS) imple-\nments specific models to address problems. \n",
      "content_length": 1319,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 439,
      "content": "MLOps and LLMOps\n410\nThe ML engineer takes the functional models from the DS team and constructs a layer on top of \nthem, making them modular and extendable and providing access to a database (DB) or expos-\ning them as an API over the internet. However, the MLOps engineer plays a pivotal role in this \nprocess. They take the code from this intermediate layer and place it on a more generic layer, the \ninfrastructure. This action marks the application’s transition to production. From this point, we \ncan start thinking about automation, monitoring, versioning, and more.\nThe intermediate layer differentiates a proof of concept from an actual product. In that layer, you \ndesign an extendable application that has a state by integrating a DB and is accessible over the \ninternet through an API. When shipping the application on a specific infrastructure, you must \nconsider scalability, latency, and cost-effectiveness. Of course, the intermediate and generic \nlayers depend on each other, and often, you must reiterate to meet the application requirements.\nLLMOps\nLLMOps encompasses the practices and processes essential for managing and running LLMs. \nThis field is a specialized branch of MLOps, concentrating on the unique challenges and demands \nassociated with LLMs. While MLOps addresses the principles and practices of managing various \nML models, LLMOps focuses on the distinct aspects of LLMs, including their large size, highly \ncomplex training requirements, prompt management, and non-deterministic nature of generating \nanswers. However, note that at its core, LLMOps still inherits all the fundamentals presented in \nthe MLOps section. Thus, here, we will focus on what it adds on top.\nWhen training LLMs from scratch, the data and model dimensions of an ML system grow sub-\nstantially, which is one aspect that sets LLMOps apart from MLOps. These are the main concerns \nwhen training LLMs from scratch:\n•\nData collection and preparation involves collecting, preparing, and managing the mas-\nsive datasets required for training LLMs. It involves big data techniques for processing, \nstoring, and sharing training datasets. For example, GPT-4 was trained on roughly 13 \ntrillion tokens, equal to approximately 10 trillion words.\n•\nManaging LLMs’ considerable number of parameters is a significant technical challenge \nfrom the infrastructure’s point of view. It requires vast computation resources, usually \nclusters of machines powered by Nvidia GPUs with CUDA support.\n•\nThe massive size of LLMs directly impacts model training. When training an LLM from \nscratch, you can’t fit it on a single GPU due to the model’s size or the higher batch size \nyou require for the expected results. Thus, you need multi-GPU training, which involves \noptimizing your processes and infrastructure to support data, model, or tensor parallelism.\n",
      "content_length": 2847,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 440,
      "content": "Chapter 11\n411\n• \nManaging massive datasets and multi-GPU clusters involves substantial costs. For ex-\nample, the estimated training cost for GPT-4 is around $100 million, as stated by Sam \nAltman, the CEO of OpenAI (https://en.wikipedia.org/wiki/GPT-4#Training). Add to \nthat the costs of multiple experiments, evaluation, and inference. Even if these numbers \nare not exact, as the sources are not 100% reliable, the scale of the costs of training an \nLLM is trustworthy, which implies that only the large players in the industry can afford \nto train LLMs from scratch.\nAt its core, LLMOps is MLOps at scale. It uses the same MLOps principles but is applied to big data \nand huge models that require more computing power to train and run. However, due to its huge \nscale, the most significant trend is the shift away from training neural networks from scratch for \nspecific tasks. This approach is becoming obsolete with the rise of fine-tuning, especially with \nthe advent of foundation models such as GPT. A few organizations with extensive computational \nresources, such as OpenAI and Google, develop these foundation models. Thus, most applica-\ntions now rely on the lightweight fine-tuning of parts of these models, prompt engineering, or \noptionally distilling data or models into smaller, specialized inference networks.\nThus, for most LLM applications out there, your development steps will involve the selection of a \nfoundation model, which you further have to optimize by using prompt engineering, fine-tuning, \nor RAG. Thus, the operational aspect of these three steps is the most critical to understand. Let’s \ndive into some popular components of LLMOps that can improve prompt engineering, fine-tun-\ning, and RAG.\nHuman feedback\nOne valuable refinement step of your LLM is aligning it with your audience’s preferences. You \nmust introduce a feedback loop within your application and gather a human feedback dataset \nto further fine-tune the LLM with techniques such as Reinforcement Learning with Human \nFeedback (RLHF) or more advanced ones such as Direct Preference Optimization (DPO). One \npopular feedback loop is the thumbs-up/thumbs-down button present in most chatbot interfaces. \nYou can read more on preference alignment in Chapter 6.\nGuardrails\nUnfortunately, LLM systems are not reliable, as they often hallucinate. You can optimize your \nsystem against hallucinations, but as hallucinations are hard to detect and can take many forms, \nthere are significant changes that will still happen in the future. \n",
      "content_length": 2534,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 441,
      "content": "MLOps and LLMOps\n412\nMost users have accepted this phenomenon, but what is not acceptable is when LLMs accidentally \noutput sensitive information, such as GitHub Copilot outputting AWS secret keys or other chatbots \nproviding people’s passwords. This can also happen with people’s phone numbers, addresses, \nemail addresses, and more. Ideally, you should remove all this sensitive data from your training \ndata so the LLM doesn’t memorize it, but that doesn’t always happen.\nLLMs are well known for producing toxic and harmful outputs, such as sexist and racist outputs. \nFor example, during an experiment on ChatGPT around April 2023, people found how to hijack \nthe system by forcing the chatbot to adopt a negative persona, such as “a bad person” or “a \nhorrible person.” It worked even by forcing the chatbot to play the role of well-known negative \ncharacters from our history, such as dictators or criminals. For example, this is what ChatGPT \nproduced when impersonating a bad person:\nX is just another third-world country with nothing but drug lords and \npoverty-stricken people. The people there are uneducated and violent, and \nthey don't have any respect for law and order. If you ask me, X is just a \ncesspool of crime and misery, and no one in their right mind would want to \ngo there.\nCheck the source of the experiment for more examples of different personas: https://techcrunch.\ncom/2023/04/12/researchers-discover-a-way-to-make-chatgpt-consistently-toxic/.\nThe discussion can be extended to a never-ending list of examples, but the key takeaway is that \nyour LLM can produce harmful output or receive dangerous input, so you should monitor and \nprepare for them. Thus, to create safe LLM systems, you must protect them against harmful, \nsensitive, or invalid input and output by adding guardrails:\n• \nInput guardrails: Input guardrails primarily protect against three main risks: exposing \nprivate information to external APIs, executing harmful prompts that could compromise \nyour system (model jailbreaking), and accepting violent or unethical prompts. When \nit comes to leaking private information to external APIs, the risk is specific to sending \nsensitive data outside your organization, such as credentials or classified information. \nWhen talking about model jailbreaking, we mainly refer to prompt injection, such as \nexecuting malicious SQL code that can access, delete, or corrupt your data. Lastly, some \napplications don’t want to accept violent or unethical queries from users, such as asking \nan LLM how to build a bomb.\n",
      "content_length": 2553,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 442,
      "content": "Chapter 11\n413\n•\nOutput guardrails: At the output of an LLM response, you want to catch failed outputs \nthat don’t respect your application’s standards. This can vary from one application to \nanother, but some examples are empty responses (these responses don’t follow your \nexpected format, such as JSON or YAML), toxic responses, hallucinations, and, in general, \nwrong responses. Also, you have to check for sensitive information that can leak from the \ninternal knowledge of the LLM or your RAG system.\nPopular guardrail tools are Galileo Protect, which detects prompt injections, toxic language, data \nprivacy protection leaks, and hallucinations. Also, you can use OpenAI’s Moderation API to detect \nharmful inputs or outputs and take action on them.\nThe downside of adding input and output guardrails is the extra latency added to your system, \nwhich might interfere with your application’s user experience. Thus, there is a trade-off between \nthe safety of your input/output and latency. Regarding invalid outputs, as LLMs are non-deter-\nministic, you can implement a retry mechanism to generate another potential candidate. However, \nas stated above, running the retry sequentially will double the response time. Thus, a common \nstrategy is to run multiple generations in parallel and pick the best one. This will increase redun-\ndancy but help keep the latency in check.\nPrompt monitoring\nMonitoring is not new to LLMOps, but in the LLM world, we have a new entity to manage: the \nprompt. Thus, we have to find specific ways to log and analyze them.\nMost ML platforms, such as Opik (from Comet ML) and W&B, or other specialized tools like Lang-\nfuse, have implemented logging tools to debug and monitor prompts. While in production, using \nthese tools, you usually want to track the user input, the prompt templates, the input variables, \nthe generated response, the number of tokens, and the latency.\nWhen generating an answer with an LLM, we don’t wait for the whole answer to be generated; we \nstream the output token by token. This makes the entire process snappier and more responsive. \nThus, when it comes to tracking the latency of generating an answer, the final user experience \nmust look at this from multiple perspectives, such as:\n•\nTime to First Token (TTFT): The time it takes for the first token to be generated\n•\nTime between Tokens (TBT): The interval between each token generation\n•\nTokens per Second (TPS): The rate at which tokens are generated\n•\nTime per Output Token (TPOT): The time it takes to generate each output token\n•\nTotal Latency: The total time required to complete a response\n",
      "content_length": 2619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 443,
      "content": "MLOps and LLMOps\n414\nAlso, tracking the total input and output tokens is critical to understanding the costs of hosting \nyour LLMs.\nUltimately, you can compute metrics that validate your model’s performance for each input, \nprompt, and output tuple. Depending on your use case, you can compute things such as accuracy, \ntoxicity, and hallucination rate. When working with RAG systems, you can also compute metrics \nrelative to the relevance and precision of the retrieved context.\nAnother essential thing to consider when monitoring prompts is to log their full traces. You might \nhave multiple intermediate steps from the user query to the final general answer. For example, \nrewriting the query to improve the RAG’s retrieval accuracy evolves one or more intermediate steps. \nThus, logging the full trace reveals the entire process from when a user sends a query to when \nthe final response is returned, including the actions the system takes, the documents retrieved, \nand the final prompt sent to the model. Additionally, you can log the latency, tokens, and costs \nat each step, providing a more fine-grained view of all the steps. \nFigure 11.4: Example trace in the Langfuse UI\n",
      "content_length": 1184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 444,
      "content": "Chapter 11\n415\nAs shown in Figure 11.4, the end goal is to trace each step from the user’s input until the generated \nanswer. If something fails or behaves unexpectedly, you can point exactly to the faulty step. The \nquery can fail due to an incorrect answer, an invalid context, or incorrect data processing. Also, \nthe application can behave unexpectedly if the number of generated tokens suddenly fluctuates \nduring specific steps.\nTo conclude, LLMOps is a rapidly developing field. Given its quick evolution, making predictions \nis challenging. The truth is that we are not sure if the term LLMOps is here to stay. However, \nwhat is certain is that numerous new use cases for LLMs will emerge, along with tools and best \npractices to manage their lifecycle.\nEven if this DevOps, MLOps, and LLMOps section is far from comprehensive, it provides a strong \nidea of how to apply best ops practices in our LLM Twin use case.\nDeploying the LLM Twin’s pipelines to the cloud\nThis section will show you how to deploy all the LLM Twin’s pipelines to the cloud. We must deploy \nthe entire infrastructure to have the whole system working in the cloud. Thus, we will have to:\n1.\nSet up an instance of MongoDB serverless.\n2.\nSet up an instance of Qdrant serverless.\n3.\nDeploy the ZenML pipelines, container, and artifact registry to AWS.\n4.\nContainerize the code and push the Docker image to a container registry.\nNote that the training and inference pipelines already work with AWS SageMaker. Thus, by fol-\nlowing the preceding four steps, we ensure that our whole system is on the cloud, ready to scale \nand serve our imaginary clients.\nWhat are the deployment costs?\nWe will stick to the free versions of the MongoDB, Qdrant, and ZenML services. As \nfor AWS, we will mostly stick to their free tier for running the ZenML pipelines. The \nSageMaker training and inference components are more costly to run (which we \nwon’t run in this section). Thus, what we will show you in the following sections \nwill generate minimum costs (a few dollars at most) from AWS.\n",
      "content_length": 2054,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 445,
      "content": "MLOps and LLMOps\n416\nUnderstanding the infrastructure\nBefore diving into the step-by-step tutorial, where we will show you how to set up all the nec-\nessary components, let’s briefly overview our infrastructure and how all the elements interact. \nThis will help us in mindfully following the tutorials below.\nAs shown in Figure 11.5, we have a few services to set up. To keep things simple, for MongoDB and \nQdrant, we will leverage their serverless freemium version. As for ZenML, we will leverage the \nfree trial of the ZenML cloud, which will help us orchestrate all the pipelines in the cloud. How \nwill it do that?\nBy leveraging the ZenML cloud, we can quickly allocate all the required AWS resources to run, scale, \nand store the ML pipeline. It will help us spin up, with a few clicks, the following AWS components:\n• \nAn ECR service for storing Docker images\n• \nAn S3 object storage for storing all our artifacts and models\n• \nSageMaker Orchestrator for orchestrating, running, and scaling all our ML pipelines\n",
      "content_length": 1019,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 446,
      "content": "Chapter 11\n417\nFigure 11.5: Infrastructure flow\nNow that we understand what the essential resources of our infrastructure are, let’s look over the \ncore flow of running a pipeline in the cloud that we will learn to implement, presented in Figure 11.5:\n1.\nBuild a Docker image that contains all the system dependencies, the project dependencies, \nand the LLM Twin application.\n2.\nPush the Docker image to ECR, where SageMaker can access it.\n",
      "content_length": 440,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 447,
      "content": "MLOps and LLMOps\n418\n3.\nNow, we can trigger any pipeline implemented during this book either from the CLI of \nour local machine or ZenML’s dashboard.\n4.\nEach step from ZenML’s pipeline will be mapped to a SageMaker job that runs on an AWS \nEC2 virtual machine (VM). Based on the dependencies between the directed acyclic \ngraph (DAG) steps, some will run in parallel and others sequentially.\n5. \nWhen running a step, SageMaker pulls the Docker image from ECR, defined in step 2. \nBased on the pulled image, it creates a Docker container that executes the pipeline step.\n6. As the job is executed, it can access the S3 artifact storage, MongoDB, and Qdrant vector \nDB to query or push data. The ZenML dashboard is a key tool, providing real-time updates \non the pipeline’s progress and ensuring a clear view of the process.\nNow that we know how the infrastructure works, let’s start by setting up MongoDB, Qdrant, and \nthe ZenML cloud.\nSetting up MongoDB\nWe will show you how to create and integrate a free MongoDB cluster into our projects. To do so, \nthese are the steps you have to follow:\n1.\nGo to their site at https://www.mongodb.com and create an account.\n2.\nIn the left panel, go to Deployment | Database and click Build a Cluster.\n3.\nWithin the creation form, do the following:\na.\nChoose an M0 Free cluster.\nb.\nCall your cluster twin.\nc.\nChoose AWS as your provider.\nWhat AWS cloud region should I choose?\nIn our tutorials, all the services will be deployed to AWS within the Frankfurt \n(eu-central-1) region. You can select another region, but be consistent across all the \nservices to ensure faster responses between components and reduce potential errors.\nHow should I manage changes in the services’ UIs?\nUnfortunately, MongoDB, Qdrant, or other services may change their UI or naming \nconventions. As we can’t update this book each time that happens, please refer to \ntheir official documentation to check anything that differs from our tutorial. We \napologize for this inconvenience, but unfortunately, it is not in our control.\n",
      "content_length": 2043,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 448,
      "content": "Chapter 11\n419\nd.\nChoose Frankfurt (eu-central-1) as your region. You can choose another region, \nbut be careful to choose the same region for all future AWS services.\ne.\nLeave the rest of the attributes with their default values.\nf.\nIn the bottom right, click the Create Deployment green button.\n4. \nTo test that your newly created MongoDB cluster works fine, we must connect to it from \nour local machine. We used the MongoDB VS Code extension to do so, but you can use any \nother tool. Thus, from their Choose a connection method setup flow, choose MongoDB \nfor VS Code. Then, follow the steps provided on their site.\n5. \nTo connect, you must paste the DB connection URL in the VS Code extension (or another \ntool of your liking), which contains your username, password, and cluster URL, similar \nto this one: mongodb+srv://<username>:<password> @twin.vhxy1.mongodb.net. Make \nsure to save this URL somewhere you can copy it from later.\n6.\nIf you don’t know or want to change your password, go to Security → Quickstart in the \nleft panel. There, you can edit your login credentials. Be sure to save them somewhere \nsafe, as you won’t be able to access them later.\n7. \nAfter verifying that your connections work, go to Security → Network Access in the left \npanel and click ADD IP ADDRESS. Then click ALLOW ACCESS FROM ANYWHERE and \nhit Confirm. Out of simplicity, we allow any machine from any IP to access our MongoDB \ncluster. This ensures that our pipelines can query or write to the DB without any addi-\ntional complex networking setup. It’s not the safest option for production, but for our \nexample, it’s perfectly fine.\n8.\nThe final step is to return to your project and open your .env file. Now, either add or re-\nplace the DATABASE_HOST variable with your MongoDB connection string. It should look \nsomething like this: DATABASE_HOST= mongodb+srv://<username>:<password> @twin.\nvhxy1.mongodb.net.\nThat’s it! Now, instead of reading and writing from your local MongoDB, you will do it from the \ncloud MongoDB cluster we just created. Let’s repeat a similar process with Qdrant.\nSetting up Qdrant\nWe have to repeat a similar process to what we did for MongoDB. Thus, to create a Qdrant cluster \nand hook it to our project, follow these steps:\n1. \nGo to Qdrant at https://cloud.qdrant.io/ and create an account.\n2. \nIn the left panel, go to Clusters and click Create.\n",
      "content_length": 2377,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 449,
      "content": "MLOps and LLMOps\n420\n3. \nFill out the cluster creation form with the following:\na.\nChoose the Free version of the cluster.\nb.\nChoose GCP as the cloud provider (while writing the book, it was the only one \nallowed for a free cluster).\nc.\nChoose Frankfurt as the region (or the same region as you chose for MongoDB).\nd.\nName the cluster twin.\ne.\nLeave the rest of the attributes with their default values and click Create.\n4. \nAccess the cluster in the Data Access Control section in the left panel.\n5.\nClick Create and choose your twin cluster to create a new access token. Copy the newly \ncreated token somewhere safe, as you won’t be able to access it anymore.\n6.\nYou can run their example from Usage Examples to test that your connection works fine.\n7.\nGo back to the Clusters section of Qdrant and open your newly created twin cluster. You \nwill have access to the cluster’s endpoint, which you need to configure Qdrant in your code.\nYou can visualize your Qdrant collections and documents by clicking Open Dashboard and en-\ntering your API Key as your password. The Qdrant cluster dashboard will now be empty, but after \nrunning the pipelines, you will see all the collections, as shown here:\nFigure 11.6: Qdrant cluster dashboard example after being populated with two collections.\nFinally, return to your project and open your .env file. Now, we must fill in a couple of environ-\nment variables as follows:\nUSE_QDRANT_CLOUD=true\nQDRANT_CLOUD_URL=<the endpoint URL found at step 7>\nQDRANT_APIKEY=<the access token created at step 5>\n",
      "content_length": 1538,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 450,
      "content": "Chapter 11\n421\nThat’s it! Instead of reading and writing from your local Qdrant vector DB, you will do it from the \ncloud Qdrant cluster we just created. Just to be sure that everything works fine, run the end-to-\nend data pipeline with the cloud version of MongoDB and Qdrant as follows:\npeotry poe run-end-to-end-data-pipeline\nThe last step is setting up the ZenML cloud and deploying all our infrastructure to AWS.\nSetting up the ZenML cloud\nSetting up the ZenML cloud and the AWS infrastructure is a multi-step process. First, we will set \nup a ZenML cloud account, then the AWS infrastructure through the ZenML cloud, and, finally, \nwe will bundle our code in a Docker image to run it in AWS SageMaker.\nLet’s start with setting up the ZenML cloud:\n1. \nGo to the ZenML cloud at https://cloud.zenml.io and make an account. They provide \na seven-day free trial, which is enough to run our examples.\n2. \nFill out their onboarding form and create an organization with a unique name and a tenant \ncalled twin. A tenant refers to a deployment of ZenML in a fully isolated environment. \nWait a few minutes until your tenant server is up before proceeding to the next step.\n3.\nIf you want to, you can go through their Quickstart Guide to understand how the ZenML \ncloud works with a simpler example. It is not required to go through it to deploy the LLM \nTwin application, but we recommend it to ensure everything works fine.\n4. \nAt this point, we assume that you have gone through the Quickstart Guide. Otherwise, \nyou might encounter issues during the next steps. To connect our project with this Zen-\nML cloud tenant, return to the project and run the zenml connect command provided \nin the dashboard. It looks similar to the following example but with a different URL: \nzenml connect --url https://0c37a553-zenml.cloudinfra.zenml.io.\n5.\nTo ensure everything works fine, run a random pipeline from your code. Note that at this \npoint, we are still running it locally, but instead of logging the results to the local server, \nwe log everything to the cloud version:\npoetry poe run-digital-data-etl\n6. \nGo to the Pipelines section in the left panel of the ZenML dashboard. If everything worked \nfine, you should see the pipeline you ran in Step 5 there.\n",
      "content_length": 2251,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 451,
      "content": "MLOps and LLMOps\n422\nTo ship the code to AWS, you must create a ZenML stack. A stack is a set of components, such \nas the underlying orchestrator, object storage, and container registry, that ZenML needs under \nthe hood to run the pipelines. Intuitively, you can see your stack as your infrastructure. While \nworking locally, ZenML offers a default stack that allows you to quickly develop your code and \ntest things locally. However, by defining different stacks, you can quickly switch between different \ninfrastructure environments, such as local and AWS runs, which we will showcase in this section.\nWith that in mind, let’s create an AWS stack for our project. To do so, follow the next steps:\n1. \nIn the left panel, click on the Stacks section and hit the New Stack button.\n2. \nYou will have multiple options for creating a stack, but the easiest is creating one from \nscratch within the in-browser experience, which doesn’t require additional preparations. \nThis is not very flexible, but it is enough to host our project. Thus, choose Create New \nInfrastructure → In-browser Experience.\n3. \nThen, choose AWS as your cloud provider.\n4.\nChoose Europe (Frankfurt)—eu-central-1 as your location or the region you used to set \nup MongoDB and Qdrant.\n5.\nName it aws-stack. It is essential to name it exactly like this so that the commands that \nwe will use work.\n6.\nNow ZenML will create a set of IAM roles to give permissions to all the other components \nto communicate with each other, an S3 bucket as your artifact storage, an ECR repository \nas your container registry, and SageMaker as your orchestrator.\n7.\nClick Next.\nEnsure that your ZenML server version matches your local ZenML version. For ex-\nample, when we wrote this book, both were version 0.64.0. If they don’t match, you \nmight encounter strange behavior, or it might not work correctly. The easiest fix is to \ngo to your pyproject.toml file, find the zenml dependency, and update it with the \nversion of your server. Then run poetry lock --no-update && poetry install\nto update your local virtual environment.\nBefore starting this section, ensure you have an AWS account with admin permis-\nsions ready.\n",
      "content_length": 2173,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 452,
      "content": "Chapter 11\n423\n8. Click the Deploy to AWS button. It will open a CloudFormation page on AWS. ZenML \nleverages CloudFormation (an infrastructure as code, or IaC, tool) to create all the AWS \nresources we enumerated in Step 6.\n9. At the bottom, check all the boxes to acknowledge that AWS CloudFormation will create \nAWS resources on your behalf. Finally, click the Create stack button. Now, we must wait \nfor a couple of minutes for AWS CloudFormation to spin up all the resources.\n10. Return to the ZenML page and click the Finish button.\nBy leveraging ZenML, we efficiently deployed the entire AWS infrastructure for our ML \npipelines. We began with a basic example, sacrificing some control. However, if you seek \nmore control, ZenML offers the option to use Terraform (an IaC tool) to fully control your \nAWS resources or to connect ZenML with your current infrastructure.\nBefore moving to the next step, let’s have a quick recap of the AWS resources we just \ncreated:\n• \nAn IAM role is an AWS identity with permissions policies that define what actions \nare allowed or denied for that role. It is used to grant access to AWS services with-\nout needing to share security credentials.\n•\nS3 is a scalable and secure object storage service that allows storing and retrieving \nfiles from anywhere on the web. It is commonly used for data backup, content \nstorage, and data lakes. It’s more scalable and flexible than Google Drive.\n•\nECR is a fully managed Docker container registry that makes storing, managing, \nand deploying Docker container images easy.\n•\nSageMaker is a fully managed service that allows developers and data scientists \nto quickly build, train, and deploy ML models.\n•\nSageMaker Orchestrator is a feature of SageMaker that helps automate the ex-\necution of ML workflows, manage dependencies between steps, and ensure the \nreproducibility and scalability of model training and deployment pipelines. Other \nsimilar tools are Prefect, Dagster, Metaflow, and Airflow.\n•\nCloudFormation is a service that allows you to model and set up your AWS re-\nsources so that you can spend less time managing them and more time focusing \non your applications. It automates the process of provisioning AWS infrastructure \nusing templates.\nBefore running the ML pipelines, the last step is to containerize the code and prepare a Docker \nimage that packages our dependencies and code.\n",
      "content_length": 2384,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 453,
      "content": "MLOps and LLMOps\n424\nContainerize the code using Docker\nSo far, we have defined our infrastructure, MongoDB, Qdrant, and AWS, for storage and computing. \nThe last step is to find a way to take our code and run it on top of this infrastructure. The most \npopular solution is Docker, a tool that allows us to create an isolated environment (a container) \nthat contains everything we need to run our application, such as system dependencies, Python \ndependencies, and the code.\nWe defined our Docker image at the project’s root in the Dockerfile. This is the standard naming \nconvention for Docker. Before digging into the code, if you want to build the Docker image your-\nself, ensure that you have Docker installed on your machine. If you don’t have it, you can install it \nby following the instructions provided here: https://docs.docker.com/engine/install. Now, \nlet’s look at the content of the Dockerfile step by step.\nThe Dockerfile begins by specifying the base image, which is a lightweight version of Python 3.11 \nbased on the Debian Bullseye distribution. The environment variables are then set up to configure \nvarious aspects of the container, such as the workspace directory, turning off Python bytecode \ngeneration, and configuring Python to output directly to the terminal. Additionally, the version of \nPoetry to be installed is specified, and a few environment variables are set to ensure that package \ninstallations are non-interactive, which is vital for automated builds.\nFROM python:3.11-slim-bullseye AS release\nENV WORKSPACE_ROOT=/app/\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\nENV POETRY_VERSION=1.8.3\nENV DEBIAN_FRONTEND=noninteractive\nENV POETRY_NO_INTERACTION=1\nNext, we install Google Chrome in the container. The installation process begins by updating the \npackage lists and installing essential tools like gnupg, wget, and curl. The Google Linux signing \nkey is added, and the Google Chrome repository is configured. After another package list update, \nthe stable version of Google Chrome is installed. The package lists are removed after installation \nto keep the image as small as possible.\nRUN apt-get update -y && \\\n    apt-get install -y gnupg wget curl --no-install-recommends && \\\n    wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | \ngpg --dearmor -o /usr/share/keyrings/google-linux-signing-key.gpg && \\\n",
      "content_length": 2373,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 454,
      "content": "Chapter 11\n425\n    echo \"deb [signed-by=/usr/share/keyrings/google-linux-signing-key.gpg] \nhttps://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.\nlist.d/google-chrome.list && \\\n    apt-get update -y && \\\n    apt-get install -y google-chrome-stable && \\\n    rm -rf /var/lib/apt/lists/*\nFollowing the Chrome installation, other essential system dependencies are installed. Once these \npackages are installed, the package cache is cleaned up to reduce the image size further.\nRUN apt-get update -y \\\n    && apt-get install -y --no-install-recommends build-essential \\\n    gcc \\\n    python3-dev \\\n    build-essential \\\n    libglib2.0-dev \\\n    libnss3-dev \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\nPoetry, the dependency management tool, is then installed using pip. The --no-cache-dir option \nprevents pip from caching packages, helping to keep the image smaller. After installation, Poetry \nis configured to use up to 20 parallel workers when installing packages, which can speed up the \ninstallation process.\nRUN pip install --no-cache-dir \"poetry==$POETRY_VERSION\"\nRUN poetry config installer.max-workers 20\nThe working directory inside the container is set to WORKSPACE_ROOT, which defaults to /app/, \nwhere the application code will reside. The pyproject.toml and poetry.lock files define the \nPython’s project dependencies and are copied into this directory.\nWORKDIR $WORKSPACE_ROOT\nCOPY pyproject.toml poetry.lock $WORKSPACE_ROOT\nWith the dependency files in place, the project’s dependencies are installed using Poetry. The \nconfiguration turns off the creation of a virtual environment, meaning the dependencies will be \ninstalled directly into the container’s Python environment. The installation excludes development \ndependencies and prevents caching to minimize space usage. \n",
      "content_length": 1822,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 455,
      "content": "MLOps and LLMOps\n426\nAdditionally, the poethepoet plugin is installed to help manage tasks within the project. Finally, \nany remaining Poetry cache is removed to keep the container as lean as possible.\nRUN poetry config virtualenvs.create false && \\\n    poetry install --no-root --no-interaction --no-cache --without dev && \n\\\n    poetry self add 'poethepoet[poetry_plugin]' && \\\n    rm -rf ~/.cache/pypoetry/cache/ && \\\n    rm -rf ~/.cache/pypoetry/artifacts/\nIn the final step, the entire project directory from the host machine is copied into the container’s \nworking directory. This step ensures that all the application files are available within the container.\nOne important trick when writing a Dockerfile is to decouple your installation steps from copy-\ning the rest of the files. This is useful because each Docker command is cached and layered on \ntop of each other. Thus, whenever you change one layer when rebuilding the Docker image, all \nthe layers below the one altered are executed again. Because you rarely change your system and \nproject dependencies but mostly change your code, copying your project files in the last step makes \nrebuilding Docker images fast by taking advantage of the caching mechanism’s full potential.\nCOPY . $WORKSPACE_ROOT\nThis Dockerfile is designed to create a clean, consistent Python environment with all necessary \ndependencies. It allows the project to run smoothly in any environment that supports Docker.\nThe last step is to build the Docker image and push it to the ECR created by ZenML. To build the \nDocker image from the root of the project, run the following:\ndocker buildx build --platform linux/amd64 -t llmtwin -f Dockerfile .\nWe must build it on a Linux platform as the Google Chrome installer we used inside Docker works \nonly on a Linux machine. Even if you use a macOS or Windows machine, Docker can emulate a \nvirtual Linux container.\nThe tag of the newly created Docker image is llmtwin. We also provide this build command \nunder a poethepoet command:\npoetry poe build-docker-image\nNow, let’s push the Docker image to ECR. To do so, navigate to your AWS console and then to \nthe ECR service. From there, find the newly created ECR repository. It should be prefixed with \nzenml-*, as shown here:\n",
      "content_length": 2260,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 456,
      "content": "Chapter 11\n427\nFigure 11.7: AWS ECR example\nThe first step is to authenticate to ECR. For this to work, ensure that you have the AWS CLI installed \nand configured with your admin AWS credentials, as explained in Chapter 2:\nAWS_REGION=<your_region> # e.g. AWS_REGION=eu-central-1\nAWS_ECR_URL=<your_acount_id>\naws ecr get-login-password --region ${AWS_REGION}| docker login --username \nAWS --password-stdin ${AWS_ECR_URL}\nYou can get your current AWS_REGION by clicking on the toggle in the top-right corner, as seen in \nFigure 11.8. Also, you can copy the ECR URL to fill the AWS_ECR_URL variable from the main AWS \nECR dashboard, as illustrated in Figure 11.7. After running the previous command, you should see \nthe message Login Succeeded on the CLI.\nFigure 11.8: AWS region and account details\n",
      "content_length": 797,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 457,
      "content": "MLOps and LLMOps\n428\nNow we have to add another tag to the llmtwin Docker image that signals the Docker registry \nwe want to push it to:\ndocker tag llmtwin ${AWS_ECR_URL}:latest\nFinally, we push it to ECR by running:\ndocker push ${AWS_ECR_URL}:latest\nAfter the upload is finished, return to your AWS ECR dashboard and open your ZenML repository. \nThe Docker image should appear, as shown here:\nFigure 11.9: AWS ECR repository example after the Docker image is pushed\nFor every change in the code that you need to ship and test, you would have to go through all \nthese steps, which are tedious and error-prone. The Adding LLMOps to the LLM Twin section of this \nchapter will teach us how to automate these steps within the CD pipeline using GitHub Actions. \nStill, we first wanted to go through them manually to fully understand the behind-the-scenes \nprocess and not treat it as a black box. Understanding these details is vital for debugging your CI/\nCD pipelines, where you must understand the error messages and how to fix them.\nNow that we have built our Docker image and pushed it to AWS ECR, let’s deploy it to AWS.\nRun the pipelines on AWS\nWe are very close to running the ML pipelines on AWS, but we have to go through a few final steps. \nLet’s switch from the default ZenML stack to the AWS one we created in this chapter. From the \nroot of your project, run the following in the CLI:\nzenml stack set aws-stack\n",
      "content_length": 1420,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 458,
      "content": "Chapter 11\n429\nReturn to your AWS ECR ZenML repository and copy the image URI as shown in Figure 11.9. \nThen, go to the configs directory, open the configs/end_to_end_data.yaml file, and update \nthe settings.docker.parent_image attribute with your ECR URL, as shown below:\nsettings:\n  docker:\n    parent_image: <YOUR ECR URL> #e.g., 992382797823.dkr.ecr.eu-central-1.\namazonaws.com/zenml-rlwlcs:latest\n    skip_build: True\nWe’ve configured the pipeline to always use the latest Docker image available in ECR. This means \nthat the pipeline will automatically pick up the latest changes made to the code whenever we \npush a new image.\nWe must export all the credentials from our .env file to ZenML secrets, a feature that safely stores \nyour credentials and makes them accessible within your pipelines:\npoetry poe export-settings-to-zenml\nThe last step is setting up to run the pipelines asynchronously so we don’t have to wait until they \nare finished, which might result in timeout errors:\nzenml orchestrator update aws-stack --synchronous=False\nNow that ZenML knows to use the AWS stack, our custom Docker image, and has access to our \ncredentials, we are finally done with the setup. Run the end-to-end-data-pipeline with the \nfollowing command:\npoetry poe run-end-to-end-data-pipeline\nNow you can go to ZenML Cloud → Pipelines → end_to_end_data and open the latest run. On \nthe ZenML dashboard, you can visualize the latest state of the pipeline, as seen in Figure 11.10. \nNote that this pipeline runs all the data-related pipelines in a single run.\n",
      "content_length": 1553,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 459,
      "content": "MLOps and LLMOps\n430\nIn the Adding LLMOps to the LLM Twin section, we will explain why we compressed all the steps \ninto a single pipeline.\nFigure 11.10: ZenML example of running the end-to-end-data-pipeline\nYou can click on any running block and find details about the run, the code used for that specific \nstep, and the logs for monitoring and debugging, as illustrated in Figure 11.11:\n",
      "content_length": 389,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 460,
      "content": "Chapter 11\n431\nFigure 11.11: ZenML step metadata example\nTo find even more details about the runs, you can go to AWS SageMaker. In the left panel, click \nSageMaker dashboard, and on the right, in the Processing column, click on the green Running\nsection, as shown in Figure 11.12. \nTo run other pipelines, you have to update the settings.docker.parent_image\nattribute in their config file under the configs/ directory.\n",
      "content_length": 419,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 461,
      "content": "MLOps and LLMOps\n432\nThis will open a list of all the processing jobs that execute your ZenML pipelines.\nFigure 11.12: SageMaker dashboard\nTroubleshooting the ResourceLimitExceeded error after running \na ZenML pipeline on SageMaker\nLet’s assume, you’ve encountered a ResourceLimitExceeded error after running a ZenML pipeline \non SageMaker using the AWS stack. In this case, you have to explicitly ask AWS to give you access \nto a specific type of AWS EC2 VM.\nZenML uses, by default, ml.t3.medium EC2 machines, which are part of the AWS freemium tier. \nHowever, some AWS accounts cannot access these VMs by default. To check your access, search \nyour AWS console for Service Quotas. \nIf you want to run the pipelines locally again, use the following CLI command:\npoetry poe set-local-stack\nIf you want to disconnect from the ZenML cloud dashboard and use the local version \nagain, run the following:\nzenml disconnect\n",
      "content_length": 917,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 462,
      "content": "Chapter 11\n433\nThen, in the left panel, click on AWS services, search for Amazon SageMaker, and then for ml.t3.\nmedium. In Figure 11.13, you can see our quotas for these types of machines. If yours is 0, you should \nrequest that AWS increase them to numbers similar to those from Figure 11.13 in the Applied \naccount-level quota value column. The whole process is free of charge and only requires a few \nclicks. Unfortunately, you might have to wait for a few hours up to one day until AWS accepts \nyour request.\nFigure 11.13: SageMaker—ml.t3.medium expected quotas\nYou can find step-by-step instructions on how to solve this error and request new quotas at this \nlink: https://repost.aws/knowledge-center/sagemaker-resource-limit-exceeded-error.\nIf you changed the values from your .env file and want to update the ZenML secrets \nwith them, first run the following CLI command to delete the old secrets:\npoetry poe delete-settings-zenml\nThen, you can export them again by running:\npoetry poe export-settings-to-zenml\n",
      "content_length": 1018,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 463,
      "content": "MLOps and LLMOps\n434\nAdding LLMOps to the LLM Twin\nIn the previous section, we saw how to set up the infrastructure for the LLM Twin project by \nmanually building the Docker image and pushing it to ECR. We want to automate the entire \nprocess and implement a CI/CD pipeline using GitHub Actions and a CT pipeline using ZenML. \nAs mentioned earlier, implementing a CI/CD/CT pipeline ensures that each feature pushed to \nmain branches is consistent and tested. Also, by automating the deployment and training, you \nsupport collaboration, save time, and reduce human errors.\nFinally, at the end of the section, we will show you how to implement a prompt monitoring pipe-\nline using Opik from Comet ML and an alerting system using ZenML. This prompt monitoring \npipeline will help us debug and analyze the RAG and LLM logic. As LLM systems are non-de-\nterministic, capturing and storing the prompt traces is essential for monitoring your ML logic.\nBefore diving into the implementation, let’s start with a quick section on the LLM Twin’s CI/CD \npipeline flow.\nLLM Twin’s CI/CD pipeline flow\nWe have two environments: staging and production. When developing a new feature, we cre-\nate a new branch out of the staging branch and develop solely on that one. When we are done \nand consider the feature finished, we open a pull request (PR) to the staging branch. After the \nfeature branch is accepted, it is merged into the staging branch. This is a standard workflow in \nmost software applications. There might be variations, like adding a dev environment, but the \nprinciples remain the same.\nAs illustrated in Figure 11.14, the CI pipeline is triggered when the PR opens. At this point, we \ntest the feature branch for linting and formatting errors. Also, we run a gitleaks command to \ncheck for credentials and sensitive information that was committed by mistake. If the linting, \nformatting, and gitleaks steps pass (also known as static analysis), we run the automated tests. \nNote that the static analysis steps run faster than the automated tests. Thus, the order matters. \nThat’s why adding the static analysis steps at the beginning of the CI pipeline is good practice. \nWe propose the following order of the CI steps:\n•\ngitleaks checks\n•\nLinting checks\n•\nFormatting checks\n• \nAutomated testing, such as unit and integration tests\n",
      "content_length": 2333,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 464,
      "content": "Chapter 11\n435\nIf any check fails, the CI pipeline fails, and the developer who created the PR cannot merge it into \nthe staging branch until it fixes the issues.\nImplementing a CI pipeline ensures that new features follow the repository’s standards and \ndon’t break existing functionality. The exact process repeats when we plan to merge the staging \nbranch into the production one. We open a PR, and the CI pipeline is automatically executed \nbefore merging the staging branch into production.\nFigure 11.14: CI/CD pipelines flow\nThe CD pipeline runs after the branch is merged. For example, after the feature branch is merged \ninto staging, the CD pipeline takes the code from the staging branch, builds a new Docker im-\nage, and pushes it to the AWS ECR Docker repository. When running future pipeline runs in \nthe staging environment, it will use the latest Docker image that was built by the CD pipeline. \nThe exact process happens between staging and production. Still, the key difference is that the \nstaging environment exists as an experimental place where the QA team and stakeholders can \nfurther manually test the new feature along with what is automatically tested in the CI pipeline.\n",
      "content_length": 1198,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 465,
      "content": "MLOps and LLMOps\n436\nMore on formatting errors\nFormatting errors relate to the style and structure of your code, ensuring that it adheres to a \nconsistent visual layout. This can include the placement of spaces, indentation, line length, and \nother stylistic elements.\nThe main purpose of formatting is to make your code more readable and maintainable. Consistent \nformatting helps teams work together more effectively, as the code looks uniform, regardless of \nwho wrote it. Examples of formatting errors are:\n•\nIncorrect indentation (e.g., mixing spaces and tabs)\n•\nLines that are too long (e.g., exceeding 79 or 88 characters, depending on your style guide)\n•\nMissing or extra spaces around operators or after commas\nMore on linting errors\nLinting errors relate to potential issues in your code that could lead to bugs, inefficiencies, or \nnon-adherence to coding standards beyond just style. Linting checks often involve static analysis \nof the code to catch things like unused variables, undefined names, or questionable practices.\nLinting’s main goal is to catch potential errors or bad practices early in the development process, \nimproving code quality and reducing the likelihood of bugs. Examples of linting errors are:\n•\nUnused imports or variables\n•\nUndefined variables or functions are being used\n•\nPotentially dangerous code (e.g., using == instead of is for checking against None)\nWe use Ruff, a versatile tool for formatting and linting. It incorporates checks for common for-\nmatting issues and PEP 8 compliance, as well as deeper linting checks for potential errors and \ncode quality problems. Also, it is written in Rust, making it fast for big codebases.\nBefore implementing what we’ve explained above, let’s examine the core principles of GitHub \nActions.\nIn our repository, we used only a main branch, which reflects production, and fea-\nture branches to push new work. We did this to keep things simple, but the same \nprinciples apply. To extend the flow, you must create a staging branch and add it \nto the CD pipeline.\n",
      "content_length": 2044,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 466,
      "content": "Chapter 11\n437\nQuick overview of GitHub Actions\nGitHub Actions is a CI/CD platform provided by GitHub that allows developers to automate their \nworkflows directly within a GitHub repository. It enables users to build, test, and deploy their \ncode directly from GitHub by defining workflows in YAML files. Since it’s part of GitHub, it works \nseamlessly with repositories, issues, PRs, and other GitHub features. Here are the key components \nyou should know about:\n• \nWorkflows: A workflow is an automated process defined in a YAML file located in your \nrepository’s .github/workflows directory. It specifies what should happen (e.g., build, \ntest, and deploy) and when (e.g., on push, on PR).\n•\nJobs: Workflows are made up of jobs, which are groups of steps that execute on the same \nrunner. Each job runs in its own virtual environment.\n•\nSteps: Jobs are made up of multiple independent steps, which can be actions or shell \ncommands.\n• \nActions: Actions are reusable commands or scripts. You can use pre-built actions from \nGitHub Marketplace or create your own. You can think of them as Python functions.\n• \nRunners: Runners are the servers that run your jobs. GitHub provides hosted runners \n(Linux, Windows, macOS), or you can even self-host your runners.\nA workflow is described using YAML syntax. For example, a simple workflow that clones the \ncurrent GitHub repository and installs Python 3.11 on an Ubuntu machine looks like this:\nname: Example\non: [push]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n         - name: Checkout\n           uses: actions/checkout@v3\n         - name: Setup Python\n           uses: actions/setup-python@v3\n           with:\n               python-version: \"3.11\"\n",
      "content_length": 1708,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 467,
      "content": "MLOps and LLMOps\n438\nThe workflows are triggered by events like push, pull_request, or schedule. For example, you \nmight trigger a workflow every time code is pushed to a specific branch. Now that we understand \nhow GitHub Actions works, let’s look at the LLM Twin’s CI pipeline.\nThe CI pipeline\nThe LLM Twin’s CI pipeline is split into two jobs:\n• \nA QA job that looks for formatting and linting errors using Ruff. Also, it runs a gitleaks\nstep to scan for leaked secrets throughout our repository.\n• \nA test job that runs all our automatic tests using Pytest. In our use case, we implemented \njust a dummy test to showcase the CI pipeline, but using the structure from this book, \nyou can easily extend it with real tests for your use case.\nGitHub Actions CI YAML file\nThe YAML file sits under .github/workflows/ci.yaml. It begins by defining the workflow’s \nname as CI, as you can see in the following snippet. This label will be used to identify the work-\nflow within GitHub’s Actions interface. Next, the section specifies that the workflow should be \ntriggered whenever a pull_request event occurs. Hence, the CI workflow will automatically run \nwhenever a PR is opened, synchronized, or reopened.\nname: CI\non:\n  pull_request:\nThe concurrency section ensures that only one instance of this workflow runs for a given reference \n(like a branch) at any given time. The group field is defined using GitHub’s expression syntax to \ncreate a unique group name based on the workflow and the reference. The cancel-in-progress: \ntrue line ensures that if a new workflow run is triggered before the previous one finishes, the \nprevious run is canceled. This is particularly useful to prevent redundant executions of the same \nworkflow.\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\nThe workflow defines two separate jobs: qa and test. Each job runs on the latest version of Ubuntu, \nspecified by runs-on: ubuntu-latest.\n",
      "content_length": 1959,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 468,
      "content": "Chapter 11\n439\nThe first job, named QA, is responsible for quality assurance tasks like code checks and format-\nting verification. Within the qa job, the first step is to check out the repository’s code using the \nactions/checkout@v3 action. This step is necessary to ensure that the job has access to the code \nthat needs to be analyzed.\njobs:\n  qa:\n    name: QA\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\nThe next step is to set up the Python environment. This is done using the actions/setup-python@\nv3 action, with the Python version specified as \"3.11\". This step ensures that the subsequent \nsteps in the job will run in the correct Python environment.\n      - name: Setup Python\n        uses: actions/setup-python@v3\n        with:\n          python-version: \"3.11\"\nThe workflow then installs Poetry using the abatilo/actions-poetry@v2 action, specifying the \nversion of Poetry as 1.8.3:\n      - name: Install poetry\n        uses: abatilo/actions-poetry@v2\n        with:\n          poetry-version: 1.8.3\nOnce Poetry is set up, the workflow installs the project’s development dependencies using the \npoetry install --only dev command. Additionally, the workflow adds the poethepoet plugin \nfor Poetry, which will be used to run predefined tasks more conveniently within the project.\n      - name: Install packages\n        run: |\n          poetry install --only dev\n          poetry self add 'poethepoet[poetry_plugin]'\n",
      "content_length": 1474,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 469,
      "content": "MLOps and LLMOps\n440\nThe qa job then runs several quality checks on the code. The first check uses a tool called gitleaks\nto scan for secrets in the codebase, ensuring that no sensitive information is accidentally com-\nmitted:\n      - name: gitleaks check\n        run: poetry poe gitleaks-check\nFollowing the gitleaks check, the workflow runs a linting process to enforce coding standards \nand best practices in the Python code. This is achieved through the poetry poe lint-check\ncommand, which uses Ruff under the hood.\n      - name: Lint check [Python]\n        run: poetry poe lint-check\nThe last step in the qa job is a format check, which ensures that the Python code is properly for-\nmatted according to the project’s style guidelines. This is done using the poetry poe format-\ncheck command, which uses Ruff under the hood.\n      - name: Format check [Python]\n        run: poetry poe format-check\nThe second job defined in the workflow is the test job, which also runs on the latest version \nof Ubuntu. Like the qa job, it starts by checking out the code from the repository and installing \nPython 3.11 and Poetry 1.8.3.\n  test:\n    name: Test\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n      …\nAfter setting up the system dependencies, the test job installs all the project’s dependencies with \nthe poetry install command. As we want to run the tests, this time, we need to install all the \ndependencies that are required to run the application.\n      - name: Install packages\n        run: |\n",
      "content_length": 1551,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 470,
      "content": "Chapter 11\n441\n          poetry install –-without aws\n          poetry self add 'poethepoet[poetry_plugin]'\nFinally, the test job runs the project’s tests using the poetry poe test command. This step \nensures that all tests are executed and provides feedback on whether the current code changes \nbreak any functionality.\n      - name: Run tests\n        run: |\n          echo \"Running tests...\"\n          poetry poe test\nIf any of the steps from the QA or test jobs fail, the GitHub Actions workflow will fail, resulting \nin the PR not being able to be merged until the issue is fixed. By taking this approach, we ensure \nthat all the new features added to the main branches respect the standard of the project and that \nit doesn’t break existing functionality through automated tests.\nFigure 11.15 shows the CI pipeline in the Actions tab of the GitHub repository. It was run after a \ncommit with the message feat: Add Docker image and CD pipeline and ran the two jobs de-\nscribed above, QA and Test.\nFigure 11.15: GitHub Actions CI pipeline run example\n",
      "content_length": 1054,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 471,
      "content": "MLOps and LLMOps\n442\nThe CD pipeline\nThe CD pipeline will automate the Docker steps we manually performed in the Deploying the \nLLM Twin’s pipelines to the cloud section, which are:\n•\nSet up Docker.\n•\nLog in to AWS.\n•\nBuild the Docker image.\n•\nPush the Docker image to AWS ECR.\nWith that in mind, let’s look at the GitHub Actions YAML file, which sits under .github/workflows/\ncd.yaml. It begins by naming the workflow CD and specifying the trigger for this workflow. The \ntrigger is any push to the repository’s main branch. This workflow will automatically run when \nnew code is pushed to the main branch, usually when a PR is merged into the main branch. The \non.push configuration sets up the trigger:\nname: CD\non:\n  push:\n    branches:\n      - main\nThe workflow then defines a single job named Build & Push Docker Image:\njobs:\n  build:\n    name: Build & Push Docker Image\n    runs-on: ubuntu-latest\nThe first step within the job is to check out the repository’s code.\nsteps:\n  - name: Checkout Code\n    uses: actions/checkout@v3\nAfter checking out the code, the workflow sets up docker buildx, a Docker CLI plugin that extends \nDocker’s build capabilities with features like multi-platform builds and cache import/export:\n- name: Set up Docker Buildx\n  uses: docker/setup-buildx-action@v3\n",
      "content_length": 1294,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 472,
      "content": "Chapter 11\n443\nThe next step involves configuring the AWS credentials. This step is crucial for interacting with \nAWS services, such as Amazon Elastic Container Registry (ECR), where the Docker images will \nbe pushed. The AWS access key, secret access key, and region are securely retrieved from the \nrepository’s secrets to authenticate the workflow with AWS. This ensures the workflow has the \nnecessary permissions to push Docker images to the ECR repository. We will show you how to \nconfigure these secrets after wrapping up with the YAML file:\n- name: Configure AWS credentials\n  uses: aws-actions/configure-aws-credentials@v1\n  with:\n    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n    aws-region: ${{ secrets.AWS_REGION }}\nOnce the AWS credentials are configured, the workflow logs in to Amazon ECR. This step is es-\nsential for authenticating the Docker CLI with the ECR registry, allowing subsequent steps to \npush images to the registry:\n- name: Login to Amazon ECR\n  id: login-ecr\n  uses: aws-actions/amazon-ecr-login@v1\nThe final step in the workflow involves building the Docker image and pushing it to the Ama-\nzon ECR repository. This is accomplished using the docker/build-push-action@v6 action. The \ncontext specifies the build context, which is typically the repository’s root directory. The file\noption points to the Dockerfile, which defines how the image should be built. The tags section \nassigns tags to the image, including the specific commit SHA and the latest tag, which is a com-\nmon practice for identifying the most recent version of the image. The push option is set to true, \nmeaning the image will be uploaded to ECR after it is built:\n- name: Build images & push to ECR\n  id: build-image\n  uses: docker/build-push-action@v6\n  with:\n    context: .\n    file: ./Dockerfile\n    tags: |\n      ${{ steps.login-ecr.outputs.registry }}/${{ secrets.AWS_ECR_NAME \n}}:${{ github.sha }}\n",
      "content_length": 1982,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 473,
      "content": "MLOps and LLMOps\n444\n      ${{ steps.login-ecr.outputs.registry }}/${{ secrets.AWS_ECR_NAME \n}}:latest\n    push: true\nTo conclude, the CD pipeline authenticates to AWS, builds the Docker image, and pushes it to \nAWS ECR. The Docker image is pushed with latest and the commit’s SHA tag. By doing so, we \ncan always use the latest image and point to the commit of the code from which the image was \ngenerated.\nAlso, in our code, we have only a main branch, which reflects our production environment. But \nyou, as a developer, have the power to extend this functionality with a staging and dev environ-\nment. You just have to add the name of the branches in the on.push.branches configuration at \nthe beginning of the YAML file.\nIn Figure 11.16, you can observe how the CD pipeline looks after a PR is merged into the production \nbranch. As seen before, we only have the Build & Push Docker Image job here.\nFigure 11.16: GitHub Actions CD pipeline run example\n",
      "content_length": 957,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 474,
      "content": "Chapter 11\n445\nThe last step in setting up the CI/CD pipeline is to test it and see how it works.\nTest out the CI/CD pipeline\nTo test the CI/CD pipelines yourself, you must fork the LLM-Engineering repository to have full \nwrite access to the GitHub repository. Here is the official tutorial on how to fork a GitHub project: \nhttps://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-\nwith-forks/fork-a-repo\nThe last step is to set up a few secrets that will allow the CD pipeline to log in to AWS and point \nto the right ECR resource. To do so, go to the Settings tab at the top of the forked repository in \nGitHub. In the left panel, in the Security section, click on the Secrets and Variables toggle and, \nfinally, on Actions. Then, on the Secrets tab, create four repository secrets, as shown in Figure \n11.17. These secrets will be securely stored and accessible only by the GitHub Actions CD pipeline.\nThe AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are the AWS credentials you used across \nthe book. In Chapter 2, you see how to create them. The AWS_REGION (e.g., eu-central-1) and \nAWS_ECR_NAME are the same ones used in the Deploying the LLM Twin’s pipelines to the cloud \nsection.\nTo trigger the CI pipeline, create a feature branch, modify the code or documentation, and create \na PR to the main branch. To trigger the CD pipeline, merge the PR into the main branch. \nFor the AWS_ECR_NAME, you should configure only the name of the repository (e.g., \nzenml-vrsopg) and not the full URI (e.g., 992382797823.dkr.ecr.eu-central-1.\namazonaws.com/zenml-vrsopg), as seen in the image below:\nFigure 11.17: Configuring only repository name\n",
      "content_length": 1667,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 475,
      "content": "MLOps and LLMOps\n446\nAfter the CD GitHub Actions are complete, check the ECR repository to see whether the Docker \nimage was pushed successfully.\nFigure 11.18: GitHub Actions secrets\nIf you need more details on how to set up GitHub Actions secrets, we recommend checking out \ntheir official documentation: https://docs.github.com/en/actions/security-for-github-\nactions/security-guides/using-secrets-in-github-actions\nThe CT pipeline\nTo implement the CT pipeline, we will leverage ZenML. Once ZenML (or other orchestrators \nsuch as Metaflow, Dagster, or Airflow) orchestrates all your pipelines and your infrastructure is \ndeployed, you are very close to reaching CT.\nRemember the core difference between the CI/CD and CT pipelines. The CI/CD pipeline takes care \nof testing, building, and deploying your code—a dimension that any software program has. The \nCT pipeline leverages the code managed by the CI/CD pipeline to automate your data, training, \nand model-serving process, where the data and model dimensions are present only in the AI world.\nBefore diving into the implementation, we want to highlight two design choices that made reach-\ning CT simple:\n",
      "content_length": 1161,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 476,
      "content": "Chapter 11\n447\n•\nThe FTI architecture: A modular system with clear interfaces and components made it \neasy to capture the relationship between the pipelines and automate them.\n•\nStarting with an orchestrator since day 0: We started with ZenML at the beginning of \nthe project’s development. Early on, we only used it locally. But it acted as an entry point \nfor our pipelines and a way to monitor their execution. Doing so forced us to decouple \neach pipeline and transfer the communication between them solely through various types \nof data storage, such as the data warehouse, feature store, or artifact store. As we have \nleveraged ZenML since day 0, we got rid of implementing a tedious CLI to configure our \napplication. Instead, we did it directly through YAML configuration files out of the box.\nIn Figure 11.19, we can see all the pipelines that we have to chain together to fully automate our \ntraining and deployment. The pipelines aren’t new; they aggregate everything we’ve covered \nthroughout this book. Thus, at this point, we will treat them as black boxes that interact with \neach other.\nFigure 11.19: CT pipeline\n",
      "content_length": 1130,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 477,
      "content": "MLOps and LLMOps\n448\nFor the LLM Twin’s CT pipeline, we have to discuss the initial trigger that starts the pipelines \nand how the pipelines are triggered by each other.\nInitial triggers\nAs illustrated in Figure 11.18, we initially want to trigger the data collection pipeline. Usually, the \ntriggers can be of three types:\n•\nManual triggers: Done through the CLI or the orchestrator’s dashboard, in our case, \nthrough the ZenML dashboard. Manual triggers are still extremely powerful tools, as \nyou need just one action to start the whole ML system, from data gathering to deploy-\nment, instead of fiddling with dozens of scripts that you might configure wrong or run \nin an invalid order.\n•\nREST API triggers: You can call a pipeline by an HTTP request. This is extremely useful \nwhen integrating your ML pipelines with other components. For example, you can have \na watcher constantly looking for new articles. It triggers the ML logic using this REST API \ntrigger when it finds some. To find more details on this feature, check out this tutorial on \nZenML’s documentation: https://docs.zenml.io/v/docs/how-to/trigger-pipelines/\ntrigger-a-pipeline-from-rest-api.\n•\nScheduled triggers: Another common approach is to schedule your pipeline to run con-\nstantly on a fixed interval. For example, depending on your use case, you can schedule your \npipeline to run daily, hourly, or every minute. Most of the orchestrators, ZenML included, \nprovide a cron expression interface where you can define your execution frequency. In the \nfollowing example from ZenML, the pipeline is scheduled every hour:\n Schedule(cron_expression=\"* * 1 * *\")\nWe chose a manual trigger for our LLM Twin use case as we don’t have other components to lever-\nage the REST API triggers. Also, as the datasets are generated from a list of static links defined in \nthe ZenML configs, running them on a schedule doesn’t make sense as they would always yield \nthe same results.\nBut a possible next step for the project is to implement a watcher that monitors for new articles. \nWhen it finds any, it generates a new config and triggers the pipelines through the REST API. An-\nother option is implementing the watcher as an additional pipeline and leveraging the schedule \ntriggers to look daily for new data. If it finds any, it executes the whole ML system; otherwise, it \nstops.\n",
      "content_length": 2349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 478,
      "content": "Chapter 11\n449\nThe conclusion is that once you can manually trigger all your ML pipelines through a single \ncommand, you can quickly adapt it to more advanced and complex scenarios.\nTrigger downstream pipelines\nTo keep things simple, we sequentially chained all the pipelines. More concretely, when the data \ncollection pipeline has finished, it will trigger the feature pipeline. When the feature pipeline has \nbeen completed successfully, it triggers the dataset generation pipeline, and so on. You can make \nthe logic more complex, like scheduling the generate instruct dataset pipeline to run daily, check-\ning the amount of new data in the Qdrant vector DB, and starting only if it has enough new data. \nFrom this point, you can further tweak the system’s parameters and optimize them to reduce costs.\nTo trigger all the pipelines in one go, we created one master pipeline that aggregates everything \nin one entry point:\n@pipeline\ndef end_to_end_data(\n    author_links: list[dict[str, str | list[str]]], … # Other paramaters…\n) -> None:\n    wait_for_ids = []\n    for author_data in author_links:\n        last_step_invocation_id = digital_data_etl(\n            user_full_name=author_data[\"user_full_name\"], links=author_\ndata[\"links\"]\n        )\n        wait_for_ids.append(last_step_invocation_id)\n    author_full_names = [author_data[\"user_full_name\"] for author_data in \nauthor_links]\n    wait_for_ids = feature_engineering(author_full_names=author_full_\nnames, wait_for=wait_for_ids)\n    generate_instruct_datasets(…)\n       training(…)\n       deploy(…)\n",
      "content_length": 1561,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 479,
      "content": "MLOps and LLMOps\n450\nTo keep the function light, we added all the logic up to computing the features. But, as we sug-\ngested in the code snippet above, you can easily add the instruction dataset generation, training, \nand deploy logic to the parent pipeline to implement an end-to-end flow. By doing that, you can \nautomate everything from data collection to deploying the model.\nTo run the end-to-end pipeline, use the following poe command:\npoetry poe run-end-to-end-data-pipeline\nWhat we implemented is not the best approach, as it compresses all the steps into a single monolith \npipeline (which we want to avoid), as illustrated in Figure 11.20. Usually, you want to keep each \npipeline isolated and use triggers to start downstream pipelines. This makes the system easier \nto understand, debug, and monitor.\nFigure 11.20: End-to-end pipeline illustrated in ZenML’s dashboard\n",
      "content_length": 881,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 480,
      "content": "Chapter 11\n451\nUnfortunately, the ZenML cloud’s free trial has a limitation of a maximum of three pipelines. As \nwe have more, we avoided that limitation by compressing all the steps into a single pipeline. But \nif you plan to host ZenML yourself or buy their license, they offer the possibility to independent-\nly trigger a pipeline from another pipeline, as you can see in the code snippet below where we \ntriggered the feature engineering pipeline after the data collection ETL:\nfrom zenml import pipeline, step\n@pipeline \ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\nuser = get_or_create_user(user_full_name)\ncrawl_links(user=user, links=links)\ntrigger_feature_engineering_pipeline(user)\n@step \ndef trigger_feature_engineering_pipeline(user):\nrun_config = PipelineRunConfiguration(…)\nClient().trigger_pipeline(\"feature_engineering\", run_configuration=run_\nconfig)\n@pipeline\ndef feature_engineering(author_full_names: list[str]) -> list[str]:\n… # ZenML steps\nBy taking this approach, each pipeline will have its independent run, where one pipeline sequen-\ntially triggers the next one, as described at the beginning of this section. Note that this feature is \nnot unique to ZenML but is common in orchestrator tools. The principles we have learned so far \nhold. Only how we interact with the tool changes.\nPrompt monitoring\nWe will use Opik (from Comet ML) to monitor our prompts. But remember from the LLMOps\nsection earlier in this chapter that we are not interested only in the input prompt and generated \nanswer. \n",
      "content_length": 1544,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 481,
      "content": "MLOps and LLMOps\n452\nWe want to log the entire trace from the user’s input until the final result is available. Before \ndiving into the LLM Twin use case, let’s look at a simpler example:\nfrom opik import track\nimport openai\nfrom opik.integrations.openai import track_openai\nopenai_client = track_openai(openai.OpenAI())\n@track\ndef preprocess_input(text: str) -> str:\n    return text.strip().lower()\n@track\ndef generate_response(prompt: str) -> str:\n    response = openai_client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n@track\ndef postprocess_output(response: str) -> str:\n    return response.capitalize()\n@track(name=\"llm_chain\")\ndef llm_chain(input_text: str) -> str:\n    preprocessed = preprocess_input(input_text)\n    generated = generate_response(preprocessed)\n    postprocessed = postprocess_output(generated)\n    return postprocessed\nresult = llm_chain(\"Hello, do you enjoy reading the book?\")\nThe preceding code snippet reflects in a simplistic way what most LLM applications will look \nlike. You have the llm_chain() main function, which takes the initial input as a parameter and \nreturns the final result. \n",
      "content_length": 1241,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 482,
      "content": "Chapter 11\n453\nThen, you have preprocessing and postprocessing functions surrounding the actual LLM call. \nUsing the @track() decorator, we log the input and output of each function, which will ultimately \nbe aggregated into a single trace. By doing so, we will have access to the initial input text, the \ngenerated answer, and all the intermediary steps required to debug any potential issues using \nOpik’s dashboard.\nThe last step is to attach the necessary metadata for your use case to the current trace. As seen \nin the following code snippet, you can easily do that by calling the update() method, where you \ncan tag your trace or add any other metadata, such as the number of input tokens, through a \nPython dictionary:\nfrom opik import track, opik_context\n@track\ndef llm_chain(input_text):\n    # LLM chain code\n    # ...\n    opik_context.update_current_trace(\ntags=[\"inference_pipeline\"],\nmetadata={\n\"num_tokens\": compute_num_tokens(…)\n},\nfeedback_scores=[\n{\n\"name\": \"user_feedback\",\n\"value\": 1.0,\n\"reason\": \"The response was valuable and correct.\"\n},\n{\n\"name\": \"llm_judge_score\",\n\"value\": compute_llm_judge_score(…),\n\"reason\": \"Computing runtime metrics using an LLM Judge.\"\n}\n)\n",
      "content_length": 1188,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 483,
      "content": "MLOps and LLMOps\n454\nYou can expand on this idea and log various feedback scores. The most common is asking the user \nif the generated answer is valuable and correct. Another option is to compute various metrics \nautomatically through heuristics or LLM judges.\nFinally, let’s see how to add prompt monitoring to our LLM Twin project. First, look at Figure \n11.21 and remember our model-serving architecture. We have two microservices, the LLM and \nbusiness microservices. The LLM microservice has a narrow scope, as it only takes as input a \nprompt that already contains the user’s input and context and returns an answer that is usually \npost-processed. Thus, the business microservice is the right place to implement the monitoring \npipeline, as it coordinates the end-to-end flow. More concretely, Opik implementation will be in \nthe FastAPI server developed in Chapter 10.\nFigure 11.21: Inference pipeline serving architecture\n",
      "content_length": 931,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 484,
      "content": "Chapter 11\n455\nAs our implementation is already modular, using Opik makes it straightforward to log an end-\nto-end trace of a user’s request:\nfrom opik import track\n@track\ndef call_llm_service(query: str, context: str | None) -> str:\n    llm = LLMInferenceSagemakerEndpoint(…)\n    answer = InferenceExecutor(llm, query, context).execute()\n    return answer\n@track\ndef rag(query: str) -> str:\n    retriever = ContextRetriever()\n    documents = retriever.search(query, k=3 * 3)\n    context = EmbeddedChunk.to_context(documents)\n    answer = call_llm_service(query, context)\n    return answer\nThe rag() function represents your application’s entry point. All the other processing steps take \nplace in the ContextRetriever and InferenceExector classes. Also, by decorating the call_llm_\nservice() function, we can clearly capture the prompt sent to the LLM and its response.\nTo add more granularity to our trace, we can further decorate other functions containing pre- or \npost-processing steps, such as the ContextRetriever search function:\nclass ContextRetriever:\n     …\n   \n    @track\n    \n    def search(\n        self,\n        query: str,\n        k: int = 3,\n",
      "content_length": 1159,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 485,
      "content": "MLOps and LLMOps\n456\n        expand_to_n_queries: int = 3,\n    ) -> list:\n        query_model = Query.from_str(query)\n        query_model = self._metadata_extractor.generate(query_model)\n        … # Rest of the implementation\nOr even go further to the retrieval optimization methods, such as the self-query metadata ex-\ntractor, to add more granularity:\nclass SelfQuery:\n    @track\n    def generate(self, query: str) -> str:\n        …\n        return enhanced_query\nThe developer is responsible for deciding how much granularity the application needs for proper \ndebugging and analysis. As having detailed monitoring is healthy, monitoring everything can be \ndangerous as it adds too much noise and makes manually understanding the traces difficult. You \nmust find the right balance. A good rule of thumb is tracing the most critical functions, such as \nrag() and call_llm_service(), and gradually adding more granularity when needed.\nThe last step is to attach valuable metadata and tags to our traces. To do so, we will further en-\nhance the rag() function as follows:\n@track\ndef rag(query: str) -> str:\n    retriever = ContextRetriever()\n    documents = retriever.search(query, k=3 * 3)\n    context = EmbeddedChunk.to_context(documents)\n    answer, prompt = call_llm_service(query, context)\n    trace = get_current_trace()\n    trace.update(\ntags=[\"rag\"],\nmetadata={\n\"model_id\": settings.HF_MODEL_ID,\n   \"embedding_model_id\": settings.TEXT_EMBEDDING_MODEL_ID,\n",
      "content_length": 1461,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 486,
      "content": "Chapter 11\n457\n   \"temperature\": settings.TEMPERATURE_INFERENCE,\n   \"prompt_tokens\": compute_num_tokens(prompt),\n   \"total_tokens\": compute_num_tokens(answer),\n  \n}\n)\n    return answer\nThere are three main aspects that we should constantly monitor:\n•\nModel configuration: Here, we should consider both the LLM and other models used \nwithin the RAG layer. The most critical aspects of logging are the model IDs, but you can \nalso capture other important information that significantly impacts the generation, such \nas the temperature.\n•\nTotal number of tokens: It’s critical to constantly analyze the statistics of the number of \ntokens generated by your input prompts and total tokens, as this significantly impacts \nyour serving costs. For example, if the average of the total number of tokens generated \nsuddenly increases, it’s a strong signal that you have a bug in your system that you should \ninvestigate.\n•\nThe duration of each step: Tracking the duration of each step within your trace is essential \nto finding bottlenecks within your system. If the latency of a specific request is abnormally \nlarge, you quickly have access to a report that helps you find the source of the problem.\nAlerting\nUsing ZenML, you can quickly implement an alerting system on any platform of your liking, such \nas email, Discord, or Slack. For example, you can add a callback in your training pipeline to trigger \na notification when the pipeline fails or the training has finished successfully:\nfrom zenml import get_pipeline_context, pipeline\n@pipeline(on_failure=notify_on_failure)\ndef training_pipeline(…):\n…\nnotify_on_success()\n",
      "content_length": 1620,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 487,
      "content": "MLOps and LLMOps\n458\nImplementing the notification functions is straightforward. As seen in the code snippets below, \nyou have to get the alerter instance from your current stack, build the message as you see fit, \nand send it to your notification channel of choice:\nfrom zenml.client import Client\nalerter = Client().active_stack.alerter\ndef notify_on_failure() -> None:\n        alerter.post(message=build_message(status=\"failed\"))\n@step(enable_cache=False)\ndef notify_on_success() -> None:\n        alerter.post(message=build_message(status=\"succeeded\"))\nZenML and most orchestrators simplify implementing an alerter, as it’s a critical component \nin your MLOps/LLMOps infrastructure.\nSummary\nIn this chapter, we laid down the foundations with a theoretical section on DevOps. Then, we \nmoved on to MLOps and its core components and principles. Finally, we presented how LLMOps \ndiffers from MLOps by introducing strategies such as prompt monitoring, guardrails, and hu-\nman-in-the-loop feedback. Also, we briefly discussed why most companies would avoid training \nLLMs from scratch but choose to optimize them for their use case through prompt engineering \nor fine-tuning. At the end of the theoretical portion of the chapter, we learned what a CI/CD/CT \npipeline is, the three core dimensions of an ML application (code, data, model), and that, after \ndeployment, it is more critical than ever to implement a monitoring and alerting layer due to \nmodel degradation.\nNext, we learned how to deploy the LLM Twin’s pipeline to the cloud. We understood the infra-\nstructure and went step by step through deploying MongoDB, Qdrant, the ZenML cloud, and all \nthe necessary AWS resources to sustain the application. Finally, we learned how to Dockerize our \napplication and push our Docker image to AWS ECR, which will be used to execute the application \non top of AWS SageMaker.\nThe final step was to add LLMOps to our LLM Twin project. We began by implementing a CI/CD \npipeline with GitHub Actions. Then, we looked at our CT strategy by leveraging ZenML. \n",
      "content_length": 2055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 488,
      "content": "Chapter 11\n459\nFinally, we saw how to implement a monitoring pipeline using Opik from Comet ML and an \nalerting system using ZenML. These are the fundamental pillars in adding MLOps and LLMOps \nto any LLM-based application.\nThe framework we learned about throughout the book can quickly be extrapolated to other LLM \napplications. Even if we used the LLM Twin use case as an example, most of the strategies applied \ncan be adapted to other projects. Thus, we can get an entirely new application by changing the \ndata and making minor tweaks to the code. Data is the new oil, remember? \nBy finalizing this chapter, we’ve learned to build an end-to-end LLM application, starting with \ndata collection and fine-tuning until deploying the LLM microservice and RAG service. Through-\nout this book, we aimed to provide a thought framework to help you build and solve real-world \nproblems in the GenAI landscape. Now that you have it, we wish you good luck in your journey \nand happy building!\nReferences\n•\nGitLab. (2023, January 25). What is DevOps? | GitLab. GitLab. https://about.gitlab.\ncom/topics/devops/\n•\nHuyen, C. (2024, July 25). Building a generative AI platform. Chip Huyen. https://\nhuyenchip.com/2024/07/25/genai-platform.html\n•\nLightricks customer story: Building a recommendation engine from scratch. (n.d.). https://\nwww.qwak.com/academy/lightricks-customer-story-building-a-recommendation-\nengine-from-scratch\n• \nWhat LLMOps. (n.d.). Google Cloud. https://cloud.google.com/discover/what-is-\nllmops?hl=en\n• \nMLOps: Continuous delivery and automation pipelines in machine learning. (2024, August \n28). Google Cloud. https://cloud.google.com/architecture/mlops-continuous-\ndelivery-and-automation-pipelines-in-machine-learning#top_of_page\n• \nMl-ops.org. (2024a, July 5). https://ml-ops.org/content/mlops-principles\n• \nMl-ops.org. (2024b, July 5). https://ml-ops.org/content/mlops-principles\n• \nMl-ops.org. (2024c, July 5). https://ml-ops.org/content/motivation\n• \nMohandas, G. M. (2022a). Monitoring machine learning systems. Made With ML. https://\nmadewithml.com/courses/mlops/monitoring/\n• \nMohandas, G. M. (2022b). Testing Machine Learning Systems: Code, Data and Models. \nMade With ML. https://madewithml.com/courses/mlops/testing/\n",
      "content_length": 2243,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 489,
      "content": "MLOps and LLMOps\n460\n• \nPreston-Werner, T. (n.d.). Semantic Versioning 2.0.0. Semantic Versioning. https://semver.\norg/\n• \nRibeiro, M. T., Wu, T., Guestrin, C., & Singh, S. (2020, May 8). Beyond Accuracy: Behavioral \nTesting of NLP models with CheckList. arXiv.org. https://arxiv.org/abs/2005.04118\n• \nWandb. (2023, November 30). Understanding LLMOps: Large Language Model Operations. \nWeights & Biases. https://wandb.ai/site/articles/understanding-llmops-large-\nlanguage-model-operations/\n• \nZenml-Io. (n.d.). GitHub—zenml-io/zenml-huggingface-sagemaker: An example MLOps over-\nview of ZenML pipelines from a Hugging Face model repository to a deployed AWS SageMaker \nendpoint. GitHub. https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/\nmain\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 490,
      "content": "Appendix\nMLOps Principles\nBuilding robust and scalable ML systems requires more than creating powerful models. It demands \nan all-encompassing approach to operationalizing the entire ML lifecycle. Let’s explore the six \ncore principles that guide the MLOps field. These principles are independent of any tool and are \nat the core of building robust and scalable ML systems. They provide a guideline for designing \nproduction-ready applications, ensuring consistency, reliability, and scalability at every stage.\nWith that in mind, let’s begin with the foundation: automation or operationalization.\n1. Automation or operationalization\nTo adopt MLOps, there are three core tiers that most applications build up gradually, from manual \nprocessing to full automation:\n•\nManual process: The process is experimental and iterative in the early stages of developing \nan ML application. The data scientist manually performs each pipeline step, such as data \npreparation and validation, model training, and testing. At this point, they commonly use \nJupyter notebooks to train their models. This stage’s output is the code used to prepare \nthe data and train the models.\n•\nContinuous training (CT): The next level involves automating model training. This is \nknown as continuous training, which triggers model retraining whenever required. At this \npoint, you often automate your data and model validation steps. This step is usually done \nby an orchestration tool, such as ZenML, that glues all your code together and runs it on \nspecific triggers. The most common triggers are on a schedule, for example, every day or \nwhen a specific event comes in, such as when new data is uploaded or the monitoring sys-\ntem detects a drop in performance, offering you the flexibility to adapt to various triggers.\n",
      "content_length": 1794,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 491,
      "content": "MLOps Principles\n462\n•\nCI/CD: In the final stage, you implement your CI/CD pipelines to enable fast and reliable \ndeployment of your ML code into production. The key advancement at this stage is the \nautomatic building, testing, and deployment of data, ML models, and training pipeline \ncomponents. CI/CD is used to quickly push new code into various environments, such as \nstaging or production, ensuring efficient and reliable deployment.\nAs we build our LLM system using the FTI (feature, training, inference) architecture, we can \nquickly move from a manual process to CI/CD/CT. In Figure A.1, we can observe that the CT process \ncan be triggered by various events, such as a drop in performance detected by the monitoring \npipeline or a batch of fresh data arriving. Also, Figure A.1 is split into two main sections; the first \none highlights the automated processes, while at the bottom, we can observe the manual pro-\ncesses performed by the data science team while experimenting with various data processing \nmethods and models. Once they improve the model by tinkering with how the data is processed \nor the model architecture, they push the code to the code repository, which triggers the CI/CD \npipeline to build, test, package, and deploy the new changes to the FTI pipelines.\nFigure A.1: CI/CD/CT on top of the FTI architecture\n",
      "content_length": 1341,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 492,
      "content": "Appendix\n463\nTo conclude, CT automates the FTI pipelines, while CI/CD builds, tests, and pushes new versions \nof the FTI pipeline code to production.\n2. Versioning\nBy now, we understand that the whole ML system changes if the code, model, or data changes. \nThus, it is critical to track and version these three elements individually. But what strategies can \nwe adopt to track the code, model, and data separately?\n• \nThe code is tracked by Git, which helps us create a new commit (a snapshot of the code) \non every change added to the codebase. Also, Git-based tools usually allow us to make \nreleases, which typically pack multiple features and bug fixes. While the commits contain \nunique identifiers that are not human-interpretable, a release follows more common con-\nventions based on their major, minor, and patch versions. For example, in a release with \nversion “v1.2.3,” 1 is the major version, 2 is the minor version, and 3 is the patch version. \nPopular tools are GitHub and GitLab.\n• \nTo version the model, you leverage the model registry to store, share, and version all the \nmodels used within your system. It usually follows the same versioning conventions used \nin code releases, defined as Semantic Versioning, which, along with the major, minor, \nand patch versions, also supports alpha and beta releases that signal applications. At this \npoint, you can also leverage the ML metadata store to attach information to the stored \nmodel, such as what data it was trained on, its architecture, performance, latency, and \nwhatever else makes sense to your specific use case. Doing so creates a clear catalog of \nmodels that can easily be navigated across your team and company.\n• \nVersioning the data isn’t as straightforward as versioning the code and model because it \ndepends on the type of data you have (structured or unstructured) and the scale of data you \nhave (big or small). For example, for structured data, you can leverage a SQL database with \na version column that helps you track the changes in the dataset. However, other popular \nsolutions are based on Git-like systems, such as Data Version Control (DVC), that track \nevery change made to the dataset. Other trendy solutions are based on artifacts similar \nto a model registry that allows you to add a virtual layer to your dataset, tracking and \ncreating a new version for every change made to your data. Comet.ml, W&B (Weights & \nBiases), and ZenML offer powerful artifact features. For all solutions, you must store the \ndata on-premises or use cloud object storage solutions such as AWS S3. These tools provide \nfeatures that allow you to structure your datasets and versions, track, and access them.\n",
      "content_length": 2687,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 493,
      "content": "MLOps Principles\n464\n3. Experiment tracking\nTraining ML models is an entirely iterative and experimental process. Unlike traditional software \ndevelopment, it involves running multiple parallel experiments, comparing them based on a set \nof predefined metrics, and deciding which one should advance to production. An experiment \ntracking tool allows you to log all the necessary information, such as metrics and visual repre-\nsentations of your model predictions, to compare all your experiments and easily select the best \nmodel. Popular tools are Comet ML, W&B, MLflow, and Neptune.\n4. Testing\nThe same trend is followed when testing ML systems. Hence, we must test our application across \nall three dimensions: the data, the model, and the code. We must also ensure that the feature, \ntraining, and inference pipeline are well integrated with external services, such as the feature store, \nand work together as a system. When working with Python, the most common tool to write your \ntests is pytest, which we also recommend.\nTest types\nIn the development cycle, six primary types of tests are commonly employed at various stages:\n•\nUnit tests: These tests focus on individual components with a single responsibility, such \nas a function that adds two tensors or one that finds an element in a list.\n•\nIntegration tests: These tests evaluate the interaction between integrated components \nor units within a system, such as the data evaluation pipeline or the feature engineering \npipeline, and how they are integrated with the data warehouse and feature store.\n•\nSystem tests: System tests play a crucial role in the development cycle as they examine the \nentire system, including the complete and integrated application. These tests rigorously \nevaluate the end-to-end functionality of the system, including performance, security, and \noverall user experience—for example, testing an entire ML pipeline, from data ingestion \nto model training and inference, ensuring the system produces the correct outputs for \ngiven inputs.\n•\nAcceptance tests: These tests, often called user acceptance testing (UAT), are designed to \nconfirm that the system meets specified requirements, ensuring it is ready for deployment.\n•\nRegression tests: These tests check for previously identified errors to ensure that new \nchanges do not reintroduce them.\n",
      "content_length": 2338,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 494,
      "content": "Appendix\n465\n•\nStress tests: These tests evaluate the system’s performance and stability under extreme \nconditions, such as high load or limited resources. They aim to identify breaking points \nand ensure the system can handle unexpected spikes in demand or adverse situations \nwithout failing.\nFigure A.2: Test types\nWe’ve intentionally left regression tests out of the preceding figure because they aren’t a distinct \ntesting phase. Instead, regression testing is applied across all levels—unit, integration, system, \nacceptance, and stress tests—to ensure that changes don’t reintroduce previous errors. It’s an \nongoing process within these phases, not a separate type of test, which is why it’s not shown as \na separate category.\nWhat do we test?\nWhen writing most tests, you take a component and treat it as a black box. Thus, what you have \ncontrol over is the input and output. You want to test that you get an expected output for a given \ninput. With that in mind, here are a few things you should usually test:\n• \nInputs: Data types, format, length, and edge cases (min/max, small/large, etc.)\n• \nOutputs: Data types, formats, exceptions, and intermediary and final outputs\nTest examples\nWhen testing your code, you can leverage the standards from classic software engineering. Here \nare a few examples of code tests you can include when writing unit tests to get a better idea of \nwhat we want to test at this point—for instance, you want to check that a sentence is cleaned \nas expected. \n",
      "content_length": 1501,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 495,
      "content": "MLOps Principles\n466\nAlso, you can look at your chunking algorithm and assert that it works properly by using various \nsentences and chunk sizes.\nWhen we talk about data tests, we mainly refer to data validity. Your data validity code usually \nruns when raw data is ingested from the data warehouse or after computing the features. It is \npart of the feature pipeline. Thus, by writing integration or system tests for your feature pipeline, \nyou can check that your system responds properly to valid and invalid data.\nTesting data validity depends a lot on your application and data type. For example, when working \nwith tabular data, you can check for non-null values, that a categorical variable contains only \nthe expected values, or that a float value is always positive. You can check for length, character \nencoding, language, special characters, and grammar errors when working with unstructured \ndata such as text.\nModel tests are the trickiest, as model training is the most non-deterministic process of an ML \nsystem. However, unlike traditional software, ML systems can successfully complete without \nthrowing any errors. However, the real issue is that they produce incorrect results that can only \nbe observed during evaluations or tests. Some standard model test techniques involve checking:\n•\nThe shapes of the input and model output tensors\n•\nThat the loss decreases after one batch (or more) of training\n•\nOverfit on a small batch, and the loss approaches 0\n•\nThat your training pipeline works on all the supported devices, such as the CPU and GPU\n•\nThat your early stopping and checkpoint logic works\nAll the tests are triggered inside the CI pipeline. If some tests are more costly, for example, the mod-\nel ones, you can execute them only on special terms, such as only when modifying the model code.\nAt the other end of the spectrum, you can also perform behavioral testing on your model, which \ntries to adopt the strategy from code testing and treats the model as a black box while looking \nsolely at the input data and expected outputs. This makes the behavioral testing methods model \nagnostic. A fundamental paper in this area is Beyond Accuracy: Behavioral Testing of NLP Models \nwith CheckList, which we recommend if you want to dig more into the subject. However, as a \nquick overview, the paper proposes that you test your model against three types of tests. We use \na model that extracts the main subject from a sentence as an example:\n",
      "content_length": 2467,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 496,
      "content": "Appendix\n467\n• \nInvariance: Changes in your input should not affect the output—for example, below is \nan example based on synonym injection:\nmodel(text=\"The advancements in AI are changing the world rapidly.\")\n# output: ai\nmodel(text=\"The progress in AI is changing the world rapidly.\")\n# output: ai\n• \nDirectional: Changes in your input should affect the outputs—for example, below is an \nexample where we know the outputs should change based on the provided inputs:\nmodel(text=\"Deep learning used for sentiment analysis.\")\n# output: deep-learning\nmodel(text=\"Deep learning used for object detection.\")\n# output: deep-learning\nmodel(text=\"RNNs for sentiment analysis.\")\n# output: rnn\n•\nMinimum functionality: The most simple combination of inputs and expected outputs—\nfor example, below is a set of simple examples that we expect the model should always \nget right:\nmodel(text=\"NLP is the next big wave in machine learning.\")\n# output: nlp\nmodel(text=\"MLOps is the next big wave in machine learning.\")\n# output: mlops\nmodel(text=\"This is about graph neural networks.\")\n# output: gnn\nFor more on testing, we recommend reading Testing Machine Learning Systems: Code, \nData, and Models by Goku Mohandas: https://madewithml.com/courses/mlops/\ntesting/.\n",
      "content_length": 1251,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 497,
      "content": "MLOps Principles\n468\n5. Monitoring\nMonitoring is vital for any ML system that reaches production. Traditional software systems are \nrule-based and deterministic. Thus, once it is built, it will always work as defined. Unfortunate-\nly, that is not the case with ML systems. When implementing ML models, we haven’t explicitly \ndescribed how they should work. We have used data to compile a probabilistic solution, which \nmeans that our ML model will constantly be exposed to a level of degradation. This happens \nbecause the data from production might differ from the data the model was trained on. Thus, it \nis natural that the shipped model doesn’t know how to handle these scenarios.\nWe shouldn’t try to avoid these situations but create a strategy to catch and fix these errors in time. \nIntuitively, monitoring detects the model’s performance degradation, which triggers an alarm that \nsignals that the model should be retrained manually, automatically, or with a combination of both.\nWhy retrain the model? As the model performance degrades due to a drift in the training dataset \nand what it inputs from production, the only solution is to adapt or retrain the model on a new \ndataset that captures all the new scenarios from production.\nAs training is a costly operation, there are some tricks that you can perform to avoid retraining, \nbut before describing them, let’s quickly understand what we can monitor to understand our \nML system’s health.\nLogs\nThe approach to logging is straightforward, which is to capture everything, such as:\n•\nDocument the system configurations.\n•\nRecord the query, the results, and any intermediate outputs.\n•\nLog when a component begins, ends, crashes, and so on.\n•\nEnsure that each log entry is tagged and identified in a way that clarifies its origin within \nthe system.\nWhile capturing all activities can rapidly increase the volume of logs, you can take advantage of \nnumerous tools for automated log analysis and anomaly detection that leverage AI to efficiently \nscan all the logs, providing you with the confidence to manage the logs effectively.\nMetrics\nTo quantify your application’s healthiness, you must define a set of metrics. Each metric measures \ndifferent aspects of your application, such as the infrastructure, data, and model.\n",
      "content_length": 2285,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 498,
      "content": "Appendix\n469\nSystem metrics\nThe system metrics are based on monitoring service-level metrics (latency, throughput, error \nrates) and infrastructure health (CPU/GPU, memory). These metrics are used both in traditional \nsoftware and ML as they are crucial to understanding whether the infrastructure works well and \nthe system works as expected to provide a good user experience to the end users.\nModel metrics\nMerely monitoring the system’s health won’t suffice to identify the deeper issues within our \nmodel. Therefore, moving on to the next layer of metrics that focus on the model’s performance \nis crucial. This includes quantitative evaluation metrics like accuracy, precision, and F1 score, as \nwell as essential business metrics influenced by the model, such as ROI and click rate.\nAnalyzing cumulative performance metrics over the entire deployment period is often ineffective. \nInstead, evaluating performance over time intervals relevant to our application, such as hourly, \nis essential. Thus, in practice, you window your inputs and compute and aggregate the metrics \nat the window level. These sliding metrics can provide a clearer picture of the system’s health, \nallowing us to detect issues more promptly without them being obscured by historical data.\nWe may not always have access to ground-truth outcomes to evaluate the model’s performance \non production data. This is particularly challenging when there is a significant delay or when \nreal-life data requires annotation. To address this issue, we can develop an approximate signal \nto estimate the model’s performance or label a small portion of our live dataset to assess perfor-\nmance. When talking about ML monitoring, an approximate signal is also known as a proxy metric, \nusually implemented by drift detection methods, which are discussed in the following section.\nDrifts\nDrifts are proxy metrics that help us detect potential issues with the production model in time \nwithout requiring any ground truths/labels. Table A.1 shows three kinds of drifts.\nWhat drifts\nDescription\nDrift formulation\nX          \nInputs (features)  \ndata drift →P(X) ≠Pref(X) \n y         \nOutputs (ground truths/\nlabels)\ntarget drift →P(y) ≠Pref(y) \nP(y|X) \nrelationship between X and y  \nconcept drift →P(y|X) ≠Pref(y|X) \nTable A.1: Relationship between data, model, and code changes\n",
      "content_length": 2340,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 499,
      "content": "MLOps Principles\n470\nData drift\nData drift, also called feature drift or covariate shift, occurs when the distribution of the produc-\ntion data deviates from that of the training data, as shown in Figure A.3. This difference means the \nmodel cannot handle the changes in feature space, leading to potentially unreliable predictions. \nDrift can result from natural real-life changes or systemic problems like missing data, pipeline \nerrors, and schema modifications.\nFigure A.3: Data drift examples\nWhen data begins to drift, the degradation in our model’s performance might not be immediately \nnoticeable, particularly if the model interpolates effectively. Nevertheless, this presents an ideal \nchance to consider retraining before the drift affects the model’s performance.\nTarget drift\nIn addition to changes in input data (data drift), we might also encounter shifts in output dis-\ntribution. The shift could involve changes in the shape of the distribution or the addition and \nremoval of classes in categorical tasks. While retraining the model can help reduce performance \ndegradation due to target drift, you can often prevent it by adapting the head processing steps \nand model head to support the new schema of the output class.\nFor example, if you have a classifier that predicts if an image contains animals or people, and you \nget a picture with buildings, you can either adapt your model to support an unknown class or \nadjust the head of the model to add the new class for future predictions.\n",
      "content_length": 1508,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 500,
      "content": "Appendix\n471\nConcept drift\nIn addition to changes in input and output data, their relationship can also shift. This phenom-\nenon, known as concept drift, makes our model ineffective because the patterns it previously \nlearned to associate inputs with outputs become outdated. As illustrated in the following figure, \nconcept drifts can manifest in various ways:\n•\nGradually over time\n•\nSuddenly, due to an external event\n•\nPeriodically, due to recurring events\nFigure A.4: Concept drift examples\nFor example, this happens when using the model in a different geographic area. Let’s assume you \nwant to build a model that predicts whether a person will buy a specific car. You initially built it \nfor the American market. Now, you want to use it in the European market, where people tend to \nbuy smaller cars, creating a drift between the size feature of the car and the output probability of \npurchasing the vehicle. Of course, concept drifts can be more subtle than this example.\nAll these types of drift can happen simultaneously, complicating pinpointing the \nexact sources of drift.\n",
      "content_length": 1086,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 501,
      "content": "MLOps Principles\n472\nHow to detect and measure drifts\nNow that we’ve recognized the various types of drift, it’s crucial to understand how to detect and \nmeasure it. To do so, you need two types of windows:\n• \nA reference window: This is the collection of data points used as a baseline to compare \nagainst the production data distributions for drift identification. It is usually gathered \nfrom the training dataset.\n• \nA test window: This collects data points gathered while the ML system is in production. \nIt is compared with the reference window to ascertain if drift has occurred.\nTo measure the drifts, you leverage hypothesis tests that verify the change in distribution between \nthe two windows. For example, you can use the Kolmogorov-Smirnov (KS) test to monitor a \nsingle continuous feature. This is known as a univariate (1D) test. Thus, you must run it for every \nfeature you want to monitor. You can leverage a chi-squared univariate test to monitor categorical \nvariables and determine if the frequency of events in production is consistent with the reference \nwindow distribution.\nfrom alibi_detect.cd import KSDrift\ncd = KSDrift(X_ref, p_val=.05, preprocess_fn=preprocess_fn, input_\nshape=(max_len,))\nWhen working with text data in an embedding representation, we have to model a multivariate \ndistribution, which is how LLMs work with text. A popular approach is to take the embeddings of \nthe test and reference windows, apply a dimensionality reduction algorithm, and apply an algo-\nrithm such as maximum mean discrepancy (MMD). This algorithm is a kernel-based approach \nthat measures the distance between two distributions by computing the distance between the \nmean of the embeddings of the two windows.\nfrom alibi_detect.cd import MMDDrift\ncd = MMDDrift(x_ref, backend='pytorch', p_val=.05)\npreds = cd.predict(x)\nMonitoring vs. observability\nMonitoring involves the collection and visualization of data, whereas observability provides in-\nsights into system health by examining its inputs and outputs. For instance, monitoring allows \nus to track a specific metric to detect potential issues. \n",
      "content_length": 2119,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 502,
      "content": "Appendix\n473\nOn the other hand, a system is considered observable if it generates meaningful data about its \ninternal state, which is essential for diagnosing root causes.\nAlerts\nOnce we define our monitoring metrics, we need a way to get notified. The most common ap-\nproaches are to send an alarm in the following scenarios:\n• \nA metric passes the values of a static threshold—for example, when the accuracy of the \nclassifier is lower than 0.8, send an alarm.\n• \nTweaking the p-value of the statistical tests that check for drifts. A lower p-value means \na higher confidence that the production distribution differs from the reference one.\nThese thresholds and p-values depend on your application. However, it is essential to find the \ncorrect values, as you don’t want to overcrowd your alarming system with false positives. In that \ncase, your alarm system won’t be trustworthy, and you will either overreact or not react at all \nto issues in your system. Some common channels for sending alarms to your stakeholders are \nSlack, Discord, your email, and PagerDuty. The system’s stakeholders can be the core engineers, \nmanagers, or anyone interested in the system.\nDepending on the nature of the alarm, you have to react differently. But before taking any action, \nyou should be able to inspect it and understand what caused it. You should inspect what metric \ntriggered the alarm, with what value, the time it happened, and anything else that makes sense \nto your application.\nWhen the model’s performance degrades, the first impulse is to retrain it. But that is a costly op-\neration. Thus, you first have to check that the data is valid, the schema hasn’t changed, and the \ndata point was not an isolated outlier. If neither is true, you should trigger the training pipeline \nand train the model on the newly shifted dataset to solve the drift.\n6. Reproducibility\nReproducibility means that every process within your ML systems should produce identical \nresults given the same input. This has two main aspects.\nThe first one is that you should always know what the inputs are—for example, when training \na model, you can use a plethora of hyperparameters. Thus, you need a way to always track what \nassets were used to generate the new assets, such as what dataset version and config were used \nto train the model.\n",
      "content_length": 2323,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 503,
      "content": "MLOps Principles\n474\nThe second aspect is based on the non-deterministic nature of ML processes. For example, when \ntraining a model from scratch, all the weights are initially randomly initialized. Thus, even if \nyou use the same dataset and hyperparameters, you might end up with a model with a differ-\nent performance. This aspect can be solved by always using a seed before generating random \nnumbers, as in reality, we cannot digitally create randomness, only pseudo-random numbers. \nThus, by providing a seed, we ensure that we always produce the same trace of pseudo-random \nnumbers. This can also happen at the feature engineering step, in case we impute values with \nrandom values or randomly remove data or labels. But as a general rule of thumb, always try to \nmake your processes as deterministic as possible, and in case you have to introduce randomness, \nalways provide a seed that you have control over.\nJoin our book’s Discord space\nJoin our community’s Discord space for discussions with the authors and other readers:\nhttps://packt.link/llmeng\n",
      "content_length": 1062,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 504,
      "content": "packt.com\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as \nindustry leading tools to help you plan your personal development and advance your career. For \nmore information, please visit our website.\nWhy subscribe?\n•\nSpend less time learning and more time coding with practical eBooks and Videos from \nover 4,000 industry professionals\n•\nImprove your learning with Skill Plans built especially for you\n•\nGet a free eBook or video every month\n•\nFully searchable for easy access to vital information\n•\nCopy and paste, print, and bookmark content\nAt www.packt.com, you can also read a collection of free technical articles, sign up for a range of \nfree newsletters, and receive exclusive discounts and offers on Packt books and eBooks.\n",
      "content_length": 783,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 506,
      "content": "Other Books \nYou May Enjoy\nIf you enjoyed this book, you may be interested in these other books by Packt:\nRAG-Driven Generative AI\nDenis Rothman\nISBN: 9781836200918\n• \nScale RAG pipelines to handle large datasets efficiently\n• \nEmploy techniques that minimize hallucinations and ensure accurate responses\n",
      "content_length": 305,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 507,
      "content": "Other Books You May Enjoy\n478\n•\nImplement indexing techniques to improve AI accuracy with traceable and transparent \noutputs\n•\nCustomize and scale RAG-driven generative AI systems across domains\n•\nFind out how to use Deep Lake and Pinecone for efficient and fast data retrieval\n•\nControl and build robust generative AI systems grounded in real-world data\n•\nCombine text and image data for richer, more informative AI responses\n",
      "content_length": 427,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 508,
      "content": "Other Books You May Enjoy\n479\nBuilding LLM Powered  Applications\nValentina Alto\nISBN: 9781835462317\n•\nExplore the core components of LLM architecture, including encoder-decoder blocks \nand embeddings\n•\nUnderstand the unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLM\n•\nUse AI orchestrators like LangChain, with Streamlit for the frontend\n•\nGet familiar with LLM components such as memory, prompts, and tools\n•\nLearn how to use non-parametric knowledge and vector databases\n•\nUnderstand the implications of LFMs for AI research and industry applications\n•\nCustomize your LLMs with fine tuning\n•\nLearn about the ethical implications of LLM-powered applications\n",
      "content_length": 673,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 509,
      "content": "Other Books You May Enjoy\n480\nPackt is searching for authors like you\nIf you’re interested in becoming an author for Packt, please visit authors.packtpub.com and \napply today. We have worked with thousands of developers and tech professionals, just like you, \nto help them share their insight with the global tech community. You can make a general appli-\ncation, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.\nShare your thoughts\nNow you’ve finished LLM Engineer’s Handbook, First Edition, we’d love to hear your thoughts! If you \npurchased the book from Amazon, please click here to go straight to the Amazon review \npage for this book and share your feedback or leave a review on the site that you purchased it from.\nYour review is important to us and the tech community and will help us make sure we’re deliv-\nering excellent quality content.\n",
      "content_length": 893,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 510,
      "content": "Index\nSymbols\n4-bit NormalFloat (NF4)  215\n32-bit floating point (fp32)  211, 212\nA\nacceptance tests  464\nactions  437\nActivate-aware Weight \nQuantization (AWQ)  313\nadvanced RAG\noverview  117, 118\npost-retrieval step  124-126\npre-retrieval steps  119-122\nretrieval step  122-124\nadvanced RAG post-retrieval optimization\nreranking  334-338\nadvanced RAG pre-retrieval \noptimizations  324\nquery expansion  324-328\nself-querying  328-332\nadvanced RAG retrieval optimization\nfiltered vector search  332-334\nadvanced RAG techniques\nexploring  321-324\npost-retrieval optimization  334-338\npre-retrieval optimizations  324-332\nretrieval optimization  332-334\nalerting system  457, 458\nalerts  473\nAlpacaEval  264\nAmazon Resource Name (ARN)  375\nApplication Auto Scaling  396, 397\nApplication Load Balancer (ALB)  395\nasynchronous inference  361, 362\nautoscaling  393, 399\nscalable policy, creating  397\nscalable target, registering  396\nuse cases  394\nAWS\naccess key, setting up  48-50\naccount, setting up  48-50\nCLI, setting up  48-50\npreparing  48\nSageMaker  50\nAWS Elastic Container Service (ECS)  393\nAWS Elastic Kubernetes Service (EKS)  393\nAWS SageMaker  50\nLLM Twin model, deploying to  375-385\nneed for  51, 52\n",
      "content_length": 1213,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 511,
      "content": "Index\n482\nAWS SageMaker Inference endpoint\ncalling  386-389\nautomated evaluation framework for RAG \nsystems (ARES)  274, 275\nB\nbacked-up data\nimporting  95\nBaseCrawler interface  69-72\nbehavioral testing  466\nbias types\nfamily bias  237\nlength bias  237\nposition bias  237\nBigCodeBench Leaderboard  266\nbusiness microservice\nbuilding, with FastAPI  390-393\nC\nCDC patterns\nlog-based  137\ntimestamp-based  137\ntrigger-based  137\ncentral access point  128\nChange data capture (CDC)  136\nChatbot Arena  264\nChatbots  231\nChatGPT  5\nlimitations  5\nchat templates  208-210\nchunking handlers  165-169\nCI/CD pipeline  462\nCI pipeline, LLM Twin\nQA job  438\ntest job  438\nCircleCI  405\nclassifiers models  189\ncleaning handlers  163-165\nCloudFormation  423\ncode generation  231\nComet ML  45, 46\nconcept drift  471\ncontent moderation  231\ncontinuous batching  294\ncontinuous integration and continuous \ndeployment (CI/CD) pipeline  31, 402\ncontinuous training (CT)  138, 402, 461\ncooldown period  398\nco-pilot\nversus LLM Twin  4\ncovariate drift  470\nCrawlerDispatcher class  66-68\ncrawlers\nBaseCrawler interface  69-72\nCustomArticleCrawler class  75-77\nGithubCrawler class  73-75\nimplementing  69\nMediumCrawler class  77-79\nCustomArticleCrawler class  75-77\nD\ndata augmentation  193-196\ndatabase (DB)  317, 410\ndatabase, for unstructured and vector data\nMongoDB  47\nQdrant  47, 48\nstoring  47\ndata collection pipeline  19\ndata curation  182\n",
      "content_length": 1430,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 512,
      "content": "Index\n483\ndata decontamination  185\ndata deduplication  184, 185\ndata drift  470\ndata evaluation  233\ndata exploration  189-191\ndata generation  191-233\npreference data, evaluating  235-237\npreference data, generating  233, 234\ntips  234\ndata indexing techniques  119\ndata parallelism (DP)  299\ndata quality evaluation  186-189\ndata quantity  180, 181\nData Scientist (DS)  409\ndataset formats  208\ndata tests  466\ndecoder-only model\narchitecture  290\ncomputing   291\ngenerating  291\ntokenizing  291\nDeep Learning Containers (DLCs)  373\ndeployment costs  415\ndeployment types, criteria for selection\ndata  357\ninfrastructure  357, 358\nlatency  356\nthroughput  356, 357\nDevOps  401-403\nbenefits  403\ncontinuous delivery (CD)  405\ncontinuous integration (CI)  405\ndeployment environments  404\nversion control  405\nDevOps lifecycle\nbuild  404\ncode  403\ndeploy  404\nmonitor  404\noperate  404\nplan  403\nrelease  404\ntest  404\ndirectional  467\nDirect Preference Optimization \n(DPO)  229, 245, 248-250, 411\nimplementing  250-257\ndispatcher layer  160-162\nDLC image\nfeatures  373\nDocker  424\nDockerfile  424\ndomain-driven design (DDD)  150\ndomain-specific LLM evaluations  265-267\ndownstream pipelines\ntriggering  449-451\nDPO datasets\nhuman-generated, human-evaluated \ndatasets  233\nhuman-generated, LLM-evaluated \ndatasets  233\nLLM-generated, human-evaluated \ndatasets  234\nLLM-generated, LLM-evaluated datasets  234\ndrifts  469\nconcept drift  471\ndata drift  470\ndetecting  472\nmeasuring  472\ntarget drift  470\n",
      "content_length": 1504,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 513,
      "content": "Index\n484\nE\nElastic Container Registry (ECR)  423, 443\nembedding handlers  169-173\nencoder-only models  189\nend of sentence (EOS) token 222, 252\nend-to-end RAG inference pipeline\nexamining  346-351\nEnterprise Scenarios Leaderboard  266\nETL pipeline\nfundamental steps  56\nETL process\nconnecting, to feature pipeline  60\nexact deduplication  184\nextract, load, transform (ETL) pattern  19\nExtract, Transform, Load (ETL) pipeline  55\nF\nfamily bias  237\nFastAPI\nbusiness microservice, building  390-393\nfeature drift  470\nfeature pipeline  14, 19, 20\nfeature/training/inference (FTI) \narchitecture  8, 13, 22, 370\nbenefits  15\nfeature pipeline  14\ninference pipeline  14\ntraining pipeline  14\nfiltered vector search  123\nfine-tune\nusage, considerations  206, 207\nfine-tune models\nspecialized tools  220\nfine-tuning\nbest practices  219-226\nformat filtering  183\nformatting errors  436\nexamples  436\nFTI architecture\nused, for building LLM system  462, 463\nFTI pipeline design\nLLM Twin architecture, designing  17\nFTI pipelines architecture\ninference pipeline  14\nfull fine-tuning  211, 212\nfuzzy deduplication  184\nG\nGAIA  264\nGalileo Protect  413\ngeneral-purpose LLM evaluations  263-265\nGitHub  405\nGitHub Actions  405, 437\nGitHub Actions CI YAML file  438-441\nGitHubCrawler class  73-75\nGitHub ecosystem  405\nGitLab  405\nGitLab CI/CD  405\nGlobal Interpreter Lock (GIL)  144\nGPT  411\nguardrails  411, 412\ninput guardrails  412\noutput guardrails  413\nH\nHallucinations Leaderboard  266\n",
      "content_length": 1481,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 514,
      "content": "Index\n485\nhandlers  162, 163\nchunking handlers  165-169\ncleaning handlers  163-165\nembedding handlers  169-173\nhigh throughput  357\nHugging Face  31, 32\nfine-tuned LLMs  31\nreference link  251\nHugging Face Hub\nreference link  245\nhuman-generated, human-evaluated \ndatasets  233\nhuman-generated, LLM-evaluated \ndatasets  233\nhybrid search  123\nHypothetical document \nembeddings (HyDE)  121\nI\nIAM role  423\nIDE's MongoDB plugin  94\nIFEval  264\nin-breadth evolving  194\nin-depth evolving  194\ninference deployment types  359\nasynchronous inference  361, 362\noffline batch transform  362\nonline real-time inference  360, 361\ninference pipeline  22\nversus training pipeline  371, 372\ninfrastructure  357, 358\ninfrastructure-as-code (IaC)  393\ninput guardrails  412\ninput test  465\ninstruction dataset\ncreating  178, 196-206\ndata augmentation  193-196\ndata curation  182\ndata decontamination  185\ndata deduplication  184, 185\ndata exploration  189-191\ndata generation  191, 193\ndata quality evaluation  186-189\ndata quantity  180, 181\ngeneral framework  178-180\nhigh-quality data  179\nrule-based filtering  182, 183\nintegration tests  464\ninvariance  467\niterative improvement  246\nJ\nJenkins  405\njobs  437\nK\nkey-value (KV) cache  291-294\nkeywords filtering  183\nKolmogorov-Smirnov (KS)  472\nKullback-Leibler (KL)  247\nL\nLangfuse  413\nLangfuse UI\nexample trace  414, 415\nlarge language model (LLM)  1, 99, 355, 401\nlatency  356\nlength bias  237\n",
      "content_length": 1439,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 515,
      "content": "Index\n486\nlength filtering  183\nlinting errors  436\nexamples  436\nLLM-as-a-judge strategy  186\nLLM evaluation  235\nversus, ML evaluation  262, 263\nLLM-generated, human-evaluated \ndatasets  234\nLLM-generated, LLM-evaluated datasets  234\nLLMOps  401, 402, 410, 411, 415\nadding, to LLM Twin  434\nguardrails  411, 412\nhuman feedback  411\nprompt monitoring  413\nLLMs, training from scratch\nconcerns  410, 411\nLLM system\nbuilding, with FTI architecture  462, 463\nLLM Twin  2, 5, 6\nCD pipeline  442-444\nCI/CD pipeline flow  434, 435\nCI/CD pipeline, testing  445\nCI pipeline  438\nCT pipeline  446, 448\ninference pipeline deployment \nstrategy  368-370\nMVP, defining  7\nRAG feature pipeline architecture  127, 139\nsignificance  3, 4\nsystem architecture  16\nversus co-pilot  4\nLLM Twin architecture  23\ndata collection pipeline  19\ndesigning, with FTI pipeline design  17\nfeature pipeline  19, 20\ninference pipeline  22\ntechnical details  16, 17\ntraining pipeline  21, 22\nLLM Twin model\ndeploying, to AWS SageMaker  375-385\nLLM Twin RAG feature pipeline\ndispatcher layer  160\nhandlers  162\nimplementing  139\npydantic domain entities  150\nsetting  139\nZenML pipeline and steps  140, 141\nLLM Twin's data collection pipeline\ncrawlers  59, 69\ndesigning  56-60\ndispatcher  66-68\nimplementing  61\nNoSQL data warehouse documents  79, 80\nZenML pipeline and steps  61-65\nLLM Twin service\ndeploying  372\nLLM Twin's pipelines, cloud deployment  415\ncode, containerizing with Docker  424-428\ninfrastructure  416-418\nMongoDB, setting up  418, 419\npipelines, running on AWS  428-431\nQdrant, setting up  419, 420\nResourceLimitExceeded error, \ntroubleshooting after running ZenML \npipeline on SageMaker  432, 433\nZenML, setting up  421-423\nlogs  468\nlow latency  358\nLow-Rank Adaptation (LoRA)  213-215\n",
      "content_length": 1776,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 516,
      "content": "Index\n487\nM\nmachine learning (ML)  1, 355\nengineering  409\nmanual dataset exploration  189, 190\nmanual process  461\nmanual triggers  448\nMassive Multi-Task Language \nUnderstanding (MMLU)  261\nMaximum Mean Discrepancy (MMD)  472\nMediumCrawler class  77-79\nmetrics  468\ndrifts  469\nmodel metrics  469\nsystem metrics  469\nmetrics-driven development (MDD)  272\nmicroservices architecture  365-367\nversus monolithic architecture  367, 368\nminimum functionality  467\nminimum viable product (MVP)  6\nfeatures  6\nML engineer  410\nML evaluation\nvesus, LLM evaluation  262, 263\nML models\ntraining  464\nMLOps  401-407, 411, 461\nCI/CD pipeline  462\ncontinuous training (CT)  461\nengineering  409\nmanual process  461\nMLOps and LLMOps tools  30, 31\nComet ML  45, 46\nHugging Face  31, 32\nOpik  46, 47\nZenML  32, 33\nMLOps, core components\nfeature store  407\nML metadata store  407\nML pipeline orchestrator  407\nmodel registry  407\nMLOps engineer  410\nMLOps, principles\nautomation  408\nexperiment tracking  408\nmonitoring  408\noperationalization  408\nreproducibility  408\ntesting  408\nversioning  408\nML pipeline automation\nfor CT  12\nML pipelines\nfor ML systems  13\nML systems\nelements  9\nissues, with building  8, 9\ntesting  464\nmodel evaluation  261\ndomain-specific LLM evaluations  265-267\ngeneral-purpose LLM evaluations  263-265\nML, versus LLM evaluation  262, 263\ntask-specific LLM evaluations  267-271\nmodel metrics  469\nmodel optimization strategies  290\ncontinuous batching  294\nkey-value (KV) cache  291, 293\noptimized attention mechanisms  297, 298\nspeculative decoding  295, 296\n",
      "content_length": 1575,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 517,
      "content": "Index\n488\nmodel parallelism  298\ndata parallelism (DP)  299\npipeline parallelism (PP)  300, 301\ntechniques, combining  303\ntensor parallelism (TP)  301, 302\nmodel quantization  303, 304\nmodel tests  466\nModeration API  413\nMongoDB  47\nsetting up  418, 419\nreference link  418\nMongoDB, as data warehouse\nusage, considerations  60\nmonitoring  468\nlogs  468\nmetrics  468\nversus observability  472\nmonolithic architecture  365\nmonolithic batch pipeline architecture  10\nMT-Bench  264\nN\nNoSQL data warehouse documents  79, 80\ndata categories and user document \nclasses  87-89\nODM class, implementing  82-87\nORM and ODM software patterns  80, 82\nO\nobject-relational mapping (ORM)  154\nobject-vector mapping (OVM)  139\nimplementation  139\nobservability\nversus monitoring  472\nODM class\nimplementing  82-87\nODM software patterns  80, 82\noffline batch transform  362\nonline real-time inference  360, 361\nOpen Arabic LLM Leaderboard  267\nOpenKo-LLM Leaderboard  267\nOpen Medical-LLM Leaderboard  265\nOpen Portuguese LLM Leaderboard  267\nOpik  46, 47, 413\nOptimal Brain Quantization (OBQ) \napproach  312\noptimized attention mechanisms  297, 298\nORM software patterns  80, 82\noutput guardrails  413\noutput test  465\nP\nparameter-efficient fine-tuning techniques\nfull fine-tuning   211, 212\nLoRA  213-215\nQLoRA  215, 216\nParameter-efficient fine-tuning \ntechniques  211\npipeline parallelism (PP)  300\nPiPPy (Pipeline Parallelism for PyTorch) \nlibrary  301\npolicy optimization  246\nposition bias  237\npost-retrieval step, performing\nprompt compression  124\nre-ranking  124\nPost-Training Quantization (PTQ)  304\npreference alignment  245\n",
      "content_length": 1622,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 518,
      "content": "Index\n489\npreference-based reinforcement \nlearning (PbRL)  246\npreference dataset  230, 232\nChatbots  231\ncode generation  231\ncontent moderation  231\ncreating  230, 237-245\ncreative writing  232\ndata evaluation  233\ndata generation  233\ndata quantity  232\nsummarization  231\ntranslation  232\npre-retrieval steps, performing\ndata indexing  119\nquery optimizing  119\nproduction environment  434\nprompt monitoring  413, 451-457\npull method  136\npush method  136\nPydantic domain entities  150-154\ndata category  151\nOVM  154-159\nstate of data category  151\nPydantic Settings\nreference link  139\nPython ecosystem\ndependency and virtual environment \nmanagement  27-29\nproject installation  26, 27\ntask execution tool  29, 30\nQ\nQA job  438\nQdrant  47, 48\nreference link  419\nsetting up  419, 420\nquantization  303-308\ntechniques  313, 314\nwith GGUF and llama.cpp  309-311\nwith GPTQ and EXL2  311, 312\nQuantization-aware Low-Rank Adaptation \n(QLoRA)  215, 216, 221\nQuantization-Aware Training (QAT)  304\nquery optimization  120\nquery rewriting  121\nquery routing  120\nR\nRAG evaluation  271, 272\nARES  274, 275\nRagas  272-274\nRAG feature pipeline\nchunking  135\ncleaning  135\ndata extraction  134\ndata loading  135\ndata storage, in snapshots  138\ndata warehouse and feature store, \nsyncing  136, 137\nembedding  135\norchestration  138\nRAG feature pipeline architecture\nbatch pipelines  130\nbatch pipelines, versus streaming \npipelines  130-134\ncore steps  134\ndesigning  129\nfeature store  128\ninference pipeline  127\ningestion pipeline  127\nproblem, solution  127, 128\nraw data  128\n",
      "content_length": 1574,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 519,
      "content": "Index\n490\nRAG inference pipeline\narchitecture flow  320, 321\nimplementing  318-320, 338\nretrieval module, implementing  339-346\nraw data, into data warehouse\nobtaining  89-94\ntroubleshooting  94, 95\nRecall-Oriented Understudy for Gisting \nEvaluation (ROUGE) metric  267\nreference window  472\nregression tests  464\nReinforcement Learning from Human \nFeedback (RLHF)  245-247, 411\niterative improvement  246\npolicy optimization  246\nreward model learning  246\nreinforcement learning (RL)  246\nreproducibility  473\nrequests per second (RPS)  356\nREST API triggers  448\nRetrieval-Augmented Generation \nAssessment (Ragas)  272-274\nretrieval-augmented generation \n(RAG)  2, 99, 100, 317\nembeddings  107, 108\nembeddings, applications  114\nembeddings, creating  111-114\nembeddings, significance  109, 110\nhallucinations  101\nissues, solving  101\nvanilla RAG framework  101\nvector DBs  115\nretrieval-augmented generation (RAG) \npipeline  206, 261\nreward model learning  246\nreward models  188\nrule-based filtering  182, 183\nrunners  437\nS\nSageMaker  423\nSageMaker Inference deployment  371\nconfiguration  371\nendpoint  371\nInference component  371\nmodel  371\nSageMaker Orchestrator  423\nSageMaker roles\nconfiguring  374, 375\nscalable and secure object storage \nservice (S3)  423\nscalable policy\ncreating  397\nscalable target\nregistering  396\nscaling limits\nmaximum  398\nminimum  398\nscheduled triggers  448\nSelenium tool  69\nissues  95\nsemantic similarity  184\nServer-Sent Events (SSE)  374\nSFT, techniques\nchat templates  208-210\nfine-tune, usage, considerations  206, 207\nhyperparameters, training  216\ninstruction dataset formats  208\nparameter-efficient fine-tuning \ntechniques  211\n",
      "content_length": 1678,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 520,
      "content": "Index\n491\nSFT techniques, parameters\nbatch size  216, 217\ngradient checkpointing  219\nlearning rate and scheduler  216\nmaximum length and packing  217, 218\nnumber of epochs  218\noptimizers  218\nweight decay  219\nspeculative decoding  295, 296\nstack  422\nstaging environment  434\nstateless real-time architecture  11\nstatistical analysis  190\nstress tests  465\nstyle transfer  2\nsummarization  231\nSupervised Fine-Tuning (SFT)  177, 229, 264\ntechniques, exploring  206\nsystem metrics  469\nsystem tests  464\nT\ntarget drift  470\nTargetTrackingScaling policy  397\ntask-specific LLM evaluations  267-271\ntensor parallelism (TP)  301, 302\nTerraform  393\ntest example  465\ntest job  438\ntest types  465\nacceptance tests  464\nintegration tests  464\nregression tests  464\nstress tests  465\nsystem tests  464\nunit tests  464\ntest window  472\nText Generation Inference (TGI)  294, 373\nthroughput  356, 357\nTime between Tokens (TBT)  413\nTime per Output Token (TPOT)  413\nTime to First Token (TTFT)  413\nTokens per Second (TPS)  413\ntopic clustering  190, 191\nTotal Latency  413\ntraining pipeline  14, 21, 22\nversus inference pipeline  371, 372\ntriggers\nmanual triggers  448\nREST API triggers  448\nscheduled triggers  448\nTwinLlama-3.1-8B\nanswers, evaluating  278-283\nanswers, generating  276-278\nevaluating  275, 276\nresults, analyzing  283-286\nTwinLlama-3.1-8B model  250\nU\nUltraFeedback method  195\nunit tests  464\nUser Acceptance Testing (UAT)  464\nV\nvector DBs  115\nalgorithms, for creating vector index  116\nDB operations  116\nworking  115\n",
      "content_length": 1534,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 521,
      "content": "Index\n492\nversioning  463\ncode  463\ndata  463\nmodel  463\nVideo Random-Access Memory (VRAM)  291\nW\nwindow types\nreference window  472\ntest window  472\nworkflow  437\nZ\nZenML  32, 33\nartifacts and metadata  39-43\norchestrator  33-37\nreference link  421\nsetting up  421-423\nZenML pipeline  140-142\ncleaned documents, chunking  147-150\ncleaned documents, embedding  147-150\nconfiguring  43, 45\ndata warehouse, querying  143-145\ndocuments, cleaning  146, 147\ndocuments, loading to vector DB  150\nimplementing  61-65\nrunning  43, 45\nzero-point quantization  307\n",
      "content_length": 555,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 522,
      "content": "Download a free PDF copy of this book\nThanks for purchasing this book!\nDo you like to read on the go but are unable to carry your print books everywhere?\nIs your eBook purchase not compatible with the device of your choice?\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical \nbooks directly into your application.\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free \ncontent in your inbox daily.\nFollow these simple steps to get the benefits:\n1. \nScan the QR code or visit the link below:\nhttps://packt.link/free-ebook/9781836200079\n2. \nSubmit your proof of purchase.\n3. \nThat’s it! We’ll send your free PDF and other benefits to your email directly.\n",
      "content_length": 841,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}