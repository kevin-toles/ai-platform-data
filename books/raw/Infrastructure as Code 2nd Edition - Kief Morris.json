{
  "metadata": {
    "title": "Infrastructure as Code 2nd Edition - Kief Morris",
    "author": "Kief Morris",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 356,
    "conversion_date": "2025-12-19T17:31:04.136414",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Infrastructure as Code 2nd Edition - Kief Morris.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "What is Infrastructure as Code?",
      "start_page": 31,
      "end_page": 52,
      "detection_method": "regex_chapter_title",
      "content": "There are two problems with this. One is that it removes the\n\nbenefits of using cloud technology; the other is that users want the benefits of cloud technology. So users bypass the people who are trying to limit the chaos. In the worst cases, people completely ignore risk management, deciding it’s not relevant in the brave\n\n3\n\nnew world of cloud. They embrace cowboy IT , which adds different problems.\n\nThe premise of this book is that you can exploit cloud and\n\nautomation technology to make changes easily, safely, quickly, and responsibly. These benefits don’t come out of the box with automation tools or cloud platforms. They depend on the way you\n\nuse this technology.\n\nDEVOPS AND INFRASTRUCTURE AS CODE\n\nDevOps is a movement to reduce barriers and friction between organizational silos - development, operations, and other stakeholders involved in planning, building, and running software. Although technology is the most visible, and in some ways simplest face of DevOps, it’s culture, people, and processes which have the most impact on flow and effectiveness. Technology and engineering practices like infrastructure as code should be used to support efforts to bridge gaps and improve collaboration.\n\nIn this chapter, I explain that modern, dynamic infrastructure requires a “Cloud Age” mindset. This mindset is fundamentally different from the traditional, “Iron Age” approach we used with static pre-cloud systems. I define three Core Practices for\n\nimplementing infrastructure as code: define everything as code, continuously validate everything as you work, and build your system from small, loosely-coupled pieces.\n\nAlso in this chapter, I describe the reasoning behind the Cloud Age approach to infrastructure. This approach discards the false dichotomy of trading speed off against quality. Instead, we use speed as a way to improve quality, and we use quality to enable delivery at speed.\n\nI’ll explain why and how this dynamic works:\n\nThe differences between Iron Age systems and Cloud Age systems, in terms of both technology and ways of working,\n\nHow this leads to Infrastructure as Code,\n\nSome common objections to going “all-in” on Infrastructure as Code,\n\nThe parts of a dynamic infrastructure,\n\nThe three core practices of Infrastructure as Code.\n\nFrom the Iron Age to the Cloud Age\n\nCloud Age technologies make it faster to provision and change infrastructure than traditional, Iron Age technologies (Table 1-1).\n\nTable 1-1. Technology changes in the Cloud Age\n\nIron Age\n\nCloud Age\n\nPhysical hardware\n\nVirtualized resources\n\nProvisioning takes weeks\n\nProvisioning takes minutes\n\nManual processes\n\nAutomated processes\n\nHowever, these technologies don’t necessarily make it easier to manage and grow your systems. Moving a system with technical\n\ndebt onto unbounded cloud infrastructure accelerates the chaos.\n\nMaybe you could use well-proven, traditional governance models\n\nto control the speed and chaos that newer technologies unleash. Thorough up-front design, rigorous change review, and strictly\n\nsegregated responsibilities will impose order!\n\nUnfortunately, these models optimize for the Iron Age, where changes are slow and expensive. They add extra work up-front,\n\nhoping to reduce the time spent making changes later. This arguably makes sense when making changes later is slow and\n\nexpensive. But cloud makes changes cheap and fast. You should\n\nexploit this speed to learn and improve your system continuously. Iron Age ways of working are a massive tax on learning and\n\nimprovement.\n\nRather than using slow-moving Iron Age processes with fast- moving Cloud Age technology, adopt a new mindset. Exploit\n\nfaster-paced technology to reduce risk and improve quality. Doing\n\nthis requires a fundamental change of approach and new ways of thinking about change and risk (Table 1-2).\n\nTable 1-2. Ways of working in the Cloud Age\n\nIron Age\n\nCloud Age\n\nCost of change is high\n\nCost of change is low\n\nChanges represent failure (changes must be “managed”, “controlled”)\n\nChanges represent learning and improvement\n\nReduce opportunities to fail\n\nMaximize speed of improvement\n\nDeliver in large batches, test at the end\n\nDeliver small changes, test continuously\n\nLong release cycles\n\nShort release cycles\n\nMonolithic architectures (fewer, larger moving parts)\n\nMicroservices architectures (more, smaller parts)\n\nGUI-driven or physical configuration\n\nConfiguration as Code\n\nInfrastructure as Code is a Cloud Age approach to managing\n\nsystems that embraces continuous change for high reliability and quality.\n\nTHE CLOUD AGE\n\nCloud Age technologies make it possible to rapidly provision and change infrastructure resources. However, you need to adopt Cloud Age ways of working to exploit the technology to deliver better reliability, security, and quality.\n\nCloud Age approaches are not only relevant when using shared public clouds, but are essential for modern on-premise and data center infrastructures as well.\n\nInfrastructure as Code\n\nInfrastructure as Code is an approach to infrastructure automation\n\nbased on practices from software development. It emphasizes\n\nconsistent, repeatable routines for provisioning and changing systems and their configuration. You make changes to code, then\n\nuse automation to test and apply those changes to your systems.\n\nThroughout this book, I explain how to use agile engineering practices such as Test Driven Development (TDD), Continuous\n\nIntegration (CI), and Continuous Delivery (CD) to make changing\n\ninfrastructure fast and safe. I also describe how modern software design can create resilient, well-maintained infrastructure. These\n\npractices and design approaches reinforce each other. Well- designed infrastructure is easier to test and deliver. Automated\n\ntesting and delivery drive simpler and cleaner design.\n\nTIP\n\nInfrastructure as Code applies software engineering practices to the management of infrastructure for better reliability, security, and quality.\n\nBenefits of Infrastructure as Code\n\nTo summarize, organizations adopting infrastructure as code hope\n\nto achieve benefits including:\n\nUsing IT infrastructure as an enabler for rapid delivery of value,\n\nReducing the effort and risk of making changes to infrastructure,\n\nEnabling users of infrastructure to get the resources they need, when they need it,\n\nProviding common tooling across development, operations, and other stakeholders,\n\nCreating systems that are reliable, secure, and cost- effective,\n\nMake governance, security, and compliance controls visible,\n\nImproving the speed to troubleshoot and resolve failures.\n\nUse Infrastructure as Code to optimize for change\n\nGiven that:\n\n4 Changes are the biggest risk to a production system ,\n\nContinuous change is inevitable, and\n\nMaking changes is the only way to improve a system,\n\nThen it makes sense to optimize your capability to make changes both rapidly and reliably. Research from the Accelerate State of\n\nthe DevOps Report backs this up. Making changes frequently and 5 reliably is correlated to organizational success .\n\nThere are several objections I hear when I recommend a team implement automation to optimize for change. I believe these\n\ncome from misunderstandings of how you can use automation.\n\nObjection: “We don’t make changes often enough to justify automating them”\n\nWe want to think that we build a system, and then it’s “done.” In this view, we don’t make many changes, so automating changes is\n\na waste of time.\n\nIn reality, very few systems stop changing, at least before they are retired. Some people assume that their current level of change is\n\ntemporary. Others create heavyweight change request processes to discourage people from asking for changes. These people are in denial. Most teams who are supporting actively used systems\n\nhandle a continuous stream of changes.\n\nConsider these common examples of infrastructure changes:\n\nAn essential new application feature requires adding a new database,\n\nA new application feature requires an upgrade to the application server,\n\nUsage levels grow faster than expected. You need more servers, new clusters, and expanded network and storage capacity,\n\nPerformance profiling shows that the current application deployment architecture is limiting performance. You need to redeploy the applications across different application servers. Doing this requires changes to the clustering and network architecture,\n\nThere is a newly announced security vulnerability in system packages for your OS. You need to patch dozens of production servers,\n\nYou need to update servers running a deprecated version of the OS and critical packages,\n\nYour web servers experience intermittent failures. You need to make a series of configuration changes to diagnose the problem. Then you need to update a module to resolve the issue,\n\nYou find a configuration change that improves the performance of your database.\n\nA fundamental truth of the Cloud Age is: Stablity comes from making changes.\n\nUnpatched systems are not stable; they are vulnerable. If you can’t\n\nfix issues as soon as you discover them, your system is not stable. If you can’t recover from failure quickly, your system is not stable. If the changes you do make involve considerable downtime, your system is not stable. If changes frequently fail, your system is not\n\nstable.\n\nObjection: “We should build first and automate later”\n\nGetting started with infrastructure as code is a steep curve. Setting\n\nup the tools, services, and working practices to automate infrastructure delivery is loads of work, especially if you’re also adopting a new infrastructure platform. The value of this work is hard to demonstrate before you start building and deploying\n\nservices with it. Even then, the value may not be apparent to people who don’t work directly with the infrastructure.\n\nSo stakeholders often pressure infrastructure teams to build new cloud-hosted systems quickly, by hand, and worry about\n\nautomating it later.\n\nThere are two reasons why automating afterward is a bad idea:\n\nAutomation should enable faster delivery, even for new things. Implementing automation after most of the work has been done sacrifices much of the benefits.\n\nAutomation makes it easier to write automated tests for what you build. And it makes it easier to quickly fix and\n\nrebuild when you find problems. Doing this as a part of the build process helps you to build better infrastructure.\n\nAutomating an existing system is very hard. Automation is a part of a system’s design and implementation. To add automation to a system built without it, you need to change the design and implementation of that system significantly. This is also true for automated testing and deployment.\n\nCloud infrastructure built without automation becomes a write-off sooner than you expect. The cost of manually maintaining and fixing the system can escalate quickly. If the service it runs is\n\nsuccessful, stakeholders will pressure you to expand and add features rather than stopping to rebuild.\n\nThe same is true when you build a system as an experiment. Once you have a proof of concept up and running, there is pressure to move on to the next thing, rather than to go back and build it right.\n\nAnd in truth, automation should be a part of the experiment. If you intend to use automation to manage your infrastructure, you need to understand how this will work, so it should be part of your proof\n\nof concept.\n\nThe solution is to build your system incrementally, automating as you go. Ensure you deliver a steady stream of value, while also building the capability to do so continuously.\n\nObjection: “We must choose between speed and quality”\n\nIt’s natural to think that you can only move fast by skimping on quality; and that you can only get quality by moving slowly. You might see this as a continuum, as shown in Figure 1-1.\n\nFigure 1-1. The idea that speed and quality are opposite ends of a spectrum is a false dichotomy\n\nHowever, the Accelerate research I mentioned earlier (“Use\n\nInfrastructure as Code to optimize for change”) shows otherwise.\n\nThese results demonstrate that there is no tradeoff between improving performance and achieving higher levels of stability and quality. Rather, high performers do better at all of these measures. This is precisely what the Agile and Lean movements predict, but much dogma in our industry still rests on the false assumption that moving faster means trading off against other performance goals, rather than enabling and reinforcing them.\n\n—Forsgren PhD, Nicole. Accelerate\n\nIn short, organizations can’t choose between being good at change or being good at stability. They tend to either be good at both or\n\nbad at both.\n\n6\n\nI prefer to see quality and speed as a quadrant rather than a continuum, as shown in Figure 1-2.\n\nFigure 1-2. Change and stability is a quadrant\n\nThis quadrant model shows why trying to choose between speed\n\nand quality leads to being mediocre at both.\n\nLower-right quadrant\n\nThis is the “move fast and break things” philosophy. Teams that optimize for speed and sacrifice quality build messy, fragile systems. They slide into the lower-left quadrant because their shoddy systems slow them down. Many startups who have been working this way for a while complain about losing their “mojo.” Simple changes that they would have whipped out quickly in the old days now take days or weeks because the system is a tangled mess.\n\nUpper-left quadrant:: Prioritize quality over speed\n\nAlso known as, “we’re doing serious and important things, so we have to do things properly.” Then deadline pressures drive “workarounds.” Heavyweight processes make the system hard to fix and improve. So technical debt grows along with lists of “known issues.” These teams slump into the lower-left quadrant. They end up with low-quality systems because it’s too hard to improve them. They add more processes in response to failures. These processes make it even harder to make improvements and increases fragility and risk. This leads to more failures and more process. Many people working in 7 organizations that work this way assume this is normal , 8 especially those who work in risk-sensitive industries .\n\nThe upper-right quadrant is the goal of modern approaches like\n\nlean, agile, and DevOps. Being able to move quickly and maintain\n\na high level of quality may seem like a fantasy. However, the Accelerate research proves that many teams do achieve this. So\n\nthis quadrant is where you find “high performers.”\n\nWHY YOU SHOULD AUTOMATE FROM THE START\n\nYou make changes more often than you may think,\n\nIt’s far easier to automate as you build something than to add it afterward,\n\nAutomating the provisioning and configuration of environments ensures consistency from the start,\n\nAutomation helps you make changes rapidly and reliably. Speed enables quality, and quality enables speed (Figure 1-3).\n\nFigure 1-3. Speed enables quality, and quality enables speed\n\nThree core practices for Infrastructure as Code\n\nThe Cloud Age concept exploits the dynamic nature of modern\n\ninfrastructure and application platforms to make changes\n\nfrequently and reliably. Infrastructure as Code is an approach to\n\nbuilding infrastructure that embraces continuous change for high\n\nreliability and quality. So how can your team do this?\n\nThere are three core practices for implementing Infrastructure as Code:\n\nDefine everything as code\n\nContinuously validate all work in progress\n\nBuild small, simple pieces that you can change independently\n\nI’ll summarize each of these now, to set the context for further discussion. Later, I’ll devote a chapter to the principles for\n\nimplementing each of these practices.\n\nCore practice: Define everything as code\n\nDefining all your stuff “as code” is a core practice for making\n\nchanges rapidly and reliably. There are a few reasons why this helps:\n\nReusability\n\nIf you define a thing as code, you can create many instances of it. You can repair and rebuild your things quickly. Other people can build identical instances of the thing.\n\nConsistency\n\nThings built from code are built the same way every time. This makes system behavior predictable. This makes testing more reliable. This enables continuous validation mentioned in “Core practice: Continuously validate all your work in progress”.\n\nTransparency\n\nEveryone can see how the thing is built by looking at the code. People can review the code and suggest improvements. They can learn things to use in other code. They gain insight to use when troubleshooting. They can review and audit for compliance.\n\nI’ll expand on concepts and implementation principles for defining\n\nthings as code in Chapter 4.\n\nCore practice: Continuously validate all your work in progress\n\nEffective infrastructure teams are rigorous about testing. They use\n\nautomation to deploy and test each component of their system.\n\nThey integrate all the work everyone has in progress. They test as they work, rather than waiting until they’ve finished.\n\nThe idea is to build quality in rather than trying to test quality in.\n\nOne part of this that people often overlook is that it involves\n\nintegrating and testing all work in progress. On many teams, people work on code in separate branches and only integrate when\n\nthey finish. According to the Accelerate research, however, teams\n\nget better results when everyone integrates their work at least daily. Continuous Integration (CI) involves merging and testing\n\neveryone’s code throughout development. Continuous Delivery\n\n(CD) takes this further, keeping the merged code always production-ready.\n\nI’ll go into more detail on how to continuously validate\n\ninfrastructure code in Chapter 9.\n\nCore practice: Build small, simple pieces that you can change independently\n\nTeams struggle when their systems are large and tightly coupled.\n\nThe larger a system is, the harder it is to change, and the easier it is to break.\n\nWhen you look at the codebase of a high performing team, you see\n\nthe difference. Their system is composed of small, simple pieces.\n\nEach piece is easy to understand and has clearly defined interfaces. They can easily change each component on its own. And, they can\n\ndeploy and test each component in isolation.\n\nI dig more deeply into implementation principles for this core\n\npractice in [Link to Come].\n\nRECAP: THE CORE PRACTICES OF INFRASTRUCTURE AS CODE\n\nDefine everything as code\n\nContinuously validate all work in progress\n\nBuild small, simple pieces that can be changed independently\n\nThe parts of an infrastructure system\n\nThere are many different parts, and types of parts, in a modern\n\ncloud infrastructure. I find it helpful to group these parts into\n\nplatform layers. I use these as rough architectural boundaries for systems and dedicate a chapter of this book to two of these layers.\n\nInfrastructure Platform\n\nThe Infrastructure Platform is the set of infrastructure resources and the tools and services that manage them. Cloud and virtualization platforms provide infrastructure resources, including compute, storage, and networking primitives. People also call this Infrastructure as a Service (IaaS). I’ll elaborate on these in Chapter 3. This layer also involves tools to define and manage infrastructure resources. An infrastructure stack is a collection of infrastructure resources managed as a unit. I’ll talk about infrastructure stacks and tools that manage them, such as Terraform and CloudFormation, in chapter Chapter 5.\n\nApplication Runtime Platform\n\nApplication Runtime Platforms build on infrastructure platforms to provide environments for running applications. Examples of services and constructs in an application runtime platform include container clusters, serverless environments, application servers, OS processes, and databases. People often refer to this as Platform as a Service (PaaS). The idea of “cloud native” systems has become popular in recent years, and fits in with this layer. I’ll talk about these more in [Link to Come].\n\nApplications\n\nApplications and services run on your infrastructure, providing services to your organization and its users.\n\nFigure 1-4 shows these layers:\n\nFigure 1-4. Layers of system elements\n\nConclusion\n\nTo get the value of cloud and infrastructure automation, you need a\n\nCloud Age mindset. This means exploiting speed to improve\n\nquality, and building quality in to gain speed. Automating your infrastructure takes work, especially when you’re learning how to\n\ndo it. But doing it helps you to make changes, including building\n\nthe system in the first place.\n\nI’ve described the parts of a typical infrastructure system, as these provide the foundations for chapters explaining how to implement\n\nInfrastructure as Code.\n\nFinally, I defined three core practices for Infrastructure as Code:\n\nBuild small, simple pieces that you can change independently\n\nDefine everything as code\n\nContinuously validate all work in progress\n\nIn the next chapter, I’ll describe the principles that drive Cloud\n\nAge approaches for managing infrastructure.\n\n1 This is as opposed to what they usually said a few years ago, which is that software\n\nis “not part of our core business.” Since then, those people realized their organizations were being overtaken by ones run by people who see better software as a way to compete, rather than as a cost to reduce.\n\n2 According to Wikipedia, a tire fire has two forms: “Fast-burning events, leading to almost immediate loss of control, and slow-burning pyrolysis which can continue for over a decade.”\n\n3 By “cowboy IT,” I mean people building IT systems without any particular method or consideration for future consequences. Often, people who have never supported production systems take the quickest path to get things working without considering security, maintenance, performance, and other operability concerns.\n\n4 According to the Visible Ops Handbook, changes cause 80% of unplanned\n\noutages.\n\n5 Reports from the Accelerate research are available in the annual State of the\n\nDevOps Report, and in the book Accelerate by Dr. Nicole Forsgren, Jez Humble, Gene Kim.\n\n6 Yes, I do work at a consultancy, why do you ask?\n\n7 This is an example of “Normalization of Deviance,” which means people get used to working in ways that increase risk. Diane Vaughan defined this term in The Challenger Launch Decision\n\n8 It’s ironic (and scary) that so many people in industries like finance, government, and healthcare consider fragile IT systems, and processes that obstruct improving them, to be normal, and even desirable.\n\nChapter 2. Principles of Cloud Age Infrastructure\n\nComputing resources in the Iron Age of IT were tightly coupled to physical hardware. We assembled CPUs, memory, and hard drives\n\nin a case, mounted it into a rack, and cabled it to switches and routers. We installed and configured an operating system and application software. We could describe where an application\n\nserver was in the data center: which floor, which row, which rack,\n\nwhich slot.\n\nThe Cloud Age decouples the computing resources from the physical hardware they run on. The hardware still exists, of course, but servers, hard drives, and routers float across it. These are no\n\nlonger physical things, having transformed into virtual constructs that we create, duplicate, change, and destroy at will.\n\nThis transformation has forced us to change how we think about,\n\ndesign, and use computing resources. We can’t rely on the physical attributes of our application server to be constant. We must be able to add and remove instances of our systems without ceremony.\n\nAnd we need to be able to easily maintain the consistency and quality of our systems even as we rapidly expand their scale.\n\nCLOUD NATIVE APPLICATION ARCHITECTURES\n\nThis book focuses on how to design and build infrastructure in the context of Cloud Age platforms. The dynamic nature of these platforms also affects the design of application software. Cloud native applications are decoupled from specific infrastructure resources like servers, hard-drives, and network locations. Developers assume their applications will be automatically scaled up and down, and that workloads will be shifted between instances. [Link to Come] and the rest of [Link to Come] describe how to build infrastructure that supports cloud native software.\n\nThere are several principles for designing and implementing infrastructure on cloud platforms. These principles articulate the\n\nreasoning for using the three core practices (define everything as code, continuously validate, build small pieces). I also list several\n\ncommon pitfalls teams make with dynamic infrastructure.\n\nTogether, these principles and pitfalls drive more specific implementation principles for the core infrastructure as code practices and help choose between different implementation patterns. They set the context for the advice throughout this book.\n\nPrinciple: Assume systems are unreliable\n\nIn the Iron Age, we assumed our systems were running on reliable hardware. In the Cloud Age, you need to assume your system runs 1 on unreliable hardware .\n\nCloud scale infrastructure involves hundreds of thousands of devices, if not more. At this scale, failures happen even when\n\nusing reliable hardware. And most cloud vendors use cheap, less reliable hardware, detecting and replacing it when it breaks.\n\nYou’ll need to take parts of your system offline for reasons other than unplanned failures. You’ll need to patch and upgrade systems. You’ll resize. Redistribute load. Troubleshoot problems.\n\nWith static infrastructure, doing these things means taking systems offline. But in many modern organizations, taking systems offline means taking the business offline.\n\nSo you can’t treat the infrastructure your system runs on as a stable foundation. Instead, you must design for uninterrupted service when underlying resources change .2\n\nTHE TRADEOFFS OF “LIFT AND SHIFT” MIGRATION TO THE CLOUD\n\nMany organizations are migrating their systems from Iron Age infrastructure to cloud platforms. A common approach is “lift and shift”, which aims to emulate existing on-premise infrastructure, so that applications can be moved with few or no changes to code and configuration. Teams replicate physical network structures and static addresses in the cloud, and even use virtual appliance versions of network devices such as firewalls and routers.\n\nThe reasons people use lift and shift include:\n\nTo use familiar, trusted technology, vendors, and topologies, rather than having to retrain,\n\nTo avoid the cost of modifying or reconfiguring applications,\n\nTo simplify the migration process, avoiding big hairy rewrites.\n\nThe problems with lift and shift include:\n\nReplicating physical infrastructure configuration in the cloud adds complexity and cost, not only for the migration process but also for ongoing maintenance.\n\nMany products, topologies, and patterns that are appropriate for data centers are not appropriate for the cloud. Transplanting them adds cost, and may not add any benefit. They can even make performance, security, and maintainability worse.\n\nResources provided by the cloud platform provide benefits for reliability, security, and scalability. Reproducing data center configurations loses those benefits.\n\nLift and shift aims to reduce the work of moving an application to the cloud by adding implementation work to its infrastructure. This may succeed if the infrastructure work can realistically be done more easily than changing the application. More often, applications can be tweaked to make them cope easier in their new environment. In any case, you should have a concrete strategy for adapting, rewriting, or managing applications after they have been shifted to the cloud.\n\nPrinciple: Make everything reproducible\n\nOne way to make a system recoverable is to make sure you can rebuild its parts effortlessly and reliably.\n\nEffortlessly means that there is no need to make any decisions\n\nabout how to build things. You should define things such as",
      "page_number": 31
    },
    {
      "number": 2,
      "title": "Principles of Cloud Age Infrastructure",
      "start_page": 53,
      "end_page": 68,
      "detection_method": "regex_chapter_title",
      "content": "configuration settings, software versions, and dependencies as code. Rebuilding is then a simple “yes/no” decision.\n\nMEASURING REPRODUCIBILITY\n\nYou and your team should track the time it takes you to build a new instance of an existing system. This best way to know this, with confidence, is to rebuild frequently, as a part of routine operations.\n\nSome teams build fresh infrastructure every time they deploy a new software release. This process is usually part of a zero-downtime release process (see [Link to Come]). Other teams rebuild infrastructure in test environments as part of a pipeline ([Link to Come]).\n\nEither approach regularly proves the speed that you can rebuild. This in turn means you can be confident you can do it in an emergency situation.\n\nNot only does reproducibility make it easy to recover a failed\n\nsystem, but it also helps you to:\n\nMake testing environments consistent with production,\n\nReplicate systems across regions for availability,\n\nAdd instances on demand to cope with high load,\n\nReplicate systems to give each customer a dedicated instance.\n\nOf course, the system generates data, content, and logs, which you\n\ncan’t define ahead of time. You need to identify these and find ways to keep them as a part of your replication strategy. Doing this\n\nmight be as simple as copying or streaming data to a backup and then restoring it when rebuilding. I’ll describe options for doing\n\nthis in [Link to Come].\n\nThe ability to effortlessly build and rebuild any part of the\n\ninfrastructure is powerful. It removes risk and fear of making\n\nchanges. You can handle failures with confidence. You can rapidly provision new services and environments.\n\nPitfall: Snowflake systems\n\nA snowflake is an instance of a system or part of a system that is difficult to rebuild. It may also be an environment that should be\n\nsimilar to other environments, such as a staging environment, but is different in ways that its team doesn’t fully understand.\n\nPeople don’t set out to build snowflake systems. They are a natural\n\noccurrence. The first time you build something with new tool, you\n\nlearn lessons along the way, which involves making mistakes. But if people are relying on the thing you’ve built, you may not have\n\ntime to go back and rebuild or improve it using what you learned. Improving what you’ve built is especially hard if you don’t have\n\nthe mechanisms and practices that make it easy to and safe to change.\n\nAnother cause of snowflakes is when people make changes to one instance of a system that they don’t make to others. They may be\n\nunder pressure to fix a problem that only appears in one system. Or they may start a major upgrade in a test environment, but run\n\nout of time to roll it out to others.\n\nYou know a system is a snowflake when you’re not confident you can safely change or upgrade it. Worse, if the system does break,\n\nit’s hard to fix it. So people avoid making changes to the system, leaving it out of date, unpatched, and maybe even partly broken.\n\nSnowflake systems create risk and waste the time of the teams that manage them. It is almost always worth the effort to replace them\n\nwith reproducible systems. If a snowflake system isn’t worth improving, then it may not be worth keeping at all.\n\nThe best way to replace a snowflake system is to write code that can replicate the system, running the new system in parallel until\n\nit’s ready. Use automated tests and pipeline to prove that it is correct, reproducible, and that you can change it easily.\n\nPrinciple: Create disposable things\n\nBuilding a system that can cope with dynamic infrastructure is one level. The next level is building a system that is itself dynamic. You should be able to gracefully add, remove, start, stop, change,\n\nand move the parts of your system. Doing this creates operational flexibility, availability, and scalability. It also simplifies and de- risks changes.\n\nMaking the pieces of your system malleable is the main idea of cloud-native software. Cloud abstracts infrastructure resources\n\n(compute, networking, and storage) from physical hardware. Cloud-native software completely decouples application functionality from the infrastructure it runs on. See [Link to Come]\n\nfor more on this.\n\nCATTLE, NOT PETS\n\n“Treat your servers like cattle, not pets,” is a popular expression about disposability . I miss giving fun names to each new server I create. But I don’t miss having to tweak and coddle every server in our estate by hand.\n\n3\n\nIf your systems are dynamic, then the tools you use to manage them need to cope with this. For example, your monitoring should\n\nnot raise an alert every time you rebuild part of your system. However, it should raise a warning if something gets into a loop rebuilding itself.\n\nTHE CASE OF THE DISAPPEARING FILE SERVER\n\nPeople can take a while to get used to ephemeral infrastructure. One team I worked with set up automated infrastructure with VMware and Chef. They deleted and replaced virtual machines as needed.\n\nA new developer on the team needed a web server to host files to share with teammates. So he manually installed an HTTP server on a development server and put the files there. A few days later, I rebuilt the VM, and his web server disappeared.\n\nAfter some confusion, the developer understood why this had happened. He added his web server to the Chef code, and persisted his files to the SAN. The team now had a reliable file sharing service.\n\nPrinciple: Minimize variation\n\nAs a system grows, it becomes harder to understand, harder to\n\nchange, and harder to fix. The work involved grows with the number of pieces, and also with the number of different types of pieces. So a useful way to keep a system manageable is to have\n\nfewer types of pieces-to keep variation low. It’s easier to manage 100 identical servers than 5 completely different servers.\n\nThe reproducibility principle (“Principle: Make everything reproducible”) complements this idea. If you define a simple\n\ncomponent and create many identical instances of it, then you can easily understand, change, and fix it.\n\nTo make this work, you must apply any change you make to all instances of the component. Otherwise, you create configuration-\n\ndrift.\n\nHere are some types of variation you may have in your system:\n\nMultiple operating systems, application runtimes, databases, and other technologies. Each one of these needs people in your team to keep up skills and knowledge.\n\nMultiple versions of software such as an operating system or database. Even if you only use one operating system, each version may need different configurations and tooling.\n\nDifferent versions of a package. When some servers have a newer version of a package, utility, or library than others, you have risk. Commands may not run consistently across them. Older versions may have vulnerabilities or bugs.\n\nOrganizations have tension between allowing each team to choose\n\ntechnologies and solutions that are appropriate to their needs, versus keeping the amount of variation in the organization to a manageable level.\n\nLIGHTWEIGHT GOVERNANCE\n\nModern, digital organizations are learning the value of Lightweight Governance in IT to balance autonomy and centralized control. This is a key element of the EDGE model for agile organizations. For more on this, see the book, EDGE: Value-Driven Digital Transformation, or Jonny LeRoy’s talk, The Goldilocks zone of lightweight architectural governance.\n\nConfiguration Drift\n\nConfiguration drift is variation that happens over time across systems that were once identical. Figure 2-1 shows this. Manually making changes can cause configuration drift. It can also happen if\n\nyou use automation tools to make ad-hoc changes to only some of the instances.\n\nFigure 2-1. Configuration drift is when instances of the same thing become different over time.\n\nConfiguration drift makes it harder to maintain consistent\n\nautomation.\n\nFOODSPIN EXAMPLE: DIVERGING INFRASTRUCTURE\n\nFoodspin is a fictional company that provides an online menu service for fast food restaurants. I’ll be using it throughout the book to demonstrate how concepts may play out in practice. First, let’s look at Foodspin as an example of configuration drift.\n\nFoodspin runs a separate instance of their application for each restaurant, each instance configured to use custom branding and menu content. In the early days, the Foodspin team ran scripts to create a new application server for each new restaurant. They managed the infrastructure manually or by writing scripts and tweaking them each time they needed to make a change.\n\nOne of the restaurants, Curry Hut, has far more traffic to its menu application than the others, so the team tweaked the configuration for the Curry Hut server. They didn’t make the changes to the other customers, because the team was busy and it didn’t seem necessary.\n\nLater, the Foodspin team adopted Ansible to automate their application server configuration. They first tested it with the server for The Fish King, a lower-volume customer, and then rolled it out to their other customers. Unfortunately, their code didn’t include the performance optimizations for Curry Hut, so it stripped those improvements. The Curry Hut server slowed to a crawl until the team caught and fixed their mistake.\n\nThe Foodspin team overcame this by parameterizing their Ansible code. It can now set resource levels different for each customer. This way, they still apply the same code across every customer, while still optimizing it for each.\n\nWe frequently need to support specific variations between\n\ninstances of otherwise identical infrastructure. I’ll describe some patterns and antipatterns for dealing with this in Chapter 8.\n\nTHE AUTOMATION FEAR SPIRAL\n\nThe automation fear spiral describes how many teams fall into configuration drift and technical debt.\n\nAt an Open Space session on configuration automation at a DevOpsDays conference, I asked the group how many of them were using automation tools like Ansible, Chef, or Puppet. The majority of hands went up. I asked how many were running these tools unattended, on an automatic schedule. Most of the hands went down.\n\nMany people have the same problem I had in my early days of using automation tools. I used automation selectively—for example, to help build new servers, or to make a specific configuration change. I tweaked the configuration each time I ran it, to suit the particular task I was doing.\n\nI was afraid to turn my back on my automation tools because I lacked confidence in what they would do.\n\nI lacked confidence in my automation because my servers were not consistent.\n\nMy servers were not consistent because I wasn’t running automation frequently and consistently.\n\nThis is the automation fear spiral, as shown in Figure 2-2. Infrastructure teams must break this spiral to use automation successfully. The most effective way to break the spiral is to face your fears. Start with one set of servers. Make sure you can apply, and then re-apply your infrastructure code to these servers. Then schedule an hourly process that continuously applies the code to those servers. Then pick another set of servers and repeat the process. Do this until every server is continuously updated.\n\nFigure 2-2. The automation fear spiral\n\nGood monitoring and automated testing builds the confidence to continuously synchronize your code. This exposes configuration drift as it happens, so you can fix it immediately.\n\nPrinciple: Ensure that you can repeat any process\n\nBuilding on the reproducibility principle, you should be able to\n\nrepeat anything you do to your infrastructure. It’s easier to repeat actions using scripts and configuration management tools than to\n\ndo it by hand. But automation can be a lot of work, especially if\n\nyou’re not used to it.\n\nFor example, let’s say I have to partition a hard drive as a one-off\n\ntask. Writing and testing a script is much more work than just logging in and running the fdisk command. So I do it by hand.\n\nThe problem comes later on, when someone else on my team,\n\nPriya, needs to partition another disk. She comes to the same\n\nconclusion I did, and does the work by hand rather than writing a script. However, she makes slightly different decisions about how to partition the disk. I made an 80 GB /var ext3 partition on my server, but Priya made /var a 100 GB xfs partition on hers. We’re creating configuration drift, which will erode our ability to\n\nautomate with confidence.\n\nEffective infrastructure teams have a strong scripting culture. If you can script a task, script it . If it’s hard to script it, dig deeper.\n\n4\n\nMaybe there’s a technique or tool that can help, or maybe you can\n\nsimplify the task or handle it differently. Breaking work down into scriptable tasks usually makes it simpler, cleaner, and more\n\nreliable.\n\nPHOENIX SERVERS\n\n5\n\nA Phoenix Server is frequently rebuilt, in order to ensure that the provisioning process is repeatable. This can be done with other infrastructure constructs, including infrastructure stacks.\n\nConclusion\n\nThe Principles of Cloud Age Infrastructure embody the differences\n\nbetween traditional, static infrastructure, and modern, dynamic\n\ninfrastructure:\n\nAssume systems are unreliable,\n\nMake everything reproducible,\n\nCreate disposable things,\n\nMinimize variation,\n\nEnsure that you can repeat any action.\n\nThese principles are the key to exploiting the nature of cloud\n\nplatforms. Rather than resisting the ability to make changes with\n\nminimal effort, exploit that ability to gain quality and reliability.\n\nThe next chapter focuses on the types of infrastructure resources provided by cloud platforms. It sets the context for what we define\n\nas code, validate continuously, in small pieces, which are the three\n\ncore practices for implementing infrastructure as code.\n\n1 I learned this idea from Sam Johnson’s article, “Simplifying Cloud: Reliability”.\n\n2 The principle of assuming systems are unreliable drives Chaos Engineering, which injects failures in controlled circumstances to test and improve the reliability of your services. I talk about this more in [Link to Come]\n\n3 I first heard this expression in Gavin McCance’s presentation “CERN Data Centre Evolution”. Randy Bias credits Bill Baker’s presentation “Architectures for Open and Scalable Clouds”. Both of these presentations are an excellent introduction to these principles.\n\n4 My colleague Florian Sellmayr says, “If it’s worth documenting, it’s worth\n\nautomating.”\n\n5 My colleague Kornelis Sietsma coined the term Phoenix Server, which Martin\n\nFowler later documented on his site.\n\nChapter 3. Infrastructure Platforms\n\nThe first thing you need for infrastructure as code is infrastructure. In Chapter 1 I shared a layered platform model (“The parts of an\n\ninfrastructure system”). The infrastructure is the foundational layer of this model.\n\nFigure 3-1. The infrastructure platform is the foundation layer of the platform model from chapter 1\n\nInfrastructure as code requires a dynamic infrastructure platform, something that you can use to provision and change resources on 1 demand with an API. This is the essential definition of a cloud .\n\nWhen I talk about an “infrastructure platform” in this book, you 2 can assume I mean a dynamic, Infrastructure as a Service (IaaS) type of platform.\n\nIn the old days-the Iron Age of computing-infrastructure was hardware. Virtualization decoupled systems from the hardware they ran on. Cloud added APIs to manage those virtualized resources. Thus began the Cloud Age.\n\n3\n\nThere are different types of infrastructure platforms, from full- blown public clouds to private clouds; from commercial vendors to open source platforms. In this chapter, I outline these variations and then describe the different types of infrastructure resources they provide.\n\nAt the basic level, infrastructure platforms provide compute, storage, and networking resources. The platforms provide these\n\nresources in various formats. For instance, you may run compute as virtual servers, container runtimes, and serverless code execution.\n\nDifferent vendors may package and offer the same resources in different ways, or at least with different names. For example, AWS object storage, Azure blob storage, and GCP cloud storage are all pretty much the same thing.\n\nI describe the common types of resources that most cloud platforms provide in this chapter. I want the implementation\n\npatterns and recommendations throughout this book to be useful no matter which cloud you use. So I try to use names that are\n\nrelevant across clouds. Rather than “VPC” and “Subnet,” I talk\n\nabout “Network address blocks.”\n\nThis chapter should be a useful reference if you are wondering how to apply infrastructure as code practices and patterns to a\n\nparticular aspect of your system, for example, container clusters.\n\nNot everyone draws the line between infrastructure resources and application runtime services like container clusters and databases\n\nin the same place. I don’t think it particular matters where you\n\ndraw the boundaries in your system. I have included some things in this chapter and others in [Link to Come], based on my personal\n\npreferences.\n\nWhat is a dynamic infrastructure platform?\n\nPublic clouds are the first thing most of us think of as platforms\n\nfor infrastructure as code. But there are other options for cloud infrastructure.\n\nThe key characteristics of a dynamic infrastructure platform are:\n\nIt provides a pool of compute, networking, and storage resources,\n\nIt allows you to provision and change these resources on demand,\n\nYou can provision, configure, and change resources programmatically.\n\nFigure 3-2. An infrastructure platform provides a pool of resources that you can provision on demand.\n\nA public IaaS cloud clearly has these characteristics. However, many private cloud platforms also qualify. A private cloud may run on a pool of hardware owned by the organization. However, its\n\nusers use an API to provision infrastructure from that hardware pool, so it still counts as a cloud.\n\nWhen performance or other requirements make it useful to run applications directly on server hardware, you can implement a bare-metal cloud. A bare-metal cloud provides an API to provision\n\noperating systems directly onto physical servers on demand.\n\nTable 3-1 lists examples of vendors, products, and tools for each type of cloud infrastructure platform.\n\nTable 3-1. Examples of dynamic infrastructure platforms\n\nType of Platform\n\nProviders or Products\n\nPublic IaaS cloud services\n\nAWS, Azure, Digital Ocean, GCE, Linode, and Oracle Cloud\n\nPrivate IaaS cloud products\n\nCloudStack, OpenStack, and VMware vCloud\n\nBare-metal cloud tools\n\nCobbler, FAI, and Foreman\n\nInfrastructure Resources\n\nThere are three essential resources provided by an infrastructure platform: compute, storage, and networking. Different platforms combine and package these resources in different ways. For\n\nexample, you may be able to provision virtual machines and",
      "page_number": 53
    },
    {
      "number": 3,
      "title": "Infrastructure Platforms",
      "start_page": 69,
      "end_page": 125,
      "detection_method": "regex_chapter_title",
      "content": "container instances on your platform. You may also be able to provision a database instance, which combines compute, storage,\n\nand networking.\n\nI call the more elemental infrastructure resources primitives. Each of the compute, networking, and storage resources described in the sections later in this chapter is a primitive. Cloud platforms\n\ncombine infrastructure primitives into composite resources, such as:\n\nDatabase as a Service (DBaaS)\n\nLoad balancing\n\nDNS\n\nIdentity management\n\nSecrets management\n\nThe line between a primitive and a composite resource is arbitrary, as is the line between a composite infrastructure resource and an\n\napplication runtime service. Even a basic storage service like object storage (think AWS S3 buckets) involves compute and networking resources to read and write data. But it’s a useful\n\ndistinction, which I’ll use now to list common forms of infrastructure primitives. These fall under three basic resource types: compute, storage, and networking.\n\nCompute Resources\n\nCompute resources execute code. At its most elemental, compute is execution time on a physical server CPU core. But platforms provide compute in more useful ways. The most common compute\n\nresource primitives are:\n\nVirtual machines\n\nPhysical servers\n\nContainers\n\nServer clusters\n\nServerless code execution (FaaS)\n\nI’ll describe each of these in more detail.\n\nVirtual machines\n\nVirtual Machines (VMs), sometimes called virtual servers, are the workhorse of cloud infrastructure. The infrastructure platform manages a pool of physical host servers, and runs virtual machine instances on hypervisors across these hosts:\n\nFigure 3-3. Each virtual machine has its own operating system kernel\n\nThe difference between ordinary server virtualization and an IaaS cloud platform is provisioning. To provision a non-cloud VM, you specify which host server it should run on. When you provision a\n\ncloud VM, the cloud platform decides where to run it, and you shouldn’t need to know or care where.\n\nSome clouds automatically migrate live server instances between hosts, transparently to processes running on them. This ability allows platform operators to balance workloads more efficiently.\n\nThey can also carry out maintenance such as patching without downtime. It’s useful to understand how your cloud provider handles this and how it may impact you.\n\nSome platform operators commit to keeping VMs running and\n\navailable, restarting them if they fail, or moving them to a new host if their host fails. Others put the responsibility for this on the user. Check your vendor documentation to understand how it works so that you can be prepared. [Link to Come] discusses\n\ncontinuity of service.\n\n[Link to Come] goes into much more detail about how servers are provisioned, configured, and managed.\n\nPhysical servers\n\nAs I mentioned earlier, although we tend to think of virtual servers\n\nor containers when we think of cloud, it’s entirely possible to provision physical servers on demand. There are several reasons why you might need this:\n\nTo avoid the performance overhead of virtualization. Applications such as real-time trading which demand low latency for processing or data access can benefit from this.\n\nTo allow direct access to physical resources, such as special devices, that might not be accessible from within a virtual machine or container.\n\nTo ensure applications are not impacted by other processes running on the same host as a virtual machine.\n\nTo more strongly enforce segregation of processes and data for security, governance, or contractual requirements.\n\nIf you are the one providing a virtualized or containerized platform to your users, you need to provision and manage the physical host servers your platform runs on.\n\nIMPLEMENTING A BARE-METAL CLOUD\n\nA bare-metal cloud platform lets you write code to install and reinstall OS images onto physical servers automatically. For example, a central process receives a request to provision a server. It:\n\n1. Selects an unused physical server from its inventory,\n\n2. Triggers the server to boot in a “network install” mode supported by the server firmware (e.g., PXE boot),\n\n3. Provides an OS image that the server’s firmware downloads from the network and copies it to the primary hard drive,\n\n4. Reboots the server so that the OS boots,\n\n5. The OS image includes a server configuration agent that runs on startup, retrieves configuration from a server, and applies it.\n\nSome tools that you can use for this process include Cobbler, FAI - Fully Automatic Installation, Foreman, and Crowbar. These tools can take advantage of the PXE specification Preboot Execution Environment to boot a basic OS image downloaded from the network, which then runs an installer to download and boot an OS installer image. The installer image runs a script, perhaps with something like Kickstart, to configure the OS.\n\nOften, triggering a server to use PXE to boot a network image requires pressing a function key while the server starts. Doing this can be tricky to do unattended. However, many hardware vendors have lights-out management (LOM) functionality that makes it possible to do this remotely.\n\nContainers\n\nContainers are a popular way to package and deploy applications\n\nonto cloud platforms. They simplify the packaging and\n\ndeployment process by abstracting the details of the system that runs the application. They are also much smaller than a virtual\n\nmachine, which makes them more efficient and faster to start.\n\nMost cloud platforms offer Containers as a Service (CaaS) to deploy and run container instances. Often, you build a container\n\nimage in a standard format (e.g., Docker), and the platform uses\n\nthis to run instances.\n\nWHAT IS THE DIFFERENCE BETWEEN A VIRTUAL MACHINE AND A CONTAINER?\n\nA Virtual Machine (VM) contains a complete operating system, while a container shares the operating system kernel with its host system.\n\nFigure 3-4. The container instances on a host share the same OS kernel\n\nIncluding the entire OS makes a VM heavier than a container, but each VM running on a host can use a different OS than its host. A container is lighter than a VM because it doesn’t need to package or instantiate an OS kernel. However, a container is coupled to the host OS kernel version.\n\nPeople tend to use containers and VMs in different ways. A single VM often runs multiple applications, while a container instance typically runs only one application.\n\nConceptually, containers are an application packaging format. They are a “fat” package, meaning they include all of the runtime dependencies for the application.\n\nMany application runtime platforms are built around containerization. [Link to Come] discusses this in more detail.\n\nContainers require host servers to run on. Some platforms provide\n\nthese hosts transparently, but many require you to define a cluster\n\nand its hosts yourself.\n\nServer clusters\n\nA server cluster is a pool of server instances-either virtual\n\nmachines or physical servers-which the infrastructure platform provisions and manages as a group. You can configure how the\n\nplatform should manage the cluster. Typical configuration properties include a minimum and a maximum number of servers\n\nto run at a time, algorithms and strategies for adding and removing\n\nservers, and specification of what type of servers to use and how to provision them.\n\nThere are two common types of server clusters, a basic server\n\ncluster, and an application hosting cluster.\n\nBASIC SERVER CLUSTERS\n\nA basic server cluster simply runs the appropriate number of\n\nservers. You need to implement the deployment of applications onto each node of the cluster. Typically, every server in a basic\n\ncluster runs an identical set of applications.\n\nEXAMPLES OF BASIC SERVER CLUSTERS PROVIDED BY THE MAJOR CLOUD PLATFORMS\n\nAWS Auto Scaling Group (ASG)\n\nAzure virtual machine scale set\n\nGoogle Managed Instance Group (MIGs)\n\nYou use a basic server cluster to automatically add compute power\n\nwhen demand increases and to remove it when demand drops. You\n\ncan also use clusters for availability because they can automatically rebuild failed instances.\n\nAPPLICATION HOSTING CLUSTER\n\nLike a basic server cluster, an application hosting cluster is a pool of servers, either virtual machines or physical servers. However,\n\nthe platform adds services that deploy and run applications across\n\nthe servers in the cluster.\n\nYou deploy an application to the cluster, defining some properties for the deployment, and the platform works out where to run the\n\napplication.\n\nFigure 3-5. An application hosting cluster is a group of servers which acts as a single deployment target\n\nUnlike a basic cluster, you typically deploy multiple applications\n\nto an application hosting cluster. The properties you specify for\n\neach application tell the platform how many instances of the application to deploy, with rules for adding and removing\n\ninstances. I go into more detail on application hosting clusters in\n\n[Link to Come].\n\nEXAMPLES OF APPLICATION HOSTING CLUSTERS ON PUBLIC CLOUD PLATFORMS\n\nAmazon Elastic Container Services (ECS)\n\nAmazon Elastic Container Service for Kubernetes (EKS)\n\nAzure Kubernetes Service (AKS)\n\nGoogle Kubernetes Engine (GKE)\n\nServerless code execution (FaaS)\n\nAn infrastructure platform executes serverless code on-demand, in response to an event or schedule, and then terminates it after it has\n\ncompleted its action. Unlike applications deployed on servers or in containers, serverless code does not run continuously, so the\n\nplatform only allocates resources when they are used.\n\nServerless code is useful for well-defined, short-lived actions\n\nwhere the code starts quickly. Typical examples are handling HTTP requests or responding to error events in a message queue.\n\nThe platform launches multiple instances of the code in parallel\n\nwhen needed, for example, to handle multiple events coming in simultaneously.\n\nServerless can be very efficient for workloads where the demand\n\nvaries greatly, scaling up when there are peaks, and not running at all when not needed.\n\n“Serverless” isn’t the most accurate term for this, because of\n\ncourse, the code does run on a server. It’s just that the server is\n\neffectively invisible to you as a developer. The same is true with containers, so what is distinctive about so-called serverless isn’t\n\nthe level of abstraction from servers. The real distinction with\n\nserverless is that it is a short-lived process rather than a long- running process.\n\nFor this reason, many people prefer the term Function as a Service\n\n4 (FaaS) rather than serverless.\n\nSERVERLESS BEYOND CODE\n\nThe serverless concept goes beyond running code. AWS Aurora is essentially a serverless database: the servers involved in hosting the database are transparent to you, unlike more traditional DBaaS offerings like AWS RDS. For that matter, you could consider S3 buckets to be storage as a service.\n\nSome cloud platforms offer specialized on-demand application execution environments. For example, you can use AWS\n\nSageMaker, Azure ML Services, or Google ML Engine to deploy and run machine learning models.\n\nI describe approaches to packaging and deploying serverless code\n\nin [Link to Come].\n\nFOODSPIN EXAMPLE: COMPUTE RESOURCES\n\nLet’s look at Foodspin’s use of compute resources. Their tech stack includes a cluster of containerized Nginx web servers, which is shared across all of their customers. Each customer’s ordering website runs as its own Java process deployed onto a Linux virtual machine. They also have several FaaS applications that carry out reporting activities.\n\nFigure 3-6. Foodspin compute resources\n\nThis example demonstrates how a single system can use multiple types of compute resources. Later examples in this chapter add storage and networking resources to Foodspin’s infrastructure. In later chapters, I use these examples to illustrate how to code, test, and evolve a system.\n\nStorage Resources\n\nMany dynamic systems need storage, such as disk volumes,\n\ndatabases, or central repositories for files. Even if your application\n\ndoesn’t use storage directly, many of the services it does use will need it, if only for storing compute images (e.g., virtual machine\n\nsnapshots and container images).\n\nA genuinely dynamic platform manages and provides storage to\n\napplications transparently. This feature differs from classic virtualization systems, where you need to explicitly specify which\n\nphysical storage to allocate and attach to each instance of compute.\n\nCommon storage resources include:\n\nBlock storage\n\nObject storage\n\nNetworked volumes\n\nStructured storage\n\nSecrets management\n\nI’ll describe each of these in more detail.\n\nBlock storage (virtual disk volumes)\n\nYou can mount a block storage volume on a server or container\n\ninstance as if it was a local disk. Examples of block storage services provided by cloud platforms include AWS EBS, Azure\n\nPage Blobs, OpenStack Cinder, and GCE Persistent Disk.\n\nFigure 3-7. A block storage volume mounted on a virtual machine\n\nSome volumes are ephemeral, automatically created and destroyed\n\nby the platform along with the compute instances that use them.\n\nBut you can also declare persistent volumes, which you can then\n\ncreate and attach, detach, and re-attach to compute instances.\n\nThese volumes can be useful to keep data when you rebuild a\n\nserver.\n\nIt can be useful to replicate volumes for availability scenarios (as\n\ndiscussed in [Link to Come]) and even scalability. For example,\n\nwith some distributed databases, you can add a node to a cluster by\n\nduplicating a disk volume and mounting it on a newly provisioned\n\nserver. Doing this cuts the time it takes for data to synchronize to\n\nthe new node since it only copies recent changes.\n\nA block volume looks like a local disk drive to an application\n\nrunning on a VM, but depending on the platform implementation,\n\nit could be hosted over a network. The latency for reads and writes\n\nover the network may cause performance issues.\n\nIf performance is a problem, you should research how your\n\nplatform implements block volumes. Run tests to emulate your use\n\ncases, and then tune your configuration and use of storage to get\n\nthe performance you need. Your platform may provide different\n\nstorage options for higher performance, such as solid-state drives or faster I/O.\n\nObject storage\n\nMost infrastructure platforms provide an object storage service,\n\nwhich you can use to store and access files over the network.\n\nAmazon’s S3, Azure Block Blobs, Google Cloud Storage, and OpenStack Swift are all examples.\n\nWhile only one compute instance can mount a block storage\n\nvolume at a time, any instance can access object storage. So object\n\nstorage is useful for creating and sharing files between instances.\n\nFigure 3-8. Multiple compute instances can access object storage\n\nObject storage is usually cheaper and more reliable than block\n\nstorage, but with higher latency.\n\nNetworked filesystems (shared network volumes)\n\nThere are many standard protocols for multiple servers to mount\n\nshared storage volumes locally. Examples include NFS, AFS, and\n\n5\n\nSMB/CIFS (Figure 3-9). Like object storage, and unlike block\n\nstorage, a networked storage volume is available to multiple\n\ncompute instances at the same time. Like block storage, compute instances treat a shared network volume as if it was a local hard\n\ndrive.\n\nFigure 3-9. Multiple compute instances can mount the same networked file volume\n\nSome platforms offer networked file systems as a service using\n\nstandard protocols, for example, AWS EFS and Azure File Storage. Alternatively, you can run a file server on compute\n\ninstances you build and manage yourself. Some people do this as\n\npart of a migration strategy, copying legacy systems from data\n\ncenters into the cloud.\n\nHowever, a cloud platform’s managed storage services are\n\noptimized for its physical networking. Optimizing networked file\n\nsharing is challenging even when you manage your own data\n\ncenters and physical networking. When using someone else’s\n\nvirtualized infrastructure, you’ll probably get better performance\n\nand reliability by using their optimized file sharing solutions than you would by building your own.\n\nOf course, it’s always best to test and measure to be sure you’re\n\ngetting the results you need from your system in terms of\n\nperformance and availability.\n\nENCRYPTION AT REST\n\nAll of these storage resource primitives should have encryption capability built in. You can easily set an option in your infrastructure code to encrypt all data stored on the resource “at rest” (as opposed to encrypting “in transit”, when it is passing over the network).\n\nYou should fully understand your threat model and how encryption does and does not address your threats. You especially need to understand how the platform manages the keys to encrypt and decrypt your data. Typically, these keys are available to the cloud platform itself. Anyone with privileged access to your platform (such as employees of your hosting vendor) may be able to gain access to these keys. Anyone with access to manage your infrastructure (such as your infrastructure team members) can use the keys to decrypt the data.\n\nFor example, even if someone is unable to gain direct access to the keys, if they have permission to use an API to read data from the encrypted storage, then the encryption does not give any protection from that person doing intentional or unintentional harm.\n\nKnow how your platform manages encryption keys, and what options you have to control access to them. For example, you may be able to create your own keys and upload them to the platform. Most important, understand your actual threat models, and make sure you understand how to use encryption to provide real value.\n\nStructured data storage\n\nMost hosted infrastructure platforms offer services for storing\n\nstructured data. These are typically Database as a Service (DBaaS) systems managed by the provider. A DBaaS may handle things\n\nlike:\n\nThe server operating system\n\nDatabase software\n\nStorage devices\n\nScaling clusters\n\nGeographical distribution\n\nReplication\n\nBackups\n\nIdeally, it gives you a certain amount of control over configuration and optimization.\n\nIn addition to the vendor managing the database for you, a DBaaS\n\ntypically integrates with the platform’s other services. For\n\nexample, your database can use the platform’s authentication and authorization, encryption, and monitoring services.\n\nMany DBaaS services offer managed instances of standard\n\ncommercial or open source database applications, such as MySQL,\n\nPostgres, and SQL Server.\n\nSome platforms also offer custom structured data storage services.\n\nExamples include key-value stores and formatted document stores,\n\ne.g., for storing and searching JSON or XML content. These\n\nservices may be optimized to make better use of the underlying infrastructure platform to deliver high performance, geographical\n\ndistribution, and availability.\n\nThe major cloud vendors offer data storage and processing\n\nservices for managing, transforming, and analyzing large amounts of data. These include services for batch data processing, map-\n\nreduce, streaming, indexing and searching, and more. Understand\n\nthe tradeoffs of each of these services, and test them for your\n\nworkloads and use cases, to get the most value from them.\n\nSecrets management\n\nAny storage resource can be encrypted so you can store\n\npasswords, keys, and other information which attackers might\n\nexploit to gain privileged access to systems and resources. A\n\nsecrets management service adds functionality specifically\n\ndesigned to help manage these kinds of resources. Features of a secrets management service may include:\n\nAutomatically generating and rotating secrets\n\nGiving access to secrets to automated services so they can carry out controlled actions\n\nTracking and auditing usage of secrets\n\nGenerating short-term, single-use secrets\n\nFOODSPIN EXAMPLE: STORAGE RESOURCES\n\nLet’s return to the Foodspin team to see how they added storage resources to their system.\n\nThe Nginx web server container images include configuration files and static content, mainly for Foodspin branding, so they don’t need any persistent local storage. They do send web server logs to object storage. The team can use other tools and services to download the logs from there to analyze user traffic.\n\nEach customer has a virtual server instance running a Java application. Each instance has a block storage volume where the application stores local files. Whenever they rebuild a server, they unmount the volume from the old server and attach it to the new one so that the application can resume service.\n\nFigure 3-10. Foodspin storage resources\n\nEach customer application also uses its own PostgreSQL database. Foodspin creates a single Postgress server cluster, using their cloud provider’s DBaaS service. When the team provisions a new customer, they create a new database in this cluster. In the future, they may need more than one cluster to scale the number of customers they can support, but for the moment, this is enough.\n\nNetwork Resources\n\nAs with the other types of infrastructure resources, the capability\n\nof dynamic platforms to provision and change networking on\n\ndemand, from code, creates great opportunities. These\n\nopportunities go beyond changing networking more quickly; they also include much safer use of networking.\n\nPart of the safety comes from the ability to quickly and accurately\n\ntest a networking configuration change before applying it to a critical environment. Beyond this, Software Defined Networking\n\n(or SDN) makes it possible to create finer-grained network\n\nsecurity constructs than you can do manually. This is especially\n\ntrue with systems where you create and destroy elements\n\ndynamically.\n\nZERO-TRUST SECURITY MODEL WITH SDN\n\n6\n\nA zero-trust security model secures every service, application, and other resource in a system at the lowest level. This is different from a traditional perimeter-based security model, which assumes that every device inside a secure network can be trusted.\n\nIt’s only feasible to implement a zero-trust model for a non-trivial system by defining the system as code. The manual work to manage controls for each process in the system would be overwhelming otherwise.\n\nFor a zero-trust system, each new application is annotated to indicate which applications and services it needs to access. The platform uses this to automatically enable the required access and ensure everything else is blocked.\n\nThe benefits of this type of approach include:\n\nEach application and service has only the privileges and access it explicitly requires, which follows the principle of least privilege,\n\nZero-trust, or perimeterless security involves putting smaller-scoped barriers and controls onto the specific resources that need to be protected. This approach avoids the need to allow broad-scoped trust zones, for example, granting trust to every device attached to a physical network. Google documents their approach to this with its BeyondCorp security model.\n\nThe security relationships between elements of the system are visible, which enables validation, auditing, and reporting.\n\nSome typical types of networking constructs and services an\n\ninfrastructure platform may support include:\n\nNetwork address blocks\n\nTraffic management and routing\n\nNetwork access rules\n\nCaches\n\nService meshes\n\nNetwork address blocks\n\nNetwork address blocks are a fundamental structure for grouping\n\nentities involved in network communication. The top-level block in AWS is a VPC (Virtual Private Cloud). In Azure, GCP, and\n\nothers, it’s a Virtual Network. The top-level block is often divided\n\ninto smaller blocks, such as subnets, as shown in Figure 3-11.\n\nFigure 3-11. Network address blocks\n\nThe primary purpose of address blocks is for routing network traffic. Given a compute instance, you assign it an interface and\n\naddress in an address block. The instance can then communicate\n\nwith other entities in the same block. Traffic routing constructs\n\n(described shortly) enable communication across blocks.\n\nYou can use blocks to segregate entities by defining rules for what\n\nconnections are allowed in and out of a block. It’s common to\n\ndefine blocks in vertical tiers. For example, a “public” tier block\n\nallows connections in from the Internet; a “private” tier block only\n\naccepts connections from the public block. You can improve\n\nsecurity with network access rules (see below), and by securing individual entities in a zero-trust model.\n\nAddress blocks may or may not correspond to physical\n\nnetworking. You can often assume that entities attached to the\n\nsame block (e.g., in the same subnet) have lower latency when communicating with each other than if they were in separate\n\nblocks. If you require low network latency, you should research\n\nhow your platform implements networking and carry out tests for\n\nyour workload to find the best configuration.\n\nAWS also uses its networking blocks (subnets) to indicate the\n\nphysical location, in terms of geography and data centers, of\n\nresources attached to them. Each subnet corresponds to a particular\n\ndata center, with a designated availability zone (AZ). This scheme\n\nhas implications for availability - if AWS has a data center outage, it impacts all of the entities in that subnet. Assigning redundant\n\nresources (e.g., compute instances) to different subnets with\n\ndifferent availability zones spreads the risk across more than one\n\nlocation.\n\nFOODSPIN EXAMPLE: ADDRESS BLOCKS\n\nOur friends at Foodspin use a single high level address block (a VPC) for everything running in a single geographical region. They divide this into smaller blocks (subnets).\n\nThey use three subnets for their web server containers. They have configured the cluster to provision a host node in each subnet. Because their cloud provider uses a different physical data center for each subnet, this arrangement spreads web server workload across different locations, which improves resilience. Although a data center failure will take down some number of customer application servers, as I’ll describe in a moment, they can still serve a branded error page.\n\nFigure 3-12. Foodspin address blocks\n\nFoodspin creates two more subnets for the application servers. They provision each new server in one of these subnets, and the team tries to keep them spread reasonably evenly across them.\n\nIf one of their cloud vendor’s data centers fails, the Foodspin team can create a new application server for each affected customer. Because these servers require persistent data, this only works if they either have access to the data volumes or recent backups. [Link to Come] discusses ways they can approach this, as well as better solutions for availability.\n\nTraffic management and routing\n\nThere are many ways to route and manage network traffic. These\n\ninclude:\n\nNames, such as DNS entries, which are mapped to lower- level network addresses, usually IP addresses,\n\nRoutes, which configure what traffic is allowed between and within address blocks,\n\nGateways and NAT (Network Address Translation) gateways, which may be needed to direct traffic in and out of blocks, depending on the platform,\n\nLoad balancers, which forward each connection coming into a single address to one of a pool of resources,\n\nProxies, which accept connections, and may carry out transformations or routing based on different aspects of the connection,\n\nAPI gateways are proxies, usually for HTTP/S connections, which can carry out activities to handle non- core aspects of APIs, such as authentication and rate throttling. See also “Service meshes”,\n\nVPNs (Virtual Private Networks), which allow different address blocks to be connected across locations so that they appear to be part of a single network,\n\nDirect connections, a dedicated connection set up between a cloud network and another location, typically a data center or office network.\n\nOne of the issues with traffic routing in a dynamic system is that the target locations may change. For example, if you run a cluster\n\nof web servers as described in “Foodspin Example: Address\n\nblocks”, there are times when you need to rebuild or even move\n\nthe cluster, which can change its IP address.\n\nDESIGNING FOR DISPOSABILITY MEANS CHANGES ARE ROUTINE RATHER THAN CATASTROPHIC\n\nThis is a concrete example of the disposability principle (“Principle: Create disposable things”). The traditional (Iron Age) approach assumes that a cluster such as the one in this example will not change very often. When it does need to be changed, it’s a disruptive event, which probably involves downtime, people working overnight, and manually fixing problems discovered after the cutover.\n\nThe Cloud Age approach recognizes that you need to make changes to a system such as a web server cluster frequently. When a security vulnerability requires applying a patch, when a system upgrade is released, or when you need to make a configuration change to improve performance or reliability, you would like to be able to carry this out quickly and with little work, rather than needing to undertake a major project.\n\nFor this reason, you design your systems assuming that any component will go away. This not only makes it safer to carry out planned changes, it also makes it easier to handle unplanned failures.\n\nYou can use dynamic routing resources to continue routing traffic\n\nto a resource after its IP address changes. A few options that you\n\ncan use to do this include:\n\nDefine DNS entries as code and integrate this code with the code that defines the cluster. When you change the cluster, you can automatically update the DNS entries by re-applying the code.\n\nDefine a static IP address (e.g., AWS Elastic IP or ENI), and direct external traffic to this address rather than to the cluster’s address. You can map the static address to the new cluster when it changes. As with the DNS-based solution, it takes little effort to safely apply this change across the resources when you have defined them as code.\n\nCreate a load balancer, again defining it as code and connecting it to the cluster configuration. Doing this has the added benefit that you may be able to create the new cluster while the old cluster is still running, giving you options for handling the change with little or no downtime (see [Link to Come]).\n\nFOODSPIN EXAMPLE: NETWORK ROUTING\n\nGiven the address blocks described earlier (“Foodspin Example: Address blocks”), the team at Foodspin need some routing. This diagram shows how they’ve configured some of this:\n\nFigure 3-13. Foodspin network routing\n\nThey have a DNS name for each of their customers, which maps to a public IP addresses on the internet gateway. Their gateway routes connections, based on which address it’s coming into, to a load balancer, which distributes requests among servers in a pool.\n\nThe details of networking are outside the scope of this book, so\n\ncheck the documentation for your platform provider, and perhaps a reference such as Craig Hunt’s TCP/IP Networking Adminstration.\n\nNetwork access rules\n\nInfrastructure platforms should provide a way to define access rules as code. These may map directly to physical firewall rules, or\n\nthe platform may use virtual firewall mechanisms. Some platforms impose these directly on compute instances.\n\nDefining these rules in code makes it easier to secure dynamic\n\nresources, and to secure them at a finer level of granularity than with other means. For example, your system may include\n\napplication servers running in a single network segment. This arrangement permits connections between application servers, which an attacker who compromises one server can exploit to gain\n\naccess to the others, as shown in Figure 3-14.\n\nFigure 3-14. When a bad actor gains access, they are trusted by everything\n\nInstead, you can define access rules for each application server to restrict access to between servers, as shown in Figure 3-15.\n\nFigure 3-15. A bad actor who gains access to one server cannot access the others\n\nIn Chapter 6, I’ll explain how to write code that you can reuse to\n\ndefine multiple instances of infrastructure. Reusable code makes it easier to secure each instance at the right level.\n\nCaches\n\nSome services cache content to improve latency. A CDN (Content Distribute Network) is a service that can distribute static content\n\n(and in some cases executable code) to multiple locations geographically, usually for content delivered using HTTP/S.\n\nDefining caching and CDN services as infrastructure code can make it easier to integrate other networking services. For example, code that defines a CDN can automatically update the CDN\n\nconfiguration when the content server is rebuilt or moves.\n\nService meshes\n\nA service mesh is a decentralized network of services that dynamically manages connectivity between parts of a distributed system. It moves networking capabilities from the infrastructure\n\nlayer to the application runtime layer of the model I described in “The parts of an infrastructure system”. In a typical service mesh\n\nimplementation, each application instance delegates communication with other instances to a sidecar process.\n\nFigure 3-16. Sidecars enable communication with other processes in a service mesh\n\nSome of the services that a service mesh can provide to applications include:\n\nRouting - Direct traffic to the most appropriate instance of a given application, wherever it is currently running. Dynamic routing with a service mesh enables advanced deployment scenarios, such as blue-green and canary, as described in [Link to Come].\n\nAvailability - Enforce rules for limiting numbers of requests, for example, circuit breakers.\n\nSecurity - handle encryption, including certificates.\n\nAuthentication - enforce rules on which services can connect to which. Manage certificates for peer to peer authentication.\n\nObservability, monitoring, and troubleshooting - record connections and other events so that people can trace requests through complex distributed systems.\n\nA service mesh works well in combination with application\n\nhosting clusters (“Application hosting cluster”). The application cluster decoupled the deployment of applications from the infrastructure layer. The service mesh then decouples application\n\ncommunication from the infrastructure layer. The benefits of this model are:\n\nSimplify application development, by moving common concerns out of the application and into the sidecar,\n\nMake it easier to build and improve common concerns across your estate, since you only need to deploy updates to the sidecar, without needing to make code changes to all of your applications and services,\n\nHandle the dynamic nature of application deployment, since the same orchestration and scheduling system that deploys and configures application instances (e.g., in containers) can deploy and configure the sidecar instances along with them.\n\nSome examples of service meshes include Hashicorp Consul, Envoy, Istio, and Linkerd.\n\nService meshes are most commonly associated with containerized\n\nsystems. However, you can implement the model in non- containerized systems, for example, by deploying sidecar processes onto virtual machines.\n\nA service mesh does add complexity. As with cloud-native architectural models like microservices, a service mesh is\n\nappealing because it simplifies the development of individual applications. However, the complexity does not disappear; you’ve only moved it out into the infrastructure. So your organization\n\nneeds to be prepared to manage this, including being ready for a steep learning process.\n\nIt’s essential to keep clear boundaries between networking\n\nimplemented at the infrastructure level, and networking implemented in the service mesh. Without a good design and\n\nimplementation discipline, you may duplicate and intermingle concerns. Your system is harder to understand, riskier to change,\n\nand harder to troubleshoot.\n\nMULTICLOUD, POLYCLOUD, HYBRID CLOUD\n\nMay organizations end up hosting across multiple platforms. A few terms crop up to describe variations of this:\n\nHybrid Cloud\n\nHosting applications and services for a system across both private infrastructure and a public cloud service. People often do this because of legacy systems that they can’t easily migrate to a public cloud service (such as services running on mainframes). In other cases, organizations have requirements that public cloud vendors can’t currently meet, such as legal requirements to host data in a country where the vendor doesn’t have a presence.\n\nCloud Agnostic\n\nBuilding systems so that they can run on multiple public cloud platforms. People often do this hoping to avoid lock-in to one vendor. In practice, this results in locks-in to software that promises to hide differences between clouds; or involves building and maintaining vast amounts of customized code; or both.\n\nPolycloud\n\nRunning different applications, services, and systems on more than one public cloud platform. This is usually to exploit different strengths of different platforms.\n\nConclusion\n\nThe different types of infrastructure resources and services\n\ncovered in this chapter are the pieces that you arrange into useful systems - the “infrastructure” part of Infrastructure as Code. In the\n\nnext chapter, I explain the first of the three core practices for infrastructure as code (define everything as code, validate continuously, use small pieces). Then I combine that practice with\n\nthis chapter’s discussion of cloud platforms to explore patterns for using code to define infrastructure stacks (Chapter 5).\n\n1 The US National Institute of Standards and Technology (NIST) has an excellent\n\ndefinition of cloud computing.\n\n2 Here’s how NIST defines Infrastructure as a Service: “The capability provided to the consumer is to provision processing, storage, networks, and other fundamental computing resources where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer\n\ndoes not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls).”\n\n3 Virtualization technology has been around since the 1960s, but emerged into the\n\nmainstream world of x86 servers in the 2000s.\n\n4 I tend to use serverless. FaaS is more technically accurate, but when I say FaaS, I usually have to explain what it means. The easiest way to explain what FaaS means is to say, “serverless.” I’m lazy, so I find it simpler to say “serverless” in the first place.\n\n5 Network File System, Andrew File System, and Server Message Block,\n\nrespectively.\n\n6 Google documents their approach to zero-trust (also known as perimeterless) with\n\ntheir BeyondCorp security model\n\nChapter 4. Core Practice: Define everything as code\n\nIn Chapter 1, I identified three core practices that help you to change infrastructure rapidly and reliably:\n\nDefine everything as code\n\nContinuously validate all work in progress\n\nBuild small, simple pieces that you can change independently\n\nIn this chapter, I explore the first of these core practices. Why would you want to define your infrastructure as code? What do you need to do this? Then I explore some issues about the nature\n\nof infrastructure coding languages.\n\nThis seemingly banal subject is a hot topic in the industry at the time of this writing, with debate raging over the nature of configuration vs. coding, and special-purpose languages vs.\n\nstandard programming languages.\n\nI’ll close this chapter with some implementation principles to\n\nguide you in deciding how to organize your infrastructure code.\n\nThe goal of this chapter is to lay out the fundamental concepts of coding infrastructure. With these in place, later chapters offer specific patterns and recommendations for implementation. I\n\nexplain how to organize infrastructure resources into useful units\n\nthat I call stacks in Chapter 5 and the following chapters. I expand\n\nthese to servers, clusters, and other application runtime platforms in later chapters.\n\nWhy you should define your infrastructure as code\n\nEven given a dynamic cloud platform, there are simpler ways to provision infrastructure than writing code and running a tool. Go\n\nto the platform’s web-based user interface and poke and click an application server cluster into being. Drop to the prompt, and using your command-line prowess, wield the vendor’s CLI (Command-\n\nLine Interface) tool to forge an unbreakable network boundary.\n\nBut seriously, the previous chapters have explained why it’s better to use code to build your systems. As a quick recap of “Core practice: Define everything as code”, some of the benefits of defining things as code are:\n\nReusability\n\nIf you define a thing as code, you can create many instances of it. You can repair and rebuild your things quickly. Other people can build identical instances of the thing.\n\nConsistency\n\nThings built from code are built the same way every time. This makes system behavior predictable. This makes testing more reliable. This enables continuous validation.\n\nTransparency\n\nEveryone can see how the thing is built by looking at the code, which helps in many ways. People can review the code and\n\nsuggest improvements. They can learn things to use in other code. They gain insight to help with troubleshooting. They can review and audit for compliance.\n\nWhat you can define as code\n\nEvery infrastructure tool has a different name for its source code-\n\nfor example, playbooks, cookbooks, manifests, and templates. I refer to these in a general sense as infrastructure code, or sometimes as an infrastructure definition.\n\nInfrastructure code specifies both the infrastructure elements you want and how you want them configured. You run an infrastructure tool to apply your code to an instance of your\n\ninfrastructure. The tool either creates new infrastructure, or it modifies existing infrastructure to match what you’ve defined in your code.\n\nSome of the things you might define as code include:\n\nAn infrastructure stack is a collection of elements provisioned from an infrastructure cloud platform. I write about infrastructure platforms in Chapter 3, and discuss stacks in Chapter 5.\n\nElements of a server’s configuration, such as packages, files, user accounts, and services. ([Link to Come])\n\nA server role is a collection of server elements that are applied together to a single server instance ([Link to Come])\n\nA server image definition generates an image for building multiple server instances. ([Link to Come])\n\nAn application package defines how to build a deployable application artifact, including containers. ([Link to Come])\n\nConfiguration and scripts for delivery services, which include pipelines and deployment. ([Link to Come])\n\nConfiguration for operations services, such as monitoring checks. ([Link to Come])\n\nValidation rules, which include automated tests and compliance rules. (Chapter 9)\n\nChoose tools that are configured with code\n\nInfrastructure as Code, by definition, involves specifying your infrastructure in text-based files. You manage these files separately\n\nfrom the tools that you use to apply them to your system. You can read, edit, analyze, and manipulate your specifications using any\n\ntools you want.\n\nOther infrastructure automation tools store your infrastructure\n\nspecifications as data that you can’t access directly. Instead, you can only use and edit the specifications by using the tool itself.\n\nThe tool may have some combination of GUI, API, and command- line.\n\nThe issue with these black-box tools is that they limit the practices\n\nand workflows you can use:\n\nYou can only version your infrastructure specifications if the tool has built-in versioning,\n\nYou can only use Continuous Integration if the tool has a way to trigger a job automatically when you make a change,\n\nYou can only create delivery pipelines if the tool makes it easy to version and promote your infrastructure specifications,\n\nYou can only split monolithic infrastructure into independent pieces if the tool supports it.\n\nLESSONS FROM SOFTWARE SOURCE CODE\n\nThe externalized configuration pattern mirrors the way most software source code works. Some development environments keep source code hidden away, such as Visual Basic for Applications. But for non-trivial systems, developers prefer keeping their source code in external files.\n\nIt is challenging to use agile engineering practices such as Test-\n\nDriven Development, Continuous Integration, and Continuous Delivery with black-box infrastructure management tools.\n\nA tool that uses external code for its specifications doesn’t\n\nconstrain you to use a specific workflow. You can use an industry- standard source control system, text editor, CI server, and\n\nautomated testing framework. You can build delivery pipelines\n\nusing the tool that works best for you.\n\nAUTOMATING BLACK BOX CONFIGURATION\n\nSometimes a black-box tool’s configuration can be exported and imported by unattended scripts. This might make it possible to integrate the tool with infrastructure as code, although it will be clumsy. It’s certainly not recommended for a core infrastructure tool, but could be useful for a specialized tool that doesn’t have a good alternative.\n\nThis model has some limitations. It’s often difficult to merge changes from different dumps, which means different team members can’t work on different parts of the configuration at the same time. People must be careful about making changes to downstream instances of the tool to avoid conflicts with upstream work.\n\nDepending on the format of the configuration dump, it may not be very transparent-it may not be easy to tell at a glance the differences between one version and the next.\n\nAnother way to approach black-box configuration is by injecting configuration automatically. Ideally, this would be done using an API provided by the tool. I have also seen this done by automating interaction with the tool’s UI—for example, using scripted tools to make HTTP requests and post forms. But UI automation is brittle, breaking whenever a new version changes the UI. A well-supported API keeps backward compatibility, and so is more reliable.\n\nThe configuration injection approach allows configuration to be defined in external files. These may simply be scripts, or could involve a configuration format or DSL. If a tool is configured by automated injection from external files, then this should be the only way configuration changes are made. Changes made through the UI are likely to cause conflicts and inconsistencies.\n\nManage your code in a version control system\n\nIf you’re defining your stuff as code, then putting that code into a\n\nversion control system (VCS) is simple and powerful. By doing this, you get:\n\nTraceability\n\nVCS provides a history of changes, who made them, and context about why . This history is invaluable when debugging problems.\n\n1\n\nRollback\n\nWhen a change breaks something—and especially when multiple changes break something—it’s useful to be able to restore things to exactly how they were before.\n\nCorrelation\n\nKeeping scripts, specifications, and configuration in version control helps when tracing and fixing gnarly problems. You can correlate across pieces with tags and version numbers.\n\nVisibility\n\nEveryone can see each change committed to the version control system, giving the team situational awareness. Someone may notice that a change has missed something important. If an incident happens, people are aware of recent commits that may have triggered it.\n\nActionability\n\nThe VCS can trigger an action automatically for each change committed. Triggers enable CI jobs and CD pipelines.\n\nRECOMMENDATION: AVOID BRANCHING\n\nBranching is version control system feature that allows people to work on code in separate streams. There are many popular workflows based around branches. I’ll explain how these workflows can conflict with the core practice of Continuous Integration in [Link to Come].\n\nSecrets and source code\n\nSystems need various secrets. Your stack tool may need a password or key to use your platform’s API to create and change infrastructure. You may also need to provision secrets into environments, for example making sure an application has the\n\npassword for its database.\n\nIt’s essential to handle these types of secrets in a secure way from the very beginning. Whether you are using a public cloud or a\n\nprivate cloud a leaked password can have terrible consequences. So even when you are only writing code to learn how to use a new\n\ntool or platform, you should never put secrets into code. There are many stories of people who checked a secret into a source repository they thought was private, only to find it had been\n\ndiscovered by hackers who exploited it to run up huge bills.\n\nOne solution to this is to encrypt secrets in order to store them in code . Infrastructure developers and unattended systems need to be able to decrypt these secrets without storing a secret. So you still have at least one secret to manage outside of source control!\n\n2\n\nThere are a few approaches for handling secrets needed by\n\ninfrastructure code without actually putting them into code. The include secretless authorization, runtime secret injection, and disposable secrets.\n\nSECRETLESS AUTHORIZATION\n\nMany services and systems provide ways to authorize actions without using secrets. Most cloud platforms can mark a compute\n\nservice-such as a virtual machine or container instance-as authorized for privileged actions.\n\nFor example, an AWS EC2 instance can be assigned an IAM profile that gives processes on the instance rights to carry out a set of API commands. If you configure a stack management tools to\n\nrun on one of these instances, you avoid the need to manage a secret that might be exploited by attackers.\n\nIn some cases, secretless authorization can be used to avoid the need to provision secrets on infrastructure when it is created. For\n\nexample, an application server might need to access a database instance. Rather than a server configuration tool provisioning a\n\npassword onto the application server, the database server might be configured to authorize connections from the application server, perhaps based on its network address.\n\nTying privileges to a compute instance or network address only shifts the possible attack vector. Anyone gaining access to that\n\ninstance can exploit those privileges. You need to put in the work to protect access to privileged instances. On the other hand, someone gaining access to an instance may be able to access\n\nsecrets stored there, so giving privileges to the instance may not be any worse. And a secret can potentially be exploited from other locations, so removing the use of secrets entirely is generally a good thing.\n\nINJECTING SECRETS AT RUNTIME\n\nWhen you can’t avoid using secrets for stacks or other infrastructure code, you can explore ways to inject secrets at\n\nruntime. You’ll normally implement it as stack parameters, which is the topic of Chapter 8. I describe the details of handling secrets as parameters with each of the patterns and antipatterns in that\n\nchapter.\n\nThere are two different runtime situations to consider, local development and unattended agents. People who work on infrastructure code will often keep secrets in a local file that isn’t\n\n3\n\nstored in version control. The stack tool could read that file directly, especially appropriate if you’re using the stack configuration file pattern (“Pattern: Stack Configuration Files”). Or the file could be a script that sets the secrets in environment\n\nvariables, which works well with the stack environment variables pattern (“Pattern: Stack Environment Variables”).\n\nThese approaches also work on unattended agents, such as those\n\n4\n\nused for CI testing or CD delivery pipelines . But you need to store the secrets on the server or container that runs the agent. Alternatively, you can use secrets management features of your agent software to provide secrets to the stack command, as with\n\nthe pipeline stack parameters pattern (“Pattern: Pipeline Stack Parameters”). Another option is to pull secrets from a secrets management service (of the type described in “Secrets\n\nmanagement”), which aligns to the stack parameter registry pattern (“Pattern: Stack Parameter Registry”).\n\nDISPOSABLE SECRETS\n\nA cool thing you can do with dynamic platforms is to create secrets on the fly, and only use them on a “need-to-know” basis. In the database password example, the code that provisions the database automatically generates a password and passes it to the\n\ncode that provisions the application server. Humans don’t ever need to see the secret, so it’s never stored anywhere else.\n\nYou can apply the code to reset the password as needed. If the application server is rebuilt, you can re-run the database server\n\ncode to generate a new password for it.\n\nSecrets management services, such as Hashicorp Vault, can also generate and set a password in other systems and services on the fly. It can then make the password available either to the stack tool when it provisions the infrastructure, or else directly to the\n\nservices that uses it, such as the application server.\n\nInfrastructure coding languages\n\nSystem administrators have been using scripts to automate infrastructure management tasks for decades. General-purpose scripting languages like Bash, Perl, Powershell, Ruby, and Python\n\nare still an essential part of an infrastructure team’s toolkit.\n\nCFEngine pioneered the use of declarative, Domain Specific Languages (DSL - see “DSLs for infrastructure” a bit later) for infrastructure management. Puppet and then Chef emerged\n\nalongside mainstream server virtualization and IaaS cloud. Ansible, Saltstack, and others followed.\n\nStack-oriented tools like Terraform and CloudFormation arrived a few years later, following the same declarative DSL model.\n\n5\n\nRecently , there is a trend of new infrastructure tools that use\n\nexisting general-purpose programming languages to define infrastructure. Pulumi and the AWS CDK (Cloud Development Kit) are examples, supporting languages like Typescript, Python, and Java. These newer languages use procedural language\n\nstructures rather than being declarative.\n\nThe principles, practices, and patterns in this book should be relevant regardless of what language you use to implement them. The languages we use should make it easy to write code that is\n\neasy to understand, test, maintain, and improve. Let’s review the evolution of infrastructure coding languages and consider how different aspects of language choice affect this goal.\n\nScripting your infrastructure\n\nBefore standard tools appeared for provisioning cloud infrastructure declaratively, we wrote scripts in general-purpose, procedural languages. These used SDK (Software Development\n\nKit) libraries to interact with the cloud provider’s API.\n\nExample 4-1 uses pseudo-code, but is similar to scripts that I wrote in Ruby with the AWS SDK. It creates a server named my_application_server and then runs the (fictional) servermaker tool to configure it.\n\nExample 4-1. Example of procedural code that creates a server import 'cloud-api-library'\n\nnetwork_segment = CloudApi.find_network_segment('private')\n\napp_server = CloudApi.find_server('my_application_server') if(app_server == null) { app_server = CloudApi.create_server( name: 'my_application_server', image: 'base_linux', cpu: 2, ram: '2GB', network: network_segment ) while(app_server.ready == false) { wait 5 } app_server.provision( provisioner: servermaker, role: tomcat_server ) }\n\nThis script combines what and how. It specifies attributes of the\n\nserver, including the CPU and memory resources to provide it, what OS image to start from, and what Ansible role to apply to the\n\nserver. It also implements logic: it checks whether the server\n\nnamed my_application_server already exists, to avoid creating a duplicate server, and then it waits for the server to become ready\n\nbefore running Ansible on it. The script would need additional logic to handle errors, which I didn’t include in the example.\n\nThe example code also doesn’t handle changes to the server’s\n\nattributes. What if you need to increase the RAM? You could\n\nchange the script so that if the server exists, the script will check each attribute and change it if necessary. Or you could write a new\n\nscript to find and change existing servers.\n\nMore realistic scenarios include multiple servers of different types.\n\nIn addition to our application server, my team had web servers and database servers. We also had multiple environments, which meant\n\nmultiple instances of each server.\n\nTeams I worked with often turned simplistic scripts like the one in this example into a multi-purpose script. This kind of script would\n\ntake arguments specifying the type of server and the environment,\n\nand use these to create the appropriate server instance. We evolved this into a script that would read configuration files that specify\n\nvarious server attributes.\n\nI was working on a script like this, wondering if it would be worth\n\nreleasing it as an open-source tool, when Hashicorp released the first version of Terraform.\n\nBuilding infrastructure with declarative code\n\nTerraform, like most other stack provisioning tools and server configuration tools, uses a declarative language. Rather than a\n\nprocedural language, which executes a series of statements using\n\ncontrol flow logic like if statements and while loops, a declarative\n\nlanguage is a set of statements that declare the result you want.\n\nExample 4-2 creates the same server as Example 4-1. The code in\n\nthis example (as with most code examples in this book) is a 6 fictional language .\n\nExample 4-2. Example of declarative code virtual_machine: name: my_application_server source_image: 'base_linux' cpu: 2 ram: 2GB network: private_network_segment provision: provisioner: servermaker role: tomcat_server\n\nThis code doesn’t include any logic to check whether the server\n\nalready exists or to wait for the server to come up before running Ansible. The tool that applies the code implements this logic,\n\nalong with error-handling. The tool also checks the current\n\nattributes of infrastructure against what is declared, and work out what changes to make to bring the infrastructure in line. So to\n\nincrease the RAM of the application server in this example, you can edit the file and re-run the tool.\n\nDeclarative infrastructure tools like Terraform and Chef keep the what and how separate. The code you write as a user of the tool\n\nonly declares the attributes of your infrastructure. The tool implements the logic for how to make it happen. As a result, your\n\ncode is cleaner and more direct.\n\nIDEMPOTENCY\n\nTo continuously synchronize system definitions, the tool you use for this must be idempotent. No matter how many times you run it, the outcome is the same. If you run a tool that isn’t idempotent multiple times, it might make a mess of things.\n\nHere’s an example of a shell script that is not idempotent:\n\necho \"spock:*:1010:1010:Spock:/home/spock:/bin/bash\" \\ >> /etc/passwd\n\nIf you run this script once you get the outcome you want: the user spock is added to the /etc/passwd file. But if you run it ten times, you’ll end up with ten identical entries for this same\n\nuser.\n\nWith an idempotent infrastructure tool, you specify how you want things to be:\n\nuser: name: spock full_name: Spock uid: 1010 gid: 1010 home: /home/spock shell: /bin/bash\n\nNo matter how many times you run the tool with this code, it will ensure that only one entryexists in the /etc/passwd file for the user spock. No unpleasant side effects.\n\nDSLs for infrastructure\n\nIn addition to being declarative, infrastructure tools often use their\n\n7 own DSL .\n\nThe advantage of a DSL for infrastructure code is keeping the code simple and focused.\n\nA general-purpose language needs extra syntax, such as\n\ndeclarations of variables and references to class structures in a DSL:\n\nimport 'cloud-api-library' app_server = CloudApi.find_server('my_application_server')\n\nif(app_server == null) { app_server = CloudApi.create_server(name: 'my_application_server')\n\nA DSL can strip this to the most relevant elements:\n\nvirtual_machine: name: my_application_server\n\nThe return of general-purpose languages for infrastructure\n\nNewer tools, such as Pulumi and the AWS CDK, bring general-\n\npurpose languages back to infrastructure coding. There are several\n\narguments for doing this, some more convincing than others.\n\nConfiguration isn’t real code\n\nSome folks take the phrase “Infrastructure as Code” to heart. They argue that declarative languages are just configuration, not a “real” language. Personally, I’m not bothered if someone disparages my code as being mere configuration. I still find it useful to keep “what” and “how” separate, and to avoid writing repetitive, verbose code.\n\nIt’s useful not to have to learn a new language\n\nUsing a popular language like JavaScript means more people can learn to write infrastructure as code since they don’t need to learn a peculiar special-purpose language. I have sympathy for making it easy for people to adopt infrastructure as code. But I don’t think a new language syntax, especially a simple declarative language, is as hard to learn as the domain-specific aspects of infrastructure code like networking constructs.\n\nInfrastructure DSLs are not well-supported by development tools\n\n8\n\nThis is generally true. There are many mature IDE’s for languages like JavaScript, Python, and Ruby. These have loads of features, like syntax highlighting and code refactoring, that\n\nhelp developers to be more productive. Rather than discouraging the use of new languages (of any kind), I would love to see vendors improve their support for infrastructure coding languages.\n\nProper support for libraries helps you to simplify code\n\nMost infrastructure DSLs let you write modules (as I’ll discuss in Chapter 6). But these are not as rich and flexible as libraries in mature, procedural languages like JavaScript, Python, and Ruby. I discuss reusable modules and libraries for stacks in Chapter 6.\n\nUsing the same language lets you combine infrastructure and application code\n\nExamples of this show application code that provisions its own infrastructure. People who’ve been developing web applications for a while recall how nifty it was to be able to embed SQL statements into HTML code. Experience has shown that this does not lead to cleanly designed, maintainable systems. It is often useful to specify actions for a tool to trigger on specific changes to infrastructure, such as provisioning newly created resources. But this can be done without intermingling the different styles of code.\n\nInfrastructure languages are less testable\n\nAlso true, although there are several reasons for this. One is that the ecosystem of testing tools for infrastructure is not as mature as that for other types of code. Another reason is that testing declarations requires a different approach to testing logic. A third reason is that testing code that provisions infrastructure has much longer feedback loops. I delve into these in “Challenges with testing infrastructure code”.\n\nThe swing back towards general-purpose languages for infrastructure is new . Some of the arguments threaten to regress\n\n9\n\nus to codebases filled with verbose spaghetti code mingling\n\nconfiguration, application logic, and repetitive utility code. But I\n\nexpect that this is just one step on a path to languages that support better coding.\n\nFor many teams today, the challenges with their codebase do not\n\ncome from the language they use. Regardless of language, they\n\nneed ways to keep their code clean, well-organized, and easy to maintain.\n\nImplementation Principles for defining infrastructure as code\n\nTo update and evolve your infrastructure systems easily and safely,\n\nyou need to keep your codebase clean: easy to understand, test,\n\nmaintain, and improve. Code quality is a familiar theme in software engineering. The following implementation principles are\n\nguidelines for designing and organizing your code to support this\n\ngoal.\n\nImplementation Principle: Avoid mixing different types of code\n\nAs discussed earlier, some infrastructure coding languages are\n\n10\n\nprocedural, and some are declarative . Each of these paradigms has its strengths and weaknesses.\n\nA particular scourge of infrastructure is code that mixes both\n\ndeclarative and procedural code, as in Example 4-3. This code\n\nincludes declarative code for defining a server as well as procedural code that determines which attributes to assign to the\n\nserver depending on the server role and environment.\n\nExample 4-3. Example of mingled procedural and declarative code for ${env} in [\"test\", \"staging\", \"prod\"] { for ${server_role} in [\"app\", \"web\", \"db\"] { server: name: ${server_role}-${env} image: 'base_linux' cpu: if(${env} == \"prod\" || ${env} == \"staging\") { 4 } else { 2 } ram: if(${server_role} == \"app\") { \"4GB\" } else if (${server_role} == \"db\") { \"2GB\" } else { \"1GB\" } provision: provisioner: servermaker role: ${server_role} } }\n\nMixed declarative and procedural code is a design\n\n11\n\nsmell https://martinfowler.com/bliki/CodeSmell.html. A “smell” is some characteristic of a system that you observe that suggests\n\nthere is an underlying problem. In the example from the text, code\n\nthat mixes declarative and procedural constructs is a smell. This smell suggests that your code may be trying to do multiple things\n\nand that it may be better to pull them apart into different pieces of\n\ncode, perhaps in different languages.]. It’s not easy to understand this code, which makes it harder to debug, and harder to change\n\nwithout breaking something.\n\nThe messiness of this code comes from intermingling two different\n\nconcerns, which leads to the next implementation principle.\n\nImplementation Principle: Separate infrastructure code concerns\n\nA common reason for messy code is that it is doing multiple\n\nthings. These things may be related, but teasing them apart can make them easier to distinguish, and improve the readability and\n\nmaintainability of the code.\n\nExample 4-3 does two things: it specifies the infrastructure\n\nresource to create, and it configures that resource differently in different contexts. The example illustrates two of the four most\n\ncommon concerns of infrastructure code:\n\nSpecification\n\nSpecifications define the shape of your infrastructure. Your server has specific packages, configuration files, and user accounts. Declarative languages work well for this because specifications are what you want. Specifications are what most people mean when they talk about infrastructure code.\n\nConfiguration\n\nConfiguration defines the things that vary when you provision different instances of infrastructure. Different application servers built from a single specification may need different amounts of RAM. You may want to deploy different applications onto otherwise identical servers. Configuration is almost always declarative.\n\nExecution\n\nExecution applies specification and configuration to the actual infrastructure resources. Together, the specification and configuration declare what you want. The orchestration is about how to make that happen. Procedural or functional code usually works best for orchestration. Ideally, you should use an off the shelf tool for this rather than writing your own code.",
      "page_number": 69
    },
    {
      "number": 4,
      "title": "Core Practice: Define everything as code",
      "start_page": 126,
      "end_page": 151,
      "detection_method": "regex_chapter_title",
      "content": "Orchestration\n\nOrchestration combines multiple specifications and configurations. For example, you may need to create a server in the cloud platform, and then install packages on the server. Or you may need to create networking structures, and then create multiple servers attached to those structures.\n\nSEPARATING SPECIFICATION AND CONFIGURATION\n\nLet’s take a simple example of code that mixes concerns and split\n\nit into two cleaner pieces of code. This example specifies an\n\napplication server, assigning it to a different network depending on which customer uses it:\n\nvirtual_machine: name: application_server_${CUSTOMER} source_image: 'base_linux' provision: tool: servermaker role: foodspin_application network: $(switch ${CUSTOMER}) { \"bomber_burrito\": us_network \"curry_hut\": uk_network \"burger_barn\": au_network }\n\nThe code uses the CUSTOMER parameter to choose the network for the server.\n\nThis code is hard to test because any test instance needs to specify\n\na customer. Using a real customer couples your test code with that\n\ncustomer’s configuration, which makes the tests brittle. You could create a test customer. But then you need to add the configuration\n\nfor your fake customer to your infrastructure code. Mingling code\n\nand configuration between test and production descends even\n\nfarther into the depths of poor code.\n\nLet’s go in the opposite direction, and pull the specification into its own code:\n\nvirtual_machine: name: application_server_${CUSTOMER} source_image: 'base_linux' provision: tool: servermaker role: foodspin_application network: ${NETWORK}\n\nThis code is more straightforward than the previous example. It\n\nonly includes code for the things which are common to all\n\napplication server instances. Because this doesn’t vary, we don’t need logic, so this fits nicely as declarative code.\n\nThe only parts of the specification which vary are set using variables, CUSTOMER to give the server a unique name, and NETWORK to assign it to a unique network. There are different patterns for assigning these variables, which are the subject of an\n\nentire chapter of this book (Chapter 8).\n\nTo illustrate the separation of concerns, let’s use a script to work\n\nout the configuration for the server. The script works out which network to assign the server to based on the environment, as\n\nbefore:\n\nswitch (${CUSTOMER}) { case 'bomber_burrito': return 'us_network' case 'curry_hut': return 'uk_network' case 'burger_barn':\n\nreturn 'au_network' }\n\nEach of these two pieces of code is easier to understand. If you need to add a new customer, you can easily add it to the\n\nconfiguration code, without confusing things. This example doesn’t really need procedural code to work out the configuration.\n\nBut it illustrates that if you find the need for more complicated\n\nlogic, it’s cleaner to separate it from the declarative code.\n\n[Link to Come] gives more detailed advice on how to implement separation of concerns in your codebase.\n\nImplementation Principle: Treat infrastructure code like real code\n\nTo keep an infrastructure codebase clean, you need to treat it as a first-class concern. Too often, people don’t consider infrastructure\n\ncode to be “real” code. They don’t give it the same level of\n\nengineering discipline as application code.\n\nDesign and manage your infrastructure code so that it is easy to understand and maintain. Follow code quality practices, such as\n\ncode reviews, pair programming, and automated testing. Your\n\nteam should be aware of technical debt and strive to minimize it.\n\nCODE AS DOCUMENTATION\n\nWriting documentation and keeping it up to date can be too much work. For some purposes, the infrastructure code is more useful than written documentation. It’s always an accurate and updated record of your system.\n\nNew joiners can browse the code to learn about the system,\n\nTeam members can read the code, and review commits, to see what other people have done,\n\nTechnical reviewers can use the code to assess what to improve,\n\nAuditors can review code and version history to gain an accurate picture of the system.\n\nInfrastructure code is rarely the only documentation required. High-level documentation is helpful for context and strategy. You may have stakeholders who need to understand aspects of your system but who don’t know your tech stack.\n\nYou may want to manage these other types of documentation as code. Many teams write Architecture Decision Records (ADRs) in a markup language and keep them in source control.\n\nYou can automatically generate useful material like architecture diagrams from code. You can put this in a change management pipeline to update diagrams every time someone makes a change to the code.\n\nConclusion\n\nIn this chapter, I explored the core practice of defining your\n\nsystem as code. This included the key prerequisites for this practice, considerations of different types of infrastructure coding\n\nlanguages, and a few principles for good code design. The next\n\ncore practice, continuously validating your code, builds on this\n\nmaterial.\n\nBut before I cover that in Chapter 9, I’ll describe specific patterns\n\nand antipatterns for implementing this first practice in the context\n\nof infrastructure stacks.\n\nChapter 5 explains how to use infrastructure code to provision\n\nresources from your cloud platform into useful groups I call\n\nstacks. Chapter 6 covers the use of modules within stacks. Then,\n\nChapter 7 describes how to structure your stack code to create\n\nmultiple environments. After that, Chapter 8 offers patterns for managing the configuration of different stack instances across\n\nenvironments.\n\n1 Context about why depends on people to write useful commit messages\n\n2 git-crypt, blackbox, sops, and transcrypt are a few tools that help you to encrypt secrets in a git repository. Some of these tools integrate with cloud platform authorization, so unattended systems can decrypt them.\n\n3 I explain how people can work on stack code locally in more detail in [Link to\n\nCome].\n\n4 I describe how these are used in [Link to Come]\n\n5 “Recently” as I write this in late 2019\n\n6 I use this pseudo-code language to illustrate the concepts I’m trying to explain,\n\nwithout tying them to any specific tool.\n\n7 Martin Fowler and Rebecca Parsons define a DSL as a “small language, focused\n\non a particular aspect of a software system, in their book Domain-Specific Languages (Addison-Wesley Professional)\n\n8 Integrated Development Environment, a specialized editor for programming\n\nlanguages.\n\n9 Again, I’m writing this in late 2019. By the time you read this, things will have\n\nmoved forward in one direction or another.\n\n10 Object-Oriented Programming (OOP) and Functional Programming are two other\n\nprogramming paradigms used in application software development. Although there are a few examples of tools that use each of these (Riemann), neither is common with infrastructure code. There is no reason tools couldn’t use them with the domain. But even with OOP or functional programming, the advice in this section would still apply: don’t write code that mixes language paradigms.\n\n11 The term design smell derives from [code smell\n\nPart II. Working With Infrastructure Stacks\n\nChapter 5. Building Infrastructure Stacks as Code\n\nIn Chapter 3, I said that an infrastructure platform is a pool of\n\ninfrastructure resources that you can provision and change on demand using an API. In Chapter 4, I explained the value of using\n\ncode to define the infrastructure for your system. This chapter puts these together by describing patterns for implementing code to manage infrastructure.\n\nThe concept that I use to talk about doing this is the infrastructure stack.\n\nWhat is an infrastructure stack?\n\nAn Infrastructure Stack is a collection of infrastructure resources that you define, provision, and update as a unit.\n\nYou write source code to define the elements of a stack, which are resources and services that your infrastructure platform provides.\n\nFor example, your stack may include a virtual machine (“Compute Resources”), disk volume (“Storage Resources”), and a subnet (“Network Resources”).\n\nYou run a stack management tool, which reads your stack source\n\ncode and uses the cloud platform’s API to assemble the elements\n\ndefined in the code to provision an instance of your stack.\n\nFigure 5-1. An infrastructure stack is a collection of infrastructure elements managed as a group.\n\nExamples of stack management tools include:\n\nHashicorp Terraform\n\nAWS CloudFormation\n\nAzure Resource Manager\n\nGoogle Cloud Deployment Manager\n\nOpenStack Heat\n\nPulumi\n\nSome server configuration tools (which I’ll talk about much more\n\nin [Link to Come]) have extensions to work with infrastructure\n\nstacks. Examples of these are Ansible Cloud Modules, Chef Provisioning (now end-of-lifed ), Puppet Cloud Management, and\n\n1\n\nSalt Cloud.\n\n“STACK” AS A TERM\n\nMost stack management tools don’t call themselves stack management tools. Each tool has its own terminology to describe the unit of infrastructure that it manages. In this book, I’m describing patterns and practices that should be relevant for any of these tools.\n\nI’ve chosen to use the word “stack.”\n\nVarious people have told me there is a far better term for this concept than “stack.” Each of these people had a completely different word in mind. As of this writing, there is no agreement in the industry as to what to call this thing. So until there is, I’ll continue to use the word “stack.”\n\nStack code\n\nEach stack is defined by source code that declares what infrastructure elements it should include. Terraform code (.tf files) and CloudFormation templates are both examples of infrastructure stack code. A stack project contains the source code\n\nthat defines the infrastructure for a stack.\n\nExample 5-1 shows the folder structure for a stack source code\n\nproject. The language and tool for this example are fictitious. I use pseudo-code examples throughout this chapter.\n\nExample 5-1. Project folder structure of a stack project using a fictitious tool stack-project/ ├── src/ │ ├── dns.infra │ ├── load_balancers.infra │ ├── networking.infra │ └── webserver.infra └── test/\n\nStack instance\n\nYou can use a single stack project to provision more than one stack instance. When you run the stack tool for the project, it uses the\n\nplatform API to ensure the stack instance exists, and to make it\n\nmatch the project code. If the stack instance doesn’t exist, the tool creates it. If the stack instance exists but doesn’t exactly match the\n\ncode, then the tool modifies the instance to make it match.\n\nI often describe this process as “applying” the code to an instance.\n\nIf you change the code and rerun the tool, it changes the stack instance to match your changes. If you run the tool one more time\n\nwithout making any changes to the code, then it should leave the\n\nstack instance as it was.\n\nConfiguring servers in a stack\n\nInfrastructure codebases for systems that aren’t fully container-\n\nbased or serverless application architecture tend to include much code to provision and configure servers. Even container-based\n\nsystems need to build host servers to run containers. The first mainstream infrastructure as code tools, like CFEngine, Puppet,\n\nand Chef, were used to configure servers.\n\nYou should decouple code that builds servers from code that builds\n\nstacks. Doing this makes the code easier to understand, simplifies changes by decoupling them, and supports reusing and testing\n\nserver code.\n\nStack code typically specifies what servers to create, and passes information about the environment they will run in, by calling a\n\nserver configuration tool. Example 5-2 is an example of a stack definition that calls the fictitious servermaker tool to configure a server.\n\nExample 5-2. Example of a stack definition calling a server configuration tool virtual_machine: name: appserver-burgerbarn-${environment} source_image: foodspin-base-appserver memory: 4GB provision: tool: servermaker parameters: maker_server: maker.foodspin.io role: appserver environment: ${environment}\n\nThis stack defines an application server instance, created from a server image called foodspin-appserver, with 4 GB of RAM.\n\nThe definition includes a clause to trigger a provisioning process that runs servermaker. The code also passes several parameters for\n\nthe servermaker tool to use. These parameters include the address of a configuration server (maker_server), which hosts configuration files, and a role, appserver, which servermaker uses to decide which configurations to apply to this particular server. It also passes the name of the environment, which the configurations can use to customize the server.\n\n[Link to Come] describes patterns for managing servers as code.\n\nPatterns and antipatterns for structuring stacks\n\nOne challenge with infrastructure design is deciding how to size\n\nand structure stacks. You could create a single stack code project to manage your entire system. But this becomes unwieldy as your system grows. In this section, I’ll describe patterns and antipatterns for structure infrastructure stacks.\n\nTHE DESCRIPTION FORMAT FOR PATTERNS AND ANTIPATTERNS\n\nI use patterns and antipatterns throughout this book to describe potential ways to solve common problems . Unlike principles, which are intended as rules to follow, a given pattern may or may not be appropriate for your situation. One way to think of these is that principles help you to decide which pattern is right in your context.\n\n2\n\nAn antipattern is a solution that is rarely appropriate. The reason for describing an antipattern is to help you to recognize it and to understand why it’s problematic. People implement antipatterns either because they don’t realize its pitfalls, or unintentionally. For example, someone might implement “Antipattern: One-shot Module” because it seems like a reasonable way to organize their project code. On the other hand, people don’t set out to code a “Antipattern: Spaghetti Module”, it just happens over time.\n\nThe description of each pattern and antipattern follows a set format, with each of these fields:\n\nName\n\nThe name of the pattern or antipattern.\n\nAlso Known As\n\nOther names you may have heard used to describe this.\n\nMotivation\n\nWhy people may decide to implement this pattern or antipattern.\n\nApplicability\n\nWhen this pattern is a good idea, and when it’s not.\n\nConsequences\n\nWhat you should think about if you choose to implement this pattern. What happens if you implement this antipattern.\n\nImplementation\n\nHow to implement the pattern.\n\nRelated patterns\n\nOther patterns and antipatterns, particularly alternatives.\n\nNot every pattern or antipattern description includes all of these fields.\n\nThe following patterns all describe ways of grouping the pieces of a system into one or more stacks. You can view them as a\n\ncontinuum:\n\nA monolithic stack puts an entire system into one stack,\n\nAn application group stack groups multiple, related pieces of a system into stacks,\n\nA service stack puts all of the infrastructure for a single application into a single stack,\n\nA microstack breaks the infrastructure for a given application or service into multiple stacks.\n\nAntipattern: Monolithic Stack\n\nA Monolithic Stack is an infrastructure stack that includes too many elements, making it difficult to maintain.\n\nFigure 5-2. A Monolithic Stack is an infrastructure stack that includes too many elements, making it difficult to maintain.\n\nWhat distinguishes a monolithic stack from other patterns is that\n\nthe number or relationship of infrastructure elements within the stack is difficult to manage well.\n\nALSO KNOWN AS\n\nSpaghetti stack, big ball of mud\n\nMOTIVATION\n\nPeople build monolithic stacks because the simplest way to add a\n\nnew element to a system is to add it to the existing project. Each new stack adds more moving parts, which may need to be orchestrated, integrated, and tested. A single stack is simpler to\n\nmanage.\n\nAPPLICABILITY\n\nA monolithic stack may be appropriate when your system is small\n\nand simple. It’s not appropriate when your system grows, taking longer to provision and update.\n\nCONSEQUENCES\n\nChanging a large stack is riskier than changing a smaller stack. More things that can go wrong-it has a larger blast radius. The impact of a failed change may be broader since there are more services and applications within the stack. Larger stacks are also\n\nslower to provision and change, which makes them harder to manage.\n\nAs a result of the speed and risk of changing a monolithic stack, people tend to make changes less frequently and take longer to do it. This added friction can lead to higher levels of technical debt.\n\nBLAST RADIUS\n\n3\n\nThe term blast radius describes the potential damage a given change could make to a system. It’s usually based on the elements of the system you’re changing, what other elements depend on them, and what elements are shared.\n\nIMPLEMENTATION\n\nYou build a monolithic stack by creating an infrastructure stack project and then continuously adding code, rather than splitting it into multiple stacks.\n\nRELATED PATTERNS\n\nThe opposite of a monolithic stack is a micro stack (“Pattern: Micro Stack”), which aims to keep stacks small so that they are easier to maintain and improve. A monolithic stack may be an\n\napplication group stack “Pattern: Application Group Stack” that has grown out of control.\n\nIS MY STACK A MONOLITH?\n\nWhether your infrastructure stack is a monolith is a matter of judgment. The symptoms of a monolithic stack include:\n\nIt’s difficult to understand how the pieces of the stack fit together (they may be too messy to understand, or perhaps they don’t fit well together),\n\nNew people take a while learning the stack’s codebase,\n\nDebugging problems with the stack is hard,\n\nChanges to the stack frequently cause problems,\n\nYou spend too much time maintaining systems and processes whose purpose is to manage the complexity of the stack.\n\nA key indicator of whether a stack is becoming monolithic is how many people are working on changes to it at any given time. The more common it is for multiple people to work on the stack simultaneously, the more time you spend coordinating changes. Multiple teams making changes to the same stack is even worse. If you frequently have failures and conflicts when deploying changes to a given stack, it may be too large.\n\nFeature branching is a strategy for coping with this, but it can add friction and overhead to delivery. The habitual use of feature branches to work on a stack suggests that the stack has become monolithic.\n\nContinuous Integration (CI) is a more sustainable way to make it safer for multiple people to work on a single stack. However, as a stack grows increasingly monolithic, the CI build takes longer to run, and it becomes harder to maintain good build discipline. If your team’s CI is sloppy, it’s another sign that your stack is a monolith.\n\nThese issues relate to a single team working on an infrastructure stack. Multiple teams working on a shared stack is a clear sign to consider splitting it into more manageable pieces.\n\nPattern: Application Group Stack\n\nAn Application Group Stack includes the infrastructure for multiple related applications or services. The infrastructure for all\n\nof these applications is provisioned and modified as a group.\n\nFigure 5-3. An Application Group Stack hosts multiple processes in a single instance of the stack.\n\nFor example, an online shopping company’s product application group may include separate services for browsing products,\n\nsearching for products, and managing a shopping basket. An application group stack could provide the infrastructure that runs\n\nall three of these services.\n\nALSO KNOWN AS\n\nCombined stack, service group stack, multi-application stack.\n\nMOTIVATION\n\nDefining the infrastructure for multiple related services together can make it easier to manage the application as a single unit.\n\nAPPLICABILITY\n\nThis pattern can work well when a single team owns the infrastructure and deployment of all of the pieces of the\n\napplication. An application group stack can align the boundaries of\n\nthe stack to the team’s responsibilities.\n\nMulti-service stacks are sometimes useful as an incremental step from a monolithic stack to service stacks.\n\nCONSEQUENCES\n\nGrouping the infrastructure for multiple applications together also combines the time, risk, and pace of changes. The team needs to\n\nmanage the risk to the entire stack for every change, even if only\n\none part is changing. This pattern is inefficient if some parts of the stack change more frequently than others.\n\nThe time to provision, change, and test a stack is based on the\n\nentire stack. So again, if it’s common to change only one part of a\n\nstack at a time, having it grouped adds unnecessary overhead and risk.\n\nIMPLEMENTATION\n\nTo create an application group stack, you define an infrastructure project that builds all of the infrastructure for a set of services. You\n\ncan provision and destroy all of the pieces of the application with a single command.\n\nRELATED PATTERNS\n\nThis pattern risks growing into a Monolithic Stack (“Antipattern: Monolithic Stack”). In the other direction, breaking each service in\n\nan application group stack into a separate stack creates a service\n\nstack (“Pattern: Service Stack”).\n\nPattern: Service Stack\n\nA Service Stack manages the infrastructure for each deployable\n\napplication component in a separate infrastructure stack.\n\nFigure 5-4. A Service Stack manages the infrastructure for each deployable application component in a separate infrastructure stack.\n\nALSO KNOWN AS\n\nStack per app, single service stack.\n\nMOTIVATION\n\nService stacks align the boundaries of infrastructure to the\n\nsoftware that runs on it. This alignment limits the blast radius for a\n\nchange to one service, which simplifies the process for scheduling changes. Service teams can own the infrastructure that relates to\n\ntheir software.\n\nAPPLICABILITY\n\nService stacks can work well with microservice application\n\n4\n\narchitectures . They also help organizations with autonomous\n\n5 teams to ensure each team owns its infrastructure .\n\nCONSEQUENCES\n\nIf you have multiple applications, each with an infrastructure\n\nstack, there could be an unnecessary duplication of code. For example, each stack may include code that specifies how to\n\nprovision an application server. Duplication can encourage\n\ninconsistency, such as using different operating system versions, or different network configurations. You can mitigate this by using\n\nmodules to share code (as in Chapter 6).\n\nIMPLEMENTATION\n\nEach application or service has a separate infrastructure code\n\nproject. When creating a new application, a team might copy code\n\nfrom another application’s infrastructure. Or the team could use a\n\nreference project, with boilerplate code for creating new stacks.\n\nIn some cases, each stack may be complete, not sharing any infrastructure with other application stacks. In other cases, teams\n\nmay create stacks with infrastructure that supports multiple\n\napplication stacks. You can learn more about different patterns for this in [Link to Come].\n\nRELATED PATTERNS\n\nThe service stack pattern falls between an application group stack (“Pattern: Application Group Stack”), which has multiple\n\napplications in a single stack, and a micro stack (“Pattern: Micro\n\nStack”), which breaks the infrastructure for a single application across multiple stacks.\n\nPattern: Micro Stack\n\nThe Micro Stack pattern divides the infrastructure for a single\n\nservice across multiple stacks.\n\nFigure 5-5. Micro stacks divide the infrastructure for a single service across multiple stacks.\n\nFor example, you may have a separate stack project each for the\n\nnetworking, servers, and database.\n\nMOTIVATION\n\nDifferent parts of a service’s infrastructure may change at different\n\nrates. Or they may have different characteristics which make them\n\neasier to manage separately. For instance, some methods for managing server instances involve frequently destroying and\n\n6\n\nrebuilding them . However, some services use persistent data in a\n\ndatabase or disk volume. Managing the servers and data in separate stacks means they can have different lifecycles, with the\n\nserver stack being rebuilt much more often than the data stack.\n\nCONSEQUENCES\n\nAlthough smaller stacks are themselves simpler, handling the\n\ndependencies between them is more complicated.\n\nIMPLEMENTATION\n\nAdding a new microstack involves creating a new stack project.\n\nYou need to draw boundaries in the right places between stacks to keep them appropriately sized and easy to manage. The related\n\npatterns include solutions to this. You may also need to integrate\n\ndifferent stacks, which I describe in [Link to Come].\n\nRELATED PATTERNS\n\nMicro stacks are the opposite end of the spectrum from a\n\nmonolithic stack (“Antipattern: Monolithic Stack”), where a single stack contains all the infrastructure for a system.\n\nConclusion\n\nInfrastructure stacks are fundamental building blocks for\n\nautomated infrastructure. The patterns in this chapter are a starting point for thinking about organizing infrastructure into stacks.\n\nGiven that a stack is a unit of change, the main principle for\n\ndeciding where to draw boundaries between stacks in your system is making it easy and safe to make changes. [Link to Come]\n\nexplores this topic in more detail.\n\nIn the next chapter, Chapter 6, I describe patterns and antipatterns\n\nfor sharing code across stacks using modules.\n\n1 https://github.com/chef-boneyard/chef-provisioning\n\n2 The architect Christopher Alexander originated the idea of design patterns in A Pattern Language: Towns, Buildings, Construction (Alexander, Jacobson, Silverstein, Ishikawa, 1977, Oxford University Press). Kent Beck, Ward Cunningham, and others adapted the concept to Software design patterns. Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, known as the “Gang of Four” (or “GoF”), cataloged common patterns and antipatterns in Design Patterns: Elements of Reusable Object-Oriented Software (1994, Addison Wesley).\n\n3 I don’t know who, if anyone, can be said to have coined the term “blast radius” in the context of software system risks, but Charity Majors popularized it, probably starting with her post Terraform, VPC, and why you want a tfstate file per env\n\n4 See Microservices by James Lewis.\n\n5 See The Art of Building Autonomous Teams by John Ferguson Smart, among\n\nmany other references\n\n6 Examples of these are the phoenix server pattern ([Link to Come]) and immutable\n\nservers ([Link to Come])\n\nChapter 6. Using Modules to Share Stack Code\n\nOne of the principles of good software design is the DRY principle (“Don’t Repeat Yourself”) . As you write code for different\n\n1\n\ninfrastructure stacks, you may find yourself writing very similar code multiple times. Rather than maintaining the same code in multiple places, it is often easier to maintain a single copy of the\n\ncode.\n\nMost stack management tools support modules, or libraries, for this reason. A Stack Code Module is a piece of infrastructure code that you can use across multiple stack projects. You can version, test, and release the module code independently of the stack that\n\nuses it.\n\nFigure 6-1. A Stack Code Module is a piece of infrastructure code that you can use across multiple stack projects.\n\nExamples of using modules\n\nAs an example, the Foodspin team has several servers in their system-container host servers, application servers, and a CD\n\nserver. The code for each of these is similar:\n\nExample 6-1. Example of stack code with duplication virtual_machine: name: foodspin-clusterhost source_image: hardened-linux-base memory: 16GB provision: tool: servermaker maker_server: maker.foodspin.io role: clusterhost\n\nvirtual_machine: name: foodspin-appserver source_image: hardened-linux-base memory: 8GB provision: tool: servermaker maker_server: maker.foodspin.io role: appserver\n\nvirtual_machine: name: foodspin-gocd-master source_image: hardened-linux-base memory: 4GB provision: tool: servermaker maker_server: maker.foodspin.io role: appserver\n\nThe Foodspin team writes a module to declare a server, with\n\nplaceholders for parameters:\n\nExample 6-2. Example of a stack module for defining a server declare module: foodspin-server parameters: name: STRING memory: STRING server_role: STRING\n\nvirtual_machine: name: ${name} source_image: hardened-linux-base memory: ${memory} provision: tool: servermaker maker_server: maker.foodspin.io role: ${server_role}\n\nEach stack that needs a server can then use this module:\n\nExample 6-3. Example of a stack module that defines a server use module: foodspin-server name: foodspin-clusterhost memory: 16GB server_role: clusterhost\n\nuse module: foodspin-server name: foodspin-appserver memory: 8GB server_role: appserver\n\nuse module: foodspin-server name: foodspin-gocd-master memory: 4GB server_role: gocd_master\n\nThe code for each server is simpler using modules since it only\n\ndeclares the elements that are different rather than duplicating the declarations for the server image and server provisioner. Most\n\nusefully, they can make changes to how they provision servers in one location, rather than needing to change code in different\n\ncodebases.",
      "page_number": 126
    },
    {
      "number": 5,
      "title": "Building Infrastructure Stacks as Code",
      "start_page": 152,
      "end_page": 173,
      "detection_method": "regex_chapter_title",
      "content": "Patterns and antipatterns for infrastructure modules\n\nModules can be useful for reusing code. However, they also add complexity. So it’s essential to use modules in a way that creates\n\nmore value than complexity. A useful module simplifies stack code, improving readability and maintainability. A poor module\n\nreduces flexibility and is difficult to understand and maintain.\n\nHere are a few patterns and antipatterns that illustrate what works well and what doesn’t:\n\nA Facade Module provides a simplified interface for a resource provided by the stack tool language, or the underlying platform API.\n\nAn Anemic Module wraps the code for an infrastructure element but does not simplify it or add any particular value.\n\nA Domain Entity Module implements a high-level concept by combining multiple low-level infrastructure resources.\n\nA Spaghetti Module is configurable to the point where it creates significantly different results depending on the parameters given to it\n\nAn Obfuscation Layer is composed of multiple modules. It is intended to hide or abstract details of the infrastructure from people writing stack code but instead makes the codebase as a whole harder to understand, maintain, and use.\n\nA One-shot Module is only used once in a codebase, rather than being reused.\n\nPattern: Facade Module\n\nA Facade Module provides a simplified interface for a resource\n\nprovided by the stack tool language, or the underlying platform API.\n\nThe facade module presents a small number of parameters to code\n\nthat uses it:\n\nExample 6-4. Example code using a facade module use module: foodspin-server name: burgerbarn-appserver memory: 8GB\n\nThe module itself includes a larger amount of code, that the calling\n\ncode doesn’t need to worry about:\n\nExample 6-5. Code for the example facade module declare module: foodspin-server virtual_machine: name: ${name} source_image: hardened-linux-base memory: ${memory} provision: tool: servermaker maker_server: maker.foodspin.io role: application_server network: vlan: application_zone_vlan gateway: application_zone_gateway firewall_inbound: port: 22 from: management_zone firewall_inbound: port: 443 from: webserver_zone\n\nMOTIVATION\n\nA facade module simplifies and standardizes a common use case for an infrastructure resource. The stack code the uses a facade\n\nmodule should be simpler and easier to read. Improvements to the quality of the module code are rapidly available to all of the stacks which use it.\n\nAPPLICABILITY\n\nFacade modules work best for simple use cases, usually involving a low-level infrastructure resource.\n\nCONSEQUENCES\n\nA facade module limits how you can use the underlying infrastructure resource. Doing this can be useful, simplifying\n\noptions and standardizing around good quality implementations. However, it can also reduce flexibility.\n\nA module is an extra layer of code between the stack code and the code that directly specifies the infrastructure resources. This extra layer adds at least some overhead to maintaining, debugging, and\n\nimproving code. It can also make it harder to understand the stack code.\n\nIMPLEMENTATION\n\nImplementing a facade module generally involves specifying an infrastructure resource with a number of hard-coded values, and a small number of values that are passed through from the code that\n\nuses the module.\n\nRELATED PATTERNS\n\nAn anemic module (“Antipattern: Anemic Module”) is a facade module that doesn’t hide much, so adds complexity without adding\n\nmuch value. A domain entity module (“Pattern: Domain Entity Module”) is similar to a facade, in that it presents a simplified interface to the code that uses it. But a domain entity combines\n\nmultiple lower-level elements to present a single higher-level entity to the calling code. In contrast, a facade module is a more direct mapping of a single element.\n\nAntipattern: Anemic Module\n\nAn Anemic Module wraps the code for an infrastructure element\n\nbut does not simplify it or add any particular value. It may be a facade module (“Pattern: Facade Module”) gone wrong, or it may be part of an obfuscation layer (“Antipattern: Obfuscation Layer”).\n\nExample 6-6. Example code using an anemic module use module: any_server server_name: burgerbarn-appserver ram: 8GB source_image: base_linux_image provisioning_tool: servermaker server_role: application_server vlan: application_zone_vlan gateway: application_zone_gateway firewall_rules: firewall_inbound: port: 22 from: management_zone firewall_inbound: port: 443 from: webserver_zone\n\nThe module itself passes the parameters directly to the stack management tool’s code:\n\nExample 6-7. Code for the example anemic module\n\ndeclare module: any_server virtual_machine: name: ${server_name} source_image: ${origin_server_image} memory: ${ram} provision: tool: ${provisioning_tool} role: ${server_role} network: vlan: ${server_vlan} gateway: ${server_gateway} firewall_inbound: ${firewall_rules}\n\nALSO KNOWN AS\n\nValue-Free Wrapper, Pass-Through Module, Obfuscator Module.\n\nMOTIVATION\n\nSometimes people write this kind of module aiming to follow the DRY (Don’t Repeat Yourself) principle. They see that code that\n\ndefines a common infrastructure element, such as a virtual server, load balancer, or security group, is used in multiple places in the codebase. So they create a module that declares that element type once and use that everywhere. But because the elements are being\n\nused differently in different parts of the code, they need to expose a large number of parameters in their module. The result is that the code that uses the module is no simpler than directly using the\n\nstack tool’s code. The codebase still has repeated code, only now it’s repeated use of the module.\n\nAPPLICABILITY\n\nNobody intentionally writes an anemic module. You may debate whether a given module is anemic or is a facade, but the debate itself is useful. You should consider whether a module adds real\n\nvalue and, if not, then refactor it into code that uses the stack language directly.\n\nCONSEQUENCES\n\nWriting, using, and maintaining module code rather than directly using the constructs provided by your stack tool adds overhead. It makes your stack code more difficult to understand, especially for\n\npeople who know the stack tool but are new to your codebase. It adds more code to maintain, usually with separate versioning and release. You can end up with different versions of a module being\n\nused by different stacks, causing people to waste time and energy managing releases, upgrades, and testing.\n\nIMPLEMENTATION\n\nIf a module does more than pass parameters, but still presents too many parameters to code that uses it, you might consider splitting it into multiple modules. Doing this makes sense when there are a few common but different use cases for the infrastructure element\n\nthe module defines. For example, rather than a single module for defining firewall rules, you may want one module to define a firewall rule for public HTTPS traffic, another for internal HTTPS\n\ntraffic, and a third for SSH traffic. Each of these may need fewer parameters than a single module that handles multiple protocols and scenarios.\n\nRELATED PATTERNS\n\nSome anemic modules are “Pattern: Facade Module” that grew out of control. Others are part of an “Antipattern: Obfuscation Layer”. It may also be a “Pattern: Domain Entity Module” that maps a bit\n\ntoo directly to a single underlying infrastructure element, rather than to a useful combination of elements.\n\nCOUPLING AND COHESION WITH INFRASTRUCTURE MODULES\n\nWhen organizing code of any kind into different components, you need to consider dependencies. A stack project that uses a module depends on that module . The level of coupling describes how much a change to one part of the codebase affects other parts. If a stack is tightly coupled to a module, then changes to the module will probably require changing the stack code as well. This is a problem when multiple stacks are tightly coupled to a module, or when there is tight coupling across multiple modules and stacks. Tight coupling creates friction and risk for making changes to code.\n\n2\n\nYou should aim to make your code loosely coupled. You can draw on lessons from software architecture to find ways to identify and avoid tight 3 coupling .\n\nThe level of cohesion of a component describes how well-focused it is. A module that does too much, like a spaghetti module (“Antipattern: Spaghetti Module”), has low cohesion. A module has high cohesion when it has a clear focus. A facade module (“Pattern: Facade Module”) can be highly cohesive around a low-level infrastructure resource, while a domain entity module (“Pattern: Domain Entity Module”) can be highly cohesive around a clear, logical entity.\n\nPattern: Domain Entity Module\n\nA Domain Entity Module implements a high-level concept by combining multiple low-level infrastructure resources. An example of a higher level concept is the infrastructure needed to run a specific Java application. This example shows how a module\n\nthat implements a Java application infrastructure instance might be used from stack project code:\n\nExample 6-8. Example of using a domain entity module\n\nuse module: java-application-infrastructure name: \"shopping_app_cluster\" application: \"shopping_app\" application_version: \"4.20\" network_access: \"public\"\n\nThe module creates a complete set of infrastructure elements, including a cluster of virtual servers with load balancer, database instance, and firewall rules.\n\nMOTIVATION\n\nInfrastructure stack languages provide language constructs that map directly to resources and services provided by infrastructure platforms (the things I described in Chapter 3). Teams combine\n\nthese low-level constructs into higher-level constructs to serve a particular purpose, such as hosting a Java application or running a data pipeline.\n\nIt can take a fair bit of low-level infrastructure code to define all of\n\nthe pieces needed to meet that high-level purpose. As I described above, to create the infrastructure to run a Java application, you may need to write code for a cluster of virtual servers, for a load\n\nbalancer, for a database instance, and for firewall rules. That’s a lot of code. Although you may write modules so you can share code for each low-level resource, if you need multiple different applications with similar infrastructure, it would be useful to share\n\ncode at this higher level as well.\n\nEXAMPLE: FOODSPIN’S APPLICATION INFRASTRUCTURE ENTITY MODULE\n\nThe Foodspin team runs several different clusters for running Java applications. They host a separate application instance for each of their customers. They also run an instance of the Atlassian Jira bug tracker, and the GoCD pipeline server. So they define one stack project for each customer , one for Jira, and one for GoCD.\n\n4\n\nThe team notices that they’re spending too much time copying changes between these stacks, so they decide to create a module for the common infrastructure they need. In their case, each instance has a cluster of virtual machines, all built from the same server image. Different instances have different minimum and maximum numbers of servers in the cluster, so the team makes these into parameters for the module. Some applications use a database, and some don’t. So they leave this out of their module and instead have an optional parameter to pass connection details to the server provisioning code.\n\nTheir module also defines the networking structures, including firewall rules and routing, that the application needs. Here, they use a parameter to indicate whether the application is public-facing (for the customer applications) or internal (for Jira and GoCD), which results in some different networking configurations.\n\nBy using this module in each stack that needs an application server, the Foodspin team has simplified each stack’s project code, so it is easier to understand. They also have a single place to make improvements to the infrastructure for multiple applications. They write a set of tests for this module (as we’ll see in Chapter 9 and later chapters), which gives them the confidence to improve their code continuously.\n\nAPPLICABILITY\n\nA domain entity module is useful for things that are fairly complex\n\nto implement, and that are used in pretty much the same way in\n\nmultiple places in your system. Don’t create a domain entity module that you only use once. And don’t create one of these\n\nmodules for multiple instances of the same thing, when each instance is significantly different. The first case is a one-shot\n\nmodule (“Antipattern: One-shot Module”), the second risks\n\nbecoming a spaghetti module (“Antipattern: Spaghetti Module”).\n\nFor example, you may run multiple application servers that use the same stack-operating system, application language, and\n\napplication deployment method. This is a candidate for a domain\n\nentity module.\n\nAs a counter-example, you may have multiple application servers,\n\nbut some run Windows, some run Linux. One runs a stateless PHP\n\napplication, another runs a Java application on Tomcat with a MySQL database, while a third hosts a .Net application that uses\n\nmessage queues to integrate with other applications. Although all\n\nthree of these are application servers, their implementation is quite different. Any module that can create a suitable infrastructure\n\nstack for all of these is unlikely to have a clean design and implementation.\n\nIMPLEMENTATION\n\nOn a concrete level, implementing a domain entity model is a matter of writing the code for a related grouping of infrastructure\n\nin a single module. But the best way to create a high-quality\n\ncodebase that is easy for people to learn and maintain is to take a design-led approach.\n\nI recommend drawing from lessons learned in software\n\narchitecture and design. The domain entity module pattern derives\n\n5\n\nfrom Domain Driven Design (DDD) , which creates a conceptual model for the business domain of a software system, and uses that\n\nto drive the design of the system itself. Infrastructure, especially\n\none designed and built as software, can be seen as a domain in its own right. The domain is building, delivering, and running\n\nsoftware.\n\nA particularly powerful approach is for an organization to use\n\nDDD to design the architecture for the business software, and then extend the domain to include the systems and services used for\n\nbuilding and running that software.\n\nThe code to implement a Java application server might look like\n\nExample 6-9. This example creates and assembles three different\n\nresources: a DNS entry, which points to a load balancer, which routes traffic to a server cluster.\n\nExample 6-9. Example domain entity module declare module: java-application-infrastructure\n\ndns_entry: id: \"${APP_NAME}-hostname\" record_type: \"A\" hostname: \"${APP_NAME}.foodspin.io\" ip_address: {$load_balancer.ip_address}\n\nserver_cluster: id: \"${APP_NAME}-cluster\" min_size: 1 max_size: 3 each_server_node: source_image: base_linux memory: 4GB provision: tool: servermaker role: appserver parameters: app_package: \"${APP_NAME}-${APP_VERSION}.war\" app_repository: \"repository.foodspin.io\"\n\nload_balancer: protocol: https target: type: server_cluster target_id: \"${APP_NAME}-cluster\"\n\nCode to provision a stack that runs a search application using this module might look like this:\n\nuse module: java-application-infrastructure app_name: foodspin_search_app app_version: 1.23\n\nRELATED PATTERNS\n\nIf you find that some stack projects have very little code other than importing a single module, you might consider putting the code\n\ndirectly into the stack project. Reusing code in the form of an entire stack, rather than a module, is the reusable stack pattern\n\n(“Pattern: Reusable Stack”). In some cases, having the entire stack\n\ncode in a module is deliberate, as in the wrapper stack pattern (“Pattern: Wrapper Stack”).\n\nA facade module (“Pattern: Facade Module”) is like a domain\n\nentity module that only creates a single low-level infrastructure\n\nelement. A spaghetti module (“Antipattern: Spaghetti Module”) is similar to a domain entity module but has too many different\n\noptions.\n\nAntipattern: Spaghetti Module\n\nA Spaghetti Module is configurable to the point where it creates\n\nsignificantly different results depending on the parameters given to it. The implementation of the module is messy and difficult to\n\nunderstand, because it has too many moving parts:\n\nExample 6-10. Example of a spaghetti module declare module: application-server-infrastructure variable: network_segment = { if ${parameter.network_access} = \"public\" id: public_subnet else if ${parameter.network_access} = \"customer\" id: customer_subnet else id: internal_subnet end }\n\nswitch ${parameter.application_type}: \"java\":\n\nvirtual_machine: origin_image: base_tomcat network_segment: ${variable.network_segment} server_configuration: if ${parameter.database} != \"none\" database_connection: ${database_instance.my_database.connection_string} end ... \"NET\": virtual_machine: origin_image: windows_server network_segment: ${variable.network_segment} server_configuration: if ${parameter.database} != \"none\" database_connection: ${database_instance.my_database.connection_string} end ... \"php\": container_group: cluster_id: ${parameter.container_cluster} container_image: nginx_php_image network_segment: ${variable.network_segment} server_configuration: if ${parameter.database} != \"none\" database_connection: ${database_instance.my_database.connection_string} end ... end\n\nswitch ${parameter.database}: \"mysql\": database_instance: my_database type: mysql ... ...\n\nThe example code above assigns the server it creates to one of three different network segments, and optionally creates a database\n\ncluster and passes a connection string to the server configuration. In some cases, it creates a group of container instances rather than\n\na virtual server. This module is a bit of a beast.\n\nALSO KNOWN AS\n\nSwiss Army module.\n\nMOTIVATION\n\nAs with other antipatterns, people create a spaghetti module by accident, often over time. You may create a facade module\n\n(“Pattern: Facade Module”) for a common infrastructure element,\n\nsuch as a server, that grows so that it can create radically different types of servers. Or you may create a domain entity module\n\n(“Pattern: Domain Entity Module”) for an application’s\n\ninfrastructure. But then you find that the module needs to optionally include a variety of elements, such as databases,\n\nmessage queues, load balancers, and public Internet gateways,\n\ndepending on the application that runs on it.\n\nCONSEQUENCES\n\nA module that does too many things is less maintainable than one\n\nwith a tighter scope. The more things a module does, and the more variations there are in the infrastructure that it can create, the\n\nharder it is to change it without breaking something. These\n\nmodules are harder to test. As I explain in a later chapter (Chapter 9), better-designed code is easier to test, so if you’re\n\nstruggling to write automated tests and build pipelines to test the module in isolation, It’s a sign that you may have a spaghetti\n\nmodule.\n\nIMPLEMENTATION\n\nA spaghetti module’s code often contains conditionals, that apply\n\ndifferent specifications in different situations. For example, a\n\ndatabase cluster module might take a parameter to choose which\n\ndatabase to provision.\n\nWhen you realize you have a spaghetti module on your hands, you should refactor it. Often, you can split it into different modules,\n\neach with a more focused remit. For example, you might\n\ndecompose your single application infrastructure module into different modules for different parts of the application’s\n\ninfrastructure. An example of a stack that uses decomposed\n\nmodules in this way, rather than using the spaghetti module from the earlier example (Example 6-10) might look like this:\n\nExample 6-11. Example of using decomposed modules rather than a single spaghetti module use module: java-application-servers name: burgerbarn_appserver application: \"shopping_app\" application_version: \"4.20\" network_segment: customer_subnet server_configuration: database_connection: ${module.mysql- database.outputs.connection_string}\n\nuse module: mysql-database cluster_minimum: 1 cluster_maximum: 3 allow_connections_from: customer_subnet\n\nEach of the modules is smaller, simpler, and so easier to maintain and tested than the original spaghetti module.\n\nRELATED PATTERNS\n\nA spaghetti module is often a domain entity module (“Pattern: Domain Entity Module”) gone wrong.\n\nAntipattern: Obfuscation Layer\n\nUnlike the other patterns and antipatterns in this chapter, an\n\nObfuscation Layer is composed of multiple modules. Intended to hide or abstract details of the infrastructure from people writing\n\nstack code, it instead makes the codebase as a whole harder to\n\nunderstand, maintain, and use.\n\nALSO KNOWN AS\n\nAbstraction layer, portability layer, in-house infrastructure model.\n\nMOTIVATION\n\nAn obfuscation layer is usually intended to simplify or standardize\n\nimplementation. A team might create a library of modules as an in-\n\nhouse model for building infrastructure. Sometimes the intention is for people to write stack code without needing to learn the stack\n\ntool itself. In other cases, the layer tries to abstract the specifics of\n\nthe infrastructure platform so that code can be written once and used across multiple platforms.\n\nCONSEQUENCES\n\nIf your team uses a heavily customized model for infrastructure code, then it becomes a barrier to new people joining your team.\n\nEven someone who knows the stack tools you use has a steep learning curve to understand your special language. An in-house\n\nobfuscation layer tends to be inflexible, forcing people to either\n\nwork around its limitations or spend time making changes to the obfuscation layer itself. Either way, people waste time creating and\n\nmaintaining extra code.\n\n6\n\nPeople, especially managers and hands-off architects , often\n\noverestimate the benefit of hiding an underlying platform or tools\n\nfrom people writing infrastructure code. I have yet to see a cloud\n\nabstraction layer that adds noticeable value, or that avoids adding cost and complexity. Most fail at both.\n\nCLOUD ABSTRACTION LAYERS CONSIDERED HARMFUL\n\nDifferent infrastructure platforms provide common types of resources, as you see when reading or skimming through Chapter 3. So it seems simple to map these to a common set of abstractions. But the resources are implemented differently. The actual code to implement a virtual server on AWS, Azure, and GCE, for example, is not trivial.\n\nAnd each platform provides hundreds of different services. Even if you limit yourself to using a few dozen services, that’s a large amount of code to implement and maintain.\n\nThe theory of an abstraction layer is that you can reduce the cost of writing and maintaining a stack across multiple platforms, as visualized here:\n\nFigure 6-2. The goal of an abstraction layer is to reduce the cost of a stack across multiple platforms\n\nIn practice, the cost of writing and maintaining an abstraction layer for multiple platforms is more than the cost of the separate code for each platform, as shown here:\n\nFigure 6-3. The cost of an abstraction layer\n\nThe flipside of the high cost of an abstraction layer is reduced value. It’s impractical to cover everything the underlying platforms offer. And if you want to ensure you can create a stack on all of the platforms, then you won’t be able to leverage any features that are unique to one platform or another. The result is using a least-common-denominator subset of cloud features, at a higher cost than using them directly.\n\nRELATED PATTERNS\n\nDomain entity modules (“Pattern: Domain Entity Module”) may\n\nbe used to create an obfuscation layer at a higher level, while\n\nfacade modules (“Pattern: Facade Module”) would be used to\n\nobfuscate lower-level resources. If each module in the layer\n\nhandles multiple platforms, they are probably spaghetti modules (“Antipattern: Spaghetti Module”).\n\nAntipattern: One-shot Module\n\nA one-shot module is only used once in a codebase, rather than\n\nbeing reused.\n\nMOTIVATION\n\nPeople usually create one-shot modules as a way to organize the\n\ncode in a project.\n\nAPPLICABILITY\n\nIf a stack project includes enough code that it becomes difficult to\n\nnavigate, you have a few options. Splitting the stack into modules\n\nis one approach. If the stack is conceptually doing too much, it might be better to divide it into multiple stacks, using an\n\nappropriate stack structural pattern (see “Patterns and antipatterns\n\nfor structuring stacks”). Otherwise, merely organizing code into\n\ndifferent files and, if necessary, different folders, can make it\n\neasier to navigate and understand the codebase without the\n\noverhead of the other options.\n\nCONSEQUENCES\n\nOrganizing the code into modules adds the overhead of declaring\n\nthe module and passing parameters. You add even more\n\ncomplexity if you manage the module code separately, with separate versioning. This overhead is worthwhile when you share\n\ncode across multiple stack projects. The benefits of code reuse\n\nmake up for the added time and energy of maintaining a separate\n\nmodule. Paying that cost when you’re not using the benefit is a\n\nwaste.\n\nRELATED PATTERNS\n\nA one-shot module may map closely to lower-level infrastructure\n\nelements, like a facade module (“Pattern: Facade Module”), or to a higher-level entity, like a domain entity module (“Pattern: Domain\n\nEntity Module”).\n\nConclusion\n\nThis chapter has explored the use of modules to share code across\n\nstacks. Modules are a popular mechanism to manage the growth of a codebase, but as you can see, it’s easy to create an\n\nunmaintainable mess.\n\nThe next chapter (Chapter-Building-Environments) is devoted to\n\none of the most common situations that require multiple instances of the same or similar infrastructure. This need leads to some\n\npatterns and antipatterns for implementing shared code at the level\n\nof the stack.\n\n1 The DRY principle, and others, can be found in The Pragmatic Programmer: From\n\nJourneyman to Master by Andrew Hunt and David Thomas\n\n2 In later chapters ([Link to Come]) we’ll see that a stack can depend on another\n\nstack, as well.\n\n3 Two resources to get started include Martin Fowler’s paper Reducing Coupling,\n\nand Mohamed Sanaulla’s post Cohesion and Coupling: Two OO Design Principles.\n\n4 Later on (“Example of how Foodspin moves to reusable stacks”) we’ll see how the\n\nFoodspin team evolves this to use a single reusable stack project\n\n5 See Domain-Driven Design: Tackling Complexity in the Heart of Software, by Eric\n\nEvans, 2003 Addison Wesley\n\n6 For a thoughtful view on how architecture, and architects, relates to\n\nimplementation, I recommend the Architect Elevator, by Gregor Hohpe.",
      "page_number": 152
    },
    {
      "number": 6,
      "title": "Using Modules to Share Stack Code",
      "start_page": 174,
      "end_page": 201,
      "detection_method": "regex_chapter_title",
      "content": "Chapter 7. Building Environments With Stacks\n\nIn Chapter 5, I described an infrastructure stack as a collection of infrastructure resources that you manage as a single unit. An\n\nenvironment is also a collection of infrastructure resources. So is a stack the same thing as an environment? In this chapter, I explain that maybe it is, but maybe it isn’t.\n\nThe difference between an environment and a stack is that an\n\nenvironment is conceptual, while a stack is concrete. You define an environment to fulfill a particular purpose, like deploying and testing software. You define a stack with code, and then provision it, change it, and destroy it using a tool. As I’ll explain in this\n\nchapter, you might implement an environment as a single stack, or you might compose an environment from multiple stacks. You could even create several environments in one stack, although you\n\nshouldn’t.\n\nWhat environments are all about\n\nThe concept of an environment is one of those things that we take for granted in IT. But we often mean slightly different things when we use the term in different contexts. In this book, an environment\n\nis a collection of operationally related infrastructure resources. That is, the resources in an environment support a particular activity, such as testing or running a system. Most often, multiple\n\nenvironments exist, each running an instance of the same system.\n\nThere are two typical use cases for having multiple environments\n\nrunning instances of the same system. One is to support a release delivery process, and the other is to run multiple production instances of the system.\n\nRelease delivery environments\n\nThe most familiar use case for multiple environments is to support\n\na progressive software release process - sometimes called the path to production. A given build of an application is deployed to each environment in turn to support different development and testing activities until it is finally deployed to the production environment.\n\nFigure 7-1. Foodspin delivery environments\n\nI’ll use this set of environments throughout this chapter to\n\nillustrate patterns for defining environments as code.\n\nMultiple production environments\n\nYou might also use multiple environments for complete, independent copies of a system in production. Reasons to do this\n\ninclude:\n\nFault tolerance\n\nIf one environment fails, others can continue to provide service. Doing this could involve a failover process to shift load from the failed environment. You can also have fault tolerance within an environment, by having multiple instances of some infrastructure, as with a server cluster. Running an additional environment duplicates all of the infrastructure, creating a higher degree of fault tolerance, although at a higher cost. See [Link to Come] for more on this.\n\nScalability\n\nYou can spread a workload across multiple environments. People often do this geographically, with a separate environment for each region. Multiple environments may be used to achieve both scalability and fault tolerance. If there is a failure in one region, the load is shifted to another region’s environment until the failure is corrected.\n\nSegregation\n\nYou may run multiple instances of an application or service for different user bases, such as different clients. Running these instances in different environments can strengthen segregation. Stronger segregation may help meet legal or compliance requirements and give greater confidence to customers.\n\nFOODSPIN EXAMPLE: MULTIPLE PRODUCTION ENVIRONMENTS\n\nIn the “Foodspin Example: Compute resources” example in Chapter 3 I explained that Foodspin runs a separate application server for each of its restaurant customers. As they expand to support customers in North America, Europe, and South Asia they decide to create a separate environment for each of these regions.\n\nFigure 7-2. Foodspin regional environments\n\nUsing fully separated environments, rather than having a single environment spread across the regions, helps them to ensure that they are complying with different regulations about storing customer data in different regions. Also, if they need to make changes that involve downtime, they can do so in each region at a different time. This makes it easier to align downtime to different timezones.\n\nLater, Foodspin lands a contract with a large fast-food chain, Hungry Hungry Horses. Hungry Hungry Horses is worried that having their customer data hosted on the same infrastructure with data from their competitors runs the risk of leaking data. So Foodspin’s team offers to run a completely separate environment dedicated to Hungry Hungry Horses, at a higher cost than running in a shared environment.\n\nEnvironments, consistency, and configuration\n\nSince multiple environments are meant to run instances of the same system, the infrastructure in each environment should be consistent. Consistency across environments is one of the main\n\ndrivers of Infrastructure as Code.\n\nDifferences between environments create the risk of inconsistent behavior. Testing software in one environment might not uncover problems that occur in another. It’s even possible that software\n\ndeploys successfully in some environments but not others.\n\nOn the other hand, you typically need some specific differences between environments. Test environments may be smaller than production environments. Different people may have different\n\nprivileges in different environments. Environments for different customers may have different features and characteristics. At the very least, names and IDs may be different (appserver-test, appserver-stage, appserver-prod). So you need to configure at\n\nleast some aspects of your environments.\n\nA key consideration for environments is your testing and validation strategy. When the same infrastructure code is applied\n\nto every environment, testing it in one environment tends to give confidence that it will work correctly in other environments. You don’t get this confidence if the infrastructure varies much across\n\ninstances, however.\n\nYou may be able to improve confidence by testing infrastructure code using different configuration values. However, it may not be practical to test many different values. In these situations, you may need additional validation, such as post-provisioning tests or\n\nmonitoring production environments. I’ll go more in-depth on testing and validation in Chapter 9.\n\nPatterns for building environments\n\nAs I mentioned earlier, an environment is a conceptual collection of infrastructure elements, and a stack is a concrete collection of infrastructure elements. A stack project is the source code you use\n\nto create one or more stack instances. So how should you use stack projects and instances to implement environments?\n\nI’ll describe two antipatterns and one pattern for implementing environments using infrastructure stacks. Each of these patterns describes a way to define multiple environments using\n\ninfrastructure stacks. Some systems are composed of multiple stacks, as I described in “Patterns and antipatterns for structuring stacks”. I’ll explain what this looks like for multiple environments\n\nin “Building environments with multiple stacks”.\n\nAntipattern: Multiple-Environment Stack\n\nA Multiple-Environment Stack defines and manages the infrastructure for multiple environments as a single stack instance.\n\nFor example, if there are three environments for testing and\n\nrunning an application, a single stack project includes the code for all three of the environments.\n\nFigure 7-3. A multiple-environment stack manages the infrastructure for multiple environments as a single stack instance.\n\nALSO KNOWN AS\n\nOne stack with many environments\n\nAll your environments in one basket\n\nMOTIVATIONS\n\nMany people create this type of structure when they’re learning a new stack tool because it seems natural to add new environments\n\ninto an existing project.\n\nCONSEQUENCES\n\nWhen running a tool to update a stack instance, the scope of a\n\npotential change is everything in the stack. If you have a mistake 1 or conflict in your code, everything in the instance is vulnerable .\n\nWhen your production environment is in the same stack instance as another environment, changing the other environment risks\n\ncausing a production issue. A coding error, unexpected dependency, or even a bug in your tool can break production when you only meant to change a test environment.\n\nRELATED PATTERNS\n\nYou can limit the blast radius of changes by dividing environments into separate stacks. One obvious way to do this is the copy-paste\n\nenvironment (“Antipattern: Copy-Paste Environments”), where each environment is a separate stack project, although this is considered an antipattern.\n\nA better approach is the reusable stack pattern (“Pattern: Reusable Stack”). A single project is used to define the generic structure for an environment and is then used to manage a separate stack\n\ninstance for each environment. Although this involves using a single project, the project is only applied to one environment instance at a time. So the blast radius for changes is limited to that one environment.\n\nAntipattern: Copy-Paste Environments\n\nThe Copy-Paste Environments antipattern uses a separate stack source code project for each infrastructure stack instance.\n\nIn our example of three environments named test, staging, and production, there is a separate infrastructure project for each of\n\nthese environments. Changes are made by editing the code in one environment and then copying the changes into each of the other environments in turn.\n\nFigure 7-4. A copy-paste environment has a separate copy of the source code project for each instance.\n\nALSO KNOWN AS\n\nSingleton stack\n\nSnowflake stack\n\nCopy-pasta\n\nMOTIVATION\n\nCopy-paste environments are an intuitive way to maintain multiple\n\nenvironments. They avoid the blast radius problem of the multi- headed stack antipattern. You can also easily customize each stack\n\ninstance.\n\nAPPLICABILITY\n\nCopy-paste environments may be appropriate when you want to\n\nmaintain and change different instances and aren’t worried about\n\ncode duplication or consistency.\n\nCONSEQUENCES\n\nIt can be challenging to maintain multiple copy-paste\n\nenvironments. When you want to make a code change, you need to copy it to every project. You probably need to test each instance\n\nseparately, as a change may work in one but not another.\n\nCopy-paste environments often suffer from configuration drift (“Configuration Drift”). Using copy-paste environments for\n\ndelivery environments reduces the reliability of the deployment\n\nprocess and the validity of testing, due to inconsistencies from one environment to the next.\n\nIMPLEMENTATION\n\nYou create a copy-paste environment by copying the project code from one stack instance into a new project. You then edit the code\n\nto customize it for the new instance. When you make a change to\n\none stack, you need to copy and paste it across all of the other stack projects, while keeping the customizations in each one.\n\nRELATED PATTERNS\n\nIn cases where stack instances are meant to represent the same stack, the reusable stack pattern (“Pattern: Reusable Stack”) is\n\nusually more appropriate. The wrapper stack pattern (“Pattern: Wrapper Stack”) avoids the disadvantages of a copy-paste stack by\n\nhaving all of the logic in a module and only using each stack\n\nproject for configuration.\n\nPattern: Reusable Stack\n\nA Reusable Stack is an infrastructure source code project that is\n\nused to create multiple instances of a stack.\n\nFigure 7-5. A Reusable Stack is an infrastructure source code project that is used to create multiple instances of a stack.\n\nALSO KNOWN AS\n\nCookie Cutter Stack\n\nTemplate Stack\n\nLibrary Stack\n\nMOTIVATION\n\nYou create a reusable stack to maintain multiple consistent\n\ninstances of infrastructure. When you make changes to the stack code, you can apply and test it in one instance, and then use the\n\nsame code version to create or update multiple additional instances. You want to provision new instances of the stack with\n\nminimal ceremony, maybe even automatically.\n\nEXAMPLE OF HOW FOODSPIN MOVES TO REUSABLE STACKS\n\nIn “Example: Foodspin’s application infrastructure entity module”, the Foodspin team extracted common code from different stack projects that all used an application server. They put the common code into a module used by each of the stack projects. Later, they realized that the stack projects for their customer applications still looked very similar. In addition to using the module to create an application server, each stack had code to create databases and dedicated logging and reporting services for each customer.\n\nChanging and testing changes to this code across multiple customers was becoming a hassle, and they were signing up new customers every month. So the team decided to create a single stack project that defines a customer application stack. This project still uses the shared Java application server module, as do a few other applications (Jira and GoCD). But the project also has the code for setting up the rest of the per-customer infrastructure as well.\n\nNow, when they sign on a new customer, the team uses the common customer stack project to create a new instance. When they fix or improve something in the project codebase, they apply it to test instances to make sure it’s OK, and then they roll it out one by one to the customer instances.\n\nAPPLICABILITY\n\nYou can use a reusable stack for delivery environments or for\n\nmultiple production environments. This pattern is useful when you don’t need much variation between the environments. It is less\n\napplicable when environments need to be heavily customized.\n\nCONSEQUENCES\n\nThe ability to provision and update multiple stacks from the same\n\nproject enhances scalability, reliability, and throughput. You can\n\nmanage more instances with less effort, make changes with a lower risk of failure, and roll changes out to more systems more\n\nrapidly.\n\nYou typically need to configure some aspects of the stack\n\ndifferently for different instances, even if it’s just what you name things. I’ll spend a whole chapter talking about this (Chapter 8).\n\nYou should test your stack project code before you apply changes\n\nto business-critical infrastructure. I’ll spend multiple chapters on\n\nthis, including Chapter 9 and Chapter 10.\n\nIMPLEMENTATION\n\nYou create a reusable stack as an infrastructure stack project, and\n\nthen run the stack management tool each time you want to create or update an instance of the stack. Use the syntax of the stack tool\n\ncommand to tell it which instance you want to create or update.\n\nWith Terraform, for example, you would specify a different state file or workspace for each instance. With CloudFormation, you\n\npass a unique stack ID for each instance.\n\nThe following example command provisions two stack instances from a single project using a fictional command called stack. The\n\ncommand takes an argument env that identifies unique instances:\n\n> stack up env=test --source mystack/src SUCCESS: stack 'test' created > stack up env=staging --source mystack/src SUCCESS: stack 'staging' created\n\nAs a rule, you should use simple parameters to define differences\n\nbetween stack instances - strings, numbers, or in some cases, lists. Additionally, the infrastructure created by a reusable stack should\n\nnot vary much across instances.\n\nRELATED PATTERNS\n\nThe reusable stack is an improvement on the copy-paste\n\nenvironment antipattern (“Antipattern: Copy-Paste\n\nEnvironments”), making it easier to keep multiple instances consistent.\n\nStack code modules (Chapter 6) allow you to define code once and\n\nthen share it across multiple stack projects. Unlike a reusable\n\nstack, a stack module is not used directly to create infrastructure- instead, a stack project imports modules. So modules are more of a\n\nsupplement to reusable stacks than a substitute.\n\nThe wrapper stack pattern (“Pattern: Wrapper Stack”) uses\n\nmodules to implement a combination of copy-paste environments (one project per instance) and a reusable stack (one copy of the\n\ncode that defines the stack).\n\nThe patterns I describe in Chapter 8 offer ways to configure instances of reusable stacks.\n\nBuilding environments with multiple stacks\n\nThe reusable stack pattern describes an approach for implementing\n\nmultiple environments. In Chapter 5 I described different ways to\n\nstructuring a system’s infrastructure across multiple stacks (“Patterns and antipatterns for structuring stacks”). There are\n\nseveral ways you can implement your stacks to combine these two\n\ndimensions of environments and system structure.\n\nThe simple case is implementing the complete system as a single stack. When you provision an instance of the stack, you have a\n\ncomplete environment. I depicted this in the diagram for the reusable stack pattern (Figure 7-5).\n\nBut you should split larger systems into multiple stacks. For\n\nexample, if you follow the service stack pattern (“Pattern: Service\n\nStack”) you have a separate stack for each service:\n\nFigure 7-6. Example using a separate infrastructure stack for each service\n\nTo create multiple environments, you provision an instance of each\n\nservice stack for each environment:\n\nFigure 7-7. Using multiple stacks to build each environment\n\nYou would use commands like the following to build a full\n\nenvironment with multiple stacks:\n\n> stack up env=staging --source product_browse_stack/src SUCCESS: stack 'product_browse-staging' created > stack up env=staging --source product_search_stack/src SUCCESS: stack 'product_search-staging' created > stack up env=staging --source shopping_basket_stack/src SUCCESS: stack 'shopping_basket-staging' created\n\nIn [Link to Come] I describe strategies for splitting systems into\n\nmultiple stacks, and how to integrate infrastructure across stacks.\n\nConclusion\n\nReusable stacks should be the workhorse pattern for most teams who need to manage large infrastructures. This pattern creates\n\nchallenges and opportunities. One challenge is how to configure\n\neach stack instance, which is the topic of Chapter 8, the next chapter.\n\nHowever, reusable stacks create the opportunity to test\n\ninfrastructure code before you apply it to environments you care\n\nabout. In Chapter 9 I’ll explain the core practice of continuously testing infrastructure code, and in Chapter 10 I’ll describe\n\nimplementation patterns following this practice with infrastructure\n\nstacks.\n\n1 Charity Majors shared her painful experiences of working with a multiple-\n\nenvironment stack in a blog post.\n\nChapter 8. Configuring Stacks\n\nUsing a single stack code project makes it easier to maintain multiple consistent instances of infrastructure, as I described in\n\n“Pattern: Reusable Stack”. However, you often need to customize stack instances. For example, you might run smaller clusters in development environments than in production. Here is an example\n\nof stack code that defines a container hosting cluster with\n\nconfigurable minimum and maximum numbers of servers:\n\ncontainer_cluster: web_cluster-${environment} min_size: ${cluster_minimum} max_size: ${cluster_maximum}\n\nYou pass different parameter values to this code for each environment, as depicted in Figure 8-1.\n\nFigure 8-1. Using the same code with different parameter values for each environment\n\nStack tools such as Terraform and CloudFormation support multiple ways of setting configuration parameter values. These typically include passing values on the command-line, reading them from a file, and having the infrastructure code retrieve them\n\nfrom a key-value store.\n\nTeams managing infrastructure need to decide how to use these features to manage and pass configuration values to their stack tool. It’s essential to ensure the values are defined and applied consistently to each environment.\n\nDESIGN PRINCIPLE: KEEP PARAMETERS SIMPLE\n\nA major reason for defining your infrastructure as code is to ensure systems are consistently configured, as described in “Principle: Minimize variation”). Configurable stack code creates the opportunity for inconsistency. The more configurable a stack project is, the more difficult it is to understand the behavior of different instances, to ensure you’re testing your code effectively, and to deliver changes regularly and reliably to all of your instances.\n\nSo it’s best to keep stack parameters simple and to use them in simple ways.\n\nPrefer simple parameter types, like strings, numbers, and perhaps lists and key-value maps. Avoid passing more complex data structures.\n\nMinimize the number of parameters that you can set for a stack. Avoid defining parameters that you “might” need. Only add parameter when you have an immediate need for it. You can always add a parameter later on if you discover you need it.\n\nAvoid using parameters as conditionals that create significant differences in the resulting infrastructure. For example, a boolean (yes/no) parameter to indicates whether to provision a service within a stack adds complexity.\n\nWhen it becomes hard to follow this advice, it’s probably a sign that you should refactor your stack code, perhaps splitting it into multiple stack projects.\n\nUsing stack parameters to create unique identifiers\n\nIf you create multiple stack instances from the same project (as per the reusable stack pattern “Pattern: Reusable Stack”), you may see\n\nfailures from infrastructure resources that require unique identifiers. To see what I mean, look at the following pseudo-code\n\nthat defines an application server:\n\nserver: id: appserver subnet_id: appserver-subnet\n\nThe fictional cloud platform requires the id value to be unique, so when I run the stack command to create the second stack, it fails:\n\n> stack up environment=test --source mystack/src SUCCESS: stack 'test' created > stack up environment=staging --source mystack/src FAILURE: server 'appserver' already exists in another stack\n\nI can use parameters in my stack code to avoid these clashes. I change my code to take a parameter called environment and use it to assign a unique server ID. I also add the server into a different subnet in each environment:\n\nserver: id: appserver-${environment} subnet_id: appserver-subnet-${environment}\"\n\nNow I can run my fictional stack command to create multiple stack instances without error.\n\nExample stack parameters\n\nI’ll use an example stack to compare and contrast the different stack configuration patterns in this chapter. The example is a\n\ntemplate stack project that defines a container cluster, composed of\n\na dynamic pool of host nodes and some networking constructs. Here is the project structure:\n\nExample 8-1. Example project structure for a template stack that defines a container cluster ├── src/ │ ├── cluster.infra │ └── networking.infra └── test/\n\nThe cluster stack uses the parameters listed in Example 8-2 for three different stack instances. environment is a unique id for each environment, which can be used to name things and create unique identifiers. cluster_minimum and cluster_maximum define the range of sizes for the container host cluster. The infrastructure code in the file cluster.infra defines the cluster on the cloud platform, which scales the number of host nodes depending on load. Each of the three environments, test, staging,\n\nand production, uses a different set of values.\n\nExample 8-2. Example parameter values used for the pattern descriptions\n\nStack Instance\n\nenvironme nt\n\ncluster_minimu m\n\ncluster_maximu m\n\ncluster_test\n\ntest\n\n1\n\n1\n\ncluster_staging\n\nstaging\n\n2\n\n3\n\ncluster_productio n\n\nproduction\n\n2\n\n6\n\nHandling secrets as parameters\n\nIn “Secrets and source code”, I explained that you should not store unencrypted secrets in source code. Two of the solutions I\n\ndescribed (“Injecting secrets at runtime” and “Disposable secrets”)\n\nare based on passing secrets as parameter to stack code. To\n\nimplement these, you’ll need a way to manage those secrets on the system that runs your stack tool, whether it’s locally to a team\n\nmember’s workstation or laptop, or on a compute instance that’s running the tool as part of a pipeline or other service.\n\nI’ll include implementation details for secrets for each of the\n\nfollowing patterns. You may want to use one pattern for normal,\n\nnon-secret parameters, and a different pattern for secrets.\n\nMy examples also use an example secret, in this case, a passphrase for an SSL certificate passphrase, ssl_cert_passphrase. The value for this parameter in all environments is +correct horse 1 battery staple+ .\n\nPatterns for configuring stacks\n\nWe’ve looked at why you need to parameterize stacks and a bit on how the tools implement parameters. Now I’ll describe some\n\npatterns and antipatterns for managing parameters and passing\n\nthem to your tool:\n\nManual Stack Parameters: Run the stack tool and type the parameter values on the command line.\n\nScripted Parameters: Hard-code parameter values for each instance in a script that runs the stack tool.\n\nStack Configuration Files: Declare parameter values for each instance in configuration files kept in the stack code project.\n\nWrapper Stack: Create a separate infrastructure stack project for each instance, and import a shared module with the stack code.\n\nPipeline Stack Parameters. Define parameter values in the configuration of a pipeline stage for each instance.\n\nStack Parameter Registry Pattern: Store parameter values in a central location.\n\nAntipattern: Manual Stack Parameters\n\nThe most natural approach to provide values for a stack instance is to type the values on the command-line manually, as in Example 8-3.\n\nExample 8-3. Example of manually typing command-line parameters > stack up environment=production --source mystck/src FAILURE: No such directory 'mystck/src' > stack up environment=production --source mystack/src SUCCESS: new stack 'production' created > stack destroy environment=production --source mystack/src SUCCESS: stack 'production' destroyed > stack up environment=production --source mystack/src SUCCESS: existing stack 'production' modified\n\nMOTIVATION\n\nIt’s dirt-simple to type values on the command-line, which is helpful when you’re learning how to use a tool.\n\nCONSEQUENCES\n\nIt’s easy to make a mistake when typing a value on the command- line. It can also be hard to remember which values to type. For infrastructure that people care about, you probably don’t want the\n\nrisk of accidentally breaking something important by mistyping a command when making an improvement or fix. When multiple\n\npeople work on an infrastructure stack, as in a team, you can’t expect everyone to remember the correct values to type for each\n\ninstance.\n\nManual stack parameters aren’t suitable for automatically applying infrastructure code to environments, such as with CI or CD.\n\nIMPLEMENTATION\n\nFor the example parameters (Example 8-2), pass the values on the command-line according to the syntax expected by the particular tool. With my fictional stack tool the command looks like this:\n\nstack up \\ environment=test \\ cluster_minimum=1 \\ cluster_maximum=1 \\ ssl_cert_passphrase=\"correct horse battery staple\"\n\nAnyone who runs the command needs to know the secrets, like passwords and keys, to use for a given environment and pass them on the command line. Your team should use a team password\n\n2\n\nmanagement tool to store and share them between team members securely, and rotate secrets when people leave the team.\n\nRELATED PATTERNS\n\nThe scripted parameters pattern (“Pattern: Scripted Parameters”) takes the command that you would type and puts it into a script. The pipeline stack parameters pattern (“Pattern: Pipeline Stack\n\nParameters”) does the same thing but puts them into the pipeline configuration instead of a script.\n\nPattern: Stack Environment Variables\n\nThe Stack Environment Variables pattern involves setting parameter values as environment variables for the stack tool to\n\nuse. This pattern is often combined with another pattern to set the environment variables.\n\nThe environment variables are set beforehand (see the implementation section for more on how):\n\nexport STACK_ENVIRONMENT=test export STACK_CLUSTER_MINIMUM=1 export STACK_CLUSTER_MAXIMUM=1 export STACK_SSL_CERT_PASSPHRASE=\"correct horse battery staple\"\n\nThere are different implementation options, but the most basic one is for the stack code to reference them directly:\n\ncontainer_cluster: web_cluster-${ENV(\"STACK_ENVIRONMENT\")} min_size: ${ENV(\"STACK_CLUSTER_MINIMUM\")} max_size: ${ENV(\"STACK_CLUSTER_MAXIMUM\")}\n\nMOTIVATION\n\nMost platforms and tools support environment variables, so it’s easy to do.\n\nAPPLICABILITY\n\nIf you’re already using environment variables in your system and have suitable mechanisms to manage them, you might find it convenient to use them for stack parameters.\n\nCONSEQUENCES\n\nYou need to use an additional pattern from this chapter to get the values to set. Doing this adds moving parts, making it hard to trace\n\nconfiguration values for any given stack instance, and more work to change the values.\n\nUsing environment variables directly in stack code, as in the\n\nearlier example (???), arguably couples stack code too tightly to the runtime environment.\n\nSetting secrets in environment variables may expose them to other processes that run on the same system.\n\nIMPLEMENTATION\n\nAgain, you need to set the environment variables to use, which means selecting another pattern from this chapter. For example, if you expect people to set environment variables in their local\n\nenvironment to apply stack code, you are using the manual stack parameters antipattern (“Antipattern: Manual Stack Parameters”). You could set them in a script that runs the stack tool (the scripted\n\nparameters pattern “Pattern: Scripted Parameters”), or have the pipeline toolset them (“Pattern: Pipeline Stack Parameters”).\n\nAnother approach is to put the values into a script that people or instances import into their local environment. This is a variation of the stack configuration files pattern (“Pattern: Stack Configuration\n\nFiles”). The script to set the variables would be exactly like the earlier example (???), and any command that runs the stack tool would import it into the environment:\n\nsource ./environments/staging.env stack up --source ./src\n\nAlternately, you could build the environment values into a compute instance that runs the stack tool. For example, if you\n\nprovision a separate CD agent node to run the stack tool to build and update stacks in each environment, the code to build the node could set the appropriate values as environment variables. Those\n\nenvironment variables would be available to any command that runs on the node, including the stack tool.\n\nBut to do this, you need to pass the values to the code that builds your agent nodes. So you need to select another pattern from this\n\nchapter to do that.\n\nThe other side of implementing this pattern is how the stack tool gets the environment values. The earlier example (<<???) shows how stack code can directly read environment variables.\n\nBut you could, instead, use a stack orchestration script ([Link to\n\nCome]) to read the environment variables and pass them to the stack tool on the command line. The code in the orchestration script would look like this:\n\nstack up \\ environment=${STACK_ENVIRONMENT} \\ cluster_minimum=${STACK_CLUSTER_MINIMUM} \\ cluster_maximum=${STACK_CLUSTER_MAXIMUM} \\ ssl_cert_passphrase=\"${STACK_SSL_CERT_PASSPHRASE}\"\n\nThis approach decouples your stack code from the environment it runs in.\n\nRELATED PATTERNS\n\nAny of the other patterns in this chapter can be combined with this one to set environment values.\n\nPattern: Scripted Parameters\n\nScripted Parameters involves hard-coding the parameter values into a script that runs the stack tool. You can write a separate script for each environment or a single script which includes the values\n\nfor all of your environments:\n\nif ${ENV} == \"test\" stack up cluster_maximum=1 env=\"test\" elsif ${ENV} == \"staging\" stack up cluster_maximum=3 env=\"staging\" elsif ${ENV} == \"production\" stack up cluster_maximum=5 env=\"production\" end\n\nALSO KNOWN AS\n\nEnvironment provisioning script\n\nMOTIVATION\n\nScripts are a simple way to capture the values for each instance, avoiding the problems with the “Antipattern: Manual Stack\n\nParameters”. You can be confident that values are used consistently for each environment. By checking the script into version control, you ensure you are tracking any changes to the\n\nconfiguration values.\n\nAPPLICABILITY\n\nA stack provisioning script is a useful way to set parameters when you have a fixed set of environments that don’t change very often.\n\nIt doesn’t require the additional moving parts of some of the other\n\npatterns in this chapter.\n\nBecause it is wrong to hard-code secrets in scripts (as you know from reading “Secrets and source code”), this pattern is not\n\nsuitable for secrets. That doesn’t mean you shouldn’t use this",
      "page_number": 174
    },
    {
      "number": 7,
      "title": "Building Environments With Stacks",
      "start_page": 202,
      "end_page": 224,
      "detection_method": "regex_chapter_title",
      "content": "pattern, only that you’ll need to implement a separate pattern for\n\ndealing with secrets.\n\nCONSEQUENCES\n\nIt’s common for the commands used to run the stack tool to\n\nbecome complicated over time. Provisioning scripts can grow into\n\nmessy beasts. [Link to Come] discusses how these scripts are used and outlines pitfalls and recommendations for keeping them\n\nmaintainable. You should test provisioning scripts since they can\n\nbe a source of issues with the systems they provision.\n\nIMPLEMENTATION\n\nThere are two common implementations for this pattern. One is a\n\nsingle script that takes the environment as a command-line argument, with hard-coded parameter values for each\n\nenvironment. Example 8-4 is a simple example of this.\n\nExample 8-4. Example of a script that includes the parameters for multiple environments #!/bin/sh\n\ncase $1 in test) CLUSTER_MINIMUM=1 CLUSTER_MAXIMUM=1 ;; staging) CLUSTER_MINIMUM=2 CLUSTER_MAXIMUM=3 ;; production) CLUSTER_MINIMUM=2 CLUSTER_MAXIMUM=6 ;; *) echo \"Unknown environment $1\" exit 1 ;;\n\nesac\n\nstack up \\ environment=$1 \\ cluster_minimum=${CLUSTER_MINIMUM} \\ cluster_maximum=${CLUSTER_MAXIMUM}\n\nAnother implementation is a separate script for each stack\n\ninstance, as in Example 8-5.\n\nExample 8-5. Example project structure with a script for each environment. our-infra-stack/ ├── bin/ │ ├── test.sh │ ├── staging.sh │ └── production.sh ├── src/ └── test/\n\nEach of these scripts is identical but has different parameter values hard-coded in it. The scripts are smaller because they don’t need\n\nlogic to select between different parameter values. However, they\n\nneed more maintenance. If you need to change the command, you need to make it across all of the scripts. Having a script for each\n\nenvironment also tempts people to customize different\n\nenvironments, creating inconsistency.\n\nCommit your provisioning script or scripts to source control. Putting it in the same project as the stack it provisions ensures that\n\nit stays in sync with the stack code. For example, if you add a new\n\nparameter, you add it to the infrastructure source code and also to your provisioning script. You always know which version of the\n\nscript to run for a given version of the stack code.\n\n[Link to Come] discusses the use of scripts to run stack tools in\n\nmuch more detail.\n\nAs mentioned earlier, you shouldn’t hard-code secrets into scripts,\n\nso you’ll need to use a different pattern for those. You can use the\n\nscript to support that pattern. In this example, a command-line tool fetches the secret from a secrets manager, following the parameter\n\nregistry pattern (“Pattern: Stack Parameter Registry”):\n\nExample 8-6. Fetching a key from a secrets manager in a script ... # (Set environment specific values as in other examples) ...\n\nSSL_CERT_PASSPHRASE=$(some-tool get-secret id=\"/ssl_cert_passphrase/${ENV}\")\n\nstack up \\ environment=${ENV} \\ cluster_minimum=${CLUSTER_MINIMUM} \\ cluster_maximum=${CLUSTER_MAXIMUM} \\ ssl_cert_passphrase=\"${SSL_CERT_PASSPHRASE}\"\n\nThe some-tool command connects to the secrets manager and retrieves the secret for the relevant environment using the ID /ssl_cert_passphrase/${ENV}. This example assumes the session is authorized to use the secrets manager. An infrastructure\n\ndeveloper may use the tool to start a session before running this script. Or the compute instance that runs the script may be\n\nauthorized to retrieve secrets using secretless authorization (as I described in “Secretless authorization”).\n\nRELATED PATTERNS\n\nProvisioning scripts run the command-line tool for you, so are a way to move beyond the manual stack parameters antipattern\n\n(“Antipattern: Manual Stack Parameters”). The stack configuration\n\nfiles pattern (“Pattern: Stack Configuration Files”) extracts the parameter values from the script into separate files.\n\nPattern: Stack Configuration Files\n\nStack Configuration Files manage parameter values for each\n\ninstance in a separate file, which you manage in version control with your stack code.\n\n├── src/ │ ├── cluster.infra │ ├── host_servers.infra │ └── networking.infra ├── environments/ │ ├── test.properties │ ├── staging.properties │ └── production.properties └── test/\n\nALSO KNOWN AS\n\nEnvironment configuration files\n\nMOTIVATION\n\nCreating configuration files for a stack’s instances is\n\nstraightforward and easy to understand. Because the file is committed to the source code repository, it is easy:\n\nTo see what values are used for any given environment (“what is the maximum cluster size for production?”),\n\nto trace the history for debugging (“when did the maximum cluster size change?”),\n\nand to audit changes (“who changed the maximum cluster size?”).\n\nStack configuration files enforce the separation of configuration from the stack code.\n\nAPPLICABILITY\n\nStack configuration files are appropriate when the number of\n\nenvironments doesn’t change often. They require you to add a file to your project to add a new stack instance. They also require (and\n\nhelp enforce) consistent logic in how different instances are\n\ncreated and updated, since the configuration files can’t include logic.\n\nCONSEQUENCES\n\nWhen you want to create a new stack instance, you need to add a new configuration file to the stack project. Doing this prevents you\n\nfrom automatically creating new environments on the fly. In\n\n“Pattern: Ephemeral test stack”, I describe an approach for managing test environments that relies on creating environments\n\nautomatically. You could work around this by creating a\n\nconfiguration file for an ephemeral environment on demand.\n\nParameter files can add friction for changing the configuration of downstream environments in a change delivery pipeline of the\n\nkind I describe in [Link to Come]. Every change to the stack\n\nproject code must progress through each stage of the pipeline before being applied to production. It can take a while for this to\n\ncomplete and doesn’t add any value when the configuration\n\nchange is only applied to production.\n\nDefining parameter values can be a source of considerable complexity in provisioning scripts. I’ll talk about this more in\n\n[Link to Come], but as a teaser, consider that teams often want to define default values for stack projects, and for environments, and\n\nthen need logic to combine these into values for a given instance\n\nof a given stack in a different environment. Inheritance models for parameter values can get messy and confusing.\n\nConfiguration files in source control should not include secrets. So\n\nfor secrets, you weither need to select an additional pattern from this chapter to handle secrets or implement a separate secrets\n\nconfiguration file outside of source control.\n\nIMPLEMENTATION\n\nYou define stack parameter values in a separate file for each\n\nenvironment, as shown in the earlier example project structure\n\n(???).\n\nThe contents of a parameter file could look like this:\n\nenv = staging cluster_minimum = 2 cluster_maximum = 3\n\nPass the path to the relevant parameter file when running the stack\n\ncommand:\n\nstack up --source ./src --config ./environments/staging.properties\n\nIf the system is composed of multiple stacks, then it can get messy to manage configuration across all of the environments. There are\n\ntwo common ways of arranging parameter files in these cases. One\n\nis to put configuration files for all of the environments with the code for each stack:\n\n├── cluster_stack/ │ ├── src/ │ │ ├── cluster.infra │ │ ├── host_servers.infra │ │ └── networking.infra │ └──environments/ │ ├── test.properties │ ├── staging.properties │ └── production.properties └── appserver_stack/\n\n├── src/ │ ├── server.infra │ └── networking.infra └──environments/ ├── test.properties ├── staging.properties └── production.properties\n\nThe other is to centralize the configuration for all of the stacks in\n\none place:\n\n├── cluster_stack/ │ ├── cluster.infra │ ├── host_servers.infra │ └── networking.infra ├── appserver_stack/ │ ├── server.infra │ └── networking.infra └── environments/ ├── test/ │ ├── cluster.properties │ └── appserver.properties ├── staging/ │ ├── cluster.properties │ └── appserver.properties └── production/ ├── cluster.properties └── appserver.properties\n\nEach approach can become messy and confusing in its own way. When you need to make a change to all of the things in an\n\nenvironment, making changes to configuration files across dozens\n\nof stack projects is painful. When you need to change the configuration for a single stack across the various environments\n\nit’s in, trawling through a tree full of configuration for dozens of\n\nother stacks is also not fun.\n\nIf you want to use configuration files to provide secrets, rather than using a separate pattern for secrets, then you need to manage\n\nthose files outside of the project code checked into source control.\n\nFor local development environments, you can require users to\n\ncreate the file in a set location manually. Pass the file location to the stack command like this:\n\nstack up --source ./src \\ --config ./environments/staging.properties \\ --config ../.secrets/staging.properties\n\nIn this example, you provide two --config arguments to the stack tool, and it reads parameter values from both. You have a directory named .secrets outside the project folder, so it is not in source control.\n\nIt can be trickier to do this when running the stack tool automatically, from a compute instance like a CD pipeline agent.\n\nYou could provision similar secrets property files onto these\n\ncompute instances, but that can expose secrets to other processes that run on the same agent. You also need to provide the secrets to\n\nthe process that builds the compute instance for the agent, so you\n\nstill have a bootstrapping problem.\n\nRELATED PATTERNS\n\nPutting configuration values into files simplifies the provisioning\n\nscripts described in “Pattern: Scripted Parameters”. You can avoid some of the limitations of environment configuration files by using\n\nthe “Pattern: Stack Parameter Registry” instead. Doing this moves\n\nparameter values out of the stack project code and into a central location, which allows you to use different workflows for code and\n\nconfiguration.\n\nPattern: Wrapper Stack\n\nA Wrapper Stack uses an infrastructure stack project for each\n\ninstance as a wrapper to import a stack code module (see Chapter 6). Each wrapper project defines the parameter values for\n\none instance of the stack. It then imports a module shared by all of\n\nthe stack instances.\n\nFigure 8-2. A Wrapper Stack uses an infrastructure stack project for each instance as a wrapper to import a stack code module\n\nMOTIVATION\n\nA wrapper stack leverages the stack tool’s module functionality to\n\nre-use shared code across stack instances. You can use the tool’s module versioning, dependency management, and artifact\n\nrepository functionality to implement a change delivery pipeline ([Link to Come]). As of this writing, most infrastructure stack\n\ntools don’t have a project packaging format that you can use to\n\nimplement pipelines for stack code. So you need to create a custom stack packaging process yourself. You can work around\n\nthis by using a wrapper stack, and versioning and promoting your\n\nstack code as a module.\n\nWith wrapper stacks, you can write the logic for provisioning and configuring stacks in the same language that you use to define\n\nyour infrastructure, rather than using a separate language as you\n\nwould with a provisioning script (“Pattern: Scripted Parameters”).\n\nCONSEQUENCES\n\nModules add an extra layer of complexity between your stack and\n\nthe code contained in the module. You now have two levels: the stack project, which contains the wrapper projects, and the module\n\nwhich contains the code for the stack.\n\nBecause you have a separate code project for each stack instance,\n\npeople may be tempted to add custom logic for each instance. Custom instance code makes your codebase inconsistent and hard\n\nto maintain.\n\nBecause you define parameter values in wrapper projects managed\n\nin source control, you can’t use this pattern to manage secrets. So you need to add a another pattern from this chapter to provide\n\nsecrets to stacks.\n\nIMPLEMENTATION\n\nEach stack instance has a separate infrastructure stack project. For\n\nexample, you would have a separate Terraform project for each environment. You can implement this like a copy-paste\n\nenvironment (“Antipattern: Copy-Paste Environments”), with each\n\nenvironment in a separate repository.\n\nAlternatively, each environment project could be a folder in a single repository:\n\nmy_stack/ ├── test/ │ └── stack.infra ├── staging/ │ └── stack.infra └── production/ └── stack.infra\n\nDefine the infrastructure code for the stack as a module, according\n\nto your tool’s implementation. You could put the module code in the same repository with your wrapper stacks. However, this\n\nwould prevent you from leveraging module versioning\n\nfunctionality. That is, you wouldn’t be able to use different\n\nversions of the infrastructure code in different environments,\n\nwhich is crucial for progressively testing your code.\n\nThe following example is a wrapper stack that imports a module called container_cluster_module, specifying the version of the module, and the configuration parameters to pass to it:\n\nmodule: name: container_cluster_module version: 1.23 parameters: env: test cluster_minimum: 1 cluster_maximum: 1\n\nThe wrapper stack code for the staging and production\n\nenvironments is similar, other than the parameter values, and\n\nperhaps the module version they use.\n\nThe project structure for the module could look like this:\n\n├── container_cluster_module/ │ ├── cluster.infra │ └── networking.infra └── test/\n\nWhen you make a change to the module code, you test and upload it to a module repository. How the repository works depends on\n\nyour particular infrastructure stack tool. You can then update your\n\ntest stack instance to import the new module version and apply it\n\nto the test environment.\n\nTerragrunt is a stack orchestration tool that implements the\n\nwrapper stack pattern.\n\nRELATED PATTERNS\n\nA wrapper stack is similar to the scripted parameters pattern. The\n\nmain differences are that it uses your stack tool’s language rather\n\nthan a separate scripting language and that the infrastructure code\n\nis in a separate module.\n\nPattern: Pipeline Stack Parameters\n\nWith the Pipeline Stack Parameters pattern, you define values for\n\neach instance in the configuration of a delivery pipeline.\n\nI explain how to use a change delivery pipeline to apply\n\ninfrastructure stack code to environments in [Link to Come]. You can implement a pipeline using a tool like Jenkins, GoCD, or\n\nConcourseCI (see “Delivery pipeline software and services” for\n\nmore on these tools).\n\nFigure 8-3. Each stage that applies the stack code passes the relevant configuration values for the environment.\n\nMOTIVATION\n\nIf you’re using a pipeline tool to run your infrastructure stack tool,\n\nit provides the mechanism for storing and passing parameter values to the tool out of the box. Assuming your pipeline tool is\n\nitself configured by code, then the values are defined as code and\n\nstored in version control.\n\nConfiguration values are kept separate from the infrastructure code. You can change configuration values for downstream\n\nenvironments and apply them immediately, without needing to\n\nprogress a new version of the infrastructure code from the start of\n\nthe pipeline.\n\nAPPLICABILITY\n\nTeams who are already using a pipeline to apply infrastructure\n\ncode to environments can easily leverage this to set stack\n\nparameters for each environment. However, if stacks require more\n\nthan a few parameter values, defining these in the pipeline configuration has serious drawbacks, so you should avoid this.\n\nCONSEQUENCES\n\nBy defining stack instance variables in the pipeline configuration, you couple configuration values with your delivery process. There\n\nis a risk of the pipeline configuration becoming complicated and\n\nhard to maintain.\n\nThe more configuration values you define in your pipeline, the harder it is to run the stack tool outside the pipeline. Your pipeline\n\ncan become a single point of failure-you may not be able to fix,\n\nrecover, or rebuild an environment in an emergency until you have\n\nrecovered your pipeline. And it can be hard for your team to develop and test stack code outside the pipeline.\n\nIn general, it’s best to keep the pipeline configuration for applying\n\na stack project as small and simple as possible. Most of the logic should live in a script called by the pipeline, rather than in the\n\npipeline configuration.\n\nCI SERVERS, PIPELINES, AND SECRETS\n\nThe first thing most attackers look for when they gain access to a corporate network is CI and CD servers. These are well-known treasure troves of passwords and keys that they can exploit to inflict the maximum evil on your users and customers.\n\nMost of the CI and CD tools that I’ve worked with do not provide a very robust security model. You should assume that anyone who has access to your pipeline tool or who can modify code that the tool executes (i.e., probably every developer in your organization) can access any secret stored by the tool.\n\nThis is true even when the tool encrypts the secrets, because the tool can also decrypt the secrets. If you can get the tool to run a command, you can usually get it to decrypt any secret it stores. You should careful analyze any CI or CD tool you use to assess how well it supports your organization’s security requirements.\n\nIMPLEMENTATION\n\nParameters should be implemented using “as code” configuration of the pipeline tool:\n\nExample 8-7. Example pipeline stage configuration\n\nstage: apply-test-stack input_artifacts: container_cluster_stack commands: unpack ${input_artifacts} stack up --source ./src environment=test cluster_minimum=1 cluster_maximum=1 stack test environment=test\n\nThis example passes the values on the command line. You may\n\nalso set them as environment variables that the stack code uses (as\n\ndescribed in “Pattern: Stack Environment Variables”):\n\nExample 8-8. Example pipeline stage configuration using environment variables stage: apply-test-stack input_artifacts: container_cluster_stack environment_vars: STACK_ENVIRONMENT=test STACK_CLUSTER_MINIMUM=1 STACK_CLUSTER_MAXIMUM=1 commands: unpack ${input_artifacts} stack up --source ./src stack test environment=test\n\nIn this example, the pipeline toolsets those environment variables before running the commands.\n\nMany pipeline tools provide secret management features that you\n\ncan use to pass secrets to your stack command. You set the secret\n\nvalues in the pipeline tool in some fashion, and can then refer to them in your pipeline job:\n\nExample 8-9. Example pipeline stage with secret stage: apply-test-stack input_artifacts: container_cluster_stack commands: unpack ${input_artifacts} stack up --source ./src environment=test \\ cluster_minimum=1 \\ cluster_maximum=1 \\ ssl_cert_passphrase=${STACK_SSL_CERT_PASSPHRASE}\n\nRELATED PATTERNS\n\nDefining the commands and parameters to apply stack code for\n\neach environment in pipeline configuration is similar to the\n\nscripted parameters pattern. The difference is where the scripting\n\nlives-in the pipeline configuration versus in script files.\n\nPattern: Stack Parameter Registry\n\nA Stack Parameter Registry manages the parameter values for stack instances in a central location, rather than with your stack\n\ncode. The stack tool retrieves the relevant values when it applies\n\nthe stack code to a given instance.\n\nFigure 8-4. Stack instance parameter values stored in a central registry.\n\nCONFIGURATION REGISTRIES AND STACK PARAMETER REGISTRIES\n\nI use the term “configuration registry” to describe a service which stores configuration values that may be used for many purposes, including service discovery, stack integration, or monitoring configuration. I’ll describe this in more detail in “Configuration Registry”.\n\nWhen talking specifically about storing configuration values for stack instances, I use the term “stack parameter registry”. So a stack parameter registry is a specific use case for a configuration registry.\n\nALSO KNOWN AS\n\nConfiguration registry for stacks, Infrastructure configuration\n\nregistry\n\nMOTIVATION\n\nStoring parameter values in a registry separates configuration from\n\nimplementation. Parameters in a registry can be set, used, and\n\nviewed by different tools, using different languages and technologies. This flexibility reduces coupling between different\n\nparts of the system. You can replace any tool that uses the registry\n\nwithout affecting any other tool that uses it.\n\nBecause they are tool-agnostic, stack parameter registries can act as a source of truth for infrastructure and even system\n\nconfiguration, acting as a Configuration Management Database\n\n(CMDB - see “Configuration Management Database (CMDB)”).\n\nThis configuration data can be useful in regulated contexts,\n\nmaking it easy to generate reports for auditing.\n\nAPPLICABILITY\n\nIf you are using a configuration registry for other purposes, it\n\nmakes sense to use it as a stack parameter registry, as well. For\n\nexample, a configuration registry is a useful way to integrate multiple stacks (see [Link to Come]).\n\nCONSEQUENCES\n\nA stack parameter registry requires a configuration registry, which is an extra moving part for your overall system. The registry is a\n\ndependency for your stack and a potential point of failure. If the\n\nregistry becomes unavailable, it may be impossible to re-provision\n\nor update the infrastructure stack until you can restore it. This\n\ndependency can be painful in disaster recovery scenarios, putting\n\nthe registry service on the critical path.\n\nManaging parameter values separately from the stack code that\n\nuses it has tradeoffs. You can change the configuration of a stack\n\ninstance without making a change to the stack project. If one team maintains a reusable stack project, other teams can use it to create\n\ntheir own stack instances without needing to add or change\n\nconfiguration files in the stack project itself.\n\nOn the other hand, making changes across more than one place- stack project and parameter registry-adds complexity and\n\nopportunity for mistakes.\n\nIMPLEMENTATION\n\nI’ll discuss ways to implement a parameter registry below\n\n(“Configuration Registry”). In short, it may be a service that stores\n\nkey/value pairs, or it could be a file or directory structure of files\n\nthat contain key/value pairs. Either way, parameter values can\n\nusually be stored in a hierarchical structure, so you can store and find them based on the environment and the stack, and perhaps\n\nother factors like the application, service, team, geography, or\n\ncustomer.\n\nThe values for this chapter’s example container cluster could look like:\n\nExample 8-10. Example of configuration registration entries └── environments/ ├── test/ │ └── container_cluster/ │ ├── cluster_minimum = 1 │ └── cluster_maximum = 1 ├── staging/ │ └── container_cluster/ │ ├── cluster_minimum = 2 │ └── cluster_maximum = 3 └── production/ └── container_cluster/ ├── cluster_minimum = 2 └── cluster_maximum = 6\n\nWhen you apply the infrastructure stack code to an instance, the\n\nstack tool uses the key to retrieve the relevant value. You will need to pass the environment parameter to the stack tool, and the code uses this to refer to the relevant location in the registry:\n\ncluster: id: container_cluster-${environment} minimum: ${get_registry_item(\"/environments/${environment}/container_cluster/c luster_minimum\")} maximum: ${get_registry_item(/environments/${environment}/container_cluster/cl uster_maximum\")}\n\nThe get_registry_item() function in the stack code looks up the value.\n\nThis implementation ties your stack code to the configuration\n\nregistry. You need the registry to run and test your code, which can\n\nbe too heavy. You could work around this by fetching the values\n\nfrom the registry in a script. The script then passes them to the\n\nstack code as normal parameters. Doing this gives you the flexibility to set parameter values in other ways. For reusable stack\n\ncode this is particularly useful, giving users of your code more\n\noptions for how to configure their stack instances.\n\nSecrets management services (“Secrets management”) are a\n\nspecial type of parameter registry. Used correctly, they ensure that\n\nsecrets are only available to people and services that require them,\n\nwithout exposing them more widely. Some configuration registry\n\nproducts and services can be used to store both secret and non-\n\nsecret values. But it’s important to avoid storing secrets in registries which don’t protect them. Doing so makes the registry\n\nan easy target for attackers.\n\nRELATED PATTERNS\n\nYou probably need to pass at least one parameter to the stack tool\n\nto indicate which stack instance’s parameters to use. You can use\n\neither the stack provisioning script or pipeline stack parameter\n\npattern for this.\n\nConfiguration Registry\n\nLarger organizations with many teams working across larger\n\nsystems with many moving parts often find a configuration\n\nregistry useful. It can be useful for configuring stacks instances, as\n\nI described in “Pattern: Stack Parameter Registry”. It can also be\n\nuseful for managing integration dependencies across different\n\nstack instances, applications, and other services, as I’ll explain in\n\n[Link to Come].\n\nAnd a registry can provide a useful source of information about\n\nthe composition and state of your infrastructure. You can use this\n\nto create tools, dashboards, and reports, as well as for monitoring\n\nand auditing your systems.\n\nSo it’s worth digging into how to implement and use a\n\nconfiguration registry.\n\nImplementing a Configuration Registry\n\nThere are different ways to build a configuration registry. You can\n\nuse a registry provided out of the box by your infrastructure\n\nautomation tool. Or you can run a general-purpose registry\n\nproduct. Most cloud providers also have configuration registry\n\nservices that you can use. If you are brave, you can hand-roll a\n\npractical registry using fairly basic pieces.\n\nINFRASTRUCTURE AUTOMATION TOOL REGISTRIES\n\nMany infrastructure automation toolchains include a configuration\n\nregistry service. These tend to be part of a centralized service that\n\nmay also include features such as source code management, monitoring, dashboards, and command orchestration. Examples of\n\nthese include:\n\nChef Infra Server\n\nPuppetDB\n\nAnsible Tower\n\nSalt Mine\n\nYou may be able to use these services with tools outside the\n\ntoolchain that provides them. Most can expose values, so you\n\ncould write a script that discovers information about the current\n\nstate of infrastructure managed by the configuration tool. Some\n\ninfrastructure tool registries are extensible, so you can use them to store the data from other tools.\n\nHowever, this creates a dependency on whatever toolchain\n\nprovides the registry service. The service may not fully support\n\nintegration with third-party tools. They might not offer a contract or API that guarantees future compatibility.\n\nSo if you’re considering using an infrastructure tool’s data store as\n\na general-purpose configuration registry, consider how well it\n\nsupports this use case, and what kind of lock-in it creates.\n\nGENERAL PURPOSE CONFIGURATION REGISTRY PRODUCTS\n\nThere are many dedicated configuration registry and key-value\n\nstore database products available outside the toolchains of a\n\nparticular automation tool. Some examples include:\n\nZookeeper\n\n3 Consul\n\netcd\n\ndoozerd\n\nThese are generally compatible with different tools, languages, and\n\nsystems, so avoid locking you into any particular tool-chain.",
      "page_number": 202
    },
    {
      "number": 8,
      "title": "Configuring Stacks",
      "start_page": 225,
      "end_page": 268,
      "detection_method": "regex_chapter_title",
      "content": "However, it can take a fair bit of work to define how data should be stored. Should keys be structured like\n\nenvironment/service/application, service/application/environment,\n\nor something else entirely? You may need to write and maintain\n\ncustom code to integrate different systems with your registry. And\n\na configuration registry gives your team yet another thing to\n\ndeploy and run.\n\nPLATFORM REGISTRY SERVICES\n\nMost cloud platforms provide a key-value store service, such as\n\nthe AWS SSM Parameter Store. These give you most of the advantages of a general-purpose configuration registry product,\n\nwithout forcing you to install and support it yourself. However, it\n\ndoes tie you to that cloud provider. In some cases, you may find\n\nyourself using a registry service on one cloud to manage\n\ninfrastructure running on another!\n\nDIY CONFIGURATION REGISTRIES\n\nRather than running a configuration registry server, some teams\n\nbuild a custom lightweight configuration registry by storing\n\nconfiguration files in a central location, or by using distributed storage. They typically use an existing file storage service like an\n\nobject store (e.g., an S3 bucket on AWS), a version control system,\n\nnetworked filesystem, or even a web server.\n\nA variation of this is packaging configuration settings into system packages, such as a .deb or .rpm file, and pushing them to an\n\ninternal APT or YUM repository. You can then download\n\nconfiguration files to local servers using the standard package\n\nmanagement tool.\n\nAnother variation is using a standard relational or document store\n\ndatabase server.\n\nAll of these approaches leverage existing services, so they can be\n\nquick to implement for a simple project rather than needing to\n\ninstall and run a new server. But when you get beyond trivial\n\nsituations, you may find yourself building and maintaining the\n\nfunctionality that you could get off the shelf.\n\nSingle or multiple configuration registries\n\nCombining all configuration values from across all of your\n\nsystems, services, and tools is an appealing idea. You could keep\n\neverything in one place rather than sprawling across many\n\ndifferent systems. “One registry to rule them all.” However, this\n\nisn’t always practical in larger, more heterogeneous environments.\n\nMany tools, such as monitoring services and server configuration\n\nsystems, have their own registry. You’ll often find different\n\nregistry and directory products that are very good at specific tasks,\n\nsuch as license management, service discovery, and user directories. Bending all of these tools to use a single system\n\ncreates an ongoing flow of work. Every update to every tool needs\n\nevaluation, testing, and potentially more work to maintain the\n\nintegration.\n\nIt may be better to pull relevant data from across the services\n\nwhere they are stored. Make sure you know which system is the\n\nsource of truth for any particular data or configuration item.\n\nDesign your systems and tools with this understanding.\n\nSome teams use messaging systems to share configuration data as events. Whenever a system changes a configuration value, it sends\n\nan event. Other systems can monitor the event queue for changes\n\nto configuration items in which they are interested.\n\nConfiguration Management Database (CMDB)\n\nA Configuration Management Database (CMDB) is a directory of\n\ninformation about IT assets. Many organizations use these for auditing, control, and governance. They usually include physical\n\nassets like servers, laptops, and racks, as well as software,\n\nconfiguration, versions, and licenses.\n\nThe cloud age is challenging for traditional approaches to CMDBs.\n\nAssumptions about relationships between software, configuration,\n\ndata, and hardware no longer hold with virtualization and cloud\n\nplatforms. Top-down processes to approve and make configuration\n\nchanges struggle to cope with systems that automatically add,\n\nremove, and reconfigure resources on the fly.\n\nIt’s useful to consider the outcomes you need from a CMDB and\n\nlook at ways to achieve these in dynamic environments. Too many\n\norganizations implement CMDBs, assuming there is value in\n\nassembling all information in one place. Instead, they should start by considering how they will use the information, and work back\n\nfrom there to find the best solutions.\n\nTwo reasons organizations use CMDBs are to provide visibility\n\nand to control changes.\n\nCMDB FOR VISIBILITY\n\nData visibility supports many use cases. These include managing\n\ncosts, identifying policy conflicts, and surfacing security\n\nvulnerabilities. For each of your organization’s use cases, be sure\n\nyou understand the requirement. Identify how information needs to be presented to support it, for example, dashboards, alerts, and\n\nreports.\n\nThen, look at ways to implement the presentation of the data, and\n\nwork backward to solutions for collating it.\n\nIt’s essential to understand which system is the source of truth, or\n\nsystem of record, for a given type of data. You may need to extract\n\ndata from multiple systems of record for a particular use case. For\n\nmany purposes, it’s best to collect data directly, rather than marshal it into a separate dataset. For example, writing scripts that\n\ndirectly probe your systems is more reliable for auditing and\n\ncompliance than examining a CMDB, which may or may not be\n\naccurate and timely.\n\nCMDB TO CONTROL CHANGES\n\nSome organizations use a CMDB system as a way to configure\n\ntheir systems from a central location. A CMDB product may offer\n\nsophisticated permission models and workflows to create tight controls over changes.\n\nYou should consider the different elements of your system and\n\nhow each is defined and changed. For configuration parameters\n\nsuch as stack instance parameters, you might want a system that ensures changes are made by authorized users and are tracked and\n\napproved where appropriate.\n\nThese solutions may not work as well for code that defines infrastructure stacks, server configuration, deployment processes,\n\nand the like. The idea of infrastructure as code is to manage\n\nchanges to these using source control systems, automated tests,\n\nand change delivery pipelines. These provide capabilities for\n\nauthorization, auditing, and quality enforcement. Adding another\n\ntool in addition to these, especially one not designed to support agile engineering practices, usually adds more complexity, friction,\n\nand risk than value.\n\nConclusion\n\nThe past few chapters have demonstrated how to apply the core practice of defining systems as code to infrastructure stacks. Each\n\nchapter has described patterns and antipatterns for implementing\n\nstacks as code.\n\nThe next chapter moves on to the next core practice of infrastructure as code, continuously validating code as you work\n\non it. Following that, I’ll describe implementation patterns to\n\napply that practice to infrastructure stacks. Afterward, I’ll show\n\nhow to use stacks to build application runtime environments-by\n\nwhich I mean servers, containers, and clusters-following these core\n\npractices.\n\n1 This passphrase comes from Randall Munroe’s XKCD comic Password Strength. I use it here so that if you haven’t read that comic, you will, because it makes an important point about good passwords. And if you have read the comic, then you and I share the smug glow that comes from recognizing a somewhat obscure reference.\n\n2 Some examples of tools teams can use to securely share passwords include GPG,\n\nKeePass, 1Password, Keeper, and LastPass\n\n3 Consul is a product of Hashicorp, which also makes Terraform, and of course,\n\nthese products work well together. But Consul was created and is maintained as an independent tool, and is not required for Terraform to function. This is why I count it as a general-purpose registry product.\n\nChapter 9. Core Practice: Continuously validate all work in progress\n\nContinuous validation is the second of the three core practices of\n\n1 infrastructure as code :\n\nDefine everything as code\n\nContinuously validate all work in progress\n\nBuild small, simple pieces that you can change independently\n\nTesting is a cornerstone of agile software engineering. Extreme\n\nProgramming (XP) emphasizes writing tests first (TDD) and 3 frequently integrating code (CI) . Continuous Delivery (CD) extends this to validate the full production readiness of code as developers work on it, rather than waiting until they finish\n\n2\n\nworking on a release.\n\nIf a strong focus on testing creates good results when writing\n\napplication code, it’s reasonable to expect it to be useful for infrastructure code as well. In this chapter, I explore strategies for testing and delivering infrastructure. I draw heavily on agile engineering approaches to quality, including TDD, CI, and CD.\n\nEven teams experienced with application testing struggle to test infrastructure very well. So I’ll explain some of the challenges of testing your infrastructure code. I’ll also discuss models for\n\nthinking about testing strategy, including the test pyramid and\n\nSwiss cheese model, and how they relate to infrastructure. Then I’ll explain how to use delivery pipelines to implement testing strategies. In the chapter following this one, I’ll describe some specific patterns and techniques for testing infrastructure stacks.\n\nBut first, let’s consider what continuous validation means for infrastructure code.\n\nWhy continuously validate infrastructure code?\n\nTesting changes to your infrastructure is clearly a good idea. But\n\nthe need to build and maintain a suite of test automation code may not be as clear. We often think of building infrastructure as a one- off activity: build it, test it, then use it. Why spend the effort to create all that test code?\n\nCreating an automated testing suite is hard work, especially when you consider the work needed to implement the delivery and\n\ntesting tools and services - CI servers, pipelines, test runners, test scaffolding, and various types of scanning and validation tools. When getting started with infrastructure as code, building all of these things may seem like more work than building the systems you’ll run on them.\n\nIn “Use Infrastructure as Code to optimize for change” I explained the rationale for implementing systems for delivering changes to\n\ninfrastructure. To recap, you make changes to any non-trivial system far more often than you might expect. You create a new system by making a series of changes. You evolve the design and\n\nimplementation as you learn more about the system you’re building, and about the technology you’re using to build it.\n\nAnd once a system is live, if it has any traction with its users, you need to continuously improve, update, patch, fix, and grow it, which again involves a continuous series of changes.\n\nThe advice in this book aims to help you to continuously validate over as broad a scope of risk as possible.\n\nWhat continuous validation means\n\nOne of the cornerstones of agile engineering is validating as you work-build quality in. The earlier you can find out whether each line of code you write is ready for production, the faster you can\n\nwork, and the sooner you can deliver value. Finding problems more quickly also means spending less time going back to investigate problems and less time fixing and rewriting code. Fixing problems continuously avoids accumulating less technical debt.\n\nMost people get the importance of fast feedback. But what differentiates genuinely high performing teams is how\n\naggressively they pursue truly continuous feedback.\n\nTraditional approaches involve testing after the team has implemented the system’s complete functionality. Timeboxed methodologies take this further. The team tests periodically during development, such as at the end of a sprint, or perhaps with nightly builds. Teams following Lean or Kanban test each story as they complete it.\n\n4\n\nLean and Kanban approaches test each “story \" as you complete it.\n\nTruly continuous validation involves testing even more frequently than this. People write and run tests as they code. And they\n\nfrequently push their code into a centralized, automated validation 5 system-ideally at least once a day .\n\nPeople need to get feedback as soon as possible when they push their code so that they can respond to it with as little interruption\n\nto their flow of work as possible. Tight feedback loops are the essence of Continuous Integration.\n\nIMMEDIATE VALIDATION AND EVENTUAL VALIDATION\n\nAnother way to think of this is to classify each of your validation activities as either immediate or eventual. Immediate validation happens when you push your code. Eventual validation happens after some delay, perhaps after a manual review, or maybe on a schedule.\n\nIdeally, validation is truly immediate, happening as you code. These are validation activities that run in your editor, such as syntax highlighting, or running unit tests. Your editor may support this, or you may use a utility like inotifywait or entr to run checks in a terminal when your code\n\nchanges.\n\nAnother example of immediate validation is pair programming, which is essentially a code review that happens as you work. Pairing provides much faster feedback than code reviews that happen after you’ve finished working on a story or feature, and someone else finds time to review what you’ve done.\n\nThe CI build and the CD pipeline should run immediately every time someone pushes a change to the codebase. Running immediately on each change not only gives them feedback faster. It also ensures a small scope of change for each run. If the pipeline only runs periodically, it may include multiple changes from multiple people. If any of the validations fail, it’s harder to work out which change caused the issue, meaning more people need to get involved and spend time to find and fix it.\n\nWhat should we validate with infrastructure?\n\nThe essence of Continuous Integration is to validate every change someone makes as soon as possible. The essence of Continuous\n\nDelivery is to maximize the scope of that validation. As Jez\n\nHumble says, “We achieve all this by ensuring our code is always 6 in a deployable state.”\n\nQuality assurance is about managing the risks of applying our code\n\nto our systems. Will the code break when we apply it? Does it create the right infrastructure? Does the infrastructure do what we\n\nneed it to do? Does it meet operational criteria for performance,\n\nreliability, and security? Does it comply with regulatory and governance rules?\n\nContinuous Delivery is about broadening the scope of risks that\n\nare immediately validated when pushing a change to the codebase, rather than waiting for eventual validation days, weeks, or even\n\nmonths afterwards. So on every push, a pipeline applies the code to realistic test environments and subjects it to comprehensive\n\nvalidation. Ideally, once the code has run through the automated\n\nstages of the pipeline, it’s fully proven as production-ready.\n\nTeams should identify the risks that come from making changes to their infrastructure code, and create a repeatable process for\n\nvalidating any given change against those risks. This process takes the form of automated test suites and manual validation activities.\n\nA test suite is a collection of automated tests that are run as a\n\ngroup.\n\nWhen people think about automated testing, they generally think about functional tests like unit tests and UI-driven journey tests.\n\nBut the scope of risks is broader than functional defects, so the scope of validation is broader as well. Examples of things that you\n\nmay want to validate, whether automatically or manually, include:\n\nCode quality\n\nIs the code readable and maintainable? Does it follow the team’s standards for how to format and structure code? Depending on the tools and languages you’re using, some tools can scan code for syntax errors, compliance with formatting rules, and run complexity analysis. Depending on how long they’ve been around, and how popular they are, infrastructure languages may not have many (or any!) of these tools. Manual review methods include gated code review processes, code showcase sessions, and pair programming.\n\nFunctionality\n\nDoes it do what it should? Ultimately, functionality is tested by deploying the applications and checking that they work correctly. Doing this indirectly tests that the infrastructure is correct, but it’s useful to catch some types of issues more quickly. An example of this for infrastructure is network routing. Can an HTTPS connection be made from the public Internet to the web servers? It’s often possible to test this kind of thing using a subset of the entire infrastructure.\n\nSecurity\n\nYou can test security at a variety of levels, from code scanning to unit testing to integration testing and production monitoring. There are some tools specific to security testing, such as vulnerability scanners. It may also be useful to write security tests into standard test suites. For example, unit tests can make assertions about open ports, user account handling, or access permissions.\n\nCompliance\n\nSystems may need to comply with laws, regulations, industry standards, contractual obligations, or organizational policies. Ensuring and proving compliance can be time-consuming for infrastructure and operations teams. Automated validation can be enormously useful with this, both to catch violations\n\nquickly and to provide evidence for auditors. As with security, you can do this at multiple levels of validation, from code- level to production validation.\n\nPerformance\n\nAutomated tools can validate how quickly specific actions complete. Testing the speed of a network connection from point A to point B can surface issues to do with the network configuration or the cloud platform if run before you even deploy an application. Finding performance issues on a subset of your system is another example of how you can get faster feedback.\n\nScalability\n\nAutomated tests can prove that scaling works correctly, for example, checking that an auto-scaled cluster adds nodes when it should. Tests can also check whether scaling gives you the outcomes that you expect. For example, perhaps adding nodes to the cluster doesn’t improve capacity, due to a bottleneck somewhere else in the system. Having these tests run frequently means you’ll discover quickly if a change to your infrastructure breaks your scaling.\n\nAvailability\n\nSimilarly, automated testing can prove that your system would be available in the face of potential outages. Your tests can destroy resources, such as nodes of a cluster, and verify that the cluster automatically replaces them. You can also test that scenarios that aren’t automatically resolved are handled gracefully, for example, showing an error page and avoiding data corruption.\n\nOperability\n\nYou can automatically validate any other system requirements needed for operations. Teams can test monitoring (inject errors and prove that monitoring detects and reports them), logging, and automated maintenance activities.\n\nEach of these types of validations can be applied at more than one level of scope, from server configuration to stack code to the fully\n\nintegrated system. I’ll discuss this in “Progressive validation”. But first I’d like to address the things which make infrastructure especially difficult to test.\n\nChallenges with testing infrastructure code\n\nMost of the teams I encounter who work with infrastructure as\n\ncode struggle to implement the same level of automated testing and delivery for their infrastructure code as they have for their application code. And many teams without a background in agile\n\nsoftware engineering find it even more difficult.\n\nThe premise of infrastructure as code is that we can apply software engineering practices such as agile testing to infrastructure. But there are significant differences between infrastructure code and application code. So we need to adapt some of the techniques and\n\nmindsets from application testing to make them practical for infrastructure.\n\nHere are a few challenges that arise from the differences between infrastructure code and application code.\n\nChallenge: Tests for declarative code often have low value\n\nAs mentioned in Chapter 4 (“Building infrastructure with declarative code”), many infrastructure tools use declarative languages, rather than procedural languages. Declarative code\n\ntypically declares the desired state for some infrastructure, such as this code that defines a networking subnet:\n\nsubnet: name: private_A address_range: 192.168.0.0/16\n\nA test for this would simply re-state the code:\n\nassert: subnet(\"private_A\").exists assert: subnet(\"private_A\").address_range is(\"192.168.0.0/16\")\n\nA suite of low-level tests of declarative code can become a\n\nbookkeeping exercise. Every time you change the infrastructure code, you change the test to match. What value do these tests provide? Well, testing is about managing risks, so let’s consider\n\nwhat risks the example test above can uncover:\n\n1. The infrastructure code was never applied\n\n2. The infrastructure code was applied, but the tool failed to apply it correctly, without returning an error\n\n3. Someone changed the infrastructure code but forgot to change the test to match\n\nThe first risk may be a real one, but it doesn’t require a test for\n\nevery single declaration. Assuming you have code that does multiple things on a server, a single test would be enough to reveal that, for whatever reason, the code wasn’t applied.\n\nThe second risk boils down to protecting yourself against a bug in\n\nthe tool you’re using. The tool developers should fix that bug or your team should switch to a more reliable tool. I’ve seen teams\n\nuse tests like this in cases where they found a specific bug, and wanted to protect themselves against it. Testing for this is okay to cover a known issue, but it is wasteful to blanket your code with\n\ndetailed tests just in case your tool has a bug.\n\nThe last risk is circular logic. Removing the test would remove the risk it addresses, and also remove work for the team.\n\nThere are some situations when it’s useful to test declarative code. Two that come to mind are when the declarative code can create\n\ndifferent results, and when you combine multiple declarations.\n\nTESTING VARIABLE DECLARATIVE CODE\n\nThat example of declarative code is simple - the values are hard-\n\ncoded, so the result of applying the code is clear. Variables introduce the possibility of creating different results, which may create risks that make testing more useful. Variables don’t always\n\ncreate variation that needs testing. What if we add some simple variables to the earlier example?\n\nsubnet: name: ${MY_APP}-${MY_ENVIRONMENT} address_range: ${SUBNET_IP_RANGE}\n\nThere isn’t much risk in this code that isn’t already managed by\n\nthe tool that applies it. If someone sets the variables to invalid values, the tool should fail with an error.\n\nThe code becomes riskier when there are more possible outcomes. Let’s add some conditional code to the example:\n\nsubnet: name: ${MY_APP}-${MY_ENVIRONMENT}\n\naddress_range: choose_unique_subset( get_vpc(${MY_ENVIRONMENT}).address_range, 16)\n\nThis code has some logic which might be worth testing. It calls two functions, choose_unique_subset and get_vpc, either of which might fail or return a result that interacts in unexpected ways with the other function.\n\nThe outcome of applying this code varies based on inputs and context, which makes it worth writing tests.\n\nNOTE\n\nImagine that instead of calling these functions, you wrote the code to select a subset of the address range as a part of this declaration for your subnet. This is an example of mixing declarative and functional code (as I discussed in “Implementation Principle: Avoid mixing different types of code”). The tests for the subnet code would need to include various edge cases of the functional code-for example, what happens if the parent range is smaller than the range needed?\n\nIf your declarative code is complex enough that it needs complex testing, it is a sign that you should pull some of the logic out of your declarations and into a library written in a procedural language. You can then write clearly separate tests for that function, and simplify the test for the subnet declaration.\n\nTESTING COMBINATIONS OF DECLARATIVE CODE\n\nAnother situation where testing is more valuable is when you have multiple declarations for infrastructure that combine into more complicated structures. For example, you may have code that\n\ndefines multiple networking structures - an address block, load balancer, routing rules, and gateway. Each piece of code would probably be simple enough that tests would be unnecessary. But\n\nthe combination of these produces an outcome that is worth testing - that someone can make a network connection from point A to point B.\n\nTesting that the tool created the things declared in code is usually\n\nless valuable than testing that they enable the outcomes you want.\n\nChallenge: Testing infrastructure code is slow\n\nTo test infrastructure code, you need to apply it to relevant infrastructure. And provisioning an instance of infrastructure is\n\noften slow, especially when you need to create it on a cloud platform. Most teams who struggle to implement automated infrastructure testing find that the time to create test infrastructure is a barrier for fast feedback.\n\nThe solution is usually a combination of strategies:\n\nDivide infrastructure into more tractable pieces\n\nIt’s useful to include testability as a factor in designing a system’s structure, as it’s one of the key ways to make the system easy to maintain, extend, and evolve. Making pieces smaller is one tactic, as smaller pieces are usually faster to provision and test. It’s easier to write and maintain tests for smaller, more loosely coupled pieces since they are simpler and have less surface area of risk. [Link to Come] discusses this topic in more depth.\n\nClarify, minimize, and isolate dependencies\n\nEach element of your system may have dependencies on other parts of your system, on platform services, and on services and systems that are external to your team, department, or organization. These impact testing, especially if you need to rely on someone else to provide instances to support your test.\n\nThey may be slow, expensive, unreliable, or have inconsistent test data, especially if other users share them. Test doubles are a useful way to isolate a component so that you can test it quickly. You may use test doubles as part of a progressive testing strategy, first testing your component with test doubles, and later testing it integrated with other components and services.\n\nProgressive testing\n\nYou’ll usually have multiple test suites to test different aspects of the system. You can run faster tests first, to get quicker feedback if they fail, and only run slower, broader-scoped tests after those have passed. I’ll delve into this in “Progressive validation”.\n\nChoice of ephemeral or persistent instances\n\nYou may create and destroy an instance of the infrastructure each time you test it (an ephemeral instance), or you may leave an instance running in between runs (persistent instances). Using ephemeral instances can make tests significantly slower, but are cleaner and give more consistent results. Keeping persistent instances cuts the time needed to run tests, but may leave changes and accumulate inconsistencies over time. Choose the appropriate strategy for a given set of tests, and revisit the decision based on how well it’s working. I provide more concrete examples of implementing ephemeral and persistent instances in “Pattern: Ephemeral test stack”.\n\nOnline and offline tests\n\nSome types of validation are online, requiring you to provision infrastructure on the “real” cloud platform. Others can run offline on your laptop or a build agent. Validation that you can run offline includes code syntax checking and tests that run in a virtual machine or container instance. Consider the nature of your various tests, and be aware of which ones can run where. Offline validation is usually much faster, so you’ll tend to run\n\nthem earlier. You can use test doubles to emulate your cloud API offline for some tests.\n\nWith any of these strategies, you should regularly asses how well\n\nthey are working. If tests are unreliable, either failing to run correctly or returning inconsistent results, then you should drill\n\ninto the reasons for this and either fix them or replace them with something else. If tests rarely fail, or if the same tests almost\n\nalways fail together, you may be able to strip them out to simplify\n\nyour test suite. If you spend more time finding and fixing problems that originate in your tests rather than in the code you’re testing,\n\nlook for ways to simplify and improve them.\n\nTEST DOUBLES\n\nMocks, fakes, and stubs are all types of test doubles. A test double replaces a dependency needed by a component so you can test it in isolation. These terms tend to be used in different ways by different people, but I’ve found the definitions used by Gerard Meszaros in his xUnit 7 patterns book to be useful.\n\n8\n\nIn the context of infrastructure, there are a growing number of tools that allow you to mock the APIs of cloud vendors. You can apply your infrastructure code to a local mocked cloud to validate some aspects of the code. These won’t tell you whether your networking structures work correctly, but they should tell you whether they’re roughly valid.\n\nProgressive validation\n\nMost non-trivial systems use multiple suites of tests to validate\n\nchanges. Different suites may validate different things (as listed in “What should we validate with infrastructure?”). One suite may\n\nvalidate one concern offline, such as checking for security\n\nvulnerabilities by scanning code syntax. Another suite may run online checks for the same concern, for example, by probing a\n\nrunning instance of an infrastructure stack for security\n\nvulnerabilities.\n\nIn theory, you could run all of your test suites in parallel when you\n\nchange your code. In practice, this is wasteful. Firstly, you\n\nprobably need to provision a lot of infrastructure to test everything at once. Secondly, it’s potentially wasteful. If the code fails to pass\n\nthe faster tests, there’s no point in running the slower, more expensive tests until you fix the problem.\n\nProgressive validation involves running test suites in a sequence. The sequence builds up, starting with simpler tests that run more\n\nquickly over a smaller scope of code, then building up to more comprehensive tests over a broader set of integrated components\n\nand services.\n\nModels like the test pyramid and Swiss cheese testing offer visual\n\nmetaphors for progressive validation. A delivery pipeline ([Link to Come]) is a technical implementation of progressive testing, with a\n\nseries of stages, each running a set of test suites.\n\nValidation stages\n\nEach system needs a unique test strategy, which includes defining\n\nthe validation stages and deciding the order for running them. There are several characteristics for each validation stage,\n\nincluding the components under test, the dependencies involved,\n\nand the environment required.\n\nSCOPE OF COMPONENTS BEING VALIDATED\n\nIn a progressive validation strategy, earlier stages validate\n\nindividual components, while later stages integrate components and test them together. For example, an earlier stage might test\n\ncode that installs and configures a web server package onto a\n\nvirtual machine. A later stage tests a server with this package\n\ninstalled, running in an infrastructure stack with firewall rules and\n\nnetwork routes.\n\nFigure 9-1. Progressive integration and testing of components\n\nOne stage might run validations for multiple components, such as\n\na suite of unit tests. Or, different components may each have a separate validation stage.\n\nUNIT TESTS\n\n9\n\nA unit test validates a subsection of a system, usually testing a single component at the lowest level for that system. With object oriented software this is often a class, or related set of classes. With infrastructure, this might be a single server configuration element-Chef cookbook, Puppet manifest, or Ansible playbook, for example.\n\nA unit test in software code executes a small subsection of a\n\nprogram, on the order of a one or two classes, to make sure that they run correctly. Most infrastructure tools have some kind of unit\n\ntesting tool or framework, such as rspec-puppet and ChefSpec.\n\nSaltstack even comes with its own built-in unit testing support.\n\nTools like this allow a particular configuration definition to be run without actually applying it to a server. They usually include\n\nfunctionality to emulate other parts of a system well enough to\n\ncheck that the definition behaves correctly. This requires ensuring that each of your definitions, scripts, and other low-level elements\n\ncan be independently executed. Restructuring things to make test\n\nisolation possible may be challenging, but results in a cleaner design.\n\nSCOPE OF DEPENDENCIES USED FOR THE STAGE\n\nMany elements of a system depend on other services. An application server stack might connect to an identity management\n\nservice to handle user authentication. To progressively validate\n\nthis, you might first run a stage that tests the application server\n\nwithout the identity management service, perhaps using a mock service to stand in for it. A later stage would run additional tests on\n\nthe application server integrated with a test instance of the identity\n\nmanagement service, and the production stage would integrate with the production instance:\n\nFigure 9-2. Progressive integration with dependencies\n\nAvoid creating unnecessary stages in your pipeline, as each stage\n\nadds time and cost to your delivery process. So, don’t create\n\nseparate stages for each component and integration just for completeness. Split validation into stages this way only when it\n\nadds enough value to be worth the overhead. Some reasons which\n\nmay drive you to do this include speed, reliability, cost, and\n\ncontrol.\n\nProvisioning a mock may be much faster than provisioning the external service. Some third -party services are not very reliable,\n\nmaking it unclear whether a test has failed because of your coding\n\nerror, or a blip with the service. The third party service may be expensive, or it may not give you enough flexibility to set up data\n\nand configuration for your tests.\n\nPLATFORM ELEMENTS NEEDED FOR VALIDATION\n\nPlatform services are a particular type of dependency for your\n\nsystem. Your system may ultimately run on your infrastructure\n\nplatform, but you may be able to run and test parts of it offline usefully.\n\nFor example, code that defines networking structures needs to\n\nprovision those structures on the cloud platform for meaningful\n\nvalidation. But you may be able to test code that installs an application server package in a local virtual machine, or even in a\n\ncontainer, rather than needing to stand up a virtual machine on your cloud platform.\n\nSo earlier validation stages may be able to run without using the\n\nfull cloud platform for some components:\n\nFigure 9-3. Progressive use of platform elements\n\nTesting in production\n\nTesting releases and changes before applying them to production is a big focus in our industry. At one client, I counted eight groups\n\n10\n\nthat needed to review and approve releases various technical teams who had to carry out tasks to install and\n\n, even apart from the\n\nconfigure various parts of the system.\n\nAs systems increase in complexity and scale, the scope of risks\n\nthat you can practically check for outside of production shrinks. This isn’t to say that there is no value in testing changes before\n\napplying them to production. But believing that pre-release testing\n\ncan comprehensively cover your risks leads to:\n\nOver-investing in pre-release testing, well past the point of diminishing returns,\n\nUnder-investing in testing in your production environment.\n\nGOING DEEPER ON TESTING IN PRODUCTION\n\nFor more on testing in production, I recommend watching Charity Majors’ talk, [Yes, I Test in Production (And So Should You)] (https://www.infoq.com/presentations/testing-production-2018/), which is a key source of my thinking on this topic.\n\nWHAT YOU CAN’T REPLICATE OUTSIDE PRODUCTION\n\nThere are several characteristics of production environments\n\nwhich you can’t realistically replicate outside of production:\n\nData\n\nYour production system may have larger data sets than you can replicate, and will undoubtedly have unexpected data values and combinations, thanks to your users.\n\nUsers\n\nDue to their sheer numbers, your users are far more creative at doing strange things than your testing staff.\n\nTraffic\n\nIf your system has a non-trivial level of traffic, you can’t replicate the number and types of activities it will regularly experience. A week-long soak test is trivial compared to a year of running in production.\n\nConcurrency\n\nTesting tools can emulate multiple users using the system at the same time, but they can’t replicate the unusual combinations of things that your users do concurrently.\n\nThe two challenges that come from these characteristics are that they create risks that you can’t predict, and they create conditions\n\nthat you can’t replicate well enough to test anywhere other than\n\nproduction.\n\nBy running tests in production, you take advantage of the conditions that exist there-large natural data sets and unpredictable\n\nconcurrent activity.\n\nWHY TEST ANYWHERE OTHER THAN PRODUCTION?\n\nObviously testing in production is not a substitute for testing changes before you apply them to production. It helps to be clear on what you realistically can (and should!) test beforehand:\n\nDoes it work?\n\nDoes my code run?\n\nDoes it fail in ways I can predict?\n\nDoes it fail in the ways it has failed before?\n\nTesting changes before production addresses the known unknowns, the things that you know might go wrong. Testing changes in production addresses the unknown unknowns, the more unpredictable risks.\n\nMANAGING THE RISKS OF TESTING IN PRODUCTION\n\nTesting in production creates new risks. There are a few things that help manage these risks:\n\nMonitoring\n\nEffective monitoring gives confidence that you can detect problems caused by your tests so you can stop them quickly. This includes detecting when tests are causing issues so you can stop them quickly (see [Link to Come]).\n\nObservability\n\nObservability gives you visibility into what’s happening within the system at a level of detail that helps you to investigate and fix problems quickly, as well as improving the quality of what you can test.\n\n11\n\nZero-Downtime Deployment\n\nBeing able to deploy and roll back changes quickly and seamlessly helps mitigate the risk of errors (see [Link to Come]).\n\nProgressive Deployment\n\nIf you can run different versions of components concurrently, or have different configurations for different sets of users, you can test changes in production conditions before exposing them to users (see [Link to Come]).\n\nData management\n\nYour production tests shouldn’t make inappropriate changes to data or expose sensitive data. You can maintain test data records, such as users and credit card numbers, that won’t trigger real-world actions.\n\nMONITORING AS TESTING\n\nMonitoring can be seen as passive testing in production. It’s not true testing, in that you aren’t taking an action and checking the result. Instead, you’re observing the natural activity of your users and watching for undesirable outcomes.\n\nMonitoring should form a part of the testing strategy, because it is a part of the mix of things you do to manage risks to your system.\n\nProgressive validation models\n\nWith the different types of things you can validate and the various\n\nways to group tests into stages for progressive validation, a model\n\ncan help think about how to structure validation and testing\n\nactivities for changes to your particular system. I’ll describe the\n\nmost popular model for this, the testing pyramid, and an\n\ninteresting alternative, the Swiss cheese model.\n\nThe guiding principle for a progressive feedback strategy is to get\n\nfast, accurate feedback. As a rule, this means running faster tests\n\nwith a narrower scope and fewer dependencies first and then\n\nrunning tests that progressively add more components and integration points. This way, small errors are quickly made visible\n\nso they can be quickly fixed and re-tested.\n\nFigure 9-4. Scope vs. speed of progressive testing\n\nWhen a broadly-scoped test fails, you have a large surface area of\n\ncomponents and dependencies to investigate. So you should try to\n\nfind any potential area at the earliest point, with the smallest scope\n\nthat you can.\n\nAnother goal of a test strategy is to keep the overall test suite\n\nmanageable. Avoid duplicating validations at different levels. For\n\nexample, you may test that your application server configuration code sets the correct directory permissions on the log folder. This\n\ntest would run in an earlier stage, that explicitly tests the server\n\nconfiguration. You should not have a test that checks file\n\npermissions in the stage that tests the full infrastructure stack\n\nprovisioned in the cloud.\n\nTest pyramid\n\nThe test pyramid is a well-known model for software\n\n12\n\ntesting https://martinfowler.com/articles/practical-test-\n\npyramid.html by Ham Vocke is a thorough reference.], so I won’t spend much time describing it.\n\nThe key idea of the test pyramid is that you should have more tests\n\nat the lower layers, which are the earlier stages in your\n\nprogression. With software, it makes sense to have many unit tests, as these are fast and can catch errors quickly:\n\nFigure 9-5. The classic test pyramid\n\nBut lower-level infrastructure tests tend to have less value (as I\n\ndiscussed in “Challenge: Tests for declarative code often have low\n\nvalue”). This means that, although you’ll almost certainly have\n\nlow-level infrastructure tests, there may not be as many as the\n\npyramid model suggests. So an infrastructure test suite may end up\n\nlooking more like a diamond:\n\nFigure 9-6. The infrastructure test diamond\n\nIt can be helpful to keep this in mind when discussing the right\n\nbalance for a test suite with people who come from a software testing background. They may be uncomfortable with the different\n\nbalance of tests.\n\nSwiss cheese testing model\n\nAnother way to think about how to organize progressive tests is\n\nthe Swiss cheese model. This concept for risk management comes\n\n13\n\nfrom outside the software industry . The idea is that a given layer of testing may have holes, like one slice of Swiss cheese, that can\n\nmiss a defect or risk. But when you combine multiple layers, it\n\nlooks more like a block of Swiss cheese, where no hole goes all\n\nthe way through.\n\nThe point of using the Swiss cheese model when thinking about\n\ninfrastructure testing is that you focus on where to catch any given\n\nrisk. You still want to catch issues in the earliest layer where it is\n\nfeasible to do so, but the important thing is that it is tested\n\nsomewhere in the overall model:\n\nFigure 9-7. Swiss cheese testing model\n\nThe key takeaway is, test based on risk, rather than based on fitting a formula.\n\nPipelines for validation\n\nYou can implement progressive validation and delivery of\n\ninfrastructure elements using a continuous delivery pipeline . I\n\n14\n\noften call this a change delivery pipeline to emphasize its relationship to the change management process.\n\nThe idea is that when someone pushes a code change to the source\n\ncontrol repository, the team uses a central system to progress the change through a series of stages to test and deliver the change.\n\nThis process is automated, although people may be involved to\n\ntrigger or approve activities. Some principles for a change delivery\n\npipeline include:\n\nAutomate processes\n\nThe pipeline system runs scripts to apply infrastructure code, deploy software, and execute automated tests. Humans may review changes, and even conduct exploratory testing on environments. But they should not be running commands by hand to deploy and apply changes. They also shouldn’t be selecting configuration options or making other decisions on the fly. These actions should be defined as code and executed by the system. Automating processes ensures they are carried out consistently every time, for every stage. Doing this improves the reliability of your tests, and creates consistency between instances of the infrastructure.\n\nPush every change from the start of the pipeline\n\nNever change the code once it’s progressing through the pipeline. If you find an error in a “downstream” (later) stage in a pipeline, don’t fix it in that stage and continue through the rest of the pipeline. Instead, fix the code in the repository and push the new change from the start of the pipeline. This practice ensures that the change is fully tested.\n\nFigure 9-8. Start a new pipeline run to correct failures\n\nIn the above figure, one change successfully passes through the\n\npipeline. The second change fails in the middle of the pipeline. A\n\nfix is made and pushed through to production as the third pipeline run.\n\nPipeline stages\n\nEach stage of the pipeline may do different things and may trigger\n\nin different ways. Some of the characteristics of a given pipeline\n\nstage include:\n\nTrigger\n\nWhat makes the stage run? It may automatically run when there is an event, such as a change pushed to the code repository or the successful execution of the stage before it in the pipeline. Or someone may trigger the stage manually, as when a tester or release manager decides to apply a code change to a given environment.\n\nActivity\n\nWhat happens when the stage runs? Multiple actions could execute for a stage. For example, a stage might apply code to provision an infrastructure stack, run tests, and then destroy the stack.\n\nApproval\n\nHow is the stage marked as passing or failing? The system could mark the stage as passing (often referred to as “green”) when commands run without errors, and automated tests all pass. Or a human may need to mark the stage as approved. For example, a tester may approve the stage after carrying out exploratory testing on the change. You can also use manual approval stages to support governance sign-offs.\n\nOutput\n\nDoes the stage produce an artifact or other material? Typical outputs include archive files containing the infrastructure code and test results.\n\nDelivery pipeline software and services\n\nYou need software or a hosted service to build a pipeline. A\n\npipeline system needs to do a few things:\n\nGive you a way to configure the pipeline stages.\n\nTrigger stages from different actions, including automated events and manual triggers. The tool should support more complex relationships such as fanning in (one stage with\n\nmultiple input stages) and fanning out (one stage with multiple output stages).\n\nSupport any actions you may need for your stages, including applying infrastructure code and running tests. You should be able to create custom activities rather than having a fixed set of supported ones.\n\nHandle artifacts and other outputs of stages, including being able to pass them from one stage to the next.\n\nHelp you trace and correlate specific versions and instances of code, artifacts, outputs, and infrastructure.\n\nThere are a few options for a pipeline system:\n\nCI Software\n\nMany teams use Continuous Integration software such as Jenkins, Team City, and Bamboo, to create pipelines. These are often “job-oriented” rather than “stream-oriented.” The core design doesn’t inherently correlate versions of code, artifacts, and runs of different jobs. Most of these products have added support for pipelines as an overlay in their UI and configuration.\n\nCD Software\n\nCD software is built around the pipeline concept. You define each stage as part of a pipeline, and code versions and artifacts are associated with the pipeline so you can trace them forwards and backward. CD tools include GoCD , ConcourseCI , and BuildKite.\n\n15\n\n16\n\nSaaS Services\n\nHosted CI and CD services include CircleCI, TravisCI, AppVeyor, Drone, and BoxFuse.\n\nCloud Platform Services\n\nMost cloud vendors include CI and CD services, including AWS CodeBuild (CI) and AWS CodePipeline (CD), and Azure DevOps\n\n17\n\nSource Code Repository Services\n\nMany source code repository products and vendors have added CI support that you can use to create pipelines. Two prominent examples are Github actions, and Gitlab CI and CD.\n\nThe products I mentioned above were all designed with application software in mind. You can use most of them to build\n\n18\n\npipelines for infrastructure, although they may need extra work.\n\nA few products and services designed for infrastructure as code are\n\nemerging as I write this. This is a rapidly changing area, so I suspect what I have to say about these tools is out of date by the\n\ntime you read this, and missing newer tools. But it’s worth looking\n\nat what exists now, to give context for evaluating tools as they\n\nemerge and evolve:\n\nAtlantis is a product that helps you to manage pull requests for Terraform projects, and to run plan and apply for a single instance. It doesn’t run tests, but you can use it to create a limited pipeline that handles code reviews and approvals for infrastructure changes.\n\nTerraform Cloud is evolving rapidly. It is Terraform- specific, and it includes more features (such as a module registry) than CI and pipelines. You can use Terraform cloud to create a limited pipeline that plans and applies a project’s code to multiple environments. But it doesn’t run tests other than policy validations with Hashicorp’s own Sentinel product.\n\nWeaveWorks makes products and services for managing Kubernetes clusters. These include tools for managing the\n\ndelivery of changes to cluster configuration as well as applications using pipelines based around git branches, an approach they call GitOps. Their solutions don’t seem applicable to general infrastructure, but even if you don’t use their stack, it’s an emerging model for pipelines that’s worth watching. I’ll touch on it a bit more in [Link to Come].\n\nThe next chapter (Chapter 10) includes details on how to structure\n\npipelines for testing and delivering infrastructure stacks. I also\n\ndiscuss pipeline implementation for building servers ([Link to\n\nCome]) and clusters ([Link to Come]).\n\nConclusion\n\nThis chapter has discussed general challenges and approaches for\n\ntesting infrastructure. I’ve avoided going very deeply into the\n\nsubjects of testing, quality, and risk management. If these aren’t\n\nareas you have much experience with, this chapter may give you\n\nenough to get started. I encourage you to read more, as testing and QA are fundamental to infrastructure as code.\n\nWhile I’ve touched on aspects of testing that are specific to\n\ninfrastructure, such as the challenges of writing tests for\n\ninfrastructure, I haven’t given many concrete examples. The next chapter, Chapter 10, should do this for infrastructure stacks.\n\nLikewise, [Link to Come] explains how to write tests and build\n\npipelines for servers, as [Link to Come] does for clusters.\n\n1 I listed these three core practices in Chapter 1\n\n2 See https://martinfowler.com/articles/continuousIntegration.html\n\n3 Jez Humble and David Farley’s book Continuous Delivery (Addison-Wesley)\n\ndefined the principles and practices for CD, raising it from an obscure phrase in the Agile Manifesto to a widespread practice among software delivery teams.\n\n4 See https://www.mountaingoatsoftware.com/agile/user-stories for an explanation\n\nof agile stories.\n\n5 The Accelerate research published in the annual State of the DevOps Report finds\n\nthat teams where everyone merges their code at least daily tend to be more effective than those who do so less often. In the most effective teams I’ve seen, developers push their code multiple times a day, sometimes as often as every hour or so.\n\n6 https://continuousdelivery.com/\n\n7 Martin Fowler’s bliki Mocks Aren’t Stubs is a useful reference for test doubles.\n\n8 Examples of cloud mocking tools and libraries include Localstack and moto.\n\nhttps://dobetterascode.com/types/mock/ maintains a current list of this kind of tool.\n\n9 See the extremeprogramming.org definition of unit tests. Martin Fowler’s bliki\n\ndefinition of UnitTest discusses a few ways of thinking of unit tests.\n\n10 These groups were: change management, infosec, risk management, service\n\nmanagement, transition management, system integration testing, user acceptance, and the technical governance board.\n\n11 Although it’s often conflated with monitoring, observability is about giving people ways to understand what’s going on inside your system. See honeycomb.io’s Introduction to Observability.\n\n12 [The Practical Test Pyramid\n\n13 See https://en.wikipedia.org/wiki/Swiss_cheese_model\n\n14 Sam Newman described the concept of build pipelines in several blog posts\n\nstarting in 2005, which he recaps in a 2009 blog post, A Brief and Incomplete History of Build Pipelines. Jez Humble and Dave Farley’s Continuous Delivery book (referenced earlier in this chapter) popularized pipelines. Jez has documented the deployment pipeline pattern on his website.\n\n15 In the interests of full disclosure, my employer, ThoughtWorks created GoCD. It\n\nwas previously a commercial product, but it is now fully open source.\n\n16 In spite of its name, ConcourseCI is designed around pipelines rather than CI jobs.\n\n17 I can’t mention Azure DevOps without pointing out the terribleness of that name for a service. DevOps is about culture first, not tools and technology first. Read Matt Skelton’s review of John Willis’ talk on DevOps culture.",
      "page_number": 225
    },
    {
      "number": 9,
      "title": "Core Practice: Continuously validate all work in progress",
      "start_page": 269,
      "end_page": 308,
      "detection_method": "regex_chapter_title",
      "content": "18 Although some of the products I listed are designed for building container images,\n\nso may be harder to adapt for infrastructure.\n\nChapter 10. Testing Infrastructure Stacks\n\nChapter 5 describes an infrastructure stack as a collection of infrastructure resources that you define, provision, and update as a\n\nunit from an infrastructure platform. I cover general topics for testing infrastructure in Chapter 9. In this chapter, I combine these two topics, delving into specific techniques for testing\n\ninfrastructure code at the level of the stack.\n\n1\n\nI use the Foodspin example to illustrate how to test a stack. I start by outlining the example infrastructure involved, to set the context for specific examples later in the chapter.\n\nI then give examples of types of testing activities, grouped into\n\noffline validation, which you can run without actually provisioning infrastructure, and online validation which does provision infrastructure on your platform. I give examples use pseudocode\n\nfor both stack definitions and tests.\n\nYou can use test fixtures, such as mocks and fakes, to help you test your stacks as standalone entities rather than needing to provision\n\nlarge swathes of infrastructure for each test.\n\nIn addition to test fixtures, you need to provision instances of your stack for online validation. I describe several patterns and one antipattern for managing the lifecycle of these test instances.\n\nTo wrap this up, you need a way to orchestrate test instances and\n\nfixtures for running tests. I describe how people use scripts and tools for this, with some pseudocode examples.\n\nExample infrastructure\n\nThe Foodspin team uses reusable stack projects (“Pattern: Reusable Stack”) to create consistent instances of application infrastructure for each of their customers. They can also use this to\n\ncreate test instances of the infrastructure in the pipeline.\n\nThe infrastructure for these examples includes the following elements (based on the examples from Chapter 3):\n\nFigure 10-1. Example infrastructure used for a Foodspin application instance\n\nWeb Server Cluster\n\nThe team runs a single web server container cluster for each region and in each test environment. Applications in the region or environment share this cluster. The examples in this chapter focus on the infrastructure that is specific to each customer, rather than shared infrastructure. So the shared cluster is a dependency in the examples here. For details of how changes are coordinated and tested across this infrastructure, see [Link to Come].\n\nApplication Server\n\nThe infrastructure for each application instance includes a virtual machine, a persistent disk volume, and networking. The networking includes an address block, gateway, routes to the server on its network port, and network access rules.\n\nDatabase Server\n\nFoodspin runs a separate database instance for each customer application instance, using their provider’s DBaaS service. Their infrastructure code also defines an address block, routing, and database authentication and access rules.\n\n2\n\nThe example stack\n\nTo start, we can define a single reusable stack that has all of the\n\ninfrastructure other than the web server cluster. The project structure could look like this:\n\nExample 10-1. Stack project for Foodspin customer application stack-project/ └── src/ ├── appserver_vm.infra ├── appserver_networking.infra ├── database.infra └── database_networking.infra\n\nWithin this project, the file appserver_vm.infra would include code along these lines:\n\nExample 10-2. Partial contents of appserver_vm.infra virtual_machine: name: appserver-${customer}-${environment} ram: 4GB address_block: ADDRESS_BLOCK.appserver-${customer}-${environment} storage_volume: STORAGE_VOLUME.app-storage-${customer}-${environment} base_image: SERVER_IMAGE.foodspin_java_server_image provision: tool: servermaker parameters: maker_server: maker.foodspin.io role: appserver customer: ${customer} environment: ${environment}\n\nstorage_volume: id: app-storage-${customer}-${environment} size: 80GB format: xfs\n\nA team member or automated process can create or update an\n\ninstance of the stack by running the stack tool. They pass values to\n\nthe instance using one of the patterns from Chapter 8.\n\nAs described in Chapter 9, the team uses multiple validation stages (“Progressive validation”), organized in a sequential pipeline\n\n(“Pipelines for validation”).\n\nPipeline for the example stack\n\n3\n\nA simple pipeline for the Foodspin application infrastructure stack has two testing stages, followed by a stage that applies the\n\ncode to each customer’s production environment:\n\nFigure 10-2. Simplified example pipeline for a stack\n\nThe first stage of the pipeline is an offline stage, and the second is\n\nan online stage. Each of these stages can run several different validation activities.\n\nOffline validation stages for stacks\n\nAn offline stage runs “locally” on an agent node of the service that runs the stage (see “Delivery pipeline software and services”),\n\nrather than needing to provision infrastructure on your infrastructure platform. Strict offline validation runs entirely within the local server or container instance, without connecting to\n\nany external services such as a database. A softer offline stage might connect to an existing service instance, perhaps even a cloud API, but doesn’t use any real stack infrastructure.\n\nAn offline stage should:\n\nRun quickly, giving fast feedback if something is incorrect,\n\nValidate the correctness of components in isolation, to give confidence in each component, and to simplify debugging failures,\n\nProve the component is cleanly decoupled.\n\nSome of the things you can check for your stack code in an offline stage are syntax checking, offline static code analysis, static code analysis with the platform API, and testing with a mock API.\n\nSyntax checking\n\nWith most stack tools, you can run a dry run command that parses your code without applying it to infrastructure. The command exits with an error if there is a syntax error. The check tells you very\n\nquickly when you’ve made a typo in your code change, but misses many other errors. Examples of syntax checking include terraform validate and aws cloudformation validate- template.\n\nThe output of a failing syntax validation might look like this:\n\n$ stack validate\n\nError: Invalid resource type\n\non appserver_vm.infra line 1, in resource \"virtual_mahcine\":\n\nstack does not support resource type \"virtual_mahcine\".\n\nOffline static code analysis\n\nSome tools can parse and analyze stack source code for a wider class of issues than just syntax, but still without connecting to an 4 infrastructure platform. This analysis is often called linting _ comes from a classic Unix utility that analyzes C source code.].\n\nThis kind of tool may look for coding errors, confusing or poor coding style, adherence to code style policy, or potential security issues. Some tools can even modify code to match a certain style, such as the terraform fmt command. There are not as many tools that can analyze infrastructure code as there are for application programming languages. Examples include tflint, CloudFormation Linter, and cfn_nag.\n\nHere’s an example of an error from a fictional analysis tool:\n\n$ stacklint 1 issue(s) found:\n\nNotice: Missing 'Name' tag (vms_must_have_standard_tags)\n\non appserver_vm.infra line 1, in resource \"virtual_machine\":\n\nIn this example, we have a custom rule named vms_must_have_standard_tags that requires all virtual machines to have a set of tags, including one called Name.\n\nStatic code analysis with API\n\nDepending on the tool, some static code analysis checks may connect to the cloud platform API to check for conflicts with what the platform supports. For example, tflint can check Terraform\n\nproject code to make sure that any instance types (virtual machine sizes) or AMIs (server images) defined in the code actually exist. Unlike previewing changes (“Preview: Seeing what changes will\n\nbe made”), this type of validation tests the code in general, rather than against a specific stack instance on the platform.\n\nThe following example output fails because the code declaring the virtual server specifies a sever image that doesn’t exist on the platform:\n\n$ stacklint 1 issue(s) found:\n\nNotice: base_image 'SERVER_IMAGE.foodspin_java_server_image' doesn't exist (validate_server_images)\n\non appserver_vm.infra line 5, in resource \"virtual_machine\":\n\nTesting with mock API\n\nYou may be able to apply your stack code to a local, mock instance of your infrastructure platform’s API. There are not many\n\ntools for mocking these APIs. The only one I’m aware of as of this writing is Localstack. Applying your stack code to a local mock can reveal coding errors that syntax or code analysis checks might not find. Depending on how functional the mocks are, you may be\n\nable to carry out some online-type validations (as I describe shortly). But in practice, what you can do is limited.\n\nOnline validation stages for stacks\n\nAn online stage involves using the infrastructure platform to create and interact with an instance of the stack. This type of stage is\n\nslower but can carry out more meaningful validations than online validation. The delivery pipeline service usually runs the stack tool on one of its nodes or agents, but it uses the platform API to interact with an instance of the stack. The service needs to\n\nauthenticate to the platform’s API, see “Secrets and source code” for ideas on how to handle this securely.\n\nAlthough an online test stage depends on the infrastructure platform, you should be able to test the stack with a minimum of\n\nother dependencies. In particular, you should design your infrastructure, stacks, and tests so that you can create and test an instance of a stack without needing to integrate with instances of other stacks.\n\nFor example, the Foodspin customer application infrastructure\n\nworks with a shared web server cluster stack. However, they implement their infrastructure, and testing stages, so that they can test the application stack code without an instance of the web\n\nserver cluster.\n\nI cover techniques for splitting stacks and keeping them loosely coupled in [Link to Come]. Assuming you have built your infrastructure in this way, you can use test fixtures to make it\n\npossible to test a stack on its own, as described a bit later in this chapter (“Using test fixtures to handle dependencies”).\n\nFirst, consider how different types of online stack tests work. The validations that an online stage can run include previewing\n\nchanges, verifying that changes are applied correctly, and proving the outcomes.\n\nPreview: Seeing what changes will be made\n\nSome stack tools can compare stack code against a stack instance to list changes it would make without actually changing anything. Terraform’s plan subcommand is a well-known example.\n\nMost often, people preview changes against production instances as a safety measure, so someone can review the list of changes to\n\nreassure themselves that nothing unexpected will happen. Applying changes to a stack can be done with a two-step process in a pipeline. The first step runs the preview, and a person triggers\n\nthe second step, to apply the changes, once they’ve reviewed the results of the preview.\n\nHaving people review changes isn’t very reliable. People might misunderstand or not notice a problematic change. You can write automated tests that check the output of a preview command. This\n\nkind of test might check changes against policies, failing if the code creates a deprecated resource type, for example. Or it might check for disruptive changes-fail if the code will rebuild or destroy\n\na database instance.\n\nAnother issue is that stack tool previews are usually not deep. A preview tells you that this code will create a new server:\n\nvirtual_machine: name: myappserver base_image: \"java_server_master\"\n\nBut the preview may not tell you that \"java_server_master\" doesn’t exist, although the apply command will fail to create the server.\n\nPreviewing stack changes is useful for checking a limited set of\n\nrisks immediately before applying a code change to an instance. But it is less useful for testing code that you intend to reuse across multiple instances, such as across test environments for release\n\ndelivery. Teams using copy-paste environments (“Antipattern: Copy-Paste Environments”) often use a preview stage as a minimal validation for each environment. But teams using reusable stacks (“Pattern: Reusable Stack”) can use test instances\n\nfor more meaningful validation of their code.\n\nVerification: Making assertions about infrastructure resources\n\nGiven a stack instance, you can have tests in an online stage that make assertions about the infrastructure in the stack. Testing\n\n5\n\n6\n\n7\n\nframeworks such as Awspec , Inspec , and Terratest use the infrastructure platform’s API, making requests to gather information about infrastructure and then making assertions about\n\nit.\n\nA set of tests for the virtual machine from the example stack code\n\nearlier in this chapter could look like this:\n\ngiven virtual_machine(name: \"appserver-testcustomerA-staging\") { it { exists } it { is_running } it { passes_healthcheck } it { has_attached storage_volume(name: \"app-storage-testcustomerA- staging\") } }\n\nMost stack testing tools provide libraries to help write assertions\n\nabout the types of infrastructure resources I describe in Chapter 3. This example test uses a virtual_machine resource to identify the VM in the stack instance for the staging environment. It makes\n\nseveral assertions about the resource, including whether it has been created (exists), whether it’s running rather than having terminated (is_running), and whether the infrastructure platform considers it healthy (passes_healthcheck).\n\nSimple assertions often have low value (see “Challenge: Tests for\n\ndeclarative code often have low value”), since they simply restate the infrastructure code they are testing. A few basic assertions (such as exists) help to sanity check that the code was applied successfully. These quickly identify basic problems with pipeline stage configuration or test setup scripts. Tests such as is_running and passes_healthcheck would tell you when the stack tool successfully creates the VM, but it crashes or has some other\n\nfundamental issue. Simple assertions like these save you time in\n\ntroubleshooting.\n\nAlthough you can create assertions that reflect each of the VM’s configuration items in the stack code, like the amount of RAM or\n\nthe network address assigned to it, these have little value and add\n\noverhead.\n\nThe fourth assertion in the example, has_attached storage_volume() is more interesting. The assertion checks that the storage volume defined in the same stack is attached to the\n\nVM. Doing this validates that the combination of multiple declarations works correctly (as discussed in “Testing\n\ncombinations of declarative code”). Depending on your platform\n\nand tooling, the stack code might apply successfully but leave the\n\nserver and volume correctly tied together. Or you might make an\n\nerror in your stack code that breaks the attachment.\n\nAnother case where assertions can be useful is when the stack code is dynamic. When passing different parameters to a stack can\n\ncreate different results, you may want to make assertions about\n\nthose results. As an example, this code creates the infrastructure for an application server that is either public facing or internally\n\nfacing:\n\nvirtual_machine: name: appserver-${customer}-${environment} address_block: if(${network_access} == \"public\") ADDRESS_BLOCK.public-${customer}-${environment} else ADDRESS_BLOCK.internal-${customer}-${environment} end\n\nYou could have a testing stage that creates each type of instance and asserts that the networking configuration is correct in each\n\ncase. You should move more complex variations into modules or libraries (see Chapter 6) and test those modules separately from\n\nthe stack code. Doing this simplifies testing the stack code.\n\nAsserting that infrastructure resources are created as expected is\n\nuseful up to a point. But the most valuable testing is proving that they do what they should.\n\nOutcomes: Proving infrastructure works correctly\n\nFunctional testing is an essential part of testing application software. The analogy with infrastructure is proving that you can\n\nuse the infrastructure as intended. Examples of outcomes you\n\ncould test with infrastructure stack code include:\n\nCan you make a network connection from the web server networking segment to an application hosting network segment on the relevant port?\n\nCan you deploy and run an application on an instance of your container cluster stack?\n\nCan you safely reattach a storage volume when you rebuild a server instance?\n\nDoes your load balancer correctly handle server instances as they are added and removed?\n\nTesting outcomes is more complicated than verifying that things\n\nexist. Not only do your tests need to create or update the stack\n\ninstance, as I discuss next (“Lifecycle patterns for test instances of stacks”), you may also need to provision test fixtures. A test\n\nfixture is an infrastructure resource that is use only to support a\n\ntest (I talk about test fixtures in “Using test fixtures to handle dependencies”).\n\nThis test makes a connection to the server to check that the port is\n\nreachable, and returns the expected HTTP response:\n\ngiven stack_instance(stack: \"foodspin_networking\", instance: \"online_test\") {\n\ncan_connect(ip_address: stack_instance.appserver_ip_address, port:443)\n\nhttp_request(ip_address: stack_instance.appserver_ip_address, port:443, url: '/').response.code is('200')\n\n}\n\nThe testing framework and libraries implement the details of validations like can_connect and http_request. You’ll need to read the documentation for your test tool to see how to write actual tests.\n\nUsing test fixtures to handle dependencies\n\nMany stack projects depend on resources created outside the stack,\n\nsuch as shared networking defined in a different stack project. A\n\ntest fixture is an infrastructure resource that you create specifically to help you provision and test a stack instance by itself, without\n\nneeding to have instances of other stacks.\n\nUsing test fixtures makes it much easier to manage tests, keep your stacks loosely coupled, and have fast feedback loops. Without test\n\nfixtures, you may need to create and maintain complicated sets of\n\ntest infrastructure.\n\nA test fixture is not a part of the stack that you are testing. It is additional infrastructure that you create to support your tests. You\n\nuse test fixtures to represent a stack’s dependencies.\n\nA given dependency is either upstream, meaning the stack you’re\n\ntesting uses resources provided by another stack, or it is downstream, in which case other stacks use resources from the\n\nstack you’re testing. People sometimes call a stack with\n\ndownstream dependencies the provider, since it provides resources. A stack with upstream dependencies is then called the\n\nconsumer.\n\nFigure 10-3. Example of a provider stack and consumer stack\n\nOur Foodspin example has a provider stack which defines shared\n\nnetworking structures. These structures are used by consumer stacks, including the stack that defines customer application\n\ninfrastructure. The application stack creates a server that it assigns\n\n8 to a network address block created by the networking stack .\n\nA given stack may be both a provider and a consumer, consuming resources from another stack and providing resources to other\n\nstacks.\n\nNEVER CREATE CIRCULAR DEPENDENCIES\n\nAlthough a stack may have both upstream and downstream dependencies, you should never have circular dependencies. A circular dependency exists when a stack has both upstream and downstream dependencies to another stack, or when there is a loop of dependencies between a group of stacks. It should always be possible to follow dependencies from one stack to another in a group, without returning to a previous stack.\n\nFIGURE 10-4. NEVER CREATE STACKS WITH CIRCULAR DEPENDENCIES\n\nIf you find you do have a circular dependency in your system, you should redesign your stacks to remove it.\n\nYou can use test fixtures to stand in for either upstream or\n\ndownstream integration points of a stack.\n\nTest doubles for upstream dependencies\n\nWhen you need to test a stack that depends on another stack, you\n\ncan create a test double (see “Test Doubles”). For stacks, this typically means creating some additional infrastructure. In our\n\nexample of the shared network stack and the application stack, the application stack needs to create its server in a network address\n\nblock that is defined by the network stack. Your test setup may be\n\nable to create an address block as a test fixture to test the application stack on its own.\n\nIt may be better to create the address block as a test fixture rather\n\nthan creating an instance of the entire network stack. The network\n\nstack may include extra infrastructure that isn’t necessary for testing. For instance, it may define network policies, routes,\n\nauditing, and other resources for production that are overkill for a\n\ntest.\n\nAlso, creating the dependency as a test fixture within the consumer stack project decouples it from the provider stack. If someone is\n\nworking on a change to the networking stack project, it doesn’t\n\nimpact work on the application stack.\n\nA potential benefit of this type of decoupling is to make stacks\n\nmore reusable and composable. The Foodspin team might want to create different network stack projects for different purposes. One\n\nstack creates tightly controlled and audited networking for services\n\nthat have stricter compliance needs, such as payment processing subject to the [PCI standard]https://www.pcisecuritystandards.org/.\n\nAnother stack creates networking that doesn’t need to be PCI\n\ncompliant. By testing application stacks without using either of these stacks, the team makes it easier to use the stack code with\n\neither one.\n\nTest fixtures for downstream dependencies\n\nYou can also use test fixtures for the reverse situation, to test a\n\nstack that provides resources for other stacks to use. In Figure 10- 5, the stack instance defines networking structures for Foodspin,\n\nincluding segments and routing for the web server container\n\ncluster and application servers. The network stack doesn’t provision the container cluster or application servers, so to test the\n\nnetworking, the setup provisions a test fixture in each of these segments.\n\nFigure 10-5. Test instance for the Foodspin network stack, with test fixtures\n\nThe test fixtures in these examples are a pair of container\n\ninstances, one assigned to each of the network segments in the stack. You can often use the same testing tools that you use for\n\nverification testing (“Verification: Making assertions about infrastructure resources”) for outcome testing. These example tests\n\nuse a fictional stack testing DSL:\n\ngiven stack_instance(stack: \"foodspin_networking\", instance: \"online_test\") {\n\ncan_connect(from: $HERE, to: get_fixture(\"web_segment_instance\").address, port:443)\n\ncan_connect(from: get_fixture(\"web_segment_instance\"), to: get_fixture(\"app_segment_instance\").address, port: 8443)\n\n}\n\nThe method can_connect executes from $HERE, which would be the agent where the test code is executing, or from a container\n\ninstance. It attempts to make an HTTPS connection on the specified port to an IP address. The get_fixture() method fetches the details of a container instance created as a text fixture.\n\nThe test framework might provide the method can_connect, or it could be a custom method that the team writes.\n\nYou can see the connections that the example test code makes in Figure 10-6.\n\nFigure 10-6. Testing connectivity in the Foodspin network stack\n\nThe diagram shows the paths for both tests. The first test connects\n\nfrom outside the stack to the test fixture in the web segment. The\n\nsecond test connects from the fixture in the web segment to the\n\nfixture in the application segment.\n\nREFACTOR COMPONENTS SO THEY CAN BE ISOLATED\n\nSometimes a particular component can’t be easily isolated. Dependencies on other components may be hardcoded or simply too messy to pull apart. One of the benefits of writing tests while designing and building systems, rather than afterward, is that it forces you to improve your designs. A component that is difficult to test in isolation is a symptom of design issues. A well- designed system should have loosely coupled components.\n\nSo when you run across components that are difficult to isolate, you should fix the design. You may need to completely rewrite components, or replace libraries, tools, or applications. As the saying goes, this is a feature, not a bug. Clean design and loosely coupled code is a byproduct of making a system testable.\n\nThere are several strategies for restructuring systems. Refactoringfootnote:[Martin Fowler has written about refactoring as well as other patterns and techniques for improving system architecture. The Strangler Application prioritizes keeping the system fully working throughout the process of restructuring a system.\n\nLifecycle patterns for test instances of stacks\n\nBefore virtualization and cloud, everyone maintained static, long-\n\nlived test environments. Although many teams still have these environments, there are advantages to creating and destroying\n\nenvironments on demand. The following patterns describe the\n\ntradeoffs of keeping a persistent stack instance, creating an\n\nephemeral instance for each test run, and ways of combining both\n\napproaches. You can also apply these patterns to application and\n\nfull system test environments as well as to testing infrastructure stack code.\n\nPattern: Persistent test stack\n\nA testing stage can use a Persistent Test Stack instance that is\n\nalways running. The stage applies each code change as an update to the existing stack instance, runs the tests, and leaves the\n\nresulting modified stack in place for the next run.\n\nFigure 10-7. Persistent test stack instance\n\nALSO KNOWN AS\n\nStatic environment.\n\nMOTIVATION\n\nIt’s usually much faster to apply changes to an existing stack\n\ninstance than to create a new instance. So the persistent test stack\n\ncan give faster feedback, not only for the stage itself but for the\n\nfull pipeline.\n\nAPPLICABILITY\n\nA persistent test stack is useful when you can reliably apply your\n\nstack code to the instance. If you find yourself spending time fixing broken instances to get the pipeline running again, you\n\nshould consider one of the other patterns in this chapter.\n\nCONSEQUENCES\n\nMost stack tools, at least as of this writing, have a pretty high rate\n\nof failure. A stack instance frequently gets “wedged,” when a\n\nchange fails and leaves it in a state where any new attempt to\n\napply stack code also fails. Often, an instance gets wedged so\n\nseverely that the stack tool can’t even destroy the stack so you can start over. So your team spends too much time manually un-\n\nwedging broken test instances.\n\nYou can often reduce the frequency of wedged stacks through\n\nbetter stack design. Breaking stacks down into smaller and simpler stacks, and simplifying dependencies between stacks can lower\n\nyour wedge rate. See [Link to Come] for more on how to do this.\n\nIMPLEMENTATION\n\nIt’s easy to implement a persistent test stack. Your pipeline stage\n\nruns the stack tool command to update the instance with the\n\nrelevant version of the stack code, runs the tests, and then leaves\n\nthe stack instance in place when finished.\n\nYou may rebuild the stack completely as an ad-hoc process, such\n\nas someone running the tool from their local computer, or using an\n\nextra stage or job outside the routine pipeline flow.\n\nRELATED PATTERNS\n\nThe periodic stack rebuild pattern “Pattern: Periodic stack rebuild”\n\nis a simple tweak to this pattern, tearing the instance down at the\n\nend of the working day and building a new one every morning.\n\nPattern: Ephemeral test stack\n\nWith the Ephemeral Test Stack pattern, the test stage creates and\n\ndestroys a new instance of the stack every time it runs.\n\nFigure 10-8. Ephemeral test stack instance\n\nALSO KNOWN AS\n\nTemporary stack, Ad-hoc stack.\n\nMOTIVATION\n\nAn ephemeral test stack provides a clean environment for each run\n\nof the tests. There is no risk from data, fixtures, or other “cruft”\n\nleft over from a previous run.\n\nAPPLICABILITY\n\nYou may want to use ephemeral instances for stacks that are quick\n\nto provision from scratch. “Quick” is relative to the feedback loop\n\nyou and your teams need. For more frequent changes, like\n\ncommits to application code during rapid development phases, the time to build a new environment is probably longer than people\n\ncan tolerate. But less frequent changes, such as OS patch updates,\n\nmay be acceptable to test with a complete rebuild.\n\nCONSEQUENCES\n\nStacks generally take a long time to provision from scratch. So\n\nstages using ephemeral stack instances make feedback loops and\n\ndelivery cycles slower.\n\nIMPLEMENTATION\n\nTo implement an ephemeral test instance, your test stage should\n\nrun the stack tool command to destroy the stack instance when\n\ntesting and reporting have completed. You may want to configure the stage to stop before destroying the instance if the tests fail so\n\nthat people can debug the failure.\n\nRELATED PATTERNS\n\nThe continuous stack reset pattern (“Pattern: Continuous stack\n\nreset”) is similar, but runs the stack creation and destruction\n\ncommands out of band from the stage, so the time taken doesn’t\n\naffect feedback loops.\n\nAntipattern: Dual Persistent and Ephemeral Stack Stages\n\nWith Persistent and Ephemeral Stack Stages, the pipeline sends\n\neach change to a stack to two different stages, one that uses an\n\nephemeral stack instance, and one that uses a persistent stack\n\ninstance. This combined the persistent test stack pattern (“Pattern: Persistent test stack”) and the ephemeral test stack pattern\n\n(“Pattern: Ephemeral test stack”).\n\nALSO KNOWN AS\n\nQuick and dirty plus slow and clean.\n\nMOTIVATION\n\nTeams usually implement this to work around the disadvantages of each of the two patterns it combines. If all works well, the “quick\n\nand dirty” stage (the one using the persistent instance) provides\n\nfast feedback. If that stage fails because the environment becomes\n\nwedged, you will get feedback eventually from the “slow and\n\nclean” stage (the one using the ephemeral instance).\n\nAPPLICABILITY\n\nIt might be worth implementing both types of stages as an interim\n\nsolution while moving to a more reliable solution.\n\nCONSEQUENCES\n\nIn practice, using both types of stack lifecycle combines the\n\ndisadvantages of both. If updating an existing stack is unreliable,\n\nthen your team will still spend time manually fixing that stage when it goes wrong. And you probably wait until the slower stage\n\npasses before being confident that a change is good.\n\nThis antipattern is also expensive, since it uses double the\n\ninfrastructure resources, at least during the test run.\n\nIMPLEMENTATION\n\nYou implement dual stages by creating two pipeline stages, both\n\ntriggered by the previous stage in the pipeline for the stack project. You may require both stages to pass before promoting the stack\n\nversion to the following stage, or you may promote it when either\n\nof the stages passes.\n\nFigure 10-9. Dual Persistent and Ephemeral Stack Stages\n\nRELATED PATTERNS\n\nThis antipattern combines the persistent test stack pattern\n\n(“Pattern: Persistent test stack”) and the ephemeral test stack\n\npattern (“Pattern: Ephemeral test stack”).\n\nPattern: Periodic stack rebuild\n\nPeriodic Stack Rebuild uses a persistent test stack instance\n\n(“Pattern: Persistent test stack”) for the stack test stage, and then has a process that runs -out-of-band to destroy and rebuild the\n\nstack instance on a schedule, such as nightly.\n\nALSO KNOWN AS\n\nNightly rebuild.\n\nMOTIVATION\n\nPeople often use periodic rebuilds to reduce costs. They destroy the stack at the end of the working day and provision a new one at\n\nthe start of the next day.\n\nPeriodic rebuilds might help with unreliable stack updates,\n\ndepending on why the updates are unreliable. In some cases, the resource usage of instances builds up over time, such as memory\n\nor storage that accumulates across test runs. Regular resets can\n\nclear these out.\n\nAPPLICABILITY\n\nRebuilding a stack instance to work around resource usage usually\n\nmasks underlying problems or design issues. In this case, this\n\npattern is, at best, a temporary hack, and at worst, a way to allow\n\nproblems to build up until they cause a disaster.\n\nDestroying a stack instance when it isn’t in use to save costs is\n\nsensible, especially when using metered resources such as with\n\npublic cloud platforms.\n\nCONSEQUENCES\n\nIf you use this pattern to free up idle resources, you need to\n\nconsider how you can be sure they aren’t needed. For example,\n\npeople working outside of office hours, or in other timezones, may be blocked without test environments.\n\nIMPLEMENTATION\n\nMost pipeline orchestration tools make it easy to create jobs that run on a schedule to destroy and build stack instances. A more\n\nsophisticated solution would run based on activity levels. For\n\nexample, you could have a job that destroys an instance if the test\n\nstage hasn’t run in the past hour.\n\nThere are three options for triggering the build of a fresh instance\n\nafter destroying the previous instance. One is to rebuild it right\n\naway after destroying it. This approach clears resources but\n\ndoesn’t save costs.\n\nA second option is to build the new environment instance at a\n\nscheduled point in time. But it may stop people from working\n\nflexible hours.\n\nThe third option is for the test stage to provision a new instance if\n\nit doesn’t currently exist. Create a separate job that destroys the\n\ninstance, either on a schedule or after a period of inactivity. Each\n\ntime the testing stage runs, it first checks whether the instance is\n\nalready running. If not, it provisions a new instance first. With this\n\napproach, people occasionally need to wait longer than usual to get\n\ntest results. If they are the first person to push a change in the\n\nmorning, they need to wait for the system to provision the stack.\n\nRELATED PATTERNS\n\nThis pattern can play out like the persistent test stack pattern\n\n(“Pattern: Persistent test stack”)-if your stack updates are\n\nunreliable, people spend time fixing broken instances.\n\nPattern: Continuous stack reset\n\nWith the Continuous Stack Reset pattern, every time the stack\n\ntesting stage completes, an out-of-band job destroys and rebuilds\n\nthe stack instance.\n\nFigure 10-10. Pipeline flow for continuous stack reset\n\nALSO KNOWN AS\n\nBehind the scenes rebuild\n\nMOTIVATION\n\nDestroying and rebuilding the stack instance every time provides a\n\nclean slate to each testing run. It may automatically remove a\n\nbroken instance unless it is too broken for the stack tool to destroy.\n\nAnd it removes the time it takes to create and destroy the stack instance from the feedback loop.\n\nAnother benefit of this pattern is that it can reliably test the update\n\nprocess that would happen for the given stack code version in\n\nproduction.\n\nAPPLICABILITY\n\nDestroying the stack instance in the background can work well if\n\nthe stack project doesn’t tend to break and need manual\n\nintervention to fix.\n\nCONSEQUENCES\n\nSince the stack is destroyed and provisioned outside the delivery\n\nflow of the pipeline, problems may not be visible. The pipeline can be green, but the test instance may break behind the scenes. When\n\nthe next change reaches the test stage, it may take time to realize it\n\nfailed because of the background job rather than because of the\n\nchange itself.\n\nIMPLEMENTATION\n\nWhen the test stage passes, it promotes the stack project code to\n\nthe next stage. It also triggers a job to destroy and rebuild the stack\n\ninstance. When someone pushes a new change to the code, the test\n\nstage applies it to the instance as an update.\n\nYou need to decide which version of the stack code to use when\n\nrebuilding the instance. You could use the same version that has\n\njust passed the stage. An alternative is to pull the last version of\n\nthe stack code applied to the production instance. This way, each version of stack code is tested as an update to the current\n\nproduction version. Depending on how your infrastructure code\n\ntypically flows to production, this may be a more accurate representation of the production upgrade process.\n\nRELATED PATTERNS\n\nIdeally, this pattern resembles the persistent test stack pattern (“Pattern: Persistent test stack”), providing feedback, while having\n\nthe reliability of the ephemeral test stack pattern (“Pattern:\n\nEphemeral test stack”).\n\nTest orchestration\n\nI’ve described each of the moving parts involved in testing stacks: the types of tests and validations you can apply, using test fixtures\n\nto handle dependencies, and lifecycles for test stack instances. But\n\nhow should you put these together to set up and run tests?\n\nMost teams use scripts to orchestrate their tests. Often, these are the same scripts they use to orchestrate running their stack tools. In\n\n[Link to Come], I’ll dig into these scripts, which may handle\n\nconfiguration, coordinating actions across multiple stacks, and\n\nother activities as well as testing.\n\nTest orchestration may involve:\n\nCreating test fixtures,\n\nLoading test data (more often needed for application testing than infrastructure testing),\n\nManaging the lifecycle of test stack instances,\n\nProviding parameters to the test tool,\n\nRunning the test tool,\n\nConsolidating test results,\n\nCleaning up test instances, fixtures, and data.\n\nMost of these topics, such as test fixtures and stack instance\n\nlifecycle, are covered earlier in this chapter. Others, including\n\nrunning the tests and consolidating the results, depend on the\n\nparticular tool.\n\nTwo guidelines to consider for orchestrating tests are supporting\n\nlocal testing and avoiding tight coupling to pipeline tools.\n\nSupport local testing\n\nPeople working on infrastructure stack code should be able to run\n\nthe tests themselves before pushing code into the shared pipeline and environments. [Link to Come] discusses approaches to help\n\npeople work with personal stack instances on an infrastructure\n\nplatform. Doing this allows you to code and run online tests before\n\npushing changes.\n\nAs well as being able to work with personal test instances of\n\nstacks, people need to have the testing tools and other elements\n\ninvolved in running tests on their local working environment.\n\nMany teams use code-driven development environments, which\n\nautomate installing and configuring tools. You can use containers\n\n9\n\nor virtual machines for packaging development environments that can run on different types of desktop systems. Alternately, your\n\nteam could use hosted workstations (hopefully configured as\n\ncode), although these may suffer from latency, especially for\n\ndistributed teams.\n\nA key to making it easy for people to run tests themselves is using the same test orchestration scripts across local work and pipeline\n\nstages. Doing this ensures that tests are set up and run consistently\n\neverywhere.\n\nAvoid tight coupling with pipeline tools\n\nMany CI and pipeline orchestration tools have features or plugins for test orchestration, even configuring and running the tests for\n\nyou. While these features may seem convenient, they make it\n\ndifficult to set up and run your tests consistently outside the\n\npipeline. Mixing test and pipeline configuration can also make it\n\npainful to make changes.\n\nInstead, you should implement your test orchestration in a separate\n\nscript or tool. The test stage should call this tool, passing a\n\nminimum of configuration parameters. This approach keeps the\n\nconcerns of pipeline orchestration and test orchestration loosely\n\ncoupled.\n\nTest orchestration tools\n\nMany teams write custom scripts to orchestrate tests. These scripts\n\nare similar to or may even be the same scripts used to orchestrate\n\nstack management (as described in [Link to Come]). People use\n\nBash scripts, batch files, Ruby, Python, Make, Rake, and others I’ve never heard of.\n\nThere are a few tools available that are specifically designed to\n\norchestrate infrastructure tests. Two I know of are [Test\n\nKitchen]https://kitchen.ci/ and [Molecule]https://molecule.readthedocs.io/en/stable/. Test Kitchen",
      "page_number": 269
    },
    {
      "number": 10,
      "title": "Testing Infrastructure Stacks",
      "start_page": 309,
      "end_page": 356,
      "detection_method": "regex_chapter_title",
      "content": "is an open-source product from Chef that was originally aimed at\n\ntesting Chef cookbooks. Molecule is an open-source tool designed\n\nfor testing Ansible playbooks. You can use either tool to test\n\ninfrastructure stacks, for example, using [Kitchen- Terraform]https://newcontext-oss.github.io/kitchen-terraform/.\n\nThe challenge with these tools is that they are designed with a\n\nparticular workflow in mind, and can be difficult to configure to\n\nsuit the workflow you need. Some people tweak and massage them, while others find it simpler to write their own scripts.\n\nConclusion\n\nThis chapter pulls together the topics of infrastructure stacks and\n\ntesting. These approaches and patterns may guide you in implementing automated testing for your infrastructure code,\n\nhopefully helping you to cope with some of the challenges which\n\nmake this harder than testing application code.\n\nOne of the challenges this book can’t help with is tooling for testing infrastructure. Although there are some tools available,\n\nmany of which I mention in this chapter, TDD, CI, and automated\n\ntesting are not very well established for infrastructure as of this\n\nwriting. You have a journey to discover the tools that you can use,\n\nand may need to cover gaps in the tooling with custom scripting. Hopefully, this will improve over time.\n\nThe next section of this book moves on to the next layer of\n\ninfrastructure, application runtimes. You use stacks to provision\n\nservers, clusters, and other resources for deploying and running\n\napplications. These chapters explain how to integrate testing and pipelines for these things with testing and pipelines for stacks.\n\n1 I give the background on Foodspin in “Foodspin Example: Diverging\n\ninfrastructure”\n\n2 See “Structured data storage” for a bit more on DBaaS\n\n3 This pipeline is much simpler than what you’d use in reality. You would probably have at least one stage to test the stack and the application together (see [Link to Come]). You might also need a customer acceptance testing stage before each customer production stage. And this doesn’t include governance and approval stages, which many organizations require.\n\n4 The term https://en.wikipedia.org/wiki/Lint(software)[lint\n\n5 Awspec is an rspec-based framework for testing AWS infrastructure\n\n6 Inspec is another rspec-based framework, which tests multiple cloud platforms, as\n\nwell as other types of infrastructure, such as servers and containers.\n\n7 Terratest is a Go-based framework specific to Terraform but applicable to other\n\ntypes of infrastructure, including servers and containers.\n\n8 [Link to Come] explains how to connect stack dependencies, in [Link to Come].\n\n9 Vagrant is handy for sharing virtual machine configuration between members of a\n\nteam.\n\nAbout the Author\n\nKief Morris is a consultant with ThoughtWorks, specializing in cloud, infrastructure, and agile IT operations. This gives him an\n\nopportunity to spend time with teams in a variety of industries, at companies ranging from global enterprises to early stage startups.\n\nHe enjoys working and talking with people to explore better engineering practices, architecture design principles, and organizational structures and processes. Kief ran his first online system, a bulletin board system (BBS) in Florida in the early 1990s. He later enrolled in a MSc program in computer science at\n\nthe University of Tennessee because it seemed like the easiest way to get a real Internet connection. Joining the CS department’s system administration team gave him exposure to managing\n\nhundreds of machines running a variety of Unix flavors. When the dot-com bubble began to inflate, Kief moved to London, drawn by the multicultural mixture of industries and people. He’s still there. Most of the companies Kief worked for before ThoughtWorks\n\nwere post-startups, looking to build and scale. The titles he’s been given or self-applied include Deputy Technical Director, R&D Manager, Hosting Manager, Technical Lead, Technical Architect,\n\nConsultant, and Infrastructure Engineering Practices Champion.\n\nColophon\n\nThe animal on the cover of Infrastructure as Code is Rüppell’s vulture (Gyps rueppellii), native to the Sahel region of Africa (a\n\ngeographic zone that serves as a transition between the Sahara Desert and the savanna). It is named in honor of a 19th-century\n\nGerman explorer and zoologist, Eduard Rüppell.\n\nIt is a large bird (with a wingspan of 7–8 feet and weighing 14–20 pounds) with mottled brown feathers and a yellowish-white neck and head. Like all vultures, this species is carnivorous and feeds\n\nalmost exclusively on carrion. They use their sharp talons and\n\nbeaks to rip meat from carcasses, and have backward-facing spines on their tongue to thoroughly scrape bones clean. While normally silent, these are very social birds who will voice a loud squealing\n\ncall at colony nesting sites or when fighting over food.\n\nThe Rüppell’s vulture is monogamous and mates for life, which can be 40–50 years long. Breeding pairs build their nests near cliffs, out of sticks lined with grass and leaves (and often use it for\n\nmultiple years). Only one egg is laid each year—by the time the next breeding season begins, the chick is just becoming independent. This vulture does not fly very fast (about 22 mph),\n\nbut will venture up to 90 miles from the nest in search of food.\n\nRüppell’s vultures are the highest-flying birds on record; there is evidence of them flying 37,000 feet above sea level, as high as\n\ncommercial aircraft. They have a special hemoglobin in their blood that allows them to absorb oxygen more efficiently at high altitudes.\n\nThis species is considered endangered and populations have been\n\nin decline. Though loss of habitat is one factor, the most serious threat is poisoning. The vulture is not even the intended target: farmers often poison livestock carcasses to retaliate against predators like lions and hyenas. As vultures identify a meal by\n\nsight and gather around it in flocks, hundreds of birds can be killed each time.\n\nMany of the animals on O’Reilly covers are endangered; all of\n\nthem are important to the world. To learn more about how you can help, go to animals.oreilly.com.\n\nThe cover image is from Cassell’s Natural History. The cover fonts are URW Typewriter and Guardian Sans. The text font is\n\nAdobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "page_number": 309
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "1. Preface\n\na. How I Learned to Stop Worrying and to Love the\n\nCloud\n\ni. The Sorcerer’s Apprentice\n\nii. Cloud from Scratch\n\niii. Legacy cloud infrastructure\n\nb. Why I Wrote This Book\n\nc. Why A Second Edition\n\nd. Who This Book Is For\n\ne. What Tools Are Covered\n\nf. Principles, Practices, and Patterns\n\ng. The FoodSpin examples\n\nh. Conventions Used in This Book\n\ni. O’Reilly Online Learning\n\nj. How to Contact Us\n\n2. I. Foundations\n\n3. 1. What is Infrastructure as Code?\n\na. From the Iron Age to the Cloud Age\n\nb. Infrastructure as Code\n\ni. Benefits of Infrastructure as Code\n\nc. Use Infrastructure as Code to optimize for\n\nchange\n\ni. Objection: “We don’t make changes often enough to justify automating",
      "content_length": 701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "them”\n\nii. Objection: “We should build first and\n\nautomate later”\n\niii. Objection: “We must choose between\n\nspeed and quality”\n\nd. Three core practices for Infrastructure as Code\n\ni. Core practice: Define everything as code\n\nii. Core practice: Continuously validate all\n\nyour work in progress\n\niii. Core practice: Build small, simple\n\npieces that you can change independently\n\ne. The parts of an infrastructure system\n\nf. Conclusion\n\n4. 2. Principles of Cloud Age Infrastructure\n\na. Principle: Assume systems are unreliable\n\nb. Principle: Make everything reproducible\n\nc. Pitfall: Snowflake systems\n\nd. Principle: Create disposable things\n\ne. Principle: Minimize variation\n\ni. Configuration Drift\n\nf. Principle: Ensure that you can repeat any process\n\ng. Conclusion\n\n5. 3. Infrastructure Platforms\n\na. What is a dynamic infrastructure platform?\n\nb. Infrastructure Resources",
      "content_length": 873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "c. Compute Resources\n\ni. Virtual machines\n\nii. Physical servers\n\niii. Containers\n\niv. Server clusters\n\nv. Serverless code execution (FaaS)\n\nd. Storage Resources\n\ni. Block storage (virtual disk volumes)\n\nii. Object storage\n\niii. Networked filesystems (shared network\n\nvolumes)\n\niv. Structured data storage\n\nv. Secrets management\n\ne. Network Resources\n\ni. Network address blocks\n\nii. Traffic management and routing\n\niii. Network access rules\n\niv. Caches\n\nv. Service meshes\n\nf. Conclusion\n\n6. 4. Core Practice: Define everything as code\n\na. Why you should define your infrastructure as\n\ncode\n\nb. What you can define as code",
      "content_length": 620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "i. Choose tools that are configured with\n\ncode\n\nii. Manage your code in a version control\n\nsystem\n\niii. Secrets and source code\n\nc. Infrastructure coding languages\n\ni. Scripting your infrastructure\n\nii. Building infrastructure with declarative\n\ncode\n\niii. DSLs for infrastructure\n\niv. The return of general-purpose languages\n\nfor infrastructure\n\nd. Implementation Principles for defining\n\ninfrastructure as code\n\ni. Implementation Principle: Avoid mixing\n\ndifferent types of code\n\nii. Implementation Principle: Separate\n\ninfrastructure code concerns\n\niii. Implementation Principle: Treat infrastructure code like real code\n\ne. Conclusion\n\n7. II. Working With Infrastructure Stacks\n\n8. 5. Building Infrastructure Stacks as Code\n\na. What is an infrastructure stack?\n\ni. Stack code\n\nii. Stack instance\n\niii. Configuring servers in a stack",
      "content_length": 835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "b. Patterns and antipatterns for structuring stacks\n\ni. Antipattern: Monolithic Stack\n\nii. Pattern: Application Group Stack\n\niii. Pattern: Service Stack\n\niv. Pattern: Micro Stack\n\nc. Conclusion\n\n9. 6. Using Modules to Share Stack Code\n\na. Examples of using modules\n\nb. Patterns and antipatterns for infrastructure\n\nmodules\n\ni. Pattern: Facade Module\n\nii. Antipattern: Anemic Module\n\niii. Pattern: Domain Entity Module\n\niv. Antipattern: Spaghetti Module\n\nv. Antipattern: Obfuscation Layer\n\nvi. Antipattern: One-shot Module\n\nc. Conclusion\n\n10. 7. Building Environments With Stacks\n\na. What environments are all about\n\ni. Release delivery environments\n\nii. Multiple production environments\n\niii. Environments, consistency, and\n\nconfiguration\n\nb. Patterns for building environments",
      "content_length": 777,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "i. Antipattern: Multiple-Environment\n\nStack\n\nii. Antipattern: Copy-Paste Environments\n\niii. Pattern: Reusable Stack\n\nc. Building environments with multiple stacks\n\nd. Conclusion\n\n11. 8. Configuring Stacks\n\na. Using stack parameters to create unique\n\nidentifiers\n\nb. Example stack parameters\n\ni. Handling secrets as parameters\n\nc. Patterns for configuring stacks\n\ni. Antipattern: Manual Stack Parameters\n\nii. Pattern: Stack Environment Variables\n\niii. Pattern: Scripted Parameters\n\niv. Pattern: Stack Configuration Files\n\nv. Pattern: Wrapper Stack\n\nvi. Pattern: Pipeline Stack Parameters\n\nvii. Pattern: Stack Parameter Registry\n\nd. Configuration Registry\n\ni. Implementing a Configuration Registry\n\nii. Single or multiple configuration\n\nregistries\n\niii. Configuration Management Database\n\n(CMDB)",
      "content_length": 793,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "e. Conclusion\n\n12. 9. Core Practice: Continuously validate all work in progress\n\na. Why continuously validate infrastructure code?\n\ni. What continuous validation means\n\nii. What should we validate with\n\ninfrastructure?\n\nb. Challenges with testing infrastructure code\n\ni. Challenge: Tests for declarative code\n\noften have low value\n\nii. Challenge: Testing infrastructure code is\n\nslow\n\nc. Progressive validation\n\ni. Validation stages\n\nii. Testing in production\n\nd. Progressive validation models\n\ni. Test pyramid\n\nii. Swiss cheese testing model\n\ne. Pipelines for validation\n\ni. Pipeline stages\n\nii. Delivery pipeline software and services\n\nf. Conclusion\n\n13. 10. Testing Infrastructure Stacks\n\na. Example infrastructure\n\ni. The example stack",
      "content_length": 739,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "ii. Pipeline for the example stack\n\nb. Offline validation stages for stacks\n\ni. Syntax checking\n\nii. Offline static code analysis\n\niii. Static code analysis with API\n\niv. Testing with mock API\n\nc. Online validation stages for stacks\n\ni. Preview: Seeing what changes will be\n\nmade\n\nii. Verification: Making assertions about\n\ninfrastructure resources\n\niii. Outcomes: Proving infrastructure works\n\ncorrectly\n\nd. Using test fixtures to handle dependencies\n\ni. Test doubles for upstream dependencies\n\nii. Test fixtures for downstream\n\ndependencies\n\ne. Lifecycle patterns for test instances of stacks\n\ni. Pattern: Persistent test stack\n\nii. Pattern: Ephemeral test stack\n\niii. Antipattern: Dual Persistent and\n\nEphemeral Stack Stages\n\niv. Pattern: Periodic stack rebuild\n\nv. Pattern: Continuous stack reset\n\nf. Test orchestration\n\ni. Support local testing",
      "content_length": 849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "ii. Avoid tight coupling with pipeline tools\n\niii. Test orchestration tools\n\ng. Conclusion",
      "content_length": 90,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Infrastructure as Code\n\nSECOND EDITION\n\nEvolving systems in the Cloud\n\nWith Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.\n\nKief Morris",
      "content_length": 297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Infrastructure as Code\n\nby Kief Morris\n\nCopyright © 2020 Kief Morris. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most\n\ntitles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\n\nEditors: John Devins and Virginia Wilson\n\nProduction Editor: Christopher Faucher\n\nCover Designer: Karen Montgomery\n\nJune 2016: First Edition\n\nJuly 2020: Second Edition\n\nRevision History for the Early Release\n\n2019-12-03: First Release\n\n2020-01-22: Second Release\n\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098114671 for release details.",
      "content_length": 844,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "The O’Reilly logo is a registered trademark of O’Reilly Media,\n\nInc. Infrastructure as Code, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nWhile the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work\n\nare accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on\n\nthis work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses\n\nor the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-098-11460-2",
      "content_length": 873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Preface\n\nModern organizations are increasingly using dynamic cloud platforms, whether public or private, to exploit digital technology to deliver services and products to their users. Automation is essential to managing continuously changing and evolving\n\nsystems, and tools that define systems “as code” have become dominant for this.\n\nAlthough cloud technology and infrastructure coding tools are becoming pervasive, most teams are still learning the best ways to\n\nput them to use.\n\nThis book is my attempt to share what I’ve learned from various people, teams, and organizations. I’m not giving you specific instructions on how to implement a specific tool or language.\n\nInstead, I’ve assembled principles, practices, and patterns that you can use to shape how you approach the design, implementation, and evolution of your infrastructure.\n\nThese draw heavily on agile engineering principles and practices. I believe that given cloud-based infrastructure is essentially just another software system, we can benefit from lessons learned in\n\nother software domains.\n\nInfrastructure as code has grown along with the DevOps movement. Andrew Clay-Shafer and Patrick Debois triggered the\n\nDevOps movement with a talk at the Agile 2008 conference. The first uses I’ve found for the term “Infrastructure as Code” are from",
      "content_length": 1316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "a talk on Agile Infrastructure that Clay-Shafer gave at the Velocity\n\nconference in 2009, and an article John Willis wrote summarizing the talk . My own Infrastructure as Code journey began a decade or so before this.\n\n1\n\nHow I Learned to Stop Worrying and to Love the Cloud\n\n2\n\nI set up my first server, a dialup BBS, in 1992. This led to Unix\n\nsystem administration and then to building and running hosted applications for various companies, from startups to enterprises. In 2001 I discovered the Infrastructures.org website, which taught me\n\n3 how to build servers in a highly consistent way using CFEngine\n\nBy 2007, my team had accumulated around 20 1U and 2U physical servers in our office’s server racks. These overflowed with test instances of our company’s software, along with a variety of miscellaneous applications and services - a couple of wikis, bug\n\ntrackers, DNS servers, mail servers, databases, and so on. Whenever someone found something else to run, we crammed it onto an existing server. When the product folks wanted a new environment, it took several weeks to order and assemble the hardware.\n\nThen we learned that virtualization was a thing. We started with a pair of beefy HP rack servers and VMWare ESX Server licenses,\n\nand before long, everything was a VM. Every application could run on a dedicated VM, and it took minutes to create a new environment.",
      "content_length": 1380,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Virtualization made it easy to create new servers, which solved one set of problems. But it led to a whole new set of problems.\n\nThe Sorcerer’s Apprentice\n\nA year later, we were running well over 100 VMs and counting.\n\nWe were well underway with virtualizing our production servers and experimenting with Amazon’s new cloud hosting service. The benefits virtualization had brought to the business people meant we had money for more ESX servers and for shiny SAN devices to feed our infrastructure’s shocking appetite for storage.\n\nWe created virtual servers, then more, then even more. They\n\noverwhelmed us. When something broke, we tracked down the VM and fixed whatever was wrong with it, but we couldn’t keep track of what changes we’d made where. We felt like Mickey Mouse in “The Sorcerer’s Apprentice” from Fantasia (an adaption of the von Goethe poem).\n\nWell, a perfect hit! See how he is split! Now there’s hope for me, and I can breathe free!\n\nWoe is me! Both pieces come to life anew, now, to do my bidding I have servants two! Help me, O great powers! Please, I’m begging you!\n\n—Excerpted from Brigitte Dubiel’s translation of “Der Zauberlehrling” (“The Sorcerer’s Apprentice”) by Johann Wolfgang von Goethe",
      "content_length": 1218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "We faced a never-ending stream of updates to the operating systems, web servers, application servers, database servers, JVMs,\n\nand other software packages we used. We struggled to keep up with them. We might apply them successfully to some VMs, but\n\non others, the upgrades broke things. We didn’t have time to stomp\n\nout every incompatibility, and over time had many combinations of versions of things strewn across hundreds of VMs.\n\nWe were using configuration automation software even before we\n\nvirtualized, which should have helped with these issues. I had used CFEngine in previous companies, and when I started this team, we\n\ntried a new tool called Puppet. Later, my colleague introduced us\n\nto Chef when he spiked out ideas for an AWS infrastructure. All of these tools were useful, but particularly in the early days, they\n\ndidn’t get us out of the quagmire of wildly different servers.\n\nIn theory, we should have configured our automation tools to run continuously, applying and reapplying the same configuration to\n\nall of our servers every hour or so. But we didn’t trust them\n\nenough to let them run unattended. We had too much variation across our servers, and it was too easy for something to break\n\nwithout us noticing. We would write a Puppet manifest to configure and manage a particular application server. But when we\n\nran it against a different server, we found that it had a different version of Java, application server software, or OS packages. The\n\nPuppet run would fail, or worse, corrupt the application server.\n\nSo we ended up using Puppet ad hoc. We could safely run it\n\nagainst new VMs, although we might need to make some tweaks to make it work. We would write a new manifest to carry out a",
      "content_length": 1722,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "specific upgrade, and then run it against our servers one at a time, carefully checking the result and making fixes as needed.\n\nConfiguration automation was a useful aid, better than shell\n\nscripts, but the way we used it didn’t save us from our sprawl of inconsistent servers.\n\nCloud from Scratch\n\nThings changed when we began moving things onto the cloud. The\n\ntechnology itself wasn’t what improved things; we could have done the same thing with our own VMware servers. But because\n\nwe were starting fresh, we adopted new ways of managing servers based on what we had learned with our virtualized farm. We also\n\nfollowed what people were doing at companies like Flickr, Etsy,\n\nand Netflix. We baked these new ideas into the way we managed services as we migrated them onto the cloud.\n\nThe key idea of our new approach was that we could rebuild every\n\nserver automatically from scratch. Our configuration tooling would run continuously, not ad hoc. Every server added into our\n\nnew infrastructure would fall under this approach. If automation\n\nbroke on some edge case, we would either change the automation to include it or else fix the design of the service so that it was no\n\nlonger an edge case.\n\nThe new regime wasn’t painless. We had to learn new habits, and we had to find ways of coping with the challenges of a highly\n\nautomated infrastructure. As the members of the team moved on to other organizations and got involved with communities such as\n\nDevOpsDays, we learned and grew. Over time, we reached the\n\npoint where we were habitually working with automated",
      "content_length": 1570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "infrastructures with hundreds of servers, with much less effort and\n\nheadache than we had been in our “Sorcerer’s Apprentice” days.\n\nJoining ThoughtWorks was an eye-opener for me. The\n\ndevelopment teams I worked with were passionate about using XP engineering practices like test-driven development (TDD),\n\ncontinuous integration (CI) and continuous delivery (CD). Because I had already learned to manage infrastructure scripts and\n\nconfiguration files in source control systems, it was natural to\n\napply these rigorous engineering practices to them.\n\nWorking with ThoughtWorks has also brought me into contact with many IT operations teams. Over the years, I’ve seen\n\norganizations experimenting with and then adopting virtualization, cloud, containers, and automation tooling. Working with them to\n\nshare and learn new ideas and techniques has been a fantastic experience.\n\nLegacy cloud infrastructure\n\nIn the past few years, I’ve seen more teams who have moved\n\nbeyond early adoption of cloud-based infrastructures using “as- code” tools. My ThoughtWorks colleagues and I have noticed\n\nsome common issues with more mature infrastructure.\n\nMany teams find that their infrastructure codebases have grown\n\ndifficult to manage. Setting up a new environment for a new customer may take a month or more. Rolling out patches to a\n\ncontainer orchestration system is a painful, disruptive process. New members of the team take too long to learn the complicated and messy set of scripts they use to run their infrastructure tools.",
      "content_length": 1523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "The interesting thing about these situations is that they look oddly familiar. For years, clients have asked us to help them with messy,\n\nmonolithic software architectures, and error-prone application deployment processes. And now we’re seeing the same issues appearing with infrastructure.\n\nTurns out, infrastructure as code isn’t just a metaphor. Code really\n\nis code!\n\nSo we’ve doubled down on the idea that software engineering practices can be useful with infrastructure codebases. TDD, CI, 4 CD, microservices , and Evolutionary\n\nArchitectures[http://shop.oreilly.com/product/0636920080237.do] are coming into their own for cloud infrastructure.\n\nWhy I Wrote This Book\n\nI’ve met and worked with many teams who are in the same place I was a few years ago: people who were using cloud, virtualization, and automation tools but hadn’t got it all running as smoothly as they know they could. I hope that this book provides a practical\n\nvision and specific techniques for managing complex IT infrastructure in the cloud.\n\nWhy A Second Edition\n\nI started working on the first edition of this book in 2014. The landscape of cloud and infrastructure was shifting as I worked, with innovations like Docker popping up, forcing me to add and\n\nrevise what I was writing. I felt like I was in a race, with people in",
      "content_length": 1308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "the industry creating things for me to write about faster than I could write it.\n\nThings haven’t slowed down since we published the book in June\n\n2016. Docker went from a curiosity to the mainstream, and Kubernetes emerged as the dominant way to orchestrate Docker containers. Existing distributed orchestration and PaaS (Platform as a Service) products either fell by the wayside or rebuilt themselves around Docker and Kubernetes. Serverless, service meshes, and observability are all emerging as core technologies for modern cloud-based systems.\n\nEven as I write the second edition, a new generation of\n\ninfrastructure tools is emerging, led (so far) by Pulumi and the AWS CDK (Cloud Development Kit). These are challenging the incumbent tools, using general-purpose procedural languages\n\nrather than dedicated declarative languages.\n\nNevertheless, it feels useful to update the book, bringing it up to the current state of the industry. Some books explain how to use a specific tool or language. The concepts in this book are relevant across tools, and even across new versions and types of tools. The\n\ncore practices, principles, and most of the implementation patterns I describe are valid even if specific technologies and tools become obsolete.\n\nI have several goals with this new edition:\n\nInclude newer technologies\n\nAlthough the concepts apply regardless of the specific tools you use, it’s useful to describe them in current contexts. People",
      "content_length": 1453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "often ask me how infrastructure as code applies with serverless, containers, and service meshes. This second edition should make this more clear.\n\nImprove relevance to mature cloud systems\n\nWhen I wrote the first edition, very few organizations were making full use of cloud and infrastructure as code. Since then, even conservative organizations like banks and governments have begun adopting public cloud. And many other teams have built up large infrastructure codebases, often accumulating technical debt and even legacy systems and code. I’ve updated the content in this book based on lessons I’ve learned working with these teams.\n\nEvolve and expand\n\nOne of the benefits that I hadn’t anticipated from writing a book was the amount of exposure I gained to different people building systems on the cloud. I’ve learned so much from giving talks, running workshops, visiting organizations at various phases of adoption, and working with clients. I’ve learned about pitfalls, good practices, and challenges from many people and teams. I’ve also learned how different ways of talking about this stuff resonates with people, so I’ve been able to hone my messaging. The result is that this edition of the book has more content and is presented in what I believe is a stronger structure.",
      "content_length": 1285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Who This Book Is For\n\nThis book is for people who work with dynamic IT infrastructure. You may be a system administrator, infrastructure engineer, team\n\nlead, architect, or a manager with technical interest. You might also be a software developer who wants to build and use infrastructure.\n\nI’m assuming you have some exposure to cloud infrastructure, so you know how to provision and configure systems. You’ve\n\nprobably at least played with tools like Ansible, Chef, CloudFormation, Puppet, or Terraform.\n\nWhile this book may introduce some readers to infrastructure as code, I hope people who work this way already will also find it\n\ninteresting. I see it as a way to share ideas and start conversations about how to do it even better.\n\nWhat Tools Are Covered\n\nThis book doesn’t offer instructions in using specific scripting languages or tools. I generally use pseudo-code for examples and try to avoid making examples specific to a particular cloud\n\nplatform. This book should be helpful to you regardless of whether you use CloudFormation and Ansible on AWS, Terraform, and Puppet on Azure, Pulumi and Chef on Google Cloud, or a completely different stack.\n\nThe concepts I explain are relevant across different platforms and\n\ntoolchains. When I introduce a concept, I often give examples from specific tools to illustrate what I mean. The tools I do",
      "content_length": 1354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "mention are ones that I am most familiar with. But there are many other tools out there-just because I don’t mention your favorite one doesn’t mean I’m dismissing it, it only means I don’t know enough\n\nabout it.\n\nPrinciples, Practices, and Patterns\n\nI use the terms principles, practices, and patterns (and\n\nantipatterns) to describe essential concepts. Here are the ways I use each of these terms:\n\nPrinciple\n\nA principle is a rule that helps you to choose between potential solutions.\n\nPractice\n\nA practice is a way of implementing something. A given practice is not always the only way to do something, and may not even be the best way for a particular situation. You should use principles to guide you in choosing the most appropriate practice for a given situation.\n\nPattern\n\nA pattern is a potential solution to a problem. It’s very similar to a practice, in that different patterns may be more effective in different contexts. Each pattern is described in a format that should help you to evaluate how relevant it is for your problem.\n\nAntipattern\n\nAn antipattern is a potential solution that I am recommending you avoid in most situations. Usually, it’s either something that seems like a good idea or else it’s something that you fall into doing without realizing it.",
      "content_length": 1276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "WHY I DON’T USE THE TERM “BEST PRACTICE”\n\nFolks in our industry love to talk about “best practices”. The problem with this term is that it often leads people to think there is only one solution to a problem, no matter what the context.\n\nI prefer to describe practices and patterns, and note when they are useful and what their limitations are. I do describe some of these as more effective or more appropriate, but I try to be open to alternatives. For practices that I believe are less effective, I hope I explain why I think this.\n\nHere are how I’m using these terms to drive the structure of this book:\n\nPrinciples of Cloud Age Infrastructure\n\nThese are rules driven by the dynamic nature of cloud systems that help you decide how to approach building and running your stuff. They are a contrast to legacy approaches from the Iron Age of infrastructure, which assume resources are static and expensive to change. These principles lead us to Infrastructure as Code as an approach.\n\n5\n\nCore Practices for Infrastructure as Code\n\nThere are many practices for implementing infrastructure as code. The three I’ve highlighted as “core” are the ones that I believe are fundamental for successfully building infrastructure in the Cloud Age. In other words, the Principles of Cloud Age Infrastructure drive the use of these Core Practices.\n\nImplementation Principles\n\nGiven each of the core practices for infrastructure as code, these principles are rules for implementing that practice. They help you to decide which patterns and approaches are most useful. I tend to list these in a chapter for each core practice.\n\nPatterns (and antipatterns)\n\nPotential solutions to designing, building, and running your systems. The decisions of which ones are appropriate are driven by the implementation principles.",
      "content_length": 1799,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "The FoodSpin examples\n\nI use the fictional company Foodspin to illustrate concepts\n\nthroughout this book. Foodspin provides an online menu service\n\nfor fast food restaurants. I doubt this would be a viable business, but it works as an example. The company has clients in different\n\ncountries, with different cuisines, including Curry Hut in the UK,\n\nThe Fish King in Australia, and Bomber Burrito and Burger Barn in North America.\n\nConventions Used in This Book\n\nThe following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\n\nConstant width\n\nUsed for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.\n\nConstant width bold\n\nShows commands or other text that should be typed literally by the user.\n\nConstant width italic\n\nShows text that should be replaced with user-supplied values or by values determined by context.",
      "content_length": 1036,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "TIP\n\nThis element signifies a tip or suggestion.\n\nNOTE\n\nThis element signifies a general note.\n\nWARNING\n\nThis element indicates a warning or caution.\n\nO’Reilly Online Learning\n\nNOTE\n\nFor more than 40 years, O’Reilly Media has provided technology and business training, knowledge, and insight to help companies succeed.\n\nOur unique network of experts and innovators share their\n\nknowledge and expertise through books, articles, conferences, and our online learning platform. O’Reilly’s online learning platform\n\ngives you on-demand access to live training courses, in-depth\n\nlearning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other\n\npublishers. For more information, please visit http://oreilly.com.\n\nHow to Contact Us",
      "content_length": 778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Please address comments and questions concerning this book to\n\nthe publisher:\n\nO’Reilly Media, Inc.\n\n1005 Gravenstein Highway North\n\nSebastopol, CA 95472\n\n800-998-9938 (in the United States or Canada)\n\n707-829-0515 (international or local)\n\n707-829-0104 (fax)\n\nWe have a web page for this book, where we list errata, examples, and any additional information. You can access this page at\n\nhttp://bit.ly/infrastructureAsCode_1e.\n\nTo comment or ask technical questions about this book, send email\n\nto bookquestions@oreilly.com.\n\nFor more information about our books, courses, conferences, and news, see our website at http://www.oreilly.com.\n\nFind us on Facebook: http://facebook.com/oreilly\n\nFollow us on Twitter: http://twitter.com/oreillymedia\n\nWatch us on YouTube: http://www.youtube.com/oreillymedia\n\n1 Adam Jacob and Luke Kanies were also using the phrase around this time. Mark Burgess pioneered the practice of Infrastructure as Code with CFEngine before anyone even used the phrase.",
      "content_length": 988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "2 Bulletin Board System.\n\n3 Sadly, Infrastructures.org hasn’t been updated since 2007, but the content was still\n\nthere the last time I looked.\n\n4 http://shop.oreilly.com/product/0636920033158.do\n\n5 As I’ll explain in Chapter 3, “cloud” isn’t exclusive to public, shared cloud\n\nplatforms like AWS, Azure, and GCP. The “Cloud Age” reaches into on-premise systems and private data centers as well.",
      "content_length": 395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Part I. Foundations",
      "content_length": 19,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Chapter 1. What is Infrastructure as Code?\n\nIf you work in a team that builds and runs IT infrastructure, then cloud and infrastructure automation technology should help you\n\ndeliver more value in less time, and to do it more reliably. But in practice, it drives ever-increasing size, complexity, and diversity of things to manage.\n\nThese technologies are especially relevant as organizations\n\nbecome digital. “Digital” is how people in business attire say that software systems are essential to what the organization does. The move to digital increases the pressure on you to do more and to do it faster. You need to add and support more services. More\n\nbusiness activities. More employees. More customers, suppliers, and other stakeholders.\n\nCloud and automation tools help by making it far easier to add and\n\nchange infrastructure. But many teams struggle to find enough time to keep up with the infrastructure they already have. Making it easier to create even more stuff to manage is unhelpful. As one\n\nof my clients told me, “Using cloud knocked down the walls that kept our tire fire contained.”\n\n2\n\nMany people respond to the threat of unbounded chaos by\n\ntightening their change management processes. They hope they can prevent chaos by limiting and controlling changes. So they wrap the cloud in chains.\n\n1",
      "content_length": 1316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "There are two problems with this. One is that it removes the\n\nbenefits of using cloud technology; the other is that users want the benefits of cloud technology. So users bypass the people who are trying to limit the chaos. In the worst cases, people completely ignore risk management, deciding it’s not relevant in the brave\n\n3\n\nnew world of cloud. They embrace cowboy IT , which adds different problems.\n\nThe premise of this book is that you can exploit cloud and\n\nautomation technology to make changes easily, safely, quickly, and responsibly. These benefits don’t come out of the box with automation tools or cloud platforms. They depend on the way you\n\nuse this technology.\n\nDEVOPS AND INFRASTRUCTURE AS CODE\n\nDevOps is a movement to reduce barriers and friction between organizational silos - development, operations, and other stakeholders involved in planning, building, and running software. Although technology is the most visible, and in some ways simplest face of DevOps, it’s culture, people, and processes which have the most impact on flow and effectiveness. Technology and engineering practices like infrastructure as code should be used to support efforts to bridge gaps and improve collaboration.\n\nIn this chapter, I explain that modern, dynamic infrastructure requires a “Cloud Age” mindset. This mindset is fundamentally different from the traditional, “Iron Age” approach we used with static pre-cloud systems. I define three Core Practices for\n\nimplementing infrastructure as code: define everything as code, continuously validate everything as you work, and build your system from small, loosely-coupled pieces.",
      "content_length": 1633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Also in this chapter, I describe the reasoning behind the Cloud Age approach to infrastructure. This approach discards the false dichotomy of trading speed off against quality. Instead, we use speed as a way to improve quality, and we use quality to enable delivery at speed.\n\nI’ll explain why and how this dynamic works:\n\nThe differences between Iron Age systems and Cloud Age systems, in terms of both technology and ways of working,\n\nHow this leads to Infrastructure as Code,\n\nSome common objections to going “all-in” on Infrastructure as Code,\n\nThe parts of a dynamic infrastructure,\n\nThe three core practices of Infrastructure as Code.\n\nFrom the Iron Age to the Cloud Age\n\nCloud Age technologies make it faster to provision and change infrastructure than traditional, Iron Age technologies (Table 1-1).\n\nTable 1-1. Technology changes in the Cloud Age\n\nIron Age\n\nCloud Age\n\nPhysical hardware\n\nVirtualized resources\n\nProvisioning takes weeks\n\nProvisioning takes minutes\n\nManual processes\n\nAutomated processes",
      "content_length": 1011,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "However, these technologies don’t necessarily make it easier to manage and grow your systems. Moving a system with technical\n\ndebt onto unbounded cloud infrastructure accelerates the chaos.\n\nMaybe you could use well-proven, traditional governance models\n\nto control the speed and chaos that newer technologies unleash. Thorough up-front design, rigorous change review, and strictly\n\nsegregated responsibilities will impose order!\n\nUnfortunately, these models optimize for the Iron Age, where changes are slow and expensive. They add extra work up-front,\n\nhoping to reduce the time spent making changes later. This arguably makes sense when making changes later is slow and\n\nexpensive. But cloud makes changes cheap and fast. You should\n\nexploit this speed to learn and improve your system continuously. Iron Age ways of working are a massive tax on learning and\n\nimprovement.\n\nRather than using slow-moving Iron Age processes with fast- moving Cloud Age technology, adopt a new mindset. Exploit\n\nfaster-paced technology to reduce risk and improve quality. Doing\n\nthis requires a fundamental change of approach and new ways of thinking about change and risk (Table 1-2).",
      "content_length": 1169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Table 1-2. Ways of working in the Cloud Age\n\nIron Age\n\nCloud Age\n\nCost of change is high\n\nCost of change is low\n\nChanges represent failure (changes must be “managed”, “controlled”)\n\nChanges represent learning and improvement\n\nReduce opportunities to fail\n\nMaximize speed of improvement\n\nDeliver in large batches, test at the end\n\nDeliver small changes, test continuously\n\nLong release cycles\n\nShort release cycles\n\nMonolithic architectures (fewer, larger moving parts)\n\nMicroservices architectures (more, smaller parts)\n\nGUI-driven or physical configuration\n\nConfiguration as Code\n\nInfrastructure as Code is a Cloud Age approach to managing\n\nsystems that embraces continuous change for high reliability and quality.\n\nTHE CLOUD AGE\n\nCloud Age technologies make it possible to rapidly provision and change infrastructure resources. However, you need to adopt Cloud Age ways of working to exploit the technology to deliver better reliability, security, and quality.\n\nCloud Age approaches are not only relevant when using shared public clouds, but are essential for modern on-premise and data center infrastructures as well.\n\nInfrastructure as Code",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Infrastructure as Code is an approach to infrastructure automation\n\nbased on practices from software development. It emphasizes\n\nconsistent, repeatable routines for provisioning and changing systems and their configuration. You make changes to code, then\n\nuse automation to test and apply those changes to your systems.\n\nThroughout this book, I explain how to use agile engineering practices such as Test Driven Development (TDD), Continuous\n\nIntegration (CI), and Continuous Delivery (CD) to make changing\n\ninfrastructure fast and safe. I also describe how modern software design can create resilient, well-maintained infrastructure. These\n\npractices and design approaches reinforce each other. Well- designed infrastructure is easier to test and deliver. Automated\n\ntesting and delivery drive simpler and cleaner design.\n\nTIP\n\nInfrastructure as Code applies software engineering practices to the management of infrastructure for better reliability, security, and quality.\n\nBenefits of Infrastructure as Code\n\nTo summarize, organizations adopting infrastructure as code hope\n\nto achieve benefits including:\n\nUsing IT infrastructure as an enabler for rapid delivery of value,\n\nReducing the effort and risk of making changes to infrastructure,\n\nEnabling users of infrastructure to get the resources they need, when they need it,",
      "content_length": 1327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Providing common tooling across development, operations, and other stakeholders,\n\nCreating systems that are reliable, secure, and cost- effective,\n\nMake governance, security, and compliance controls visible,\n\nImproving the speed to troubleshoot and resolve failures.\n\nUse Infrastructure as Code to optimize for change\n\nGiven that:\n\n4 Changes are the biggest risk to a production system ,\n\nContinuous change is inevitable, and\n\nMaking changes is the only way to improve a system,\n\nThen it makes sense to optimize your capability to make changes both rapidly and reliably. Research from the Accelerate State of\n\nthe DevOps Report backs this up. Making changes frequently and 5 reliably is correlated to organizational success .\n\nThere are several objections I hear when I recommend a team implement automation to optimize for change. I believe these\n\ncome from misunderstandings of how you can use automation.\n\nObjection: “We don’t make changes often enough to justify automating them”\n\nWe want to think that we build a system, and then it’s “done.” In this view, we don’t make many changes, so automating changes is\n\na waste of time.",
      "content_length": 1132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "In reality, very few systems stop changing, at least before they are retired. Some people assume that their current level of change is\n\ntemporary. Others create heavyweight change request processes to discourage people from asking for changes. These people are in denial. Most teams who are supporting actively used systems\n\nhandle a continuous stream of changes.\n\nConsider these common examples of infrastructure changes:\n\nAn essential new application feature requires adding a new database,\n\nA new application feature requires an upgrade to the application server,\n\nUsage levels grow faster than expected. You need more servers, new clusters, and expanded network and storage capacity,\n\nPerformance profiling shows that the current application deployment architecture is limiting performance. You need to redeploy the applications across different application servers. Doing this requires changes to the clustering and network architecture,\n\nThere is a newly announced security vulnerability in system packages for your OS. You need to patch dozens of production servers,\n\nYou need to update servers running a deprecated version of the OS and critical packages,\n\nYour web servers experience intermittent failures. You need to make a series of configuration changes to diagnose the problem. Then you need to update a module to resolve the issue,\n\nYou find a configuration change that improves the performance of your database.",
      "content_length": 1427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "A fundamental truth of the Cloud Age is: Stablity comes from making changes.\n\nUnpatched systems are not stable; they are vulnerable. If you can’t\n\nfix issues as soon as you discover them, your system is not stable. If you can’t recover from failure quickly, your system is not stable. If the changes you do make involve considerable downtime, your system is not stable. If changes frequently fail, your system is not\n\nstable.\n\nObjection: “We should build first and automate later”\n\nGetting started with infrastructure as code is a steep curve. Setting\n\nup the tools, services, and working practices to automate infrastructure delivery is loads of work, especially if you’re also adopting a new infrastructure platform. The value of this work is hard to demonstrate before you start building and deploying\n\nservices with it. Even then, the value may not be apparent to people who don’t work directly with the infrastructure.\n\nSo stakeholders often pressure infrastructure teams to build new cloud-hosted systems quickly, by hand, and worry about\n\nautomating it later.\n\nThere are two reasons why automating afterward is a bad idea:\n\nAutomation should enable faster delivery, even for new things. Implementing automation after most of the work has been done sacrifices much of the benefits.\n\nAutomation makes it easier to write automated tests for what you build. And it makes it easier to quickly fix and",
      "content_length": 1402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "rebuild when you find problems. Doing this as a part of the build process helps you to build better infrastructure.\n\nAutomating an existing system is very hard. Automation is a part of a system’s design and implementation. To add automation to a system built without it, you need to change the design and implementation of that system significantly. This is also true for automated testing and deployment.\n\nCloud infrastructure built without automation becomes a write-off sooner than you expect. The cost of manually maintaining and fixing the system can escalate quickly. If the service it runs is\n\nsuccessful, stakeholders will pressure you to expand and add features rather than stopping to rebuild.\n\nThe same is true when you build a system as an experiment. Once you have a proof of concept up and running, there is pressure to move on to the next thing, rather than to go back and build it right.\n\nAnd in truth, automation should be a part of the experiment. If you intend to use automation to manage your infrastructure, you need to understand how this will work, so it should be part of your proof\n\nof concept.\n\nThe solution is to build your system incrementally, automating as you go. Ensure you deliver a steady stream of value, while also building the capability to do so continuously.\n\nObjection: “We must choose between speed and quality”\n\nIt’s natural to think that you can only move fast by skimping on quality; and that you can only get quality by moving slowly. You might see this as a continuum, as shown in Figure 1-1.",
      "content_length": 1538,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Figure 1-1. The idea that speed and quality are opposite ends of a spectrum is a false dichotomy\n\nHowever, the Accelerate research I mentioned earlier (“Use\n\nInfrastructure as Code to optimize for change”) shows otherwise.\n\nThese results demonstrate that there is no tradeoff between improving performance and achieving higher levels of stability and quality. Rather, high performers do better at all of these measures. This is precisely what the Agile and Lean movements predict, but much dogma in our industry still rests on the false assumption that moving faster means trading off against other performance goals, rather than enabling and reinforcing them.\n\n—Forsgren PhD, Nicole. Accelerate\n\nIn short, organizations can’t choose between being good at change or being good at stability. They tend to either be good at both or\n\nbad at both.\n\n6\n\nI prefer to see quality and speed as a quadrant rather than a continuum, as shown in Figure 1-2.",
      "content_length": 944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Figure 1-2. Change and stability is a quadrant\n\nThis quadrant model shows why trying to choose between speed\n\nand quality leads to being mediocre at both.\n\nLower-right quadrant\n\nThis is the “move fast and break things” philosophy. Teams that optimize for speed and sacrifice quality build messy, fragile systems. They slide into the lower-left quadrant because their shoddy systems slow them down. Many startups who have been working this way for a while complain about losing their “mojo.” Simple changes that they would have whipped out quickly in the old days now take days or weeks because the system is a tangled mess.\n\nUpper-left quadrant:: Prioritize quality over speed\n\nAlso known as, “we’re doing serious and important things, so we have to do things properly.” Then deadline pressures drive “workarounds.” Heavyweight processes make the system hard to fix and improve. So technical debt grows along with lists of “known issues.” These teams slump into the lower-left quadrant. They end up with low-quality systems because it’s too hard to improve them. They add more processes in response to failures. These processes make it even harder to make improvements and increases fragility and risk. This leads to more failures and more process. Many people working in 7 organizations that work this way assume this is normal , 8 especially those who work in risk-sensitive industries .\n\nThe upper-right quadrant is the goal of modern approaches like\n\nlean, agile, and DevOps. Being able to move quickly and maintain\n\na high level of quality may seem like a fantasy. However, the Accelerate research proves that many teams do achieve this. So\n\nthis quadrant is where you find “high performers.”",
      "content_length": 1697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "WHY YOU SHOULD AUTOMATE FROM THE START\n\nYou make changes more often than you may think,\n\nIt’s far easier to automate as you build something than to add it afterward,\n\nAutomating the provisioning and configuration of environments ensures consistency from the start,\n\nAutomation helps you make changes rapidly and reliably. Speed enables quality, and quality enables speed (Figure 1-3).\n\nFigure 1-3. Speed enables quality, and quality enables speed\n\nThree core practices for Infrastructure as Code\n\nThe Cloud Age concept exploits the dynamic nature of modern\n\ninfrastructure and application platforms to make changes",
      "content_length": 614,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "frequently and reliably. Infrastructure as Code is an approach to\n\nbuilding infrastructure that embraces continuous change for high\n\nreliability and quality. So how can your team do this?\n\nThere are three core practices for implementing Infrastructure as Code:\n\nDefine everything as code\n\nContinuously validate all work in progress\n\nBuild small, simple pieces that you can change independently\n\nI’ll summarize each of these now, to set the context for further discussion. Later, I’ll devote a chapter to the principles for\n\nimplementing each of these practices.\n\nCore practice: Define everything as code\n\nDefining all your stuff “as code” is a core practice for making\n\nchanges rapidly and reliably. There are a few reasons why this helps:\n\nReusability\n\nIf you define a thing as code, you can create many instances of it. You can repair and rebuild your things quickly. Other people can build identical instances of the thing.\n\nConsistency\n\nThings built from code are built the same way every time. This makes system behavior predictable. This makes testing more reliable. This enables continuous validation mentioned in “Core practice: Continuously validate all your work in progress”.",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Transparency\n\nEveryone can see how the thing is built by looking at the code. People can review the code and suggest improvements. They can learn things to use in other code. They gain insight to use when troubleshooting. They can review and audit for compliance.\n\nI’ll expand on concepts and implementation principles for defining\n\nthings as code in Chapter 4.\n\nCore practice: Continuously validate all your work in progress\n\nEffective infrastructure teams are rigorous about testing. They use\n\nautomation to deploy and test each component of their system.\n\nThey integrate all the work everyone has in progress. They test as they work, rather than waiting until they’ve finished.\n\nThe idea is to build quality in rather than trying to test quality in.\n\nOne part of this that people often overlook is that it involves\n\nintegrating and testing all work in progress. On many teams, people work on code in separate branches and only integrate when\n\nthey finish. According to the Accelerate research, however, teams\n\nget better results when everyone integrates their work at least daily. Continuous Integration (CI) involves merging and testing\n\neveryone’s code throughout development. Continuous Delivery\n\n(CD) takes this further, keeping the merged code always production-ready.\n\nI’ll go into more detail on how to continuously validate\n\ninfrastructure code in Chapter 9.",
      "content_length": 1369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Core practice: Build small, simple pieces that you can change independently\n\nTeams struggle when their systems are large and tightly coupled.\n\nThe larger a system is, the harder it is to change, and the easier it is to break.\n\nWhen you look at the codebase of a high performing team, you see\n\nthe difference. Their system is composed of small, simple pieces.\n\nEach piece is easy to understand and has clearly defined interfaces. They can easily change each component on its own. And, they can\n\ndeploy and test each component in isolation.\n\nI dig more deeply into implementation principles for this core\n\npractice in [Link to Come].\n\nRECAP: THE CORE PRACTICES OF INFRASTRUCTURE AS CODE\n\nDefine everything as code\n\nContinuously validate all work in progress\n\nBuild small, simple pieces that can be changed independently\n\nThe parts of an infrastructure system\n\nThere are many different parts, and types of parts, in a modern\n\ncloud infrastructure. I find it helpful to group these parts into\n\nplatform layers. I use these as rough architectural boundaries for systems and dedicate a chapter of this book to two of these layers.\n\nInfrastructure Platform",
      "content_length": 1149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "The Infrastructure Platform is the set of infrastructure resources and the tools and services that manage them. Cloud and virtualization platforms provide infrastructure resources, including compute, storage, and networking primitives. People also call this Infrastructure as a Service (IaaS). I’ll elaborate on these in Chapter 3. This layer also involves tools to define and manage infrastructure resources. An infrastructure stack is a collection of infrastructure resources managed as a unit. I’ll talk about infrastructure stacks and tools that manage them, such as Terraform and CloudFormation, in chapter Chapter 5.\n\nApplication Runtime Platform\n\nApplication Runtime Platforms build on infrastructure platforms to provide environments for running applications. Examples of services and constructs in an application runtime platform include container clusters, serverless environments, application servers, OS processes, and databases. People often refer to this as Platform as a Service (PaaS). The idea of “cloud native” systems has become popular in recent years, and fits in with this layer. I’ll talk about these more in [Link to Come].\n\nApplications\n\nApplications and services run on your infrastructure, providing services to your organization and its users.\n\nFigure 1-4 shows these layers:",
      "content_length": 1303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Figure 1-4. Layers of system elements\n\nConclusion\n\nTo get the value of cloud and infrastructure automation, you need a\n\nCloud Age mindset. This means exploiting speed to improve\n\nquality, and building quality in to gain speed. Automating your infrastructure takes work, especially when you’re learning how to\n\ndo it. But doing it helps you to make changes, including building\n\nthe system in the first place.\n\nI’ve described the parts of a typical infrastructure system, as these provide the foundations for chapters explaining how to implement\n\nInfrastructure as Code.\n\nFinally, I defined three core practices for Infrastructure as Code:\n\nBuild small, simple pieces that you can change independently\n\nDefine everything as code\n\nContinuously validate all work in progress\n\nIn the next chapter, I’ll describe the principles that drive Cloud\n\nAge approaches for managing infrastructure.\n\n1 This is as opposed to what they usually said a few years ago, which is that software\n\nis “not part of our core business.” Since then, those people realized their organizations were being overtaken by ones run by people who see better software as a way to compete, rather than as a cost to reduce.\n\n2 According to Wikipedia, a tire fire has two forms: “Fast-burning events, leading to almost immediate loss of control, and slow-burning pyrolysis which can continue for over a decade.”",
      "content_length": 1370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "3 By “cowboy IT,” I mean people building IT systems without any particular method or consideration for future consequences. Often, people who have never supported production systems take the quickest path to get things working without considering security, maintenance, performance, and other operability concerns.\n\n4 According to the Visible Ops Handbook, changes cause 80% of unplanned\n\noutages.\n\n5 Reports from the Accelerate research are available in the annual State of the\n\nDevOps Report, and in the book Accelerate by Dr. Nicole Forsgren, Jez Humble, Gene Kim.\n\n6 Yes, I do work at a consultancy, why do you ask?\n\n7 This is an example of “Normalization of Deviance,” which means people get used to working in ways that increase risk. Diane Vaughan defined this term in The Challenger Launch Decision\n\n8 It’s ironic (and scary) that so many people in industries like finance, government, and healthcare consider fragile IT systems, and processes that obstruct improving them, to be normal, and even desirable.",
      "content_length": 1015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Chapter 2. Principles of Cloud Age Infrastructure\n\nComputing resources in the Iron Age of IT were tightly coupled to physical hardware. We assembled CPUs, memory, and hard drives\n\nin a case, mounted it into a rack, and cabled it to switches and routers. We installed and configured an operating system and application software. We could describe where an application\n\nserver was in the data center: which floor, which row, which rack,\n\nwhich slot.\n\nThe Cloud Age decouples the computing resources from the physical hardware they run on. The hardware still exists, of course, but servers, hard drives, and routers float across it. These are no\n\nlonger physical things, having transformed into virtual constructs that we create, duplicate, change, and destroy at will.\n\nThis transformation has forced us to change how we think about,\n\ndesign, and use computing resources. We can’t rely on the physical attributes of our application server to be constant. We must be able to add and remove instances of our systems without ceremony.\n\nAnd we need to be able to easily maintain the consistency and quality of our systems even as we rapidly expand their scale.",
      "content_length": 1154,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "CLOUD NATIVE APPLICATION ARCHITECTURES\n\nThis book focuses on how to design and build infrastructure in the context of Cloud Age platforms. The dynamic nature of these platforms also affects the design of application software. Cloud native applications are decoupled from specific infrastructure resources like servers, hard-drives, and network locations. Developers assume their applications will be automatically scaled up and down, and that workloads will be shifted between instances. [Link to Come] and the rest of [Link to Come] describe how to build infrastructure that supports cloud native software.\n\nThere are several principles for designing and implementing infrastructure on cloud platforms. These principles articulate the\n\nreasoning for using the three core practices (define everything as code, continuously validate, build small pieces). I also list several\n\ncommon pitfalls teams make with dynamic infrastructure.\n\nTogether, these principles and pitfalls drive more specific implementation principles for the core infrastructure as code practices and help choose between different implementation patterns. They set the context for the advice throughout this book.\n\nPrinciple: Assume systems are unreliable\n\nIn the Iron Age, we assumed our systems were running on reliable hardware. In the Cloud Age, you need to assume your system runs 1 on unreliable hardware .\n\nCloud scale infrastructure involves hundreds of thousands of devices, if not more. At this scale, failures happen even when",
      "content_length": 1504,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "using reliable hardware. And most cloud vendors use cheap, less reliable hardware, detecting and replacing it when it breaks.\n\nYou’ll need to take parts of your system offline for reasons other than unplanned failures. You’ll need to patch and upgrade systems. You’ll resize. Redistribute load. Troubleshoot problems.\n\nWith static infrastructure, doing these things means taking systems offline. But in many modern organizations, taking systems offline means taking the business offline.\n\nSo you can’t treat the infrastructure your system runs on as a stable foundation. Instead, you must design for uninterrupted service when underlying resources change .2",
      "content_length": 657,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "THE TRADEOFFS OF “LIFT AND SHIFT” MIGRATION TO THE CLOUD\n\nMany organizations are migrating their systems from Iron Age infrastructure to cloud platforms. A common approach is “lift and shift”, which aims to emulate existing on-premise infrastructure, so that applications can be moved with few or no changes to code and configuration. Teams replicate physical network structures and static addresses in the cloud, and even use virtual appliance versions of network devices such as firewalls and routers.\n\nThe reasons people use lift and shift include:\n\nTo use familiar, trusted technology, vendors, and topologies, rather than having to retrain,\n\nTo avoid the cost of modifying or reconfiguring applications,\n\nTo simplify the migration process, avoiding big hairy rewrites.\n\nThe problems with lift and shift include:\n\nReplicating physical infrastructure configuration in the cloud adds complexity and cost, not only for the migration process but also for ongoing maintenance.\n\nMany products, topologies, and patterns that are appropriate for data centers are not appropriate for the cloud. Transplanting them adds cost, and may not add any benefit. They can even make performance, security, and maintainability worse.\n\nResources provided by the cloud platform provide benefits for reliability, security, and scalability. Reproducing data center configurations loses those benefits.\n\nLift and shift aims to reduce the work of moving an application to the cloud by adding implementation work to its infrastructure. This may succeed if the infrastructure work can realistically be done more easily than changing the application. More often, applications can be tweaked to make them cope easier in their new environment. In any case, you should have a concrete strategy for adapting, rewriting, or managing applications after they have been shifted to the cloud.\n\nPrinciple: Make everything reproducible\n\nOne way to make a system recoverable is to make sure you can rebuild its parts effortlessly and reliably.\n\nEffortlessly means that there is no need to make any decisions\n\nabout how to build things. You should define things such as",
      "content_length": 2131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "configuration settings, software versions, and dependencies as code. Rebuilding is then a simple “yes/no” decision.\n\nMEASURING REPRODUCIBILITY\n\nYou and your team should track the time it takes you to build a new instance of an existing system. This best way to know this, with confidence, is to rebuild frequently, as a part of routine operations.\n\nSome teams build fresh infrastructure every time they deploy a new software release. This process is usually part of a zero-downtime release process (see [Link to Come]). Other teams rebuild infrastructure in test environments as part of a pipeline ([Link to Come]).\n\nEither approach regularly proves the speed that you can rebuild. This in turn means you can be confident you can do it in an emergency situation.\n\nNot only does reproducibility make it easy to recover a failed\n\nsystem, but it also helps you to:\n\nMake testing environments consistent with production,\n\nReplicate systems across regions for availability,\n\nAdd instances on demand to cope with high load,\n\nReplicate systems to give each customer a dedicated instance.\n\nOf course, the system generates data, content, and logs, which you\n\ncan’t define ahead of time. You need to identify these and find ways to keep them as a part of your replication strategy. Doing this\n\nmight be as simple as copying or streaming data to a backup and then restoring it when rebuilding. I’ll describe options for doing\n\nthis in [Link to Come].",
      "content_length": 1439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "The ability to effortlessly build and rebuild any part of the\n\ninfrastructure is powerful. It removes risk and fear of making\n\nchanges. You can handle failures with confidence. You can rapidly provision new services and environments.\n\nPitfall: Snowflake systems\n\nA snowflake is an instance of a system or part of a system that is difficult to rebuild. It may also be an environment that should be\n\nsimilar to other environments, such as a staging environment, but is different in ways that its team doesn’t fully understand.\n\nPeople don’t set out to build snowflake systems. They are a natural\n\noccurrence. The first time you build something with new tool, you\n\nlearn lessons along the way, which involves making mistakes. But if people are relying on the thing you’ve built, you may not have\n\ntime to go back and rebuild or improve it using what you learned. Improving what you’ve built is especially hard if you don’t have\n\nthe mechanisms and practices that make it easy to and safe to change.\n\nAnother cause of snowflakes is when people make changes to one instance of a system that they don’t make to others. They may be\n\nunder pressure to fix a problem that only appears in one system. Or they may start a major upgrade in a test environment, but run\n\nout of time to roll it out to others.\n\nYou know a system is a snowflake when you’re not confident you can safely change or upgrade it. Worse, if the system does break,\n\nit’s hard to fix it. So people avoid making changes to the system, leaving it out of date, unpatched, and maybe even partly broken.",
      "content_length": 1557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Snowflake systems create risk and waste the time of the teams that manage them. It is almost always worth the effort to replace them\n\nwith reproducible systems. If a snowflake system isn’t worth improving, then it may not be worth keeping at all.\n\nThe best way to replace a snowflake system is to write code that can replicate the system, running the new system in parallel until\n\nit’s ready. Use automated tests and pipeline to prove that it is correct, reproducible, and that you can change it easily.\n\nPrinciple: Create disposable things\n\nBuilding a system that can cope with dynamic infrastructure is one level. The next level is building a system that is itself dynamic. You should be able to gracefully add, remove, start, stop, change,\n\nand move the parts of your system. Doing this creates operational flexibility, availability, and scalability. It also simplifies and de- risks changes.\n\nMaking the pieces of your system malleable is the main idea of cloud-native software. Cloud abstracts infrastructure resources\n\n(compute, networking, and storage) from physical hardware. Cloud-native software completely decouples application functionality from the infrastructure it runs on. See [Link to Come]\n\nfor more on this.\n\nCATTLE, NOT PETS\n\n“Treat your servers like cattle, not pets,” is a popular expression about disposability . I miss giving fun names to each new server I create. But I don’t miss having to tweak and coddle every server in our estate by hand.\n\n3",
      "content_length": 1471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "If your systems are dynamic, then the tools you use to manage them need to cope with this. For example, your monitoring should\n\nnot raise an alert every time you rebuild part of your system. However, it should raise a warning if something gets into a loop rebuilding itself.\n\nTHE CASE OF THE DISAPPEARING FILE SERVER\n\nPeople can take a while to get used to ephemeral infrastructure. One team I worked with set up automated infrastructure with VMware and Chef. They deleted and replaced virtual machines as needed.\n\nA new developer on the team needed a web server to host files to share with teammates. So he manually installed an HTTP server on a development server and put the files there. A few days later, I rebuilt the VM, and his web server disappeared.\n\nAfter some confusion, the developer understood why this had happened. He added his web server to the Chef code, and persisted his files to the SAN. The team now had a reliable file sharing service.\n\nPrinciple: Minimize variation\n\nAs a system grows, it becomes harder to understand, harder to\n\nchange, and harder to fix. The work involved grows with the number of pieces, and also with the number of different types of pieces. So a useful way to keep a system manageable is to have\n\nfewer types of pieces-to keep variation low. It’s easier to manage 100 identical servers than 5 completely different servers.\n\nThe reproducibility principle (“Principle: Make everything reproducible”) complements this idea. If you define a simple\n\ncomponent and create many identical instances of it, then you can easily understand, change, and fix it.",
      "content_length": 1594,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "To make this work, you must apply any change you make to all instances of the component. Otherwise, you create configuration-\n\ndrift.\n\nHere are some types of variation you may have in your system:\n\nMultiple operating systems, application runtimes, databases, and other technologies. Each one of these needs people in your team to keep up skills and knowledge.\n\nMultiple versions of software such as an operating system or database. Even if you only use one operating system, each version may need different configurations and tooling.\n\nDifferent versions of a package. When some servers have a newer version of a package, utility, or library than others, you have risk. Commands may not run consistently across them. Older versions may have vulnerabilities or bugs.\n\nOrganizations have tension between allowing each team to choose\n\ntechnologies and solutions that are appropriate to their needs, versus keeping the amount of variation in the organization to a manageable level.\n\nLIGHTWEIGHT GOVERNANCE\n\nModern, digital organizations are learning the value of Lightweight Governance in IT to balance autonomy and centralized control. This is a key element of the EDGE model for agile organizations. For more on this, see the book, EDGE: Value-Driven Digital Transformation, or Jonny LeRoy’s talk, The Goldilocks zone of lightweight architectural governance.",
      "content_length": 1356,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Configuration Drift\n\nConfiguration drift is variation that happens over time across systems that were once identical. Figure 2-1 shows this. Manually making changes can cause configuration drift. It can also happen if\n\nyou use automation tools to make ad-hoc changes to only some of the instances.",
      "content_length": 297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Figure 2-1. Configuration drift is when instances of the same thing become different over time.\n\nConfiguration drift makes it harder to maintain consistent\n\nautomation.\n\nFOODSPIN EXAMPLE: DIVERGING INFRASTRUCTURE\n\nFoodspin is a fictional company that provides an online menu service for fast food restaurants. I’ll be using it throughout the book to demonstrate how concepts may play out in practice. First, let’s look at Foodspin as an example of configuration drift.\n\nFoodspin runs a separate instance of their application for each restaurant, each instance configured to use custom branding and menu content. In the early days, the Foodspin team ran scripts to create a new application server for each new restaurant. They managed the infrastructure manually or by writing scripts and tweaking them each time they needed to make a change.\n\nOne of the restaurants, Curry Hut, has far more traffic to its menu application than the others, so the team tweaked the configuration for the Curry Hut server. They didn’t make the changes to the other customers, because the team was busy and it didn’t seem necessary.\n\nLater, the Foodspin team adopted Ansible to automate their application server configuration. They first tested it with the server for The Fish King, a lower-volume customer, and then rolled it out to their other customers. Unfortunately, their code didn’t include the performance optimizations for Curry Hut, so it stripped those improvements. The Curry Hut server slowed to a crawl until the team caught and fixed their mistake.\n\nThe Foodspin team overcame this by parameterizing their Ansible code. It can now set resource levels different for each customer. This way, they still apply the same code across every customer, while still optimizing it for each.\n\nWe frequently need to support specific variations between\n\ninstances of otherwise identical infrastructure. I’ll describe some patterns and antipatterns for dealing with this in Chapter 8.",
      "content_length": 1964,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "THE AUTOMATION FEAR SPIRAL\n\nThe automation fear spiral describes how many teams fall into configuration drift and technical debt.\n\nAt an Open Space session on configuration automation at a DevOpsDays conference, I asked the group how many of them were using automation tools like Ansible, Chef, or Puppet. The majority of hands went up. I asked how many were running these tools unattended, on an automatic schedule. Most of the hands went down.\n\nMany people have the same problem I had in my early days of using automation tools. I used automation selectively—for example, to help build new servers, or to make a specific configuration change. I tweaked the configuration each time I ran it, to suit the particular task I was doing.\n\nI was afraid to turn my back on my automation tools because I lacked confidence in what they would do.\n\nI lacked confidence in my automation because my servers were not consistent.\n\nMy servers were not consistent because I wasn’t running automation frequently and consistently.\n\nThis is the automation fear spiral, as shown in Figure 2-2. Infrastructure teams must break this spiral to use automation successfully. The most effective way to break the spiral is to face your fears. Start with one set of servers. Make sure you can apply, and then re-apply your infrastructure code to these servers. Then schedule an hourly process that continuously applies the code to those servers. Then pick another set of servers and repeat the process. Do this until every server is continuously updated.",
      "content_length": 1526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "Figure 2-2. The automation fear spiral\n\nGood monitoring and automated testing builds the confidence to continuously synchronize your code. This exposes configuration drift as it happens, so you can fix it immediately.\n\nPrinciple: Ensure that you can repeat any process\n\nBuilding on the reproducibility principle, you should be able to\n\nrepeat anything you do to your infrastructure. It’s easier to repeat actions using scripts and configuration management tools than to",
      "content_length": 469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "do it by hand. But automation can be a lot of work, especially if\n\nyou’re not used to it.\n\nFor example, let’s say I have to partition a hard drive as a one-off\n\ntask. Writing and testing a script is much more work than just logging in and running the fdisk command. So I do it by hand.\n\nThe problem comes later on, when someone else on my team,\n\nPriya, needs to partition another disk. She comes to the same\n\nconclusion I did, and does the work by hand rather than writing a script. However, she makes slightly different decisions about how to partition the disk. I made an 80 GB /var ext3 partition on my server, but Priya made /var a 100 GB xfs partition on hers. We’re creating configuration drift, which will erode our ability to\n\nautomate with confidence.\n\nEffective infrastructure teams have a strong scripting culture. If you can script a task, script it . If it’s hard to script it, dig deeper.\n\n4\n\nMaybe there’s a technique or tool that can help, or maybe you can\n\nsimplify the task or handle it differently. Breaking work down into scriptable tasks usually makes it simpler, cleaner, and more\n\nreliable.\n\nPHOENIX SERVERS\n\n5\n\nA Phoenix Server is frequently rebuilt, in order to ensure that the provisioning process is repeatable. This can be done with other infrastructure constructs, including infrastructure stacks.\n\nConclusion\n\nThe Principles of Cloud Age Infrastructure embody the differences\n\nbetween traditional, static infrastructure, and modern, dynamic",
      "content_length": 1470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "infrastructure:\n\nAssume systems are unreliable,\n\nMake everything reproducible,\n\nCreate disposable things,\n\nMinimize variation,\n\nEnsure that you can repeat any action.\n\nThese principles are the key to exploiting the nature of cloud\n\nplatforms. Rather than resisting the ability to make changes with\n\nminimal effort, exploit that ability to gain quality and reliability.\n\nThe next chapter focuses on the types of infrastructure resources provided by cloud platforms. It sets the context for what we define\n\nas code, validate continuously, in small pieces, which are the three\n\ncore practices for implementing infrastructure as code.\n\n1 I learned this idea from Sam Johnson’s article, “Simplifying Cloud: Reliability”.\n\n2 The principle of assuming systems are unreliable drives Chaos Engineering, which injects failures in controlled circumstances to test and improve the reliability of your services. I talk about this more in [Link to Come]\n\n3 I first heard this expression in Gavin McCance’s presentation “CERN Data Centre Evolution”. Randy Bias credits Bill Baker’s presentation “Architectures for Open and Scalable Clouds”. Both of these presentations are an excellent introduction to these principles.\n\n4 My colleague Florian Sellmayr says, “If it’s worth documenting, it’s worth\n\nautomating.”\n\n5 My colleague Kornelis Sietsma coined the term Phoenix Server, which Martin\n\nFowler later documented on his site.",
      "content_length": 1412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Chapter 3. Infrastructure Platforms\n\nThe first thing you need for infrastructure as code is infrastructure. In Chapter 1 I shared a layered platform model (“The parts of an\n\ninfrastructure system”). The infrastructure is the foundational layer of this model.",
      "content_length": 258,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Figure 3-1. The infrastructure platform is the foundation layer of the platform model from chapter 1\n\nInfrastructure as code requires a dynamic infrastructure platform, something that you can use to provision and change resources on 1 demand with an API. This is the essential definition of a cloud .\n\nWhen I talk about an “infrastructure platform” in this book, you 2 can assume I mean a dynamic, Infrastructure as a Service (IaaS) type of platform.\n\nIn the old days-the Iron Age of computing-infrastructure was hardware. Virtualization decoupled systems from the hardware they ran on. Cloud added APIs to manage those virtualized resources. Thus began the Cloud Age.\n\n3\n\nThere are different types of infrastructure platforms, from full- blown public clouds to private clouds; from commercial vendors to open source platforms. In this chapter, I outline these variations and then describe the different types of infrastructure resources they provide.\n\nAt the basic level, infrastructure platforms provide compute, storage, and networking resources. The platforms provide these\n\nresources in various formats. For instance, you may run compute as virtual servers, container runtimes, and serverless code execution.\n\nDifferent vendors may package and offer the same resources in different ways, or at least with different names. For example, AWS object storage, Azure blob storage, and GCP cloud storage are all pretty much the same thing.",
      "content_length": 1437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "I describe the common types of resources that most cloud platforms provide in this chapter. I want the implementation\n\npatterns and recommendations throughout this book to be useful no matter which cloud you use. So I try to use names that are\n\nrelevant across clouds. Rather than “VPC” and “Subnet,” I talk\n\nabout “Network address blocks.”\n\nThis chapter should be a useful reference if you are wondering how to apply infrastructure as code practices and patterns to a\n\nparticular aspect of your system, for example, container clusters.\n\nNot everyone draws the line between infrastructure resources and application runtime services like container clusters and databases\n\nin the same place. I don’t think it particular matters where you\n\ndraw the boundaries in your system. I have included some things in this chapter and others in [Link to Come], based on my personal\n\npreferences.\n\nWhat is a dynamic infrastructure platform?\n\nPublic clouds are the first thing most of us think of as platforms\n\nfor infrastructure as code. But there are other options for cloud infrastructure.\n\nThe key characteristics of a dynamic infrastructure platform are:\n\nIt provides a pool of compute, networking, and storage resources,\n\nIt allows you to provision and change these resources on demand,",
      "content_length": 1276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "You can provision, configure, and change resources programmatically.",
      "content_length": 68,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "Figure 3-2. An infrastructure platform provides a pool of resources that you can provision on demand.\n\nA public IaaS cloud clearly has these characteristics. However, many private cloud platforms also qualify. A private cloud may run on a pool of hardware owned by the organization. However, its\n\nusers use an API to provision infrastructure from that hardware pool, so it still counts as a cloud.\n\nWhen performance or other requirements make it useful to run applications directly on server hardware, you can implement a bare-metal cloud. A bare-metal cloud provides an API to provision\n\noperating systems directly onto physical servers on demand.\n\nTable 3-1 lists examples of vendors, products, and tools for each type of cloud infrastructure platform.\n\nTable 3-1. Examples of dynamic infrastructure platforms\n\nType of Platform\n\nProviders or Products\n\nPublic IaaS cloud services\n\nAWS, Azure, Digital Ocean, GCE, Linode, and Oracle Cloud\n\nPrivate IaaS cloud products\n\nCloudStack, OpenStack, and VMware vCloud\n\nBare-metal cloud tools\n\nCobbler, FAI, and Foreman\n\nInfrastructure Resources\n\nThere are three essential resources provided by an infrastructure platform: compute, storage, and networking. Different platforms combine and package these resources in different ways. For\n\nexample, you may be able to provision virtual machines and",
      "content_length": 1336,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "container instances on your platform. You may also be able to provision a database instance, which combines compute, storage,\n\nand networking.\n\nI call the more elemental infrastructure resources primitives. Each of the compute, networking, and storage resources described in the sections later in this chapter is a primitive. Cloud platforms\n\ncombine infrastructure primitives into composite resources, such as:\n\nDatabase as a Service (DBaaS)\n\nLoad balancing\n\nDNS\n\nIdentity management\n\nSecrets management\n\nThe line between a primitive and a composite resource is arbitrary, as is the line between a composite infrastructure resource and an\n\napplication runtime service. Even a basic storage service like object storage (think AWS S3 buckets) involves compute and networking resources to read and write data. But it’s a useful\n\ndistinction, which I’ll use now to list common forms of infrastructure primitives. These fall under three basic resource types: compute, storage, and networking.\n\nCompute Resources\n\nCompute resources execute code. At its most elemental, compute is execution time on a physical server CPU core. But platforms provide compute in more useful ways. The most common compute\n\nresource primitives are:",
      "content_length": 1221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Virtual machines\n\nPhysical servers\n\nContainers\n\nServer clusters\n\nServerless code execution (FaaS)\n\nI’ll describe each of these in more detail.\n\nVirtual machines\n\nVirtual Machines (VMs), sometimes called virtual servers, are the workhorse of cloud infrastructure. The infrastructure platform manages a pool of physical host servers, and runs virtual machine instances on hypervisors across these hosts:",
      "content_length": 401,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Figure 3-3. Each virtual machine has its own operating system kernel\n\nThe difference between ordinary server virtualization and an IaaS cloud platform is provisioning. To provision a non-cloud VM, you specify which host server it should run on. When you provision a\n\ncloud VM, the cloud platform decides where to run it, and you shouldn’t need to know or care where.\n\nSome clouds automatically migrate live server instances between hosts, transparently to processes running on them. This ability allows platform operators to balance workloads more efficiently.\n\nThey can also carry out maintenance such as patching without downtime. It’s useful to understand how your cloud provider handles this and how it may impact you.\n\nSome platform operators commit to keeping VMs running and\n\navailable, restarting them if they fail, or moving them to a new host if their host fails. Others put the responsibility for this on the user. Check your vendor documentation to understand how it works so that you can be prepared. [Link to Come] discusses\n\ncontinuity of service.\n\n[Link to Come] goes into much more detail about how servers are provisioned, configured, and managed.\n\nPhysical servers\n\nAs I mentioned earlier, although we tend to think of virtual servers\n\nor containers when we think of cloud, it’s entirely possible to provision physical servers on demand. There are several reasons why you might need this:",
      "content_length": 1407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "To avoid the performance overhead of virtualization. Applications such as real-time trading which demand low latency for processing or data access can benefit from this.\n\nTo allow direct access to physical resources, such as special devices, that might not be accessible from within a virtual machine or container.\n\nTo ensure applications are not impacted by other processes running on the same host as a virtual machine.\n\nTo more strongly enforce segregation of processes and data for security, governance, or contractual requirements.\n\nIf you are the one providing a virtualized or containerized platform to your users, you need to provision and manage the physical host servers your platform runs on.\n\nIMPLEMENTING A BARE-METAL CLOUD\n\nA bare-metal cloud platform lets you write code to install and reinstall OS images onto physical servers automatically. For example, a central process receives a request to provision a server. It:\n\n1. Selects an unused physical server from its inventory,\n\n2. Triggers the server to boot in a “network install” mode supported by the server firmware (e.g., PXE boot),\n\n3. Provides an OS image that the server’s firmware downloads from the network and copies it to the primary hard drive,\n\n4. Reboots the server so that the OS boots,\n\n5. The OS image includes a server configuration agent that runs on startup, retrieves configuration from a server, and applies it.\n\nSome tools that you can use for this process include Cobbler, FAI - Fully Automatic Installation, Foreman, and Crowbar. These tools can take advantage of the PXE specification Preboot Execution Environment to boot a basic OS image downloaded from the network, which then runs an installer to download and boot an OS installer image. The installer image runs a script, perhaps with something like Kickstart, to configure the OS.\n\nOften, triggering a server to use PXE to boot a network image requires pressing a function key while the server starts. Doing this can be tricky to do unattended. However, many hardware vendors have lights-out management (LOM) functionality that makes it possible to do this remotely.",
      "content_length": 2115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Containers\n\nContainers are a popular way to package and deploy applications\n\nonto cloud platforms. They simplify the packaging and\n\ndeployment process by abstracting the details of the system that runs the application. They are also much smaller than a virtual\n\nmachine, which makes them more efficient and faster to start.\n\nMost cloud platforms offer Containers as a Service (CaaS) to deploy and run container instances. Often, you build a container\n\nimage in a standard format (e.g., Docker), and the platform uses\n\nthis to run instances.",
      "content_length": 540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "WHAT IS THE DIFFERENCE BETWEEN A VIRTUAL MACHINE AND A CONTAINER?\n\nA Virtual Machine (VM) contains a complete operating system, while a container shares the operating system kernel with its host system.",
      "content_length": 202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Figure 3-4. The container instances on a host share the same OS kernel\n\nIncluding the entire OS makes a VM heavier than a container, but each VM running on a host can use a different OS than its host. A container is lighter than a VM because it doesn’t need to package or instantiate an OS kernel. However, a container is coupled to the host OS kernel version.\n\nPeople tend to use containers and VMs in different ways. A single VM often runs multiple applications, while a container instance typically runs only one application.\n\nConceptually, containers are an application packaging format. They are a “fat” package, meaning they include all of the runtime dependencies for the application.\n\nMany application runtime platforms are built around containerization. [Link to Come] discusses this in more detail.\n\nContainers require host servers to run on. Some platforms provide\n\nthese hosts transparently, but many require you to define a cluster\n\nand its hosts yourself.\n\nServer clusters\n\nA server cluster is a pool of server instances-either virtual\n\nmachines or physical servers-which the infrastructure platform provisions and manages as a group. You can configure how the\n\nplatform should manage the cluster. Typical configuration properties include a minimum and a maximum number of servers\n\nto run at a time, algorithms and strategies for adding and removing\n\nservers, and specification of what type of servers to use and how to provision them.\n\nThere are two common types of server clusters, a basic server\n\ncluster, and an application hosting cluster.\n\nBASIC SERVER CLUSTERS",
      "content_length": 1581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "A basic server cluster simply runs the appropriate number of\n\nservers. You need to implement the deployment of applications onto each node of the cluster. Typically, every server in a basic\n\ncluster runs an identical set of applications.\n\nEXAMPLES OF BASIC SERVER CLUSTERS PROVIDED BY THE MAJOR CLOUD PLATFORMS\n\nAWS Auto Scaling Group (ASG)\n\nAzure virtual machine scale set\n\nGoogle Managed Instance Group (MIGs)\n\nYou use a basic server cluster to automatically add compute power\n\nwhen demand increases and to remove it when demand drops. You\n\ncan also use clusters for availability because they can automatically rebuild failed instances.\n\nAPPLICATION HOSTING CLUSTER\n\nLike a basic server cluster, an application hosting cluster is a pool of servers, either virtual machines or physical servers. However,\n\nthe platform adds services that deploy and run applications across\n\nthe servers in the cluster.\n\nYou deploy an application to the cluster, defining some properties for the deployment, and the platform works out where to run the\n\napplication.",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Figure 3-5. An application hosting cluster is a group of servers which acts as a single deployment target\n\nUnlike a basic cluster, you typically deploy multiple applications\n\nto an application hosting cluster. The properties you specify for\n\neach application tell the platform how many instances of the application to deploy, with rules for adding and removing\n\ninstances. I go into more detail on application hosting clusters in\n\n[Link to Come].\n\nEXAMPLES OF APPLICATION HOSTING CLUSTERS ON PUBLIC CLOUD PLATFORMS\n\nAmazon Elastic Container Services (ECS)\n\nAmazon Elastic Container Service for Kubernetes (EKS)\n\nAzure Kubernetes Service (AKS)\n\nGoogle Kubernetes Engine (GKE)\n\nServerless code execution (FaaS)\n\nAn infrastructure platform executes serverless code on-demand, in response to an event or schedule, and then terminates it after it has\n\ncompleted its action. Unlike applications deployed on servers or in containers, serverless code does not run continuously, so the\n\nplatform only allocates resources when they are used.\n\nServerless code is useful for well-defined, short-lived actions\n\nwhere the code starts quickly. Typical examples are handling HTTP requests or responding to error events in a message queue.\n\nThe platform launches multiple instances of the code in parallel\n\nwhen needed, for example, to handle multiple events coming in simultaneously.",
      "content_length": 1367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Serverless can be very efficient for workloads where the demand\n\nvaries greatly, scaling up when there are peaks, and not running at all when not needed.\n\n“Serverless” isn’t the most accurate term for this, because of\n\ncourse, the code does run on a server. It’s just that the server is\n\neffectively invisible to you as a developer. The same is true with containers, so what is distinctive about so-called serverless isn’t\n\nthe level of abstraction from servers. The real distinction with\n\nserverless is that it is a short-lived process rather than a long- running process.\n\nFor this reason, many people prefer the term Function as a Service\n\n4 (FaaS) rather than serverless.\n\nSERVERLESS BEYOND CODE\n\nThe serverless concept goes beyond running code. AWS Aurora is essentially a serverless database: the servers involved in hosting the database are transparent to you, unlike more traditional DBaaS offerings like AWS RDS. For that matter, you could consider S3 buckets to be storage as a service.\n\nSome cloud platforms offer specialized on-demand application execution environments. For example, you can use AWS\n\nSageMaker, Azure ML Services, or Google ML Engine to deploy and run machine learning models.\n\nI describe approaches to packaging and deploying serverless code\n\nin [Link to Come].",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "FOODSPIN EXAMPLE: COMPUTE RESOURCES\n\nLet’s look at Foodspin’s use of compute resources. Their tech stack includes a cluster of containerized Nginx web servers, which is shared across all of their customers. Each customer’s ordering website runs as its own Java process deployed onto a Linux virtual machine. They also have several FaaS applications that carry out reporting activities.",
      "content_length": 385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Figure 3-6. Foodspin compute resources\n\nThis example demonstrates how a single system can use multiple types of compute resources. Later examples in this chapter add storage and networking resources to Foodspin’s infrastructure. In later chapters, I use these examples to illustrate how to code, test, and evolve a system.\n\nStorage Resources\n\nMany dynamic systems need storage, such as disk volumes,\n\ndatabases, or central repositories for files. Even if your application\n\ndoesn’t use storage directly, many of the services it does use will need it, if only for storing compute images (e.g., virtual machine\n\nsnapshots and container images).\n\nA genuinely dynamic platform manages and provides storage to\n\napplications transparently. This feature differs from classic virtualization systems, where you need to explicitly specify which\n\nphysical storage to allocate and attach to each instance of compute.\n\nCommon storage resources include:\n\nBlock storage\n\nObject storage\n\nNetworked volumes\n\nStructured storage\n\nSecrets management\n\nI’ll describe each of these in more detail.\n\nBlock storage (virtual disk volumes)",
      "content_length": 1111,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "You can mount a block storage volume on a server or container\n\ninstance as if it was a local disk. Examples of block storage services provided by cloud platforms include AWS EBS, Azure\n\nPage Blobs, OpenStack Cinder, and GCE Persistent Disk.\n\nFigure 3-7. A block storage volume mounted on a virtual machine\n\nSome volumes are ephemeral, automatically created and destroyed\n\nby the platform along with the compute instances that use them.\n\nBut you can also declare persistent volumes, which you can then\n\ncreate and attach, detach, and re-attach to compute instances.\n\nThese volumes can be useful to keep data when you rebuild a\n\nserver.\n\nIt can be useful to replicate volumes for availability scenarios (as\n\ndiscussed in [Link to Come]) and even scalability. For example,\n\nwith some distributed databases, you can add a node to a cluster by\n\nduplicating a disk volume and mounting it on a newly provisioned",
      "content_length": 904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "server. Doing this cuts the time it takes for data to synchronize to\n\nthe new node since it only copies recent changes.\n\nA block volume looks like a local disk drive to an application\n\nrunning on a VM, but depending on the platform implementation,\n\nit could be hosted over a network. The latency for reads and writes\n\nover the network may cause performance issues.\n\nIf performance is a problem, you should research how your\n\nplatform implements block volumes. Run tests to emulate your use\n\ncases, and then tune your configuration and use of storage to get\n\nthe performance you need. Your platform may provide different\n\nstorage options for higher performance, such as solid-state drives or faster I/O.\n\nObject storage\n\nMost infrastructure platforms provide an object storage service,\n\nwhich you can use to store and access files over the network.\n\nAmazon’s S3, Azure Block Blobs, Google Cloud Storage, and OpenStack Swift are all examples.\n\nWhile only one compute instance can mount a block storage\n\nvolume at a time, any instance can access object storage. So object\n\nstorage is useful for creating and sharing files between instances.",
      "content_length": 1137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "Figure 3-8. Multiple compute instances can access object storage\n\nObject storage is usually cheaper and more reliable than block\n\nstorage, but with higher latency.\n\nNetworked filesystems (shared network volumes)\n\nThere are many standard protocols for multiple servers to mount\n\nshared storage volumes locally. Examples include NFS, AFS, and\n\n5\n\nSMB/CIFS (Figure 3-9). Like object storage, and unlike block\n\nstorage, a networked storage volume is available to multiple\n\ncompute instances at the same time. Like block storage, compute instances treat a shared network volume as if it was a local hard\n\ndrive.",
      "content_length": 606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "Figure 3-9. Multiple compute instances can mount the same networked file volume\n\nSome platforms offer networked file systems as a service using\n\nstandard protocols, for example, AWS EFS and Azure File Storage. Alternatively, you can run a file server on compute\n\ninstances you build and manage yourself. Some people do this as\n\npart of a migration strategy, copying legacy systems from data\n\ncenters into the cloud.\n\nHowever, a cloud platform’s managed storage services are\n\noptimized for its physical networking. Optimizing networked file\n\nsharing is challenging even when you manage your own data\n\ncenters and physical networking. When using someone else’s\n\nvirtualized infrastructure, you’ll probably get better performance\n\nand reliability by using their optimized file sharing solutions than you would by building your own.\n\nOf course, it’s always best to test and measure to be sure you’re\n\ngetting the results you need from your system in terms of\n\nperformance and availability.",
      "content_length": 985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "ENCRYPTION AT REST\n\nAll of these storage resource primitives should have encryption capability built in. You can easily set an option in your infrastructure code to encrypt all data stored on the resource “at rest” (as opposed to encrypting “in transit”, when it is passing over the network).\n\nYou should fully understand your threat model and how encryption does and does not address your threats. You especially need to understand how the platform manages the keys to encrypt and decrypt your data. Typically, these keys are available to the cloud platform itself. Anyone with privileged access to your platform (such as employees of your hosting vendor) may be able to gain access to these keys. Anyone with access to manage your infrastructure (such as your infrastructure team members) can use the keys to decrypt the data.\n\nFor example, even if someone is unable to gain direct access to the keys, if they have permission to use an API to read data from the encrypted storage, then the encryption does not give any protection from that person doing intentional or unintentional harm.\n\nKnow how your platform manages encryption keys, and what options you have to control access to them. For example, you may be able to create your own keys and upload them to the platform. Most important, understand your actual threat models, and make sure you understand how to use encryption to provide real value.\n\nStructured data storage\n\nMost hosted infrastructure platforms offer services for storing\n\nstructured data. These are typically Database as a Service (DBaaS) systems managed by the provider. A DBaaS may handle things\n\nlike:\n\nThe server operating system\n\nDatabase software\n\nStorage devices",
      "content_length": 1694,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Scaling clusters\n\nGeographical distribution\n\nReplication\n\nBackups\n\nIdeally, it gives you a certain amount of control over configuration and optimization.\n\nIn addition to the vendor managing the database for you, a DBaaS\n\ntypically integrates with the platform’s other services. For\n\nexample, your database can use the platform’s authentication and authorization, encryption, and monitoring services.\n\nMany DBaaS services offer managed instances of standard\n\ncommercial or open source database applications, such as MySQL,\n\nPostgres, and SQL Server.\n\nSome platforms also offer custom structured data storage services.\n\nExamples include key-value stores and formatted document stores,\n\ne.g., for storing and searching JSON or XML content. These\n\nservices may be optimized to make better use of the underlying infrastructure platform to deliver high performance, geographical\n\ndistribution, and availability.\n\nThe major cloud vendors offer data storage and processing\n\nservices for managing, transforming, and analyzing large amounts of data. These include services for batch data processing, map-\n\nreduce, streaming, indexing and searching, and more. Understand\n\nthe tradeoffs of each of these services, and test them for your\n\nworkloads and use cases, to get the most value from them.",
      "content_length": 1283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Secrets management\n\nAny storage resource can be encrypted so you can store\n\npasswords, keys, and other information which attackers might\n\nexploit to gain privileged access to systems and resources. A\n\nsecrets management service adds functionality specifically\n\ndesigned to help manage these kinds of resources. Features of a secrets management service may include:\n\nAutomatically generating and rotating secrets\n\nGiving access to secrets to automated services so they can carry out controlled actions\n\nTracking and auditing usage of secrets\n\nGenerating short-term, single-use secrets",
      "content_length": 583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "FOODSPIN EXAMPLE: STORAGE RESOURCES\n\nLet’s return to the Foodspin team to see how they added storage resources to their system.\n\nThe Nginx web server container images include configuration files and static content, mainly for Foodspin branding, so they don’t need any persistent local storage. They do send web server logs to object storage. The team can use other tools and services to download the logs from there to analyze user traffic.\n\nEach customer has a virtual server instance running a Java application. Each instance has a block storage volume where the application stores local files. Whenever they rebuild a server, they unmount the volume from the old server and attach it to the new one so that the application can resume service.",
      "content_length": 745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "Figure 3-10. Foodspin storage resources\n\nEach customer application also uses its own PostgreSQL database. Foodspin creates a single Postgress server cluster, using their cloud provider’s DBaaS service. When the team provisions a new customer, they create a new database in this cluster. In the future, they may need more than one cluster to scale the number of customers they can support, but for the moment, this is enough.\n\nNetwork Resources\n\nAs with the other types of infrastructure resources, the capability\n\nof dynamic platforms to provision and change networking on\n\ndemand, from code, creates great opportunities. These\n\nopportunities go beyond changing networking more quickly; they also include much safer use of networking.\n\nPart of the safety comes from the ability to quickly and accurately\n\ntest a networking configuration change before applying it to a critical environment. Beyond this, Software Defined Networking\n\n(or SDN) makes it possible to create finer-grained network\n\nsecurity constructs than you can do manually. This is especially\n\ntrue with systems where you create and destroy elements\n\ndynamically.",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "ZERO-TRUST SECURITY MODEL WITH SDN\n\n6\n\nA zero-trust security model secures every service, application, and other resource in a system at the lowest level. This is different from a traditional perimeter-based security model, which assumes that every device inside a secure network can be trusted.\n\nIt’s only feasible to implement a zero-trust model for a non-trivial system by defining the system as code. The manual work to manage controls for each process in the system would be overwhelming otherwise.\n\nFor a zero-trust system, each new application is annotated to indicate which applications and services it needs to access. The platform uses this to automatically enable the required access and ensure everything else is blocked.\n\nThe benefits of this type of approach include:\n\nEach application and service has only the privileges and access it explicitly requires, which follows the principle of least privilege,\n\nZero-trust, or perimeterless security involves putting smaller-scoped barriers and controls onto the specific resources that need to be protected. This approach avoids the need to allow broad-scoped trust zones, for example, granting trust to every device attached to a physical network. Google documents their approach to this with its BeyondCorp security model.\n\nThe security relationships between elements of the system are visible, which enables validation, auditing, and reporting.\n\nSome typical types of networking constructs and services an\n\ninfrastructure platform may support include:\n\nNetwork address blocks\n\nTraffic management and routing\n\nNetwork access rules\n\nCaches\n\nService meshes\n\nNetwork address blocks\n\nNetwork address blocks are a fundamental structure for grouping\n\nentities involved in network communication. The top-level block in AWS is a VPC (Virtual Private Cloud). In Azure, GCP, and",
      "content_length": 1829,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "others, it’s a Virtual Network. The top-level block is often divided\n\ninto smaller blocks, such as subnets, as shown in Figure 3-11.",
      "content_length": 132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Figure 3-11. Network address blocks\n\nThe primary purpose of address blocks is for routing network traffic. Given a compute instance, you assign it an interface and\n\naddress in an address block. The instance can then communicate\n\nwith other entities in the same block. Traffic routing constructs\n\n(described shortly) enable communication across blocks.\n\nYou can use blocks to segregate entities by defining rules for what\n\nconnections are allowed in and out of a block. It’s common to\n\ndefine blocks in vertical tiers. For example, a “public” tier block\n\nallows connections in from the Internet; a “private” tier block only\n\naccepts connections from the public block. You can improve\n\nsecurity with network access rules (see below), and by securing individual entities in a zero-trust model.\n\nAddress blocks may or may not correspond to physical\n\nnetworking. You can often assume that entities attached to the\n\nsame block (e.g., in the same subnet) have lower latency when communicating with each other than if they were in separate\n\nblocks. If you require low network latency, you should research\n\nhow your platform implements networking and carry out tests for\n\nyour workload to find the best configuration.\n\nAWS also uses its networking blocks (subnets) to indicate the\n\nphysical location, in terms of geography and data centers, of\n\nresources attached to them. Each subnet corresponds to a particular\n\ndata center, with a designated availability zone (AZ). This scheme\n\nhas implications for availability - if AWS has a data center outage, it impacts all of the entities in that subnet. Assigning redundant\n\nresources (e.g., compute instances) to different subnets with",
      "content_length": 1671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "different availability zones spreads the risk across more than one\n\nlocation.",
      "content_length": 77,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "FOODSPIN EXAMPLE: ADDRESS BLOCKS\n\nOur friends at Foodspin use a single high level address block (a VPC) for everything running in a single geographical region. They divide this into smaller blocks (subnets).\n\nThey use three subnets for their web server containers. They have configured the cluster to provision a host node in each subnet. Because their cloud provider uses a different physical data center for each subnet, this arrangement spreads web server workload across different locations, which improves resilience. Although a data center failure will take down some number of customer application servers, as I’ll describe in a moment, they can still serve a branded error page.",
      "content_length": 686,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Figure 3-12. Foodspin address blocks\n\nFoodspin creates two more subnets for the application servers. They provision each new server in one of these subnets, and the team tries to keep them spread reasonably evenly across them.\n\nIf one of their cloud vendor’s data centers fails, the Foodspin team can create a new application server for each affected customer. Because these servers require persistent data, this only works if they either have access to the data volumes or recent backups. [Link to Come] discusses ways they can approach this, as well as better solutions for availability.\n\nTraffic management and routing\n\nThere are many ways to route and manage network traffic. These\n\ninclude:\n\nNames, such as DNS entries, which are mapped to lower- level network addresses, usually IP addresses,\n\nRoutes, which configure what traffic is allowed between and within address blocks,\n\nGateways and NAT (Network Address Translation) gateways, which may be needed to direct traffic in and out of blocks, depending on the platform,\n\nLoad balancers, which forward each connection coming into a single address to one of a pool of resources,\n\nProxies, which accept connections, and may carry out transformations or routing based on different aspects of the connection,\n\nAPI gateways are proxies, usually for HTTP/S connections, which can carry out activities to handle non- core aspects of APIs, such as authentication and rate throttling. See also “Service meshes”,\n\nVPNs (Virtual Private Networks), which allow different address blocks to be connected across locations so that they appear to be part of a single network,",
      "content_length": 1615,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Direct connections, a dedicated connection set up between a cloud network and another location, typically a data center or office network.\n\nOne of the issues with traffic routing in a dynamic system is that the target locations may change. For example, if you run a cluster\n\nof web servers as described in “Foodspin Example: Address\n\nblocks”, there are times when you need to rebuild or even move\n\nthe cluster, which can change its IP address.\n\nDESIGNING FOR DISPOSABILITY MEANS CHANGES ARE ROUTINE RATHER THAN CATASTROPHIC\n\nThis is a concrete example of the disposability principle (“Principle: Create disposable things”). The traditional (Iron Age) approach assumes that a cluster such as the one in this example will not change very often. When it does need to be changed, it’s a disruptive event, which probably involves downtime, people working overnight, and manually fixing problems discovered after the cutover.\n\nThe Cloud Age approach recognizes that you need to make changes to a system such as a web server cluster frequently. When a security vulnerability requires applying a patch, when a system upgrade is released, or when you need to make a configuration change to improve performance or reliability, you would like to be able to carry this out quickly and with little work, rather than needing to undertake a major project.\n\nFor this reason, you design your systems assuming that any component will go away. This not only makes it safer to carry out planned changes, it also makes it easier to handle unplanned failures.\n\nYou can use dynamic routing resources to continue routing traffic\n\nto a resource after its IP address changes. A few options that you\n\ncan use to do this include:",
      "content_length": 1701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Define DNS entries as code and integrate this code with the code that defines the cluster. When you change the cluster, you can automatically update the DNS entries by re-applying the code.\n\nDefine a static IP address (e.g., AWS Elastic IP or ENI), and direct external traffic to this address rather than to the cluster’s address. You can map the static address to the new cluster when it changes. As with the DNS-based solution, it takes little effort to safely apply this change across the resources when you have defined them as code.\n\nCreate a load balancer, again defining it as code and connecting it to the cluster configuration. Doing this has the added benefit that you may be able to create the new cluster while the old cluster is still running, giving you options for handling the change with little or no downtime (see [Link to Come]).",
      "content_length": 848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "FOODSPIN EXAMPLE: NETWORK ROUTING\n\nGiven the address blocks described earlier (“Foodspin Example: Address blocks”), the team at Foodspin need some routing. This diagram shows how they’ve configured some of this:",
      "content_length": 211,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Figure 3-13. Foodspin network routing\n\nThey have a DNS name for each of their customers, which maps to a public IP addresses on the internet gateway. Their gateway routes connections, based on which address it’s coming into, to a load balancer, which distributes requests among servers in a pool.\n\nThe details of networking are outside the scope of this book, so\n\ncheck the documentation for your platform provider, and perhaps a reference such as Craig Hunt’s TCP/IP Networking Adminstration.\n\nNetwork access rules\n\nInfrastructure platforms should provide a way to define access rules as code. These may map directly to physical firewall rules, or\n\nthe platform may use virtual firewall mechanisms. Some platforms impose these directly on compute instances.\n\nDefining these rules in code makes it easier to secure dynamic\n\nresources, and to secure them at a finer level of granularity than with other means. For example, your system may include\n\napplication servers running in a single network segment. This arrangement permits connections between application servers, which an attacker who compromises one server can exploit to gain\n\naccess to the others, as shown in Figure 3-14.",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "Figure 3-14. When a bad actor gains access, they are trusted by everything\n\nInstead, you can define access rules for each application server to restrict access to between servers, as shown in Figure 3-15.",
      "content_length": 204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Figure 3-15. A bad actor who gains access to one server cannot access the others\n\nIn Chapter 6, I’ll explain how to write code that you can reuse to\n\ndefine multiple instances of infrastructure. Reusable code makes it easier to secure each instance at the right level.\n\nCaches\n\nSome services cache content to improve latency. A CDN (Content Distribute Network) is a service that can distribute static content\n\n(and in some cases executable code) to multiple locations geographically, usually for content delivered using HTTP/S.\n\nDefining caching and CDN services as infrastructure code can make it easier to integrate other networking services. For example, code that defines a CDN can automatically update the CDN\n\nconfiguration when the content server is rebuilt or moves.\n\nService meshes\n\nA service mesh is a decentralized network of services that dynamically manages connectivity between parts of a distributed system. It moves networking capabilities from the infrastructure\n\nlayer to the application runtime layer of the model I described in “The parts of an infrastructure system”. In a typical service mesh\n\nimplementation, each application instance delegates communication with other instances to a sidecar process.",
      "content_length": 1224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Figure 3-16. Sidecars enable communication with other processes in a service mesh\n\nSome of the services that a service mesh can provide to applications include:\n\nRouting - Direct traffic to the most appropriate instance of a given application, wherever it is currently running. Dynamic routing with a service mesh enables advanced deployment scenarios, such as blue-green and canary, as described in [Link to Come].\n\nAvailability - Enforce rules for limiting numbers of requests, for example, circuit breakers.\n\nSecurity - handle encryption, including certificates.\n\nAuthentication - enforce rules on which services can connect to which. Manage certificates for peer to peer authentication.\n\nObservability, monitoring, and troubleshooting - record connections and other events so that people can trace requests through complex distributed systems.\n\nA service mesh works well in combination with application\n\nhosting clusters (“Application hosting cluster”). The application cluster decoupled the deployment of applications from the infrastructure layer. The service mesh then decouples application\n\ncommunication from the infrastructure layer. The benefits of this model are:\n\nSimplify application development, by moving common concerns out of the application and into the sidecar,\n\nMake it easier to build and improve common concerns across your estate, since you only need to deploy updates to the sidecar, without needing to make code changes to all of your applications and services,",
      "content_length": 1487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Handle the dynamic nature of application deployment, since the same orchestration and scheduling system that deploys and configures application instances (e.g., in containers) can deploy and configure the sidecar instances along with them.\n\nSome examples of service meshes include Hashicorp Consul, Envoy, Istio, and Linkerd.\n\nService meshes are most commonly associated with containerized\n\nsystems. However, you can implement the model in non- containerized systems, for example, by deploying sidecar processes onto virtual machines.\n\nA service mesh does add complexity. As with cloud-native architectural models like microservices, a service mesh is\n\nappealing because it simplifies the development of individual applications. However, the complexity does not disappear; you’ve only moved it out into the infrastructure. So your organization\n\nneeds to be prepared to manage this, including being ready for a steep learning process.\n\nIt’s essential to keep clear boundaries between networking\n\nimplemented at the infrastructure level, and networking implemented in the service mesh. Without a good design and\n\nimplementation discipline, you may duplicate and intermingle concerns. Your system is harder to understand, riskier to change,\n\nand harder to troubleshoot.",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "MULTICLOUD, POLYCLOUD, HYBRID CLOUD\n\nMay organizations end up hosting across multiple platforms. A few terms crop up to describe variations of this:\n\nHybrid Cloud\n\nHosting applications and services for a system across both private infrastructure and a public cloud service. People often do this because of legacy systems that they can’t easily migrate to a public cloud service (such as services running on mainframes). In other cases, organizations have requirements that public cloud vendors can’t currently meet, such as legal requirements to host data in a country where the vendor doesn’t have a presence.\n\nCloud Agnostic\n\nBuilding systems so that they can run on multiple public cloud platforms. People often do this hoping to avoid lock-in to one vendor. In practice, this results in locks-in to software that promises to hide differences between clouds; or involves building and maintaining vast amounts of customized code; or both.\n\nPolycloud\n\nRunning different applications, services, and systems on more than one public cloud platform. This is usually to exploit different strengths of different platforms.\n\nConclusion\n\nThe different types of infrastructure resources and services\n\ncovered in this chapter are the pieces that you arrange into useful systems - the “infrastructure” part of Infrastructure as Code. In the\n\nnext chapter, I explain the first of the three core practices for infrastructure as code (define everything as code, validate continuously, use small pieces). Then I combine that practice with\n\nthis chapter’s discussion of cloud platforms to explore patterns for using code to define infrastructure stacks (Chapter 5).\n\n1 The US National Institute of Standards and Technology (NIST) has an excellent\n\ndefinition of cloud computing.\n\n2 Here’s how NIST defines Infrastructure as a Service: “The capability provided to the consumer is to provision processing, storage, networks, and other fundamental computing resources where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer",
      "content_length": 2081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls).”\n\n3 Virtualization technology has been around since the 1960s, but emerged into the\n\nmainstream world of x86 servers in the 2000s.\n\n4 I tend to use serverless. FaaS is more technically accurate, but when I say FaaS, I usually have to explain what it means. The easiest way to explain what FaaS means is to say, “serverless.” I’m lazy, so I find it simpler to say “serverless” in the first place.\n\n5 Network File System, Andrew File System, and Server Message Block,\n\nrespectively.\n\n6 Google documents their approach to zero-trust (also known as perimeterless) with\n\ntheir BeyondCorp security model",
      "content_length": 821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Chapter 4. Core Practice: Define everything as code\n\nIn Chapter 1, I identified three core practices that help you to change infrastructure rapidly and reliably:\n\nDefine everything as code\n\nContinuously validate all work in progress\n\nBuild small, simple pieces that you can change independently\n\nIn this chapter, I explore the first of these core practices. Why would you want to define your infrastructure as code? What do you need to do this? Then I explore some issues about the nature\n\nof infrastructure coding languages.\n\nThis seemingly banal subject is a hot topic in the industry at the time of this writing, with debate raging over the nature of configuration vs. coding, and special-purpose languages vs.\n\nstandard programming languages.\n\nI’ll close this chapter with some implementation principles to\n\nguide you in deciding how to organize your infrastructure code.\n\nThe goal of this chapter is to lay out the fundamental concepts of coding infrastructure. With these in place, later chapters offer specific patterns and recommendations for implementation. I\n\nexplain how to organize infrastructure resources into useful units",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "that I call stacks in Chapter 5 and the following chapters. I expand\n\nthese to servers, clusters, and other application runtime platforms in later chapters.\n\nWhy you should define your infrastructure as code\n\nEven given a dynamic cloud platform, there are simpler ways to provision infrastructure than writing code and running a tool. Go\n\nto the platform’s web-based user interface and poke and click an application server cluster into being. Drop to the prompt, and using your command-line prowess, wield the vendor’s CLI (Command-\n\nLine Interface) tool to forge an unbreakable network boundary.\n\nBut seriously, the previous chapters have explained why it’s better to use code to build your systems. As a quick recap of “Core practice: Define everything as code”, some of the benefits of defining things as code are:\n\nReusability\n\nIf you define a thing as code, you can create many instances of it. You can repair and rebuild your things quickly. Other people can build identical instances of the thing.\n\nConsistency\n\nThings built from code are built the same way every time. This makes system behavior predictable. This makes testing more reliable. This enables continuous validation.\n\nTransparency\n\nEveryone can see how the thing is built by looking at the code, which helps in many ways. People can review the code and",
      "content_length": 1322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "suggest improvements. They can learn things to use in other code. They gain insight to help with troubleshooting. They can review and audit for compliance.\n\nWhat you can define as code\n\nEvery infrastructure tool has a different name for its source code-\n\nfor example, playbooks, cookbooks, manifests, and templates. I refer to these in a general sense as infrastructure code, or sometimes as an infrastructure definition.\n\nInfrastructure code specifies both the infrastructure elements you want and how you want them configured. You run an infrastructure tool to apply your code to an instance of your\n\ninfrastructure. The tool either creates new infrastructure, or it modifies existing infrastructure to match what you’ve defined in your code.\n\nSome of the things you might define as code include:\n\nAn infrastructure stack is a collection of elements provisioned from an infrastructure cloud platform. I write about infrastructure platforms in Chapter 3, and discuss stacks in Chapter 5.\n\nElements of a server’s configuration, such as packages, files, user accounts, and services. ([Link to Come])\n\nA server role is a collection of server elements that are applied together to a single server instance ([Link to Come])\n\nA server image definition generates an image for building multiple server instances. ([Link to Come])",
      "content_length": 1322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "An application package defines how to build a deployable application artifact, including containers. ([Link to Come])\n\nConfiguration and scripts for delivery services, which include pipelines and deployment. ([Link to Come])\n\nConfiguration for operations services, such as monitoring checks. ([Link to Come])\n\nValidation rules, which include automated tests and compliance rules. (Chapter 9)\n\nChoose tools that are configured with code\n\nInfrastructure as Code, by definition, involves specifying your infrastructure in text-based files. You manage these files separately\n\nfrom the tools that you use to apply them to your system. You can read, edit, analyze, and manipulate your specifications using any\n\ntools you want.\n\nOther infrastructure automation tools store your infrastructure\n\nspecifications as data that you can’t access directly. Instead, you can only use and edit the specifications by using the tool itself.\n\nThe tool may have some combination of GUI, API, and command- line.\n\nThe issue with these black-box tools is that they limit the practices\n\nand workflows you can use:\n\nYou can only version your infrastructure specifications if the tool has built-in versioning,\n\nYou can only use Continuous Integration if the tool has a way to trigger a job automatically when you make a change,",
      "content_length": 1300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "You can only create delivery pipelines if the tool makes it easy to version and promote your infrastructure specifications,\n\nYou can only split monolithic infrastructure into independent pieces if the tool supports it.\n\nLESSONS FROM SOFTWARE SOURCE CODE\n\nThe externalized configuration pattern mirrors the way most software source code works. Some development environments keep source code hidden away, such as Visual Basic for Applications. But for non-trivial systems, developers prefer keeping their source code in external files.\n\nIt is challenging to use agile engineering practices such as Test-\n\nDriven Development, Continuous Integration, and Continuous Delivery with black-box infrastructure management tools.\n\nA tool that uses external code for its specifications doesn’t\n\nconstrain you to use a specific workflow. You can use an industry- standard source control system, text editor, CI server, and\n\nautomated testing framework. You can build delivery pipelines\n\nusing the tool that works best for you.",
      "content_length": 1013,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "AUTOMATING BLACK BOX CONFIGURATION\n\nSometimes a black-box tool’s configuration can be exported and imported by unattended scripts. This might make it possible to integrate the tool with infrastructure as code, although it will be clumsy. It’s certainly not recommended for a core infrastructure tool, but could be useful for a specialized tool that doesn’t have a good alternative.\n\nThis model has some limitations. It’s often difficult to merge changes from different dumps, which means different team members can’t work on different parts of the configuration at the same time. People must be careful about making changes to downstream instances of the tool to avoid conflicts with upstream work.\n\nDepending on the format of the configuration dump, it may not be very transparent-it may not be easy to tell at a glance the differences between one version and the next.\n\nAnother way to approach black-box configuration is by injecting configuration automatically. Ideally, this would be done using an API provided by the tool. I have also seen this done by automating interaction with the tool’s UI—for example, using scripted tools to make HTTP requests and post forms. But UI automation is brittle, breaking whenever a new version changes the UI. A well-supported API keeps backward compatibility, and so is more reliable.\n\nThe configuration injection approach allows configuration to be defined in external files. These may simply be scripts, or could involve a configuration format or DSL. If a tool is configured by automated injection from external files, then this should be the only way configuration changes are made. Changes made through the UI are likely to cause conflicts and inconsistencies.\n\nManage your code in a version control system\n\nIf you’re defining your stuff as code, then putting that code into a\n\nversion control system (VCS) is simple and powerful. By doing this, you get:\n\nTraceability\n\nVCS provides a history of changes, who made them, and context about why . This history is invaluable when debugging problems.\n\n1\n\nRollback\n\nWhen a change breaks something—and especially when multiple changes break something—it’s useful to be able to restore things to exactly how they were before.",
      "content_length": 2213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Correlation\n\nKeeping scripts, specifications, and configuration in version control helps when tracing and fixing gnarly problems. You can correlate across pieces with tags and version numbers.\n\nVisibility\n\nEveryone can see each change committed to the version control system, giving the team situational awareness. Someone may notice that a change has missed something important. If an incident happens, people are aware of recent commits that may have triggered it.\n\nActionability\n\nThe VCS can trigger an action automatically for each change committed. Triggers enable CI jobs and CD pipelines.\n\nRECOMMENDATION: AVOID BRANCHING\n\nBranching is version control system feature that allows people to work on code in separate streams. There are many popular workflows based around branches. I’ll explain how these workflows can conflict with the core practice of Continuous Integration in [Link to Come].\n\nSecrets and source code\n\nSystems need various secrets. Your stack tool may need a password or key to use your platform’s API to create and change infrastructure. You may also need to provision secrets into environments, for example making sure an application has the\n\npassword for its database.\n\nIt’s essential to handle these types of secrets in a secure way from the very beginning. Whether you are using a public cloud or a",
      "content_length": 1327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "private cloud a leaked password can have terrible consequences. So even when you are only writing code to learn how to use a new\n\ntool or platform, you should never put secrets into code. There are many stories of people who checked a secret into a source repository they thought was private, only to find it had been\n\ndiscovered by hackers who exploited it to run up huge bills.\n\nOne solution to this is to encrypt secrets in order to store them in code . Infrastructure developers and unattended systems need to be able to decrypt these secrets without storing a secret. So you still have at least one secret to manage outside of source control!\n\n2\n\nThere are a few approaches for handling secrets needed by\n\ninfrastructure code without actually putting them into code. The include secretless authorization, runtime secret injection, and disposable secrets.\n\nSECRETLESS AUTHORIZATION\n\nMany services and systems provide ways to authorize actions without using secrets. Most cloud platforms can mark a compute\n\nservice-such as a virtual machine or container instance-as authorized for privileged actions.\n\nFor example, an AWS EC2 instance can be assigned an IAM profile that gives processes on the instance rights to carry out a set of API commands. If you configure a stack management tools to\n\nrun on one of these instances, you avoid the need to manage a secret that might be exploited by attackers.\n\nIn some cases, secretless authorization can be used to avoid the need to provision secrets on infrastructure when it is created. For",
      "content_length": 1536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "example, an application server might need to access a database instance. Rather than a server configuration tool provisioning a\n\npassword onto the application server, the database server might be configured to authorize connections from the application server, perhaps based on its network address.\n\nTying privileges to a compute instance or network address only shifts the possible attack vector. Anyone gaining access to that\n\ninstance can exploit those privileges. You need to put in the work to protect access to privileged instances. On the other hand, someone gaining access to an instance may be able to access\n\nsecrets stored there, so giving privileges to the instance may not be any worse. And a secret can potentially be exploited from other locations, so removing the use of secrets entirely is generally a good thing.\n\nINJECTING SECRETS AT RUNTIME\n\nWhen you can’t avoid using secrets for stacks or other infrastructure code, you can explore ways to inject secrets at\n\nruntime. You’ll normally implement it as stack parameters, which is the topic of Chapter 8. I describe the details of handling secrets as parameters with each of the patterns and antipatterns in that\n\nchapter.\n\nThere are two different runtime situations to consider, local development and unattended agents. People who work on infrastructure code will often keep secrets in a local file that isn’t\n\n3\n\nstored in version control. The stack tool could read that file directly, especially appropriate if you’re using the stack configuration file pattern (“Pattern: Stack Configuration Files”). Or the file could be a script that sets the secrets in environment",
      "content_length": 1638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "variables, which works well with the stack environment variables pattern (“Pattern: Stack Environment Variables”).\n\nThese approaches also work on unattended agents, such as those\n\n4\n\nused for CI testing or CD delivery pipelines . But you need to store the secrets on the server or container that runs the agent. Alternatively, you can use secrets management features of your agent software to provide secrets to the stack command, as with\n\nthe pipeline stack parameters pattern (“Pattern: Pipeline Stack Parameters”). Another option is to pull secrets from a secrets management service (of the type described in “Secrets\n\nmanagement”), which aligns to the stack parameter registry pattern (“Pattern: Stack Parameter Registry”).\n\nDISPOSABLE SECRETS\n\nA cool thing you can do with dynamic platforms is to create secrets on the fly, and only use them on a “need-to-know” basis. In the database password example, the code that provisions the database automatically generates a password and passes it to the\n\ncode that provisions the application server. Humans don’t ever need to see the secret, so it’s never stored anywhere else.\n\nYou can apply the code to reset the password as needed. If the application server is rebuilt, you can re-run the database server\n\ncode to generate a new password for it.\n\nSecrets management services, such as Hashicorp Vault, can also generate and set a password in other systems and services on the fly. It can then make the password available either to the stack tool when it provisions the infrastructure, or else directly to the\n\nservices that uses it, such as the application server.",
      "content_length": 1614,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "Infrastructure coding languages\n\nSystem administrators have been using scripts to automate infrastructure management tasks for decades. General-purpose scripting languages like Bash, Perl, Powershell, Ruby, and Python\n\nare still an essential part of an infrastructure team’s toolkit.\n\nCFEngine pioneered the use of declarative, Domain Specific Languages (DSL - see “DSLs for infrastructure” a bit later) for infrastructure management. Puppet and then Chef emerged\n\nalongside mainstream server virtualization and IaaS cloud. Ansible, Saltstack, and others followed.\n\nStack-oriented tools like Terraform and CloudFormation arrived a few years later, following the same declarative DSL model.\n\n5\n\nRecently , there is a trend of new infrastructure tools that use\n\nexisting general-purpose programming languages to define infrastructure. Pulumi and the AWS CDK (Cloud Development Kit) are examples, supporting languages like Typescript, Python, and Java. These newer languages use procedural language\n\nstructures rather than being declarative.\n\nThe principles, practices, and patterns in this book should be relevant regardless of what language you use to implement them. The languages we use should make it easy to write code that is\n\neasy to understand, test, maintain, and improve. Let’s review the evolution of infrastructure coding languages and consider how different aspects of language choice affect this goal.\n\nScripting your infrastructure",
      "content_length": 1444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "Before standard tools appeared for provisioning cloud infrastructure declaratively, we wrote scripts in general-purpose, procedural languages. These used SDK (Software Development\n\nKit) libraries to interact with the cloud provider’s API.\n\nExample 4-1 uses pseudo-code, but is similar to scripts that I wrote in Ruby with the AWS SDK. It creates a server named my_application_server and then runs the (fictional) servermaker tool to configure it.\n\nExample 4-1. Example of procedural code that creates a server import 'cloud-api-library'\n\nnetwork_segment = CloudApi.find_network_segment('private')\n\napp_server = CloudApi.find_server('my_application_server') if(app_server == null) { app_server = CloudApi.create_server( name: 'my_application_server', image: 'base_linux', cpu: 2, ram: '2GB', network: network_segment ) while(app_server.ready == false) { wait 5 } app_server.provision( provisioner: servermaker, role: tomcat_server ) }\n\nThis script combines what and how. It specifies attributes of the\n\nserver, including the CPU and memory resources to provide it, what OS image to start from, and what Ansible role to apply to the\n\nserver. It also implements logic: it checks whether the server",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "named my_application_server already exists, to avoid creating a duplicate server, and then it waits for the server to become ready\n\nbefore running Ansible on it. The script would need additional logic to handle errors, which I didn’t include in the example.\n\nThe example code also doesn’t handle changes to the server’s\n\nattributes. What if you need to increase the RAM? You could\n\nchange the script so that if the server exists, the script will check each attribute and change it if necessary. Or you could write a new\n\nscript to find and change existing servers.\n\nMore realistic scenarios include multiple servers of different types.\n\nIn addition to our application server, my team had web servers and database servers. We also had multiple environments, which meant\n\nmultiple instances of each server.\n\nTeams I worked with often turned simplistic scripts like the one in this example into a multi-purpose script. This kind of script would\n\ntake arguments specifying the type of server and the environment,\n\nand use these to create the appropriate server instance. We evolved this into a script that would read configuration files that specify\n\nvarious server attributes.\n\nI was working on a script like this, wondering if it would be worth\n\nreleasing it as an open-source tool, when Hashicorp released the first version of Terraform.\n\nBuilding infrastructure with declarative code\n\nTerraform, like most other stack provisioning tools and server configuration tools, uses a declarative language. Rather than a\n\nprocedural language, which executes a series of statements using",
      "content_length": 1577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "control flow logic like if statements and while loops, a declarative\n\nlanguage is a set of statements that declare the result you want.\n\nExample 4-2 creates the same server as Example 4-1. The code in\n\nthis example (as with most code examples in this book) is a 6 fictional language .\n\nExample 4-2. Example of declarative code virtual_machine: name: my_application_server source_image: 'base_linux' cpu: 2 ram: 2GB network: private_network_segment provision: provisioner: servermaker role: tomcat_server\n\nThis code doesn’t include any logic to check whether the server\n\nalready exists or to wait for the server to come up before running Ansible. The tool that applies the code implements this logic,\n\nalong with error-handling. The tool also checks the current\n\nattributes of infrastructure against what is declared, and work out what changes to make to bring the infrastructure in line. So to\n\nincrease the RAM of the application server in this example, you can edit the file and re-run the tool.\n\nDeclarative infrastructure tools like Terraform and Chef keep the what and how separate. The code you write as a user of the tool\n\nonly declares the attributes of your infrastructure. The tool implements the logic for how to make it happen. As a result, your\n\ncode is cleaner and more direct.",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "IDEMPOTENCY\n\nTo continuously synchronize system definitions, the tool you use for this must be idempotent. No matter how many times you run it, the outcome is the same. If you run a tool that isn’t idempotent multiple times, it might make a mess of things.\n\nHere’s an example of a shell script that is not idempotent:\n\necho \"spock:*:1010:1010:Spock:/home/spock:/bin/bash\" \\ >> /etc/passwd\n\nIf you run this script once you get the outcome you want: the user spock is added to the /etc/passwd file. But if you run it ten times, you’ll end up with ten identical entries for this same\n\nuser.\n\nWith an idempotent infrastructure tool, you specify how you want things to be:\n\nuser: name: spock full_name: Spock uid: 1010 gid: 1010 home: /home/spock shell: /bin/bash\n\nNo matter how many times you run the tool with this code, it will ensure that only one entryexists in the /etc/passwd file for the user spock. No unpleasant side effects.\n\nDSLs for infrastructure\n\nIn addition to being declarative, infrastructure tools often use their\n\n7 own DSL .\n\nThe advantage of a DSL for infrastructure code is keeping the code simple and focused.\n\nA general-purpose language needs extra syntax, such as\n\ndeclarations of variables and references to class structures in a DSL:\n\nimport 'cloud-api-library' app_server = CloudApi.find_server('my_application_server')",
      "content_length": 1343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "if(app_server == null) { app_server = CloudApi.create_server(name: 'my_application_server')\n\nA DSL can strip this to the most relevant elements:\n\nvirtual_machine: name: my_application_server\n\nThe return of general-purpose languages for infrastructure\n\nNewer tools, such as Pulumi and the AWS CDK, bring general-\n\npurpose languages back to infrastructure coding. There are several\n\narguments for doing this, some more convincing than others.\n\nConfiguration isn’t real code\n\nSome folks take the phrase “Infrastructure as Code” to heart. They argue that declarative languages are just configuration, not a “real” language. Personally, I’m not bothered if someone disparages my code as being mere configuration. I still find it useful to keep “what” and “how” separate, and to avoid writing repetitive, verbose code.\n\nIt’s useful not to have to learn a new language\n\nUsing a popular language like JavaScript means more people can learn to write infrastructure as code since they don’t need to learn a peculiar special-purpose language. I have sympathy for making it easy for people to adopt infrastructure as code. But I don’t think a new language syntax, especially a simple declarative language, is as hard to learn as the domain-specific aspects of infrastructure code like networking constructs.\n\nInfrastructure DSLs are not well-supported by development tools\n\n8\n\nThis is generally true. There are many mature IDE’s for languages like JavaScript, Python, and Ruby. These have loads of features, like syntax highlighting and code refactoring, that",
      "content_length": 1547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "help developers to be more productive. Rather than discouraging the use of new languages (of any kind), I would love to see vendors improve their support for infrastructure coding languages.\n\nProper support for libraries helps you to simplify code\n\nMost infrastructure DSLs let you write modules (as I’ll discuss in Chapter 6). But these are not as rich and flexible as libraries in mature, procedural languages like JavaScript, Python, and Ruby. I discuss reusable modules and libraries for stacks in Chapter 6.\n\nUsing the same language lets you combine infrastructure and application code\n\nExamples of this show application code that provisions its own infrastructure. People who’ve been developing web applications for a while recall how nifty it was to be able to embed SQL statements into HTML code. Experience has shown that this does not lead to cleanly designed, maintainable systems. It is often useful to specify actions for a tool to trigger on specific changes to infrastructure, such as provisioning newly created resources. But this can be done without intermingling the different styles of code.\n\nInfrastructure languages are less testable\n\nAlso true, although there are several reasons for this. One is that the ecosystem of testing tools for infrastructure is not as mature as that for other types of code. Another reason is that testing declarations requires a different approach to testing logic. A third reason is that testing code that provisions infrastructure has much longer feedback loops. I delve into these in “Challenges with testing infrastructure code”.\n\nThe swing back towards general-purpose languages for infrastructure is new . Some of the arguments threaten to regress\n\n9\n\nus to codebases filled with verbose spaghetti code mingling",
      "content_length": 1767,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "configuration, application logic, and repetitive utility code. But I\n\nexpect that this is just one step on a path to languages that support better coding.\n\nFor many teams today, the challenges with their codebase do not\n\ncome from the language they use. Regardless of language, they\n\nneed ways to keep their code clean, well-organized, and easy to maintain.\n\nImplementation Principles for defining infrastructure as code\n\nTo update and evolve your infrastructure systems easily and safely,\n\nyou need to keep your codebase clean: easy to understand, test,\n\nmaintain, and improve. Code quality is a familiar theme in software engineering. The following implementation principles are\n\nguidelines for designing and organizing your code to support this\n\ngoal.\n\nImplementation Principle: Avoid mixing different types of code\n\nAs discussed earlier, some infrastructure coding languages are\n\n10\n\nprocedural, and some are declarative . Each of these paradigms has its strengths and weaknesses.\n\nA particular scourge of infrastructure is code that mixes both\n\ndeclarative and procedural code, as in Example 4-3. This code\n\nincludes declarative code for defining a server as well as procedural code that determines which attributes to assign to the\n\nserver depending on the server role and environment.",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Example 4-3. Example of mingled procedural and declarative code for ${env} in [\"test\", \"staging\", \"prod\"] { for ${server_role} in [\"app\", \"web\", \"db\"] { server: name: ${server_role}-${env} image: 'base_linux' cpu: if(${env} == \"prod\" || ${env} == \"staging\") { 4 } else { 2 } ram: if(${server_role} == \"app\") { \"4GB\" } else if (${server_role} == \"db\") { \"2GB\" } else { \"1GB\" } provision: provisioner: servermaker role: ${server_role} } }\n\nMixed declarative and procedural code is a design\n\n11\n\nsmell https://martinfowler.com/bliki/CodeSmell.html. A “smell” is some characteristic of a system that you observe that suggests\n\nthere is an underlying problem. In the example from the text, code\n\nthat mixes declarative and procedural constructs is a smell. This smell suggests that your code may be trying to do multiple things\n\nand that it may be better to pull them apart into different pieces of\n\ncode, perhaps in different languages.]. It’s not easy to understand this code, which makes it harder to debug, and harder to change\n\nwithout breaking something.\n\nThe messiness of this code comes from intermingling two different\n\nconcerns, which leads to the next implementation principle.",
      "content_length": 1183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Implementation Principle: Separate infrastructure code concerns\n\nA common reason for messy code is that it is doing multiple\n\nthings. These things may be related, but teasing them apart can make them easier to distinguish, and improve the readability and\n\nmaintainability of the code.\n\nExample 4-3 does two things: it specifies the infrastructure\n\nresource to create, and it configures that resource differently in different contexts. The example illustrates two of the four most\n\ncommon concerns of infrastructure code:\n\nSpecification\n\nSpecifications define the shape of your infrastructure. Your server has specific packages, configuration files, and user accounts. Declarative languages work well for this because specifications are what you want. Specifications are what most people mean when they talk about infrastructure code.\n\nConfiguration\n\nConfiguration defines the things that vary when you provision different instances of infrastructure. Different application servers built from a single specification may need different amounts of RAM. You may want to deploy different applications onto otherwise identical servers. Configuration is almost always declarative.\n\nExecution\n\nExecution applies specification and configuration to the actual infrastructure resources. Together, the specification and configuration declare what you want. The orchestration is about how to make that happen. Procedural or functional code usually works best for orchestration. Ideally, you should use an off the shelf tool for this rather than writing your own code.",
      "content_length": 1554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "Orchestration\n\nOrchestration combines multiple specifications and configurations. For example, you may need to create a server in the cloud platform, and then install packages on the server. Or you may need to create networking structures, and then create multiple servers attached to those structures.\n\nSEPARATING SPECIFICATION AND CONFIGURATION\n\nLet’s take a simple example of code that mixes concerns and split\n\nit into two cleaner pieces of code. This example specifies an\n\napplication server, assigning it to a different network depending on which customer uses it:\n\nvirtual_machine: name: application_server_${CUSTOMER} source_image: 'base_linux' provision: tool: servermaker role: foodspin_application network: $(switch ${CUSTOMER}) { \"bomber_burrito\": us_network \"curry_hut\": uk_network \"burger_barn\": au_network }\n\nThe code uses the CUSTOMER parameter to choose the network for the server.\n\nThis code is hard to test because any test instance needs to specify\n\na customer. Using a real customer couples your test code with that\n\ncustomer’s configuration, which makes the tests brittle. You could create a test customer. But then you need to add the configuration\n\nfor your fake customer to your infrastructure code. Mingling code",
      "content_length": 1238,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "and configuration between test and production descends even\n\nfarther into the depths of poor code.\n\nLet’s go in the opposite direction, and pull the specification into its own code:\n\nvirtual_machine: name: application_server_${CUSTOMER} source_image: 'base_linux' provision: tool: servermaker role: foodspin_application network: ${NETWORK}\n\nThis code is more straightforward than the previous example. It\n\nonly includes code for the things which are common to all\n\napplication server instances. Because this doesn’t vary, we don’t need logic, so this fits nicely as declarative code.\n\nThe only parts of the specification which vary are set using variables, CUSTOMER to give the server a unique name, and NETWORK to assign it to a unique network. There are different patterns for assigning these variables, which are the subject of an\n\nentire chapter of this book (Chapter 8).\n\nTo illustrate the separation of concerns, let’s use a script to work\n\nout the configuration for the server. The script works out which network to assign the server to based on the environment, as\n\nbefore:\n\nswitch (${CUSTOMER}) { case 'bomber_burrito': return 'us_network' case 'curry_hut': return 'uk_network' case 'burger_barn':",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "return 'au_network' }\n\nEach of these two pieces of code is easier to understand. If you need to add a new customer, you can easily add it to the\n\nconfiguration code, without confusing things. This example doesn’t really need procedural code to work out the configuration.\n\nBut it illustrates that if you find the need for more complicated\n\nlogic, it’s cleaner to separate it from the declarative code.\n\n[Link to Come] gives more detailed advice on how to implement separation of concerns in your codebase.\n\nImplementation Principle: Treat infrastructure code like real code\n\nTo keep an infrastructure codebase clean, you need to treat it as a first-class concern. Too often, people don’t consider infrastructure\n\ncode to be “real” code. They don’t give it the same level of\n\nengineering discipline as application code.\n\nDesign and manage your infrastructure code so that it is easy to understand and maintain. Follow code quality practices, such as\n\ncode reviews, pair programming, and automated testing. Your\n\nteam should be aware of technical debt and strive to minimize it.",
      "content_length": 1076,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "CODE AS DOCUMENTATION\n\nWriting documentation and keeping it up to date can be too much work. For some purposes, the infrastructure code is more useful than written documentation. It’s always an accurate and updated record of your system.\n\nNew joiners can browse the code to learn about the system,\n\nTeam members can read the code, and review commits, to see what other people have done,\n\nTechnical reviewers can use the code to assess what to improve,\n\nAuditors can review code and version history to gain an accurate picture of the system.\n\nInfrastructure code is rarely the only documentation required. High-level documentation is helpful for context and strategy. You may have stakeholders who need to understand aspects of your system but who don’t know your tech stack.\n\nYou may want to manage these other types of documentation as code. Many teams write Architecture Decision Records (ADRs) in a markup language and keep them in source control.\n\nYou can automatically generate useful material like architecture diagrams from code. You can put this in a change management pipeline to update diagrams every time someone makes a change to the code.\n\nConclusion\n\nIn this chapter, I explored the core practice of defining your\n\nsystem as code. This included the key prerequisites for this practice, considerations of different types of infrastructure coding\n\nlanguages, and a few principles for good code design. The next\n\ncore practice, continuously validating your code, builds on this\n\nmaterial.\n\nBut before I cover that in Chapter 9, I’ll describe specific patterns\n\nand antipatterns for implementing this first practice in the context\n\nof infrastructure stacks.\n\nChapter 5 explains how to use infrastructure code to provision\n\nresources from your cloud platform into useful groups I call",
      "content_length": 1793,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "stacks. Chapter 6 covers the use of modules within stacks. Then,\n\nChapter 7 describes how to structure your stack code to create\n\nmultiple environments. After that, Chapter 8 offers patterns for managing the configuration of different stack instances across\n\nenvironments.\n\n1 Context about why depends on people to write useful commit messages\n\n2 git-crypt, blackbox, sops, and transcrypt are a few tools that help you to encrypt secrets in a git repository. Some of these tools integrate with cloud platform authorization, so unattended systems can decrypt them.\n\n3 I explain how people can work on stack code locally in more detail in [Link to\n\nCome].\n\n4 I describe how these are used in [Link to Come]\n\n5 “Recently” as I write this in late 2019\n\n6 I use this pseudo-code language to illustrate the concepts I’m trying to explain,\n\nwithout tying them to any specific tool.\n\n7 Martin Fowler and Rebecca Parsons define a DSL as a “small language, focused\n\non a particular aspect of a software system, in their book Domain-Specific Languages (Addison-Wesley Professional)\n\n8 Integrated Development Environment, a specialized editor for programming\n\nlanguages.\n\n9 Again, I’m writing this in late 2019. By the time you read this, things will have\n\nmoved forward in one direction or another.\n\n10 Object-Oriented Programming (OOP) and Functional Programming are two other\n\nprogramming paradigms used in application software development. Although there are a few examples of tools that use each of these (Riemann), neither is common with infrastructure code. There is no reason tools couldn’t use them with the domain. But even with OOP or functional programming, the advice in this section would still apply: don’t write code that mixes language paradigms.\n\n11 The term design smell derives from [code smell",
      "content_length": 1802,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "Part II. Working With Infrastructure Stacks",
      "content_length": 43,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Chapter 5. Building Infrastructure Stacks as Code\n\nIn Chapter 3, I said that an infrastructure platform is a pool of\n\ninfrastructure resources that you can provision and change on demand using an API. In Chapter 4, I explained the value of using\n\ncode to define the infrastructure for your system. This chapter puts these together by describing patterns for implementing code to manage infrastructure.\n\nThe concept that I use to talk about doing this is the infrastructure stack.\n\nWhat is an infrastructure stack?\n\nAn Infrastructure Stack is a collection of infrastructure resources that you define, provision, and update as a unit.\n\nYou write source code to define the elements of a stack, which are resources and services that your infrastructure platform provides.\n\nFor example, your stack may include a virtual machine (“Compute Resources”), disk volume (“Storage Resources”), and a subnet (“Network Resources”).\n\nYou run a stack management tool, which reads your stack source\n\ncode and uses the cloud platform’s API to assemble the elements",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "defined in the code to provision an instance of your stack.",
      "content_length": 59,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "Figure 5-1. An infrastructure stack is a collection of infrastructure elements managed as a group.\n\nExamples of stack management tools include:\n\nHashicorp Terraform\n\nAWS CloudFormation\n\nAzure Resource Manager\n\nGoogle Cloud Deployment Manager\n\nOpenStack Heat\n\nPulumi\n\nSome server configuration tools (which I’ll talk about much more\n\nin [Link to Come]) have extensions to work with infrastructure\n\nstacks. Examples of these are Ansible Cloud Modules, Chef Provisioning (now end-of-lifed ), Puppet Cloud Management, and\n\n1\n\nSalt Cloud.\n\n“STACK” AS A TERM\n\nMost stack management tools don’t call themselves stack management tools. Each tool has its own terminology to describe the unit of infrastructure that it manages. In this book, I’m describing patterns and practices that should be relevant for any of these tools.\n\nI’ve chosen to use the word “stack.”\n\nVarious people have told me there is a far better term for this concept than “stack.” Each of these people had a completely different word in mind. As of this writing, there is no agreement in the industry as to what to call this thing. So until there is, I’ll continue to use the word “stack.”\n\nStack code",
      "content_length": 1163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Each stack is defined by source code that declares what infrastructure elements it should include. Terraform code (.tf files) and CloudFormation templates are both examples of infrastructure stack code. A stack project contains the source code\n\nthat defines the infrastructure for a stack.\n\nExample 5-1 shows the folder structure for a stack source code\n\nproject. The language and tool for this example are fictitious. I use pseudo-code examples throughout this chapter.\n\nExample 5-1. Project folder structure of a stack project using a fictitious tool stack-project/ ├── src/ │ ├── dns.infra │ ├── load_balancers.infra │ ├── networking.infra │ └── webserver.infra └── test/\n\nStack instance\n\nYou can use a single stack project to provision more than one stack instance. When you run the stack tool for the project, it uses the\n\nplatform API to ensure the stack instance exists, and to make it\n\nmatch the project code. If the stack instance doesn’t exist, the tool creates it. If the stack instance exists but doesn’t exactly match the\n\ncode, then the tool modifies the instance to make it match.\n\nI often describe this process as “applying” the code to an instance.\n\nIf you change the code and rerun the tool, it changes the stack instance to match your changes. If you run the tool one more time\n\nwithout making any changes to the code, then it should leave the\n\nstack instance as it was.",
      "content_length": 1389,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Configuring servers in a stack\n\nInfrastructure codebases for systems that aren’t fully container-\n\nbased or serverless application architecture tend to include much code to provision and configure servers. Even container-based\n\nsystems need to build host servers to run containers. The first mainstream infrastructure as code tools, like CFEngine, Puppet,\n\nand Chef, were used to configure servers.\n\nYou should decouple code that builds servers from code that builds\n\nstacks. Doing this makes the code easier to understand, simplifies changes by decoupling them, and supports reusing and testing\n\nserver code.\n\nStack code typically specifies what servers to create, and passes information about the environment they will run in, by calling a\n\nserver configuration tool. Example 5-2 is an example of a stack definition that calls the fictitious servermaker tool to configure a server.\n\nExample 5-2. Example of a stack definition calling a server configuration tool virtual_machine: name: appserver-burgerbarn-${environment} source_image: foodspin-base-appserver memory: 4GB provision: tool: servermaker parameters: maker_server: maker.foodspin.io role: appserver environment: ${environment}\n\nThis stack defines an application server instance, created from a server image called foodspin-appserver, with 4 GB of RAM.",
      "content_length": 1314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "The definition includes a clause to trigger a provisioning process that runs servermaker. The code also passes several parameters for\n\nthe servermaker tool to use. These parameters include the address of a configuration server (maker_server), which hosts configuration files, and a role, appserver, which servermaker uses to decide which configurations to apply to this particular server. It also passes the name of the environment, which the configurations can use to customize the server.\n\n[Link to Come] describes patterns for managing servers as code.\n\nPatterns and antipatterns for structuring stacks\n\nOne challenge with infrastructure design is deciding how to size\n\nand structure stacks. You could create a single stack code project to manage your entire system. But this becomes unwieldy as your system grows. In this section, I’ll describe patterns and antipatterns for structure infrastructure stacks.",
      "content_length": 911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "THE DESCRIPTION FORMAT FOR PATTERNS AND ANTIPATTERNS\n\nI use patterns and antipatterns throughout this book to describe potential ways to solve common problems . Unlike principles, which are intended as rules to follow, a given pattern may or may not be appropriate for your situation. One way to think of these is that principles help you to decide which pattern is right in your context.\n\n2\n\nAn antipattern is a solution that is rarely appropriate. The reason for describing an antipattern is to help you to recognize it and to understand why it’s problematic. People implement antipatterns either because they don’t realize its pitfalls, or unintentionally. For example, someone might implement “Antipattern: One-shot Module” because it seems like a reasonable way to organize their project code. On the other hand, people don’t set out to code a “Antipattern: Spaghetti Module”, it just happens over time.\n\nThe description of each pattern and antipattern follows a set format, with each of these fields:\n\nName\n\nThe name of the pattern or antipattern.\n\nAlso Known As\n\nOther names you may have heard used to describe this.\n\nMotivation\n\nWhy people may decide to implement this pattern or antipattern.\n\nApplicability\n\nWhen this pattern is a good idea, and when it’s not.\n\nConsequences\n\nWhat you should think about if you choose to implement this pattern. What happens if you implement this antipattern.\n\nImplementation\n\nHow to implement the pattern.\n\nRelated patterns\n\nOther patterns and antipatterns, particularly alternatives.\n\nNot every pattern or antipattern description includes all of these fields.\n\nThe following patterns all describe ways of grouping the pieces of a system into one or more stacks. You can view them as a\n\ncontinuum:",
      "content_length": 1740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "A monolithic stack puts an entire system into one stack,\n\nAn application group stack groups multiple, related pieces of a system into stacks,\n\nA service stack puts all of the infrastructure for a single application into a single stack,\n\nA microstack breaks the infrastructure for a given application or service into multiple stacks.\n\nAntipattern: Monolithic Stack\n\nA Monolithic Stack is an infrastructure stack that includes too many elements, making it difficult to maintain.",
      "content_length": 476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Figure 5-2. A Monolithic Stack is an infrastructure stack that includes too many elements, making it difficult to maintain.\n\nWhat distinguishes a monolithic stack from other patterns is that\n\nthe number or relationship of infrastructure elements within the stack is difficult to manage well.\n\nALSO KNOWN AS\n\nSpaghetti stack, big ball of mud\n\nMOTIVATION\n\nPeople build monolithic stacks because the simplest way to add a\n\nnew element to a system is to add it to the existing project. Each new stack adds more moving parts, which may need to be orchestrated, integrated, and tested. A single stack is simpler to\n\nmanage.\n\nAPPLICABILITY\n\nA monolithic stack may be appropriate when your system is small\n\nand simple. It’s not appropriate when your system grows, taking longer to provision and update.\n\nCONSEQUENCES\n\nChanging a large stack is riskier than changing a smaller stack. More things that can go wrong-it has a larger blast radius. The impact of a failed change may be broader since there are more services and applications within the stack. Larger stacks are also\n\nslower to provision and change, which makes them harder to manage.",
      "content_length": 1135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "As a result of the speed and risk of changing a monolithic stack, people tend to make changes less frequently and take longer to do it. This added friction can lead to higher levels of technical debt.\n\nBLAST RADIUS\n\n3\n\nThe term blast radius describes the potential damage a given change could make to a system. It’s usually based on the elements of the system you’re changing, what other elements depend on them, and what elements are shared.\n\nIMPLEMENTATION\n\nYou build a monolithic stack by creating an infrastructure stack project and then continuously adding code, rather than splitting it into multiple stacks.\n\nRELATED PATTERNS\n\nThe opposite of a monolithic stack is a micro stack (“Pattern: Micro Stack”), which aims to keep stacks small so that they are easier to maintain and improve. A monolithic stack may be an\n\napplication group stack “Pattern: Application Group Stack” that has grown out of control.",
      "content_length": 912,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "IS MY STACK A MONOLITH?\n\nWhether your infrastructure stack is a monolith is a matter of judgment. The symptoms of a monolithic stack include:\n\nIt’s difficult to understand how the pieces of the stack fit together (they may be too messy to understand, or perhaps they don’t fit well together),\n\nNew people take a while learning the stack’s codebase,\n\nDebugging problems with the stack is hard,\n\nChanges to the stack frequently cause problems,\n\nYou spend too much time maintaining systems and processes whose purpose is to manage the complexity of the stack.\n\nA key indicator of whether a stack is becoming monolithic is how many people are working on changes to it at any given time. The more common it is for multiple people to work on the stack simultaneously, the more time you spend coordinating changes. Multiple teams making changes to the same stack is even worse. If you frequently have failures and conflicts when deploying changes to a given stack, it may be too large.\n\nFeature branching is a strategy for coping with this, but it can add friction and overhead to delivery. The habitual use of feature branches to work on a stack suggests that the stack has become monolithic.\n\nContinuous Integration (CI) is a more sustainable way to make it safer for multiple people to work on a single stack. However, as a stack grows increasingly monolithic, the CI build takes longer to run, and it becomes harder to maintain good build discipline. If your team’s CI is sloppy, it’s another sign that your stack is a monolith.\n\nThese issues relate to a single team working on an infrastructure stack. Multiple teams working on a shared stack is a clear sign to consider splitting it into more manageable pieces.\n\nPattern: Application Group Stack\n\nAn Application Group Stack includes the infrastructure for multiple related applications or services. The infrastructure for all\n\nof these applications is provisioned and modified as a group.",
      "content_length": 1937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "Figure 5-3. An Application Group Stack hosts multiple processes in a single instance of the stack.\n\nFor example, an online shopping company’s product application group may include separate services for browsing products,\n\nsearching for products, and managing a shopping basket. An application group stack could provide the infrastructure that runs\n\nall three of these services.\n\nALSO KNOWN AS\n\nCombined stack, service group stack, multi-application stack.\n\nMOTIVATION\n\nDefining the infrastructure for multiple related services together can make it easier to manage the application as a single unit.\n\nAPPLICABILITY\n\nThis pattern can work well when a single team owns the infrastructure and deployment of all of the pieces of the\n\napplication. An application group stack can align the boundaries of\n\nthe stack to the team’s responsibilities.\n\nMulti-service stacks are sometimes useful as an incremental step from a monolithic stack to service stacks.\n\nCONSEQUENCES\n\nGrouping the infrastructure for multiple applications together also combines the time, risk, and pace of changes. The team needs to\n\nmanage the risk to the entire stack for every change, even if only\n\none part is changing. This pattern is inefficient if some parts of the stack change more frequently than others.",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "The time to provision, change, and test a stack is based on the\n\nentire stack. So again, if it’s common to change only one part of a\n\nstack at a time, having it grouped adds unnecessary overhead and risk.\n\nIMPLEMENTATION\n\nTo create an application group stack, you define an infrastructure project that builds all of the infrastructure for a set of services. You\n\ncan provision and destroy all of the pieces of the application with a single command.\n\nRELATED PATTERNS\n\nThis pattern risks growing into a Monolithic Stack (“Antipattern: Monolithic Stack”). In the other direction, breaking each service in\n\nan application group stack into a separate stack creates a service\n\nstack (“Pattern: Service Stack”).\n\nPattern: Service Stack\n\nA Service Stack manages the infrastructure for each deployable\n\napplication component in a separate infrastructure stack.",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "Figure 5-4. A Service Stack manages the infrastructure for each deployable application component in a separate infrastructure stack.\n\nALSO KNOWN AS\n\nStack per app, single service stack.\n\nMOTIVATION\n\nService stacks align the boundaries of infrastructure to the\n\nsoftware that runs on it. This alignment limits the blast radius for a\n\nchange to one service, which simplifies the process for scheduling changes. Service teams can own the infrastructure that relates to\n\ntheir software.\n\nAPPLICABILITY\n\nService stacks can work well with microservice application\n\n4\n\narchitectures . They also help organizations with autonomous\n\n5 teams to ensure each team owns its infrastructure .\n\nCONSEQUENCES\n\nIf you have multiple applications, each with an infrastructure\n\nstack, there could be an unnecessary duplication of code. For example, each stack may include code that specifies how to\n\nprovision an application server. Duplication can encourage\n\ninconsistency, such as using different operating system versions, or different network configurations. You can mitigate this by using\n\nmodules to share code (as in Chapter 6).\n\nIMPLEMENTATION\n\nEach application or service has a separate infrastructure code\n\nproject. When creating a new application, a team might copy code",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "from another application’s infrastructure. Or the team could use a\n\nreference project, with boilerplate code for creating new stacks.\n\nIn some cases, each stack may be complete, not sharing any infrastructure with other application stacks. In other cases, teams\n\nmay create stacks with infrastructure that supports multiple\n\napplication stacks. You can learn more about different patterns for this in [Link to Come].\n\nRELATED PATTERNS\n\nThe service stack pattern falls between an application group stack (“Pattern: Application Group Stack”), which has multiple\n\napplications in a single stack, and a micro stack (“Pattern: Micro\n\nStack”), which breaks the infrastructure for a single application across multiple stacks.\n\nPattern: Micro Stack\n\nThe Micro Stack pattern divides the infrastructure for a single\n\nservice across multiple stacks.",
      "content_length": 838,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "Figure 5-5. Micro stacks divide the infrastructure for a single service across multiple stacks.\n\nFor example, you may have a separate stack project each for the\n\nnetworking, servers, and database.\n\nMOTIVATION\n\nDifferent parts of a service’s infrastructure may change at different\n\nrates. Or they may have different characteristics which make them\n\neasier to manage separately. For instance, some methods for managing server instances involve frequently destroying and\n\n6\n\nrebuilding them . However, some services use persistent data in a\n\ndatabase or disk volume. Managing the servers and data in separate stacks means they can have different lifecycles, with the\n\nserver stack being rebuilt much more often than the data stack.\n\nCONSEQUENCES\n\nAlthough smaller stacks are themselves simpler, handling the\n\ndependencies between them is more complicated.\n\nIMPLEMENTATION\n\nAdding a new microstack involves creating a new stack project.\n\nYou need to draw boundaries in the right places between stacks to keep them appropriately sized and easy to manage. The related\n\npatterns include solutions to this. You may also need to integrate\n\ndifferent stacks, which I describe in [Link to Come].\n\nRELATED PATTERNS\n\nMicro stacks are the opposite end of the spectrum from a\n\nmonolithic stack (“Antipattern: Monolithic Stack”), where a single stack contains all the infrastructure for a system.",
      "content_length": 1380,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "Conclusion\n\nInfrastructure stacks are fundamental building blocks for\n\nautomated infrastructure. The patterns in this chapter are a starting point for thinking about organizing infrastructure into stacks.\n\nGiven that a stack is a unit of change, the main principle for\n\ndeciding where to draw boundaries between stacks in your system is making it easy and safe to make changes. [Link to Come]\n\nexplores this topic in more detail.\n\nIn the next chapter, Chapter 6, I describe patterns and antipatterns\n\nfor sharing code across stacks using modules.\n\n1 https://github.com/chef-boneyard/chef-provisioning\n\n2 The architect Christopher Alexander originated the idea of design patterns in A Pattern Language: Towns, Buildings, Construction (Alexander, Jacobson, Silverstein, Ishikawa, 1977, Oxford University Press). Kent Beck, Ward Cunningham, and others adapted the concept to Software design patterns. Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, known as the “Gang of Four” (or “GoF”), cataloged common patterns and antipatterns in Design Patterns: Elements of Reusable Object-Oriented Software (1994, Addison Wesley).\n\n3 I don’t know who, if anyone, can be said to have coined the term “blast radius” in the context of software system risks, but Charity Majors popularized it, probably starting with her post Terraform, VPC, and why you want a tfstate file per env\n\n4 See Microservices by James Lewis.\n\n5 See The Art of Building Autonomous Teams by John Ferguson Smart, among\n\nmany other references\n\n6 Examples of these are the phoenix server pattern ([Link to Come]) and immutable\n\nservers ([Link to Come])",
      "content_length": 1622,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Chapter 6. Using Modules to Share Stack Code\n\nOne of the principles of good software design is the DRY principle (“Don’t Repeat Yourself”) . As you write code for different\n\n1\n\ninfrastructure stacks, you may find yourself writing very similar code multiple times. Rather than maintaining the same code in multiple places, it is often easier to maintain a single copy of the\n\ncode.\n\nMost stack management tools support modules, or libraries, for this reason. A Stack Code Module is a piece of infrastructure code that you can use across multiple stack projects. You can version, test, and release the module code independently of the stack that\n\nuses it.",
      "content_length": 653,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "Figure 6-1. A Stack Code Module is a piece of infrastructure code that you can use across multiple stack projects.\n\nExamples of using modules\n\nAs an example, the Foodspin team has several servers in their system-container host servers, application servers, and a CD\n\nserver. The code for each of these is similar:\n\nExample 6-1. Example of stack code with duplication virtual_machine: name: foodspin-clusterhost source_image: hardened-linux-base memory: 16GB provision: tool: servermaker maker_server: maker.foodspin.io role: clusterhost\n\nvirtual_machine: name: foodspin-appserver source_image: hardened-linux-base memory: 8GB provision: tool: servermaker maker_server: maker.foodspin.io role: appserver\n\nvirtual_machine: name: foodspin-gocd-master source_image: hardened-linux-base memory: 4GB provision: tool: servermaker maker_server: maker.foodspin.io role: appserver\n\nThe Foodspin team writes a module to declare a server, with\n\nplaceholders for parameters:",
      "content_length": 961,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Example 6-2. Example of a stack module for defining a server declare module: foodspin-server parameters: name: STRING memory: STRING server_role: STRING\n\nvirtual_machine: name: ${name} source_image: hardened-linux-base memory: ${memory} provision: tool: servermaker maker_server: maker.foodspin.io role: ${server_role}\n\nEach stack that needs a server can then use this module:\n\nExample 6-3. Example of a stack module that defines a server use module: foodspin-server name: foodspin-clusterhost memory: 16GB server_role: clusterhost\n\nuse module: foodspin-server name: foodspin-appserver memory: 8GB server_role: appserver\n\nuse module: foodspin-server name: foodspin-gocd-master memory: 4GB server_role: gocd_master\n\nThe code for each server is simpler using modules since it only\n\ndeclares the elements that are different rather than duplicating the declarations for the server image and server provisioner. Most\n\nusefully, they can make changes to how they provision servers in one location, rather than needing to change code in different\n\ncodebases.",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Patterns and antipatterns for infrastructure modules\n\nModules can be useful for reusing code. However, they also add complexity. So it’s essential to use modules in a way that creates\n\nmore value than complexity. A useful module simplifies stack code, improving readability and maintainability. A poor module\n\nreduces flexibility and is difficult to understand and maintain.\n\nHere are a few patterns and antipatterns that illustrate what works well and what doesn’t:\n\nA Facade Module provides a simplified interface for a resource provided by the stack tool language, or the underlying platform API.\n\nAn Anemic Module wraps the code for an infrastructure element but does not simplify it or add any particular value.\n\nA Domain Entity Module implements a high-level concept by combining multiple low-level infrastructure resources.\n\nA Spaghetti Module is configurable to the point where it creates significantly different results depending on the parameters given to it\n\nAn Obfuscation Layer is composed of multiple modules. It is intended to hide or abstract details of the infrastructure from people writing stack code but instead makes the codebase as a whole harder to understand, maintain, and use.\n\nA One-shot Module is only used once in a codebase, rather than being reused.",
      "content_length": 1280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "Pattern: Facade Module\n\nA Facade Module provides a simplified interface for a resource\n\nprovided by the stack tool language, or the underlying platform API.\n\nThe facade module presents a small number of parameters to code\n\nthat uses it:\n\nExample 6-4. Example code using a facade module use module: foodspin-server name: burgerbarn-appserver memory: 8GB\n\nThe module itself includes a larger amount of code, that the calling\n\ncode doesn’t need to worry about:\n\nExample 6-5. Code for the example facade module declare module: foodspin-server virtual_machine: name: ${name} source_image: hardened-linux-base memory: ${memory} provision: tool: servermaker maker_server: maker.foodspin.io role: application_server network: vlan: application_zone_vlan gateway: application_zone_gateway firewall_inbound: port: 22 from: management_zone firewall_inbound: port: 443 from: webserver_zone\n\nMOTIVATION",
      "content_length": 888,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "A facade module simplifies and standardizes a common use case for an infrastructure resource. The stack code the uses a facade\n\nmodule should be simpler and easier to read. Improvements to the quality of the module code are rapidly available to all of the stacks which use it.\n\nAPPLICABILITY\n\nFacade modules work best for simple use cases, usually involving a low-level infrastructure resource.\n\nCONSEQUENCES\n\nA facade module limits how you can use the underlying infrastructure resource. Doing this can be useful, simplifying\n\noptions and standardizing around good quality implementations. However, it can also reduce flexibility.\n\nA module is an extra layer of code between the stack code and the code that directly specifies the infrastructure resources. This extra layer adds at least some overhead to maintaining, debugging, and\n\nimproving code. It can also make it harder to understand the stack code.\n\nIMPLEMENTATION\n\nImplementing a facade module generally involves specifying an infrastructure resource with a number of hard-coded values, and a small number of values that are passed through from the code that\n\nuses the module.\n\nRELATED PATTERNS",
      "content_length": 1154,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "An anemic module (“Antipattern: Anemic Module”) is a facade module that doesn’t hide much, so adds complexity without adding\n\nmuch value. A domain entity module (“Pattern: Domain Entity Module”) is similar to a facade, in that it presents a simplified interface to the code that uses it. But a domain entity combines\n\nmultiple lower-level elements to present a single higher-level entity to the calling code. In contrast, a facade module is a more direct mapping of a single element.\n\nAntipattern: Anemic Module\n\nAn Anemic Module wraps the code for an infrastructure element\n\nbut does not simplify it or add any particular value. It may be a facade module (“Pattern: Facade Module”) gone wrong, or it may be part of an obfuscation layer (“Antipattern: Obfuscation Layer”).\n\nExample 6-6. Example code using an anemic module use module: any_server server_name: burgerbarn-appserver ram: 8GB source_image: base_linux_image provisioning_tool: servermaker server_role: application_server vlan: application_zone_vlan gateway: application_zone_gateway firewall_rules: firewall_inbound: port: 22 from: management_zone firewall_inbound: port: 443 from: webserver_zone\n\nThe module itself passes the parameters directly to the stack management tool’s code:\n\nExample 6-7. Code for the example anemic module",
      "content_length": 1294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "declare module: any_server virtual_machine: name: ${server_name} source_image: ${origin_server_image} memory: ${ram} provision: tool: ${provisioning_tool} role: ${server_role} network: vlan: ${server_vlan} gateway: ${server_gateway} firewall_inbound: ${firewall_rules}\n\nALSO KNOWN AS\n\nValue-Free Wrapper, Pass-Through Module, Obfuscator Module.\n\nMOTIVATION\n\nSometimes people write this kind of module aiming to follow the DRY (Don’t Repeat Yourself) principle. They see that code that\n\ndefines a common infrastructure element, such as a virtual server, load balancer, or security group, is used in multiple places in the codebase. So they create a module that declares that element type once and use that everywhere. But because the elements are being\n\nused differently in different parts of the code, they need to expose a large number of parameters in their module. The result is that the code that uses the module is no simpler than directly using the\n\nstack tool’s code. The codebase still has repeated code, only now it’s repeated use of the module.\n\nAPPLICABILITY\n\nNobody intentionally writes an anemic module. You may debate whether a given module is anemic or is a facade, but the debate itself is useful. You should consider whether a module adds real",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "value and, if not, then refactor it into code that uses the stack language directly.\n\nCONSEQUENCES\n\nWriting, using, and maintaining module code rather than directly using the constructs provided by your stack tool adds overhead. It makes your stack code more difficult to understand, especially for\n\npeople who know the stack tool but are new to your codebase. It adds more code to maintain, usually with separate versioning and release. You can end up with different versions of a module being\n\nused by different stacks, causing people to waste time and energy managing releases, upgrades, and testing.\n\nIMPLEMENTATION\n\nIf a module does more than pass parameters, but still presents too many parameters to code that uses it, you might consider splitting it into multiple modules. Doing this makes sense when there are a few common but different use cases for the infrastructure element\n\nthe module defines. For example, rather than a single module for defining firewall rules, you may want one module to define a firewall rule for public HTTPS traffic, another for internal HTTPS\n\ntraffic, and a third for SSH traffic. Each of these may need fewer parameters than a single module that handles multiple protocols and scenarios.\n\nRELATED PATTERNS\n\nSome anemic modules are “Pattern: Facade Module” that grew out of control. Others are part of an “Antipattern: Obfuscation Layer”. It may also be a “Pattern: Domain Entity Module” that maps a bit",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "too directly to a single underlying infrastructure element, rather than to a useful combination of elements.\n\nCOUPLING AND COHESION WITH INFRASTRUCTURE MODULES\n\nWhen organizing code of any kind into different components, you need to consider dependencies. A stack project that uses a module depends on that module . The level of coupling describes how much a change to one part of the codebase affects other parts. If a stack is tightly coupled to a module, then changes to the module will probably require changing the stack code as well. This is a problem when multiple stacks are tightly coupled to a module, or when there is tight coupling across multiple modules and stacks. Tight coupling creates friction and risk for making changes to code.\n\n2\n\nYou should aim to make your code loosely coupled. You can draw on lessons from software architecture to find ways to identify and avoid tight 3 coupling .\n\nThe level of cohesion of a component describes how well-focused it is. A module that does too much, like a spaghetti module (“Antipattern: Spaghetti Module”), has low cohesion. A module has high cohesion when it has a clear focus. A facade module (“Pattern: Facade Module”) can be highly cohesive around a low-level infrastructure resource, while a domain entity module (“Pattern: Domain Entity Module”) can be highly cohesive around a clear, logical entity.\n\nPattern: Domain Entity Module\n\nA Domain Entity Module implements a high-level concept by combining multiple low-level infrastructure resources. An example of a higher level concept is the infrastructure needed to run a specific Java application. This example shows how a module\n\nthat implements a Java application infrastructure instance might be used from stack project code:\n\nExample 6-8. Example of using a domain entity module",
      "content_length": 1799,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "use module: java-application-infrastructure name: \"shopping_app_cluster\" application: \"shopping_app\" application_version: \"4.20\" network_access: \"public\"\n\nThe module creates a complete set of infrastructure elements, including a cluster of virtual servers with load balancer, database instance, and firewall rules.\n\nMOTIVATION\n\nInfrastructure stack languages provide language constructs that map directly to resources and services provided by infrastructure platforms (the things I described in Chapter 3). Teams combine\n\nthese low-level constructs into higher-level constructs to serve a particular purpose, such as hosting a Java application or running a data pipeline.\n\nIt can take a fair bit of low-level infrastructure code to define all of\n\nthe pieces needed to meet that high-level purpose. As I described above, to create the infrastructure to run a Java application, you may need to write code for a cluster of virtual servers, for a load\n\nbalancer, for a database instance, and for firewall rules. That’s a lot of code. Although you may write modules so you can share code for each low-level resource, if you need multiple different applications with similar infrastructure, it would be useful to share\n\ncode at this higher level as well.",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "EXAMPLE: FOODSPIN’S APPLICATION INFRASTRUCTURE ENTITY MODULE\n\nThe Foodspin team runs several different clusters for running Java applications. They host a separate application instance for each of their customers. They also run an instance of the Atlassian Jira bug tracker, and the GoCD pipeline server. So they define one stack project for each customer , one for Jira, and one for GoCD.\n\n4\n\nThe team notices that they’re spending too much time copying changes between these stacks, so they decide to create a module for the common infrastructure they need. In their case, each instance has a cluster of virtual machines, all built from the same server image. Different instances have different minimum and maximum numbers of servers in the cluster, so the team makes these into parameters for the module. Some applications use a database, and some don’t. So they leave this out of their module and instead have an optional parameter to pass connection details to the server provisioning code.\n\nTheir module also defines the networking structures, including firewall rules and routing, that the application needs. Here, they use a parameter to indicate whether the application is public-facing (for the customer applications) or internal (for Jira and GoCD), which results in some different networking configurations.\n\nBy using this module in each stack that needs an application server, the Foodspin team has simplified each stack’s project code, so it is easier to understand. They also have a single place to make improvements to the infrastructure for multiple applications. They write a set of tests for this module (as we’ll see in Chapter 9 and later chapters), which gives them the confidence to improve their code continuously.\n\nAPPLICABILITY\n\nA domain entity module is useful for things that are fairly complex\n\nto implement, and that are used in pretty much the same way in\n\nmultiple places in your system. Don’t create a domain entity module that you only use once. And don’t create one of these\n\nmodules for multiple instances of the same thing, when each instance is significantly different. The first case is a one-shot\n\nmodule (“Antipattern: One-shot Module”), the second risks\n\nbecoming a spaghetti module (“Antipattern: Spaghetti Module”).\n\nFor example, you may run multiple application servers that use the same stack-operating system, application language, and\n\napplication deployment method. This is a candidate for a domain\n\nentity module.",
      "content_length": 2463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "As a counter-example, you may have multiple application servers,\n\nbut some run Windows, some run Linux. One runs a stateless PHP\n\napplication, another runs a Java application on Tomcat with a MySQL database, while a third hosts a .Net application that uses\n\nmessage queues to integrate with other applications. Although all\n\nthree of these are application servers, their implementation is quite different. Any module that can create a suitable infrastructure\n\nstack for all of these is unlikely to have a clean design and implementation.\n\nIMPLEMENTATION\n\nOn a concrete level, implementing a domain entity model is a matter of writing the code for a related grouping of infrastructure\n\nin a single module. But the best way to create a high-quality\n\ncodebase that is easy for people to learn and maintain is to take a design-led approach.\n\nI recommend drawing from lessons learned in software\n\narchitecture and design. The domain entity module pattern derives\n\n5\n\nfrom Domain Driven Design (DDD) , which creates a conceptual model for the business domain of a software system, and uses that\n\nto drive the design of the system itself. Infrastructure, especially\n\none designed and built as software, can be seen as a domain in its own right. The domain is building, delivering, and running\n\nsoftware.\n\nA particularly powerful approach is for an organization to use\n\nDDD to design the architecture for the business software, and then extend the domain to include the systems and services used for\n\nbuilding and running that software.",
      "content_length": 1528,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "The code to implement a Java application server might look like\n\nExample 6-9. This example creates and assembles three different\n\nresources: a DNS entry, which points to a load balancer, which routes traffic to a server cluster.\n\nExample 6-9. Example domain entity module declare module: java-application-infrastructure\n\ndns_entry: id: \"${APP_NAME}-hostname\" record_type: \"A\" hostname: \"${APP_NAME}.foodspin.io\" ip_address: {$load_balancer.ip_address}\n\nserver_cluster: id: \"${APP_NAME}-cluster\" min_size: 1 max_size: 3 each_server_node: source_image: base_linux memory: 4GB provision: tool: servermaker role: appserver parameters: app_package: \"${APP_NAME}-${APP_VERSION}.war\" app_repository: \"repository.foodspin.io\"\n\nload_balancer: protocol: https target: type: server_cluster target_id: \"${APP_NAME}-cluster\"\n\nCode to provision a stack that runs a search application using this module might look like this:\n\nuse module: java-application-infrastructure app_name: foodspin_search_app app_version: 1.23",
      "content_length": 1002,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "RELATED PATTERNS\n\nIf you find that some stack projects have very little code other than importing a single module, you might consider putting the code\n\ndirectly into the stack project. Reusing code in the form of an entire stack, rather than a module, is the reusable stack pattern\n\n(“Pattern: Reusable Stack”). In some cases, having the entire stack\n\ncode in a module is deliberate, as in the wrapper stack pattern (“Pattern: Wrapper Stack”).\n\nA facade module (“Pattern: Facade Module”) is like a domain\n\nentity module that only creates a single low-level infrastructure\n\nelement. A spaghetti module (“Antipattern: Spaghetti Module”) is similar to a domain entity module but has too many different\n\noptions.\n\nAntipattern: Spaghetti Module\n\nA Spaghetti Module is configurable to the point where it creates\n\nsignificantly different results depending on the parameters given to it. The implementation of the module is messy and difficult to\n\nunderstand, because it has too many moving parts:\n\nExample 6-10. Example of a spaghetti module declare module: application-server-infrastructure variable: network_segment = { if ${parameter.network_access} = \"public\" id: public_subnet else if ${parameter.network_access} = \"customer\" id: customer_subnet else id: internal_subnet end }\n\nswitch ${parameter.application_type}: \"java\":",
      "content_length": 1321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "virtual_machine: origin_image: base_tomcat network_segment: ${variable.network_segment} server_configuration: if ${parameter.database} != \"none\" database_connection: ${database_instance.my_database.connection_string} end ... \"NET\": virtual_machine: origin_image: windows_server network_segment: ${variable.network_segment} server_configuration: if ${parameter.database} != \"none\" database_connection: ${database_instance.my_database.connection_string} end ... \"php\": container_group: cluster_id: ${parameter.container_cluster} container_image: nginx_php_image network_segment: ${variable.network_segment} server_configuration: if ${parameter.database} != \"none\" database_connection: ${database_instance.my_database.connection_string} end ... end\n\nswitch ${parameter.database}: \"mysql\": database_instance: my_database type: mysql ... ...\n\nThe example code above assigns the server it creates to one of three different network segments, and optionally creates a database\n\ncluster and passes a connection string to the server configuration. In some cases, it creates a group of container instances rather than\n\na virtual server. This module is a bit of a beast.",
      "content_length": 1158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "ALSO KNOWN AS\n\nSwiss Army module.\n\nMOTIVATION\n\nAs with other antipatterns, people create a spaghetti module by accident, often over time. You may create a facade module\n\n(“Pattern: Facade Module”) for a common infrastructure element,\n\nsuch as a server, that grows so that it can create radically different types of servers. Or you may create a domain entity module\n\n(“Pattern: Domain Entity Module”) for an application’s\n\ninfrastructure. But then you find that the module needs to optionally include a variety of elements, such as databases,\n\nmessage queues, load balancers, and public Internet gateways,\n\ndepending on the application that runs on it.\n\nCONSEQUENCES\n\nA module that does too many things is less maintainable than one\n\nwith a tighter scope. The more things a module does, and the more variations there are in the infrastructure that it can create, the\n\nharder it is to change it without breaking something. These\n\nmodules are harder to test. As I explain in a later chapter (Chapter 9), better-designed code is easier to test, so if you’re\n\nstruggling to write automated tests and build pipelines to test the module in isolation, It’s a sign that you may have a spaghetti\n\nmodule.\n\nIMPLEMENTATION\n\nA spaghetti module’s code often contains conditionals, that apply\n\ndifferent specifications in different situations. For example, a",
      "content_length": 1343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "database cluster module might take a parameter to choose which\n\ndatabase to provision.\n\nWhen you realize you have a spaghetti module on your hands, you should refactor it. Often, you can split it into different modules,\n\neach with a more focused remit. For example, you might\n\ndecompose your single application infrastructure module into different modules for different parts of the application’s\n\ninfrastructure. An example of a stack that uses decomposed\n\nmodules in this way, rather than using the spaghetti module from the earlier example (Example 6-10) might look like this:\n\nExample 6-11. Example of using decomposed modules rather than a single spaghetti module use module: java-application-servers name: burgerbarn_appserver application: \"shopping_app\" application_version: \"4.20\" network_segment: customer_subnet server_configuration: database_connection: ${module.mysql- database.outputs.connection_string}\n\nuse module: mysql-database cluster_minimum: 1 cluster_maximum: 3 allow_connections_from: customer_subnet\n\nEach of the modules is smaller, simpler, and so easier to maintain and tested than the original spaghetti module.\n\nRELATED PATTERNS\n\nA spaghetti module is often a domain entity module (“Pattern: Domain Entity Module”) gone wrong.\n\nAntipattern: Obfuscation Layer",
      "content_length": 1285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Unlike the other patterns and antipatterns in this chapter, an\n\nObfuscation Layer is composed of multiple modules. Intended to hide or abstract details of the infrastructure from people writing\n\nstack code, it instead makes the codebase as a whole harder to\n\nunderstand, maintain, and use.\n\nALSO KNOWN AS\n\nAbstraction layer, portability layer, in-house infrastructure model.\n\nMOTIVATION\n\nAn obfuscation layer is usually intended to simplify or standardize\n\nimplementation. A team might create a library of modules as an in-\n\nhouse model for building infrastructure. Sometimes the intention is for people to write stack code without needing to learn the stack\n\ntool itself. In other cases, the layer tries to abstract the specifics of\n\nthe infrastructure platform so that code can be written once and used across multiple platforms.\n\nCONSEQUENCES\n\nIf your team uses a heavily customized model for infrastructure code, then it becomes a barrier to new people joining your team.\n\nEven someone who knows the stack tools you use has a steep learning curve to understand your special language. An in-house\n\nobfuscation layer tends to be inflexible, forcing people to either\n\nwork around its limitations or spend time making changes to the obfuscation layer itself. Either way, people waste time creating and\n\nmaintaining extra code.\n\n6\n\nPeople, especially managers and hands-off architects , often\n\noverestimate the benefit of hiding an underlying platform or tools",
      "content_length": 1459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "from people writing infrastructure code. I have yet to see a cloud\n\nabstraction layer that adds noticeable value, or that avoids adding cost and complexity. Most fail at both.",
      "content_length": 175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "CLOUD ABSTRACTION LAYERS CONSIDERED HARMFUL\n\nDifferent infrastructure platforms provide common types of resources, as you see when reading or skimming through Chapter 3. So it seems simple to map these to a common set of abstractions. But the resources are implemented differently. The actual code to implement a virtual server on AWS, Azure, and GCE, for example, is not trivial.\n\nAnd each platform provides hundreds of different services. Even if you limit yourself to using a few dozen services, that’s a large amount of code to implement and maintain.\n\nThe theory of an abstraction layer is that you can reduce the cost of writing and maintaining a stack across multiple platforms, as visualized here:",
      "content_length": 705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Figure 6-2. The goal of an abstraction layer is to reduce the cost of a stack across multiple platforms\n\nIn practice, the cost of writing and maintaining an abstraction layer for multiple platforms is more than the cost of the separate code for each platform, as shown here:",
      "content_length": 274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "Figure 6-3. The cost of an abstraction layer\n\nThe flipside of the high cost of an abstraction layer is reduced value. It’s impractical to cover everything the underlying platforms offer. And if you want to ensure you can create a stack on all of the platforms, then you won’t be able to leverage any features that are unique to one platform or another. The result is using a least-common-denominator subset of cloud features, at a higher cost than using them directly.\n\nRELATED PATTERNS\n\nDomain entity modules (“Pattern: Domain Entity Module”) may\n\nbe used to create an obfuscation layer at a higher level, while\n\nfacade modules (“Pattern: Facade Module”) would be used to\n\nobfuscate lower-level resources. If each module in the layer\n\nhandles multiple platforms, they are probably spaghetti modules (“Antipattern: Spaghetti Module”).\n\nAntipattern: One-shot Module\n\nA one-shot module is only used once in a codebase, rather than\n\nbeing reused.\n\nMOTIVATION\n\nPeople usually create one-shot modules as a way to organize the\n\ncode in a project.\n\nAPPLICABILITY\n\nIf a stack project includes enough code that it becomes difficult to\n\nnavigate, you have a few options. Splitting the stack into modules\n\nis one approach. If the stack is conceptually doing too much, it might be better to divide it into multiple stacks, using an\n\nappropriate stack structural pattern (see “Patterns and antipatterns\n\nfor structuring stacks”). Otherwise, merely organizing code into\n\ndifferent files and, if necessary, different folders, can make it",
      "content_length": 1522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "easier to navigate and understand the codebase without the\n\noverhead of the other options.\n\nCONSEQUENCES\n\nOrganizing the code into modules adds the overhead of declaring\n\nthe module and passing parameters. You add even more\n\ncomplexity if you manage the module code separately, with separate versioning. This overhead is worthwhile when you share\n\ncode across multiple stack projects. The benefits of code reuse\n\nmake up for the added time and energy of maintaining a separate\n\nmodule. Paying that cost when you’re not using the benefit is a\n\nwaste.\n\nRELATED PATTERNS\n\nA one-shot module may map closely to lower-level infrastructure\n\nelements, like a facade module (“Pattern: Facade Module”), or to a higher-level entity, like a domain entity module (“Pattern: Domain\n\nEntity Module”).\n\nConclusion\n\nThis chapter has explored the use of modules to share code across\n\nstacks. Modules are a popular mechanism to manage the growth of a codebase, but as you can see, it’s easy to create an\n\nunmaintainable mess.\n\nThe next chapter (Chapter-Building-Environments) is devoted to\n\none of the most common situations that require multiple instances of the same or similar infrastructure. This need leads to some",
      "content_length": 1200,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "patterns and antipatterns for implementing shared code at the level\n\nof the stack.\n\n1 The DRY principle, and others, can be found in The Pragmatic Programmer: From\n\nJourneyman to Master by Andrew Hunt and David Thomas\n\n2 In later chapters ([Link to Come]) we’ll see that a stack can depend on another\n\nstack, as well.\n\n3 Two resources to get started include Martin Fowler’s paper Reducing Coupling,\n\nand Mohamed Sanaulla’s post Cohesion and Coupling: Two OO Design Principles.\n\n4 Later on (“Example of how Foodspin moves to reusable stacks”) we’ll see how the\n\nFoodspin team evolves this to use a single reusable stack project\n\n5 See Domain-Driven Design: Tackling Complexity in the Heart of Software, by Eric\n\nEvans, 2003 Addison Wesley\n\n6 For a thoughtful view on how architecture, and architects, relates to\n\nimplementation, I recommend the Architect Elevator, by Gregor Hohpe.",
      "content_length": 880,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "Chapter 7. Building Environments With Stacks\n\nIn Chapter 5, I described an infrastructure stack as a collection of infrastructure resources that you manage as a single unit. An\n\nenvironment is also a collection of infrastructure resources. So is a stack the same thing as an environment? In this chapter, I explain that maybe it is, but maybe it isn’t.\n\nThe difference between an environment and a stack is that an\n\nenvironment is conceptual, while a stack is concrete. You define an environment to fulfill a particular purpose, like deploying and testing software. You define a stack with code, and then provision it, change it, and destroy it using a tool. As I’ll explain in this\n\nchapter, you might implement an environment as a single stack, or you might compose an environment from multiple stacks. You could even create several environments in one stack, although you\n\nshouldn’t.\n\nWhat environments are all about\n\nThe concept of an environment is one of those things that we take for granted in IT. But we often mean slightly different things when we use the term in different contexts. In this book, an environment\n\nis a collection of operationally related infrastructure resources. That is, the resources in an environment support a particular activity, such as testing or running a system. Most often, multiple\n\nenvironments exist, each running an instance of the same system.",
      "content_length": 1386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "There are two typical use cases for having multiple environments\n\nrunning instances of the same system. One is to support a release delivery process, and the other is to run multiple production instances of the system.\n\nRelease delivery environments\n\nThe most familiar use case for multiple environments is to support\n\na progressive software release process - sometimes called the path to production. A given build of an application is deployed to each environment in turn to support different development and testing activities until it is finally deployed to the production environment.",
      "content_length": 588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "Figure 7-1. Foodspin delivery environments\n\nI’ll use this set of environments throughout this chapter to\n\nillustrate patterns for defining environments as code.\n\nMultiple production environments\n\nYou might also use multiple environments for complete, independent copies of a system in production. Reasons to do this\n\ninclude:\n\nFault tolerance\n\nIf one environment fails, others can continue to provide service. Doing this could involve a failover process to shift load from the failed environment. You can also have fault tolerance within an environment, by having multiple instances of some infrastructure, as with a server cluster. Running an additional environment duplicates all of the infrastructure, creating a higher degree of fault tolerance, although at a higher cost. See [Link to Come] for more on this.\n\nScalability\n\nYou can spread a workload across multiple environments. People often do this geographically, with a separate environment for each region. Multiple environments may be used to achieve both scalability and fault tolerance. If there is a failure in one region, the load is shifted to another region’s environment until the failure is corrected.\n\nSegregation\n\nYou may run multiple instances of an application or service for different user bases, such as different clients. Running these instances in different environments can strengthen segregation. Stronger segregation may help meet legal or compliance requirements and give greater confidence to customers.",
      "content_length": 1484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "FOODSPIN EXAMPLE: MULTIPLE PRODUCTION ENVIRONMENTS\n\nIn the “Foodspin Example: Compute resources” example in Chapter 3 I explained that Foodspin runs a separate application server for each of its restaurant customers. As they expand to support customers in North America, Europe, and South Asia they decide to create a separate environment for each of these regions.",
      "content_length": 365,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Figure 7-2. Foodspin regional environments\n\nUsing fully separated environments, rather than having a single environment spread across the regions, helps them to ensure that they are complying with different regulations about storing customer data in different regions. Also, if they need to make changes that involve downtime, they can do so in each region at a different time. This makes it easier to align downtime to different timezones.\n\nLater, Foodspin lands a contract with a large fast-food chain, Hungry Hungry Horses. Hungry Hungry Horses is worried that having their customer data hosted on the same infrastructure with data from their competitors runs the risk of leaking data. So Foodspin’s team offers to run a completely separate environment dedicated to Hungry Hungry Horses, at a higher cost than running in a shared environment.\n\nEnvironments, consistency, and configuration\n\nSince multiple environments are meant to run instances of the same system, the infrastructure in each environment should be consistent. Consistency across environments is one of the main\n\ndrivers of Infrastructure as Code.\n\nDifferences between environments create the risk of inconsistent behavior. Testing software in one environment might not uncover problems that occur in another. It’s even possible that software\n\ndeploys successfully in some environments but not others.\n\nOn the other hand, you typically need some specific differences between environments. Test environments may be smaller than production environments. Different people may have different\n\nprivileges in different environments. Environments for different customers may have different features and characteristics. At the very least, names and IDs may be different (appserver-test, appserver-stage, appserver-prod). So you need to configure at\n\nleast some aspects of your environments.",
      "content_length": 1851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "A key consideration for environments is your testing and validation strategy. When the same infrastructure code is applied\n\nto every environment, testing it in one environment tends to give confidence that it will work correctly in other environments. You don’t get this confidence if the infrastructure varies much across\n\ninstances, however.\n\nYou may be able to improve confidence by testing infrastructure code using different configuration values. However, it may not be practical to test many different values. In these situations, you may need additional validation, such as post-provisioning tests or\n\nmonitoring production environments. I’ll go more in-depth on testing and validation in Chapter 9.\n\nPatterns for building environments\n\nAs I mentioned earlier, an environment is a conceptual collection of infrastructure elements, and a stack is a concrete collection of infrastructure elements. A stack project is the source code you use\n\nto create one or more stack instances. So how should you use stack projects and instances to implement environments?\n\nI’ll describe two antipatterns and one pattern for implementing environments using infrastructure stacks. Each of these patterns describes a way to define multiple environments using\n\ninfrastructure stacks. Some systems are composed of multiple stacks, as I described in “Patterns and antipatterns for structuring stacks”. I’ll explain what this looks like for multiple environments\n\nin “Building environments with multiple stacks”.\n\nAntipattern: Multiple-Environment Stack",
      "content_length": 1538,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "A Multiple-Environment Stack defines and manages the infrastructure for multiple environments as a single stack instance.\n\nFor example, if there are three environments for testing and\n\nrunning an application, a single stack project includes the code for all three of the environments.",
      "content_length": 284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "Figure 7-3. A multiple-environment stack manages the infrastructure for multiple environments as a single stack instance.\n\nALSO KNOWN AS\n\nOne stack with many environments\n\nAll your environments in one basket\n\nMOTIVATIONS\n\nMany people create this type of structure when they’re learning a new stack tool because it seems natural to add new environments\n\ninto an existing project.\n\nCONSEQUENCES\n\nWhen running a tool to update a stack instance, the scope of a\n\npotential change is everything in the stack. If you have a mistake 1 or conflict in your code, everything in the instance is vulnerable .\n\nWhen your production environment is in the same stack instance as another environment, changing the other environment risks\n\ncausing a production issue. A coding error, unexpected dependency, or even a bug in your tool can break production when you only meant to change a test environment.\n\nRELATED PATTERNS\n\nYou can limit the blast radius of changes by dividing environments into separate stacks. One obvious way to do this is the copy-paste\n\nenvironment (“Antipattern: Copy-Paste Environments”), where each environment is a separate stack project, although this is considered an antipattern.",
      "content_length": 1190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "A better approach is the reusable stack pattern (“Pattern: Reusable Stack”). A single project is used to define the generic structure for an environment and is then used to manage a separate stack\n\ninstance for each environment. Although this involves using a single project, the project is only applied to one environment instance at a time. So the blast radius for changes is limited to that one environment.\n\nAntipattern: Copy-Paste Environments\n\nThe Copy-Paste Environments antipattern uses a separate stack source code project for each infrastructure stack instance.\n\nIn our example of three environments named test, staging, and production, there is a separate infrastructure project for each of\n\nthese environments. Changes are made by editing the code in one environment and then copying the changes into each of the other environments in turn.",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "Figure 7-4. A copy-paste environment has a separate copy of the source code project for each instance.\n\nALSO KNOWN AS\n\nSingleton stack\n\nSnowflake stack\n\nCopy-pasta\n\nMOTIVATION\n\nCopy-paste environments are an intuitive way to maintain multiple\n\nenvironments. They avoid the blast radius problem of the multi- headed stack antipattern. You can also easily customize each stack\n\ninstance.\n\nAPPLICABILITY\n\nCopy-paste environments may be appropriate when you want to\n\nmaintain and change different instances and aren’t worried about\n\ncode duplication or consistency.\n\nCONSEQUENCES\n\nIt can be challenging to maintain multiple copy-paste\n\nenvironments. When you want to make a code change, you need to copy it to every project. You probably need to test each instance\n\nseparately, as a change may work in one but not another.\n\nCopy-paste environments often suffer from configuration drift (“Configuration Drift”). Using copy-paste environments for\n\ndelivery environments reduces the reliability of the deployment\n\nprocess and the validity of testing, due to inconsistencies from one environment to the next.",
      "content_length": 1100,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "IMPLEMENTATION\n\nYou create a copy-paste environment by copying the project code from one stack instance into a new project. You then edit the code\n\nto customize it for the new instance. When you make a change to\n\none stack, you need to copy and paste it across all of the other stack projects, while keeping the customizations in each one.\n\nRELATED PATTERNS\n\nIn cases where stack instances are meant to represent the same stack, the reusable stack pattern (“Pattern: Reusable Stack”) is\n\nusually more appropriate. The wrapper stack pattern (“Pattern: Wrapper Stack”) avoids the disadvantages of a copy-paste stack by\n\nhaving all of the logic in a module and only using each stack\n\nproject for configuration.\n\nPattern: Reusable Stack\n\nA Reusable Stack is an infrastructure source code project that is\n\nused to create multiple instances of a stack.",
      "content_length": 846,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Figure 7-5. A Reusable Stack is an infrastructure source code project that is used to create multiple instances of a stack.\n\nALSO KNOWN AS\n\nCookie Cutter Stack\n\nTemplate Stack\n\nLibrary Stack\n\nMOTIVATION\n\nYou create a reusable stack to maintain multiple consistent\n\ninstances of infrastructure. When you make changes to the stack code, you can apply and test it in one instance, and then use the\n\nsame code version to create or update multiple additional instances. You want to provision new instances of the stack with\n\nminimal ceremony, maybe even automatically.\n\nEXAMPLE OF HOW FOODSPIN MOVES TO REUSABLE STACKS\n\nIn “Example: Foodspin’s application infrastructure entity module”, the Foodspin team extracted common code from different stack projects that all used an application server. They put the common code into a module used by each of the stack projects. Later, they realized that the stack projects for their customer applications still looked very similar. In addition to using the module to create an application server, each stack had code to create databases and dedicated logging and reporting services for each customer.\n\nChanging and testing changes to this code across multiple customers was becoming a hassle, and they were signing up new customers every month. So the team decided to create a single stack project that defines a customer application stack. This project still uses the shared Java application server module, as do a few other applications (Jira and GoCD). But the project also has the code for setting up the rest of the per-customer infrastructure as well.\n\nNow, when they sign on a new customer, the team uses the common customer stack project to create a new instance. When they fix or improve something in the project codebase, they apply it to test instances to make sure it’s OK, and then they roll it out one by one to the customer instances.\n\nAPPLICABILITY",
      "content_length": 1900,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "You can use a reusable stack for delivery environments or for\n\nmultiple production environments. This pattern is useful when you don’t need much variation between the environments. It is less\n\napplicable when environments need to be heavily customized.\n\nCONSEQUENCES\n\nThe ability to provision and update multiple stacks from the same\n\nproject enhances scalability, reliability, and throughput. You can\n\nmanage more instances with less effort, make changes with a lower risk of failure, and roll changes out to more systems more\n\nrapidly.\n\nYou typically need to configure some aspects of the stack\n\ndifferently for different instances, even if it’s just what you name things. I’ll spend a whole chapter talking about this (Chapter 8).\n\nYou should test your stack project code before you apply changes\n\nto business-critical infrastructure. I’ll spend multiple chapters on\n\nthis, including Chapter 9 and Chapter 10.\n\nIMPLEMENTATION\n\nYou create a reusable stack as an infrastructure stack project, and\n\nthen run the stack management tool each time you want to create or update an instance of the stack. Use the syntax of the stack tool\n\ncommand to tell it which instance you want to create or update.\n\nWith Terraform, for example, you would specify a different state file or workspace for each instance. With CloudFormation, you\n\npass a unique stack ID for each instance.\n\nThe following example command provisions two stack instances from a single project using a fictional command called stack. The",
      "content_length": 1495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "command takes an argument env that identifies unique instances:\n\n> stack up env=test --source mystack/src SUCCESS: stack 'test' created > stack up env=staging --source mystack/src SUCCESS: stack 'staging' created\n\nAs a rule, you should use simple parameters to define differences\n\nbetween stack instances - strings, numbers, or in some cases, lists. Additionally, the infrastructure created by a reusable stack should\n\nnot vary much across instances.\n\nRELATED PATTERNS\n\nThe reusable stack is an improvement on the copy-paste\n\nenvironment antipattern (“Antipattern: Copy-Paste\n\nEnvironments”), making it easier to keep multiple instances consistent.\n\nStack code modules (Chapter 6) allow you to define code once and\n\nthen share it across multiple stack projects. Unlike a reusable\n\nstack, a stack module is not used directly to create infrastructure- instead, a stack project imports modules. So modules are more of a\n\nsupplement to reusable stacks than a substitute.\n\nThe wrapper stack pattern (“Pattern: Wrapper Stack”) uses\n\nmodules to implement a combination of copy-paste environments (one project per instance) and a reusable stack (one copy of the\n\ncode that defines the stack).\n\nThe patterns I describe in Chapter 8 offer ways to configure instances of reusable stacks.",
      "content_length": 1276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "Building environments with multiple stacks\n\nThe reusable stack pattern describes an approach for implementing\n\nmultiple environments. In Chapter 5 I described different ways to\n\nstructuring a system’s infrastructure across multiple stacks (“Patterns and antipatterns for structuring stacks”). There are\n\nseveral ways you can implement your stacks to combine these two\n\ndimensions of environments and system structure.\n\nThe simple case is implementing the complete system as a single stack. When you provision an instance of the stack, you have a\n\ncomplete environment. I depicted this in the diagram for the reusable stack pattern (Figure 7-5).\n\nBut you should split larger systems into multiple stacks. For\n\nexample, if you follow the service stack pattern (“Pattern: Service\n\nStack”) you have a separate stack for each service:",
      "content_length": 829,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Figure 7-6. Example using a separate infrastructure stack for each service\n\nTo create multiple environments, you provision an instance of each\n\nservice stack for each environment:",
      "content_length": 179,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Figure 7-7. Using multiple stacks to build each environment\n\nYou would use commands like the following to build a full\n\nenvironment with multiple stacks:\n\n> stack up env=staging --source product_browse_stack/src SUCCESS: stack 'product_browse-staging' created > stack up env=staging --source product_search_stack/src SUCCESS: stack 'product_search-staging' created > stack up env=staging --source shopping_basket_stack/src SUCCESS: stack 'shopping_basket-staging' created\n\nIn [Link to Come] I describe strategies for splitting systems into\n\nmultiple stacks, and how to integrate infrastructure across stacks.\n\nConclusion\n\nReusable stacks should be the workhorse pattern for most teams who need to manage large infrastructures. This pattern creates\n\nchallenges and opportunities. One challenge is how to configure\n\neach stack instance, which is the topic of Chapter 8, the next chapter.\n\nHowever, reusable stacks create the opportunity to test\n\ninfrastructure code before you apply it to environments you care\n\nabout. In Chapter 9 I’ll explain the core practice of continuously testing infrastructure code, and in Chapter 10 I’ll describe\n\nimplementation patterns following this practice with infrastructure\n\nstacks.\n\n1 Charity Majors shared her painful experiences of working with a multiple-\n\nenvironment stack in a blog post.",
      "content_length": 1327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "Chapter 8. Configuring Stacks\n\nUsing a single stack code project makes it easier to maintain multiple consistent instances of infrastructure, as I described in\n\n“Pattern: Reusable Stack”. However, you often need to customize stack instances. For example, you might run smaller clusters in development environments than in production. Here is an example\n\nof stack code that defines a container hosting cluster with\n\nconfigurable minimum and maximum numbers of servers:\n\ncontainer_cluster: web_cluster-${environment} min_size: ${cluster_minimum} max_size: ${cluster_maximum}\n\nYou pass different parameter values to this code for each environment, as depicted in Figure 8-1.",
      "content_length": 671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "Figure 8-1. Using the same code with different parameter values for each environment\n\nStack tools such as Terraform and CloudFormation support multiple ways of setting configuration parameter values. These typically include passing values on the command-line, reading them from a file, and having the infrastructure code retrieve them\n\nfrom a key-value store.\n\nTeams managing infrastructure need to decide how to use these features to manage and pass configuration values to their stack tool. It’s essential to ensure the values are defined and applied consistently to each environment.\n\nDESIGN PRINCIPLE: KEEP PARAMETERS SIMPLE\n\nA major reason for defining your infrastructure as code is to ensure systems are consistently configured, as described in “Principle: Minimize variation”). Configurable stack code creates the opportunity for inconsistency. The more configurable a stack project is, the more difficult it is to understand the behavior of different instances, to ensure you’re testing your code effectively, and to deliver changes regularly and reliably to all of your instances.\n\nSo it’s best to keep stack parameters simple and to use them in simple ways.\n\nPrefer simple parameter types, like strings, numbers, and perhaps lists and key-value maps. Avoid passing more complex data structures.\n\nMinimize the number of parameters that you can set for a stack. Avoid defining parameters that you “might” need. Only add parameter when you have an immediate need for it. You can always add a parameter later on if you discover you need it.\n\nAvoid using parameters as conditionals that create significant differences in the resulting infrastructure. For example, a boolean (yes/no) parameter to indicates whether to provision a service within a stack adds complexity.\n\nWhen it becomes hard to follow this advice, it’s probably a sign that you should refactor your stack code, perhaps splitting it into multiple stack projects.\n\nUsing stack parameters to create unique identifiers",
      "content_length": 1986,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "If you create multiple stack instances from the same project (as per the reusable stack pattern “Pattern: Reusable Stack”), you may see\n\nfailures from infrastructure resources that require unique identifiers. To see what I mean, look at the following pseudo-code\n\nthat defines an application server:\n\nserver: id: appserver subnet_id: appserver-subnet\n\nThe fictional cloud platform requires the id value to be unique, so when I run the stack command to create the second stack, it fails:\n\n> stack up environment=test --source mystack/src SUCCESS: stack 'test' created > stack up environment=staging --source mystack/src FAILURE: server 'appserver' already exists in another stack\n\nI can use parameters in my stack code to avoid these clashes. I change my code to take a parameter called environment and use it to assign a unique server ID. I also add the server into a different subnet in each environment:\n\nserver: id: appserver-${environment} subnet_id: appserver-subnet-${environment}\"\n\nNow I can run my fictional stack command to create multiple stack instances without error.\n\nExample stack parameters\n\nI’ll use an example stack to compare and contrast the different stack configuration patterns in this chapter. The example is a\n\ntemplate stack project that defines a container cluster, composed of",
      "content_length": 1303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "a dynamic pool of host nodes and some networking constructs. Here is the project structure:\n\nExample 8-1. Example project structure for a template stack that defines a container cluster ├── src/ │ ├── cluster.infra │ └── networking.infra └── test/\n\nThe cluster stack uses the parameters listed in Example 8-2 for three different stack instances. environment is a unique id for each environment, which can be used to name things and create unique identifiers. cluster_minimum and cluster_maximum define the range of sizes for the container host cluster. The infrastructure code in the file cluster.infra defines the cluster on the cloud platform, which scales the number of host nodes depending on load. Each of the three environments, test, staging,\n\nand production, uses a different set of values.\n\nExample 8-2. Example parameter values used for the pattern descriptions\n\nStack Instance\n\nenvironme nt\n\ncluster_minimu m\n\ncluster_maximu m\n\ncluster_test\n\ntest\n\n1\n\n1\n\ncluster_staging\n\nstaging\n\n2\n\n3\n\ncluster_productio n\n\nproduction\n\n2\n\n6\n\nHandling secrets as parameters\n\nIn “Secrets and source code”, I explained that you should not store unencrypted secrets in source code. Two of the solutions I",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "described (“Injecting secrets at runtime” and “Disposable secrets”)\n\nare based on passing secrets as parameter to stack code. To\n\nimplement these, you’ll need a way to manage those secrets on the system that runs your stack tool, whether it’s locally to a team\n\nmember’s workstation or laptop, or on a compute instance that’s running the tool as part of a pipeline or other service.\n\nI’ll include implementation details for secrets for each of the\n\nfollowing patterns. You may want to use one pattern for normal,\n\nnon-secret parameters, and a different pattern for secrets.\n\nMy examples also use an example secret, in this case, a passphrase for an SSL certificate passphrase, ssl_cert_passphrase. The value for this parameter in all environments is +correct horse 1 battery staple+ .\n\nPatterns for configuring stacks\n\nWe’ve looked at why you need to parameterize stacks and a bit on how the tools implement parameters. Now I’ll describe some\n\npatterns and antipatterns for managing parameters and passing\n\nthem to your tool:\n\nManual Stack Parameters: Run the stack tool and type the parameter values on the command line.\n\nScripted Parameters: Hard-code parameter values for each instance in a script that runs the stack tool.\n\nStack Configuration Files: Declare parameter values for each instance in configuration files kept in the stack code project.",
      "content_length": 1352,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "Wrapper Stack: Create a separate infrastructure stack project for each instance, and import a shared module with the stack code.\n\nPipeline Stack Parameters. Define parameter values in the configuration of a pipeline stage for each instance.\n\nStack Parameter Registry Pattern: Store parameter values in a central location.\n\nAntipattern: Manual Stack Parameters\n\nThe most natural approach to provide values for a stack instance is to type the values on the command-line manually, as in Example 8-3.\n\nExample 8-3. Example of manually typing command-line parameters > stack up environment=production --source mystck/src FAILURE: No such directory 'mystck/src' > stack up environment=production --source mystack/src SUCCESS: new stack 'production' created > stack destroy environment=production --source mystack/src SUCCESS: stack 'production' destroyed > stack up environment=production --source mystack/src SUCCESS: existing stack 'production' modified\n\nMOTIVATION\n\nIt’s dirt-simple to type values on the command-line, which is helpful when you’re learning how to use a tool.\n\nCONSEQUENCES\n\nIt’s easy to make a mistake when typing a value on the command- line. It can also be hard to remember which values to type. For infrastructure that people care about, you probably don’t want the\n\nrisk of accidentally breaking something important by mistyping a command when making an improvement or fix. When multiple",
      "content_length": 1405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "people work on an infrastructure stack, as in a team, you can’t expect everyone to remember the correct values to type for each\n\ninstance.\n\nManual stack parameters aren’t suitable for automatically applying infrastructure code to environments, such as with CI or CD.\n\nIMPLEMENTATION\n\nFor the example parameters (Example 8-2), pass the values on the command-line according to the syntax expected by the particular tool. With my fictional stack tool the command looks like this:\n\nstack up \\ environment=test \\ cluster_minimum=1 \\ cluster_maximum=1 \\ ssl_cert_passphrase=\"correct horse battery staple\"\n\nAnyone who runs the command needs to know the secrets, like passwords and keys, to use for a given environment and pass them on the command line. Your team should use a team password\n\n2\n\nmanagement tool to store and share them between team members securely, and rotate secrets when people leave the team.\n\nRELATED PATTERNS\n\nThe scripted parameters pattern (“Pattern: Scripted Parameters”) takes the command that you would type and puts it into a script. The pipeline stack parameters pattern (“Pattern: Pipeline Stack\n\nParameters”) does the same thing but puts them into the pipeline configuration instead of a script.\n\nPattern: Stack Environment Variables",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "The Stack Environment Variables pattern involves setting parameter values as environment variables for the stack tool to\n\nuse. This pattern is often combined with another pattern to set the environment variables.\n\nThe environment variables are set beforehand (see the implementation section for more on how):\n\nexport STACK_ENVIRONMENT=test export STACK_CLUSTER_MINIMUM=1 export STACK_CLUSTER_MAXIMUM=1 export STACK_SSL_CERT_PASSPHRASE=\"correct horse battery staple\"\n\nThere are different implementation options, but the most basic one is for the stack code to reference them directly:\n\ncontainer_cluster: web_cluster-${ENV(\"STACK_ENVIRONMENT\")} min_size: ${ENV(\"STACK_CLUSTER_MINIMUM\")} max_size: ${ENV(\"STACK_CLUSTER_MAXIMUM\")}\n\nMOTIVATION\n\nMost platforms and tools support environment variables, so it’s easy to do.\n\nAPPLICABILITY\n\nIf you’re already using environment variables in your system and have suitable mechanisms to manage them, you might find it convenient to use them for stack parameters.\n\nCONSEQUENCES\n\nYou need to use an additional pattern from this chapter to get the values to set. Doing this adds moving parts, making it hard to trace",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "configuration values for any given stack instance, and more work to change the values.\n\nUsing environment variables directly in stack code, as in the\n\nearlier example (???), arguably couples stack code too tightly to the runtime environment.\n\nSetting secrets in environment variables may expose them to other processes that run on the same system.\n\nIMPLEMENTATION\n\nAgain, you need to set the environment variables to use, which means selecting another pattern from this chapter. For example, if you expect people to set environment variables in their local\n\nenvironment to apply stack code, you are using the manual stack parameters antipattern (“Antipattern: Manual Stack Parameters”). You could set them in a script that runs the stack tool (the scripted\n\nparameters pattern “Pattern: Scripted Parameters”), or have the pipeline toolset them (“Pattern: Pipeline Stack Parameters”).\n\nAnother approach is to put the values into a script that people or instances import into their local environment. This is a variation of the stack configuration files pattern (“Pattern: Stack Configuration\n\nFiles”). The script to set the variables would be exactly like the earlier example (???), and any command that runs the stack tool would import it into the environment:\n\nsource ./environments/staging.env stack up --source ./src\n\nAlternately, you could build the environment values into a compute instance that runs the stack tool. For example, if you",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "provision a separate CD agent node to run the stack tool to build and update stacks in each environment, the code to build the node could set the appropriate values as environment variables. Those\n\nenvironment variables would be available to any command that runs on the node, including the stack tool.\n\nBut to do this, you need to pass the values to the code that builds your agent nodes. So you need to select another pattern from this\n\nchapter to do that.\n\nThe other side of implementing this pattern is how the stack tool gets the environment values. The earlier example (<<???) shows how stack code can directly read environment variables.\n\nBut you could, instead, use a stack orchestration script ([Link to\n\nCome]) to read the environment variables and pass them to the stack tool on the command line. The code in the orchestration script would look like this:\n\nstack up \\ environment=${STACK_ENVIRONMENT} \\ cluster_minimum=${STACK_CLUSTER_MINIMUM} \\ cluster_maximum=${STACK_CLUSTER_MAXIMUM} \\ ssl_cert_passphrase=\"${STACK_SSL_CERT_PASSPHRASE}\"\n\nThis approach decouples your stack code from the environment it runs in.\n\nRELATED PATTERNS\n\nAny of the other patterns in this chapter can be combined with this one to set environment values.\n\nPattern: Scripted Parameters",
      "content_length": 1272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "Scripted Parameters involves hard-coding the parameter values into a script that runs the stack tool. You can write a separate script for each environment or a single script which includes the values\n\nfor all of your environments:\n\nif ${ENV} == \"test\" stack up cluster_maximum=1 env=\"test\" elsif ${ENV} == \"staging\" stack up cluster_maximum=3 env=\"staging\" elsif ${ENV} == \"production\" stack up cluster_maximum=5 env=\"production\" end\n\nALSO KNOWN AS\n\nEnvironment provisioning script\n\nMOTIVATION\n\nScripts are a simple way to capture the values for each instance, avoiding the problems with the “Antipattern: Manual Stack\n\nParameters”. You can be confident that values are used consistently for each environment. By checking the script into version control, you ensure you are tracking any changes to the\n\nconfiguration values.\n\nAPPLICABILITY\n\nA stack provisioning script is a useful way to set parameters when you have a fixed set of environments that don’t change very often.\n\nIt doesn’t require the additional moving parts of some of the other\n\npatterns in this chapter.\n\nBecause it is wrong to hard-code secrets in scripts (as you know from reading “Secrets and source code”), this pattern is not\n\nsuitable for secrets. That doesn’t mean you shouldn’t use this",
      "content_length": 1261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "pattern, only that you’ll need to implement a separate pattern for\n\ndealing with secrets.\n\nCONSEQUENCES\n\nIt’s common for the commands used to run the stack tool to\n\nbecome complicated over time. Provisioning scripts can grow into\n\nmessy beasts. [Link to Come] discusses how these scripts are used and outlines pitfalls and recommendations for keeping them\n\nmaintainable. You should test provisioning scripts since they can\n\nbe a source of issues with the systems they provision.\n\nIMPLEMENTATION\n\nThere are two common implementations for this pattern. One is a\n\nsingle script that takes the environment as a command-line argument, with hard-coded parameter values for each\n\nenvironment. Example 8-4 is a simple example of this.\n\nExample 8-4. Example of a script that includes the parameters for multiple environments #!/bin/sh\n\ncase $1 in test) CLUSTER_MINIMUM=1 CLUSTER_MAXIMUM=1 ;; staging) CLUSTER_MINIMUM=2 CLUSTER_MAXIMUM=3 ;; production) CLUSTER_MINIMUM=2 CLUSTER_MAXIMUM=6 ;; *) echo \"Unknown environment $1\" exit 1 ;;",
      "content_length": 1024,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "esac\n\nstack up \\ environment=$1 \\ cluster_minimum=${CLUSTER_MINIMUM} \\ cluster_maximum=${CLUSTER_MAXIMUM}\n\nAnother implementation is a separate script for each stack\n\ninstance, as in Example 8-5.\n\nExample 8-5. Example project structure with a script for each environment. our-infra-stack/ ├── bin/ │ ├── test.sh │ ├── staging.sh │ └── production.sh ├── src/ └── test/\n\nEach of these scripts is identical but has different parameter values hard-coded in it. The scripts are smaller because they don’t need\n\nlogic to select between different parameter values. However, they\n\nneed more maintenance. If you need to change the command, you need to make it across all of the scripts. Having a script for each\n\nenvironment also tempts people to customize different\n\nenvironments, creating inconsistency.\n\nCommit your provisioning script or scripts to source control. Putting it in the same project as the stack it provisions ensures that\n\nit stays in sync with the stack code. For example, if you add a new\n\nparameter, you add it to the infrastructure source code and also to your provisioning script. You always know which version of the\n\nscript to run for a given version of the stack code.\n\n[Link to Come] discusses the use of scripts to run stack tools in\n\nmuch more detail.",
      "content_length": 1271,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "As mentioned earlier, you shouldn’t hard-code secrets into scripts,\n\nso you’ll need to use a different pattern for those. You can use the\n\nscript to support that pattern. In this example, a command-line tool fetches the secret from a secrets manager, following the parameter\n\nregistry pattern (“Pattern: Stack Parameter Registry”):\n\nExample 8-6. Fetching a key from a secrets manager in a script ... # (Set environment specific values as in other examples) ...\n\nSSL_CERT_PASSPHRASE=$(some-tool get-secret id=\"/ssl_cert_passphrase/${ENV}\")\n\nstack up \\ environment=${ENV} \\ cluster_minimum=${CLUSTER_MINIMUM} \\ cluster_maximum=${CLUSTER_MAXIMUM} \\ ssl_cert_passphrase=\"${SSL_CERT_PASSPHRASE}\"\n\nThe some-tool command connects to the secrets manager and retrieves the secret for the relevant environment using the ID /ssl_cert_passphrase/${ENV}. This example assumes the session is authorized to use the secrets manager. An infrastructure\n\ndeveloper may use the tool to start a session before running this script. Or the compute instance that runs the script may be\n\nauthorized to retrieve secrets using secretless authorization (as I described in “Secretless authorization”).\n\nRELATED PATTERNS\n\nProvisioning scripts run the command-line tool for you, so are a way to move beyond the manual stack parameters antipattern\n\n(“Antipattern: Manual Stack Parameters”). The stack configuration\n\nfiles pattern (“Pattern: Stack Configuration Files”) extracts the parameter values from the script into separate files.",
      "content_length": 1503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Pattern: Stack Configuration Files\n\nStack Configuration Files manage parameter values for each\n\ninstance in a separate file, which you manage in version control with your stack code.\n\n├── src/ │ ├── cluster.infra │ ├── host_servers.infra │ └── networking.infra ├── environments/ │ ├── test.properties │ ├── staging.properties │ └── production.properties └── test/\n\nALSO KNOWN AS\n\nEnvironment configuration files\n\nMOTIVATION\n\nCreating configuration files for a stack’s instances is\n\nstraightforward and easy to understand. Because the file is committed to the source code repository, it is easy:\n\nTo see what values are used for any given environment (“what is the maximum cluster size for production?”),\n\nto trace the history for debugging (“when did the maximum cluster size change?”),\n\nand to audit changes (“who changed the maximum cluster size?”).\n\nStack configuration files enforce the separation of configuration from the stack code.\n\nAPPLICABILITY",
      "content_length": 954,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "Stack configuration files are appropriate when the number of\n\nenvironments doesn’t change often. They require you to add a file to your project to add a new stack instance. They also require (and\n\nhelp enforce) consistent logic in how different instances are\n\ncreated and updated, since the configuration files can’t include logic.\n\nCONSEQUENCES\n\nWhen you want to create a new stack instance, you need to add a new configuration file to the stack project. Doing this prevents you\n\nfrom automatically creating new environments on the fly. In\n\n“Pattern: Ephemeral test stack”, I describe an approach for managing test environments that relies on creating environments\n\nautomatically. You could work around this by creating a\n\nconfiguration file for an ephemeral environment on demand.\n\nParameter files can add friction for changing the configuration of downstream environments in a change delivery pipeline of the\n\nkind I describe in [Link to Come]. Every change to the stack\n\nproject code must progress through each stage of the pipeline before being applied to production. It can take a while for this to\n\ncomplete and doesn’t add any value when the configuration\n\nchange is only applied to production.\n\nDefining parameter values can be a source of considerable complexity in provisioning scripts. I’ll talk about this more in\n\n[Link to Come], but as a teaser, consider that teams often want to define default values for stack projects, and for environments, and\n\nthen need logic to combine these into values for a given instance\n\nof a given stack in a different environment. Inheritance models for parameter values can get messy and confusing.",
      "content_length": 1644,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "Configuration files in source control should not include secrets. So\n\nfor secrets, you weither need to select an additional pattern from this chapter to handle secrets or implement a separate secrets\n\nconfiguration file outside of source control.\n\nIMPLEMENTATION\n\nYou define stack parameter values in a separate file for each\n\nenvironment, as shown in the earlier example project structure\n\n(???).\n\nThe contents of a parameter file could look like this:\n\nenv = staging cluster_minimum = 2 cluster_maximum = 3\n\nPass the path to the relevant parameter file when running the stack\n\ncommand:\n\nstack up --source ./src --config ./environments/staging.properties\n\nIf the system is composed of multiple stacks, then it can get messy to manage configuration across all of the environments. There are\n\ntwo common ways of arranging parameter files in these cases. One\n\nis to put configuration files for all of the environments with the code for each stack:\n\n├── cluster_stack/ │ ├── src/ │ │ ├── cluster.infra │ │ ├── host_servers.infra │ │ └── networking.infra │ └──environments/ │ ├── test.properties │ ├── staging.properties │ └── production.properties └── appserver_stack/",
      "content_length": 1165,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "├── src/ │ ├── server.infra │ └── networking.infra └──environments/ ├── test.properties ├── staging.properties └── production.properties\n\nThe other is to centralize the configuration for all of the stacks in\n\none place:\n\n├── cluster_stack/ │ ├── cluster.infra │ ├── host_servers.infra │ └── networking.infra ├── appserver_stack/ │ ├── server.infra │ └── networking.infra └── environments/ ├── test/ │ ├── cluster.properties │ └── appserver.properties ├── staging/ │ ├── cluster.properties │ └── appserver.properties └── production/ ├── cluster.properties └── appserver.properties\n\nEach approach can become messy and confusing in its own way. When you need to make a change to all of the things in an\n\nenvironment, making changes to configuration files across dozens\n\nof stack projects is painful. When you need to change the configuration for a single stack across the various environments\n\nit’s in, trawling through a tree full of configuration for dozens of\n\nother stacks is also not fun.\n\nIf you want to use configuration files to provide secrets, rather than using a separate pattern for secrets, then you need to manage\n\nthose files outside of the project code checked into source control.",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "For local development environments, you can require users to\n\ncreate the file in a set location manually. Pass the file location to the stack command like this:\n\nstack up --source ./src \\ --config ./environments/staging.properties \\ --config ../.secrets/staging.properties\n\nIn this example, you provide two --config arguments to the stack tool, and it reads parameter values from both. You have a directory named .secrets outside the project folder, so it is not in source control.\n\nIt can be trickier to do this when running the stack tool automatically, from a compute instance like a CD pipeline agent.\n\nYou could provision similar secrets property files onto these\n\ncompute instances, but that can expose secrets to other processes that run on the same agent. You also need to provide the secrets to\n\nthe process that builds the compute instance for the agent, so you\n\nstill have a bootstrapping problem.\n\nRELATED PATTERNS\n\nPutting configuration values into files simplifies the provisioning\n\nscripts described in “Pattern: Scripted Parameters”. You can avoid some of the limitations of environment configuration files by using\n\nthe “Pattern: Stack Parameter Registry” instead. Doing this moves\n\nparameter values out of the stack project code and into a central location, which allows you to use different workflows for code and\n\nconfiguration.\n\nPattern: Wrapper Stack",
      "content_length": 1372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "A Wrapper Stack uses an infrastructure stack project for each\n\ninstance as a wrapper to import a stack code module (see Chapter 6). Each wrapper project defines the parameter values for\n\none instance of the stack. It then imports a module shared by all of\n\nthe stack instances.",
      "content_length": 277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "Figure 8-2. A Wrapper Stack uses an infrastructure stack project for each instance as a wrapper to import a stack code module\n\nMOTIVATION\n\nA wrapper stack leverages the stack tool’s module functionality to\n\nre-use shared code across stack instances. You can use the tool’s module versioning, dependency management, and artifact\n\nrepository functionality to implement a change delivery pipeline ([Link to Come]). As of this writing, most infrastructure stack\n\ntools don’t have a project packaging format that you can use to\n\nimplement pipelines for stack code. So you need to create a custom stack packaging process yourself. You can work around\n\nthis by using a wrapper stack, and versioning and promoting your\n\nstack code as a module.\n\nWith wrapper stacks, you can write the logic for provisioning and configuring stacks in the same language that you use to define\n\nyour infrastructure, rather than using a separate language as you\n\nwould with a provisioning script (“Pattern: Scripted Parameters”).\n\nCONSEQUENCES\n\nModules add an extra layer of complexity between your stack and\n\nthe code contained in the module. You now have two levels: the stack project, which contains the wrapper projects, and the module\n\nwhich contains the code for the stack.\n\nBecause you have a separate code project for each stack instance,\n\npeople may be tempted to add custom logic for each instance. Custom instance code makes your codebase inconsistent and hard\n\nto maintain.",
      "content_length": 1456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "Because you define parameter values in wrapper projects managed\n\nin source control, you can’t use this pattern to manage secrets. So you need to add a another pattern from this chapter to provide\n\nsecrets to stacks.\n\nIMPLEMENTATION\n\nEach stack instance has a separate infrastructure stack project. For\n\nexample, you would have a separate Terraform project for each environment. You can implement this like a copy-paste\n\nenvironment (“Antipattern: Copy-Paste Environments”), with each\n\nenvironment in a separate repository.\n\nAlternatively, each environment project could be a folder in a single repository:\n\nmy_stack/ ├── test/ │ └── stack.infra ├── staging/ │ └── stack.infra └── production/ └── stack.infra\n\nDefine the infrastructure code for the stack as a module, according\n\nto your tool’s implementation. You could put the module code in the same repository with your wrapper stacks. However, this\n\nwould prevent you from leveraging module versioning\n\nfunctionality. That is, you wouldn’t be able to use different\n\nversions of the infrastructure code in different environments,\n\nwhich is crucial for progressively testing your code.\n\nThe following example is a wrapper stack that imports a module called container_cluster_module, specifying the version of the module, and the configuration parameters to pass to it:",
      "content_length": 1319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "module: name: container_cluster_module version: 1.23 parameters: env: test cluster_minimum: 1 cluster_maximum: 1\n\nThe wrapper stack code for the staging and production\n\nenvironments is similar, other than the parameter values, and\n\nperhaps the module version they use.\n\nThe project structure for the module could look like this:\n\n├── container_cluster_module/ │ ├── cluster.infra │ └── networking.infra └── test/\n\nWhen you make a change to the module code, you test and upload it to a module repository. How the repository works depends on\n\nyour particular infrastructure stack tool. You can then update your\n\ntest stack instance to import the new module version and apply it\n\nto the test environment.\n\nTerragrunt is a stack orchestration tool that implements the\n\nwrapper stack pattern.\n\nRELATED PATTERNS\n\nA wrapper stack is similar to the scripted parameters pattern. The\n\nmain differences are that it uses your stack tool’s language rather\n\nthan a separate scripting language and that the infrastructure code\n\nis in a separate module.\n\nPattern: Pipeline Stack Parameters",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "With the Pipeline Stack Parameters pattern, you define values for\n\neach instance in the configuration of a delivery pipeline.\n\nI explain how to use a change delivery pipeline to apply\n\ninfrastructure stack code to environments in [Link to Come]. You can implement a pipeline using a tool like Jenkins, GoCD, or\n\nConcourseCI (see “Delivery pipeline software and services” for\n\nmore on these tools).",
      "content_length": 397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Figure 8-3. Each stage that applies the stack code passes the relevant configuration values for the environment.\n\nMOTIVATION\n\nIf you’re using a pipeline tool to run your infrastructure stack tool,\n\nit provides the mechanism for storing and passing parameter values to the tool out of the box. Assuming your pipeline tool is\n\nitself configured by code, then the values are defined as code and\n\nstored in version control.\n\nConfiguration values are kept separate from the infrastructure code. You can change configuration values for downstream\n\nenvironments and apply them immediately, without needing to\n\nprogress a new version of the infrastructure code from the start of\n\nthe pipeline.\n\nAPPLICABILITY\n\nTeams who are already using a pipeline to apply infrastructure\n\ncode to environments can easily leverage this to set stack\n\nparameters for each environment. However, if stacks require more\n\nthan a few parameter values, defining these in the pipeline configuration has serious drawbacks, so you should avoid this.\n\nCONSEQUENCES\n\nBy defining stack instance variables in the pipeline configuration, you couple configuration values with your delivery process. There\n\nis a risk of the pipeline configuration becoming complicated and\n\nhard to maintain.\n\nThe more configuration values you define in your pipeline, the harder it is to run the stack tool outside the pipeline. Your pipeline",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "can become a single point of failure-you may not be able to fix,\n\nrecover, or rebuild an environment in an emergency until you have\n\nrecovered your pipeline. And it can be hard for your team to develop and test stack code outside the pipeline.\n\nIn general, it’s best to keep the pipeline configuration for applying\n\na stack project as small and simple as possible. Most of the logic should live in a script called by the pipeline, rather than in the\n\npipeline configuration.\n\nCI SERVERS, PIPELINES, AND SECRETS\n\nThe first thing most attackers look for when they gain access to a corporate network is CI and CD servers. These are well-known treasure troves of passwords and keys that they can exploit to inflict the maximum evil on your users and customers.\n\nMost of the CI and CD tools that I’ve worked with do not provide a very robust security model. You should assume that anyone who has access to your pipeline tool or who can modify code that the tool executes (i.e., probably every developer in your organization) can access any secret stored by the tool.\n\nThis is true even when the tool encrypts the secrets, because the tool can also decrypt the secrets. If you can get the tool to run a command, you can usually get it to decrypt any secret it stores. You should careful analyze any CI or CD tool you use to assess how well it supports your organization’s security requirements.\n\nIMPLEMENTATION\n\nParameters should be implemented using “as code” configuration of the pipeline tool:\n\nExample 8-7. Example pipeline stage configuration",
      "content_length": 1541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "stage: apply-test-stack input_artifacts: container_cluster_stack commands: unpack ${input_artifacts} stack up --source ./src environment=test cluster_minimum=1 cluster_maximum=1 stack test environment=test\n\nThis example passes the values on the command line. You may\n\nalso set them as environment variables that the stack code uses (as\n\ndescribed in “Pattern: Stack Environment Variables”):\n\nExample 8-8. Example pipeline stage configuration using environment variables stage: apply-test-stack input_artifacts: container_cluster_stack environment_vars: STACK_ENVIRONMENT=test STACK_CLUSTER_MINIMUM=1 STACK_CLUSTER_MAXIMUM=1 commands: unpack ${input_artifacts} stack up --source ./src stack test environment=test\n\nIn this example, the pipeline toolsets those environment variables before running the commands.\n\nMany pipeline tools provide secret management features that you\n\ncan use to pass secrets to your stack command. You set the secret\n\nvalues in the pipeline tool in some fashion, and can then refer to them in your pipeline job:\n\nExample 8-9. Example pipeline stage with secret stage: apply-test-stack input_artifacts: container_cluster_stack commands: unpack ${input_artifacts} stack up --source ./src environment=test \\ cluster_minimum=1 \\ cluster_maximum=1 \\ ssl_cert_passphrase=${STACK_SSL_CERT_PASSPHRASE}",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "RELATED PATTERNS\n\nDefining the commands and parameters to apply stack code for\n\neach environment in pipeline configuration is similar to the\n\nscripted parameters pattern. The difference is where the scripting\n\nlives-in the pipeline configuration versus in script files.\n\nPattern: Stack Parameter Registry\n\nA Stack Parameter Registry manages the parameter values for stack instances in a central location, rather than with your stack\n\ncode. The stack tool retrieves the relevant values when it applies\n\nthe stack code to a given instance.",
      "content_length": 537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "Figure 8-4. Stack instance parameter values stored in a central registry.\n\nCONFIGURATION REGISTRIES AND STACK PARAMETER REGISTRIES\n\nI use the term “configuration registry” to describe a service which stores configuration values that may be used for many purposes, including service discovery, stack integration, or monitoring configuration. I’ll describe this in more detail in “Configuration Registry”.\n\nWhen talking specifically about storing configuration values for stack instances, I use the term “stack parameter registry”. So a stack parameter registry is a specific use case for a configuration registry.\n\nALSO KNOWN AS\n\nConfiguration registry for stacks, Infrastructure configuration\n\nregistry\n\nMOTIVATION\n\nStoring parameter values in a registry separates configuration from\n\nimplementation. Parameters in a registry can be set, used, and\n\nviewed by different tools, using different languages and technologies. This flexibility reduces coupling between different\n\nparts of the system. You can replace any tool that uses the registry\n\nwithout affecting any other tool that uses it.\n\nBecause they are tool-agnostic, stack parameter registries can act as a source of truth for infrastructure and even system\n\nconfiguration, acting as a Configuration Management Database\n\n(CMDB - see “Configuration Management Database (CMDB)”).\n\nThis configuration data can be useful in regulated contexts,\n\nmaking it easy to generate reports for auditing.",
      "content_length": 1445,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "APPLICABILITY\n\nIf you are using a configuration registry for other purposes, it\n\nmakes sense to use it as a stack parameter registry, as well. For\n\nexample, a configuration registry is a useful way to integrate multiple stacks (see [Link to Come]).\n\nCONSEQUENCES\n\nA stack parameter registry requires a configuration registry, which is an extra moving part for your overall system. The registry is a\n\ndependency for your stack and a potential point of failure. If the\n\nregistry becomes unavailable, it may be impossible to re-provision\n\nor update the infrastructure stack until you can restore it. This\n\ndependency can be painful in disaster recovery scenarios, putting\n\nthe registry service on the critical path.\n\nManaging parameter values separately from the stack code that\n\nuses it has tradeoffs. You can change the configuration of a stack\n\ninstance without making a change to the stack project. If one team maintains a reusable stack project, other teams can use it to create\n\ntheir own stack instances without needing to add or change\n\nconfiguration files in the stack project itself.\n\nOn the other hand, making changes across more than one place- stack project and parameter registry-adds complexity and\n\nopportunity for mistakes.\n\nIMPLEMENTATION\n\nI’ll discuss ways to implement a parameter registry below\n\n(“Configuration Registry”). In short, it may be a service that stores\n\nkey/value pairs, or it could be a file or directory structure of files",
      "content_length": 1455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "that contain key/value pairs. Either way, parameter values can\n\nusually be stored in a hierarchical structure, so you can store and find them based on the environment and the stack, and perhaps\n\nother factors like the application, service, team, geography, or\n\ncustomer.\n\nThe values for this chapter’s example container cluster could look like:\n\nExample 8-10. Example of configuration registration entries └── environments/ ├── test/ │ └── container_cluster/ │ ├── cluster_minimum = 1 │ └── cluster_maximum = 1 ├── staging/ │ └── container_cluster/ │ ├── cluster_minimum = 2 │ └── cluster_maximum = 3 └── production/ └── container_cluster/ ├── cluster_minimum = 2 └── cluster_maximum = 6\n\nWhen you apply the infrastructure stack code to an instance, the\n\nstack tool uses the key to retrieve the relevant value. You will need to pass the environment parameter to the stack tool, and the code uses this to refer to the relevant location in the registry:\n\ncluster: id: container_cluster-${environment} minimum: ${get_registry_item(\"/environments/${environment}/container_cluster/c luster_minimum\")} maximum: ${get_registry_item(/environments/${environment}/container_cluster/cl uster_maximum\")}\n\nThe get_registry_item() function in the stack code looks up the value.",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "This implementation ties your stack code to the configuration\n\nregistry. You need the registry to run and test your code, which can\n\nbe too heavy. You could work around this by fetching the values\n\nfrom the registry in a script. The script then passes them to the\n\nstack code as normal parameters. Doing this gives you the flexibility to set parameter values in other ways. For reusable stack\n\ncode this is particularly useful, giving users of your code more\n\noptions for how to configure their stack instances.\n\nSecrets management services (“Secrets management”) are a\n\nspecial type of parameter registry. Used correctly, they ensure that\n\nsecrets are only available to people and services that require them,\n\nwithout exposing them more widely. Some configuration registry\n\nproducts and services can be used to store both secret and non-\n\nsecret values. But it’s important to avoid storing secrets in registries which don’t protect them. Doing so makes the registry\n\nan easy target for attackers.\n\nRELATED PATTERNS\n\nYou probably need to pass at least one parameter to the stack tool\n\nto indicate which stack instance’s parameters to use. You can use\n\neither the stack provisioning script or pipeline stack parameter\n\npattern for this.\n\nConfiguration Registry\n\nLarger organizations with many teams working across larger\n\nsystems with many moving parts often find a configuration\n\nregistry useful. It can be useful for configuring stacks instances, as\n\nI described in “Pattern: Stack Parameter Registry”. It can also be\n\nuseful for managing integration dependencies across different",
      "content_length": 1581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "stack instances, applications, and other services, as I’ll explain in\n\n[Link to Come].\n\nAnd a registry can provide a useful source of information about\n\nthe composition and state of your infrastructure. You can use this\n\nto create tools, dashboards, and reports, as well as for monitoring\n\nand auditing your systems.\n\nSo it’s worth digging into how to implement and use a\n\nconfiguration registry.\n\nImplementing a Configuration Registry\n\nThere are different ways to build a configuration registry. You can\n\nuse a registry provided out of the box by your infrastructure\n\nautomation tool. Or you can run a general-purpose registry\n\nproduct. Most cloud providers also have configuration registry\n\nservices that you can use. If you are brave, you can hand-roll a\n\npractical registry using fairly basic pieces.\n\nINFRASTRUCTURE AUTOMATION TOOL REGISTRIES\n\nMany infrastructure automation toolchains include a configuration\n\nregistry service. These tend to be part of a centralized service that\n\nmay also include features such as source code management, monitoring, dashboards, and command orchestration. Examples of\n\nthese include:\n\nChef Infra Server\n\nPuppetDB\n\nAnsible Tower",
      "content_length": 1167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "Salt Mine\n\nYou may be able to use these services with tools outside the\n\ntoolchain that provides them. Most can expose values, so you\n\ncould write a script that discovers information about the current\n\nstate of infrastructure managed by the configuration tool. Some\n\ninfrastructure tool registries are extensible, so you can use them to store the data from other tools.\n\nHowever, this creates a dependency on whatever toolchain\n\nprovides the registry service. The service may not fully support\n\nintegration with third-party tools. They might not offer a contract or API that guarantees future compatibility.\n\nSo if you’re considering using an infrastructure tool’s data store as\n\na general-purpose configuration registry, consider how well it\n\nsupports this use case, and what kind of lock-in it creates.\n\nGENERAL PURPOSE CONFIGURATION REGISTRY PRODUCTS\n\nThere are many dedicated configuration registry and key-value\n\nstore database products available outside the toolchains of a\n\nparticular automation tool. Some examples include:\n\nZookeeper\n\n3 Consul\n\netcd\n\ndoozerd\n\nThese are generally compatible with different tools, languages, and\n\nsystems, so avoid locking you into any particular tool-chain.",
      "content_length": 1199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "However, it can take a fair bit of work to define how data should be stored. Should keys be structured like\n\nenvironment/service/application, service/application/environment,\n\nor something else entirely? You may need to write and maintain\n\ncustom code to integrate different systems with your registry. And\n\na configuration registry gives your team yet another thing to\n\ndeploy and run.\n\nPLATFORM REGISTRY SERVICES\n\nMost cloud platforms provide a key-value store service, such as\n\nthe AWS SSM Parameter Store. These give you most of the advantages of a general-purpose configuration registry product,\n\nwithout forcing you to install and support it yourself. However, it\n\ndoes tie you to that cloud provider. In some cases, you may find\n\nyourself using a registry service on one cloud to manage\n\ninfrastructure running on another!\n\nDIY CONFIGURATION REGISTRIES\n\nRather than running a configuration registry server, some teams\n\nbuild a custom lightweight configuration registry by storing\n\nconfiguration files in a central location, or by using distributed storage. They typically use an existing file storage service like an\n\nobject store (e.g., an S3 bucket on AWS), a version control system,\n\nnetworked filesystem, or even a web server.\n\nA variation of this is packaging configuration settings into system packages, such as a .deb or .rpm file, and pushing them to an\n\ninternal APT or YUM repository. You can then download\n\nconfiguration files to local servers using the standard package\n\nmanagement tool.",
      "content_length": 1506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "Another variation is using a standard relational or document store\n\ndatabase server.\n\nAll of these approaches leverage existing services, so they can be\n\nquick to implement for a simple project rather than needing to\n\ninstall and run a new server. But when you get beyond trivial\n\nsituations, you may find yourself building and maintaining the\n\nfunctionality that you could get off the shelf.\n\nSingle or multiple configuration registries\n\nCombining all configuration values from across all of your\n\nsystems, services, and tools is an appealing idea. You could keep\n\neverything in one place rather than sprawling across many\n\ndifferent systems. “One registry to rule them all.” However, this\n\nisn’t always practical in larger, more heterogeneous environments.\n\nMany tools, such as monitoring services and server configuration\n\nsystems, have their own registry. You’ll often find different\n\nregistry and directory products that are very good at specific tasks,\n\nsuch as license management, service discovery, and user directories. Bending all of these tools to use a single system\n\ncreates an ongoing flow of work. Every update to every tool needs\n\nevaluation, testing, and potentially more work to maintain the\n\nintegration.\n\nIt may be better to pull relevant data from across the services\n\nwhere they are stored. Make sure you know which system is the\n\nsource of truth for any particular data or configuration item.\n\nDesign your systems and tools with this understanding.",
      "content_length": 1471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "Some teams use messaging systems to share configuration data as events. Whenever a system changes a configuration value, it sends\n\nan event. Other systems can monitor the event queue for changes\n\nto configuration items in which they are interested.\n\nConfiguration Management Database (CMDB)\n\nA Configuration Management Database (CMDB) is a directory of\n\ninformation about IT assets. Many organizations use these for auditing, control, and governance. They usually include physical\n\nassets like servers, laptops, and racks, as well as software,\n\nconfiguration, versions, and licenses.\n\nThe cloud age is challenging for traditional approaches to CMDBs.\n\nAssumptions about relationships between software, configuration,\n\ndata, and hardware no longer hold with virtualization and cloud\n\nplatforms. Top-down processes to approve and make configuration\n\nchanges struggle to cope with systems that automatically add,\n\nremove, and reconfigure resources on the fly.\n\nIt’s useful to consider the outcomes you need from a CMDB and\n\nlook at ways to achieve these in dynamic environments. Too many\n\norganizations implement CMDBs, assuming there is value in\n\nassembling all information in one place. Instead, they should start by considering how they will use the information, and work back\n\nfrom there to find the best solutions.\n\nTwo reasons organizations use CMDBs are to provide visibility\n\nand to control changes.\n\nCMDB FOR VISIBILITY",
      "content_length": 1425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "Data visibility supports many use cases. These include managing\n\ncosts, identifying policy conflicts, and surfacing security\n\nvulnerabilities. For each of your organization’s use cases, be sure\n\nyou understand the requirement. Identify how information needs to be presented to support it, for example, dashboards, alerts, and\n\nreports.\n\nThen, look at ways to implement the presentation of the data, and\n\nwork backward to solutions for collating it.\n\nIt’s essential to understand which system is the source of truth, or\n\nsystem of record, for a given type of data. You may need to extract\n\ndata from multiple systems of record for a particular use case. For\n\nmany purposes, it’s best to collect data directly, rather than marshal it into a separate dataset. For example, writing scripts that\n\ndirectly probe your systems is more reliable for auditing and\n\ncompliance than examining a CMDB, which may or may not be\n\naccurate and timely.\n\nCMDB TO CONTROL CHANGES\n\nSome organizations use a CMDB system as a way to configure\n\ntheir systems from a central location. A CMDB product may offer\n\nsophisticated permission models and workflows to create tight controls over changes.\n\nYou should consider the different elements of your system and\n\nhow each is defined and changed. For configuration parameters\n\nsuch as stack instance parameters, you might want a system that ensures changes are made by authorized users and are tracked and\n\napproved where appropriate.",
      "content_length": 1455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "These solutions may not work as well for code that defines infrastructure stacks, server configuration, deployment processes,\n\nand the like. The idea of infrastructure as code is to manage\n\nchanges to these using source control systems, automated tests,\n\nand change delivery pipelines. These provide capabilities for\n\nauthorization, auditing, and quality enforcement. Adding another\n\ntool in addition to these, especially one not designed to support agile engineering practices, usually adds more complexity, friction,\n\nand risk than value.\n\nConclusion\n\nThe past few chapters have demonstrated how to apply the core practice of defining systems as code to infrastructure stacks. Each\n\nchapter has described patterns and antipatterns for implementing\n\nstacks as code.\n\nThe next chapter moves on to the next core practice of infrastructure as code, continuously validating code as you work\n\non it. Following that, I’ll describe implementation patterns to\n\napply that practice to infrastructure stacks. Afterward, I’ll show\n\nhow to use stacks to build application runtime environments-by\n\nwhich I mean servers, containers, and clusters-following these core\n\npractices.\n\n1 This passphrase comes from Randall Munroe’s XKCD comic Password Strength. I use it here so that if you haven’t read that comic, you will, because it makes an important point about good passwords. And if you have read the comic, then you and I share the smug glow that comes from recognizing a somewhat obscure reference.\n\n2 Some examples of tools teams can use to securely share passwords include GPG,\n\nKeePass, 1Password, Keeper, and LastPass",
      "content_length": 1612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "3 Consul is a product of Hashicorp, which also makes Terraform, and of course,\n\nthese products work well together. But Consul was created and is maintained as an independent tool, and is not required for Terraform to function. This is why I count it as a general-purpose registry product.",
      "content_length": 288,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "Chapter 9. Core Practice: Continuously validate all work in progress\n\nContinuous validation is the second of the three core practices of\n\n1 infrastructure as code :\n\nDefine everything as code\n\nContinuously validate all work in progress\n\nBuild small, simple pieces that you can change independently\n\nTesting is a cornerstone of agile software engineering. Extreme\n\nProgramming (XP) emphasizes writing tests first (TDD) and 3 frequently integrating code (CI) . Continuous Delivery (CD) extends this to validate the full production readiness of code as developers work on it, rather than waiting until they finish\n\n2\n\nworking on a release.\n\nIf a strong focus on testing creates good results when writing\n\napplication code, it’s reasonable to expect it to be useful for infrastructure code as well. In this chapter, I explore strategies for testing and delivering infrastructure. I draw heavily on agile engineering approaches to quality, including TDD, CI, and CD.\n\nEven teams experienced with application testing struggle to test infrastructure very well. So I’ll explain some of the challenges of testing your infrastructure code. I’ll also discuss models for",
      "content_length": 1158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "thinking about testing strategy, including the test pyramid and\n\nSwiss cheese model, and how they relate to infrastructure. Then I’ll explain how to use delivery pipelines to implement testing strategies. In the chapter following this one, I’ll describe some specific patterns and techniques for testing infrastructure stacks.\n\nBut first, let’s consider what continuous validation means for infrastructure code.\n\nWhy continuously validate infrastructure code?\n\nTesting changes to your infrastructure is clearly a good idea. But\n\nthe need to build and maintain a suite of test automation code may not be as clear. We often think of building infrastructure as a one- off activity: build it, test it, then use it. Why spend the effort to create all that test code?\n\nCreating an automated testing suite is hard work, especially when you consider the work needed to implement the delivery and\n\ntesting tools and services - CI servers, pipelines, test runners, test scaffolding, and various types of scanning and validation tools. When getting started with infrastructure as code, building all of these things may seem like more work than building the systems you’ll run on them.\n\nIn “Use Infrastructure as Code to optimize for change” I explained the rationale for implementing systems for delivering changes to\n\ninfrastructure. To recap, you make changes to any non-trivial system far more often than you might expect. You create a new system by making a series of changes. You evolve the design and",
      "content_length": 1495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "implementation as you learn more about the system you’re building, and about the technology you’re using to build it.\n\nAnd once a system is live, if it has any traction with its users, you need to continuously improve, update, patch, fix, and grow it, which again involves a continuous series of changes.\n\nThe advice in this book aims to help you to continuously validate over as broad a scope of risk as possible.\n\nWhat continuous validation means\n\nOne of the cornerstones of agile engineering is validating as you work-build quality in. The earlier you can find out whether each line of code you write is ready for production, the faster you can\n\nwork, and the sooner you can deliver value. Finding problems more quickly also means spending less time going back to investigate problems and less time fixing and rewriting code. Fixing problems continuously avoids accumulating less technical debt.\n\nMost people get the importance of fast feedback. But what differentiates genuinely high performing teams is how\n\naggressively they pursue truly continuous feedback.\n\nTraditional approaches involve testing after the team has implemented the system’s complete functionality. Timeboxed methodologies take this further. The team tests periodically during development, such as at the end of a sprint, or perhaps with nightly builds. Teams following Lean or Kanban test each story as they complete it.",
      "content_length": 1395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "4\n\nLean and Kanban approaches test each “story \" as you complete it.\n\nTruly continuous validation involves testing even more frequently than this. People write and run tests as they code. And they\n\nfrequently push their code into a centralized, automated validation 5 system-ideally at least once a day .\n\nPeople need to get feedback as soon as possible when they push their code so that they can respond to it with as little interruption\n\nto their flow of work as possible. Tight feedback loops are the essence of Continuous Integration.\n\nIMMEDIATE VALIDATION AND EVENTUAL VALIDATION\n\nAnother way to think of this is to classify each of your validation activities as either immediate or eventual. Immediate validation happens when you push your code. Eventual validation happens after some delay, perhaps after a manual review, or maybe on a schedule.\n\nIdeally, validation is truly immediate, happening as you code. These are validation activities that run in your editor, such as syntax highlighting, or running unit tests. Your editor may support this, or you may use a utility like inotifywait or entr to run checks in a terminal when your code\n\nchanges.\n\nAnother example of immediate validation is pair programming, which is essentially a code review that happens as you work. Pairing provides much faster feedback than code reviews that happen after you’ve finished working on a story or feature, and someone else finds time to review what you’ve done.\n\nThe CI build and the CD pipeline should run immediately every time someone pushes a change to the codebase. Running immediately on each change not only gives them feedback faster. It also ensures a small scope of change for each run. If the pipeline only runs periodically, it may include multiple changes from multiple people. If any of the validations fail, it’s harder to work out which change caused the issue, meaning more people need to get involved and spend time to find and fix it.\n\nWhat should we validate with infrastructure?\n\nThe essence of Continuous Integration is to validate every change someone makes as soon as possible. The essence of Continuous\n\nDelivery is to maximize the scope of that validation. As Jez",
      "content_length": 2186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "Humble says, “We achieve all this by ensuring our code is always 6 in a deployable state.”\n\nQuality assurance is about managing the risks of applying our code\n\nto our systems. Will the code break when we apply it? Does it create the right infrastructure? Does the infrastructure do what we\n\nneed it to do? Does it meet operational criteria for performance,\n\nreliability, and security? Does it comply with regulatory and governance rules?\n\nContinuous Delivery is about broadening the scope of risks that\n\nare immediately validated when pushing a change to the codebase, rather than waiting for eventual validation days, weeks, or even\n\nmonths afterwards. So on every push, a pipeline applies the code to realistic test environments and subjects it to comprehensive\n\nvalidation. Ideally, once the code has run through the automated\n\nstages of the pipeline, it’s fully proven as production-ready.\n\nTeams should identify the risks that come from making changes to their infrastructure code, and create a repeatable process for\n\nvalidating any given change against those risks. This process takes the form of automated test suites and manual validation activities.\n\nA test suite is a collection of automated tests that are run as a\n\ngroup.\n\nWhen people think about automated testing, they generally think about functional tests like unit tests and UI-driven journey tests.\n\nBut the scope of risks is broader than functional defects, so the scope of validation is broader as well. Examples of things that you\n\nmay want to validate, whether automatically or manually, include:",
      "content_length": 1569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "Code quality\n\nIs the code readable and maintainable? Does it follow the team’s standards for how to format and structure code? Depending on the tools and languages you’re using, some tools can scan code for syntax errors, compliance with formatting rules, and run complexity analysis. Depending on how long they’ve been around, and how popular they are, infrastructure languages may not have many (or any!) of these tools. Manual review methods include gated code review processes, code showcase sessions, and pair programming.\n\nFunctionality\n\nDoes it do what it should? Ultimately, functionality is tested by deploying the applications and checking that they work correctly. Doing this indirectly tests that the infrastructure is correct, but it’s useful to catch some types of issues more quickly. An example of this for infrastructure is network routing. Can an HTTPS connection be made from the public Internet to the web servers? It’s often possible to test this kind of thing using a subset of the entire infrastructure.\n\nSecurity\n\nYou can test security at a variety of levels, from code scanning to unit testing to integration testing and production monitoring. There are some tools specific to security testing, such as vulnerability scanners. It may also be useful to write security tests into standard test suites. For example, unit tests can make assertions about open ports, user account handling, or access permissions.\n\nCompliance\n\nSystems may need to comply with laws, regulations, industry standards, contractual obligations, or organizational policies. Ensuring and proving compliance can be time-consuming for infrastructure and operations teams. Automated validation can be enormously useful with this, both to catch violations",
      "content_length": 1746,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "quickly and to provide evidence for auditors. As with security, you can do this at multiple levels of validation, from code- level to production validation.\n\nPerformance\n\nAutomated tools can validate how quickly specific actions complete. Testing the speed of a network connection from point A to point B can surface issues to do with the network configuration or the cloud platform if run before you even deploy an application. Finding performance issues on a subset of your system is another example of how you can get faster feedback.\n\nScalability\n\nAutomated tests can prove that scaling works correctly, for example, checking that an auto-scaled cluster adds nodes when it should. Tests can also check whether scaling gives you the outcomes that you expect. For example, perhaps adding nodes to the cluster doesn’t improve capacity, due to a bottleneck somewhere else in the system. Having these tests run frequently means you’ll discover quickly if a change to your infrastructure breaks your scaling.\n\nAvailability\n\nSimilarly, automated testing can prove that your system would be available in the face of potential outages. Your tests can destroy resources, such as nodes of a cluster, and verify that the cluster automatically replaces them. You can also test that scenarios that aren’t automatically resolved are handled gracefully, for example, showing an error page and avoiding data corruption.\n\nOperability\n\nYou can automatically validate any other system requirements needed for operations. Teams can test monitoring (inject errors and prove that monitoring detects and reports them), logging, and automated maintenance activities.",
      "content_length": 1645,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "Each of these types of validations can be applied at more than one level of scope, from server configuration to stack code to the fully\n\nintegrated system. I’ll discuss this in “Progressive validation”. But first I’d like to address the things which make infrastructure especially difficult to test.\n\nChallenges with testing infrastructure code\n\nMost of the teams I encounter who work with infrastructure as\n\ncode struggle to implement the same level of automated testing and delivery for their infrastructure code as they have for their application code. And many teams without a background in agile\n\nsoftware engineering find it even more difficult.\n\nThe premise of infrastructure as code is that we can apply software engineering practices such as agile testing to infrastructure. But there are significant differences between infrastructure code and application code. So we need to adapt some of the techniques and\n\nmindsets from application testing to make them practical for infrastructure.\n\nHere are a few challenges that arise from the differences between infrastructure code and application code.\n\nChallenge: Tests for declarative code often have low value\n\nAs mentioned in Chapter 4 (“Building infrastructure with declarative code”), many infrastructure tools use declarative languages, rather than procedural languages. Declarative code",
      "content_length": 1347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "typically declares the desired state for some infrastructure, such as this code that defines a networking subnet:\n\nsubnet: name: private_A address_range: 192.168.0.0/16\n\nA test for this would simply re-state the code:\n\nassert: subnet(\"private_A\").exists assert: subnet(\"private_A\").address_range is(\"192.168.0.0/16\")\n\nA suite of low-level tests of declarative code can become a\n\nbookkeeping exercise. Every time you change the infrastructure code, you change the test to match. What value do these tests provide? Well, testing is about managing risks, so let’s consider\n\nwhat risks the example test above can uncover:\n\n1. The infrastructure code was never applied\n\n2. The infrastructure code was applied, but the tool failed to apply it correctly, without returning an error\n\n3. Someone changed the infrastructure code but forgot to change the test to match\n\nThe first risk may be a real one, but it doesn’t require a test for\n\nevery single declaration. Assuming you have code that does multiple things on a server, a single test would be enough to reveal that, for whatever reason, the code wasn’t applied.\n\nThe second risk boils down to protecting yourself against a bug in\n\nthe tool you’re using. The tool developers should fix that bug or your team should switch to a more reliable tool. I’ve seen teams",
      "content_length": 1307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "use tests like this in cases where they found a specific bug, and wanted to protect themselves against it. Testing for this is okay to cover a known issue, but it is wasteful to blanket your code with\n\ndetailed tests just in case your tool has a bug.\n\nThe last risk is circular logic. Removing the test would remove the risk it addresses, and also remove work for the team.\n\nThere are some situations when it’s useful to test declarative code. Two that come to mind are when the declarative code can create\n\ndifferent results, and when you combine multiple declarations.\n\nTESTING VARIABLE DECLARATIVE CODE\n\nThat example of declarative code is simple - the values are hard-\n\ncoded, so the result of applying the code is clear. Variables introduce the possibility of creating different results, which may create risks that make testing more useful. Variables don’t always\n\ncreate variation that needs testing. What if we add some simple variables to the earlier example?\n\nsubnet: name: ${MY_APP}-${MY_ENVIRONMENT} address_range: ${SUBNET_IP_RANGE}\n\nThere isn’t much risk in this code that isn’t already managed by\n\nthe tool that applies it. If someone sets the variables to invalid values, the tool should fail with an error.\n\nThe code becomes riskier when there are more possible outcomes. Let’s add some conditional code to the example:\n\nsubnet: name: ${MY_APP}-${MY_ENVIRONMENT}",
      "content_length": 1379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "address_range: choose_unique_subset( get_vpc(${MY_ENVIRONMENT}).address_range, 16)\n\nThis code has some logic which might be worth testing. It calls two functions, choose_unique_subset and get_vpc, either of which might fail or return a result that interacts in unexpected ways with the other function.\n\nThe outcome of applying this code varies based on inputs and context, which makes it worth writing tests.\n\nNOTE\n\nImagine that instead of calling these functions, you wrote the code to select a subset of the address range as a part of this declaration for your subnet. This is an example of mixing declarative and functional code (as I discussed in “Implementation Principle: Avoid mixing different types of code”). The tests for the subnet code would need to include various edge cases of the functional code-for example, what happens if the parent range is smaller than the range needed?\n\nIf your declarative code is complex enough that it needs complex testing, it is a sign that you should pull some of the logic out of your declarations and into a library written in a procedural language. You can then write clearly separate tests for that function, and simplify the test for the subnet declaration.\n\nTESTING COMBINATIONS OF DECLARATIVE CODE\n\nAnother situation where testing is more valuable is when you have multiple declarations for infrastructure that combine into more complicated structures. For example, you may have code that\n\ndefines multiple networking structures - an address block, load balancer, routing rules, and gateway. Each piece of code would probably be simple enough that tests would be unnecessary. But",
      "content_length": 1631,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "the combination of these produces an outcome that is worth testing - that someone can make a network connection from point A to point B.\n\nTesting that the tool created the things declared in code is usually\n\nless valuable than testing that they enable the outcomes you want.\n\nChallenge: Testing infrastructure code is slow\n\nTo test infrastructure code, you need to apply it to relevant infrastructure. And provisioning an instance of infrastructure is\n\noften slow, especially when you need to create it on a cloud platform. Most teams who struggle to implement automated infrastructure testing find that the time to create test infrastructure is a barrier for fast feedback.\n\nThe solution is usually a combination of strategies:\n\nDivide infrastructure into more tractable pieces\n\nIt’s useful to include testability as a factor in designing a system’s structure, as it’s one of the key ways to make the system easy to maintain, extend, and evolve. Making pieces smaller is one tactic, as smaller pieces are usually faster to provision and test. It’s easier to write and maintain tests for smaller, more loosely coupled pieces since they are simpler and have less surface area of risk. [Link to Come] discusses this topic in more depth.\n\nClarify, minimize, and isolate dependencies\n\nEach element of your system may have dependencies on other parts of your system, on platform services, and on services and systems that are external to your team, department, or organization. These impact testing, especially if you need to rely on someone else to provide instances to support your test.",
      "content_length": 1584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "They may be slow, expensive, unreliable, or have inconsistent test data, especially if other users share them. Test doubles are a useful way to isolate a component so that you can test it quickly. You may use test doubles as part of a progressive testing strategy, first testing your component with test doubles, and later testing it integrated with other components and services.\n\nProgressive testing\n\nYou’ll usually have multiple test suites to test different aspects of the system. You can run faster tests first, to get quicker feedback if they fail, and only run slower, broader-scoped tests after those have passed. I’ll delve into this in “Progressive validation”.\n\nChoice of ephemeral or persistent instances\n\nYou may create and destroy an instance of the infrastructure each time you test it (an ephemeral instance), or you may leave an instance running in between runs (persistent instances). Using ephemeral instances can make tests significantly slower, but are cleaner and give more consistent results. Keeping persistent instances cuts the time needed to run tests, but may leave changes and accumulate inconsistencies over time. Choose the appropriate strategy for a given set of tests, and revisit the decision based on how well it’s working. I provide more concrete examples of implementing ephemeral and persistent instances in “Pattern: Ephemeral test stack”.\n\nOnline and offline tests\n\nSome types of validation are online, requiring you to provision infrastructure on the “real” cloud platform. Others can run offline on your laptop or a build agent. Validation that you can run offline includes code syntax checking and tests that run in a virtual machine or container instance. Consider the nature of your various tests, and be aware of which ones can run where. Offline validation is usually much faster, so you’ll tend to run",
      "content_length": 1849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "them earlier. You can use test doubles to emulate your cloud API offline for some tests.\n\nWith any of these strategies, you should regularly asses how well\n\nthey are working. If tests are unreliable, either failing to run correctly or returning inconsistent results, then you should drill\n\ninto the reasons for this and either fix them or replace them with something else. If tests rarely fail, or if the same tests almost\n\nalways fail together, you may be able to strip them out to simplify\n\nyour test suite. If you spend more time finding and fixing problems that originate in your tests rather than in the code you’re testing,\n\nlook for ways to simplify and improve them.\n\nTEST DOUBLES\n\nMocks, fakes, and stubs are all types of test doubles. A test double replaces a dependency needed by a component so you can test it in isolation. These terms tend to be used in different ways by different people, but I’ve found the definitions used by Gerard Meszaros in his xUnit 7 patterns book to be useful.\n\n8\n\nIn the context of infrastructure, there are a growing number of tools that allow you to mock the APIs of cloud vendors. You can apply your infrastructure code to a local mocked cloud to validate some aspects of the code. These won’t tell you whether your networking structures work correctly, but they should tell you whether they’re roughly valid.\n\nProgressive validation\n\nMost non-trivial systems use multiple suites of tests to validate\n\nchanges. Different suites may validate different things (as listed in “What should we validate with infrastructure?”). One suite may\n\nvalidate one concern offline, such as checking for security\n\nvulnerabilities by scanning code syntax. Another suite may run online checks for the same concern, for example, by probing a\n\nrunning instance of an infrastructure stack for security\n\nvulnerabilities.",
      "content_length": 1841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "In theory, you could run all of your test suites in parallel when you\n\nchange your code. In practice, this is wasteful. Firstly, you\n\nprobably need to provision a lot of infrastructure to test everything at once. Secondly, it’s potentially wasteful. If the code fails to pass\n\nthe faster tests, there’s no point in running the slower, more expensive tests until you fix the problem.\n\nProgressive validation involves running test suites in a sequence. The sequence builds up, starting with simpler tests that run more\n\nquickly over a smaller scope of code, then building up to more comprehensive tests over a broader set of integrated components\n\nand services.\n\nModels like the test pyramid and Swiss cheese testing offer visual\n\nmetaphors for progressive validation. A delivery pipeline ([Link to Come]) is a technical implementation of progressive testing, with a\n\nseries of stages, each running a set of test suites.\n\nValidation stages\n\nEach system needs a unique test strategy, which includes defining\n\nthe validation stages and deciding the order for running them. There are several characteristics for each validation stage,\n\nincluding the components under test, the dependencies involved,\n\nand the environment required.\n\nSCOPE OF COMPONENTS BEING VALIDATED\n\nIn a progressive validation strategy, earlier stages validate\n\nindividual components, while later stages integrate components and test them together. For example, an earlier stage might test\n\ncode that installs and configures a web server package onto a",
      "content_length": 1517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "virtual machine. A later stage tests a server with this package\n\ninstalled, running in an infrastructure stack with firewall rules and\n\nnetwork routes.\n\nFigure 9-1. Progressive integration and testing of components",
      "content_length": 214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "One stage might run validations for multiple components, such as\n\na suite of unit tests. Or, different components may each have a separate validation stage.\n\nUNIT TESTS\n\n9\n\nA unit test validates a subsection of a system, usually testing a single component at the lowest level for that system. With object oriented software this is often a class, or related set of classes. With infrastructure, this might be a single server configuration element-Chef cookbook, Puppet manifest, or Ansible playbook, for example.\n\nA unit test in software code executes a small subsection of a\n\nprogram, on the order of a one or two classes, to make sure that they run correctly. Most infrastructure tools have some kind of unit\n\ntesting tool or framework, such as rspec-puppet and ChefSpec.\n\nSaltstack even comes with its own built-in unit testing support.\n\nTools like this allow a particular configuration definition to be run without actually applying it to a server. They usually include\n\nfunctionality to emulate other parts of a system well enough to\n\ncheck that the definition behaves correctly. This requires ensuring that each of your definitions, scripts, and other low-level elements\n\ncan be independently executed. Restructuring things to make test\n\nisolation possible may be challenging, but results in a cleaner design.\n\nSCOPE OF DEPENDENCIES USED FOR THE STAGE\n\nMany elements of a system depend on other services. An application server stack might connect to an identity management\n\nservice to handle user authentication. To progressively validate",
      "content_length": 1543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "this, you might first run a stage that tests the application server\n\nwithout the identity management service, perhaps using a mock service to stand in for it. A later stage would run additional tests on\n\nthe application server integrated with a test instance of the identity\n\nmanagement service, and the production stage would integrate with the production instance:",
      "content_length": 366,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "Figure 9-2. Progressive integration with dependencies\n\nAvoid creating unnecessary stages in your pipeline, as each stage\n\nadds time and cost to your delivery process. So, don’t create\n\nseparate stages for each component and integration just for completeness. Split validation into stages this way only when it\n\nadds enough value to be worth the overhead. Some reasons which",
      "content_length": 373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "may drive you to do this include speed, reliability, cost, and\n\ncontrol.\n\nProvisioning a mock may be much faster than provisioning the external service. Some third -party services are not very reliable,\n\nmaking it unclear whether a test has failed because of your coding\n\nerror, or a blip with the service. The third party service may be expensive, or it may not give you enough flexibility to set up data\n\nand configuration for your tests.\n\nPLATFORM ELEMENTS NEEDED FOR VALIDATION\n\nPlatform services are a particular type of dependency for your\n\nsystem. Your system may ultimately run on your infrastructure\n\nplatform, but you may be able to run and test parts of it offline usefully.\n\nFor example, code that defines networking structures needs to\n\nprovision those structures on the cloud platform for meaningful\n\nvalidation. But you may be able to test code that installs an application server package in a local virtual machine, or even in a\n\ncontainer, rather than needing to stand up a virtual machine on your cloud platform.\n\nSo earlier validation stages may be able to run without using the\n\nfull cloud platform for some components:",
      "content_length": 1139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "Figure 9-3. Progressive use of platform elements\n\nTesting in production\n\nTesting releases and changes before applying them to production is a big focus in our industry. At one client, I counted eight groups\n\n10\n\nthat needed to review and approve releases various technical teams who had to carry out tasks to install and\n\n, even apart from the\n\nconfigure various parts of the system.",
      "content_length": 383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "As systems increase in complexity and scale, the scope of risks\n\nthat you can practically check for outside of production shrinks. This isn’t to say that there is no value in testing changes before\n\napplying them to production. But believing that pre-release testing\n\ncan comprehensively cover your risks leads to:\n\nOver-investing in pre-release testing, well past the point of diminishing returns,\n\nUnder-investing in testing in your production environment.\n\nGOING DEEPER ON TESTING IN PRODUCTION\n\nFor more on testing in production, I recommend watching Charity Majors’ talk, [Yes, I Test in Production (And So Should You)] (https://www.infoq.com/presentations/testing-production-2018/), which is a key source of my thinking on this topic.\n\nWHAT YOU CAN’T REPLICATE OUTSIDE PRODUCTION\n\nThere are several characteristics of production environments\n\nwhich you can’t realistically replicate outside of production:\n\nData\n\nYour production system may have larger data sets than you can replicate, and will undoubtedly have unexpected data values and combinations, thanks to your users.\n\nUsers\n\nDue to their sheer numbers, your users are far more creative at doing strange things than your testing staff.",
      "content_length": 1198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "Traffic\n\nIf your system has a non-trivial level of traffic, you can’t replicate the number and types of activities it will regularly experience. A week-long soak test is trivial compared to a year of running in production.\n\nConcurrency\n\nTesting tools can emulate multiple users using the system at the same time, but they can’t replicate the unusual combinations of things that your users do concurrently.\n\nThe two challenges that come from these characteristics are that they create risks that you can’t predict, and they create conditions\n\nthat you can’t replicate well enough to test anywhere other than\n\nproduction.\n\nBy running tests in production, you take advantage of the conditions that exist there-large natural data sets and unpredictable\n\nconcurrent activity.\n\nWHY TEST ANYWHERE OTHER THAN PRODUCTION?\n\nObviously testing in production is not a substitute for testing changes before you apply them to production. It helps to be clear on what you realistically can (and should!) test beforehand:\n\nDoes it work?\n\nDoes my code run?\n\nDoes it fail in ways I can predict?\n\nDoes it fail in the ways it has failed before?\n\nTesting changes before production addresses the known unknowns, the things that you know might go wrong. Testing changes in production addresses the unknown unknowns, the more unpredictable risks.",
      "content_length": 1321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "MANAGING THE RISKS OF TESTING IN PRODUCTION\n\nTesting in production creates new risks. There are a few things that help manage these risks:\n\nMonitoring\n\nEffective monitoring gives confidence that you can detect problems caused by your tests so you can stop them quickly. This includes detecting when tests are causing issues so you can stop them quickly (see [Link to Come]).\n\nObservability\n\nObservability gives you visibility into what’s happening within the system at a level of detail that helps you to investigate and fix problems quickly, as well as improving the quality of what you can test.\n\n11\n\nZero-Downtime Deployment\n\nBeing able to deploy and roll back changes quickly and seamlessly helps mitigate the risk of errors (see [Link to Come]).\n\nProgressive Deployment\n\nIf you can run different versions of components concurrently, or have different configurations for different sets of users, you can test changes in production conditions before exposing them to users (see [Link to Come]).\n\nData management\n\nYour production tests shouldn’t make inappropriate changes to data or expose sensitive data. You can maintain test data records, such as users and credit card numbers, that won’t trigger real-world actions.",
      "content_length": 1222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "MONITORING AS TESTING\n\nMonitoring can be seen as passive testing in production. It’s not true testing, in that you aren’t taking an action and checking the result. Instead, you’re observing the natural activity of your users and watching for undesirable outcomes.\n\nMonitoring should form a part of the testing strategy, because it is a part of the mix of things you do to manage risks to your system.\n\nProgressive validation models\n\nWith the different types of things you can validate and the various\n\nways to group tests into stages for progressive validation, a model\n\ncan help think about how to structure validation and testing\n\nactivities for changes to your particular system. I’ll describe the\n\nmost popular model for this, the testing pyramid, and an\n\ninteresting alternative, the Swiss cheese model.\n\nThe guiding principle for a progressive feedback strategy is to get\n\nfast, accurate feedback. As a rule, this means running faster tests\n\nwith a narrower scope and fewer dependencies first and then\n\nrunning tests that progressively add more components and integration points. This way, small errors are quickly made visible\n\nso they can be quickly fixed and re-tested.",
      "content_length": 1178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "Figure 9-4. Scope vs. speed of progressive testing\n\nWhen a broadly-scoped test fails, you have a large surface area of\n\ncomponents and dependencies to investigate. So you should try to\n\nfind any potential area at the earliest point, with the smallest scope\n\nthat you can.\n\nAnother goal of a test strategy is to keep the overall test suite\n\nmanageable. Avoid duplicating validations at different levels. For\n\nexample, you may test that your application server configuration code sets the correct directory permissions on the log folder. This\n\ntest would run in an earlier stage, that explicitly tests the server\n\nconfiguration. You should not have a test that checks file\n\npermissions in the stage that tests the full infrastructure stack\n\nprovisioned in the cloud.\n\nTest pyramid",
      "content_length": 778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "The test pyramid is a well-known model for software\n\n12\n\ntesting https://martinfowler.com/articles/practical-test-\n\npyramid.html by Ham Vocke is a thorough reference.], so I won’t spend much time describing it.\n\nThe key idea of the test pyramid is that you should have more tests\n\nat the lower layers, which are the earlier stages in your\n\nprogression. With software, it makes sense to have many unit tests, as these are fast and can catch errors quickly:",
      "content_length": 455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "Figure 9-5. The classic test pyramid\n\nBut lower-level infrastructure tests tend to have less value (as I\n\ndiscussed in “Challenge: Tests for declarative code often have low\n\nvalue”). This means that, although you’ll almost certainly have",
      "content_length": 237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "low-level infrastructure tests, there may not be as many as the\n\npyramid model suggests. So an infrastructure test suite may end up\n\nlooking more like a diamond:",
      "content_length": 161,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "Figure 9-6. The infrastructure test diamond",
      "content_length": 43,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "It can be helpful to keep this in mind when discussing the right\n\nbalance for a test suite with people who come from a software testing background. They may be uncomfortable with the different\n\nbalance of tests.\n\nSwiss cheese testing model\n\nAnother way to think about how to organize progressive tests is\n\nthe Swiss cheese model. This concept for risk management comes\n\n13\n\nfrom outside the software industry . The idea is that a given layer of testing may have holes, like one slice of Swiss cheese, that can\n\nmiss a defect or risk. But when you combine multiple layers, it\n\nlooks more like a block of Swiss cheese, where no hole goes all\n\nthe way through.\n\nThe point of using the Swiss cheese model when thinking about\n\ninfrastructure testing is that you focus on where to catch any given\n\nrisk. You still want to catch issues in the earliest layer where it is\n\nfeasible to do so, but the important thing is that it is tested\n\nsomewhere in the overall model:",
      "content_length": 960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "Figure 9-7. Swiss cheese testing model\n\nThe key takeaway is, test based on risk, rather than based on fitting a formula.\n\nPipelines for validation\n\nYou can implement progressive validation and delivery of\n\ninfrastructure elements using a continuous delivery pipeline . I\n\n14\n\noften call this a change delivery pipeline to emphasize its relationship to the change management process.\n\nThe idea is that when someone pushes a code change to the source\n\ncontrol repository, the team uses a central system to progress the change through a series of stages to test and deliver the change.\n\nThis process is automated, although people may be involved to\n\ntrigger or approve activities. Some principles for a change delivery\n\npipeline include:\n\nAutomate processes\n\nThe pipeline system runs scripts to apply infrastructure code, deploy software, and execute automated tests. Humans may review changes, and even conduct exploratory testing on environments. But they should not be running commands by hand to deploy and apply changes. They also shouldn’t be selecting configuration options or making other decisions on the fly. These actions should be defined as code and executed by the system. Automating processes ensures they are carried out consistently every time, for every stage. Doing this improves the reliability of your tests, and creates consistency between instances of the infrastructure.\n\nPush every change from the start of the pipeline",
      "content_length": 1441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "Never change the code once it’s progressing through the pipeline. If you find an error in a “downstream” (later) stage in a pipeline, don’t fix it in that stage and continue through the rest of the pipeline. Instead, fix the code in the repository and push the new change from the start of the pipeline. This practice ensures that the change is fully tested.\n\nFigure 9-8. Start a new pipeline run to correct failures\n\nIn the above figure, one change successfully passes through the\n\npipeline. The second change fails in the middle of the pipeline. A\n\nfix is made and pushed through to production as the third pipeline run.\n\nPipeline stages\n\nEach stage of the pipeline may do different things and may trigger\n\nin different ways. Some of the characteristics of a given pipeline\n\nstage include:\n\nTrigger",
      "content_length": 800,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "What makes the stage run? It may automatically run when there is an event, such as a change pushed to the code repository or the successful execution of the stage before it in the pipeline. Or someone may trigger the stage manually, as when a tester or release manager decides to apply a code change to a given environment.\n\nActivity\n\nWhat happens when the stage runs? Multiple actions could execute for a stage. For example, a stage might apply code to provision an infrastructure stack, run tests, and then destroy the stack.\n\nApproval\n\nHow is the stage marked as passing or failing? The system could mark the stage as passing (often referred to as “green”) when commands run without errors, and automated tests all pass. Or a human may need to mark the stage as approved. For example, a tester may approve the stage after carrying out exploratory testing on the change. You can also use manual approval stages to support governance sign-offs.\n\nOutput\n\nDoes the stage produce an artifact or other material? Typical outputs include archive files containing the infrastructure code and test results.\n\nDelivery pipeline software and services\n\nYou need software or a hosted service to build a pipeline. A\n\npipeline system needs to do a few things:\n\nGive you a way to configure the pipeline stages.\n\nTrigger stages from different actions, including automated events and manual triggers. The tool should support more complex relationships such as fanning in (one stage with",
      "content_length": 1469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "multiple input stages) and fanning out (one stage with multiple output stages).\n\nSupport any actions you may need for your stages, including applying infrastructure code and running tests. You should be able to create custom activities rather than having a fixed set of supported ones.\n\nHandle artifacts and other outputs of stages, including being able to pass them from one stage to the next.\n\nHelp you trace and correlate specific versions and instances of code, artifacts, outputs, and infrastructure.\n\nThere are a few options for a pipeline system:\n\nCI Software\n\nMany teams use Continuous Integration software such as Jenkins, Team City, and Bamboo, to create pipelines. These are often “job-oriented” rather than “stream-oriented.” The core design doesn’t inherently correlate versions of code, artifacts, and runs of different jobs. Most of these products have added support for pipelines as an overlay in their UI and configuration.\n\nCD Software\n\nCD software is built around the pipeline concept. You define each stage as part of a pipeline, and code versions and artifacts are associated with the pipeline so you can trace them forwards and backward. CD tools include GoCD , ConcourseCI , and BuildKite.\n\n15\n\n16\n\nSaaS Services\n\nHosted CI and CD services include CircleCI, TravisCI, AppVeyor, Drone, and BoxFuse.\n\nCloud Platform Services",
      "content_length": 1345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "Most cloud vendors include CI and CD services, including AWS CodeBuild (CI) and AWS CodePipeline (CD), and Azure DevOps\n\n17\n\nSource Code Repository Services\n\nMany source code repository products and vendors have added CI support that you can use to create pipelines. Two prominent examples are Github actions, and Gitlab CI and CD.\n\nThe products I mentioned above were all designed with application software in mind. You can use most of them to build\n\n18\n\npipelines for infrastructure, although they may need extra work.\n\nA few products and services designed for infrastructure as code are\n\nemerging as I write this. This is a rapidly changing area, so I suspect what I have to say about these tools is out of date by the\n\ntime you read this, and missing newer tools. But it’s worth looking\n\nat what exists now, to give context for evaluating tools as they\n\nemerge and evolve:\n\nAtlantis is a product that helps you to manage pull requests for Terraform projects, and to run plan and apply for a single instance. It doesn’t run tests, but you can use it to create a limited pipeline that handles code reviews and approvals for infrastructure changes.\n\nTerraform Cloud is evolving rapidly. It is Terraform- specific, and it includes more features (such as a module registry) than CI and pipelines. You can use Terraform cloud to create a limited pipeline that plans and applies a project’s code to multiple environments. But it doesn’t run tests other than policy validations with Hashicorp’s own Sentinel product.\n\nWeaveWorks makes products and services for managing Kubernetes clusters. These include tools for managing the",
      "content_length": 1623,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "delivery of changes to cluster configuration as well as applications using pipelines based around git branches, an approach they call GitOps. Their solutions don’t seem applicable to general infrastructure, but even if you don’t use their stack, it’s an emerging model for pipelines that’s worth watching. I’ll touch on it a bit more in [Link to Come].\n\nThe next chapter (Chapter 10) includes details on how to structure\n\npipelines for testing and delivering infrastructure stacks. I also\n\ndiscuss pipeline implementation for building servers ([Link to\n\nCome]) and clusters ([Link to Come]).\n\nConclusion\n\nThis chapter has discussed general challenges and approaches for\n\ntesting infrastructure. I’ve avoided going very deeply into the\n\nsubjects of testing, quality, and risk management. If these aren’t\n\nareas you have much experience with, this chapter may give you\n\nenough to get started. I encourage you to read more, as testing and QA are fundamental to infrastructure as code.\n\nWhile I’ve touched on aspects of testing that are specific to\n\ninfrastructure, such as the challenges of writing tests for\n\ninfrastructure, I haven’t given many concrete examples. The next chapter, Chapter 10, should do this for infrastructure stacks.\n\nLikewise, [Link to Come] explains how to write tests and build\n\npipelines for servers, as [Link to Come] does for clusters.\n\n1 I listed these three core practices in Chapter 1\n\n2 See https://martinfowler.com/articles/continuousIntegration.html",
      "content_length": 1479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "3 Jez Humble and David Farley’s book Continuous Delivery (Addison-Wesley)\n\ndefined the principles and practices for CD, raising it from an obscure phrase in the Agile Manifesto to a widespread practice among software delivery teams.\n\n4 See https://www.mountaingoatsoftware.com/agile/user-stories for an explanation\n\nof agile stories.\n\n5 The Accelerate research published in the annual State of the DevOps Report finds\n\nthat teams where everyone merges their code at least daily tend to be more effective than those who do so less often. In the most effective teams I’ve seen, developers push their code multiple times a day, sometimes as often as every hour or so.\n\n6 https://continuousdelivery.com/\n\n7 Martin Fowler’s bliki Mocks Aren’t Stubs is a useful reference for test doubles.\n\n8 Examples of cloud mocking tools and libraries include Localstack and moto.\n\nhttps://dobetterascode.com/types/mock/ maintains a current list of this kind of tool.\n\n9 See the extremeprogramming.org definition of unit tests. Martin Fowler’s bliki\n\ndefinition of UnitTest discusses a few ways of thinking of unit tests.\n\n10 These groups were: change management, infosec, risk management, service\n\nmanagement, transition management, system integration testing, user acceptance, and the technical governance board.\n\n11 Although it’s often conflated with monitoring, observability is about giving people ways to understand what’s going on inside your system. See honeycomb.io’s Introduction to Observability.\n\n12 [The Practical Test Pyramid\n\n13 See https://en.wikipedia.org/wiki/Swiss_cheese_model\n\n14 Sam Newman described the concept of build pipelines in several blog posts\n\nstarting in 2005, which he recaps in a 2009 blog post, A Brief and Incomplete History of Build Pipelines. Jez Humble and Dave Farley’s Continuous Delivery book (referenced earlier in this chapter) popularized pipelines. Jez has documented the deployment pipeline pattern on his website.\n\n15 In the interests of full disclosure, my employer, ThoughtWorks created GoCD. It\n\nwas previously a commercial product, but it is now fully open source.\n\n16 In spite of its name, ConcourseCI is designed around pipelines rather than CI jobs.\n\n17 I can’t mention Azure DevOps without pointing out the terribleness of that name for a service. DevOps is about culture first, not tools and technology first. Read Matt Skelton’s review of John Willis’ talk on DevOps culture.",
      "content_length": 2415,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "18 Although some of the products I listed are designed for building container images,\n\nso may be harder to adapt for infrastructure.",
      "content_length": 132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "Chapter 10. Testing Infrastructure Stacks\n\nChapter 5 describes an infrastructure stack as a collection of infrastructure resources that you define, provision, and update as a\n\nunit from an infrastructure platform. I cover general topics for testing infrastructure in Chapter 9. In this chapter, I combine these two topics, delving into specific techniques for testing\n\ninfrastructure code at the level of the stack.\n\n1\n\nI use the Foodspin example to illustrate how to test a stack. I start by outlining the example infrastructure involved, to set the context for specific examples later in the chapter.\n\nI then give examples of types of testing activities, grouped into\n\noffline validation, which you can run without actually provisioning infrastructure, and online validation which does provision infrastructure on your platform. I give examples use pseudocode\n\nfor both stack definitions and tests.\n\nYou can use test fixtures, such as mocks and fakes, to help you test your stacks as standalone entities rather than needing to provision\n\nlarge swathes of infrastructure for each test.\n\nIn addition to test fixtures, you need to provision instances of your stack for online validation. I describe several patterns and one antipattern for managing the lifecycle of these test instances.",
      "content_length": 1286,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "To wrap this up, you need a way to orchestrate test instances and\n\nfixtures for running tests. I describe how people use scripts and tools for this, with some pseudocode examples.\n\nExample infrastructure\n\nThe Foodspin team uses reusable stack projects (“Pattern: Reusable Stack”) to create consistent instances of application infrastructure for each of their customers. They can also use this to\n\ncreate test instances of the infrastructure in the pipeline.\n\nThe infrastructure for these examples includes the following elements (based on the examples from Chapter 3):",
      "content_length": 568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "Figure 10-1. Example infrastructure used for a Foodspin application instance\n\nWeb Server Cluster\n\nThe team runs a single web server container cluster for each region and in each test environment. Applications in the region or environment share this cluster. The examples in this chapter focus on the infrastructure that is specific to each customer, rather than shared infrastructure. So the shared cluster is a dependency in the examples here. For details of how changes are coordinated and tested across this infrastructure, see [Link to Come].\n\nApplication Server\n\nThe infrastructure for each application instance includes a virtual machine, a persistent disk volume, and networking. The networking includes an address block, gateway, routes to the server on its network port, and network access rules.\n\nDatabase Server\n\nFoodspin runs a separate database instance for each customer application instance, using their provider’s DBaaS service. Their infrastructure code also defines an address block, routing, and database authentication and access rules.\n\n2\n\nThe example stack\n\nTo start, we can define a single reusable stack that has all of the\n\ninfrastructure other than the web server cluster. The project structure could look like this:\n\nExample 10-1. Stack project for Foodspin customer application stack-project/ └── src/ ├── appserver_vm.infra ├── appserver_networking.infra ├── database.infra └── database_networking.infra",
      "content_length": 1432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "Within this project, the file appserver_vm.infra would include code along these lines:\n\nExample 10-2. Partial contents of appserver_vm.infra virtual_machine: name: appserver-${customer}-${environment} ram: 4GB address_block: ADDRESS_BLOCK.appserver-${customer}-${environment} storage_volume: STORAGE_VOLUME.app-storage-${customer}-${environment} base_image: SERVER_IMAGE.foodspin_java_server_image provision: tool: servermaker parameters: maker_server: maker.foodspin.io role: appserver customer: ${customer} environment: ${environment}\n\nstorage_volume: id: app-storage-${customer}-${environment} size: 80GB format: xfs\n\nA team member or automated process can create or update an\n\ninstance of the stack by running the stack tool. They pass values to\n\nthe instance using one of the patterns from Chapter 8.\n\nAs described in Chapter 9, the team uses multiple validation stages (“Progressive validation”), organized in a sequential pipeline\n\n(“Pipelines for validation”).\n\nPipeline for the example stack\n\n3\n\nA simple pipeline for the Foodspin application infrastructure stack has two testing stages, followed by a stage that applies the\n\ncode to each customer’s production environment:",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "Figure 10-2. Simplified example pipeline for a stack\n\nThe first stage of the pipeline is an offline stage, and the second is\n\nan online stage. Each of these stages can run several different validation activities.\n\nOffline validation stages for stacks",
      "content_length": 250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "An offline stage runs “locally” on an agent node of the service that runs the stage (see “Delivery pipeline software and services”),\n\nrather than needing to provision infrastructure on your infrastructure platform. Strict offline validation runs entirely within the local server or container instance, without connecting to\n\nany external services such as a database. A softer offline stage might connect to an existing service instance, perhaps even a cloud API, but doesn’t use any real stack infrastructure.\n\nAn offline stage should:\n\nRun quickly, giving fast feedback if something is incorrect,\n\nValidate the correctness of components in isolation, to give confidence in each component, and to simplify debugging failures,\n\nProve the component is cleanly decoupled.\n\nSome of the things you can check for your stack code in an offline stage are syntax checking, offline static code analysis, static code analysis with the platform API, and testing with a mock API.\n\nSyntax checking\n\nWith most stack tools, you can run a dry run command that parses your code without applying it to infrastructure. The command exits with an error if there is a syntax error. The check tells you very\n\nquickly when you’ve made a typo in your code change, but misses many other errors. Examples of syntax checking include terraform validate and aws cloudformation validate- template.",
      "content_length": 1365,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "The output of a failing syntax validation might look like this:\n\n$ stack validate\n\nError: Invalid resource type\n\non appserver_vm.infra line 1, in resource \"virtual_mahcine\":\n\nstack does not support resource type \"virtual_mahcine\".\n\nOffline static code analysis\n\nSome tools can parse and analyze stack source code for a wider class of issues than just syntax, but still without connecting to an 4 infrastructure platform. This analysis is often called linting _ comes from a classic Unix utility that analyzes C source code.].\n\nThis kind of tool may look for coding errors, confusing or poor coding style, adherence to code style policy, or potential security issues. Some tools can even modify code to match a certain style, such as the terraform fmt command. There are not as many tools that can analyze infrastructure code as there are for application programming languages. Examples include tflint, CloudFormation Linter, and cfn_nag.\n\nHere’s an example of an error from a fictional analysis tool:\n\n$ stacklint 1 issue(s) found:\n\nNotice: Missing 'Name' tag (vms_must_have_standard_tags)\n\non appserver_vm.infra line 1, in resource \"virtual_machine\":\n\nIn this example, we have a custom rule named vms_must_have_standard_tags that requires all virtual machines to have a set of tags, including one called Name.",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "Static code analysis with API\n\nDepending on the tool, some static code analysis checks may connect to the cloud platform API to check for conflicts with what the platform supports. For example, tflint can check Terraform\n\nproject code to make sure that any instance types (virtual machine sizes) or AMIs (server images) defined in the code actually exist. Unlike previewing changes (“Preview: Seeing what changes will\n\nbe made”), this type of validation tests the code in general, rather than against a specific stack instance on the platform.\n\nThe following example output fails because the code declaring the virtual server specifies a sever image that doesn’t exist on the platform:\n\n$ stacklint 1 issue(s) found:\n\nNotice: base_image 'SERVER_IMAGE.foodspin_java_server_image' doesn't exist (validate_server_images)\n\non appserver_vm.infra line 5, in resource \"virtual_machine\":\n\nTesting with mock API\n\nYou may be able to apply your stack code to a local, mock instance of your infrastructure platform’s API. There are not many\n\ntools for mocking these APIs. The only one I’m aware of as of this writing is Localstack. Applying your stack code to a local mock can reveal coding errors that syntax or code analysis checks might not find. Depending on how functional the mocks are, you may be\n\nable to carry out some online-type validations (as I describe shortly). But in practice, what you can do is limited.",
      "content_length": 1409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "Online validation stages for stacks\n\nAn online stage involves using the infrastructure platform to create and interact with an instance of the stack. This type of stage is\n\nslower but can carry out more meaningful validations than online validation. The delivery pipeline service usually runs the stack tool on one of its nodes or agents, but it uses the platform API to interact with an instance of the stack. The service needs to\n\nauthenticate to the platform’s API, see “Secrets and source code” for ideas on how to handle this securely.\n\nAlthough an online test stage depends on the infrastructure platform, you should be able to test the stack with a minimum of\n\nother dependencies. In particular, you should design your infrastructure, stacks, and tests so that you can create and test an instance of a stack without needing to integrate with instances of other stacks.\n\nFor example, the Foodspin customer application infrastructure\n\nworks with a shared web server cluster stack. However, they implement their infrastructure, and testing stages, so that they can test the application stack code without an instance of the web\n\nserver cluster.\n\nI cover techniques for splitting stacks and keeping them loosely coupled in [Link to Come]. Assuming you have built your infrastructure in this way, you can use test fixtures to make it\n\npossible to test a stack on its own, as described a bit later in this chapter (“Using test fixtures to handle dependencies”).\n\nFirst, consider how different types of online stack tests work. The validations that an online stage can run include previewing",
      "content_length": 1591,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "changes, verifying that changes are applied correctly, and proving the outcomes.\n\nPreview: Seeing what changes will be made\n\nSome stack tools can compare stack code against a stack instance to list changes it would make without actually changing anything. Terraform’s plan subcommand is a well-known example.\n\nMost often, people preview changes against production instances as a safety measure, so someone can review the list of changes to\n\nreassure themselves that nothing unexpected will happen. Applying changes to a stack can be done with a two-step process in a pipeline. The first step runs the preview, and a person triggers\n\nthe second step, to apply the changes, once they’ve reviewed the results of the preview.\n\nHaving people review changes isn’t very reliable. People might misunderstand or not notice a problematic change. You can write automated tests that check the output of a preview command. This\n\nkind of test might check changes against policies, failing if the code creates a deprecated resource type, for example. Or it might check for disruptive changes-fail if the code will rebuild or destroy\n\na database instance.\n\nAnother issue is that stack tool previews are usually not deep. A preview tells you that this code will create a new server:\n\nvirtual_machine: name: myappserver base_image: \"java_server_master\"",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "But the preview may not tell you that \"java_server_master\" doesn’t exist, although the apply command will fail to create the server.\n\nPreviewing stack changes is useful for checking a limited set of\n\nrisks immediately before applying a code change to an instance. But it is less useful for testing code that you intend to reuse across multiple instances, such as across test environments for release\n\ndelivery. Teams using copy-paste environments (“Antipattern: Copy-Paste Environments”) often use a preview stage as a minimal validation for each environment. But teams using reusable stacks (“Pattern: Reusable Stack”) can use test instances\n\nfor more meaningful validation of their code.\n\nVerification: Making assertions about infrastructure resources\n\nGiven a stack instance, you can have tests in an online stage that make assertions about the infrastructure in the stack. Testing\n\n5\n\n6\n\n7\n\nframeworks such as Awspec , Inspec , and Terratest use the infrastructure platform’s API, making requests to gather information about infrastructure and then making assertions about\n\nit.\n\nA set of tests for the virtual machine from the example stack code\n\nearlier in this chapter could look like this:\n\ngiven virtual_machine(name: \"appserver-testcustomerA-staging\") { it { exists } it { is_running } it { passes_healthcheck } it { has_attached storage_volume(name: \"app-storage-testcustomerA- staging\") } }",
      "content_length": 1401,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "Most stack testing tools provide libraries to help write assertions\n\nabout the types of infrastructure resources I describe in Chapter 3. This example test uses a virtual_machine resource to identify the VM in the stack instance for the staging environment. It makes\n\nseveral assertions about the resource, including whether it has been created (exists), whether it’s running rather than having terminated (is_running), and whether the infrastructure platform considers it healthy (passes_healthcheck).\n\nSimple assertions often have low value (see “Challenge: Tests for\n\ndeclarative code often have low value”), since they simply restate the infrastructure code they are testing. A few basic assertions (such as exists) help to sanity check that the code was applied successfully. These quickly identify basic problems with pipeline stage configuration or test setup scripts. Tests such as is_running and passes_healthcheck would tell you when the stack tool successfully creates the VM, but it crashes or has some other\n\nfundamental issue. Simple assertions like these save you time in\n\ntroubleshooting.\n\nAlthough you can create assertions that reflect each of the VM’s configuration items in the stack code, like the amount of RAM or\n\nthe network address assigned to it, these have little value and add\n\noverhead.\n\nThe fourth assertion in the example, has_attached storage_volume() is more interesting. The assertion checks that the storage volume defined in the same stack is attached to the\n\nVM. Doing this validates that the combination of multiple declarations works correctly (as discussed in “Testing\n\ncombinations of declarative code”). Depending on your platform",
      "content_length": 1672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "and tooling, the stack code might apply successfully but leave the\n\nserver and volume correctly tied together. Or you might make an\n\nerror in your stack code that breaks the attachment.\n\nAnother case where assertions can be useful is when the stack code is dynamic. When passing different parameters to a stack can\n\ncreate different results, you may want to make assertions about\n\nthose results. As an example, this code creates the infrastructure for an application server that is either public facing or internally\n\nfacing:\n\nvirtual_machine: name: appserver-${customer}-${environment} address_block: if(${network_access} == \"public\") ADDRESS_BLOCK.public-${customer}-${environment} else ADDRESS_BLOCK.internal-${customer}-${environment} end\n\nYou could have a testing stage that creates each type of instance and asserts that the networking configuration is correct in each\n\ncase. You should move more complex variations into modules or libraries (see Chapter 6) and test those modules separately from\n\nthe stack code. Doing this simplifies testing the stack code.\n\nAsserting that infrastructure resources are created as expected is\n\nuseful up to a point. But the most valuable testing is proving that they do what they should.\n\nOutcomes: Proving infrastructure works correctly\n\nFunctional testing is an essential part of testing application software. The analogy with infrastructure is proving that you can",
      "content_length": 1408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "use the infrastructure as intended. Examples of outcomes you\n\ncould test with infrastructure stack code include:\n\nCan you make a network connection from the web server networking segment to an application hosting network segment on the relevant port?\n\nCan you deploy and run an application on an instance of your container cluster stack?\n\nCan you safely reattach a storage volume when you rebuild a server instance?\n\nDoes your load balancer correctly handle server instances as they are added and removed?\n\nTesting outcomes is more complicated than verifying that things\n\nexist. Not only do your tests need to create or update the stack\n\ninstance, as I discuss next (“Lifecycle patterns for test instances of stacks”), you may also need to provision test fixtures. A test\n\nfixture is an infrastructure resource that is use only to support a\n\ntest (I talk about test fixtures in “Using test fixtures to handle dependencies”).\n\nThis test makes a connection to the server to check that the port is\n\nreachable, and returns the expected HTTP response:\n\ngiven stack_instance(stack: \"foodspin_networking\", instance: \"online_test\") {\n\ncan_connect(ip_address: stack_instance.appserver_ip_address, port:443)\n\nhttp_request(ip_address: stack_instance.appserver_ip_address, port:443, url: '/').response.code is('200')\n\n}",
      "content_length": 1307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "The testing framework and libraries implement the details of validations like can_connect and http_request. You’ll need to read the documentation for your test tool to see how to write actual tests.\n\nUsing test fixtures to handle dependencies\n\nMany stack projects depend on resources created outside the stack,\n\nsuch as shared networking defined in a different stack project. A\n\ntest fixture is an infrastructure resource that you create specifically to help you provision and test a stack instance by itself, without\n\nneeding to have instances of other stacks.\n\nUsing test fixtures makes it much easier to manage tests, keep your stacks loosely coupled, and have fast feedback loops. Without test\n\nfixtures, you may need to create and maintain complicated sets of\n\ntest infrastructure.\n\nA test fixture is not a part of the stack that you are testing. It is additional infrastructure that you create to support your tests. You\n\nuse test fixtures to represent a stack’s dependencies.\n\nA given dependency is either upstream, meaning the stack you’re\n\ntesting uses resources provided by another stack, or it is downstream, in which case other stacks use resources from the\n\nstack you’re testing. People sometimes call a stack with\n\ndownstream dependencies the provider, since it provides resources. A stack with upstream dependencies is then called the\n\nconsumer.",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "Figure 10-3. Example of a provider stack and consumer stack\n\nOur Foodspin example has a provider stack which defines shared\n\nnetworking structures. These structures are used by consumer stacks, including the stack that defines customer application",
      "content_length": 247,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "infrastructure. The application stack creates a server that it assigns\n\n8 to a network address block created by the networking stack .\n\nA given stack may be both a provider and a consumer, consuming resources from another stack and providing resources to other\n\nstacks.",
      "content_length": 269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "NEVER CREATE CIRCULAR DEPENDENCIES\n\nAlthough a stack may have both upstream and downstream dependencies, you should never have circular dependencies. A circular dependency exists when a stack has both upstream and downstream dependencies to another stack, or when there is a loop of dependencies between a group of stacks. It should always be possible to follow dependencies from one stack to another in a group, without returning to a previous stack.",
      "content_length": 451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "FIGURE 10-4. NEVER CREATE STACKS WITH CIRCULAR DEPENDENCIES\n\nIf you find you do have a circular dependency in your system, you should redesign your stacks to remove it.\n\nYou can use test fixtures to stand in for either upstream or\n\ndownstream integration points of a stack.\n\nTest doubles for upstream dependencies\n\nWhen you need to test a stack that depends on another stack, you\n\ncan create a test double (see “Test Doubles”). For stacks, this typically means creating some additional infrastructure. In our\n\nexample of the shared network stack and the application stack, the application stack needs to create its server in a network address\n\nblock that is defined by the network stack. Your test setup may be\n\nable to create an address block as a test fixture to test the application stack on its own.\n\nIt may be better to create the address block as a test fixture rather\n\nthan creating an instance of the entire network stack. The network\n\nstack may include extra infrastructure that isn’t necessary for testing. For instance, it may define network policies, routes,\n\nauditing, and other resources for production that are overkill for a\n\ntest.\n\nAlso, creating the dependency as a test fixture within the consumer stack project decouples it from the provider stack. If someone is\n\nworking on a change to the networking stack project, it doesn’t\n\nimpact work on the application stack.",
      "content_length": 1386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "A potential benefit of this type of decoupling is to make stacks\n\nmore reusable and composable. The Foodspin team might want to create different network stack projects for different purposes. One\n\nstack creates tightly controlled and audited networking for services\n\nthat have stricter compliance needs, such as payment processing subject to the [PCI standard]https://www.pcisecuritystandards.org/.\n\nAnother stack creates networking that doesn’t need to be PCI\n\ncompliant. By testing application stacks without using either of these stacks, the team makes it easier to use the stack code with\n\neither one.\n\nTest fixtures for downstream dependencies\n\nYou can also use test fixtures for the reverse situation, to test a\n\nstack that provides resources for other stacks to use. In Figure 10- 5, the stack instance defines networking structures for Foodspin,\n\nincluding segments and routing for the web server container\n\ncluster and application servers. The network stack doesn’t provision the container cluster or application servers, so to test the\n\nnetworking, the setup provisions a test fixture in each of these segments.",
      "content_length": 1121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "Figure 10-5. Test instance for the Foodspin network stack, with test fixtures",
      "content_length": 77,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "The test fixtures in these examples are a pair of container\n\ninstances, one assigned to each of the network segments in the stack. You can often use the same testing tools that you use for\n\nverification testing (“Verification: Making assertions about infrastructure resources”) for outcome testing. These example tests\n\nuse a fictional stack testing DSL:\n\ngiven stack_instance(stack: \"foodspin_networking\", instance: \"online_test\") {\n\ncan_connect(from: $HERE, to: get_fixture(\"web_segment_instance\").address, port:443)\n\ncan_connect(from: get_fixture(\"web_segment_instance\"), to: get_fixture(\"app_segment_instance\").address, port: 8443)\n\n}\n\nThe method can_connect executes from $HERE, which would be the agent where the test code is executing, or from a container\n\ninstance. It attempts to make an HTTPS connection on the specified port to an IP address. The get_fixture() method fetches the details of a container instance created as a text fixture.\n\nThe test framework might provide the method can_connect, or it could be a custom method that the team writes.\n\nYou can see the connections that the example test code makes in Figure 10-6.",
      "content_length": 1138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "Figure 10-6. Testing connectivity in the Foodspin network stack\n\nThe diagram shows the paths for both tests. The first test connects\n\nfrom outside the stack to the test fixture in the web segment. The\n\nsecond test connects from the fixture in the web segment to the\n\nfixture in the application segment.\n\nREFACTOR COMPONENTS SO THEY CAN BE ISOLATED\n\nSometimes a particular component can’t be easily isolated. Dependencies on other components may be hardcoded or simply too messy to pull apart. One of the benefits of writing tests while designing and building systems, rather than afterward, is that it forces you to improve your designs. A component that is difficult to test in isolation is a symptom of design issues. A well- designed system should have loosely coupled components.\n\nSo when you run across components that are difficult to isolate, you should fix the design. You may need to completely rewrite components, or replace libraries, tools, or applications. As the saying goes, this is a feature, not a bug. Clean design and loosely coupled code is a byproduct of making a system testable.\n\nThere are several strategies for restructuring systems. Refactoringfootnote:[Martin Fowler has written about refactoring as well as other patterns and techniques for improving system architecture. The Strangler Application prioritizes keeping the system fully working throughout the process of restructuring a system.\n\nLifecycle patterns for test instances of stacks\n\nBefore virtualization and cloud, everyone maintained static, long-\n\nlived test environments. Although many teams still have these environments, there are advantages to creating and destroying\n\nenvironments on demand. The following patterns describe the\n\ntradeoffs of keeping a persistent stack instance, creating an\n\nephemeral instance for each test run, and ways of combining both\n\napproaches. You can also apply these patterns to application and\n\nfull system test environments as well as to testing infrastructure stack code.",
      "content_length": 1998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "Pattern: Persistent test stack\n\nA testing stage can use a Persistent Test Stack instance that is\n\nalways running. The stage applies each code change as an update to the existing stack instance, runs the tests, and leaves the\n\nresulting modified stack in place for the next run.",
      "content_length": 277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "Figure 10-7. Persistent test stack instance\n\nALSO KNOWN AS",
      "content_length": 58,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "Static environment.\n\nMOTIVATION\n\nIt’s usually much faster to apply changes to an existing stack\n\ninstance than to create a new instance. So the persistent test stack\n\ncan give faster feedback, not only for the stage itself but for the\n\nfull pipeline.\n\nAPPLICABILITY\n\nA persistent test stack is useful when you can reliably apply your\n\nstack code to the instance. If you find yourself spending time fixing broken instances to get the pipeline running again, you\n\nshould consider one of the other patterns in this chapter.\n\nCONSEQUENCES\n\nMost stack tools, at least as of this writing, have a pretty high rate\n\nof failure. A stack instance frequently gets “wedged,” when a\n\nchange fails and leaves it in a state where any new attempt to\n\napply stack code also fails. Often, an instance gets wedged so\n\nseverely that the stack tool can’t even destroy the stack so you can start over. So your team spends too much time manually un-\n\nwedging broken test instances.\n\nYou can often reduce the frequency of wedged stacks through\n\nbetter stack design. Breaking stacks down into smaller and simpler stacks, and simplifying dependencies between stacks can lower\n\nyour wedge rate. See [Link to Come] for more on how to do this.\n\nIMPLEMENTATION",
      "content_length": 1230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "It’s easy to implement a persistent test stack. Your pipeline stage\n\nruns the stack tool command to update the instance with the\n\nrelevant version of the stack code, runs the tests, and then leaves\n\nthe stack instance in place when finished.\n\nYou may rebuild the stack completely as an ad-hoc process, such\n\nas someone running the tool from their local computer, or using an\n\nextra stage or job outside the routine pipeline flow.\n\nRELATED PATTERNS\n\nThe periodic stack rebuild pattern “Pattern: Periodic stack rebuild”\n\nis a simple tweak to this pattern, tearing the instance down at the\n\nend of the working day and building a new one every morning.\n\nPattern: Ephemeral test stack\n\nWith the Ephemeral Test Stack pattern, the test stage creates and\n\ndestroys a new instance of the stack every time it runs.",
      "content_length": 804,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "Figure 10-8. Ephemeral test stack instance\n\nALSO KNOWN AS\n\nTemporary stack, Ad-hoc stack.\n\nMOTIVATION\n\nAn ephemeral test stack provides a clean environment for each run\n\nof the tests. There is no risk from data, fixtures, or other “cruft”\n\nleft over from a previous run.\n\nAPPLICABILITY\n\nYou may want to use ephemeral instances for stacks that are quick\n\nto provision from scratch. “Quick” is relative to the feedback loop\n\nyou and your teams need. For more frequent changes, like\n\ncommits to application code during rapid development phases, the time to build a new environment is probably longer than people\n\ncan tolerate. But less frequent changes, such as OS patch updates,\n\nmay be acceptable to test with a complete rebuild.\n\nCONSEQUENCES\n\nStacks generally take a long time to provision from scratch. So\n\nstages using ephemeral stack instances make feedback loops and\n\ndelivery cycles slower.\n\nIMPLEMENTATION\n\nTo implement an ephemeral test instance, your test stage should\n\nrun the stack tool command to destroy the stack instance when\n\ntesting and reporting have completed. You may want to configure the stage to stop before destroying the instance if the tests fail so\n\nthat people can debug the failure.",
      "content_length": 1211,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "RELATED PATTERNS\n\nThe continuous stack reset pattern (“Pattern: Continuous stack\n\nreset”) is similar, but runs the stack creation and destruction\n\ncommands out of band from the stage, so the time taken doesn’t\n\naffect feedback loops.\n\nAntipattern: Dual Persistent and Ephemeral Stack Stages\n\nWith Persistent and Ephemeral Stack Stages, the pipeline sends\n\neach change to a stack to two different stages, one that uses an\n\nephemeral stack instance, and one that uses a persistent stack\n\ninstance. This combined the persistent test stack pattern (“Pattern: Persistent test stack”) and the ephemeral test stack pattern\n\n(“Pattern: Ephemeral test stack”).\n\nALSO KNOWN AS\n\nQuick and dirty plus slow and clean.\n\nMOTIVATION\n\nTeams usually implement this to work around the disadvantages of each of the two patterns it combines. If all works well, the “quick\n\nand dirty” stage (the one using the persistent instance) provides\n\nfast feedback. If that stage fails because the environment becomes\n\nwedged, you will get feedback eventually from the “slow and\n\nclean” stage (the one using the ephemeral instance).\n\nAPPLICABILITY\n\nIt might be worth implementing both types of stages as an interim\n\nsolution while moving to a more reliable solution.",
      "content_length": 1234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "CONSEQUENCES\n\nIn practice, using both types of stack lifecycle combines the\n\ndisadvantages of both. If updating an existing stack is unreliable,\n\nthen your team will still spend time manually fixing that stage when it goes wrong. And you probably wait until the slower stage\n\npasses before being confident that a change is good.\n\nThis antipattern is also expensive, since it uses double the\n\ninfrastructure resources, at least during the test run.\n\nIMPLEMENTATION\n\nYou implement dual stages by creating two pipeline stages, both\n\ntriggered by the previous stage in the pipeline for the stack project. You may require both stages to pass before promoting the stack\n\nversion to the following stage, or you may promote it when either\n\nof the stages passes.",
      "content_length": 753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "Figure 10-9. Dual Persistent and Ephemeral Stack Stages\n\nRELATED PATTERNS\n\nThis antipattern combines the persistent test stack pattern\n\n(“Pattern: Persistent test stack”) and the ephemeral test stack",
      "content_length": 199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "pattern (“Pattern: Ephemeral test stack”).\n\nPattern: Periodic stack rebuild\n\nPeriodic Stack Rebuild uses a persistent test stack instance\n\n(“Pattern: Persistent test stack”) for the stack test stage, and then has a process that runs -out-of-band to destroy and rebuild the\n\nstack instance on a schedule, such as nightly.\n\nALSO KNOWN AS\n\nNightly rebuild.\n\nMOTIVATION\n\nPeople often use periodic rebuilds to reduce costs. They destroy the stack at the end of the working day and provision a new one at\n\nthe start of the next day.\n\nPeriodic rebuilds might help with unreliable stack updates,\n\ndepending on why the updates are unreliable. In some cases, the resource usage of instances builds up over time, such as memory\n\nor storage that accumulates across test runs. Regular resets can\n\nclear these out.\n\nAPPLICABILITY\n\nRebuilding a stack instance to work around resource usage usually\n\nmasks underlying problems or design issues. In this case, this\n\npattern is, at best, a temporary hack, and at worst, a way to allow\n\nproblems to build up until they cause a disaster.\n\nDestroying a stack instance when it isn’t in use to save costs is\n\nsensible, especially when using metered resources such as with",
      "content_length": 1197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "public cloud platforms.\n\nCONSEQUENCES\n\nIf you use this pattern to free up idle resources, you need to\n\nconsider how you can be sure they aren’t needed. For example,\n\npeople working outside of office hours, or in other timezones, may be blocked without test environments.\n\nIMPLEMENTATION\n\nMost pipeline orchestration tools make it easy to create jobs that run on a schedule to destroy and build stack instances. A more\n\nsophisticated solution would run based on activity levels. For\n\nexample, you could have a job that destroys an instance if the test\n\nstage hasn’t run in the past hour.\n\nThere are three options for triggering the build of a fresh instance\n\nafter destroying the previous instance. One is to rebuild it right\n\naway after destroying it. This approach clears resources but\n\ndoesn’t save costs.\n\nA second option is to build the new environment instance at a\n\nscheduled point in time. But it may stop people from working\n\nflexible hours.\n\nThe third option is for the test stage to provision a new instance if\n\nit doesn’t currently exist. Create a separate job that destroys the\n\ninstance, either on a schedule or after a period of inactivity. Each\n\ntime the testing stage runs, it first checks whether the instance is\n\nalready running. If not, it provisions a new instance first. With this\n\napproach, people occasionally need to wait longer than usual to get",
      "content_length": 1370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "test results. If they are the first person to push a change in the\n\nmorning, they need to wait for the system to provision the stack.\n\nRELATED PATTERNS\n\nThis pattern can play out like the persistent test stack pattern\n\n(“Pattern: Persistent test stack”)-if your stack updates are\n\nunreliable, people spend time fixing broken instances.\n\nPattern: Continuous stack reset\n\nWith the Continuous Stack Reset pattern, every time the stack\n\ntesting stage completes, an out-of-band job destroys and rebuilds\n\nthe stack instance.",
      "content_length": 519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "Figure 10-10. Pipeline flow for continuous stack reset\n\nALSO KNOWN AS\n\nBehind the scenes rebuild\n\nMOTIVATION\n\nDestroying and rebuilding the stack instance every time provides a\n\nclean slate to each testing run. It may automatically remove a\n\nbroken instance unless it is too broken for the stack tool to destroy.\n\nAnd it removes the time it takes to create and destroy the stack instance from the feedback loop.",
      "content_length": 411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "Another benefit of this pattern is that it can reliably test the update\n\nprocess that would happen for the given stack code version in\n\nproduction.\n\nAPPLICABILITY\n\nDestroying the stack instance in the background can work well if\n\nthe stack project doesn’t tend to break and need manual\n\nintervention to fix.\n\nCONSEQUENCES\n\nSince the stack is destroyed and provisioned outside the delivery\n\nflow of the pipeline, problems may not be visible. The pipeline can be green, but the test instance may break behind the scenes. When\n\nthe next change reaches the test stage, it may take time to realize it\n\nfailed because of the background job rather than because of the\n\nchange itself.\n\nIMPLEMENTATION\n\nWhen the test stage passes, it promotes the stack project code to\n\nthe next stage. It also triggers a job to destroy and rebuild the stack\n\ninstance. When someone pushes a new change to the code, the test\n\nstage applies it to the instance as an update.\n\nYou need to decide which version of the stack code to use when\n\nrebuilding the instance. You could use the same version that has\n\njust passed the stage. An alternative is to pull the last version of\n\nthe stack code applied to the production instance. This way, each version of stack code is tested as an update to the current\n\nproduction version. Depending on how your infrastructure code",
      "content_length": 1336,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "typically flows to production, this may be a more accurate representation of the production upgrade process.\n\nRELATED PATTERNS\n\nIdeally, this pattern resembles the persistent test stack pattern (“Pattern: Persistent test stack”), providing feedback, while having\n\nthe reliability of the ephemeral test stack pattern (“Pattern:\n\nEphemeral test stack”).\n\nTest orchestration\n\nI’ve described each of the moving parts involved in testing stacks: the types of tests and validations you can apply, using test fixtures\n\nto handle dependencies, and lifecycles for test stack instances. But\n\nhow should you put these together to set up and run tests?\n\nMost teams use scripts to orchestrate their tests. Often, these are the same scripts they use to orchestrate running their stack tools. In\n\n[Link to Come], I’ll dig into these scripts, which may handle\n\nconfiguration, coordinating actions across multiple stacks, and\n\nother activities as well as testing.\n\nTest orchestration may involve:\n\nCreating test fixtures,\n\nLoading test data (more often needed for application testing than infrastructure testing),\n\nManaging the lifecycle of test stack instances,\n\nProviding parameters to the test tool,\n\nRunning the test tool,",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "Consolidating test results,\n\nCleaning up test instances, fixtures, and data.\n\nMost of these topics, such as test fixtures and stack instance\n\nlifecycle, are covered earlier in this chapter. Others, including\n\nrunning the tests and consolidating the results, depend on the\n\nparticular tool.\n\nTwo guidelines to consider for orchestrating tests are supporting\n\nlocal testing and avoiding tight coupling to pipeline tools.\n\nSupport local testing\n\nPeople working on infrastructure stack code should be able to run\n\nthe tests themselves before pushing code into the shared pipeline and environments. [Link to Come] discusses approaches to help\n\npeople work with personal stack instances on an infrastructure\n\nplatform. Doing this allows you to code and run online tests before\n\npushing changes.\n\nAs well as being able to work with personal test instances of\n\nstacks, people need to have the testing tools and other elements\n\ninvolved in running tests on their local working environment.\n\nMany teams use code-driven development environments, which\n\nautomate installing and configuring tools. You can use containers\n\n9\n\nor virtual machines for packaging development environments that can run on different types of desktop systems. Alternately, your\n\nteam could use hosted workstations (hopefully configured as\n\ncode), although these may suffer from latency, especially for\n\ndistributed teams.",
      "content_length": 1384,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "A key to making it easy for people to run tests themselves is using the same test orchestration scripts across local work and pipeline\n\nstages. Doing this ensures that tests are set up and run consistently\n\neverywhere.\n\nAvoid tight coupling with pipeline tools\n\nMany CI and pipeline orchestration tools have features or plugins for test orchestration, even configuring and running the tests for\n\nyou. While these features may seem convenient, they make it\n\ndifficult to set up and run your tests consistently outside the\n\npipeline. Mixing test and pipeline configuration can also make it\n\npainful to make changes.\n\nInstead, you should implement your test orchestration in a separate\n\nscript or tool. The test stage should call this tool, passing a\n\nminimum of configuration parameters. This approach keeps the\n\nconcerns of pipeline orchestration and test orchestration loosely\n\ncoupled.\n\nTest orchestration tools\n\nMany teams write custom scripts to orchestrate tests. These scripts\n\nare similar to or may even be the same scripts used to orchestrate\n\nstack management (as described in [Link to Come]). People use\n\nBash scripts, batch files, Ruby, Python, Make, Rake, and others I’ve never heard of.\n\nThere are a few tools available that are specifically designed to\n\norchestrate infrastructure tests. Two I know of are [Test\n\nKitchen]https://kitchen.ci/ and [Molecule]https://molecule.readthedocs.io/en/stable/. Test Kitchen",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "is an open-source product from Chef that was originally aimed at\n\ntesting Chef cookbooks. Molecule is an open-source tool designed\n\nfor testing Ansible playbooks. You can use either tool to test\n\ninfrastructure stacks, for example, using [Kitchen- Terraform]https://newcontext-oss.github.io/kitchen-terraform/.\n\nThe challenge with these tools is that they are designed with a\n\nparticular workflow in mind, and can be difficult to configure to\n\nsuit the workflow you need. Some people tweak and massage them, while others find it simpler to write their own scripts.\n\nConclusion\n\nThis chapter pulls together the topics of infrastructure stacks and\n\ntesting. These approaches and patterns may guide you in implementing automated testing for your infrastructure code,\n\nhopefully helping you to cope with some of the challenges which\n\nmake this harder than testing application code.\n\nOne of the challenges this book can’t help with is tooling for testing infrastructure. Although there are some tools available,\n\nmany of which I mention in this chapter, TDD, CI, and automated\n\ntesting are not very well established for infrastructure as of this\n\nwriting. You have a journey to discover the tools that you can use,\n\nand may need to cover gaps in the tooling with custom scripting. Hopefully, this will improve over time.\n\nThe next section of this book moves on to the next layer of\n\ninfrastructure, application runtimes. You use stacks to provision\n\nservers, clusters, and other resources for deploying and running",
      "content_length": 1509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "applications. These chapters explain how to integrate testing and pipelines for these things with testing and pipelines for stacks.\n\n1 I give the background on Foodspin in “Foodspin Example: Diverging\n\ninfrastructure”\n\n2 See “Structured data storage” for a bit more on DBaaS\n\n3 This pipeline is much simpler than what you’d use in reality. You would probably have at least one stage to test the stack and the application together (see [Link to Come]). You might also need a customer acceptance testing stage before each customer production stage. And this doesn’t include governance and approval stages, which many organizations require.\n\n4 The term https://en.wikipedia.org/wiki/Lint(software)[lint\n\n5 Awspec is an rspec-based framework for testing AWS infrastructure\n\n6 Inspec is another rspec-based framework, which tests multiple cloud platforms, as\n\nwell as other types of infrastructure, such as servers and containers.\n\n7 Terratest is a Go-based framework specific to Terraform but applicable to other\n\ntypes of infrastructure, including servers and containers.\n\n8 [Link to Come] explains how to connect stack dependencies, in [Link to Come].\n\n9 Vagrant is handy for sharing virtual machine configuration between members of a\n\nteam.",
      "content_length": 1239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "About the Author\n\nKief Morris is a consultant with ThoughtWorks, specializing in cloud, infrastructure, and agile IT operations. This gives him an\n\nopportunity to spend time with teams in a variety of industries, at companies ranging from global enterprises to early stage startups.\n\nHe enjoys working and talking with people to explore better engineering practices, architecture design principles, and organizational structures and processes. Kief ran his first online system, a bulletin board system (BBS) in Florida in the early 1990s. He later enrolled in a MSc program in computer science at\n\nthe University of Tennessee because it seemed like the easiest way to get a real Internet connection. Joining the CS department’s system administration team gave him exposure to managing\n\nhundreds of machines running a variety of Unix flavors. When the dot-com bubble began to inflate, Kief moved to London, drawn by the multicultural mixture of industries and people. He’s still there. Most of the companies Kief worked for before ThoughtWorks\n\nwere post-startups, looking to build and scale. The titles he’s been given or self-applied include Deputy Technical Director, R&D Manager, Hosting Manager, Technical Lead, Technical Architect,\n\nConsultant, and Infrastructure Engineering Practices Champion.",
      "content_length": 1300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "Colophon\n\nThe animal on the cover of Infrastructure as Code is Rüppell’s vulture (Gyps rueppellii), native to the Sahel region of Africa (a\n\ngeographic zone that serves as a transition between the Sahara Desert and the savanna). It is named in honor of a 19th-century\n\nGerman explorer and zoologist, Eduard Rüppell.\n\nIt is a large bird (with a wingspan of 7–8 feet and weighing 14–20 pounds) with mottled brown feathers and a yellowish-white neck and head. Like all vultures, this species is carnivorous and feeds\n\nalmost exclusively on carrion. They use their sharp talons and\n\nbeaks to rip meat from carcasses, and have backward-facing spines on their tongue to thoroughly scrape bones clean. While normally silent, these are very social birds who will voice a loud squealing\n\ncall at colony nesting sites or when fighting over food.\n\nThe Rüppell’s vulture is monogamous and mates for life, which can be 40–50 years long. Breeding pairs build their nests near cliffs, out of sticks lined with grass and leaves (and often use it for\n\nmultiple years). Only one egg is laid each year—by the time the next breeding season begins, the chick is just becoming independent. This vulture does not fly very fast (about 22 mph),\n\nbut will venture up to 90 miles from the nest in search of food.\n\nRüppell’s vultures are the highest-flying birds on record; there is evidence of them flying 37,000 feet above sea level, as high as\n\ncommercial aircraft. They have a special hemoglobin in their blood that allows them to absorb oxygen more efficiently at high altitudes.",
      "content_length": 1556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "This species is considered endangered and populations have been\n\nin decline. Though loss of habitat is one factor, the most serious threat is poisoning. The vulture is not even the intended target: farmers often poison livestock carcasses to retaliate against predators like lions and hyenas. As vultures identify a meal by\n\nsight and gather around it in flocks, hundreds of birds can be killed each time.\n\nMany of the animals on O’Reilly covers are endangered; all of\n\nthem are important to the world. To learn more about how you can help, go to animals.oreilly.com.\n\nThe cover image is from Cassell’s Natural History. The cover fonts are URW Typewriter and Guardian Sans. The text font is\n\nAdobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "content_length": 801,
      "extraction_method": "Unstructured"
    }
  ]
}